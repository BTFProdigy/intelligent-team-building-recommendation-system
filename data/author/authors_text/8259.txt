Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1017?1024
Manchester, August 2008
Bayesian Semi-Supervised Chinese Word Segmentation
for Statistical Machine Translation
Jia Xu
?
, Jianfeng Gao
?
, Kristina Toutanova
?
, Hermann Ney
?
Computer Science 6
?
Microsoft Corporation
?
RWTH Aachen University One Microsoft Way
D-52056 Aachen, Germany Redmond, WA 98052, USA
{xujia,ney}@cs.rwth-aachen.de {jfgao,kristout}@microsoft.com
Abstract
Words in Chinese text are not naturally
separated by delimiters, which poses a
challenge to standard machine translation
(MT) systems. In MT, the widely used
approach is to apply a Chinese word seg-
menter trained from manually annotated
data, using a fixed lexicon. Such word
segmentation is not necessarily optimal
for translation. We propose a Bayesian
semi-supervised Chinese word segmenta-
tion model which uses both monolingual
and bilingual information to derive a seg-
mentation suitable for MT. Experiments
show that our method improves a state-of-
the-art MT system in a small and a large
data environment.
1 Introduction
Chinese sentences are written in the form of a se-
quence of Chinese characters, and words are not
separated by white spaces. This is different from
most European languages and poses difficulty in
many natural language processing tasks, such as
machine translation.
It is difficult to define ?correct? Chinese word
segmentation (CWS) and various definitions have
been proposed. In this work, we explore the idea
that the best segmentation depends on the task, and
concentrate on developing a CWS method for MT,
which leads to better translation performance.
The common solution in Chinese-to-English
translation has been to segment the Chinese text
using an off-the-shelf CWS method, and to apply
a standard translation model given the fixed seg-
mentation. The most widely applied method for
MT is unigram segmentation, such as segmenta-
tion using the LDC (LDC, 2003) tool, which re-
quires a manual lexicon containing a list of Chi-
nese words and their frequencies. The lexicon and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
frequencies are obtained using manually annotated
data. This method is sub-optimal for MT. For ex-
ample, ?(paper) and ](card) can be two words
or composed into one word ?](cards). Since ?
]does not exist in the manual lexicon, it cannot
be generated by this method.
In addition to unigram segmentation, other
methods have been proposed. For example, (Gao
et al, 2005) described an adaptive CWS system,
and (Andrew, 2006) employed a conditional ran-
dom field model for sequence segmentation. How-
ever, these methods are not specifically devel-
oped for the MT application, and significant im-
provements in translation performance need to be
shown.
In (Xu et al, 2004) and (Xu et al, 2005),
word segmentations are integrated into MT sys-
tems during model training and translation. We re-
fine the method in training using a Bayesian semi-
supervised CWS approach motivated by (Goldwa-
ter et al, 2006). We describe a generative model
which consists of a word model and two alignment
models, representing the monolingual and bilin-
gual information, respectively. In our methods, we
first segment Chinese text using a unigram seg-
menter, and then learn new word types and word
distributions, which are suitable for MT.
Our experiments on both large (NIST) and small
(IWSLT) data tracks of Chinese-to-English trans-
lation show that our method improves the per-
formance of a state-of-the-art machine translation
system.
2 Review of the Baseline System
2.1 Word segmentation
In statistical machine translation, we are given a
Chinese sentence in characters c
K
1
= c
1
. . . c
K
which is to be translated into an English sentence
e
I
1
= e
1
. . . e
I
. In order to obtain a more adequate
mapping between Chinese and English words, c
K
1
is usually segmented into words f
J
1
= f
1
. . . f
J
in
preprocessing.
In our baseline system, we apply the commonly
1017
used unigram model to generate the segmenta-
tion. Given a manually compiled lexicon contain-
ing words and their relative frequencies P
s
(f
?
j
),
the best segmentation f
J
1
is the one that maximizes
the joint probability of all words in the sentence,
with the assumption that words are independent of
each other
1
:
f
J
1
= argmax
f
?
J
?
1
Pr(f
?
J
?
1
|c
K
1
)
? argmax
f
?
J
?
1
J
?
?
j=1
P
s
(f
?
j
),
where the maximization is taken over Chinese
word sequences whose character sequence is c
K
1
.
2.2 Translation system
Once we have segmented the Chinese sentences
into words, we train standard alignment models
in both directions with GIZA++ (Och and Ney,
2002) using models of IBM-1 (Brown et al, 1993),
HMM (Vogel et al, 1996) and IBM-4 (Brown et
al., 1993).
Our MT system uses a phrase-based decoder
and the log-linear model described in (Zens and
Ney, 2004). Features in the log-linear model in-
clude translation models in two directions, a lan-
guage model, a distortion model and a sentence
length penalty. The feature weights are tuned on
the development set using a downhill simplex al-
gorithm (Press et al, 2002). The language model
is a statistical ngram model estimated using modi-
fied Kneser-Ney smoothing.
3 Unigram Dirichlet Process Model for
CWS
The simplest version of our model is based on a
unigram Dirichlet Process (DP) model, using only
monolingual information. Different from a stan-
dard unigram model for CWS, our model can in-
troduce new Chinese word types and learn word
distributions automatically from unlabeled data.
According to this model, a corpus of Chinese
words f
1
, . . . f
m
, . . . , f
M
is generated via:
G|?, P
0
? DP (?, P
0
)
f
m
|G ? G
where G is a distribution over words drawn from a
Dirichlet Process prior with base measure P
0
and
concentration parameter ?.
We never explicitly estimate G but instead
integrate over its possible values and perform
Bayesian inference. It is easy to compute the
1
The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol P (?).
probability of a Chinese word given a set of al-
ready generated words, while integrating over G.
This is done by casting Chinese word generation
as a Chinese restaurant process (CRP) (Aldous,
1985), i.e. a restaurant with an infinite num-
ber of tables (approximately corresponding to Chi-
nese word types), each table with infinite number
of seats (approximately corresponding to Chinese
word frequencies).
The Dirichlet Process model can be viewed in-
tuitively as a cache model (Goldwater et al, 2006).
Each word f
j
in the corpus is either retrieved from
a cache or generated anew given the previously ob-
served words f
?j
:
P (f
j
|f
?j
) =
N(f
j
)
+
?P
0
(f
j
)
N + ?
, (1)
whereN(f
j
) is the number of Chinese words f
j
in
the previous context. N is the total number of Chi-
nese words, P
0
is the base probability over words,
and ? influences the probability of introducing a
new word at each step and controls the size of the
lexicon. The probability of generating a word from
the cache increases as more instances of that word
are seen.
For the base distribution P
0
, which governs the
generation of new words, we use the following dis-
tribution (called the spelling model):
P
0
(f) = P (L)P
0
(f |L)
=
?
L
L!
e
??
u
L
(2)
where
1
u
is the number of characters in the docu-
ment, i.e. character vocabulary size, and L is the
number of Chinese characters of word f . We note
that this is a Poisson distribution on word length
and a unigram distribution on characters given the
length. We used ? = 2 and ? = 0.3 in our experi-
ments.
4 CWS Model for MT
As a solution to the problems with the conventional
approach to CWS mentioned in Section 1, we pro-
pose a generative model for CWS in Section 4.1,
and then extend the model to a more general but
deficient model, similar to a maximum entropy
model in which most features are derived from the
submodels of the generative model.
4.1 Generative Model
The generative model assume that a corpus of par-
allel sentences (c
1
K
,e
1
I
) is generated along with a
hidden sequence of Chinese words f
1
J
and a hid-
den word alignment b
1
I
for every sentence. The
alignment indicates the aligned Chinese word f
b
i
for each English word e
i
, where f
0
indicates a spe-
cial null word as in the IBM models.
1018
Without assuming any special form for the prob-
ability of a sentence pair along with hidden vari-
ables, we can factor it into a monolingual Chi-
nese sentence probability and a bilingual transla-
tion probability as follows:
Pr(c
1
K
, e
1
I
, f
1
J
, b
1
I
)
=Pr(c
K
1
, f
J
1
)Pr(e
I
1
, b
I
1
|f
J
1
)
=Pr(f
J
1
)?(f
1
J
, c
1
K
)Pr(e
I
1
, b
I
1
|f
J
1
),
where ?(f
J
1
, c
K
1
) is 1 if the characters of the se-
quence of words f
1
J
are c
1
K
, and to 0 other-
wise. We can drop the conditioning on c
1
K
in
Pr(e
I
1
, b
I
1
|f
J
1
), because the characters are deter-
ministic given the words.
The joint probability of the observations
(c
1
K
, e
I
1
) can be obtained by summing over all
possible values of the hidden variables f
J
1
and b
I
1
.
In Sections 4.1.1 and 4.1.2, we will describe
the modeling assumptions behind the monolingual
Chinese sentence model and the translation model,
respectively.
4.1.1 Monolingual Chinese sentence model
We use the Dirichlet Process unigram word
model introduced in section 3. In this model, the
parameters of a distribution over words G are first
drawn from the Dirichlet prior DP (?, P
0
). Words
are then independently generated according to G.
The probability of a sequence of Chinese words in
a sentence is thus:
Pr(f
J
1
) ?
J
?
j=1
P (f
j
|G) (3)
4.1.2 Translation model
We employ the Dirichlet Process inverse IBM
model 1 to generate English words and alignment
given the Chinese words. In this model, for every
Chinese word f (including the null word), a distri-
bution over English words G
f
is first drawn from
a Dirichlet Process prior DP (?, P
0
(e)), where
P
0
(e) we used the empirical distribution over En-
glish words in the parallel data. Then, given these
parameters, the probability of an English sentence
and alignment given a Chinese sentence (sequence
of words) is given by:
P (e
I
1
, b
I
1
|f
J
1
, G
f
) =
I
?
i=1
1
J + 1
P (e
i
|G
f
b
i
)
This is the same model form as inverse IBM
model 1, except we have placed Dirichlet Process
priors on the Chinese-word specific distributions
over English words.
2
2
f
b
i
is the Chinese word aligned to e
i
and G
f
b
i
is the
distribution over English words conditioned on the word f
b
i
.
Similarly, e
a
j
is the English word aligned to f
j
in the other di-
rection and G
e
a
j
is the distribution over Chinese words con-
ditioned on e
a
j
.
In practice, we observed that using a word-
alignment model in one direction is not sufficient.
We then added a factor to our model which in-
cludes word alignment in the other direction , i.e. a
Dirichlet Process IBM model 1. We ignore the de-
tailed description here, because the calculation is
the same as that of the inverse IBM model 1. Ac-
cording to this model, for every English word e (in-
cluding the null word), a distribution over Chinese
words G
e
is first drawn from a Dirichlet Process
prior DP (?, P
0
(f)). Here, for the base distribu-
tion P
0
(f) we used the same spelling model as for
the monolingual unigram Dirichlet Process prior.
The probability of a sequence of Chinese words
f
J
1
and a word alignment a
J
1
given a sequence of
English words e
I
1
is then:
P (f
J
1
, a
J
1
|e
I
1
, G
e
) =
J
?
j=1
1
I + 1
P (f
j
|G
e
a
j
)
4.2 Final Model
We put the monolingual model and the transla-
tion models in both directions together into a sin-
gle model, where each of the component models
is weighted by a scaling factor. This is similar to
a maximum entropy model. We fit the weights of
the sub-models on a development set by maximiz-
ing the BLEU score of the final translation.
P (c
K
1
, e
I
1
, f
J
1
, a
J
1
, b
I
1
) (4)
?
1
Z
P (f
J
1
)
?
1
? P (e
I
1
, b
I
1
|f
J
1
)
?
2
?P (f
J
1
, a
J
1
|e
I
1
)
?
3
,
where Z is the normalization factor.
In practice we do not re-normalize the proba-
bilities and our model is thus deficient because it
does not sum to 1 over valid observations. How-
ever, we found the model work very good in our
experiments. Similar deficient models have been
used very successfully before, for example, in the
IBM models 3?6 and in the unsupervised grammar
induction model of (Klein and Manning, 2002).
5 Gibbs Sampling Training
It is generally impossible to find the most likely
segmentation according to our Bayesian model us-
ing exact inference, because the hidden variables
do not allow exact computation of the integrals.
Nonetheless, it is possible to define algorithms us-
ing Markov chain Monte Carlo (MCMC) that pro-
duce a stream of samples from the posterior dis-
tribution of the hidden variables given the obser-
vations. We applied the Gibbs sampler (Geman
and Geman, 1984) ? one of the simplest MCMC
methods, in which transitions between states of the
1019
Figure 1: Case I, transition from a no-boundary to
a boundary state, f to f
?
f
??
.
Figure 2: Case II, transition from a boundary to a
no-boundary state, f
?
f
??
to f .
Markov chain result from sampling each compo-
nent of the state conditioned on the current value
of all other variables.
In our problem, the observations are D =
(d
1
, ..d
n
, .., d
N
), where d
n
=(c
K
1
, e
I
1
) indicates a
bilingual sentence pair, the hidden variables are the
word segmentations f
J
1
and the alignments in two
directions a
J
1
and b
I
1
.
To perform Gibbs sampling, we start with
an initial word segmentation and initial word
alignments, and iteratively re-sample the word-
segmentation and alignments according to our
model of Equation 4.
Note that for efficiency, we only allow limited
modifications to the initial word alignments. Thus
we only use models derived from IBM-1 (instead
of IBM-4) for comparing different word segmenta-
tions. On the other hand, re-sampling the segmen-
tation causes re-linking alignment points to parts
or groups of the original words.
Hence, we organize our sampling process
around possible word boundaries. For each char-
acter c
k
in each sentence, we consider two alterna-
tive segmentations: c
k
+
indicates the segmentation
where there is a boundary after c
k
and c
k
?
indi-
cates the segmentation where there is no boundary
after c
k
, keeping all other boundaries fixed. Let f
denote the single word spanning character c
k
when
there is no boundary after it, and f
?
,f
??
denote the
two adjacent words resulting if there is a bound-
ary: f
?
includes c
k
and f
??
starts just to the right,
with character c
k+1
. The introduction of f
?
and
f
??
leads to M new possible alignments in the E-
to-C direction b
+
k1
, . . . , b
+
kM
, such as in Figure 1.
Together with the boundary vs no-boundary state
at each character position, we re-sample a set of
alignment links between English words and any of
the Chinese words f ,f
?
, and f
??
, keeping all other
word alignments in the sentence pair fixed. (See
Figures 1 and 2.)
Table 1: General Algorithm of GS for CWS.
Input: D with an initial segmentation and alignments
Output: D with sampled segmentation and alignments
for n = 1 to
?
N
for k = 1 to K that c
k
? d
n
Create M+1 candidates, cba
+
k,m
and cba
?
k
, where
cba
+
k,m
: there is a word boundary after c
k
cba
?
k
: there is no word boundary after c
k
Compute probabilities
P (cba
+
k,m
|dh
nk
?
)
P (cba
?
k
|dh
nk
?
)
Sample boundary and relevant alignments
Update counts
Thus at each step in the Gibbs sampler, we con-
sider a set of alternatives for the boundary after
c
k
and relevant alignment links, keeping all other
hidden variables fixed. At each step, we need to
compute the probability of each of the alternatives,
given the fixed values of the other hidden variables.
We introduce some notation to make the presen-
tation easier. For every position k in sentence pair
n, we denote by dh
nk
?
the observations and hid-
den variables for all sentences other than sentence
n, and the observations and hidden variables in-
side sentence n, not involving character position
c
k
. The fixed variables inside the sentence are the
words not neighboring position k, and the align-
ments in both directions to these words.
In the process of sampling, we consider a set
of alternatives: segmentation c
k
+
along with the
product space of relevant alignments in both direc-
tions b
+
k1
, . . . , b
+
kM
, and a
+
k
, and segmentation c
?
k
along with relevant alignments b
k
?
and a
?
k
. For
brevity, we denote these alternatives by cba
k,m
+
and cba
k
?
.
We describe how we derive the set of alterna-
tives in section 5.2 and how we compute their
probabilities in section 5.1.
Table 1 shows schematically one iteration of
Gibbs sampling through the whole training corpus
of parallel sentences, where
?
N is the number of
parallel sentences.
5.1 Computing probabilities of alternatives
For the Gibbs sampling algorithm in Table 1, we
need to compute the probability of each alternative
segmentation/alignments, given the fixed values of
the rest of the data dh
nk
?
. The probability of the
hidden variables in the alternatives is proportional
to the joint probability of the hidden variables and
observations, and thus it is sufficient to compute
the probability of the latter. We compute these
probabilities using the Chinese restaurant process
sampling scheme for the Dirichlet Process, thus in-
1020
tegrating over all of the possible values of the dis-
tributions G, G
f
and G
e
.
Let cba
k
denote an alternative hypothesis in-
cluding boundary or no boundary at position k,
and relevant alignments to English words in both
directions of the one or two Chinese words result-
ing from the segmentation at k. The probability of
this configuration given by our model is:
P (cba
k
|dh
nk
?
) ? P
m
(cba
k
|dh
nk
?
)
?
1
(5)
?P
ef
(cba
k
|dh
nk
?
)
?
2
? P
fe
(cba
k
|dh
nk
?
)
?
3
,
where P
m
(cba
k
|dh
nk
?
) is the monolingual
word probability, and P
fe
(cba
k
|dh
nk
?
) and
P
ef
(cba
k
|dh
nk
?
) are the translation probabilities
in the two directions.
We now describe the computations of each of
the component probabilities.
5.1.1 Word model probability
The word model probability P
m
(cab
k
|dh
nk
?
)
in Equation 5 is derived from Equations 3 and 1:
There are two cases, depending on whether the
hypothesis specifies that there is a boundary after
character c
k
, in which case we need the probabili-
ties of the two resulting words f
?
, and f
??
, or there
is no boundary, in which case we need the proba-
bility of the single word f . (See the initial states in
Figures 1 and 2, respectively.)
Let N denote the total number of word tokens
in the rest of the corpus dh
nk
?
, and N(f) denote
the number of instances of word f in dh
nk
?
. The
probabilities in the two cases are
P
m
(c
+
k
|dh
nk
?
) ?
N(f
?
) + ?P
0
(f
?
)
N + ?
?
N(f
??
) + ?P
0
(f
??
)
N + ?
P
m
(c
?
k
|dh
nk
?
) ?
N(f) + ?P
0
(f)
N + ?
Here P
0
(f) is computed using Equation 2.
5.1.2 Translation model probability
The translation model probabilities depend on
whether or not there is a segmentation boundary
at c
k
and which English words are aligned to the
relevant Chinese words.
In the first case, assume that there is a word
boundary in cab
k
, and that English words {e
?
} are
aligned to f
?
and words {e
??
} are aligned to f
??
in
the E-to-C direction according to the alignment b
k
,
and that f
?
is aligned to e
?
?
and f
??
is aligned to e
?
??
in the C-to-E direction according to the alignment
a
k
(see the initial state in Figure 1). Here we over-
loaded notations and use b
k
and a
k
to indicate the
alignments of the relevant Chinese words at posi-
tion k to any English words. Let I denote the total
number of English words in the sentence, and J+1
denote the number of Chinese words according to
this segmentation. We also denote the total num-
ber of English words aligned to either f
?
or f
??
in
the E-to-C direction by P .
The translation model probability in the E-to-C
direction is thus:
P
ef
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
1
(J + 2
)
P
?
e
?
P (e
?
|f
?
, dh
nk
?
)
?
e
??
P (e
??
|f
??
, dh
nk
?
)
Here we compute P (e|f, dh
nk
?
) as:
P (e|f, dh
nk
?
) =
N(e, f) + ?P
0
(e)
N(f) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
The translation probability in the other direction
is similarly computed as:
P
fe
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
(
1
I + 1
)
2
P (f
?
|e
?
, dh
nk
?
)P (f
??
|e
?
, dh
nk
?
)
And P (f |e, dh
nk
?
) is computed as:
P (f |e, dh
nk
?
) =
N(f, e) + ?P
0
(f)
N(e) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
In the second case, if the hypothesis in evalua-
tion does not have a word boundary at position k,
the total number of Chinese words would be one
less, i.e. J instead of J +1 in the equations above,
and there would be a single set of English words
aligned to the word f in the E-to-C direction, and a
single word e
?
aligned to f in the C-to-E direction
(see the initial state in Figure 2. The probability of
this hypothesis is computed analogously.
5.2 Determining the set of alternative
hypotheses
As mentioned earlier, we consider alternative
alignments which deviate minimally from the cur-
rent alignments, and which satisfy the constraints
of the IBM model 1 in both directions. In order
to describe the set of alternatives, we consider two
cases, depending on whether there is a boundary at
the current character before sampling at position k.
Case 1. There was no boundary at c
k
in the previ-
ous state (see Figure 1).
1021
If there is no boundary at c
k
, there is a sin-
gle word f spanning that position. We denote by
{e} the set of English words aligned to f at that
state in the E-to-C direction and by e
?
the En-
glish word aligned to f in the C-to-E direction.
Since every state we consider satisfies the IBM
one-to-many constraints, there is exactly one En-
glish word aligned to f in the C-to-E direction and
the words {e} have no other words aligned to them
in the E-to-C direction.
In this case, we consider as hypothesis cba
k
?
the same segmentation and alignment as in the pre-
vious state. (see Table 1 for an overview of the
alternative hypotheses.)
We consider M different hypotheses which in-
clude a boundary at k in this case, where M de-
pends on the number of words {e} aligned to f
in the previous state. Because we are breaking
the word f into two words f
?
and f
??
by placing
a boundary at c
k
, we need to re-align the words
{e} to either f
?
or f
??
. Additionally we need to
align f
?
and f
??
to English words in the C-to-E
direction. The number of different hypotheses is
equal to 2
P
where P = |{e}|. These alternatives
arise by considering that each of the words in {e}
needs to align to either f
?
or f
??
, and there are 2
P
combinations of these alignments. For example, if
{e} = {e
1
, e
2
}, after splitting the word f there are
four possible alignments, illustrated in Figure 1:
I. (f
?
, e
1
) and (f
??
, e
2
), II. (f
?
, e
2
) and (f
??
, e
1
),
III. (f
?
, e
1
) and (f
?
, e
2
), IV. (f
??
, e
1
) and (f
??
, e
2
).
For the alignment a
k
in the C-to-E direction, we
consider only one option, in which both resulting
words f
?
and f
??
align to e
?
. These alternatives
form cba
k,m
+
in Table 1.
Case 2. There was a boundary at c
k
in the previous
state (see Figure 2).
In this case, for the hypotheses c
+
k
we consider
only one alternative, which is exactly the same as
the assignment of segmentation and alignments in
the previous state. Thus we have M = 1 in Table
1.
Let f
?
and f
??
denote the two words at position
k in the previous state, {e
?
} and {e
??
} denote the
sets of English words aligned to them in the E-to-C
direction, respectively, and e
?
?
and e
?
??
denote the
English words aligned to f
?
and f
??
in the C-to-E
direction.
We consider only one hypothesis cba
k
?
where
there is no boundary at c
k
. In this hypothesis, there
is a single word f = f
?
f
??
spanning position k,
and all words {e
?
} ? {e
??
} align to f in the E-to-
C direction. For the C-to-E direction we consider
the ?better? of the alignments (f, e
?
?
) and (f, e
??
?
)
where the better alignment is defined as the one
having higher probability according to the C-to-E
word translation probabilities.
Table 2: Complete Algorithm of Gibbs Sampler
for CWS including Alignment Models.
Input: D, F
0
Output: A
T
, F
T
for t = 1 to T
Run GIZA++ on (D,F
t?1
) to obtain A
t
Run GS on (D,F
t?1
, A
t
) to obtain F
t
5.3 Complete segmentation algorithm
So far, we have described how we re-sample word
segmentation and alignments according to our
model, starting from an initial segmentation and
alignments from GIZA++. Putting these pieces to-
gether, the algorithm is summarized in Table 1.
We found that we can further improve perfor-
mance by repeatedly aligning the corpus using
GIZA++, after deriving a new segmentation us-
ing our model. The complete algorithm which in-
cludes this step is shown in Table 2, where F
t
in-
dicates the word segmentation at iteration t and A
t
denotes the GIZA++ corpus alignment in both di-
rections. The GS re-segmentation step is done ac-
cording to the algorithm in Table 1.
Using this algorithm, we obtain a new segmen-
tation of the Chinese data and train the translation
models using this segmentation as in the baseline
MT system. To segment the test data for transla-
tion, we use a unigram model, trained with maxi-
mum likelihood estimation off of the final segmen-
tation of the training corpus F
T
.
6 Translation Experiments
We performed experiments using our models for
CWS on a large and a small data track. We evalu-
ated performance by measuring WER (word error
rate), PER (position-independent word error rate),
BLEU (Papineni et al, 2002) and TER (translation
error rate) (Snover et al, 2006) using multiple ref-
erences.
6.1 Translation Task: Large Track NIST
We first report the experiments using our mono-
lingual unigram Dirichlet Process model for word
segmentation on the NIST machine translation task
(NIST, 2005). Because of the computational re-
quirements, we only employed the monolingual
word model for this large data track, i.e. the fea-
ture weights were ?
1
= 1, ?
2
= 0, ?
3
= 0. There-
fore, no alignment information needs to be main-
tained in this case.
The bilingual training corpus is a superset of
corpora in the news domain collected from differ-
ent sources.
We took LDC (LDC, 2003) as a baseline CWS
method (Base). As shown in Table 3, the training
corpus in each language contains more than two
million sentences. There are 56 million Chinese
1022
Table 3: Statistics of corpora in task NIST.
Data Sents. Words[K] Voc.[K]
Cn. En. Cn. En.
Chars 2M 56M 49.5M 65.4 211
Base 39.2M 95.7
GS 40.5M 95.4
02 878 23.1 28.0 2.04 4.34
03 919 24.6 29.2 2.21 4.91
04 1788 49.8 60.7 2.61 6.71
05 1082 30.8 34.2 2.30 5.39
Table 4: Translation performance [% BLEU] with
the baseline(LDC) and GS method on NIST.
MT-eval LDC(Base) GS
2005 32.85 33.26
2002 34.32 34.36
2003 33.41 33.75
2004 33.74 34.06
characters. The LDC and GS word segmentation
methods generated 39.2 and 40.5 million running
words, respectively.
The scaling factors of the translation models de-
scribed in Section 2.2 were optimized on the devel-
opment corpus, MT-eval 05 with 1082 sentences.
The resulting systems were evaluated on the test
corpora MT-eval 02-04. For convenience, we only
list the statistics of the first English reference.
Starting from the baseline LDC output as ini-
tial word segmentation, we performed Gibbs sam-
pling (GS) of word segmentations using 30 itera-
tions over the Chinese training corpus.
Since BLEU is the official NIST measure of
translation performance, we show the translation
results measured in BLEU score only. As shown
in Table 4, on the development data MT-eval 05,
the BLEU score was improved by 0.4% absolute or
more than 1% relative using GS. Similarly, the ab-
solute BLEU scores are also improved on all other
test sets, in the range of 0.04% to 0.4%.
We can see that even a monolingual semi-
supervised word segmentation method can outper-
form a supervised one in MT, probably because the
training/test corpora contain many unknown words
and words have different frequencies in our MT
data from they do in the manually labeled CWS
data.
6.2 Translation Task: Small Track IWSLT
We evaluate our full model, using both monolin-
gual and bilingual information, on the IWSLT data.
As shown in Table 5, the Chinese training
corpus was segmented using the unigram seg-
menter (Base) described in Section 2.1 and our GS
method. Since the unigram segmenter performs
better in our experiments, we took it as the base-
line and the method for initialization in later ex-
periments. We see that the vocabulary size of the
Chinese training corpus was reduced more signif-
icantly by GS than by the baseline method, even
Table 5: Statistics of corpora in task IWSLT.
Test Sents. Words[K] Voc.
Cn. En. Cn. En.
Chars 42.9K 520 420 2780 9930
Base 394 8800
GS 398 6230
Dev2 500 3.74 3.82 1004 821
Dev3 506 4.01 3.90 980 820
Eval 489 3.39 3.72 904 810
Table 6: Translation performance with different
CWS methods on IWSLT[%].
Test Method WER PER BLEU TER
Dev2 Unigram (Base) 38.2 31.2 55.4 37.0
GS 36.8 30.0 56.6 35.5
Dev3 Unigram (Base) 33.5 27.5 60.4 32.1
GS 32.3 26.6 61.0 31.4
Eval Characters 49.3 41.8 35.4 47.5
LDC 46.2 40.0 39.2 45.0
ICT 45.9 40.4 40.1 44.9
Unigram (Base) 46.8 40.2 41.6 45.6
9-gram 46.9 40.4 40.1 45.4
GS 45.9 40.0 41.6 44.8
though they resulted in a similar number of run-
ning words. This shows that the distribution of
Chinese words is more concentrated when using
GS.
The parameter optimizations were performed on
the Dev2 data with 500 sentences, and evaluations
were done both on Dev3 and on Eval data, i.e. the
evaluation corpus of (IWSLT, 2007).
The model weights ? of GS from Section 5.1.2
were optimized using the Powell (Press et al,
2002) algorithm with respect to the BLEU score.
We obtained ?
1
= 1.4, ?
2
= 1 and ?
3
= 0.8 as
optimal values and T = 4 as the optimal number
of iterations of re-alignment with GIZA++.
For a fair comparison, we evaluated on various
CWS methods including translation on characters
, LDC (LDC, 2003), ICT (Zhang et al, 2003), uni-
gram, 9-gram and GS. Improvements using GS can
be seen in Table 6. Under all test sets and evalua-
tion criteria, GS outperforms the baseline method.
The absolute WER decreases with 1.2% on Dev3
and with 1.1% on Eval data over baseline.
We compared the translation outputs using GS
with the baseline method. On the Eval data, 196
sentences are different out of 489 lines, where 64
sentences from GS are better, 33 sentences are
worse, and the rests have similar translation qual-
ities. Table 7 shows two examples from the Eval
corpus. We list segmentations produced by the
baseline and GS methods, as well as the transla-
tions corresponding to these segmentations. The
GS method generates better translation results than
the baseline method in these cases.
1023
Table 7: Segmentation and translation outputs with
baseline and GS methods.
a) Baseline ? ?4 m?
do you have a ?
GS ??4m?
do you have a shorter way ?
REF is there a shorter route ?
b) Baseline >????
please show me the in .
GS >????
please show me the total price .
REF can you tell me the total amount ?
7 Conclusion and future work
We showed that it is possible to learn Chinese word
boundaries such that the translation performance
of Chinese-to-English MT systems is improved.
We presented a Bayesian generative model for
parallel Chinese-English sentences which uses
word segmentation and alignment as hidden vari-
ables, and incorporates both monolingual and
bilingual information to derive a segmentation
suitable for MT.
Starting with an initial word segmentation, our
method learns both new Chinese words and dis-
tributions for these words. In a large and a small
data environment, our method outperformed the
standard Chinese word segmentation approach in
terms of the Chinese to English translation quality.
In future work, we plan to enrich our monolingual
and bilingual models to better represent the true
distribution of the data.
8 Acknowledgments
Jia Xu conducted this research during her intern-
ship at Microsoft Research. This material is also
partly based upon work supported by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023.
References
Aldous, D. 1985. Exchangeability and related topics.
In
?
Ecole d??et?e de probabilit?es de Saint-Flour, XIII-
1983, pages 1?198, Springer, Berlin.
Andrew, G. 2006. A hybrid markov/semi-markov con-
ditional random field for sequence segmentation. In
Proceedings of EMNLP, Sydney, July.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
Gao, J., M. Li, A. Wu, and C. Huang. 2005. Chi-
nese word segmentation and named entity recogni-
tion: A pragmatic approach. Computational Lin-
guistics, 31(4).
Goldwater, S., T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of Coling/ACL, Sydney,
July.
IWSLT. 2007. International workshop on
spoken language translation home page.
http://www.slt.atr.jp/IWSLT2007.
Klein, D. and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of ACL, pages 128?135.
LDC. 2003. Linguistic data consor-
tium Chinese resource home page.
http://www.ldc.upenn.edu/Projects/Chinese.
NIST. 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
Och, F. J. and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302,
Philadelphia, PA, July.
Papineni, K. A., S. Roukos, T. W., and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, July.
Press, W. H., S. A. Teukolsky, W. T. Vetterling, and
B. P. Flannery. 2002. Numerical Recipes in C++.
Cambridge University Press, Cambridge, UK.
Snover, M., B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA, pages 223?231, Cambridge, MA, August.
Vogel, S., H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of COLING.
Xu, J., R. Zens, and H. Ney. 2004. Do we need
Chinese word segmentation for statistical machine
translation? In Proceedings of the SIGHAN Work-
shop on Chinese Language Learning, pages 122?
128, Barcelona, Spain, July.
Xu, J., E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proceedings of IWSLT, pages
141?147, Pittsburgh, PA, October.
Zens, R. and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceedings
of HLT/NAACL, Boston, MA, May.
Zhang, H., H. Yu, D. Xiong, and Q. Liu. 2003.
HHMM-based Chinese lexical analyzer ICTCLAS.
In Proceedings of the Second SIGHAN Workshop on
Chinese Language Learning, pages 184?187, July.
1024
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1202?1211,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Joint Optimization for Machine Translation System Combination 
 
 
Xiaodong He 
Microsoft Research  
One Microsoft Way, Redmond, WA 
xiaohe@microsoft.com 
Kristina Toutanova 
Microsoft Research  
One Microsoft Way, Redmond, WA 
kristout@microsoft.com 
  
 
Abstract 
System combination has emerged as a 
powerful method for machine translation 
(MT). This paper pursues a joint optimization 
strategy for combining outputs from multiple 
MT systems, where word alignment, ordering, 
and lexical selection decisions are made 
jointly according to a set of feature functions 
combined in a single log-linear model. The 
decoding algorithm is described in detail and a 
set of new features that support this joint 
decoding approach is proposed. The approach 
is evaluated in comparison to state-of-the-art 
confusion-network-based system combination 
methods using equivalent features and shown 
to outperform them significantly.   
1 Introduction 
System combination for machine translation 
(MT) has emerged as a powerful method of 
combining the strengths of multiple MT systems 
and achieving results which surpass those of 
each individual system (e.g. Bangalore, et. al., 
2001, Matusov, et. al., 2006, Rosti, et. al., 
2007a). Most state-of-the-art system combination 
methods are based on constructing a confusion 
network (CN) from several input translation 
hypotheses, and choosing the best output from 
the CN based on several scoring functions (e.g. 
Rosti et. al., 2007a, He et. al., 2008, Matusov et 
al. 2008). Confusion networks allow word-level 
system combination, which was shown to 
outperform sentence re-ranking methods and 
phrase-level combination (Rosti, et. al. 2007a). 
We will review confusion-network-based 
system combination with the help of the 
examples in Figures 1 and 2. Figure 1 shows 
translation hypotheses from three Chinese-to-
English MT systems. The general idea is to 
combine hypotheses in a representation such as 
the ones in Figure 2, where for each word 
position there is a set of possible words, shown 
in columns.1The final output is determined by 
choosing one word from each column, which can 
be a real word or the empty word ?. For example, 
the CN in Figure 2a) can generate eight distinct 
sequences of words, including e.g. ?she bought 
the Jeep? and ?she bought the SUV Jeep?. The 
choice is performed to maximize a scoring 
function using a set of features and a log-linear 
model (Matusov, et. al 2006, Rosti, et al 2007a). 
We can view a confusion network as an 
ordered sequence of columns (correspondence 
sets). Each word from each input hypothesis 
belongs to exactly one correspondence set. Each 
correspondence set contains at most one word 
from each input hypothesis and contributes 
exactly one of its words (including the possible 
?) to the final output. Final words are output in 
the order of correspondence sets. In order to 
construct such a representation, we need to solve 
the following two sub-problems:  arrange words 
from all input hypotheses into correspondence 
sets (alignment problem) and order 
correspondence sets (ordering problem).  After 
constructing the confusion network we need to 
solve a third sub-problem: decide which words to 
output from each correspondence set (lexical 
choice problem). 
In current state-of-the-art approaches, the 
construction of the confusion network is 
performed as follows: first, a backbone 
hypothesis is selected. The backbone hypothesis 
determines the order of words in the final system 
output, and guides word-level alignments for 
construction of columns of possible words at 
each position. Let us assume that for our 
example in Figure 1, the second hypothesis is 
selected as a backbone. All other hypotheses are 
aligned to the backbone such that these 
alignments are one-to-one; empty words are 
inserted where necessary to make one-to-one 
                                                 
1 This representation is alternative to directed acyclic 
graph representations of confusion networks. 
1202
alignment possible. Words in all hypotheses are 
sorted by the position of the backbone word they 
align to and the confusion network is determined.  
It is clear that the quality of selection of the 
backbone and alignments has a large impact on 
the performance, because the word order is 
determined by the backbone, and the set of 
possible words at each position is determined by 
alignment. Since the space of possible 
alignments is extremely large, approximate and 
heuristic techniques have been employed to 
derive them. In pair-wise alignment, each 
hypothesis is aligned to the backbone in turn, 
with separate processing to combine the multiple 
alignments. Several models have been used for 
pair-wise alignment, starting with TER and 
proceeding with more sophisticated techniques 
such as HMM models, ITG, and IHMM (Rosti 
et. al 2007a, Matusov et al2008, Krakos et al 
2008, He et al 2008). A major problem with 
such methods is that each hypothesis is aligned 
to the backbone independently, leading to sub-
optimal behavior. For example, suppose that we 
use a state-of-the-art word alignment model for 
pairs of hypotheses, such as the IHMM. Figure 1 
shows likely alignment links between every pair 
of hypotheses. If Hypothesis 1 is aligned to 
Hypothesis 2 (the backbone), Jeep is likely to 
align to SUV because they express similar 
Chinese content. Hypothesis 3 is separately 
aligned to the backbone and since the alignment 
is constrained to be one-to-one, SUV is aligned to 
SUV and Jeep to an empty word which is 
inserted after SUV. The network in Figure 2a) is 
the result of this process. An undesirable 
property of this CN is that the two instances of 
Jeep are placed in separate columns and cannot 
vote to reinforce each other. 
Incremental alignment methods have been 
proposed to relax the independence assumption 
of pair-wise alignment (Rosti et al 2008, Li et al 
2009). Such methods align hypotheses to a 
partially constructed CN in some order. For 
example, if in such method, Hypothesis 3 is first 
aligned to the backbone, followed by Hypothesis 
1, we are likely to arrive at the CN in Figure 2b) 
in which the two instances of Jeep are aligned. 
However, if Hypothesis 1 is aligned to the 
backbone first, we would still get the CN in 
Figure 2a).  Notice that the desirable output ?She 
bought the Jeep SUV? cannot be generated from 
either of the confusion networks because a re-
reordering of columns would be required. 
A common characteristic of CN-based 
approaches is that the order of words (backbone) 
and the alignment of words (correspondence 
sets) are decided as greedy steps independently 
of the lexical choice for the final output. The 
backbone and alignment are optimized according 
to auxiliary scoring functions and heuristics 
which may or may not be optimal with respect to 
producing CNs leading to good translations. In 
some recent approaches, these assumptions are 
relaxed to allow each input hypothesis as a 
backbone. Each backbone produces a separate 
CN and the decision of which CN to choose is 
taken at a later decoding stage, but this still 
restricts the possible orders and alignments 
greatly (Rosti et al 2008, Matusov et al 2008). 
In this paper, we present a joint optimization 
method for system combination. In this method, 
the alignment, ordering and lexical selection sub-
problems are solved jointly in a single decoding 
framework based on a log-linear model. 
she
sh
she
bought
bought
buys
the
the
the
Jeep
SUV
SUV Jeep
?
? ?
 
 Figure 1. Three MT system hypotheses with pair-
wise alignments. 
 
she bought the Jeep ? 
she buys the SUV ? 
she bought the SUV Jeep 
 
a) Confusion network with pair-wise alignment. 
 
she bought the ? Jeep 
she buys the SUV ? 
she bought the SUV Jeep 
 
b) Confusion network with incremental alignment. 
 
Figure 2. Correspondence sets of confusion networks 
under pair-wise and incremental alignment, using the 
second hypothesis as a backbone. 
2 Related Work 
There has been a large body of work on MT 
system combination. Among confusion-network-
based algorithms, most relevant to our work are 
state-of-the-art methods for constructing word 
alignments (correspondence sets) and methods 
for improving the selection of a backbone 
hypothesis. We have already reviewed such work 
in the introduction and will note relation to 
1203
specific models throughout the paper as we 
discuss specifics of our scoring functions. 
In confusion network algorithms which use 
pair-wise (or incremental) word-level alignment 
algorithms for correspondence set construction, 
problems of converting many-to-many 
alignments and handling multiple insertions and 
deletions need to be addressed. Prior work has 
used a number of heuristics to deal with these 
problems (Matusov, et. al., 2006, He et al08). 
Some work has made such decisions in a more 
principled fashion by computing model-based 
scores (Matusov et al 2008), but still special-
purpose algorithms and heuristics are needed and 
a single alignment is fixed.  
In our approach, no heuristics are used to 
convert alignments and no concept of a backbone 
is used. Instead, the globally highest scoring 
combination of alignment, order, and lexical 
choice is selected (subject to search error). 
Other than confusion-network-based 
algorithms, work most closely related to ours is 
the method of MT system combination proposed 
in (Jayaraman and Lavie 2005), which we will 
refer to as J&L. Like our method, this approach 
performs word-level system combination and is 
not limited to following the word order of a 
single backbone hypothesis; it also allows more 
flexibility in the selection of correspondence sets 
during decoding, compared to a confusion-
network-based approach. Even though their 
algorithm and ours are broadly similar, there are 
several important differences.   
Firstly, the J&L approach is based on pair-
wise alignments between words in different 
hypotheses, which are hard and do not have 
associated probabilities. Every word in every 
hypothesis is aligned to at most one word from 
each of the remaining hypotheses. Thus there is 
no uncertainty about which words should belong 
to the correspondence set of an aligned word w, 
once that word is selected to extend a partial 
hypothesis during search. If words do not have 
corresponding matching words in some 
hypotheses, heuristic matching to currently 
unused words is attempted.  
In contrast, our algorithm is based on the 
definition of a joint scoring model, which takes 
into account alignment uncertainty and combines 
information from word-level alignment models, 
ordering and lexical selection models, to address 
the three sub-problems of word-level system 
combination. In addition to the language model 
and word-voting features used by the J&L 
model, we incorporate features which measure 
alignment confidence via word-level alignment 
models and features which evaluate re-ordering 
via distortion models with respect to original 
hypotheses. While the J&L search algorithm 
incorporates a number of special-purpose 
heuristics to address phenomena of unused words 
lagging behind the last used words, the goal in 
our work is to minimize heuristics and perform 
search to jointly optimize the assignment of 
hidden variables (ordered correspondence sets) 
and observed output variables (words in final  
translations). 
Finally, the J&L method has not been 
evaluated in comparison to confusion-network-
based methods to study the impact of performing 
joint decoding for the three sub-problems. 
3 Notation 
Before elaborating the models and decoding 
algorithms, we first clarify the notation that will 
be used in the paper.  
We denote by ? =  ?1 ,? ,?? the set of 
hypotheses from multiple MT systems, where ??  
is the hypothesis from the i-th system and ??  is a 
word sequence ??,1 ,? ,??,?(?)  with length ?(?) .  
For simplicity, we assume that each system 
contributes only its 1-best hypothesis for 
combination. Accordingly, the i-th hypothesis ??  
will be associated with a weight ?(?) which is 
the weight of the i-th system. In the scenario that 
N-best lists are available from individual systems 
for combination, the weight of each hypothesis 
can be computed based on its rank in the N-best 
list (Rosti et. al. 2007a).  
Like in CN-based system combination, we 
construct a set of ordered correspondence sets 
(CS) from input hypotheses, and select one word 
from each CS to form the final output. A CS is 
defined as a set of (possibly empty) words, one 
from each hypothesis, that implicitly align to 
each other and that contributes exactly one of its 
words to the final output. A valid complete set of 
CS includes each non-empty word from each 
hypothesis in exactly one CS. As opposed to CN-
based algorithms, our ordered correspondence 
sets are constructed during a joint decoding 
process which performs lexical selection at the 
same time. 
To facilitate the presentation of our features, 
we define notation for ordered CS. A sequence 
of correspondence sets is denoted by 
C=??1,? ,??? . Each correspondence set is 
specified by listing the positions of each of the 
words in the CS in their respective input 
1204
hypotheses. Each input hypothesis is assumed 
to have one special empty word ? at position 0. 
A CS is denoted by ?? ?1 ,? , ??  
= ?1,?1 ,? ,??,??  , where ??,??  is the li-th word in 
the i-th hypothesis and the word position vector                
? =  ?1 ,? , ?? 
?  specifies the position of each 
word in its original hypothesis. Correspondingly, 
word ?? ,??  has the same weight ?(?)  as its 
original hypothesis?? . As an example, the last 
two correspondence sets specified by the CN in 
Figure 2a) would be specified as ??4 =
?? 4,4,4 = {????, ???, ???}  and ??5 =
?? 0,0,5 = {?, ?, ????}. 
As opposed to the CS defined in a 
conventional CN, words that have the same 
surface form but come from different hypotheses 
are not collapsed to be one single candidate since 
they have different original word positions. We 
need to trace each of them separately during the 
decoding process.  
4 A Joint Optimization Framework For 
System Combination 
The joint decoding framework chooses optimal 
output according to the following log-linear 
model: 
 
?? =  argmax
???,???,???
???  ?? ? ??(?,?,?,?)
?
?=1
  
              
where we denote by C the set of all possible 
valid arrangements of CS, O the set of all 
possible orders of CS, W the set of all possible 
word sequences, consisting of words from the 
input hypotheses. {??(?,?,?,?)}  are the 
features and {??} are the feature weights in the 
log-linear model, respectively. 
4.1 Features  
A set of features are used in this framework. 
Each of them models one or more of the 
alignment, ordering, and lexical selection sub-
problems. Features are defined as follows.  
 
Word posterior model:  
The word posterior feature is the same as the 
one proposed by Rosti et. al. (2007a). i.e.,  
???  ?,?,?,? =  ??? ? ??  ???  
?
?=1
 
 
where the posterior of a single word in a CS is 
computed based on a weighted voting score: 
 
? ?? ,??  ?? = ?  ??,??  ?? ?1 ,? , ??   
=  ?(?)
?
?=1
?(?? ,?? = ?? ,??) 
 
and M is the number of CS generated. Note 
that M may be larger than the length of the 
output word sequence w since some CS may 
generate empty words. 
 
Bi-gram voting model: 
 The second feature we used is a bi-gram 
voting feature proposed by Zhao and He (2009), 
i.e., for each bi-gram  ?? ,??+1  , a weighted 
position-independent voting score is computed: 
?  ?? ,??+1  ? =  ?(?)
?
?=1
?( ?? ,??+1 ? ??) 
 
And the global bi-gram voting feature is 
defined as: 
????  ?,?,?,? =  ??? ?  ?? ,??+1  ?  
|? |?1
?=1
 
 
Distortion model: 
Unlike in the conventional CN-based system 
combination, flexible orders of CS are allowed in 
this joint decoding framework. In order to model 
the distortion of different orderings, a distortion 
model between two CS is defined as follows: 
First we define the distortion cost between two 
words at a single hypothesis. Similarly to the 
distortion penalty in the conventional phrase-
based decoder (Koehn 2004b), the distortion cost 
of jumping from a word at position i to another 
word at position j, d(i,j), is proportional to the 
distance between i and j, e.g., |i-j|. Then, the 
distortion cost of jumping from one CS, which 
has a position vector recording the original 
position of each word in that CS, to another CS 
is a weighted sum of single-hypothesis-based 
distortion costs: 
?(??? ,???+1)  =  ?(?)
?
?=1
? |?? ,? ? ??+1,? |  
 
where ?? ,?  and ??+1,?  are the k-th element of 
the word position vector of CSm and CSm+1, 
respectively. For the purpose of computing the 
distortion feature, the position of an empty 
word is taken to be the same as the position of 
1205
the last visited non-empty word from the same 
hypothesis. 
The overall ordering feature can then be 
computed based on ?(??? ,???+1): 
 
???? ?,?,?,? = ?  ?(??? ,???+1)
??1
?=1
 
 
It is worth noting that this is not the only 
feature modeling the re-ordering behavior.  
Under the joint decoding framework, other 
features such as the language model and bi-gram 
voting affect the ordering as well.  
 
Alignment model: 
Each CS consists of a set of words, one from 
each hypothesis, that are implicitly aligned to 
each other. Therefore, a valid complete set of CS 
defines the word alignment among different 
hypotheses. In this paper, we derive an alignment 
score of a CS based on alignment scores of word 
pairs in that CS. To compute scores for word 
pairs, we perform pair-wise hypothesis alignment 
using the indirect HMM (He et al 2008) for 
every pair of input hypotheses. Note that this 
involves a total of N by (N-1)/2 bi-directional 
hypothesis alignments. The alignment score for a 
pair of words  ?? ,??  and  ?? ,??  is defined as the 
average of posterior probabilities of alignment 
links in both directions and is thus direction 
independent: 
 
?  ?? ,??  ,?? ,??   =  
1
2
  ?(??? = ?? |?? ,??) +  ?(??? = ?? |?? ,?? )  
 
If one of the two words is ?, the posterior of 
aligning word ? to state j is computed as 
suggested by Liang et al (2006), i.e., 
 
? ?0 = ??  ?? ,??  =  1? ? ?? = ??  ?? ,??   
?(?)
?=1
 
 
And ?(??? = 0|?? ,??) can be computed by the 
HMM directly.  
If both words are ?, then a pre-defined  ???  is 
assigned, i.e., ? ?0 = 0 ?? ,??  = ??? , where ???  
can be optimized on a held-out validation set. 
For a CS of words, if we set the j-th word as 
an anchor word, the probability that all other 
words align to that word is: 
?(?|??)  =  ?  ?? ,??  ,?? ,??   
?
?=1
???
 
 
The alignment score of the whole CS is a 
weighted sum of the logarithm of the above 
alignment probabilities, i.e., 
 
???? (??)  = ?(?)
?
?=1
??? ?(?|??)  
 
and the global alignment score is computed as: 
 
????  ?,?,?,? =  ???? (???)
?
?=1
 
 
Entropy model: 
In general, it is preferable to align the same 
word from different hypotheses into a common 
CS. Therefore, we use entropy to model the 
purity of a CS. The entropy of a CS is defined as: 
 
??? ?? = ???(?? ?1 ,? , ?? )  = 
 ? ?? ,??  ?? ???? ??,??  ?? 
??
?=1
 
 
where the sum is taken over all distinct words in 
the CS. Then the global entropy score is 
computed as: 
 
????  ?,?,?,? =  ???(
?
?=1
???) 
 
Other features used in our log-linear model 
include the count of real words |w|, a n-gram 
language model, and the count M of CS sets. 
These features address one or more of the 
three sub-problems of MT system combination. 
By performing joint decoding with all these 
features working together, we hope to derive 
better decisions on alignment, ordering and 
lexical selection.  
5 Joint Decoding 
5.1 Core algorithm 
Decoding is based on a beam search algorithm 
similar to that of the phrase-based MT decoder 
(Koehn 2004b). The input is a set of translation 
hypotheses to be combined, and the final output 
1206
sentence is generated left to right. Figure 3 
illustrates the decoding process, using the 
example input hypotheses from Figure 1.  Each 
decoding state represents a partial sequence of 
correspondence sets covering some of the words 
in the input hypotheses and a sequence of words 
selected from the CS to form a partial output 
hypothesis. The initial decoding state has an 
empty sequence of CS and an empty output 
sequence. A state corresponds to a complete 
output candidate if its CS covers all input words.  
 
lm: ? bought the
 
    a) a decoding state 
lm: ? bought the lm: ? bought the
 
     b)  seed states         
lm: ? bought the lm: ? bought the
 
    c) correspondence set states 
lm: ? the Jeep lm: ? the Jeep
 
    d) decoding states 
Figure 3. Illustration of the decoding process. 
 
In practice, because the features over 
hypotheses can be decomposed, we do not need 
to encode all of this information in a decoding 
state. It suffices to store a few attributes. They 
include positions of words from each input 
hypothesis that have been visited, the last two 
non-empty words generated (if a tri-gram LM is 
used), and an "end position vector (EPV)" 
recording positions of words in the last CS, 
which were just visited. In the figure, the visited 
words are shown with filled circles and the EPV 
is shown with a dotted pattern in the filled 
circles. Words specified by the EPV are 
implicitly aligned. In the state in Figure 3 a) the 
first three words of each hypothesis have been 
visited, the third word of each hypothesis is the 
last word visited (in the EPV), and the last two 
words produced are ?bought the?. The states also 
record the decoding score accumulated so far and 
an estimated future score to cover words that 
have not been visited yet (not shown).  
The expansion from one decoding state to a 
set of new decoding states is illustrated in Figure 
3. The expansion is done in three steps with the 
help of intermediate states. Starting from a 
decoding state as shown in Figure 3a), first a set 
of ?seed states? as shown in Figure 3b) are 
generated. Each seed state represents a choice of 
one of unvisited words, called a ?seed word? 
which is selected and marked as visited. For 
example, the word Jeep from the first hypothesis 
and the word SUV from the second hypothesis 
are selected in the two seed states shown in 
Figure 3b), respectively. These seed states 
further expand into a set of "CS states" as shown 
in Figure 3c). I.e., a CS is formed by picking one 
word from each of the other hypotheses which is 
unvisited and has a valid alignment link to the 
seed word. Figure 3c) shows two CS states 
expanded from the first seed state of Figure 3b), 
using Jeep from the first hypothesis as a seed 
word. In one of them the empty word from the 
second hypothesis is chosen, and in the other, the 
word SUV is chosen. Both are allowed by the 
alignments illustrated in Figure 1. Finally, each 
CS state generates one or more complet  
decoding states, in which a word is chosen from 
the current CS and the EPV vector is advanced to 
reflect the last newly visited words. Figure 3d) 
shows two such states, descending from the 
corresponding CS states in 3c). After one more 
expansion the state in 3d) on the left can generate 
the translation ?She bought the Jeep SUV?, 
which cannot be produced by either confusion 
network in Figure 2. 
5.2 Pruning 
The full search space of joint decoding is a 
product of the alignment, ordering, and lexical 
selection spaces. Its size is exponential in the 
length of the sentence and the number of 
hypotheses involved in combination. Therefore, 
pruning techniques are necessary to reduce the 
search space.  
First we will prune down the alignment space. 
Instead of allowing any alignment link between 
1207
arbitrary words of two hypotheses, only links 
that have alignment score higher than a threshold 
are allowed, plus links in the union of the Viterbi 
alignments in both directions. In order to prevent 
the garbage collection problem where many 
words align to a rare word at the other side 
(Moore, 2004), we further impose the limit that if 
one word is aligned to more than T words, these 
links are sorted by their alignment score and only 
the top T links are kept. Meanwhile, alignments 
between a real word and ? are always allowed.  
We then prune down the ordering space by 
limiting the expansion of new states. Only states 
that are adjacent to their preceding states are 
created. Two states are called adjacent if their 
EPVs are adjacent, i.e., given the EPV of the 
preceding state m as  ?? ,1 ,? , ?? ,? 
?
 and the 
EPV of the next state m+1 as 
 ??+1,1,? , ??+1,? 
?
, if at least at one 
dimension k, ??+1,?  = ?? ,?+1, then these two 
states are adjacent. When checking the 
adjacency of two states, the position of an 
empty word is taken to be the same as the 
position of the last visited non-empty word 
from the same hypothesis.  
The number of possible CS states expanded 
from a decoding state is exponential in the 
number of hypotheses. In decoding, these CS 
states are sorted by their alignment scores and 
only the top K CS states are kept.  
The search space can be further pruned down 
by the widely used technique of path 
recombination and by best-first pruning.  
Path recombination is a risk-free pruning 
method. Two paths can be recombined if they 
agree on a) words from each hypothesis that have 
been visited so far, b) the last two real words 
generated, and c) their EPVs. In such case, we 
only need to keep the path with the higher score. 
Best-first pruning can help to reduce the 
search space even further. In the decoding 
process we compare paths that have generated 
the same number of words (both real and empty 
words) and only keep a certain number of most 
promising paths. Pruning is based on an 
estimated overall score of each path, which is the 
sum of the decoding score accumulated so far 
and an estimated future score to cover the words 
that have not been visited. Next we discuss the 
future score computation. 
5.3 Computing the future score 
In order to estimate the future cost of an 
unfinished path, we treat the unvisited words of 
one input hypothesis as a backbone, and apply a 
greedy search for alignment based on it; i.e., for 
each word of this backbone, the most likely 
words (based on the alignment link scores) from 
other hypotheses, one word from each 
hypothesis, are collected to form a CS. These CS 
are ordered according to the word order of the 
backbone and form a CN. Then, a light decoding 
process with a search beam of size one is applied 
to decode this CN and find the approximate 
future path, with future feature scores computed 
during the decoding process. If there are leftover 
words not included in this CN, they are treated in 
the way described in section 5.4. Additionally, 
caching techniques are applied to speed up the 
computation of future scores further.   
Given the method discussed above, we can 
estimate a future score based on each input 
hypothesis, and the final future score is estimated 
as the best of these hypothesis-dependent scores.  
5.4  Dealing with leftover input words  
At a certain point a path will reach the end, i.e., 
no more states can be generated from it 
according to the state expansion requirement. 
Then it is marked as a finished path. However, 
sometimes the state may contain a few input 
words that have not been visited. An example of 
this situation is the second state in Figure 3d). 
The word SUV in the third input hypothesis is 
left unvisited and it cannot be selected next 
because there is no adjacent state that can be 
generated. For such cases, we need to compute 
an extra score of covering these leftover words. 
Our approach is to create a state that produces 
the same output translation, but also covers all 
remaining words. For each leftover word, we 
create a pseudo CS that contains just that word 
plus ??s from all other hypotheses, and let it 
output ?. Moreover, that CS is inserted at a place 
such that no extra distortion cost is incurred. 
Figure 4 shows an example using the second 
state in Figure 3d). The last two words from the 
first two MT hypotheses ?the Jeep? and ?the 
SUV? align to the third and fifth words of the 
third hypothesis ?the Jeep?; the word w3,4 from 
the third hypothesis is left unvisited. The original 
path has two CS and one left-over word w3,4. It is 
expanded to have three CS, with a pseudo CS 
inserted between the two CS.  
It is worth noting that the new inserted pseudo 
CS will not affect the word count feature and 
contextually dependent feature scores such as the 
LM and bi-gram voting, since it only generates 
an empty word. Moreover, it will not affect the 
1208
distortion score either. For example, as shown in 
Figure 4, the distortion cost of jumping from 
word w2,3  to ?2  and then to w2,4 is the same as  
the cost of jumping from w2,3  to w2,4 given the 
way we assign position to empty word and the 
fact that the distortion cost is proportional to the 
difference between word positions.  
Scores of other features for this pseudo CS 
such as word posterior (of ?), alignment score, 
CS entropy, and CS count are all local scores and 
can be computed easily. Unlike future scores 
which are approximate, the score computed in 
this process is exact. Adding this extra score to 
the existing score accumulated in the final state 
gives the complete score of this finished path. 
When all paths are finished, the one with the best 
complete score is returned as the final output 
sentence. 
 
w1,3 w1,4   w1,3 ?1 w1,4  
w2,3 w2,4  =>  w2,3  ?2  w2,4  
w3,3 w3,4 w3,5  w3,3 w3,4 w3,5  
Figure 4. Expanding a leftover word to a pseudo 
correspondence set. 
6 Evaluation  
6.1 Experimental conditions 
For the joint decoding method, the threshold for 
alignment-score-based pruning is set to 0.25 and 
the maximum number of words that can align to 
the same word is limited to 3. We call this the 
standard setting. The joint decoding approach is 
evaluated on the Chinese-to-English (C2E) test 
set of the 2008 NIST Open MT Evaluation 
(NIST 2008). Results are reported in case 
insensitive BLEU score in percentages 
(Papineni et. al., 2002).  
The NIST MT08 C2E test set contains 691 
and 666 sentences of data from two genres, 
newswire and web-data, respectively. Each test 
sentence has four references provided by human 
translators. Individual systems in our 
experiments belong to the official submissions of 
the MT08 C2E constraint-training track. Each 
submission provides 1-best translation of the 
whole test set. In order to train feature weights, 
the original test set is divided into two parts, 
called the dev and test set, respectively. The dev 
set consists of the first half of both newswire and 
web-data, and the test set consists of the second 
half of data of both genres.   
There are 20 individual systems available. We 
ranked them by their BLEU score results on the 
dev set and picked the top five systems, 
excluding systems ranked 5th and 6th since they 
are subsets of the first entry (NIST 2008). 
Performance of these systems on the dev and test 
sets is shown in Table 1.  
The baselines include a pair-wise hypothesis 
alignment approach using the indirect HMM 
(IHMM) proposed by He et al (2008), and an 
incremental hypothesis alignment approach using 
the incremental HMM (IncHMM) proposed by 
Li et al (2009). The lexical translation model 
used to compute the semantic similarity is 
estimated from two million parallel sentence-
pairs selected from the training corpus of MT08. 
The backbone for the IHMM-based approach is 
selected based on Minimum Bayes Risk (MBR) 
using a BLEU-based loss function. The various 
parameters of the IHMM and the IncHMM are 
tuned on the dev set. The same IHMM is used to 
compute the alignment feature score for the joint 
decoding approach.  
The final combination output can be obtained 
by decoding the CN with a set of features. The 
features used for the baseline systems are the 
same as the features used by the joint decoding 
approach. Some of these features are constant 
across decoding hypotheses and can be ignored. 
The non-constant features are word posterior, bi-
gram voting, language model score, and word 
count. They are computed in the same way as for 
the joint decoding approach.  
System weights and feature weights are 
trained together using Powell's search for the 
IHMM-based approach. Then the same system 
weights are applied to both IncHMM and Joint 
Decoding -based approaches, and the feature 
weights of them are trained using the max-BLEU 
training method proposed by Och (2003) and 
refined by Moore and Quirk (2008).  
Table 1: Performance of individual systems on 
the dev and test set 
System ID dev test 
System A 32.88 31.81 
System B 32.82 32.03 
System C 32.16 31.87 
System D 31.40 31.32 
System E 27.44 27.67 
6.2 Comparison against baselines 
Table 2 lists the BLEU scores achieved by the 
two baselines and the joint decoding approach. 
Both baselines surpass the best individual system 
1209
significantly. However, the gain of incremental 
HMM over IHMM is smaller than that reported 
in Li et al (2009). One possible reason of such 
discrepancy could be that fewer hypotheses are 
used for combination in this experiment 
compared to that of Li et al (2009), so the 
performance difference between them is 
narrowed accordingly. Despite that, the proposed 
joint decoding method outperforms both IHMM 
and IncHMM baselines significantly.  
Table 2: Comparison between the joint decoding 
approach and the two baselines 
method dev test 
IHMM 36.91 35.85 
IncHMM 37.32 36.38 
Joint Decoding 37.94 37.20* 
* The gains of Joint Decoding over IHMM and 
IncHMM are both with a statistical significance level > 
99%, measured based on the paired bootstrap re-
sampling method (Koehn 2004a) 
6.3 Comparison of alignment pruning  
The effect of alignment pruning is also studied. 
We tested with limiting the allowable links to 
just those that in the union of bi-directional 
Viterbi alignments.  
The results are presented in Table 3. 
Compared to the standard setting, allowing only 
links in the union of the bi-directional Viterbi 
alignments causes slight performance 
degradation. On the other hand, it still 
outperforms the IHMM baseline by a fair margin. 
This is because the joint decoding approach is 
effectively resolving the ambiguous 1-to-many 
alignments and deciding proper places to insert 
empty words during decoding. 
Table 3: Comparison between different settings 
of alignment pruning 
Setting Test 
standard settings 37.20 
union of Viterbi 36.88 
6.4 Comparison of ordering constraints 
In order to investigate the effect of allowing 
flexible word ordering, we conducted 
experiments using different constraints on the 
ordering of CS in the decoding process. In the 
first case, we restrict the order of CS to follow 
the word order of a backbone, which is one of 
the input hypotheses selected by MBR-BLEU. In 
the second case, the order of CS is constrained to 
follow the word order of at least one of the input 
hypotheses.  As shown in Table 4, in comparison 
to the standard setting that allows backbone-free 
word ordering, the constrained settings did not 
lead to significant performance degradation. This 
indicates that most of the gain due to the joint 
decoding approach comes from the joint 
optimization of alignment and word selection. It 
is possible, though, that if we lift the CS 
adjacency constraint during search, we might 
derive more benefit from flexible word ordering.  
Table 4: Effect of ordering constraints 
Setting test 
standard settings 37.20 
monotone w.r.t. backbone 37.22 
monotone w.r.t. any hyp. 37.12 
7 Discussion 
This paper proposed a joint optimization 
approach for word-level combination of 
translation hypotheses from multiple machine 
translation systems. Unlike conventional 
confusion-network-based methods, alignments 
between words from different hypotheses are not 
pre-determined and flexible word orderings are 
allowed. Decisions on word alignment between 
hypotheses, word ordering, and the lexical choice 
of the final output are made jointly according to 
a set of features in the decoding process. A new 
set of features to model alignment and re-
ordering behavior is also proposed. The method 
is evaluated against state-of-the-art baselines on 
the NIST MT08 C2E task. The joint decoding 
approach is shown to outperform baselines 
significantly. 
Because of the complexity of search, a 
challenge for our approach is combining a large 
number of input hypotheses. When N-best 
hypotheses from the same system are added, it is 
possible to pre-compute and fix the one-to-one 
word alignment among the same-system 
hypotheses; such pre-computation is reasonable 
given our observation that the disagreement 
among hypotheses from different systems is 
larger than that among hypotheses from the same 
system. This will reduce the alignment search 
space to be the same as that for 1-best case. We 
plan to study this setting in future work. 
To further improve the performance of our 
approach we see the biggest opportunity in 
developing better estimates of future scores and 
incorporating additional features. Beside 
potential performance improvement, they may 
help on more effective pruning and speed up the 
overall decoding process as well. 
1210
References  
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In 
Proceedings of IEEE ASRU. 
 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore 2008. Indirect HMM 
based Hypothesis Alignment for Combining 
Outputs from Machine Translation Systems. In 
Proceedings of EMNLP. 
 
Shyamsundar Jayaraman and Alon Lavie. 2005. 
Multi-engine machine translation guided by explicit 
word matching. In Proceedings of EAMT. 
 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer 2008. Machine Translation 
System Combination using ITG-based Alignments. 
In Proceedings of ACL. 
 
Philipp Koehn, 2004a, Statistical Significance Tests 
for Machine Translation Evaluation. In Proceedings 
of EMNLP. 
 
Philipp Koehn. 2004b. Pharaoh: A Beam Search 
Decoder For Phrase Based Statistical Machine 
Translation Models. In Proceedings of AMTA. 
 
Chi-Ho Li, Xiaodong He, Yupeng Liu and Ning Xi, 
2009. Incremental HMM Alignment for MT 
System Combination. In Proceedings of ACL. 
 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Personal Communication  
 
Evgeny Matusov, Nicola Ueffing and Hermann Ney. 
2006. Computing Consensus Translation from 
Multiple Machine Translation Systems using 
Enhanced Hypothesis Alignment. In Proceedings of 
EACL. 
 
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs, 
Nicola Bertoldi, Daniel D?chelotte, Marcello 
Federico, Muntsin Kolss, Young-Suk Lee, Jos? B. 
Mari?o, Matthias Paulik, Salim Roukos, Holger 
Schwenk, and Hermann Ney. 2008. System 
combination for  machine translation of spoken and 
written language. IEEE transactions on audio 
speech and language processing 16(7). 
 
Robert C. Moore and Chris Quirk. 2008. Random 
Restarts in Minimum Error Rate Training for 
Statistical Machine Translation, In Proceedings of 
COLING 
 
Robert C. Moore. 2004. Improving IBM Word 
Alignment Model 1, In Proceedings of ACL. 
 
NIST 2008. The NIST Open Machine Translation 
Evaluation.www.nist.gov/speech/tests/mt/2008/doc/ 
 
Franz J. Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL.  
 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei- Jing Zhu 2002. BLEU: a Method for 
Automatic Evaluation of Machine Translation. In 
Proceedings of ACL. 
 
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, 
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr. 2007a. Combining outputs from multiple 
machine translation systems. In Proceedings of 
NAACL-HLT. 
 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz 2007b. Improved Word-level System 
Combination for Machine Translation. In 
Proceedings of ACL. 
 
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz 2008. Incremental 
Hypothesis Alignment for Building Confusion 
Networks with Application to Machine Translation 
System Combination. In Proceedings of the 3rd 
ACL Workshop on SMT. 
 
Yong Zhao and Xiaodong He, 2009. Using N-gram 
based Features for Machine Translation System 
Combination. In Proceedings of NAACL-HLT 
 
 
 
1211
A Global Joint Model for Semantic
Role Labeling
Kristina Toutanova?
Microsoft Research
Aria Haghighi??
University of California Berkeley
Christopher D. Manning?
Stanford University
We present a model for semantic role labeling that effectively captures the linguistic intuition
that a semantic argument frame is a joint structure, with strong dependencies among the
arguments. We show how to incorporate these strong dependencies in a statistical joint model
with a rich set of features over multiple argument phrases. The proposed model substantially
outperforms a similar state-of-the-art local model that does not include dependencies among
different arguments.
We evaluate the gains from incorporating this joint information on the Propbank corpus,
when using correct syntactic parse trees as input, and when using automatically derived parse
trees. The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments
for gold-standard parse trees on Propbank. For automatic parse trees, the error reductions are
8.3% and 10.3% on all and core arguments, respectively. We also present results on the CoNLL
2005 shared task data set. Additionally, we explore considering multiple syntactic analyses to
cope with parser noise and uncertainty.
1. Introduction
Since the release of the FrameNet (Baker, Fillmore, and Lowe 1998) and Propbank
(Palmer, Gildea, and Kingsbury 2005) corpora, there has been a large amount of work
on statistical models for semantic role labeling. Most of this work relies heavily on local
classifiers: ones that decide the semantic role of each phrase independently of the roles
of other phrases.
However, linguistic theory tells us that a core argument frame is a joint struc-
ture, with strong dependencies between arguments. For instance, in the sentence
? One Microsoft Way, Redmond, WA 98052, USA. E-mail: kristout@microsoft.com.
?? Department of Electrical Engineering and Computer Sciences, Soda Hall, Berkeley, CA 94720, USA.
E-mail: aria42@cs.berkeley.edu.
? Department of Computer Science, Gates Building 1A, 353 Serra Mall, Stanford CA 94305, USA. E-mail:
manning@cs.stanford.edu.
Submission received: 15 July 2006; Revised submission received: 1 May 2007; Accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
[Final-hour trading]THEME accelerated [to 108.1 million shares]TARGET [yesterday]ARGM-TMP, the
first argument is the subject noun phrase final-hour trading of the active verb accelerated.
If we did not consider the rest of the sentence, it would look more like an AGENT argu-
ment, but when we realize that there is no other good candidate for a THEME argument,
because to 108.1 million sharesmust be a TARGET and yesterday is most likely ARGM-TMP,
we can correctly label it THEME.
Even though previous work has modeled some correlations between the labels of
parse tree nodes (see Section 2), many important phenomena have not been modeled.
The key properties needed to model this joint structure are: (1) no finite Markov horizon
assumption for dependencies among node labels, (2) features looking at the labels of
multiple argument nodes and internal features of these nodes, and (3) a statistical model
capable of incorporating these long-distance dependencies and generalizing well. We
show how to build a joint model of argument frames, incorporating novel features into
a discriminative log-linear model. This system achieves an error reduction of 24.1%
on ALL arguments and 36.8% on CORE arguments over a state-of-the-art independent
classifier for gold-standard parse trees on Propbank.
If we consider the linguistic basis for joint modeling of a verb?s arguments (includ-
ing modifiers), there are at least three types of information to be captured. The most
basic is to limit occurrences of each kind of argument. For instance, there is usually
at most one argument of a verb that is an ARG0 (agent), and although some modifier
roles such as ARGM-TMP can fairly easily be repeated, others such as ARGM-MNR also
generally occur at most once.1 The remaining two types of information apply mainly to
core arguments (the strongly selected arguments of a verb: ARG0?ARG5 in Propbank),
which in most linguistic theories are modeled as belonging together in an argument
frame (set of arguments). The information is only marginally useful for adjuncts (the
ARGM arguments of Propbank), which are usually treated as independent realizational
choices not included in the argument frame of a verb.
Firstly, many verbs take a number of different argument frames. Previous work
has shown that these are strongly correlated with the word sense of the verb (Roland
and Jurafsky 2002). If verbs were disambiguated for sense, the semantic roles of
phrases would be closer to independent given the sense of the verb. However, be-
cause in almost all semantic role labeling work (including ours), the word sense is
unknown and the model conditions only on the lemma, there is much joint informa-
tion between arguments when conditioning only on the verb lemma. For example,
compare:
(1) Britain?s House of Commons passed a law on steroid use.
(2) The man passed the church on his way to the factory.
In the first case the noun phrase after passed is an ARG1, whereas in the second case it is a
ARGM-LOC, with the choice governed by the sense of the verb pass. Secondly, even with
same sense of a verb, different patterns of argument realization lead to joint information
between arguments. Consider:
(3) The day that the ogre cooked the children is still remembered.
1 The Propbank semantic role names which we use here are defined in Section 3.
162
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
(4) The meal that the ogre cooked the children is still remembered.
Despite both examples having an identical surface syntax, knowing that the ARG1 of
cook is expressed by the initial noun meal in the second example gives evidence that the
children is the ARG2 (beneficiary), not the ARG1 in this case.
Let us think of a graphical model over a set of m variables, one for each node in
the parse tree t, representing the labels of the nodes and the dependencies between
them. In order for a model over these variables to capture, for example, the statistical
tendency of some semantic roles to occur at most once (e.g., that there is usually at most
one constituent labeled AGENT), there must be a dependency link between any two
variables. To estimate the probability that a certain node gets the role AGENT, we need
to know if any of the other nodes were labeled with this role.
We propose such a model, with a very rich graphical model structure, which is
globally conditioned on the observation (the parse tree).2 Such a model is formally
a Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). However,
note that in practice this term has previously been used almost exclusively to describe
the restricted case of linear chain Conditional Markov Random Fields (sequence mod-
els) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least models
that have strong Markov properties, which allow efficient dynamic programming al-
gorithms (Cohn and Blunsom 2005). Instead, we consider a densely connected CRF
structure, with no Markov properties, and use approximate inference by re-ranking the
n-best solutions of a simpler model with stronger independence assumptions (for which
exact inference is possible).
Such a rich graphical model can represent many dependencies but there are two
dangers?one is that the computational complexity of training the model and search-
ing for the most likely labeling given the tree can be prohibitive, and the other is
that if too many dependencies are encoded, the model will over-fit the training
data and will not generalize well. We propose a model which circumvents these
two dangers and achieves significant performance gains over a similar local model
that does not add any dependency arcs among the random variables. To tackle the
efficiency problem, we adopt dynamic programming and re-ranking algorithms. To
avoid overfitting we encode only a small set of linguistically motivated dependencies
in features over sets of the random variables. Our re-ranking approach, like the ap-
proach to parse re-ranking of Collins (2000), employs a simpler model?a local semantic
role labeling algorithm?as a first pass to generate a set of n likely complete assign-
ments of labels to all parse tree nodes. The joint model is restricted to these n assign-
ments and does not have to search the exponentially large space of all possible joint
labelings.
2. Related Work
There has been a substantial amount of work on automatic semantic role labeling,
starting with the statistical model of Gildea and Jurafsky (2002). Researchers have
worked on defining new useful features, and different system architectures andmodels.
Here we review the work most closely related to ours, concentrating on methods for
incorporating joint information and for increasing robustness to parser error.
2 That is, it defines a conditional distribution of labels of all nodes given the parse tree.
163
Computational Linguistics Volume 34, Number 2
2.1 Methods for Incorporating Joint Information
Gildea and Jurafsky (2002) propose a method to model global dependencies by includ-
ing a probability distribution over multi-sets of semantic role labels given a predicate.
In this way the model can consider the assignment of all nodes in the parse tree and
evaluate whether the set of realized semantic roles is likely. If a necessary role is missing
or if an unusual set of arguments is assigned by the local model, this additional factor
can correct some of the mistakes. The distribution over label multi-sets is estimated
using interpolation of a relative frequency and a back-off distribution. The back-off
distribution assumes each argument label is present or absent independently of the
other labels, namely, it assumes a Bernoulli Naive Bayes model.
The most likely assignment of labels according to such a joint model is found ap-
proximately using re-scoring of the top k = 10 assignments according to a local model,
which does not include dependencies among arguments. Using this model improves
the performance of the system in F-measure from 59.2 to 62.85. This shows that adding
global information improves the performance of a role labeling system considerably.
However, the type of global information in this model is limited to label multi-sets.
We will show that much larger gains are possible from joint modeling, adding richer
sources of joint information using a more flexible statistical model.
The model of Pradhan, Hacioglu, et al (2004, 2005) is a state-of-the-art model, based
on Support Vector Machines, and incorporating a large set of structural and lexical
features. At the heart of the model lies a local classifier, which labels each parse tree
node with one of the possible argument labels or NONE. Joint information is integrated
into the model in two ways:
Dynamic class context: Using the labels of the two nodes to the left as features for
classifying the current node. This is similar to the Conditional Markov Models (CMM)
often used in information extraction (McCallum, Freitag, and Pereira 2000). Notice
that here the previous two nodes classified are not in general the previous two nodes
assigned non-NONE labels. If a linear order on all nodes is imposed, then the previous
two nodes classified most likely bear the label NONE.
Language model lattice re-scoring: Re-scoring of an N-best lattice with a trigram
language model over semantic role label sequences. The target predicate is also part of
the sequence.
These ways of incorporating joint information resulted in small gains over a base-
line system using only the features of Gildea and Jurafsky (2002). The performance
gain due to joint information over a system using all features was not reported. The
joint information captured by this model is limited by the n-gram Markov assumption
of the language model over labels. In our work, we improve the modeling of joint
dependencies by looking at longer-distance context, by defining richer features over
the sequence of labels and input features, and by estimating the model parameters
discriminatively.
A system which can integrate longer-distance dependencies is that of Punyakanok
et al (2004) and Punyakanok, Roth, and Yih (2005). The idea is to build a semantic role
labeling system that is based on local classifiers but also uses a global component that
ensures that several linguistically motivated global constraints on argument frames
are satisfied. The constraints are categorical and specified by hand. For example, one
global constraint is that the argument phrases cannot overlap?that is, if a node is
labeled with a non-NONE label, all of its descendants have to be labeled NONE. The
proposed framework is integer linear programming (ILP), which makes it possible
to find the most likely assignment of labels to all nodes of the parse tree subject to
164
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
specified constraints. Solving the ILP problem is NP-hard but it is very fast in practice
(Punyakanok et al 2004). The authors report substantial gains in performance due
to these global consistency constraints. This method was applied to improve the
performance both of a system based on labeling syntactic chunks and one based on
labeling parse tree nodes. Our work differs from that work in that our constraints are
not categorical (either satisfied or not), but are rather statistical preferences, and that
they are learned automatically based on features specified by the knowledge engineer.
On the other hand, we solve the search/estimation problem through re-ranking and
n-best search only approximately, not exactly.
So far we have mainly discussed systems which label nodes in a parse tree. Many
systems that only use shallow syntactic information have also been presented (Hacioglu
2004; Punyakanok et al 2004); using full syntactic parse information was not allowed in
the CoNLL 2004 shared task on Semantic Role Labeling and description of such systems
can be found in (Carreras and Ma`rquez 2004). Most systems which use only shallow
syntactic information represent the input sentence as a sequence of tokens (words or
phrases), which they label with a BIO tagging representation (beginning, inside, and
outside argument labels) (Hacioglu 2004). Limited joint information is used by such sys-
tems, provided as a fixed size context of tags on previous tokens; for example, a length
five window is used in the chunk-based system in (Pradhan, Hacioglu et al 2005).
A method that models joint information in a different way was proposed by Cohn
and Blunsom (2005). It uses a tree-structured CRF, where the statistical dependency
structure is exactly defined by the edges in the syntactic parse tree. The only depen-
dencies captured are between the label of a node and the label of each of its chil-
dren. However, the arguments of a predicate can be arbitrarily far from each other
in the syntactic parse tree and therefore a tree-CRF model is limited in its ability to
model dependencies among different arguments. For instance, the dependency between
the meal and the children for the sentence in example (4) will not be captured because
these phrases are not in the same local tree according to Penn Treebank syntax.
2.2 Increasing Robustness to Parser Error
There have been multiple approaches to reducing the sensitivity of semantic role
labeling systems to syntactic parser error. Promising approaches have been to consider
multiple syntactic analyses?the top k parses from a single or multiple full parsers
(Punyakanok, Roth, and Yih 2005), or a shallow parse and a full parse (Ma`rquez et al
2005; Pradhan et al 2005), or several types of full syntactic parses (Pradhan, Ward
et al 2005). Such techniques are important for achieving good performance: The top
four systems in the CoNLL 2005 shared task competition all used multiple syntactic
analyses (Carreras and Ma`rquez 2005).
These previous methods develop special components to combine the labeling deci-
sions obtained using different syntactic annotation. The method of Punyakanok, Roth,
and Yih (2005) uses ILP to derive a consistent set of arguments, each of which could
be derived using a different parse tree. Pradhan, Ward et al (2005) use stacking to
train a classifier which combines decisions based on different annotations, andMa`rquez
et al (2005) use special-purpose filtering and inference stages which combine arguments
proposed by systems using shallow and full analyses.
Our approach to increasing robustness uses the top k parses from a single parser
and is a simple general method to factor in the uncertainty of the parser by apply-
165
Computational Linguistics Volume 34, Number 2
ing Bayesian inference. It is most closely related to the method described in Finkel,
Manning, and Ng (2006) and can be seen as an approximation of that method.
We describe our system in detail by first introducing simpler local semantic role
labeling models in Section 4, and later building on them to define joint models in Sec-
tion 5. Before we start presenting models, we describe the data and evaluation measures
used in Section 3. Readers can skip the next section and continue on to Section 4 if they
are not interested in the details of the evaluation.
3. Data and Evaluation Measures
3.1 Data
For most of our experiments we used the February 2004 release of Propbank. We also
report results on the CoNLL 2005 shared task data (Propbank I) in Section 6.2. For
the latter, we used the standard CoNLL evaluation measures, and we refer readers
to the description of that task for details of the evaluation (Carreras and Ma`rquez
2005). In this section we describe the data and evaluation measures we used for the
February 2004 data. We use our own set of measures on the February 2004 data for
three reasons. Firstly, we wish to present a richer set of measures, which can better
illustrate the performance of the system on core arguments as against adjuncts and
the performance on identifying versus classifying arguments. Secondly, we technically
could not use the CoNLL measure on the February 2004 data, because this earlier
data was not available in a format which specifies which arguments should have the
additional R-ARGX labels used in the CoNLL evaluation.3 Finally, these measures are
better for comparison with early papers, because most research before 2005 did not
distinguish referring arguments. We describe our argument-based measures in detail
here in case researchers are interested in replicating our results for the February 2004
data.
For the February 2004 data, we used the standard split into training, development,
and test sets?the annotations from sections 02?21 formed the training set, section 24 the
development, and section 23 the test set. The set of argument labels considered is the
set of core argument labels (ARG0 through ARG5) plus the modifier labels (see Figure 1).
The training set contained 85,392 propositions, the test set 4,615, and the development
set 2,626.
We evaluate semantic role labeling models on gold-standard parse trees and parse
trees produced by Charniak?s automatic parser (Charniak 2000). For gold-standard
parse trees, we preprocess the trees to discard empty constituents and strip functional
tags. Using the trace information provided by empty constituents is very useful for
improving performance (Palmer, Gildea, and Kingsbury 2005; Pradhan, Ward et al
2005), but we have not used this information so that we can compare our results to
previous work and since automatic systems that recover it are not widely available.
3.2 Evaluation Measures
Since 2004, there has been a precise, standard evaluation measure for semantic role la-
beling, formulated by the organizers of the CoNLL shared tasks (Carreras and Ma`rquez
3 Currently, a script which can convert original Propbank annotations to CoNLL format is available as part
of the CoNLL software distribution.
166
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 1
Labels of modifying arguments occurring in Propbank.
2004, 2005). An evaluation script is also distributed as part of the provided software for
the shared task and can be used to evaluate systems on Propbank I data.
For papers published between 2000 and 2005, there are several details of the eval-
uation measures for semantic role labeling that make it difficult to compare results
obtained by different researchers, because researchers use their own implementations
of evaluation measures, without making all the exact details clear in their papers. The
first issue is the existence of arguments consisting of multiple constituents. In this case
it is not clear whether partial credit is to be given for guessing only some of the con-
stituents comprising the argument correctly. The second issue is whether the bracketing
of constituents should be required to be recovered correctly, in other words, whether
pairs of labelings, such as [the]ARG0 [man]ARG0 and [the man]ARG0 are to be considered the
same or not. If they are considered the same, there are multiple labelings of nodes in
a parse tree that are equivalent. The third issue is that when using automatic parsers,
some of the constituents that are fillers of semantic roles are not recovered by the parser.
In this case it is not clear how various research groups have scored their systems (using
headword match, ignoring these arguments altogether, or using exact match). If we
vary the choice taken for these three issues, we can come up with many (at least eight)
different evaluationmeasures, and these details are important, because different choices
can lead to rather large differences in reported performance.
Here we describe in detail our evaluation measures for the results on the February
2004 data reported in this article. The measures are similar to the CoNLL evaluation
measure, but report a richer set of statistics; the exact differences are discussed at the
end of this section.
For both gold-standard and automatic parses we use one evaluation measure,
which we call argument-based evaluation. To describe the evaluation measure, we will
use as an example the correct and guessed semantic role labelings shown in Figures 2(a)
and 2(b). Both are shown as labelings on parse tree nodes with labels of the form ARGX
and C-ARGX. The label C-ARGX is used to represent multi-constituent arguments. A con-
stituent labeled C-ARGX is assumed to be a continuation of the closest constituent to the
left labeled ARGX. Our semantic role labeling system produces labelings of this form and
the gold standard Propbank annotations are converted to this form as well.4 The evalua-
tion is carried out individually for each predicate and its associated argument frame. If a
sentence contains several clauses, the several argument frames are evaluated separately.
4 This representation is not powerful enough to represent all valid labelings of multi-constituent
arguments, because it cannot represent the case where a new argument with label ARGX starts before
a previous multi-constituent argument with the same label ARGX has finished. This case, however, is
very rare.
167
Computational Linguistics Volume 34, Number 2
Our argument-based measures do not require exact bracketing (if the set of words
constituting an argument is correct, there is no need to know how this set is broken
into constituents) and do not give partial credit for labeling correctly only some of
several constituents in a multi-constituent argument. They are illustrated in Figure 2.
For these measures, a semantic role labeling of a sentence is viewed as a labeling on sets
of words. These sets can encompass several non-contiguous spans. Figure 2(c) gives the
representation of the correct and guessed labelings shown in Figures 2(a) and 2(b), in the
first and second rows of the table, respectively. To convert a labeling on parse tree nodes
to this form, we create a labeled set for each possibly multi-constituent argument. All
remaining sets of words are implicitly labeled with NONE. We can see that, in this way,
exact bracketing is not necessary and also no partial credit is given when only some of
several constituents in a multi-constituent argument are labeled correctly.
We will refer to word sets as ?spans.? To compute the measures, we are comparing
a guessed set of labeled spans to a correct set of labeled spans. We briefly define the
various measures of comparison used herein, using the example guessed and correct
Figure 2
Argument-based scoring measures for the guessed labeling.
168
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
labelings shown in Figure 2(c). All spans not listed explicitly are assumed to have label
NONE. The scoring measures are illustrated in Figure 2(d).
The figure shows performance measures?F-Measure (F1) and Whole Frame Accu-
racy (Acc.)?across nine different conditions. When the sets of labeled spans are com-
pared directly, we obtain the complete taskmeasures, corresponding to the ID&CLS row
and ALL column in Figure 2(d). We also define several other measures to understand the
performance of the system on different types of labels. We measure the performance on
identification (ID), classification (CLS), and the complete task (ID&CLS), when consider-
ing only the core arguments (CORE), all arguments but with a single ARGM label for the
modifier arguments (COARSEARGM), and all arguments (ALL). This defines nine sub-tasks,
which we now describe. For each of them, we compute the Whole Frame Accuracy and
F-Measure as follows:
Whole Frame Accuracy (Acc.). This is the percentage of propositions for which
there is an exact match between the proposed and correct labelings. For example, the
whole frame accuracy for ID&CLS and ALL is 0, because the correct and guessed sets
of labeled spans shown in Figure 2(c) do not match exactly. In the figures, ?Acc.? is
always an abbreviation for this whole frame accuracy. Even though this measure has
not been used extensively in previous work, we find it useful to track. Most importantly,
potential applications of role labeling may require correct labeling of all (or at least the
core) arguments in a sentence in order to be effective, and partially correct labelings may
not be very useful. Moreover, a joint model for semantic role labeling optimizes Whole
Frame Accuracy more directly than a local model does.
F-Measure (F1). Because there may be confusion about what wemean by F-Measure
in this multi-class setting, we define it here. F-Measure is defined as the harmonic mean
of precision and recall: f = 2?p?rp+r ; p =
true positive
true positive+false positive ; r =
true positive
true positve+false negative .
This formula uses the number of true positive, false positive, and false nega-
tive spans in a given guessed labeling. True positive is the number of spans whose correct
label is one of the core or modifier argument labels (not NONE) and whose guessed label
is the same as the correct label. False positive is the number of spans whose guessed
label is non-NONE and whose correct label is different from the guessed label (possibly
NONE). False negative is the number of spans whose correct label is non-NONE andwhose
guessed label is not the same as the correct one (possibly NONE). In the figures in this
paper we show F-Measure multiplied by 100 so that it is in the same range as Whole
Frame Accuracy.
Core Argument Measures (CORE). These measures score the system on core
arguments only, without regard to modifier arguments. They can be obtained by first
mapping all non-core argument labels in the guessed and correct labelings to NONE.
Coarse Modifier Argument Measures (COARSEARGM). Sometimes it is sufficient to
know a given span has a modifier role, without knowledge of the specific role label. In
addition, deciding exact modifier argument labels was one of the decisions with highest
disagreement among annotators (Palmer, Gildea, and Kingsbury 2005). To estimate
performance under this setting, we relabel all ARGM-X arguments to ARGM in the
proposed and correct labeling. Such a performance measure was also used by Xue and
Palmer (2004). Note that these measures do not exclude the core arguments but instead
consider the core plus a coarse version of the modifier arguments. Thus for COARSEARGM
169
Computational Linguistics Volume 34, Number 2
ALL we count {0} as a true positive span, {1, 2} , {3, 4}, and {7, 8, 9} as false positive,
and {1, 2, 3, 4} and {7, 8, 9} as false negative.
Identification Measures (ID). These measure how well we do on the ARG vs. NONE
distinction. For the purposes of this evaluation, all spans labeled with a non-NONE label
are considered to have the generic label ARG. For example, to compute CORE ID, we
compare the following sets of labeled spans:
Correct: {0}-ARG, {7, 8, 9}-ARG
Guessed: {0}-ARG, {7, 8, 9}-ARG
The F-Measure is 1.0 and the Whole Frame Accuracy is 100%.
Classification Measures (CLS). These are performance on argument spans which
were also guessed to be argument spans (but possibly the exact label was wrong). In
other words, thesemeasures ignore the ARG vs. NONE confusions. They ignore all spans,
which were incorrectly labeled NONE, or incorrectly labeled with an argument label,
when the correct label was NONE. This is different from ?classification accuracy? used
in previous work to mean the accuracy of the system in classifying spans when the
correct set of argument spans is given. To compute CLS measures, we remove all spans
from Sguessed and Scorrect that do not occur in both sets, and compare the resulting sets.
For example, to compute the ALL CLS measures, we need to compare the following sets
of labeled spans:
Correct: {0}-ARG0, {7, 8, 9}-ARG2
Guessed: {0}-ARG0, {7, 8, 9}-ARG3
The rest of the spans were removed from both sets because they were labeled NONE
according to one of the labelings and non-NONE according to the other. The F-Measure
is .50 and the Whole Frame Accuracy is 0%.
As we mentioned before, we label and evaluate the semantic frame of every
predicate in the sentence separately. It is possible for a sentence to contain several
propositions?annotations of predicates occurring in the sentence. For example, in the
sentence The spacecraft faces a six-year journey to explore Jupiter, there are two propositions,
for the verbs faces and explore. These are:
[The spacecraft]ARG0 [faces]PRED [a six-year journey to explore Jupiter]ARG1.
[The spacecraft]ARG0 faces a six-year journey to [explore]PRED [Jupiter]ARG1.
Our evaluation measures compare the guessed and correct set of labeled spans for each
proposition.
3.3 Relation to the CoNLL Evaluation Measure
The CoNLL evaluation measure (Carreras and Ma`rquez 2004, 2005) is almost the same
as our argument-based measure. The only difference is that the CoNLL measure intro-
duces an additional label type for arguments, of the form R-ARGX, used for referring ex-
170
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
pressions. The Propbank distribution contains a specification of whichmulti-constituent
arguments are in a coreference chain. The CoNLL evaluation script considers these
multi-constituent arguments as several separate arguments having different labels,
where one argument has an ARGX label and the others have R-ARGX labels. The decision
of which constituents were to be labeled with referring labels was made using a set of
rules expressed with regular expressions.5 A script that converts Propbank annotations
to CoNLL format is available as part of the shared task software.
For example, in the following sentence, the CoNLL specification annotates the
arguments of began as follows:
[The deregulation]ARG1 of railroads [that]R-ARG1 [began]PRED enabled shippers to bargain
for transportation.
In contrast, we treat all multi-constituent arguments in the same way, and do not
distinguish coreferential versus non-coreferential split arguments. According to our
argument-based evaluation, the annotation of the arguments of the verb began is:
[The deregulation]ARG1 of railroads [that]C-ARG1 [began]PRED enabled shippers to bargain
for transportation.
The difference between our argument based measure and the CoNLL evaluation
measure is such that we cannot say that the value of one is always higher than the value
of the other. Either measure could be higher depending on the kinds of errors made.
For example, if the guessed labeling is: [The deregulation]ARG0 of railroads [that]R-ARG1
[began]PRED enabled shippers to bargain for transportation, the CoNLL script would
count the argument that as correct and report precision and recall of .5, whereas our
argument-based measure would not count any argument correct and report precision
and recall of 0. On the other hand, if the guessed labeling is [The deregulation]ARG1 of
railroads [that]C-ARG1 [began]PRED enabled shippers to bargain for transportation, the CoNLL
measure would report a precision and recall of 0, whereas our argument-based measure
would report precision and recall of 1. If the guessed labeling is [The deregulation]ARG1
of railroads [that]R-ARG1 [began]PRED enabled shippers to bargain for transportation, both
measures would report precision and recall of 1. (For our argument-based measure
it does not make sense to propose R-ARGX labels and we assume such labels would
be converted to C-ARGX labels if they are after the phrase they refer to.) Nevertheless,
overall we expect the two measures to yield very similar results.
4. Local Classifiers
A classifier is local if it assigns a probability (or score) to the label of an individual parse
tree node ni independently of the labels of other nodes.
In defining our models, we use the standard separation of the task of semantic role
labeling into identification and classification phases. Formally, let L denote a mapping
of the nodes in a tree t to a label set of semantic roles (including NONE) with respect to
a predicate v. Let Id(L) be the mapping which collapses L?s non-NONE values into ARG.
5 The regular expressions look for phrases containing pronouns with part-of-speech tags WDT, WRB, WP,
or WP$ (Xavier Carreras, personal communication).
171
Computational Linguistics Volume 34, Number 2
Then, like the Gildea and Jurafsky (2002) system, we decompose the probability of a
labeling L into probabilities according to an identification model PID and a classification
model PCLS.
PSRL(L|t, v) = PID(Id(L)|t, v)PCLS(L|t, v, Id(L)) (1)
This decomposition does not encode any independence assumptions, but is a use-
ful way of thinking about the problem. Our local models for semantic role labeling
use this decomposition. We use the same features for local identification and classi-
fication models, but use the decomposition for efficiency of training. The identifica-
tion models are trained to classify each node in a parse tree as ARG or NONE, and
the classification models are trained to label each argument node in the training set
with its specific label. In this way the training set for the classification models is
smaller. Note that we do not do any hard pruning at the identification stage in testing
and can find the exact labeling of the complete parse tree, which is the maximizer of
Equation (1).
We use log-linear models for multi-class classification for the local models. Because
they produce probability distributions, identification and classification models can be
chained in a principled way, as in Equation (1). The baseline features we used for the
local identification and classification models are outlined in Figure 3. These features are
a subset of the features used in previous work. The standard features at the top of the
figure were defined by Gildea and Jurafsky (2002), and the rest are other useful lexical
and structural features identified in more recent work (Surdeanu et al 2003; Pradhan
et al 2004; Xue and Palmer 2004). We also incorporated several novel features which
we describe next.
Figure 3
Baseline features.
172
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 4
Example of displaced arguments.
4.1 Additional Features for Displaced Constituents
We found that a large source of errors for ARG0 and ARG1 stemmed from cases such
as those illustrated in Figure 4, where arguments were dislocated by raising or control
verbs. Here, the predicate, expected, does not have a subject in the typical position?
indicated by the empty NP?because the auxiliary is has raised the subject to its current
position. In order to capture this class of examples, we use a binary feature, MISSING
SUBJECT, indicating whether the predicate is ?missing? its subject, and use this feature
in conjunction with the PATH feature, so that we learn typical paths to raised subjects
conditioned on the absence of the subject in its typical position.6
In the particular case of Figure 4, there is another instance of an argument being
quite far from its predicate. The predicate widen shares the phrase the trade gap with
expect as an ARG1 argument. However, as expect is a raising verb, widen?s subject is not
in its typical position either, and we should expect to find it in the same position as
expected?s subject. This indicates it may be useful to use the path relative to expected
to find arguments for widen. In general, to identify certain arguments of predicates
embedded in auxiliary and infinitival VPs we expect it to be helpful to take the path
from the maximum extended projection of the predicate?the highest VP in the chain
of VPs dominating the predicate. We introduce a new path feature, PROJECTED PATH,
which takes the path from the maximal extended projection to an argument node. This
feature applies only when the argument is not dominated by the maximal projection
(e.g., direct objects). These features also handle other cases of discontinuous and non-
local dependencies, such as those arising due to control verbs. The performance gain
from these new features was notable, especially in identification. The performance on
ALL arguments for the model using only the features in Figure 3, and the model using
the additional features as well, are shown in Figure 5. For these results, the constraint
that argument phrases do not overlap was enforced using the algorithm presented in
Section 4.2.
4.2 Enforcing the Non-Overlapping Constraint
The most direct way to use trained local identification and classification models in
testing is to select a labeling L of the parse tree that maximizes the product of the
6 We consider a verb to be missing its subject if the highest VP in the chain of VPs dominating the verb
does not have an NP or S(BAR) as its immediate left sister.
173
Computational Linguistics Volume 34, Number 2
Figure 5
Performance of local classifiers on ALL arguments, using the features in Figure 3 only and using
the additional local features. Using gold standard parse trees on Section 23.
probabilities according to the two models, as in Equation (1). Because these models are
local, this is equivalent to independently maximizing the product of the probabilities of
the twomodels for the label li of each parse tree node ni as shown below in Equation (2).
PSRL(L|t, v) =
?
ni?t
PID(Id(li)|t, v)
?
ni?t
PCLS(li|t, v, Id(li)) (2)
A problem with this approach is that a maximizing labeling of the nodes could possibly
violate the constraint that argument nodes should not overlap with each other. There-
fore, to produce a consistent set of arguments with local classifiers, we must have a way
of enforcing the non-overlapping constraint.
When labeling parse tree nodes, previous work has either used greedy algorithms
to find a non-overlapping assignment, or the general-purpose ILP approach of
Punyakanok et al (2004). For labeling chunks an exact algorithm based on shortest
paths was proposed in Punyakanok and Roth (2001). Its complexity is quadratic in the
length of the sentence.
Here we describe a faster exact dynamic programming algorithm to find the most
likely non-overlapping (consistent) labeling of all nodes in the parse tree, according
to a product of probabilities from local models, as in Equation (2). For simplicity, we
describe the dynamic program for the case where only two classes are possible: ARG and
NONE. The generalization to more classes is straightforward. Intuitively, the algorithm
is similar to the Viterbi algorithm for context-free grammars, because we can describe
the non-overlapping constraint by a ?grammar? that disallows ARG nodes having ARG
descendants.
Subsequently, we will talk about maximizing the sum of the logs of local proba-
bilities rather than the product of local probabilities, which is equivalent. The dynamic
program works from the leaves of the tree up and finds a best assignment for each
subtree, using already computed assignments for its children. Suppose we want the
most likely consistent assignment for subtree t with child trees t1, . . . , tk each storing
the most likely consistent assignment of its nodes, as well as the log-probability of the
ALLNONE assignment: the assignment of NONE to all nodes in the tree. The most likely
assignment for t is the one that corresponds to the maximum of:
 The sum of the log-probabilities of the most likely assignments of the child
subtrees t1, . . . , tk plus the log-probability of assigning the node t to NONE.
 The sum of the log-probabilities of the ALLNONE assignments of t1, . . . , tk
plus the log-probability of assigning the node t to ARG.
174
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 6
Performance of local model on ALL arguments when enforcing the non-overlapping constraint
or not.
The log-probability of the ALLNONE assignment for a tree t is the log-probability
of assigning the root node of t to NONE plus the sum of the log-probabilities of the
ALLNONE assignments of the child subtrees of t.
Propagating this procedure from the leaves to the root of t we have our most
likely non-overlapping assignment. By slightly modifying this procedure, we obtain the
most likely assignment according to a product of local identification and classification
models. We use the local models in conjunction with this search procedure to select a
most-likely labeling in testing.
The complexity of this algorithm is linear in the number of nodes in the parse tree,
which is usually much less than the square of the number of words in the sentence (l2),
the complexity of the Punyakanok and Roth (2001) algorithm. For example, for a binary-
branching parse tree, the number of nodes is approximately 2l. The speedup is due to
the fact that when we label parse tree nodes, we make use of the bracketing constraints
imposed by the parse tree. The shortest path algorithm proposed by Punyakanok and
Roth can also be adapted to achieve this lower computational complexity.
It turns out that enforcing the non-overlapping constraint does not lead to large
gains in performance. The results in Figure 5 are frommodels that use the dynamic pro-
gram for selecting non-overlapping arguments. To evaluate the gain from enforcing the
constraint, Figure 6 shows the performance of the same local model using all features,
when the dynamic program is used versus when a most likely possibly overlapping
assignment is chosen in testing.
The local model with basic plus additional features is our first pass model used
in re-ranking. The non-overlapping constraint is enforced using the dynamic program.
This is a state-of-the-art model. Its F-Measure on ALL arguments is 88.4 according to our
argument-based scoring measure. This is very similar to the best reported results (as of
2004) using gold-standard parse trees without null constituents and functional tags: 89.4
F-Measure reported for the Pradhan et al (2004) model.7
A more detailed analysis of the results obtained by the local model is given in Fig-
ure 7(a), and the two confusion matrices in Figures 7(b) and 7(c), which display the
number of errors of each type that the model made. The first confusion matrix con-
centrates on CORE arguments and merges all modifying argument labels into a single
ARGM label. The second concentrates on confusions among modifying arguments.
From the confusion matrix in Figure 7(b), we can see that the largest number of
errors are confusions of argument labels with NONE. The number of confusions between
pairs of core arguments is low, as is the number of confusions between core andmodifier
labels. If we ignore the column and row corresponding to NONE in Figure 7(b), the
number of off-diagonal entries is very small. This corresponds to the high F-Measures
7 The results in Pradhan et al (2004) are based on a measure which is more lenient than our
argument-based scoring (Sameer Pradhan, personal communication, July 2005). Our estimated
performance using his measure is 89.9.
175
Computational Linguistics Volume 34, Number 2
Figure 7
Performance measures for local model using all local features and enforcing the
non-overlapping constraint. Results are on Section 23 using gold standard parse trees.
on COARSEARGM CLS and CORE CLS, 98.1 and 98.0 respectively, shown in Figure 7(a).
The number of confusions of argument labels with NONE, shown in the NONE column,
is larger than the number of confusions of NONE with argument labels, shown in the
NONE row. This shows that the model generally has higher precision than recall. We
experimented with the precision?recall tradeoff but this did not result in an increase in
F-Measure.
From the confusion matrix in Figure 7(c) we can see that the number of confusions
between modifier argument labels is higher than the number of confusions between
core argument labels. This corresponds to the ALL CLS F-Measure of 95.7 versus the
CORE CLS F-Measure of 98.0. The per-label F-Measures in the last column show that the
performance on some very frequent modifier labels is in the low sixties or seventies.
The confusions between modifier labels and NONE are quite numerous.
Thus, to improve the performance on CORE arguments, we need to improve recall
without lowering precision. In particular, when the model is uncertain which of several
likely CORE labels to assign, we need to find additional sources of evidence to improve
its confidence. To improve the performance on modifier arguments, we also need to
lower the confusions among different modifier arguments. We will see that our joint
model improves the overall performance mainly by improving the performance on
176
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
CORE arguments, through increasing recall and precision by looking at wider sentence
context.
4.3 On Split Constituents
As discussed in Section 3, multiple constituents can be part of the same semantic
argument as specified by Propbank. An automatic system that has to recover such
information needs to have a way of indicating when multiple constituents labeled with
the same semantic role are a part of the same argument. Some researchers (Pradhan et al
2004; Punyakanok et al 2004) have chosen to make labels of the form C-ARGX distinct
argument labels that become additional classes in a multi-class constituent classifier.
These C-ARGX are used to indicate continuing arguments as illustrated in the two trees
in Figure 2. We chose to not introduce additional labels of this form, because they might
unnecessarily fragment the training data. Our automatic classifiers label constituents
with one of the core or modifier semantic role labels, and a simple post-processing
rule is applied to the output of the system to determine which constituents that are
labeled the same are to be merged as the same argument. The post-processing rule is
the following: For every constituent that bears a core argument label ARGX, if there is
a preceding constituent with the same label, re-label the current constituent C-ARGX.
Therefore, according to our algorithm, all constituents having the same core argument
label are part of the same argument, and all constituents having the samemodifier labels
are separate arguments by themselves. This rule is fairly accurate for core arguments but
is not always correct; it fails more often on modifier arguments. An evaluation of this
rule using the CoNLL data set and evaluation measure shows that our upper bound in
performance because of this rule is approximately 99.0 F-Measure on ALL arguments.
5. Joint Classifiers
We proceed to describe our models incorporating dependencies between labels of nodes
in the parse tree. As we discussed briefly before, the dependencies we would like to
model are highly non-local. A factorized sequence model that assumes a finite Markov
horizon, such as a chain CRF (Lafferty, McCallum, and Pereira 2001), would not be
able to encode such dependencies. We define a CRF with a much richer dependency
structure.
5.1 Form of the Joint Classifiers
Motivation for Re-Ranking. For argument identification, the number of possible as-
signments for a parse tree with n nodes is 2n. This number can run into the hundreds
of billions for a normal-sized tree. For argument labeling, the number of possible
assignments is ? 20m, if m is the number of arguments of a verb (typically between
2 and 5), and 20 is the approximate number of possible labels if considering both core
and modifying arguments. Training a model which has such a huge number of classes
is infeasible if the model does not factorize due to strong independence assumptions.
Therefore, in order to be able to incorporate long-range dependencies in our models,
we chose to adopt a re-ranking approach (Collins 2000), which selects from likely as-
signments generated by a model which makes stronger independence assumptions. We
utilize the top n assignments of our local semantic role labeling model PSRL to generate
likely assignments. As can be seen from Figure 8(a), for relatively small values of n, our
177
Computational Linguistics Volume 34, Number 2
re-ranking approach does not present a serious bottleneck to performance. We used a
value of n = 10 for training. In Figure 8(a) we can see that if we could pick, using an
oracle, the best assignment out of the top 10 assignments according to the local model,
we would achieve an F-Measure of 97.3 on all arguments. Increasing the number of n to
30 results in a very small gain in the upper bound on performance and a large increase
in memory requirements. We therefore selected n = 10 as a good compromise.
Generation of top n Most Likely Joint Assignments. We generate the top n most
likely non-overlapping joint assignments of labels to nodes in a parse tree according
to a local model PSRL, using an exact dynamic programming algorithm, which is a
direct generalization of the algorithm for finding the top non-overlapping assignment
described in Section 4.2.
Parametric Models.We learn log-linear re-ranking models for joint semantic role label-
ing, which use feature maps from a parse tree and label sequence to a vector space. The
form of the models is as follows. Let ?(t, v,L) ? Rs denote a feature map from a tree t,
target verb v, and joint assignment L of the nodes of the tree, to the vector space Rs. Let
L1,L2, ? ? ? ,LN denote the top N possible joint assignments. We learn a log-linear model
with a parameter vectorW, with one weight for each of the s dimensions of the feature
vector. The probability (or score) of an assignment L according to this re-ranking model
is defined as
PrSRL(L|t, v) =
e<?(t,v,L),W>
?N
j=1 e
<?(t,v,Lj ),W>
(3)
The score of an assignment L not in the top n is zero. We train the model to maximize the
sum of log-likelihoods of the best assignments minus a quadratic regularization term.
In this framework, we can define arbitrary features of labeled trees that capture general
properties of predicate?argument structure.
5.2 Joint Model Features
Wewill introduce the features of the joint re-rankingmodel in the context of the example
parse tree shown in Figure 9. We model dependencies not only between the label of a
Figure 8
Oracle upper bounds for top n non-overlapping assignments from local model on CORE and ALL
arguments, using gold-standard parse trees.
178
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 9
An example tree from Propbank with semantic role annotations, for the sentence Final-hour
trading accelerated to 108.1 million shares yesterday.
node and the labels of other nodes, but also dependencies between the label of a node
and input features of other argument nodes. The features are specified by instantiation
of templates and the value of a feature is the number of times a particular pattern occurs
in the labeled tree.
For a tree t, predicate v, and joint assignment L of labels to the nodes of the tree, we
define the candidate argument sequence as the sequence of non-NONE labeled nodes
[n1, l1, . . . , vPRED, . . . ,nm, lm] (li is the label of node ni). A reasonable candidate argument
sequence usually contains very few of the nodes in the tree?about 2 to 7?as this is
the typical number of arguments for a verb. To make it more convenient to express
our feature templates, we include the predicate node v in the sequence. This sequence
of labeled nodes is defined with respect to the left-to-right order of constituents in the
parse tree. Because non-NONE labeled nodes do not overlap, there is a strict left-to-right
order among these nodes. The candidate argument sequence that corresponds to the
correct assignment in Figure 9 is then:
[NP1-ARG1, VBD1-PRED, PP1-ARG4, NP3-ARGM-TMP]
Features from Local Models.All features included in the local models are also included
in our joint models. In particular, each template for local features is included as a joint
template that concatenates the local template and the node label. For example, for the
local feature PATH, we define a joint feature template that extracts PATH from every
node in the candidate argument sequence and concatenates it with the label of the
node. Both a feature with the specific argument label and a feature with the generic
back-off ARG label are created. This is similar to adding features from identification
and classification models. In the case of the example candidate argument sequence
provided, for the node NP1 we have the features:
{(NP?S?VP?VBD)-ARG1, (NP?S?VP?VBD)-ARG}
When comparing a local and a joint model, we use the same set of local feature
templates in the two models. If these were the only features that a joint model used,
we would expect its performance to be roughly the same as the performance of a
local model. This is because the two models will in fact be in the same parametric
family but will only differ slightly in the way the parameters are estimated. In
particular, the likelihood of an assignment according to the joint model with local
features will differ from the likelihood of the same assignment according to the local
model only in the denominator (the partition function). The joint model sums over
179
Computational Linguistics Volume 34, Number 2
a few likely assignments in the denominator, whereas the local model sums over all
assignments; also, the joint model does not treat the decomposition into identification
and classification models in exactly the same way as the local model.
Whole Label Sequence Features. As observed in previous work (Gildea and Jurafsky
2002; Pradhan et al 2004), including information about the set or sequence of labels
assigned to argument nodes should be very helpful for disambiguation. For example,
including such information will make the model less likely to pick multiple nodes to
fill the same role or to come up with a labeling that does not contain an obligatory
argument. We added a whole label sequence feature template that extracts the labels
of all argument nodes, and preserves information about the position of the predicate.
Two templates for whole label sequences were added: one having the predicate voice
only, and another also including the predicate lemma. These templates are instantiated
as follows for the example candidate argument sequence:
[voice:active, ARG1, PRED, ARG4, ARGM-TMP]
[voice:active, lemma:accelerate, ARG1, PRED, ARG4, ARGM-TMP]
We also add variants of these templates that use a generic ARG label instead of
specific labels for the arguments. These feature templates have the effect of counting the
number of arguments to the left and right of the predicate, which provides useful global
information about argument structure. A local model is not able to represent the count
of arguments since the label of each node is decided independently. This feature can
very directly and succinctly encode preferences for required arguments and expected
number of arguments.
As previously observed (Pradhan et al 2004), including modifying arguments in
sequence features is not helpful. This corresponds to the standard linguistic understand-
ing that there are no prevalent constraints on the position or presence of adjuncts in an
argument frame, and was confirmed in our experiments. We redefined the whole label
sequence features to exclude modifying arguments.
The whole label sequence features are the first type of features we add to relax
the independence assumptions of the local model. Because these features look at
the sequence of labels of all arguments, they capture joint information. There is no
limit on the length of the label sequence and thus there is no n-gram Markov order
independence assumption (in practice the candidate argument sequences in the top
n complete assignments are rarely more than 7 nodes long). Additionally, the nodes
in the candidate argument sequences are in general not in the same local tree in the
syntactic analysis and a tree-CRF model (Cohn and Blunsom 2005) would not be able
to encode these dependencies.
Joint Syntactic?Semantic Features. This class of features is similar to the whole label
sequence features, but in addition to labels of argument nodes, it includes syntactic
features of the nodes. These features can capture the joint mapping from the syntactic
realization of the predicate?s arguments to its semantic frame. The idea of these features
is to capture knowledge about the label of a constituent given the syntactic realization
and labels of all other arguments of the verb. This is helpful in capturing syntactic
alternations, such as the dative alternation. For example, consider the sentence (i) [Shaw
Publishing]ARG0 [offered]PRED [Mr. Smith]ARG2 [a reimbursement]ARG1 and the alternative re-
alization (ii) [Shaw Publishing]ARG0 [offered]PRED [a reimbursement]ARG1 [to Mr. Smith]ARG2.
180
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
When classifying the NP in object position, it is useful to know whether the following
argument is a PP. If it is, the NP will more likely be an ARG1, and if not, it will more
likely be an ARG2. A feature template that captures such information extracts, for each
candidate argument node, its phrase type and label. For example, the instantiations of
such templates in (ii), including only the predicate voice or also the predicate lemma,
would be:
[voice:active, NP-ARG0, PRED, NP-ARG1, PP-ARG2]
[voice:active,lemma:offer, NP-ARG0, PRED, NP-ARG1, PP-ARG2]
We experimented with extracting several kinds of features from each argument
node and found that the phrase type and the head of a directly dominating PP?if one
exists?were most helpful.
Local models normally consider only features of the phrase being classified in
addition to features of the predicate. They cannot take into account the features of other
argument nodes, because they are only given the input (parse tree), and the identity of
the argument nodes is unknown. It is conceivable that a local model could condition on
the features of all nodes in the tree but the number of parameters (features) would be
extremely large. The joint syntactic?semantic features proposed here encode important
dependencies using a very small number of parameters, as we will show in Section 5.4.
We should note that Xue and Palmer (2004) define a similar feature template, called
syntactic frame, which often captures similar information. The important difference is
that their template extracts contextual information from noun phrases surrounding the
predicate, rather than from the sequence of argument nodes. Because we use a joint
model, we are able to use information about other argument nodes when labeling a
node.
Repetition Features. We also add features that detect repetitions of the same label in
a candidate argument sequence, together with the phrase types of the nodes labeled
with that label. For example, (NP-ARG0, WHNP-ARG0) is a common pattern of this form.
Variants of this feature template also indicate whether all repeated arguments are sisters
in the parse tree, or whether all repeated arguments are adjacent in terms of word spans.
These features can provide robustness to parser errors, making it more likely to assign
the same label to adjacent phrases that may have been incorrectly split by the parser. In
Section 5.4 we report results from the joint model and an ablation study to determine
the contribution of each of the types of joint features.
5.3 Applying Joint Models in Testing
Here we describe the application in testing of a joint model for semantic role labeling,
using a local model PSRL and a joint re-ranking model P
r
SRL. The local model P

SRL is used
to generate N non-overlapping joint assignments L1, . . . ,LN.
One option is to select the best Li according to P
r
SRL, as in Equation (3), ignoring the
score from the local model. In our experiments, we noticed that for larger values of N,
the performance of our re-ranking model PrSRL decreased. This was probably due to the
fact that at test time the local classifier produces very poor argument frames near the
bottom of the top n for large n. Because the re-ranking model is trained on relatively
181
Computational Linguistics Volume 34, Number 2
few good argument frames, it cannot easily rule out very bad frames. It makes sense
then to incorporate the local model into our final score. Our final score is given by:
PSRL(L|t, v) = (PSRL(L|t, v))
? PrSRL(L|t, v)
where ? is a tunable parameter determining the amount of influence the local score has
on the final score (we found ? = 1.0 to work best). Such interpolation with a score from
a first-pass model was also used for parse re-ranking in (Collins 2000). Given this score,
at test time we choose among the top n local assignments L1, . . . ,Ln according to:
argmaxL?L1,...,Ln ? logP

SRL(L|t, v)+ logP
r
SRL(L|t, v) (4)
5.4 Joint Model Results
We compare the performance of joint re-ranking models and local models. We used
n = 10 joint assignments for training re-ranking models, and n = 15 for testing. The
weight ? of the local model was set to 1. Using different numbers of joint assignments
in training and testing is in general not ideal, but due to memory requirements, we
could not experiment with larger values of n for training.
Figure 10 shows the summary performance of the local model (LOCAL), repeated
from earlier figures, a joint model using only local features (JOINTLOCAL), a joint model
using local + whole label sequence features (LABELSEQ), and a joint model using all
described types of features (ALLJOINT). The evaluation is on gold-standard parse trees.
In addition to performance measures, the figure shows the number of binary features
included in the model. The number of features is a measure of the complexity of the
hypothesis space of the parametric model.
We can see that a joint model using only local features outperforms a local model
by .5 points of F-Measure. The joint model using local features estimates the feature
weights only using the top n consistent assignments, thus making the labels of different
nodes non-independent according to the estimation procedure, which may be a cause
of the improved performance. Another factor could be that the model JOINTLOCAL is a
combination of two models as specified in Equation (4), which may lead to gains (as is
usual for classifier combination).
The label sequence features added in Model LABELSEQ result in another 1.5 points
jump in F-Measure on all arguments. An additional .8 gain results from the inclusion
of syntactic?semantic and repetition features. The error reduction of model ALLJOINT
Figure 10
Performance of local and joint models on ID&CLS on Section 23, using gold-standard parse trees.
The number of features of each model is shown in thousands.
182
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
over the local model is 36.8% in CORE arguments F-Measure, 33.3% in CORE arguments
whole frame accuracy, 24.1% in ALL arguments F-Measure, and 21.7% in ALL arguments
whole frame accuracy. All differences in ALL arguments F-Measure are statistically
significant according to a paired Wilcoxon signed rank test. JOINTLOCAL is significantly
better than LOCAL (p < .001), LABELSEQ is significantly better than JOINTLOCAL (p <
.001), and ALLJOINT is significantly better than LABELSEQ (p < .001). We performed the
Wilcoxon signed rank test on per-proposition ALL arguments F-Measure for all models.
We also note that the joint models have fewer features than the local model. This is
due to the fact that the local model has seenmanymore negative examples and therefore
more unique features. The joint features are not very numerous compared to the local
features in the joint models. The ALLJOINT model has around 30% more features than
the JOINTLOCAL model.
These experiments showed that the label sequence features were very useful, es-
pecially on CORE arguments, increasing the F-Measure on these arguments by two
points when added to the JOINTLOCAL model. This shows that even though the
local model is optimized to use a large set of features and achieve state-of-the-art
performance, it is still advantageous to model the joint information in the sequence
of labels in a predicate?s argument frame. Additionally, the joint syntactic?semantic
features improved performance further, showing that when predicting the label of an
argument, it is useful to condition on the features of other arguments, in addition to
their labels.
A more detailed analysis of the results obtained by the joint model ALLJOINT
is given in Figure 11(a) (Summary results), and the two confusion matrices in Fig-
ures 11(b) and 11(c), which display the number of errors of each type that the model
made. The first confusion matrix concentrates on CORE arguments and merges all
modifying argument labels into a single ARGM label. The second confusion matrix
concentrates on confusions among modifying arguments. This figure can be compared
to Figure 7, which summarizes the results for the local model in the same form. The
biggest differences are in the performance on CORE arguments, which can be seen by
comparing the confusion matrices in Figures 7(b) and 11(b). The F-Measure on each
of the core argument labels has increased by at least three points: the F-Measure on
ARG2 by 5.7 points, and the F-Measure on ARG3 by eight points. The confusions of
core argument labels with NONE have gone down significantly, and also there is a
large decrease in the confusions of NONE with ARG1. There is generally a slight increase
in F-Measure on modifier labels as well, but the performance on some of the modifier
labels has gone down. This makes sense because our joint features are targeted at
capturing the dependencies among core arguments. There may be useful regularities
for modifier arguments as well, but capturing them may require different joint feature
templates.
Figure 12 lists the frequency with which each of the top k assignments from the
LOCAL model was ranked first by the re-ranking model ALLJOINT. For example, for
84.1% of the propositions, the re-ranking model chose the same assignment that the
LOCAL model would have chosen. The second best assignment according to the LOCAL
model was promoted to first 8.6% of the time. The figure shows statistics for the top ten
assignments only. The rest of the assignments, ranked 11 through 15, were chosen as
best by the re-ranking model for a total of 0.3% of the propositions.
The labeling of the tree in Figure 9 is a specific example of the kind of errors
fixed by the joint models. The local classifier labeled the first argument in the tree as
ARG0 instead of ARG1, probably because an ARG0 label is more likely for the subject
position.
183
Computational Linguistics Volume 34, Number 2
6. Semantic Role Labeling of Automatic Parses
We now evaluate our models when trained and tested using automatic parses produced
by Charniak?s parser. The Propbank training set Sections 2?21 is also the training set of
the parser. The performance of the parser is therefore better on the training set.When the
constituents of an argument do not have corresponding constituents in an automatically
produced parse tree, it will be very hard for a model to get the semantic role labeling
correct. However, this is not impossible and systems which are more robust to parser
error have been proposed (Pradhan et al 2005; Ma`rquez et al 2005). Our system can also
theoretically guess the correct set of words by labeling a set of constituents that cover
Figure 11
Performance measures for joint model using all features (AllJoint). Results are on Section 23
using gold-standard parse trees.
Figure 12
Percentage of test set propositions for which each of the top ten assignments from the Local
model was selected as best by the joint model AllJoint.
184
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 13
Percentage of argument constituents that are not present in the automatic parses of Charniak?s
parser. Constituents shows the percentage of missing constituents and Propositions shows the
percentage of propositions that have missing constituents.
the argument words, but we found that this rarely happens in practice. Figure 13 shows
the percentage of argument constituents that are missing in the automatic parse trees
produced by Charniak?s parser. We can see that the percentage of missing constituents
is quite high.
We report local and joint model results in Figures 14(a) and 14(b), respectively. As
for gold-standard parses, we test on all arguments regardless of whether they corre-
spond to constituents that have been recovered by the parser and use the samemeasures
detailed in Section 3.2. We also compare the confusion matrices for the local and joint
models, ignoring the confusions among modifier argument labels (COARSEARGM setting)
in Figure 15. The error reduction of the joint over the local model is 10.3% in CORE
arguments F-Measure and 8.3% in ALL arguments F-Measure.
6.1 Using Multiple Automatic Parse Guesses
Semantic role labeling is very sensitive to the correctness of the given parse tree, as the
results show. If an argument does not correspond to any constituent in a parse tree, or
a constituent exists but is not attached or labeled correctly, our model will have a very
hard time guessing the correct labeling.
Thus, if the syntactic parser makes errors, these errors influence directly the seman-
tic role labeling system. The theoretically correct way to propagate the uncertainty of
the syntactic parser is to consider (sum over) multiple possible parse trees, weighted by
their likelihood. In Finkel, Manning, and Ng (2006), this is approximated by sampling
Figure 14
Comparison of local and joint model results on Section 23 using Chaniak?s automatic parser.
185
Computational Linguistics Volume 34, Number 2
Figure 15
COARSEARGM argument confusion matrices for local and joint model using Charniak?s
automatic parses.
parse trees. We implement this idea by an argmax approximation, using the top k parse
trees from the parser of Charniak (2000).
We use these alternative parses as follows: Suppose t1, . . . , tk are trees for sentence
swith probabilities P(ti|s) given by the parser. Then for a fixed predicate v, let Li denote
the best joint labeling of tree ti, with score scoreSRL(Li|ti) according to our final joint
model. Then we choose the labeling Lwhich maximizes
argmaxi?{1,...,k}? logP(ti|S)+ scoreSRL(Li|ti)
This method of using multiple parse trees is very simple to implement and factors
in the uncertainty of the parser to some extent. However, according to this method (due
to the argmax operation) we are choosing a single parse and a complete semantic frame
derived from that parse. Other methods are able to derive different arguments of the
semantic frame from different syntactic annotations which maymake themmore robust
(Ma`rquez et al 2005; Pradhan, Ward et al 2005; Punyakanok, Roth, and Yih 2005).
Figure 16 shows summary results for the test set when using the top ten parses and
the joint model. The weighting parameter for the parser probabilities was ? = 1. We did
not experiment extensively with different values of ?. Preliminary experiments showed
that considering 15 parses was a bit better, and considering the top 20 was a bit worse.
6.2 Evaluation on the CoNLL 2005 Shared Task
The CoNLL 2005 data is derived from Propbank version I, which is the first official
release in 2005, whereas the results we have been reporting in the previous sections used
the pre-final February 2004 data. Using the CoNLL 2005 evaluation standard ensures
that results obtained by different groups are evaluated in exactly the same way. In
186
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
Figure 16
Performance of the joint model using the top ten parses from Charniak?s parser. Results are on
Section 23.
Propbank I, there have been several changes in the annotation conventions, as well as
error fixes and addition of new propositions. There was also a change in the way PP
arguments are annotated: In the February 2004 data some PP arguments are annotated
at the head NP child, but in Propbank I all PP arguments are annotated at the PP nodes.
In order to achieve maximal performance with respect to these annotations, it would
probably be best to change the feature definitions to account for the changes. However,
we did no adaptation of the features.
The training set consists of the annotations in Sections 2 to 21, the development set
is section 24 (Devset), and one of the test sets is section 23 (Test WSJ). The other test set
is from the Brown corpus (Test Brown). The CoNLL annotations distinguish referring
arguments, of the form R-ARGX, as discussed in Section 3.
Our approach to dealing with referring arguments and deciding when multiple
identically labeled constituents are part of the same argument was to label constituents
with only the set of argument labels and NONE and then map some of these labels
into referring or continuation labels. We converted an ARGX into a R-ARGX if and
only if the label of the constituent began with ?WH?. The rule for deciding when to
add continuation labels was the same as for our systems for the February 2004 data
described in Section 4.3: A constituent label becomes continuing if and only if it is a
core argument label and there is another constituent with the same core argument label
to the left. Therefore, for the CoNLL 2005 shared task we employ the same semantic
role labeling system, just using a different post-processing rule to map to CoNLL-style
labelings of sets of words.
We tested the upper bound in performance due to our conversion scheme in the
following way: Take the gold-standard CoNLL annotations for the development set
(including referring and continuing labels), convert these to basic argument labels of the
form ARGX, then convert the resulting labeling to CoNLL-style labeling using our rules
to recover the referring and continuing annotations. The F-Measure obtained was 99.0.
Figure 17 shows the performance of the local and joint model on one of the CoNLL
test sets?Test WSJ (Section 23)?when using gold-standard parse trees. Performance
on gold-standard parse trees was not measured in the CoNLL 2005 shared task, but we
report it here to provide a basis for comparison with the results of other researchers.
Figure 17
Results on the CoNLL WSJ Test set, when using gold-standard parse trees.
187
Computational Linguistics Volume 34, Number 2
Figure 18
Results on the CoNLL data set, when using Charniak automatic parse trees as provided in the
CoNLL 2005 shared task data.
Figure 19
Results on the CoNLL data set, using automatic parse trees from the May 2005 version of the
Charniak parser with correct treatment of forward quotes.
Next we present results using Charniak?s automatic parses on the development
and two test sets. We present results for the local and joint models using the max-
scoring Charniak parse tree. Additionally, we report results for the joint model using
the top five Charniak parse trees according to the algorithm described in Section 6.1.
The performance measures reported here are higher than the results of our submission
in the CoNLL 2005 shared task (Haghighi, Toutanova, and Manning 2005), because of
two changes. One was changing the rule that produces continuing arguments to only
add continuation labels to core argument labels; in the previous version the rule added
continuation labels to all repeated labels. Another was fixing a bug in the way the
sentences were passed in as input to Charniak?s parser, leading to incorrect analyses
of forward quotes.8
We first present results of our local and joint model using the parses provided as
part of the CoNLL 2005 data (and having wrong forward quotes) in Figure 18. We
then report results from the same local and joint model, and the joint model using the
top five Charniak parses, where the parses have correct representation of the forward
quotes in Figure 19. For these results we used the version of the Charniak parser from
4 May 2005. The results were very similar to the results we obtained with the version
from 18March 2005. We did not experiment with the new re-ranking model of Charniak
and Johnson (2005), even though it improves upon Charniak (2000) significantly.
For comparison, the system we submitted to CoNLL 2005 had an F-Measure of
78.45 on the WSJ Test set. The winning system (Punyakanok, Roth, and Yih 2005) had
an F-Measure of 79.44 and our current system has an F-Measure of 80.32. For the Brown
Test set, our submitted version had an F-Measure of 67.71, the winning system had
67.75, and our current system has 68.81.
Figure 20 shows the per-label performance of our joint model using the top five
Charniak parse trees on the Test WSJ test set. The columns show the Precision, Recall,
F-Measure, and the total number of arguments for each label.
8 The Charniak parses provided as part of the CoNLL shared task data uniformly ignore the distinction
between forward and backward quotes and all quotes are backward. We re-ran the parser and obtained
analyses with correct treatment of quotes.
188
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
7. Conclusions
In accord with standard linguistic assumptions, we have shown that there are sub-
stantial gains to be had by jointly modeling the argument frames of verbs. This is
especially true when we model the dependencies with discriminative models capable
of incorporating non-local features. We incorporated joint information by using two
types of features: features of the complete sequence of argument labels and features
modeling dependencies between the labels of arguments and syntactic features of other
arguments.We showed that both types of features yielded significant performance gains
over a state-of-the-art local model.
For further improving performance in the presence of perfect syntactic parses, we
see at least three promising avenues for improvement. First, one could improve the
identification of argument nodes, by better handling of long-distance dependencies; for
example, by incorporatingmodels which recover the trace and null element information
in Penn Treebank parse trees, as in Levy andManning (2004). Second, it may be possible
to improve the accuracy on modifier labels, by enhancing the knowledge about the
semantic characteristics of specific words and phrases, such as by improving lexical
statistics; for instance, our performance on ARGM-TMP roles is rather worse than that
of some other groups. Finally, it is worth exploring alternative handling of multi-
constituent arguments; our current model uses a simple rule in a post-processing step
Figure 20
Per-label performance of joint model using the top five Charniak automatic parse trees on the
Test WSJ test set.
189
Computational Linguistics Volume 34, Number 2
to decide which constituents given the same label are part of the same argument. This
could be done more intelligently by the machine learning model.
Because perfect syntactic parsers do not yet exist and the major bottleneck to the
performance of current semantic role labeling systems is syntactic parser performance,
the more important question is how to improve performance in the presence of parser
errors. We explored a simple approach of choosing from among the top k parses from
Charniak?s parser, which resulted in an improvement. Other methods have also been
proposed, as we discussed in Section 2 (Ma`rquez et al 2005; Pradhan, Ward et al 2005;
Punyakanok, Roth, and Yih 2005; Yi and Palmer 2005; Finkel, Manning, and Ng 2006).
This is a very promising line of research.
Acknowledgments
This research was carried out while all the
authors were at Stanford University. We
thank the journal reviewers and the
reviewers and audience at ACL 2005 and
CoNLL 2005 for their helpful comments. We
also thank Dan Jurafsky for his insightful
comments and useful discussions. This work
was supported in part by the Disruptive
Technology Organization (DTO)?s Advanced
Question Answering for Intelligence
(AQUAINT) Program.
References
Baker, Collin, Charles Fillmore, and John
Lowe. 1998. The Berkeley Framenet
project. In Proceedings of COLING-ACL,
pages 86?90, San Francisco, CA.
Carreras, Xavier and Lu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of CoNLL, pages 89?97, Boston, MA.
Carreras, Xavier and Lu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of CoNLL, pages 152?164, Ann Arbor, MI.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL, pages 132?139,
Seattle, WA.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
discriminative reranking. In Proceedings of
ACL, pages 173?180, Ann Arbor, MI.
Cohn, Trevor and Philip Blunsom. 2005.
Semantic role labelling with tree
conditional random fields. In Proceedings of
CoNLL, pages 169?172, Ann Arbor, MI.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In Proceedings of ICML, pages 175?182,
Stanford, CA.
Finkel, Jenny, Christopher Manning, and
Andrew Ng. 2006. Solving the problem of
cascading errors: Approximate bayesian
inference for linguistic annotation
pipelines. In Proceedings of EMNLP,
pages 618?626, Sydney, Australia.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Hacioglu, Kadri. 2004. A lightweight
semantic chunking model based on
tagging. In Proceedings of HLT-NAACL:
Short Papers, pages 145?148, Boston, MA.
Haghighi, Aria, Kristina Toutanova, and
Christopher D. Manning. 2005. A joint
model for semantic role labeling. In
Proceedings of CoNLL, pages 173?176,
Ann Arbor, MI.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of ICML, pages 282?289,
Williamstown, MA.
Levy, Roger and Chris Manning. 2004.
Deep dependencies from context-free
statistical parsers: correcting the
surface dependency approximation.
In Proceedings of ACL, pages 327?334,
Barcelona, Spain.
Ma`rquez, Lu??s, Mihai Surdeanu, Pere
Comas, and Jordi Turmo. 2005. A robust
combination strategy for semantic role
labeling. In Proceedings of EMNLP,
pages 644?651, Vancouver, Canada.
McCallum, Andrew, Dayne Freitag, and
Fernando Pereira. 2000. Maximum entropy
Markov models for information extraction
and segmentation. In Proceedings of ICML,
pages 591?598, Stanford, CA.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic
roles. Computational Linguistics,
31(1):71?105.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James Martin,
and Dan Jurafsky. 2005. Support
vector learning for semantic argument
190
Toutanova, Haghighi, and Manning A Global Joint Model for SRL
classification.Machine Learning Journal,
60(1):11?39.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan
Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In
Proceedings of HLT-NAACL, pages 233?240,
Boston, MA.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Daniel
Jurafsky. 2005. Semantic role labeling using
different syntactic views. In Proceedings of
ACL, pages 581?588, Ann Arbor, MI.
Punyakanok, Vasin and Dan Roth. 2001. The
use of classifiers in sequential inference.
In Proceedings of NIPS, pages 995?1001,
Vancouver, Canada.
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2005. The necessity of syntactic
parsing for semantic role labeling. In
Proceedings of IJCAI, pages 1117?1123,
Acapulco, Mexico.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
Dav Zimak, and Yuancheng Tu. 2004.
Semantic role labeling via generalized
inference over classifiers. In Proceedings
of CoNLL, pages 130?133, Boston, MA.
Roland, Douglas and Daniel Jurafsky. 2002.
Verb sense and verb subcategorization
probabilities. In Paola Merlo and
Suzanne Stevenson, editors, The Lexical
Basis of Sentence Processing: Formal,
Computational, and Experimental
Issues. John Benjamins, Amsterdam,
pages 325?345.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional
random fields. In Proceedings of
HLT-NAACL, pages 134?141,
Edmonton, Canada.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth.
2003. Using predicate-argument
structures for information extraction.
In Proceedings of ACL, pages 8?15,
Sapporo, Japan.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of EMNLP,
pages 88?94, Barcelona, Spain.
Yi, Szu-ting and Martha Palmer. 2005. The
integration of syntactic parsing and
semantic role labeling. In Proceedings of
CoNLL, pages 237?240, Ann Arbor, MI.
191

Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 309?310,
New York City, June 2006. c?2006 Association for Computational Linguistics
6. Automatic Semantic Role Labeling
Scott Wen-tau Yih and Kristina Toutanova, Microsoft Research
The goal of semantic role labeling is to map sentences to domain-independent semantic representations,
which abstract away from syntactic structure and are important for deep NLP tasks such as question answer-
ing, textual entailment, and complex information extraction. Semantic role labeling has recently received
significant interest in the natural language processing community. In this tutorial, we will first describe the
problem and history of semantic role labeling, and introduce existing corpora and other related tasks. Next,
we will provide a detailed survey of state-of-the-art machine learning approaches to building a semantic role
labeling system. Finally, we will conclude the tutorial by discussing directions for improving semantic role
labeling systems and their application to other natural language problems.
6.1 Tutorial Outline
1. Introduction
? What is semantic role labeling?
? Why is SRL important?
? Existing corpora: FrameNet & PropBank
? Corpora in development
? Relation to other tasks
2. Survey of Existing SRL Systems
? History of the development of automatic SRL systems
? Pioneering Work
? Basic architecture of a generic SRL system
? Major components
? Machine learning technologies
? CoNLL-04 and CoNLL-05 shared tasks on SRL
? Details of several CoNLL-05 systems
? Overall comparisons of CoNLL-05 systems
3. Analysis of Systems and Future Directions
? Error Analysis
? Influence of parser errors
? Per argument performance
? Directions for improving SRL
4. Applications
? Information Extraction
? Textual Entailment
? Machine Translation
6.2 Target Audience
The main target audience is NLP students and researchers who are interested in learning about semantic role
labeling, but have not followed all developments in the field. Additionally, researchers already working on
semantic role labeling should profit from a global view and summary of relevant work. The tutorial will
also be valuable for researchers working in the related areas of information extraction and spoken language
understanding.
309
Scott Wen-tau Yih received his PhD in Computer Science from the University of Illinois at Urbana-Champaign
in 2005 and is currently a Post-Doc Researcher in the Machine Learning and Applied Statistics group at Mi-
crosoft Research. His research focuses on different problems in natural language processing and machine
learning, such as information extraction and semantic parsing. Scott has published several papers on seman-
tic role labeling in CoNLL-04&05, COLING-04 and IJCAI-05. The SRL system he built at UIUC was the
best system in the CoNLL-05 shared task.
Kristina Toutanova obtained her PhD in Computer Science from Stanford University in 2005 and joined
Microsoft Research as a Researcher in the Natural Language Processing group. Her areas of expertise
include semantic role labeling, syntactic parsing, machine learning, and machine translation. Kristina has
published two papers on semantic role labeling in CoNLL-05 and ACL-05. The SRL system she built at
Stanford was the runner-up system in the CoNLL-05 shared task.
310
Proceedings of NAACL HLT 2007, pages 49?56,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Generating Case Markers in Machine Translation 
Kristina Toutanova  Hisami Suzuki 
Microsoft Research 
One Microsoft Way, Redmond WA 98052 USA 
{hisamis,kristout}@microsoft.com 
 
Abstract 
We study the use of rich syntax-based 
statistical models for generating gram-
matical case for the purpose of machine 
translation from a language which does 
not indicate case explicitly (English) to a 
language with a rich system of surface 
case markers (Japanese). We propose an 
extension of n-best re-ranking as a 
method of integrating such models into a 
statistical MT system and show that this 
method substantially outperforms stan-
dard n-best re-ranking. Our best perform-
ing model achieves a statistically signifi-
cant improvement over the baseline MT 
system according to the BLEU metric. 
Human evaluation also confirms the re-
sults. 
1 Introduction 
Generation of grammatical elements such as in-
flectional endings and case markers is an impor-
tant component technology for machine transla-
tion (MT). Statistical machine translation (SMT) 
systems, however, have not yet successfully in-
corporated components that generate grammati-
cal elements in the target language. Most state-
of-the-art SMT systems treat grammatical ele-
ments in exactly the same way as content words, 
and rely on general-purpose phrasal translations 
and target language models to generate these ele-
ments (e.g., Och and Ney, 2002; Koehn et al, 
2003; Quirk et al, 2005; Chiang, 2005; Galley et 
al., 2006). However, since these grammatical 
elements in the target language often correspond 
to long-range dependencies and/or do not have 
any words corresponding in the source, they may 
be difficult to model, and the output of an SMT 
system is often ungrammatical.  
 For example, Figure 1 shows an output from 
our baseline English-to-Japanese SMT system on 
a sentence from a computer domain. The SMT 
system, trained on this domain, produces a natu-
ral lexical translation for the English word patch 
as correction program, and translates replace 
into passive voice, which is more appropriate in 
Japanese. 1  However, there is a problem in the 
case marker assignment: the accusative marker 
wo, which was output by the SMT system, is 
completely inappropriate when the main verb is 
passive. This type of mistake in case marker as-
signment is by no means isolated in our SMT 
system: a manual analysis showed that 16 out of 
100 translations had mistakes solely in the as-
signment of case markers. A better model of case 
assignment could therefore improve the quality 
of an SMT system significantly.  
S: The patch replaces the .dll file.  
O: ????????.dll?????????????? 
    shuusei puroguramu-wo    .dll fairu-ga   okikae-raremasu 
    correction program-ACC dll file-NOM replace-PASS 
C: ????????.dll?????????????? 
    shuusei puroguramu-de    .dll fairu-ga   okikae-raremasu 
    correction program-with dll file-NOM replace-PASS  
Figure 1: Example of SMT (S: source; O: output of 
MT; C: correct translation) 
 In this paper, we explore the use of a statisti-
cal model for case marker generation in  English-
to-Japanese SMT. Though we focus on the gen-
eration of case markers in this paper, there are 
many other surface grammatical phenomena that 
can be modeled in a similar way, so any SMT 
system dealing with morpho-syntactically diver-
gent language pairs may benefit from a similar 
approach to modeling grammatical elements. Our 
model uses a rich set of syntactic features of both 
the source (English) and the target (Japanese) 
sentences, using context which is broader than 
that utilized by existing SMT systems. We show 
that the use of such features results in very high 
case assignment quality and also leads to a nota-
ble improvement in MT quality.  
Previous work has discussed the building of 
special-purpose classifiers which generate gram-
matical elements such as prepositions (Haji? et al 
2002), determiners (Knight and Chander, 1994) 
and case markers (Suzuki and Toutanova, 2006) 
with an eye toward improving MT output. How-
                                               
1
 There is a strong tendency to avoid transitive sentences 
with an inanimate subject in Japanese.  
49
ever, these components have not actually been 
integrated in an MT system. To our knowledge, 
this is the first work to integrate a grammatical 
element production model in an SMT system and 
to evaluate its impact in the context of end-to-
end MT.  
 A common approach of integrating new mod-
els with a statistical MT system is to add them as 
new feature functions which are used in decod-
ing or in models which re-rank n-best lists from 
the MT system (Och et al, 2004). In this paper 
we propose an extension of the n-best re-ranking 
approach, where we expand n-best candidate lists 
with multiple case assignment variations, and 
define new feature functions on this expanded 
candidate set. We show that expanding the n-best 
lists significantly outperforms standard n-best re-
ranking. We also show that integrating our case 
prediction model improves the quality of transla-
tion according to BLEU (Papineni et al, 2002) 
and human evaluation. 
2 Background 
In this section, we provide necessary background 
of the current work. 
2.1 Task of case marker prediction 
Our definition of the case marker prediction task 
follows Suzuki and Toutanova (2006). That is, 
we assume that we are given a source English 
sentence, and its translation in Japanese which 
does not include case markers. Our task is to pre-
dict all case markers in the Japanese sentence.   
We determine the location of case marker in-
sertion using the notion of bunsetsu. A bunsetsu 
consists of one content (head) word followed by 
any number of function words. We can therefore 
segment any sentence into a sequence of bun-
setsu by using a part-of-speech (POS) tagger. 
Once a sentence is segmented into bunsetsu, it 
is trivial to determine the location of case mark-
ers in a sentence: each bunsetsu can have at most 
one case marker, and the position of the case 
maker within a phrase is predictable, i.e., the 
rightmost position before any punctuation marks. 
The sentence in Figure 1 thus has the following 
bunsetsu analysis (denoted by square brackets), 
with the locations of potential case marker inser-
tion indicated by ?:  
[??'correction'?][?????'program'?][.dll?][?
???'file'?][????????'replace-PASS'??] 
For each of these positions, our task is to predict 
the case marker or to predict NONE, which means 
that the phrase does not have a case marker. 
The case markers we used for the prediction 
task are the same as those defined in Suzuki and 
Toutatnova (2006), and are summarized in Table 
1: in addition to the case markers in a strict sense, 
the topic marker wa is also included as well as 
the combination of a case marker plus the topic 
marker for the case markers with the column 
+wa checked in the table. In total, there are 18 
case markers to predict: ten simple case markers, 
the topic marker wa, and seven case+wa combi-
nations. The case prediction task is therefore a 
19-fold classification task: for each phrase, we 
assign one of the 18 case markers or NONE. 
2.2 Treelet translation system 
We constructed and evaluated our case predic-
tion model in the context of a treelet-based trans-
lation system, described in Quirk et al (2005).2 
In this approach, translation is guided by treelet 
translation pairs, where a treelet is a connected 
subgraph of a dependency tree.  
A sentence is translated in the treelet system 
as follows. The input sentence is first parsed into 
a dependency structure, which is then partitioned 
into treelets, assuming a uniform probability dis-
tribution over all partitions. Each source treelet is 
then matched to a treelet translation pair, the col-
lection of which will form the target translation. 
The target language treelets are then joined to 
form a single tree, and the ordering of all the 
nodes is determined, using the method described 
in Quirk et al (2005).  
Translations are scored according to a linear 
combination of feature functions:  
( ) ( )j j
j
score t f t?= ?  (1) 
                                               
2
 Though this paper reports results in the context of a treelet 
system, the model is also applicable to other syntax-based 
or phrase-based SMT systems.  
case markers grammatical functions +wa 
? 
ga subject; object  
? wo object; path  
? 
no genitive; subject  
? 
ni dative object, location 
? 
?? 
kara source 
? 
? 
to quotative, reciprocal, as 
? 
? 
de location,instrument, cause 
? 
? e goal, direction ? 
?? 
made goal (up to, until) 
? 
?? 
yori source, comparison target 
? 
? 
wa Topic  
Table 1. Case markers to be predicted 
50
where 
 
j are the model parameters and fj(t) is the 
value of the feature function j on the candidate t. 
There are ten feature functions in the treelet sys-
tem, including log-probabilities according to in-
verted and direct channel models estimated by 
relative frequency, lexical weighting channel 
models following Vogel et al (2003), a trigram 
target language model, an order model, word 
count, phrase count, average phrase size func-
tions, and whole-sentence IBM Model 1 log-
probabilities in both directions (Och et al 2004). 
The weights of these models are determined us-
ing the max-BLEU method described in Och 
(2003). As we describe in Section 4, the case 
prediction model is integrated into the system as 
an additional feature function.  
The treelet translation model is estimated us-
ing a parallel corpus. First, the corpus is word-
aligned using GIZA++ (Och and Ney, 2000); 
then the source sentences are parsed into a de-
pendency structure, and the dependency is pro-
jected onto the target side following the heuris-
tics described in Quirk et al (2005). Figure 2 
shows an example of an aligned sentence pair: on 
the source (English) side, POS tags and word 
dependency structure are assigned (solid arcs); 
the word alignments between English and Japa-
nese words are indicated by the dotted lines. On 
the target (Japanese) side, projected word de-
pendencies (solid arcs) are available. Additional 
annotations in Figure 2, namely the POS tags and 
the bunsetsu dependency structure (bold arcs) on 
the target side, are derived from the treelet sys-
tem to be used for building a case prediction 
model, which we describe in Section 3.  
2.3 Data 
All experiments reported in this paper are run 
using parallel data from a technical (computer) 
domain. We used two main data sets: train-500K, 
consisting of 500K sentence pairs which we used 
for training the baseline treelet system as well as 
the case prediction model, and a disjoint set of 
three data sets, lambda-1K, dev-1K and test-2K, 
which are used to integrate and evaluate the case 
prediction model in an end-to-end MT scenario. 
Some characteristics of these data sets are given 
in Table 2. We will refer to this table as we de-
scribe our experiments in later sections.  
# sent 
pairs 
# of words  
(average sent length in words) 
data set 
 English Japanese 
train-500K 500K 7,909,198 
(15.81) 
9,379,240 
(18.75) 
lambda-1K 1,000 15,219(15.2) 20,660 (20.7) 
dev-1K 1,000 15,397(15.4) 21,280 (21.3) 
test-2K 2,000 30,198(15.1) 41,269 (20.6) 
Table 2: Data set characteristics 
3 Statistical Models for Case Prediction 
in MT 
3.1 Case prediction model  
Our model of case marker prediction closely fol-
lows our previous work of case prediction in a 
non-MT context (Suzuki and Toutanova, 2006). 
The model is a multi-class log-linear (maximum 
entropy) classifier using 19 classes (18 case 
markers and NONE). It assigns a probability dis-
tribution over case marker assignments given a 
source English sentence, all non-case marker 
words of a candidate Japanese translation, and 
additional annotation information. Let t denote a 
Japanese translation, s a corresponding source 
sentence, and A additional annotation informa-
tion such as alignment, dependency structure, 
and POS tags (such as shown in Figure 2). Let 
rest(t) denote the sequence of words in t exclud-
ing all case markers, and case(t) a case marking 
assignment for all phrases in t. Our case marking 
model estimates the probability of a case as-
signment given all other information:  
),),(|)(( AstresttcasePcase  
The probability of a complete case assignment is 
a product over all phrases of the probability of 
the case marker of the phrase given all context 
features used by the model. Our model assumes 
that the case markers in a sentence are independ-
ent of each other given the input features. This 
independence assumption may seem strong, but 
the results presented in our previous work (Su-
zuki and Toutanova, 2006) showed that a joint 
model did not result in large improvements over 
a local one in predicting case markers in a non-
MT context. 
 
Figure 2. Aligned English-Japanese sentence pair 
51
3.2 Model features and feature selection  
The features of our model are similar to the ones 
described in Suzuki and Toutanova (2006). The 
main difference is that in the current model we 
applied a feature selection and induction algo-
rithm to determine the most useful features and 
feature combinations. This is important for un-
derstanding what sources of information are im-
portant for predicting grammatical elements, but 
are currently absent from SMT systems. We 
used 490K sentence pairs for training the case 
prediction model, which is a subset of the train-
500K set of Table 2. We divided the remaining 
10K sentences for feature selection (5K-feat) and 
for evaluating the case prediction models on ref-
erence translations (5K-test, discussed in Section 
3.3). The paired data is annotated using the 
treelet translation system: as shown in Figure 2, 
we have source and target word dependency 
structure, source language POS and word align-
ment directly from the aligned treelet structure. 
Additionally, we used a POS tagger of Japanese 
to assign POS to the target sentence as well as to 
parse the sentence into bunsetsu (indicated by 
brackets in Figure 2), using the method described 
in Section 2.1. We then compute bunsetsu de-
pendency structure on the target side (indicated 
by bold arcs in Figure 2) based on the word de-
pendency structure projected from English. We 
apply this procedure to annotate a paired corpus 
(in which case the Japanese sentence is a refer-
ence translation) as well as translations generated 
by the SMT system (which may potentially be 
ill-formed).  
We derived a large set of possible features 
from these annotations. The features are repre-
sented as feature templates, such as "Headword 
POS=X", which generate a set of binary features 
corresponding to different instantiations of the 
template, such as "Headword POS=NOUN". We 
applied an automatic feature selection and induc-
tion algorithm to the base set of templates. 
The feature selection algorithm considers the 
original templates as well as arbitrary (bigram 
and trigram) conjunctions of these templates. 
The algorithm performs forward stepwise feature 
selection, choosing templates which result in the 
highest increase in model accuracy on the 5K-
feat set mentioned above. The algorithm is simi-
lar to the one described in McCallum (2003).  
The application of this feature selection pro-
cedure gave us 17 templates, some of which are 
shown in Table 3, along with example instantia-
tions for the phrase headed by saabisu ?service? 
from Figure 2. Conjunctions are indicated by &. 
Note that many features that refer to POS and 
syntactic (parent) information are selected, on 
both the target and source sides. We also note 
that the context required by these features is 
more extensive than what is usually available 
during decoding in an SMT system due to a limit 
imposed on the treelet or phrase size. For exam-
ple, our model uses word lemma and POS tags of 
up to six words (previous word, next word, word 
in position +2, head word, previous head word 
and parent word), which covers more context 
than the treelet system we used (the system im-
poses the treelet size limit of four words). This 
means that the case model can make use of much 
richer information from both the source and tar-
get than the baseline MT system. Furthermore, 
our model makes better use of the context by 
combining the contributions of multiple sources 
of knowledge using a maximum entropy model, 
rather than using the relative frequency estimates 
with a very limited amount of smoothing, which 
are used by most state-of-the art SMT systems. 
3.3 Performance on reference translations 
Before discussing the integration of the case pre-
diction model with the MT system, we present an 
evaluation of the model on the task of predicting 
the case assignment of reference translations. 
This performance constitutes an upper bound on 
the model?s performance in MT, because in ref-
erence translations, the word choice and the word 
order are perfect. 
 Table 4 summarizes the results of the refer-
ence experiments on the 5K-test set using two 
metrics: accuracy, which denotes the percentage 
of phrases for which the respective model 
guessed the case marker correctly, and BLEU 
score against the reference translation. For com-
Features Example 
Words in position  ?1 and +2 kono,moodo 
Headword & previous headword saabisu&kono 
Parent word kaishi 
Aligned word  service 
Parent of word aligned to headword started 
Next word POS NOUN 
Next word & next word POS seefu&NN 
Headword POS NOUN 
Parent headword POS VN 
Aligned to parent word POS & next word 
POS & prev word POS 
VERB&NN&an
d 
Parent POS of word aligned to headword VERB 
Aligned word POS & headword POS & 
prev word POS 
NN&NN&ADN 
POS of word aligned to headword NOUN 
Table 3: Features for the case prediction model 
52
parison, we also include results from two base-
lines: a frequency-based baseline, which always 
assigns the most likely class (NONE), and a lan-
guage model (LM) baseline, which is one of the 
standard methods of generating grammatical 
elements in MT. We trained a word-trigram LM 
using the CMU toolkit (Clarkson and Rosenfeld, 
1997) on the same 490K sentences which we 
used for training the case prediction model. 
Table 4 shows that our model performs sub-
stantially better than both baselines: the accuracy 
of the frequency-based baseline is 59%, and an 
LM-based model improves it to 87.2%. In con-
trast, our model achieves an accuracy of 95%, 
which is a 60% error reduction over the LM 
baseline. It is also interesting to note that as the 
accuracy goes up, so does the BLEU score.   
 These results show that our best model can 
very effectively predict case markers when the 
input to the model is clean, i.e., when the input 
has correct words in correct order. Next, we see 
the impact of applying this model to improve MT 
output.  
4 Integrating Case Prediction Models in 
MT 
In the end-to-end MT scenario, we integrate our 
case assignment model with the SMT system and 
evaluate its contribution to the final MT output.  
 As a method of integration with the MT sys-
tem, we chose an n-best re-ranking approach, 
where the baseline MT system is left unchanged 
and additional models are integrated in the form 
of feature functions via re-ranking of n-best lists 
from the system. Such an approach has been 
taken by Och et al (2004) for integrating sophis-
ticated syntax-informed models in a phrase-
based SMT system. We also chose this approach 
for ease of implementation: as discussed in Sec-
tion 3.2, the features we use in our case model 
extend over long distance, and are not readily 
available during decoding. Though a tighter inte-
gration with the decoding process is certainly 
worth exploring in the future, we have taken an 
approach here that allows fast experimentation.  
 Within the space of n-best re-ranking, we 
have considered two variations: the standard n-
best re-ranking method, and our significantly 
better performing extension. These are now dis-
cussed in turn.  
4.1 Method 1: Standard n-best re-ranking 
This method is a straightforward application of 
the n-best re-ranking approach described in Och 
et al (2004). As described in Section 2.2, our 
baseline SMT system is a linear model which 
weighs the values of ten feature functions. To 
integrate a case prediction model, we simply add 
it to the linear model as an 11th feature function, 
whose value is the log-probability of the case 
assignment of the candidate hypothesis t accord-
ing to our model. The weights of all feature func-
tions are then re-estimated using max-BLEU 
training on the n-best list of the lambda-1K set in 
Table 2. As we show in Section 5, this re-ranking 
method did not result in good performance.  
4.2 Method 2: Re-ranking of expanded 
candidate lists 
A drawback of the previous method is that in an 
n-best list, there may not be sufficiently many 
case assignment variations of existing hypothe-
ses. If this is the case, the model cannot be effec-
tive in choosing a hypothesis with a good case 
assignment. We performed a simple experiment 
to test this. We took the first (best) hypothesis t 
from the MT system and generated the top 40 
case variations t? of t, according to the case as-
signment model. These variations differ from t 
only in their case markers. We wanted to see 
what fraction of these new hypotheses t? oc-
curred in a 1000-best list of the MT system. In 
the dev-1K set of Table 2, the fraction of new 
case variations of the first hypothesis occurring 
in the 1000-best list of hypotheses was 0.023. 
This means that only less than one (2.3% of 40 = 
0.92) case variant of the first hypothesis is ex-
pected to be found in the 1000-best list, indicat-
ing that even an n-best list for a reasonably large 
n (such as 1000) does not contain enough candi-
dates varying in case marker assignment. 
 In order to allow more case marking candi-
dates to be considered, we propose the following 
method to expand the candidate translation list: 
for each translation t in the n-best list of the base-
line SMT system, we also consider case assign-
ment variations of t. For simplicity, we chose to 
consider the top k case assignment variations of 
each hypothesis according to our case model,3 
for 1 ? k ? 40.4  
                                               
3
 From a computational standpoint, it is non-trivial to con-
Model ACC BLEU 
Baseline (frequency) 58.9 40.0 
Baseline (490K LM) 87.2 83.6 
Log-linear model 94.9 93.0 
Table 4: Accuracy (%) and BLEU score for case 
prediction when given correct context (reference 
translations) on the 5K-test set 
53
  After we expand the translation candidate set, 
we compute feature functions for all candidates 
and train a linear model which chooses from this 
larger set. While some features (e.g., word count 
feature) are easy to recompute for a new candi-
date, other features (e.g., treelet phrase transla-
tion probability) are difficult to recompute. We 
have chosen to recompute only four features of 
the baseline model:  the language model feature, 
the word count feature, and the direct and reverse 
whole-sentence IBM Model 1 features,  assum-
ing that the values of the other baseline model 
features for a casing variation t? of t are the same 
as their values for t. In addition, we added the 
following four feature functions, specifically 
meant to capture the extent to which the newly 
generated case marking variations differ from the 
original baseline system hypotheses they are de-
rived from: 
 Generated: a binary feature with a value of 0 
for original baseline system candidates, and a 
value of 1 for newly generated candidates. 
 Number NONE?non-NONE: the count of case 
markers changed from NONE to non-NONE 
with respect to an original translation candi-
date. 
 Number non-NONE?NONE: the count of case 
markers changed from non-NONE to NONE. 
 Number non-NONE?non-NONE: the count of 
case markers changed from non-NONE to an-
other non-NONE case marker. 
Note that these newly defined features all have a 
value of 0 for original baseline system candidates 
(i.e., when k=0) and therefore would have no 
effect in Method 1. Therefore, the only differ-
ence between our two methods of integration is 
the presence or absence of case-expanded candi-
date translations. 
5 Experiments and Results  
5.1 Data and settings 
For our end-to-end MT experiments, we used 
three datasets in Table 2 that are disjoint from 
the train-500K data set. They consist of source 
English sentences and their top 1000 candidate 
translations produced by the baseline SMT sys-
                                                                       
sider all possible case assignment variations of a hypothesis: 
even though the case assignment score for a sentence is 
locally decomposable, there are still global dependencies in 
the linear model from Equation (1) due to the reverse 
whole-sentence IBM model 1 score used as a feature func-
tion.  
4
 Our results indicate that additional case variations would 
not be helpful. 
tem. These datasets are the lambda-1K set for 
training the weights   of the linear model from 
Equation (1), the dev-1K set for model selection, 
and the test-2K set for final testing including 
human evaluation. 
5.2 Results  
The results for the end-to-end experiments on the 
dev-1K set are summarized in Table 5. The table 
is divided into four sections. The first section 
(row) shows the BLEU score of the baseline 
SMT system, which is equivalent to the 1-best 
re-ranking scenario with no case expansion. The 
BLEU score for the baseline was 37.99. In the 
table, we also show the oracle BLEU scores for 
each model, which are computed by greedily se-
lecting the translation in the candidate list with 
the highest BLEU score.5 
The second section of Table 5 corresponds to 
the results obtained by Method 1, i.e., the stan-
dard n-best re-ranking, for n = 20, 100, and 1000. 
Even though the oracle scores improve as n is 
increased, the actual performance improves only 
slightly. These results show that the strategy of 
only including the new information as features in 
a standard n-best re-ranking scenario does not 
lead to an improvement over the baseline. 
 In contrast, Method 2 obtains notable im-
provements over the baseline. Recall that we ex-
pand the n-best SMT candidates with their k-best 
case marking variations in this method, and re-
                                               
5
 A modified version of BLEU was used to compute sen-
tence-level BLEU in order to select the best hypothesis per 
sentence. The table shows corpus-level BLEU on the result-
ing set of translations. 
Models #MT 
hypothe
ses 
#case  
expan-
sions 
BLEU Oracle 
BLEU 
Baseline 1 0 37.99 37.99 
 20 0 37.83 41.79 
Method 1 100 0 38.02 42.79 
 1000 0 38.08 43.14 
 1 1 38.18 38.75 
Method 2 1 10 38.42 40.51 
 1 20 38.54 41.15 
 1 40 38.41 41.74 
 20 10 38.91 45.32 
 20 20 38.72 45.94 
Method 2 20 40 38.78 46.56 
 100 10 38.73 46.87 
 100 20 38.64 47.47 
 100 40 38.74 47.96 
Table 5. Results of end-to-end experiments on the 
dev-1K set 
54
train the model parameters on the resulting can-
didate lists. For the values n=1 and k=1 (which 
we refer to as 1best-1case), we observe a small 
BLEU gain of .19 over the baseline. Even though 
this is not a big improvement, it is still better 
than the improvement of standard n-best re-
ranking with a 1000-best list. By considering 
more case marker variations (k = 10, 20 and 40), 
we are able to gain about a half BLEU point over 
the baseline. The fact that using more case varia-
tions performs better than using only the best 
case assignment candidate proposed by the case 
model suggests that the proposed approach, 
which integrates the case prediction model as a 
feature function and retrains the weights of the 
linear model, works better than using the case 
prediction model as a post-processor of the MT 
output.  
The last section of the table explores combi-
nations of the values for n and k. Considering 20 
best SMT candidates and their top 10 case varia-
tions gave the highest BLEU score on the dev-
1K set of 38.91, which is an 0.92 BLEU points 
improvement over the baseline. Considering 
more case variations (20 or 40), and more SMT 
candidates (100) resulted in a similar but slightly 
lower performance in BLEU. This is presumably 
because the case model does affect the choice of 
content words as well, but this influence is lim-
ited and can be best captured when using a small 
number (n=20) of baseline system candidates.  
Based on these results on the dev-1K set, we 
chose the best model (i.e., 20-best-10case) and 
evaluated it on the test-2K set against the base-
line. Using the pair-wise statistical test design 
described in Collins et al (2005), the BLEU im-
provement (35.53 vs. 36.29) was statistically 
significant (p < .01) according to the Wilcoxon 
signed-rank test. 
5.3 Human evaluation 
These results demonstrate that the proposed 
model is effective at improving the translation 
quality according to the BLEU score. In this sec-
tion, we report the results of human evaluation to 
ensure that the improvements in BLEU lead to 
better translations according to human evaluators. 
 We performed human evaluation on the 
20best-10case (n=20, k=10) and 1best-40case 
(n=1, k=40) models against the baseline using 
our final test set, the test-2K data. The perform-
ance in BLEU of these models on the full test-2K 
data was 35.53 for the baseline, 36.09 for the 
1best-40case model, and 36.29 for the 20best-
10case model, respectively. 
In our human evaluation, two annotators were 
asked to evaluate a random set of 100 sentences 
for which the models being compared produced 
different translations. The judges were asked to 
compare two translations, the baseline output 
from the original SMT system and the output 
chosen by the system augmented with the case 
marker generation component. Each judge was 
asked to run two separate evaluations along dif-
ferent evaluation criteria. In the evaluation of 
fluency, the judges were asked to decide which 
translation is more readable/grammatical, ignor-
ing the reference translation. In the evaluation of 
adequacy, they were asked to judge which trans-
lation more correctly reflects the meaning of the 
reference translation. In either setting, they were 
not given the source sentence.  
 Table 6 summarizes the results of the evalua-
tion of the 20best-10case model. The table shows 
the results along two evaluation criteria sepa-
rately, fluency on the left and adequacy on the 
right. The evaluation results of Annotator #1 are 
shown in the columns, while those of Annotator 
#2 are in the rows. Each grid in the table shows 
the number of sentences the annotators classified 
as the proposed system output better (S), the 
baseline system better (B) or the translations are 
of equal quality (E). Along the diagonal (in bold-
face) are the judgments that were agreed on by 
the two annotators: both annotators judged the 
output of the proposed system to be more fluent 
in 27 translations, less fluent in 9 translations; 
they judged that our system output was more 
adequate in 17 translations and less adequate in 9 
translations. Our system output was thus judged 
better under both criteria, though according to a 
sign test, the improvement is statistically signifi-
cant (p < .01) in fluency, but not in adequacy.  
One of the reasons for this inconclusive result 
is that human evaluation may be very difficult 
and can be unreliable when evaluating very dif-
ferent translation candidates, which happens of-
ten when comparing the results of models that 
consider n-best candidates where n>1, as is the 
case with the 20best-10case model. In Table 6, 
Fluency Adequacy 
Annotator #1 Annotator #1 
 
S B E S B E 
S 27 1 8 17 0 9 
B 1 9 16 0 9 12 
Anno- 
tator 
#2 E 7 4 27 9 8 36 
Table 6. Results of human evaluation comparing 
20best-10case vs. baseline. S: proposed system is bet-
ter; B: baseline is better; E: of equal quality  
55
we can see that the raw agreement rate between 
the two annotators (i.e., number of agreed judg-
ments over all judgments) is only 63% (27+9+27 
/100) in fluency and 62% (17+9+36/100) in ade-
quacy. We therefore performed an additional 
human evaluation where translations being com-
pared differ only in case markers: the baseline vs. 
the 1best-40case model output. The results are 
shown in Table 7.  
This evaluation has a higher rate of agreement, 
74% for fluency and 71% for adequacy, indicat-
ing that comparing two translations that differ 
only minimally (i.e., in case markers) is more 
reliable. The improvements achieved by our 
model are statistically significant in both fluency 
and adequacy according to a sign test; in particu-
lar, it is remarkable that on 42 sentences, the 
judges agreed that our system was better in flu-
ency, and there were no sentences on which the 
judges agreed that our system caused degradation. 
This means that the proposed system, when 
choosing among candidates differing only in case 
markers, can improve the quality of MT output 
in an extremely precise manner, i.e. making im-
provements without causing degradations. 
6 Conclusion 
We have described a method of using a case 
marker generation model to improve the quality 
of English-to-Japanese MT output. We have 
shown that the use of such a model contributes to 
improving MT output, both in BLEU and human 
evaluation. We have also proposed an extension 
of n-best re-ranking which significantly outper-
formed standard n-best re-ranking. This method 
should be generally applicable to integrating 
models which target specific phenomena in 
translation, and for which an extremely large n-
best list would be needed to cover enough vari-
ants of the phenomena in question. 
Our model improves the quality of generated 
case markers in an extremely precise manner. 
We believe this result is significant, as there are 
many phenomena in the target language of MT 
that may be improved by using special-purpose 
models, including the generation of articles, aux-
iliaries, inflection and agreement. We plan to 
extend and generalize the current approach to 
cover these phenomena in morphologically com-
plex languages in general in the future. 
References 
Clarkson, P.R. and R. Rosenfeld. 1997. Statistical 
Language Modeling Using the CMU-Cambridge 
Toolkit. In ESCA Eurospeech, pp. 2007-2010. 
Collins, M., P. Koehn and I. Ku?erov?. 2005. Clause 
Restructuring for Statistical Machine Translation. 
In ACL, pp.531-540.  
Chiang, D. 2005. A Hierarchical Phrase-based Model 
for Statistical Machine Translation. In ACL. 
Galley, M., J. Graehl, K. Knight, D. Marcu, S. 
DeNeefe, W. Wang and I. Thayer. 2006. Scalable 
Inference and Training of Context-Rich Syntactic 
Translation Models. In ACL. 
Koehn, P., F. J. Och and D. Marcu. 2003. Statistical 
Phrase-based Translation. In HLT-NAACL. 
Haji?, J., M. ?mejrek, B. Dorr, Y. Ding, J. Eisner, D. 
Gildea, T. Koo, K. Parton, G. Penn, D. Radev and 
O. Rambow. 2002. Natural Language Generation 
in the Context of Machine Translation. Technical 
report, Center for Language and Speech Process-
ing, Johns Hopkins University 2002 Summer Work-
shop Final Report.  
Knight, K. and I. Chander. 1994. Automatic Postedit-
ing of Documents. In AAAI.  
McCallum, A. 2003. Efficiently inducing features of 
conditional random fields. In UAI. 
Och, F. J. 2003. Minimum Error-rate Training for 
Statistical Machine Translation. In ACL. 
Och, F. J. and H. Ney. 2000. Improved Statistical 
Alignment Models. In ACL.  
Och, F. J. and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Ma-
chine Translation. In ACL 2002. 
Och, F. J., D. Gildea, S. Khudanpur, A. Sarkar, K. 
Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, 
K. Eng, V. Jain, Z. Jin and D. Radev. 2004. A 
Smorgasbord of Features for Statistical Machine 
Translation. In NAACL. 
Papineni, K., S. Roukos, T. Ward and W.J. Zhu. 2002. 
BLEU: A Method for Automatic Evaluation of 
Machine Translation. In ACL.  
Quirk, C., A. Menezes and C. Cherry. 2005. Depend-
ency Tree Translation: Syntactically Informed 
Phrasal SMT. In ACL. 
Suzuki, H. and K. Toutanova. 2006. Learning to Pre-
dict Case Markers in Japanese. In ACL-COLING. 
Vogel, S., Y. Zhang, F. Huang, A. Tribble, A. 
Venugopal, B. Zhao and A. Waibel. 2003. The 
CMU Statistical Machine Translation System. In 
Proceedings of the MT Summit.  
Fluency Adequacy 
Annotator #1 Annotator #1 
 
S B E S B E 
S 42 0 9 30 1 9 
B 1 0 7 0 9 7 
Anno- 
tator 
#2 E 7 2 32 9 3 32 
Table 7. Results of human evaluation comparing 
1best-40case vs. baseline  
56
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209?217,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Morphological Segmentation with Log-Linear Models
Hoifung Poon?
Dept. of Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
hoifung@cs.washington.edu
Colin Cherry
Microsoft Research
Redmond, WA 98052
colinc@microsoft.com
Kristina Toutanova
Microsoft Research
Redmond, WA 98052
kristout@microsoft.com
Abstract
Morphological segmentation breaks words
into morphemes (the basic semantic units). It
is a key component for natural language pro-
cessing systems. Unsupervised morphologi-
cal segmentation is attractive, because in ev-
ery language there are virtually unlimited sup-
plies of text, but very few labeled resources.
However, most existing model-based systems
for unsupervised morphological segmentation
use directed generative models, making it dif-
ficult to leverage arbitrary overlapping fea-
tures that are potentially helpful to learning.
In this paper, we present the first log-linear
model for unsupervised morphological seg-
mentation. Our model uses overlapping fea-
tures such as morphemes and their contexts,
and incorporates exponential priors inspired
by the minimum description length (MDL)
principle. We present efficient algorithms
for learning and inference by combining con-
trastive estimation with sampling. Our sys-
tem, based on monolingual features only, out-
performs a state-of-the-art system by a large
margin, even when the latter uses bilingual in-
formation such as phrasal alignment and pho-
netic correspondence. On the Arabic Penn
Treebank, our system reduces F1 error by 11%
compared to Morfessor.
1 Introduction
The goal of morphological segmentation is to seg-
ment words into morphemes, the basic syntac-
tic/semantic units. This is a key subtask in many
? This research was conducted during the author?s intern-
ship at Microsoft Research.
NLP applications, including machine translation,
speech recognition and question answering. Past
approaches include rule-based morphological an-
alyzers (Buckwalter, 2004) and supervised learn-
ing (Habash and Rambow, 2005). While successful,
these require deep language expertise and a long and
laborious process in system building or labeling.
Unsupervised approaches are attractive due to the
the availability of large quantities of unlabeled text,
and unsupervised morphological segmentation has
been extensively studied for a number of languages
(Brent et al, 1995; Goldsmith, 2001; Dasgupta and
Ng, 2007; Creutz and Lagus, 2007). The lack
of supervised labels makes it even more important
to leverage rich features and global dependencies.
However, existing systems use directed generative
models (Creutz and Lagus, 2007; Snyder and Barzi-
lay, 2008b), making it difficult to extend them with
arbitrary overlapping dependencies that are poten-
tially helpful to segmentation.
In this paper, we present the first log-linear model
for unsupervised morphological segmentation. Our
model incorporates simple priors inspired by the
minimum description length (MDL) principle, as
well as overlapping features such as morphemes and
their contexts (e.g., in Arabic, the string Al is likely
a morpheme, as is any string between Al and a word
boundary). We develop efficient learning and infer-
ence algorithms using a novel combination of two
ideas from previous work on unsupervised learn-
ing with log-linear models: contrastive estimation
(Smith and Eisner, 2005) and sampling (Poon and
Domingos, 2008).
We focus on inflectional morphology and test our
209
approach on datasets in Arabic and Hebrew. Our
system, using monolingual features only, outper-
forms Snyder & Barzilay (2008b) by a large mar-
gin, even when their system uses bilingual informa-
tion such as phrasal alignment and phonetic corre-
spondence. On the Arabic Penn Treebank, our sys-
tem reduces F1 error by 11% compared to Mor-
fessor Categories-MAP (Creutz and Lagus, 2007).
Our system can be readily applied to supervised
and semi-supervised learning. Using a fraction of
the labeled data, it already outperforms Snyder &
Barzilay?s supervised results (2008a), which further
demonstrates the benefit of using a log-linear model.
2 Related Work
There is a large body of work on the unsupervised
learning of morphology. In addition to morpholog-
ical segmentation, there has been work on unsuper-
vised morpheme analysis, where one needs to deter-
mine features of word forms (Kurimo et al, 2007)
or identify words with the same lemma by model-
ing stem changes (Schone and Jurafsky, 2001; Gold-
smith, 2001). However, we focus our review specif-
ically on morphological segmentation.
In the absence of labels, unsupervised learning
must incorporate a strong learning bias that reflects
prior knowledge about the task. In morphological
segmentation, an often-used bias is the minimum
description length (MDL) principle, which favors
compact representations of the lexicon and corpus
(Brent et al, 1995; Goldsmith, 2001; Creutz and La-
gus, 2007). Other approaches use statistics on mor-
pheme context, such as conditional entropy between
adjacent n-grams, to identify morpheme candidates
(Harris, 1955; Keshava and Pitler, 2006). In this pa-
per, we incorporate both intuitions into a simple yet
powerful model, and show that each contributes sig-
nificantly to performance.
Unsupervised morphological segmentation sys-
tems also differ from the engineering perspective.
Some adopt a pipeline approach (Schone and Ju-
rafsky, 2001; Dasgupta and Ng, 2007; Demberg,
2007), which works by first extracting candidate
affixes and stems, and then segmenting the words
based on the candidates. Others model segmenta-
tion using a joint probabilistic distribution (Goldwa-
ter et al, 2006; Creutz and Lagus, 2007; Snyder and
Barzilay, 2008b); they learn the model parameters
from unlabeled data and produce the most proba-
ble segmentation as the final output. The latter ap-
proach is arguably more appealing from the mod-
eling standpoint and avoids error propagation along
the pipeline. However, most existing systems use
directed generative models; Creutz & Lagus (2007)
used an HMM, while Goldwater et al (2006) and
Snyder & Barzilay (2008b) used Bayesian models
based on Pitman-Yor or Dirichlet processes. These
models are difficult to extend with arbitrary overlap-
ping features that can help improve accuracy.
In this work we incorporate novel overlapping
contextual features and show that they greatly im-
prove performance. Non-overlapping contextual
features previously have been used in directed gen-
erative models (in the form of Markov models) for
unsupervised morphological segmentation (Creutz
and Lagus, 2007) or word segmentation (Goldwater
et al, 2007). In terms of feature sets, our model is
most closely related to the constituent-context model
proposed by Klein and Manning (2001) for grammar
induction. If we exclude the priors, our model can
also be seen as a semi-Markov conditional random
field (CRF) model (Sarawagi and Cohen, 2004).
Semi-Markov CRFs previously have been used for
supervised word segmentation (Andrew, 2006), but
not for unsupervised morphological segmentation.
Unsupervised learning with log-linear models has
received little attention in the past. Two notable ex-
ceptions are Smith & Eisner (2005) for POS tagging,
and Poon & Domingos (2008) for coreference res-
olution. Learning with log-linear models requires
computing the normalization constant (a.k.a. the
partition function) Z . This is already challenging in
supervised learning. In unsupervised learning, the
difficulty is further compounded by the absence of
supervised labels. Smith & Eisner (2005) proposed
contrastive estimation, which uses a small neighbor-
hood to compute Z . The neighborhood is carefully
designed so that it not only makes computation eas-
ier but also offers sufficient contrastive information
to aid unsupervised learning. Poon & Domingos
(2008), on the other hand, used sampling to approx-
imate Z .1 In this work, we benefit from both tech-
niques: contrastive estimation creates a manageable,
1Rosenfeld (1997) also did this for language modeling.
210
wvlAvwn
(##__##)
w
(##__vl)
vlAv
(#w__wn)
wn
(Av__##)
Figure 1: The morpheme and context (in parentheses)
features for the segmented word w-vlAv-wn.
informative Z , while sampling enables the use of
powerful global features.
3 Log-Linear Model for Unsupervised
Morphological Segmentation
Central to our approach is a log-linear model that
defines the joint probability distribution for a cor-
pus (i.e., the words) and a segmentation on the cor-
pus. The core of this model is a morpheme-context
model, with one feature for each morpheme,2 and
one feature for each morpheme context. We rep-
resent contexts using the n-grams before and after
the morpheme, for some constant n. To illustrate
this, a segmented Arabic corpus is shown below
along with its features, assuming we are tracking bi-
gram contexts. The segmentation is indicated with
hyphens, while the hash symbol (#) represents the
word boundary.
Segmented Corpus hnAk w-vlAv-wn bn-w
Al-ywm Al-jmAEp
Morpheme Feature:Value hnAk:1 w:2 vlAv:1
wn:1 bn:1 Al:2 ywm:1 jmAEp:1
hnAk:1 wvlAvwn:1 bnw:1 Alywm:1 Alj-
mAEp:1
Bigram Context Feature:Value ## vl:1
#w wn:1 Av ##:1 ## w#:1 bn ##:1
## yw:1 Al ##:2 ## jm:1 ## ##:5
Furthermore, the corresponding features for the seg-
mented word w-vlAv-wn are shown in Figure 1.
Each feature is associated with a weight, which
correlates with the likelihood that the correspond-
ing morpheme or context marks a valid morpholog-
ical segment. Such overlapping features allow us to
capture rich segmentation regularities. For example,
given the Arabic word Alywm, to derive its correct
segmentation Al-ywm, it helps to know that Al and
ywm are likely morphemes whereas Aly or lyw are
2The word as a whole is also treated as a morpheme in itself.
not; it also helps to know that Al ## or ## yw are
likely morpheme contexts whereas ly ## or ## wm
are not. Ablation tests verify the importance of these
overlapping features (see Section 7.2).
Our morpheme-context model is inspired by
the constituent-context model (CCM) proposed by
Klein and Manning (2001) for grammar induction.
The morphological segmentation of a word can be
viewed as a flat tree, where the root node corre-
sponds to the word and the leaves correspond to
morphemes (see Figure 1). The CCM uses uni-
grams for context features. For this task, however,
we found that bigrams and trigrams lead to much
better accuracy. We use trigrams in our full model.
For learning, one can either view the corpus as
a collection of word types (unique words) or tokens
(word occurrences). Some systems (e.g., Morfessor)
use token frequency for parameter estimation. Our
system, however, performs much better using word
types. This has also been observed for other mor-
phological learners (Goldwater et al, 2006). Thus
we use types in learning and inference, and effec-
tively enforce the constraint that words can have
only one segmentation per type. Evaluation is still
based on tokens to reflect the performance in real
applications.
In addition to the features of the morpheme-
context model, we incorporate two priors which cap-
ture additional intuitions about morphological seg-
mentations. First, we observe that the number of
distinct morphemes used to segment a corpus should
be small. This is achieved when the same mor-
phemes are re-used across many different words.
Our model incorporates this intuition by imposing
a lexicon prior: an exponential prior with nega-
tive weight on the length of the morpheme lexi-
con. We define the lexicon to be the set of unique
morphemes identified by a complete segmentation
of the corpus, and the lexicon length to be the to-
tal number of characters in the lexicon. In this
way, we can simultaneously emphasize that a lexi-
con should contain few unique morphemes, and that
those morphemes should be short. However, the lex-
icon prior alone incorrectly favors the trivial seg-
mentation that shatters each word into characters,
which results in the smallest lexicon possible (sin-
gle characters). Therefore, we also impose a corpus
prior: an exponential prior on the number of mor-
211
phemes used to segment each word in the corpus,
which penalizes over-segmentation. We notice that
longer words tend to have more morphemes. There-
fore, each word?s contribution to this prior is nor-
malized by the word?s length in characters (e.g., the
segmented word w-vlAv-wn contributes 3/7 to the to-
tal corpus size). Notice that it is straightforward to
incorporate such a prior in a log-linear model, but
much more challenging to do so in a directed gen-
erative model. These two priors are inspired by the
minimum description length (MDL) length princi-
ple; the lexicon prior favors fewer morpheme types,
whereas the corpus prior favors fewer morpheme to-
kens. They are vital to the success of our model,
providing it with the initial inductive bias.
We also notice that often a word is decomposed
into a stem and some prefixes and suffixes. This is
particularly true for languages with predominantly
inflectional morphology, such as Arabic, Hebrew,
and English. Thus our model uses separate lexicons
for prefixes, stems, and suffixes. This results in a
small but non-negligible accuracy gain in our exper-
iments. We require that a stem contain at least two
characters and no fewer characters than any affixes
in the same word.3 In a given word, when a mor-
pheme is identified as the stem, any preceding mor-
pheme is identified as a prefix, whereas any follow-
ing morpheme as a suffix. The sample segmented
corpus mentioned earlier induces the following lex-
icons:
Prefix w Al
Stem hnAk vlAv bn ywm jmAEp
Suffix wn w
Before presenting our formal model, we first in-
troduce some notation. Let W be a corpus (i.e., a set
of words), and S be a segmentation that breaks each
word in W into prefixes, a stem, and suffixes. Let ?
be a string (character sequence). Each occurrence of
? will be in the form of ?1??2, where ?1, ?2 are the
adjacent character n-grams, and c = (?1, ?2) is the
context of ? in this occurrence. Thus a segmentation
can be viewed as a set of morpheme strings and their
contexts. For a string x, L(x) denotes the number of
characters in x; for a word w, MS(w) denotes the
3In a segmentation where several morphemes have the max-
imum length, any of them can be identified as the stem, each
resulting in a distinct segmentation.
number of morphemes in w given the segmentation
S; Pref(W,S), Stem(W,S), Suff(W,S) denote
the lexicons of prefixes, stems, and suffixes induced
by S for W . Then, our model defines a joint proba-
bility distribution over a restricted set of W and S:
P?(W,S) = 1Z ? u?(W,S)
where
u?(W,S) = exp(
?
?
??f?(S) +
?
c
?cfc(S)
+ ? ? ?
??Pref(W,S)
L(?)
+ ? ? ?
??Stem(W,S)
L(?)
+ ? ? ?
??Suff(W,S)
L(?)
+ ? ? ?
w?W
MS(w)/L(w) )
Here, f?(S) and fc(S) are respectively the occur-
rence counts of morphemes and contexts under S,
and ? = (??, ?c : ?, c) are their feature weights.
?, ? are the weights for the priors. Z is the nor-
malization constant, which sums over a set of cor-
pora and segmentations. In the next section, we will
define this set for our model and show how to effi-
ciently perform learning and inference.
4 Unsupervised Learning
As mentioned in Smith & Eisner (2005), learning
with probabilistic models can be viewed as moving
probability mass to the observed data. The question
is from where to take this mass. For log-linear mod-
els, the answer amounts to defining the set that Z
sums over. We use contrastive estimation and define
the set to be a neighborhood of the observed data.
The instances in the neighborhood can be viewed
as pseudo-negative examples, and learning seeks to
discriminate them from the observed instances.
Formally, let W ? be the observed corpus, and let
N(?) be a function that maps a string to a set of
strings; let N(W ?) denote the set of all corpora that
can be derived from W ? by replacing every word
w ?W ? with one in N(w). Then,
Z = ?
W?N(W ?)
?
S
u(W,S).
212
Unsupervised learning maximizes the log-likelihood
of observing W ?
L?(W ?) = log
?
S
P (W ?, S)
We use gradient descent for this optimization; the
partial derivatives for feature weights are
?
??i
L?(W ?) = ES|W ?[fi]? ES,W [fi]
where i is either a string ? or a context c. The first
expected count ranges over all possible segmenta-
tions while the words are fixed to those observed in
W ?. For the second expected count, the words also
range over the neighborhood.
Smith & Eisner (2005) considered various neigh-
borhoods for unsupervised POS tagging, and
showed that the best neighborhoods are TRANS1
(transposing any pair of adjacent words) and
DELORTRANS1 (deleting any word or transposing
any pair of adjacent words). We can obtain their
counterparts for morphological segmentation by
simply replacing ?words? with ?characters?. As
mentioned earlier, the instances in the neighbor-
hood serve as pseudo-negative examples from which
probability mass can be taken away. In this regard,
DELORTRANS1 is suitable for POS tagging since
deleting a word often results in an ungrammatical
sentence. However, in morphology, a word less a
character is often a legitimate word too. For exam-
ple, deleting l from the Hebrew word lyhwh (to the
lord) results in yhwh (the lord). Thus DELORTRANS1
forces legal words to compete against each other for
probability mass, which seems like a misguided ob-
jective. Therefore, in our model we use TRANS1. It
is suited for our task because transposing a pair of
adjacent characters usually results in a non-word.
To combat overfitting in learning, we impose a
Gaussian prior (L2 regularization) on all weights.
5 Supervised Learning
Our learning algorithm can be readily applied to su-
pervised or semi-supervised learning. Suppose that
gold segmentation is available for some words, de-
noted as S?. If S? contains gold segmentations
for all words in W , we are doing supervised learn-
ing; otherwise, learning is semi-supervised. Train-
ing now maximizes L?(W ?, S?); the partial deriva-
tives become
?
??i
L?(W ?, S?) = ES|W ?,S?[fi] ? ES,W [fi]
The only difference in comparison with unsuper-
vised learning is that we fix the known segmenta-
tion when computing the first expected counts. In
Section 7.3, we show that when labels are available,
our model also learns much more effectively than a
directed graphical model.
6 Inference
In Smith & Eisner (2005), the objects (sentences) are
independent from each other, and exact inference is
tractable. In our model, however, the lexicon prior
renders all objects (words) interdependent in terms
of segmentation decisions. Consider the simple cor-
pus with just two words: Alrb, lAlrb. If lAlrb is seg-
mented into l-Al-rb, Alrb can be segmented into Al-
rb without paying the penalty imposed by the lexi-
con prior. If, however, lAlrb remains a single mor-
pheme, and we still segment Alrb into Al-rb, then
we introduce two new morphemes into the lexicons,
and we will be penalized by the lexicon prior ac-
cordingly. As a result, we must segment the whole
corpus jointly, making exact inference intractable.
Therefore, we resort to approximate inference. To
compute ES|W ?[fi], we use Gibbs sampling. To de-
rive a sample, the procedure goes through each word
and samples the next segmentation conditioned on
the segmentation of all other words. With m sam-
ples S1, ? ? ? , Sm, the expected count can be approx-
imated as
ES|W ?[fi] ? 1m
?
j
fi(Sj)
There are 2n?1 ways to segment a word of n char-
acters. To sample a new segmentation for a partic-
ular word, we need to compute conditional proba-
bility for each of these segmentations. We currently
do this by explicit enumeration.4 When n is large,
4These segmentations could be enumerated implicitly us-
ing the dynamic programming framework employed by semi-
Markov CRFs (Sarawagi and Cohen, 2004). However, in such a
setting, our lexicon prior would likely need to be approximated.
We intend to investigate this in future work.
213
this is very expensive. However, we observe that
the maximum number of morphemes that a word
contains is usually a small constant for many lan-
guages; in the Arabic Penn Treebank, the longest
word contains 14 characters, but the maximum num-
ber of morphemes in a word is only 5. Therefore,
we impose the constraint that a word can be seg-
mented into no more than k morphemes, where k
is a language-specific constant. We can determine
k from prior knowledge or use a development set.
This constraint substantially reduces the number of
segmentation candidates to consider; with k = 5, it
reduces the number of segmentations to consider by
almost 90% for a word of 14 characters.
ES,W [fi] can be computed by Gibbs sampling in
the same way, except that in each step we also sam-
ple the next word from the neighborhood, in addition
to the next segmentation.
To compute the most probable segmentation, we
use deterministic annealing. It works just like a sam-
pling algorithm except that the weights are divided
by a temperature, which starts with a large value and
gradually drops to a value close to zero. To make
burn-in faster, when computing the expected counts,
we initialize the sampler with the most probable seg-
mentation output by annealing.
7 Experiments
We evaluated our system on two datasets. Our main
evaluation is on a multi-lingual dataset constructed
by Snyder & Barzilay (2008a; 2008b). It consists of
6192 short parallel phrases in Hebrew, Arabic, Ara-
maic (a dialect of Arabic), and English. The paral-
lel phrases were extracted from the Hebrew Bible
and its translations via word alignment and post-
processing. For Arabic, the gold segmentation was
obtained using a highly accurate Arabic morpholog-
ical analyzer (Habash and Rambow, 2005); for He-
brew, from a Bible edition distributed by Westmin-
ster Hebrew Institute (Groves and Lowery, 2006).
There is no gold segmentation for English and Ara-
maic. Like Snyder & Barzilay, we evaluate on the
Arabic and Hebrew portions only; unlike their ap-
proach, our system does not use any bilingual in-
formation. We refer to this dataset as S&B . We
also report our results on the Arabic Penn Treebank
(ATB), which provides gold segmentations for an
Arabic corpus with about 120,000 Arabic words.
As in previous work, we report recall, precision,
and F1 over segmentation points. We used 500
phrases from the S&B dataset for feature develop-
ment, and also tuned our model hyperparameters
there. The weights for the lexicon and corpus pri-
ors were set to ? = ?1, ? = ?20. The feature
weights were initialized to zero and were penalized
by a Gaussian prior with ?2 = 100. The learning
rate was set to 0.02 for all experiments, except the
full Arabic Penn Treebank, for which it was set to
0.005.5 We used 30 iterations for learning. In each
iteration, 200 samples were collected to compute
each of the two expected counts. The sampler was
initialized by running annealing for 2000 samples,
with the temperature dropping from 10 to 0.1 at 0.1
decrements. The most probable segmentation was
obtained by running annealing for 10000 samples,
using the same temperature schedule. We restricted
the segmentation candidates to those with no greater
than five segments in all experiments.
7.1 Unsupervised Segmentation on S&B
We followed the experimental set-up of Snyder &
Barzilay (2008b) to enable a direct comparison. The
dataset is split into a training set with 4/5 of the
phrases, and a test set with the remaining 1/5. First,
we carried out unsupervised learning on the training
data, and computed the most probable segmentation
for it. Then we fixed the learned weights and the seg-
mentation for training, and computed the most prob-
able segmentation for the test set, on which we eval-
uated.6 Snyder & Barzilay (2008b) compared sev-
eral versions of their systems, differing in how much
bilingual information was used. Using monolingual
information only, their system (S&B-MONO) trails
the state-of-the-art system Morfessor; however, their
best system (S&B-BEST), which uses bilingual in-
formation that includes phrasal alignment and pho-
netic correspondence between Arabic and Hebrew,
outperforms Morfessor and achieves the state-of-
the-art results on this dataset.
5The ATB set is more than an order of magnitude larger and
requires a smaller rate.
6With unsupervised learning, we can use the entire dataset
for training since no labels are provided. However, this set-
up is necessary for S&B?s system because they used bilingual
information in training, which is not available at test time.
214
ARABIC Prec. Rec. F1
S&B-MONO 53.0 78.5 63.2
S&B-BEST 67.8 77.3 72.2
FULL 76.0 80.2 78.1
HEBREW Prec. Rec. F1
S&B-MONO 55.8 64.4 59.8
S&B-BEST 64.9 62.9 63.9
FULL 67.6 66.1 66.9
Table 1: Comparison of segmentation results on the S&B
dataset.
Table 1 compares our system with theirs. Our sys-
tem outperforms both S&B-MONO and S&B-BEST
by a large margin. For example, on Arabic, our sys-
tem reduces F1 error by 21% compared to S&B-
BEST, and by 40% compared to S&B-MONO. This
suggests that the use of monolingual morpheme con-
text, enabled by our log-linear model, is more help-
ful than their bilingual cues.
7.2 Ablation Tests
To evaluate the contributions of the major compo-
nents in our model, we conducted seven ablation
tests on the S&B dataset, each using a model that
differed from our full model in one aspect. The first
three tests evaluate the effect of priors, whereas the
next three test the effect of context features. The
last evaluates the impact of using separate lexicons
for affixes and stems.
NO-PRIOR The priors are not used.
NO-COR-PR The corpus prior is not used.
NO-LEX-PR The lexicon prior is not used.
NO-CONTEXT Context features are not used.
UNIGRAM Unigrams are used in context.
BIGRAM Bigrams are used in context.
SG-LEXICON A single lexicon is used, rather than
three distinct ones for the affixes and stems.
Table 2 presents the ablation results in compari-
son with the results of the full model. When some or
all priors are excluded, the F1 score drops substan-
tially (over 10 points in all cases, and over 40 points
in some). In particular, excluding the corpus prior,
as in NO-PRIOR and NO-COR-PR, results in over-
segmentation, as is evident from the high recalls and
low precisions. When the corpus prior is enacted
but not the lexicon priors (NO-LEX-PR), precision
ARABIC Prec. Rec. F1
FULL 76.0 80.2 78.1
NO-PRIOR 24.6 89.3 38.6
NO-COR-PR 23.7 87.4 37.2
NO-LEX-PR 79.1 51.3 62.3
NO-CONTEXT 71.2 62.1 66.3
UNIGRAM 71.3 76.5 73.8
BIGRAM 73.1 78.4 75.7
SG-LEXICON 72.8 82.0 77.1
HEBREW Prec. Rec. F1
FULL 67.6 66.1 66.9
NO-PRIOR 34.0 89.9 49.4
NO-COR-PR 35.6 90.6 51.1
NO-LEX-PR 65.9 49.2 56.4
NO-CONTEXT 63.0 47.6 54.3
UNIGRAM 63.0 63.7 63.3
BIGRAM 69.5 66.1 67.8
SG-LEXICON 67.4 65.7 66.6
Table 2: Ablation test results on the S&B dataset.
is much higher, but recall is low; the system now errs
on under-segmentation because recurring strings are
often not identified as morphemes.
A large accuracy drop (over 10 points in F1
score) also occurs when the context features are
excluded (NO-CONTEXT), which underscores the
importance of these overlapping features. We also
notice that the NO-CONTEXT model is compara-
ble to the S&B-MONO model; they use the same
feature types, but different priors. The accuracies of
the two systems are comparable, which suggests that
we did not sacrifice accuracy by trading the more
complex and restrictive Dirichlet process prior for
exponential priors. A priori, it is unclear whether us-
ing contexts larger than unigrams would help. While
potentially beneficial, they also risk aggravating the
data sparsity and making our model more prone to
overfitting. For this problem, however, enlarging the
context (using higher n-grams up to trigrams) helps
substantially. For Arabic, the highest accuracy is at-
tained by using trigrams, which reduces F1 error by
16% compared to unigrams; for Hebrew, by using
bigrams, which reduces F1 error by 17%. Finally, it
helps to use separate lexicons for affixes and stems,
although the difference is small.
215
ARABIC %Lbl. Prec. Rec. F1
S&B-MONO-S 100 73.2 92.4 81.7
S&B-BEST-S 200 77.8 92.3 84.4
FULL-S 25 84.9 85.5 85.2
50 88.2 86.8 87.5
75 89.6 86.4 87.9
100 91.7 88.5 90.0
HEBREW %Lbl. Prec. Rec. F1
S&B-MONO-S 100 71.4 79.1 75.1
S&B-BEST-S 200 76.8 79.2 78.0
FULL-S 25 78.7 73.3 75.9
50 82.8 74.6 78.4
75 83.1 77.3 80.1
100 83.0 78.9 80.9
Table 3: Comparison of segmentation results with super-
vised and semi-supervised learning on the S&B dataset.
7.3 Supervised and Semi-Supervised Learning
To evaluate our system in the supervised and semi-
supervised learning settings, we report the perfor-
mance when various amounts of labeled data are
made available during learning, and compare them
to the results of Snyder & Barzilay (2008a). They
reported results for supervised learning using mono-
lingual features only (S&B-MONO-S), and for su-
pervised bilingual learning with labels for both lan-
guages (S&B-BEST-S). On both languages, our sys-
tem substantially outperforms both S&B-MONO-S
and S&B-BEST-S. E.g., on Arabic, our system re-
duces F1 errors by 46% compared to S&B-MONO-
S, and by 36% compared to S&B-BEST-S. More-
over, with only one-fourth of the labeled data, our
system already outperforms S&B-MONO-S. This
demonstrates that our log-linear model is better
suited to take advantage of supervised labels.
7.4 Arabic Penn Treebank
We also evaluated our system on the Arabic Penn
Treebank (ATB). As is common in unsupervised
learning, we trained and evaluated on the entire set.
We compare our system with Morfessor (Creutz and
Lagus, 2007).7 In addition, we compare with Mor-
fessor Categories-MAP, which builds on Morfessor
and conducts an additional greedy search specifi-
cally tailored to segmentation. We found that it per-
7We cannot compare with Snyder & Barzilay?s system as its
strongest results require bilingual data, which is not available.
ATB-7000 Prec. Rec. F1
MORFESSOR-1.0 70.6 34.3 46.1
MORFESSOR-MAP 86.9 46.4 60.5
FULL 83.4 77.3 80.2
ATB Prec. Rec. F1
MORFESSOR-1.0 80.7 20.4 32.6
MORFESSOR-MAP 77.4 72.6 74.9
FULL 88.5 69.2 77.7
Table 4: Comparison of segmentation results on the Ara-
bic Penn Treebank.
forms much better than Morfessor on Arabic but
worse on Hebrew. To test each system in a low-
data setting, we also ran experiments on the set con-
taining the first 7,000 words in ATB with at least
two characters (ATB-7000). Table 4 shows the re-
sults. Morfessor performs rather poorly on ATB-
7000. Morfessor Categories-MAP does much bet-
ter, but its performance is dwarfed by our system,
which further cuts F1 error by half. On the full ATB
dataset, Morfessor performs even worse, whereas
Morfessor Categories-MAP benefits from the larger
dataset and achieves an F1 of 74.9. Still, our system
substantially outperforms it, further reducing F1 er-
ror by 11%.8
8 Conclusion
This paper introduces the first log-linear model for
unsupervised morphological segmentation. It lever-
ages overlapping features such as morphemes and
their contexts, and enables easy extension to incor-
porate additional features and linguistic knowledge.
For Arabic and Hebrew, it outperforms the state-
of-the-art systems by a large margin. It can also
be readily applied to supervised or semi-supervised
learning when labeled data is available. Future di-
rections include applying our model to other in-
flectional and agglutinative languages, modeling in-
ternal variations of morphemes, leveraging parallel
data in multiple languages, and combining morpho-
logical segmentation with other NLP tasks, such as
machine translation.
8Note that the ATB and ATB-7000 experiments each mea-
sure accuracy on their entire training set. This difference in
testing conditions explains why some full ATB results are lower
than ATB-7000.
216
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in minimum description length induction.
In Proceedings of the 15th Annual Conference of the
Cognitive Science Society.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In Proceedings of Human Language
Technology (NAACL).
Vera Demberg. 2007. A language-independent unsuper-
vised model for morphological segmentation. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Repub-
lic.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In Advances in Neu-
ral Information Processing Systems 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word segmenta-
tion: Context is important. In Proceedings of the 31st
Boston University Conference on Language Develop-
ment.
Alan Groves and Kirk Lowery, editors. 2006. The West-
minster Hebrew Bible Morphology Database. West-
minster Hebrew Institute, Philadelphia, PA, USA.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Samarth Keshava and Emily Pitler. 2006. A simple, intu-
itive approach to morpheme induction. In Proceedings
of 2nd Pascal Challenges Workshop, Venice, Italy.
Dan Klein and Christopher D. Manning. 2001. Natu-
ral language grammar induction using a constituent-
context model. In Advances in Neural Information
Processing Systems 14.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 649?
658, Honolulu, HI. ACL.
Ronald Rosenfeld. 1997. A whole sentence maximum
entropy language model. In IEEE workshop on Auto-
matic Speech Recognition and Understanding.
Sunita Sarawagi and William Cohen. 2004. Semimarkov
conditional random fields for information extraction.
In Proceedings of the Twenty First International Con-
ference on Machine Learning.
Patrick Schone and Daniel Jurafsky. 2001. Knowlege-
free induction of inflectional morphologies. In Pro-
ceedings of Human Language Technology (NAACL).
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
Proceedings of the Twenty Third National Conference
on Artificial Intelligence.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics.
217
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1049?1056,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning to Predict Case Markers in Japanese                
 
     Hisami Suzuki    Kristina Toutanova1 
Microsoft Research 
One Microsoft Way, Redmond WA 98052 USA 
{hisamis,kristout}@microsoft.com 
 
Abstract 
Japanese case markers, which indicate the gram-
matical relation of the complement NP to the 
predicate, often pose challenges to the generation 
of Japanese text, be it done by a foreign language 
learner, or by a machine translation (MT) system. 
In this paper, we describe the task of predicting 
Japanese case markers and propose machine 
learning methods for solving it in two settings: (i) 
monolingual, when given information only from 
the Japanese sentence; and (ii) bilingual, when 
also given information from a corresponding Eng-
lish source sentence in an MT context. We formu-
late the task after the well-studied task of English 
semantic role labelling, and explore features from 
a syntactic dependency structure of the sentence. 
For the monolingual task, we evaluated our models 
on the Kyoto Corpus and achieved over 84% ac-
curacy in assigning correct case markers for each 
phrase. For the bilingual task, we achieved an ac-
curacy of 92% per phrase using a bilingual dataset 
from a technical domain. We show that in both 
settings, features that exploit dependency informa-
tion, whether derived from gold-standard annota-
tions or automatically assigned, contribute signifi-
cantly to the prediction of case markers.1  
1 Introduction: why predict case? 
Generation of grammatical elements such as inflec-
tional endings and case markers has become an impor-
tant component technology, particularly in the context 
of machine translation (MT). In an English-to-Japanese 
MT system, for example, Japanese case markers, 
which indicate grammatical relations (e.g., subject, 
object, location) of the complement noun phrase to the 
predicate, are among the most difficult to generate 
appropriately. This is because the case markers often 
do not correspond to any word in the source language 
as many grammatical relations are expressed via word 
order in English. It is also difficult because the map-
ping between the case markers and the grammatical 
                                                        
1
 Author names arranged alphabetically 
relations they express is very complex. For the same 
reasons, generation of case markers is challenging to 
foreign language learners. This difficulty in generation, 
however, does not mean the choice of case markers is 
insignificant: when a generated sentence contains mis-
takes in grammatical elements, they often lead to se-
vere unintelligibility, sometimes resulting in a different 
semantic interpretation from the intended one. There-
fore, having a model that makes reasonable predictions 
about which case marker to generate given the content 
words of a sentence, is expected to help MT and gen-
eration in general, particularly when the source (or 
native) and the target languages are morphologically 
divergent.  
But how reliably can we predict case markers in 
Japanese using the information that exists only in the 
sentence? Consider the example in Figure 1. This sen-
tence contains two case markers, kara 'from' and ni, the 
latter not corresponding to any word in English. If we 
were to predict the case markers in this sentence, there 
are multiple valid answers for each decision, many of 
which correspond to different semantic relations. For 
example, for the first case marker slot in Figure 1 filled 
by kara, wa (topic marker), ni 'in' or no case marker at 
all are all reasonable choices, while other markers such 
as wo (object marker), de 'at', made 'until', etc. are not 
considered reasonable. For the second slot filled by ni, 
ga (subject marker) is also a grammatically reasonable 
choice, making Einstein the subject of idolize, thus 
changing the meaning of the sentence. As is obvious in 
this example, the choice among the correct answers is 
determined by the speaker's intent in uttering the sen-
tence, and is therefore impossible to recover from the 
content words or the sentence structure alone. At the 
same time, many impossible or unlikely case marking 
decisions can be eliminated by a case prediction model. 
Combined with an external component (for example an 
MT component) that can resolve semantic and inten-
tional ambiguity, a case prediction model can be quite 
useful in sentence generation. 
This paper discusses the task of case marker as-
signment in two distinct but related settings. After 
defining the task in Section 2 and describing our mod-
els in Section 3, we first discuss the monolingual task 
in Sections 4, whose goal is to predict the case markers 
1049
 using Japanese sentences and their dependency struc-
ture alone. We formulated this task after the 
well-studied task of semantic role labeling in English 
(e.g., Gildea and Jurafsky, 2002; Carreras and M?rques, 
2005), whose goal is to assign one of 20 semantic role 
labels to each phrase in a sentence with respect to a 
given predicate, based on the annotations provided by 
PropBank (Palmer et al, 2005). Though the task of 
case marker prediction is more ambiguous and subject 
to uncertainty than the semantic role labeling task, we 
obtained some encouraging results which we present in 
Section 4. Next, in Section 5, we describe the bilingual 
task, in which information about case assignment can 
be extracted from a corresponding source language 
sentence. Though the process of MT introduces uncer-
tainties in generating the features we use, we show that 
the benefit of using dependency structure in our mod-
els is far greater than not using it even when the as-
signed structure is not perfect.  
2 The task of case prediction 
In this section, we define the task of case prediction. 
We start with the description of the case markers we 
used in this study. 
2.1 Nominal particles in Japanese 
Traditionally, Japanese nominal postpositions are clas-
sified into the following three categories (e.g., Tera-
mura, 1991; Masuoka and Takubo, 1992):   
Case particles (or case markers). They indicate 
grammatical relations of the complement NP to the 
predicate. As they are jointly determined by the NP 
and the predicate, case markers often do not allow a 
simple mapping to a word in another language, which 
makes their generation more difficult. The relationship 
between the case marker and the grammatical relation 
it indicates is not straightforward either: a case marker 
can (and often does) indicate multiple grammatical 
relations as in Ainshutain-ni akogareru "idolize Ein-
stein" where ni marks the Object relation, and in To-
kyo-ni sumu "live in Tokyo" where ni indicates Loca-
tion. Conversely, the same grammatical relation may 
be indicated by different case markers: both ni and de 
in Tokyo-ni sumu "live in Tokyo" and Tokyo-de au 
"meet in Tokyo" indicate the Location relation. We 
included 10 case markers as the primary target of pre-
diction, as shown in the first 10 lines of Table 1. 
Conjunctive particles. These particles are used to 
conjoin words and phrases, corresponding to English 
"and" and "or". As their occurrence is not predictable 
from the sentence structure alone, we did not include 
them in the current prediction task.  
Focus particles. These particles add focus to a phrase 
against a given background or contextual knowledge, 
for example shika and mo in pasuta-shika tabenakatta 
"ate only pasta" and pasuta-mo tabeta "also ate pasta", 
corresponding to only and also respectively. Note that 
they often replace case markers: in the above examples, 
the object marker wo is no longer present when shika 
or mo is used. As they add information to the predi-
cate-argument structure and are in principle not pre-
dictable given the sentence structure alone, we did not 
consider them as the target of our task. One exception 
is the topic marker wa, which we included as a target 
of prediction for the following reasons:  
 Some linguists recognize wa as a topic marker, 
separately from other focus particles (e.g. Masuoka 
and Takubo, 1992). The main function of wa is to 
introduce a topic in the sentence, which is to a some 
extent predictable from the structure of the sentence.  
 wa is extremely frequent in Japanese text. For ex-
ample, it accounts for 13.2% of all postpositions in 
Kyoto University Text Corpus (henceforth Kyoto 
Corpus, Kurohashi and Nagao, 1997), making it the 
third most frequent postposition after no (20.57%) 
and wo (13.5%). Generating wa appropriately thus 
greatly enhances the readability of the text.   
 Unlike other focus particles such as shika and mo, 
wa does not translate into any word in English, 
which makes it difficult to generate by using the in-
formation from the source language.  
Therefore, in addition to the 10 true case markers, we 
also included wa as a case marker in our study.2 Fur-
thermore, we also included the combination of case 
particles plus wa as a secondary target of prediction. 
The case markers that can appear followed by wa are 
indicated by a check mark in the column "+wa" in 
Table 1. Thus there are seven secondary targets: niwa, 
karawa, towa, dewa, ewa, madewa, yoriwa. Therefore, 
we have in total 18 case particles to assign to phrases.  
2.2 Task definition 
The case prediction task we are solving is as follows. 
We are given a sentence as a list of bunsetsu together 
                                                        
2
 This set comprises the majority (92.5%) of the nominal parti-
cles, while conjunctive and focus particles account for only 
7.5% of the nominal particles in Kyoto Corpus. 
 
Figure 1. Example of case markers in Japanese (taken 
from the Kyoto Corpus). Square brackets indicate bun-
setsu (phrase) boundaries, to be discussed below. Ar-
rows between phrases indicate dependency relations.  
1050
 with a dependency structure. For our monolingual 
experiments, we used the dependency structure annota-
tion in the Kyoto Corpus; for our bilingual experiments, 
we used automatically derived dependency structure 
(Quirk et al, 2005). Each bunsetsu (or simply phrase 
in this paper) is defined as consisting of one content 
word (or n-content words in the case of compounds 
with n-components) plus any number of function 
words (including particles, auxiliaries and affixes). 
Case markers are classified as function words, and 
there is at most one case marker per phrase.3 In testing, 
the case marker for each phrase is hidden; the task is to 
assign to each phrase one of the 18 case markers de-
fined above or NONE; NONE indicates that the phrase 
does not have a case marker.  
2.3 Related work 
Though the task of case marker prediction as formu-
lated in this paper is novel, similar tasks have been 
defined in the past. The semantic role labeling task 
mentioned in Section 1 is one example; the task of 
function tag assignment in English (e.g., Blaheta and 
Charniak, 2000) is another. These tasks are similar to 
the case prediction task in that they try to assign se-
mantic or function tags to a parsed structure. However, 
there is one major difference between these tasks and 
the current task: semantic role labels and function tags 
can for the most part be uniquely determined given the 
sentence and its parse structure; decisions about case 
markers, on the other hand, are highly ambiguous 
given the sentence structure alone, as mentioned in 
Section 1. This makes our task more ambiguous than 
the related tasks. As a concrete comparison, the two 
most frequent semantic role labels (ARG0 and ARG1) 
account for 60% of the labeled arguments in PropBank 
                                                        
3
 One exception is that no can appear after certain case markers; 
in such cases, we considered no to be the case for the phrase.  
4
 no is typically not considered as a case marker but rather as a 
conjunctive particle indicating adnominal relation; however, as
no can also be used to indicate the subject in a relative clause, 
we included it in our study.  
(Carreras and M?rquez, 2005), whereas our 2 most 
frequent case markers (no and wo) account for only 
43% of the case-marked phrases. We should also note 
that semantic role labels and function tags have been 
artificially defined in accordance with theoretical deci-
sions about what annotations should be useful for 
natural language understanding tasks; in contrast, the 
case markers are part of the surface sentence string and 
do not reflect any theoretical decisions. 
The task of case prediction in Japanese has previ-
ously focused on recovering implicit case relations, 
which result when noun phrases are relativized or 
topicalized (e.g., Baldwin, 2000; Kawahara et al, 
2004; Murata and Isahara, 2005). Their goal is differ-
ent form ours, as we aim to generate surface forms of 
case markers rather than recover deeper case relations 
for which surface case marker are often used as a 
proxy.  
In the context of sentence generation, Gamon et al 
(2002) used a decision tree to classify nouns into one 
of the four cases in German, as part of their sentence 
realization from a semantic representation, achieving 
high accuracy (87% to 93.5%). Again, this is a sub-
stantially easier task than ours, because there are only 
four classes and one of them (nominative) accounts for 
70% of all cases. Uchimoto et al (2002), which is the 
work most related to ours, propose a model of generat-
ing function words (not limited to case markers) from 
"keywords" or headwords of phrases in Japanese. The 
components of their model are based on n-gram lan-
guage models using the surface word strings and bun-
setsu dependency information, and the results they 
report are not comparable to ours, as they limit their 
test sentences to the ones consisting only of two or 
three content words. We will see in the next section 
that our models are also quite different from theirs as 
we employ a much richer set of features.  
3 Classifiers for case prediction  
We implemented two types of models for the task of 
case prediction: local models, which choose the case 
marker of each phrase independently of the case mark-
ers of other phrases, and joint models, which incorpo-
rate dependencies among the case markers of depend-
ents of the same head phrase. We describe the two 
types of models in turn.  
3.1 Local classifiers 
Following the standard practice in semantic role label-
ing, we divided the case prediction task into the tasks 
of identification and classification (Gildea and Juraf-
sky, 2002; Pradhan et al, 2004). In the identification 
task, we assign to each phrase one of two labels: HAS-
CASE, meaning that the phrase has a case marker, or 
NONE, meaning that it does not have a case. In the 
case markers grammatical functions (e.g.) +wa 
 
 ga subject; object  

 wo object; path  
4
 
no genitive; subject  

 ni dative object, location  

 kara source  

 to quotative, reciprocal, as  

 de location, instrument, cause  
	
 e goal, direction  



 made goal (up to, until)  

 yori source, object of comparison  

 wa topic  
Table 1. Case markers included in this study 
1051
 classification task, we assign one of the 18 case mark-
ers to each phrase that has been labeled with HASCASE 
by the identification model. 
We train a binary classifier for identification and a 
multi-class classifier (with 18 classes) for classification. 
We obtain a classifier for the complete task by chain-
ing the two classifiers. Let PID(c|b) and  PCLS(c|b) 
denote the probability of class c for bunsetsu b accord-
ing to the identification and classification models, re-
spectively. We define the probability distribution over 
classes of the complete model for case assignment as 
follows: 
 PCaseAssign(NONE |b) = PID(NONE |b)  
 PCaseAssign(l|b) = PID(HASCASE |b)* PCLS(l|b) 
Here, l denotes one of the 18 case markers. 
We employ this decomposition mainly for effi-
ciency in training: that is, the decomposition allows us 
to train the classification models on a subset of training 
examples consisting only of those phrases that have a 
case marker, following Toutanova et al (2005). 
Among various machine learning methods that can be 
used to train the classifiers, we chose log-linear models 
for both identification and classification tasks, as they 
produce probability distributions which allows chain-
ing of  the two component models and easy integra-
tion into an MT system. 
3.2 Joint classifiers 
Toutanova et al (2005) report a substantial improve-
ment in performance on the semantic role labeling task 
by building a joint classifier, which takes the labels of 
other phrases into account when classifying a given 
phrase. This is motivated by the fact that the argument 
structure is a joint structure, with strong dependencies 
among arguments. Since the case markers also reflect 
the argument structure to some extent, we implemented 
a joint classifier for the case prediction task as well.  
We applied the joint classifiers in the framework of 
N-best reranking (Collins, 2000), following Toutanova 
et al (2005). That is, we produced N-best (N=5 in our 
experiments) case assignment sequence candidates for 
a set of sister phrases using the local models, and 
trained a joint classifier that learns to choose the best 
candidate from the set of sisters. The oracle accuracy 
of the 5-best candidate list was 95.9% per phrase.   
4 Monolingual case prediction task 
In this section we describe our models trained and 
evaluated using the gold-standard dependency annota-
tions provided by the Kyoto Corpus. These annotations 
allow us to define a rich set of features exploring the 
syntactic structure. 
4.1 Features 
The basic local model features we used for the identi-
fication and classification models are listed in Table 2. 
They consist of features for a phrase, for its parent 
phrase and for their relations. Only one feature 
(GrandparentNounSubPos) currently refers to the 
grandparent of the phrase; all other features are be-
tween the phrase, its parent and its sibling nodes, and 
are a superset of the dependency-based features used 
by Hacioglu (2004) for the semantic labeling task. In 
addition to these basic features, we added 20 combined 
features, some of which are shown at the bottom of 
Table 2. 
For the joint model, we implemented only two 
types of features: sequence of non-NONE case markers 
for a set of sister phrases, and repetition of non-NONE 
case markers. These features are intended to capture 
regularities in the sequence of case markers of phrases 
that modify the same head phrase.  
All of these features are represented as binary fea-
tures: that is, when the value of a feature is not binary, 
we have treated the combination of the feature name 
plus the value as a unique feature. With a count cut-off 
of 2 (i.e., features must occur at least twice to be in the 
model), we have 724,264 features in the identification 
Basic features for phrases (self, parent) 
HeadPOS, PrevHeadPOS, NextHeadPOS  
PrevPOS,Prev2POS,NextPOS,Next2POS 
HeadNounSubPos: time, formal nouns, adverbial 
HeadLemma 
HeadWord, PrevHeadWord, NextHeadWord 
PrevWord, Prev2Word, NextWord, Next2Word 
LastWordLemma (excluding case markers) 
LastWordInfl (excluding case markers) 
IsFiniteClause 
IsDateExpression 
IsNumberExpression 
HasPredicateNominal 
HasNominalizer 
HasPunctuation: comma, period 
HasFiniteClausalModifier 
RelativePosition: sole, first, mid, last 
NSiblings (number of siblings) 
Position (absolute position among siblings) 
Voice: pass, caus, passcaus 
Negation 
Basic features for phrase relations (parent-child pair) 
DependencyType: D,P,A,I 
Distance: linear distance in bunsetsu, 1, 2-5, >6 
Subcat: POS tag of parent + POS tag of all children + 
indication for current 
Combined features (selected) 
HeadPOS + HeadLemma 
ParentLemma + HeadLemma 
Position + NSiblings 
IsFiniteClause + GrandparentNounSubPos 
Table 2: Basic and combined features for local classifiers 
1052
 model, and 3,963,096 features in the classification 
model. The number of joint features in the joint model 
is 3,808. All models are trained using a Gaussian prior.  
4.2 Data and baselines 
We divided the Kyoto Corpus (version 3.0) into the 
following three sections:   
 Training: contains news articles of January 1, 3-11 
and editorial articles of January-August; 24,263 
sentences, 234,474 phrases.  
 Devtest: contains news articles of January 12-13 and 
editorial article of September. 4,833 sentences, 
47,580 phrases.  
 Test: contains news articles of January 14-17 and 
editorial articles of October-December. 9,287 sen-
tences, 89,982 phrases.  
The devtest set was used only for tuning model pa-
rameters and for performing error analysis.  
As no previous work exists on the task of predicting 
case markers on the Kyoto Corpus, it is important to 
establish a good baseline. The simplest baseline of 
always selecting the most frequent label (NONE) gives 
us an accuracy of 47.5% on the test set. Out of the 
non-NONE case markers, the most frequent is no, 
which occurs in 26.6% of all case-marked phrases.   
A more reasonable baseline is to use a language 
model to predict case. We trained and tested two lan-
guage models: the first model, called KCLM, is trained 
on the same data as our log-linear models (24,263 sen-
tences); the second model, called BigCLM, is trained 
on much more data from the same domain (826,373 
sentences), taking advantage of the fact that language 
models do not require dependency annotation for 
training. The language models were trained using the 
CMU language modeling toolkit with default parame-
ter settings (Clarkson and Rosenfeld, 1997). 
We tested the language model baselines using the 
same task set-up as for our classifier: for each phrase, 
each of the 18 possible case markers and NONE is 
evaluated. The position for insertion of a case marker 
in each phrase is given according to our task set-up, i.e., 
at the end of a phrase preceding any punctuation. We 
choose the case assignment of the sequence of phrases 
in the sentence that maximizes the language model 
probability of the resulting sentence. We computed the 
most likely case assignment sequence using a dynamic 
programming algorithm.  
4.3 Results and discussion 
The results of running our models on case marker pre-
diction are shown in Table 3. The first three rows cor-
respond to the components of the local model: the 
identification task (Id, for all phrases), the classifica-
tion task (Cls, only for case-marked phrases) and the 
complete task (Both, for all phrases). The accuracy on 
the complete task using the local model is 83.9%; the 
joint model improves it to 84.3%.   
The improvement due to the joint model is small in 
absolute percentage points (0.4%), but is statistically 
significant according to a test for the difference of 
proportions (p< 0.05). The use of a joint classifier did 
not lead to as large an improvement over the local 
classifier as for the semantic role labeling task.  There 
are several reasons for that we can think of. First, we 
have only used a limited set of features for the joint 
model, i.e., case sequence and repetition features. A 
more extensive use of global features might lead to a 
larger improvement. Secondly, unlike the task of se-
mantic role labeling, where there are about 20 phrases 
that need to be labeled with respect to a predicate, 
about 50% of all phrases in the Kyoto Corpus do not 
have sister nodes. This means that these phrases cannot 
take advantage of the joint classifier using the current 
model formulation. Finally, case markers are much 
shallower than semantic role labels in the level of lin-
guistic analysis, and so are inherently subject to more 
variations, including missing arguments (so called zero 
pronouns) and repeated case markers corresponding to 
different semantic roles.  
From Table 3, it is clear that our models outperform 
the baseline model significantly. The language model 
trained on the same data has much lower performance 
(67.0% vs. 84.3%), which shows that our system is 
exploiting the training data much more efficiently by 
looking at the dependency and other syntactic features. 
An inspection of the 500 most highly weighted features 
also indicates that phrase dependency-based features 
are very useful for both identification and classification. 
Given much more data, though, the language model 
improves significantly to 78%, but our classifier still 
achieves a 29% error reduction over it. The differences 
between the language models and the log-linear models 
are statistically significant at level p < 0.01 according 
to a test for the difference of proportions. 
 Figure 2 plots the recall and precision for the fre-
quently occurring (>500) cases. We achieve good re-
sults on NONE and no, which are the least ambiguous 
decisions. Cases such as ni, wa, ga, and de are highly 
confusable with other markers as they indicate multiple 
grammatical relations, and the performance of our 
Models Task Training  Test  
log-linear Id 99.8 96.9 
log-linear Cls 96.6 74.3 
log-linear (local) Both 98.0 83.9 
log-linear( joint) Both 97.8 84.3 
baseline (frequency) Both 48.2 47.5 
baseline (KCLM) Both 93.9 67.0 
baseline (BigCLM) Both ? 78.0 
Table 3: Accuracy of case prediction models (%) 
1053
 models on them is therefore limited. As expected, per-
formance (especially recall) on secondary targets 
(dewa, niwa) suffers greatly due to the ambiguity with 
their primary targets.  
5 Bilingual case prediction task: simulating 
case prediction in MT 
Incorporating a case prediction model into MT requires 
taking additional factors into consideration, compared 
to the monolingual task described above. On the one 
hand, we need to extend our model to handle the addi-
tional knowledge source, i.e., the source sentence. This 
can potentially provide very useful features to our 
model, which are not available in the monolingual task. 
On the other hand, since gold-standard dependency 
annotation is not available in the MT context, we must 
deal with the imperfections in structural annotations.  
In this section, we describe our case prediction 
models in the context of English-to-Japanese MT. In 
this setting, dependency information for the target 
language (Japanese) is available only through projec-
tion of a dependency structure from the source lan-
guage (English) in a tree-to-string-based statistical MT 
system (Quirk et al, 2005). We conducted experiments 
using the English source sentences and the reference 
translations in Japanese: that is, our task is to predict 
the case markers of the Japanese reference translations 
correctly using all other words in the reference sen-
tence, information from the source sentence through 
word alignment, and the Japanese dependency struc-
ture projected via an MT component. Ultimately, our 
goal is to improve the case marker assignment of a 
candidate translation using a case prediction model; the 
experiments described in this section on reference 
translations serve as an important preliminary step 
toward achieving that final goal. We will show in this 
section that even the automatically derived syntactic 
information is very useful in assigning case markers in 
the target language, and that utilizing the information 
from the source language also greatly contributes to 
reducing case marking errors.  
5.1 Data and task set-up 
The dataset we used is a collection of parallel Eng-
lish-Japanese sentences from a technical (computer) 
domain. We used 15,000 sentence pairs for training, 
5,000 for development, and 4,241 for testing.  
The parallel sentences were word-aligned using 
GIZA++ (Och and Ney, 2000), and submitted to a 
tree-to-string-based MT system (Quirk et al, 2005) 
which utilizes the dependency structure of the source 
language and projects dependency structure to the 
target language. Figure 3 shows an example of an 
aligned sentence pair: on the source (English) side, 
part-of-speech (POS) tags and word dependency 
structure are assigned (solid arcs). The alignments 
between English and Japanese words are indicated by 
the dotted lines. In order to create phrase-level de-
pendency structures like the ones utilized in the Kyoto 
Corpus monolingual task, we derived some additional 
information for the Japanese sentence in the following 
manner.  
 
Figure 3. Aligned English-Japanese sentence pair 
First, we tagged the sentence using an automatic 
tagger with a set of 19 POS tags. We used these POS 
tags to parse the words into phrases (bunsetsu): each 
bunsetsu consists of one content word plus any number 
of function words, where content and function words 
are defined via POS. We then constructed a 
phrase-level dependency structure using a breadth-first 
traversal of the word dependency structure projected 
from English. These phrase dependencies are indicated 
by bold arcs in Figure 3. The case markers to be pre-
dicted (wa and de in this case) are underlined.   
 The task of case marker prediction is the same as 
described in Section 2: to assign one of the 18 case 
markers described in Section 2 or NONE to each phrase. 
5.2 Baseline models 
We implemented the baseline models discussed in 
Section 4.2 for this domain as well. The most frequent 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
niwa (523)
dewa (548)
kara (868)
de (2582)
to (3664)
ga (5797)
wa (5937)
ni (6457)
wo (7782)
no (12570)
NONE (42756)
precision
recall
 
Figure 2: Precision and recall per case marker (frequency 
in parentheses) 
1054
 case assignment is again NONE, which accounts for 
62.0% of the test set. The frequency of NONE is higher 
in this task than in the Kyoto Corpus, because our 
bunsetsu-parsing algorithm prefers to err on the side of 
making too many rather than too few phrases. This is 
because our final goal is to generate all case markers, 
and if we mistakenly joined two bunsetsu into one, our 
case assigner would be able to propose only one case 
marker for the resulting bunsetsu, which would be 
necessarily wrong if both bunsetsu had case markers. 
The most frequent case marker is again no, which oc-
curs in 29.4% of all case-marked phrases. As in the 
monolingual task, we trained two trigram language 
models: one was trained on the training set of our case 
prediction models (15,000 sentences); another was 
trained on a much larger set of 450,000 sentences from 
the same domain. The results of these baselines are 
discussed in Section 5.4. 
5.3 Log-linear models 
The models we built for this task are log-linear models 
as described in Section 3. In order to isolate the impact 
of information from the source language available for 
the case prediction task, we built two kinds of models: 
monolingual models, which do not use any information 
from the source English sentences, and bilingual mod-
els, which use information from the source. Both mod-
els are local models in the sense discussed in Section 3.  
Table 4 shows the features used in the monolingual 
and bilingual models, along with the examples (the 
value of the feature for the phrase [saabisu wa] in Fig-
ure 3); in addition to these, we also provided some 
feature combinations for both monolingual and bilin-
gual models. Many of the monolingual features (i.e., 
first 11 lines in Table 4) are also present in Table 2. 
Note that lexically based features are of greater impor-
tance for this task, as the dependency information 
available in this context is of much poorer quality than 
that provided by the Kyoto Corpus. In addition to the 
features in Table 2, we added a Direction feature (with 
values left and right), and an Alternative Parent feature. 
Alternative parents are all words which are the parents 
of any word in the phrase, according to the word-based 
dependency tree, with the constraint that case markers 
cannot be alternative parents. This feature captures the 
information that is potentially lost in the process of 
building a phrase dependency structure from word 
dependency information in the target language.  
The bottom half of Table 4 shows bilingual features. 
The features of the source sentence are obtained 
through word alignments. We create features from the 
source words aligned to the head of the phrase, to the 
head of the parent phrase, or to any alterative parents. 
If any word in the phrase is aligned to a preposition in 
the source language, our model can use the information 
as well. In addition to word- and POS-features for 
aligned source words, we also refer to the correspond-
ing dependency between the phrase and its parent 
phrase in the English source. If the head of the Japa-
nese phrase is aligned to a single source word s1, and 
the head of its parent phrase is aligned to a single 
source word s2, we extract the relationship between s1 
and s2, and define subcategorization, direction, distance, 
and number of siblings features, in order to capture the 
grammatical relation in the source, which is more reli-
able than in the projected target dependency structure. 
 
5.4 Results and discussion 
Table 5 summarizes the results on the complete case 
assignment task in the MT context. Compared to the 
language model trained on the same data (15kLM), our 
Monolingual features  
Feature Example 
HeadWord /HeadPOS saabisu/NN 
PrevWord/PrevPOS kono/AND 
Prev2Word/Prev2WordPOS none/none 
NextWord/NextPOS seefu/NN 
Next2Word/Net2POS moodo/NN 
PrevHeadWord/PrevHeadPOS kono/AND 
NextHeadWord/NextHeadPOS seefu/NN 
ParentHeadWord/ParentHeadPOS kaishi/VN 
Subcat: POS tags of all sisters and parent NN-c,NN,VN-h 
NSiblings (including self) 2 
Distance 1 
Direction left 
Alternative Parent Word /POS saabisu/NN 
Bilingual features 
Feature Example 
Word/POS of source words aligned to the 
head of the phrase 
service/NN 
Word/POS of all source words aligned to 
any word in the phrase 
service/NN 
Word/POS of all source words aligned to 
the head word of the parent phrase 
started/VERB 
Word/POS of all source words aligned to 
alternative parent words of the phrase 
service/NN, 
started/VERB 
All source preposition words in 
Word/POS of parent of source word aligned 
to any word in the phrase 
started/VERB 
Aligned Subcat          NN-c,VERB,VERB,VERB-h,PREP 
Aligned NSiblings 4 
Aligned Distance 2 
Aligned Direction left 
Table 4: Monolingual and bilingual features 
Model Test data 
baseline (frequency) 62.0 
baseline (15kLM) 79.0 
baseline (450kLM) 83.6 
log-linear monolingual 85.3 
log-linear bilingual 92.3 
Table 5: Accuracy of bilingual case prediction (%) 
1055
 monolingual model performs significantly better, 
achieving a 30% error reduction (85.3% vs. 79.0%). 
Our monolingual model outperforms even the language 
model trained on 30 times more data (85.3% vs. 
83.6%), with an error reduction of 10%. The difference 
is statistically significant at level p < 0.01 according to 
a test for the difference of proportions. This means that 
even though the projected dependency information is 
not perfect, it is still useful for the case prediction task.  
When we add the bilingual features, the error rate of 
our model is cut almost in half: the bilingual model 
achieves an error reduction of 48% over the monolin-
gual model (92.3% vs. 85.3%, statistically significant 
at level p < 0.01). This result is very encouraging: it 
indicates that information from the source sentence can 
be exploited very effectively to improve the accuracy 
of case assignment. The usefulness of the source lan-
guage information is also obvious when we inspect 
which case markers had the largest gains in accuracy 
due to this information: the top three cases were kara 
(0.28 to 0.65, a 57% gain), dewa (0.44 to 0.65, a 32% 
gain) and to (0.64 to 0.85, a 24% gain), all of which 
have translations as English prepositions. Markers such 
as ga (subject marker, 0.68 to 0.74, a 8% gain) and wo 
(object marker, 0.83 to 0.86, a 3.5% gain), on the other 
hand, showed only a limited gain.  
6 Conclusion and future directions 
This paper described the task of predicting case mark-
ers in Japanese, and reported results in a monolingual 
and a bilingual settings. The results show that the mod-
els we proposed, which explore syntax-based features 
and features from the source language in the bilingual 
task, can effectively predict case markers.  
There are a number of extensions and next steps we 
can think of at this point, the most immediate and im-
portant one of which is to incorporate the proposed 
model in an end-to-end MT system to make improve-
ments in the output of MT. We would also like to per-
form a more extensive analysis of features and feature 
ablation experiments. Finally, we would also like to 
extend the proposed model to include languages with 
inflectional morphology and the prediction of gram-
matical elements in general.  
Acknowledgements 
We would like to thank the anonymous reviewers for 
their comments, and Bob Moore, Arul Menezes, Chris 
Quirk, and Lucy Vanderwende for helpful discussions. 
References 
Baldwin, T. 2004. Making Sense of Japanese Relative 
Clause Constructions, In Proceedings of the 2nd 
Workshop on Text Meaning and Interpretation.  
Blaheta, D. and E. Charniak. 2000. Assigning function 
tags to parsed text. In Proceedings of NAACL, 
pp.234-240. 
Carreras, X. and L. M?rquez. 2005. Introduction to the 
CoNLL-2005 Shared Task: Semantic Role Labeling. In 
Proceedings of CoNLL-2005.  
Clarkson, P.R. and R. Rosenfeld. 1997. Statistical Lan-
guage Modeling Using the CMU-Cambridge Toolkit. 
In Proceedings of ESCA Eurospeech, pp. 2007-2010. 
Collins, M. 2000. Discriminative reranking for natural 
language parsing. In Proceedings of ICML.  
Gamon, M., E. Ringger, S. Corston-Oliver and R. Moore. 
2002. Machine-learned Context for Linguistic Opera-
tions in German Sentence Realization. In Proceeding 
of ACL.  
Gildea, D. and D. Jurafsky. 2002. Automatic Labeling of 
Semantic Roles. In Computational Linguistics 28(3): 
245-288.  
Hacioglu, K. 2004.  Semantic Role Labeling using De-
pendency Trees. In Proceedings of COLING 2004. 
Kawahara, D., N. Kaji and S. Kurohashi. 2000. Japanese 
Case Structure Analysis by Unsupervised Construction 
of a Case Frame Dictionary. In Proceedings of COL-
ING, pp. 432-438.  
Kurohashi, S. and M.Nagao. 1997. Kyoto University Text 
Corpus Project. In Proceedings of ANLP, pp.115-118.   
Masuoka, T. and Y. Takubo. 1992. Kiso Nihongo Bunpou 
(Fundamental Japanese grammar), revised version. 
Kuroshio Shuppan, Tokyo.   
Murata, M., and H. Isahara. 2005. Japanese Case Analysis 
Based on Machine Learning Method that Uses Bor-
rowed Supervised Data. In Proceedings of IEEE 
NLP-KE-2005, pp.774-779.  
Och, F.J. and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL: pp.440-447.  
Palmer, M., D. Gildea and P. Kingsbury. 2005. The 
Proposition Bank: An Annotated Corpus of Semantic 
Roles. In Computational Linguistics 31(1).  
Pradhan, S., W. Ward, K. Hacioglu, L. Martin, D. Juraf-
sky. 2004. Shallow Semantic Parsing Using Support 
Vector Machines. In Proceedings of HLT/NAACL. 
Quirk, C., A. Menezes and C. Cherry. 2005. Dependency 
Tree Translation: Syntactically Informed Phrasal SMT. In 
Proceedings of ACL. 
Teramura, H. 1991. Nihongo-no shintakusu-to imi (Japa-
nese syntax and meaning). Volume III. Kuroshio 
Shuppan, Tokyo.   
Toutanova, K., A. Haghighi and C. D. Manning. 2005. 
Joint Learning Improves Semantic Role Labeling. In 
Proceeding of ACL, pp.589-596.  
Uchimoto, K., S. Sekine and H. Isahara. 2002. Text Gen-
eration from Keywords. In Proceedings of COLING 
2002, pp.1037-1043.  
1056
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 9?16,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Discriminative Syntactic Word Order Model for Machine Translation
Pi-Chuan Chang?
Computer Science Department
Stanford University
Stanford, CA 94305
pichuan@stanford.edu
Kristina Toutanova
Microsoft Research
Redmond, WA
kristout@microsoft.com
Abstract
We present a global discriminative statistical
word order model for machine translation.
Our model combines syntactic movement
and surface movement information, and is
discriminatively trained to choose among
possible word orders. We show that com-
bining discriminative training with features
to detect these two different kinds of move-
ment phenomena leads to substantial im-
provements in word ordering performance
over strong baselines. Integrating this word
order model in a baseline MT system results
in a 2.4 points improvement in BLEU for
English to Japanese translation.
1 Introduction
The machine translation task can be viewed as con-
sisting of two subtasks: predicting the collection of
words in a translation, and deciding the order of the
predicted words. For some language pairs, such as
English and Japanese, the ordering problem is es-
pecially hard, because the target word order differs
significantly from the source word order.
Previous work has shown that it is useful to model
target language order in terms of movement of syn-
tactic constituents in constituency trees (Yamada
and Knight, 2001; Galley et al, 2006) or depen-
dency trees (Quirk et al, 2005), which are obtained
using a parser trained to determine linguistic con-
stituency. Alternatively, order is modelled in terms
of movement of automatically induced hierarchical
structure of sentences (Chiang, 2005; Wu, 1997).
? This research was conducted during the author?s intern-
ship at Microsoft Research.
The advantages of modeling how a target lan-
guage syntax tree moves with respect to a source lan-
guage syntax tree are that (i) we can capture the fact
that constituents move as a whole and generally re-
spect the phrasal cohesion constraints (Fox, 2002),
and (ii) we can model broad syntactic reordering
phenomena, such as subject-verb-object construc-
tions translating into subject-object-verb ones, as is
generally the case for English and Japanese.
On the other hand, there is also significant amount
of information in the surface strings of the source
and target and their alignment. Many state-of-the-art
SMT systems do not use trees and base the ordering
decisions on surface phrases (Och and Ney, 2004;
Al-Onaizan and Papineni, 2006; Kuhn et al, 2006).
In this paper we develop an order model for machine
translation which makes use of both syntactic and
surface information.
The framework for our statistical model is as fol-
lows. We assume the existence of a dependency tree
for the source sentence, an unordered dependency
tree for the target sentence, and a word alignment
between the target and source sentences. Figure 1
(a) shows an example of aligned source and target
dependency trees. Our task is to order the target de-
pendency tree.
We train a statistical model to select the best or-
der of the unordered target dependency tree. An im-
portant advantage of our model is that it is global,
and does not decompose the task of ordering a tar-
get sentence into a series of local decisions, as in the
recently proposed order models for Machine Transi-
tion (Al-Onaizan and Papineni, 2006; Xiong et al,
2006; Kuhn et al, 2006). Thus we are able to define
features over complete target sentence orders, and
avoid the independence assumptions made by these
9
all constraints are satisfied
[??] [??] [?] [???][???] [????]
?restriction??condition? TOPIC ?all? ?satisfy? PASSIVE-PRES
c d e f g h
(a)
fe cd g h
fe cd gh
fe cdg h
(b)
Figure 1: (a) A sentence pair with source depen-
dency tree, projected target dependency tree, and
word alignments. (b) Example orders violating the
target tree projectivity constraints.
models. Our model is discriminatively trained to se-
lect the best order (according to the BLEU measure)
(Papineni et al, 2001) of an unordered target depen-
dency tree from the space of possible orders.
Since the space of all possible orders of an un-
ordered dependency tree is factorially large, we train
our model on N-best lists of possible orders. These
N-best lists are generated using approximate search
and simpler models, as in the re-ranking approach of
(Collins, 2000).
We first evaluate our model on the task of ordering
target sentences, given correct (reference) unordered
target dependency trees. Our results show that com-
bining features derived from the source and tar-
get dependency trees, distortion surface order-based
features (like the distortion used in Pharaoh (Koehn,
2004)) and language model-like features results in a
model which significantly outperforms models using
only some of the information sources.
We also evaluate the contribution of our model
to the performance of an MT system. We inte-
grate our order model in the MT system, by simply
re-ordering the target translation sentences output
by the system. The model resulted in an improve-
ment from 33.6 to 35.4 BLEU points in English-to-
Japanese translation on a computer domain.
2 Task Setup
The ordering problem in MT can be formulated as
the task of ordering a target bag of words, given a
source sentence and word alignments between tar-
get and source words. In this work we also assume
a source dependency tree and an unordered target
dependency tree are given. Figure 1(a) shows an ex-
ample. We build a model that predicts an order of
the target dependency tree, which induces an order
on the target sentence words. The dependency tree
constrains the possible orders of the target sentence
only to the ones that are projective with respect to
the tree. An order of the sentence is projective with
respect to the tree if each word and its descendants
form a contiguous subsequence in the ordered sen-
tence. Figure 1(b) shows several orders of the sen-
tence which violate this constraint.1
Previous studies have shown that if both the
source and target dependency trees represent lin-
guistic constituency, the alignment between subtrees
in the two languages is very complex (Wellington et
al., 2006). Thus such parallel trees would be difficult
for MT systems to construct in translation. In this
work only the source dependency trees are linguisti-
cally motivated and constructed by a parser trained
to determine linguistic structure. The target depen-
dency trees are obtained through projection of the
source dependency trees, using the word alignment
(we use GIZA++ (Och and Ney, 2004)), ensuring
better parallelism of the source and target structures.
2.1 Obtaining Target Dependency Trees
Through Projection
Our algorithm for obtaining target dependency trees
by projection of the source trees via the word align-
ment is the one used in the MT system of (Quirk
et al, 2005). We describe the algorithm schemat-
ically using the example in Figure 1. Projection
of the dependency tree through alignments is not at
all straightforward. One of the reasons of difficulty
is that the alignment does not represent an isomor-
phism between the sentences, i.e. it is very often
not a one-to-one and onto mapping.2 If the align-
ment were one-to-one we could define the parent of
a word wt in the target to be the target word aligned
to the parent of the source word si aligned to wt. An
additional difficulty is that such a definition could re-
sult in a non-projective target dependency tree. The
projection algorithm of (Quirk et al, 2005) defines
heuristics for each of these problems. In case of
one-to-many alignments, for example, the case of
?constraints? aligning to the Japanese words for ?re-
striction? and ?condition?, the algorithm creates a
1For example, in the first order shown, the descendants of
word 6 are not contiguous and thus this order violates the con-
straint.
2In an onto mapping, every word on the target side is asso-
ciated with some word on the source side.
10
subtree in the target rooted at the rightmost of these
words and attaches the other word(s) to it. In case of
non-projectivity, the dependency tree is modified by
re-attaching nodes higher up in the tree. Such a step
is necessary for our example sentence, because the
translations of the words ?all? and ?constraints? are
not contiguous in the target even though they form a
constituent in the source.
An important characteristic of the projection algo-
rithm is that all of its heuristics use the correct target
word order.3 Thus the target dependency trees en-
code more information than is present in the source
dependency trees and alignment.
2.2 Task Setup for Reference Sentences vs MT
Output
Our model uses input of the same form when
trained/tested on reference sentences and when used
in machine translation: a source sentence with a de-
pendency tree, an unordered target sentence with
and unordered target dependency tree, and word
alignments.
We train our model on reference sentences. In this
setting, the given target dependency tree contains the
correct bag of target words according to a reference
translation, and is projective with respect to the cor-
rect word order of the reference by construction. We
also evaluate our model in this setting; such an eval-
uation is useful because we can isolate the contribu-
tion of an order model, and develop it independently
of an MT system.
When translating new sentences it is not possible
to derive target dependency trees by the projection
algorithm described above. In this setting, we use
target dependency trees constructed by our baseline
MT system (described in detail in 6.1). The system
constructs dependency trees of the form shown in
Figure 1 for each translation hypothesis. In this case
the target dependency trees very often do not con-
tain the correct target words and/or are not projective
with respect to the best possible order.
3For example, checking which word is the rightmost for the
heuristic for one-to-many mappings and checking whether the
constructed tree is projective requires knowledge of the correct
word order of the target.
3 Language Model with Syntactic
Constraints: A Pilot Study
In this section we report the results of a pilot study to
evaluate the difficulty of ordering a target sentence if
we are given a target dependency tree as the one in
Figure 1, versus if we are just given an unordered
bag of target language words.
The difference between those two settings is that
when ordering a target dependency tree, many of the
orders of the sentence are not allowed, because they
would be non-projective with respect to the tree.
Figure 1 (b) shows some orders which violate the
projectivity constraint. If the given target depen-
dency tree is projective with respect to the correct
word order, constraining the possible orders to the
ones consistent with the tree can only help perfor-
mance. In our experiments on reference sentences,
the target dependency trees are projective by con-
struction. If, however, the target dependency tree
provided is not necessarily projective with respect
to the best word order, the constraint may or may
not be useful. This could happen in our experiments
on ordering MT output sentences.
Thus in this section we aim to evaluate the use-
fulness of the constraint in both settings: reference
sentences with projective dependency trees, and MT
output sentences with possibly non-projective de-
pendency trees. We also seek to establish a baseline
for our task. Our methodology is to test a simple
and effective order model, which is used by all state
of the art SMT systems ? a trigram language model
? in the two settings: ordering an unordered bag of
words, and ordering a target dependency tree.
Our experimental design is as follows. Given an
unordered sentence t and an unordered target de-
pendency tree tree(t), we define two spaces of tar-
get sentence orders. These are the unconstrained
space of all permutations, denoted by Permutations(t)
and the space of all orders of t which are projec-
tive with respect to the target dependency tree, de-
noted by TargetProjective(t,tree(t)). For both spaces
S, we apply a standard trigram target language
model to select a most likely order from the space;
i.e., we find a target order order?S (t) such that:
order?S (t) = argmaxorder(t)?SPrLM (order(t)).
The operator which finds order?S (t) is difficult to
implement since the task is NP-hard in both set-
11
Reference Sentences
Space BLEU Avg. Size
Permutations 58.8 261
TargetProjective 83.9 229
MT Output Sentences
Space BLEU Avg. Size
Permutations 26.3 256
TargetProjective 31.7 225
Table 1: Performance of a tri-gram language model
on ordering reference and MT output sentences: un-
constrained or subject to target tree projectivity con-
straints.
tings, even for a bi-gram language model (Eisner
and Tromble, 2006).4 We implemented left-to-right
beam A* search for the Permutations space, and a
tree-based bottom up beam A* search for the Tar-
getProjective space. To give an estimate of the search
error in each case, we computed the number of times
the correct order had a better language model score
than the order returned by the search algorithm.5
The lower bounds on search error were 4% for Per-
mutations and 2% for TargetProjective, computed on
reference sentences.
We compare the performance in BLEU of orders
selected from both spaces. We evaluate the perfor-
mance on reference sentences and on MT output
sentences. Table 1 shows the results. In addition
to BLEU scores, the table shows the median number
of possible orders per sentence for the two spaces.
The highest achievable BLEU on reference sen-
tences is 100, because we are given the correct bag
of words. The highest achievable BLEU on MT out-
put sentences is well below 100 (the BLEU score of
the MT output sentences is 33). Table 3 describes
the characteristics of the main data-sets used in the
experiments in this paper; the test sets we use in the
present pilot study are the reference test set (Ref-
test) of 1K sentences and the MT test set (MT-test)
of 1,000 sentences.
The results from our experiment show that the tar-
get tree projectivity constraint is extremely powerful
on reference sentences, where the tree given is in-
deed projective. (Recall that in order to obtain the
target dependency tree in this setting we have used
information from the true order, which explains in
part the large performance gain.)
4Even though the dependency tree constrains the space, the
number of children of a node is not bounded by a constant.
5This is an underestimate of search error, because we don?t
know if there was another (non-reference) order which had a
better score, but was not found.
The gain in BLEU due to the constraint was not
as large on MT output sentences, but was still con-
siderable. The reduction in search space size due
to the constraint is enormous. There are about 230
times fewer orders to consider in the space of tar-
get projective orders, compared to the space of all
permutations. From these experiments we conclude
that the constraints imposed by a projective target
dependency tree are extremely informative. We also
conclude that the constraints imposed by the target
dependency trees constructed by our baseline MT
system are very informative as well, even though
the trees are not necessarily projective with respect
to the best order. Thus the projectivity constraint
with respect to a reasonably good target dependency
tree is useful for addressing the search and modeling
problems for MT ordering.
4 A Global Order Model for Target
Dependency Trees
In the rest of the paper we present our new word or-
der model and evaluate it on reference sentences and
in machine translation. In line with previous work
on NLP tasks such as parsing and recent work on
machine translation, we develop a discriminative or-
der model. An advantage of such a model is that we
can easily combine different kinds of features (such
as syntax-based and surface-based), and that we can
optimize the parameters of our model directly for the
evaluation measures of interest.
Additionally, we develop a globally normalized
model, which avoids the independence assumptions
in locally normalized conditional models.6 We train
a global log-linear model with a rich set of syntactic
and surface features. Because the space of possible
orders of an unordered dependency tree is factori-
ally large, we use simpler models to generate N-best
orders, which we then re-rank with a global model.
4.1 Generating N-best Orders
The simpler models which we use to generate N-best
orders of the unordered target dependency trees are
the standard trigram language model used in Section
3, and another statistical model, which we call a Lo-
cal Tree Order Model (LTOM). The LTOM model
6Those models often assume that current decisions are inde-
pendent of future observations.
12
[??]
this-1 eliminates the six minute delay+1
[? ? -2] [?? ? ] [6] [?] [?] [? ] [?? -1] [? ] [? ?? ? ]
Pron Verb Det Funcw Funcw Noun
[kore] [niyori] [roku] [fun] [kan] [no] [okure] [ga] [kaishou] [saremasu]
Pron Posp Noun Noun Noun Posp Noun Posp Vn Auxv
?this? ?by? 6 ?minute? ?period? ?of? ?delay? ?eliminate? PASSIVE
Figure 2: Dependency parse on the source (English)
sentence, alignment and projected tree on the target
(Japanese) sentence. Notice that the projected tree
is only partial and is used to show the head-relative
movement.
uses syntactic information from the source and tar-
get dependency trees, and orders each local tree of
the target dependency tree independently. It follows
the order model defined in (Quirk et al, 2005).
The model assigns a probability to the position
of each target node (modifier) relative to its par-
ent (head), based on information in both the source
and target trees. The probability of an order of the
complete target dependency tree decomposes into a
product over probabilities of positions for each node
in the tree as follows:
P (order(t)|s, t) =
?
n?t
P (pos(n, parent(n))|s, t)
Here, position is modelled in terms of closeness
to the head in the dependency tree. The closest
pre-modifier of a given head has position ?1; the
closest post-modifier has a position 1. Figure 2
shows an example dependency tree pair annotated
with head-relative positions. A small set of features
is used to reflect local information in the dependency
tree to model P (pos(n, parent(n))|s, t): (i) lexical
items of n and parent(n), (ii) lexical items of the
source nodes aligned to n and parent(n), (iii) part-
of-speech of the source nodes aligned to the node
and its parent, and (iv) head-relative position of the
source node aligned to the target node.
We train a log-linear model which uses these fea-
tures on a training set of aligned sentences with
source and target dependency trees in the form of
Figure 2. The model is a local (non-sequence) clas-
sifier, because the decision on where to place each
node does not depend on the placement of any other
nodes.
Since the local tree order model learns to order
whole subtrees of the target dependency tree, and
since it uses syntactic information from the source, it
provides an alternative view compared to the trigram
language model. The example in Figure 2 shows
that the head word ?eliminates? takes a dependent
?this? to the left (position ?1), and on the Japanese
side, the head word ?kaishou? (corresponding to
?eliminates?) takes a dependent ?kore? (correspond-
ing to ?this?) to the left (position ?2). The trigram
language model would not capture the position of
?kore? with respect to ?kaishou?, because the words
are farther than three positions away.
We use the language model and the local tree or-
der model to create N-best target dependency tree
orders. In particular, we generate the N-best lists
from a simple log-linear combination of the two
models:
P (o(t)|s, t) ? PLM (o(t)|t)PLTOM (o(t)|s, t)?
where o(t) denotes an order of the target.7 We used
a bottom-up beam A* search to generate N-best or-
ders. The performance of each of these two models
and their combination, together with the 30-best or-
acle performance on reference sentences is shown in
Table 2. As we can see, the 30-best oracle perfor-
mance of the combined model (98.0) is much higher
than the 1-best performance (92.6) and thus there is
a lot of room for improvement.
4.2 Model
The log-linear reranking model is defined as fol-
lows. For each sentence pair spl (l = 1, 2, ..., L) in
the training data, we have N candidate target word
orders ol,1, ol,2, ..., ol,N , which are the orders gener-
ated from the simpler models. Without loss of gen-
erality, we define ol,1 to be the order with the highest
BLEU score with respect to the correct order.8
We define a set of feature functions fm(ol,n, spl)
to describe a target word order ol,n of a given sen-
tence pair spl. In the log-linear model, a correspond-
ing weights vector ? is used to define the distribution
over all possible candidate orders:
p(ol,n|spl, ?) = e
?F (ol,n,spl)
?
n? e
?F (ol,n? ,spl)
7We used the value ? = .5, which we selected on a devel-
opment set to maximize BLEU.
8To avoid the problem that all orders could have a BLEU
score of 0 if none of them contains a correct word four-gram,
we define sentence-level k-gram BLEU, where k is the highest
order, k ? 4, for which there exists a correct k-gram in at least
one of the N-Best orders.
13
We train the parameters ? by minimizing the neg-
ative log-likelihood of the training data plus a
quadratic regularization term:
L(?) = ??l log p(ol,1|spi, ?) + 12?2
?
m ?m2
We also explored maximizing expected BLEU as
our objective function, but since it is not convex, the
performance was less stable and ultimately slightly
worse, as compared to the log-likelihood objective.
4.3 Features
We design features to capture both the head-relative
movement and the surface sequence movement of
words in a sentence. We experiment with different
combinations of features and show their contribu-
tion in Table 2 for reference sentences and Table 4
in machine translation. The notations used in the ta-
bles are defined as follows:
Baseline: LTOM+LM as described in Section 4.1
Word Bigram: Word bigrams of the target sen-
tence. Examples from Figure 2: ?kore?+?niyori?,
?niyori?+?roku?.
DISP: Displacement feature. For each word posi-
tion in the target sentence, we examine the align-
ment of the current word and the previous word, and
categorize the possible patterns into 3 kinds: (a) par-
allel, (b) crossing, and (c) widening. Figure 3 shows
how these three categories are defined.
Pharaoh DISP: Displacement as used in Pharaoh
(Koehn, 2004). For each position in the sentence,
the value of the feature is one less than the difference
(absolute value) of the positions of the source words
aligned to the current and the previous target word.
POSs and POSt: POS tags on the source and target
sides. For Japanese, we have a set of 19 POS tags.
?+? means making conjunction of features and
prev() means using the information associated with
the word from position ?1.
In all explored models, we include the log-
probability of an order according to the language
model and the log-probability according to the lo-
cal tree order model, the two features used by the
baseline model.
5 Evaluation on Reference Sentences
Our experiments on ordering reference sentences
use a set of 445K English sentences with their ref-
erence Japanese translations. This is a subset of the
(N (N
-L -L
(a) parallel
(N (NQ
-L -L
(b) crossing
(N (NQ
-L -L
(c) widening
Figure 3: Displacement feature: different alignment
patterns of two contiguous words in the target sen-
tence.
set MT-train in Table 3. The sentences were anno-
tated with alignment (using GIZA++ (Och and Ney,
2004)) and syntactic dependency structures of the
source and target, obtained as described in Section
2. Japanese POS tags were assigned by an automatic
POS tagger, which is a local classifier not using tag
sequence information.
We used 400K sentence pairs from the complete
set to train the first pass models: the language model
was trained on 400K sentences, and the local tree
order model was trained on 100K of them. We gen-
erated N-best target tree orders for the rest of the
data (45K sentence pairs), and used it for training
and evaluating the re-ranking model. The re-ranking
model was trained on 44K sentence pairs. All mod-
els were evaluated on the remaining 1,000 sentence
pairs set, which is the set Ref-test in Table 3.
The top part of Table 2 presents the 1-best
BLEU scores (actual performance) and 30-best or-
acle BLEU scores of the first-pass models and their
log-linear combination, described in Section 4. We
can see that the combination of the language model
and the local tree order model outperformed either
model by a large margin. This indicates that combin-
ing syntactic (from the LTOM model) and surface-
based (from the language model) information is very
effective even at this stage of selecting N-best orders
for re-ranking. According to the 30-best oracle per-
formance of the combined model LTOM+LM, 98.0
BLEU is the upper bound on performance of our re-
ranking approach.
The bottom part of the table shows the perfor-
mance of the global log-linear model, when features
in addition to the scores from the two first-pass mod-
els are added to the model. Adding word-bigram
features increased performance by about 0.6 BLEU
points, indicating that training language-model like
features discriminatively to optimize ordering per-
formance, is indeed worthwhile. Next we compare
14
First-pass models
Model BLEU
1 best 30 best
Lang Model (Permutations) 58.8 71.2
Lang Model (TargetProjective) 83.9 95.0
Local Tree Order Model 75.8 87.3
Local Tree Order Model + Lang Model 92.6 98.0
Re-ranking Models
Features BLEU
Baseline 92.60
Word Bigram 93.19
Pharaoh DISP 92.94
DISP 93.57
DISP+POSs 94.04
DISP+POSs+POSt 94.14
DISP+POSs+POSt, prev(DISP)+POSs+POSt 94.34
DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB 94.50
Table 2: Performance of the first-pass order models
and 30-best oracle performance, followed by perfor-
mance of re-ranking model for different feature sets.
Results are on reference sentences.
the Pharaoh displacement feature to the displace-
ment feature we illustrated in Figure 3. We can
see that the Pharaoh displacement feature improves
performance of the baseline by .34 points, whereas
our displacement feature improves performance by
nearly 1 BLEU point. Concatenating the DISP fea-
ture with the POS tag of the source word aligned to
the current word improved performance slightly.
The results show that surface movement features
(i.e. the DISP feature) improve the performance
of a model using syntactic-movement features (i.e.
the LTOM model). Additionally, adding part-of-
speech information from both languages in combi-
nation with displacement, and using a higher order
on the displacement features was useful. The per-
formance of our best model, which included all in-
formation sources, is 94.5 BLEU points, which is a
35% improvement over the fist-pass models, relative
to the upper bound.
6 Evaluation in Machine Translation
We apply our model to machine translation by re-
ordering the translation produced by a baseline MT
system. Our baseline MT system constructs, for
each target translation hypothesis, a target depen-
dency tree. Thus we can apply our model to MT
output in exactly the same way as for reference sen-
tences, but using much noisier input: a source sen-
tence with a dependency tree, word alignment and
an unordered target dependency tree as the example
shown in Figure 2. The difference is that the target
dependency tree will likely not contain the correct
data set num sent. English Japanese
avg. len vocab avg. len vocab
MT-train 500K 15.8 77K 18.7 79K
MT-test 1K 17.5 ? 20.9 ?
Ref-test 1K 17.5 ? 21.2 ?
Table 3: Main data sets used in experiments.
target words and/or will not be projective with re-
spect to the best possible order.
6.1 Baseline MT System
Our baseline SMT system is the system of Quirk et
al. (2005). It translates by first deriving a depen-
dency tree for the source sentence and then trans-
lating the source dependency tree to a target depen-
dency tree, using a set of probabilistic models. The
translation is based on treelet pairs. A treelet is a
connected subgraph of the source or target depen-
dency tree. A treelet translation pair is a pair of
word-aligned source and target treelets.
The baseline SMT model combines this treelet
translation model with other feature functions ? a
target language model, a tree order model, lexical
weighting features to smooth the translation prob-
abilities, word count feature, and treelet-pairs count
feature. These models are combined as feature func-
tions in a (log)linear model for predicting a target
sentence given a source sentence, in the framework
proposed by (Och and Ney, 2002). The weights
of this model are trained to maximize BLEU (Och
and Ney, 2004). The SMT system is trained using
the same form of data as our order model: parallel
source and target dependency trees as in Figure 2.
Of particular interest are the components in the
baseline SMT system contributing most to word or-
der decisions. The SMT system uses the same target
language trigram model and local tree order model,
as we are using for generating N-best orders for re-
ranking. Thus the baseline system already uses our
first-pass order models and only lacks the additional
information provided by our re-ranking order model.
6.2 Data and Experimental Results
The baseline MT system was trained on the MT-train
dataset described in Table 3. The test set for the MT
experiment is a 1K sentences set from the same do-
main (shown as MT-test in the table). The weights
in the linear model used by the baseline SMT system
were tuned on a separate development set.
Table 4 shows the performance of the first-pass
models in the top part, and the performance of our
15
First-pass models
Model BLEU
1 best 30 best
Baseline MT System 33.0 ?
Lang Model (Permutations) 26.3 28.7
Lang Model (TargetCohesive) 31.7 35.0
Local Tree Order Model 27.2 31.5
Local Tree Order Model + Lang Model 33.6 36.0
Re-ranking Models
Features BLEU
Baseline 33.56
Word Bigram 34.11
Pharaoh DISP 34.67
DISP 34.90
DISP+POSs 35.28
DISP+POSs+POSt 35.22
DISP+POSs+POSt, prev(DISP)+POSs+POSt 35.33
DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB 35.37
Table 4: Performance of the first pass order models
and 30-best oracle performance, followed by perfor-
mance of re-ranking model for different feature sets.
Results are in MT.
re-ranking model in the bottom part. The first row
of the table shows the performance of the baseline
MT system, which is a BLEU score of 33. Our first-
pass and re-ranking models re-order the words of
this 1-best output from the MT system. As for ref-
erence sentences, the combination of the two first-
pass models outperforms the individual models. The
1-best performance of the combination is 33.6 and
the 30-best oracle is 36.0. Thus the best we could
do with our re-ranking model in this setting is 36
BLEU points.9 Our best re-ranking model achieves
2.4 BLEU points improvement over the baseline MT
system and 1.8 points improvement over the first-
pass models, as shown in the table. The trends here
are similar to the ones observed in our reference ex-
periments, with the difference that target POS tags
were less useful (perhaps due to ungrammatical can-
didates) and the displacement features were more
useful. We can see that our re-ranking model al-
most reached the upper bound oracle performance,
reducing the gap between the first-pass models per-
formance (33.6) and the oracle (36.0) by 75%.
7 Conclusions and Future Work
We have presented a discriminative syntax-based or-
der model for machine translation, trained to to se-
9Notice that the combination of our two first-pass models
outperforms the baseline MT system by half a point (33.6 ver-
sus 33.0). This is perhaps due to the fact that the MT system
searches through a much larger space (possible word transla-
tions in addition to word orders), and thus could have a higher
search error.
lect from the space of orders projective with respect
to a target dependency tree. We investigated a com-
bination of features modeling surface movement and
syntactic movement phenomena and showed that
these two information sources are complementary
and their combination is powerful. Our results on or-
dering MT output and reference sentences were very
encouraging. We obtained substantial improvement
by the simple method of post-processing the 1-best
MT output to re-order the proposed translation. In
the future, we would like to explore tighter integra-
tion of our order model with the SMT system and to
develop more accurate algorithms for constructing
projective target dependency trees in translation.
References
Y. Al-Onaizan and K. Papineni. 2006. Distortion models for
statistical machine translation. In ACL.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In ACL.
M. Collins. 2000. Discriminative reranking for natural language
parsing. In ICML, pages 175?182.
J Eisner and R. W. Tromble. 2006. Local search with very
large-scale neighborhoods for optimal permutations in ma-
chine translation. In HLT-NAACL Workshop.
H. Fox. 2002. Phrasal cohesion and statistical machine transla-
tion. In EMNLP.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In ACL.
P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-
based statistical machine translation models. In AMTA.
R. Kuhn, D. Yuen, M. Simard, P. Paul, G. Foster, E. Joanis, and
H. Johnson. 2006. Segment choice models: Feature-rich
models for global distortion in statistical machine transla-
tion. In HLT-NAACL.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
ACL.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguistics,
30(4).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. BLEU: a
method for automatic evaluation of machine translation. In
ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet
translation: Syntactically informed phrasal SMT. In ACL.
B. Wellington, S. Waxmonsky, and I. Dan Melamed. 2006.
Empirical lower bounds on the complexity of translational
equivalence. In ACL-COLING.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based
phrase reordering model for statistical machine translation.
In ACL.
K. Yamada and Kevin Knight. 2001. A syntax-based statistical
translation model. In ACL.
16
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128?135,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generating Complex Morphology for Machine Translation
Einat Minkov?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
einatm@cs.cmu.edu
Kristina Toutanova
Microsoft Research
Redmond, WA, USA
kristout@microsoft.com
Hisami Suzuki
Microsoft Research
Redmond, WA, USA
hisamis@microsoft.com
Abstract
We present a novel method for predicting in-
flected word forms for generating morpho-
logically rich languages in machine trans-
lation. We utilize a rich set of syntactic
and morphological knowledge sources from
both source and target sentences in a prob-
abilistic model, and evaluate their contribu-
tion in generating Russian and Arabic sen-
tences. Our results show that the proposed
model substantially outperforms the com-
monly used baseline of a trigram target lan-
guage model; in particular, the use of mor-
phological and syntactic features leads to
large gains in prediction accuracy. We also
show that the proposed method is effective
with a relatively small amount of data.
1 Introduction
Machine Translation (MT) quality has improved
substantially in recent years due to applying data
intensive statistical techniques. However, state-of-
the-art approaches are essentially lexical, consider-
ing every surface word or phrase in both the source
sentence and the corresponding translation as an in-
dependent entity. A shortcoming of this word-based
approach is that it is sensitive to data sparsity. This is
an issue of importance as aligned corpora are an ex-
pensive resource, which is not abundantly available
for many language pairs. This is particularly prob-
lematic for morphologically rich languages, where
word stems are realized in many different surface
forms, which exacerbates the sparsity problem.
? This research was conducted during the author?s intern-
ship at Microsoft Research.
In this paper, we explore an approach in which
words are represented as a collection of morpholog-
ical entities, and use this information to aid in MT
for morphologically rich languages. Our goal is two-
fold: first, to allow generalization over morphology
to alleviate the data sparsity problem in morphology
generation. Second, to model syntactic coherence in
the form of morphological agreement in the target
language to improve the generation of morphologi-
cally rich languages. So far, this problem has been
addressed in a very limited manner in MT, most typ-
ically by using a target language model.
In the framework suggested in this paper, we train
a model that predicts the inflected forms of a se-
quence of word stems in a target sentence, given
the corresponding source sentence. We use word
and word alignment information, as well as lexi-
cal resources that provide morphological informa-
tion about the words on both the source and target
sides. Given a sentence pair, we also obtain syntactic
analysis information for both the source and trans-
lated sentences. We generate the inflected forms of
words in the target sentence using all of the available
information, using a log-linear model that learns the
relevant mapping functions.
As a case study, we focus on the English-Russian
and English-Arabic language pairs. Unlike English,
Russian and Arabic have very rich systems of mor-
phology, each with distinct characteristics. Trans-
lating from a morphology-poor to a morphology-
rich language is especially challenging since de-
tailed morphological information needs to be de-
coded from a language that does not encode this in-
formation or does so only implicitly (Koehn, 2005).
We believe that these language pairs are represen-
128
tative in this respect and therefore demonstrate the
generality of our approach.
There are several contributions of this work. First,
we propose a general approach that shows promise
in addressing the challenges of MT into morpholog-
ically rich languages. We show that the use of both
syntactic and morphological information improves
translation quality. We also show the utility of
source language information in predicting the word
forms of the target language. Finally, we achieve
these results with limited morphological resources
and training data, suggesting that the approach is
generally useful for resource-scarce language pairs.
2 Russian and Arabic Morphology
Table 1 describes the morphological features rele-
vant to Russian and Arabic, along with their possible
values. The rightmost column in the table refers to
the morphological features that are shared by Rus-
sian and Arabic, including person, number, gender
and tense. While these features are fairly generic
(they are also present in English), note that Rus-
sian includes an additional gender (neuter) and Ara-
bic has a distinct number notion for two (dual). A
central dimension of Russian morphology is case
marking, realized as suffixation on nouns and nom-
inal modifiers1. The Russian case feature includes
six possible values, representing the notions of sub-
ject, direct object, location, etc. In Arabic, like other
Semitic languages, word surface forms may include
proclitics and enclitics (or prefixes and suffixes as
we refer to them in this paper), concatenated to in-
flected stems. For nouns, prefixes include conjunc-
tions (wa: ?and?, fa: ?and, so?), prepositions (bi:
?by, with?, ka: ?like, such as?, li: ?for, to?) and a de-
terminer, and suffixes include possessive pronouns.
Verbal prefixes include conjunction and negation,
and suffixes include object pronouns. Both object
and possessive pronouns are captured by an indica-
tor function for its presence or absence, as well as
by the features that indicate their person, number
and gender. As can be observed from the table, a
large number of surface inflected forms can be gen-
erated by the combination of these features, making
1Case marking also exists in Arabic. However, in many in-
stances, it is realized by diacritics which are ignored in standard
orthography. In our experiments, we include case marking in
Arabic only when it is reflected in the orthography.
the morphological generation of these languages a
non-trivial task.
Morphologically complex languages also tend to
display a rich system of agreements. In Russian, for
example, adjectives agree with head nouns in num-
ber, gender and case, and verbs agree with the sub-
ject noun in person and number (past tense verbs
agree in gender and number). Arabic has a similarly
rich system of agreement, with unique characteris-
tics. For example, in addition to agreement involv-
ing person, number and gender, it also requires a de-
terminer for each word in a definite noun phrase with
adjectival modifiers; in a noun compound, a deter-
miner is attached to the last noun in the chain. Also,
non-human subject plural nouns require the verb to
be inflected in a singular feminine form. Generating
these morphologically complex languages is there-
fore more difficult than generating English in terms
of capturing the agreement phenomena.
3 Related Work
The use of morphological features in language mod-
elling has been explored in the past for morphology-
rich languages. For example, (Duh and Kirchhoff,
2004) showed that factored language models, which
consider morphological features and use an opti-
mized backoff policy, yield lower perplexity.
In the area of MT, there has been a large body
of work attempting to modify the input to a transla-
tion system in order to improve the generated align-
ments for particular language pairs. For example,
it has been shown (Lee, 2004) that determiner seg-
mentation and deletion in Arabic sentences in an
Arabic-to-English translation system improves sen-
tence alignment, thus leading to improved over-
all translation quality. Another work (Koehn and
Knight, 2003) showed improvements by splitting
compounds in German. (Nie?en and Ney, 2004)
demonstrated that a similar level of alignment qual-
ity can be achieved with smaller corpora applying
morpho-syntactic source restructuring, using hierar-
chical lexicon models, in translating from German
into English. (Popovic? and Ney, 2004) experimented
successfully with translating from inflectional lan-
guages into English making use of POS tags, word
stems and suffixes in the source language. More re-
cently, (Goldwater and McClosky, 2005) achieved
improvements in Czech-English MT, optimizing a
129
Features Russian Arabic Both
POS (11 categories) (18 categories)
Person 1,2,3
Number dual sing(ular), pl(ural)
Gender neut(er) masc(uline), fem(inine)
Tense gerund present, past, future, imperative
Mood subjunctive, jussive
Case dat(ive), prep(ositional), nom(inative), acc(usative), gen(itive)
instr(umental)
Negation yes, no
Determiner yes, no
Conjunction wa, fa, none
Preposition bi, ka, li, none
ObjectPronoun yes, no
Pers/Numb/Gend of pronoun, none
PossessivePronoun Same as ObjectPronoun
Table 1: Morphological features used for Russian and Arabic
set of possible source transformations, incorporat-
ing morphology. In general, this line of work fo-
cused on translating from morphologically rich lan-
guages into English; there has been limited research
in MT in the opposite direction. Koehn (2005) in-
cludes a survey of statistical MT systems in both di-
rections for the Europarl corpus, and points out the
challenges of this task. A recent work (El-Kahlout
and Oflazer, 2006) experimented with English-to-
Turkish translation with limited success, suggesting
that inflection generation given morphological fea-
tures may give positive results.
In the current work, we suggest a probabilistic
framework for morphology generation performed as
post-processing. It can therefore be considered as
complementary to the techniques described above.
Our approach is general in that it is not specific to
a particular language pair, and is novel in that it al-
lows modelling of agreement on the target side. The
framework suggested here is most closely related to
(Suzuki and Toutanova, 2006), which uses a proba-
bilistic model to generate Japanese case markers for
English-to-Japanese MT. This work can be viewed
as a generalization of (Suzuki and Toutanova, 2006)
in that our model generates inflected forms of words,
and is not limited to generating a small, closed set of
case markers. In addition, the morphology genera-
tion problem is more challenging in that it requires
handling of complex agreement phenomena along
multiple morphological dimensions.
4 Inflection Prediction Framework
In this section, we define the task of of morphologi-
cal generation as inflection prediction, as well as the
lexical operations relevant for the task.
4.1 Morphology Analysis and Generation
Morphological analysis can be performed by ap-
plying language specific rules. These may include
a full-scale morphological analysis with contextual
disambiguation, or, when such resources are not
available, simple heuristic rules, such as regarding
the last few characters of a word as its morphogical
suffix. In this work, we assume that lexicons LS and
LT are available for the source and translation lan-
guages, respectively. Such lexicons can be created
manually, or automatically from data. Given a lexi-
con L and a surface word w, we define the following
operations:
? Stemming - let Sw = {s1, ..., sl} be the set of
possible morphological stems (lemmas) of w
according to L.2
? Inflection - let Iw = {i1, ..., im} be the set of
surface form words that have the same stem as
w. That is, i ? Iw iff Si?Sw 6= ?.
? Morphological analysis - let Aw = {a1, ..., av}
be the set of possible morphological analyses
for w. A morphological analysis a is a vector of
categorical values, where the dimensions and
possible values for each dimension in the vector
representation space are defined by L.
4.2 The Task
We assume that we are given aligned sentence pairs,
where a sentence pair includes a source and a tar-
2Multiple stems are possible due to ambiguity in morpho-
logical analysis.
130
NN+sg+nom+neut
the
DET
allocation of resources has completed
NN+sg PREP NN+pl AUXV+sg VERB+pastpart
?????????????
NN+sg+gen+pl+masc
????????
VERB+perf+pass+part+neut+sg
?????????
raspredelenie resursov zaversheno
Figure 1: Aligned English-Russian sentence pair
with syntactic and morphological annotation
get sentence, and lexicons LS and LT that support
the operations described in the section above. Let
a sentence w1, ...wt, ...wn be the output of a MT
system in the target language. This sentence can
be converted into the corresponding stem set se-
quence S1, ...St, ...Sn, applying the stemming op-
eration. Then the task is, for every stem set St in
the output sentence, to predict an inflection yt from
its inflection set It. The predicted inflections should
both reflect the meaning conveyed by the source sen-
tence, and comply with the agreement rules of the
target language. 3
Figure 1 shows an example of an aligned English-
Russian sentence pair: on the source (English) side,
POS tags and word dependency structure are indi-
cated by solid arcs. The alignments between En-
glish and Russian words are indicated by the dot-
ted lines. The dependency structure on the Russian
side, indicated by solid arcs, is given by a treelet MT
system in our case (see Section 6.1), projected from
the word dependency structure of English and word
alignment information. Note that the Russian sen-
tence displays agreement in number and gender be-
tween the subject noun (raspredelenie) and the pred-
icate (zaversheno); note also that resursov is in gen-
itive case, as it modifies the noun on its left.
5 Models for Inflection Prediction
5.1 A Probabilistic Model
Our learning framework uses a Maximum Entropy
Markov model (McCallum et al, 2000). The model
decomposes the overall probability of a predicted
inflection sequence into a product of local proba-
bilities for individual word predictions. The local
3That is, assuming that the stem sequence that is output by
the MT system is correct.
probabilities are conditioned on the previous k pre-
dictions. The model implemented here is of second
order: at any decision point t we condition the prob-
ability distribution over labels on the previous two
predictions yt?1 and yt?2 in addition to the given
(static) word context from both the source and tar-
get sentences. That is, the probability of a predicted
inflection sequence is defined as follows:
p(y | x) =
n
?
t=1
p(yt | yt?1, yt?2, xt), yt ? It
where xt denotes the given context at position t
and It is the set of inflections corresponding to St,
from which the model should choose yt.
The features we constructed pair up predicates on
the context ( x?, yt?1, yt?2) and the target label (yt).
In the suggested framework, it is straightforward to
encode the morphological properties of a word, in
addition to its surface inflected form. For example,
for a particular inflected word form yt and its con-
text, the derived paired features may include:
?k =
{
1 if surface word yt is y? and s? ? St+1
0 otherwise
?k+1 =
{ 1 if Gender(yt) =?Fem? and Gender(yt?1) =?Fem?
0 otherwise
In the first example, a given neighboring stem set
St+1 is used as a context feature for predicting the
target word yt. The second feature captures the gen-
der agreement with the previous word. This is possi-
ble because our model is of second order. Thus, we
can derive context features describing the morpho-
logical properties of the two previous predictions.4
Note that our model is not a simple multi-class clas-
sifier, because our features are shared across mul-
tiple target labels. For example, the gender fea-
ture above applies to many different inflected forms.
Therefore, it is a structured prediction model, where
the structure is defined by the morphological proper-
ties of the target predictions, in addition to the word
sequence decomposition.
5.2 Feature Categories
The information available for estimating the distri-
bution over yt can be split into several categories,
4Note that while we decompose the prediction task left-to-
right, an appealing alternative is to define a top-down decompo-
sition, traversing the dependency tree of the sentence. However,
this requires syntactic analysis of sufficient quality.
131
corresponding to feature source. The first ma-
jor distinction is monolingual versus bilingual fea-
tures: monolingual features refer only to the context
(and predicted label) in the target language, while
bilingual features have access to information in the
source sentences, obtained by traversing the word
alignment links from target words to a (set of) source
words, as shown in Figure 1.
Both monolingual and bilingual features can be
further split into three classes: lexical, morpholog-
ical and syntactic. Lexical features refer to surface
word forms, as well as their stems. Since our model
is of second order, our monolingual lexical fea-
tures include the features of a standard word trigram
language model. Furthermore, since our model is
discriminative (predicting word forms given their
stems), the monolingual lexical model can use stems
in addition to predicted words for the left and cur-
rent position, as well as stems from the right con-
text. Morphological features are those that refer to
the features given in Table 1. Morphological infor-
mation is used in describing the target label as well
as its context, and is intended to capture morpho-
logical generalizations. Finally, syntactic features
can make use of syntactic analyses of the source
and target sentences. Such analyses may be derived
for the target language, using the pre-stemmed sen-
tence. Without loss of generality, we will use here
a dependency parsing paradigm. Given a syntactic
analysis, one can construct syntactic features; for ex-
ample, the stem of the parent word of yt. Syntactic
features are expected to be useful in capturing agree-
ment phenomena.
5.3 Features
Table 2 gives the full set of suggested features for
Russian and Arabic, detailed by type. For monolin-
gual lexical features, we consider the stems of the
predicted word and its immediately adjacent words,
in addition to traditional word bigram and trigram
features. For monolingual morphological features,
we consider the morphological attributes of the two
previously predicted words and the current predic-
tion; for monolingual syntactic features, we use the
stem of the parent node.
The bilingual features include the set of words
aligned to the focus word at position t, where they
are treated as bag-of-words, i.e., each aligned word
Feature categories Instantiations
Monolingual lexical
Word stem st?1,st?2,st,st+1
Predicted word yt, yt?1, yt?2
Monolingual morphological
f : POS, Person, Number, Gender, Tense f(yt?2),f(yt?1),f(yt)
Neg, Det, Prep, Conj, ObjPron, PossPron
Monolingual syntactic
Parent stem sHEAD(t)
Bilingual lexical
Aligned word set Al Alt, Alt?1, Alt+1
Bilingual morph & syntactic
f : POS, Person, Number, Gender, Tense f(Alt), f(Alt?1),
Neg, Det, Prep, Conj, ObjPron, PossPron, f(Alt+1), f(AlHEAD(t))
Comp
Table 2: The feature set suggested for English-
Russian and English-Arabic pairs
is assigned a separate feature. Bilingual lexical fea-
tures can refer to words aligned to yt as all as words
aligned to its immediate neighbors yt?1 and yt+1.
Bilingual morphological and syntactic features re-
fer to the features of the source language, which
are expected to be useful for predicting morphol-
ogy in the target language. For example, the bilin-
gual Det (determiner) feature is computed accord-
ing to the source dependency tree: if a child of a
word aligned to wt is a determiner, then the fea-
ture value is assigned its surface word form (such
as a or the). The bilingual Prep feature is com-
puted similarly, by checking the parent chain of the
word aligned to wt for the existence of a preposi-
tion. This feature is hoped to be useful for predict-
ing Arabic inflected forms with a prepositional pre-
fix, as well as for predicting case marking in Rus-
sian. The bilingual ObjPron and PossPron features
represent any object pronoun of the word aligned to
wt and a preceding possessive pronoun, respectively.
These features are expected to map to the object and
possessive pronoun features in Arabic. Finally, the
bilingual Compound feature checks whether a word
appears as part of a noun compound in the English
source. f this is the case, the feature is assigned the
value of ?head? or ?dependent?. This feature is rel-
evant for predicting a genitive case in Russian and
definiteness in Arabic.
6 Experimental Settings
In order to evaluate the effectiveness of the sug-
gested approach, we performed reference experi-
ments, that is, using the aligned sentence pairs of
132
Data Eng-Rus Eng-Ara
Avg. sentlen Eng Rus Eng Ara
Training 1M 470K
14.06 12.90 12.85 11.90
Development 1,000 1,000
13.73 12.91 13.48 12.90
Test 1,000 1,000
13.61 12.84 8.49 7.50
Table 3: Data set statistics: corpus size and average
sentence length (in words)
reference translations rather than the output of an
MT system as input.5 This allows us to evaluate
our method with a reduced noise level, as the words
and word order are perfect in reference translations.
These experiments thus constitute a preliminary step
for tackling the real task of inflecting words in MT.
6.1 Data
We used a corpus of approximately 1 million aligned
sentence pairs for English-Russian, and 0.5 million
pairs for English-Arabic. Both corpora are from a
technical (software manual) domain, which we be-
lieve is somewhat restricted along some morpho-
logical dimensions, such as tense and person. We
used 1,000 sentence pairs each for development and
testing for both language pairs. The details of the
datasets used are given in Table 3.
The sentence pairs were word-aligned using
GIZA++ (Och and Ney, 2000) and submitted to a
treelet-based MT system (Quirk et al, 2005), which
uses the word dependency structure of the source
language and projects word dependency structure to
the target language, creating the structure shown in
Figure 1 above.
6.2 Lexicon
Table 4 gives some relevant statistics of the lexicons
we used. For Russian, a general-domain lexicon was
available to us, consisting of about 80,000 lemmas
(stems) and 9.4 inflected forms per stem.6 Limiting
the lexicon to word types that are seen in the train-
ing set reduces its size substantially to about 14,000
stems, and an average of 3.8 inflections per stem.
We will use this latter ?domain-adapted? lexicon in
our experiments.
5In this case, yt should equal wt, according to the task defi-
nition.
6The averages reported in Table 4 are by type and do not
consider word frequencies in the data.
Source Stems Avg(| I |) Avg(| S |)
Rus. Lexicon 79,309 9.4
Lexicon ? Train 13,929 3.8 1.6
Ara. Lexicon ? Train 12,670 7.0 1.7
Table 4: Lexicon statistics
For Arabic, as a full-size Arabic lexicon was not
available to us, we used the Buckwalter morpholog-
ical analyzer (Buckwalter, 2004) to derive a lexicon.
To acquire the stemming and inflection operators, we
submit all words in our training data to the Buckwal-
ter analyzer. Note that Arabic displays a high level
of ambiguity, each word corresponding to many pos-
sible segmentations and morphological analyses; we
considered all of the different stems returned by the
Buckwalter analyzer in creating a word?s stem set.
The lexicon created in this manner contains 12,670
distinct stems and 89,360 inflected forms.
For the generation of word features, we only con-
sider one dominant analysis for any surface word
for simplicity. In case of ambiguity, we considered
only the first (arbitrary) analysis for Russian. For
Arabic, we apply the following heuristic: use the
most frequent analysis estimated from the gold stan-
dard labels in the Arabic Treebank (Maamouri et al,
2005); if a word does not appear in the treebank, we
choose the first analysis returned by the Buckwal-
ter analyzer. Ideally, the best word analysis should
be provided as a result of contextual disambiguation
(e.g., (Habash and Rambow, 2005)); we leave this
for future work.
6.3 Baseline
As a baseline, we pick a morphological inflection yt
at random from It. This random baseline serves as
an indicator for the difficulty of the problem. An-
other more competitive baseline we implemented
is a word trigram language model (LM). The LMs
were trained using the CMU language modelling
toolkit (Clarkson and Rosenfeld, 1997) with default
settings on the training data described in Table 3.
6.4 Experiments
In the experiments, our primary goal is to evaluate
the effectiveness of the proposed model using all
features available to us. Additionally, we are inter-
ested in knowing the contribution of each informa-
tion source, namely of morpho-syntactic and bilin-
gual features. Therefore, we study the performance
133
of models including the full feature schemata as well
as models that are restricted to feature subsets ac-
cording to the feature types as described in Section
5.2. The models are as follows: Monolingual-Word,
including LM-like and stem n-gram features only;
Bilingual-Word, which also includes bilingual lex-
ical features;7 Monolingual-All, which has access
to all the information available in the target lan-
guage, including morphological and syntactic fea-
tures; and finally, Bilingual-All, which includes all
feature types from Table 2.
For each model and language, we perform feature
selection in the following manner. The features are
represented as feature templates, such as ?POS=X?,
which generate a set of binary features correspond-
ing to different instantiations of the template, as in
?POS=NOUN?. In addition to individual features, con-
junctions of up to three features are also considered
for selection (e.g., ?POS=NOUN & Number=plural?).
Every conjunction of feature templates considered
contains at least one predicate on the prediction yt,
and up to two predicates on the context. The feature
selection algorithm performs a greedy forward step-
wise feature selection on the feature templates so as
to maximize development set accuracy. The algo-
rithm is similar to the one described in (Toutanova,
2006). After this process, we performed some man-
ual inspection of the selected templates, and finally
obtained 11 and 36 templates for the Monolingual-
All and Bilingual-All settings for Russian, respec-
tively. These templates generated 7.9 million and
9.3 million binary feature instantiations in the fi-
nal model, respectively. The corresponding num-
bers for Arabic were 27 feature templates (0.7 mil-
lion binary instantiations) and 39 feature templates
(2.3 million binary instantiations) for Monolingual-
All and Bilingual-All, respectively.
7 Results and Discussion
Table 5 shows the accuracy of predicting word forms
for the baseline and proposed models. We report ac-
curacy only on words that appear in our lexicons.
Thus, punctuation, English words occurring in the
target sentence, and words with unknown lemmas
are excluded from the evaluation. The reported ac-
curacy measure therefore abstracts away from the is-
7Overall, this feature set approximates the information that
is available to a state-of-the-art statistical MT system.
Model Eng-Rus Eng-Ara
Random 31.7 16.3
LM 77.6 31.7
Monolingual Word 85.1 69.6
Bilingual Word 87.1 71.9
Monolingual All 87.1 71.6
Bilingual All 91.5 73.3
Table 5: Accuracy (%) results by model
sue of incomplete coverage of the lexicon. When
we encounter these words in the true MT scenario,
we will make no predictions about them, and simply
leave them unmodified. In our current experiments,
in Russian, 68.2% of all word tokens were in Cyril-
lic, of which 93.8% were included in our lexicon.
In Arabic, 85.5% of all word tokens were in Arabic
characters, of which 99.1% were in our lexicon.8
The results in Table 5 show that the suggested
models outperform the language model substantially
for both languages. In particular, the contribution of
both bilingual and non-lexical features is notewor-
thy: adding non-lexical features consistently leads
to 1.5% to 2% absolute gain in both monolingual
and bilingual settings in both language pairs. We
obtain a particularly large gain in the Russian bilin-
gual case, in which the absolute gain is more than
4%, translating to 34% error rate reduction. Adding
bilingual features has a similar effect of gaining
about 2% (and 4% for Russian non-lexical) in ac-
curacy over monolingual models. The overall accu-
racy is lower in Arabic than in Russian, reflecting
the inherent difficulty of the task, as indicated by the
random baseline (31.7 in Russian vs. 16.3 in Ara-
bic).
In order to evaluate the effectiveness of the model
in alleviating the data sparsity problem in morpho-
logical generation, we trained inflection prediction
models on various subsets of the training data de-
scribed in Table 3, and tested their accuracy. The
results are given in Figure 2. We can see that with as
few as 5,000 training sentences pairs, the model ob-
tains much better accuracy than the language model,
which is trained on data that is larger by a few orders
of magnitude. We also note that the learning curve
8For Arabic, the inflection ambiguity was extremely high:
there were on average 39 inflected forms per stem set in our
development corpus (per token), as opposed to 7 in Russian.
We therefore limited the evaluation of Arabic to those stems that
have up to 30 inflected forms, resulting in 17 inflected forms per
stem set on average in the development data.
134
50
55
60
65
70
75
80
85
90
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 10
0
Training data size (x1,000)
Ac
cu
ra
cy
 
(%
)
RUS-bi-word
RUS-bi-all
ARA-bi-word
ARA-bi-all
Figure 2: Accuracy, varying training data size
becomes less steep as we use more training data,
suggesting that the models are successfully learning
generalizations.
We have also manually examined some repre-
sentative cases where the proposed model failed to
make a correct prediction. In both Russian and Ara-
bic, a very common pattern was a mistake in pre-
dicting the gender (as well as number and person in
Arabic) of pronouns. This may be attributed to the
fact that the correct choice of the pronoun requires
coreference resolution, which is not available in our
model. A more thorough analysis of the results will
be helpful to bring further improvements.
8 Conclusions and Future Work
We presented a probabilistic framework for mor-
phological generation given aligned sentence pairs,
incorporating morpho-syntactic information from
both the source and target sentences. The re-
sults, using reference translations, show that the pro-
posed models achieve substantially better accuracy
than language models, even with a relatively small
amount of training data. Our models using morpho-
syntactic information also outperformed models us-
ing only lexical information by a wide margin. This
result is very promising for achieving our ultimate
goal of improving MT output by using a special-
ized model for target language morphological gener-
ation. Though this goal is clearly outside the scope
of this paper, we conducted a preliminary experi-
ment where an English-to-Russian MT system was
trained on a stemmed version of the aligned data and
used to generate stemmed word sequences, which
were then inflected using the suggested framework.
This simple integration of the proposed model with
the MT system improved the BLEU score by 1.7.
The most obvious next step of our research, there-
fore, is to further pursue the integration of the pro-
posed model to the end-to-end MT scenario.
There are multiple paths for obtaining further im-
provements over the results presented here. These
include refinement in feature design, word analysis
disambiguation, morphological and syntactic anal-
ysis on the source English side (e.g., assigning se-
mantic role tags), to name a few. Another area of
investigation is capturing longer-distance agreement
phenomena, which can be done by implementing a
global statistical model, or by using features from
dependency trees more effectively.
References
Tim Buckwalter. 2004. Buckwalter arabic morphological ana-
lyzer version 2.0.
Philip Clarkson and Roni Rosenfeld. 1997. Statistical language
modelling using the CMU cambridge toolkit. In Eurospeech.
Kevin Duh and Kathrin Kirchhoff. 2004. Automatic learning of
language model structure. In COLING.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Initial ex-
plorations in English to Turkish statistical machine transla-
tion. In NAACL workshop on statistical machine translation.
Sharon Goldwater and David McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In EMNLP.
Nizar Habash and Owen Rambow. 2005. Arabic tokenization,
part-of-speech tagging and morphological disambiguation in
one fell swoop. In ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for
compound splitting. In EACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In MT Summit.
Young-Suk Lee. 2004. Morphological analysis for statistical
machine translation. In HLT-NAACL.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Hubert
Jin. 2005. Arabic Treebank: Part 1 v 3.0. Linguistic Data
Consortium.
Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira.
2000. Maximum entropy markov models for information
extraction and segmentation. In ICML.
Sonja Nie?en and Hermann Ney. 2004. Statistical machine
translation with scarce resources using morpho-syntactic in-
formation. Computational Linguistics, 30(2):181?204.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In ACL.
Maja Popovic? and Hermann Ney. 2004. Towards the use of
word stems and suffixes for statistical machine translation.
In LREC.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Depen-
dency tree translation: Syntactically informed phrasal SMT.
In ACL.
Hisami Suzuki and Kristina Toutanova. 2006. Learning to pre-
dict case markers in Japanese. In COLING-ACL.
Kristina Toutanova. 2006. Competitive generative models with
structure learning for NLP classification tasks. In EMNLP.
135
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824?831,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Comparative Study of Parameter Estimation Methods for 
Statistical Natural Language Processing 
Jianfeng Gao*, Galen Andrew*, Mark Johnson*&, Kristina Toutanova* 
*Microsoft Research, Redmond WA 98052, {jfgao,galena,kristout}@microsoft.com 
&Brown University, Providence, RI 02912,  mj@cs.brown.edu 
 
Abstract 
This paper presents a comparative study of 
five parameter estimation algorithms on four 
NLP tasks. Three of the five algorithms are 
well-known in the computational linguistics 
community: Maximum Entropy (ME) estima-
tion with L2 regularization, the Averaged 
Perceptron (AP), and Boosting.  We also in-
vestigate ME estimation with L1 regularization 
using a novel optimization algorithm, and 
BLasso, which is a version of Boosting with 
Lasso (L1) regularization.  We first investigate 
all of our estimators on two re-ranking tasks: a 
parse selection task and a language model 
(LM) adaptation task.  Then we apply the best 
of these estimators to two additional tasks 
involving conditional sequence models: a 
Conditional Markov Model (CMM) for part of 
speech tagging and a Conditional Random 
Field (CRF) for Chinese word segmentation. 
Our experiments show that across tasks, three 
of the estimators ? ME estimation with L1 or 
L2 regularization, and AP ? are in a near sta-
tistical tie for first place. 
1 Introduction 
Parameter estimation is fundamental to many sta-
tistical approaches to NLP. Because of the 
high-dimensional nature of natural language, it is 
often easy to generate an extremely large number of 
features.  The challenge of parameter estimation is 
to find a combination of the typically noisy, re-
dundant features that accurately predicts the target 
output variable and avoids overfitting. Intuitively, 
this can be achieved either by selecting a small 
number of highly-effective features and ignoring 
the others, or by averaging over a large number of 
weakly informative features.  The first intuition 
motivates feature selection methods such as 
Boosting and BLasso (e.g., Collins 2000; Zhao and 
Yu, 2004), which usually work best when many 
features are completely irrelevant. L1 or Lasso 
regularization of linear models, introduced by 
Tibshirani (1996), embeds feature selection into 
regularization so that both an assessment of the 
reliability of a feature and the decision about 
whether to remove it are done in the same frame-
work, and has generated a large amount of interest 
in the NLP community recently (e.g., Goodman 
2003; Riezler and Vasserman 2004).  If on the other 
hand most features are noisy but at least weakly 
correlated with the target, it may be reasonable to 
attempt to reduce noise by averaging over all of the 
features.  ME estimators with L2 regularization, 
which have been widely used in NLP tasks (e.g., 
Chen and Rosenfeld 2000; Charniak and Johnson 
2005; Johnson et al 1999), tend to produce models 
that have this property.  In addition, the perceptron 
algorithm and its variants, e.g., the voted or aver-
aged perceptron, is becoming increasingly popular 
due to their competitive performance, simplicity in 
implementation and low computational cost in 
training (e.g., Collins 2002). 
While recent studies claim advantages for L1 
regularization, this study is the first of which we are 
aware to systematically compare it to a range of 
estimators on a diverse set of NLP tasks.  Gao et al 
(2006) showed that BLasso, due to its explicit use of 
L1 regularization, outperformed Boosting in the LM 
adaptation task.  Ng (2004) showed that for logistic 
regression, L1 regularization outperforms L2 regu-
larization on artificial datasets which contain many 
completely irrelevant features.  Goodman (2003) 
showed that in two out of three tasks, an ME esti-
mator with a one-sided Laplacian prior (i.e., L1 
regularization with the constraint that all feature 
weights are positive) outperformed a comparable 
estimator using a Gaussian prior (i.e., L2 regulari-
zation).  Riezler and Vasserman (2004) showed that 
an L1-regularized ME estimator outperformed an 
L2-regularized estimator for ranking the parses of a 
stochastic unification-based grammar. 
824
While these individual estimators are well de-
scribed in the literature, little is known about the 
relative performance of these methods because the 
published results are generally not directly compa-
rable.  For example, in the parse re-ranking task, 
one cannot tell whether the L2- regularized ME 
approach used by Charniak and Johnson (2005) 
significantly outperforms the Boosting method by 
Collins (2000) because different feature sets and 
n-best parses were used in the evaluations of these 
methods.  
This paper conducts a much-needed comparative 
study of these five parameter estimation algorithms 
on four NLP tasks: ME estimation with L1 and L2 
regularization, the Averaged Perceptron (AP), 
Boosting, and BLasso, a version of Boosting with 
Lasso (L1) regularization.  We first investigate all of 
our estimators on two re-ranking tasks: a parse 
selection task and a language model adaptation task. 
Then we apply the best of these estimators to two 
additional tasks involving conditional sequence 
models: a CMM for POS tagging and a CRF for 
Chinese word segmentation.  Our results show that 
ME estimation with L2 regularization achieves the 
best performing estimators in all of the tasks, and 
AP achieves almost as well and requires much less 
training time. L1 (Lasso) regularization also per-
forms well and leads to sparser models. 
2 Estimators 
All the four NLP tasks studied in this paper are 
based on linear models (Collins 2000) which re-
quire learning a mapping from inputs ? ? ? to 
outputs ? ? ?.  We are given: 
? Training samples (?? ,??) for ? = 1??, 
? A procedure ??? to generate a set of candi-
dates ???(?) for an input x,  
? A feature mapping ?:? ? ? ? ??  to map 
each (?,?) to a vector of feature values, and 
? A parameter vector ? ? ?? , which assigns a 
real-valued weight to each feature. 
For all models except the CMM sequence model for 
POS tagging, the components ???, ? and ? di-
rectly define a mapping from an input ? to an output 
?(?) as follows: 
? ? = arg max????? ? ? ?,? ? ?. (1) 
In the CMM sequence classifier, locally normalized 
linear models to predict the tag of each word token 
are chained together to arrive at a probability esti-
mate for the entire tag sequence, resulting in a 
slightly different decision rule. 
Linear models, though simple, can capture very 
complex dependencies because the features can be 
arbitrary functions of the input/output pair.  For 
example, we can define a feature to be the log con-
ditional probability of the output as estimated by 
some other model, which may in turn depend on 
arbitrarily complex interactions of ?basic? features.  
In practice, with an appropriate feature set, linear 
models achieve very good empirical results on 
various NLP tasks.  The focus of this paper however 
is not on feature definition (which requires domain 
knowledge and varies from task to task), but on 
parameter estimation (which is generic across 
tasks).  We assume we are given fixed feature 
templates from which a large number of features are 
generated.  The task of the estimator is to use the 
training samples to choose a parameter vector ?, 
such that the mapping ?(?) is capable of correctly 
classifying unseen examples. We will describe the 
five estimators in our study individually. 
2.1 ME estimation with L2 regularization 
Like many linear models, the ME estimator chooses 
? to minimize the sum of the empirical loss on the 
training set and a regularization term: 
? = arg min?  ? ? + ? ?   . (2) 
In this case, the loss term L(w) is the negative con-
ditional log-likelihood of the training data, 
 ? ? = ? log? ??  ??)
?
?=1 ,  where 
? ? ?) =
exp ? ?,? ? ? 
 exp(? ?,? ? ? ?)? ????? ? 
 
and the regularizer term ? ? = ? ??
2
?  is the 
weighted squared L2 norm of the parameters. Here, 
? is a parameter that controls the amount of regu-
larization, optimized on held-out data.  
This is one of the most popular estimators,  
largely due to its appealing computational proper-
ties: both ? ?  and ?(?) are convex and differen-
tiable, so gradient-based numerical algorithms can 
be used to find the global minimum efficiently.  
In our experiments, we used the limited memory 
quasi-Newton algorithm (or L-BFGS, Nocedal and 
Wright 1999) to find the optimal ? because this 
method has been shown to be substantially faster 
than other methods such as Generalized Iterative 
Scaling (Malouf 2002).  
825
Because for some sentences there are multiple 
best parses (i.e., parses with the same F-Score), we 
used the variant of ME estimator described in 
Riezler et al (2002), where ? ?  is defined as the 
likelihood of the best parses ? ? ?(?) relative to 
the n-best parser output ??? ? ,  (i.e., ? ? ?
???(?)): ? ? = ? log ?(?? |??)????(??)
?
?=1 . 
We applied this variant in our experiments of 
parse re-ranking and LM adaptation, and found that 
on both tasks it leads to a significant improvement 
in performance for the L2-regularied ME estimator 
but not for the L1-regularied ME estimator. 
2.2 ME estimation with L1 regularization 
This estimator also minimizes the negative condi-
tional log-likelihood, but uses an L1 (or Lasso) 
penalty. That is, ?(?) in Equation (2) is defined 
according to ? ? = ?  ??  ? . L1 regularization 
typically leads to sparse solutions in which many 
feature weights are exactly zero, so it is a natural 
candidate when feature selection is desirable. By 
contrast, L2 regularization produces solutions in 
which most weights are small but non-zero. 
Optimizing the L1-regularized objective function 
is challenging because its gradient is discontinuous 
whenever some parameter equals zero. Kazama and 
Tsujii (2003) described an estimation method that 
constructs an equivalent constrained optimization 
problem with twice the number of variables.  
However, we found that this method is impracti-
cally slow for large-scale NLP tasks. In this work 
we use the orthant-wise limited-memory qua-
si-Newton algorithm (OWL-QN), which is a mod-
ification of L-BFGS that allows it to effectively 
handle the discontinuity of the gradient (Andrew 
and Gao 2007). We provide here a high-level de-
scription of the algorithm. 
A quasi-Newton method such as L-BFGS uses 
first order information at each iterate to build an 
approximation to the Hessian matrix, ?, thus mod-
eling the local curvature of the function. At each 
step, a search direction is chosen by minimizing a 
quadratic approximation to the function: 
? ? =
1
2
 ? ? ?0 
?? ? ? ?0 + ?0
? (? ? ?0) 
where ?0 is the current iterate, and ?0 is the func-
tion gradient at ?0 .  If ? is positive definite, the 
minimizing value of ? can be computed analytically 
according to: ?? = ?0 ??
?1?0. 
L-BFGS maintains vectors of the change in gradient 
?? ? ???1 from the most recent iterations, and uses 
them to construct an estimate of the inverse Hessian 
???. Furthermore, it does so in such a way that 
??1?0 can be computed without expanding out the 
full matrix, which is typically unmanageably large. 
The computation requires a number of operations 
linear in the number of variables. 
OWL-QN is based on the observation that when 
restricted to a single orthant, the L1 regularizer is 
differentiable, and is in fact a linear function of ?.  
Thus, so long as each coordinate of any two con-
secutive search points does not pass through zero, 
?(?) does not contribute at all to the curvature of 
the function on the segment joining them.  There-
fore, we can use L-BFGS to approximate the Hes-
sian of ? ?  alone, and use it to build an approxi-
mation to the full regularized objective that is valid 
on a given orthant. To ensure that the next point is in 
the valid region, we project each point during the 
line search back onto the chosen orthant.1 At each 
iteration, we choose the orthant containing the 
current point and into which the direction giving the 
greatest local rate of function decrease points. 
This algorithm, although only a simple modifi-
cation of L-BFGS, works quite well in practice. It 
typically reaches convergence in even fewer itera-
tions than standard L-BFGS takes on the analogous 
L2-regularized objective (which translates to less 
training time, since the time per iteration is only 
negligibly higher, and total time is dominated by 
function evaluations). We describe OWL-QN more 
fully in (Andrew and Gao 2007). We also show that 
it is significantly faster than Kazama and Tsujii?s 
algorithm for L1 regularization and prove that it is 
guaranteed converge to a parameter vector that 
globally optimizes the L1-regularized objective. 
2.3 Boosting 
The Boosting algorithm we used is based on Collins 
(2000).  It optimizes the pairwise exponential loss 
(ExpLoss) function (rather than the logarithmic loss 
optimized by ME).  Given a training sample 
(?? ,??), for each possible output ?? ? ???(??), we 
                                                     
1 This projection just entails zeroing-out any coordinates 
that change sign. Note that it is possible for a variable to 
change sign in two iterations, by moving from a negative 
value to zero, and on a the next iteration moving from 
zero to a positive value. 
826
define the margin of the pair (?? ,?? ) with respect to 
? as ? ?? ,??  = ? ?? ,?? ? ? ?  ? ?? ,??  ? ?. 
Then ExpLoss is defined as 
ExpLoss ? =  exp  ?M yi , yj  
?????? ?? ?
 (3) 
Figure 1 summarizes the Boosting algorithm we 
used. It is an incremental feature selection proce-
dure. After initialization, Steps 2 and 3 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated as follows.  
First, we define Upd(?,?, ?)  as an updated 
model, with the same parameter values as ? with 
the exception of ?? , which is incremented by ?: 
Upd ?, ?, ? = (?1 ,? ,?? + ?,? ,??)  
Then, Steps 2 and 3 in Figure 1 can be rewritten as 
Equations (4) and (5), respectively. 
 ??, ?? = arg min
? ,?
ExpLoss(Upd ?, ?, ? ) (4) 
?? = Upd(???1, ??, ??) (5) 
Because Boosting can overfit we update the weight 
of ??? by a small fixed step size ?, as in Equation (6), 
following the FSLR algorithm (Hastie et al 2001).  
?? = Upd(???1, ??, ? ? sign ?? ) (6) 
By taking such small steps, Boosting imposes a 
kind of implicit regularization, and can closely 
approximate the effect of L1 regularization in a local 
sense (Hastie et al 2001).  Empirically, smaller 
values of ? lead to smaller numbers of test errors. 
2.4 Boosted Lasso 
The Boosted Lasso (BLasso) algorithm was origi-
nally proposed in Zhao and Yu (2004), and was 
adapted for language modeling by Gao et al (2006). 
BLasso can be viewed as a version of Boosting with 
L1 regularization. It optimizes an L1-regularized 
ExpLoss function: 
LassoLoss ? = ExpLoss(?) + ?(?) (7) 
where ? ? = ?  ??  ?  . 
BLasso also uses an incremental feature selec-
tion procedure to learn parameter vector ?, just as 
Boosting does.  Due to the explicit use of the regu-
larization term ?(?), however, there are two major 
differences from Boosting.  
At each iteration, BLasso takes either a forward 
step or a backward step.  Similar to Boosting, at 
each forward step, a feature is selected and its 
weight is updated according to Eq. (8) and (9). 
 ??, ?? = ??? ???
? ,?=??
ExpLoss(Upd ?, ?, ? ) (8) 
?? = Upd(???1, ??, ? ? sign ?? ) (9) 
There is a small but important difference between 
Equations (8) and (4). In Boosting, as shown in 
Equation (4), a feature is selected by its impact on 
reducing the loss with its optimal update ?? . By 
contrast, in BLasso, as shown in Equation (8), 
rather than optimizing over ? for each feature, the 
loss is calculated with an update of either +? or ??, 
i.e., grid search is used for feature weight estima-
tion.  We found in our experiments that this mod-
ification brings a consistent improvement. 
The backward step is unique to BLasso.  At each 
iteration, a feature is selected and the absolute value 
of its weight is reduced by ? if and only if it leads to 
a decrease of the LassoLoss, as shown in Equations 
(10) and (11), where ?  is a tolerance parameter. 
?? = arg min
? :???0
ExpLoss(Upd(?, ?,??sign ?? ) (10) 
?? = Upd(???1 , ??,sign(???) ? ?)  (11) 
if LassoLoss ???1,???1 ? LassoLoss ?? ,?? > ? 
Figure 2 summarizes the BLasso algorithm we 
used. After initialization, Steps 4 and 5 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated either backward or forward by a 
fixed amount ?.  Notice that the value of ? is adap-
tively chosen according to the reduction of ExpLoss 
during training.  The algorithm starts with a large 
initial ?, and then at each forward step the value of 
? decreases until ExpLoss stops decreasing.  This is 
intuitively desirable: it is expected that most highly 
effective features are selected in early stages of 
training, so the reduction of ExpLoss at each step in 
early stages are more substantial than in later stages.  
These early steps coincide with the Boosting steps 
most of the time.  In other words, the effect of 
backward steps is more visible at later stages.  It can 
be proved that for a finite number of features and 
? =0, the BLasso algorithm shown in Figure 2 
converges to the Lasso solution when ? ? 0. See 
Gao et al (2006) for implementation details, and 
Zhao and Yu (2004) for a theoretical justification 
for BLasso. 
1 Set w0 = argminw0ExpLoss(w); and wd = 0 for d=1?D 
2 Select a feature fk* which has largest estimated 
impact on reducing ExpLoss of Equation (3) 
3 Update ?k* ?  ?k* + ?*, and return to Step 2 
Figure 1: The boosting algorithm 
827
2.5 Averaged Perceptron 
The perceptron algorithm can be viewed as a form 
of incremental training procedure (e.g., using sto-
chastic approximation) that optimizes a minimum 
square error (MSE) loss function (Mitchell, 1997).  
As shown in Figure 3, it starts with an initial pa-
rameter setting and updates it for each training 
example. In our experiments, we used the Averaged 
Perceptron algorithm of Freund and Schapire 
(1999), a variation that has been shown to be more 
effective than the standard algorithm (Collins 
2002).  Let ??,?  be the parameter vector after the ?th 
training sample has been processed in pass ? over 
the training data. The average parameters are de-
fined as?  =
?
??
  ??,???  where T is the number of 
epochs, and N is the number of training samples. 
3 Evaluations 
From the four tasks we consider, parsing and lan-
guage model adaptation are both examples of 
re-ranking.  In these tasks, we assume that we have 
been given a list of candidates ???(?) for each 
training or test sample  ?,? , generated using a 
baseline model.  Then, a linear model of the form in 
Equation (1) is used to discriminatively re-rank the 
candidate list using additional features which may 
or may not be included in the baseline model.  Since 
the mapping from ? to ? by the linear model may 
make use of arbitrary global features of the output 
and is performed ?all at once?, we call such a linear 
model a global model.  
In the other two tasks (i.e., Chinese word seg-
mentation and POS tagging), there is no explicit 
enumeration of ???(?).  The mapping from ? to ? 
is determined by a sequence model which aggre-
gates the decisions of local linear models via a 
dynamic program.  In the CMM, the local linear 
models are trained independently, while in the CRF 
model, the local models are trained jointly.  We call 
these two linear models local models because they 
dynamically combine the output of models that use 
only local features. 
While it is straightforward to apply the five es-
timators to global models in the re-ranking 
framework, the application of some estimators to 
the local models is problematic. Boosting and 
BLasso are too computationally expensive to be 
applied to CRF training and we compared the other 
three better performing estimation methods for this 
model. The CMM is a probabilistic sequence model 
and the log-loss used by ME estimation is most 
natural for it; thus we limit the comparison to the 
two kinds of ME models for CMMs. Note that our 
goal is not to compare locally trained models to 
globally trained ones; for a study which focuses on 
this issue, see (Punyakanok et al 2005). 
In each task we compared the performance of 
different estimators using task-specific measures. 
We used the Wilcoxon signed rank test to test the 
statistical significance of the difference among the 
competing estimators. We also report other results 
such as number of non-zero features after estima-
tion, number of training iterations, and computation 
time (in minutes of elapsed time on an XEONTM MP 
3.6GHz machine). 
3.1 Parse re-ranking 
We follow the experimental paradigm of parse 
re-ranking outlined in Charniak and Johnson 
(2005), and fed the features extracted by their pro-
gram to the five rerankers we developed.  Each uses 
a linear model trained using one of the five esti-
mators. These rerankers attempt to select the best 
parse ?  for a sentence ?  from the 50-best list of 
possible parses ??? ?  for the sentence. The li-
near model combines the log probability calculated 
by the Charniak (2000) parser as a feature with 
1,219,272 additional features.  We trained the fea-
1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 
for d=1?D. 
2 Take a forward step according to Eq. (8) and (9), and 
the updated model is denoted by w1 
3 Initialize ? = (ExpLoss(w0)-ExpLoss(w1))/? 
4 Take a backward step if and only if it leads to a de-
crease of LassoLoss according to Eq. (10) and (11), 
where ?  = 0; otherwise 
5 Take a forward step according to Eq. (8) and (9); 
update ? = min(?, (ExpLoss(wt-1)-ExpLoss(wt))/? ); 
and return to Step 4. 
Figure 2: The BLasso algorithm 
1 Set w0 = 1 and wd = 0 for d=1?D 
2 For t = 1?T (T = the total number of iterations) 
3    For each training sample (xi, yi), i = 1?N 
4 
?? = arg max
????? ?_? 
? ?? , ? ? ? 
Choose the best candidate zi from GEN(xi) using 
the current model w, 
5       w = w +  ?(?(xi, yi) ? ?(xi, zi)), where ? is the size of 
learning step, optimized on held-out data. 
Figure 3: The perceptron algorithm 
 
828
ture weights w on Sections 2-19 of the Penn Tree-
bank, adjusted the regularizer constant ? to max-
imize the F-Score on Sections 20-21 of the Tree-
bank, and evaluated the rerankers on Section 22.  
The results are presented in Tables 12 and 2, where 
Baseline results were obtained using the parser by 
Charniak (2000).  
The ME estimation with L2 regularization out-
performs all of the other estimators significantly 
except for the AP, which performs almost as well 
and requires an order of magnitude less time in 
training.  Boosting and BLasso are feature selection 
methods in nature, so they achieve the sparsest 
models, but at the cost of slightly lower perfor-
mance and much longer training time. The 
L1-regularized ME estimator also produces a rela-
tively sparse solution whereas the Averaged Per-
ceptron and the L2-regularized ME estimator assign 
almost all features a non-zero weight.  
3.2 Language model adaptation 
Our experiments with LM adaptation are based on 
the work described in Gao et al (2006). The va-
riously trained language models were evaluated 
according to their impact on Japanese text input 
accuracy, where input phonetic symbols ?  are 
mapped into a word string ?. Performance of the 
application is measured in terms of character error 
                                                     
2
 The result of ME/L2 is better than that reported in 
Andrew and Gao (2007) due to the use of the variant of 
L2-regularized ME estimator, as described in Section 2.1. 
 CER # features time (min) #train iter 
Baseline 10.24%    
MAP 7.98%    
ME/L2 6.99% 295,337 27 665 
ME/L1 7.01% 53,342 25 864 
AP 7.23% 167,591 6 56 
Boost 7.54% 32,994 175 71,000 
BLasso 7.20% 33,126 238 250,000 
Table 3. Performance summary of estimators 
(lower is better) on language model adaptation 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  ~ >> >> >> 
ME/L1 ~  >> >> >> 
AP << <<  >> ~ 
Boost << << <<  << 
BLasso << << ~ >>  
Table 4. Statistical significance test results. 
rate (CER), which is the number of characters 
wrongly converted from ? divided by the number of 
characters in the correct transcript. 
Again we evaluated five linear rerankers, one for 
each estimator. These rerankers attempt to select the 
best conversions ? for an input phonetic string ? 
from a 100-best list ???(?)of possible conver-
sions proposed by a baseline system. The linear 
model combines the log probability under a trigram 
language model as base feature and additional 
865,190 word uni/bi-gram features.  These 
uni/bi-gram features were already included in the 
trigram model which was trained on a background 
domain corpus (Nikkei Newspaper). But in the 
linear model their feature weights were trained 
discriminatively on an adaptation domain corpus 
(Encarta Encyclopedia). Thus, this forms a cross 
domain adaptation paradigm.  This also implies that 
the portion of redundant features in this task could 
be much larger than that in the parse re-ranking 
task, especially because the background domain is 
reasonably similar to the adaptation domain.  
We divided the Encarta corpus into three sets 
that do not overlap.  A 72K-sentences set was used 
as training data, a 5K-sentence set as development 
data, and another 5K-sentence set as testing data. 
The results are presented in Tables 3 and 4, where 
Baseline is the word-based trigram model trained 
on background domain corpus, and MAP (maxi-
mum a posteriori) is a traditional model adaptation 
method, where the parameters of the background 
model are adjusted so as to maximize the likelihood 
of the adaptation data.  
 F-Score # features time (min) # train iter 
Baseline 0.8986     
ME/L2 0.9176 1,211,026 62     129  
ME/L1 0.9165 19,121 37 174  
AP 0.9164 939,248 2 8  
Boosting 0.9131 6,714 495 92,600  
BLasso 0.9133 8,085 239 56,500  
Table 1: Performance summary of estimators on 
parsing re-ranking (ME/L2: ME with L2 regulari-
zation; ME/L1:  ME with L1 regularization) 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  >> ~ >> >> 
ME/L1 <<  ~ > ~ 
AP ~ ~  >> > 
Boost << < <<  ~ 
Blasso << ~ < ~  
Table 2: Statistical significance test results (?>>? 
or ?<<? means P-value < 0.01; > or < means 0.01 < 
P-value ? 0.05; ?~? means P-value > 0.05)  
829
The results are more or less similar to those in 
the parsing task with one visible difference: L1 
regularization achieved relatively better perfor-
mance in this task.  For example, while in the 
parsing task ME with L2 regularization significantly 
outperforms ME with L1 regularization, their per-
formance difference is not significant in this task. 
While in the parsing task the performance differ-
ence between BLasso and Boosting is not signifi-
cant, BLasso outperforms Boosting significantly in 
this task.  Considering that a much higher propor-
tion of the features are redundant in this task than 
the parsing task, the results seem to corroborate the 
observation that L1 regularization is robust to the 
presence of many redundant features. 
3.3 Chinese word segmentation 
Our third task is Chinese word segmentation 
(CWS). The goal of CWS is to determine the 
boundaries between words in a section of Chinese 
text.  The model we used is the hybrid Mar-
kov/semi- Markov CRF described by Andrew 
(2006), which was shown to have state-of-the-art 
accuracy. We tested models trained with the various 
estimation methods on the Microsoft Research Asia 
corpus from the Second International Chinese Word 
Segmentation, and we used the same train/test split 
used in the competition.  The model and experi-
mental setup is identical with that of Andrew (2006) 
except for two differences.  First, we extracted 
features from both positive and negative training 
examples, while Andrew (2006) uses only features 
that occur in some positive training example. 
Second, we used the last 4K sentences of the 
training data to select the weight of the regularizers 
and to determine when to stop perceptron training. 
We compared three of the best performing es-
timation procedures on this task: ME with L2 regu-
larization, ME with L1 regularization, and the Av-
eraged Perceptron.  In this case, ME refers to mi-
nimizing the negative log-probability of the correct 
segmentation, which is globally normalized, while 
the perceptron is trained using at each iteration the 
exact maximum-scoring segmentation with the 
current weights. We observed the same pattern as in 
the other tasks: the three algorithms have nearly 
identical performance, while L1 uses only 6% of the 
features, and the Averaged Perceptron requires 
significantly fewer training iterations.  In this case, 
L1 was also several times faster than L2. The results 
are summarized in Table 5.3 
We note that all three algorithms performed 
slightly better than the model used by Andrew 
(2006), which also used L2 regularization (96.84 
F1).  We believe the difference is due to the use of 
features derived from negative training examples. 
3.4 POS tagging 
Finally we studied the impact of the regularization 
methods on a Maximum Entropy conditional 
Markov Model (MEMM, McCallum et al 2000) for 
POS tagging. MEMMs decompose the conditional 
probability of a tag sequence given a word sequence 
as follows: 
? ?1 ? ??  ?1 ??? = ?(??|???1 ????? ,?1 ???)
?
?=1
 
where the probability distributions for each tag 
given its context are ME models.  Following pre-
vious work (Ratnaparkhi, 1996), we assume that the 
tag of a word is independent of the tags of all pre-
ceding words given the tags of the previous two 
words (i.e., ?=2 in the equation above). The local 
models at each position include features of the 
current word, the previous word, the next word, and 
features of the previous two tags.  In addition to 
lexical identity of the words, we used features of 
word suffixes, capitalization, and number/special 
character signatures of the words. 
We used the standard splits of the Penn Treebank 
from the tagging literature (Toutanova et al 2003) 
for training, development and test sets.  The training 
set comprises Sections 0-18, the development set ? 
Sections 19-21, and the test set ? Sections 22-24.  
We compared training the ME models using L1 and 
L2 regularization.  For each of the two types of 
regularization we selected the best value of the 
regularization constant using grid search to optim-
ize the accuracy on the development set.  We report 
final accuracy measures on the test set in Table 6.  
The results on this task confirm the trends we 
have seen so far.  There is almost no difference in 
                                                     
3 Only the L2 vs. AP comparison is significant at a 0.05 
level according to the Wilcoxon signed rank test. 
 Test F1 # features # train iter 
ME/L2 0.9719 8,084,086 713 
ME/L1 0.9713 317,146 201 
AP 0.9703 1,965,719 162 
Table 5. Performance summary of estimators on 
CWS 
 
830
accuracy of the two kinds of regularizations, and 
indeed the differences were not statistically signif-
icant.  Estimation with L1 regularization required 
considerably less time than estimation with L2, and 
resulted in a model which is more than ten times 
smaller.  
4 Conclusions 
We compared five of the most competitive para-
meter estimation methods on four NLP tasks em-
ploying a variety of models, and the results were 
remarkably consistent across tasks.  Three of the 
methods ? ME estimation with L2 regularization, 
ME estimation with L1 regularization, and the Av-
eraged Perceptron ? were nearly indistinguishable 
in terms of test set accuracy, with ME estimation 
with L2 regularization perhaps enjoying a slight 
lead.  Meanwhile, ME estimation with L1 regulari-
zation achieves the same level of performance while 
at the same time producing sparse models, and the 
Averaged Perceptron provides an excellent com-
promise of high performance and fast training. 
These results suggest that when deciding which 
type of parameter estimation to use on these or 
similar NLP tasks, one may choose any of these 
three popular methods and expect to achieve com-
parable performance.  The choice of which to im-
plement should come down to other considerations: 
if model sparsity is desired, choose ME estimation 
with L1 regularization (or feature selection methods 
such as BLasso); if quick implementation and 
training is necessary, use the Averaged Perceptron; 
and ME estimation with L2 regularization may be 
used if it is important to achieve the highest ob-
tainable level of performance. 
References 
Andrew, G. 2006. A hybrid Markov/semi-Markov condi-
tional random field for sequence segmentation. In EMNLP, 
465-472. 
Andrew, G. and Gao, J. 2007. Scalable training of 
L1-regularized log-linear models. In ICML. 
Charniak, E. 2000. A maximum-entropy-inspired parser. In 
NAACL, 132-139. 
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best 
parsing and MaxEnt discriminative re-ranking. In ACL. 
173-180. 
Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing 
techniques for ME models. IEEE Trans. On Speech and Audio 
Processing, 8(2): 37-50. 
Collins, M. 2000. Discriminative re-ranking for natural 
language parsing. In ICML, 175-182. 
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with per-
ceptron algorithms. In EMNLP, 1-8. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. In 
ICML?98.  
Freund, Y. and Schapire, R. E. 1999. Large margin classifica-
tion using the perceptron algorithm. In Machine Learning, 
37(3): 277-296. 
Hastie, T., R. Tibshirani and J. Friedman. 2001. The elements of 
statistical learning. Springer-Verlag, New York. 
Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso 
methods for language modeling. In ACL. 
Goodman, J. 2004. Exponential priors for maximum entropy 
models. In NAACL. 
Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S. 
1999. Estimators for stochastic ?Unification-based? 
grammars. In ACL. 
Kazama, J. and Tsujii, J. 2003. Evaluation and extension of 
maximum entropy models with inequality constraints. In 
EMNLP. 
Malouf, R. 2002. A comparison of algorithms for maximum 
entropy parameter estimation. In HLT. 
McCallum A, D. Freitag and F. Pereira. 2000. Maximum 
entropy markov models for information extraction and 
segmentation. In ICML. 
Mitchell, T. M. 1997. Machine learning. The McGraw-Hill 
Companies, Inc. 
Ng, A. Y. 2004. Feature selection, L1 vs. L2 regularization, 
and rotational invariance. In ICML. 
Nocedal, J., and Wright, S. J. 1999. Numerical Optimization. 
Springer, New York. 
Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005. 
Learning and inference over constrained output. In IJCAI. 
Ratnaparkhi, A. 1996. A maximum entropy part-of-speech 
tagger. In EMNLP. 
Riezler, S., and Vasserman, A. 2004. Incremental feature 
selection and L1 regularization for relax maximum entro-
py modeling. In EMNLP.  
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J., 
and Johnson, M. 2002. Parsing the wall street journal using 
a lexical-functional grammar and discriminative estima-
tion techniques. In ACL. 271-278.  
Tibshirani, R. 1996. Regression shrinkage and selection via 
the lasso. J. R. Statist. Soc. B, 58(1): 267-288. 
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. 
2003. Feature-rich Part-of-Speech tagging with a cyclic 
dependency network. In HLT-NAACL, 252-259. 
Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, Statistics 
Department, U. C. Berkeley. 
 Accuracy (%) # features # train iter 
MEMM/L2 96.39 926,350 467 
MEMM/L1 96.41 84,070 85 
Table 6. Performance summary of estimators on 
POS tagging 
831
Proceedings of ACL-08: HLT, pages 514?522,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Applying Morphology Generation Models to Machine Translation
Kristina Toutanova
Microsoft Research
Redmond, WA, USA
kristout@microsoft.com
Hisami Suzuki
Microsoft Research
Redmond, WA, USA
hisamis@microsoft.com
Achim Ruopp
Butler Hill Group
Redmond, WA, USA
v-acruop@microsoft.com
Abstract
We improve the quality of statistical machine
translation (SMT) by applying models that
predict word forms from their stems using
extensive morphological and syntactic infor-
mation from both the source and target lan-
guages. Our inflection generation models are
trained independently of the SMT system. We
investigate different ways of combining the in-
flection prediction component with the SMT
system by training the base MT system on
fully inflected forms or on word stems. We
applied our inflection generation models in
translating English into two morphologically
complex languages, Russian and Arabic, and
show that our model improves the quality of
SMT over both phrasal and syntax-based SMT
systems according to BLEU and human judge-
ments.
1 Introduction
One of the outstanding problems for further improv-
ing machine translation (MT) systems is the diffi-
culty of dividing the MT problem into sub-problems
and tackling each sub-problem in isolation to im-
prove the overall quality of MT. Evidence for this
difficulty is the fact that there has been very little
work investigating the use of such independent sub-
components, though we started to see some success-
ful cases in the literature, for example in word align-
ment (Fraser and Marcu, 2007), target language cap-
italization (Wang et al, 2006) and case marker gen-
eration (Toutanova and Suzuki, 2007).
This paper describes a successful attempt to in-
tegrate a subcomponent for generating word inflec-
tions into a statistical machine translation (SMT)
system. Our research is built on previous work in
the area of using morpho-syntactic information for
improving SMT. Work in this area is motivated by
two advantages offered by morphological analysis:
(1) it provides linguistically motivated clustering of
words and makes the data less sparse; (2) it cap-
tures morphological constraints applicable on the
target side, such as agreement phenomena. This sec-
ond problem is very difficult to address with word-
based translation systems, when the relevant mor-
phological information in the target language is ei-
ther non-existent or implicitly encoded in the source
language. These two aspects of morphological pro-
cessing have often been addressed separately: for
example, morphological pre-processing of the input
data is a common method of addressing the first as-
pect, e.g. (Goldwater and McClosky, 2005), while
the application of a target language model has al-
most solely been responsible for addressing the sec-
ond aspect. Minkov et al (2007) introduced a way
to address these problems by using a rich feature-
based model, but did not apply the model to MT.
In this paper, we integrate a model that predicts
target word inflection in the translations of English
into two morphologically complex languages (Rus-
sian and Arabic) and show improvements in the MT
output. We study several alternative methods for in-
tegration and show that it is best to propagate un-
certainty among the different components as shown
by other research, e.g. (Finkel et al, 2006), and in
some cases, to factor the translation problem so that
the baseline MT system can take advantage of the
reduction in sparsity by being able to work on word
stems. We also demonstrate that our independently
trained models are portable, showing that they can
improve both syntactic and phrasal SMT systems.
514
2 Related work
There has been active research on incorporating
morphological knowledge in SMT. Several ap-
proaches use pre-processing schemes, including
segmentation of clitics (Lee, 2004; Habash and Sa-
dat, 2006), compound splitting (Nie?en and Ney,
2004) and stemming (Goldwater and McClosky,
2005). Of these, the segmentation approach is dif-
ficult to apply when the target language is morpho-
logically rich as the segmented morphemes must be
put together in the output (El-Kahlout and Oflazer,
2006); and in fact, most work using pre-processing
focused on translation into English. In recent
work, Koehn and Hoang (2007) proposed a general
framework for including morphological features in
a phrase-based SMT system by factoring the repre-
sentation of words into a vector of morphological
features and allowing a phrase-based MT system to
work on any of the factored representations, which
is implemented in the Moses system. Though our
motivation is similar to that of Koehn and Hoang
(2007), we chose to build an independent compo-
nent for inflection prediction in isolation rather than
folding morphological information into the main
translation model. While this may lead to search er-
rors due to the fact that the models are not integrated
as tightly as possible, it offers some important ad-
vantages, due to the very decoupling of the compo-
nents. First, our approach is not affected by restric-
tions on the allowable context size or a phrasal seg-
mentation that are imposed by current MT decoders.
This also makes the model portable and applicable
to different types of MT systems. Second, we avoid
the problem of the combinatorial expansion in the
search space which currently arises in the factored
approach of Moses.
Our inflection prediction model is based on
(Minkov et al, 2007), who build models to predict
the inflected forms of words in Russian and Arabic,
but do not apply their work to MT. In contrast, we
focus on methods of integration of an inflection pre-
diction model with an MT system, and on evaluation
of the model?s impact on translation. Other work
closely related to ours is (Toutanova and Suzuki,
2007), which uses an independently trained case
marker prediction model in an English-Japanese
translation system, but it focuses on the problem of
generating a small set of closed class words rather
than generating inflected forms for each word in
translation, and proposes different methods of inte-
gration of the components.
3 Inflection prediction models
This section describes the task and our model for in-
flection prediction, following (Minkov et al, 2007).
We define the task of inflection prediction as the
task of choosing the correct inflections of given tar-
get language stems, given a corresponding source
sentence. The stemming and inflection operations
we use are defined by lexicons.
3.1 Lexicon operations
For each target language we use a lexicon L which
determines the following necessary operations:
Stemming: returns the set of possible morpholog-
ical stems Sw = {s1, ..., sl} for the word w accord-
ing to L. 1
Inflection: returns the set of surface word forms
Iw = {i1, ..., im} for the stems Sw according to L.
Morphological analysis: returns the set of possible
morphological analyses Aw = {a1, ..., av} for w. A
morphological analysis a is a vector of categorical
values, where each dimension and its possible values
are defined by L.
For the morphological analysis operation, we
used the same set of morphological features de-
scribed in (Minkov et al, 2007), that is, seven fea-
tures for Russian (POS, Person, Number, Gender,
Tense, Mood and Case) and 12 for Arabic (POS,
Person, Number, Gender, Tense, Mood, Negation,
Determiner, Conjunction, Preposition, Object and
Possessive pronouns). Each word is factored into
a stem (uninflected form) and a subset of these fea-
tures, where features can have either binary (as in
Determiner in Arabic) or multiple values. Some fea-
tures are relevant only for a particular (set of) part-
of-speech (POS) (e.g., Gender is relevant only in
nouns, pronouns, verbs, and adjectives in Russian),
while others combine with practically all categories
(e.g., Conjunction in Arabic). The number of possi-
ble inflected forms per stem is therefore quite large:
as we see in Table 1 of Section 3, there are on av-
erage 14 word forms per stem in Russian and 24 in
1Alternatively, stemming can return a disambiguated stem
analysis; in which case the set Sw consists of one item. The
same is true with the operation of morphological analysis.
515
Arabic for our dataset. This makes the generation of
correct forms a challenging problem in MT.
The Russian lexicon was obtained by intersecting
a general domain lexicon with our training data (Ta-
ble 2), and the Arabic lexicon was obtained by run-
ning the Buckwalter morphological analyser (Buck-
walter, 2004) on the training data. Contextual dis-
ambiguation of morphology was not performed in
either of these languages. In addition to the forms
supposed by our lexicon, we also treated capitaliza-
tion as an inflectional feature in Russian, and defined
all true-case word variants as possible inflections of
its stem(s). Arabic does not use capitalization.
3.2 Task
More formally, our task is as follows: given a source
sentence e, a sequence of stems in the target lan-
guage S1, . . . St, . . . Sn forming a translation of e,
and additional morpho-syntactic annotations A de-
rived from the input, select an inflection yt from its
inflection set It for every stem set St in the target
sentence.
3.3 Models
We built a Maximum Entropy Markov model for in-
flection prediction following (Minkov et al, 2007).
The model decomposes the probability of an inflec-
tion sequence into a product of local probabilities for
the prediction for each word. The local probabilities
are conditioned on the previous k predictions (k is
set to four in Russian and two in Arabic in our ex-
periments). The probability of a predicted inflection
sequence, therefore, is given by:
p(y | x) =
n?
t=1
p(yt | yt?1...yt?k, xt), yt ? It,
where It is the set of inflections corresponding to St,
and xt refers to the context at position t. The con-
text available to the task includes extensive morpho-
logical and syntactic information obtained from the
aligned source and target sentences. Figure 1 shows
an example of an aligned English-Russian sentence
pair: on the source (English) side, POS tags and
word dependency structure are indicated by solid
arcs. The alignments between English and Russian
words are indicated by the dotted lines. The de-
pendency structure on the Russian side, indicated by
solid arcs, is given by a treelet MT system (see Sec-
tion 4.1), projected from the word dependency struc-
NN+sg+nom+neut
the
DET
allocation of resources has completed
NN+sg PREP NN+pl AUXV+sg VERB+pastpart
?????????????
NN+pl+gen+masc
????????
VERB+perf+pass+neut+sg
?????????
raspredelenie resursov zaversheno
Figure 1: Aligned English-Russian sentence pair with
syntactic and morphological annotation.
ture of English and word alignment information.
The features for our inflection prediction model
are binary and pair up predicates on the context
(x?, yt?1...yt?k) and the target label (yt). The fea-
tures at a certain position t can refer to any word
in the source sentence, any word stem in the tar-
get language, or any morpho-syntactic information
in A. This is the source of the power of a model
used as an independent component ? because it does
not need to be integrated in the main search of an
MT decoder, it is not subject to the decoder?s local-
ity constraints, and can thus make use of more global
information.
3.4 Performance on reference translations
Table 1 summarizes the results of applying the in-
flection prediction model on reference translations,
simulating the ideal case where the translations in-
put to our model contain correct stems in correct
order. We stemmed the reference translations, pre-
dicted the inflection for each stem, and measured the
accuracy of prediction, using a set of sentences that
were not part of the training data (1K sentences were
used for Arabic and 5K for Russian).2 Our model
performs significantly better than both the random
and trigram language model baselines, and achieves
an accuracy of over 91%, which suggests that the
model is effective when its input is clean in its stem
choice and order. Next, we apply our model in the
more noisy but realistic scenario of predicting inflec-
tions of MT output sentences.
2The accuracy is based on the words in our lexicon. We
define the stem of an out-of-vocabulary (OOV) word to be it-
self, so in the MT scenario described below, we will not predict
the word forms for an OOV item, and will simply leave it un-
changed.
516
Russian Arabic
Random 16.4 8.7
LM 81.0 69.4
Model 91.6 91.0
Avg | I | 13.9 24.1
Table 1: Results on reference translations (accuracy, %).
4 Machine translation systems and data
We integrated the inflection prediction model with
two types of machine translation systems: systems
that make use of syntax and surface phrase-based
systems.
4.1 Treelet translation system
This is a syntactically-informed MT system, de-
signed following (Quirk et al, 2005). In this ap-
proach, translation is guided by treelet translation
pairs, where a treelet is a connected subgraph of a
syntactic dependency tree. Translations are scored
according to a linear combination of feature func-
tions. The features are similar to the ones used in
phrasal systems, and their weights are trained us-
ing max-BLEU training (Och, 2003). There are
nine feature functions in the treelet system, includ-
ing log-probabilities according to inverted and direct
channel models estimated by relative frequency, lex-
ical weighting channel models following Vogel et
al. (2003), a trigram target language model, two or-
der models, word count, phrase count, and average
phrase size functions.
The treelet translation model is estimated using
a parallel corpus. First, the corpus is word-aligned
using an implementation of lexicalized-HMMs (He,
2007); then the source sentences are parsed into a
dependency structure, and the dependency is pro-
jected onto the target side following the heuristics
described in (Quirk et al, 2005). These aligned sen-
tence pairs form the training data of the inflection
models as well. An example was given in Figure 1.
4.2 Phrasal translation system
This is a re-implementation of the Pharaoh trans-
lation system (Koehn, 2004). It uses the same
lexicalized-HMM model for word alignment as the
treelet system, and uses the standard extraction
heuristics to extract phrase pairs using forward and
backward alignments. In decoding, the system uses
a linear combination of feature functions whose
weights are trained using max-BLEU training. The
features include log-probabilities according to in-
verted and direct channel models estimated by rel-
ative frequency, lexical weighting channel models,
a trigram target language model, distortion, word
count and phrase count.
4.3 Data sets
For our English-Russian and English-Arabic experi-
ments, we used data from a technical (computer) do-
main. For each language pair, we used a set of paral-
lel sentences (train) for training the MT system sub-
models (e.g., phrase tables, language model), a set
of parallel sentences (lambda) for training the com-
bination weights with max-BLEU training, a set of
parallel sentences (dev) for training a small number
of combination parameters for our integration meth-
ods (see Section 5), and a set of parallel sentences
(test) for final evaluation. The details of these sets
are shown in Table 2. The training data for the in-
flection models is always a subset of the training set
(train). All MT systems for a given language pair
used the same datasets.
Dataset sent pairs word tokens (avg/sent)
English-Russian
English Russian
train 1,642K 24,351K (14.8) 22,002K (13.4)
lambda 2K 30K (15.1) 27K (13.7)
dev 1K 14K (13.9) 13K (13.5)
test 4K 61K (15.3) 60K(14.9)
English-Arabic
English Arabic
train 463K 5,223K (11.3) 4,761K (10.3)
lambda 2K 22K (11.1) 20K (10.0)
dev 1K 11K (11.1) 10K (10.0)
test 4K 44K (11.0) 40K (10.1)
Table 2: Data set sizes, rounded up to the nearest 1000.
5 Integration of inflection models with MT
systems
We describe three main methods of integration we
have considered. The methods differ in the extent to
which the factoring of the problem into two subprob-
lems ? predicting stems and predicting inflections
? is reflected in the base MT systems. In the first
method, the MT system is trained to produce fully
inflected target words and the inflection model can
change the inflections. In the other two methods, the
517
MT system is trained to produce sequences of tar-
get language stems S, which are then inflected by
the inflection component. Before we motivate these
methods, we first describe the general framework for
integrating our inflection model into the MT system.
For each of these methods, we assume that the
output of the base MT system can be viewed as a
ranked list of translation hypotheses for each source
sentence e. More specifically, we assume an out-
put {S1,S2,. . . ,Sm} of m-best translations which
are sequences of target language stems. The transla-
tions further have scores {w1,w2,. . . ,wm} assigned
by the base MT system. We also assume that each
translation hypothesis Si together with source sen-
tence e can be annotated with the annotation A, as
illustrated in Figure 1. We discuss how we convert
the output of the base MT systems to this form in the
subsections below.
Given such a list of candidate stem sequences, the
base MT model together with the inflection model
and a language model choose a translation Y? as
follows:
(1) Yi = argmaxY ?i ?Infl(Si)?1logPIM (Y ?i |Si)+
?2logPLM (Y ?i ), i = 1 . . . n
(2) Y ? = argmaxi=1...n ?1logPIM (Yi|Si) +
?2logPLM (Yi) + ?3wi
In these formulas, the dependency on e and A
is omitted for brevity in the expression for the
probability according to the inflection model PIM .
PLM (Y ?i ) is the joint probability of the sequence
of inflected words according to a trigram language
model (LM). The LM used for the integration is the
same LM used in the base MT system that is trained
on fully inflected word forms (the base MT system
trained on stems uses an LM trained on a stem se-
quence). Equation (1) shows that the model first se-
lects the best sequence of inflected forms for each
MT hypothesis Si according to the LM and the in-
flection model. Equation (2) shows that from these
n fully inflected hypotheses, the model then selects
the one which has the best score, combined with
the base MT score wi for Si. We should note that
this method does not represent standard n-best re-
ranking because the input from the base MT system
contains sequences of stems, and the model is gen-
erating fully inflected translations from them. Thus
the chosen translation may not be in the provided n-
best list. This method is more similar to the one used
in (Wang et al, 2006), with the difference that they
use only 1-best input from a base MT system.
The interpolation weights ? in Equations (1) and
(2) as well as the optimal number of translations n
from the base MT system to consider, given a maxi-
mum of m=100 hypotheses, are trained using a sep-
arate dataset. We performed a grid search on the
values of ? and n, to maximize the BLEU score of
the final system on a development set (dev) of 1000
sentences (Table 2).
The three methods of integration differ in the way
the base MT engine is applied. Since we always dis-
card the choices of specific inflected forms for the
target stems by converting candidate translations to
sequences of stems, it is interesting to know whether
we need a base MT system that produces fully in-
flected translations or whether we can do as well
or better by training the base MT systems to pro-
duce sequences of stems. Stemming the target sen-
tences is expected to be helpful for word alignment,
especially when the stemming operation is defined
so that the word alignment becomes more one-to-
one (Goldwater and McClosky, 2005). In addition,
stemming the target sentences reduces the sparsity
in the translation tables and language model, and is
likely to impact positively the performance of an MT
system in terms of its ability to recover correct se-
quences of stems in the target. Also, machine learn-
ing tells us that solving a more complex problem
than we are evaluated on (in our case for the base
MT, predicting stems together with their inflections
instead of just predicting stems) is theoretically un-
justified (Vapnik, 1995).
However, for some language pairs, stemming one
language can make word alignment worse, if it
leads to more violations in the assumptions of cur-
rent word alignment models, rather than making the
source look more like the target. In addition, using a
trigram LM on stems may lead to larger violations of
the Markov independence assumptions, than using a
trigram LM on fully inflected words. Thus, if we ap-
ply the exact same base MT system to use stemmed
forms in alignment and/or translation, it is not a pri-
ori clear whether we would get a better result than if
we apply the system to use fully inflected forms.
518
5.1 Method 1
In this method, the base MT system is trained in
the usual way, from aligned pairs of source sen-
tences and fully inflected target sentences. The in-
flection model is then applied to re-inflect the 1-best
or m-best translations and to select an output trans-
lation. The hypotheses in the m-best output from the
base MT system are stemmed and the scores of the
stemmed hypotheses are assumed to be equal to the
scores of the original ones.3 Thus we obtain input of
the needed form, consisting of m sequences of target
language stems along with scores.
For this and other methods, if we are working
with an m-best list from the treelet system, every
translation hypothesis contains the annotations A
that our model needs, because the system maintains
the alignment, parse trees, etc., as part of its search
space. Thus we do not need to do anything further
to obtain input of the form necessary for application
of the inflection model.
For the phrase-based system, we generated the
annotations needed by first parsing the source sen-
tence e, aligning the source and candidate transla-
tions with the word-alignment model used in train-
ing, and projected the dependency tree to the target
using the algorithm of (Quirk et al, 2005). Note that
it may be better to use the word alignment main-
tained as part of the translation hypotheses during
search, but our solution is more suitable to situations
where these can not be easily obtained.
For all methods, we study two settings for integra-
tion. In the first, we only consider (n=1) hypotheses
from the base MT system. In the second setting, we
allow the model to use up to 100 translations, and
to automatically select the best number to use. As
seen in Table 3, (n=16) translations were chosen for
Russian and as seen in Table 5, (n=2) were chosen
for Arabic for this method.
5.2 Method 2
In this method, the base MT system is trained to pro-
duce sequences of stems in the target language. The
most straightforward way to achieve this is to stem
the training parallel data and to train the MT sys-
tem using this input. This is our Method 3 described
3It may be better to take the max of the scores for a stem
sequence occurring more than once in the list, or take the log-
sum-exp of the scores.
below. We formulated Method 2 as an intermedi-
ate step, to decouple the impact of stemming at the
alignment and translation stages.
In Method 2, word alignment is performed us-
ing fully inflected target language sentences. After
alignment, the target language is stemmed and the
base MT systems? sub-models are trained using this
stemmed input and alignment. In addition to this
word-aligned corpus the MT systems use another
product of word alignment: the IBM model 1 trans-
lation tables. Because the trained translation tables
of IBM model 1 use fully inflected target words, we
generated stemmed versions of the translation tables
by applying the rules of probability.
5.3 Method 3
In this method the base MT system produces se-
quences of target stems. It is trained in the same way
as the baseline MT system, except its input parallel
training data are preprocessed to stem the target sen-
tences. In this method, stemming can impact word
alignment in addition to the translation models.
6 MT performance results
Before delving into the results for each method, we
discuss our evaluation measures. For automatically
measuring performance, we used 4-gram BLEU
against a single reference translation. We also report
oracle BLEU scores which incorporate two kinds of
oracle knowledge. For the methods using n=1 trans-
lation from a base MT system, the oracle BLEU
score is the BLEU score of the stemmed translation
compared to the stemmed reference, which repre-
sents the upper bound achievable by changing only
the inflected forms (but not stems) of the words in a
translation. For models using n > 1 input hypothe-
ses, the oracle also measures the gain from choos-
ing the best possible stem sequence in the provided
(m=100-best) hypothesis list, in addition to choos-
ing the best possible inflected forms for these stems.
For the models in the tables, even if, say, n=16 was
chosen in parameter fitting, the oracle is measured
on the initially provided list of 100-best.
6.1 English-Russian treelet system
Table 3 shows the results of the baseline and the
model using the different methods for the treelet
MT system on English-Russian. The baseline is the
519
Model BLEU Oracle BLEU
Base MT (n=1) 29.24 -
Method 1 (n=1) 30.44 36.59
Method 1 (n=16) 30.61 45.33
Method 2 (n=1) 30.79 37.38
Method 2 (n=16) 31.24 48.48
Method 3 (n=1) 31.42 38.06
Method 3 (n=32) 31.80 49.19
Table 3: Test set performance for English-to-Russian MT
(BLEU) results by model using a treelet MT system.
treelet system described in Section 4.1 and trained
on the data in Table 2.
We can see that Method 1 results in a good im-
provement of 1.2 BLEU points, even when using
only the best (n = 1) translation from the baseline.
The oracle improvement achievable by predicting
inflections is quite substantial: more than 7 BLEU
points. Propagating the uncertainty of the baseline
system by using more input hypotheses consistently
improves performance across the different methods,
with an additional improvement of between .2 and
.4 BLEU points.
From the results of Method 2 we can see that re-
ducing sparsity at translation modeling is advanta-
geous. Both the oracle BLEU of the first hypothe-
sis and the achieved performance of the model im-
proved; the best performance achieved by Method 2
is .63 points higher than the performance of Method
1. We should note that the oracle performance for
Method 2, n > 1 is measured using 100-best lists of
target stem sequences, whereas the one for Method
1 is measured using 100-best lists of inflected target
words. This can be a disadvantage for Method 1,
because a 100-best list of inflected translations actu-
ally contains about 50 different sequences of stems
(the rest are distinctions in inflections). Neverthe-
less, even if we measure the oracle for Method 2
using 40-best, it is higher than the 100-best oracle
of Method 1. In addition, it appears that using a hy-
pothesis list larger than n > 1=100 is not be helpful
for our method, as the model chose to use only up to
32 hypotheses.
Finally, we can see that using stemming at the
word alignment stage further improved both the or-
acle and the achieved results. The performance of
the best model is 2.56 BLEU points better than the
baseline. Since stemming in Russian for the most
part removes properties of words which are not ex-
pressed in English at the word level, these results
are consistent with previous results using stemming
to improve word alignment. From these results, we
also see that about half of the gain from using stem-
ming in the base MT system came from improving
word alignment, and half came from using transla-
tion models operating at the less sparse stem level.
Overall, the improvement achieved by predicting
morphological properties of Russian words with a
feature-rich component model is substantial, given
the relatively large size of the training data (1.6 mil-
lion sentences), and indicates that these kinds of
methods are effective in addressing the problems
in translating morphology-poor to morphology-rich
languages.
6.2 English-Russian phrasal system
For the phrasal system, we performed integration
only with Method 1, using the top 1 or 100-
best translations. This is the most straightforward
method for combining with any system, and we ap-
plied it as a proof-of-concept experiment.
Model BLEU Oracle BLEU
Base MT (n=1) 36.00 -
Method 1 (n=1) 36.43 42.33
Method 1 (n=100) 36.72 55.00
Table 4: Test set performance for English-to-Russian MT
(BLEU) results by model using a phrasal MT system.
The phrasal MT system is trained on the same
data as the treelet system. The phrase size and dis-
tortion limit were optimized (we used phrase size of
7 and distortion limit of 3). This system achieves a
substantially better BLEU score (by 6.76) than the
treelet system. The oracle BLEU score achievable
by Method 1 using n=1 translation, though, is still
6.3 BLEU point higher than the achieved BLEU.
Our model achieved smaller improvements for the
phrasal system (0.43 improvement for n=1 transla-
tions and 0.72 for the selected n=100 translations).
However, this improvement is encouraging given the
large size of the training data. One direction for
potentially improving these results is to use word
alignments from the MT system, rather than using
an alignment model to predict them.
520
Model BLEU Oracle BLEU
Base MT (n=1) 35.54 -
Method 1 (n=1) 37.24 42.29
Method 1 (n=2) 37.41 52.21
Method 2 (n=1) 36.53 42.46
Method 2 (n=4) 36.72 54.74
Method 3 (n=1) 36.87 42.96
Method 3 (n=2) 36.92 54.90
Table 5: Test set performance for English-to-Arabic MT
(BLEU) results by model using a treelet MT system.
6.3 English-Arabic treelet system
The Arabic system also improves with the use of our
mode: the best system (Method 1, n=2) achieves
the BLEU score of 37.41, a 1.87 point improve-
ment over the baseline. Unlike the case of Rus-
sian, Method 2 and 3 do not achieve better results
than Method 1, though the oracle BLEU score im-
proves in these models (54.74 and 54.90 as opposed
to 52.21 of Method 1). We do notice, however, that
the oracle improvement for the 1-best analysis is
much smaller than what we obtained in Russian.
We have been unable to closely diagnose why per-
formance did not improve using Methods 2 and 3
so far due to the absence of expertise in Arabic, but
one factor we suspect is affecting performance the
most in Arabic is the definition of stemming: the
effect of stemming is most beneficial when it is ap-
plied specifically to normalize the distinctions not
explicitly encoded in the other language; it may hurt
performance otherwise. We believe that in the case
of Arabic, this latter situation is actually happen-
ing: grammatical properties explicitly encoded in
English (e.g., definiteness, conjunction, pronominal
clitics) are lost when the Arabic words are stemmed.
This may be having a detrimental effect on the MT
systems that are based on stemmed input. Further
investigation is necessary to confirm this hypothesis.
6.4 Human evaluation
In this section we briefly report the results of human
evaluation on the output of our inflection prediction
system, as the correlation between BLEU scores and
human evaluation results is not always obvious. We
compared the output of our component against the
best output of the treelet system without our com-
ponent. We evaluated the following three scenarios:
(1) Arabic Method 1 with n=1, which corresponds
to the best performing system in BLEU according to
Table 5; (2) Russian, Method 1 with n=1; (3) Rus-
sian, Method 3 with n=32, which corresponds to the
best performing system in BLEU in Table 3. Note
that in (1) and (2), the only differences in the com-
pared outputs are the changes in word inflections,
while in (3) the outputs may differ in the selection
of the stems.
In all scenarios, two human judges (native speak-
ers of these languages) evaluated 100 sentences that
had different translations by the baseline system and
our model. The judges were given the reference
translations but not the source sentences, and were
asked to classify each sentence pair into three cate-
gories: (1) the baseline system is better (score=-1),
(2) the output of our model is better (score=1), or (3)
they are of the same quality (score=0).
human eval score BLEU diff
Arabic Method 1 0.1 1.9
Russian Method 1 0.255 1.2
Russian Method 3 0.26 2.6
Table 6: Human evaluation results
Table 6 shows the results of the averaged, aggre-
gated score across two judges per evaluation sce-
nario, along with the BLEU score improvements
achieved by applying our model. We see that in all
cases, the human evaluation scores are positive, indi-
cating that our models produce translations that are
better than those produced by the baseline system. 4
We also note that in Russian, the human evaluation
scores are similar for Method 1 and 3 (0.255 and
0.26), though the BLEU score gains are quite differ-
ent (1.2 vs 2.6). This may be attributed to the fact
that human evaluation typically favors the scenario
where only word inflections are different (Toutanova
and Suzuki, 2007).
7 Conclusion and future work
We have shown that an independent model of mor-
phology generation can be successfully integrated
with an SMT system, making improvements in both
phrasal and syntax-based MT. In the future, we
would like to include more sophistication in the de-
sign of a lexicon for a particular language pair based
on error analysis, and extend our pre-processing to
include other operations such as word segmentation.
4However, the improvement in Arabic is not statistically sig-
nificant on this 100 sentence set.
521
References
Tim Buckwalter. 2004. Buckwalter arabic morphological
analyzer version 2.0.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Ini-
tial explorations in English to Turkish statistical ma-
chine translation. In NAACL workshop on statistical
machine translation.
Jenny Finkel, Christopher Manning, and Andrew Ng.
2006. Solving the problem of cascading errors: ap-
proximate Bayesian inference for linguistic annotation
pipelines. In EMNLP.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
EMNLP.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
HLT-NAACL.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL Workshop on Statistical
Machine Translation.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In EMNLP-CoNNL.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
AMTA.
Young-Suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In ACL.
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204.
Franz Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency tree translation: Syntactically informed
phrasal SMT. In ACL.
Kristina Toutanova and Hisami Suzuki. 2007. Generating
case markers in machine translation. In NAACL-HLT.
Vladimir Vapnik. 1995. The nature of Statistical Learn-
ing Theory. Springer-Verlag.
Wei Wang, Kevin Knight, and Daniel Marcu. 2006. Cap-
italizing machine translation. In HLT-NAACL.
522
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 486?494,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A global model for joint lemmatization and part-of-speech prediction
Kristina Toutanova
Microsoft Research
Redmond, WA 98052
kristout@microsoft.com
Colin Cherry
Microsoft Research
Redmond, WA 98052
colinc@microsoft.com
Abstract
We present a global joint model for
lemmatization and part-of-speech predic-
tion. Using only morphological lexicons
and unlabeled data, we learn a partially-
supervised part-of-speech tagger and a
lemmatizer which are combined using fea-
tures on a dynamically linked dependency
structure of words. We evaluate our
model on English, Bulgarian, Czech, and
Slovene, and demonstrate substantial im-
provements over both a direct transduction
approach to lemmatization and a pipelined
approach, which predicts part-of-speech
tags before lemmatization.
1 Introduction
The traditional problem of morphological analysis
is, given a word form, to predict the set of all of
its possible morphological analyses. A morpho-
logical analysis consists of a part-of-speech tag
(POS), possibly other morphological features, and
a lemma (basic form) corresponding to this tag and
features combination (see Table 1 for examples).
We address this problem in the setting where we
are given a morphological dictionary for training,
and can additionally make use of un-annotated text
in the language. We present a new machine learn-
ing model for this task setting.
In addition to the morphological analysis task
we are interested in performance on two subtasks:
tag-set prediction (predicting the set of possible
tags of words) and lemmatization (predicting the
set of possible lemmas). The result of these sub-
tasks is directly useful for some applications.1 If
we are interested in the results of each of these two
1Tag sets are useful, for example, as a basis of sparsity-
reducing features for text labeling tasks; lemmatization is
useful for information retrieval and machine translation from
a morphologically rich to a morphologically poor language,
where full analysis may not be important.
subtasks in isolation, we might build independent
solutions which ignore the other subtask.
In this paper, we show that there are strong de-
pendencies between the two subtasks and we can
improve performance on both by sharing infor-
mation between them. We present a joint model
for these two subtasks: it is joint not only in that
it performs both tasks simultaneously, sharing in-
formation, but also in that it reasons about multi-
ple words jointly. It uses component tag-set and
lemmatization models and combines their predic-
tions while incorporating joint features in a log-
linear model, defined on a dynamically linked de-
pendency structure of words.
The model is formalized in Section 5 and eval-
uated in Section 6. We report results on English,
Bulgarian, Slovene, and Czech and show that joint
modeling reduces the lemmatization error by up to
19%, the tag-prediction error by up to 26% and the
error on the complete morphological analysis task
by up to 22.6%.
2 Task formalization
The main task that we would like to solve is
as follows: given a lexicon L which contains
all morphological analyses for a set of words
{w1, . . . , wn}, learn to predict all morphological
analyses for other words which are outside of L.
In addition to the lexicon, we are allowed to make
use of unannotated text T in the language. We will
predict morphological analyses for words which
occur in T. Note that the task is defined on word
types and not on words in context.
A morphological analysis of a word w consists
of a (possibly structured) POS tag t, together with
one or several lemmas, which are the possible ba-
sic forms of w when it has tag t. As an exam-
ple, Table 1 illustrates the morphological analy-
ses of several words taken from the CELEX lexi-
cal database of English (Baayen et al, 1995) and
the Multext-East lexicon of Bulgarian (Erjavec,
2004). The Bulgarian words are transcribed in
486
Word Forms Morphological Analyses Tags Lemmas
tell verb base (VB), tell VB tell
told verb past tense (VBD), tell VBD,VBN tell
verb past participle (VBN), tell
tells verb present 3rd person sing (VBZ), tell VBZ tell
telling verb present continuous (VBG), tell VBG,JJ tell
adjective (JJ), telling telling
izpravena adjective fem sing indef (A?FS-N), izpraven A?FS-N izpraven
verb main part past sing fem pass indef (VMPS-SFP-N), izpravia VMPS-SFP-N izpravia
izpraviha verb main indicative 3rd person plural (VMIA3P), izpravia VMIA3P izpravia
Table 1: Examples of morphological analyses of words in English and Bulgarian.
Latin characters. Here by ?POS tags? we mean
both simple main pos-tags such as noun or verb,
and detailed tags which include grammatical fea-
tures, such as VBZ for English indicating present
tense third person singular verb and A?FS-N for
Bulgarian indicating a feminine singular adjective
in indefinite form. In this work we predict only
main POS tags for the Multext-East languages, as
detailed tags were less useful for lemmatization.
Since the predicted elements are sets, we use
precision, recall, and F-measure (F1) to evaluate
performance. The two subtasks, tag-set prediction
and lemmatization are also evaluated in this way.
Table 1 shows the correct tag-sets and lemmas for
each of the example words in separate columns.
Our task setting differs from most work on lemma-
tization which uses either no or a complete rootlist
(Wicentowski, 2002; Dreyer et al, 2008).2 We can
use all forms occurring in the unlabeled text T but
there are no guarantees about the coverage of the
target lemmas or the number of noise words which
may occur in T (see Table 2 for data statistics).
Our setting is thus more realistic since it is what
one would have in a real application scenario.
3 Related work
In work on morphological analysis using machine
learning, the task is rarely addressed in the form
described above. Some exceptions are the work
(Bosch and Daelemans, 1999) which presents a
model for segmenting, stemming, and tagging
words in Dutch, and requires the prediction of
all possible analyses, and (Antal van den Bosch
and Soudi, 2007) which similarly requires the pre-
diction of all morpho-syntactically annotated seg-
mentations of words for Arabic. As opposed to
2These settings refer to the availability of a set of word
forms which are possible lemmas; in the no rootlist setting,
no other word forms in the language are given in addition to
the forms in the training set; in the complete rootlist setting,
a set of word forms which consists of exactly all correct lem-
mas for the words in the test set is given.
our work, these approaches do not make use of un-
labeled data and make predictions for each word
type in isolation.
In machine learning work on lemmatization for
highly inflective languages, it is most often as-
sumed that a word form and a POS tag are given,
and the task is to predict the set of corresponding
lemma(s) (Mooney and Califf, 1995; Clark, 2002;
Wicentowski, 2002; Erjavec and Dz?eroski, 2004;
Dreyer et al, 2008). In our task setting, we do
not assume the availability of gold-standard POS
tags. As a component model, we use a lemmatiz-
ing string transducer which is related to these ap-
proaches and draws on previous work in this and
related string transduction areas. Our transducer is
described in detail in Section 4.1.
Another related line of work approaches the dis-
ambiguation problem directly, where the task is
to predict the correct analysis of word-forms in
context (in sentences), and not all possible anal-
yses. In such work it is often assumed that the cor-
rect POS tags can be predicted with high accuracy
using labeled POS-disambiguated sentences (Er-
javec and Dz?eroski, 2004; Habash and Rambow,
2005). A notable exception is the work of (Adler
et al, 2008), which uses unlabeled data and a
morphological analyzer to learn a semi-supervised
HMM model for disambiguation in context, and
also guesses analyses for unknown words using a
guesser of likely POS-tags. It is most closely re-
lated to our work, but does not attempt to predict
all possible analyses, and does not have to tackle
a complex string transduction problem for lemma-
tization since segmentation is mostly sufficient for
the focus language of that study (Hebrew).
The idea of solving two related tasks jointly to
improve performance on both has been success-
ful for other pairs of tasks (e.g., (Andrew et al,
2004)). Doing joint inference instead of taking a
pipeline approach has also been shown useful for
other problems (e.g., (Finkel et al, 2006; Cohen
and Smith, 2007)).
487
4 Component models
We use two component models as the basis of
addressing the task: one is a partially-supervised
POS tagger which is trained using L and the unla-
beled text T; the other is a lemmatizing transducer
which is trained from L and can use T. The trans-
ducer can optionally be given input POS tags in
training and testing, which can inform the lemma-
tization. The tagger is described in Section 4.2 and
the transducer is described in Section 4.1.
In a pipeline approach to combining the tagging
and lemmatization components, we first predict a
set of tags for each word using the tagger, and then
ask the lemmatizer to predict one lemma for each
of the possible tags. In a direct transduction ap-
proach to the lemmatization subtask, we train the
lemmatizer without access to tags and ask it to
predict a single lemma for each word in testing.
Our joint model, described in Section 5, is defined
in a re-ranking framework, and can choose from
among k-best predictions of tag-sets and lemmas
generated from the component tagger and lemma-
tizer models.
4.1 Morphological analyser
We employ a discriminative character transducer
as a component morphological analyzer. The input
to the transducer is an inflected word (the source)
and possibly an estimated part-of-speech; the out-
put is the lemma of the word (the target). The
transducer is similar to the one described by Ji-
ampojamarn et al (2008) for letter-to-phoneme
conversion, but extended to allow for whole-word
features on both the input and the output. The core
of our engine is the dynamic programming algo-
rithm for monotone phrasal decoding (Zens and
Ney, 2004). The main feature of this algorithm is
its capability to transduce many consecutive char-
acters with a single operation; the same algorithm
is employed to tag subsequences in semi-Markov
CRFs (Sarawagi and Cohen, 2004).
We employ three main categories of features:
context, transition, and vocabulary (rootlist) fea-
tures. The first two are described in detail by Ji-
ampojamarn et al (2008), while the final is novel
to this work. Context features are centered around
a transduction operation such as es ? e , as em-
ployed in gives ? give. Context features include
an indicator for the operation itself, conjoined with
indicators for all n-grams of source context within
a fixed window of the operation. We also employ a
copy feature that indicates if the operation simply
copies the source character, such as e ? e. Tran-
sition features are our Markov, or n-gram features
on transduction operations. Vocabulary features
are defined on complete target words, according
to the frequency of said word in a provided unla-
beled text T. We have chosen to bin frequencies;
experiments on a development set suggested that
two indicators are sufficient: the first fires for any
word that occurred fewer than five times, while a
second also fires for those words that did not oc-
cur at all. By encoding our vocabulary in a trie and
adding the trie index to the target context tracked
by our dynamic programming chart, we can ef-
ficiently track these frequencies during transduc-
tion.
We incorporate the source part-of-speech tag by
appending it to each feature, thus the context fea-
ture es ? e may become es ? e, VBZ. To en-
able communication between the various parts-of-
speech, a universal set of unannotated features also
fires, regardless of the part-of-speech, acting as a
back-off model of how words in general behave
during stemming.
Linear weights are assigned to each of the trans-
ducer?s features using an averaged perceptron for
structure prediction (Collins, 2002). Note that
our features are defined in terms of the operations
employed during transduction, therefore to cre-
ate gold-standard feature vectors, we require not
only target outputs, but also derivations to pro-
duce those outputs. We employ a deterministic
heuristic to create these derivations; given a gold-
standard source-target pair, we construct a deriva-
tion that uses only trivial copy operations until
the first character mismatch. The remainder of
the transduction is performed with a single multi-
character replacement. For example, the deriva-
tion for living ? live would be l ? l , i ? i ,
v ? v , ing ? e. For languages with morpholo-
gies affecting more than just the suffix, one can
either develop a more complex heuristic, or deter-
mine the derivations using a separate aligner such
as that of Ristad and Yianilos (1998).
4.2 Tag-set prediction model
The tag-set model uses a training lexicon L and
unlabeled text T to learn to predict sets of tags
for words. It is based on the semi-supervised tag-
ging model of (Toutanova and Johnson, 2008). It
has two sub-models: one is an ambiguity class
488
or a tag-set model, which can assign probabili-
ties for possible sets of tags of words PTSM (ts|w)
and the other is a word context model, which can
assign probabilities PCM (contextsw|w, ts) to all
contexts of occurrence of word w in an unlabeled
text T. The word-context model is Bayesian and
utilizes a sparse Dirichlet prior on the distributions
of tags given words. In addition, it uses informa-
tion on a four word context of occurrences of w in
the unlabeled text.
Note that the (Toutanova and Johnson, 2008)
model is a tagger that assigns tags to occurrences
of words in the text, whereas we only need to pre-
dict sets of possible tags for word types, such as
the set {VBD, VBN} for the word told. Their com-
ponent sub-model PTSM predicts sets of tags and
it is possible to use it on its own, but by also us-
ing the context model we can take into account
information from the context of occurrence of
words and compute probabilities of tag-sets given
the observed occurrences in T. The two are com-
bined to make a prediction for a tag-set of a test
word w, given unlabeled text T, using Bayes rule:
p(ts|w) ? PTSM (ts|w)PCM (contextsw|w, ts).
We use a direct re-implementation of the word-
context model, using variational inference follow-
ing (Toutanova and Johnson, 2008). For the tag-
set sub-model, we employ a more sophisticated
approach. First, we learn a log-linear classifier in-
stead of a Naive Bayes model, and second, we use
features derived from related words appearing in
T. The possible classes predicted by the classifier
are as many as the observed tag-sets in L. The
sparsity is relieved by adding features for individ-
ual tags t which get shared across tag-sets contain-
ing t.
There are two types of features in the model:
(i) word-internal features: word suffixes, capital-
ization, existence of hyphen, and word prefixes
(such features were also used in (Toutanova and
Johnson, 2008)), and (ii) features based on re-
lated words. These latter features are inspired by
(Cucerzan and Yarowsky, 2000) and are defined as
follows: for a word w such as telling, there is an
indicator feature for every combination of two suf-
fixes ? and ?, such that there is a prefix p where
telling= p? and p? exists in T. For example, if the
word tells is found in T, there would be a feature
for the suffixes ?=ing,?=s that fires. The suffixes
are defined as all character suffixes up to length
three which occur with at least 100 words.
b o u n c e d
VBD     VBN JJ  VBD  VBN
b o u n c e r
JJR NN
bounce
bouncer bounce
?
bounc
bouncer
boucer
fbounce bounce
bounced bounced b o u n c e
VB     NN VB
bounce bounce
? ?
?
Figure 1: A small subset of the graphical model. The
tag-sets and lemmas active in the illustrated assignment are
shown in bold. The extent of joint features firing for the
lemma bounce is shown as a factor indicated by the blue cir-
cle and connected to the assignments of the three words.
5 A global joint model for morphological
analysis
The idea of this model is to jointly predict the set
of possible tags and lemmas of words. In addi-
tion to modeling dependencies between the tags
and lemmas of a single word, we incorporate de-
pendencies between the predictions for multiple
words. The dependencies among words are deter-
mined dynamically. Intuitively, if two words have
the same lemma, their tag-sets are dependent. For
example, imagine that we need to determine the
tag-set and lemmas of the word bouncer. The tag-
set model may guess that the word is an adjective
in comparative form, because of its suffix, and be-
cause its occurrences in T might not strongly in-
dicate that it is a noun. The lemmatizer can then
lemmatize the word like an adjective and come up
with bounce as a lemma. If the tag-set model is
fairly certain that bounce is not an adjective, but
is a verb or a noun, a joint model which looks si-
multaneously at the tags and lemmas of bouncer
and bounce will detect a problem with this assign-
ments and will be able to correct the tagging and
lemmatization error for bouncer.
The main source of information our joint model
uses is information about the assignments of all
words that have the same lemma l. If the tag-set
model is better able to predict the tags of some of
these words, the information can propagate to the
other words. If some of them are lemmatized cor-
rectly, the model can be pushed to lemmatize the
others correctly as well. Since the lemmas of test
words are not given, the dependencies between as-
489
signments of words are determined dynamically
by the currently chosen set of lemmas.
As an example, Figure 1 shows three sample
English words and their possible tag-sets and lem-
mas determined by the component models. It also
illustrates the dependencies between the variables
induced by the features of our model active for the
current (incorrect) assignment.
5.1 Formal model description
Given a set of test words w1, . . . wn and additional
word forms occurring in unlabeled data T, we de-
rive an extended set of words w1, . . . , wm which
contains the original test words and additional re-
lated words, which can provide useful information
about the test words. For example, if bouncer is a
test word and bounce and bounced occur in T these
two words can be added to the set of test words
because they can contribute to the classification
of bouncer. The algorithm for selecting related
words is simple: we add any word for which the
pipelined model predicts a lemma which is also
predicted as one of the top k lemmas for a word
from the test set.
We define a joint model over tag-sets and lem-
mas for all words in the extended set, using fea-
tures defined on a dynamically linked structure
of words and their assigned analyses. It is a re-
ranking model because the tag-sets and possible
lemmas are limited to the top k options provided
by the pipelined model.3 Our model is defined
on a very large set of variables, each of which
can take a large set of values. For example, for
a test set of size about 4,000 words for Slovene an
additional about 9,000 words from T were added
to the extended set. Each of these words has a
corresponding variable which indicates its tag-set
and lemma assignment. The possible assignments
range over all combinations available from the tag-
ging and lemmatizer component models; using the
top three tag-sets per word and top three lemmas
per tag gives an average of around 11.2 possible
assignments per word. This is because the tag-
sets have about 1.2 tags on average and we need
to choose a lemma for each. While it is not the
case that all variables are connected to each other
by features, the connectivity structure can be com-
plex.
More formally, let tsji denote possible tag-sets
3We used top three tag-sets and top three lemmas for each
tag for training.
for word wi, for j = 1 . . . k. Also, let li(t)j de-
note the top lemmas for word wi given tag t. An
assignment of a tag-set and lemmas to a word wi
consists of a choice of a tag-set, tsi (one of the
possible k tag-sets for the word) and, for each tag
t in the chosen tag-set, a choice of a lemma out
of the possible lemmas for that tag and word. For
brevity, we denote such joint assignment by tli.
As a concrete example, in Figure 1, we can see the
current assignments for three words: the assigned
tag-sets are shown underlined and in bolded boxes
(e.g., for bounced, the tag-set {VBD,VBN} is cho-
sen; for both tags, the lemma bounce is assigned).
Other possible tag-sets and other possible lemmas
for each chosen tag are shown in greyed boxes.
Our joint model defines a distribution over as-
signments to all words w1, . . . , wm. The form of
the model is as follows:
P (tl1, . . . , tlm) = eF (tl1,...,tlm)
???
tl?1,...,tl
?m
eF (tl?1,...,tl?m)??
Here F denotes the vector of features defined
over an assignment for all words in the set and ?
is a vector of parameters for the features. Next we
detail the types of features used.
Word-local features. The aim of such features is
to look at the set of all tags assigned to a word to-
gether with all lemmas and capture coarse-grained
dependencies at this level. These features intro-
duce joint dependencies between the tags and lem-
mas of a word, but they are still local to the as-
signment of single words. One such feature is the
number of distinct lemmas assigned across the dif-
ferent tags in the assigned tag-set. Another such
feature is the above joined with the identity of
the tag-set. For example, if a word?s tag-set is
{VBD,VBN}, it will likely have the same lemma
for both tags and the number of distinct lemmas
will be one (e.g., the word bounced), whereas if it
has the tags VBG, JJ the lemmas will be distinct for
the two tags (e.g. telling). In this class of features
are also the log-probabilities from the tag-set and
lemmatizer models.
Non-local features. Our non-local features look,
for every lemma l, at all words which have that
lemma as the lemma for at least one of their as-
signed tags, and derive several predicates on the
joint assignment to these words. For example,
using our word graph in the figure, the lemma
bounce is assigned to bounced for tags VBD and
VBN, to bounce for tags VB and NN, and to
bouncer for tag JJR. One feature looks at the
combination of tags corresponding to the differ-
490
ent forms of the lemma. In this case this would
be [JJR,NN+VB-lem,VBD+VBN]. The feature also
indicates any word which is exactly equal to the
lemma with lem as shown for the NN and VB tags
corresponding to bounce. Our model learns a neg-
ative weight for this feature, because the lemma
of a word with tag JJR is most often a word with
at least one tag equal to JJ. A variant of this
feature also appends the final character of each
word, like this: [JJR+r,NN+VB+e-lem,VBD+VBN-
d]. This variant was helpful for the Slavic lan-
guages because when using only main POS tags,
the granularity of the feature is too coarse. An-
other feature simply counts the number of distinct
words having the same lemma, encouraging re-
using the same lemma for different words. An ad-
ditional feature fires for every distinct lemma, in
effect counting the number of assigned lemmas.
5.2 Training and inference
Since the model is defined to re-rank candidates
from other component models, we need two differ-
ent training sets: one for training the component
models, and another for training the joint model
features. This is because otherwise the accuracy
of the component models would be overestimated
by the joint model. Therefore, we train the com-
ponent models on the training lexicons LTrain and
select their hyperparameters on the LDev lexicons.
We then train the joint model on the LDev lexicons
and evaluate it on the LTest lexicons. When apply-
ing models to the LTest set, the component mod-
els are first retrained on the union of LTrain and
LDev so that all models can use the same amount
of training data, without giving unfair advantage
to the joint model. Such set-up is also used for
other re-ranking models (Collins, 2000).
For training the joint model, we maximize the
log-likelihood of the correct assignment to the
words in LDev, marginalizing over the assign-
ments of other related words added to the graph-
ical model. We compute the gradient approx-
imately by computing expectations of features
given the observed assignments and marginal ex-
pectations of features. For computing these ex-
pectations we use Gibbs sampling to sample com-
plete assignments to all words in the graph.4 We
4We start the Gibbs sampler by the assignments found by
the pipeline method and then use an annealing schedule to
find a neighborhood of high-likelihood assignments, before
taking about 10 complete samples from the graph to compute
expectations.
use gradient descent with a small learning rate, se-
lected to optimize the accuracy on the LDev set.
For finding a most likely assignment at test time,
we use the sampling procedure, this time using a
slower annealing schedule before taking a single
sample to output as a guessed answer.
For the Gibbs sampler, we need to sample an
assignment for each word in turn, given the current
assignments of all other words. Let us denote the
current assignment to all words except wi as tl?i.
The conditional probability of an assignment tli
for word wi is given by:
P (tli|tl?i) = eF (tli,tl
?i)???
tl?i
eF (tl?i,tl?i)??
The summation in the denominator is over all
possible assignments for word wi. To compute
these quantities we need to consider only the fea-
tures involving the current word. Because of the
nature of the features in our model, it is possible
to isolate separate connected components which
do not share features for any assignment. If two
words do not share lemmas for any of their possi-
ble assignments, they will be in separate compo-
nents. Block sampling within a component could
be used if the component is relatively small; how-
ever, for the common case where there are five or
more words in a fully connected component ap-
proximate inference is necessary.
6 Experiments
6.1 Data
We use datasets for four languages: English, Bul-
garian, Slovene, and Czech. For each of the lan-
guages, we need a lexicon with morphological
analyses L and unlabeled text.
For English we derive the lexicon from CELEX
(Baayen et al, 1995), and for the other lan-
guages we use the Multext-East resources (Er-
javec, 2004). For English we use only open-class
words (nouns, verbs, adjectives, and adverbs), and
for the other languages we use words of all classes.
The unlabeled data for English we use is the union
of the Penn Treebank tagged WSJ data (Marcus et
al., 1993) and the BLLIP corpus.5 For the rest of
the languages we use only the text of George Or-
well?s novel 1984, which is provided in morpho-
logically disambiguated form as part of Multext-
East (but we don?t use the annotations). Table 2
5The BLLIP corpus contains approximately 30 million
words of automatically parsed WSJ data. We used these cor-
pora as plain text, without the annotations.
491
Lang LTrain LDev LTest Text
ws tl nf ws tl nf ws tl nf
Eng 5.2 1.5 0.3 7.4 1.4 0.8 7.4 1.4 0.8 320
Bgr 6.9 1.2 40.8 3.8 1.1 53.6 3.8 1.1 52.8 16.3
Slv 7.5 1.2 38.3 4.2 1.2 49.1 4.2 1.2 49.8 17.8
Cz 7.9 1.1 32.8 4.5 1.1 43.2 4.5 1.1 43.0 19.1
Table 2: Data sets used in experiments. The number of
word types (ws) is shown approximately in thousands. Also
shown are average number of complete analyses (tl) and per-
cent target lemmas not found in the unlabeled text (nf).
details statistics about the data set sizes for differ-
ent languages.
We use three different lexicons for each lan-
guage: one for training (LTrain), one for devel-
opment (LDev), and one for testing (LTest). The
global model weights are trained on the develop-
ment set as described in section 5.2. The lex-
icons are derived such that very frequent words
are likely to be in the training lexicon and less
frequent words in the dev and test lexicons, to
simulate a natural process of lexicon construction.
The English lexicons were constructed as follows:
starting with the full CELEX dictionary and the
text of the Penn Treebank corpus, take all word
forms appearing in the first 2000 sentences (and
are found in CELEX) to form the training lexi-
con, and then take all other words occurring in
the corpus and split them equally between the de-
velopment and test lexicons (every second word
is placed in the test set, in the order of first oc-
currence in the corpus). For the rest of the lan-
guages, the same procedure is applied, starting
with the full Multext-East lexicons and the text of
the novel 1984. Note that while it is not possi-
ble for training words to be included in the other
lexicons, it is possible for different forms of the
same lemma to be in different lexicons. The size
of the training lexicons is relatively small and we
believe this is a realistic scenario for application of
such models. In Table 2 we can see the number of
words in each lexicon and the unlabeled corpora
(by type), the average number of tag-lemma com-
binations per word,6 as well as the percentage of
word lemmas which do not occur in the unlabeled
text. For English, the large majority of target lem-
mas are available in T (with only 0.8% missing),
whereas for the Multext-East languages around 40
to 50% of the target lemmas are not found in T;
this partly explains the lower performance on these
languages.
6The tags are main tags for the Multext-East languages
and detailed tags for English.
Language Tag Model Tag Lem T+L
English none ? 94.0 ?
full 89.9 95.3 88.9
no unlab data 80.0 94.1 78.3
Bulgarian none ? 73.2 ?
full 87.9 79.9 75.3
no unlab data 80.2 76.3 70.4
Table 3: Development set results using different tag-set
models and pipelined prediction.
6.2 Evaluation of direct and pipelined models
for lemmatization
As a first experiment which motivates our joint
modeling approach, we present a comparison on
lemmatization performance in two settings: (i)
when no tags are used in training or testing by the
transducer, and (ii) when correct tags are used in
training and tags predicted by the tagging model
are used in testing. In this section, we report per-
formance on English and Bulgarian only. Compa-
rable performance on the other Multext-East lan-
guages is shown in Section 6.
Results are presented in Table 3. The experi-
ments are performed using LTrain for training and
LDev for testing. We evaluate the models on tag-
set F-measure (Tag), lemma-set F-measure(Lem)
and complete analysis F-measure (T+L). We show
the performance on lemmatization when tags are
not predicted (Tag Model is none), and when tags
are predicted by the tag-set model. We can see that
on both languages lemmatization is significantly
improved when a latent tag-set variable is used as
a basis for prediction: the relative error reduction
in Lem F-measure is 21.7% for English and 25%
for Bulgarian. For Bulgarian and the other Slavic
languages we predicted only main POS tags, be-
cause this resulted in better lemmatization perfor-
mance.
It is also interesting to evaluate the contribution
of the unlabeled data T to the performance of the
tag-set model. This can be achieved by remov-
ing the word-context sub-model of the tagger and
also removing related word features. The results
achieved in this setting for English and Bulgarian
are shown in the rows labeled ?no unlab data?. We
can see that the tag-set F-measure of such models
is reduced by 8 to 9 points and the lemmatization
F-measure is similarly reduced. Thus a large por-
tion of the positive impact tagging has on lemma-
tization is due to the ability of tagging models to
exploit unlabeled data.
The results of this experiment show there are
strong dependencies between the tagging and
492
lemmatization subtasks, which a joint model could
exploit.
6.3 Evaluation of joint models
Since our joint model re-ranks candidates pro-
duced by the component tagger and lemmatizer,
there is an upper bound on the achievable perfor-
mance. We report these upper bounds for the four
languages in Table 4, at the rows which list m-best
oracle under Model. The oracle is computed using
five-best tag-set predictions and three-best lemma
predictions per tag. We can see that the oracle per-
formance on tag F-measure is quite high for all
languages, but the performance on lemmatization
and the complete task is close to only 90 percent
for the Slavic languages. As a second oracle we
also report the perfect tag oracle, which selects
the lemmas determined by the transducer using the
correct part-of-speech tags. This shows how well
we could do if we made the tagging model perfect
without changing the lemmatizer. For the Slavic
languages this is quite a bit lower than the m-best
oracles, showing that the majority of errors of the
pipelined approach cannot be fixed by simply im-
proving the tagging model. Our global model has
the potential to improve lemma assignments even
given correct tags, by sharing information among
multiple words.
The actual achieved performance for three dif-
ferent models is also shown. For comparison,
the lemmatization performance of the direct trans-
duction approach which makes no use of tags is
also shown. The pipelined models select one-
best tag-set predictions from the tagging model,
and the 1-best lemmas for each tag, like the mod-
els used in Section 6.2. The model name lo-
cal FS denotes a joint log-linear model which
has only word-internal features. Even with only
word-internal features, performance is improved
for most languages. The the highest improvement
is for Slovene and represents a 7.8% relative re-
duction in F-measure error on the complete task.
When features looking at the joint assignments
of multiple words are added, the model achieves
much larger improvements (models joint FS in the
Table) across all languages.7 The highest overall
improvement compared to the pipelined approach
is again for Slovene and represents 22.6% reduc-
tion in error for the full task; the reduction is 40%
7Since the optimization is stochastic, the results are av-
eraged over four runs. The standard deviations are between
0.02 and 0.11.
Language Model Tag Lem T+L
English tag oracle 100 98.9 98.7
English m-best oracle 97.9 99.0 97.5
English no tags ? 94.3 ?
English pipelined 90.9 95.9 90.0
English local FS 90.8 95.9 90.0
English joint FS 91.7 96.1 91.0
Bulgarian tag oracle 100 84.3 84.3
Bulgarian m-best oracle 98.4 90.7 89.9
Bulgarian no tags ? 73.2 ?
Bulgarian pipelined 87.9 78.5 74.6
Bulgarian local FS 88.9 79.2 75.8
Bulgarian joint FS 89.5 81.0 77.8
Slovene tag oracle 100 85.9 85.9
Slovene m-best oracle 98.7 91.2 90.5
Slovene no tags ? 78.4 ?
Slovene pipelined 89.7 82.1 78.3
Slovene local FS 90.8 82.7 80.0
Slovene joint FS 92.4 85.5 83.2
Czech tag oracle 100 83.2 83.2
Czech m-best oracle 98.1 88.7 87.4
Czech no tags ? 78.7 ?
Czech pipelined 92.3 80.7 77.5
Czech local FS 92.3 80.9 78.0
Czech joint FS 93.7 83.0 80.5
Table 4: Results on the test set achieved by joint and
pipelined models and oracles. The numbers represent tag-set
prediction F-measure (Tag), lemma-set prediction F-measure
(Lem) and F-measure on predicting complete tag, lemma
analysis sets (T+L).
relative to the upper bound achieved by the m-best
oracle. The smallest overall improvement is for
English, representing a 10% error reduction over-
all, which is still respectable. The larger improve-
ment for Slavic languages might be due to the fact
that there are many more forms of a single lemma
and joint reasoning allows us to pool information
across the forms.
7 Conclusion
In this paper we concentrated on the task of mor-
phological analysis, given a lexicon and unanno-
tated data. We showed that the tasks of tag pre-
diction and lemmatization are strongly dependent
and that by building state-of-the art models for
the two subtasks and performing joint inference
we can improve performance on both tasks. The
main contribution of our work was that we intro-
duced a joint model for the two subtasks which in-
corporates dependencies between predictions for
multiple word types. We described a set of fea-
tures and an approximate inference procedure for a
global log-linear model capturing such dependen-
cies, and demonstrated its effectiveness on English
and three Slavic languages.
Acknowledgements
We would like to thank Galen Andrew and Lucy Vander-
wende for useful discussion relating to this work.
493
References
Meni Adler, Yoav Goldberg, and Michael Elhadad. 2008.
Unsupervised lexicon-based resolution of unknown words
for full morpholological analysis. In Proceedings of ACL-
08: HLT.
Galen Andrew, Trond Grenager, and Christopher Manning.
2004. Verb sense and subcategorization: Using joint in-
ference to improve performance on complementary tasks.
In EMNLP.
Erwin Marsi Antal van den Bosch and Abdelhadi Soudi.
2007. Memory-based morphological analysis and part-
of-speech tagging of arabic. In Abdelhadi Soudi, An-
tal van den Bosch, and Gunter Neumann, editors, Arabic
Computational Morphology Knowledge-based and Em-
pirical Methods. Springer.
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX lexical database.
Antal Van Den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proceedings
of the 37th Annual Meeting of the Association for Compu-
tational Linguistics.
Alexander Clark. 2002. Memory-based learning of mor-
phology with stochastic transducers. In Proceedings of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 513?520.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpholog-
ical and syntactic disambiguation. In EMNLP.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In ICML.
M. Collins. 2002. Discriminative training methods for hid-
den markov models: Theory and experiments with percep-
tron algorithms. In EMNLP.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL 2000.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 1080?1089, Honolulu, October.
Tomaz? Erjavec and Saa?o Dz?eroski. 2004. Machine learn-
ing of morphosyntactic structure: lemmatizing unknown
Slovene words. Applied Artificial Intelligence, 18:17?
41.
Tomaz? Erjavec. 2004. Multext-east version 3: Multilingual
morphosyntactic specifications, lexicons and corpora. In
Proceedings of LREC-04.
Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate bayesian inference for linguistic annotation
pipelines. In EMNLP.
Nizar Habash and Owen Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological disam-
biguation in one fell swoop. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kon-
drak. 2008. Joint processing and discriminative training
for letter-to-phoneme conversion. In Proceedings of ACL-
08: HLT, pages 905?913, Columbus, Ohio, June.
M. Marcus, B. Santorini, and Marcinkiewicz. 1993. Build-
ing a large annotated coprus of english: the penn treebank.
Computational Linguistics, 19.
Raymond J. Mooney and Mary Elaine Califf. 1995. Induc-
tion of first-order decision lists: Results on learning the
past tense of english verbs. Journal of Artificial Intelli-
gence Research, 3:1?24.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning
string-edit distance. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 20(5):522?532.
Sunita Sarawagi and William Cohen. 2004. Semimarkov
conditional random fields for information extraction. In
ICML.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
LDA-based model for semi-supervised part-of-speech tag-
ging. In nips08.
Richard Wicentowski. 2002. Modeling and Learning Mul-
tilingual Inflectional Morphology in a Minimally Super-
vised Framework. Ph.D. thesis, Johns-Hopkins Univer-
sity.
R. Zens and H. Ney. 2004. Improvements in phrase-based
statistical machine translation. In HLT-NAACL, pages
257?264, Boston, USA, May.
494
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 576?584,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Competitive generative models with structure learning for NLP
classification tasks
Kristina Toutanova
Microsoft Research
Redmond, WA
kristout@microsoft.com
Abstract
In this paper we show that generative
models are competitive with and some-
times superior to discriminative models,
when both kinds of models are allowed to
learn structures that are optimal for dis-
crimination. In particular, we compare
Bayesian Networks and Conditional log-
linear models on two NLP tasks. We ob-
serve that when the structure of the gen-
erative model encodes very strong inde-
pendence assumptions (a la Naive Bayes),
a discriminative model is superior, but
when the generative model is allowed to
weaken these independence assumptions
via learning a more complex structure, it
can achieve very similar or better perfor-
mance than a corresponding discrimina-
tive model. In addition, as structure learn-
ing for generative models is far more ef-
ficient, they may be preferable for some
tasks.
1 Introduction
Discriminative models have become the models
of choice for NLP tasks, because of their ability
to easily incorporate non-independent features and
to more directly optimize classification accuracy.
State of the art models for many NLP tasks are ei-
ther fully discriminative or trained using discrim-
inative reranking (Collins, 2000). These include
models for part-of-speech tagging (Toutanova et
al., 2003), semantic-role labeling (Punyakanok et
al., 2005; Pradhan et al, 2005b) and Penn Tree-
bank parsing (Charniak and Johnson, 2005).
The superiority of discriminative models has
been shown on many tasks when the discrimina-
tive and generative models use exactly the same
model structure (Klein and Manning, 2002). How-
ever, the advantage of the discriminative mod-
els can be very slight (Johnson, 2001) and for
small training set sizes generative models can
be better because they need fewer training sam-
ples to converge to the optimal parameter setting
(Ng and Jordan, 2002). Additionally, many dis-
criminative models use a generative model as a
base model and add discriminative features with
reranking (Collins, 2000; Charniak and Johnson,
2005; Roark et al, 2004), or train discriminatively
a small set of weights for features which are gener-
atively estimated probabilities (Raina et al, 2004;
Och and Ney, 2002). Therefore it is important to
study generative models and to find ways of mak-
ing them better even when they are used only as
components of discriminative models.
Generative models may often perform poorly
due to making strong independence assumptions
about the joint distribution of features and classes.
To avoid this problem, generative models for
NLP tasks have often been manually designed
to achieve an appropriate representation of the
joint distribution, such as in the parsing models of
(Collins, 1997; Charniak, 2000). This shows that
when the generative models have a good model
structure, they can perform quite well.
In this paper, we look differently at compar-
ing generative and discriminative models. We ask
the question: given the same set of input features,
what is the best a generative model can do if it is
allowed to learn an optimal structure for the joint
distribution, and what is the best a discriminative
model can do if it is also allowed to learn an op-
timal structure. That is, we do not impose any in-
dependence assumptions on the generative or dis-
criminative models and let them learn the best rep-
resentation of the data they can.
Structure learning is very efficient for genera-
tive models in the form of directed graphical mod-
els (Bayesian Networks (Pearl, 1988)), since the
optimal parameters for such models can be esti-
mated in closed form. We compare Bayesian Net-
576
works with structure learning to their closely re-
lated discriminative counterpart ? conditional log-
linear models with structure learning. Our condi-
tional log-linear models can also be seen as Con-
ditional Random Fields (Lafferty et al, 2001), ex-
cept we do not have a structure on the labels, but
want to learn a structure on the features.
We compare the two kinds of models on two
NLP classification tasks ? prepositional phrase at-
tachment and semantic role labelling. Our re-
sults show that the generative models are compet-
itive with or better than the discriminative mod-
els. When a small set of interpolation parame-
ters for the conditional probability tables are fit
discriminatively, the resulting hybrid generative-
discriminative models perform better than the gen-
erative only models and sometimes better than the
discriminative models.
In Section 2, we describe in detail the form of
the generative and discriminative models we study
and our structure search methodology. In Section
3 we present the results of our empirical study.
2 Model Classes and Methodology
2.1 Generative Models
In classification tasks, given a training set of in-
stances D = {[xi, yi]}, where xi are the input
features for the i-th instance, and yi is its label,
the task is to learn a classifier that predicts the la-
bels of new examples. If X is the space of inputs
and Y is the space of labels, a classifier is a func-
tion f : X ? Y . A generative model is one that
models the joint probability of inputs and labels
PD(x, y) through a distribution P?(x, y), depen-
dent on some parameter vector ?. The classifier
based on this generative model chooses the most
likely label given an input according to the con-
ditionalized estimated joint distribution. The pa-
rameters ? of the fitted distribution are usually es-
timated using the maximum joint likelihood esti-
mate, possibly with a prior.
We study generative models represented as
Bayesian Networks (Pearl, 1988), because their
parameters can be estimated extremely fast as the
maximizer of the joint likelihood is the closed
form relative frequency estimate. A Bayesian Net-
work is an acyclic directed graph over a set of
nodes. For every variable Z, let Pa(Z) denote the
set of parents of Z. The structure of the Bayesian
Network encodes the following set of indepen-
Y
X1 X2 Xm......
Figure 1: Naive Bayes Bayesian Network
dence assumptions: every variable is conditionally
independent of its non-descendants given its par-
ents. For example, the structure of the Bayesian
Network model in Figure 1 encodes the indepen-
dence assumption that the input features are con-
ditionally independent given the class label.
Let the input be represented as a vector of m
nominal features. We define Bayesian Networks
over the m input variables X1, X2, . . . , Xm and
the class variable Y . In all networks, we add links
from the class variable Y to all input features.
In this way we have generative models which
estimate class-specific distributions over features
P (X|Y ) and a prior over labels P (Y ). Figure 1
shows a simple Bayesian Network of this form,
which is the well-known Naive Bayes model.
A specific joint distribution for a given Bayesian
Network (BN) is given by a set of condi-
tional probability tables (CPTs) which spec-
ify the distribution over each variable given its
parents P (Z|Pa(Z)). The joint distribution
P (Z1, Z2, . . . , Zm) is given by:
P (Z1, Z2, . . . , Zm) =
?
i=1...m
P (Zi|Pa(Zi))
The parameters of a Bayesian Network model
given its graph structure are the values of
the conditional probabilities P (Zi|Pa(Zi)). If
the model is trained through maximizing the
joint likelihood of the data, the optimal pa-
rameters are the relative frequency estimates:
P? (Zi = v|Pa(Zi) = ~u) = count(Zi=v,Pa(Zi)=~u)count(Pa(Zi)=~u) Here
v denotes a value of Zi and ~u denotes a vector of
values for the parents of Zi.
Most often smoothing is applied to avoid zero
probability estimates. A simple form of smooth-
ing is add-? smoothing which is equivalent to a
Dirichlet prior. For NLP tasks it has been shown
that other smoothing methods are far superior to
add-? smoothing ? see, for example, Goodman
577
(2001). In particular, it is important to incorpo-
rate lower-order information based on subsets of
the conditioning information. Therefore we as-
sume a structural form of the conditional proba-
bility tables which implements a more sophisti-
cated type of smoothing ? interpolated Witten-Bell
(Witten and Bell, 1991). This kind of smooth-
ing has also been used in the generative parser of
(Collins, 1997) and has been shown to have a rel-
atively good performance for language modeling
(Goodman, 2001).
To describe the form of the conditional proba-
bility tables, we introduce some notation. Let Z
denote a variable in the BN and Z1, Z2, . . . , Zk
denote the set of its parents. The probabil-
ity P (Z = z|Z1 = z1, Z2 = z2, . . . , Zk = zk) is estimated
using Witten-Bell smoothing as follows: (below
the tuple of values z1, z2, . . . , zk is denoted by
z1k).
PWB(z|z1k) = ?(z1k) ? P? (z|z1k) + (1 ? ?(z1k)) ? PWB(z|z1k?1)
In the above equation, P? is the relative fre-
quency estimator. The recursion is ended by inter-
polating with a uniform distribution 1Vz , where Vz
is the vocabulary of values for the prediction vari-
able Z. We determine the interpolation back-off
order by looking at the number of values of each
variable. We apply the following rule: the variable
with the highest number of values observed in the
training set is backed off first, then the variable
with the next highest number of values, and so on.
Typically, the class variable will be backed-off last
according to this rule.
In Witten-Bell smoothing, the values of the in-
terpolation coefficients are as follows: ?(z1k) =
count(z1k)
count(z1k)+d?|z:count(z,z1k)>0| . The weight of the
relative frequency estimate based on a given con-
text increases if the context has been seen more
often in the training data and decreases if the con-
text has been seen with more different values for
the predicted variable z.
Looking at the form of our conditional proba-
bility tables, we can see that the major parame-
ters are estimated directly based on the counts of
the events in the training data. In addition, there
are interpolation parameters (denoted by d above),
which participate in computing the interpolation
weights ?. The d parameters are hyper-parameters
and we learn them on a development set of sam-
ples. We experimented with learning a single d
parameter which is shared by all CPTs and learn-
ing multiple d parameters ? one for every type of
conditioning context in every CPT ? i.e., each CPT
has as many d parameters as there are back-off lev-
els.
We place some restrictions on the Bayesian Net-
works learned, for closer correspondence with the
discriminative models and for tractability: Every
input variable node has the label node as a parent,
and at most three parents per variable are allowed.
2.1.1 Structure Search Methodology
Our structure search method differs slightly
from previously proposed methods in the literature
(Heckerman, 1999; Pernkopf and Bilmes, 2005).
The search space is defined as follows. We start
with a Bayesian Network containing only the class
variable. We denote by CHOSEN the set of vari-
ables already in the network and by REMAINING
the set of unplaced variables. Initially, only the
class variable Y is in CHOSEN and all other vari-
ables are in REMAINING. Starting from the cur-
rent BN, the set of next candidate structures is de-
fined as follows: For every unplaced variable R
in REMAINING, and for every subset Sub of size
at most two from the already placed variables in
CHOSEN, consider adding R with parents Sub?Y
to the current BN. Thus the number of candidate
structures for extending a current BN is on the or-
der of m3, where m is the number of variables.
We perform a greedy search. At each step, if the
best variable B with the best set of parents Pa(B)
improves the evaluation criterion, move B from
REMAINING to CHOSEN, and continue the search
until there are no variables in REMAINING or the
evaluation criterion can not be improved.
The evaluation criterion for BNs we use is clas-
sification accuracy on a development set of sam-
ples. Thus our structure search method is dis-
criminative, in the terminology of (Grossman and
Domingos, 2004; Pernkopf and Bilmes, 2005). It
is very easy to evaluate candidate BN structures.
The main parameters in the CPTs are estimated
via the relative frequency estimator on the training
set, as discussed in the previous section. We do not
fit the hyper-parameters d during structure search.
We fit these parameters only after we have se-
lected a final BN structure. Throughout the struc-
ture search, we use a fixed value of 1 for d for all
CPTs and levels of back-off. Therefore we are us-
ing generative parameter estimation and discrimi-
native structure search. See Section 4 for discus-
sion on how this method relates to previous work.
578
Notice that the optimal parameters of the con-
ditional probability tables of variables already in
the current BN do not change at all when a new
variable is added, thus making update very ef-
ficient. After the stopping criterion is met, the
hyper-parameters of the resulting BN are fit on
the development set. As discussed in the previ-
ous subsection, we fit either a single or multiple
hyper-parameters d. The fitting criterion for the
generative Bayesian Networks is joint likelihood
of the development set of samples with a Gaussian
prior on the values log(d). 1
Additionally, we explore fitting the hyper-
parameters of the Bayesian Networks by opti-
mizing the conditional likelihood of the develop-
ment set of samples. In this case we call the
resulting models Hybrid Bayesian Network mod-
els, since they incorporate a number of discrimi-
natively trained parameters. Hybrid models have
been proposed before and shown to perform very
competitively (Raina et al, 2004; Och and Ney,
2002). In Section 3.2 we compare generative and
hybrid Bayesian Networks.
2.2 Discriminative Models
Discriminative models learn a conditional distri-
bution P?(Y | ~X) or discriminant functions that
discriminate between classes. Here we concen-
trate on conditional log-linear models. A sim-
ple example of such model is logistic regression,
which directly corresponds to Naive Bayes but is
trained to maximize the conditional likelihood. 2
To describe the form of models we study, let us
introduce some notation. We represent a tuple of
nominal variables (X1,X2,. . . ,Xm) as a vector of
0s and 1s in the following standard way: We map
the tuple of values of nominal variables to a vector
space with dimensionality the sum of possible val-
ues of all variables. There is a single dimension in
the vector space for every value of each input vari-
able Xi. The tuple (X1,X2,. . . ,Xm) is mapped to
a vector which has 1s in m places, which are the
corresponding dimensions for the values of each
variable Xi. We denote this mapping by ?.
In logistic regression, the probability of a label
Y = y given input features ?(X1, X2, . . . , Xk) =
1Since the d parameters are positive we convert the prob-
lem to unconstrained optimization over parameters ? such
that d = e? .
2Logistic regression additionally does not have the sum to
one constraint on weights but it can be shown that this does
not increase the representational power of the model.
~x is estimated as:
P (y|~x) = exp ? ~wy, ~x??
y? exp ? ~wy? , ~x?
There is a parameter vector of feature weights
~wy for each label y. We fit the parameters of the
log-linear model by maximizing the conditional
likelihood of the training set including a gaussian
prior on the parameters. The prior has mean 0 and
variance ?2. The variance is a hyper-parameter,
which we optimize on a development set.
In addition to this simple logistic regression
model, as for the generative models, we consider
models with much richer structure. We consider
more complex mappings ?, which incorporate
conjunctions of combinations of input variables.
We restrict the number of variables in the com-
binations to three, which directly corresponds to
our limit on number of parents in the Bayesian
Network structures. This is similar to consider-
ing polynomial kernels of up to degree three, but
is more general, because, for example, we can
add only some and not all bigram conjunctions
of variables. Structure search (or feature selec-
tion) for log-linear models has been done before
e.g. (Della Pietra et al, 1997; McCallum, 2003).
We devise our structure search methodology in a
way that corresponds as closely as possible to our
structure search for Bayesian Networks. The ex-
act hypothesis space considered is defined by the
search procedure for an optimal structure we ap-
ply, which we describe next.
2.2.1 Structure Search Methodology
We start with an initial empty feature set and a
candidate feature set consisting of all input fea-
tures: CANDIDATES={X1,X2,. . . ,Xm}. In the
course of the search, the set CANDIDATES may
contain feature conjunctions in addition to the ini-
tial input features. After a feature is selected from
the candidates set and added to the model, the fea-
ture is removed from CANDIDATES and all con-
junctions of that feature with all input features are
added to CANDIDATES. For example, if a fea-
ture conjunction ?Xi1 ,Xi2 ,. . .,Xin? is selected, all
of its expansions of the form ?Xi1 ,Xi2 ,. . .,Xin ,Xi?,
where Xi is not in the conjunction already, are
added to CANDIDATES.
We perform a greedy search and at each step
select the feature which maximizes the evaluation
criterion, add it to the model and extend the set
579
CANDIDATES as described above. The evaluation
criterion for selecting features is classification ac-
curacy on a development set of samples, as for the
Bayesian Network structure search.
At each step, we evaluate all candidate fea-
tures. This is computationally expensive, because
it requires iterative re-estimation. In addition to
estimating weights for the new features, we re-
estimate the old parameters, since their optimal
values change. We did not preform search for the
hyper-parameter ? when evaluating models. We fit
? by optimizing the development set accuracy af-
ter a model was selected. Note that our feature se-
lection algorithm adds an input variable or a vari-
able conjunction with all of its possible values in a
single step of the search. Therefore we are adding
hundreds or thousands of binary features at each
step, as opposed to only one as in (Della Pietra
et al, 1997). This is why we can afford to per-
form complete re-estimation of the parameters of
the model at each step.
3 Experiments
3.1 Problems and Datasets
We study two classification problems ? preposi-
tional phrase (PP) attachment, and semantic role
labeling.
Following most of the literature on preposi-
tional phrase attachment (e.g., (Hindle and Rooth,
1993; Collins and Brooks, 1995; Vanschoen-
winkel and Manderick, 2003)), we focus on the
most common configuration that leads to ambi-
guities: V NP PP. Here, we are given a verb
phrase with a following noun phrase and a prepo-
sitional phrase. The goal is to determine if the
PP should be attached to the verb or to the ob-
ject noun phrase. For example, in the sentence:
Never [hang]V [a painting]NP [with a peg]PP , the
prepositional phrase with a peg can either modify
the verb hang or the object noun phrase a painting.
Here, clearly, with a peg modifies the verb hang.
We follow the common practice in representing
the problem using only the head words of these
constituents and of the NP inside the PP. Thus the
example sentence is represented as the following
quadruple: [v:hang n1:painting p:with n2:peg].
Thus for the PP attachment task we have binary
labels Att , and four input variables ? v, n1, p, n2.
We work with the standard dataset previously
used for this task by other researchers (Ratna-
Task Training Devset Test
PP 20,801 4,039 3,097
SRL 173,514 5,115 9,272
Table 1: Data sizes for the PP attachment and SRL
tasks.
parkhi et al, 1994; Collins and Brooks, 1995). It is
extracted from the the Penn Treebank Wall Street
Journal data (Ratnaparkhi et al, 1994). Table 1
shows summary statistics for the dataset.
The second task we concentrate on is semantic
role labeling in the context of PropBank (Palmer
et al, 2005). The PropBank corpus annotates
phrases which fill semantic roles for verbs on top
of Penn Treebank parse trees. The annotated roles
specify agent, patient, direction, etc. The labels
for semantic roles are grouped into two groups,
core argument labels and modifier argument la-
bels, which correspond approximately to the tradi-
tional distinction between arguments and adjuncts.
There has been plenty of work on machine
learning models for semantic role labeling, start-
ing with the work of Gildea and Jurafsky (2002),
and including CoNLL shared tasks (Carreras and
Ma`rquez, 2005). The most successful formulation
has been as learning to classify nodes in a syn-
tactic parse tree. The possible labels are NONE,
meaning that the corresponding phrase has no se-
mantic role and the set of core and modifier la-
bels. We concentrate on the subproblem of clas-
sification for core argument nodes. The problem
is, given that a node has a core argument label, de-
cide what the correct label is. Other researchers
have also looked at this subproblem (Gildea and
Jurafsky, 2002; Toutanova et al, 2005; Pradhan et
al., 2005a; Xue and Palmer, 2004).
Many features have been proposed for build-
ing models for semantic role labeling. Initially,
7 features were proposed by (Gildea and Juraf-
sky, 2002), and all following research has used
these features and some additional ones. These
are the features we use as well. Table 2 lists the
features. State-of-the-art models for the subprob-
lem of classification of core arguments addition-
ally use other features of individual nodes (Xue
and Palmer, 2004; Pradhan et al, 2005a), as well
as global features including the labels of other
nodes in parse tree. Nevertheless it is interesting
to see how well we can do with these 7 features
only.
We use the standard training, development, and
580
Feature Types (Gildea and Jurafsky, 2002)
PHRASE TYPE: Syntactic Category of node
PREDICATE LEMMA: Stemmed Verb
PATH: Path from node to predicate
POSITION: Before or after predicate?
VOICE: Active or passive relative to predicate
HEAD WORD OF PHRASE
SUB-CAT: CFG expansion of predicate?s parent
Table 2: Features for Semantic Role Labeling.
test sets from the February 2004 version of Prop-
bank. The training set consists of sections 2 to 21,
the development set is from section 24, and the test
set is from section 23. The number of samples is
listed in Table 1. As we can see, the training set
size is much larger compared to the PP attachment
training set.
3.2 Results
In line with previous work (Ng and Jordan, 2002;
Klein and Manning, 2002), we first compare Naive
Bayes and Logistic regression on the two NLP
tasks. This lets us see how they compare when the
generative model is making strong independence
assumptions and when the two kinds of models
have the same structure. Then we compare the
generative and discriminative models with learned
richer structures.
Table 3 shows the Naive Bayes/Logistic re-
gression results for PP attachment. We list re-
sults for several conditions of training the Naive
Bayes classifier, depending on whether it is trained
as strictly generative or as a hybrid model, and
whether a single or multiple hyper-parameters d
are trained. In the table, we see results for gen-
erative Naive Bayes, where the d parameters are
trained to maximize the joint likelihood of the de-
velopment set, and for Hybrid Naive Bayes, where
the hyper-parameters are trained to optimize the
conditional likelihood. The column H-Params (for
hyper-parameters) indicates whether a single or
multiple d parameters are learned.
Logistic regression is more fairly comparable
to Naive Bayes trained using a single hyper-
parameter, because it also uses a single hyper-
parameter ? trained on a development set. How-
ever, for the generative model it is very easy to
train multiple weights d since the likelihood of a
development set is differentiable with respect to
the parameters. For logistic regression, we may
want to choose different variances for the differ-
ent types of features but the search would be pro-
Model H-params Test set acc
Naive Bayes 1 81.2
Naive Bayes 9 81.2
Logistic regression 1 82.6
Hybrid Naive Bayes 1 81.2
Hybrid Naive Bayes 9 81.5
Table 3: Naive Bayes and Logistic regression PP
attachment results.
hibitively expensive. Thus we think it is also fair
to fit multiple interpolation weights for the gener-
ative model and we show these results as well.
As we can see from the table, logistic regression
outperforms both Naive Bayes and Hybrid Naive
Bayes. The performance of Hybrid Naive Bayes
with multiple interpolation weights improves the
accuracy, but performance is still better for logis-
tic regression. This suggests that the strong in-
dependence assumptions are hurting the classifier.
According to McNemar?s test, logistic regression
is statistically significantly better than the Naive
Bayes models and than Hybrid Naive Bayes with a
single interpolation weight (p < 0.025), but is not
significantly better than Hybrid Naive Bayes with
multiple interpolation parameters at level 0.05.
However, when both the generative and dis-
criminative models are allowed to learn optimal
structures, the generative model outperforms the
discriminative model. As seen from Table 4,
the Bayesian Network with a single interpolation
weight achieves an accuracy of 84.6%, whereas
the discriminative model performs at 83.8%. The
hybrid model with a single interpolation weight
does even better, achieving 85.0% accuracy. For
comparison, the model of Collins & Brooks has
accuracy of 84.15% on this test set, and the high-
est result obtained through a discriminative model
with this feature set is 84.8%, using SVMs and a
polynomial kernel with multiple hyper-parameters
(Vanschoenwinkel and Manderick, 2003). The
Hybrid Bayes Nets are statistically significantly
better than the Log-linear model (p < 0.05), and
the Bayes Nets are not significantly better than the
Log-linear model. All models from Table 4 are
significantly better than all models in Table 3.
For semantic role labelling classification of core
arguments, the results are listed in Tables 5 and
6. We can see that the difference in performance
between Naive Bayes with a single interpolation
parameter d ? 83.3% and the performance of Lo-
gistic regression ? 91.1%, is very large. This
shows that the independence assumptions are quite
581
Model H-params Test set acc
Bayes Net 1 84.6
Bayes Net 13 84.6
Log-linear model 1 83.8
Hybrid Bayes Net 1 85.0
Hybrid Bayes Net 13 84.8
Table 4: Bayesian Network and Conditional log-
linear model PP attachment results.
Model H-params Test set acc
Naive Bayes 1 83.3
Naive Bayes 15 85.2
Logistic regression 1 91.1
Hybrid Naive Bayes 1 84.1
Hybrid Naive Bayes 15 86.5
Table 5: Naive Bayes and Logistic regression SRL
classificaion results.
strong, and since many of the features are not
sparse lexical features and training data for them
is sufficient, the Naive Bayes model has no ad-
vantage over the discriminative logistic regression
model. The Hybrid Naive Bayes model with mul-
tiple interpolation weights does better than Naive
Bayes, performing at 86.5%. All differences be-
tween the classifiers in Table 5 are statistically sig-
nificant at level 0.01. Compared to the PP attach-
ment task, here we are getting more benefit from
multiple hyper-parameters, perhaps due to the di-
versity of the features for SRL: In SRL, we use
both sparse lexical features and non-sparse syntac-
tic ones, whereas all features for PP attachment are
lexical.
From Table 6 we can see that when we com-
pare general Bayesian Network structures to gen-
eral log-linear models, the performance gap be-
tween the generative and discriminative models
is much smaller. The Bayesian Network with a
single interpolation weight d has 93.5% accuracy
and the log-linear model has 93.9% accuracy. The
hybrid model with multiple interpolation weights
performs at 93.7%. All models in Table 6 are in
a statistical tie according to McNemar?s test, and
thus the log-linear model is not significantly bet-
ter than the Bayes Net models. We can see that
the generative model was able to learn a structure
with a set of independence assumptions which are
not as strong as the ones the Naive Bayes model
makes, thus resulting in a model with performance
competitive with the discriminative model.
Figures 2(a) and 2(b) show the Bayesian Net-
works learned for PP Attachment and Semantic
Role Labeling. Table 7 shows the conjunctions
Model H-params Test set acc
Bayes Net 1 93.5
Bayes Net 20 93.6
Log-linear model 1 93.9
Hybrid Bayes Net 1 93.5
Hybrid Bayes Net 20 93.7
Table 6: Bayesian Network and Conditional log-
linear model SRL classification results.
PP Attachment Model
?P?, ?P,V?, ?P,N1?, ?P,N2?
?N1?,?V?, ?P,N1,N2?
SRL Model
?PATH?, ?PATH,PLEMMA?,?SUB-CAT?,?PLEMMA?
?HW,PLEMMA?,?PATH,PLEMMA,VOICE?
,?HW,PLEMMA,PTYPE?,?SUB-CAT,PLEMMA?
?SUB-CAT,PLEMMA,POS?,?HW?
Table 7: Log-linear models learned for PP attach-
ment and SRL.
learned by the Log-linear models for PP attach-
ment and SRL.
We should note that it is much faster to do
structure search for the generative Bayesian Net-
work model, as compared to structure search for
the log-linear model. In our implementation, we
did not do any computation reuse between succes-
sive steps of structure search for the Bayesian Net-
work or log-linear models. Structure search took 2
hours for the Bayesian Network and 24 hours for
the log-linear model.
To put our results in the context of previous
work, other results on core arguments using the
same input features have been reported, the best
being 91.4% for an SVM with a degree 2 poly-
nomial kernel (Pradhan et al, 2005a).3 The
highest reported result for independent classifica-
tion of core arguments is 96.0% for a log-linear
model using more than 20 additional basic features
(Toutanova et al, 2005). Therefore our resulting
models with 93.5% and 93.9% accuracy compare
favorably to the SVM model with polynomial ker-
nel and show the importance of structure learning.
4 Comparison to Related Work
Previous work has compared generative and dis-
criminative models having the same structure,
such as the Naive Bayes and Logistic regression
models (Ng and Jordan, 2002; Klein and Man-
ning, 2002) and other models (Klein and Manning,
2002; Johnson, 2001).
3This result is on an older version of Propbank from July
2002.
582
Att
P
N1
N2V
(a) Learned Bayesian Network
for PP attachment.
Role
Sub-
cat Path
Voice
PLem HW
Pos
(b) Learned Bayesian Network
for SRL.
Figure 2: Learned Bayesian Network structures
for PP attachment and SRL.
Bayesian Networks with special structure of the
CPTs ? e.g. decision trees, have been previously
studied in e.g. (Friedman and Goldszmidt, 1996),
but not for NLP tasks and not in comparison to dis-
criminative models. Studies comparing generative
and discriminative models with structure learn-
ing have been previously performed ((Pernkopf
and Bilmes, 2005) and (Grossman and Domingos,
2004)) for other, non-NLP domains. There are
several important algorithmic differences between
our work and that of (Pernkopf and Bilmes, 2005;
Grossman and Domingos, 2004). We detail the
differences here and perform an empirical evalua-
tion of the impact of some of these differences.
Form of the generative models. The genera-
tive models studied in that previous work do not
employ any special form of the conditional prob-
ability tables. Pernkopf and Bilmes (2005) use a
simple smoothing method: fixing the probability
of every event that has a zero relative frequency
estimate to a small fixed . Thus the model does
not take into account information from lower or-
der distributions and has no hyper-parameters that
are being fit. Grossman and Domingos (2004) do
not employ a special form of the CPTs either and
do not mention any kind of smoothing used in the
generative model learning.
Form of the discriminative models. The
works (Pernkopf and Bilmes, 2005; Grossman
and Domingos, 2004) study Bayesian Networks
whose parameters are trained discriminatively (by
maximizing conditional likelihood), as represen-
tatives of discriminative models. We study more
general log-linear models, equivalent to Markov
Random Fields. Our models are more general
in that their parameters do not need to be inter-
pretable as probabilities (sum to 1 and between 0
and 1), and the structures do not need to corre-
spond to Bayes Net structures. For discriminative
classifiers, it is not important that their compo-
nent parameters be interpretable as probabilities;
thus this restriction is probably unnecessary. Like
for the generative models, another major differ-
ence is in the smoothing algorithms. We smooth
the models both by fitting a gaussian prior hyper-
parameter and by incorporating features of subsets
of cliques. Smoothing in (Pernkopf and Bilmes,
2005) is done by substituting zero-valued param-
eters with a small fixed . Grossman and Domin-
gos (2004) employ early stopping using held-out
data which can achieve similar effects to smooth-
ing with a gaussian prior.
To evaluate the importance of the differences
between our algorithm and the ones presented in
these works, and to evaluate the importance of fit-
ting hyper-parameters for smoothing, we imple-
mented a modified version of our structure search.
The modifications were as follows. For Bayes
Net structure learning: (i) no Witten-Bell smooth-
ing is employed in the CPTs, and (ii) no backoffs
to lower-order distributions are considered. The
only smoothing remaining in the CPTs is an inter-
polation with a uniform distribution with a fixed
weight of ? = .1. For discriminative log-linear
model structure learning: (i) the gaussian prior
was fixed to be very weak, serving only to keep the
weights away from infinity (? = 100) and (ii) the
conjunction selection was restricted to correspond
to a Bayes Net structure with no features for sub-
sets of feature conjunctions. Thus the only differ-
ence between the class of our modified discrimina-
tive log-linear models and the class of models con-
sidered in (Pernkopf and Bilmes, 2005; Grossman
and Domingos, 2004) is that we do not restrict the
parameters to be interpretable as probabilities.
The results shown in Table 8 summarize the re-
sults obtained by the modified algorithm on the
two tasks. Both the generative and discriminative
learners suffered a statistically significant (at level
.01) loss in performance. Notably, the log-linear
model for PP attachment performs worse than lo-
gistic regression with better smoothing.
583
PP Attachment Results
Model H-params Test set acc
Bayes Net 0 82.8
Log-linear model 0 81.2
SRL Classification Results
Model H-params Test set acc
Bayes Net 0 92.5
Log-linear model 0 92.7
Table 8: Bayesian Network and Conditional log-
linear model: PP & SRL classification results us-
ing minimal smoothing and no backoff to lower
order distributions.
In summary, our results showed that by learning
the structure for generative models, we can obtain
models which are competitive with or better than
corresponding discriminative models. We also
showed the importance of employing sophisti-
cated smoothing techniques in structure search al-
gorithms for natural language classification tasks.
References
Xavier Carreras and Lu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking. In
Proceedings of ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139.
Michael Collins and James Brooks. 1995. Prepositional at-
tachment through a backed-off model. In Proceedings of
the Third Workshop on Very Large Corpora, pages 27?38.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL, pages 16 ?
23.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of ICML, pages 175?
182.
Stephen Della Pietra, Vincent J. Della Pietra, and John D.
Lafferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(4):380?393.
Nir Friedman and Moises Goldszmidt. 1996. Learning
Bayesian networks with local structure. In Proceeding of
UAI, pages 252?262.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3):245?
288.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. In MSR Technical Report MSR-TR-2001-72.
Daniel Grossman and Pedro Domingos. 2004. Learning
bayesian network classifiers by maximizing conditional
likelihood. In Proceedings of ICML, pages 361?368.
David Heckerman. 1999. A tutorial on learning with bayesian
networks. In Learning in Graphical Models. MIT Press.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proceedings of ACL.
Dan Klein and Christopher Manning. 2002. Conditional
structure versus conditional estimation in NLP models. In
Proceedings of EMNLP.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. 18th In-
ternational Conf. on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.
Andrew McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of UAI.
Andrew Ng and Michael Jordan. 2002. On discriminative vs.
generative classifiers: A comparison of logistic regression
and Naive Bayes. In NIPS 14.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL, pages 295?302.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics.
Judea Pearl. 1988. Probabilistic reasoning in intelligent
systems: Networks of plausible inference. Morgan Kauf-
mann.
Franz Pernkopf and Jeff Bilmes. 2005. Discriminative versus
generative parameter and structure learning of bayesian
network classifiers. In Proceedings of ICML.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005a. Support
vector learning for semantic argument classification. Ma-
chine Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2005b. Semantic role labeling
using different syntactic views. In Proceedings of ACL.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005. The
necessity of syntactic parsing for semantic role labeling.
In Proceedings of IJCAI.
Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew
McCallum. 2004. Classification with hybrid genera-
tive/discriminative models. In Sebastian Thrun, Lawrence
Saul, and Bernhard Scho?lkopf, editors, Advances in Neu-
ral Information Processing Systems 16. MIT Press, Cam-
bridge, MA.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A
maximum entropy model for prepositional phrase attach-
ment. In Workshop on Human Language Technology.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling with
conditional random fields and the perceptron algorithm.
In Proceedings of ACL.
Kristina Toutanova, Dan Klein, and Christopher D. Manning.
2003. Feature-rich part-of-speech tagging with a cyclic
dependency network. In Proceedings of HLT-NAACL.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL.
Bram Vanschoenwinkel and Bernard Manderick. 2003. A
weighted polynomial information gain kernel for resolv-
ing prepositional phrase attachment ambiguities with sup-
port vector machines. In IJCAI.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE Transactions on Infor-
mation Theory, 37,4:1085?1094.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP.
584
Proceedings of the Workshop on Statistical Machine Translation, pages 158?161,
New York City, June 2006. c?2006 Association for Computational Linguistics
Microsoft Research Treelet Translation System:
NAACL 2006 Europarl Evaluation
Arul Menezes, Kristina Toutanova and Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{arulm,kristout,chrisq}@microsoft.com
Abstract
The  Microsoft  Research  translation  system  is  a
syntactically  informed  phrasal  SMT  system  that
uses  a  phrase  translation  model  based  on
dependency treelets and a global reordering model
based  on  the  source  dependency  tree.  These
models  are  combined  with  several  other
knowledge  sources  in  a  log-linear  manner.  The
weights of the individual components in the log-
linear model  are set  by an automatic  parameter-
tuning method.  We give a brief  overview of the
components  of  the  system  and  discuss  our
experience with the Europarl data translating from
English to Spanish.
1. Introduction
The  dependency  treelet  translation  system
developed at MSR is a statistical MT system that
takes  advantage  of  linguistic  tools,  namely  a
source language dependency parser,  as well as a
word alignment component. [1]
To  train  a  translation  system,  we  require  a
sentence-aligned parallel  corpus.  First  the source
side is parsed to obtain dependency trees. Next the
corpus  is  word-aligned,  and  the  source
dependencies  are  projected  onto  the  target
sentences  using  the  word  alignments.  From  the
aligned dependency corpus we extract  all  treelet
translation pairs,  and train an order model and a
bi-lexical dependency model.
To translate, we parse the input sentence, and
employ  a  decoder  to  find  a  combination  and
ordering of treelet translation pairs that cover the
source tree and are optimal according to a set of
models.  In  a  now-common generalization  of  the
classic  noisy-channel  framework,  we  use  a  log-
linear combination of models [2], as in below:
translation?S , F ,? ?=argmax
T {?f ?F ? f f ?S ,T ?}
Such an approach toward translation scoring has
proven  very  effective  in  practice,  as  it  allows  a
translation system to incorporate information from
a  variety  of  probabilistic  or  non-probabilistic
sources.  The weights  ? = {  ?f } are selected by
discriminatively training against held out data.
2. System Details
A brief word on notation: s and t represent source
and target lexical nodes; S and T represent source
and target trees; s and t represent source and target
treelets  (connected  subgraphs  of  the  dependency
tree).  The  expression  ?t? T refers  to  all  the
lexical items in the target language tree T and |T|
refers to the count of lexical items in  T. We use
subscripts to indicate selected words: Tn represents
the n
th
 lexical item in an in-order traversal of T.
2.1. Training
We  use  the  broad  coverage  dependency  parser
NLPWIN  [3]  to  obtain  source  language
dependency  trees,  and  we  use  GIZA++  [4]  to
produce  word  alignments.  The  GIZA++ training
regimen  and  parameters  are  tuned  to  optimize
BLEU [5] scores on held-out data. Using the word
alignments,  we  follow a  set  of  dependency  tree
projection  heuristics  [1]  to  construct  target
dependency  trees,  producing  a  word-aligned
parallel  dependency  tree  corpus.  Treelet
translation pairs are extracted by enumerating all
source treelets (to a maximum size) aligned to a
target treelet.
2.2. Decoding
We use a tree-based decoder, inspired by dynamic
programming. It searches for an approximation of
158
the n-best translations of each subtree of the input
dependency  tree.  Translation  candidates  are
composed from treelet  translation pairs  extracted
from the training corpus. This process is described
in more detail in [1].
2.3. Models
2.3.1. Channel models
We  employ  several  channel  models:  a  direct
maximum likelihood estimate of the probability of
target  given  source,  as  well  as  an  estimate  of
source given target and target given source using
the word-based IBM Model 1 [6]. For MLE, we
use  absolute  discounting  to  smooth  the
probabilities:
PMLE ? t?s ?= c ? s , t ???c ? s ,* ?
Here,  c represents  the  count  of  instances  of  the
treelet pair  ?s, t? in the training corpus, and  ? is
determined empirically.
For Model 1 probabilities we compute the sum
over all possible alignments of the treelet without
normalizing for length. The calculation of source
given  target  is  presented  below;  target  given
source is calculated symmetrically.
PM1? t?s ?=?t?t ?s?s P ? t?s ?
2.3.2. Bilingual n-gram channel models
Traditional  phrasal  SMT systems  are  beset  by a
number of theoretical problems, such as the ad hoc
estimation  of  phrasal  probability,  the  failure  to
model  the  partition  probability,  and  the  tenuous
connection  between  the  phrases  and  the
underlying  word-based  alignment  model.  In
string-based  SMT  systems,  these  problems  are
outweighed by the key role played by phrases in
capturing  ?local?  order.  In  the  absence  of  good
global  ordering  models,  this  has  led  to  an
inexorable  push  towards  longer  and  longer
phrases, resulting in serious practical problems of
scale, without, in the end, obviating the need for a
real global ordering story.
In [13] we discuss these issues in greater detail
and  also  present  our  approach  to  this  problem.
Briefly,  we  take  as  our  basic  unit  the  Minimal
Translation Unit (MTU) which we define as a set
of source and target word pairs such that there are
no word alignment links between distinct MTUs,
and  no  smaller  MTUs  can  be  extracted  without
violating the previous constraint.  In other words,
these are the minimal non-compositional phrases.
We then build models based on n-grams of MTUs
in  source  string,  target  string  and  source
dependency  tree  order.  These  bilingual  n-gram
models  in  combination  with  our  global  ordering
model allow us to use shorter phrases without any
loss  in  quality,  or  alternately  to  improve  quality
while keeping phrase size constant.
As an example,  consider the aligned sentence
pair in Figure 1. There are seven MTUs:
m1 = <we should / hemos>
m2 = <NULL / de>
m3 = <follow / cumplir>
m4 = <the / el>
m5 = <Rio / Rio>
m6 = <agenda / programa>
m7 = <NULL / de>
We can then predict the probability of each MTU
in the context of (a) the previous MTUs in source
order,  (b) the previous MTUs in target order,  or
(c) the ancestor MTUs in the tree. We consider all
of these traversal orders, each acting as a separate
feature function in the log linear combination. For
source and target traversal order we use a trigram
model, and a bigram model for tree order.
2.3.3. Target language models
We  use  both  a  surface  level  trigram  language
model  and a dependency-based bigram language
model  [7],  similar  to  the  bilexical  dependency
modes  used  in  some  English  Treebank  parsers
(e.g. [8]).
Psurf ?T ?=?i=1
?T?
Ptrisurf ?T i?T i?2 ,T i?1 ?
Pbilex ?T ?=?i=1
?T?
Pbidep ?T i?parent ?T i ??
Ptrisurf is a Kneser-Ney smoothed trigram language
model  trained  on  the  target  side  of  the  training
corpus,  and  Pbilex is  a  Kneser-Ney  smoothed
we??2 should??1 follow the??2 Rio??1 agenda?+1
hemos??1 de?+1 cumplir el??1 programa?+1 de??1 R?o?+1
Figure 1: Aligned dependency tree pair, annotated with head-
relative positions
159
bigram language model trained on target language
dependencies  extracted  from the aligned  parallel
dependency tree corpus.
2.3.4. Order model
The  order  model  assigns  a  probability  to  the
position  (pos)  of  each target  node relative  to its
head based on information in both the source and
target trees:
Porder ?order ?T ??S ,T ?=?t?T P ? pos ? t , parent ? t ???S ,T ?
Here, position is modeled in terms of closeness to
the head in the dependency tree. The closest pre-
modifier  of  a  given  head  has  position  -1;  the
closest  post-modifier  has  a  position  1.  Figure  1
shows an example dependency tree pair annotated
with head-relative positions.
We use a small set of features reflecting local
information in the dependency tree to model P(pos
(t,parent(t)) | S, T):
? Lexical items of t and parent(t), the parent of t
in the dependency tree.
? Lexical items of the source nodes aligned to t
and head(t).
? Part-of-speech  ("cat")  of  the  source  nodes
aligned to the head and modifier.
? Head-relative  position  of  the  source  node
aligned to the source modifier. 
These  features  along  with  the  target  feature  are
gathered  from  the  word-aligned  parallel
dependency  tree  corpus  and  used  to  train  a
statistical  model.  In  previous  versions  of  the
system,  we trained a decision tree model  [9].  In
the  current  version,  we  explored  log-linear
models. In addition to providing a different way of
combining  information  from  multiple  features,
log-linear models allow us to model the similarity
among different classes (target positions), which is
advantageous for our task.
 We  implemented  a  method  for  automatic
selection  of  features  and  feature  conjunctions  in
the log-linear model. The method greedily selects
feature  conjunction  templates  that  maximize  the
accuracy  on  a  development  set.  Our  feature
selection  study  showed  that  the  part-of-speech
labels of the source nodes aligned to the head and
the modifier and the head-relative position of the
source  node  corresponding  to  the  modifier  were
the  most  important  features.  It  was  useful  to
concatenate the part-of-speech of the source head
with  every  feature.  This  effectively  achieves
learning  of  separate  movement  models  for  each
source head category. Lexical information on the
pairs  of  head  and  dependent  in  the  source  and
target was also very useful.
To model the similarity among different target
classes  and  to  achieve  pooling  of  data  across
similar classes, we added multiple features of the
target position. These features let our model know,
for  example,  that  position  -5  looks  more  like
position  -6  than  like  position  3.  We  added  a
feature  ?positive?/?negative?  which  is  shared  by
all  positive/negative  positions.  We  also  added  a
feature looking at the displacement of a position in
the target from the corresponding position in the
source  and  features  which  group  the  target
positions  into  bins.  These  features  of  the  target
position are combined with features of the input.
This  model  was  trained  on  the  provided
parallel  corpus.  As  described  in  Section  2.1  we
parsed the source sentences,  and projected target
dependencies.  Each  head-modifier  pair  in  the
resulting target trees constituted a training instance
for the order model.
The  score  computed  by  the  log-linear  order
model is used as a single feature in the overall log-
linear  combination  of  models  (see  Section  1),
whose  parameters  were  optimized  using
MaxBLEU  [2].  This  order  model  replaced  the
decision tree-based model described in [1]. 
We compared  the  decision  tree  model  to  the
log-linear  model  on  predicting  the  position  of  a
modifier  using  reference  parallel  sentences,
independent of the full MT system. The decision
tree  achieved  per  decision  accuracy  of  69%
whereas  the  log-linear  model  achieved  per
decision accuracy of 79%.
1
 In the context of the
full  MT system,  however,  the  new order  model
provided  a  more  modest  improvement  in  the
BLEU score of 0.39%.
2.3.5. Other models
We include two pseudo-models that help balance
certain biases inherent in our other models.
? Treelet  count.  This  feature  is  a  count  of
treelets  used  to  construct  the  candidate.  It
acts as a bias toward translations that use a
smaller  number  of  treelets;  hence  toward
larger  sized  treelets  incorporating  more
context.
? Word count. We also include a count of the
words  in  the  target  sentence.  This  feature
1
 The per-decision accuracy numbers were obtained on
different (random) splits of training and test data.
160
helps  to  offset  the  bias  of  the  target
language model toward shorter sentences.
3. Discussion
We participated in  the English to  Spanish  track,
using  the  supplied  bilingual  data  only.  We used
only the target side of the bilingual corpus for the
target  language  model,  rather  than  the  larger
supplied  language  model.  We  did  find  that
increasing the target language order from 3 to 4
had a noticeable impact on translation quality. It is
likely that a larger target language corpus would
have an impact, but we did not explore this.
BLEU
Baseline treelet system 27.60
Add bilingual MTU models 28.42
Replace DT order model with log-linear model 28.81
Table 1: Results on development set
We found  that  the  addition of  bilingual  n-gram
based  models  had  a  substantial  impact  on
translation  quality.  Adding  these  models  raised
BLEU scores about 0.8%, but anecdotal evidence
suggests  that  human-evaluated  quality  rose  by
much more than the BLEU score difference would
suggest. In general, we felt that in this corpus, due
to the great diversity in translations for the same
source language words and phrases, and given just
one reference translation, BLEU score correlated
rather  poorly  with  human  judgments.  This  was
borne out in the human evaluation of the final test
results.  Humans  ranked  our  system  first  and
second,  in-domain  and  out-of-domain
respectively, even though it was in the middle of a
field of ten systems by BLEU score. Furthermore,
n-gram  channel  models  may  provide  greater
robustness. While our BLEU score dropped 3.61%
on out-of-domain data, the average BLEU score of
the other nine competing systems dropped 5.11%.
4. References
[1] Quirk,  C.,  Menezes,  A.,  and Cherry,  C.,  "Dependency
Tree Translation: Syntactically Informed Phrasal SMT",
Proceedings of ACL 2005, Ann Arbor, MI, USA, 2005.
[2] Och, F.  J.,  and Ney, H.,  "Discriminative  Training and
Maximum  Entropy  Models  for  Statistical  Machine
Translation",  Proceedings  of  ACL  2002,  Philadelphia,
PA, USA, 2002.
[3] Heidorn, G., ?Intelligent writing assistance?, in Dale et
al.  Handbook of Natural Language Processing, Marcel
Dekker, 2000.
[4] Och, F.  J.,  and Ney H.,  "A Systematic Comparison of
Various  Statistical  Alignment  Models",  Computational
Linguistics, 29(1):19-51, March 2003.
[5] Papineni,  K.,  Roukos,  S.,  Ward,  T.,  and  Zhu,  W.-J.,
"BLEU: a method for automatic evaluation of machine
translation",  Proceedings  of  ACL  2002,  Philadelphia,
PA, USA, 2002.
[6] Brown,  P.  F.,  Della Pietra,  S.,  Della Pietra,  V. J.,  and
Mercer, R. L., "The Mathematics of Statistical Machine
Translation:  Parameter  Estimation",  Computational
Linguistics 19(2): 263-311, 1994.
[7] Aue,  A.,  Menezes,  A.,  Moore,  R.,  Quirk,  C.,  and
Ringger,  E.,  "Statistical  Machine  Translation  Using
Labeled Semantic Dependency Graphs." Proceedings of
TMI 2004, Baltimore, MD, USA, 2004.
[8] Collins,  M.,  "Three  generative,  lexicalised  models  for
statistical  parsing",  Proceedings of ACL 1997,  Madrid,
Spain, 1997.
[9] Chickering,  D.M.,  "The  WinMine  Toolkit",  Microsoft
Research  Technical  Report  MSR-TR-2002-103,
Redmond, WA, USA, 2002.
[10] Och,  F.  J.,  Gildea,  D.,  Khudanpur,  S.,  Sarkar,  A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D.,
Eng,  K.,  Jain,  V.,  Jin,  Z.,  and  Radev,  D.,  "A
Smorgasbord  of  Features  for  Statistical  Machine
Translation". Proceedings of HLT/NAACL 2004, Boston,
MA, USA, 2004.
[11] Bender,  O.,  Zens,  R.,  Matsuov,  E.  and  Ney,  H.,
"Alignment  Templates:  the  RWTH  SMT  System".
IWSLT Workshop at INTERSPEECH 2004, Jeju Island,
Korea, 2004.
[12] Och, F. J., "Minimum Error Rate Training for Statistical
Machine  Translation",  Proceedings  of  ACL  2003,
Sapporo, Japan, 2003.
[13] Quirk,  C  and  Menezes,  A,  ?Do  we  need  phrases?
Challenging  the  conventional  wisdom  in  Statistical
Machine  Translation?,  Proceedings  of  HLT/NAACL
2006, New York, NY, USA, 2006
161
The LinGO Redwoods Treebank
Motivation and Preliminary Applications
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten Brants
{oe |kristina |manning |dan}@csli.stanford.edu,
shieber@deas.harvard.edu, brants@parc.xerox.com
Abstract
The LinGO Redwoods initiative is a seed activity in the de-
sign and development of a new type of treebank. While sev-
eral medium- to large-scale treebanks exist for English (and
for other major languages), pre-existing publicly available re-
sources exhibit the following limitations: (i) annotation is
mono-stratal, either encoding topological (phrase structure) or
tectogrammatical (dependency) information, (ii) the depth of
linguistic information recorded is comparatively shallow, (iii)
the design and format of linguistic representation in the tree-
bank hard-wires a small, predefined range of ways in which
information can be extracted from the treebank, and (iv) rep-
resentations in existing treebanks are static and over the (often
year- or decade-long) evolution of a large-scale treebank tend
to fall behind the development of the field. LinGO Redwoods
aims at the development of a novel treebanking methodology,
rich in nature and dynamic both in the ways linguistic data can
be retrieved from the treebank in varying granularity and in the
constant evolution and regular updating of the treebank itself.
Since October 2001, the project is working to build the foun-
dations for this new type of treebank, to develop a basic set of
tools for treebank construction and maintenance, and to con-
struct an initial set of 10,000 annotated trees to be distributed
together with the tools under an open-source license.
1 Why Another (Type of) Treebank?
For the past decade or more, symbolic, linguistically ori-
ented methods and statistical or machine learning ap-
proaches to NLP have often been perceived as incompat-
ible or even competing paradigms. While shallow and
probabilistic processing techniques have produced use-
ful results in many classes of applications, they have not
met the full range of needs for NLP, particularly where
precise interpretation is important, or where the variety
of linguistic expression is large relative to the amount
of training data available. On the other hand, deep
approaches to NLP have only recently achieved broad
enough grammatical coverage and sufficient processing
efficiency to allow the use of precise linguistic grammars
in certain types of real-world applications.
In particular, applications of broad-coverage analyti-
cal grammars for parsing or generation require the use of
sophisticated statistical techniques for resolving ambigu-
ities; the transfer of Head-Driven Phrase Structure Gram-
mar (HPSG) systems into industry, for example, has am-
plified the need for general parse ranking, disambigua-
tion, and robust recovery techniques. We observe general
consensus on the necessity for bridging activities, com-
bining symbolic and stochastic approaches to NLP. But
although we find promising research in stochastic pars-
ing in a number of frameworks, there is a lack of appro-
priately rich and dynamic language corpora for HPSG.
Likewise, stochastic parsing has so far been focussed on
information-extraction-type applications and lacks any
depth of semantic interpretation. The Redwoods initia-
tive is designed to fill in this gap.
In the next section, we present some of the motivation
for the LinGO Redwoods project as a treebank develop-
ment process. Although construction of the treebank is
in its early stages, we present in Section 3 some prelim-
inary results of using the treebank data already acquired
on concrete applications. We show, for instance, that
even simple statistical models of parse ranking trained
on the Redwoods corpus built so far can disambiguate
parses with close to 80% accuracy.
2 A Rich and Dynamic Treebank
The Redwoods treebank is based on open-source HPSG
resources developed by a broad consortium of re-
search groups including researchers at Stanford (USA),
Saarbru?cken (Germany), Cambridge, Edinburgh, and
Sussex (UK), and Tokyo (Japan). Their wide distribution
and common acceptance make the HPSG framework and
resources an excellent anchor point for the Redwoods
treebanking initiative.
The key innovative aspect of the Redwoods ap-
proach to treebanking is the anchoring of all linguis-
tic data captured in the treebank to the HPSG frame-
work and a generally-available broad-coverage gram-
mar of English, the LinGO English Resource Grammar
(Flickinger, 2000) as implemented with the LKB gram-
mar development environment (Copestake, 2002). Un-
like existing treebanks, there is no need to define a (new)
form of grammatical representation specific to the tree-
bank. Instead, the treebank records complete syntacto-
semantic analyses as defined by the LinGO ERG and pro-
vide tools to extract different types of linguistic informa-
tion at varying granularity.
The treebanking environment, building on the [incr
tsdb()] profiling environment (Oepen & Callmeier,
2000), presents annotators, one sentence at a time, with
the full set of analyses produced by the grammar. Using
a pre-existing tree comparison tool in the LKB (similar
in kind to the SRI Cambridge TreeBanker; Carter, 1997),
annotators can quickly navigate through the parse for-
est and identify the correct or preferred analysis in the
current context (or, in rare cases, reject all analyses pro-
posed by the grammar). The tree selection tool presents
users, who need little expert knowledge of the underly-
ing grammar, with a range of basic properties that distin-
guish competing analyses and that are relatively easy to
judge. All disambiguating decisions made by annotators
are recorded in the [incr tsdb()] database and thus become
available for (i) later dynamic extraction from the anno-
tated profile or (ii) dynamic propagation into a more re-
cent profile obtained from re-running a newer version of
the grammar on the same corpus.
Important innovative research aspects in this approach
to treebanking are (i) enabling users of the treebank to
extract information of the type they need and to trans-
form the available representation into a form suited to
their needs and (ii) the ability to update the treebank with
an enhanced version of the grammar in an automated
fashion, viz. by re-applying the disambiguating decisions
on the corpus with an updated version of the grammar.
Depth of Representation and Transformation of In-
formation Internally, the [incr tsdb()] database records
analyses in three different formats, viz. (i) as a deriva-
tion tree composed of identifiers of lexical items and con-
structions used to build the analysis, (ii) as a traditional
phrase structure tree labeled with an inventory of some
fifty atomic labels (of the type ?S?, ?NP?, ?VP? et al), and
(iii) as an underspecified MRS (Copestake, Lascarides,
& Flickinger, 2001) meaning representation. While rep-
resentation (ii) will in many cases be similar to the rep-
resentation found in the Penn Treebank, representation
(iii) subsumes the functor ? argument (or tectogrammati-
cal) structure advocated in the Prague Dependency Tree-
bank or the German TiGer corpus. Most importantly,
however, representation (i) provides all the information
required to replay the full HPSG analysis (using the orig-
inal grammar and one of the open-source HPSG process-
ing environments, e.g., the LKB or PET, which already
have been interfaced to [incr tsdb()]). Using the latter ap-
proach, users of the treebank are enabled to extract infor-
mation in whatever representation they require, simply
by reconstructing full analyses and adapting the exist-
ing mappings (e.g., the inventory of node labels used for
phrase structure trees) to their needs. Likewise, the ex-
isting [incr tsdb()] facilities for comparing across compe-
tence and performance profiles can be deployed to evalu-
ate results of a (stochastic) parse disambiguation system,
essentially using the preferences recorded in the treebank
as a ?gold standard? target for comparison.
Automating Treebank Construction Although a pre-
cise HPSG grammar like the LinGO ERG will typically
assign a small number of analyses to a given sentence,
choosing among a few or sometimes a few dozen read-
ings is time-consuming and error-prone. The project is
exploring two approaches to automating the disambigua-
tion task, (i) seeding lexical selection from a part-of-
speech (POS) tagger and (ii) automated inter-annotator
comparison and assisted resolution of conflicts.
Treebank Maintenance and Evolution One of the
challenging research aspects of the Redwoods initiative
is about developing a methodology for automated up-
dates of the treebank to reflect the continuous evolution
of the underlying linguistic framework and of the LinGO
grammar. Again building on the notion of elementary
linguistic discriminators, we expect to explore the semi-
automatic propagation of recorded disambiguating deci-
sions into newer versions of the parsed corpus. While
it can be assumed that the basic phrase structure inven-
tory and granularity of lexical distinctions have stabilized
to a certain degree, it is not guaranteed that one set of
discriminators will always fully disambiguate a more re-
cent set of analyses for the same utterance (as the gram-
mar may introduce new ambiguity), nor that re-playing
a history of disambiguating decisions will necessarily
identify the correct, preferred analysis for all sentences.
A better understanding of the nature of discriminators
and relations holding among them is expected to provide
the foundations for an update procedure that, ultimately,
should be mostly automated, with minimal manual in-
spection, and which can become part of the regular re-
gression test cycle for the grammar.
Scope and Current State of Seeding Initiative The
first 10,000 trees to be hand-annotated as part of the
kick-off initiative are taken from a domain for which the
English Resource Grammar is known to exhibit broad
and accurate coverage, viz. transcribed face-to-face dia-
logues in an appointment scheduling and travel arrange-
ment domain.1 For the follow-up phase of the project, it
is expected to move into a second domain and text genre,
presumably more formal, edited text taken from newspa-
per text or another widely available on-line source. As
of June 2002, the seeding initiative is well underway.
The integrated treebanking environment, combining [incr
tsdb()] and the LKB tree selection tool, has been estab-
lished and has been deployed in a first iteration of anno-
tating the VerbMobil utterances. The approach to parse
selection through minimal discriminators turned out to
be not hard to learn for a second-year Stanford under-
graduate in linguistics, and allowed completion of the
first iteration in less than ten weeks. Table 1 summarizes
the current Redwoods status.
1Corpora of some 50,000 such utterances are readily available from
the VerbMobil project (Wahlster, 2000) and have already been studied
extensively among researchers world-wide.
2Of the four data sets only VM32 has been double-checked by
an expert grammarian and (almost) completely disambiguated to date;
therefore it exhibits an interestingly higher degree of phrasal ambiguity
in the ?active = 1? subset.
total active = 0 active = 1 active > 1 unannotated
corpus ] ?  ? ] ?  ? ] ?  ? ] ?  ? ] ?  ?
VM6 2422 7?7 4?2 32?9 218 8?0 4?4 9?7 1910 7?0 4?0 7?5 80 10?0 4?8 23?8 214 14?9 4?3 287?5
VM13 1984 8?5 4?0 37?9 175 8?5 4?1 9?9 1491 7?2 3?9 7?5 85 9?9 4?5 22?1 233 14?1 4?2 22?1
VM31 1726 6?2 4?5 22?4 164 7?9 4?6 8?0 1360 6?6 4?5 5?9 61 10?1 4?2 14?5 141 13?5 4?7 201?5
VM32 608 7?4 4?3 25?6 51 10?7 4?3 54?4 551 7?9 4?4 19?0 5 12?2 3?9 27?2 1 21?0 6?1 2220?0
Table 1: Redwoods development status as of June 2002: four sets of transcribed and hand-segmented VerbMobil dialogues have
been annotated. The columns are, from left to right, the total number of sentences (excluding fragments) for which the LinGO
grammar has at least one analysis (?]?), average length (???), lexical and structural ambiguity (?? and ???, respectively), followed
by the last four metrics broken down for the following subsets: sentences (i) for which the annotator rejected all analyses (no active
trees), (ii) where annotation resulted in exactly one preferred analysis (one active tree), (iii) those where full disambiguation was
not accomplished through the first round of annotation (more than one active tree), and (iv) massively ambiguous sentences that
have yet to be annotated.2
3 Early Experimental Results
Development of the treebank has just started. Nonethe-
less, we have performed some preliminary experiments
on concrete applications to motivate the utility of the re-
source being developed. In this section, we describe ex-
periments using the Redwoods treebank to build and test
systems for parse disambiguation. As a component, we
build a tagger for the HPSG lexical tags in the treebank,
and report results on this application as well.
Any linguistic system that allows multiple parses
of strings must address the problem of selecting from
among the admitted parses the preferred one. A variety
of approaches for building statistical models of parse se-
lection are possible. At the simplest end, we might look
only at the lexical type sequence assigned to the words
by each parse and rank the parse based on the likelihood
of that sequence. These lexical types ? the preterminals
in the derivation ? are essentially part-of-speech tags, but
encode considerably finer-grained information about the
words. Well-understood statistical part-of-speech tag-
ging technology is sufficient for this approach.
In order to use more information about the parse,
we might examine the entire derivation of the string.
Most probabilistic parsing research ? including, for ex-
ample, work by by Collins (1997), and Charniak (1997)
? is based on branching process models (Harris, 1963).
The HPSG derivations that the treebank makes available
can be viewed as just such a branching process, and
a stochastic model of the trees can be built as a prob-
abilistic context-free grammar (PCFG) model. Abney
(1997) notes important problems with the soundness of
the approach when a unification-based grammar is ac-
tually determining the derivations, motivating the use
of log-linear models (Agresti, 1990) for parse ranking
that Johnson and colleagues further developed (Johnson,
Geman, Canon, Chi, & Riezler, 1999). These models
can deal with the many interacting dependencies and
the structural complexity found in constraint-based or
unification-based theories of syntax.
Nevertheless, the naive PCFG approach has the advan-
tage of simplicity, so we pursue it and the tagging ap-
proach to parse ranking in these proof-of-concept exper-
iments (more recently, we have begun work on building
log-linear models over HPSG signs (Toutanova & Man-
ning, 2002)). The learned models were used to rank
possible parses of unseen test sentences according to the
probabilities they assign to them. We report parse se-
lection performance as percentage of test sentences for
which the correct parse was highest ranked by the model.
(We restrict attention in the test corpus to sentences that
are ambiguous according to the grammar, that is, for
which the parse selection task is nontrivial.) We examine
four models: an HMM tagging model, a simple PCFG, a
PCFG with grandparent annotation, and a hybrid model
that combines predictions from the PCFG and the tagger.
These models will be described in more detail presently.
The tagger that we have implemented is a standard tri-
gram HMM tagger, defining a joint probability distribu-
tion over the preterminal sequences and yields of these
trees. Trigram probabilities are smoothed by linear in-
terpolation with lower-order models. For comparison,
we present the performance of a unigram tagger and an
upper-bound oracle tagger that knows the true tag se-
quence and scores highest the parses that have the correct
preterminal sequence.
The PCFG models define probability distributions
over the trees of derivational types corresponding to the
HPSG analyses of sentences. A PCFG model has parame-
ters ?i, j for each rule Ai ? ? j in the corresponding con-
text free grammar.3 In our application, the nonterminals
in the PCFG Ai are rules of the HPSG grammar used to
build the parses (such as HEAD-COMPL or HEAD-ADJ).
We set the parameters to maximize the likelihood of the
set of derivation trees for the preferred parses of the sen-
tences in a training set. As noted above, estimating prob-
abilities from local tree counts in the treebank does not
provide a maximum likelihood estimate of the observed
data, as the grammar rules further constrain the possible
derivations. Essentially, we are making an assumption of
context-freeness of rule application that does not hold in
the case of the HPSG grammar. Nonetheless, we can still
build the model and use it to rank parses.
3For an introduction to PCFG grammars see, for example, Manning
& Schu?tze (1999).
As previously noted by other researchers (Charniak &
Caroll, 1994), extending a PCFG with grandparent an-
notation improves the accuracy of the model. We imple-
mented an extended PCFG that conditions each node?s
expansion on its parent in the phrase structure tree. The
extended PCFG (henceforth PCFG-GP) has parameters
P(Ak Ai ? ? j |Ak, Ai) . The resulting grammar can be
viewed as a PCFG whose nonterminals are pairs of the
nonterminals of the original PCFG.
The combined model scores possible parses using
probabilities from the PCFG-GP model together with the
probability of the preterminal sequence of the parse tree
according to a trigram tag sequence model. More specif-
ically, for a tree T ,
Score(t) = log(PPCFG-GP(T )) + ? log(PTRIG(tags(T ))
where PTRIG(tags(T )) is the probability of the sequence
of preterminals t1 ? ? ? tn in T according to a trigram tag
model:
PTRIG(t1 ? ? ? tn) =
?n
i=1
P(ti |ti?1, ti?2)
with appropriate treatment of boundaries. The trigram
probabilities are smoothed as for the HMM tagger. The
combined model is relatively insensitive to the relative
weights of the two component models, as specified by ?;
in any case, exact optimization of this parameter was not
performed. We refer to this model as Combined. The
Combined model is not a sound probabilistic model as it
does not define a probability distribution over parse trees.
It does however provide a crude way to combine ancestor
and left context information.
The second column in Table 2 shows the accuracy
of parse selection using the models described above.
For comparison, a baseline showing the expected perfor-
mance of choosing parses randomly according to a uni-
form distribution is included as the first row. The accu-
racy results are averaged over a ten-fold cross-validation
on the data set summarized in Table 1. The data we used
for this experiment was the set of disambiguated sen-
tences that have exactly one preferred parse (comprising
a total of 5312 sentences). Often the stochastic models
we are considering give the same score to several differ-
ent parses. When a model ranks a set of m parses highest
with equal scores and one of those parses is the preferred
parse in the treebank, we compute the accuracy on this
sentence as 1/m.
Since our approach of defining the probability of anal-
yses using derivation trees is different from the tradi-
tional approach of learning PCFG grammars from phrase
structure trees, a comparison of the two is probably in
order. We tested the model PCFG-GP defined over the
corresponding phrase structure trees and its average ac-
curacy was 65.65% which is much lower than the accu-
racy of the same model over derivation trees (71.73%).
This result suggests that the information about grammar
constructions is very helpful for parse disambiguation.
Method Task
tag sel. parse sel.
Random 90.13% 25.81%
Tagger unigram 96.75% 44.15%
trigram 97.87% 47.74%
oracle 100.00% 54.59%
PCFG simple 97.40% 66.26%
grandparent 97.43% 71.73%
combined 98.08% 74.03%
Table 2: Performance of the HMM and PCFG models for the
tag and parse selection tasks (accuracy).
The results in Table 2 indicate that high disambigua-
tion accuracy can be achieved using very simple statisti-
cal models. The performance of the perfect tagger shows
that, informally speaking, roughly half of the information
necessary to disambiguate parses is available in the lexi-
cal types alone. About half of the remaining information
is recovered by our best method, Combined.
An alternative (more primitive) task is the tagging task
itself. It is interesting to know how much the tagging
task can be improved by perfecting parse disambigua-
tion. With the availability of a parser, we can examine the
accuracy of the tag sequence of the highest scoring parse,
rather than trying to tag the word sequence directly. We
refer to this problem as the tag selection problem, by
analogy with the relation between the parsing problem
and the parse selection problem. The first column of Ta-
ble 2 presents the performance of the models on the tag
selection problem. The results are averaged accuracies
over 10 cross-validation splits of the same corpus as the
previous experiment, and show that parse disambigua-
tion using information beyond the lexical type sequence
slightly improves tag selection performance. Note that
in these experiments, the models are used to rank the tag
sequences of the possible parses and not to find the most
probable tag sequence. Therefore tagging accuracy re-
sults are higher than they would be in the latter case.
Since our corpus has relatively short sentences and low
ambiguity it is interesting to see how much the perfor-
mance degrades as we move to longer and more highly
ambiguous sentences. For this purpose, we report in Ta-
ble 3 the parse ranking accuracy of the Combined model
as a function of the number of possible analyses for sen-
tences. Each row corresponds to a set of sentences with
number of possible analyses greater or equal to the bound
shown in the first column. For example, the first row con-
tains information for the sentences with ambiguity ? 2,
which is all ambiguous sentences. The columns show the
total number of sentences in the set, the expected accu-
racy of guessing at random, and the accuracy of the Com-
bined model. We can see that the parse ranking accuracy
is decreasing quickly and more powerful models will be
needed to achieve good accuracy for highly ambiguous
sentences.
Despite several differences in corpus size and compo-
Analyses Sentences Random Combined
? 2 3824 25.81% 74.03%
? 5 1789 9.66% 59.64%
? 10 1027 5.33% 51.61%
? 20 525 3.03% 45.33%
Table 3: Parse ranking accuracy by number of possible parses.
sition, it is perhaps nevertheless useful to compare this
work with other work on parse selection for unification-
based grammars. Johnson et al (1999) estimate a
Stochastic Unification Based Grammar (SUBG) using a
log-linear model. The features they include in the model
are not limited to production rule features but also ad-
junct and argument and other linguistically motivated
features. On a dataset of 540 sentences (total training
and test set) from a Verbmobil corpus they report parse
disambiguation accuracy of 58.7% given a baseline accu-
racy for choosing at random of 9.7%. The random base-
line is much lower than ours for the full data set, but it is
comparable for the random baseline for sentences with
more than 5 analyses. The accuracy of our Combined
model for these sentences is 59.64%, so the accuracies
of the two models seem fairly similar.
4 Related Work
To the best of our knowledge, no prior research has
been conducted exploring the linguistic depth, flexibil-
ity in available information, and dynamic nature of tree-
banks that we have proposed. Earlier work on building
corpora of hand-selected analyses relative to an exist-
ing broad-coverage grammar was carried out at Xerox
PARC, SRI Cambridge, and Microsoft Research. As all
these resources are tuned to proprietary grammars and
analysis engines, the resulting treebanks are not publicly
available, nor have reported research results been repro-
ducible. Yet, especially in light of the successful LinGO
open-source repository, it seems vital that both the tree-
bank and associated processing schemes and stochastic
models be available to the general (academic) public. An
on-going initiative at Rijksuniversiteit Groningen (NL) is
developing a treebank of dependency structures (Mullen,
Malouf, & Noord, 2001), derived from an HPSG-like
grammar of Dutch (Bouma, Noord, & Malouf, 2001).
The general approach resembles the Redwoods initiative
(specifically the discriminator-based method of tree se-
lection; the LKB tree comparison tool was originally de-
veloped by Malouf, after all), but it provides only a sin-
gle stratum of representation, and has no provision for
evolving analyses in tandem with the grammar. Dipper
(2000) presents the application of a broad-coverage LFG
grammar for German to constructing tectogrammatical
structures for the TiGer corpus. The approach is similar
to the Groningen framework, and shares its limitations.
References
Abney, S. P. (1997). Stochastic attribute-value grammars.
Computational Linguistics, 23, 597 ? 618.
Agresti, A. (1990). Categorical data analysis. John Wiley &
Sons.
Bouma, G., Noord, G. van, & Malouf, R. (2001).
Alpino. Wide-coverage computational analysis of Dutch. In
W. Daelemans, K. Sima-an, J. Veenstra, & J. Zavrel (Eds.),
Computational linguistics in the Netherlands (pp. 45 ? 59).
Amsterdam, The Netherlands: Rodopi.
Carter, D. (1997). The TreeBanker. A tool for supervised
training of parsed corpora. In Proceedings of the Workshop
on Computational Environments for Grammar Development
and Linguistic Engineering. Madrid, Spain.
Charniak, E. (1997). Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the Four-
teenth National Conference on Artificial Intelligence (pp.
598 ? 603). Providence, RI.
Charniak, E., & Caroll, G. (1994). Context-sensitive statistics
for improved grammatical language models. In Proceedings
of the Twelth National Conference on Artificial Intelligence
(pp. 742 ? 747). Seattle, WA.
Collins, M. J. (1997). Three generative, lexicalised models for
statistical parsing. In Proceedings of the 35th Meeting of
the Association for Computational Linguistics and the 7th
Conference of the European Chapter of the ACL (pp. 16 ?
23). Madrid, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Copestake, A., Lascarides, A., & Flickinger, D. (2001). An
algebra for semantic construction in constraint-based gram-
mars. In Proceedings of the 39th Meeting of the Association
for Computational Linguistics. Toulouse, France.
Dipper, S. (2000). Grammar-based corpus annotation. In
Workshop on linguistically interpreted corpora LINC-2000
(pp. 56 ? 64). Luxembourg.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with HPSG), 15 ? 28.
Harris, T. E. (1963). The theory of branching processes.
Berlin, Germany: Springer.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Associa-
tion for Computational Linguistics (pp. 535 ? 541). College
Park, MD.
Manning, C. D., & Schu?tze, H. (1999). Foundations of statis-
tical Natural Language Processing. Cambridge, MA: MIT
Press.
Mullen, T., Malouf, R., & Noord, G. van. (2001). Statistical
parsing of Dutch using Maximum Entropy models with fea-
ture merging. In Proceedings of the Natural Language Pro-
cessing Pacific Rim Symposium. Tokyo, Japan.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased compo-
nent comparability and exchange. In Proceedings of the 6th
International Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Toutanova, K., & Manning, C. D. (2002). Feature selection
for a rich HPSG grammar using decision trees. In Proceed-
ings of the sixth conference on natural language learning
(CoNLL-2002). Taipei.
Wahlster, W. (Ed.). (2000). Verbmobil. Foundations of speech-
to-speech translation. Berlin, Germany: Springer.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network
Kristina Toutanova Dan Klein
Computer Science Dept. Computer Science Dept.
Stanford University Stanford University
Stanford, CA 94305-9040 Stanford, CA 94305-9040
kristina@cs.stanford.edu klein@cs.stanford.edu
Christopher D. Manning Yoram Singer
Computer Science Dept. School of Computer Science
Stanford University The Hebrew University
Stanford, CA 94305-9040 Jerusalem 91904, Israel
manning@stanford.edu singer@cs.huji.ac.il
Abstract
We present a new part-of-speech tagger that
demonstrates the following ideas: (i) explicit
use of both preceding and following tag con-
texts via a dependency network representa-
tion, (ii) broad use of lexical features, includ-
ing jointly conditioning on multiple consecu-
tive words, (iii) effective use of priors in con-
ditional loglinear models, and (iv) fine-grained
modeling of unknown word features. Using
these ideas together, the resulting tagger gives
a 97.24% accuracy on the Penn Treebank WSJ,
an error reduction of 4.4% on the best previous
single automatically learned tagging result.
1 Introduction
Almost all approaches to sequence problems such as part-
of-speech tagging take a unidirectional approach to con-
ditioning inference along the sequence. Regardless of
whether one is using HMMs, maximum entropy condi-
tional sequence models, or other techniques like decision
trees, most systems work in one direction through the
sequence (normally left to right, but occasionally right
to left, e.g., Church (1988)). There are a few excep-
tions, such as Brill?s transformation-based learning (Brill,
1995), but most of the best known and most successful
approaches of recent years have been unidirectional.
Most sequence models can be seen as chaining to-
gether the scores or decisions from successive local mod-
els to form a global model for an entire sequence. Clearly
the identity of a tag is correlated with both past and future
tags? identities. However, in the unidirectional (causal)
case, only one direction of influence is explicitly consid-
ered at each local point. For example, in a left-to-right
first-order HMM, the current tag t0 is predicted based on
the previous tag t?1 (and the current word).1 The back-
ward interaction between t0 and the next tag t+1 shows
up implicitly later, when t+1 is generated in turn. While
unidirectional models are therefore able to capture both
directions of influence, there are good reasons for sus-
pecting that it would be advantageous to make informa-
tion from both directions explicitly available for condi-
tioning at each local point in the model: (i) because of
smoothing and interactions with other modeled features,
terms like P(t0|t+1, . . .) might give a sharp estimate of t0
even when terms like P(t+1|t0, . . .) do not, and (ii) jointly
considering the left and right context together might be
especially revealing. In this paper we exploit this idea,
using dependency networks, with a series of local con-
ditional loglinear (aka maximum entropy or multiclass
logistic regression) models as one way of providing ef-
ficient bidirectional inference.
Secondly, while all taggers use lexical information,
and, indeed, it is well-known that lexical probabilities
are much more revealing than tag sequence probabilities
(Charniak et al, 1993), most taggers make quite limited
use of lexical probabilities (compared with, for example,
the bilexical probabilities commonly used in current sta-
tistical parsers). While modern taggers may be more prin-
cipled than the classic CLAWS tagger (Marshall, 1987),
they are in some respects inferior in their use of lexical
information: CLAWS, through its IDIOMTAG module,
categorically captured many important, correct taggings
of frequent idiomatic word sequences. In this work, we
incorporate appropriate multiword feature templates so
that such facts can be learned and used automatically by
1Rather than subscripting all variables with a position index,
we use a hopefully clearer relative notation, where t0 denotes
the current position and t?n and t+n are left and right context
tags, and similarly for words.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 173-180
                                                         Proceedings of HLT-NAACL 2003
w1 w2 w3 . . . . wn
t1 t2 t3 tn
(a) Left-to-Right CMM
w1 w2 w3 . . . . wn
t1 t2 t3 tn
(b) Right-to-Left CMM
w1 w2 w3 . . . . wn
t1 t2 t3 tn
(c) Bidirectional Dependency Network
Figure 1: Dependency networks: (a) the (standard) left-to-right
first-order CMM, (b) the (reversed) right-to-left CMM, and (c)
the bidirectional dependency network.
the model.
Having expressive templates leads to a large number
of features, but we show that by suitable use of a prior
(i.e., regularization) in the conditional loglinear model ?
something not used by previous maximum entropy tag-
gers ? many such features can be added with an overall
positive effect on the model. Indeed, as for the voted per-
ceptron of Collins (2002), we can get performance gains
by reducing the support threshold for features to be in-
cluded in the model. Combining all these ideas, together
with a few additional handcrafted unknown word fea-
tures, gives us a part-of-speech tagger with a per-position
tag accuracy of 97.24%, and a whole-sentence correct
rate of 56.34% on Penn Treebank WSJ data. This is the
best automatically learned part-of-speech tagging result
known to us, representing an error reduction of 4.4% on
the model presented in Collins (2002), using the same
data splits, and a larger error reduction of 12.1% from the
more similar best previous loglinear model in Toutanova
and Manning (2000).
2 Bidirectional Dependency Networks
When building probabilistic models for tag sequences,
we often decompose the global probability of sequences
using a directed graphical model (e.g., an HMM (Brants,
2000) or a conditional Markov model (CMM) (Ratna-
parkhi, 1996)). In such models, the probability assigned
to a tagged sequence of words x = ?t, w? is the product
of a sequence of local portions of the graphical model,
one from each time slice. For example, in the left-to-right
CMM shown in figure 1(a),
P(t, w) =
?
i
P(ti |ti?1, wi )
That is, the replicated structure is a local model
P(t0|t?1, w0).2 Of course, if there are too many con-
ditioned quantities, these local models may have to be
estimated in some sophisticated way; it is typical in tag-
ging to populate these models with little maximum en-
tropy models. For example, we might populate a model
for P(t0|t?1, w0) with a maxent model of the form:
P?(t0|t?1, w0) =
exp(??t0,t?1? + ??t0,w0?)
?
t ?0
exp(??t ?0,t?1? + ??t ?0,w0?)
In this case, the w0 and t?1 can have joint effects on t0, but
there are not joint features involving all three variables
(though there could have been such features). We say that
this model uses the feature templates ?t0, t?1? (previous
tag features) and ?t0, w0? (current word features).
Clearly, both the preceding tag t?1 and following tag
t+1 carry useful information about a current tag t0. Unidi-
rectional models do not ignore this influence; in the case
of a left-to-right CMM, the influence of t?1 on t0 is ex-
plicit in the P(t0|t?1, w0) local model, while the influ-
ence of t+1 on t0 is implicit in the local model at the next
position (via P(t+1|t0, w+1)). The situation is reversed
for the right-to-left CMM in figure 1(b).
From a seat-of-the-pants machine learning perspective,
when building a classifier to label the tag at a certain posi-
tion, the obvious thing to do is to explicitly include in the
local model all predictive features, no matter on which
side of the target position they lie. There are two good
formal reasons to expect that a model explicitly condi-
tioning on both sides at each position, like figure 1(c)
could be advantageous. First, because of smoothing
effects and interaction with other conditioning features
(like the words), left-to-right factors like P(t0|t?1, w0)
do not always suffice when t0 is implicitly needed to de-
termine t?1. For example, consider a case of observation
bias (Klein and Manning, 2002) for a first-order left-to-
right CMM. The word to has only one tag (TO) in the PTB
tag set. The TO tag is often preceded by nouns, but rarely
by modals (MD). In a sequence will to fight, that trend
indicates that will should be a noun rather than a modal
verb. However, that effect is completely lost in a CMM
like (a): P(twill |will, ?star t?) prefers the modal tagging,
and P(TO|to, twill ) is roughly 1 regardless of twill . While
the model has an arrow between the two tag positions,
that path of influence is severed.3 The same problem ex-
ists in the other direction. If we use the symmetric right-
2Throughout this paper we assume that enough boundary
symbols always exist that we can ignore the differences which
would otherwise exist at the initial and final few positions.
3Despite use of names like ?label bias? (Lafferty et al, 2001)
or ?observation bias?, these effects are really just unwanted
explaining-away effects (Cowell et al, 1999, 19), where two
nodes which are not actually in causal competition have been
modeled as if they were.
A B A B A B
(a) (b) (c)
Figure 2: Simple dependency nets: (a) the Bayes? net for
P(A)P(B|A), (b) the Bayes? net for P(A|B)P(B), (c) a bidi-
rectional net with models of P(A|B) and P(B|A), which is not
a Bayes? net.
to-left model, fight will receive its more common noun
tagging by symmetric reasoning. However, the bidirec-
tional model (c) discussed in the next section makes both
directions available for conditioning at all locations, us-
ing replicated models of P(t0|t?1, t+1, w0), and will be
able to get this example correct.4
2.1 Semantics of Dependency Networks
While the structures in figure 1(a) and (b) are well-
understood graphical models with well-known semantics,
figure 1(c) is not a standard Bayes? net, precisely because
the graph has cycles. Rather, it is a more general de-
pendency network (Heckerman et al, 2000). Each node
represents a random variable along with a local condi-
tional probability model of that variable, conditioned on
the source variables of all incoming arcs. In this sense,
the semantics are the same as for standard Bayes? nets.
However, because the graph is cyclic, the net does not
correspond to a proper factorization of a large joint prob-
ability estimate into local conditional factors. Consider
the two-node cases shown in figure 2. Formally, for the
net in (a), we can write P(a, b) = P(a)P(b|a). For (b)
we write P(a, b) = P(b)P(a|b). However, in (c), the
nodes A and B carry the information P(a|b) and P(b|a)
respectively. The chain rule doesn?t allow us to recon-
struct P(a, b) by multiplying these two quantities. Un-
der appropriate conditions, we could reconstruct P(a, b)
from these quantities using Gibbs sampling, and, in gen-
eral, that is the best we can do. However, while recon-
structing the joint probabilities from these local condi-
tional probabilities may be difficult, estimating the local
probabilities themselves is no harder than it is for acyclic
models: we take observations of the local environments
and use any maximum likelihood estimation method we
desire. In our experiments, we used local maxent models,
but if the event space allowed, (smoothed) relative counts
would do.
4The effect of indirect influence being weaker than direct in-
fluence is more pronounced for conditionally structured models,
but is potentially an issue even with a simple HMM. The prob-
abilistic models for basic left-to-right and right-to-left HMMs
with emissions on their states can be shown to be equivalent us-
ing Bayes? rule on the transitions, provided start and end sym-
bols are modeled. However, this equivalence is violated in prac-
tice by the addition of smoothing.
function bestScore()
return bestScoreSub(n + 2, ?end, end, end?);
function bestScoreSub(i + 1, ?ti?1, ti , ti+1?)
% memoization
if (cached(i + 1, ?ti?1, ti , ti+1?))
return cache(i + 1, ?ti?1, ti , ti+1?);
% left boundary case
if (i = ?1)
if (?ti?1, ti , ti+1? == ?star t, star t, star t?)
return 1;
else
return 0;
% recursive case
return maxti?2 bestScoreSub(i, ?ti?2, ti?1, ti ?)?
P(ti |ti?1, ti+1, wi );
Figure 3: Pseudocode for polynomial inference for the first-
order bidirectional CMM (memoized version).
2.2 Inference for Linear Dependency Networks
Cyclic or not, we can view the product of local probabil-
ities from a dependency network as a score:
score(x) =
?
i
P(xi |Pa(xi ))
where Pa(xi ) are the nodes with arcs to the node xi . In the
case of an acyclic model, this score will be the joint prob-
ability of the event x , P(x). In the general case, it will not
be. However, we can still ask for the event, in this case the
tag sequence, with the highest score. For dependency net-
works like those in figure 1, an adaptation of the Viterbi
algorithm can be used to find the maximizing sequence
in polynomial time. Figure 3 gives pseudocode for the
concrete case of the network in figure 1(d); the general
case is similar, and is in fact just a max-plus version of
standard inference algorithms for Bayes? nets (Cowell et
al., 1999, 97). In essence, there is no difference between
inference on this network and a second-order left-to-right
CMM or HMM. The only difference is that, when the
Markov window is at a position i , rather than receiving
the score for P(ti |ti?1, ti?2, wi ), one receives the score
for P(ti?1|ti , ti?2, wi?1).
There are some foundational issues worth mention-
ing. As discussed previously, the maximum scoring se-
quence need not be the sequence with maximum likeli-
hood according to the model. There is therefore a worry
with these models about a kind of ?collusion? where the
model locks onto conditionally consistent but jointly un-
likely sequences. Consider the two-node network in fig-
ure 2(c). If we have the following distribution of ob-
servations (in the form ab) ?11, 11, 11, 12, 21, 33?, then
clearly the most likely state of the network is 11. How-
ever, the score of 11 is P(a = 1|b = 1)P(b = 1|a = 1)
= 3/4 ? 3/4 = 9/16, while the score of 33 is 1. An ad-
ditional related problem is that the training set loss (sum
of negative logarithms of the sequence scores) does not
bound the training set error (0/1 loss on sequences) from
Data Set Sect?ns Sent. Tokens Unkn
Training 0?18 38,219 912,344 0
Develop 19?21 5,527 131,768 4,467
Test 22?24 5,462 129,654 3,649
Table 1: Data set splits used.
above. Consider the following training set, for the same
network, with each entire data point considered as a label:
?11, 22?. The relative-frequency model assigns loss 0 to
both training examples, but cannot do better than 50%
error in regenerating the training data labels. These is-
sues are further discussed in Heckerman et al (2000).
Preliminary work of ours suggests that practical use of
dependency networks is not in general immune to these
theoretical concerns: a dependency network can choose a
sequence model that is bidirectionally very consistent but
does not match the data very well. However, this problem
does not appear to have prevented the networks from per-
forming well on the tagging problem, probably because
features linking tags and observations are generally much
sharper discriminators than tag sequence features.
It is useful to contrast this framework with the con-
ditional random fields of Lafferty et al (2001). The
CRF approach uses similar local features, but rather than
chaining together local models, they construct a sin-
gle, globally normalized model. The principal advan-
tage of the dependency network approach is that advan-
tageous bidirectional effects can be obtained without the
extremely expensive global training required for CRFs.
To summarize, we draw a dependency network in
which each node has as neighbors all the other nodes
that we would like to have influence it directly. Each
node?s neighborhood is then considered in isolation and
a local model is trained to maximize the conditional like-
lihood over the training data of that node. At test time,
the sequence with the highest product of local conditional
scores is calculated and returned. We can always find the
exact maximizing sequence, but only in the case of an
acyclic net is it guaranteed to be the maximum likelihood
sequence.
3 Experiments
The part of speech tagged data used in our experiments is
the Wall Street Journal data from Penn Treebank III (Mar-
cus et al, 1994). We extracted tagged sentences from the
parse trees.5 We split the data into training, development,
and test sets as in (Collins, 2002). Table 1 lists character-
5Note that these tags (and sentences) are not identical to
those obtained from the tagged/pos directories of the same disk:
hundreds of tags in the RB/RP/IN set were changed to be more
consistent in the parsed/mrg version. Maybe we were the last to
discover this, but we?ve never seen it in print.
istics of the three splits.6 Except where indicated for the
model BEST, all results are on the development set.
One innovation in our reporting of results is that we
present whole-sentence accuracy numbers as well as the
traditional per-tag accuracy measure (over all tokens,
even unambiguous ones). This is the quantity that most
sequence models attempt to maximize (and has been mo-
tivated over doing per-state optimization as being more
useful for subsequent linguistic processing: one wants to
find a coherent sentence interpretation). Further, while
some tag errors matter much more than others, to a first
cut getting a single tag wrong in many of the more com-
mon ways (e.g., proper noun vs. common noun; noun vs.
verb) would lead to errors in a subsequent processor such
as an information extraction system or a parser that would
greatly degrade results for the entire sentence. Finally,
the fact that the measure has much more dynamic range
has some appeal when reporting tagging results.
The per-state models in this paper are log-linear mod-
els, building upon the models in (Ratnaparkhi, 1996) and
(Toutanova and Manning, 2000), though some models are
in fact strictly simpler. The features in the models are
defined using templates; there are different templates for
rare words aimed at learning the correct tags for unknown
words.7 We present the results of three classes of experi-
ments: experiments with directionality, experiments with
lexicalization, and experiments with smoothing.
3.1 Experiments with Directionality
In this section, we report experiments using log-linear
CMMs to populate nets with various structures, exploring
the relative value of neighboring words? tags. Table 2 lists
the discussed networks. All networks have the same ver-
tical feature templates: ?t0, w0? features for known words
and various ?t0, ? (w1n)? word signature features for all
words, known or not, including spelling and capitaliza-
tion features (see section 3.3).
Just this vertical conditioning gives an accuracy of
93.69% (denoted as ?Baseline? in table 2).8 Condition-
6Tagger results are only comparable when tested not only on
the same data and tag set, but with the same amount of training
data. Brants (2000) illustrates very clearly how tagging perfor-
mance increases as training set size grows, largely because the
percentage of unknown words decreases while system perfor-
mance on them increases (they become increasingly restricted
as to word class).
7Except where otherwise stated, a count cutoff of 2 was used
for common word features and 35 for rare word features (tem-
plates need a support set strictly greater in size than the cutoff
before they are included in the model).
8Charniak et al (1993) noted that such a simple model got
90.25%, but this was with no unknown word model beyond
a prior distribution over tags. Abney et al (1999) raise this
baseline to 92.34%, and with our sophisticated unknown word
model, it gets even higher. The large number of unambiguous
tokens and ones with very skewed distributions make the base-
Model Feature Templates? Features Sentence Token Unkn. Word
Accuracy Accuracy Accuracy
Baseline ? 56,805 26.74% 93.69% 82.61%
L ?t0, t?1? 27,474 41.89% 95.79% 85.49%
R ?t0, t+1? 27,648 36.31% 95.14% 85.65%
L+L2 ?t0, t?1?, ?t0, t?2? 32,935 44.04% 96.05% 85.92%
R+R2 ?t0, t+1?, ?t0, t+2? 33,423 37.20% 95.25% 84.49%
L+R ?t0, t?1?, ?t0, t+1? 32,610 49.50% 96.57% 87.15%
LL ?t0, t?1, t?2? 45,532 44.60% 96.10% 86.48%
RR ?t0, t+1, t+2? 45,446 38.41% 95.40% 85.58%
LR ?t0, t?1, t+1? 45,478 49.30% 96.55% 87.26%
L+LL+LLL ?t0, t?1?, ?t0, t?1, t?2?, ?t0, t?1, t?2, t?3? 118,752 45.14% 96.20% 86.52%
R+LR+LLR ?t0, t+1?, ?t0, t?1, t+1?, ?t0, t?1, t?2, t+1? 115,790 51.69% 96.77% 87.91%
L+LL+LR+RR+R ?t0, t?1?, ?t0, t?1, t?2?, ?t0, t?1, t+1?, ?t0, t+1?, ?t0, t+1, t+2? 81,049 53.23% 96.92% 87.91%
Table 2: Tagging accuracy on the development set with different sequence feature templates. ?All models include the same vertical
word-tag features (?t0, w0? and various ?t0, ? (w1n)?), though the baseline uses a lower cutoff for these features.
Model Feature Templates Support Features Sentence Token Unknown
Cutoff Accuracy Accuracy Accuracy
BASELINE ?t0, w0? 2 6,501 1.63% 60.16% 82.98%
?t0, w0? 0 56,805 26.74% 93.69% 82.61%
3W ?t0, w0?, ?t0, w?1?, ?t0, w+1? 2 239,767 48.27% 96.57% 86.78%
3W+TAGS tag sequences, ?t0, w0?, ?t0, w?1?, ?t0, w+1? 2 263,160 53.83% 97.02% 88.05%
BEST see text 2 460,552 55.31% 97.15% 88.61%
Table 3: Tagging accuracy with different lexical feature templates on the development set.
Model Feature Templates Support Features Sentence Token Unknown
Cutoff Accuracy Accuracy Accuracy
BEST see text 2 460,552 56.34% 97.24% 89.04%
Table 4: Final tagging accuracy for the best model on the test set.
ing on the previous tag as well (model L, ?t0, t?1? fea-
tures) gives 95.79%. The reverse, model R, using the
next tag instead, is slightly inferior at 95.14%. Model
L+R, using both tags simultaneously (but with only the
individual-direction features) gives a much better accu-
racy of 96.57%. Since this model has roughly twice as
many tag-tag features, the fact that it outperforms the uni-
directional models is not by itself compelling evidence
for using bidirectional networks. However, it also out-
performs model L+L2 which adds the ?t0, t?2? second-
previous word features instead of next word features,
which gives only 96.05% (and R+R2 gives 95.25%). We
conclude that, if one wishes to condition on two neigh-
boring nodes (using two sets of 2-tag features), the sym-
metric bidirectional model is superior.
High-performance taggers typically also include joint
three-tag counts in some way, either as tag trigrams
(Brants, 2000) or tag-triple features (Ratnaparkhi, 1996,
Toutanova and Manning, 2000). Models LL, RR, and CR
use only the vertical features and a single set of tag-triple
features: the left tags (t?2, t?1 and t0), right tags (t0, t+1,
t+2), or centered tags (t?1, t0, t+1) respectively. Again,
with roughly equivalent feature sets, the left context is
better than the right, and the centered context is better
than either unidirectional context.
line for this task high, while substantial annotator noise creates
an unknown upper bound on the task.
3.2 Lexicalization
Lexicalization has been a key factor in the advance of
statistical parsing models, but has been less exploited
for tagging. Words surrounding the current word have
been occasionally used in taggers, such as (Ratnaparkhi,
1996), Brill?s transformation based tagger (Brill, 1995),
and the HMM model of Lee et al (2000), but neverthe-
less, the only lexicalization consistently included in tag-
ging models is the dependence of the part of speech tag
of a word on the word itself.
In maximum entropy models, joint features which look
at surrounding words and their tags, as well as joint fea-
tures of the current word and surrounding words are in
principle straightforward additions, but have not been in-
corporated into previous models. We have found these
features to be very useful. We explore here lexicaliza-
tion both alone and in combination with preceding and
following tag histories.
Table 3 shows the development set accuracy of several
models with various lexical features. All models use the
same rare word features as the models in Table 2. The
first two rows show a baseline model using the current
word only. The count cutoff for this feature was 0 in the
first model and 2 for the model in the second row. As
there are no tag sequence features in these models, the ac-
curacy drops significantly if a higher cutoff is used (from
a per tag accuracy of about 93.7% to only 60.2%).
The third row shows a model where a tag is de-
cided solely by the three words centered at the tag po-
sition (3W). As far as we are aware, models of this
sort have not been explored previously, but its accu-
racy is surprisingly high: despite having no sequence
model at all, it is more accurate than a model which uses
standard tag fourgram HMM features (?t0, w0?, ?t0, t?1?,
?t0, t?1, t?2?, ?t0, t?1, t?2, t?3?, shown in Table 2, model
L+LL+LLL).
The fourth and fifth rows show models with bi-
directional tagging features. The fourth model
(3W+TAGS) uses the same tag sequence features as
the last model in Table 2 (?t0, t?1?, ?t0, t?1, t?2?,
?t0, t?1, t+1?, ?t0, t+1?, ?t0, t+1, t+2?) and current, previ-
ous, and next word. The last model has in ad-
dition the feature templates ?t0, w0, t?1?, ?t0, w0, t+1?,
?t0, w?1, w0?, and ?t0, w0, w+1?, and includes the im-
provements in unknown word modeling discussed in sec-
tion 3.3.9 We call this model BEST. BEST has a to-
ken accuracy on the final test set of 97.24% and a sen-
tence accuracy of 56.34% (see Table 4). A 95% confi-
dence interval for the accuracy (using a binomial model)
is (97.15%, 97.33%).
In order to understand the gains from using right con-
text tags and more lexicalization, let us look at an exam-
ple of an error that the enriched models learn not to make.
An interesting example of a common tagging error of the
simpler models which could be corrected by a determinis-
tic fixup rule of the kind used in the IDIOMTAG module
of (Marshall, 1987) is the expression as X as (often, as
far as). This should be tagged as/RB X/{RB,JJ} as/IN in
the Penn Treebank. A model using only current word and
two left tags (model L+L2 in Table 2), made 87 errors on
this expression, tagging it as/IN X as/IN ? since the tag
sequence probabilities do not give strong reasons to dis-
prefer the most common tagging of as (it is tagged as IN
over 80% of the time). However, the model 3W+TAGS,
which uses two right tags and the two surrounding words
in addition, made only 8 errors of this kind, and model
BEST made only 6 errors.
3.3 Unknown word features
Most of the models presented here use a set of un-
known word features basically inherited from (Ratna-
parkhi, 1996), which include using character n-gram pre-
fixes and suffixes (for n up to 4), and detectors for a few
other prominent features of words, such as capitaliza-
tion, hyphens, and numbers. Doing error analysis on un-
known words on a simple tagging model (with ?t0, t?1?,
?t0, t?1, t?2?, and ?w0, t0? features) suggested several ad-
ditional specialized features that can usefully improve
9Thede and Harper (1999) use ?t?1, t0, w0? templates in
their ?full-second order? HMM, achieving an accuracy of
96.86%. Here we can add the opposite tiling and other features.
Smoothed Features Sentence Token Unk. W.
Accuracy Acc. Acc.
yes 45,532 44.60% 96.10% 86.48%
no 45,532 42.81% 95.88% 83.08%
yes 292,649 54.88% 97.10% 88.20%
no 292,649 48.85% 96.54% 85.20%
Table 5: Accuracy with and without quadratic regularization.
performance. By far the most significant is a crude com-
pany name detector which marks capitalized words fol-
lowed within 3 words by a company name suffix like Co.
or Inc. This suggests that further gains could be made by
incorporating a good named entity recognizer as a prepro-
cessor to the tagger (reversing the most common order of
processing in pipelined systems!), and is a good example
of something that can only be done when using a condi-
tional model. Minor gains come from a few additional
features: an allcaps feature, and a conjunction feature of
words that are capitalized and have a digit and a dash in
them (such words are normally common nouns, such as
CFC-12 or F/A-18). We also found it advantageous to
use prefixes and suffixes of length up to 10. Together
with the larger templates, these features contribute to our
unknown word accuracies being higher than those of pre-
viously reported taggers.
3.4 Smoothing
With so many features in the model, overtraining is a dis-
tinct possibility when using pure maximum likelihood es-
timation. We avoid this by using a Gaussian prior (aka
quadratic regularization or quadratic penalization) which
resists high feature weights unless they produce great
score gain. The regularized objective F is:
F(?) =
?
i
log(P?(ti |w, t)) +
?n
j=1
?2j
2? 2
Since we use a conjugate-gradient procedure to maximize
the data likelihood, the addition of a penalty term is eas-
ily incorporated. Both the total size of the penalty and
the partial derivatives with repsect to each ? j are triv-
ial to compute; these are added to the log-likelihood and
log-likelihood derivatives, and the penalized optimization
procedes without further modification.
We have not extensively experimented with the value
of ? 2 ? which can even be set differently for different pa-
rameters or parameter classes. All the results in this paper
use a constant ? 2 = 0.5, so that the denominator disap-
pears in the above expression. Experiments on a simple
model with ? made an order of magnitude higher or lower
both resulted in worse performance than with ? 2 = 0.5.
Our experiments show that quadratic regularization
is very effective in improving the generalization perfor-
mance of tagging models, mostly by increasing the num-
ber of features which could usefully be incorporated. The
Tagger Support cutoff Accuracy
Collins (2002) 0 96.60%
5 96.72%
Model 3W+TAGS variant 1 96.97%
5 96.93%
Table 6: Effect of changing common word feature cutoffs (fea-
tures with support ? cutoff are excluded from the model).
number of features used in our complex models ? in the
several hundreds of thousands, is extremely high in com-
parison with the data set size and the number of features
used in other machine learning domains. We describe two
sets of experiments aimed at comparing models with and
without regularization. One is for a simple model with a
relatively small number of features, and the other is for a
model with a large number of features.
The usefulness of priors in maximum entropy models
is not new to this work: Gaussian prior smoothing is ad-
vocated in Chen and Rosenfeld (2000), and used in all
the stochastic LFG work (Johnson et al, 1999). How-
ever, until recently, its role and importance have not been
widely understood. For example, Zhang and Oles (2001)
attribute the perceived limited success of logistic regres-
sion for text categorization to a lack of use of regular-
ization. At any rate, regularized conditional loglinear
models have not previously been applied to the prob-
lem of producing a high quality part-of-speech tagger:
Ratnaparkhi (1996), Toutanova and Manning (2000), and
Collins (2002) all present unregularized models. Indeed,
the result of Collins (2002) that including low support
features helps a voted perceptron model but harms a max-
imum entropy model is undone once the weights of the
maximum entropy model are regularized.
Table 5 shows results on the development set from two
pairs of experiments. The first pair of models use com-
mon word templates ?t0, w0?, ?t0, t?1, t?2? and the same
rare word templates as used in the models in table 2. The
second pair of models use the same features as model
BEST with a higher frequency cutoff of 5 for common
word features.
For the first pair of models, the error reduction from
smoothing is 5.3% overall and 20.1% on unknown words.
For the second pair of models, the error reduction is
even bigger: 16.2% overall after convergence and 5.8% if
looking at the best accuracy achieved by the unsmoothed
model (by stopping training after 75 iterations; see be-
low). The especially large reduction in unknown word er-
ror reflects the fact that, because penalties are effectively
stronger for rare features than frequent ones, the presence
of penalties increases the degree to which more general
cross-word signature features (which apply to unknown
words) are used, relative to word-specific sparse features
(which do not apply to unknown words).
Secondly, use of regularization allows us to incorporate
features with low support into the model while improving
96,3
96,4
96,5
96,6
96,7
96,8
96,9
97
97,1
97,2
0 100 200 300 400
Training Iterations
Ac
cu
ra
cy

No Smoothing
Smoothing
Figure 4: Accuracy by training iterations, with and without
quadratic regularization.
performance. Whereas Ratnaparkhi (1996) used feature
support cutoffs and early stopping to stop overfitting of
the model, and Collins (2002) contends that including
low support features harms a maximum entropy model,
our results show that low support features are useful in a
regularized maximum entropy model. Table 6 contrasts
our results with those from Collins (2002). Since the
models are not the same, the exact numbers are incompa-
rable, but the difference in direction is important: in the
regularized model, performance improves with the inclu-
sion of low support features.
Finally, in addition to being significantly more accu-
rate, smoothed models train much faster than unsmoothed
ones, and do not benefit from early stopping. For ex-
ample, the first smoothed model in Table 5 required 80
conjugate gradient iterations to converge (somewhat ar-
bitrarily defined as a maximum difference of 10?4 in fea-
ture weights between iterations), while its corresponding
unsmoothed model required 335 iterations, thus training
was roughly 4 times slower.10 The second pair of models
required 134 and 370 iterations respectively. As might
be expected, unsmoothed models reach their highest gen-
eralization capacity long before convergence and accu-
racy on an unseen test set drops considerably with fur-
ther iterations. This is not the case for smoothed mod-
els, as their test set accuracy increases almost monoton-
ically with training iterations.11 Figure 4 shows a graph
of training iterations versus accuracy for the second pair
of models on the development set.
4 Conclusion
We have shown how broad feature use, when combined
with appropriate model regularization, produces a supe-
rior level of tagger performance. While experience sug-
10On a 2GHz PC, this is still an important difference: our
largest models require about 25 minutes per iteration to train.
11In practice one notices some wiggling in the curve, but
the trend remains upward even beyond our chosen convergence
point.
gests that the final accuracy number presented here could
be slightly improved upon by classifier combination, it is
worth noting that not only is this tagger better than any
previous single tagger, but it also appears to outperform
Brill and Wu (1998), the best-known combination tagger
(they report an accuracy of 97.16% over the same WSJ
data, but using a larger training set, which should favor
them).
While part-of-speech tagging is now a fairly well-worn
road, and our ability to win performance increases in
this domain is starting to be limited by the rate of er-
rors and inconsistencies in the Penn Treebank training
data, this work also has broader implications. Across
the many NLP problems which involve sequence mod-
els over sparse multinomial distributions, it suggests that
feature-rich models with extensive lexicalization, bidirec-
tional inference, and effective regularization will be key
elements in producing state-of-the-art results.
Acknowledgements
This work was supported in part by the Advanced Re-
search and Development Activity (ARDA)?s Advanced
Question Answering for Intelligence (AQUAINT) Pro-
gram, by the National Science Foundation under Grant
No. IIS-0085896, and by an IBM Faculty Partnership
Award.
References
Steven Abney, Robert E. Schapire, and Yoram Singer. 1999.
Boosting applied to tagging and PP attachment. In
EMNLP/VLC 1999, pages 38?45.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP 6, pages 224?231.
Eric Brill and Jun Wu. 1998. Classifier combination for
improved lexical disambiguation. In ACL 36/COLING 17,
pages 191?195.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike
Perkowitz. 1993. Equations for part-of-speech tagging. In
AAAI 11, pages 784?789.
Stanley F. Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for maximum entropy models. IEEE
Transactions on Speech and Audio Processing, 8(1):37?50.
Kenneth W. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In ANLP 2, pages
136?143.
Michael Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with per-
ceptron algorithms. In EMNLP 2002.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and
David J. Spiegelhalter. 1999. Probabilistic Networks and
Expert Systems. Springer-Verlag, New York.
David Heckerman, David Maxwell Chickering, Christopher
Meek, Robert Rounthwaite, and Carl Myers Kadie. 2000.
Dependency networks for inference, collaborative filtering
and data visualization. Journal of Machine Learning Re-
search, 1(1):49?75.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In ACL 37, pages 535?541.
Dan Klein and Christopher D. Manning. 2002. Conditional
structure versus conditional estimation in NLP models. In
EMNLP 2002, pages 9?16.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML-2001, pages
282?289.
Sang-Zoo Lee, Jun ichi Tsujii, and Hae-Chang Rim. 2000. Part-
of-speech tagging based on Hidden Markov Model assuming
joint independence. In ACL 38, pages 263?169.
Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkie-
wicz. 1994. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19:313?
330.
Ian Marshall. 1987. Tag selection using probabilistic methods.
In Roger Garside, Geoffrey Sampson, and Geoffrey Leech,
editors, The Computational analysis of English: a corpus-
based approach, pages 42?65. Longman, London.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP 1, pages 133?142.
Scott M. Thede and Mary P. Harper. 1999. Second-order hidden
Markov model for part-of-speech tagging. In ACL 37, pages
175?182.
Kristina Toutanova and Christopher Manning. 2000. Enriching
the knowledge sources used in a maximum entropy part-of-
speech tagger. In EMNLP/VLC 1999, pages 63?71.
Tong Zhang and Frank J. Oles. 2001. Text categorization based
on regularized linear classification methods. Information Re-
trieval, 4:5?31.
Pronunciation Modeling for Improved Spelling Correction
Kristina Toutanova
Computer Science Department
Stanford University
Stanford, CA 94305 USA
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
Abstract
This paper presents a method for incor-
porating word pronunciation information
in a noisy channel model for spelling cor-
rection. The proposed method builds an
explicit error model for word pronuncia-
tions. By modeling pronunciation simi-
larities between words we achieve a sub-
stantial performance improvement over
the previous best performing models for
spelling correction.
1 Introduction
Spelling errors are generally grouped into two
classes (Kuckich, 1992) ? typographic and cogni-
tive. Cognitive errors occur when the writer does
not know how to spell a word. In these cases the
misspelling often has the same pronunciation as the
correct word ( for example writing latex as latecks).
Typographic errors are mostly errors related to the
keyboard; e.g., substitution or transposition of two
letters because their keys are close on the keyboard.
Damerau (1964) found that 80% of misspelled
words that are non-word errors are the result of a sin-
gle insertion, deletion, substitution or transposition
of letters. Many of the early algorithms for spelling
correction are based on the assumption that the cor-
rect word differs from the misspelling by exactly
one of these operations (M. D. Kernigan and Gale,
1990; Church and Gale, 1991; Mayes and F. Dam-
erau, 1991).
By estimating probabilities or weights for the
different edit operations and conditioning on the
left and right context for insertions and deletions
and allowing multiple edit operations, high spelling
correction accuracy has been achieved. At ACL
2000, Brill and Moore (2000) introduced a new error
model, allowing generic string-to-string edits. This
model reduced the error rate of the best previous
model by nearly 50%. It proved advantageous to
model substitutions of up to 5-letter sequences (e.g
ent being mistyped as ant, ph as f, al as le, etc.) This
model deals with phonetic errors significantly better
than previous models since it allows a much larger
context size.
However this model makes residual errors, many
of which have to do with word pronunciation. For
example, the following are triples of misspelling,
correct word and (incorrect) guess that the Brill and
Moore model made:
edelvise edelweiss advise
bouncie bouncy bounce
latecks latex lacks
In this work we take the approach of modeling
phonetic errors explicitly by building a separate er-
ror model for phonetic errors. More specifically,
we build two different error models using the Brill
and Moore learning algorithm. One of them is a
letter-based model which is exactly the Brill and
Moore model trained on a similar dataset. The other
is a phone-sequence-to-phone-sequence error model
trained on the same data as the first model, but using
the pronunciations of the correct words and the es-
timated pronunciations of the misspellings to learn
phone-sequence-to-phone-sequence edits and esti-
mate their probabilities. At classification time, N -
best list predictions of the two models are combined
using a log linear model.
A requirement for our model is the availability of
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 144-151.
                         Proceedings of the 40th Annual Meeting of the Association for
a letter-to-phone model that can generate pronunci-
ations for misspellings. We build a letter-to-phone
model automatically from a dictionary.
The rest of the paper is structured as follows:
Section 2 describes the Brill and Moore model and
briefly describes how we use it to build our er-
ror models. Section 3 presents our letter-to-phone
model, which is the result of a series of improve-
ments on a previously proposed N-gram letter-to-
phone model (Fisher, 1999). Section 4 describes the
training and test phases of our algorithm in more de-
tail and reports on experiments comparing the new
model to the Brill and Moore model. Section 6 con-
tains conclusions and ideas for future work.
2 Brill and Moore Noisy Channel Spelling
Correction Model
Many statistical spelling correction methods can be
viewed as instances of the noisy channel model. The
misspelling of a word is viewed as the result of cor-
ruption of the intended word as it passes through a
noisy communications channel.
The task of spelling correction is a task of finding,
for a misspelling w, a correct word r 2 D, where
D is a given dictionary and r is the most probable
word to have been garbled into w. Equivalently, the
problem is to find a word r for which
P (rjw) =
P (r)P (wjr)
P (w)
is maximized. Since the denominator is constant,
this is the same as maximizing P (r)P (wjr). In the
terminology of noisy channel modeling, the distribu-
tion P (r) is referred to as the source model, and the
distribution P (wjr) is the error or channel model.
Typically, spelling correction models are not used
for identifying misspelled words, only for propos-
ing corrections for words that are not found in a
dictionary. Notice, however, that the noisy chan-
nel model offers the possibility of correcting mis-
spellings without a dictionary, as long as sufficient
data is available to estimate the source model fac-
tors. For example, if r = Osama bin Laden and
w = Ossama bin Laden, the model will predict that
the correct spelling r is more likely than the incor-
rect spelling w, provided that
P (w)
P (r)
<
P (wjr)
P (wjw)
where P (wjr)=P (wjw) would be approximately the
odds of doubling the s in Osama. We do not pursue
this, here, however.
Brill and Moore (2000) present an improved er-
ror model for noisy channel spelling correction that
goes beyond single insertions, deletions, substitu-
tions, and transpositions. The model has a set of pa-
rameters P ( ! ) for letter sequences of lengths
up to 5. An extension they presented has refined pa-
rameters P ( ! jPSN) which also depend on
the position of the substitution in the source word.
According to this model, the misspelling is gener-
ated by the correct word as follows: First, a person
picks a partition of the correct word and then types
each partition independently, possibly making some
errors. The probability for the generation of the mis-
spelling will then be the product of the substitution
probabilities for each of the parts in the partition.
For example, if a person chooses to type the word
bouncy and picks the partition boun cy, the proba-
bility that she mistypes this word as boun cie will
be P (boun ! boun)P (cie ! cy). The probability
P (wjr) is estimated as the maximum over all parti-
tions of r of the probability that w is generated from
r given that partition.
We use this method to build an error model for
letter strings and a separate error model for phone
sequences. Two models are learned; one model LTR
(standing for ?letter?) has a set of substitution prob-
abilities P ( ! ) where  and  are character
strings, and another model PH (for ?phone?) has a
set of substitution probabilities P ( ! ) where 
and  are phone sequences.
We learn these two models on the same data set
of misspellings and correct words. For LTR, we use
the training data as is and run the Brill and Moore
training algorithm over it to learn the parameters of
LTR. For PH, we convert the misspelling/correct-
word pairs into pairs of pronunciations of the mis-
spelling and the correct word, and run the Brill and
Moore training algorithm over that.
For PH, we need word pronunciations for the cor-
rect words and the misspellings. As the misspellings
are certainly not in the dictionary we need a letter-
to-phone converter that generates possible pronun-
ciations for them. The next section describes our
letter-to-phone model.
NETtalk MS Speech
Set Words Set Words
Training 14,876 Training 106,650
Test 4,964 Test 30,003
Table 1: Text-to-phone conversion data
3 Letter-to-Phone Model
There has been a lot of research on machine learn-
ing methods for letter-to-phone conversion. High
accuracy is achieved, for example, by using neural
networks (Sejnowski and Rosenberg, 1987), deci-
sion trees (Jiang et al, 1997), and N -grams (Fisher,
1999). We use a modified version of the method pro-
posed by Fisher, incorporating several extensions re-
sulting in substantial gains in performance. In this
section we first describe how we do alignment at
the phone level, then describe Fisher?s model, and fi-
nally present our extensions and the resulting letter-
to-phone conversion accuracy.
The machine learning algorithms for converting
text to phones usually start off with training data
in the form of a set of examples, consisting of let-
ters in context and their corresponding phones (clas-
sifications). Pronunciation dictionaries are the ma-
jor source of training data for these algorithms, but
they do not contain information for correspondences
between letters and phones directly; they have cor-
respondences between sequences of letters and se-
quences of phones.
A first step before running a machine learning
algorithm on a dictionary is, therefore, alignment
between individual letters and phones. The align-
ment algorithm is dependent on the phone set used.
We experimented with two dictionaries, the NETtalk
dataset and the Microsoft Speech dictionary. Statis-
tics about them and how we split them into training
and test sets are shown in Table 1. The NETtalk
dataset contains information for phone level align-
ment and we used it to test our algorithm for auto-
matic alignment. The Microsoft Speech dictionary
is not aligned at the phone level but it is much big-
ger and is the dictionary we used for learning our
final letter-to-phone model.
The NETtalk dictionary has been designed so that
each letter correspond to at most one phone, so a
word is always longer, or of the same length as, its
pronunciation. The alignment algorithm has to de-
cide which of the letters correspond to phones and
which ones correspond to nothing (i.e., are silent).
For example, the entry in NETtalk (when we remove
the empties, which contain information for phone
level alignment) for the word able is ABLE e b L.
The correct alignment is A/e B/b L/L E/?, where ? de-
notes the empty phone. In the Microsoft Speech dic-
tionary, on the other hand, each letter can naturally
correspond to 0, 1, or 2 phones. For example, the en-
try in that dictionary for able is ABLE ey b ax l. The
correct alignment is A/ey B/b L/ax&l E/?. If we also
allowed two letters as a group to correspond to two
phones as a group, the correct alignment might be
A/ey B/b LE/ax&l, but that would make it harder for
the machine learning algorithm.
Our alignment algorithm is an implementa-
tion of hard EM (Viterbi training) that starts off
with heuristically estimated initial parameters for
P (phonesjletter) and, at each iteration, finds the
most likely alignment for each word given the pa-
rameters and then re-estimates the parameters col-
lecting counts from the obtained alignments. Here
phones ranges over sequences of 0 (empty), 1,
and 2 phones for the Microsoft Speech dictionary
and 0 or 1 phones for NETtalk. The parameters
P (phonesjletter) were initialized by a method sim-
ilar to the one proposed in (Daelemans and van den
Bosch, 1996). Word frequencies were not taken into
consideration here as the dictionary contains no fre-
quency information.
3.1 Initial Letter-to-Phone Model
The method we started with was the N-gram model
of Fisher (1999). From training data, it learns rules
that predict the pronunciation of a letter based on m
letters of left and n letters of right context. The rules
are of the following form:
[Lm:T:Rn ! ph
1
p
1
ph
2
p
2
: : :]
Here Lm stands for a sequence of m letters to the
left of T and Rn is a sequence of n letters to the
right. The number of letters in the context to the left
and right varies. We used from 0 to 4 letters on each
side. For example, two rules learned for the letter B
were: [AB:B:OT !   1:0] and [B ! b :96   :04],
meaning that in the first context the letter B is silent
with probability 1:0, and in the second it is pro-
nounced as b with probability :96 and is silent with
probability :04.
Training this model consists of collecting counts
for the contexts that appear in the data with the se-
lected window size to the left and right. We col-
lected counts for all configurations Lm:T:Rn for
m 2 f0; 1; 2; 3; 4g, n 2 f0; 1; 2; 3; 4g that occurred
in the data. The model is applied by choosing for
each letter T the most probable translation as pre-
dicted by the most specific rule for the context of
occurrence of the letter. For example, if we want
to find how to pronounce the second b in abbot we
would chose the empty phone because the first rule
mentioned above is more specific than the second.
3.2 Extensions
We implemented five extensions to the initial model
which together decreased the error rate of the letter-
to-phone model by around 20%. These are :
 Combination of the predictions of several ap-
plicable rules by linear interpolation
 Rescoring of N -best proposed pronunciations
for a word using a trigram phone sequence lan-
guage model
 Explicit distinction between middle of word
versus start or end
 Rescoring of N -best proposed pronunciations
for a word using a fourgram vowel sequence
language model
The performance figures reported by Fisher
(1999) are significantly higher than our figures us-
ing the basic model, which is probably due to the
cleaner data used in their experiments and the dif-
ferences in phoneset size.
The extensions we implemented are inspired
largely by the work on letter-to-phone conversion
using decision trees (Jiang et al, 1997). The last
extension, rescoring based on vowel fourgams, has
not been proposed previously. We tested the algo-
rithms on the NETtalk and Microsoft Speech dic-
tionaries, by splitting them into training and test
sets in proportion 80%/20% training-set to test-set
size. We trained the letter-to-phone models using
the training splits and tested on the test splits. We
Model Phone Acc Word Acc
Initial 88.83% 53.28%
Interpolation
of contexts 90.55% 59.04%
Distinction
of middle 91.09% 60.81%
Phonetic
trigram 91.38% 62.95%
Vowel
fourgram 91.46% 63.63%
Table 2: Letter-to-phone accuracies
are reporting accuracy figures only on the NETtalk
dataset since this dataset has been used extensively
in building letter-to-phone models, and because
phone accuracy is hard to determine for the non-
phonetically-aligned Microsoft Speech dictionary.
For our spelling correction algorithm we use a letter-
to-phone model learned from the Microsoft Speech
dictionary, however.
The results for phone accuracy and word accuracy
of the initial model and extensions are shown in Ta-
ble 2. The phone accuracy is the percentage cor-
rect of all phones proposed (excluding the empties)
and the word accuracy is the percentage of words
for which pronunciations were guessed without any
error.
For our data we noticed that the most specific
rule that matches is often not a sufficiently good
predictor. By linearly interpolating the probabili-
ties given by the five most specific matching rules
we decreased the word error rate by 14.3%. The
weights for the individual rules in the top five were
set to be equal. It seems reasonable to combine the
predictions from several rules especially because the
choice of which rule is more specific of two is arbi-
trary when neither is a substring of the other. For
example, of the two rules with contexts A:B: and
:B:B, where the first has 0 right context and the
second has 0 left letter context, one heuristic is to
choose the latter as more specific since right context
seems more valuable than left (Fisher, 1999). How-
ever this choice may not always be the best and it
proves useful to combine predictions from several
rules. In Table 2 the row labeled ?Interpolation of
contexts? refers to this extension of the basic model.
Adding a symbol for interior of word produced a
gain in accuracy. Prior to adding this feature, we
had features for beginning and end of word. Explic-
itly modeling interior proved helpful and further de-
creased our error rate by 4.3%. The results after this
improvement are shown in the third row of Table 2.
After linearly combining the predictions from the
top matching rules we have a probability distribu-
tion over phones for each letter. It has been shown
that modeling the probability of sequences of phones
can greatly reduce the error (Jiang et al, 1997). We
learned a trigram phone sequence model and used
it to re-score the N -best predictions from the basic
model. We computed the score for a sequence of
phones given a sequence of letters, as follows:
Score(p
1
; p
2
; : : : ; p
n
jl
1
; l
2
: : : l
n
) =
log
Y
i=1:::n
P (p
i
jl
1
; l
2
: : : l
n
) +
 log
Y
i=1:::n
P (p
i
jp
i 1
; p
i 2
) (1)
Here the probabilities P (p
i
jl
1
; l
2
: : : l
n
) are the
distributions over phones that we obtain for each let-
ter from combination of the matching rules. The
weight  for the phone sequence model was esti-
mated from a held-out set by a linear search. This
model further improved our performance and the re-
sults it achieves are in the fourth row of Table 2.
The final improvement is adding a term from a
vowel fourgram language model to equation 1 with
a weight . The term is the log probability of the
sequence of vowels in the word according to a four-
gram model over vowel sequences learned from the
data. The final accuracy we achieve is shown in
the fifth row of the same table. As a comparison,
the best accuracy achieved by Jiang et al (1997)
on NETalk using a similar proportion of training
and test set sizes was 65:8%. Their system uses
more sources of information, such as phones in the
left context as features in the decision tree. They
also achieve a large performance gain by combining
multiple decision trees trained on separate portions
of the training data. The accuracy of our letter-to-
phone model is comparable to state of the art sys-
tems. Further improvements in this component may
lead to higher spelling correction accuracy.
4 Combining Pronunciation and
Letter-Based Models
Our combined error model gives the probability
P
CMB
(wjr) where w is the misspelling and r is a
word in the dictionary. The spelling correction algo-
rithm selects for a misspelling w the word r in the
dictionary for which the product P (r)P
CMB
(wjr)
is maximized. In our experiments we used a uniform
source language model over the words in the dictio-
nary. Therefore our spelling correction algorithm se-
lects the word r that maximizes P
CMB
(wjr). Brill
and Moore (2000) showed that adding a source lan-
guage model increases the accuracy significantly.
They also showed that the addition of a language
model does not obviate the need for a good error
model and that improvements in the error model lead
to significant improvements in the full noisy channel
model.
We build two separate error models, LTR and
PH (standing for ?letter? model and ?phone?
model). The letter-based model estimates a prob-
ability distribution P
LTR
(wjr) over words, and
the phone-based model estimates a distribution
P
PH
(pron wjpron r) over pronunciations. Using
the PH model and the letter-to-phone model, we de-
rive a distribution P
PHL
(wjr) in a way to be made
precise shortly. We combine the two models to esti-
mate scores as follows:
S
CMB
(wjr) =
logP
LTR
(wjr) +
 logP
PHL
(wjr)
The r that maximizes this score will also maxi-
mize the probability P
CMB
(wjr). The probabilities
P
PHL
(wjr) are computed as follows:
P
PHL
(wjr)
=
X
pron r
P (pron r;wjr)
=
X
pron r
P (pron rjr) 
P (wjpron r; r)
This equation is approximated by the expression
for P
PHL
shown in Figure 1 after several simplify-
ing assumptions. The probabilities P (pron rjr) are
PPHL
(wjr) 
X
pron r
1
num pron r
max
pron w
(
P
PH
(pron wjpron r) 
P (pron wjw)
)
Figure 1: Equation for approximation of P
PHL
taken to be equal for all possible pronunciations of r
in the dictionary. Next we assume independence of
the misspelling from the right word given the pro-
nunciation of the right word i.e. P (wjr; pron r) =
P (wjpron r). By inversion of the conditional prob-
ability this is equal to P (pron rjw) multiplied by
P (w)=P (pron r). Since we do not model these
marginal probabilities, we drop the latter factor.
Next the probability P (pron rjw) is expressed as
X
pron w
P (pron w; pron rjw)
which is approximated by the maximum term in the
sum. After the following decomposition:
P (pron w; pron rjw)
= P (pron wjw)P (pron rjw; pron w)
 P (pron wjw)P (pron rjpron w)
where the second part represents a final indepen-
dence assumption, we get the expression in Figure 1.
The probabilities P (pron wjw) are given by the
letter-to-phone model. In the following subsections,
we first describe how we train and apply the individ-
ual error models, and then we show performance re-
sults for the combined model compared to the letter-
based error model.
4.1 Training Individual Error Models
The error model LTR was trained exactly as de-
scribed originally by Brill and Moore (2000). Given
a training set of pairs fw
i
; r
i
g the algorithm es-
timates a set of rewrite probabilities p( ! )
which are the basis for computing probabilities
P
LTR
(wjr).
The parameters of the PH model
P
PH
(pron wjpron r) are obtained by training
a phone-sequence-to-phone-sequence error model
starting from the same training set of pairs fw
i
; r
i
g
of misspelling and correct word as for the LTR
model. We convert this set to a set of pronunciations
of misspellings and pronunciations of correct
words in the following way: For each training
sample fw
i
; r
i
g we generate m training samples
of corresponding pronunciations where m is the
number of pronunciations of the correct word r
i
in our dictionary. Each of those m samples is the
most probable pronunciation of w
i
according to
our letter-to-phone model paired with one of the
possible pronunciations of r
i
. Using this training
set, we run the algorithm of Brill and Moore to es-
timate a set of substitution probabilities  !  for
sequences of phones to sequences of phones. The
probability P
PH
(pron wjpron r) is then computed
as a product of the substitution probabilities in the
most probable alignment, as Brill and Moore did.
4.2 Results
We tested our system and compared it to the Brill
and Moore model on a dataset of around 10; 000
pairs of misspellings and corresponding correct
words, split into training and test sets. The ex-
act data sizes are 7; 385 word pairs in the training
set and 1; 812 word pairs in the test set. This set
is slightly different from the dataset used in Brill
and Moore?s experiments because we removed from
the original dataset the pairs for which we did not
have the correct word in the pronunciation dictio-
nary. Both models LTR and PH were trained on the
same training set. The interpolation weight that the
combined model CMB uses is also set on the train-
ing set to maximize the classification accuracy.
At test time we do not search through all possible
words r in the dictionary to find the one maximizing
Score
CMB
(wjr). Rather, we compute the combi-
nation score only for candidate words r that are in
the top N according to the P
LTR
(wjr) or are in the
top N according to P
PH
(pron wjpron r) for any
of the pronunciations of r from the dictionary and
any of the pronunciations for w that were proposed
by the letter-to-phone model. The letter-to-phone
Model 1-Best 2-Best 3-Best 4-Best
LTR 94.21% 98.18% 98.90 % 99.06%
PH 86.36% 93.65% 95.69 % 96.63%
CMB 95.58% 98.90% 99.34% 99.50%
Error
Reduction 23.8% 39.6% 40% 46.8%
Table 3: Spelling Correction Accuracy Results
model returned for each w the 3 most probable pro-
nunciations only. Our performance was better when
we considered the top 3 pronunciations of w rather
than a single most likely hypothesis. That is prob-
ably due to the fact that the 3-best accuracy of the
letter-to-phone model is significantly higher than its
1-best accuracy.
Table 3 shows the spelling correction accuracy
when using the model LTR, PH, or both in com-
bination. The table shows N -best accuracy results.
The N -best accuracy figures represent the percent
test cases for which the correct word was in the top
N words proposed by the model. We chose the con-
text size of 3 for the LTR model as this context size
maximized test set accuracy. Larger context sizes
neither helped nor hurt accuracy.
As we can see from the table, the phone-based
model alone produces respectable accuracy results
considering that it is only dealing with word pronun-
ciations. The error reduction of the combined model
compared to the letters-only model is substantial:
for 1-Best, the error reduction is over 23%; for 2-
Best, 3-Best, and 4-Best it is even higher, reaching
over 46% for 4-Best.
As an example of the influence of pronuncia-
tion modeling, in Table 4 we list some misspelling-
correct word pairs where the LTR model made
an incorrect guess and the combined model CMB
guessed accurately.
5 Conclusions and Future Work
We have presented a method for using word pro-
nunciation information to improve spelling correc-
tion accuracy. The proposed method substantially
reduces the error rate of the previous best spelling
correction model.
A subject of future research is looking for a bet-
ter way to combine the two error models or building
Misspelling Correct LTR Guess
bouncie bouncy bounce
edelvise edelweiss advise
grissel gristle grizzle
latecks latex lacks
neut newt nut
rench wrench ranch
saing saying sang
stail stale stall
Table 4: Examples of Corrected Errors
a single model that can recognize whether there is
a phonetic or typographic error. Another interest-
ing task is exploring the potential of our model in
different settings such as the Web, e-mail, or as a
specialized model for non-native English speakers
of particular origin.
References
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. In Proc.
of the 38th Annual Meeting of the ACL, pages 286?
293.
K. Church and W. Gale. 1991. Probability scoring for
spelling correction. In Statistics and Computing, vol-
ume 1, pages 93?103.
W. Daelemans and A. van den Bosch. 1996. Language-
independent data-oriented grapheme-to-phoneme con-
version. In Progress in Speech Synthesis, pages 77?90.
F. J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. In Communications
of the ACM, volume 7(3), pages 171?176.
W. M. Fisher. 1999. A statistical text-to-phone function
using ngrams and rules. In Proc. of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, pages 649?652.
L. Jiang, H.W. Hon, and X. Huang. 1997. Improvements
on a trainable letter-to-sound converter. In Proceed-
ings of the 5th European Conference on Speech Com-
munication and Technology.
K. Kuckich. 1992. Techniques for automatically correct-
ing words in text. In ACM Computing Surveys, volume
24(4), pages 377?439.
W. Church M. D. Kernigan and W. A. Gale. 1990. A
spelling correction program based on a noisy channel
model. In Proc. of COLING-90, volume II, pages 205?
211.
F. Mayes and et al F. Damerau. 1991. Conext based
spelling correction. In Information Processing and
Management, volume 27(5), pages 517?522.
T. J. Sejnowski and C. R. Rosenberg. 1987. Parallel net-
works that learn to pronounce english text. In Complex
Systems, pages 145?168.
Proceedings of the 43rd Annual Meeting of the ACL, pages 589?596,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Joint Learning Improves Semantic Role Labeling
Kristina Toutanova
Dept of Computer Science
Stanford University
Stanford, CA, 94305
kristina@cs.stanford.edu
Aria Haghighi
Dept of Computer Science
Stanford University
Stanford, CA, 94305
aria42@stanford.edu
Christopher D. Manning
Dept of Computer Science
Stanford University
Stanford, CA, 94305
manning@cs.stanford.edu
Abstract
Despite much recent progress on accu-
rate semantic role labeling, previous work
has largely used independent classifiers,
possibly combined with separate label se-
quence models via Viterbi decoding. This
stands in stark contrast to the linguistic
observation that a core argument frame is
a joint structure, with strong dependen-
cies between arguments. We show how to
build a joint model of argument frames,
incorporating novel features that model
these interactions into discriminative log-
linear models. This system achieves an
error reduction of 22% on all arguments
and 32% on core arguments over a state-
of-the art independent classifier for gold-
standard parse trees on PropBank.
1 Introduction
The release of semantically annotated corpora such
as FrameNet (Baker et al, 1998) and PropBank
(Palmer et al, 2003) has made it possible to develop
high-accuracy statistical models for automated se-
mantic role labeling (Gildea and Jurafsky, 2002;
Pradhan et al, 2004; Xue and Palmer, 2004). Such
systems have identified several linguistically mo-
tivated features for discriminating arguments and
their labels (see Table 1). These features usually
characterize aspects of individual arguments and the
predicate.
It is evident that the labels and the features of ar-
guments are highly correlated. For example, there
are hard constraints ? that arguments cannot overlap
with each other or the predicate, and also soft con-
straints ? for example, is it unlikely that a predicate
will have two or more AGENT arguments, or that a
predicate used in the active voice will have a THEME
argument prior to an AGENT argument. Several sys-
tems have incorporated such dependencies, for ex-
ample, (Gildea and Jurafsky, 2002; Pradhan et al,
2004; Thompson et al, 2003) and several systems
submitted in the CoNLL-2004 shared task (Carreras
and Ma`rquez, 2004). However, we show that there
are greater gains to be had by modeling joint infor-
mation about a verb?s argument structure.
We propose a discriminative log-linear joint
model for semantic role labeling, which incorpo-
rates more global features and achieves superior
performance in comparison to state-of-the-art mod-
els. To deal with the computational complexity of
the task, we employ dynamic programming and re-
ranking approaches. We present performance re-
sults on the February 2004 version of PropBank on
gold-standard parse trees as well as results on auto-
matic parses generated by Charniak?s parser (Char-
niak, 2000).
2 Semantic Role Labeling: Task Definition
and Architectures
Consider the pair of sentences,
? [The GM-Jaguar pact]AGENT gives
[the car market]RECIPIENT
[a much-needed boost]THEME
? [A much-needed boost]THEME was given to
[the car market]RECIPIENT
by [the GM-Jaguar pact]AGENT
Despite the different syntactic positions of the la-
beled phrases, we recognize that each plays the same
589
role ? indicated by the label ? in the meaning of
this sense of the verb give. We call such phrases
fillers of semantic roles and our task is, given a sen-
tence and a target verb, to return all such phrases
along with their correct labels. Therefore one sub-
task is to group the words of a sentence into phrases
or constituents. As in most previous work on se-
mantic role labeling, we assume the existence of a
separate parsing model that can assign a parse tree t
to each sentence, and the task then is to label each
node in the parse tree with the semantic role of the
phrase it dominates, or NONE, if the phrase does not
fill any role. We do stress however that the joint
framework and features proposed here can also be
used when only a shallow parse (chunked) represen-
tation is available as in the CoNLL-2004 shared task
(Carreras and Ma`rquez, 2004).
In the February 2004 version of the PropBank cor-
pus, annotations are done on top of the Penn Tree-
Bank II parse trees (Marcus et al, 1993). Possi-
ble labels of arguments in this corpus are the core
argument labels ARG[0-5], and the modifier argu-
ment labels. The core arguments ARG[3-5] do not
have consistent global roles and tend to be verb spe-
cific. There are about 14 modifier labels such as
ARGM-LOC and ARGM-TMP, for location and tem-
poral modifiers respectively.1 Figure 1 shows an ex-
ample parse tree annotated with semantic roles.
We distinguish between models that learn to la-
bel nodes in the parse tree independently, called lo-
cal models, and models that incorporate dependen-
cies among the labels of multiple nodes, called joint
models. We build both local and joint models for se-
mantic role labeling, and evaluate the gains achiev-
able by incorporating joint information. We start
by introducing our local models, and later build on
them to define joint models.
3 Local Classifiers
In the context of role labeling, we call a classifier
local if it assigns a probability (or score) to the label
of an individual parse tree node ni independently of
the labels of other nodes.
We use the standard separation of the task of se-
mantic role labeling into identification and classifi-
1For a full listing of PropBank argument labels see (Palmer
et al, 2003)
cation phases. In identification, our task is to clas-
sify nodes of t as either ARG, an argument (includ-
ing modifiers), or NONE, a non-argument. In clas-
sification, we are given a set of arguments in t and
must label each one with its appropriate semantic
role. Formally, let L denote a mapping of the nodes
in t to a label set of semantic roles (including NONE)
and let Id(L) be the mapping which collapses L?s
non-NONE values into ARG. Then we can decom-
pose the probability of a labeling L into probabili-
ties according to an identification model PID and a
classification model PCLS .
PSRL(L|t, v) = PID(Id(L)|t, v) ?
PCLS(L|t, v, Id(L)) (1)
This decomposition does not encode any indepen-
dence assumptions, but is a useful way of thinking
about the problem. Our local models for semantic
role labeling use this decomposition. Previous work
has also made this distinction because, for example,
different features have been found to be more effec-
tive for the two tasks, and it has been a good way
to make training and search during testing more ef-
ficient.
Here we use the same features for local identifi-
cation and classification models, but use the decom-
position for efficiency of training. The identification
models are trained to classify each node in a parse
tree as ARG or NONE, and the classification models
are trained to label each argument node in the train-
ing set with its specific label. In this way the train-
ing set for the classification models is smaller. Note
that we don?t do any hard pruning at the identifica-
tion stage in testing and can find the exact labeling
of the complete parse tree, which is the maximizer
of Equation 1. Thus we do not have accuracy loss
as in the two-pass hard prune strategy described in
(Pradhan et al, 2005).
In previous work, various machine learning meth-
ods have been used to learn local classifiers for role
labeling. Examples are linearly interpolated rela-
tive frequency models (Gildea and Jurafsky, 2002),
SVMs (Pradhan et al, 2004), decision trees (Sur-
deanu et al, 2003), and log-linear models (Xue and
Palmer, 2004). In this work we use log-linear mod-
els for multi-class classification. One advantage of
log-linear models over SVMs for us is that they pro-
duce probability distributions and thus identification
590
Standard Features (Gildea and Jurafsky, 2002)
PHRASE TYPE: Syntactic Category of node
PREDICATE LEMMA: Stemmed Verb
PATH: Path from node to predicate
POSITION: Before or after predicate?
VOICE: Active or passive relative to predicate
HEAD WORD OF PHRASE
SUB-CAT: CFG expansion of predicate?s parent
Additional Features (Pradhan et al, 2004)
FIRST/LAST WORD
LEFT/RIGHT SISTER PHRASE-TYPE
LEFT/RIGHT SISTER HEAD WORD/POS
PARENT PHRASE-TYPE
PARENT POS/HEAD-WORD
ORDINAL TREE DISTANCE: Phrase Type with
appended length of PATH feature
NODE-LCA PARTIAL PATH Path from constituent
to Lowest Common Ancestor with predicate node
PP PARENT HEAD WORD If parent is a PP
return parent?s head word
PP NP HEAD WORD/POS For a PP, retrieve
the head Word / POS of its rightmost NP
Selected Pairs (Xue and Palmer, 2004)
PREDICATE LEMMA & PATH
PREDICATE LEMMA & HEAD WORD
PREDICATE LEMMA & PHRASE TYPE
VOICE & POSITION
PREDICATE LEMMA & PP PARENT HEAD WORD
Table 1: Baseline Features
and classification models can be chained in a princi-
pled way, as in Equation 1.
The features we used for local identification and
classification models are outlined in Table 1. These
features are a subset of features used in previous
work. The standard features at the top of the table
were defined by (Gildea and Jurafsky, 2002), and
the rest are other useful lexical and structural fea-
tures identified in more recent work (Pradhan et al,
2004; Surdeanu et al, 2003; Xue and Palmer, 2004).
The most direct way to use trained local identifi-
cation and classification models in testing is to se-
lect a labeling L of the parse tree that maximizes
the product of the probabilities according to the two
models as in Equation 1. Since these models are lo-
cal, this is equivalent to independently maximizing
the product of the probabilities of the two models
for the label li of each parse tree node ni as shown
below in Equation 2.
P `SRL(L|t, v) =
?
ni?t
PID(Id(li)|t, v) (2)
?
?
ni?t
PCLS(li|t, v, Id(li))
A problem with this approach is that a maximizing
labeling of the nodes could possibly violate the con-
straint that argument nodes should not overlap with
each other. Therefore, to produce a consistent set of
arguments with local classifiers, we must have a way
of enforcing the non-overlapping constraint.
3.1 Enforcing the Non-overlapping Constraint
Here we describe a fast exact dynamic programming
algorithm to find the most likely non-overlapping
(consistent) labeling of all nodes in the parse tree,
according to a product of probabilities from local
models, as in Equation 2. For simplicity, we de-
scribe the dynamic program for the case where only
two classes are possible ? ARG and NONE. The gen-
eralization to more classes is straightforward. In-
tuitively, the algorithm is similar to the Viterbi al-
gorithm for context-free grammars, because we can
describe the non-overlapping constraint by a ?gram-
mar? that disallows ARG nodes to have ARG descen-
dants.
Below we will talk about maximizing the sum of
the logs of local probabilities rather than the prod-
uct of local probabilities, which is equivalent. The
dynamic program works from the leaves of the tree
up and finds a best assignment for each tree, using
already computed assignments for its children. Sup-
pose we want the most likely consistent assignment
for subtree t with children trees t1, . . . , tk each stor-
ing the most likely consistent assignment of nodes
it dominates as well as the log-probability of the as-
signment of all nodes it dominates to NONE. The
most likely assignment for t is the one that corre-
sponds to the maximum of:
? The sum of the log-probabilities of the most
likely assignments of the children subtrees
t1, . . . , tk plus the log-probability for assigning
the node t to NONE
? The sum of the log-probabilities for assign-
ing all of ti?s nodes to NONE plus the log-
probability for assigning the node t to ARG.
Propagating this procedure from the leaves to the
root of t, we have our most likely non-overlapping
assignment. By slightly modifying this procedure,
we obtain the most likely assignment according to
591
a product of local identification and classification
models. We use the local models in conjunction with
this search procedure to select a most likely labeling
in testing. Test set results for our local model P `SRL
are given in Table 2.
4 Joint Classifiers
As discussed in previous work, there are strong de-
pendencies among the labels of the semantic argu-
ment nodes of a verb. A drawback of local models
is that, when they decide the label of a parse tree
node, they cannot use information about the labels
and features of other nodes in the tree.
Furthermore, these dependencies are highly non-
local. For instance, to avoid repeating argument la-
bels in a frame, we need to add a dependency from
each node label to the labels of all other nodes.
A factorized sequence model that assumes a finite
Markov horizon, such as a chain Conditional Ran-
dom Field (Lafferty et al, 2001), would not be able
to encode such dependencies.
The need for Re-ranking
For argument identification, the number of possi-
ble assignments for a parse tree with n nodes is
2n. This number can run into the hundreds of bil-
lions for a normal-sized tree. For argument label-
ing, the number of possible assignments is ? 20m,
if m is the number of arguments of a verb (typi-
cally between 2 and 5), and 20 is the approximate
number of possible labels if considering both core
and modifying arguments. Training a model which
has such huge number of classes is infeasible if the
model does not factorize due to strong independence
assumptions. Therefore, in order to be able to in-
corporate long-range dependencies in our models,
we chose to adopt a re-ranking approach (Collins,
2000), which selects from likely assignments gener-
ated by a model which makes stronger independence
assumptions. We utilize the top N assignments of
our local semantic role labeling model P `SRL to gen-
erate likely assignments. As can be seen from Table
3, for relatively small values of N , our re-ranking
approach does not present a serious bottleneck to
performance. We used a value of N = 20 for train-
ing. In Table 3 we can see that if we could pick, us-
ing an oracle, the best assignment out for the top 20
assignments according to the local model, we would
achieve an F-Measure of 98.8 on all arguments. In-
creasing the number of N to 30 results in a very
small gain in the upper bound on performance and
a large increase in memory requirements. We there-
fore selected N = 20 as a good compromise.
Generation of top N most likely joint
assignments
We generate the top N most likely non-
overlapping joint assignments of labels to nodes in
a parse tree according to a local model P `SRL, by
an exact dynamic programming algorithm, which
is a generalization of the algorithm for finding the
top non-overlapping assignment described in section
3.1.
Parametric Models
We learn log-linear re-ranking models for joint se-
mantic role labeling, which use feature maps from a
parse tree and label sequence to a vector space. The
form of the models is as follows. Let ?(t, v, L) ?
Rs denote a feature map from a tree t, target verb
v, and joint assignment L of the nodes of the tree,
to the vector space Rs. Let L1, L2, ? ? ? , LN denote
top N possible joint assignments. We learn a log-
linear model with a parameter vector W , with one
weight for each of the s dimensions of the feature
vector. The probability (or score) of an assignment
L according to this re-ranking model is defined as:
P rSRL(L|t, v) =
e??(t,v,L),W ?
?N
j=1 e??(t,v,Lj ).W ?
(3)
The score of an assignment L not in the top N
is zero. We train the model to maximize the sum
of log-likelihoods of the best assignments minus a
quadratic regularization term.
In this framework, we can define arbitrary fea-
tures of labeled trees that capture general properties
of predicate-argument structure.
Joint Model Features
We will introduce the features of the joint re-
ranking model in the context of the example parse
tree shown in Figure 1. We model dependencies not
only between the label of a node and the labels of
592
S1
NP1-ARG1
Final-hour trading
VP1
VBD1 PRED
accelerated
PP1 ARG4
TO1
to
NP2
108.1 million shares
NP3 ARGM-TMP
yesterday
Figure 1: An example tree from the PropBank with Semantic Role Annotations.
other nodes, but also dependencies between the la-
bel of a node and input features of other argument
nodes. The features are specified by instantiation of
templates and the value of a feature is the number of
times a particular pattern occurs in the labeled tree.
Templates
For a tree t, predicate v, and joint assignment L
of labels to the nodes of the tree, we define the can-
didate argument sequence as the sequence of non-
NONE labeled nodes [n1, l1, . . . , vPRED, nm, lm] (li
is the label of node ni). A reasonable candidate ar-
gument sequence usually contains very few of the
nodes in the tree ? about 2 to 7 nodes, as this is the
typical number of arguments for a verb. To make
it more convenient to express our feature templates,
we include the predicate node v in the sequence.
This sequence of labeled nodes is defined with re-
spect to the left-to-right order of constituents in the
parse tree. Since non-NONE labeled nodes do not
overlap, there is a strict left-to-right order among
these nodes. The candidate argument sequence that
corresponds to the correct assignment in Figure 1
will be:
[NP1-ARG1,VBD1-PRED,PP1-ARG4,NP3-ARGM-TMP]
Features from Local Models: All features included
in the local models are also included in our joint
models. In particular, each template for local fea-
tures is included as a joint template that concatenates
the local template and the node label. For exam-
ple, for the local feature PATH, we define a joint fea-
ture template, that extracts PATH from every node in
the candidate argument sequence and concatenates
it with the label of the node. Both a feature with
the specific argument label is created and a feature
with the generic back-off ARG label. This is similar
to adding features from identification and classifi-
cation models. In the case of the example candidate
argument sequence above, for the node NP1 we have
the features:
(NP?S?)-ARG1, (NP?S?)-ARG
When comparing a local and a joint model, we use
the same set of local feature templates in the two
models.
Whole Label Sequence: As observed in previous
work (Gildea and Jurafsky, 2002; Pradhan et al,
2004), including information about the set or se-
quence of labels assigned to argument nodes should
be very helpful for disambiguation. For example, in-
cluding such information will make the model less
likely to pick multiple fillers for the same role or
to come up with a labeling that does not contain an
obligatory argument. We added a whole label se-
quence feature template that extracts the labels of
all argument nodes, and preserves information about
the position of the predicate. The template also
includes information about the voice of the predi-
cate. For example, this template will be instantiated
as follows for the example candidate argument se-
quence:
[ voice:active ARG1,PRED,ARG4,ARGM-TMP]
We also add a variant of this feature which uses a
generic ARG label instead of specific labels. This
feature template has the effect of counting the num-
ber of arguments to the left and right of the predi-
cate, which provides useful global information about
argument structure. As previously observed (Prad-
han et al, 2004), including modifying arguments in
sequence features is not helpful. This was confirmed
in our experiments and we redefined the whole label
sequence features to exclude modifying arguments.
One important variation of this feature uses the
actual predicate lemma in addition to ?voice:active?.
Additionally, we define variations of these feature
templates that concatenate the label sequence with
features of individual nodes. We experimented with
593
variations, and found that including the phrase type
and the head of a directly dominating PP ? if one
exists ? was most helpful. We also add a feature that
detects repetitions of the same label in a candidate
argument sequence, together with the phrase types
of the nodes labeled with that label. For example,
(NP-ARG0,WHNP-ARG0) is a common pattern of this
form.
Frame Features: Another very effective class of fea-
tures we defined are features that look at the label of
a single argument node and internal features of other
argument nodes. The idea of these features is to cap-
ture knowledge about the label of a constituent given
the syntactic realization of all arguments of the verb.
This is helpful to capture syntactic alternations, such
as the dative alternation. For example, consider
the sentence (i) ?[Shaw Publishing]ARG0 offered [Mr.
Smith]ARG2 [a reimbursement]ARG1 ? and the alterna-
tive realization (ii) ?[Shaw Publishing]ARG0 offered
[a reimbursement]ARG1 [to Mr. Smith]ARG2 ?. When
classifying the NP in object position, it is useful to
know whether the following argument is a PP. If
yes, the NP will more likely be an ARG1, and if not,
it will more likely be an ARG2. A feature template
that captures such information extracts, for each ar-
gument node, its phrase type and label in the con-
text of the phrase types for all other arguments. For
example, the instantiation of such a template for [a
reimbursement] in (ii) would be
[ voice:active NP,PRED,NP-ARG1,PP]
We also add a template that concatenates the identity
of the predicate lemma itself.
We should note that Xue and Palmer (2004) define
a similar feature template, called syntactic frame,
which often captures similar information. The im-
portant difference is that their template extracts con-
textual information from noun phrases surrounding
the predicate, rather than from the sequence of ar-
gument nodes. Because our model is joint, we are
able to use information about other argument nodes
when labeling a node.
Final Pipeline
Here we describe the application in testing of a
joint model for semantic role labeling, using a local
model P `SRL, and a joint re-ranking model P rSRL.
P `SRL is used to generate top N non-overlapping
joint assignments L1, . . . , LN .
One option is to select the best Li according to
P rSRL, as in Equation 3, ignoring the score from
the local model. In our experiments, we noticed that
for larger values of N , the performance of our re-
ranking model P rSRL decreased. This was probably
due to the fact that at test time the local classifier
produces very poor argument frames near the bot-
tom of the top N for large N . Since the re-ranking
model is trained on relatively few good argument
frames, it cannot easily rule out very bad frames. It
makes sense then to incorporate the local model into
our final score. Our final score is given by:
PSRL(L|t, v) = (P `SRL(L|t, v))? P rSRL(L|t, v)
where ? is a tunable parameter 2 for how much in-
fluence the local score has in the final score. Such in-
terpolation with a score from a first-pass model was
also used for parse re-ranking in (Collins, 2000).
Given this score, at test time we choose among the
top N local assignments L1, . . . , LN according to:
arg max
L?{L1,...,LN}
? log P `SRL(L|t, v) + log P rSRL(L|t, v)
5 Experiments and Results
For our experiments we used the February 2004 re-
lease of PropBank. 3 As is standard, we used the
annotations from sections 02?21 for training, 24 for
development, and 23 for testing. As is done in
some previous work on semantic role labeling, we
discard the relatively infrequent discontinuous argu-
ments from both the training and test sets. In addi-
tion to reporting the standard results on individual
argument F-Measure, we also report Frame Accu-
racy (Acc.), the fraction of sentences for which we
successfully label all nodes. There are reasons to
prefer Frame Accuracy as a measure of performance
over individual-argument statistics. Foremost, po-
tential applications of role labeling may require cor-
rect labeling of all (or at least the core) arguments
in a sentence in order to be effective, and partially
correct labelings may not be very useful.
2We found ? = 0.5 to work best
3Although the first official release of PropBank was recently
released, we have not had time to test on it.
594
Task CORE ARGM
F1 Acc. F1 Acc.
Identification 95.1 84.0 95.2 80.5
Classification 96.0 93.3 93.6 85.6
Id+Classification 92.2 80.7 89.9 71.8
Table 2: Performance of local classifiers on identification, classification, and identification+classification on
section 23, using gold-standard parse trees.
N CORE ARGM
F1 Acc. F1 Acc.
1 92.2 80.7 89.9 71.8
5 97.8 93.9 96.8 89.5
20 99.2 97.4 98.8 95.3
30 99.3 97.9 99.0 96.2
Table 3: Oracle upper bounds for performance on the complete identification+classification task, using
varying numbers of top N joint labelings according to local classifiers.
Model CORE ARGM
F1 Acc. F1 Acc.
Local 92.2 80.7 89.9 71.8
Joint 94.7 88.2 92.1 79.4
Table 4: Performance of local and joint models on identification+classification on section 23, using gold-
standard parse trees.
We report results for two variations of the seman-
tic role labeling task. For CORE, we identify and
label only core arguments. For ARGM, we identify
and label core as well as modifier arguments. We
report results for local and joint models on argu-
ment identification, argument classification, and the
complete identification and classification pipeline.
Our local models use the features listed in Table 1
and the technique for enforcing the non-overlapping
constraint discussed in Section 3.1.
The labeling of the tree in Figure 1 is a specific
example of the kind of errors fixed by the joint mod-
els. The local classifier labeled the first argument in
the tree as ARG0 instead of ARG1, probably because
an ARG0 label is more likely for the subject position.
All joint models for these experiments used the
whole sequence and frame features. As can be seen
from Table 4, our joint models achieve error reduc-
tions of 32% and 22% over our local models in F-
Measure on CORE and ARGM respectively. With re-
spect to the Frame Accuracy metric, the joint error
reduction is 38% and 26% for CORE and ARGM re-
spectively.
We also report results on automatic parses (see
Table 5). We trained and tested on automatic parse
trees from Charniak?s parser (Charniak, 2000). For
approximately 5.6% of the argument constituents
in the test set, we could not find exact matches in
the automatic parses. Instead of discarding these
arguments, we took the largest constituent in the
automatic parse having the same head-word as the
gold-standard argument constituent. Also, 19 of the
propositions in the test set were discarded because
Charniak?s parser altered the tokenization of the in-
put sentence and tokens could not be aligned. As our
results show, the error reduction of our joint model
with respect to the local model is more modest in this
setting. One reason for this is the lower upper bound,
due largely to the the much poorer performance of
the identification model on automatic parses. For
ARGM, the local identification model achieves 85.9
F-Measure and 59.4 Frame Accuracy; the local clas-
sification model achieves 92.3 F-Measure and 83.1
Frame Accuracy. It seems that the largest boost
would come from features that can identify argu-
ments in the presence of parser errors, rather than
the features of our joint model, which ensure global
coherence of the argument frame. We still achieve
10.7% and 18.5% error reduction for CORE argu-
ments in F-Measure and Frame Accuracy respec-
tively.
595
Model CORE ARGM
F1 Acc. F1 Acc.
Local 84.1 66.5 81.4 55.6
Joint 85.8 72.7 82.9 60.8
Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak
automatically generated parse trees.
6 Related Work
Several semantic role labeling systems have success-
fully utilized joint information. (Gildea and Juraf-
sky, 2002) used the empirical probability of the set
of proposed arguments as a prior distribution. (Prad-
han et al, 2004) train a language model over label
sequences. (Punyakanok et al, 2004) use a linear
programming framework to ensure that the only ar-
gument frames which get probability mass are ones
that respect global constraints on argument labels.
The key differences of our approach compared
to previous work are that our model has all of the
following properties: (i) we do not assume a finite
Markov horizon for dependencies among node la-
bels, (ii) we include features looking at the labels
of multiple argument nodes and internal features of
these nodes, and (iii) we train a discriminative model
capable of incorporating these long-distance depen-
dencies.
7 Conclusions
Reflecting linguistic intuition and in line with cur-
rent work, we have shown that there are substantial
gains to be had by jointly modeling the argument
frames of verbs. This is especially true when we
model the dependencies with discriminative models
capable of incorporating long-distance features.
8 Acknowledgements
The authors would like to thank the review-
ers for their helpful comments and Dan Juraf-
sky for his insightful suggestions and useful dis-
cussions. This work was supported in part by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) Program.
References
Collin Baker, Charles Fillmore, and John Lowe. 1998. The
Berkeley Framenet project. In Proceedings of COLING-
ACL-1998.
Xavier Carreras and Lu??s M a`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL-2004.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL, pages 132?139.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of ICML-2000.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?288.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML-2001.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003. The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin,
and Dan Jurafsky. 2004. Shallow semantic parsing using
support vector machines. In Proceedings of HLT/NAACL-
2004.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support vec-
tor learning for semantic argument classification. Machine
Learning Journal.
Vasin Punyakanok, Dan Roth, Wen tau Yih, Dav Zimak, and
Yuancheng Tu. 2004. Semantic role labeling via generalized
inference over classifiers. In Proceedings of CoNLL-2004.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In Proceedings of ACL-2003.
Cynthia A. Thompson, Roger Levy, and Christopher D. Man-
ning. 2003. A generative model for semantic role labeling.
In Proceedings of ECML-2003.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP-2004.
596
Enriching the Knowledge Sources Used in a Maximum Entropy 
Part-of-Speech Tagger 
Kristina Toutanova 
Dept of Computer Science 
Gates Bldg 4A, 353 Serra Mall 
Stanford, CA 94305-9040, USA 
kristina @cs.stanford.edu 
Christopher D. Manning 
Depts of Computer Science and Linguistics 
Gates Bldg 4A, 353 Serra Mall 
Stanford, CA 94305-9040, USA 
manning @cs.stanford.edu 
Abstract 
This paper presents results for a maximum- 
entropy-based part of speech tagger, which 
achieves superior performance principally 
by enriching the information sources used 
for tagging. In particular, we get improved 
results by incorporating these features: 
(i) more extensive treatment of capitaliza- 
tion for unknown words; (ii) features for the 
disambiguation f the tense forms of verbs; 
(iii) features for disambiguating particles 
from prepositions and adverbs. The best 
resulting accuracy for the tagger on the 
Penn Treebank is 96.86% overall, and 
86.91% on previously unseen words. 
Introduction I 
There are now numerous systems for automatic 
assignment of parts of speech ("tagging"), 
employing many different machine learning 
methods. Among recent op performing methods 
are Hidden Markov Models (Brants 2000), 
maximum entropy approaches (Ratnaparkhi 
1996), and transformation-based learning (Brill 
1994). An overview of these and other 
approaches can be found in Manning and 
Schiitze (1999, ch. 10). However, all these 
methods use largely the same information 
sources for tagging, and often almost he same 
features as well, and as a consequence they also 
offer very similar levels of performance. This 
stands in contrast to the (manually-built) EngCG 
tagger, which achieves better performance by 
using lexical and contextual information sources 
and generalizations beyond those available to 
such statistical taggers, as Samuelsson and 
Voutilainen (1997) demonstrate. 
i We thank Dan Klein and Michael Saunders for 
useful discussions, and the anonymous reviewers for 
many helpful comments. 
This paper explores the notion that automat- 
ically built tagger performance can be further 
improved by expanding the knowledge sources 
available to the tagger. We pay special attention 
to unknown words, because the markedly lower 
accuracy on unknown word tagging means that 
this is an area where significant performance 
gains seem possible. 
We adopt a maximum entropy approach 
because it allows the inclusion of diverse 
sources of information without causing frag- 
mentation and without necessarily assuming 
independence b tween the predictors. A maxi- 
mum entropy approach as been applied to part- 
of-speech tagging before (Ratnaparkhi 1996), 
but the approach's ability to incorporate non- 
local and non-HMM-tagger-type evidence has 
not been fully explored. This paper describes the 
models that we developed and the experiments 
we performed to evaluate them. 
1 The Baseline Maximum Entropy Model 
We started with a maximum entropy based 
tagger that uses features very similar to the ones 
proposed in Ratnaparkhi (1996). The tagger 
learns a loglinear conditional probability model 
from tagged text, using a maximum entropy 
method. 
The model assigns a probability for every 
tag t in the set T of possible tags given a word 
and its context h, which is usually def'med as the 
sequence of several words and tags preceding 
the word. This model can be used for estimating 
the probability of a tag sequence h...tn given a 
sentence w~. . .wn:  
n n 
p(t,...t n I wl.. .w,) = I~I p(t, \[t~.. 2,_,,w~...w,) =I I  p(ti I h~) 
iffil i=!  
As usual, tagging is the process of assigning the 
maximum likelihood tag sequence to a string of 
words. 
The idea of maximum entr?py modeling is 
to choose the probability distribution p that has 
the highest entropy out of those distributions 
63 
that satisfy a certain set of constraints. The 
constraints restrict the model to behave in 
accordance with a set of statistics collected from 
the training data. The statistics are expressed as 
the expected values of appropriate functions 
defined on the contexts h and tags t. In particu- 
lar, the constraints demand that the expectations 
of the features for the model match the 
empirical expectations of the ?features over the 
training data. 
For example, if we want to constrain the 
model to tag make as a verb or noun with the 
same frequency as the empirical model induced 
by the training data, we define the features: 
f l (h , t )= l  iff w i=makeandt=NN 
f2 (h , t )= l  iff w i=makeandt=VB 
Some commonly used statistics for part of 
speech tagging are: how often a certain word 
was tagged in a certain way; how often two tags 
appeared in sequence or how often three tags 
appeared in sequence. These look a lot like the 
statistics a Markov Model would use. However, 
in the maximum entropy framework it is 
possible to easily define and incorporate much 
more complex statistics, not restricted to n-gram 
sequences. 
The constraints in our model are that the 
expectations of these features according to the 
joint distribution p are equal to the expectations 
of the features in the empirical (training data) 
distribution ~ : Ep~h.,)fi (h, t) = E~h,,) ~ (h, t). 
Having defined a set of constraints that our 
model should accord with, we proceed to find 
the model satisfying the constraints that maxi- 
mizes the conditional entropy of p .  The intu- 
ition is that such a model assumes nothing apart 
from that it should satisfy the given constraints. 
Following Berger et al (1996), we approxi- 
mate p(h,t), the joint distribution of contexts 
and tags, by the product of ~(h) ,  the empirical 
distribution of histories h, and the conditional 
distribution p(t l h): p(h,t) = ~(h). p(t lh). 
Then for the example above, our constraints 
would be the following, for j E {1,2}: 
~(h, t)f  i (h, t) = ~ ,~(h)p(t \[h)f  i (h, t) 
hEH.tET hsH, t~T 
This approximation is used to enable 
efficient computation. The expectation for a fea- 
ture f is: 
E f= ~(h)p( t lh ) f (h , t  ) 
h~ H ,tE T 
where H is the space of possible contexts h 
when predicting a part of speech tag t. Since the 
contexts contain sequences of words and tags 
and other information, the space H is huge. But 
using this approximation, we can instead sum 
just over the smaller space of observed contexts 
X in the training sample, because the empirical 
prior ~(h) is zero for unseen contexts h: 
E f = 2~(h)p( t lh ) f (h , t )  (1) 
h~ X, t~T 
The model that is a solution to this 
constrained optimization task is an exponential 
(or equivalently, loglinear) model with the para- 
metric form: 
p(t\[h) = j=L...K 
n ea//"h") 
t~T j=I...,K 
where the denominator is a normalizing term 
(sometimes referred to as the partition function). 
The parameters X: correspond to weights for the 
features 3T- 
We will not discuss in detail the characteris- 
tics of the model or the parameter estimation 
procedure used - Improved Iterative Scaling. 
For a more extensive discussion of maximum 
entropy methods, see Berger et al (1996) and 
Jelinek (1997). However, we note that our pa- 
rameter estimation algorithm directly uses equa- 
tion (1). Ratnaparkhi (1996: 134) suggests use 
of an approximation summing over the training 
data, which does not sum over possible tags: 
" h E f j = 2 P( ~)p(ti lhi)f j(hi,ti) 
i=1 
However, we believe this passage is in error: 
such an estimate is ineffective in the iterative 
scaling algorithm. Further, we note that expecta- 
tions of the form (1) appear in Ratnaparkhi 
(1998: 12). 
1.1 Features in the Baseline Model 
In our baseline model, the context available 
when predicting the part of speech tag of a word 
wi in a sentence of words {wl... wn} with tags 
{tl... t~} is {ti.l tin wi wi+l}. The features that 
define the constraints on the model are obtained 
by instantiation of feature templates as in 
Ratnaparkhi (1996). Special feature templates 
exist for rare words in the training data, to 
increase the model's predictioff-capacity for 
unknown words. 
64 
The actual feature templates for this model 
are shown in the next table. They are a subset of 
the features used in Ratnaparkhi (1996). 
No. Feature Type Template 
1. General wi=X & ti =T 
2. General b.l=Tl & ti=T 
3. General tia=T\] & ti.2=T2 & ti=T 
4. General Wi+I=X & ti =T 
5. Rare Suffix of wi =S, 
IS1<5 ,~ t,=T 
6. Rare Prefix of w~=P, l<IPI<5 
& ti=T 
7. Rare w~ contains anumber 
& t~=T 
8. Rare wi contains an uppercase 
character & t~=T 
9. Rare w~ contains ahyphen 
& ti=T 
Table 1 Baseline Model Features 
General feature templates can be instantiated by 
arbitrary contexts, whereas rare feature tem- 
plates are instantiated only by histories where 
the current word wi is rare. Rare words are 
defined to be words that appear less than a 
certain number of times in the training data 
(here, the value 7 was used). 
In order to be able to throw out features that 
would give misleading statistics due to sparse- 
ness or noise in the data, we use two different 
cutoff values for general and rare feature 
templates (in this implementation, 5 and 45 
respectively). As seen in Table 1 the features are 
conjunctions of a boolean function on the 
history h and a boolean function on the tag t. 
Features whose first conjuncts are true for more 
than the corresponding threshold number of 
histories in the training data are included in the 
model. 
The feature templates in Ratnaparkhi (1996) 
that were left out were the ones that look at the 
previous word, the word two positions before 
the current, and the word two positions after the 
current. These features are of the same form as 
template 4 in Table 1, but they look at words in 
different positions. 
Our motivation for leaving these features 
out was the results from some experiments on 
successively adding feature templates. Adding 
template 4 to a model that incorporated the 
general feature templates 1 to 3 only and the 
rare feature templates 5-8 significantly 
increased the accuracy on the development set - 
from 96.0% to 96.52%. The addition of a 
feature template that looked at the preceding 
word and the current ag to the resulting model 
slightly reduced the accuracy. 
1.2 Testing and Performance 
The model was trained and tested on the part-of- 
speech tagged WSJ section of the Penn 
Treebank. The data was divided into contiguous 
parts: sections 0-20 were used for training, 
sections 21-22 as a development test set, and 
sections 23-24 as a final test set. The data set 
sizes are shown below together with numbers of 
unknown words. 
Data Set Tokens Unknown 
Training 1,061,768 
Development 116,206 3271 (2.81%) 
Test 111,221 2879 (2.59%) 
Table 2 Data Sizes 
The testing procedure uses a beam search to 
find the tag sequence with maximal probability 
given a sentence. In our experiments we used a 
beam of size 5. Increasing the beam size did not 
result in improved accuracy. 
The preceding tags for the word at the 
beginning of the sentence are regarded as 
having the pseudo-tag NA. In this way, the 
information that a word is the first word in a 
sentence is available to the tagger. We do not 
have a special end-of-sentence symbol. 
We used a tag dictionary for known words 
in testing. This was built from tags found in the 
training data but augmented so as to capture a 
few basic systematic tag ambiguities that are 
found in English. Namely, for regular verbs the 
-ed form can be either a VBD or a VBN and 
similarly the stem form can be either a VBP or 
VB. Hence for words that had occurred with 
only one of these tags in the training data the 
other was also included as possible for 
assignment. 
The results on the test set for the Baseline 
model are shown in Table 3. 
Model Overall Unknown Word 
Accuracy Accuracy 
Baseline , 96.72% 84.5% 
J 
Ratnaparkhi 96.63% 85.56% 
(1996) 
Table 3 Baseline model performance 
This table also shows the results reported in 
Ratnaparkhi (1996: 142)for COnvenience. The 
accuracy figure for our model is higher overall 
65 
but lower for unknown words. This may stem 
from the differences between the two models' 
feature templates, thresholds, and approxi- 
mations of the expected values for the features, 
as discussed in the beginning of the section, or 
may just reflect differences in the choice of 
training and test sets (which are not precisely 
specified in Ratnaparkhi (1996)). 
The differences are not great enough to 
justify any definite statement about the different 
use of feature templates or other particularities 
of the model estimation. One conclusion that we 
can draw is that at present he additional word 
features used in Ratnaparkhi (1996) - looking at 
words more than one position away from the 
current - do not appear to be helping the overall 
performance of the models. 
1.3 Discussion of Problematic Cases 
A large number of words, including many of the 
most common words, can have more than one 
syntactic category. This introduces a lot of 
ambiguities that the tagger has to resolve. Some 
of the ambiguities are easier for taggers to 
resolve and others are harder. 
Some of the most significant confusions that 
the Baseline model made on the test set can be 
seen in Table 5. The row labels in Table 5 
signify the correct ags, and the column labels 
signify the assigned tags. For example, the num- 
ber 244 in the (NN, JJ) position is the number of 
words that were NNs but were incorrectly 
assigned the JJ category. These particular confu- 
sions, shown in the table, account for a large 
percentage of the total error (2652/3651 = 
72.64%). Table 6 shows part of the Baseline 
model's confusion matrix for just unknown 
words. 
Table 4 shows the Baseline model's overall 
assignment accuracies for different parts of 
speech. For example, the accuracy on nouns is 
greater than the accuracy on adjectives. The 
accuracy on NNPS (plural proper nouns) is a 
surprisingly low 41.1%. 
Tag Accuracy Tag 
IN 97.3% JJ 
NN 96.5% RB 
NNP 96.2% VBN 
VBD 95.2% RP 
VB 94.0% NNPS 
VBP 93.4% 
Accuracy 
93.0% 
92.2% 
90.4% 
41.5% 
41.1% 
Table 4 Accuracy of assignments for different parts 
of speech for the Baseline model. 
Tagger errors are of various types. Some are the 
result of inconsistency in labeling in the training 
data (Ratnaparkhi 1996), which usually reflects 
a lack of linguistic clarity or determination of
the correct part of speech in context. For 
instance, the status of various noun premodifiers 
(whether chief or maximum is NN or JJ, or 
whether a word in -ing is acting as a JJ or VBG) 
is of this type. Some, such as errors between 
NN/NNP/NNPS/NNS largely reflect difficulties 
with unknown words. But other cases, such as 
VBN/VBD and VB/VBP/NN, represent syste- 
matic tag ambiguity patterns in English, for 
which the fight answer is invariably clear in 
context, and for which there are in general good 
structural contextual clues that one should be 
able to use to disarnbiguate. Finally, in another 
class of cases, of which the most prominent is 
probably the RP/IN/RB ambiguity of words like 
up, out, and on, the linguistic distinctions, while 
having a sound empirical basis (e.g., see Baker 
(1995: 198-201), are quite subtle, and often 
require semantic intuitions. There are not good 
syntactic ues for the correct ag (and further- 
more, human taggers not infrequently make 
errors). Within this classification, the greatest 
hopes for tagging improvement appear to come 
from minimizing errors in the second and third 
classes of this classification. 
In the following sections we discuss how we 
include additional knowledge sources to help in 
the assignment of tags to forms of verbs, 
capitalized unknown words, particle words, and 
in the overall accuracy of part of speech 
assignments. 
2 Improving the Unknown Words Model 
The accuracy of the baseline model is markedly 
lower for unknown words than for previously 
seen ones. This is also the case for all other 
taggers, and reflects the importance of lexical 
information to taggers: in the best accuracy 
figures punished for corpus-based taggers, 
known word accuracy is around 97%, whereas 
unknown word accuracy is around 85%. 
In following experiments, we examined 
ways of using additional features to improve the 
accuracy of tagging unknown words. As previ- 
ously discussed in Mikbeev (1999), it is possible 
to improve the accuracy on capitalized words 
that might be proper nouns or the first word in a 
sentence, etc. 
. r .  
66 
JJ NN NNP NNPS R.B RP IN VB VBD VBN VBP Total 
JJ 0 177 56 0 61 2 5 10 15 108 0 488 
NN 244 0 103 0 12 1 1 29 5 6 19 525 
NNP 107/ 106 0 132 5 0 7 5 I 2 0 427 
NNPS 1 0 110 0 0 0 0 0 0 0 0 142 
RB 72 21 7 0 0 16 138 1 0 0 0 295 
RP 0 0 0 0 39 0 65 0 0 0 0 104 
IN 11 0 1 0 169 103 0 1 0 0 0 323 
VB 17 64 9 0 2 0 1 0 4 7 85 189 
VBD 10 5 3 0 0 0 0 3 0 143 2 166 
VBN 101 3 3 0 0 0 0 3 108 0 1 221 
VBP 5 34 3 1 1 0 2 49 6 3 0 104 
Total 626 536 348 144 317 122 279 102 140 269 108 3651 
Table 5 Confusion matrix of the Baseline model showing top confusion pairs overall 
JJ NN NNP NNS NNPS VBN Total 
JJ 0 55 25 1 0 10 107 
NN 55 0 26 5 0 2 98 
NNP 20 41 0 5 4 0 87 
NNPS 0 0 10 11 0 0 23 
NNS 1 3 6 0 1 0 15 
VBN 12 1 1 0 0 0 20 
Total 109 121 98 33 7 19 448 
Table 6 Confusion matrix of the Baseline model for unknown words showing top confusion pairs 
Accuracy Test Set 
Unknown Words Accuracy Test Set 
Accuracy Development Set 
Unknown Words Accuracy Development Set 
Baseline Model 1 Model 2 Model 3 
Capitalization Verb forms Particles 
96.72% 96.76% 96.83% 96.86% 
84.50% 86.76% 86.87% 86.91% 
96.53% 96.55% 96.58% 96.62% 
85.48% 86.03% 86.03% 86.06% 
Table 7 Accuracies of all models on the test and development sets 
Baseline Model 1 Model 2 Model 3 
Capitalization Verb Forms Particles 
1. Current word 15,832 15,832 15,837 15,927 
2. Previous tag 1,424 1,424 1,424 1,424 
3. Previous two tags 16,124 16,124 16,124 16,124 
4. Next word 80,075 80,075 80,075 80,075 
5. Suffixes 3,361 3,361 3,361 3,387 
6. Prefixes 5,311 0 0 0 
7. Contains uppercase character 34 34 34 34 
8. Contains number 7 7 7 7 
9. Contains hyphen 20 20 20 20 
10. Capitalized and mid. sentence 0 33 33 33 
11. All letters uppercase 0 30 30 30 
12. VBPIVB feature 0 0 2 2 
13. VBDIVBN feature 0 0 3 3 
14. Particles, type 1 0 0 0 9 
15. Particles, type 2 0 0 0 2,178 
Total 122,188 116,940 116,960 118,944 
Table 8 Number of features of different ypes 
67 
For example, the error on the proper noun 
category (NNP) accounts :for a significantly 
larger percent of the total error for unknown 
words than for known words. In the Baseline 
model, of the unknown word error 41.3% is due 
to words being NNP and assigned to some other 
category, or being of other category and assigned 
NNP. The percentage of the same type of error 
for known words is 16.2%. 
The incorporation of the following two 
feature schemas greatly improved NNP accuracy: 
(1) A feature that looks at whether all the letters 
of a word are uppercase. The feature that 
looked at capitalization before (cf. Table 1, 
feature No. 8) is activated when the word 
contains an uppercase character. This turns 
out to be a notable distinction because, for 
example, in titles in the WSJ data all words 
are in all uppercase, and the distribution of 
tags for these words is different from the 
overall distribution for words that contain an 
uppercase character. 
(2) A feature that is activated when the word 
contains an uppercase character and it is not 
at the start of a sentence. These word tokens 
also have a different ag distribution from the 
distribution for all tokens that contain an 
uppercase character. 
Conversely, empirically it was found that the 
prefix features for rare words were having a net 
negative ffect on accuracy. We do not at present 
have a good explanation for this phenomenon. 
The addition of the features (1) and (2) and 
the removal of the prefix features considerably 
improved the accuracy on unknown words and 
the overall accuracy. The results on the test set 
after adding these features are shown below: 
Overall Accuracy Unknown Word Accuracy \[ 
96.76% 86.76% I 
Table 9 Accuracy when adding capitalization features 
and removing prefix features. 
Unknown word error is reduced by 15% as 
compared to the Baseline model. 
It is important to note that (2) is composed of 
information already 'known' to the tagger in 
some sense. This feature can be viewed as the 
conjunction of two features, one of which is 
already in the baseline model, and the other of 
which is the negation of a feature existing in the 
baseline model - since for words at the beginning 
of a sentence, the preceding tag is the pseudo-tag 
NA, and there is a feature looking at the 
preceding tag. Even though-our maximum 
entropy model does not require independence 
among the predictors, it provides for free only a 
simple combination of feature weights, and 
additional 'interaction terms' are needed to model 
non-additive interactions (in log-space terms) 
between features. 
3 Features for Disambiguating Verb Forms 
Two of the most significant sources of classifier 
errors are the VBN/VBD ambiguity and the 
VBP/VB ambiguity. As seen in Table 5, 
VBN/VBD confusions account for 6.9% of the 
total word error. The VBP/VB confusions are a 
smaller 3.7% of the errors. In many cases it is 
easy for people (and for taggers) to determine the 
correct form. For example, if there is a to 
infinitive or a modal directly preceding the 
VB/VBP ambiguous word, the form is certainly 
non-finite. But often the modal can be several 
positions away from the current position - still 
obvious to a human, but out of sight for the 
baseline model. 
To help resolve a VB/VBP ambiguity in such 
cases, we can add a feature that looks at the 
preceding several words (we have chosen 8 as a 
threshold), but not across another verb, and 
activates if there is a to there, a modal verb, or a 
form of do, let, make, or help (verbs that 
frequently take a bare infinitive complement). 
Rather than having a separate feature look at 
each preceding position, we define one feature 
that looks at the chosen number of positions to 
the left. This both increases the scope of the 
available history for the tagger and provides a 
better statistic because it avoids fragmentation. 
We added a similar feature for resolving 
VBD/VBN confusions. It activates if there is a 
have or be auxiliary form in the preceding several 
positions (again the value 8 is used in the 
implementation). 
The form of these two feature templates was 
motivated by the structural rules of English and 
not induced from the training data, but it should 
be possible to look for "predictors" for certain 
parts of speech in the preceding words in the 
sentence by, for example, computing association 
strengths. 
The addition of the two feature schemas 
helped reduce the VB/VBP and VBD/VBN con- 
fusions. Below is the performance on the test set 
of the resulting model when features for disam- 
biguating verb forms are added to the model of 
Section 2. The number of VB/VBP confusions 
68 
was reduced by 23.1% as compared to the base- 
line. The number of VBD/VBN confusions was 
reduced by 12.3%. 
Overall Accuracy Unknown Word Accuracy 
96.83% 86.87% 
Table 10 Accuracy of the extended model 
4 Features for Particle Disambiguation 
As discussed in section 1.3 above, the task of 
determining RB/RP/IN tags for words like down, 
out, up is difficult and in particular examples, 
there are often no good local syntactic indicators. 
For instance, in (2), we find the exact same 
sequence of parts of speech, but (2a) is a particle 
use of on, while (2b) is a prepositional use. 
Consequently, the accuracy on the rarer RP 
(particles) category is as low as 41.5% for the 
Baseline model (cf. Table 4). 
(2) a. Kim took on the monster. 
b. Kim sat on the monster. 
We tried to improve the tagger's capability to 
resolve these ambiguities through adding infor- 
mation on verbs' preferences to take specific 
words as particles, or adverbs, or prepositions. 
There are verbs that take particles more than 
others, and particular words like out are much 
more likely to be used as a particle in the context 
of some verb than other words ambiguous 
between these tags. 
We added two different feature templates to 
capture this information, consisting as usual of a 
predicate on the history h, and a condition on the 
tag t. The first predicate is true if the current word 
is often used as a particle, and if there is a verb at 
most 3 positions to the left, which is "known" to 
have a good chance of taking the current word as 
a particle. The verb-particle pairs that are known 
by the system to be very common were collected 
through analysis of the training data in a 
preprocessing stage. 
The second feature template has the form: 
The last verb is v and the current word is w and w 
has been tagged as a particle and the current ag 
is t. The last verb is the pseudo-symbol NA if 
there is no verb in the previous three positions. 
These features were some help in reducing 
the RB/IN/RP confusions. The accuracy on the 
RP category rose to 44.3%. Although the overall 
confusions in this class were reduced, some of the 
errors were increased, for example, the number of 
INs classified as RBs rose slightly. There seems 
to be still considerable room to improve these 
results, though the attainable accuracy is limited 
by the accuracy with which these distinctions are 
marked in the Penn Treebank (on a quick 
informal study, this accuracy seems to be around 
85%). The next table shows the final performance 
on the test set. 
OverallAccuracy Unknown Word Accuracy \[ 
96.86% 86.91% 
Table 11 Accuracy of the final model 
For ease of comparison, the accuracies of all 
models on the test and development sets are 
shown in Table 7. We note that accuracy is lower 
on the development set. This presumably corre- 
sponds with Charniak's (2000: 136) observation 
that Section 23 of the Penn Treebank is easier 
than some others. Table 8 shows the different 
number of feature templates of each kind that 
have been instantiated for the different models as 
well as the total number of features each model 
has. It can be seen that the features which help 
disambiguate verb forms, which look at capital- 
ization and the first of the feature templates for 
particles are a very small number as compared to 
the features of the other kinds. The improvement 
in classification accuracy therefore comes at the 
price of adding very few parameters to the 
maximum entropy model and does not result in 
increased model complexity. 
Conclusion 
Even when the accuracy figures for corpus-based 
part-of-speech taggers start to look extremely 
similar, it is still possible to move performance 
levels up. The work presented in this paper 
explored just a few information sources in 
addition to the ones usually used for tagging. 
While progress is slow, because ach new feature 
applies only to a limited range of cases, 
nevertheless the improvement in accuracy as 
compared to previous results is noticeable, 
particularly for the individual decisions on which 
we focused. 
The potential of maximum entropy methods 
has not previously been fully exploited for the 
task of assignment of parts of speech. We incor- 
porated into a maximum entropy-based tagger 
more linguistically sophisticated features, which 
are non-local and do not look just at particular 
positions in the text. We also added features that 
model the interactions of previously employed 
69 
predictors. All of these changes led to modest 
increases in tagging accuracy. 
This paper has thus presented some initial 
experiments in improving tagger accuracy 
through using additional information sources. In 
the future we hope to explore automatically 
discovering information sources that can be 
profitably incorporated into maximum entropy 
part-of-speech prediction. 
References 
Baker, C. L. 1995. English Syntax. Cambridge, MA: 
MIT Press, 2 nd edition. 
nerger, Adam L., Della Pietra, Stephen A., and Della 
Pietra, Vincent J. 1996. A Maximum Entropy 
Approach to Natural Language Processing. Compu- 
tational Linguistics 22:39-71. 
Brants, Thorsten. 2000. TnT - A Statistical Part-of- 
Speech Tagger. In Proceedings of the Sixth Applied 
Natural Language Processing Conference (ANLP 
2000), Seattle, WA, pp. 224-23 I. 
Brill, Eric. 1994. Some Advances in Transformation- 
Based Part of Speech Tagging. Proceedings of 
AAAL Vol. 1, pp. 722-727. 
Charniak, Eugene. 2000. A Maximum-Entropy- 
Inspired Parser. Proceedings of the I st Meeting of 
the North American Chapter of the Association for 
Computational Linguistics, pp. 132-139. 
Jelinek, Frederick. 1997. Statistical Methods for 
Speech Recognition. Cambridge, MA: MIT Press. 
Manning, Christopher D. and Hinrich Schiitze. 1999. 
Foundations of Statistical Natural Language 
Processing. Cambridge, MA: MIT Press. 
Mikheev, Andrei. 1999. Periods, Capitalized Words, 
etc. Ms., University of Edinburgh. Available at: 
http:llwww.ltg.ed.ac.ukl-mikheevlpapers.html 
Ratnaparkhi, Adwait. 1996. A maximum entropy 
model for part-of-speech tagging. In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, University of Pennsylvania, 
pp. 133--142. 
Ratnaparkhi, Adwait. 1998. Maximum Entropy 
Models for Natural Language Ambiguity Resolu- 
tion. PhD Thesis, University of Pennsylvania. 
Samuelsson, Christer and Atro Voutilainen. 1997. 
Comparing a Linguistic and a Stochastic Tagger. In 
Proceedings of the 25 th Annual Meeting of the 
Association for Computational Linguistics, pp. 246- 
253. 
70 
Combining Heterogeneous Classifiers for Word-Sense Disambiguation
Dan Klein, Kristina Toutanova, H. Tolga Ilhan,
Sepandar D. Kamvar and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040, USA
Abstract
This paper discusses ensembles of simple but het-
erogeneous classifiers for word-sense disambigua-
tion, examining the Stanford-CS224N system en-
tered in the SENSEVAL-2 English lexical sample
task. First-order classifiers are combined by a
second-order classifier, which variously uses ma-
jority voting, weighted voting, or a maximum en-
tropy model. While individual first-order classifiers
perform comparably to middle-scoring teams? sys-
tems, the combination achieves high performance.
We discuss trade-offs and empirical performance.
Finally, we present an analysis of the combination,
examining how ensemble performance depends on
error independence and task difficulty.
1 Introduction
The problem of supervised word sense disambigua-
tion (WSD) has been approached using many differ-
ent classification algorithms, including naive-Bayes,
decision trees, decision lists, and memory-based
learners. While it is unquestionable that certain al-
gorithms are better suited to the WSD problem than
others (for a comparison, see Mooney (1996)), it
seems that, given similar input features, various al-
gorithms exhibit roughly similar accuracies.1 This
was supported by the SENSEVAL-2 results, where a
This paper is based on work supported in part by the Na-
tional Science Foundation under Grants IIS-0085896 and IIS-
9982226, by an NSF Graduate Fellowship, and by the Research
Collaboration between NTT Communication Science Labora-
tories, Nippon Telegraph and Telephone Corporation and CSLI,
Stanford University.
1In fact, we have observed that differences between imple-
mentations of a single classifier type, such as smoothing or win-
dow size, impacted accuracy far more than the choice of classi-
fication algorithm.
large fraction of systems had scores clustered in a
fairly narrow region (Senseval-2, 2001).
We began building our system with 23 supervised
WSD systems, each submitted by a student taking
the natural language processing course (CS224N) at
Stanford University in Spring 2000. Students were
free to implement whatever WSD method they chose.
While most implemented variants of naive-Bayes,
others implemented a range of other methods, in-
cluding n-gram models, vector space models, and
memory-based learners. Taken individually, the best
of these systems would have turned in an accuracy
of 61.2% in the SENSEVAL-2 English lexical sam-
ple task (which would have given it 6th place), while
others would have produced middling to low perfor-
mance. In this paper, we investigate how these clas-
sifiers behave in combination.
In section 2, we discuss the first-order classifiers
and describe our methods of combination. In sec-
tion 3, we discuss performance, analyzing what ben-
efit was found from combination, and when. We also
discuss aspects of the component systems which
substantially influenced overall performance.
2 The System
2.1 Training Procedure
Figure 1 shows the high-level organization of our
system. Individual first-order classifiers each map
lists of context word tokens to word-sense predic-
tions, and are self-contained WSD systems. The first-
order classifiers are combined in a variety of ways
with second-order classifiers. Second-order classi-
fiers are selectors, taking a list of first-order out-
                     July 2002, pp. 74-80.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
rankingorder2nd.
Entropy
Validation
Voting
1 2 3 4 5 6 7 8
MaximumWeighted
Voting
Majority
classifiersorder1st.
2nd. classifiersorder
Chosen
ClassifierFinal classifier
Cross
rankingorder1st.
Figure 1: High-level system organization.
1 Split data into multiple training and held-out parts.
2 Rank first-order classifiers globally (across all words).
3 Rank first-order classifiers locally (per word),
breaking ties with global ranks.
4 For each word w
5 For each size k
6 Choose the ensemble Ew,k to be the top k classifiers
7 For each voting method m
8 Train the (k, m) second-order classifier with Ew,k
9 Rank the second-order classifier types (k, m) globally.
10 Rank the second-order classifier instances locally.
11 Choose the top-ranked second-order classifier for each word.
12 Retrain chosen per-word classifiers on entire training data.
13 Run these classifiers on test data, and evaluate results.
Table 1: The classifier construction process.
puts and choosing from among them. An outline
of the classifier construction process is given in ta-
ble 1. First, the training data was split into training
and held-out sets for each word. This was done us-
ing 5 random bootstrap splits. Each split allocated
75% of the examples to training and 25% to held-
out testing.2 Held-out data was used both to select
the subsets of first-order classifiers to be combined,
and to select the combination methods.
For each word and each training split, the 23 first-
order classifiers were (independently) trained and
tested on held-out data. For each word, the first-
order classifiers were ranked by their average per-
formance on the held-out data, with the most accu-
rate classifiers at the top of the rankings. Ties were
broken by the classifiers? (weighted) average perfo-
mance across all words.
For each word, we then constructed a set of can-
2Bootstrap splits were used rather than standard n-fold
cross-validation for two reasons. First, it allowed us to gen-
erate an arbitrary number of training/held-out pairs while still
leaving substantial held-out data set sizes. Second, this ap-
proach is commonly used in the literature on ensembles. Its
well-foundedness and theoretical properties are discussed in
Breiman (1996). In retrospect, since we did not take proper ad-
vantage of the ability to generate numerous splits, it might have
been just as well to use cross-validation.
didate second-order classifiers. Second-order clas-
sifier types were identified by an ensemble size k
and a combination method m. One instance of each
second-order type was constructed for each word.
We originally considered ensemble sizes k in the
range {1, 3, 5, 7, 9, 11, 13, 15}. For a second-order
classifier with ensemble size k, the ensemble mem-
bers were the top k first-order classifiers according
to the local rank described above.
We combined first-order ensembles using one of
three methods m:
? Majority voting: The sense output by the most
first-order classifiers in the ensemble was chosen.
Ties were broken by sense frequency, in favor of
more frequent senses.
? Weighted voting: Each first-order classifier was
assigned a voting weight (see below). The sense
receiving the greatest total weighted vote was
chosen.
? Maximum entropy: A maximum entropy classifier
was trained (see below) and run on the outputs of
the first-order classifiers.
We considered all pairs of k and m, and so for
each word there were 24 possible second-order clas-
sifiers, though for k = 1 all three values of m are
equivalent and were merged. The k = 1 ensemble,
as well as the larger ensembles (k ? {9, 11, 13, 15}),
did not help performance once we had good first-
order classifier rankings (see section 3.4).
For m = Majority, there are no parameters to set.
For the other two methods, we set the parameters of
the (k, m) second-order classifier for a word w using
the bootstrap splits of the training data for w.
In the same manner as for the first-order classi-
fiers, we then ranked the second-order classifiers.
For each word, there was the local ranking of the
second-order classifiers, given by their (average) ac-
curacy on held-out data. Ties in these rankings were
broken by the average performance of the classifier
type across all words. The top second-order classi-
fier for each word was selected from these tie-broken
rankings.
At this point, all first-order ensemble members
and chosen second-order combination methods were
retrained on the unsplit training data and run on the
final test data.
It is important to stress that each target word was
considered an entirely separate task, and different
first- and second-order choices could be, and were,
made for each word (see the discussion of table 2
below). Aggregate performance across words was
only used for tie-breaking.
2.2 Combination Methods
Our second-order classifiers take training instances
of the form s? = (s, s1, . . . , sk) where s is the correct
sense and each si is the sense chosen by classifier i .
All three of the combination schemes which we used
can be seen as weighted voting, with different ways
of estimating the voting weights ?i of the first-order
voters. In the simplest case, majority voting, we skip
any attempt at statistical estimation and simply set
each ?i to be 1/k.
For the method we actually call ?weighted vot-
ing,? we view the combination output as a mixture
model in which each first-order system is a mixture
component:
P(s|s1, . . . , sk) =
?
i
?i P(s|si )
The conditional probabilties P(s|si ) assign mass
one to the sense si chosen by classifier i . The mix-
ture weights ?i were estimated using EM to max-
imize the likelihood of the second-order training
instances. In testing, the sense with the highest
weighted vote, and hence highest posterior likeli-
hood, is the selected sense.
For the maximum entropy classifier, we have a
different model for the chosen sense s. In this case,
it is an exponential model of the form:
P(s|s1, . . . , sk) =
exp
?
x ?x fx(s, s1, . . . , sk)
?
t exp
?
x ?x fx(t, s1, . . . , sk)
The features fx are functions which are true over
some subset of vectors s?. The original intent was to
design features to recognize and exploit ?sense ex-
pertise? in the individual classifiers. For example,
one classifier might be trustworthy when reporting
a certain sense but less so for other senses. How-
ever, there was not enough data to accurately esti-
mate parameters for such models.3 In fact, we no-
3The number of features was not large: only one for each
(classifier, chosen sense, correct sense) triple. However, most
senses are rarely chosen and rarely correct, so most features
had zero or singleton support.
ticed that, for certain words, simple majority voting
performed better than the maximum entropy model.
It also turned out that the most complex features we
could get value from were features of the form:
fi(s, s1, . . . , sk) = 1 ?? s = si
That is, for each first-order classifier, there is a sin-
gle feature which is true exactly when that classi-
fier is correct. With only these features, the maxi-
mum entropy approach also reduces to a weighted
vote; the s which maximizes the posterior probabil-
ity P(s|s1, . . . , sk) also maximizes the vote:
v(s) =
?
i ?i?(si = s)
The indicators ? are true for exactly one sense, and
correspond to the simple f i defined above.4 The
sense with the largest vote v(s) will be the sense
with the highest posterior probability P(s|s1, . . . sk)
and will be chosen.
For the maximum entropy classifier, we estimate
the weights by maximizing the likelihood of a held-
out set, using the standard IIS algorithm (Berger et
al., 1996). For both weighted schemes, we found
that stopping the iterative procedures before conver-
gence gave better results. IIS was halted after 50
rounds, while EM was halted after a single round.
Both methods were initialized to uniform starting
weights.
More importantly than changing the exact weight
estimates, moving from method to method triggers
broad qualitative changes in what kind of weights
are allowed. With majority voting, classifiers all
have equal, positive weights. With weighted vot-
ing, the weights are no longer required to be equal,
but are still non-negative. With maximum entropy
weighting, this non-negativity constraint is also re-
laxed, allowing classifiers? votes to actually reduce
the score for the sense that classifier has chosen.
Negative weights are in fact assigned quite fre-
quently, and often seem to have the effect of using
poor classifiers as ?error masks? to cancel out com-
mon errors.
As we move from majority voting to weighted
voting to maximum entropy, the estimation becomes
4If the i th classifier returns the correct sense s, then
?(si = s) is 1, otherwise it is zero.
more sophisticated, but also more prone to overfit-
ting. Since solving the overfitting problem is hard,
while choosing between classifiers based on held-
out data is relatively easy, this spectrum gives us a
way to gracefully handle the range of sparsities in
the training corpora for different words.
2.3 Individual Classifiers
While our first-order classifiers implemented a va-
riety of classification algorithms, the differences in
their individual accuracies did not primarily stem
from the algorithm chosen. Rather, implementation
details led to the largest differences. For example,
naive-Bayes classifiers which chose sensible win-
dow sizes, or dynamically chose between window
sizes, tended to outperform those which chose poor
sizes. Generally, the optimal windows were either
of size one (for words with strong local syntactic or
collocational cues) or of very large size (which de-
tected more topical cues). Programs with hard-wired
window sizes of, say, 5, performed poorly. Iron-
ically, such middle-size windows were commonly
chosen by students, but rarely useful; either extreme
was a better design.5
Another implementation choice dramatically af-
fecting performance of naive-Bayes systems was the
amount and type of smoothing. Heavy smoothing
and smoothing which backed off conditional dis-
tributions P(w j |si) to the relevant marginal P(w j)
gave good results, while insufficient smoothing or
backing off to uniform marginals gave substantially
degraded results.6
There is one significant way in which our first-
order classifiers were likely different from other
teams? systems. In the original class project, stu-
dents were guaranteed that the ambiguous word
would appear only in a single orthographic form,
and many of the systems depended on the input sat-
isfying this guarantee. Since this was not true of
the SENSEVAL-2 data, we mapped the ambiguous
5Such window sizes were also apparently chosen by other
SENSEVAL-2 systems, which commonly used ?long distance?
and ?local? features, but defined local as a window size of 3?5
words on each side of the ambiguous word.
6In particular, there is a defective behavior with naive-Bayes
where, when one smooths far too little, the chosen sense is the
one which has occurred with the most words in the context
window. For small training sets of skewed-prior data like the
SENSEVAL-2 sets, this is invariably the common sense, regard-
less of the context words.
words (but not context words) to a citation form.
We suspect that this lost quite a bit of information
and negatively affected the system?s overall perfor-
mance, since there is considerable correlation be-
tween form and sense, especially for verbs. Nev-
ertheless, we have made no attempt to re-engineer
the student systems, and have not thoroughly inves-
tigated how big a difference this stemming made.
3 Results and Discussion
3.1 Results
Table 2 shows the results per word, and table 3
shows results by part-of-speech and overall, for the
SENSEVAL-2 English lexical sample task. It also
shows what second-order classifiers were selected
for each word. 54.2% of the time, we made an opti-
mal second-order classifier choice. When we chose
wrong, we usually made a mistake in either ensem-
ble size or method, rarely both. A wide range of
second-order classifier types were chosen. As an
overview of the benefit of combination, the globally
best single classifier scored 61.2%, the locally best
single classifier (best on test data) scored 62.2%, the
globally best second order classifier (ME-7, best on
test data) scored 63.2%, and our dynamic selection
method scored 63.9%. Section 3.3 examines combi-
nation effectiveness more closely.
3.2 Changes from SENSEVAL-2
The system we originally submitted to the
SENSEVAL-2 competition had an overall accu-
racy of 61.7%, putting it in 4th place in the revised
rankings (among 21 supervised and 28 total sys-
tems). Assuming that our first-order classifiers
were fixed black-boxes, we wanted an idea of how
good our combination and selection methods were.
To isolate the effectiveness of our second-order
classifier choices, we compared our system to an
oracle method (OR-BEST) which chose a word?s
second-order classifier based on test data (rather
than held-out data). The overall accuracy of this
oracle method was 65.4% at the time, a jump of
3.7%.7 This gap was larger than the gap between
the various top-scoring teams? systems. Therefore,
while the test-set performance of the second-order
classifiers is obviously not available, it was clear
7With other changes, OR-BEST rose to 66.1%.
LB Baselines Combination OR UB System
Word ALL MFS SNG MJ-7 WT-7 ME-7 BEST SOME ACC CL
art-n 28.6 41.8 50.6 52.0 54.1 52.0 58.2 69.4 58.2 WT-5
authority-n 45.7 33.7 61.3 69.6 69.6 65.2 69.6 78.3 66.3 WT-3
bar-n 31.1 39.7 63.7 61.6 69.5 72.2 72.2 81.5 72.2 ME-7
begin-v 50.0 58.6 70.0 83.6 84.3 88.2 88.2 94.6 83.6 MJ-7
blind-a 65.5 83.6 77.8 83.6 83.6 85.5 85.5 90.9 83.6 WT-7
bum-n 71.1 75.6 71.3 75.6 75.6 77.8 77.8 82.2 77.8 ME-7
call-v 1.5 25.8 33.3 25.8 30.3 27.3 34.8 62.1 30.3 WT-7
carry-v 9.1 22.7 27.8 34.8 33.3 33.3 37.9 62.1 33.3 MJ-5
chair-n 76.8 79.7 84.2 82.6 82.6 82.6 82.6 84.1 81.2 ME-3
channel-n 46.6 27.4 61.1 60.3 60.3 65.8 67.1 78.1 67.1 ME-3
child-n 34.4 54.7 57.9 67.2 70.3 70.3 75.0 90.6 71.9 WT-5
church-n 56.2 53.1 63.1 73.4 73.4 75.0 75.0 85.9 73.4 WT-7
circuit-n 52.9 27.1 70.9 65.9 65.9 78.8 78.8 80.0 78.8 ME-5
collaborate-v 90.0 90.0 92.9 90.0 90.0 90.0 90.0 90.0 90.0 WT-5
colorless-a 48.6 65.7 80.0 68.6 68.6 68.6 68.6 82.9 68.6 ME-5
cool-a 15.4 46.2 65.0 57.7 55.8 59.6 59.6 80.8 59.6 ME-5
day-n 36.6 59.3 58.4 69.0 68.3 66.2 69.0 82.8 63.4 WT-3
develop-v 11.6 29.0 35.2 42.0 43.5 42.0 43.5 68.1 42.0 MJ-3
draw-v 4.9 9.8 23.4 29.3 26.8 24.4 29.3 41.5 26.8 WT-5
dress-v 25.4 42.4 49.9 52.5 52.5 55.9 59.3 72.9 55.9 ME-7
drift-v 3.1 25.0 31.7 37.5 37.5 34.4 37.5 65.6 37.5 WT-5
drive-v 16.7 28.6 40.0 45.2 45.2 40.5 45.2 61.9 42.9 MJ-3
dyke-n 85.7 89.3 86.5 89.3 89.3 89.3 92.9 96.4 92.9 WT-3
face-v 82.8 83.9 80.9 83.9 83.9 82.8 83.9 84.9 83.9 WT-5
facility-n 36.2 48.3 70.5 67.2 70.7 65.5 74.1 86.2 70.7 WT-7
faithful-a 56.5 78.3 65.0 78.3 78.3 78.3 82.6 100.0 78.3 MJ-3
fatigue-n 67.4 76.7 83.9 88.4 90.7 90.7 90.7 93.0 90.7 MJ-5
feeling-n 29.4 56.9 76.7 62.7 70.6 72.5 74.5 86.3 72.5 WT-3
find-v 7.4 14.7 37.6 30.9 27.9 30.9 32.4 48.5 32.4 WT-3
fine-a 32.9 38.6 46.9 51.4 57.1 54.3 57.1 67.1 52.9 MJ-3
fit-a 51.7 51.7 87.7 89.7 89.7 86.2 93.1 96.6 93.1 MJ-5
free-a 26.8 39.0 58.2 65.9 65.9 61.0 65.9 74.4 64.6 ME-3
graceful-a 62.1 75.9 81.4 79.3 79.3 79.3 79.3 82.8 79.3 WT-5
green-a 69.1 78.7 80.0 83.0 83.0 83.0 85.1 88.3 83.0 MJ-3
grip-n 25.5 54.9 49.2 60.8 60.8 58.8 74.5 84.3 60.8 MJ-7
hearth-n 46.9 75.0 56.3 75.0 71.9 65.6 75.0 84.4 62.5 WT-3
holiday-n 77.4 83.9 89.7 83.9 83.9 80.6 83.9 87.1 83.9 WT-5
keep-v 19.4 37.3 36.1 38.8 49.3 52.2 52.2 65.7 52.2 WT-5
lady-n 60.4 69.8 67.7 75.5 75.5 77.4 77.4 81.1 75.5 WT-3
leave-v 21.2 31.8 29.1 43.9 53.0 50.0 54.5 68.2 54.5 WT-5
live-v 20.9 50.7 54.6 53.7 59.7 65.7 71.6 77.6 71.6 MJ-3
local-a 15.8 57.9 76.8 71.1 68.4 68.4 71.1 92.1 71.1 MJ-7
match-v 11.9 35.7 30.4 52.4 52.4 57.1 57.1 78.6 47.6 WT-3
material-n 39.1 42.0 56.0 55.1 55.1 50.7 66.7 73.9 66.7 WT-3
mouth-n 15.0 45.0 40.5 53.3 53.3 45.0 56.7 78.3 53.3 MJ-5
nation-n 70.3 70.3 71.1 70.3 70.3 70.3 70.3 70.3 70.3 WT-5
natural-a 18.4 27.2 50.4 49.5 50.5 58.3 58.3 76.7 55.3 WT-3
nature-n 23.9 45.7 51.3 63.0 67.4 65.2 67.4 82.6 60.9 MJ-5
oblique-a 51.7 69.0 73.7 82.8 82.8 82.8 86.2 89.7 79.3 WT-5
play-v 12.1 19.7 35.6 40.9 51.5 50.0 51.5 62.1 51.5 WT-5
post-n 26.6 31.6 66.5 49.4 57.0 65.8 67.1 73.4 67.1 ME-3
pull-v 1.7 21.7 27.7 21.7 21.7 28.3 28.3 46.7 23.3 WT-3
replace-v 28.9 53.3 49.0 57.8 53.3 60.0 60.0 77.8 57.8 MJ-7
restraint-n 35.6 31.1 53.9 71.1 68.9 71.1 71.1 82.2 66.7 ME-5
see-v 29.0 31.9 40.0 42.0 42.0 42.0 42.0 55.1 42.0 MJ-5
sense-n 18.9 22.6 46.3 64.2 60.4 50.9 64.2 79.2 64.2 MJ-7
serve-v 35.3 29.4 54.4 60.8 64.7 66.7 66.7 74.5 62.7 WT-5
simple-a 51.5 51.5 43.0 51.5 51.5 51.5 51.5 54.5 51.5 ME-3
solemn-a 96.0 96.0 89.2 96.0 96.0 96.0 96.0 96.0 96.0 WT-3
spade-n 66.7 63.6 81.8 75.8 75.8 78.8 78.8 81.8 78.8 WT-3
stress-n 7.7 46.2 47.0 43.6 43.6 35.9 51.3 82.1 48.7 WT-5
strike-v 5.6 16.7 32.3 31.5 29.6 29.6 40.7 55.6 31.5 MJ-5
train-v 22.2 30.2 48.3 57.1 57.1 54.0 57.1 76.2 57.1 WT-7
treat-v 36.4 38.6 51.8 54.5 54.5 52.3 54.5 70.5 52.3 WT-3
turn-v 1.5 14.9 38.8 32.8 29.9 32.8 35.8 52.2 31.3 MJ-5
use-v 61.8 65.8 69.6 65.8 65.8 72.4 72.4 75.0 72.4 ME-3
vital-a 84.2 92.1 91.5 92.1 92.1 92.1 92.1 92.1 92.1 WT-5
wander-v 70.0 80.0 83.2 80.0 82.0 82.0 82.0 84.0 80.0 ME-3
wash-v 16.7 25.0 40.0 58.3 58.3 25.0 58.3 83.3 58.3 MJ-7
work-v 10.0 26.7 28.1 43.3 43.3 41.7 45.0 63.3 45.0 WT-3
yew-n 75.0 78.6 81.4 78.6 78.6 78.6 78.6 82.1 78.6 WT-5
Table 2: Results by word for the SENSEVAL-2 English lexi-
cal sample task. Lower bound (LB): ALL is how often all of
the first-orders chose correctly. Baselines (BL): MFS is the
most-frequent-sense baseline, SNG is the best single first-order
classifier as chosen on held-out data for that word. Fixed com-
binations: majority vote (MJ), weighted vote (WT), maximum
entropy (ME). Oracle bound (OR): BEST is the best second-
order classifier as measured on the test data. Upper bound (UB):
SOME is how often at least one first-order classifier produced
the correct answer. Methods which are ensemble-size depen-
dent are shown for k = 7. System choices: ACC is the accuracy
of the selection the system makes based on held-out data. CL is
the 2nd-order classifier selected.
that a more sophisticated or better-tuned method
of selecting combination models could lead to
significant improvement. In fact, changing only
ranking methods, which are discussed further in the
next section, resulted in an increase in final accu-
racy for our system to the current score of 63.9%,
which would have placed it 1st in the SENSEVAL-2
preliminary results or 2nd in the revised results. Our
LB Baselines Combination OR UB System
ALL MFS SNG MJ-7 WT-7 ME-7 BEST SOME ACC
noun 42.5 50.5 63.8 66.4 67.9 67.8 71.9 81.2 69.7
adj. 45.1 57.8 66.7 69.0 69.4 69.9 71.6 81.0 69.9
verb 28.8 40.2 48.7 53.4 54.7 55.8 58.2 71.2 55.7
avg. 46.5 47.5 62.2 61.5 62.7 63.2 68.9 72.0 63.9
Table 3: Results by part-of-speech, and overall.
58
59
60
61
62
63
64
65
1 3 5 7 9 11 13 15
 	
 	   	 
 





 Extensions to HMM-based Statistical Word Alignment Models
Kristina Toutanova, H. Tolga Ilhan and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305-9040 USA
kristina@cs.stanford.edu
ilhan@stanford.edu
manning@cs.stanford.edu
Abstract
This paper describes improved HMM-based word
level alignment models for statistical machine
translation. We present a method for using part of
speech tag information to improve alignment accu-
racy, and an approach to modeling fertility and cor-
respondence to the empty word in an HMM align-
ment model. We present accuracy results from eval-
uating Viterbi alignments against human-judged
alignments on the Canadian Hansards corpus, as
compared to a bigram HMM, and IBM model 4.
The results show up to 16% alignment error reduc-
tion.
1 Introduction
The main task in statistical machine translation is
to model the string translation probability   	 

where the string




in one language is translated
into another language as string


. We refer to


as the source language string and 
  as the target
language string in accordance with the noisy chan-
nel terminology used in the IBM models of (Brown
et al, 1993). Word-level translation models assume
a pairwise mapping between the words of the source
and target strings. This mapping is generated by
alignment models. In this paper we present exten-
sions to the HMM alignment model of (Vogel et al,
1996; Och and Ney, 2000b). Some of our extensions
are applicable to other alignment models as well and
are of general utility.1
For most language pairs huge amounts of parallel
corpora are not readily available whereas monolin-
gual resources such as taggers are more often avail-
able. Little research has gone into exploring the po-
1This paper was supported in part by the National Science
Foundation under Grants IIS-0085896 and IIS-9982226. The
authors would also like to thank the various reviewers for their
helpful comments on earlier versions.
tential of part of speech information to better model
translation probabilities and permutation probabili-
ties. Melamed (2000) uses a very broad classifica-
tion of words (content, function and several punctu-
ation classes) to estimate class-specific parameters
for translation models. Fung and Wu (1995) adapt
English tags for Chinese language modeling using
Coerced Markov Models. They use English POS
classes as states of the Markov Model to generate
Chinese language words. In this paper we use POS
tag information to incorporate prior knowledge of
word translation and to model local word order vari-
ation. We show that using this information can help
in the translation modeling task.
Many alignment models assume a one to many
mapping from source language words to target lan-
guage words, such as the IBM models 1-5 of Brown
et al (1993) and the HMM alignment model of (Vo-
gel et al, 1996). In addition, the IBM Models 3,
4 and 5 include a fertility model    where  is
the number of words aligned to a source word  . In
HMM-based alignment word fertilities are not mod-
eled. The alignment positions of target words are the
states in an HMM. The alignment probabilities for
word 
 depend only on the alignment of the pre-
vious word 


if using a first order HMM. There-
fore, source words are not awarded/penalized for be-
ing aligned to more than one target word. We present
an extension to HMM alignment that approximately
models word fertility.
Another assumption of existing alignment mod-
els is that there is a special Null word in the source
sentence from which all target words that do not
have other correspondences in the source language
are generated. Use of such a Null word has proven
problematic in many models. We also assume the
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 87-94.
                         Proceedings of the Conference on Empirical Methods in Natural
existence of a special Null word in the source lan-
guage that generates words in the target language.
However, we define a different model that better
constrains and conditions generation from Null. We
assume that the generation probability of words by
Null depends on other words in the target sentence.
Next we present the general equations for decom-
position of the translation probability using part of
speech tags and later we will go into more detail of
our extensions.
2 Part of Speech Tags in a Translation
Model
Augmenting the model     
 Feature Selection for a Rich HPSG Grammar Using Decision Trees
Kristina Toutanova and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040, USA
Abstract
This paper examines feature selection for log linear
models over rich constraint-based grammar (HPSG)
representations by building decision trees over fea-
tures in corresponding probabilistic context free
grammars (PCFGs). We show that single decision
trees do not make optimal use of the available in-
formation; constructed ensembles of decision trees
based on different feature subspaces show signifi-
cant performance gains (14% parse selection error
reduction). We compare the performance of the
learned PCFG grammars and log linear models over
the same features.
1 Introduction
Hand-built NLP grammars frequently have a depth
of linguistic representation and constraints not
present in current treebanks, giving them poten-
tial importance for tasks requiring deeper process-
ing. On the other hand, these manually built gram-
mars need to solve the disambiguation problem to
be practically usable.
This paper presents work on the problem of prob-
abilistic parse selection from among a set of al-
ternatives licensed by a hand-built grammar in the
context of the newly developed Redwoods HPSG
treebank (Oepen et al, 2002). HPSG (Head-driven
Phrase Structure Grammar) is a modern constraint-
based lexicalist (unification) grammar, described in
Pollard and Sag (1994).
The Redwoods treebank makes available syntac-
tic and semantic analyses of much greater depth
than, for example, the Penn Treebank. Therefore
there are a large number of features available that
could be used by stochastic models for disambigua-
tion. Other researchers have worked on extracting
features useful for disambiguation from unification
grammar analyses and have built log linear mod-
els a.k.a. Stochastic Unification Based Grammars
(Johnson et al, 1999; Riezler et al, 2000). Here
we also use log linear models to estimate condi-
tional probabilities of sentence analyses. Since fea-
ture selection is almost prohibitive for these mod-
els, because of high computational costs, we use
PCFG models to select features for log linear mod-
els. Even though this method may be expected to be
suboptimal, it proves to be useful. We select fea-
tures for PCFGs using decision trees and use the
same features in a conditional log linear model. We
compare the performance of the two models using
equivalent features.
Our PCFG models are comparable to branching
process models for parsing the Penn Treebank, in
which the next state of the model depends on a his-
tory of features. In most recent parsing work the his-
tory consists of a small number of manually selected
features (Charniak, 1997; Collins, 1997). Other
researchers have proposed automatically selecting
the conditioning information for various states of
the model, thus potentially increasing greatly the
space of possible features and selectively choosing
the best predictors for each situation. Decision trees
have been applied for feature selection for statistical
parsing models by Magerman (1995) and Haruno et
al. (1998). Another example of automatic feature
selection for parsing is in the context of a determin-
istic parsing model that chooses parse actions based
on automatically induced decision structures over
a very rich feature set (Hermjakob and Mooney,
1997).
Our experiments in feature selection using deci-
sion trees suggest that single decision trees may not
be able to make optimal use of a large number of rel-
evant features. This may be due to the greedy search
procedures or to the fact that trees combine informa-
tion from different features only through partition-
ing of the space. For example they have difficulty in
weighing evidence from different features without
fully partitioning the space.
A common approach to overcoming some of the
problems with decision trees ? such as reducing
their variance or increasing their representational
power ? has been building ensembles of decision
trees by, for example, bagging (Breiman, 1996) or
boosting (Freund and Schapire, 1996). Haruno et
al. (1998) have experimented with boosting deci-
sion trees, reporting significant gains. Our approach
is to build separate decision trees using different (al-
though not disjoint) subsets of the feature space and
then to combine their estimates by using the aver-
age of their predictions. A similar method based
on random feature subspaces has been proposed by
Ho (1998), who found that the random feature sub-
space method outperformed bagging and boosting
for datasets with a large number of relevant features
where there is redundancy in the features. Other ex-
amples of ensemble combination based on different
feature subspaces include Zheng (1998) who learns
combinations of Naive Bayes classifiers and Zenobi
and Cunningham (2001) who create ensembles of
kNN classifiers.
We begin by describing the information our HPSG
corpus makes available and the subset we have at-
tempted to use in our models. Next we describe our
ensembles of decision trees for learning parameter-
izations of branching process models. Finally, we
report parse disambiguation results for these models
and corresponding conditional log linear models.
2 Characteristics of the Treebank and
Features Used
The Redwoods treebank (Oepen et al, 2002) is
an under-construction treebank of sentences corre-
sponding to a particular HPSG grammar, the LinGO
ERG (Flickinger, 2000). The current preliminary
version contains 10,000 sentences of spoken di-
alog material drawn from the Verbmobil project.
The Redwoods treebank makes available the en-
tire HPSG signs for sentence analyses, but we have
used in our experiments only small subsets of this
representation. These are (i) derivation trees com-
posed of identifiers of lexical items and construc-
tions used to build the analysis, and (ii) semantic
dependency trees which encode semantic head-to-
head relations. The Redwoods treebank provides
deeper semantics expressed in the Minimum Recur-
sion Semantics formalism (Copestake et al, 2001),
but in the present experiments we have not explored
this fully.
The nodes in the derivation trees represent com-
bining rule schemas of the HPSG grammar, and not
IMPER
HCOMP
HCOMP
BSE_VERB_INFL
LET_V1
let
US
us
BSE_VERB_INFL
SEE_V3
see
Figure 1: Derivation tree for the sentence Let us see
phrasal categories of the standard sort. The whole
HPSG analyses can be recreated from the deriva-
tion trees, using the grammar. The preterminals of
the derivation trees are lexical labels. These are
much finer grained than Penn Treebank pretermi-
nals tags, and more akin to those used in Tree-
Adjoining Grammar models (Bangalore and Joshi,
1999). There are a total of about 8, 000 lexical la-
bels occurring in the treebank. One might conjec-
ture that a supertagging approach could go a long
way toward parse disambiguation. However, an up-
per bound for such an approach for our corpus is
below 55 percent parse selection accuracy, which
is the accuracy of an oracle tagger that chooses at
random among the parses having the correct tag se-
quence (Oepen et al, 2002).
The semantic dependency trees are labelled with
relations most of which correspond to words in the
sentence. These labels provide some abstraction be-
cause some classes of words have the same semantic
label ? for example all days of week are grouped
in one class, as are all numbers.
As an example the derivation tree for one analysis
of the short sentence Let us see is shown in figure 1.
The semantic dependency tree for the same sentence
is:
let_rel
pron_rel see_understand_rel
In addition to this information we have used the
main part of speech information of the lexical head
to annotate nodes in the derivation trees with labels
like verb, noun, preposition, etc.
Other information that we have not explored in-
cludes subcategorization information, lexical types
(these are a rich set of about 500 syntactic types), in-
dividual features such as tense, aspect, gender, etc.
Another resource is the type hierarchy which can be
explored to form equivalence classes on which to
base statistical estimation.
3 Models
3.1 Generative Models
We learn generative models that assign probabilities
to the derivation trees and the dependency trees. We
train these models separately and in the final stage
we combine them to yield a probability or score for
an entire sentence analysis. We rank the possible
analyses produced by the HPSG grammar in accor-
dance with the estimated scores.
We first describe learning such a generative
model for derivation trees using a single decision
tree and a set of available features. We will call
the set of available features {f1, . . . , fm} a history.
We estimate the probability of a derivation tree as
P (t) = ?n?t P (expansion(n)|history(n)). In
other words, the probability of the derivation tree
is the product of the probabilities of the expansion
of each node given its history of available features.
Given a training corpus of derivation trees cor-
responding to preferred analyses of sentences we
learn the distribution P (expansion|history) us-
ing decision trees. We used a standard decision
tree learning algorithm where splits are determined
based on gain ratio (Quinlan, 1993). We grew the
trees fully and we calculated final expansion prob-
abilities at the leaves by linear interpolation with
estimates one level above. This is a similar, but
more limited, strategy to the one used by Magerman
(1995).
The features over derivation trees which we made
available to the learner are shown in Table 1. The
node direction features indicate whether a node is a
left child, a right child, or a single child. A num-
ber of ancestor features were added to the history.
The grammar used, the LinGO ERG has rules which
are maximally binary, and the complements and ad-
juncts of a head are collected through multiple rules.
Moreover, it makes extensive use of unary rules for
various kinds of ?type changing? operations. A sim-
ple PCFG is reasonably effective to the extent that
important dependencies are jointly expressed in a
local tree, as is mostly the case for the much flatter
representations used in the Penn Treebank. Here,
this is not the case, and the inclusion of ancestor
nodes in the history makes necessary information
more often local in our models. Grandparent anno-
tation was used previously by Charniak and Carroll
(1994) and Johnson (1998).
No. Name Example
0 Node Label HCOMP
1 Parent Node Label HCOMP
2 Node Direction left
3 Parent Node Direction none
4 Grandparent Node Label IMPER
5 Great Grandparent Top ? yes
6 Left Sister Node Label HCOMP
7 Left Preterminal US
8 Preterminal to the Left of 7 LET_V1
9 Category of Node verb
Table 1: Features over derivation trees
No Name Example
0 Node Label let_rel
1 Direction of Dependent left
2 Number of Intervening Dependents 1
3 Parent Node Label top
4 Label to the Left or Right pron_rel
Table 2: Features over semantic dependency trees
Similarly we learn generative models over se-
mantic dependency trees. For these trees the ex-
pansion of a node is viewed as consisting of sepa-
rate trials for each dependent. Any conditional de-
pendencies among children of a node can be cap-
tured by expanding the history. The features used
for the semantic dependency trees are shown in Ta-
ble 2. This set of only 5 features for semantic trees
makes the feature subset selection method less ap-
plicable since there is no obvious redundancy in the
set. However the method still outperforms a single
decision tree. The model for generation of semantic
dependents to the left and right is as follows: First
the left dependents are generated from right to left
given the head, its parent, right sister, and the num-
ber of dependents to the left that have already been
generated. After that, the right dependents are gen-
erated from left to right, given the head, its parent,
left sister and number of dependents to the right that
have already been generated. We also add stop sym-
bols at the ends to the left and right. This model
is very similar to the markovized rule models in
Collins (1997). For example, the joint probability
of the dependents of let_rel in the above example
would be:
P (stop|let_rel,left,0,top,none)?
P (pron_rel|let_rel,right,0,top,stop)?
P (see_understand_rel|let_rel,right,1,top,pron_rel)?
P (stop|let_rel,right,2,top,see_understand_rel)
3.2 Conditional Log Linear Models
A conditional log linear model for estimating the
probability of an HPSG analysis given a sentence has
a set of features {f1, . . . , fm} defined over analyses
and a set of corresponding weights {?1, . . . , ?m}
for them. In this work we have defined features over
derivation trees and syntactic trees as described for
the branching process models.
For a sentence s with possible analyses t1, . . . , tk,
the conditional probability for analysis ti is given
by:
P (ti|s) =
exp
(?m
j=1 fj(ti)?j
)
?k
i?=1 exp
(?m
j=1 fj(ti?)?j
) (1)
As in Johnson et al (1999) we trained the model
by maximizing the conditional likelihood of the
preferred analyses and using a Gaussian prior for
smoothing (Chen and Rosenfeld, 1999). We used
conjugate gradient for optimization.
Given an ensemble of decision trees estimating
probabilities P (expansion|history) we define fea-
tures for a corresponding log linear model as fol-
lows: For each path from the root to a leaf in any of
the decision trees, and for each possible expansion
for that path that was seen in the training set, we
add a feature feh(t). For a tree t, this feature has as
value the number of time the expansion e occurred
in t with the history h.
4 Experiments
We present experimental results comparing the
parse ranking performance of different models. The
accuracy results are averaged over a ten-fold cross-
validation on the data set summarized in Table 3.
The sentences in this data set have exactly one pre-
ferred parse selected by a human annotator. At this
early stage, the treebank is expected to be noisy be-
cause all annotation was done by a single annotator.
Accuracy results denote the percentage of test sen-
tences for which the highest ranked analysis was the
correct one. This measure scores whole sentence ac-
curacy and is therefore more strict than the labelled
precision/recall measures and more appropriate for
the task of parse ranking. When a model ranks a
set of m parses highest with equal scores and one of
those parses is the preferred parse in the treebank,
we compute the accuracy on this sentence as 1/m.
To give an idea about the difficulty of the task on
the corpus we have used, we also show a baseline
which is the expected accuracy of choosing a parse
sentences length lex ambig struct ambig
5277 7.0 4.1 7.3
Table 3: Annotated corpus used in experiments: The
columns are, from left to right, the total number of sen-
tences, average length, and average lexical and structural
ambiguity
Model Generative Log Linear
Test Train Test Train
Random 26.00 26.00 26.00 26.00
PCFG-S 67.27 72.23 79.34 85.31
PCFG-GP 72.39 83.89 81.52 91.56
PCFG-DTAll 75.57 96.51 81.82 97.61
Table 4: Parse ranking accuracy of syntactic mod-
els: single decision tree compared to simpler mod-
els
at random and accuracy results from simpler models
that have been used broadly in NLP. PCFG-S is a
simple PCFG model where we only have the node
label (feature 0) in the history, and PCFG-GP has
only the node and its parent?s labels (features 0 and
1) as in PCFG grammars with grandparent annota-
tion.
Table 4 shows the accuracy of parse selection of
the three simple models mentioned above defined
over derivation trees and the accuracy achieved by
a single decision tree (PCFG-DTAll) using all fea-
tures in Table 1. The third column contains accuracy
results for log linear models using the same features.
We can note from Table 4 that the genera-
tive models greatly benefit from the addition of
more conditioning information, while the log lin-
ear model performs very well even with only simple
rule features, and its accuracy does not increase so
sharply with the addition of more complex features.
The error reduction from PCFG-S to PCFG-DTAll
is 25.36%, while the corresponding error reduction
for the log linear model is 12%. The error reduction
for the log linear model from PCFG-GP to PCFG-
DTAll is very small which suggests an overfitting ef-
fect. PCFG-S is doing much worse than the log lin-
ear model with the same features, and this is true for
the training data as well as for the test data. A partial
explanation for this is the fact that PCFG-S tries to
maximize the likelihood of the correct parses under
strong independence assumptions, whereas the log
linear model need only worry about making the cor-
rect parses more probable than the incorrect ones.
Next we show results comparing the single deci-
Type of Model All Features Feature Subspaces
PCFG Log linear PCFG Log linear
Derivation Trees 75.57 81.82 78.97 82.24
Dependency Trees 67.38 69.91 68.88 73.50
Combined Feature Subspaces Accuracy 80.10 83.32
Table 5: Parse ranking accuracy: single decision trees and ensembles
sion tree model (PCFG-DTAll) to an ensemble of 11
decision trees based on different feature subspaces.
The decision trees in the ensemble are used to rank
the possible parses of a sentence individually and
then their votes are combined using a simple ma-
jority vote. The sets of features in each decision
tree are obtained by removing two features from the
whole space. The left preterminal features (features
with numbers 7 and 8) participate in only one de-
cision tree. Also, features 2, 3, and 5 participate
in all decision trees since they have very few pos-
sible values and should not partition the space too
quickly. The feature space of each of the 10 de-
cision trees not containing the left preterminal fea-
tures was formed by removing two of the features
from among those with numbers {0, 1, 4, 6, and 9}
from the initial feature space (minus features 7 and
8). This method for constructing feature subspaces
is heuristic, but is based on the intuition of removing
the features that have the largest numbers of possi-
ble values.1
Table 5 shows the accuracy results for mod-
els based on derivation trees, semantic dependency
trees, and a combined model. The first row shows
parse ranking accuracy using derivation trees of
generative and log linear models over the same fea-
tures. Results are shown for features selected by a a
single decision tree, and an ensemble of 11 decision
tree models based on different feature subspaces as
described above. The relative improvement in accu-
racy of the log linear model from single to multiple
decision trees is fairly small.
The second row shows corresponding models for
the semantic dependency trees. Since there are a
small number of features used for this task, the
performance gain from using feature subspaces is
1We also preformed an experiment where we removed ev-
ery combination of two features from the whole space of fea-
tures 0?8 to obtain subspaces. This results in a large number
of feature subspaces (36). The performance of this method was
slightly worse than the result reported in Table 5 (78.52%). We
preferred to work with an ensemble of 11 decision trees for
computational reasons.
not so large. It should be noted that there is a
90.9% upper bound on parse ranking accuracy us-
ing semantic trees only. This is because for many
sentences there are several analyses with the same
semantic dependency structure. Interestingly, for
semantic trees the difference between the log lin-
ear and generative models is not so large. Finally,
the last row shows the combination of models over
derivation trees and semantic trees. The feature sub-
space ensemble of 11 decision tree models for the
derivation trees is combined with the ensemble of
5 feature subspace models over semantic dependen-
cies to yield a larger ensemble that ranks possible
sentence analyses based on weighted majority vote
(with smaller weights for the semantic models). The
improvement for PCFG models from combining the
syntactic and semantic models is about 5.4% error
reduction from the error rate of the better (syntac-
tic) models. The corresponding log linear model
contains all features from the syntactic and semantic
decision trees in the ensemble. The error reduction
due to the addition of semantics is 6.1% for the log
linear model. Overall the gains from using semantic
information are not as good as we expected. Further
research remains to be done in this area.
The results show that decision trees and ensem-
bles of decision trees can be used to greatly improve
the performance of generative models over deriva-
tion trees and dependency trees. The performance
of generative models using a lot of conditioning in-
formation approaches the performance of log linear
models although the latter remain clearly superior.
The corresponding improvement in log linear mod-
els when adding more complex features is not as
large as the improvement in generative models. On
the other hand, there might be better ways to incor-
porate the information from additional history in log
linear models.
5 Error Analysis
It is interesting to see what the hard disambiguation
decisions are, that the combined syntactic-semantic
models can not at present get right.
We analyzed some of the errors made by the best
log linear model defined over derivation trees and
semantic dependency trees. We selected for analysis
sentences that the model got wrong on one of the
training - test splits in the 10 fold cross-validation
on the whole corpus. The error analysis suggests
the following breakdown:
? About 40% of errors are due to inconsistency
or errors in annotation
? About 15% of the errors are due to grammar
limitations
? About 45% of the errors are real errors and we
could hope to get them right
The inconsistency in annotation hurts the perfor-
mance of the model both when in the training data
some sentences were annotated incorrectly and the
model tried to fit its parameters to explain them, and
when in the test data the model chose the correct
analysis but it was scored as incorrect because of
incorrect annotation. It is not straightforward to de-
tect inconsistencies in the training data by inspect-
ing test data errors. Therefore the percentages we
have reported are not exact.
The log linear model seems to be more suscepti-
ble to errors in the training set annotation than the
PCFG models, because it can easily adjust its pa-
rameters to fit the noise (causing overfitting), espe-
cially when given a large number of features. This
might partly explain why the log linear model does
not profit greatly over this data set from the addition
of a large number of features.
A significant portion of the real errors made by
the model are PP attachment errors. Another class
of errors come from parallel structures and long dis-
tance dependencies. For example, the model did
not disambiguate correctly the sentence Is anywhere
from two thirty to five on Thursday fine?, preferring
the interpretation from [two thirty] to [five on Thurs-
day] rather than what would be the more common
meaning [from [two thirty] to [five]] [on Thurs-
day]. This disambiguation decision seems to re-
quire common world knowledge or it might be ad-
dressable with addition of knowledge about paral-
lel structures. ( (Johnson et al, 1999) add features
measuring parallelism).
We also compared the errors made by the best log
linear model using only derivation tree features to
the ones made by the combined model. The large
majority of the errors made by the combined model
were also made by the syntactic model. Examples
of errors corrected with the help of semantic infor-
mation include:
The sentence How about on the twenty fourth
Monday? (punctuation is not present in the corpus)
was analyzed by the model based on derivation trees
to refer to the Monday after twenty three Mondays
from now, whereas the more common interpretation
would be that the day being referred to is the twenty
fourth day of the month, and it is also a Monday.
There were several errors of this sort corrected by
the dependency trees model.
Another interesting error corrected by the seman-
tic model was for the sentence: We will get a cab
and go. The syntactic model chose the interpreta-
tion of that sentence in the sense: We will become
a cab and go, which was overruled by the semantic
model.
6 Conclusions and Future Work
We have presented work on building probabilistic
models for HPSG parse disambiguation. As the
number of available features increases it becomes
more important to select relevant features automati-
cally. We have shown that decision trees using dif-
ferent feature subspaces can be combined in ensem-
bles that choose the correct parse with higher accu-
racy than individual models.
Our plans for future work include exploring more
information from the HPSG signs and defining fea-
tures that capture long distance dependencies. An-
other line of future research is defining models over
the deeper MRS semantic representations, possibly
in conjunction with clustering of semantic types.
7 Acknowledgements
We would particularly like to thank Stephan Oepen,
for directing the Redwoods treebanking project and
getting us set up with the HPSG development envi-
ronment, and Dan Flickinger, for explanations of
the LinGO ERG and help with the error analysis.
Thanks also to Stuart Shieber and other participants
in the Redwoods project meetings for many discus-
sions. And our thanks to Dan Klein for letting us
use his implementation of conjugate gradient, and
to the anonymous CoNLL-2002 reviewers for help-
ful comments. This paper is based upon work sup-
ported in part by the National Science Foundation
under Grant No. 0085896.
References
Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2).
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Eugene Charniak and G. Carroll. 1994. Context-
sensitive statistics for improved grammatical lan-
guage models. In Proceedings of the Twelth
National Conference on Artificial Intelligence,
pages 742 ? 747, Seattle, WA.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference
on Artificial Intelligence, pages 598 ? 603, Provi-
dence, RI.
S. Chen and R. Rosenfeld. 1999. A gaussian prior
for smoothing maximum entropy models. In
Technical Report CMUCS -99-108.
Michael John Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Proceed-
ings of the 35th Meeting of the Association for
Computational Linguistics and the 7th Confer-
ence of the European Chapter of the ACL, pages
16 ? 23, Madrid, Spain.
Ann Copestake, Alex Lascarides, and Dan Flickin-
ger. 2001. An algebra for semantic construction
in constraint-based grammars. In Proceedings of
the 39th Meeting of the Association for Compu-
tational Linguistics, Toulouse, France.
Dan Flickinger. 2000. On building a more effi-
cient grammar by exploiting types. Natural Lan-
guage Engineering, 6 (1) (Special Issue on Effi-
cient Processing with HPSG):15 ? 28.
Yoav Freund and Robert E. Schapire. 1996. Exper-
iments with a new boosting algorithm. In Inter-
national Conference on Machine Learning, pages
148?156.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1998. Using decision trees to con-
struct a practical parser. In Proceedings of the
36th Meeting of the Association for Computa-
tional Linguistics, pages 505 ? 511.
Ulf Hermjakob and Reymond J. Mooney. 1997.
Learning parse and translation decisions from ex-
amples with rich context. In Proceedings of the
35th Meeting of the Association for Computa-
tional Linguistics and the 7th Conference of the
European Chapter of the ACL, pages 482 ? 489.
Tin Kam Ho. 1998. The random subspace method
for constructing decision forests. IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, 20(8):832?844.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estimators
for stochastic ?unification-based? grammars. In
Proceedings of the 37th Meeting of the Associ-
ation for Computational Linguistics, pages 535 ?
541, College Park, MD.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24:613?632.
D. M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd
Meeting of the Association for Computational
Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thor-
sten Brants. 2002. The LinGo Redwoods tree-
bank: Motivation and preliminary applications.
In COLING 19.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press.
J. R. Quinlan. 1993. C4.5 Programs for Machine
Learning. Morgan Kaufmann.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and
Mark Johnson. 2000. Lexicalized stochastic
modeling of constraint-based grammars using
log-linear measures and EM training. In Pro-
ceedings of the 38th Meeting of the Association
for Computational Linguistics, Hong Kong.
Gabriele Zenobi and Padraig Cunningham. 2001.
Using diversity in preparing ensembles of classi-
fiers based on different feature subsets to mini-
mize generalization error. In ECML, pages 576?
587.
Z. Zheng. 1998. Naive Bayesian classifier commit-
tees. In ECML, pages 196?207.
The Leaf Projection Path View of Parse Trees: Exploring String Kernels for
HPSG Parse Selection
Kristina Toutanova
CS Dept, Stanford University
353 Serra Mall
Stanford 94305, CA
USA,
kristina@cs.stanford.edu
Penka Markova
EE Dept, Stanford University
350 Serra Mall
Stanford 94305, CA,
USA,
penka@cs.stanford.edu
Christopher Manning
CS Dept, Stanford University
353 Serra Mall
Stanford 94305, CA,
USA,
manning@cs.stanford.edu
Abstract
We present a novel representation of parse trees as
lists of paths (leaf projection paths) from leaves to
the top level of the tree. This representation allows
us to achieve significantly higher accuracy in the
task of HPSG parse selection than standard models,
and makes the application of string kernels natural.
We define tree kernels via string kernels on projec-
tion paths and explore their performance in the con-
text of parse disambiguation. We apply SVM rank-
ing models and achieve an exact sentence accuracy
of 85.40% on the Redwoods corpus.
1 Introduction
In this work we are concerned with building sta-
tistical models for parse disambiguation ? choos-
ing a correct analysis out of the possible analyses
for a sentence. Many machine learning algorithms
for classification and ranking require data to be rep-
resented as real-valued vectors of fixed dimension-
ality. Natural language parse trees are not readily
representable in this form, and the choice of repre-
sentation is extremely important for the success of
machine learning algorithms.
For a large class of machine learning algorithms,
such an explicit representation is not necessary, and
it suffices to devise a kernel function  
	 which
measures the similarity between inputs  and  . In
addition to achieving efficient computation in high
dimensional representation spaces, the use of ker-
nels allows for an alternative view on the mod-
elling problem as defining a similarity between in-
puts rather than a set of relevant features.
In previous work on discriminative natural lan-
guage parsing, one approach has been to define fea-
tures centered around lexicalized local rules in the
trees (Collins, 2000; Shen and Joshi, 2003), simi-
lar to the features of the best performing lexicalized
generative parsing models (Charniak, 2000; Collins,
1997). Additionally non-local features have been
defined measuring e.g. parallelism and complexity
of phrases in discriminative log-linear parse ranking
models (Riezler et al, 2000).
Another approach has been to define tree kernels:
for example, in (Collins and Duffy, 2001), the all-
subtrees representation of parse trees (Bod, 1998)
is effectively utilized by the application of a fast
dynamic programming algorithm for computing the
number of common subtrees of two trees. Another
tree kernel, more broadly applicable to Hierarchi-
cal Directed Graphs, was proposed in (Suzuki et al,
2003). Many other interesting kernels have been de-
vised for sequences and trees, with application to se-
quence classification and parsing. A good overview
of kernels for structured data can be found in (Gaert-
ner et al, 2002).
Here we propose a new representation of parse
trees which (i) allows the localization of broader
useful context, (ii) paves the way for exploring ker-
nels, and (iii) achieves superior disambiguation ac-
curacy compared to models that use tree representa-
tions centered around context-free rules.
Compared to the usual notion of discriminative
models (placing classes on rich observed data) dis-
criminative PCFG parsing with plain context free
rule features may look naive, since most of the fea-
tures (in a particular tree) make no reference to ob-
served input at all. The standard way to address this
problem is through lexicalization, which puts an el-
ement of the input on each tree node, so all features
do refer to the input. This paper explores an alterna-
tive way of achieving this that gives a broader view
of tree contexts, extends naturally to exploring ker-
nels, and performs better.
We represent parse trees as lists of paths (leaf pro-
jection paths) from words to the top level of the tree,
which includes both the head-path (where the word
is a syntactic head) and the non-head path. This al-
lows us to capture for example cases of non-head
dependencies which were also discussed by (Bod,
1998) and were used to motivate large subtree fea-
tures, such as ?more careful than his sister? where
?careful? is analyzed as head of the adjective phrase,
but ?more? licenses the ?than? comparative clause.
This representation of trees as lists of projection
IMPER verb
HCOMPverb
HCOMPverb
LET V1
let
US
us
HCOMP verb
PLAN ON V2
plan
HCOMP prep*
ON
on
THAT DEIX
that
Figure 1: Derivation tree for the sentence Let us
plan on that.
paths (strings) allows us to explore string kernels on
these paths and combine them into tree kernels.
We apply these ideas in the context of parse
disambiguation for sentence analyses produced by
a Head-driven Phrase Structure Grammar (HPSG),
the grammar formalism underlying the Redwoods
corpus (Oepen et al, 2002). HPSG is a modern
constraint-based lexicalist (or ?unification?) gram-
mar formalism.1 We build discriminative mod-
els using Support Vector Machines for ranking
(Joachims, 1999). We compare our proposed rep-
resentation to previous approaches and show that it
leads to substantial improvements in accuracy.
2 The Leaf Projection Paths View of Parse
Trees
2.1 Representing HPSG Signs
In HPSG, sentence analyses are given in the form
of HPSG signs, which are large feature structures
containing information about syntactic and seman-
tic properties of the phrases.
As in some of the previous work on the Red-
woods corpus (Toutanova et al, 2002; Toutanova
and Manning, 2002), we use the derivation trees as
the main representation for disambiguation. Deriva-
tion trees record the combining rule schemas of
the HPSG grammar which were used to license
the sign by combining initial lexical types. The
derivation tree is also the fundamental data stored
in the Redwoods treebank, since the full sign can
be reconstructed from it by reference to the gram-
mar. The internal nodes represent, for example,
head-complement, head-specifier, and head-adjunct
schemas, which were used to license larger signs
out of component parts. A derivation tree for the
1For an introduction to HPSG, see (Pollard and Sag, 1994).
IMPER verb
HCOMP verb
HCOMP verb
LET V1
let (v sorb)
IMPER verb
HCOMP verb
HCOMP verb
PLAN ON V2
plan (v e p itrs)
IMPER verb
HCOMP verb
HCOMP verb
HCOMP prep*
ON
on (p reg)
Figure 2: Paths to top for three leaves. The nodes
in bold are head nodes for the leaf word and the rest
are non-head nodes.
sentence Let us plan on that is shown in Figure 1. 2
Additionally, we annotate the nodes of the deriva-
tion trees with information extracted from the HPSG
sign. The annotation of nodes is performed by ex-
tracting values of feature paths from the feature
structure or by propagating information from chil-
dren or parents of a node. In theory with enough
annotation at the nodes of the derivation trees, we
can recover the whole HPSG signs.
Here we describe three node annotations that
proved very useful for disambiguation. One is
annotation with the values of the feature path
synsem.local.cat.head ? its values are basic parts
of speech such as noun, verb, prep, adj, adv. An-
other is phrase structure category information asso-
ciated with the nodes, which summarizes the values
of several feature paths and is available in the Red-
woods corpus as Phrase-Structure trees. The third is
annotation with lexical type (le-type), which is the
type of the head word at a node. The preterminals in
Figure 1 are lexical item identifiers ? identifiers of
the lexical entries used to construct the parse. The
le-types are about   types in the HPSG type hier-
archy and are the direct super-types of the lexical
item identifiers. The le-types are not shown in this
figure, but can be seen at the leaves in Figure 2. For
example, the lexical type of LET V1 in the figure is
v sorb. In Figure 1, the only annotation performed
is with the values of synsem.local.cat.head.
2.2 The Leaf Projection Paths View
The projection path of a leaf is the sequence of
nodes from the leaf to the root of the tree. In Figure
2, the leaf projection paths for three of the words
are shown.
We can see that a node in the derivation tree par-
2This sentence has three possible analyses depending on the
attachment of the preposition ?on? and whether ?on? is an ad-
junct or complement of ?plan?.
ticipates in the projection paths of all words domi-
nated by that node. The original local rule config-
urations ? a node and its children, do not occur
jointly in the projection paths; thus, if special anno-
tation is not performed to recover it, this informa-
tion is lost.
As seen in Figure 2, and as is always true for a
grammar that produces non-crossing lexical depen-
dencies, there is an initial segment of the projec-
tion path for which the leaf word is a syntactic head
(called head path from here on), and a final segment
for which the word is not a syntactic head (called
non-head path from here on). In HPSG non-local
dependencies are represented in the final semantic
representation, but can not be obtained via syntactic
head annotation.
If, in a traditional parsing model that estimates
the likelihood of a local rule expansion given a node
(such as e.g (Collins, 1997)), the tree nodes are an-
notated with the word of the lexical head, some in-
formation present in the word projection paths can
be recovered. However, this is only the information
in the head path part of the projection path. In fur-
ther experiments we show that the non-head part of
the projection path is very helpful for disambigua-
tion.
Using this representation of derivation trees, we
can apply string kernels to the leaf projection paths
and combine those to obtain kernels on trees. In the
rest of this paper we explore the application of string
kernels to this task, comparing the performance of
the new models to models using more standard rule
features.
3 Tree and String Kernels
3.1 Kernels and SVM ranking
From a machine learning point of view, the parse se-
lection problem can be formulated as follows: given
  training examples (     	 
 	    		 ,
where each   is a natural language sentence,   is
the number of such sentences,     ,    is
a parse tree for  , Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 173?176, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Joint Model for Semantic Role Labeling
Aria Haghighi
Dept of Computer Science
Stanford University
Stanford, CA, 94305
aria42@stanford.edu
Kristina Toutanova
Dept of Computer Science
Stanford University
Stanford, CA, 94305
kristina@cs.stanford.edu
Christopher D. Manning
Dept of Computer Science
Stanford University
Stanford, CA, 94305
manning@cs.stanford.edu
Abstract
We present a semantic role labeling sys-
tem submitted to the closed track of the
CoNLL-2005 shared task. The system, in-
troduced in (Toutanova et al, 2005), im-
plements a joint model that captures de-
pendencies among arguments of a predi-
cate using log-linear models in a discrimi-
native re-ranking framework. We also de-
scribe experiments aimed at increasing the
robustness of the system in the presence
of syntactic parse errors. Our final system
achieves F1-Measures of 76.68 and 78.45
on the development and the WSJ portion
of the test set, respectively.
1 Introduction
It is evident that there are strong statistical patterns
in the syntactic realization and ordering of the argu-
ments of verbs; for instance, if an active predicate
has an A0 argument it is very likely to come before
an A1 argument. Our model aims to capture such de-
pendencies among the labels of nodes in a syntactic
parse tree.
However, building such a model is computation-
ally expensive. Since the space of possible joint la-
belings is exponential in the number of parse tree
nodes, a model cannot exhaustively consider these
labelings unless it makes strong independence as-
sumptions. To overcome this problem, we adopt
a discriminative re-ranking approach reminiscent of
(Collins, 2000). We use a local model, which la-
bels arguments independently, to generate a smaller
number of likely joint labelings. These candidate la-
belings are in turn input to a joint model which can
use global features and re-score the candidates. Both
the local and global re-ranking models are log-linear
(maximum entropy) models.
In the following sections, we briefly describe our
local and joint models and the system architecture
for combining them. We list the features used by our
models, with an emphasis on new features, and com-
pare the performance of a local and a joint model on
the CoNLL shared task. We also study an approach
to increasing the robustness of the semantic role la-
beling system to syntactic parser errors, by consid-
ering multiple parse trees generated by a statistical
parser.
2 Local Models
Our local model labels nodes in a parse tree inde-
pendently. We decompose the probability over la-
bels (all argument labels plus NONE), into a product
of the probability over ARG and NONE, and a prob-
ability over argument labels given that a node is an
ARG. This can be seen as chaining an identification
and a classification model. The identification model
classifies each phrase as either an argument or non-
argument and our classification model labels each
potential argument with a specific argument label.
The two models use the same features.
Previous research (Gildea and Jurafsky, 2002;
Pradhan et al, 2004; Carreras and Ma`rquez, 2004)
has identified many useful features for local iden-
tification and classification. Below we list the fea-
tures and hand-picked conjunctions of features used
in our local models. The ones denoted with asterisks
(*) were not present in (Toutanova et al, 2005). Al-
though most of these features have been described in
previous work, some features, described in the next
section, are ? to our knowledge ? novel.
173
? Phrase-Type Syntactic category of node
? Predicate Lemma Stemmed target verb
? Path Sequence of phrase types between the predicate and
node, with ?, ? to indicate direction
? Position Before or after predicate
? Voice Voice of predicate
? Head-Word of Phrase
? Head-POS POS tag of head word
? Sub-Cat CFG expansion of predicate?s parent
? First/Last Word
? Left/Right Sister Phrase-Type
? Left/Right Sister Head-Word/Head-POS
? Parent Phrase-Type
? Parent POS/Head-Word
? Ordinal Tree Distance Phrase-type concatenated with the
length of the Path feature
? Node-LCA Partial Path Path from the node to the lowest
common ancestor of the predicate and the node
? PP Parent Head-Word If the parent of the node is a PP, the
parent?s head-word
? PP NP Head-Word/Head-POS For a PP, retrieve the head-
word /head-POS of its rightmost NP
? Temporal Keywords* Is the head of the node a temporal
word e.g ?February? or ?afternoon?
? Missing subject* Is the predicate missing a subject in
the?standard? location
? Projected path* Path from the maximal extended projection
of the predicate to the node
? Predicate Lemma & Path
? Predicate Lemma & Head-Word
? Predicate Lemma & Phrase-Type
? Voice & Position
? Predicate Lemma & PP Parent Head-Word
? Path & Missing subject*
? Projected path & Missing subject*
2.1 Additional Local Features
We found that a large source of errors for A0 and A1
stemmed from cases such as those illustrated in Fig-
ure 1, where arguments were dislocated by raising
or controlling verbs. Here, the predicate, expected,
does not have a subject in the typical position ? in-
dicated by the empty NP ? since the auxiliary is has
raised the subject to its current position. In order to
capture this class of examples, we use a binary fea-
ture, Missing Subject, indicating whether the pred-
icate is ?missing? its subject, and use this feature in
conjunction with the Path feature, so that we learn
typical paths to raised subjects conditioned on the
absence of the subject in its typical position.
In the particular case of Figure 1, there is an-
other instance of an argument being quite far from
SPPPP

NPi-A1aaa
!!!
the trade gap
VPPPPP

is SPPPP

NPi-A1
-NONE-
VP
HHH

expected VP
QQ
to widen
Figure 1: Example of displaced arguments
its predicate. The predicate widen shares the trade
gap with expect as a A1 argument. However, as ex-
pect is a raising verb, widen?s subject is not in its
typical position either, and we should expect to find
it in the same positions as expected?s subject. This
indicates it may be useful to use the path relative to
expected to find arguments for widen. In general,
to identify certain arguments of predicates embed-
ded in auxiliary and infinitival VPs we expect it to
be helpful to take the path from the maximum ex-
tended projection of the predicate ? the highest VP
in the chain of VP?s dominating the predicate. We
introduce a new path feature, Projected Path, which
takes the path from the maximal extended projec-
tion to an argument node. This feature applies only
when the argument is not dominated by the maxi-
mal projection, (e.g., direct objects). These features
also handle other cases of discontinuous and non-
local dependencies, such as those arising due to con-
troller verbs. For a local model, these new features
and their conjunctions improved F1-Measure from
73.80 to 74.52 on the development set. Notably, the
F1-Measure of A0 increased from 81.02 to 83.08.
3 Joint Model
Our joint model, in contrast to the local model, col-
lectively scores a labeling of all nodes in the parse
tree. The model is trained to re-rank a set of N likely
labelings according to the local model. We find the
exact top N consistent1 most likely local model la-
belings using a simple dynamic program described
in (Toutanova et al, 2005).
1A labeling is consistent if satisfies the constraint that argu-
ment phrases do not overlap.
174
SNP1-A1
Crude oil prices
VP
VBD-V
fell
PP1-A3
TO
to
NP
$27.80
PP2-A4
FROM
from
NP
$37.80
NP2-AM-TMP
yesterday
Figure 2: An example tree with semantic role annotations.
Most of the features we use are described in more
detail in (Toutanova et al, 2005). Here we briefly
describe these features and introduce several new
joint features (denoted by *). A labeling L of all
nodes in the parse tree specifies a candidate argu-
ment frame ? the sequence of all nodes labeled with
a non-NONE label according to L. The joint model
features operate on candidate argument frames, and
look at the labels and internal features of the candi-
date arguments. We introduce them in the context
of the example in Figure 2. The candidate argument
frame corresponding to the correct labeling for the
tree is: [NP1-A1,VBD-V,PP1-A3,PP2-A4,NP2-AM-TMP].
? Core arguments label sequence: The sequence
of labels of core arguments concatenated with
the predicate voice. Example: [voice:active:
A1,V,A3,A4] A back-off feature which substitutes
specific argument labels with a generic argument
(A) label is also included.
? Flattened core arguments label sequence*:
Same as the previous but merging consecutive
equal labels.
? Core arguments label and annotated phrase
type sequence: The sequence of labels of core
arguments together with annotated phrase types.
Phrase types are annotated with the head word for
PP nodes, and with the head POS tag for S and VP
nodes. Example: [voice:active: NP-A1,V,PP-to-
A3,PP-from-A4]. A back-off to generic A labels
is also included. Also a variant that adds the pred-
icate stem.
? Repeated core argument labels with phrase
types: Annotated phrase types for nodes with
the same core argument label. This feature cap-
tures, for example, the tendency of WHNP refer-
ring phrases to occur as the second phrase having
the same label as a preceding NP phrase.
? Repeated core argument labels with phrase
types and sister/adjacency information*: Sim-
ilar to the previous feature, but also indicates
whether all repeated arguments are sisters in the
parse tree, or whether all repeated arguments are
adjacent in terms of word spans. These features
can provide robustness to parser errors, making it
more likely to label adjacent phrases incorrectly
split by the parser with the same label.
4 Combining Local and Joint Models
It is useful to combine the joint model score with
a local model score, because the local model has
been trained using all negative examples, whereas
the joint model has been trained only on likely
argument frames . Our final score is given by
a mixture of the local and joint model?s log-
probabilities: scoreSRL(L|t) = ? score`(L|t) +
scoreJ(L|t), where score`(L|t) is the local score of
L, scoreJ(L|t) is the corresponding joint score, and
? is a tunable parameter. We search among the top
N candidate labelings proposed by the local model,
for the labeling that maximizes the final score.
5 Increasing Robustness to Parser Errors
It is apparent that role labeling is very sensitive to the
correctness of the given parse tree. If an argument
does not correspond to a constituent in a parse tree,
our model will not be able to consider the correct
phrase.
One way to address this problem is to utilize alter-
native parses. Recent releases of the Charniak parser
(Charniak, 2000) have included an option to provide
the top k parses of a given sentence according to
the probability model of the parser. We use these
alternative parses as follow: Suppose t1, . . . , tk are
trees for sentence s with given probabilities P (ti|s)
by the parser. Then for a fixed predicate v, let Li
175
Precision Recall F?=1
Development 77.66% 75.72% 76.68
Test WSJ 79.54% 77.39% 78.45
Test Brown 70.24% 65.37% 67.71
Test WSJ+Brown 78.34% 75.78% 77.04
Test WSJ Precision Recall F?=1
Overall 79.54% 77.39% 78.45
A0 88.32% 88.30% 88.31
A1 78.61% 78.40% 78.51
A2 72.55% 68.11% 70.26
A3 73.08% 54.91% 62.71
A4 77.42% 70.59% 73.85
A5 100.00% 80.00% 88.89
AM-ADV 58.20% 51.19% 54.47
AM-CAU 63.93% 53.42% 58.21
AM-DIR 52.56% 48.24% 50.31
AM-DIS 76.56% 80.62% 78.54
AM-EXT 73.68% 43.75% 54.90
AM-LOC 61.52% 55.92% 58.59
AM-MNR 58.33% 56.98% 57.65
AM-MOD 97.85% 99.09% 98.47
AM-NEG 97.41% 98.26% 97.84
AM-PNC 49.50% 43.48% 46.30
AM-PRD 100.00% 20.00% 33.33
AM-REC 0.00% 0.00% 0.00
AM-TMP 74.85% 67.34% 70.90
R-A0 92.63% 89.73% 91.16
R-A1 81.53% 82.05% 81.79
R-A2 61.54% 50.00% 55.17
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 28.57% 33.33% 30.77
R-AM-TMP 61.54% 76.92% 68.38
V 97.32% 97.32% 97.32
Table 1: Overall results (top) and detailed results
on the WSJ test (bottom) on the closed track of the
CoNLL shared task.
denote the best joint labeling of tree ti, with score
scoreSRL(Li|ti) according to our final joint model.
Then we choose the labeling L which maximizes:
arg max
i?{1,...,k}
? log P (ti|S) + scoreSRL(Li|ti) (1)
Considering top k = 5 parse trees using this al-
gorithm resulted in up to 0.4 absolute increase in
F-Measure. In future work, we plan to experiment
with better ways to combine information from mul-
tiple parse trees.
6 Experiments and Results
For our final results we used a joint model with ? =
1.5 (local model weight), ? = 1 (parse tree log-
probability weight) , N = 15 (candidate labelings
from the local model to consider) , and k = 5 (num-
ber of alternative parses). The whole training set for
the CoNLL-2005 task was used to train the mod-
els. It takes about 2 hours to train a local identifi-
cation model, 40 minutes to train a local classifica-
tion model, and 7 hours to train a joint re-ranking
model.2
In Table 1, we present our final development and
test results using this model. The percentage of
perfectly labeled propositions for the three sets is
55.11% (development), 56.52% (test), and 37.06%
(Brown test). The improvement achieved by the
joint model relative to the local model is about 2
points absolute in F-Measure, similar to the im-
provement when gold-standard syntactic parses are
used (Toutanova et al, 2005). The relative error re-
duction is much lower for automatic parses, possi-
bly due to a lower upper bound on performance. It
is clear from the drop in performance from the WSJ
to Brown test set that our learned model?s features
do not generalize very well to related domains.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2004.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML-2000.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of HLT/NAACL-2004.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-2005.
2On a 3.6GHz machine with 4GB of RAM.
176
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 251?261,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Translingual Document Representations from Discriminative Projections
John C. Platt Kristina Toutanova
Microsoft Research
1 Microsoft Way
Redmond, WA 98005, USA
{jplatt,kristout,scottyih}@microsoft.com
Wen-tau Yih
Abstract
Representing documents by vectors that are
independent of language enhances machine
translation and multilingual text categoriza-
tion. We use discriminative training to create
a projection of documents from multiple lan-
guages into a single translingual vector space.
We explore two variants to create these pro-
jections: Oriented Principal Component Anal-
ysis (OPCA) and Coupled Probabilistic Latent
Semantic Analysis (CPLSA). Both of these
variants start with a basic model of docu-
ments (PCA and PLSA). Each model is then
made discriminative by encouraging compa-
rable document pairs to have similar vector
representations. We evaluate these algorithms
on two tasks: parallel document retrieval
for Wikipedia and Europarl documents, and
cross-lingual text classification on Reuters.
The two discriminative variants, OPCA and
CPLSA, significantly outperform their corre-
sponding baselines. The largest differences in
performance are observed on the task of re-
trieval when the documents are only compa-
rable and not parallel. The OPCA method is
shown to perform best.
1 Introduction
Given the growth of multiple languages on the In-
ternet, Natural Language Processing must operate
on dozens of languages. It is becoming critical that
computers reach high performance on the following
two tasks:
? Comparable and parallel document re-
trieval ? Cross-language information retrieval
and text categorization have become impor-
tant with the growth of the Web (Oard and
Diekema, 1998). In addition, machine trans-
lation (MT) systems can be improved by
training on sentences extracted from paral-
lel or comparable documents mined from the
Web (Munteanu and Marcu, 2005). Compa-
rable documents can also be used for learning
word-level translation lexicons (Fung and Yee,
1998; Rapp, 1999).
? Cross-language text categorization ? Appli-
cations of text categorization, such as sentiment
classification (Pang et al, 2002), are now re-
quired to run on multiple languages. Catego-
rization is usually trained on the language of
the developer: it needs to be easily extended to
other languages.
There are two broad approaches to comparable
document retrieval and cross-language text catego-
rization. One approach is to translate queries or a
training set from different languages into a single
target language. Standard monolingual retrieval and
classification algorithms can then be applied in the
target language.
Alternatively, a cross-language system can project
a bag-of-words vector into a translingual lower-
dimensional vector space. Ideally, vectors in this
space represent the semantics of a document, inde-
pendent of the language.
The advantage of pre-translation is that MT sys-
tems tend to preserve the meaning of documents.
However, MT can be very slow (more than 1 second
per document), preventing its use on large training
sets. When full MT is not practical, a fast word-by-
word translation model can be used instead, (Balles-
teros and Croft, 1996) but may be less accurate.
Conversely, applying a projection into a low-
dimensional space is quick. Linear projection al-
gorithms use matrix-sparse vector multiplication,
which can be easily parallelized. However, as seen
in section 3, the accuracies of previous projection
251
techniques are not as high as machine translation.
This paper presents two techniques: Oriented
PCA and Coupled PLSA. These techniques retain
the high speed of projection, while approaching or
exceeding the quality level of word glossing. We im-
prove the quality of the projections by the use of dis-
criminative training: we minimize the difference be-
tween comparable documents in the projected vec-
tor space. Oriented PCA minimizes the difference
by modifying the eigensystem of PCA (Diamantaras
and Kung, 1996), while Coupled PLSA uses poste-
rior regularization (Graca et al, 2008; Ganchev et
al., 2009) on the topic assignments of the compara-
ble documents.
1.1 Previous work
There has been extensive work in projecting mono-
lingual documents into a vector space. The ini-
tial algorithm for projecting documents was Latent
Semantic Analysis (LSA), which modeled bag-of-
word vectors as low-rank Gaussians (Deerwester et
al., 1990). Subsequent projection algorithms were
based on generative models of individual terms in
the documents, including Probabilistic Latent Se-
mantic Analysis (PLSA) (Hofmann, 1999) and La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003).
Work on cross-lingual projections followed a sim-
ilar pattern of moving from Gaussian models to
term-wise generative models. Cross-language La-
tent Semantic Indexing (CL-LSI) (Dumais et al,
1997) applied LSA to concatenated comparable doc-
uments from multiple languages. Similarly, Polylin-
gual Topic Models (PLTM) (Mimno et al, 2009)
generalized LDA to tuples of documents from mul-
tiple languages. The experiments in section 3 use
CL-LSI and an algorithm similar to PLTM as bench-
marks.
The closest previous work to this paper is the
use of Canonical Correlation Analysis (CCA) to find
projections for multiple languages whose results are
maximally correlated with each other (Vinokourov
et al, 2003).
PLSA-, LDA-, and CCA-based cross-lingual
models have also been trained without the use of par-
allel or comparable documents, using only knowl-
edge from a translation dictionary to achieve sharing
of topics across languages (Haghighi et al, 2008; Ja-
garlamudi and Daume?, 2010; Zhang et al, 2010).
Such work is complementary to ours and can be
used to extend the models to domains lacking par-
allel documents.
Outside of NLP, researchers have designed al-
gorithms to find discriminative projections. We
build on the Oriented Principal Component Analysis
(OPCA) algorithm (Diamantaras and Kung, 1996),
which finds projections that maximize a signal-to-
noise ratio (as defined by the user). OPCA has been
used to create discriminative features for audio fin-
gerprinting (Burges et al, 2003).
1.2 Structure of paper
This paper now presents two algorithms for translin-
gual document projection (in section 2): OPCA and
Coupled PLSA (CPLSA). To explain OPCA, we
first review CL-LSI in section 2.1, then discuss the
details of OPCA (section 2.2), and compare it to
CCA (section 2.3). To explain CPLSA, we first
introduce Joint PLSA (JPLSA), analogous to CL-
LSI, in section 2.4, and then describe the details of
CPLSA (section 2.5).
We have evaluated these algorithms on two dif-
ferent tasks: comparable document retrieval (sec-
tion 3.2) and cross-language text categorization
(section 3.3). We discuss the findings of the evalua-
tions and extensions to the algorithms in section 4.
2 Algorithms for translingual document
projection
2.1 Cross-language Latent Semantic Indexing
Cross-language Latent Semantic Indexing (CL-LSI)
is Latent Semantic Analysis (LSA) applied to multi-
ple languages. First, we review the mathematics of
LSA.
LSA models an n ? k document-term matrix D,
where n is the number of documents and k is the
number of terms. The model of the document-term
matrix is a low-rank Gaussian. Originally, LSA was
presented as performing a Singular Value Decompo-
sition (Deerwester et al, 1990), but here we present
it as eigendecomposition, to clarify its relationship
with OPCA.
LSA first computes the correlation matrix be-
tween terms:
C = DTD. (1)
252
The Rayleigh quotient for a vector ~v with the matrix
C is
~vTC~v
~vT~v
, (2)
and is equal to the variance of the data projected us-
ing the vector ~v, normalized by the length of ~v, if D
has columns that are zero mean. Good projections
retain a large amount of variance. LSA maximizes
the Rayleigh ratio by taking its derivative against ~v
and setting it to zero. This yields a set of projections
that are eigenvectors of C,
C~vj = ?j~vj , (3)
where ?j is the jth-largest eigenvalue. Each eigen-
value is also the variance of the data when projected
by the corresponding eigenvector ~vj . LSA simply
uses top d eigenvectors as projections.
LSA is very similar to Principal Components
Analysis (PCA). The only difference is that the cor-
relation matrix C is used, instead of the covariance
matrix. In practice, the document-term matrix D is
sparse, so the column means are close to zero, and
the correlation matrix is close to the covariance ma-
trix.
There are a number of methods to form the
document-term matrix D. One method that works
well in practice is to compute the log(tf)-idf weight-
ing: (Dumais, 1990; Wild et al, 2005)
Dij = log2(fij + 1) log2(n/dj), (4)
where fij is the number of times term j occurs in
document i, n is the total number of documents,
and dj is the total number of documents that con-
tain term j. Applying a logarthm to the term counts
makes the distribution of matrix entries approach
Gaussian, which makes the LSA model more valid.
Cross-language LSI is an application of LSA
where each row of D is formed by concatenating
comparable or parallel documents in multiple lan-
guages. If a single term occurs in multiple lan-
guages, the term only has one slot in the concate-
nation, and the term count accumulates for all lan-
guages. Such terms could be proper nouns, such as
?Smith? or ?Merkel?.
In general, the elements of D are computed via
Dij = log2
(
?
m
fmij + 1
)
log2(n/dj), (5)
where fmij is the number of times term j occurs in
document i in language m. Here, dj is the number
of documents term j appears in, and n is the total
number of documents across all languages.
Because CL-LSI is simply LSA applied to con-
catenated documents, it models terms in document
vectors jointly across languages as a single low-rank
Gaussian.
2.2 Oriented Principal Component Analysis
The limitations of CL-LSI can be illustrated by con-
sidering Oriented Principal Components Analysis
(OPCA), a generalization of PCA. A user of OPCA
computes a signal covariance matrix S and a noise
covariance matrix N. OPCA projections ~vj max-
imize the ratio of the variance of the signal pro-
jected by ~vj to the variance of the noise projected
by ~vj . This signal-to-noise ratio is the generalized
Rayleigh quotient: (Diamantaras and Kung, 1996)
~vTS~v
~vTN~v
. (6)
Taking the derivative of the Rayleigh quotient with
respect to the projections ~v and setting it to zero
yields the generalized eigenproblem
S~vj = ?jN~vj . (7)
This eigenproblem has no local minima, and can be
solved with commonly available parallel code.
PCA is a specialization of OPCA, where the noise
covariance matrix is assumed to be the identity (i.e.,
uncorrelated noise). PCA projections maximize the
signal-to-noise ratio where the signal is the empiri-
cal covariance of the data, and the noise is spherical
white noise. PCA projections are not truly appropri-
ate for forming multilingual document projections.
Instead, we want multilingual document projec-
tions to maximize the projected covariance of doc-
ument vectors across all languages, while simulta-
neously minimizing the projected distance between
comparable documents (see Figure 1). OPCA gives
us a framework for finding such discriminative pro-
jections. The covariance matrix for all documents
is the signal covariance in OPCA, and captures the
meaning of documents across all languages. The
projection of this covariance matrix should be max-
imized. The covariance matrix formed from differ-
ences between comparable documents is the noise
253
covariance in OPCA: we wish to minimize the lat-
ter covariance, to make the projection language-
independent.
Specifically, we create the weighted document-
term matrix Dm for each language:
Dij,m = log2(f
m
ij + 1)log2(n/dj). (8)
We then derive a signal covariance matrix over all
languages:
S =
?
m
DTmDm/n? ~?
T
m~?m, (9)
where ~?m is the mean of each Dm over its columns,
and a noise covariance matrix,
N =
?
m
(Dm ?D)T (Dm ?D)/n+ ?I, (10)
where D is the mean across all languages of the
document-term matrix,
D =
1
M
?
m
Dm, (11)
and M is the number of languages. Applying equa-
tion (7) to these matrices and taking the top gener-
alized eigenvectors yields the projection matrix for
OPCA.
Note the regularization term of ?I in equation
(10). The empirical sample of comparable docu-
ments may not cover the entire space of translation
noise the system will encounter in the test set. For
safety, we add a regularizer that prevents the vari-
ance of a term from getting too small. We tuned ?
on the development sets in section 3.2: for log(tf)-
idf weighted vectors, C = 0.1 works well for the
data sets and dimensionalities that we tried. We use
C = 0.1 for all final tests.
2.3 Canonical Correlation Analysis
Canonical Correlation Analysis (CCA) is a tech-
nique that is related to OPCA. CCA was kernelized
and applied to creating cross-language document
models by (Vinokourov et al, 2003). In CCA, a lin-
ear projection is found for each language, such that
the projections of the corpus from each language are
maximally correlated with each other. Similar to
OPCA, this linear projection can be found by find-
ing the top generalized eigenvectors of the system
en
esen e
n
enes es
es
Maxim
izes ov
erall v
arianc
e
? whi
le min
imizin
g dista
nce 
betwe
en com
parab
le pair
s
Figure 1: OPCA finds a projection that maximizes the
variance of all documents, while minimizing distance be-
tween comparable documents
(7), where S is now a matrix of cross-correlations
that the projection maximizes,
S =
[
0 C12
C21 0
]
, (12)
and N is a matrix of autocorrelations that the projec-
tion minimizes,
N =
[
C11 + ?I 0
0 C22 + ?I
]
. (13)
Here, Cij is the (cross-)covariance matrix, with di-
mension equal to the vocabulary size, that is com-
puted between the document vectors for languages
i and j. Analogous to OPCA, ? is a regularization
term, set by optimizing performance on a validation
set. Like OPCA, these matrices can be generalized
to more than two languages. Unlike OPCA, CCA
finds projections that maximize the cross-covariance
between the projected vectors, instead of minimiz-
ing Euclidean distance.1
By definition, CCA cannot take advantage of the
information that same term occurs simultaneously in
comparable documents. As shown in section 3, this
1Note that the eigenvectors have length equal to the sum of
the length of the vocabularies of each language. The projections
for each language are created by splitting the eigenvectors into
sections, each with length equal to the vocabulary size for each
language.
254
information is useful and helps OPCA perform bet-
ter then CCA. In addition, CCA encourages compa-
rable documents to be projected to vectors that are
mutually linearly predictable. This is not the same
OPCA?s projected vectors that have low Euclidean
distance: the latter may be preferred by algorithms
that consume the projections.
2.4 Cross-language Topic Models
We now turn to a baseline generative model that
is analogous to CL-LSI. Our baseline joint PLSA
model (JPLSA) is closely related to the poly-lingual
LDA model of (Mimno et al, 2009). The graphical
model for JPLSA is shown at the top in Figure 2.
We describe the model for two languages, but it is
straightforward to generalize to more than two lan-
guages, as in (Mimno et al, 2009).
z
z
??
w
w
?
TD
N1
N2
z
z
?1
?
w
w
?
TD
N1
N2
?2
?
?
Figure 2: Graphical models for JPLSA (top) and CPLSA
(bottom)
The model sees documents di as sequences of
words w1, w2, . . . , wni from a vocabulary V . There
are T cross-language topics, each of which has a dis-
tribution ?t over words in V . In the case of mod-
els for two languages, we define the vocabulary V
to contain word types from both languages. In this
way, each topic is shared across languages.
Each topic-specific distribution ?t, for t =
1 . . . T , is drawn from a symmetric Dirichlet prior
with concentration parameter ?. Given the topic-
specific word distributions, the generative process
for a corpus of paired documents [d1i , d
2
i ] in two lan-
guages L1 and L2 is described in the next paragraph.
For each pair of documents, pick a distribution
over topics ?i, from a symmetric Dirichlet prior with
concentration parameter ?. Then generate the doc-
uments d1i and d
2
i in turn. Each word token in each
document is generated independently by first pick-
ing a topic z from a multinomial distribution with
parameter ?i (MULTI(?i)), and then generating the
word token from the topic-specific word distribution
for the chosen topic MULTI(?z).
The probability of a document pair [d1, d2] with
words [w11, w
1
2, . . . , w
1
n1 ], [w
2
1, w
2
2, . . . , w
2
n2 ], topic
assignments [z11 , . . . , z
1
n1 ], [z
2
1 , . . . , z
2
n2 ], and a com-
mon topic vector ? is given by:
P (?|?)
n1?
j=1
P (z1j |?)P (w
1
j |?z1j )
n2?
j=1
P (z2j |?)P (w
2
j |?z2j )
The difference between the JPLSA model and the
poly-lingual topic model of (Mimno et al, 2009)
is that we merge the vocabularies in the two lan-
guages and learn topic-specific word distributions
over these merged vocabularies, instead of having
pairs of topic-specific word distributions, one for
each language, like in (Mimno et al, 2009). Thus
our model is more similar to the CL-LSI model, be-
cause it can be seen as viewing a pair of documents
in two languages as one bigger document containing
the words in both documents.
Another difference between our model and the
poly-lingual LDA model of (Mimno et al, 2009)
is that we use maximum aposteriori (MAP) instead
of Bayesian inference. Recently, MAP inference
was shown to perform comparably to the best in-
ference method for LDA (Asuncion et al, 2009),
if the hyper-parameters are chosen optimally for
the inference method. Our initial experiments with
Bayesian versus MAP inference for parallel docu-
ment retrieval using JPLSA confirmed this result.
In practice our baseline model outperforms poly-
lingual LDA as mentioned in our experiments.
2.5 Coupled Probabilistic Latent Semantic
Analysis
The JPLSA model assumes that a pair of translated
or comparable documents have a common topic dis-
tribution ?. JPLSA fits its parameters to optimize the
probability of the data, given this assumption.
For the task of comparable document retrieval, we
want our topic model to assign similar topic distri-
butions ? to a pair of corresponding documents. But
255
this is not exactly what the JPLSA model is doing.
Instead, it derives a common topic vector ? which
explains the union of all tokens in the English and
foreign documents, instead of making sure that the
best topic assignment for the English document is
close to the best topic assignment of the foreign doc-
ument. This difference becomes especially appar-
ent when corresponding documents have different
lengths. In this case, the model will tend to derive
a topic vector ? which explains the longer document
best, making the sum of the two documents? log-
likelihoods higher. Modeling the shorter document?s
best topic carries little weight.
Modeling both documents equally is what Cou-
pled PLSA (CPLSA) is designed to do. The graphi-
cal model for CPLSA is shown at the bottom of Fig-
ure 2. In this figure, the topic vectors of a pair of
documents in two languages are shown completely
independent. We use the log-likelihood according to
this model, but also add a regularization term, which
tries to make the topic assignments of correspond-
ing documents close. In particular, we use poste-
rior regularization (Graca et al, 2008; Ganchev et
al., 2009) to place linear constraints on the expec-
tations of topic assignments to two corresponding
documents.
For two linked documents d1 and d2, we would
like our model to be such that the expected fraction
of tokens in d1 that get assigned topic t is approxi-
mately the same as the expected fraction of tokens in
d2 that get assigned the same topic t, for each topic
t = 1 . . . T . This is exactly what we need to make
each pair of corresponding documents close.
Let z1 and z2 denote vectors of topic assignments
to the tokens in document d1 and d2, respectively.
Their dimensionality is equal to the lengths of the
two documents, n1 and n2. We define a space of
posterior distributions Q over hidden topic assign-
ments to the tokens in d1 and d2, that has the desired
property: the expected fraction of each topic is ap-
proximately equal in d1 and d2. We can formulate
this constrained space Q as follows:
Q = {q1(z1), q2(z2)}
such that
Eq1 [
?n1
j=1 1(z
1
j = t)
n1
]?Eq2 [
?n2
j=1 1(z
2
j = t)
n2
] ? t
Eq2 [
?n2
j=1 1(z
2
j = t)
n2
]?Eq1 [
?n1
j=1 1(z
1
j = t)
n1
] ? t
We then formulate an objective function that max-
imizes the log-likelihood of the data while simulta-
neously minimizing the KL-divergence between the
desired distribution set Q and the posterior distri-
bution according to the model: P (z1|d1, ?1, ?) and
P (z2|d2, ?2, ?).
The objective function for a single document pair
is as follows:
logP (d1|?1, ?) + logP (d2|?2, ?)
?KL(Q||P (z1|d1, ?1, ?), P (z2|d2, ?2, ?))
?||||
The final corpus-wide objective is summed over
document-pairs, and also contains terms for the
probabilities of the parameters ? and ? given the
Dirichlet priors. The norm of  is minimized, which
makes the expected proportions of topics in two doc-
uments as close as possible.
Following (Ganchev et al, 2009), we fit the pa-
rameters by an EM-like algorithm, where for each
document pair, after finding the posterior distri-
bution of the hidden variables, we find the KL-
projection of this posterior onto the constraint set,
and take expected counts with respect to this projec-
tion; these expected counts are used in the M-step.
The projection is found using a simple projected gra-
dient algorithm.2
For both the baseline JPLSA and the CPLSA
models, we performed learning through MAP infer-
ence using EM (with a projection step for CPLSA).
We did up to 500 iterations for each model, and did
early stopping based on task performance on the de-
velopment set. The JPLSA model required more it-
erations before reaching its peak accuracy, tending
to require around 300 to 450 iterations for conver-
gence. CPLSA required fewer iterations, but each
iteration was slower due to the projection step.
2We initialized the models deterministically by assigning
each word to exactly one topic to begin with, such that all topics
have roughly the same number of words. Words were sorted by
frequency and thus words of similar frequency are more likely
to be assigned to the same topic.This initialization method out-
performed random initialization and we use it for all models.
256
All models use ? = 1.1 and ? = 1.01 for the
values of the concentration parameters. We found
that the performance of the models was not very sen-
sitive to these values, in the region that we tested
(?, ? ? [1.001, 1.1]). Higher hyper-parameter val-
ues resulted in faster convergence, but the final per-
formance was similar across these different values.
3 Experimental validation
We test the proposed discriminative projections ver-
sus more established cross-language models on the
two tasks described in the introduction: retrieving
comparable documents from a corpus, and training
a classifier in one language and using it in another.
We measure accuracy on a test set, and also examine
the sensitivity to dimensionality of the projection on
development sets.
3.1 Speed of training and evaluation
We first test the speed of the various algorithms dis-
cussed in this paper, compared to a full machine
translation system. When finding document projec-
tions, CL-LSI, OPCA, CCA, JPLSA, and CPLSA
are equally fast: they perform a matrix multiplica-
tion and require O(nk) operations, where n is the
number of distinct words in the documents and k is
the dimensionality of the projection.3 A single CPU
core can read the indexed documents into memory
and take logarithms at 216K words per second. Pro-
jecting into a 2000-dimensional space operates at
41K words per second. Translating word-by-word
operates at 274K words per second. In contrast, ma-
chine translation processes 50 words per second, ap-
proximately 3 orders of magnitude slower.
Total training time for OPCA on 43,380 pairs of
comparable documents was 90 minutes, running on
an 8-core CPU for 2000 dimensions. On the same
corpus, JPLSA requires 31 minutes per iteration and
CPLSA requires 377 minutes per iteration. CPLSA
requires a factor of five times fewer iterations: over-
all, it is twice as slow as JPLSA.
3.2 Retrieval of comparable documents
In comparable document retrieval, a query is a doc-
ument in one language, which is compared to a cor-
3For JPLSA and CPLSA this is the case only when perform-
ing a single EM iteration at test time, which we found to per-
form best.
pus of documents in another language. By mapping
all documents into the same vector space, the com-
parison is a vector comparison. For our experiments
with CL-LSI, OPCA, and CCA, we use cosine sim-
ilarity between vectors to rank the documents.
For the JPLSA and CPLSA models, we map the
documents to corresponding topic vectors ?, and
compute distance between these probability vectors.
The mapping to topic vectors requires EM iterations,
or folding-in (Hofmann, 1999). We found that per-
forming a single EM iteration resulted in best per-
formance so we used this for all models. For com-
puting distance we used the L1-norm of the differ-
ence, which worked a bit better than the Jensen-
Shannon divergence between the topic vectors used
in (Mimno et al, 2009).
We test all algorithms on the Europarl data set
of documents in English and Spanish, and a set of
Wikipedia articles in English and Spanish that con-
tain interlanguage links between them (i.e., articles
that the Wikipedia community have identified as
comparable across languages).
For the Europarl data set, we use 52,685 doc-
uments as training, 11,933 documents as a devel-
opment set, and 18,415 documents as a final test
set. Documents are defined as speeches by a sin-
gle speaker, as in (Mimno et al, 2009).4 For the
Wikipedia set, we use 43,380 training documents,
8,675 development documents, and 8,675 final test
set documents.
For both corpora, the terms are extracted by word-
breaking all documents, removing the top 50 most
frequent terms and keeping the next 20,000 most fre-
quent terms. No stemming or folding is applied.
We assess performance by testing each document
in English against all possible documents in Span-
ish, and vice versa. We measure the Top-1 accu-
racy (i.e., whether the true comparable is the clos-
est in the test set), and the Mean Reciprocal Rank
of the true comparable, and report the average per-
formance over the two retrieval directions. Ties are
counted as errors.
We tuned the dimensionality of the projections on
the development set, as shown in Figures 3 and 4.
4The training section contains documents from the years 96
through 99 and the year 02; the dev section contains documents
from 01, and the test section contains documents from 00 plus
the first 9 months of 03.
257
We chose the best dimension on the development set
for each algorithm, and used it on the final test set.
The regularization ? was tuned for CCA: ? = 10 for
Europarl, and ? = 3 for Wikipedia.
Figure 3: Mean reciprocal rank versus dimension for Eu-
roparl
Figure 4: Mean reciprocal rank versus dimension for
Wikipedia
In the two figures, we evaluate the five projec-
tion methods, as well as a word-by-word transla-
tion method (denoted by WbW in the graphs). Here
?word-by-word? refers to using cosine distance after
applying a word-by-word translation model to the
Spanish documents.
The word-by-word translation model was trained
on the Europarl training set, using the WDHMM
model (He, 2007), which performs similarly to IBM
Model 4. The probability matrix of generating
English words from Spanish words was multiplied
by each document?s log(tf)-idf vector to produce a
translated document vector. We found that multi-
plying the probability matrix to the log(tf)-idf vector
was more accurate on the development set than mul-
tiplying the tf vector directly. This vector was either
tested as-is, or mapped through LSA learned from
the English training set of the corpus. In the figures,
the dimensionality of WbW translation refers to the
dimensionality of monolingual LSA.
The overall ordering of the six models is dif-
ferent for the Europarl and Wikipedia development
datasets. The discriminative models outperform
the corresponding generative ones (OPCA vs CL-
LSI) and (CPLSA vs JPLSA) for both datasets, and
OPCA performs best overall, dominating the best
fast-translation based model, as well as the other
projection methods, including CCA.
On Europarl, JPLSA and CPLSA outperform CL-
LSI, with the best dimension or JPLSA also slightly
outperforming the best setting for the word-by-word
translation model, whereas on Wikipedia the PLSA-
based models are significantly worse than the other
models.
The results on the final test set, evaluating each
model using its best dimensionality setting, confirm
the trends observed on the development set. The fi-
nal results are shown in Tables 1 and 2. For these
experiments, we use the unpaired t-test with Bon-
ferroni correction to determine the smallest set of
algorithms that have statistically significantly better
accuracy than the rest. The p-value threshold for sig-
nificance is chosen to be 0.05. The accuracies for
these significantly superior algorithms are shown in
boldface.
For Wikipedia and Europarl, we include an ad-
ditional baseline model,?Untranslated?: this refers
to applying cosine distance to both the Spanish and
English documents directly (since they share some
vocabulary terms). For Wikipedia, comparable doc-
uments seem to share many common terms, so co-
sine distance between untranslated documents is a
reasonable benchmark.
From the final Europarl results we can see that the
best models can learn to retrieve parallel documents
from the narrow Europarl domain very well. All
dimensionality reduction methods can learn from
258
cleanly parallel data, but discriminative training can
bring additional error reduction.
In previously reported work, (Mimno et al, 2009)
evaluate parallel document retrieval using PLTM on
Europarl speeches in English and Spanish, using
training and test sets of size similar to ours. They
report an accuracy of 81.2% when restricting to test
documents of length at least 100 and using 50 topics.
JPLSA with 50 topics obtains accuracy of 98.9% for
documents of that length.
The final Wikipedia results are also similar to the
the development set results. The problem setting for
Wikipedia is different, because corresponding doc-
uments linked in Wikipedia may have widely vary-
ing degrees of parallelism. While most linked doc-
uments share some main topics, they could cover
different numbers of sub-topics at varying depths.
Thus the training data of linked documents is noisy,
which makes it hard for projection methods to learn.
The word-by-word translation model in this setting
is trained on clean, but out-of-domain parallel data
(Europarl), so it has the disadvantage that it may not
have a good coverage of the vocabulary; however,
it is not able to make use of the Wikipedia train-
ing data since it requires sentence-aligned transla-
tions. We find it encouraging that the best projection
method OPCA outperformed word-by-word trans-
lation. This means that OPCA is able to uncover
topic correspondence given only comparable docu-
ment pairs, and to learn well in this noisy setting.
The PLSA-based models fare worse on Wikipedia
document retrieval. CPLSA outperforms JPLSA
more strongly, but both are worse than CL-LSI and
even the Untranslated baseline. We think this is
partly explained by the diverse vocabulary in the het-
erogenous Wikipedia collection. All other models
use log(tf)-idf weighting, which automatically as-
signs importance weights to terms, whereas the topic
models use word counts. This weighting is very use-
ful for Wikipedia. For example, if we apply the
untranslated matching using raw word counts, the
MRR is 0.1024 on the test set, compared to 0.5383
for log(tf)-idf. We hypothesize that using a hierar-
chical topic model that automatically learns about
more general and more topic-specific words would
be helpful in this case. It is also possible that PLSA-
based models require cleaner data to learn well.
The overall conclusion is that OPCA outper-
Algorithm Dimension Accuracy MRR
OPCA 1000 0.9742 0.9806
CPLSA 1000 0.9716 0.9782
Word-by-word N/A 0.9707 0.9779
Word-by-word 5000 0.9706 0.9778
JPLSA 1000 0.9645 0.9726
CCA 1500 0.9613 0.9705
CL-LSI 3000 0.9457 0.9595
Untranslated N/A 0.1595 0.2564
Table 1: Test results for comparable document retrieval
in Europarl. Boldface indicates statistically significant
superior results.
Algorithm Dimension Accuracy MRR
OPCA 2000 0.7255 0.7734
Word-by-word N/A 0.7033 0.7467
CCA 1500 0.6894 0.7378
Word-by-word 5000 0.6786 0.7236
CL-LSI 5000 0.5302 0.6130
Untranslated N/A 0.4692 0.5383
CPLSA 200 0.4579 0.5130
JPLSA 1000 0.3322 0.3619
Table 2: Test results for comparable document retrieval
in Wikipedia. Boldface indicates statistically significant
best result.
formed all other document retrieval methods we
tested, including fast machine translation of docu-
ments. Additionally, both discriminative projection
methods outperformed their generative counterparts.
3.3 Cross-language text classification
The second task is to train a text categorization sys-
tem in one language, and test it with documents in
another. To evaluate on this task, we use the Mul-
tilingual Reuters Collection, defined and provided
by (Amini et al, 2009). We test the English/Spanish
language pair. The collection has news articles in
English and Spanish, each of which has been trans-
lated to the other by the Portage translation sys-
tem (Ueffing et al, 2007).
From the English news corpus, we take 13,131
documents as training, 1,875 documents as develop-
ment, and 1,875 documents as test. We take the En-
glish training documents translated into Spanish as
our comparable training data. For testing, we use the
entire Spanish news corpus of 12,342 documents, ei-
259
ther mapped with cross-lingual projection, or trans-
lated by Portage.
The data set was provided by (Amini et al,
2009) as already-processed document vectors, using
BM25 weighting. Thus, we only test OPCA, CL-
LSI, and related methods: JPLSA and CPLSA re-
quire modeling the term counts directly.
The performance on the task is measured by clas-
sification accuracy on the six disjoint category la-
bels defined by (Amini et al, 2009). To introduce
minimal bias due to the classifier model, we use 1-
nearest neighbor on top of the cosine distance be-
tween vectors as a classifier. For all of the tech-
niques, we treated the vocabulary in each language
as completely separate, using the top 10,000 terms
from each language.
Note that no Spanish labeled data is provided
for training any of these algorithms: only English
and translated English news is labeled. The op-
timal dimension (and ? for CCA) on the devel-
opment set was chosen to maximize the accuracy
of English classification and translated English-to-
Spanish classification.
Algorithm Dim. English Spanish
Accuracy Accuracy
Full MT 50 0.8483 0.6484
OPCA 100 0.8412 0.5954
Word-by-word 50 0.8483 0.5780
CCA 150 0.8388 0.5384
Full MT N/A 0.8046 0.5323
CL-LSI 150 0.8401 0.5105
Word-by-word N/A 0.8046 0.4481
Table 3: Test results for cross-language text categoriza-
tion
The test classification accuracy is shown in Ta-
ble 3. As above, the smallest set of superior al-
gorithms as determined by Bonferroni-corrected t-
tests are shown in boldface. The results for MT and
word-by-word translation use the log(tf)-idf vector
directly for documents that were written in English,
and use a Spanish-to-English translated vector if the
document was written in Spanish. As in section 3.2,
word-by-word translation multiplied each log(tf)-idf
vector by the translation probability matrix trained
on Europarl.
The tests show that OPCA is better than CCA,
CL-LSI, plain word-by-word translation, and even
full translation for Spanish documents. However,
if we post-process full translation by an LSI model
trained on the English training set, full translation
is the most accurate. If full translation is time-
prohibitive, then OPCA is the best method: it is sig-
nificantly better than word-by-word translation fol-
lowed by LSI.
4 Discussion and Extensions
OPCA extends naturally to multiple languages.
However, it requires memory and computation time
that scales quadratically with the size of the vocab-
ulary. As the number of languages goes up, it may
become impractical to perform OPCA directly on a
large vocabulary.
Researchers have solved the problem of scaling
OPCA by using Distortion Discriminant Analysis
(DDA) (Burges et al, 2003). DDA performs OPCA
in two stages which avoids the need for solving a
very large generalized eigensystem. As future work,
DDA could be applied to mapping documents in
many languages simultaneously.
Spherical Admixture Models (Reisinger et al,
2010) have recently been proposed that combine an
LDA-like hierarchical generative model with the use
of tf-idf representations. A similar model could be
used for CPLSA: future work will show whether
such a model can outperform OPCA.
5 Conclusions
This paper presents two different methods for creat-
ing discriminative projections: OPCA and CPLSA.
Both of these methods avoid the use of artificial
concatenated documents. Instead, they model docu-
ments in multiple languages, with the constraint that
comparable documents should map to similar loca-
tions in the projected space.
When compared to other techniques, OPCA had
the highest accuracy while still having a run-time
that allowed scaling to large data sets. We therefore
recommend the use of OPCA as a pre-processing
step for large-scale comparable document retrieval
or cross-language text categorization.
260
References
Massih-Reza Amini, Nicolas Usunier, and Cyril Goutte.
2009. Learning from multiple partially observed
views - an application to multilingual text categoriza-
tion. In Advances in Neural Information Processing
Systems 22 (NIPS 2009), pages 28?36.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of Uncertainty in Ar-
tificial Intelligence, pages 27?34.
Lisa Ballesteros and Bruce Croft. 1996. Dictionary
methods for cross-lingual information retrieval. In
Proceedings of the 7th International DEXA Confer-
ence on Database and Expert Systems Applications,
pages 791?801.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet alocation.
Journal of Machine Learning Research, 3:993?1022.
Christopher J.C. Burges, John C. Platt, and Soumya Jana.
2003. Distortion discriminant analysis for audio fin-
gerprinting. IEEE Transactions on Speech and Audio
Processing, 11(3):165?174.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Konstantinos I. Diamantaras and S.Y. Kung. 1996. Prin-
cipal Component Neural Networks: Theory and Appli-
cations. Wiley-Interscience.
Susan T. Dumais, Todd A. Letsche, Michael L. Littman,
and Thomas K. Landauer. 1997. Automatic cross-
language retrieval using latent semantic indexing. In
AAAI-97 Spring Symposium Series: Cross-Language
Text and Speech Retrieval.
Susan T. Dumais. 1990. Enhancing performance in la-
tent semantic indexing (LSI) retrieval. Technical Re-
port TM-ARH-017527, Bellcore.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of COLING-ACL, pages
414?420.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical Report MS-
CIS-09-16, University of Pennsylvania.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, edi-
tors, Advances in Neural Information Processing Sys-
tems 20, pages 569?576. MIT Press, Cambridge, MA.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. ACL, pages 771?
779.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL 2nd Statistical MT work-
shop, pages 80?87.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence, pages 289?296.
Jagadeesh Jagarlamudi and Hal Daume?, III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In ECIR.
David Mimno, Hanna W. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of Empir-
ical Methods in Natural Language Processing, pages
880?889.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Douglas W. Oard and Anne R. Diekema. 1998. Cross-
language information retrieval. In Martha Williams,
editor, Annual Review of Information Science (ARIST),
volume 33, pages 223?256.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proc. EMNLP, pages
79?86.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the ACL, pages 519?526.
Joseph Reisinger, Austin Waters, Bryan Silverthorn, and
Raymond J. Mooney. 2010. Spherical topic models.
In Proc. ICML.
Nicola Ueffing, Michel Simard, Samuel Larkin, and
J. Howard Johnson. 2007. NRC?s PORTAGE system
for WMT 2007. In ACL-2007 2nd Workshop on SMT,
pages 185?188.
Alexei Vinokourov, John Shawe-Taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
S. Thrun S. Becker and K. Obermayer, editors, Ad-
vances in Neural Information Processing Systems 15,
pages 1473?1480, Cambridge, MA. MIT Press.
Fridolin Wild, Christina Stahl, Gerald Stermsek, and
Gustaf Neumann. 2005. Parameters driving effective-
ness of automated essay scoring with LSA. In Pro-
ceedings 9th Internaional Computer-Assisted Assess-
ment Conference, pages 485?494.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proc. ACL,
pages 1128?1137, Uppsala, Sweden. Association for
Computational Linguistics.
261
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1948?1959,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Regularized Minimum Error Rate Training
Michel Galley
Microsoft Research
mgalley@microsoft.com
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Colin Cherry
National Research Council
colin.cherry@nrc-cnrc.gc.ca
Kristina Toutanova
Microsoft Research
kristout@microsoft.com
Abstract
Minimum Error Rate Training (MERT) re-
mains one of the preferred methods for tun-
ing linear parameters in machine translation
systems, yet it faces significant issues. First,
MERT is an unregularized learner and is there-
fore prone to overfitting. Second, it is com-
monly used on a noisy, non-convex loss func-
tion that becomes more difficult to optimize
as the number of parameters increases. To ad-
dress these issues, we study the addition of
a regularization term to the MERT objective
function. Since standard regularizers such as
`2 are inapplicable to MERT due to the scale
invariance of its objective function, we turn to
two regularizers?`0 and a modification of `2?
and present methods for efficiently integrating
them during search. To improve search in large
parameter spaces, we also present a new direc-
tion finding algorithm that uses the gradient of
expected BLEU to orient MERT?s exact line
searches. Experiments with up to 3600 features
show that these extensions of MERT yield re-
sults comparable to PRO, a learner often used
with large feature sets.
1 Introduction
Minimum Error Rate Training emerged a decade
ago (Och, 2003) as a superior training method for
small numbers of linear model parameters of machine
translation systems, improving over prior work using
maximum likelihood criteria (Och and Ney, 2002).
This technique quickly rose to prominence, becom-
ing standard in many research and commercial MT
systems. Variants operating over lattices (Macherey
et al, 2008) or hypergraphs (Kumar et al, 2009) were
subsequently developed, with the benefit of reducing
the approximation error from n-best lists.
The primary advantages of MERT are twofold. It
directly optimizes the evaluation metric under consid-
eration (e.g., BLEU) instead of some surrogate loss.
Secondly, it offers a globally optimal line search. Un-
fortunately, there are several potential difficulties in
scaling MERT to larger numbers of features, due
to its non-convex loss function and its lack of reg-
ularization. These challenges have prompted some
researchers to move away from MERT, in favor of lin-
early decomposable approximations of the evaluation
metric (Chiang et al, 2009; Hopkins and May, 2011;
Cherry and Foster, 2012), which correspond to easier
optimization problems and which naturally incorpo-
rate regularization. In particular, recent work (Chiang
et al, 2009) has shown that adding thousands or tens
of thousands of features can improve MT quality
when weights are optimized using a margin-based
approximation. On simulated datasets, Hopkins and
May (2011) found that conventional MERT strug-
gles to find reasonable parameter vectors, where a
smooth loss function based on Pairwise Ranking Op-
timization (PRO) performs much better; on real data,
this PRO method appears at least as good as MERT
on small feature sets, and also scales better as the
number of features increases.
In this paper, we seek to preserve the advantages
of MERT while addressing its shortcomings in terms
of regularization and search. The idea of adding a
regularization term to the MERT objective function
can be perplexing at first, because the most common
regularizers, such as `1 and `2, are not directly appli-
cable to MERT. Indeed, these regularizers are scale
sensitive, while the MERT objective function is not:
scaling the weight vector neither changes the predic-
tions of the linear model nor affects the error count.
Hence, MERT can hedge any regularization penalty
by maximally scaling down linear model weights.
The first contribution of this paper is to analyze var-
ious forms of regularization that are not susceptible
to this scaling problem. We analyze and experiment
with `0, a form of regularization that is scale insen-
sitive. We also present new parameterizations of `2
1948
regularization, where we apply `2 regularization to
scale-senstive linear transforms of the original linear
model. In addition, we introduce efficient methods
of incorporating regularization in Och (2003)?s exact
line searches. For all of these regularizers, our meth-
ods let us find the true optimum of the regularized
objective function along the line.
Finally, we address the issue of searching in a
high-dimensional space by using the gradient of ex-
pected BLEU (Smith and Eisner, 2006) to find better
search directions for our line searches. This direction
finder addresses one of the serious concerns raised
by Hopkins and May (2011): MERT widely failed
to reach the optimum of a synthetic linear objective
function. In replicating Hopkins and May?s experi-
ments, we confirm that existing search algorithms for
MERT?including coordinate ascent, Powell?s algo-
rithm (Powell, 1964), and random direction sets (Cer
et al, 2008)?perform poorly in this experimental
condition. However, when using our gradient-based
direction finder, MERT has no problem finding the
true optimum even in a 1000-dimensional space.
Our results suggest that the combination of a reg-
ularized objective function and a gradient-informed
line search algorithm enables MERT to scale well
with a large number of features. Experiments with
up to 3600 features show that these extensions of
MERT yield results comparable to PRO (Hopkins
and May, 2011), a parameter tuning method known
to be effective with large feature sets.
2 Unregularized MERT
Prior to introducing regularized MERT, we briefly
review standard unregularized MERT (Och, 2003).
We use fS1 = {f1 . . . fS} to denote the S input sen-
tences of a given tuning set. For each sentence fs, let
Cs = {es,1 . . . es,M} denote the list of M -best can-
didate translations. Each input and output sentence
pair (fs, es,m) is weighted using a linear model that
applies model parameters w = (w1 . . . wD) ? RD
to D feature functions h1(f , e,?) . . . hD(f , e,?),
where ? is the hidden state associated with the
derivation from f to e, such as phrase segmenta-
tion and alignment. Furthermore, let hs,m ? RD
denote the feature vector representing the translation
pair (fs, es,m).
In MERT, the goal is to minimize a loss function
E(r, e) that scores translation hypotheses against a
set of reference translations rS1 = {r1 . . . rS}. This
yields the following optimization problem:
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w))
}
=
argmin
w
{ S?
s=1
M?
m=1
E(rs, es,m)?(es,m, e?(fs;w))
}
(1)
where
e?(fs;w) = argmax
m?{1...M}
{
w?hs,m
}
(2)
While the error surface of Equation 1 is only an
approximation of the true error surface of the MT
decoder, the quality of this approximation depends
on the size of the hypothesis space represented by the
M -best list. Therefore, the hypothesis list is grown
iteratively: decoding with an initial parameter vector
seeds the M -best lists; next, parameter estimation
and M -best list gathering alternate until the cumula-
tive M -best list no longer grows, or until changes of
w between two decoding runs are deemed too small.
To increase the size of the hypothesis space, subse-
quent work (Macherey et al, 2008) instead operated
on lattices, but this paper focuses on M -best lists.
A crucial observation is that the unsmoothed error
count represented in Equation 1 is a piecewise con-
stant function. This enabled Och (2003) to devise a
line search algorithm guaranteed to find the optimum
point along the line. To extend the search from one
to multiple dimensions, MERT applies a sequence
of line optimizations along some fixed or variable
set of search directions {dt} until some convergence
criteria are met. Considering a given point wt and
a given direction dt at iteration t, finding the most
probable translation hypothesis in the set of candi-
dates translations Cs = {es,1 . . . es,M} corresponds
to solving the following optimization problem:
e?(fs; ?) = argmax
m?{1...M}
{
(wt + ? ? dt)
?hs,m
}
(3)
The function in this equation is piecewise linear (Pa-
pineni, 1999), which enables an efficient exhaustive
computation. Specifically, this function is optimized
by enumerating the up to M hypotheses that form
the upper envelope of the model score function. The
error count, then, is a piecewise constant function
1949
defined by the points ?fs1 < ? ? ? < ?
fs
M at which an in-
crease in ? causes a change of optimum in Equation 3.
Error counts for the whole corpus are simply the sums
of sentence-level piecewise constant functions aggre-
gated over all sentences of the corpus.1 The optimal ?
is finally computed by enumerating all piecewise con-
stant intervals of the corpus-level error function, and
by selecting the one that has the lowest error count
(or, correspondingly, highest BLEU score). Assum-
ing the optimum is found in the interval [?k?1, ?k],
we define ?opt = (?k?1 + ?k)/2 and change the pa-
rameters using the update wt+1 = wt + ?opt ? dt.
Finally, this method is turned into a global D-
dimensional search using algorithms that repeat-
edly use the aforementioned exact line search algo-
rithm. Och (2003) first advocated the use of Powell?s
method (Powell, 1964; Press et al, 2007). Pharaoh
(Koehn, 2004) and subsequently Moses (Koehn et al,
2007) instead use coordinate ascent, and more recent
work often uses random search directions (Cer et al,
2008; Macherey et al, 2008). In Section 4, we will
present a novel direction finder for maximum-BLEU
optimization, which uses the gradient of expected
BLEU to find directions where the BLEU score is
most likely to increase.
3 Regularization for MERT
Because MERT is prone to overfitting when a large
number of parameters must be optimized, we study
the addition of a regularization term to the objective
function. One conventional approach is to regularize
the objective function with a penalty based on the
Euclidean norm ||w||2 =
??
iw
2
i , also known as `2
regularization. In the case of MERT, this yields the
following objective function:2
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w)) +
||w||22
2?2
}
(4)
1This assumes that the sufficient statistics of the metric under
consideration are additively decomposable by sentence, which
is the case with most popular evaluation metrics such as BLEU
(Papineni et al, 2001).
2The `2 regularizer is often used in conjunction with log-
likelihood objectives. The regularization term of Equation 4
could similarly be added to the log of an objective?e.g.,
log(BLEU) instead of BLEU?but we found that the distinc-
tion doesn?t have much of an impact in practice.
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT
Max at 0.225
?
?
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT? `2
Max at -0.018
?
?
?`2
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
?, the step size in the current direction
MERT? `0
Max at 0
?
?
`0
Figure 1: Example MERT values along one coordi-
nate, first unregularized. When regularized with `2, the
piecewise constant function becomes piecewise quadratic.
When using `0, the function remains piecewise constant
with a point discontinuity at 0.
where the regularization term 1/2?2 is a free param-
eter that controls the strength of the regularization
penalty. Similar regularizers have also been used
in conjunction with other norms, such as `1 and `0
norms. The `1 norm, defined as ||w||1 =
?
i |wi|,
applies a constant force toward zero, preferring vec-
tors with fewer non-zero components; `0, defined as
||w||0 = |{i | wi 6= 0}|, simply counts the number of
non-zero components of the weight vector, encoding
a preference for sparse vectors.
Geometrically, `2 is a parabola, `1 is the wedge-
shaped absolute value function, and `0 is an impulse
function with a spike at 0. The original formulation
(Equation 1) of MERT consists of a piecewise con-
stant representation of the loss, as a function of the
step size in a given direction. But with these three reg-
1950
ularization terms, the function respectively becomes
piecewise quadratic, piecewise linear, or piecewise
constant with a potential impulse jump for each dis-
tinct choice of regularizer. Figure 1 demonstrates this
effect graphically.
As discussed in (McAllester and Keshet, 2011),
the problem with optimizing Equation 4 directly is
that the output of the underlying linear classifier, and
therefore the error count, are not sensitive to the scale
of w. Moreover, `2 regularization (as well as `1 reg-
ularization) is scale sensitive, which means any op-
timizer of this function can drive the regularization
term down to zero by scaling down w. As special
treatments for `2, we evaluate three linear transforms
of the weight vector, where the vector w of the regu-
larization term ||w||22/2?
2 is replaced with either:
1. an affine transform: w? w0
2. a vector with only (D ? 1) free parameters, e.g.,
(1, w?2, ? ? ? , w
?
D)
3. an `1 renormalization: w/||w||1
In (1), regularization is biased towards w0, a weight
vector previously optimized using a competitive yet
much smaller feature set, such as core features of
a phrase-based (Koehn et al, 2007) or hierarchical
(Chiang, 2007) system. The requirement that this
feature set be small is to prevent overfitting. Other-
wise, any regularization toward an overfit parameter
vector w0 would defeat the purpose of introducing
a regularization term in the first place.3 In (2), the
transformation is motivated by the observation that
the D-parameter linear model of Equation 2 only
needs (D ? 1) degrees of freedom. Fixing one of
the components of w to any non-zero constant and
allowing the others to vary, the new linear model re-
tains the same modeling power, but the (D ? 1) free
parameters are no longer scale invariant, i.e., scaling
the (D ? 1)-dimensional vector now has an effect on
linear model predictions. In (3), the weight vector
is normalized as to have an `1-norm equal to 1. In
contrast, the `0 norm is scale insensitive, thus not
affected by this problem.
3.1 Exact line search with regularization
Optimizing with a regularized error surface requires
a change in the line search algorithm presented in
3(Gimpel and Smith, 2012, footnote 6) briefly mentions the
use of such a regularizer with its ramp loss objective function.
Section 2, but the other aspects of MERT remain the
same, and we can still use global search algorithms
such as coordinate ascent, Powell, and random di-
rections exactly the same way as with unregularized
MERT. Line search with a regularization term is still
as efficient as in (Och, 2003), and it is still guar-
anteed to find the optimum of the (now regularized)
objective function along the line. Considering again a
given point wt and a given direction dt at line search
iteration t, finding the optimum ?opt corresponds to
finding ? that minimizes:
S?
s=1
E(rs, e?(fs; ?)) +
||wt + ? ? dt||22
2?2
(5)
Since regularization does not affect the points at
which e?(fs; ?) changes its optimum, the points
?fs1 < ? ? ? < ?
fs
M of intersection in the upper enve-
lope remain the same, so the points of discontinuity
in the error surface remain the same. The difference
now is that the error count on each segment [?i?1, ?i]
is no longer constant. This means we need to adjust
the final step of line search, which consists of enu-
merating all [?i?1, ?i], and keeping the optimum of
Equation 5 for each segment. e?(fs; ?) remains con-
stant within the segment, so we only need to consider
the expression ||wt + ? ? dt||22 to select a segment
point. The optimum is either at the left edge, the right
edge, or in the middle if the vertex of the parabola
happens to lie within that segment.4 We compute
this optimum by finding the value ? for which the
derivative of the regularization term is zero. There is
an easy closed-form solution:
d
d?
[
||wt + ? ? dt||22
2?2
]
= 0
d
d?
[
?
i
(w2t,i + 2 ? ? ? wt,i ? dt,i + ?
2 ? d2t,i)
]
= 0
?
i
(2 ? wt,i ? dt,i + 2 ? ? ? d
2
t,i) = 0
? = ?
(?
i
wt,i ? dt,i
)/(?
i
d2t,i
)
= ?
wt?dt
dt?dt
This closed-form solution is computed in time pro-
portional to D, which doesn?t slow down the com-
4When the optimum is either at the left edge ?i?1 or right
edge ?i of a segment, we select a point at a small relative distance
within the segment (.999?i?1 + .001?i, in the former case) to
avoid ties in objective values.
1951
putation of Equation 5 for each segment (the con-
struction of each segment of the upper envelope is
proportional to D anyway).
We also use `0 regularization. While minimiza-
tion of the `0-norm is known to be NP-hard in gen-
eral (Hyder and Mahata, 2009), this optimization is
relatively trivial in the case of a line search. Indeed,
for a given segment, the value in Equation 5 is con-
stant everywhere except where we intersect any of
the coordinate hyperplanes, i.e., where one of the
coordinates is zero. Thus, our method consists of
evaluating Equation 5 at the intersection points be-
tween the line and coordinate hyperplanes, returning
the optimal point within the given segment. For any
segment that doesn?t cross any of these hyperplanes,
we evaluate the objective function at any point of the
segment (since the value is constant across the entire
segment).
4 Direction finding
4.1 A Gradient-based direction finder
Perhaps the greatest obstacle in scaling MERT
to many dimensions is finding good search direc-
tions. In problems of lower dimensions, iterating
through all the coordinates is computationally feasi-
ble, though not guaranteed to find a global maximum
even in the case of a perfect line search. As the
number of dimensions increases by orders of mag-
nitude, this coordinate direction approach becomes
less and less tractable, and the quality of the search
also suffers (Hopkins and May, 2011).
Optimization has traditionally relied on finding the
direction of steepest ascent: the gradient. Unfortu-
nately, the objective function optimized by MERT is
piecewise constant; while it may admit a subgradi-
ent, this direction is generally not very informative.
Instead we may consider a smoothed variation of the
original approximation. While some variants have
been considered (Och, 2003; Flanigan et al, 2013),
we use an expected BLEU approximation, assum-
ing hypotheses are drawn from a log-linear distri-
bution according to their parameter values (Smith
and Eisner, 2006). That is, we assume the proba-
bility of a translation candidate es,m is proportional
to (exp (w?hs,m))
?, where w are the parameters be-
ing optimized, hs,m is the vector of the features for
es,m, and ? is a scaling parameter. As ? approaches
infinity, the distribution places all its weight on the
highest scoring candidate.
The log of the BLEU score may be written as:
min
(
1?
R
C
, 0
)
+
1
N
N?
n=1
(logmn ? log cn)
where R is the sum of reference lengths across the
corpus, C is the sum of candidate lengths, mn is the
number of matched n-grams (potentially clipped),
and cn is the number of n-grams in all candidates.
Given a distribution over candidates, we can use
the expected value of the log of the BLEU score. This
is a smooth approximation to the BLEU score, which
asymptotically approaches the true BLEU score as
the scaling parameter ? approaches infinity. While
this expectation is difficult to compute exactly, we
can compute approximations thereof using Taylor se-
ries. Although prior work demonstrates that a second-
order Taylor approximation is feasible to compute
(Smith and Eisner, 2006), we find that a first-order
approximation is faster and very close to the second-
order approximation.5 The first order Taylor approxi-
mation is as follows:
min
(
1?
R
E[C]
, 0
)
+
1
N
N?
n=1
(logE[mn]? logE[cn])
where E is the expectation operator using the proba-
bility distribution P (h;w, ?).
First we note that the gradient ??wiP (h;w, ?) is
P (h;w, ?)
(
hi ?
?
h?
h?iP (h
?;w, ?)
)
Using the chain rule, the gradient of the first order
approximation to BLEU is as follows:
1
N
N?
n=1
( 1
E[mn]
?
h
mn(h)
?P (h;w, ?)
?wi
?
1
E[cn]
?
h
cn(h)
?P (h;w, ?)
?wi
)
+
{
0 if E[C] > R
R
E[C]2
?
h c1(h)
?P (h;w,?)
?wi
otherwise
5Experimentally, we compared our analytical gradient of
the first-order Taylor approximation with the finite-difference
gradients of the first- and second-order approximations, and we
found these three gradients to be very close in terms of cosine
similarity (> 0.99). We performed these measurements both at
arbitrary points and at points of convergence of MERT.
1952
In the case of `2-regularized MERT, the final gradi-
ent also includes the partial derivative of the regular-
ization penalty of Equation 4, which is wi/?2 for a
given component i of the gradient. We do not update
the gradient in the case of `0 regularization since the
`0-norm is not differentiable.
4.2 Search
Our search strategy consists of looking at the direc-
tions of steepest increase of expected BLEU, which
is similar to that of Smith and Eisner (2006), but with
the difference that we do so in the context of MERT.
We think this difference provides two benefits. First,
while the smooth approximation of BLEU reduces
the likelihood of remaining trapped in a local opti-
mum, we avoid approximation error by retaining the
original objective function. Second, the benefit of
exact line searches in MERT is that there is no need
to be concerned about step size, since step size in
MERT line searches is guaranteed to be optimal with
respect to the direction under consideration.
Finally, our gradient-based search algorithm oper-
ates as follows. Considering the current point wt, we
compute the gradient gt of the first order Taylor ap-
proximation at that point, using the current scaling pa-
rameter ?. (We initialize the search with ? = 0.01.)
We find the optimum along the line wt+? ?gt. When-
ever any given line search yields no improvement
larger than a small tolerance threshold, we multiply
? by two and perform a new line search. The increase
of this parameter ? corresponds to a cooling schedule
(Smith and Eisner, 2006), which progressively sharp-
ens the objective function to get a better estimate of
BLEU as the search converges to an optimum. We
repeatedly perform new line searches until ? exceeds
1000. The inability to improve the current optimum
with a sharp approximation (? > 1000) doesn?t mean
line searches would fail with smaller values, so we
find it helpful to repeat the above procedure until a
full pass of updates of ? from 0.01 to 1000 yields no
improvement.
4.3 Computational complexity
Computing the gradient increases the computational
cost of MERT, though not its asymptotic complexity.
The cost of a single exhaustive line search is
O (SM(D + logM + logS))
where S is the number of sentences, each with M
possible translations, andD is the number of features.
For each sentence, we first identify the model score
as a linear function of the step size, requiring two
dot products for an overall cost of O(SMD).6 Next
we construct the upper envelope for each sentence:
first the equations are sorted in increasing order of
slope, and then they are merged in linear time to form
an envelope, with an overall cost of O(SM logM).
A linear pass through the envelope converts these
into piecewise constant (or linear, or quadratic) repre-
sentations of the (regularized) loss function. Finally
the per-sentence envelopes are merged into a global
representation of the loss along that direction. Our
implementation successively merges adjacent pairs
of piecewise smooth loss function representations
until a single list remains. These logS passes lead to
a merging runtime of O(SM logS).
The time required to compute a gradient is pro-
portional to O(SMD). For each sentence, we first
gather the probability and its gradient, then use this to
compute expected n-gram counts and matches as well
as those gradients in time O(MD). A constant num-
ber of arithmetic operations suffice to compute the
final expected loss value and its gradient. Therefore,
computing the gradient does not increase the algo-
rithmic complexity when compared to conventional
approaches using coordinate ascent and random di-
rections. Likewise the runtime of a single iteration
is competitive with PRO, given that gradient finding
is generally the most expensive part of convex opti-
mization. Of course, it is difficult to compare overall
runtime of convex optimization with that of MERT,
as we know of no way to bound the number of gradi-
ent evaluations required for convergence with MERT.
Therefore, we resort to empirical comparison later in
the paper, and find that the two methods appear to
have comparable runtime.
6In the special case where the difference between the prior
direction and the current direction is sparse, we may update the
individual linear functions in time proportional to the number of
changed dimensions. Coordinate ascent in particular can update
the linear functions in time O(SM): to the intercept of the
equation for each translation, we may add the prior step size
multiplied by the feature value in the prior coordinate, and the
slope becomes the feature value in the new coordinate. However,
this optimization does not appear to be widely adopted, likely
because it does not lead to any speedup when random vectors,
conjugate directions, or other non-sparse directions are used.
1953
Language pair Train Tune Dev Test
G
B
M
Chinese-English 0.99M 1,797 1,000 1,082
(mt02+03) (mt05)
Finnish-English 2.20M 11,935 2,001 4,855
S
pa
rs
eH
R
M Chinese-English 3.51M 1,894 1,664 1,357
(mt05) (mt06) (mt08)
Arabic-English 1.49M 1,663 1,360 1,313
(mt06) (mt08) (mt09)
Table 1: Datasets for the two experimental conditions.
5 Experimental Design
Following Hopkins and May (2011), our experimen-
tal setup utilizes both real and synthetic data. The
motivation for using synthetic data is that it is a way
of gauging the quality of optimization methods, since
the data is constructed knowing the global optimum.
Hopkins and May also note that the use of an ob-
jective function that is linear in some gold weight
vector makes the search much simpler than in a real
translation setting, and they suggest that a learner
that performs poorly in such a simple scenario has
little hope of succeeding in a more complex one.
The setup of our synthetic data experiment is al-
most the same as that performed by Hopkins and
May (2011). We generate feature vectors of dimen-
sionality ranging from 10 to 1000. These features are
generated by drawing random numbers uniformly in
the interval [0, 500]. This synthetic dataset consists
of S=1000 source ?sentences?, and M=500 ?trans-
lation? hypotheses for each sentence. A pseudo
?BLEU? score is then computed for each hypothe-
sis, by computing the dot product between a prede-
fined gold weight vector w? and each feature vector
hs,m. By this linear construction, w? is guaranteed
to be a global optimum.7 The pseudo-BLEU score is
normalized for each M -best list, so that the transla-
tion with highest model score according to w? has
a BLEU score of 1, and so that the translation with
lowest model score for the sentence gets a BLEU of
zero. This normalization has no impact on search,
but makes results more interpretable.
For our translation experiments, we use multi-
stack phrase-based decoding (Koehn et al, 2007).
We report results for two feature sets: non-linear
features induced using Gradient Boosting Machines
(Toutanova and Ahn, 2013) and sparse lexicalized
7The objective function remains piecewise constant, and the
plateau containingw? maps to the optimal value of the function.
reordering features (Cherry, 2013). We exploit these
feature sets (GBM and SparseHRM, respectively) in
two distinct experimental conditions, which we de-
tail in the two next paragraphs. Both GBM and
SparseHRM augment baseline features similar to
Moses?: relative frequency and lexicalized phrase
translation scores for both translation directions; one
or two language model features, depending on the
language pair; distortion penalty; word and phrase
count; six lexicalized reordering features. For both
experimental conditions, phrase tables have maxi-
mum phrase length of 7 words on either side. In
reference to Table 1, we used the training set (Train)
for extracting phrase tables and language models; the
Tune set for optimization with MERT or PRO; the
Dev set for selecting hyperparameters of PRO and
regularized MERT; and the Test set for reporting fi-
nal results. In each experimental condition, we first
trained weights for the base feature sets, and then
decoded the Tune, Dev, and Test datasets, generating
500-best lists for each set. All results report rerank-
ing performance on these lists with different feature
sets and optimization methods, based on lower-cased
BLEU (Papineni et al, 2001).
The GBM feature set (Toutanova and Ahn, 2013)
consists of about 230 features automatically induced
using decision tree weak learners, which derive fea-
tures using various word-level, phrase-level, and mor-
phological attributes. For Chinese-English, the train-
ing corpus consists of approximately one million sen-
tence pairs from the FBIS and Hong Kong portions
of the LDC data for the NIST MT evaluation and the
Tune and Test sets are from NIST competitions. A
4-gram language model was trained on the Xinhua
portion of the English Gigaword corpus and on the
target side of the bitext. For Finnish-English we used
a dataset from a technical domain of software man-
uals. For this language pair we used two language
models: one very large model trained on billions of
words, and another language model trained from the
target side of the parallel training set.
The SparseHRM set (Cherry, 2013) contains 3600
sparse reordering features. For each phrase, the fea-
tures take the form of indicators describing its orienta-
tion in the derivation, and its lexical content in terms
of word clusters or frequent words. For both Chinese-
English and Arabic-English, systems are trained on
data from the NIST 2012 MT evaluation. 4-gram
1954
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
BL
EU
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
co
sin
e
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
Figure 2: Change in BLEU score and cosine similarity
to the gold weight vector w? as the number of features
increases, using the noisy synthetic experiments. The
gradient-based direction finding method is barely affected
by the noise. The increase of the number of dimensions en-
ables our direction finder to find a slightly better optimum,
which moved away from w? due to noise.
language models were trained on the target side of
the parallel training data for both Arabic and Chinese.
The Chinese systems development set is taken from
the NIST mt05 evaluation set, augmented with some
material reserved from our NIST training corpora in
order to better cover newsgroup and weblog domains.
6 Results
We conducted experiments with the synthetic data
scenario described in the previous section, as well
as with noise added to the data (Hopkins and May,
2011). The purpose of adding noise is to make the
optimization task more realistic. Specifically, af-
ter computing all pseudo-BLEU scores, we added
noise to each feature vector hs,m by drawing from
a zero-mean Gaussian with standard deviation 200.
Our results with both noiseless and noisy data yield
the same conclusion as Hopkins and May: standard
MERT struggles with many dimensions, and fails
to recover w?. However, our experiments with the
gradient direction finder of Section 4 are much more
positive. This direction finder not only recovers w?
 40
 50
 60
 70
 80
 90
 100
 1  10  100  1000
BL
EU
line search iteration
expected BLEU gradient(noisy) expected BLEU gradient
coordinate ascent(noisy) coordinate ascent
Figure 3: Comparison of rate of convergence between
coordinate ascent and our expected BLEU direction finder
(D = 500). Noisy refers to the noisy experimental setting.
(cosine > 0.999) even with 1000 dimensions, but its
effectiveness is also visible with noisy data, as seen
in Figure 2. The decrease of its cosine is relatively
small compared to other search algorithms, and this
decrease is not necessarily a sign of search errors
since the addition of noise causes the true optimum
to be different from w?. Finally, Figure 3 shows our
rate of convergence compared to coordinate ascent.
Our experimental results with the GBM feature
set data are shown in Table 2. Each table is di-
vided into three sections corresponding respectively
to MERT (Och, 2003) with Koehn-style coordinate
ascent (Koehn, 2004), PRO, and our optimizer featur-
ing both regularization and the gradient-based direc-
tion finder. All variants of MERT are initialized with
a single starting point, which is either uniform weight
or w0. Instead of providing MERT with additional
random starting points as in Moses, we use random
walks as in (Moore and Quirk, 2008) to attempt to
move out of local optima.8 Since PRO and our opti-
mizer have hyperparameters, we use a held-out set
(Dev) for adjusting them. For PRO, we adjust three
parameters: a regularization penalty for `2, the pa-
rameter ? in the add-? smoothed sentence-level ver-
sion of BLEU (Lin and Och, 2004), and a parameter
for scaling the corpus-level length of the references.
The latter scaling parameter is discussed in (He and
8In the case of the gradient-based direction finder, we also
use the following strategy whenever optimization converges to
a (possibly local) optimum. We run one round of coordinate
ascent, and continue with the gradient direction finder as soon as
the optimum improves. If the none of the coordinate directions
helped, we stop the search.
1955
Chinese-English Finnish-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 33.2 19.9 32.9 15 53.0 52.6 54.8
MERT uniform 224 33.0 19.2 32.1 232 53.2 51.7 53.8
MERT w0 224 34.1 20.1 33.0 232 53.9 52.5 54.7
PRO w0 224 33.4 20.1 33.3 232 53.3 52.9 55.3
`2 MERT (v1: ||w ?w0||) w0 224 33.2 20.3 33.5 232 53.2 52.7 55.2
`2 MERT (v2: D ? 1 dimensions) w0 224 33.0 20.4 33.2 232 52.9 52.6 55.0
`2 MERT (v3: `1-renormalized) w0 224 33.1 20.0 33.3 232 53.1 52.5 55.1
`0 MERT w0 224 33.4 20.3 33.2 232 53.2 52.6 55.1
Table 2: BLEU scores for GBM features. Model parameters were optimized on the Tune set. For PRO and regularized
MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental
condition the model that worked best on Dev. The table shows the performance of these retained models.
 51.2
 51.4
 51.6
 51.8
 52
 52.2
 52.4
 52.6
 1e-05  0.0001  0.001  0.01  0.1  1  10
BL
EU
regularization weight
expected BLEU gradient
coordinate ascent
Figure 4: BLEU score on the Finnish Dev set (GBM)
with different values for the 1/2?2 regularization weight.
To enable comparable results, the other hyperparameter
(length) is kept fixed.
Deng, 2012; Nakov et al, 2012) and addresses the
problem that systems tuned with PRO tend to pro-
duce sentences that are too short. On the other hand,
regularized MERT only requires one hyperparameter
to tune: a regularization penalty for `2 or `0. How-
ever, since PRO optimizes translation length on the
Dev dataset and MERT does so using the Tune set, a
comparison of the two systems would yield a discrep-
ancy in length that would be undesirable. Therefore,
we add another hyperparameter to regularized MERT
to tune length in the same manner using the Dev set.
Table 2 offers several findings. First, unregular-
ized MERT can achieve competitive results with a
small set of highly engineered features, but adding a
large set of more than 200 features causes MERT to
perform poorly, particularly on the test set. However,
unregularized MERT can recover much of this drop
of performance if it is given a good sparse initializer
w0. Regularized MERT (v1) provides an increase in
the order of 0.5 BLEU on the test set compared to
the best results with unregularized MERT. Regular-
ized MERT is competitive with PRO, even though the
number of features is relatively large. Using the same
GBM experimental setting, Figure 4 compares regu-
larized MERT using the gradient direction finder and
coordinate ascent. At the best regularization setting,
the two algorithms are comparable in terms of BLEU
(though coordinate ascent is slower due to its lack of
a good direction finder), but our method seems more
robust with suboptimal regularization parameters.
Our results with the SparseHRM feature set data
are shown in Table 3. As with the GBM feature set,
we find again that the version of `2 MERT regular-
ized towards ||w ?w0|| is competitive with PRO,
even though we train MERT with a large set of 3601
features.9 One remaining question is whether MERT
remains practical with large feature sets. As noted
in the complexity analysis of Section 4.3, MERT
has a dependence on the number of features that is
comparable to PRO, i.e., it is linear in both cases.
Practically, we find that optimization time is com-
parable between the two systems. In the case of
Chinese-English for the GBM feature set, one run of
the PRO optimizer took 26 minutes on average, while
regularized MERT with the gradient direction finder
took 37 minutes on average, taking into account the
time to compute w0. In the case of Chinese-English
for the SparseHRM feature set, average optimization
times for PRO and our method were 3.10 hours and
3.84 hours on average, respectively.
9We note that the experimental setup of (Cherry, 2013) inte-
grates the Sparse HRM features into the decoder, while we use
them in an M -best reranking scenario. The reranking setup of
this paper yields smaller improvements for both PRO and MERT
than those of (Cherry, 2013).
1956
Chinese-English Arabic-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 25.7 34.0 27.8 14 43.2 42.8 45.5
MERT uniform 3601 25.4 33.1 27.3 3601 45.7 42.3 44.9
MERT w0 3601 27.7 33.5 27.5 3601 46.0 42.4 45.2
PRO w0 3601 25.9 34.3 28.1 3601 44.6 43.4 46.1
`2 MERT (v1: ||w ?w0||) w0 3601 26.3 34.3 28.3 3601 45.2 43.2 46.0
`2 MERT (v2: D ? 1 dimensions) w0 3601 26.4 34.1 28.2 3601 45.0 43.4 45.9
`2 MERT (v3: `1-renormalized) w0 3601 26.1 34.0 27.9 3601 44.9 43.3 45.7
`0 MERT w0 3601 26.5 34.2 28.1 3601 45.4 43.1 46.0
Table 3: BLEU scores for SparseHRM features. Notes in Table 2 also apply here.
Finally, as shown in Table 2, we see that MERT ex-
periments that rely on a good initial starting point w0
generally perform better than when starting from
a uniform vector. While having to compute w0 in
the first place is a bit of a disadvantage compared
to standard MERT, the need for good initializer is
hardly surprising in the context of non-convex op-
timization. Other non-convex problems in machine
learning, such as deep neural networks (DNN) and
word alignment models, commonly require such ini-
tializers in order to obtain decent performance. In
the case of DNN, extensive research is devoted to the
problem of finding good initializers.10 In the case of
word alignment, it is common practice to initialize
search in non-convex optimization problems?such
as IBM Model 3 and 4 (Brown et al, 1993)?with
solutions of simpler models?such as IBM Model 1.
7 Related work
MERT and its extensions have been the target of ex-
tensive research (Och, 2003; Macherey et al, 2008;
Cer et al, 2008; Moore and Quirk, 2008; Kumar et
al., 2009; Galley and Quirk, 2011). More recent work
has focused on replacing MERT with a linearly de-
composable approximations of the evaluation metric
(Smith and Eisner, 2006; Liang et al, 2006; Watan-
abe et al, 2007; Chiang et al, 2008; Hopkins and
May, 2011; Rosti et al, 2011; Gimpel and Smith,
2012; Cherry and Foster, 2012), which generally
involve a surrogate loss function incorporating a reg-
ularization term such as the `2-norm. While we are
not aware of any previous work adding a penalty on
10For example, (Larochelle et al, 2009) presents a pre-trained
DNN that outperforms a shallow network, but the performance
of the DNN becomes much worse relative to the shallow network
once pre-training is turned off.
the weights in the context of MERT, (Cer et al, 2008)
achieves a related effect. Cer et al?s goal is to achieve
a more regular or smooth objective function, while
ours is to obtain a more regular set of parameters.
The two approaches may be complementary.
More recently, new research has explored direction
finding using a smooth surrogate loss function (Flani-
gan et al, 2013). Although this method is successful
in helping MERT find better directions, it also exac-
erbates the tendency of MERT to overfit.11 As an
indirect way of controlling overfitting on the tuning
set, their line searches are performed over directions
estimated over a separate dataset.
8 Conclusion
In this paper, we have shown that MERT can scale to
a much larger number of features than previously
thought, thanks to regularization and a direction
finder that directs the search towards the greatest
increase of expected BLEU score. While our best
results are comparable to PRO and not significantly
better, we think that this paper provides a deeper un-
derstanding of why standard MERT can fail when
handling an increasingly larger number of features.
Furthermore, this paper complements the analysis
by Hopkins and May (2011) of the differences be-
tween MERT and optimization with a surrogate loss
function.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments and suggestions.
11Indeed, in their Table 3, a comparison between HILS and
HOLS suggests tuning set performance improves substantially,
while held out performance degrades.
1957
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Comput. Linguist., 19(2):263?311.
Daniel Cer, Dan Jurafsky, and Christopher D. Manning.
2008. Regularization and search for minimum error
rate training. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 26?34.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 427?436.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 22?31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural
translation features. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 248?258.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49.
Kevin Gimpel and Noah A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 221?231.
Xiaodong He and Li Deng. 2012. Maximum expected
BLEU training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1, pages 292?301.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Acoustics, Speech and Signal Processing,
2009. ICASSP 2009. IEEE International Conference
on, pages 3365?3368.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Demonstration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation models.
In Proc. of AMTA, pages 115?124.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163?171.
Hugo Larochelle, Yoshua Bengio, Je?ro?me Louradour, and
Pascal Lamblin. 2009. Exploring strategies for training
deep neural networks. J. Mach. Learn. Res., 10:1?40.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In International Conference on Com-
putational Linguistics and Association for Computa-
tional Linguistics (COLING/ACL).
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of the 20th
international conference on Computational Linguistics,
Stroudsburg, PA, USA.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 725?734.
David McAllester and Joseph Keshet. 2011. Generaliza-
tion bounds and consistency for latent structural probit
and ramp loss. In Advances in Neural Information
Processing Systems 24, pages 2205?2212.
Robert C. Moore and Chris Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
pages 585?592.
1958
Preslav Nakov, Francisco Guzman, and Stephan Vogel.
2012. Optimizing for sentence-level BLEU+1 yields
short translations. In Proceedings of COLING 2012,
pages 1979?1994.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of ACL.
Kishore Papineni. 1999. Discriminative training via linear
programming. In Proceedings IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), volume 2, pages 561?564, Vol. 2.
M.J.D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Comput. J., 7(2):155?162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes:
The Art of Scientific Computing. Cambridge University
Press, 3rd edition.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected BLEU training
for graphs: BBN system description for WMT11 sys-
tem combination task. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
159?165.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 787?794.
Kristina Toutanova and Byung-Gyu Ahn. 2013. Learn-
ing non-linear features for machine translation using
gradient boosting machines. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 406?411.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 764?773.
1959
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 159?164,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Asymmetric Features of Human Generated Translation
Sauleh Eetemadi
Michigan State University, East Lansing, MI
Microsoft Research, Redmond, WA
saulehe@microsoft.com
Kristina Toutanova
Microsoft Research
Redmond, WA
kristout@microsoft.com
Abstract
Distinct properties of translated text have
been the subject of research in linguistics
for many year (Baker, 1993). In recent
years computational methods have been
developed to empirically verify the lin-
guistic theories about translated text (Ba-
roni and Bernardini, 2006). While many
characteristics of translated text are more
apparent in comparison to the original
text, most of the prior research has fo-
cused on monolingual features of trans-
lated and original text. The contribution
of this work is introducing bilingual fea-
tures that are capable of explaining dif-
ferences in translation direction using lo-
calized linguistic phenomena at the phrase
or sentence level, rather than using mono-
lingual statistics at the document level.
We show that these bilingual features out-
perform the monolingual features used in
prior work (Kurokawa et al., 2009) for the
task of classifying translation direction.
1 Introduction
It has been known for many years in linguis-
tics that translated text has distinct patterns com-
pared to original or authored text (Baker, 1993).
The term ?Translationese? is often used to refer
to the characteristics of translated text. Patterns
of Translationese can be categorized as follows
(Volansky et al., 2013):
1. Simplification: The process of translation is
often coupled with a simplification process at
several levels. For example, there tends to be
less lexical variety in translated text and rare
words are often avoided.
2. Explicitation: Translators often have to be
more explicit in their translations due to lack
of the cultural context that speakers of the
source language have. Another manifesta-
tion of this pattern is making arguments more
explicit which can be observed in the heavy
use of cohesive markers like ?therefore? and
?moreover? in translated text (Koppel and
Ordan, 2011).
3. Normalization: Translated text often con-
tains more formal and repeating language.
4. Interference: A translator is likely to pro-
duce a translation that is structurally and
grammatically closer to the source text or
their native language.
In Figure 1 the size of a word in the ?Translated?
section is proportional to the difference between
the frequency of the word in original and in the
translated text (Fellows, 2013). For example, it is
apparent that the word ?the? is over-represented
in translated English as noted by other research
(Volansky et al., 2013). In addition, cohesive
markers are clearly more common in translated
text.
In the past few years there has been work on ma-
chine learning techniques for identifying Trans-
lationese. Standard machine learning algorithms
like SVMs (Baroni and Bernardini, 2006) and
Bayesian Logistic Regression (Koppel and Ordan,
2011) have been employed to train classifiers for
one of the following tasks:
i. Given a chunk of text in a specific language,
classify it as ?Original? or ?Translated?.
ii. Given a chunk of translated text, predict the
source language of the translation.
iii. Given a text chunk pair and their languages,
predict the direction of translation.
There are two stated motivations for the tasks
above: first, empirical validation of linguistic the-
ories about Translationese (Volansky et al., 2013),
and second, improving statistical machine trans-
lation by leveraging the knowledge of the trans-
lation direction in training and test data (Lember-
159
Figure 1: EuroParl Word Cloud Data Visualiza-
tion (Translated vs Original)
1
sky et al., 2012a; Lembersky et al., 2013; Lember-
sky et al., 2012b). Few parallel corpora includ-
ing a customized version of EuroParl (Islam and
Mehler, 2012) and a processed version of Hansard
(Kurokawa et al., 2009) are labeled for translated
versus original text. Using these limited resources,
it has been shown that taking the translation direc-
tion into account when training a statistical ma-
chine translation system can improve translation
quality (Lembersky et al., 2013). However, im-
proving statistical machine translation using trans-
lation direction information has been limited by
several factors.
1. Limited Labeled Data: The amount of la-
beled data is limited by language and domain
and therefore by itself is not enough to make
a significant improvement in statistical ma-
chine translation.
2. Cross-Domain Scalability: Current meth-
ods of Translationese detection do not scale
across different corpora. For example, a
classifier trained on EuroParl corpus (Koehn,
2005) had in-domain accuracy of 92.7% but
out-of-domain accuracy of 64.8% (Koppel
and Ordan, 2011).
3. Text Chunk Size: The reported high accu-
racy of Translationese detection is based on
relatively large (approximately 1500 tokens)
text chunks (Koppel and Ordan, 2011). When
similar tasks are performed at the sentence
1
This word cloud was created using the word-
cloud and tm R packages (Fellows, 2013) from
EuroParl parallel data annotated for translation di-
rection (Islam and Mehler, 2012) obtained from
http://www.hucompute.org/ressourcen/corpora/56.
level the accuracy drops by 15 percentage
points or more (Kurokawa et al., 2009). Fig-
ure 2 shows how detection accuracy drops
with the reduction of the input text chunk
size. Since parallel data are often available
at the sentence level or small chunks of text,
existing detection methods aren?t suitable for
this type of data.
Figure 2: Effects of Chunk Size on Translationese
Detection Accuracy
2
Motivated by these limitations, in this work we
focus on improving sentence-level classification
accuracy by using non-domain-specific bilingual
features at the sentence level. In addition to im-
proving accuracy, these fine-grained features may
be better able to confirm existing theories or dis-
cover new linguistic phenomena that occur in the
translation process. We use a fast linear classi-
fier trained with online learning, Vowpal Wabbit
(Langford et al., 2007). The Hansard French-
English dataset (Kurokawa et al., 2009) is used for
training and test data in all experiments.
2 Related Work
While distinct patterns of Translationese have
been studied widely in the past, the work of Ba-
roni and Bernardini (2006) is the first to intro-
duce a computational method for detecting Trans-
lationese with high accuracy. Prior work has
shown in-domain accuracy can be very high at
the chunk-level if fully lexicalized features are
used (Volansky et al., 2013), but then the phenom-
ena learned are clearly not generalizable across
domains. For example, in Figure 1, it can be
observed that content words like ?commission?,
?council? or ?union? can be used effectively for
classification while they do not capture any gen-
eral linguistic phenomena and are unlikely to scale
2
This is a reproduction of the results of Koppel and Or-
dan (2011) using function word frequencies as features for a
logistic regression classifier. Based on the description of how
text chunks were created, the results of the paper (92.7% ac-
curacy) are based on text chunk sizes of approximately 1500
tokens.
160
Figure 3: POS Tagged Aligned Sentence Pairs
to other corpora. This is also confirmed by an
average human performance of 72.7% precision
with 82.1% recall on a similar task where the test
subjects were not familiar with the domain and
were not able to use domain-specific lexical fea-
tures (Baroni and Bernardini, 2006). A more gen-
eral feature set still with high in-domain accuracy
is POS tags with lexicalization of function words
(Baroni and Bernardini, 2006; Kurokawa et al.,
2009). We build on this feature set and explore
bilingual features.
The only work to consider features of the two
parallel chunks (one original, one translated) is the
work of Kurokawa et al. (2009). They simply used
the union of the n-gram mixed-POS
3
features of
the two sides; these are monolingual features of
the original and translated text and do not look at
translation phenomena directly. Their work is also
the only work to look at sentence level detection
accuracy and report 15 percentage points drop in
accuracy when going from chunk level to sentence
level classification.
3 Bilingual Features for Translation
Direction Classification
We are interested in learning common localized
linguistic phenomena that occur during the trans-
lation process when translating in one direction
but not the other.
3.1 POS Tag MTUs
Minimal translation units (MTUs) for a sentence
pair are defined as pairs of source and target word
sets that satisfy the following conditions (Quirk
and Menezes, 2006).
1. No alignment links between distinct MTUs.
2. MTUs are not decomposable into smaller
MTUs without violating the previous rule.
We use POS tags to capture linguistic struc-
tures and MTUs to map linguistic structures of
3
Only replacing content words with their POS tags while
leaving function words as is.
the two languages. To obtain POS MTUs from
a parallel corpus, first, the parallel corpus is word
aligned. Next, the source and target side of the
corpus are tagged independently. Finally, words
are replaced with their corresponding POS tag
in word-aligned sentence pairs. MTUs were ex-
tracted from the POS tagged word-aligned sen-
tence pairs from left to right and listed in source
order. Unigram, bi-gram, and higher order n-
gram features were built over this sequence of
POS MTUs. For example, for the sentence pair
in Figure 3, the following POS MTUs will be ex-
tracted: VBZ?D, PRP?(N,V), RB?ADV,
JJ?N, .?PUNC.
3.2 Distortion
In addition to the mapping of linguistic structures,
another interesting phenomenon is the reordering
of linguistic structures during translation. One hy-
pothesis is that when translating from a fixed-order
to a free-order language, the order of the target will
be very influenced by the source (almost mono-
tone translation), but when translating into a fixed
order language, more re-ordering is required to
ensure grammaticality of the target. To capture
this pattern we add distortion to POS Tag MTU
features. We experiment with absolute distortion
(word position difference between source and tar-
get of a link) as well as HMM distortion (word
position difference between the target of a link and
the target of the previous link). We bin the distor-
tions into three bins: ?= 0?, ?> 0? and ?< 0?, to
reduce sparsity.
4 Experimental Setup
For the translation direction detection task ex-
plained in section 1, we use a fast linear classi-
fier trained with online learning, Vowpal Wabbit
(Langford et al., 2007). Training data and classi-
fication features are explained in section 4.1 and
4.2.
161
Figure 4: Sentence level translation direction detection precision using different features with n-gram
lengths of 1 through 5.
4.1 Data
For this task we require a parallel corpus with sen-
tence pairs available in both directions (sentences
authored in language A and then translated to lan-
guage B and vice versa). While the customized
version of EuroParl (Islam and Mehler, 2012) con-
tains sentence pairs for many language pairs, none
of the language pairs have sentence pairs available
in both directions (e.g., it does contain sentences
authored in English and translated into French but
not vice versa). The Canadian Hansard corpus
on the other hand fits the requirement as it has
742,408 sentence pairs translated from French to
English and 2,203,504 sentences pairs that were
translated from English to French (Kurokawa et
al., 2009). We use the Hansard data for training
classifiers. For training the HMM word alignment
model used to define features, we use a larger set
of ten billion words of parallel text from the WMT
English-French corpus.
4.2 Preprocessing and Feature Extraction
We used a language filter
4
, deduplication filter
5
and length ratio filter to clean the data. After fil-
tering we were left with 1,890,603 English-French
sentence pairs and 640,117 French-English sen-
tence pairs. The Stanford POS tagger (Toutanova
and Manning, 2000) was used to tag the English
and the French sides of the corpus. The HMM
alignment model (Vogel et al., 1996) trained on
4
A character n-gram language model is used to detect the
language of source and target side text and filter them out if
they do not match their annotated language.
5
Duplicate sentences pairs are filtered out.
WMT data was used to word-align the Hansard
corpus while replacing words with their corre-
sponding POS tags. Due to differences in word
breaking between the POS tagger tool and our
word alignment tool there were some mismatches.
For simplicity we dropped the entire sentence pair
whenever a token mismatch occurred. This left us
with 401,569 POS tag aligned sentence pairs in the
French to English direction and 1,184,702 pairs in
the other direction. We chose to create a balanced
dataset and reduced the number of English-French
sentences to 401,679 with 20,000 sentence pairs
held out for testing in each direction.
5 Results
The results of our experiments on the translation
direction detection task are listed in Table 4. We
would like to point out several results from the
table. First, when using only unigram features,
the highest accuracy is achieved by the ?POS-
MTU + HMM Distortion? feature, which uses
POS minimal translation units together with dis-
tortion. The highest accuracy overall if obtained
by a ?POS-MTU? trigram model, showing the ad-
vantage of bilingual features over prior work us-
ing only a union of monolingual features (repro-
duced by the ?English-POS + French-POS? con-
figuration). While higher order features generally
show better in-domain accuracy, the advantage of
low-order bilingual features might be even higher
in cross-domain classification.
6
For description of English POS tags see (Marcus et al.,
1993) and (Abeill?e et al., 2003) for French
162
POS MTU (E?F) FE# EF# Example
1 NNPS?(N,C) 336 12 quebecers(NNPS)? qu?eb?ecoises(N) et(C) des qu?eb?ecois
2 IN?(CL,V) 69 1027 a few days ago(IN)? il y(CL) a(V) quelques
3 PRP?(N,V) 18 663 he(PRP) is? le d?eput?e(N) `a(V)
4 (NNP,POS)?A 155 28 quebec(NNP) ?s(POS) history? histoire qu?eb?ecoises(A)
5 (FW,FW)?ADV 7 195 pro(FW) bono(FW) work? b?en?evolement(ADV) travailler
6 (RB,MD)?V 2 112 money alone(RB) could(MD) solve? argent suffirait(V) `a r?esoudre
Table 1: POS MTU features with highest weight. FE# indicates the number of times this feature ap-
peared when translating from French to English.
6
6 Analysis
An interesting aspect of this work is that it is able
to extract features that can be linguistically inter-
preted. Although linguistic analysis of these fea-
tures is outside the scope of this work, we list
POS MTU features with highest positive or neg-
ative weights in Table 1. Although the top feature,
NNPS?(N,C)
7
, in this context is originating
from a common phrase used by French speaking
members of the Canadian Parliament, qu?eb?ecoises
et des qu?eb?ecois, it does highlight an underlying
linguistic phenomenon that is not specific to the
Canadian Parliament. When translating a plural
noun from English to French it is likely that only
the masculine form of the noun appears, while if
it was authored in French with both forms of the
nouns, a single plural noun would appear in En-
glish as English doesn?t have masculine and femi-
nine forms of the word. A more complete form of
this feature would have been NNPS?(N,C,N),
but since word alignment models, in general, dis-
courage one-to-many alignments, the extracted
MTU only covers the first noun and conjunction.
7 Conclusion and Future Work
In this work we introduce new features for transla-
tion direction detection that leverage word align-
ment, source POS and target POS in the form
of POS MTUs. POS MTUs are a powerful tool
for capturing linguistic interactions between lan-
guages during the translation process. Since POS
MTUs are not lexical features they are more likely
to scale across corpora and domains compared to
lexicalized features. Although most of the high
weight POS MTU features used in classification
(Table 1) are not corpus specific, unfortunately,
due to lack of training data in multiple domains,
experiments were not run to validate this claim.
In future work, we intend to obtain training data
7
NNPS: Plural Noun, N: Noun, C:Conjunction
from multiple domains that enables us to verify
cross-domain scalability of POS-MTUs. In addi-
tion, observing linguistic phenomena that occur in
one translation direction but not the other can be
very informative in improving statistical machine
translation quality. Another future direction for
this work is leveraging sentence level translation
direction detection to improve statistical machine
translation output quality. Finally, further investi-
gation of the linguistic interpretation of individual
feature that are most discriminating between op-
posite translation directions can lead to discovery
of new linguistic phenomena that occur during the
translation process.
Acknowledgement
The authors would like to thank Lee Schwartz for
analyzing classification features and providing lin-
guistic insight for them. We would like to also ac-
knowledge the thoughtful comments and detailed
feedback of the reviewers which helped us im-
prove the paper.
References
Anne Abeill?e, Lionel Cl?ement, and Franc?ois Tou-
ssenel. 2003. Building a treebank for french. In
Anne Abeill?e, editor, Treebanks, volume 20 of Text,
Speech and Language Technology, pages 165?187.
Springer Netherlands.
Mona Baker. 1993. Corpus linguistics and transla-
tion studies: Implications and applications. Text and
technology: in honour of John Sinclair, 233:250.
Marco Baroni and Silvia Bernardini. 2006. A new
approach to the study of translationese: Machine-
learning the difference between original and trans-
lated text. Literary and Linguistic Computing,
21(3):259?274.
Ian Fellows, 2013. wordcloud: Word Clouds. R pack-
age version 2.4.
163
Zahurul Islam and Alexander Mehler. 2012. Cus-
tomization of the europarl corpus for translation
studies. In LREC, page 2505?2510.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, page 1318?1326. Association for Computational
Linguistics.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and
its impact on machine translation. Proceedings. MT
Summit XII, The twelfth Machine Translation Sum-
mit International Association for Machine Transla-
tion hosted by the Association for Machine Transla-
tion in the Americas.
J Langford, L Li, and A Strehl, 2007. Vowpal wabbit
online learning project.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012a. Adapting translation models to trans-
lationese improves SMT. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, page 255?265.
Association for Computational Linguistics.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012b. Language models for machine trans-
lation: Original vs. translated texts. Computational
Linguistics, 38(4):799?825.
Gennadi Lembersky, Noam Ordan, and Shuly Wintner.
2013. Improving statistical machine translation by
adapting translation models to translationese.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
put. Linguist., 19(2):313?330, June.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases?: Challenging the conventional wisdom in
statistical machine translation. In Proceedings of
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, HLT-
NAACL ?06, pages 9?16, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Process-
ing and Very Large Corpora: Held in Conjunction
with the 38th Annual Meeting of the Association
for Computational Linguistics - Volume 13, EMNLP
?00, pages 63?70, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
Vered Volansky, Noam Ordan, and Shuly Wintner.
2013. On the features of translationese. Literary
and Linguistic Computing, page fqt031.
164
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403?411,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Parallel Sentences from Comparable Corpora using Document
Level Alignment
Jason R. Smith?
Center for Lang. and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
jsmith@cs.jhu.edu
Chris Quirk and Kristina Toutanova
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{chrisq,kristout}@microsoft.com
Abstract
The quality of a statistical machine transla-
tion (SMT) system is heavily dependent upon
the amount of parallel sentences used in train-
ing. In recent years, there have been several
approaches developed for obtaining parallel
sentences from non-parallel, or comparable
data, such as news articles published within
the same time period (Munteanu and Marcu,
2005), or web pages with a similar structure
(Resnik and Smith, 2003). One resource not
yet thoroughly explored is Wikipedia, an on-
line encyclopedia containing linked articles
in many languages. We advance the state
of the art in parallel sentence extraction by
modeling the document level alignment, mo-
tivated by the observation that parallel sen-
tence pairs are often found in close proximity.
We also include features which make use of
the additional annotation given by Wikipedia,
and features using an automatically induced
lexicon model. Results for both accuracy
in sentence extraction and downstream im-
provement in an SMT system are presented.
1 Introduction
For any statistical machine translation system, the
size of the parallel corpus used for training is a ma-
jor factor in its performance. For some language
pairs, such as Chinese-English and Arabic-English,
large amounts of parallel data are readily available,
but for most language pairs this is not the case. The
?This research was conducted during the author?s intern-
ship at Microsoft Research.
domain of the parallel corpus also strongly influ-
ences the quality of translations produced. Many
parallel corpora are taken from the news domain, or
from parliamentary proceedings. Translation qual-
ity suffers when a system is not trained on any data
from the domain it is tested on.
While parallel corpora may be scarce, compara-
ble, or semi-parallel corpora are readily available
in several domains and language pairs. These cor-
pora consist of a set of documents in two languages
containing similar information. (See Section 2.1
for a more detailed description of the types of non-
parallel corpora.) In most previous work on ex-
traction of parallel sentences from comparable cor-
pora, some coarse document-level similarity is used
to determine which document pairs contain paral-
lel sentences. For identifying similar web pages,
Resnik and Smith (2003) compare the HTML struc-
ture. Munteanu and Marcu (2005) use publication
date and vector-based similarity (after projecting
words through a bilingual dictionary) to identify
similar news articles.
Once promising document pairs are identified,
the next step is to extract parallel sentences. Usu-
ally, some seed parallel data is assumed to be avail-
able. This data is used to train a word align-
ment model, such as IBM Model 1 (Brown et al,
1993) or HMM-based word alignment (Vogel et al,
1996). Statistics from this word alignment model
are used to train a classifier which identifies bilin-
gual sentence pairs as parallel or not parallel. This
classifier is applied to all sentence pairs in docu-
ments which were found to be similar. Typically,
some pruning is done to reduce the number of sen-
403
tence pairs that need to be classified.
While these methods have been applied to news
corpora and web pages, very little attention has
been given to Wikipedia as a source of parallel sen-
tences. This is surprising, given that Wikipedia
contains annotated article alignments, and much
work has been done on extracting bilingual lexi-
cons on this dataset. Adafre and de Rijke (2006)
extracted similar sentences from Wikipedia article
pairs, but only evaluated precision on a small num-
ber of extracted sentences.
In this paper, we more thoroughly investigate
Wikipedia?s viability as a comparable corpus, and
describe novel methods for parallel sentence ex-
traction. Section 2 describes the multilingual re-
sources available in Wikipedia. Section 3 gives fur-
ther background on previous methods for parallel
sentence extraction on comparable corpora, and de-
scribes our approach, which finds a global sentence
alignment between two documents. In Section
4, we compare our approach with previous meth-
ods on datasets derived from Wikipedia for three
language pairs (Spanish-English, German-English,
and Bulgarian-English), and show improvements in
downstream SMT performance by adding the paral-
lel data we extracted.
2 Wikipedia as a Comparable Corpus
Wikipedia (Wikipedia, 2004) is an online collabo-
rative encyclopedia available in a wide variety of
languages. While the English Wikipedia is the
largest, with over 3 million articles, there are 24
language editions with at least 100,000 articles.
Articles on the same topic in different languages
are also connected via ?interwiki? links, which are
annotated by users. This is an extremely valuable
resource when extracting parallel sentences, as the
document alignment is already provided. Table
1 shows how many of these ?interwiki? links are
present between the English Wikipedia and the 16
largest non-English Wikipedias.
Wikipedia?s markup contains other useful indica-
tors for parallel sentence extraction. The many hy-
perlinks found in articles have previously been used
as a valuable source of information. (Adafre and
de Rijke, 2006) use matching hyperlinks to iden-
tify similar sentences. Two links match if the arti-
Figure 1: Captions for an image of a foil in English and
Spanish
cles they refer to are connected by an ?interwiki?
link. Also, images in Wikipedia are often stored
in a central source across different languages; this
allows identification of captions which may be par-
allel (see Figure 1). Finally, there are other minor
forms of markup which may be useful for finding
similar content across languages, such as lists and
section headings. In Section 3.3, we will explain
how features are derived from this markup.
2.1 Types of Non-Parallel Corpora
Fung and Cheung (2004) give a more fine-grained
description of the types of non-parallel corpora,
which we will briefly summarize. A noisy parallel
corpus has documents which contain many parallel
sentences in roughly the same order. Comparable
corpora contain topic aligned documents which are
not translations of each other. The corpora Fung
and Cheung (2004) examine are quasi-comparable:
they contain bilingual documents which are not
necessarily on the same topic.
Wikipedia is a special case, since the aligned
article pairs may range from being almost com-
pletely parallel (e.g., the Spanish and English en-
tries for ?Antiparticle?) to containing almost no par-
allel sentences (the Spanish and English entries for
?John Calvin?), despite being topic-aligned. It is
best characterized as a mix of noisy parallel and
comparable article pairs. Some Wikipedia authors
will translate articles from another language; others
404
French German Polish Italian Dutch Portuguese Spanish Japanese
496K 488K 384K 380K 357K 323K 311K 252K
Russian Swedish Finnish Chinese Norwegian Volapu?k Catalan Czech
232K 197K 146K 142K 141K 106K 103K 87K
Table 1: Number of aligned bilingual articles in Wikipedia by language (paired with English).
write the content themselves. Furthermore, even ar-
ticles created through translations may later diverge
due to independent edits in either language.
3 Models for Parallel Sentence Extraction
In this section, we will focus on methods for ex-
tracting parallel sentences from aligned, compara-
ble documents. The related problem of automatic
document alignment in news and web corpora has
been explored by a number of researchers, includ-
ing Resnik and Smith (2003), Munteanu and Marcu
(2005), Tillmann and Xu (2009), and Tillmann
(2009). Since our corpus already contains docu-
ment alignments, we sidestep this problem, and will
not discuss further details of this issue. That said,
we believe that our methods will be effective in cor-
pora without document alignments when combined
with one of the aforementioned algorithms.
3.1 Binary Classifiers and Rankers
Much of the previous work involves building a
binary classifier for sentence pairs to determine
whether or not they are parallel (Munteanu and
Marcu, 2005; Tillmann, 2009). The training data
usually comes from a standard parallel corpus.
There is a substantial class imbalance (O(n) pos-
itive examples, and O(n2) negative examples), and
various heuristics are used to mitigate this prob-
lem. Munteanu and Marcu (2005) filter out neg-
ative examples with high length difference or low
word overlap (based on a bilingual dictionary).
We propose an alternative approach: we learn
a ranking model, which, for each sentence in the
source document, selects either a sentence in the
target document that it is parallel to, or ?null?. This
formulation of the problem avoids the class imbal-
ance issue of the binary classifier.
In both the binary classifier approach and the
ranking approach, we use a Maximum Entropy
classifier, following Munteanu and Marcu (2005).
3.2 Sequence Models
In Wikipedia article pairs, it is common for par-
allel sentences to occur in clusters. A global sen-
tence alignment model is able to capture this phe-
nomenon. For both parallel and comparable cor-
pora, global sentence alignments have been used,
though the alignments were monotonic (Gale and
Church, 1991; Moore, 2002; Zhao and Vogel,
2002). Our model is a first order linear chain Condi-
tional Random Field (CRF) (Lafferty et al, 2001).
The set of source and target sentences are observed.
For each source sentence, we have a hidden vari-
able indicating the corresponding target sentence
to which it is aligned (or null). The model is simi-
lar to the discriminative CRF-based word alignment
model of (Blunsom and Cohn, 2006).
3.3 Features
Our features can be grouped into four categories.
Features derived from word alignments
We use a feature set inspired by (Munteanu and
Marcu, 2005), who defined features primarily based
on IBM Model 1 alignments (Brown et al, 1993).
We also use HMM word alignments (Vogel et al,
1996) in both directions (source to target and target
to source), and extract the following features based
on these four alignments:1
1. Log probability of the alignment
2. Number of aligned/unaligned words
3. Longest aligned/unaligned sequence of words
4. Number of words with fertility 1, 2, and 3+
We also define two more features which are in-
dependent of word alignment models. One is a
sentence length feature taken from (Moore, 2002),
1These are all derived from the one best alignment, and
normalized by sentence length.
405
which models the length ratio between the source
and target sentences with a Poisson distribution.
The other feature is the difference in relative doc-
ument position of the two sentences, capturing the
idea that the aligned articles have a similar topic
progression.
The above features are all defined on sentence
pairs, and are included in the binary classifier and
ranking model.
Distortion features
In the sequence model, we use additional dis-
tortion features, which only look at the difference
between the position of the previous and current
aligned sentences. One set of features bins these
distances; another looks at the absolute difference
between the expected position (one after the previ-
ous aligned sentence) and the actual position.
Features derived from Wikipedia markup
Three features are derived from Wikipedia?s
markup. The first is the number of matching links
in the sentence pair. The links are weighted by their
inverse frequency in the document, so a link that
appears often does not contribute much to this fea-
ture?s value. The image feature fires whenever two
sentences are captions of the same image, and the
list feature fires when two sentences are both items
in a list. These last two indicator features fire with
a negative value when the feature matches on one
sentence and not the other.
None of the above features fire on a null align-
ment, in either the ranker or CRF. There is also a
bias feature for these two models, which fires on all
non-null alignments.
Word-level induced lexicon features
A common problem with approaches for paral-
lel sentence classification, which rely heavily on
alignment models trained from unrelated corpora,
is low recall due to unknown words in the candi-
date sentence-pairs. One approach that begins to
address this problem is the use of self-training, as
in (Munteanu and Marcu, 2005). However, a self-
trained sentence pair extraction system is only able
to acquire new lexical items that occur in parallel
sentences. Within Wikipedia, many linked article
pairs do not contain any parallel sentences, yet con-
tain many words and phrases that are good transla-
tions of each other.
In this paper we explore an alternative approach
to lexicon acquisition for use in parallel sentence
extraction. We build a lexicon model using an ap-
proach similar to ones developed for unsupervised
lexicon induction from monolingual or compara-
ble corpora (Rapp, 1999; Koehn and Knight, 2002;
Haghighi et al, 2008). We briefly describe the lex-
icon model and its use in sentence-extraction.
The lexicon model is based on a probabilistic
modelP (wt|ws, T, S) wherewt is a word in the tar-
get language, ws is a word in the source language,
and T and S are linked articles in the target and
source languages, respectively.
We train this model similarly to the sentence-
extraction ranking model, with the difference that
we are aligning word pairs and not sentence pairs.
The model is trained from a small set of annotated
Wikipedia article pairs, where for some words in
the source language we have marked one or more
words as corresponding to the source word (in the
context of the article pair), or have indicated that the
source word does not have a corresponding transla-
tion in the target article. The word-level annotated
articles are disjoint from the sentence-aligned arti-
cles described in Section 4. The following features
are used in the lexicon model:
Translation probability. This is the translation
probability p(wt|ws) from the HMM word align-
ment model trained on the seed parallel data. We
also use the probability in the other direction, as
well as the log-probabilities in the two directions.
Position difference. This is the absolute value of
the difference in relative position of words ws and
wt in the articles S and T .
Orthographic similarity. This is a function of the
edit distance between source and target words. The
edit distance between words written in different al-
phabets is computed by first performing a determin-
istic phonetic translation of the words to a common
alphabet. The translation is inexact and this is a
promising area for improvement. A similar source
of information has been used to create seed lexicons
in (Koehn and Knight, 2002) and as part of the fea-
ture space in (Haghighi et al, 2008).
Context translation probability. This feature
looks at all words occurring next to word ws in the
406
article S and next to wt in the article T in a local
context window (we used one word to the left and
one word to the right), and computes several scor-
ing functions measuring the translation correspon-
dence between the contexts (using the IBM Model
1 trained from seed parallel data). This feature is
similar to distributional similarity measures used in
previous work, with the difference that it is limited
to contexts of words within a linked article pair.
Distributional similarity. This feature corre-
sponds more closely to context similarity measures
used in previous work on lexicon induction. For
each source headword ws, we collect a distribu-
tion over context positions o ? {?2,?1,+1,+2}
and context words vs in those positions based on a
count of times a context word occurred at that off-
set from a headword: P (o, vs|ws) ? weight(o) ?
C(ws, o, vs). Adjacent positions ?1 and +1 have
a weight of 2; other positions have a weight of 1.
Likewise we gather a distribution over target words
and contexts for each target headword P (o, vt|wt).
Using an IBM Model 1 word translation table
P (vt|vs) estimated on the seed parallel corpus,
we estimate a cross-lingual context distribution as
P (o, vt|ws) =
?
vs P (vt|vs) ? P (o, vs|ws). We de-
fine the similarity of a words ws and wt as one mi-
nus the Jensen-Shannon divergence of the distribu-
tions over positions and target words.2
Given this small set of feature functions, we
train the weights of a log-linear ranking model for
P (wt|ws, T, S), based on the word-level annotated
Wikipedia article pairs. After a model is trained,
we generate a new translation table Plex(t|s) which
is defined as Plex(t|s) ?
?
t?T,s?S P (t|s, T, S).
The summation is over occurrences of the source
and target word in linked Wikipedia articles. This
new translation table is used to define another
HMM word-alignment model (together with dis-
tortion probabilities trained from parallel data) for
use in the sentence extraction models. Two copies
of each feature using the HMM word alignment
model are generated: one using the seed data HMM
2We restrict our attention to words with ten or more occur-
rences, since rare words have poorly estimated distributions.
Also we discard the contribution from any context position and
word pair that relates to more than 1,000 distinct source or tar-
get words, since it explodes the computational overhead and
has little impact on the final similarity score.
model, and another using this new HMM model.
The training data for Bulgarian consisted of two
partially annotated Wikipedia article pairs. For
German and Spanish we used the feature weights
of the model trained on Bulgarian, because we did
not have word-level annotated Wikipedia articles.
4 Experiments
4.1 Data
We annotated twenty Wikipedia article pairs for
three language pairs: Spanish-English, Bulgarian-
English, and German-English. Each sentence
in the source language was annotated with pos-
sible parallel sentences in the target language
(the target language was English in all experi-
ments). The pairs were annotated with a quality
level: 1 if the sentences contained some parallel
fragments, 2 if the sentences were mostly paral-
lel with some missing words, and 3 if the sen-
tences appeared to be direct translations. In all
experiments, sentence pairs with quality 2 or 3
were taken as positive examples. The resulting
datasets are available at http://research.microsoft.com/en-
us/people/chrisq/wikidownload.aspx.
For our seed parallel data, we used the Europarl
corpus (Koehn, 2005) for Spanish and German and
the JRC-Aquis corpus for Bulgarian, plus the article
titles for parallel Wikipedia documents, and trans-
lations available from Wiktionary entries.3
4.2 Intrinsic Evaluation
Using 5-fold cross-validation on the 20 document
pairs for each language condition, we compared the
binary classifier, ranker, and CRF models for paral-
lel sentence extraction. To tune for precision/recall,
we used minimum Bayes risk decoding. We define
the loss L(?, ?) of picking target sentence ? when
the correct target sentence is ? as 0 if ? = ?, ?
if ? = NULL and ? 6= NULL, and 1 otherwise.
By modifying the null loss ?, the precision/recall
trade-off can be adjusted. For the CRF model, we
used posterior decoding to make the minimum risk
decision rule tractable. As a summary measure of
the performance of the models at different levels of
recall we use average precision as defined in (Ido
3Wiktionary is an online collaborative dictionary, similar to
Wikipedia.
407
Language Pair Binary Classifier Ranker CRF
Avg Prec R@90 R@80 Avg Prec R@90 R@80 Avg Prec R@90 R@80
English-Bulgarian 75.7 33.9 56.2 76.3 38.8 57.0 80.6 52.9 59.5
English-Spanish 90.4 81.3 87.6 93.4 81.0 84.5 94.7 87.6 90.2
English-German 61.8 9.4 27.5 66.4 25.7 42.4 78.9 52.2 54.7
Table 2: Average precision, recall at 90% precision, and recall at 80% precision for each model in all three language
pairs. In these experiments, the Wikipedia features and lexicon features are omitted.
Setting Ranker CRF
Avg Prec R@90 R@80 Avg Prec R@90 R@80
English-Bulgarian
One Direction 76.3 38.8 57.0 80.6 52.9 59.5
Intersected 78.2 47.9 60.3 79.9 38.8 57.0
Intersected +Wiki 80.8 39.7 68.6 82.1 53.7 62.8
Intersected +Wiki +Lex 89.3 64.4 79.3 90.9 72.0 81.8
English-Spanish
One Direction 93.4 81.0 84.5 94.7 87.6 90.2
Intersected 94.3 82.4 89.0 95.4 88.5 91.8
Intersected +Wiki 94.5 82.4 89.0 95.6 89.2 92.7
Intersected +Wiki +Lex 95.8 87.4 91.1 96.4 90.4 93.7
English-German
One Direction 66.4 25.7 42.4 78.9 52.2 54.7
Intersected 71.9 36.2 43.8 80.9 54.0 67.0
Intersected +Wiki 74.0 38.8 45.3 82.4 56.9 71.0
Intersected +Wiki +Lex 78.7 46.4 59.1 83.9 58.7 68.8
Table 3: Average precision, recall at 90% precision, and recall at 80% precision for the Ranker and CRF in all three
language pairs. ?+Wiki? indicates that Wikipedia features were used, and ?+Lex? means the lexicon features were
used.
et al, 2006). We also report recall at precision of
90 and 80 percent. Table 2 compares the different
models in all three language pairs.
In our next set of experiments, we looked at the
effects of the Wikipedia specific features. Since the
ranker and CRF are asymmetric models, we also
experimented with running the models in both di-
rections and combining their outputs by intersec-
tion. These results are shown in Table 3.
Identifying the agreement between two asym-
metric models is a commonly exploited trick else-
where in machine translation. It is mostly effec-
tive here as well, improving all cases except for
the Bulgarian-English CRF where the regression is
slight. More successful are the Wikipedia features,
which provide an auxiliary signal of potential par-
allelism.
The gains from adding the lexicon-based features
can be dramatic as in the case of Bulgarian (the
CRF model average precision increased by nearly
9 points). The lower gains on Spanish and German
may be due in part to the lack of language-specific
training data. These results are very promising and
motivate further exploration. We also note that this
is perhaps the first successful practical application
of an automatically induced word translation lexi-
con.
4.3 SMT Evaluation
We also present results in the context of a full ma-
chine translation system to evaluate the potential
utility of this data. A standard phrasal SMT sys-
tem (Koehn et al, 2003) serves as our testbed, us-
ing a conventional set of models: phrasal mod-
408
els of source given target and target given source;
lexical weighting models in both directions, lan-
guage model, word count, phrase count, distortion
penalty, and a lexicalized reordering model. Given
that the extracted Wikipedia data takes the standard
form of parallel sentences, it would be easy to ex-
ploit this same data in a number of systems.
For each language pair we explored two training
conditions. The ?Medium? data condition used eas-
ily downloadable corpora: Europarl for German-
English and Spanish-English, and JRC/Acquis for
Bulgarian-English. Additionally we included titles
of all linked Wikipedia articles as parallel sentences
in the medium data condition. The ?Large? data
condition includes all the medium data, and also in-
cludes using a broad range of available sources such
as data scraped from the web (Resnik and Smith,
2003), data from the United Nations, phrase books,
software documentation, and more.
In each condition, we explored the impact of in-
cluding additional parallel sentences automatically
extracted from Wikipedia in the system training
data. For German-English and Spanish-English,
we extracted data with the null loss adjusted to
achieve an estimated precision of 95 percent, and
for English-Bulgarian a precision of 90 percent. Ta-
ble 4 summarizes the characteristics of these data
sets. We were pleasantly surprised at the amount
of parallel sentences extracted from such a var-
ied comparable corpus. Apparently the average
Wikipedia article contains at least a handful of
parallel sentences, suggesting this is a very fertile
ground for training MT systems.
The extracted Wikipedia data is likely to make
the greatest impact on broad domain test sets ? in-
deed, initial experimentation showed little BLEU
gain on in-domain test sets such as Europarl, where
out-of-domain training data is unlikely to provide
appropriate phrasal translations. Therefore, we ex-
perimented with two broad domain test sets.
First, Bing Translator provided a sample of trans-
lation requests along with translations in German-
English and Spanish-English, which acted our stan-
dard development and test set. Unfortunately no
such tagged set was available in Bulgarian-English,
so we held out a portion of the large system?s train-
ing data to use for development and test. In each
language pair, the test set was split into a devel-
opment portion (?Dev A?) used for minimum error
rate training (Och, 2003) and a test set (?Test A?)
used for final evaluation.
Second, we created new test sets in each of
the three language pairs by sampling parallel sen-
tences from held out Wikipedia articles. To
ensure that this test data was clean, we man-
ually filtered the sentence pairs that were not
truly parallel and edited them as necessary to
improve adequacy. We called this ?Wikitest?.
This test set is available at http://research.microsoft.com/en-
us/people/chrisq/wikidownload.aspx. Characteristics of these
test sets are summarized in Table 5.
We evaluated the resulting systems using BLEU-
4 (Papineni et al, 2002); the results are pre-
sented in Table 6. First we note that the extracted
Wikipedia data are very helpful in medium data
conditions, significantly improving translation per-
formance in all conditions. Furthermore we found
that the extracted Wikipedia sentences substantially
improved translation quality on held-out Wikipedia
articles. In every case, training on medium data
plus Wikipedia extracts led to equal or better trans-
lation quality than the large system alone. Further-
more, adding the Wikipedia data to the large data
condition still made substantial improvements.
5 Conclusions
Our first substantial contribution is to demonstrate
that Wikipedia is a useful resource for mining par-
allel data. The sheer volume of extracted parallel
sentences within Wikipedia is a somewhat surpris-
ing result in the light of Wikipedia?s construction.
We are also releasing several valuable resources to
the community to facilitate further research: man-
ually aligned document pairs, and an edited test
set. Hopefully this will encourage research into
Wikipedia as a resource for machine translation.
Secondly, we improve on prior pairwise mod-
els by introducing a ranking approach for sentence
pair extraction. This ranking approach sidesteps the
problematic class imbalance issue, resulting in im-
proved average precision while retaining simplicity
and clarity in the models.
Also by modeling the sentence alignment of the
articles globally, we were able to show a substan-
tial improvement in task accuracy. Furthermore a
409
German English Spanish English Bulgarian English
sentences 924,416 924,416 957,884 957,884 413,514 413,514
Medium types 351,411 320,597 272,139 247,465 115,756 69,002
tokens 11,556,988 11,751,138 18,229,085 17,184,070 10,207,565 10,422,415
sentences 6,693,568 6,693,568 7,727,256 7,727,256 1,459,900 1,459,900
Large types 1,050,832 875,041 1,024,793 952,161 239,076 137,227
tokens 100,456,622 96,035,475 155,626,085 137,559,844 29,741,936 29,889,020
sentences 1,694,595 1,694,595 1,914,978 1,914,978 146,465 146,465
Wiki types 578,371 525,617 569,518 498,765 107,690 74,389
tokens 21,991,377 23,290,765 29,859,332 28,270,223 1,455,458 1,516,231
Table 4: Statistics of the training data size in all three language pairs.
German English Spanish English Bulgarian English
Dev A sentences 2,000 2,000 2,000 2,000 2,000 2,000
tokens 16,367 16,903 24,571 21,493 39,796 40,503
Test A sentences 5,000 5,000 5,000 5,000 2,473 2,473
tokens 42,766 43,929 68,036 60,380 52,370 52,343
Wikitest sentences 500 500 500 500 516 516
tokens 8,235 9,176 10,446 9,701 7,300 7,701
Table 5: Statistics of the test data sets.
Language pair Training data Dev A Test A Wikitest
Spanish-English Medium 32.6 30.5 33.0
Medium+Wiki 36.7 (+4.1) 33.8 (+3.3) 39.1 (+6.1)
Large 39.2 37.4 38.9
Large+Wiki 39.5 (+0.3) 37.3 (-0.1) 41.1 (+2.2)
German-English Medium 28.7 26.6 13.0
Medium+Wiki 31.5 (+2.8) 29.6 (+3.0) 18.2 (+5.2)
Large 35.0 33.7 17.1
Large+Wiki 34.8 (-0.2) 33.9 (+0.2) 20.2 (+3.1)
Bulgarian-English Medium 36.9 26.0 27.8
Medium+Wiki 37.9 (+1.0) 27.6 (+1.6) 37.9 (+10.1)
Large 51.7 49.6 36.0
Large+Wiki 51.7(+0.0) 49.4 (-0.2) 39.5(+3.5)
Table 6: BLEU scores under various training and test conditions. The first column is from minimum error rate training;
the next two columns are on held-out test sets. For training data conditions including extracted Wikipedia sentences,
parenthesized values indicate absolute BLEU difference against the corresponding system without Wikipedia extracts.
small sample of annotated articles is sufficient to
train these global level features, and the learned
classifiers appear very portable across languages. It
is difficult to say whether such improvement will
carry over to other comparable corpora with less
document structure and meta-data. We plan to ad-
dress this question in future work.
Finally, initial investigations have shown that
substantial gains can be achieved by using an in-
duced word-level lexicon in combination with sen-
tence extraction. This helps address modeling word
pairs that are out-of-vocabulary with respect to the
seed parallel lexicon, while avoiding some of the
issues in bootstrapping.
410
References
S. F Adafre and M. de Rijke. 2006. Finding similar
sentences across multiple languages in wikipedia. In
Proceedings of EACL, pages 62?69.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of ACL.
P. F Brown, V. J Della Pietra, S. A Della Pietra, and
R. L Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational linguistics, 19(2):263?311.
P. Fung and P. Cheung. 2004. Multi-level bootstrap-
ping for extracting parallel sentences from a quasi-
comparable corpus. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
page 1051.
W. A Gale and K. W Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings
of the workshop on Speech and Natural Language,
pages 152?157.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Roy Bar-Haim Ido, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
the ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL, pages 127?133, Edmonton,
Canada, May.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit, volume 5.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Machine
Learning, pages 282?289.
R. C Moore. 2002. Fast and accurate sentence align-
ment of bilingual corpora. Lecture Notes in Computer
Science, 2499:135?144.
D. S Munteanu and D. Marcu. 2005. Improv-
ing machine translation performance by exploiting
non-parallel corpora. Computational Linguistics,
31(4):477?504.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelpha, Pennsylva-
nia, USA.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of ACL.
P. Resnik and N. A Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
C. Tillmann and J. Xu. 2009. A simple sentence-level
extraction algorithm for comparable data. In Pro-
ceedings of HLT/NAACL, pages 93?96.
C. Tillmann. 2009. A Beam-Search extraction algo-
rithm for comparable data. In Proceedings of ACL,
pages 225?228.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841.
Wikipedia. 2004. Wikipedia, the free encyclopedia.
[Online; accessed 20-November-2009].
B. Zhao and S. Vogel. 2002. Adaptive parallel sentences
mining from web bilingual news collection. In Pro-
ceedings of the 2002 IEEE International Conference
on Data Mining, page 745. IEEE Computer Society.
411
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of NAACL-HLT 2013, pages 12?21,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Beyond Left-to-Right: Multiple Decomposition Structures for SMT
Hui Zhang?
USC/ISI
Los Angeles, CA 90089
hzhang@isi.edu
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Chris Quirk
Microsoft Research
Redmond, WA 98502
chrisq@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA 98502
jfgao@microsoft.com
Abstract
Standard phrase-based translation models do
not explicitly model context dependence be-
tween translation units. As a result, they rely
on large phrase pairs and target language mod-
els to recover contextual effects in translation.
In this work, we explore n-gram models over
Minimal Translation Units (MTUs) to explic-
itly capture contextual dependencies across
phrase boundaries in the channel model. As
there is no single best direction in which con-
textual information should flow, we explore
multiple decomposition structures as well as
dynamic bidirectional decomposition. The
resulting models are evaluated in an intrin-
sic task of lexical selection for MT as well
as a full MT system, through n-best rerank-
ing. These experiments demonstrate that ad-
ditional contextual modeling does indeed ben-
efit a phrase-based system and that the direc-
tion of conditioning is important. Integrating
multiple conditioning orders provides consis-
tent benefit, and the most important directions
differ by language pair.
1 Introduction
The translation procedure of a classical phrase-
based translation model (Koehn et al, 2003) first di-
vides the input sentence into a sequence of phrases,
translates each phrase, explores reorderings of these
translations, and then scores the resulting candi-
dates with a linear combination of models. Conven-
tional models include phrase-based channel models
that effectively model each phrase as a large uni-
gram, reordering models, and target language mod-
els. Of these models, only the target language model
?This research was conducted during the author?s internship
at Microsoft Research
(and, to some weak extent, the lexicalized reordering
model) captures some lexical dependencies that span
phrase boundaries, though it is not able to model in-
formation from the source side. Larger phrases cap-
ture more contextual dependencies within a phrase,
but individual phrases are still translated almost in-
dependently.
To address this limitation, several researchers
have proposed bilingual n-gram Markov models
(Marino et al, 2006) to capture contextual depen-
dencies between phrase pairs. Much of their work
is limited by the requirement ?that the source and
target side of a tuple of words are synchronized, i.e.
that they occur in the same order in their respective
languages? (Crego and Yvon, 2010).
For language pairs with significant typological di-
vergences, such as Chinese-English, it is quite dif-
ficult to extract a synchronized sequence of units;
in the limit, the smallest synchronized unit may be
the whole sentence. Other approaches explore incor-
poration into syntax-based MT systems or replacing
the phrasal translation system altogether.
We investigate the addition of MTUs to a phrasal
translation system to improve modeling of con-
text and to provide more robust estimation of long
phrases. However, in a phrase-based system there
is no single synchronized traversal order; instead,
we may consider the translation units in many pos-
sible orders: left-to-right or right-to-left according
to either the source or the target are natural choices.
Alternatively we consider translating a particularly
unambiguous unit in the middle of the sentence
and building outwards from there. We investigate
both consistent and dynamic decomposition orders
in several language pairs, looking at distinct orders
in isolation and combination.
12
2 Related work
Marino et al (2006) proposed a translation model
using a Markov model of bilingual n-grams, demon-
strating state-of-the-art performance compared to
conventional phrase-based models. Crego and
Yvon (2010) further explored factorized n-gram ap-
proaches, though both models considered rather
large n-grams; this paper focuses on small units with
asynchronous orders in source and target. Durrani
et al (2011) developed a joint model that captures
translation of contiguous and gapped units as well as
reordering. Two prior approaches explored similar
models in syntax based systems. MTUs have been
used in dependency translation models (Quirk and
Menezes, 2006) to augment syntax directed trans-
lation systems. Likewise in target language syntax
systems, one can consider Markov models over min-
imal rules, where the translation probability of each
rule is adjusted to include context information from
parent rules (Vaswani et al, 2011).
Most prior work tends to replace the existing
probabilities rather than augmenting them. We be-
lieve that Markov rules provide an additional sig-
nal but are not a replacement. Their distributions
should be more informative than the so-called ?lex-
ical weighting? models, and less sparse than rela-
tive frequency estimates, though potentially not as
effective for truly non-compositional units. There-
fore, we explore the inclusion of all such informa-
tion. Also, unlike prior work, we explore combina-
tions of multiple decomposition orders, as well as
dynamic decompositions. The most useful context
for translation differs by language pair, an important
finding when working with many language pairs.
We build upon a standard phrase-based approach
(Koehn et al, 2003). This acts as a proposal dis-
tribution for translations; the MTU Markov models
provide additional signal as to which translations are
correct.
3 MTU n-gram Markov models
We begin by defining Minimal Translation Units
(MTUs) and describing how to identify them in
word-aligned text. Next we define n-gram Markov
models over MTUs, which requires us to define
traversal orders over MTUs.
?Yu ??ZuoTian ??JuXing
held the meeting
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 1: Word alignment and minimum translation units.
3.1 Definition of an MTU
Informally, the notion of a minimal translation unit
is simple: it is a translation rule that cannot be
broken down any further without violating the con-
straints of the rules. We restrict ourselves to contigu-
ous MTUs. They are similar to small phrase pairs,
though unlike phrase pairs we allow MTUs to have
either an empty source or empty target side, thereby
allowing insertion and deletion phrases. Conven-
tional phrase pairs may be viewed as compositions
of these MTUs up to a given size limit.
Consider a word-aligned sentence pair consisting
of a sequence of source words s = s1 . . . sm, a se-
quence of target words t = t1 . . . tn, and a word align-
ment relation between the source and target words
? ? {1..m} ? {1..n}. A translation unit is a sequence
of source words si..s j and a sequence of target words
tk..tl (one of which may be empty) such that for all
aligned pairs i? ? k?, we have i ? i? ? j if and only
if k ? k? ? l. This definition, nearly identical to
that of a phrase pair (Koehn et al, 2003), relaxes the
constraint that one aligned word must be present.
A set of translation units is a partition of the sen-
tence pair if each source and target word is covered
exactly once. Minimal translation units is the par-
tition with the smallest average unit size, or, equiv-
alently, the largest number of units. For example,
Figure 1 shows a word-aligned sentence pair and its
corresponding set of MTUs. We extract these min-
imal translation units with an algorithm similar to
that of phrase extraction.
We train n-gram Markov models only over min-
13
imal rules for two reasons. First, the segmentation
of the sentence pair is not unique under composed
rules, which makes probability estimation compli-
cated. Second, some phrase pairs are very large,
which results in sparse data issues and compromises
the model quality. Therefore, training an n-gram
model over minimal translation units turns out to
be a simple and clean choice: the resulting segmen-
tation is unique, and the distribution is smooth. If
we want to capture more context, we can simply in-
crease the order of the Markov model.
Such Markov models address issues in large
phrase-based translation approaches. Where stan-
dard phrase-based models rely upon large unigrams
to capture contextual information, n-grams of mini-
mal translation units allow a robust contextual model
that is less constrained by segmentation.
3.2 MTU enumeration orders
When defining a joint probability distribution over
MTUs of an aligned sentence pair, it is necessary
to define a decomposition, or generation order for
the sentence pair. For a single sequence in lan-
guage modeling or synchronized sequences in chan-
nel modeling, the default enumeration order has
been left-to-right.
Different decomposition orders have been used
in part-of-speech tagging and named entity recog-
nition (Tsuruoka and Tsujii, 2005). Intuitively, in-
formation from the left or right could be more use-
ful for particular disambiguation choices. Our re-
search on different decomposition orders was moti-
vated by this work. When applying such ideas to
machine translation, there are additional challenges
and opportunities. The task exhibits much more am-
biguity ? the number of possible MTUs is in the
millions. An opportunity arises from the reordering
phenomenon in machine translation: while in POS
tagging the natural decomposition orders to study
are only left-to-right and right-to-left, in machine
translation we can further distinguish source and tar-
get sentence orders.
We first define the source left-to-right and the tar-
get left-to-right orders of the aligned sets of MTUs.
The definition is straightforward when there are no
inserted or deleted word. To place the nulls corre-
sponding to such word we use the following defi-
nition: the source position of the null for a target
inserted word is just after the position of the last
source word aligned to the closest preceding non-
null aligned target word. The target position for a
null corresponding to a source deleted MTU is de-
fined analogously. In Figure 1 we define the posi-
tion of M4 to be right after M3 (because ?the? is
after ?held? in left-to-right order on the target side).
The complete MTU sequence in source left-to-
right order is M1-M2-M3-M4-M5. The sequence
in target left-to-right order is M3-M4-M5-M1-M2.
This illustrates that decomposition structure may
differ significantly depending on which language is
used to define the enumeration order.
Once a sentence pair is represented as a sequence
of MTUs, we can define the probability of the
sentence pair using a conventional n-gram Markov
model (MM) over MTUs. For example, the 3-gram
MM probability of the sentence pair in Figure 1
under the source left-to-right order is as follows:
P(M1)?P(M2|M1)?P(M3|M1,M2)?P(M4|M2,M3)?
P(M5|M3,M4).
Different decomposition orders use different con-
text for disambiguation and it is not clear apriori
which would perform best. We compare all four
decomposition orders (source order left-to-right and
right-to-left, and target order left-to-right and right-
to-left). Although the independence assumptions of
left-to-right and right-to-left are the same, the result-
ing models may be different due to smoothing.
In addition to studying these four basic decompo-
sition orders, we report performance of two cyclic
orders: cyclic in source or target sentence order.
These models are inspired by the cyclic depen-
dency network model proposed for POS tagging
(Toutanova et al, 2003) and also used as a baseline
in previous work on dynamic decomposition orders
(Tsuruoka and Tsujii, 2005). 1
The probability according to the cyclic orders is
defined by conditioning each MTU on both its left
and right neighbor MTUs. For example, the prob-
ability of the sentence pair in Figure 1 under the
source cyclic order, using a 3-gram model is defined
as: P(M1|M2) ? P(M2|M1,M3) ? P(M3|M2,M4) ?
P(M4|M3,M5) ? P(M5|M4).
All n-gram Markov models over MTUs are esti-
1The correct application of such models requires sampling
to find the highest scoring sequence, but we apply the max prod-
uct approximation as done in previous work.
14
mated using Kneser-Ney smoothing. Each MTU is
treated as an atomic unit in the vocabulary of the
n-gram model. Counts of all n-grams are obtained
from the parallel MT training data, using different
MTU enumeration orders.
Note that if we use a target-order decomposition,
the model provides a distribution over target sen-
tences and the corresponding source sides of MTUs,
albeit unordered. Likewise source order based mod-
els provide distributions over source sentences and
unordered target sides of MTUs. We attempted to
introduce reordering models to predict an order over
the resulting MTU sequences using approaches sim-
ilar to reordering models for phrases. Although
these models produced gains in some language pairs
when used without translation MTU MMs, there
were no additional gains over a model using mul-
tiple translation MTU MMs.
4 Lexical selection
We perform an empirical evaluation of different
MTU decomposition orders on a simplified machine
translation task: lexical selection. In this task we
assume that the source sentence segmentation into
minimal translation units is given and that the or-
der of the corresponding target sides of the minimal
translation units is also given. The problem is to
predict the target sides of the MTUs, called target
MTUs for brevity (see Figure 2). The lexical selec-
tion task is thus similar to sequence tagging tasks
like part-of-speech tagging, though much more dif-
ficult: the predicted variables are sequences of target
language words with millions of possible outcomes.
?Yu ??ZuoTian ??JuXing
held the meet ng
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 2: Lexical selection.
We use this constrained MT setting to evaluate the
performance of models using different MTU decom-
position orders and models using combinations of
decomposition orders. The simplified setting allows
controlled experimentation while lessening the im-
pact of complicating factors in a full machine trans-
lation setting (search error, reordering limits, phrase
table pruning, interaction with other models).
To perform the tagging task, we use trigram MTU
models. The four basic decomposition orders for
MTU Markov models we use are left-to-right in tar-
get sentence order, right-to-left in target sentence or-
der, left-to-right in source sentence order, and right-
to-left in source sentence order. We also consider
cyclic orders in source and target.
Regardless of the decomposition order used, we
perform decoding using a beam search decoder, sim-
ilar to ones used in phrase-based machine transla-
tion. The decoder builds target hypotheses in left-
to-right target sentence order. At each step, it fills in
the translation of the next source MTU, in the con-
text of the already predicted MTUs to its left. The
top scoring complete hypotheses covering the first m
MTUs are maintained in a beam. When scoring with
a target left-to-right MTU Markov model (L2RT),
we can score each partial hypothesis exactly at each
step. When scoring using a R2LT model or a source
order model, we use lower-order approximations to
the trigram MTU Markov model scores as future
scores, since not all needed context is available for a
hypothesis at the time of construction. As additional
context becomes available, the exact score can be
computed. 2
4.1 Basic decomposition order combinations
We first introduce two methods of combining differ-
ent decomposition orders: product and system com-
bination.
The product method arises naturally in the ma-
chine translation setting, where probabilities from
different models are multiplied together and further
weighted to form the log-linear model for machine
translation (Och and Ney, 2002). We define a similar
scoring function using a set of MTU Markov models
MM1, ...,MMk for a hypothesis h as follows:
Score(h) = ?1logPMM1(h) + ... + ?klogPMMk (h)
2We apply hypothesis recombination, which can merge hy-
potheses that are indistinguishable with respect to future contin-
uations. This is similar to recombination in a standard-phrase
based decoder with the difference that it is not always the last
two target MTUs that define the context needed by future ex-
tensions.
15
The weights ? of different models are trained on a
development set using MER training to maximize
the BLEU score of the resulting model. Note that
this method of model combination was not consid-
ered in any of the previous works comparing differ-
ent decompositions.
The system combination method is motivated
by prior work in machine translation which com-
bined left-to-right and right-to-left machine trans-
lation systems (Finch and Sumita, 2009). Simi-
larly, we perform sentence-level system combina-
tion between systems using different MTU Markov
models to come up with most likely translations.
If we have k systems guessing hypotheses based
on MM1, . . . ,MMk respectively, we generate 1000-
best lists from each system, resulting in a pool of
up to 1000k possible distinct translations. Each of
the candidate hypotheses from MMi is scored with
its Markov model log-probability logPMMi(h). We
compute normalized probabilities for each system?s
n-best by exponentiating and normalizing: Pi(h) ?
PMMi(h). If a hypothesis h is not in system i?s n-
best list, we assume its probability is zero according
to that system. The final scoring function for each
hypothesis in the combined list of candidates is:
Score(h) = ?1P1(h) + ... + ?kPk(h)
The weights ? for the combination are tuned using
MERT as for the product model.
4.2 Dynamic decomposition orders
A more complex combination method chooses the
best possible decomposition order for each transla-
tion dynamically, using a set of constraints to de-
fine the possible decomposition orders, and a set of
features to score the candidate decompositions. We
term this method dynamic combination. The score
of each translation is defined as its score according
to the highest-scoring decomposition order for that
translation.
This method is very similar to the bidirectional
tagging approach of Tsuruoka and Tsujii (2005).
For this approach we only explored combinations of
target language orders (L2RT, CycT, and R2LT). If
source language orders were included, the complex-
ity of decoding would increase substantially.
Figure 3 shows two possible decompositions for
a short MTU sequence. The structures displayed are
1
 1
2
1  2| 1
3
2  3| 2,	 1
4
2  4| 3,	 2
 1  2| 1,	 3 1  3| 4  4
Figure 3: Different decompositions.
directed graphical models. They define the set of
parents (context) used to predict each target MTU.
The decomposition structures we consider are lim-
ited to acyclic graphs where each node can have one
of the following parent configurations: no parents
(C = 0 in the Figure), one left parent (C = 1L),
one right parent (C = 1R), one left and one right
parent (C = LR), two left parents (C = 2L), and
two right parents (C = 2R). If all nodes have two
left parents, we recover the left-to-right decomposi-
tion order, and if all nodes have two right parents,
the right-to-left decomposition order. A mixture of
parent configurations defines a mixed, dynamic de-
composition order. The decomposition order chosen
varies from translation to translation.
A directed graphical model defines the probability
of an assignment of MTUs to the variable nodes as a
product of local probabilities of MTUs given their
parents. Here we extend this definition to scores
of assignments by using a linear model with con-
figuration features and log-probability features. The
configuration features are indicators of which par-
ent configuration is active at a node and the settings
of these features for the decompositions in Figure
3 are shown as assignments to the C variables. The
log-probability feature values are obtained by query-
ing the appropriate n-gram model: L2RT, CycT, or
R2LT. For a node with one or two left parents, the
log-probability is computed according to the L2RT
model. For a node with one or two right parents, the
R2LT model is queried. The CycT model is used for
nodes with one left and one right parent.
To find the best translation of a sentence the
model now searches over hidden decomposition or-
16
ders in addition to assignments to target MTUs. The
final score of a translation and decomposition is a
linear combination of the two types of feature values
? model log-probabilities and configuration types.
There is one feature weight for each parent con-
figuration (six configuration weights) and one fea-
ture weight for each component model (three model
weights). The final score of the second decomposi-
tion and assignment in Figure 3 is:
Score(h)
= 2 ? wC0 + wCLR + wC1R
+ wL2RlogPLR(m1) + wCyclogPCyc(m2|m1,m3)
+ wR2LlogPRL(m3|m4) + wL2RlogPLR(m4)
There are two main differences between our ap-
proach and that of Tsuruoka and Tsujii (2005): we
perform beam search with hypothesis recombination
instead of exact decoding (due to the larger size of
the hypothesis set), and we use parameters to be
able to globally weight the probabilities from dif-
ferent models and to develop preferences for using
certain types of decompositions. For example, the
model can learn to prefer right-to-left decomposi-
tions for one language pair, and left-to-right decom-
positions for another. An additional difference from
prior work is the definition of the possible decompo-
sition orders that are searched over.
Compared to the structures allowed in (Tsuruoka
and Tsujii, 2005) for a trigram baseline model, our
allowed structures are a subset; in (Tsuruoka and
Tsujii, 2005) there are sixteen possible parent con-
figurations (up to two left and two right parents),
whereas we allow only six. We train and use only
three n-gram Markov models to assign probabilities:
L2RT, R2LT, and CycT, whereas the prior work used
sixteen models. One could potentially see additional
gains from considering a larger space of structures
but the training time and runtime memory require-
ments might become prohibitive for the machine
translation task.
Because of the maximization over decomposition
structures, the score of a translation is not a simple
linear function of the features, but rather a maximum
over linear functions. The score of a translation for
a fixed decomposition is a linear function of the fea-
tures, but the score of a translation is a maximum of
linear functions (over decompositions). Therefore,
if we define hypotheses as just containing transla-
tions, MERT training does not work directly for op-
timizing the weights of the dynamic combination
method. 3 We used a combination of approaches;
we did MERT training followed by local simplex-
method search starting from three starting points:
the MERT solution, a weight vector that strongly
prefers left-to-right decompositions, and a weight-
vector that strongly prefers right-to-left decomposi-
tions. In the Experiments section, we report results
for the weights that achieved the best development
set performance.
5 N-best reranking
To evaluate the impact of these models in a full MT
system, we investigate n-best reranking. We use a
phrase-based MT system to output 1000-best can-
didate translations. For each candidate translation,
we have access to the phrase pairs it used as well as
the alignments inside each phrase pair. Thus, each
source sentence and its candidate translation form a
word-aligned parallel sentence pair. We can extract
MTU sequences from this sentence pair and com-
pute its probability according to MTU Markov mod-
els. These MTU MM log-probabilities are appended
to the original MT features and used to rerank the
1000-best list. The weight vectors for systems using
the original features along with one or more MTU
Markov model log-probabilities are trained on the
development set using MERT.
6 Experiments
We report experimental results on the lexical selec-
tion task and the reranking task on three language
pairs. The datasets used for the different languages
are described in detail in Section 6.2.
6.1 Lexical selection experiments
The data used for the lexical selection experiments
consists of the training portion of the datasets used
for MT. These training sets are split into three sec-
tions: lex-train, for training MTU Markov models
and extracting possible translations for each source
3If we include the decompositions in the hypotheses we
could use MERT but then the n-best lists used for training might
not contain much variety in terms of translation options. This is
an interesting direction for future research.
17
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline 06.45 06.30 11.60 10.98 15.09 14.40
Oracle 69.79 70.78 72.28 75.39 85.15 84.32
L2RT 24.02 25.09 28.69 28.70 49.86 46.45
R2LT 23.79 24.91 30.14 30.14* 49.22 46.58
CycT 18.59 20.33 25.91 26.83 41.30 38.85
L2RS 25.81 27.89* 25.52 25.10 45.69 43.98
R2LS 26.48 27.96* 26.03 26.30 47.36 43.91
CycS 21.62 23.38 22.68 23.58 39.11 36.44
Table 1: Lexical selection results for individual MTU
Markov models.
MTU, lex-dev for tuning combination weights for
systems using several MTU MMs, and lex-test, for
final evaluation results. The possible translations for
each source MTU are defined as the most frequent
100 translations seen in lex-train. The lex-dev sets
contain 200 sentence pairs each and the lex-test sets
contains 1000 sentence pairs each. These develop-
ment and test sets consist of equally spaced sen-
tences taken from the full MT training sets.
We start by reporting BLEU scores of the six in-
dividual MTU MMs on the three language pairs in
Table 1. The baseline predicts the most frequent tar-
get MTU for each source MTU (unigram MM not
using context). The oracle looks at the correct trans-
lation and always chooses the correct target MTU if
it is in the vocabulary of available MTUs.
We can see that there is a large difference between
the baseline and oracle performance, underscoring
the importance of modeling context for accurate pre-
diction. The best decomposition order varies from
language to language: right-to-left in source order is
best for Chinese-English, right-to-left in target order
is best for German-English and left-to-right or right-
to-left in target order are best in English-Bulgarian.
We computed statistical significance tests, testing
the difference between the L2RT model (the stan-
dard in prior work) and models achieving higher test
set performance. The models that are significantly
better at significance ? < 0.01 are marked with a
star in the table. We used a paired bootstrap test with
10,000 trials (Koehn, 2004).
Next we evaluate the methods for combining de-
composition orders introduced in Sections 4.1 and
4.2. The results are reported in Table 2. The up-
per part of the table focuses on combining different
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline-1 24.04 25.09 30.14 30.14 49.86 46.45
TgtProduct 25.27 25.84* 30.47 30.49 51.04 47.27*
TgtSysComb 24.49 25.27 30.20 30.15 50.46 46.31
TgtDynamic 24.07 25.10 30.60 30.41 49.99 46.52
Baseline-2 26.48 27.96 30.14 30.14 49.86 46.45
AllProduct 28.68 29.59* 31.54 31.36* 51.50 48.10*
AllSyscomb 27.02 28.30 30.20 30.17 50.90 46.53
Table 2: Lexical selection results for combinations of
MTU Markov models.
target-order decompositions. The lower part of the
table looks at combining all six decomposition or-
ders. The baseline for the target order combinations,
Baseline-1, is the best single target MTU Markov
model from Table 1. Baseline-2 in the lower part
of the table is the best individual model out of all
six. We can see that the product models TgtProduct
(a product of the three target-order MTU MMs) and
AllProduct (a product of all six MTU MMs) are con-
sistently best. The dynamic decomposition models
TgtDynamic achieve slight but not significant gains
over the baseline. The combination models that are
statistically significantly better than corresponding
baselines (? < 0.01) are marked with a star.
Our takeaway from these experiments is that mul-
tiple decomposition orders are good, and thus taking
a product (which encourages agreement among the
models) is a good choice for this task. The dynamic
decomposition method shows some promise, but it
does not outperform the simpler product approach.
Perhaps a lager space of decompositions would
achieve better results, especially given a larger pa-
rameter set to trade off decompositions and better
tuning for those parameters.
6.2 Datasets and reranking settings
For Chinese-English, the training corpus consists
of 1 million sentence pairs from the FBIS and
HongKong portions of the LDC data for the NIST
MT evaluation. We used the union of the NIST
2002 and 2003 test sets as the development set and
the NIST 2005 test set as our test set. The baseline
phrasal system uses a 5-gram language model with
modified Kneser-Ney smoothing (Kenser and Ney,
1995), trained on the Xinhua portion of the English
Gigaword corpus (238M English words).
For German-English we used the dataset from
18
Language Training Dev Test
Chs-En 1 Mln NIST02+03 NIST05
Deu-En 751 K WMT06dev WMT06test
En-Bgr 4 Mln 1,497 2,498
Table 3: Data sets for different language pairs.
the WMT 2006 shared task on machine translation
(Koehn and Monz, 2006). The parallel training set
contains approximately 751K sentences. We also
used the English monolingual data of around 1 mil-
lion sentences for language model training. The de-
velopment set contains 2000 sentences. The final
test set (the in-domain test set for the shared task)
also contains 2000 sentences. Two Kneser-Ney lan-
guage models were used as separate features: a 4-
gram LM trained on the parallel portion of the data,
and a 5-gram LM trained on the monolingual corpus.
For English-Bulgarian we used a dataset con-
taining sentences from several data sources: JRC-
Acquis (Steinberger et al, 2006), TAUS4, and web-
scraped data. The development set consists of 1,497
sentences, the English side from WMT 2009 news
test data, and the Bulgarian side a human translation
thereof. The test set comes from the same mixture of
sources as the training set. For this system we used
a single four-gram target language model trained on
the target side of the parallel corpus.
All systems used phrase tables with a maximum
length of seven words on either side and lexicalized
reordering models. For the Chinese-English sys-
tem we used GIZA++ alignments, and for the other
two we used alignments by an HMM model aug-
mented with word-based distortion (He, 2007). The
alignments were symmetrized and then combined
with the heuristics ?grow-diag-final-and?. 5 We tune
parameters using MERT (Och, 2003) with random
restarts (Moore and Quirk, 2008) on the develop-
ment set. Case-insensitive BLEU-4 is our evaluation
metric (Papineni et al, 2002).
3-gram models 5-gram models
Model Dev Test Dev Test
Baseline 32.58 31.78 32.58 31.78
L2RT 33.05 32.78* 33.16 32.88*
R2LT 33.05 32.96* 33.16 32.81*
L2RS 32.90 33.00* 32.98 32.98*
R2LS 32.94 32.98* 33.09 32.96*
4 MMs 33.22 33.07* 33.37 33.00*
4 MMs phrs 32.58 31.78 32.58 31.78
Table 4: Reranking with 3-gram and 5-gram MTU trans-
lation models on Chinese-English. Starred results on the
test set indicate significantly better performance than the
baseline.
6.3 MT reranking experiments
We first report detailed experiments on Chinese-
English, and then verify our main conclusions on the
other language pairs. Table 4 looks at the impact of
individual 3-gram and 5-gram MTU Markov models
and their combination. Amongst the decomposition
orders tested (L2RT, R2LT, L2RS, and R2LS), each
of the individual MTU MMs was able to achieve
significant improvement over the baseline, around 1
BLEU point.6 The results achieved by the individ-
ual models differ, and the combination of four direc-
tions is better than the best individual direction, but
the difference is not statistically significant.
We ran an additional experiment to test whether
MTU MMs make effective use of context across
phrase boundaries, or whether they simply pro-
vide better smoothed estimates of phrasal transla-
tion probabilities. The last row of the table reports
the results achieved by a combination of MTU MMs
that do not use context across the phrasal bound-
aries. Since an MTU MM limited to look only inside
phrases can provide improved smoothing compared
to whole phrase relative frequency counts, it is con-
ceivable it could provide a large improvement. How-
ever, there is no improvement in practice for this lan-
guage pair; the additional improvements from MTU
MMs stem from modeling cross-phrase context.
4www.tausdata.org
5The combination heuristic was further refined to disallow
crossing one-to-many alignments, which would result in the ex-
traction of larger minimum translation units. We found that this
further refinement on the combination heuristic consistently im-
proved the BLEU scores by between 0.3 and 0.7.
6Here again we call a difference significant if the paired
bootstrap p-value is less than 0.01.
19
Table 5 shows the test set results of individ-
ual 3-gram MTU Markov models and the com-
bination of 3-gram and 5-gram models on the
English-Bulgarian and German-English datasets.
For English-Bulgarian all individual 3-gram Markov
models achieve significant improvements of close to
one point; their combination is better than the best
individual model (but not significantly). The indi-
vidual 5-gram models and their combination bring
much larger improvement, for a total increase of
2.82 points over the baseline. We believe the 5-
gram models were more effective in this setting be-
cause the larger training set alowed for successful
training of models of larger capacity. Also the in-
creased context size helps to resolve ambiguity in
the forms of morphologically-rich Bulgarian words.
For German-English we see a similar pattern, with
the combination of models outperforming the in-
dividual ones, and the 5-gram models being better
than the 3-gram. Here the individual 3-gram models
are better than the baseline at significance level 0.02
and their combination is better than the baseline at
our earlier defined threshold of 0.01. The within-
phrase MTU MMs (results shown in the last two
rows) improve upon the baseline slightly, but here
again the improvements mostly stem from the use of
context across phrase boundaries. Our final results
on German-English are better than the best result of
27.30 from the shared task (Koehn and Monz, 2006).
Thanks to the reviewers for referring us to re-
cent work by (Clark et al, 2011) that pointed out
problems with significance tests for machine trans-
lation, where the randomness and local optima in the
MERT weight tuning method lead to a large vari-
ance in development and test set performance across
different runs of optimization (using a different ran-
dom seed or starting point). (Clark et al, 2011) pro-
posed a stratified approximate randomization statis-
tical significance test, which controls for optimizer
instability. Using this test, for the English-Bulgarian
system, we confirmed that the combination of four
3-gram MMs and the combination of 5-gram MMs
is better than the baseline (p = .0001 for both, using
five runs of parameter tuning). We have not run the
test for the other language pairs.
Model En-Bgr Deu-En
Baseline 45.75 27.92
L2RT 3-gram 47.07* 28.15
R2LT 3-gram 47.06* 28.19
L2RS 3-gram 46.44* 28.15
R2LS 3-gram 47.04* 28.18
4 3-gram 47.17* 28.37*
4 5-gram 48.57* 28.47*
4 3-gram phrs 46.08 27.92
4 5-gram phrs 46.17* 27.93
Table 5: English-Bulgarian and German-English test set
results: reranking with MTU translation models.
7 Conclusions
We introduced models of Minimal Translation Units
for phrasal systems, and showed that they make a
substantial and statistically significant improvement
on three distinct language-pairs. Additionally we
studied the importance of decomposition order when
defining the probability of MTU sequences. In a
simplified lexical selection task, we saw that there
were large differences in performance among the
different decompositions, with the best decomposi-
tions differing by language. We investigated multi-
ple methods to combine decompositions and found
that a simple product approach was most effective.
Results in the lexical selection task were consistent
with those obtained in a full MT system, although
the differences among decompositions were smaller.
In future work, perhaps we would see larger gains
by including additional decomposition orders (e.g.,
top-down in a dependency tree), and taking this idea
deeper into the machine translation model, down to
the word-alignment and language-modeling levels.
We were surprised to find n-best reranking so ef-
fective. We are incorporating the models into first
pass decoding, in hopes of even greater gains.
References
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. ACL-11.
JM Crego and F Yvon. 2010. Factored bilingual n-
gram language models for statistical machine transla-
tion. Machine Translation, Special Issue: Pushing the
frontiers of SMT, 24(2):159?175.
20
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1045?
1054, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based machine translation. In In proceedings
of EMNLP.
Xiaodong He. 2007. Using word-dependent transition
models in hmm based word alignment for statistical
machine translation. In WMT workshop.
Reinhard Kenser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc.
ICASSP 1995, pages 181?184.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proceedings of
EMNLP.
JB Marino, RE Banchs, JM Crego, A de Gispert, P Lam-
bert, JA Fonollosa, and MR Costa-Jussa. 2006. N-
gram-based machine translation. Computational Lin-
guistics, 32(4):527?549.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error training for statistical ma-
chine translation. In Proc. Coling-08.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In In Proceedings of ACL,
pages 295?302.
Franz Joseph Och. 2003. Minimum error training in sta-
tistical machine translation. In Proc. ACL-03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the ACL, pages 311?318.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 9?16, New York City, USA,
June. Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, Dan Tufis, and Dniel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In LREC,
Genoa, Italy.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of HLT-NAACL.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In In proceedings of
HLT/EMNLP.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
21
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 895?904,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Bilingual Morpheme Segmentation and Alignment with
Context-rich Hidden Semi-Markov Models
Jason Naradowsky?
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003
narad@cs.umass.edu
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Abstract
This paper describes an unsupervised dynamic
graphical model for morphological segmen-
tation and bilingual morpheme alignment for
statistical machine translation. The model ex-
tends Hidden Semi-Markov chain models by
using factored output nodes and special struc-
tures for its conditional probability distribu-
tions. It relies on morpho-syntactic and lex-
ical source-side information (part-of-speech,
morphological segmentation) while learning a
morpheme segmentation over the target lan-
guage. Our model outperforms a competi-
tive word alignment system in alignment qual-
ity. Used in a monolingual morphological seg-
mentation setting it substantially improves ac-
curacy over previous state-of-the-art models
on three Arabic and Hebrew datasets.
1 Introduction
An enduring problem in statistical machine trans-
lation is sparsity. The word alignment models of
modern MT systems attempt to capture p(ei|fj),
the probability that token ei is a translation of fj .
Underlying these models is the assumption that the
word-based tokenization of each sentence is, if not
optimal, at least appropriate for specifying a concep-
tual mapping between the two languages.
However, when translating between unrelated lan-
guages ? a common task ? disparate morphological
systems can place an asymmetric conceptual bur-
den on words, making the lexicon of one language
much more coarse. This intensifies the problem of
sparsity as the large number of word forms created
?This research was conducted during the author?s internship
at Microsoft Research
through morphologically productive processes hin-
ders attempts to find concise mappings between con-
cepts.
For instance, Bulgarian adjectives may contain
markings for gender, number, and definiteness. The
following tree illustrates nine realized forms of the
Bulgarian word for red, with each leaf listing the
definite and indefinite markings.
Feminine Neuter
Singular Plural
Root
Masculine
cherven(iq)(iqt) cherveni(te)chervena(ta) cherveno(to)
Table 1: Bulgarian forms of red
Contrast this with English, in which this informa-
tion is marked either on the modified word or by sep-
arate function words.
In comparison to a language which isn?t mor-
phologically productive on adjectives, the alignment
model must observe nine times as much data (as-
suming uniform distribution of the inflected forms)
to yield a comparable statistic. In an area of research
where the amount of data available plays a large role
in a system?s overall performance, this sparsity can
be extremely problematic. Further complications are
created when lexical sparsity is compounded with
the desire to build up alignments over increasingly
larger contiguous phrases.
To address this issue we propose an alternative
to word alignment: morpheme alignment, an align-
ment that operates over the smallest meaningful sub-
sequences of words. By striving to keep a direct 1-
to-1 mapping between corresponding semantic units
across languages, we hope to find better estimates
895
??
the red flower
cherven tsvet
DET ADJ NN
iai
s
they want to
h nA^
PRN VB INF
dyrsr~d y
teach him
VB PRN
nw y
?????? ???? ?? ??
? ?? ?????
?
? ? ?
te
Figure 1: A depiction of morpheme-level alignment. Here dark lines indicate the more stem-focused alignment
strategy of a traditional word or phrasal alignment model, while thin lines indicate a more fine-grained alignment
across morphemes. In the alignment between English and Bulgarian (a) the morpheme-specific alignment reduces
sparsity in the adjective and noun (red flowers) by isolating the stems from their inflected forms. Despite Arabic
exhibiting templatic morphology, there are still phenomena which can be accounted for with a simpler segmentational
approach. The Arabic alignment (b) demonstrates how the plural marker on English they would normally create
sparsity by being marked in three additional places, two of them inflections in larger wordforms.
for the alignment statistics. Our results show that
this improves alignment quality.
In the following sections we describe an un-
supervised dynamic graphical model approach to
monolingual morphological segmentation and bilin-
gual morpheme alignment using a linguistically mo-
tivated statistical model. In a bilingual setting,
the model relies on morpho-syntactic and lexical
source-side information (part-of-speech, morpho-
logical segmentation, dependency analysis) while
learning a morpheme segmentation over the tar-
get language. In a monolingual setting we intro-
duce effective use of context by feature-rich mod-
eling of the probabilities of morphemes, morpheme-
transitions, and word boundaries. These additional
sources of information provide powerful bias for un-
supervised learning, without increasing the asymp-
totic running time of the inference algorithm.
Used as a monolingual model, our system sig-
nificantly improves the state-of-the-art segmenta-
tion performance on three Arabic and Hebrew data-
sets. Used as a bilingual model, our system out-
performs the state-of-the-art WDHMM (He, 2007)
word alignment model as measured by alignment er-
ror rate (AER).
In agreement with some previous work on to-
kenization/morpheme segmentation for alignment
(Chung and Gildea, 2009; Habash and Sadat, 2006),
we find that the best segmentation for alignment
does not coincide with the gold-standard segmenta-
tion and our bilingual model does not outperform
our monolingual model in segmentation F-Measure.
2 Model
Our model defines the probability of a target lan-
guage sequence of words (each consisting of a se-
quence of morphemes), and alignment from target
to source morphemes, given a source language se-
quence of words (each consisting of a sequence of
morphemes).
An example morpheme segmentation and align-
ment of phrases in English-Arabic and English-
Bulgarian is shown in Figure 1. In our task setting,
the words of the source and target language as well
as the morpheme segmentation of the source (En-
glish) language are given. The morpheme segmen-
tation of the target language and the alignments be-
tween source and target morphemes are hidden.
The source-side input, which we assume to be
English, is processed with a gold morphological
segmentation, part-of-speech, and dependency tree
analysis. While these tools are unavailable in
resource-poor languages, they are often available for
at least one of the modeled languages in common
translation tasks. This additional information then
provides a source of features and conditioning infor-
mation for the translation model.
Our model is derived from the hidden-markov
model for word alignment (Vogel et al, 1996; Och
and Ney, 2000). Based on it, we define a dynamic
896
cherven.i.te 
flowerthe red
= 'cherven'
?1
= 2
a1
= OFF
b1
= OFF
b2
= ON
b3
 = 'i'
?2
 = 'te'
?3
= 4 = 1
a2 a3
s
= stem
t1
= suffix = suffix
t2 t3
Figure 2: A graphical depiction of the model generating
the transliteration of the first Bulgarian word from Figure
1. Trigram dependencies and some incoming/outgoing
arcs have been omitted for clarity.
graphical model which lets us encode more lin-
guistic intuition about morpheme segmentation and
alignment: (i) we extend it to a hidden semi-markov
model to account for hidden target morpheme seg-
mentation; (ii) we introduce an additional observa-
tion layer to model observed word boundaries and
thus truly represent target sentences as words com-
posed of morphemes, instead of just a sequence
of tokens; (iii) we employ hierarchically smoothed
models and log-linear models to capture broader
context and to better represent the morpho-syntactic
mapping between source and target languages. (iv)
we enrich the hidden state space of the model to en-
code morpheme types {prefix,suffix,stem}, in ad-
dition to morpheme alignment and segmentation in-
formation.
Before defining our model formally, we introduce
some notation. Each possible morphological seg-
mentation and alignment for a given sentence pair
can be described by the following random variables:
Let ?1?2 . . . ?I denote I morphemes in the seg-
mentation of the target sentence. For the Example
in Figure 1 (a) I=5 and ?1=cherven, ?2=i . . . , and
?5=ia. Let b1, b2, . . . bI denote Bernoulli variables
indicating whether there is a word boundary after
morpheme ?i. For our example, b3 = 1, b5 = 1,
and the other bi are 0. Let c1, c2, . . . , cT denote
the non-space characters in the target string, and
wb1, . . . , wbT denote Bernoulli variables indicating
whether there is a word boundary after the corre-
sponding target character. For our example, T = 14
(for the Cyrillic version) and the only wb variables
that are on are wb9 and wb14. The c and wb vari-
ables are observed. Let s1s2 . . . sT denote Bernoulli
segmentation variables indicating whether there is a
morpheme boundary after the corresponding char-
acter. The values of the hidden segmentation vari-
ables s together with the values of the observed c
and wb variables uniquely define the values of the
morpheme variables ?i and the word boundary vari-
ables bi. Naturally we enforce the constraint that
a given word boundary wbt = 1 entails a segmen-
tation boundary st = 1. If we use bold letters
to indicate a vector of corresponding variables, we
have that c,wb, s=?,b. We will define the assumed
parametric form of the learned distribution using the
?,b but the inference algorithms are implemented
in terms of the s and wb variables.
We denote the observed source language mor-
phemes by e1 . . . eJ . Our model makes use of ad-
ditional information from the source which we will
mention when necessary.
The last part of the hidden model state repre-
sents the alignment between target and source mor-
phemes and the type of target morphemes. Let
tai = [ai, ti], i = 1 . . . I indicate a factored state
where ai represents one of the J source words (or
NULL) and ti represents one of the three morpheme
types {prefix,suffix,stem}. ai is the source mor-
pheme aligned to ?i and ti is the type of ?i.
We are finally ready to define the desired proba-
bility of target morphemes, morpheme types, align-
ments, and word boundaries given source:
P (?, ta,b|e) =
I?
i=1
PT (?i|tai, bi?1, bi?2, ?i?1, e)
? PB(bi|?i, ?i?1, tai, bi?1, bi?2, e)
? PD(tai|tai?1, bi?1, e) ? LP (|?i|)
We now describe each of the factors used by our
model in more detail. The formulation makes ex-
plicit the full extent of dependencies we have ex-
plored in this work. By simplifying the factors
897
we can recover several previously used models for
monolingual segmentation and bilingual joint seg-
mentation and alignment. We discuss the relation-
ship of this model to prior work and study the impact
of the novel components in our experiments.
When the source sentence is assumed to be empty
(and thus contains no morphemes to align to) our
model turns into a monolingual morpheme segmen-
tation model, which we show exceeds the perfor-
mance of previous state-of-the-art models. When we
remove the word boundary component, reduce the
order of the alignment transition, omit the morpho-
logical type component of the state space, and retain
only minimal dependencies in the morpheme trans-
lation model, we recover the joint tokenization and
alignment model based on IBM Model-1 proposed
by (Chung and Gildea, 2009).
2.1 Morpheme Translation Model
In the model equation, PT denotes the morpheme
translation probability. The standard dependence on
the aligned source morpheme is represented as a de-
pendence on the state tai and the whole annotated
source sentence e. We experimented with multiple
options for the amount of conditioning context to be
included. When most context is used, there is a bi-
gram dependency of target language morphemes as
well as dependence on two previous boundary vari-
ables and dependence on the aligned source mor-
pheme eai as well as its POS tag.
When multiple conditioning variables are used we
assume a special linearly interpolated backoff form
of the model, similar to models routinely used in lan-
guage modeling.
As an example, suppose we estimate the mor-
pheme translation probability as PT (?i|eai , ti). We
estimate this in the M-step, given expected joint
counts c(?i, eai , ti) and marginal counts derived
from these as follows:
PT (?i|eai , ti) =
c(?i,eai ,ti)+?2P2(?i|ti)
c(eai ,ti)+?2
The lower order distributions are estimated recur-
sively in a similar way:
P2(?i|ti) =
c(?i,ti)+?1P1(?i)
c(ti)+?1
P1(?i) =
c(?i)+?0P0(?i)
c(.)+?0
For P0 we used a unigram character language
model. This hierarchical smoothing can be seen
as an approximation to hierarchical Dirichlet priors
with maximum aposteriori estimation.
Note how our explicit treatment of word bound-
ary variables bi allows us to use a higher order de-
pendence on these variables. If word boundaries are
treated as morphemes on their own, we would need
to have a four-gram model on target morphemes to
represent this dependency which we are now repre-
senting using only a bigram model on hidden mor-
phemes.
2.2 Word Boundary Generation Model
The PB distribution denotes the probability of gen-
erating word boundaries. As a sequence model of
sentences the basic hidden semi-markov model com-
pletely ignores word boundaries. However, they can
be powerful predictors of morpheme segments (by
for example, indicating that common prefixes fol-
low word boundaries, or that common suffixes pre-
cede them). The log-linear model of (Poon et al,
2009) uses word boundaries as observed left and
right context features, and Morfessor (Creutz and
Lagus, 2007) includes boundaries as special bound-
ary symbols which can inform about the morpheme
state of a morpheme (but not its identity).
Our model includes a special generative process
for boundaries which is conditioned not only on the
previous morpheme state but also the previous two
morphemes and other boundaries. Due to the fact
that boundaries are observed their inclusion in the
model does not increase the complexity of inference.
The inclusion of this distribution lets us estimate
the likelihood of a word consisting of one,two,three,
or more morphemes. It also allows the estimation of
likelihood that particular morphemes are in the be-
ginning/middle/end of words. Through the included
factored state variable tai word boundaries can also
inform about the likelihood of a morpheme aligned
to a source word of a particular pos tag to end a
word. We discuss the particular conditioning con-
text for this distribution we found most helpful in
our experiments.
Similarly to the PT distribution, we make use of
multiple context vectors by hierarchical smoothing
of distributions of different granularities.
898
2.3 Distortion Model
PD indicates the distortion modeling distribution
we use. 1 Traditional distortion models represent
P (aj |aj?1, e), the probability of an alignment given
the previous alignment, to bias the model away from
placing large distances between the aligned tokens
of consecutively sequenced tokens. In addition to
modeling a larger state space to also predict mor-
pheme types, we extend this model by using a spe-
cial log-linear model form which allows the integra-
tion of rich morpho-syntactic context. Log-linear
models have been previously used in unsupervised
learning for local multinomial distributions like this
one in e.g. (Berg-Kirkpatrick et al, 2010), and for
global distributions in (Poon et al, 2009).
The special log-linear form allows the inclusion
of features targeted at learning the transitions among
morpheme types and the transitions between corre-
sponding source morphemes. The set of features
with example values for this model is depicted in
Table 3. The example is focussed on the features
firing for the transition from the Bulgarian suffix
te aligned to the first English morpheme ?i?1 =
te, ti?1=suffix, ai?1=1, to the Bulgarian root tsvet
aligned to the third English morpheme ?i = tsvet,
ti=root, ai=3. The first feature is the absolute dif-
ference between ai and ai?1 + 1 and is similar to
information used in other HMM word alignment
models (Och and Ney, 2000) as well as phrase-
translation models (Koehn, 2004). The alignment
positions ai are defined as indices of the aligned
source morphemes. We additionally compute distor-
tion in terms of distance in number of source words
that are skipped. This distance corresponds to the
feature name WORD DISTANCE. Looking at both
kinds of distance is useful to capture the intuition
that consecutive morphemes in the same target word
should prefer to have a higher proximity of their
aligned source words, as compared to consecutive
morphemes which are not part of the same target
word. The binned distances look at the sign of the
distortion and bin the jumps into 5 bins, pooling the
distances greater than 2 together. The feature SAME
TARGET WORD indicates whether the two consecu-
1To reduce complexity of exposition we have omitted the
final transition to a special state beyond the source sentence end
after the last target morpheme.
Feature Value
MORPH DISTANCE 1
WORD DISTANCE 1
BINNED MORPH DISTANCE fore1
BINNED WORD DISTANCE fore1
MORPH STATE TRANSITION suffix-root
SAME TARGET WORD False
POS TAG TRANSITION DET-NN
DEP RELATION DET?NN
NULL ALIGNMENT False
conjunctions ...
Figure 3: Features in log-linear distortion model firing
for the transition from te:suffix:1 to tsvet:root:3 in the
example sentence pair in Figure 1a.
tive morphemes are part of the same word. In this
case, they are not. This feature is not useful on its
own because it does not distinguish between differ-
ent alignment possibilities for tai, but is useful in
conjunction with other features to differentiate the
transition behaviors within and across target words.
The DEP RELATION feature indicates the direct de-
pendency relation between the source words con-
taining the aligned source morphemes, if such rela-
tionship exists. We also represent alignments to null
and have one null for each source word, similarly to
(Och and Ney, 2000) and have a feature to indicate
null. Additionally, we make use of several feature
conjunctions involving the null, same target word,
and distance features.
2.4 Length Penalty
Following (Chung and Gildea, 2009) and (Liang and
Klein, 2009) we use an exponential length penalty
on morpheme lengths to bias the model away from
the maximum likelihood under-segmentation solu-
tion. The form of the penalty is:
LP (|?i|) = 1
e|?i|
lp
Here lp is a hyper-parameter indicating the power
that the morpheme length is raised to. We fit this pa-
rameter using an annotated development set, to op-
timize morpheme-segmentation F1. The model is
extremely sensitive to this value and performs quite
poorly if such penalty is not used.
2.5 Inference
We perform inference by EM training on the aligned
sentence pairs. In the E-step we compute expected
899
counts of all hidden variable configurations that are
relevant for our model. In the M-step we re-estimate
the model parameters (using LBFGS in the M-step
for the distortion model and using count interpola-
tion for the translation and word-boundary models).
The computation of expectations in the E-step
is of the same order as an order two semi-markov
chain model using hidden state labels of cardinality
(J ? 3 = number of source morphemes times num-
ber of target morpheme types). The running time
of the forward and backward dynamic programming
passes is T ? l2 ? (3J)2, where T is the length of
the target sentence in characters, J is the number
of source morphemes, and l is the maximum mor-
pheme length. Space does not permit the complete
listing of the dynamic programming solution but it
is not hard to derive by starting from the dynamic
program for the IBM-1 like tokenization model of
(Chung and Gildea, 2009) and extending it to ac-
count for the higher order on morphemes and the
factored alignment state space.
Even though the inference algorithm is low poly-
nomial it is still much more expensive than the infer-
ence for an HMM model for word-alignment with-
out segmentation. To reduce the running time of the
model we limit the space of considered morpheme
boundaries as follows:
Given the target side of the corpus, we derive a
list of K most frequent prefixes and suffixes using a
simple trie-based method proposed by (Schone and
Jurafsky, 2000).2 After we determine a list of al-
lowed prefixes and suffixes we restrict our model to
allow only segmentations of the form : ((p*)r(s*))+
where p and s belong to the allowed prefixes and
suffixes and r can match any substring.
We determine the number of prefixes and suffixes
to consider using the maximum recall achievable by
limiting the segmentation points in this way. Re-
stricting the allowable segmentations in this way not
only improves the speed of inference but also leads
to improvements in segmentation accuracy.
2Words are inserted into a trie with each complete branch
naturally identifying a potential suffix, inclusive of its sub-
branches. The list comprises of the K most frequent of these
complete branches. Inserting the reversed words will then yield
potential prefixes.
3 Evaluation
For a majority of our testing we borrow the paral-
lel phrases corpus used in previous work (Snyder
and Barzilay, 2008), which we refer to as S&B.
The corpus consists of 6,139 short phrases drawn
from English, Hebrew, and Arabic translations of
the Bible. We use an unmodified version of this
corpus for the purpose of comparing morphological
segmentation accuracy. For evaluating morpheme
alignment accuracy, we have also augmented the En-
glish/Arabic subset of the corpus with a gold stan-
dard alignment between morphemes. Here mor-
phological segmentations were obtained using the
previously-annotated gold standard Arabic morpho-
logical segmentation, while the English was prepro-
cessed with a morphological analyzer and then fur-
ther hand annotated with corrections by two native
speakers. Morphological alignments were manually
annotated. Additionally, we evaluate monolingual
segmentation models on the full Arabic Treebank
(ATB), also used for unsupervised morpheme seg-
mentation in (Poon et al, 2009).
4 Results
4.1 Morpheme Segmentation
We begin by evaluating a series of models which are
simplifications of our complete model, to assess the
impact of individual modeling decisions. We focus
first on a monolingual setting, where the source sen-
tence aligned to each target sentence is empty.
Unigram Model with Length Penalty
The first model we study is the unigram mono-
lingual segmentation model using an exponential
length penalty as proposed by (Liang and Klein,
2009; Chung and Gildea, 2009), which has been
shown to be quite accurate. We refer to this model as
Model-UP (for unigram with penalty). It defines the
probability of a target morpheme sequence as fol-
lows: (?1 . . . ?I) = (1? ?)
?I
i=1 ?PT (?i)LP (|?i|)
This model can be (almost) recovered as a spe-
cial case of our full model, if we drop the transition
and word boundary probabilities, do not model mor-
pheme types, and use no conditioning for the mor-
pheme translation model. The only parameter not
present in our model is the probability ? of gener-
ating a morpheme as opposed to stopping to gener-
900
ate morphemes (with probability 1 ? ?). We exper-
imented with this additional parameter, but found it
had no significant impact on performance, and so we
do not report results including it.
We select the value of the length penalty power
by a gird search in the range 1.1 to 2.0, using .1 in-
crements and choosing the values resulting in best
performance on a development set containing 500
phrase pairs for each language. We also select the
optimal number of prefixes/suffixes to consider by
measuring performance on the development set. 3
Morpheme Type Models
The next model we consider is similar to the un-
igram model with penalty, but introduces the use
of the hidden ta states which indicate only mor-
pheme types in the monolingual setting. We use
the ta states and test different configurations to de-
rive the best set of features that can be used in the
distortion model utilizing these states, and the mor-
pheme translation model. We consider two vari-
ants: (1) Model-HMMP-basic (for HMM model
with length penalty), which includes the hidden
states but uses them with a simple uniform transition
matrix P (tai|tai?1, bi?1) (uniform over allowable
transitions but forbidding the prefixes from transi-
tioning directly to suffixes, and preventing suffixes
from immediately following a word boundary), and
(2) a richer model Model-HMMP which is allowed
to learn a log-linear distortion model and a feature
rich translation model as detailed in the model defi-
nition. This model is allowed to use word boundary
information for conditioning (because word bound-
aries are observed), but does not include the PB pre-
dictive word boundary distribution.
Full Model with Word Boundaries
Finally we consider our full monolingual model
which also includes the distribution predicting word
boundary variables bi. We term this model Model-
FullMono. We detail the best context features for
the conditional PD distribution for each language.
We initialize this model with the morpheme trans-
3For the S&B Arabic dataset, we selected to use seven pre-
fixes and seven suffixes, which correspond to maximum achiev-
able recall of 95.3. For the S&B Hebrew dataset, we used six
prefixes and six suffixes, for a maximum recall of 94.3. The
Arabic treebank data required a larger number of affixes: we
used seven prefixes and 20 suffixes, for a maximum recall of
98.3.
lation unigram distribution of ModelHMMP-basic,
trained for 5 iterations.
Table 4 details the test set results of the different
model configurations, as well as previously reported
results on these datasets. For our main results we use
the automatically derived list of prefixes and suffixes
to limit segmentation points. The names of models
that use such limited lists are prefixed by Dict in the
Table. For comparison, we also report the results
achieved by models that do not limit the segmenta-
tion points in this way.
As we can see the unigram model with penalty,
Dict-Model-UP, is already very strong, especially
on the S&B Arabic dataset. When the segmenta-
tion points are not limited, its performance is much
worse. The introduction of hidden morpheme states
in Dict-HMMP-basic gives substantial improvement
on Arabic and does not change results much on the
other datasets. A small improvement is observed
for the unconstrained models.4 When our model in-
cludes all components except word boundary pre-
diction, Dict-Model-HMMP, the results are substan-
tially improved on all languages. Model-HMMP is
also the first unconstrained model in our sequence
to approach or surpass previous state-of-the-art seg-
mentation performance.
Finally, when the full model Dict-MonoFull is
used, we achieve a substantial improvement over
the previous state-of-the-art results on all three cor-
pora, a 6.5 point improvement on Arabic, 6.2 point
improvement on Hebrew, and a 9.3 point improve-
ment on ATB. The best configuration of this model
uses the same distortion model for all languages: us-
ing the morph state transition and boundary features.
The translation models used only ti for Hebrew and
ATB and ti and ?i?1 for Arabic. Word bound-
ary was predicted using ti in Arabic and Hebrew,
and additionally using bi?1 and bi?2 for ATB. The
unconstrained models without affix dictionaries are
also very strong, outperforming previous state-of-
the-art models. For ATB, the unconstrained model
slightly outperforms the constrained one.
The segmentation errors made by this system shed
light on how it might be improved. We find the dis-
4Note that the inclusion of states in HMMP-basic only
serves to provide a different distribution over the number of
morphemes in a word, so it is interesting it can have a positive
impact.
901
Arabic Hebrew ATB
P R F1 P R F1 P R F1
UP 88.1 55.1 67.8 43.2 87.6 57.9 79.0 54.6 64.6
Dict-UP 85.8 73.1 78.9 57.0 79.4 66.3 61.6 91.0 73.5
HMMP-basic 83.3 58.0 68.4 43.5 87.8 58.2 79.0 54.9 64.8
Dict-HMMP-basic 84.8 76.3 80.3 56.9 78.8 66.1 69.3 76.2 72.6
HMMP 73.6 76.9 75.2 70.2 73.0 71.6 94.0 76.1 84.1
Dict-HMMP 82.4 81.3 81.8 62.7 77.6 69.4 85.2 85.8 85.5
MonoFull 80.5 87.3 83.8 72.2 71.7 72.0 86.2 88.5 87.4
Dict-MonoFull 86.1 83.2 84.6 73.7 72.5 73.1 92.9 81.8 87.0
Poon et. al 76.0 80.2 78.1 67.6 66.1 66.9 88.5 69.2 77.7
S&B-Best 67.8 77.3 72.2 64.9 62.9 63.9 ? ? ?
Morfessor 71.1 60.5 65.4 65.4 57.7 61.3 77.4 72.6 74.9
Figure 4: Results on morphological segmentation achieved by monolingual variants of our model (top) with results
from prior work are included for comparison (bottom). Results from models with a small, automatically-derived list
of possible prefixes and suffixes are labeled as ?Dict-? followed by the model name.
tributions over the frequencies of particular errors
follow a Zipfian skew across both S&B datasets,
with the Arabic being more pronounced (the most
frequent error being made 27 times, with 627 er-
rors being made just once) in comparison with the
Hebrew (with the most frequent error being made
19 times, and with 856 isolated errors). However,
in both the Arabic and Hebrew S&B tasks we find
that a tendency to over-segment certain characters
off of their correct morphemes and on to other fre-
quently occurring, yet incorrect, particles is actually
the cause of many of these isolated errors. In Ara-
bic the system tends to over segment the character
aleph (totally about 300 errors combined). In He-
brew the source of error is not as overwhelmingly
directed at a single character, but yod and he, the
latter functioning quite similarly to the problematic
Arabic character and frequently turn up in the corre-
sponding places of cognate words in Biblical texts.
We should note that our models select a large
number of hyper-parameters on an annotated devel-
opment set, including length penalty, hierarchical
smoothing parameters ?, and the subset of variables
to use in each of three component sub-models. This
might in part explain their advantage over previous-
state-of-the-art models, which might use fewer (e.g.
(Poon et al, 2009) and (Snyder and Barzilay, 2008))
or no specifically tuned for these datasets hyper-
parameters (Morfessor (Creutz and Lagus, 2007)).
4.2 Alignment
Next we evaluate our full bilingual model and a sim-
pler variant on the task of word alignment. We use
the morpheme-level annotation of the S&B English-
Arabic dataset and project the morpheme alignments
to word alignments. We can thus compare align-
ment performance of the results of different segmen-
tations. Additionally, we evaluate against a state-
of-the-art word alignment system WDHMM (He,
2007), which performs comparably or better than
IBM-Model4. The table in Figure 5 presents the re-
sults. In addition to reporting alignment error rate
for different segmentation models, we report their
morphological segmentation F1.
The word-alignment WDHMM model performs
best when aligning English words to Arabic words
(using Arabic as source). In this direction it is
able to capture the many-to-one correspondence be-
tween English words and arabic morphemes. When
we combine alignments in both directions using the
standard grow-diag-final method, the error goes up.
We compare the (Chung and Gildea, 2009) model
(termed Model-1) to our full bilingual model. We
can recover Model-1 similarly to Model-UP, except
now every morpheme is conditioned on an aligned
source morpheme. Our full bilingual model outper-
forms Model-1 in both AER and segmentation F1.
The specific form of the full model was selected as
in the previous experiments, by choosing the model
with best segmentations of the development set.
For Arabic, the best model conditions target mor-
902
Arabic Hebrew
Align P Align R AER P R F1 P R F1
Model-1 (C&G 09) 91.6 81.2 13.9 72.4 76.2 74.3 61.0 71.8 65.9
Bilingual full 91.0 88.3 10.3 90.0 72.0 80.0 63.3 71.2 67.0
WDHMM E-to-A 82.4 96.7 11.1
WDHMM GDF 82.1 94.6 12.1
Figure 5: Alignment Error Rate (AER) and morphological segmentation F1 achieved by bilingual variants of our
model. AER performance of WDHMM is also reported. Gold standard alignments are not available for the Hebrew
data set.
phemes on source morphemes only, uses the bound-
ary model with conditioning on number of mor-
phemes in the word, aligned source part-of-speech,
and type of target morpheme. The distortion model
uses both morpheme and word-based absolute dis-
tortion, binned distortion, morpheme types of states,
and aligned source-part-of-speech tags. Our best
model for Arabic outperforms WDHMM in word
alignment error rate. For Hebrew, the best model
uses a similar boundary model configuration but a
simpler uniform transition distortion distribution.
Note that the bilingual models perform worse than
the monolingual ones in segmentation F1. This
finding is in line with previous work showing that
the best segmentation for MT does not necessarily
agree with a particular linguistic convention about
what morphemes should contain (Chung and Gildea,
2009; Habash and Sadat, 2006), but contradicts
other results (Snyder and Barzilay, 2008). Further
experimentation is required to make a general claim.
We should note that the Arabic dataset used
for word-alignment evaluation is unconventionally
small and noisy (the sentences are very short
phrases, automatically extracted using GIZA++).
Thus the phrases might not be really translations,
and the sentence length is much smaller than in stan-
dard parallel corpora. This warrants further model
evaluation in a large-scale alignment setting.
5 Related Work
This work is most closely related to the unsupervised
tokenization and alignment models of Chung and
Gildea (2009), Xu et al (2008), Snyder and Barzilay
(2008), and Nguyen et al (2010).
Chung & Gildea (2009) introduce a unigram
model of tokenization based on IBM Model-1,which
is a special case of our model. Snyder and Barzi-
lay (2008) proposes a hierarchical Bayesian model
that combines the learning of monolingual segmen-
tations and a cross-lingual alignment; their model is
very different from ours.
Incorporating morphological information into
MT has received reasonable attention. For exam-
ple, Goldwater & McClosky (2005) show improve-
ments when preprocessing Czech input to reflect
a morphological decomposition using combinations
of lemmatization, pseudowords, and morphemes.
Yeniterzi and Oflazer (2010) bridge the morpholog-
ical disparity between languages in a unique way
by effectively aligning English syntactic elements
(function words connected by dependency relations)
to Turkish morphemes, using rule-based postpro-
cessing of standard word alignment. Our work is
partly inspired by that work and attempts to auto-
mate both the morpho-syntactic alignment and mor-
phological analysis tasks.
6 Conclusion
We have described an unsupervised model for mor-
pheme segmentation and alignment based on Hid-
den Semi-Markov Models. Our model makes use
of linguistic information to improve alignment qual-
ity. On the task of monolingual morphological seg-
mentation it produces a new state-of-the-art level on
three datasets. The model shows quantitative im-
provements in both word segmentation and word
alignment, but its true potential lies in its finer-
grained interpretation of word alignment, which will
hopefully yield improvements in translation quality.
Acknowledgements
We thank the ACL reviewers for their valuable
comments on earlier versions of this paper, and
Michael J. Burling for his contributions as a corpus
annotator and to the Arabic aspects of this paper.
903
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,
John DeNero, and Dan Klein. 2010. Unsupervised
learning with features. In Proceedings of the North
American chapter of the Association for Computa-
tional Linguistics (NAACL).
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
North American Chapter of the Association for Com-
putational Linguistics.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL 2nd Statistical MT work-
shop, pages 80?87.
Philip Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
P. Liang and D. Klein. 2009. Online EM for unsu-
pervised models. In North American Association for
Computational Linguistics (NAACL).
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proceedings of the International Con-
ference on Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In North American Chap-
ter of the Association for Computation Linguistics
- Human Language Technologies 2009 conference
(NAACL/HLT-09).
Patrick Schone and Daniel Jurafsky. 2000. Knowlege-
free induction of morphology using latent semantic
analysis. In Proceedings of the Conference on Compu-
tational Natural Language Learning (CoNLL-2000).
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In ACL.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In In COLING 96: The 16th Int. Conf. on Com-
putational Linguistics.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann
Ney. 2008. Bayesian semi-supervised chinese word
segmentation for statistical machine translation. In
COLING.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from english to turkish. In
Proceedings of Association of Computational Linguis-
tics.
904
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 461?466,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Why Initialization Matters for IBM Model 1:
Multiple Optima and Non-Strict Convexity
Kristina Toutanova
Microsoft Research
Redmond, WA 98005, USA
kristout@microsoft.com
Michel Galley
Microsoft Research
Redmond, WA 98005, USA
mgalley@microsoft.com
Abstract
Contrary to popular belief, we show that the
optimal parameters for IBM Model 1 are not
unique. We demonstrate that, for a large
class of words, IBM Model 1 is indifferent
among a continuum of ways to allocate prob-
ability mass to their translations. We study the
magnitude of the variance in optimal model
parameters using a linear programming ap-
proach as well as multiple random trials, and
demonstrate that it results in variance in test
set log-likelihood and alignment error rate.
1 Introduction
Statistical alignment models have become widely
used in machine translation, question answering,
textual entailment, and non-NLP application areas
such as information retrieval (Berger and Lafferty,
1999) and object recognition (Duygulu et al, 2002).
The complexity of the probabilistic models
needed to explain the hidden correspondence among
words has necessitated the development of highly
non-convex and difficult to optimize models, such
as HMMs (Vogel et al, 1996) and IBM Models 3
and higher (Brown et al, 1993). To reduce the im-
pact of getting stuck in bad local optima the orig-
inal IBM paper (Brown et al, 1993) proposed the
idea of training a sequence of models from simpler
to complex, and using the simpler models to initial-
ize the more complex ones. IBM Model 1 was the
first model in this sequence and was considered a
reliable initializer due to its convexity.
In this paper we show that although IBM Model 1
is convex, it is not strictly convex, and there is a large
space of parameter values that achieve the same op-
timal value of the objective.
We study the magnitude of this problem by for-
mulating the space of optimal parameters as solu-
tions to a set of linear equalities and seek maximally
different parameter values that reach the same objec-
tive, using a linear programming approach. This lets
us quantify the percentage of model parameters that
are not uniquely defined, as well as the number of
word types that have uncertain translation probabil-
ities. We additionally study the achieved variance in
parameters resulting from different random initial-
ization in EM, and the impact of initialization on test
set log-likelihood and alignment error rate. These
experiments suggest that initialization does matter
in practice, contrary to what is suggested in (Brown
et al, 1993, p. 273).1
2 Preliminaries
In Appendix A we define convexity and strict con-
vexity of functions following (Boyd and Vanden-
berghe, 2004). In this section we detail the gener-
ative model for Model 1.
2.1 IBM Model 1
IBM Model 1 (Brown et al, 1993) defines a genera-
tive process for a source sentences f = f1 . . . fm and
alignments a = a1 . . . am given a corresponding tar-
get translation e = e0 . . . el. The generative process
is as follows: (i) pick a length m using a uniform
distribution with mass function proportional to ; (ii)
for each source word position j, pick an alignment
1When referring to Model 1, Brown et al (1993) state that
?details of our initial guesses for t(f |e) are unimportant?.
461
position in the target sentence aj ? 0, 1, . . . , l from
a uniform distribution; and (iii) generate a source
word using the translation probability distribution
t(fj |eaj ). A special empty word (NULL) is assumed
to be part of the target vocabulary and to occupy
the first position in each target language sentence
(e0=NULL).
The trainable parameters of Model 1 are the lex-
ical translation probabilities t(f |e), where f and e
range over the source and target vocabularies, re-
spectively. The log-probability of a single source
sentence f given its corresponding target sentence e
and values for the translation parameters t(f |e) can
be written as follows (Brown et al, 1993):
m?
j=1
log
l?
i=0
t(fj |ei)?m log(l + 1) + log 
The parameters of IBM Model 1 are usu-
ally derived via maximum likelihood estimation
from a corpus, which is equivalent to negative
log-likelihood minimization. The negative log-
likelihood for a parallel corpus D is:
LD(T ) = ?
?
f ,e
m?
j=1
log
l?
i=0
t(fj |ei) +B (1)
where T is the matrix of translation probabilities
and B represents the other terms of Model 1 (string
length probability and alignment probability), which
are constant with respect to the translation parame-
ters t(f |e).
We can define the optimization problem as the
one of minimizing negative log-likelihood LD(T )
subject to constraints ensuring that the parameters
are well-formed probabilities, i.e., that they are non-
negative and summing to one. It is well-known that
the EM algorithm for this problem converges to a lo-
cal optimum of the objective function (Dempster et
al., 1977).
3 Convexity analysis for IBM Model 1
In this section we show that, contrary to the claim in
(Brown et al, 1993), the optimization problem for
IBM Model 1 is not strictly convex, which means
that there could be multiple parameter settings that
achieve the same globally optimal value of the ob-
jective.2
The function ? log(x) is strictly convex (Boyd
and Vandenberghe, 2004). Each term in the nega-
tive log-likelihood is a negative logarithm of a sum
of parameters. The negative logarithm of a sum is
not strictly convex, as illustrated by the following
simple counterexample. Let?s look at the function
? log(x1 +x2). We can express it in vector notation
using ? log(1Tx), where 1 is a vector with all ele-
ments equal to 1. We will come up with two param-
eter settings x,y and a value ? that violate the defini-
tion of strict convexity. Take x = [x1, x2] = [.1, .2],
y = [y1, y2] = [.2, .1] and ? = .5. We have
z = ?x + (1 ? ?)y = [z1, z2] = [.15, .15]. Also
? log(1T (?x + (1 ? ?)y)) = ? log(z1 + z2) =
? log(.3). On the other hand, ?? log(x1 + x2) ?
(1??) log(y1+y2) = ? log(.3). Strict convexity re-
quires that the former expression be strictly smaller
than the latter, but we have equality. Therefore, this
function is not strictly convex. It is however con-
vex as stated in (Brown et al, 1993), because it is a
composition of log and a linear function.
We thus showed that every term in the negative
log-likelihood objective is convex but not strictly
convex and thus the overall objective is convex, but
not strictly convex. Because the objective is con-
vex, the inequality constraints are convex, and the
equality constraints are affine, the IBM Model 1 op-
timization problem is a convex optimization prob-
lem. Therefore every local optimum is a global op-
timum. But since the objective is not strictly con-
vex, there might be multiple distinct parameter val-
ues achieving the same optimal value. In the next
section we study the actual space of optima for small
and realistically-sized parallel corpora.
2Brown et al (1993, p. 303) claim the following about
the log-likelihood function (Eq. 51 and 74 in their paper, and
Eq. 1 in ours): ?The objective function (51) for this model is a
strictly concave function of the parameters?, which is equivalent
to claiming that the negative log-likelihood function is strictly
convex. In this section, we will theoretically demonstrate that
Brown et al?s claim is in fact incorrect. Furthermore, we will
empirically show in Sections 4 and 5 that multiple distinct pa-
rameter values can achieve the global optimum of the objective
function, which also disproves Brown et al?s claim about the
strict convexity of the objective function. Indeed, if a function
is strictly convex, it admits a unique globally optimum solution
(Boyd and Vandenberghe, 2004, p. 151), so our experiments
prove by modus tollens that Brown et al?s claim is wrong.
462
4 Solution Space
In this section, we characterize the set of parameters
that achieve the maximum of the log-likelihood of
IBM Model 1. As illustrated with the following
simple example, it is relatively easy to establish
cases where the set of optimal parameters t(f |e) is
not unique:
e : short sentence f : phrase courte
If the above sentence pair represents the entire
training data, Model 1 likelihood (ignoring NULL
words) is proportional to
[
t(phrase|short) + t(phrase|sentence)
]
?
[
t(courte|short) + t(courte|sentence)
]
which can be maximized in infinitely many differ-
ent ways. For instance, setting t(phrase|sentence) =
t(courte|short) = 1 yields the maximum likelihood
value with (0 + 1)(1 + 0) = 1, but the most
divergent set of parameters (t(courte|sentence) =
t(phrase|sentence) = 1) also reaches the same op-
timum: (1+0)(0+1) = 1. While this example may
not seem representative given the small size of this
data, the laxity of Model 1 that we observe in this
example also surfaces in real and much larger train-
ing sets. Indeed, it suffices that a given pair of target
words (e1,e2) systematically co-occurs in the data
(as with e1 = short e2 = sentence) to cause Model 1
to fail to distinguish the two.3
To characterize the solution space, we use the def-
inition of IBM Model 1 log-likelihood from Eq. 1 in
Section 2.1. We ask whether distinct sets of parame-
ters yield the same minimum negative log-likelihood
value of Eq. 1, i.e., whether we can find distinct
models t(f |e) and t?(f |e) so that:
?
f ,e
m?
j=1
log
l?
i=0
t(fj |ei) =
?
f ,e
m?
j=1
log
l?
i=0
t?(fj |ei)
Since the negative logarithm is strictly convex, the
3Since e1 and e2 co-occur with exactly the same source
words, one can redistribute the probability mass between
t(f |e1) and t(f |e2) without affecting the log-likelihood.
This is true if (a) the two distributions remain well-formed:
?
j t(fj |ei) = 1 for i ? {1, 2}; (b) any adjustments to param-
eters of fj leave each estimate t(fj |e1) + t(fj |e2) unchanged.
above equation can be satisfied for optimal parame-
ters only if the following holds for each f , e pair:
l?
i=0
t(fj |ei) =
l?
i=0
t?(fj |ei), j = 1 . . .m (2)
We can further simplify the above equation if we re-
call that both t(f |e) and t?(f |e) are maximum log-
likelihood parameters, and noting it is generally easy
to obtain one such set of parameters, e.g., by run-
ning the EM algorithm until convergence. Using
these EM parameters (?) in the right hand side of
the equation, we replace these right hand sides with
EM?s estimate t?(fj |e). This finally gives us the fol-
lowing linear program (LP), which characterizes the
solution space of the maximum log-likelihood:4
l?
i=0
t(fj |ei) = t?(fj |e), j = 1 . . .m ?f , e (3)
?
f
t(f |e) = 1, ?e (4)
t(f |e) ? 0, ?e, f (5)
The two conditions in Eq. 4-5 are added to ensure
that t(f |e) is well-formed. To solve this LP, we use
the interior-point method of (Karmarkar, 1984).
To measure the maximum divergence in optimal
model parameters, we solve the LP of Eq. 3-5 by
minimizing the linear objective function xTk?1xk,
where xk is the column-vector representing all pa-
rameters of the model t(f |e) currently optimized,
and where xk?1 is a pre-existing set of maximum
log-likelihood parameters. Starting with x0 defined
using EM parameters, we are effectively searching
for the vector x1 with lowest cosine similarity to x0.
We repeat with k > 1 until xk doesn?t reduce the
cosine similarity with any of the previous parameter
vectors x0 . . .xk?1 (which generally happens with
k = 3).5
4In general, an LP admits either (a) an infinity of solutions,
when the system is underconstrained; (b) exactly one solution;
(c) zero solutions, when it is ill-posed. The latter case never
occurs in our case, since the system was explicitly constructed
to allow at least one solution: the parameter set returned by EM.
5Note that this greedy procedure is not guaranteed to find the
two points of the feasible region (a convex polytope) with mini-
mum cosine similarity. This problem is related to finding the di-
ameter of this polytope, which is known to be NP-hard when the
number of variables is unrestricted (Kaibel et al, 2002). Never-
theless, divergences found by this procedure are fairly substan-
tial, as shown in Section 5.
463
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
EM-LP-1
EM-LP-8
EM-LP-32
EM-LP-128
EM-rand-1
EM-rand-8
EM-rand-32
EM-rand-128
EM-rand-1K
EM-rand-10K
cum
ula
tive
 pe
rce
nta
ge 
cosine similarity [c] 
Figure 1: Percentage of target words for which we found
pairs of distributions t(f |e) and t?(f |e) whose cosine
similarity drops below a given threshold c (x-axis).
5 Experiments
In this section, we show that the solution space
defined by the LP of Eq. 3-5 can be fairly large.
We demonstrate this with Bulgarian-English paral-
lel data drawn from the JRC-AQUIS corpus (Stein-
berger et al, 2006). Our training data consists of up
to 10,000 sentence pairs, which is representative of
the amount of data used to train SMT systems for
language pairs that are relatively resource-poor.
Figure 1 relies on two methods for determining to
what extent the model t(f |e) can vary while remain-
ing optimal. The EM-LP-N method consists of ap-
plying the method described at the end of Section 4
with N training sentence pairs. For EM-rand-N , we
instead run EM 100 times (also onN sentence pairs)
until convergence using different random starting
points, and then use cosine similarity to compare the
resulting models.6 Figure 1 shows some surprising
results: First, EM-LP-128 finds that, for about 68%
of target token types, cosine similarity between con-
trastive models is equal to 0. A cosine of zero es-
sentially means that we can turn 1?s into 0?s with-
out affecting log-likelihood, as in the short sentence
example in Section 4. Second, with a much larger
training set, EM-rand-10K finds a cosine similarity
lower or equal to 0.5 for 30% of word types, which
is a large portion of the vocabulary.
6While the first method is better at finding divergent optimal
model parameters, it needs to construct large linear programs
that do not scale with large training sets (linear systems quickly
reach millions of entries, even with 128 sentence pairs). We use
EM-rand to assess the model space on larger training set, while
we use EM-LP mainly to illustrate that divergence between op-
timal models can be much larger than suggested by EM-rand.
train coupled non-unique log-lik
all c. non-c. stdev unif
1 100 100 100 - 2.9K -4.9K
8 83.6 89.0 100 33.3 2.3K -2.3K
32 77.8 81.8 100 17.9 874 74.4
128 67.8 73.3 99.7 17.7 270 272
1K 52.6 64.1 99.8 24.0 220 281
10K 30.3 47.33 99.9 24.4 150 300
Table 1: Results using 100 random initialization trials.
In Table 1 we show additional statistics computed
from the EM-rand-N experiments. Every row repre-
sents statistics for a given training set size (in num-
ber of sent. pairs, first column); the second column
shows the percent of target word types that always
co-occur with another word type (we term these
words coupled); the third, fourth, and fifth columns
show the percent of word types whose translation
distributions were found to be non-unique, where
we define the non-unique types to be ones where the
minimum cosine between any two different optimal
parameter vectors was less than .95. The percent
of non-unique types are reported overall, as well as
only among coupled words (c.) and non-coupled
words (non-c.). The last two columns show the stan-
dard deviation in test set log-likelihood across differ-
ent random trials, as well as the difference between
the log-likelihood of the uniformly initialized model
and the best model from the random trials.
We can see that as the training set size increases,
the percentage of words that have non-unique trans-
lation probabilities goes down but is still very large.
The coupled words almost always end up having
varying translation parameters at convergence (more
than 99.5% of these words). This also happens for
a sizable portion of the non-coupled words, which
suggests that there are additional patterns of co-
occurrence that result in non-determinism.7 We also
computed the percent of word types that are coupled
for two more-realistically sized data-sets: we found
that in a 1.6 million sent pair English-Bulgarian cor-
pus 15% of Bulgarian word types were coupled and
in a 1.9 million English-German corpus from the
WMT workshop (Callison-Burch et al, 2010), 13%
of the German word types were coupled.
The log-likelihood statistics show that although
7We did not perform such experiments for larger data-sets,
since EM takes thousands of iterations to converge.
464
the standard deviation goes down with training set
size, it is still large at reasonable data sizes. Inter-
estingly, the uniformly initialized model performs
worse for a very small data size, but it catches up and
surpasses the random models at data sizes greater
than 100 sentence pairs.
To further evaluate the impact of initialization for
IBM Model 1, we report on a set of experiments
looking at alignment error rate achieved by differ-
ent models. We report the performance of Model 1,
as well as the performance of the more competitive
HMM alignment model (Vogel et al, 1996), initial-
ized from IBM-1 parameters. The dataset for these
experiments is English-French parallel data from
Hansards. The manually aligned data for evaluation
consists of 137 sentences (a development set from
(Och and Ney, 2000)).
We look at two different training set sizes, a
small set consisting of 1000 sentence pairs, and
a reasonably-sized dataset containing 100,000 sen-
tence pairs. In each data size condition, we report on
the performance achieved by IBM-1, and the perfor-
mance achieved by HMM initialized from the IBM-
1 parameters. For IBM Model 1 training, we either
perform only 5 EM iterations (the standard setting
in GIZA++), or run it to convergence. For each of
these two settings, we either start training from uni-
form t(f |e) parameters, or random parameters. Ta-
ble 2 details the results of these experiments.
Each row in the table represents an experimental
condition, indicating the training data size (1K in the
first four rows and 100K in the next four rows), the
type of initialization (uniform versus random) and
the number of iterations EM was run for Model 1 (5
iterations versus unlimited (to convergence, denoted
?)). The numbers in the table are alignment error
rates, achieved at the end of Model 1 training, and
at 5 iterations of HMM. When random initialization
is used, we run 20 random trials with different ini-
tialization, and report the min, max, and mean AER
achieved in each setting.
From the table, we can draw several conclusions.
First, in agreement with current practice using only
5 iterations of Model 1 training results in better fi-
nal performance of the HMM model (even though
the performance of Model 1 is higher when ran to
convergence). Second, the minimum AER achieved
by randomly initialized models was always smaller
setting IBM-1 HMM
min mean max min mean max
1K-unif-5 42.99 - - 22.53 - -
1K-rand-5 42.90 44.07 45.08 22.26 22.99 24.01
1K-unif-? 42.10 - - 28.09 - -
1K-rand-? 41.72 42.61 43.63 27.88 28.47 28.89
100K-unif-5 28.98 - - 12.68 - -
100K-rand-5 28.63 28.99 30.13 12.25 12.62 12.89
100K-unif-? 28.18 - - 16.84 - -
100K-rand-? 27.95 28.22 30.13 16.66 16.78 16.85
Table 2: AER results for Model 1 and HMM using uni-
form and random initialization. We do not report mean
and max for uniform, since they are identical to min.
than the AER of the uniform-initialized models. In
some cases, even the mean of the random trials was
better than the corresponding uniform model. Inter-
estingly, the advantage of the randomly initialized
models in AER does not seem to diminish with in-
creased training data size like their advantage in test
set perplexity.
6 Conclusions
Through theoretical analysis and three sets of ex-
periments, we showed that IBM Model 1 is not
strictly convex and that there is large variance in
the set of optimal parameter values. This variance
impacts a significant fraction of word types and re-
sults in variance in predictive performance of trained
models, as measured by test set log-likelihood and
word-alignment error rate. The magnitude of this
non-uniqueness further supports the development of
models that can use information beyond simple co-
occurrence, such as positional and fertility informa-
tion like higher order alignment models, as well as
models that look beyond the surface form of a word
and reason about morphological or other properties
(Berg-Kirkpatrick et al, 2010).
In future work we would like to study the im-
pact of non-determinism on higher order models in
the standard alignment model sequence and to gain
more insight into the impact of finer-grained features
in alignment.
Acknowledgements
We thank Chris Quirk and Galen Andrew for valu-
able discussions and suggestions.
465
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics. Association for Computational
Linguistics.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of the
1999 ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval.
Stephen Boyd and Lieven Vandenberghe. 2004. Convex
Optimization. Cambridge University Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society, se-
ries B, 39(1).
Pinar Duygulu, Kobus Barnard, Nando de Freitas,
P. Duygulu, K. Barnard, and David Forsyth. 2002.
Object recognition as machine translation: Learning a
lexicon for a fixed image vocabulary. In Proceedings
of ECCV.
Volker Kaibel, Marc E. Pfetsch, and TU Berlin. 2002.
Some algorithmic problems in polytope theory. In
Dagstuhl Seminars, pages 23?47.
N. Karmarkar. 1984. A new polynomial-time algorithm
for linear programming. Combinatorica, 4:373?395,
December.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, and Dan Tufis. 2006.
The JRC-acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation (LREC).
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th Int. Conf. on
Computational Linguistics (COLING). Association for
Computational Linguistics.
Appendix A: Convex functions and convex
optimization problems
We denote the domain of a function f by dom f .
Definition A function f : Rn ? R is convex if and only
if dom f is a convex set and for all x, y ? dom f and
? ? 0, ? ? 1:
f(?x+ (1? ?)y) ? ?f(x) + (1? ?)f(y) (6)
Definition A function f is strictly convex iff dom f is a
convex set and for all x 6= y ? dom f and ? > 0, ? < 1:
f(?x+ (1? ?)y) < ?f(x) + (1? ?)f(y) (7)
Definition A convex optimization problem is defined by:
min f0(x)
subject to
fi(x) ? 0, i = 1 . . . k
aTj x = bj , j = 1 . . . l
Where the functions f0 to fk are convex and the equal-
ity constraints are affine.
It can be shown that the feasible set (the set of points
that satisfy the constraints) is convex and that any local
optimum for the problem is a global optimum. If f0
is strictly convex then any local optimum is the unique
global optimum.
466
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 694?702,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Multilingual Named Entity Recognition using Parallel Data and Metadata
from Wikipedia
Sungchul Kim?
POSTECH
Pohang, South Korea
subright@postech.ac.kr
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Hwanjo Yu
POSTECH
Pohang, South Korea
hwanjoyu@postech.ac.kr
Abstract
In this paper we propose a method to auto-
matically label multi-lingual data with named
entity tags. We build on prior work utiliz-
ing Wikipedia metadata and show how to ef-
fectively combine the weak annotations stem-
ming from Wikipedia metadata with infor-
mation obtained through English-foreign lan-
guage parallel Wikipedia sentences. The com-
bination is achieved using a novel semi-CRF
model for foreign sentence tagging in the con-
text of a parallel English sentence. The model
outperforms both standard annotation projec-
tion methods and methods based solely on
Wikipedia metadata.
1 Introduction
Named Entity Recognition (NER) is a frequently
needed technology in NLP applications. State-of-
the-art statistical models for NER typically require
a large amount of training data and linguistic exper-
tise to be sufficiently accurate, which makes it nearly
impossible to build high-accuracy models for a large
number of languages.
Recently, there have been two lines of work which
have offered hope for creating NER analyzers in
many languages. The first has been to devise an
algorithm to tag foreign language entities using
metadata from the semi-structured Wikipedia repos-
itory: inter-wiki links, article categories, and cross-
language links (Richman and Schone, 2008). The
second has been to use parallel English-foreign lan-
guage data, a high-quality NER tagger for English,
and projected annotations for the foreign language
(Yarowsky et al, 2001; Das and Petrov, 2011). Par-
allel data has also been used to improve existing
monolingual taggers or other analyzers in two lan-
guages (Burkett et al, 2010a; Burkett et al, 2010b).
?This research was conducted during the author?s internship
at Microsoft Research
The goal of this work is to create high-accuracy
NER annotated data for foreign languages. Here
we combine elements of both Wikipedia metadata-
based approaches and projection-based approaches,
making use of parallel sentences extracted from
Wikipedia. We propose a statistical model which
can combine the two types of information. Simi-
larly to the joint model of Burkett et al (2010a), our
model can incorporate both monolingual and bilin-
gual features in a log-linear framework. The advan-
tage of our model is that it is much more efficient
as it does not require summing over matchings of
source and target entities. It is a conditional model
for target sentence annotation given an aligned En-
glish source sentence, where the English sentence is
used only as a source of features. Exact inference is
performed using standard semi-markov CRF model
inference techniques (Sarawagi and Cohen, 2004).
Our results show that the semi-CRF model im-
proves on the performance of projection models by
more than 10 points in F-measure, and that we can
achieve tagging F-measure of over 91 using a very
small number of annotated sentence pairs.
The paper is organized as follows: We first
describe the datasets and task setting in Section
2. Next, we present our two baseline methods:
A Wikipedia metadata-based tagger and a cross-
lingual projection tagger in Sections 3 and 4, re-
spectively. We present our direct semi-CRF tagging
model in Section 5.
2 Data and task
As a case study, we focus on two very dif-
ferent foreign languages: Korean and Bulgarian.
The English and foreign language sentences that
comprise our training and test data are extracted
from Wikipedia (http://www.wikipedia.org). Cur-
rently there are more than 3.8 million articles in
the English Wikipedia, 125,000 in the Bulgarian
Wikipedia, and 131,000 in the Korean Wikipedia.
694
Figure 1: A parallel sentence-pair showing gold-standard NE labels and word alignments.
To create our dataset, we followed Smith et al
(2010) to find parallel-foreign sentences using com-
parable documents linked by inter-wiki links. The
approach uses a small amount of manually annotated
article-pairs to train a document-level CRF model
for parallel sentence extraction. A total of 13,410
English-Bulgarian and 8,832 English-Korean sen-
tence pairs were extracted.
Of these, we manually annotated 91 English-
Bulgarian and 79 English-Korean sentence pairs
with source and target named entities as well as
word-alignment links among named entities in the
two languages. Figure 1 illustrates a Bulgarian-
English sentence pair with alignment.
The named entity annotation scheme followed has
the labels GPE (Geopolitical entity), PER (Person),
ORG (Organization), and DATE. It is based on the
MUC-7 annotation guidelines, and GPE is synony-
mous with Location. The annotation process was
not as rigorous as one might hope, due to lack of re-
sources. The English-Bulgarian and English-Korean
datasets were labeled by one annotator each and then
annotations on the English sentences were double-
checked by the other annotator. Disagreements were
rare and were resolved after discussion.
The task we evaluate on is tagging of foreign lan-
guage sentences. We measure performance by la-
beled precision, recall, and F-measure. We give par-
tial credit if entities partially overlap on their span of
words and match on their labels.
Table 1 shows the total number of English,
Bulgarian and Korean entities and the percent-
age of entities that were manually aligned to an
entity of the same type in the other language.
The data sizes are fairly small as the data is
Language Entities Aligned %
English 342 93.9%
Bulgarian 344 93.3%
English 414 88.4%
Korean 423 86.5%
Table 1: English-Bulgarian and English-Korean data
characteristics.
used only to train models with very few coarse-
grained features and for evaluation. These datasets
are available at http://research.microsoft.com/en-
us/people/kristout/nerwikidownload.aspx.
As we can see, less than 100% of entities have
parallels in the other language. This is due to two
phenomena: one is that the parallel sentences some-
times contain different amounts of information and
one language might use more detail than the other.
The other is that the same information might be ex-
pressed using a named entity in one language, and
using a non-entity phrase in the other language (e.g.
?He is from Bulgaria? versus ?He is Bulgarian?).
Both of these causes of divergence are much more
common in the English-Korean dataset than in the
English-Bulgarian one.
3 Wiki-based tagger: annotating sentences
based on Wikipedia metadata
We followed the approach of Richman and Schone
(2008) to derive named entity annotations of both
English and foreign phrases in Wikipedia, using
Wikipedia metadata. The following sources of in-
formation were used from Wikipedia: category an-
notations on English documents, article links which
link from phrases in an article to another article in
the same language, and interwiki links which link
695
Figure 2: Candidate NEs for the English and Bulgarian
sentences according to baseline taggers.
from articles in one language to comparable (seman-
tically equivalent) articles in the other language. In
addition to the Wikipedia-derived resources, the ap-
proach requires a manually specified map from En-
glish category key-phrases to NE tags, but does not
require expert knowledge for any non-English lan-
guage. We implemented the main ideas of the ap-
proach but some implementation details may differ.
To tag English language phrases, we first derived
named entity categorizations of English article titles,
by assigning a tag based on the article?s category
information. The category-to-NE map used for the
assignment is a small manually specified map from
phrases appearing in category titles to NE tags. For
example, if an article has categories ?People by?,
?People from?, ?Surnames? etc., it is classified as
PER. Looking at the example in Figure 1, the article
with title ?Igor Tudor? is classified as PER because
one of its categories is ?Living people?. The full
map we use is taken from the paper (Richman and
Schone, 2008).
Using the article-level annotations and article
links we define a local English wiki-based tagger
and a global English wiki-based tagger, which will
be described in detail next.
Local EnglishWiki-based tagger. This Wiki-based
tagger tags phrases in an English article based on the
article links from these phrases to NE-tagged arti-
cles. For example, suppose that the phrase ?Split? in
the article with title ?Igor Tudor? is linked to the ar-
ticle with title ?Split?, which is classified as GPE.
Thus the local English Wiki-based tagger can tag
this phrase as GPE. If, within the same article, the
phrase ?Split? occurs again, it can be tagged again
even if it is not linked to a tagged article (this is
the one sense per document assumption). Addition-
ally, the tagger tags English phrases as DATE if they
match a set of manually specified regular expres-
sions. As a filter, phrases that do not contain a cap-
italized word or a number are not tagged with NE
tags.
Global English Wiki-based tagger. This tagger
tags phrases with NE tags if these phrases have ever
been linked to a categorized article (the most fre-
quent label is used). For example, if ?Split? does
not have a link anywhere in the current article, but
has been linked to the GPE-labeled article with ti-
tle ?Split? in another article, it will still be tagged
as GPE. We also apply a local+global Wiki-tagger,
which tags entities according to the local Wiki-
tagger and additionally tags any non-conflicting en-
tities according to the global tagger.
Local foreign Wiki-based tagger. The idea is the
same as for the local English tagger, with the dif-
ference that we first assign NE tags to foreign lan-
guage articles by using the NE tags assigned to En-
glish articles to which they are connected with inter-
wiki links. Because we do not have maps from cate-
gory phrases to NE tags for foreign languages, using
inter-wiki links is a way to transfer this knowledge
to the foreign languages. After we have categorized
foreign language articles we follow the same algo-
rithm as for the local English Wiki-based tagger. For
Bulgarian we also filtered out entities based on cap-
italization and numbers, but did not do that for Ko-
rean as it has no concept of capitalization.
Global foreign Wiki-based tagger The global and
local+global taggers are analogous, using the cate-
gorization of foreign articles as above.
Figure 2 shows the tags assigned to English and
Bulgarian strings according to the local and global
Wiki-based taggers. The global Wiki-based tag-
ger could assign multiple labels to the same string
(corresponding to different senses in different oc-
currences). In case of multiple possible labels, the
most frequent one is denoted by * in the Figure. The
Figure also shows the results of the Stanford NER
tagger for English (Finkel et al, 2005) (we used the
MUC-7 classifier).
Table 2 reports the performance of the local (L
Wiki-tagger), local+global (LG Wiki tagger) and the
Stanford tagger. We can see that the local Wiki tag-
gers have higher precision but lower recall than the
local+global Wiki taggers. The local+global taggers
696
Language L Wiki-tagger LG Wiki-tagger Stanford Tagger
Prec Rec F1 Prec Rec F1 Prec Rec F1
English 92.8 75.1 83.0 79.7 89.5 84.3 86.5 77.5 81.7
Bulgarian 94.1 48.7 64.2 86.8 79.9 83.2
English 92.6 75.6 83.2 84.1 86.7 85.4 82.2 71.9 76.7
Korean 89.5 57.3 69.9 43.2 78.0 55.6
Table 2: English-Bulgarian and English-Korean Wiki-based tagger performance.
are overall best for English and Bulgarian. The lo-
cal tagger is best for Korean, as the precision suffers
too much due to the global tagger. This is perhaps
due in part to the absence of the capitalization filter
for Korean which improved precision for Bulgarian
and English. The Stanford tagger is worse than the
Wiki-based tagger, but it is different enough that it
contributes useful information to the task.
4 Projection Model
From Table 2 we can see that the English Wiki-
based taggers are better than the Bulgarian and Ko-
rean ones, which is due to the abundance and com-
pleteness of English data in Wikipedia. In such cir-
cumstances, previous research has shown that one
can project annotations from English to the more
resource-poor language (Yarowsky et al, 2001).
Here we follow the approach of Feng et al (2004)
to train a log-linear model for projection.
Note that the Wiki-based taggers do not require
training data and can be applied to any sentences
from Wikipedia articles. The projection model de-
scribed in this section and the Semi-CRF model
described in Section 5 are trained using annotated
data. They can be applied to tag foreign sen-
tences in English-foreign sentence pairs extracted
from Wikipedia.
The task of projection is re-cast as a ranking task,
where for each source entity Si, we rank all possible
candidate target entity spans Tj and select the best
span as corresponding to this source entity. Each
target span is labeled with the NE label of the corre-
sponding source entity. The probability distribution
over target spans Tj for a given source entity Si is
defined as follows:
p(Si|Tj) =
exp(?f(Si, Tj))
?
j? exp(?f(Si, T
?
j))
where ? is a parameter vector, and f(Si, Tj) is a fea-
ture vector for the candidate entity pair.
From this formulation we can see that a fixed set
of English source entities Si is required as input.
The model projects these entities to corresponding
foreign entities. We train and evaluate the projection
model using 10-fold cross-validation on the dataset
from Table 1. For training, we use the human-
annotated gold English entities and the manually-
specified entity alignments to derive corresponding
target entities. At test time we use the local+global
Wiki-based tagger to define the English entities and
we don?t use the manually annotated alignments.
4.1 Features
We present the features for this model in a lot of
detail since analogous feature types are also used in
our final direct semi-CRF model. The features are
grouped into four categories.
Word alignment features
We exploit a feature set based on HMM word align-
ments in both directions (Och and Ney, 2000). To
define the features we make use of the posterior
alignment link probabilities as well as the most
likely (Viterbi) alignments. The posterior proba-
bilities are the probabilities of links in both direc-
tions given the source and target sentences: P (ai =
j|s, t) and P (aj = i|s, t).
If a source entity consists of positions i1, . . . , im
and a potential corresponding target entity consists
of positions j1, . . . , jn, the word-alignment derived
features are:
? Probability that each word from one of the en-
tities is aligned to a word from the other entity,
estimated as:
?
i?i1...im
?
j?j1...jn P (ai = j|s, t) We use an
analogous estimate for the probability in the
other direction.
697
? Sum of posterior probabilities of links from
words inside one entity to words outside an-
other entity
?
i?i1...im(1 ?
?
j?j1...jn P (ai =
j|s, t)). Probabilities from the other HMM di-
rection are estimated analogously.
? Indicator feature for whether the source and
target entity can be extracted as a phrase pair
according to the combined Viterbi alignments
(grow-diag-final) and the standard phrase ex-
traction heuristic (Koehn et al, 2003).
Phonetic similarity features
These features measure the similarity between a
source and target entity based on pronunciation. We
utilize a transliteration model (Cherry and Suzuki,
2009), trained from pairs of English person names
and corresponding foreign language names, ex-
tracted from Wikipedia. The transliteration model
can return an n-best list of transliterations of a for-
eign string, together with scores. For example the
top 3 transliterations in English of the Bulgarian
equivalent of ?Igor Tudor? from Figure 1 are Igor
Twoodor, Igor Twoodore, and Igore Twoodore.
We estimate phonetic similarity between a source
and target entity by computing Levenshtein and
other distance metrics between the source entity
and the closest transliteration of the target (out of a
10-best list of transliterations). We use normalized
and un-normalized Levenshtein distance. We
also use a BLEU-type measure which estimates
character n-gram overlap.
Position/Length features
These report relative length and position of the
English and foreign entity following (Feng et al,
2004).
Wiki-based tagger features
These features look at the degree of match between
the source and target entities based on the tags as-
signed to them by the local and global Wiki-taggers
for English and the foreign language, and by the
Stanford tagger for English. These are indicator fea-
tures separate for the different source-target tagger
combinations, looking at whether the taggers agree
in their assignments to the candidate entities.
4.2 Model Evaluation
We evaluate the tagging F-measure for projec-
tion models on the English-Bulgarian and English-
Korean datasets. 10-fold cross-validation was used
to estimate model performance. The foreign lan-
guage NE F-measure is reported in Table 3. The best
Wiki-based tagger performance is shown on the last
line as a baseline (repeated from Table 2).
We present a detailed evaluation of the model to
gain understanding of the strengths and limitations
of the projection approach and to motivate our direct
semi-CRF model. To give an estimate of the upper
bound on performance for the projection model, we
first present two oracles. The goal of the oracles it
to estimate the impact of two sources of error for the
projection model: the first is the error in detecting
English entities, and the second is the error in deter-
mining the corresponding foreign entity for a given
English entity.
The first oracle ORACLE1 has access to the gold-
standard English entities and gold-standard word
alignments among English and foreign words. For
each source entity, ORACLE1 selects the longest for-
eign language sequence of words that could be ex-
tracted in a phrase pair coupled with the source en-
tity word sequence (according the standard phrase
extraction heuristic (Koehn et al, 2003)), and labels
it with the label of the source entity. Note that the
word alignments do not uniquely identify the corre-
sponding foreign phrase for each English phrase and
some error is possible due to this. The performance
of this oracle is closely related to the percentage of
linked source-target entities reported in Table 1. The
second oracle ORACLE2 provides the performance
of the projection model when gold-standard source
entities are known, but the corresponding target en-
tities still have to be determined by the projection
model (gold-standard alignments are not known). In
other words, ORACLE2 is the projection model with
all features, where in the test set we provide the gold
standard English entities as input. The performance
of ORACLE2 is determined by the error in automatic
word alignment and in determining phonetic corre-
spondence. As we can see the drop due to this error
is very large, especially on Korean, where perfor-
mance drops from 90.0 to 81.9 F-measure.
The next section in the Table presents the perfor-
698
Method English-Bulgarian English-Korean
Prec Rec F1 Prec Rec F1
ORACLE1 98.3 92.9 95.5 95.5 85.1 90.0
ORACLE2 96.7 86.3 91.2 90.5 74.7 81.9
PM-WF 71.7 80.0 75.7 85.1 72.2 78.1
PM+WF 73.6 81.3 77.2 87.6 74.9 80.8
Wiki-tagger 86.8 79.9 83.2 89.5 57.3 69.9
Table 3: English-Bulgarian and English-Korean Projection tagger performance.
mance of non-oracle projection models, which do
not have access to any manually labeled informa-
tion. The local+global Wiki-based tagger is used to
define English entities, and only automatically de-
rived alignment information is used. PM+WF is the
projection model using all features. The line above,
PM-WF represents the projection model without
the Wiki-tagger derived features, and is included to
show that the gain from using these features is sub-
stantial. The difference in accuracy between the pro-
jection model and ORACLE2 is very large, and is due
to the error of the Wiki-based English taggers. The
drop for Bulgarian is so large that the best projec-
tion model PM+WF does not reach the performance
of 83.2 achieved by the baseline Wiki-based tagger.
When source entities are assigned with error for this
language pair, projecting entity annotations from the
source is not better than using the target Wiki-based
annotations directly. For Korean while the trend in
model performance is similar as oracle information
is removed, the projection model achieves substan-
tially better performance (80.8 vs 69.9) due to the
much larger difference in performance between the
English and Korean Wiki-based taggers.
The drawback of the projection model is that it
determines target entities only by assigning the best
candidate for each source entity. It cannot create tar-
get entities that do not correspond to source entities,
it is not able to take into account multiple conflicting
source NE taggers as sources of information, and it
does not make use of target sentence context and en-
tity consistency constraints. To address these short-
comings we propose a direct semi-CRF model, de-
scribed in the next section.
5 Semi-CRF Model
Semi-Markov conditional random fields (semi-
CRFs) are a generalization of CRFs. They assign la-
bels to segments of an input sequence x, rather than
to individual elements xi and features can be de-
fined on complete segments. We apply Semi-CRFs
to learn a NE tagger for labeling foreign sentences in
the context of corresponding source sentences with
existing NE annotations.
The semi-CRF defines a distribution over foreign
sentence labeled segmentations (where the segments
are named entities with their labels, or segments of
length one with label ?NONE?). To formally define
the distribution, we introduce some notation follow-
ing Sarawagi and Cohen (2005):
Let s = ?s1, . . . , sp? denote a segmentation of
the foreign sentence x, where a segment sj =
?tj , uj , yj? is determined by its start position tj , end
position uj , and label yj . Features are defined on
segments and adjacent segment labels. In our appli-
cation, we only use features on segments. The fea-
tures on segments can also use information from the
corresponding English sentence e along with exter-
nal annotations on the sentence pair A.
The feature vector for each segment can be de-
noted by F (j, s,x, e,A) and the weight vector for
features by w. The probability of a segmentation is
then defined as:
P (s|x, e,A) =
?
j expw
?F (j, s,x, e,A)
Z(x, e,A)
In the equation above Z represents a normalizer
summing over valid segmentations.
5.1 Features
We use both boolean and real-valued features in the
semi-CRF model. Example features and their val-
ues are given in Table 4. The features are the ones
that fire on the segment of length 1 containing the
Bulgarian equivalent of the word ?Split? and la-
beled with label GPE (tj=13,uj=13,yj=GPE), from
the English-Bulgarian sentence pair in Figure 1.
699
The features look at the English and foreign sen-
tence as well as external annotations A. Note that
the semi-CRF model formulation does not require a
fixed labeling of the English sentence. Different and
possibly conflicting NE tags for candidate English
and foreign sentence substrings according to the
Wiki-based taggers and the Stanford tagger are spec-
ified as one type of external annotations (see Figure
2). Another annotation type is derived from HMM-
based word alignments and the transliteration model
described in Section 4. They provide two kinds of
alignment links between English and foreign tokens:
one based on the HMM-word alignments (poste-
rior probability of the link in both directions), and
another based on different character-based distance
metrics between transliterations of foreign words
and English words. The transliteration model and
distance metrics were described in Section 4 as well.
For the example Bulgarian correspondent of ?Split?
in the figure, the English ?Split? is linked to it ac-
cording to both the forward and backward HMM,
and according to two out of the three transliteration
distance measures. A third annotation type is au-
tomatically derived links between foreign candidate
entity strings (sequences of tokens) and best corre-
sponding English candidate entities. The candidate
English entities are defined by the union of entities
proposed by the Wiki-based taggers and the Stan-
ford tagger. Note that these English candidate en-
tities can be overlapping and inconsistent without
harming the model. We link foreign candidate seg-
ments with English candidate entities based on the
projection model described in Section 4 and trained
on the same data. The projection model scores every
source-target entity pair and selects the best source
for each target candidate entity. For our example
target segment, the corresponding source candidate
entity is ?Split?, labeled GPE by the local+global
Wiki-tagger and by the global Wiki-tagger.
The features are grouped into three categories:
Group 1. Foreign Wiki-based tagger features.
These features look at target segments and extract
indicators of whether the label of the segment agrees
with the label assigned by the local, global, and/or
local+global wiki tagger. For the example segment
from the sentence in Figure 1, since neither the local
nor global tagger have assigned a label GPE, the first
three features have value zero. In addition to tags on
the whole segment, we look at tag combinations for
individual words within the segment as well as two
words to the left and right outside the segment. In
the first section in Table 4 we can see several feature
types and and their values for our example.
Group 2. Foreign surface-based features. These
features look at orthographic properties of the words
and distinguish several word types. The types are
based on capitalization and also distinguish numbers
and punctuation. In addition, we make use of word-
clusters generated by JCluster. 1
We look at properties of the individual words as
well as the concatenation for all words in the seg-
ment. In addition, there are features for words two
words to the left and two words to the right outside
the segment. The second section in the Table shows
several features of this type with their values.
Group 3. Label match between English and
aligned foreign entities. These features look at
the linked English segment for the candidate tar-
get segment and compare the tags assigned to the
English segment by the different English taggers to
the candidate target label. In addition to segment-
level comparisons, they also look at tag assignments
for individual source tokens linked to the individual
target tokens (by word alignment and transliteration
links). The last section in the Table contains sample
features with their values. The feature SOURCE-E-
WIKI-TAG-MATCH looks at whether the correspond-
ing source entity has the same local+global Wiki-
tagger assigned tag as the candidate target entity.
The next two features look at the Stanford tagger
and the global Wiki-tagger. The real-valued fea-
tures like SCORE-SOURCE-E-WIKI-TAG-MATCH re-
turn the score of the matching between the source
and target candidate entities (according to the pro-
jection model), if the labels match. In this way, more
confident matchings can impact the target tags more
than less confident ones.
5.2 Experimental results
Our main results are listed in Table 5. We perform
10-fold cross-validation as in the projection experi-
ments. The best Wiki-based and projection models
are listed as baselines at the bottom of the table.
1Software distributed by Joshua Goodman
http://research.microsoft.com/en-us/downloads/0183a49d-
c86c-4d80-aa0d-53c97ba7350a/default.aspx.
700
Method English-Bulgarian English-Korean
Prec Rec F1 Prec Rec F1
MONO 86.7 79.4 82.9 89.1 57.1 69.6
BI 90.1 83.3 86.6 88.6 79.8 84.0
MONO-ALL 94.7 86.2 90.3 90.2 84.3 87.2
BI-ALL-WT 95.7 87.6 91.5 92.4 87.6 89.9
BI-ALL 96.4 89.4 92.8 94.7 87.9 91.2
Wiki-tagger 86.8 79.9 83.2 89.5 57.3 69.9
PM+WF 73.6 81.3 77.2 87.6 74.9 80.8
Table 5: English-Bulgarian and English-Korean semi-CRF tagger performance.
Feature Description Example Value
WIKI-TAG-MATCH 0
WIKI-GLOBAL-TAG-MATCH 0
WIKIGLOBAL-POSSIBLE-TAG 0
WIKI-TAG&LABEL NONE&GPE
WIKI-GLOBAL-TAG&LABEL NONE&GPE
FIRST-WORD-CAP 1
CONTAINS-NUMBER 0
PREV-WORD-CAP 0
WORD-TYPE&LABEL Xxxx&GPE
WORD-CLUSTER& LABEL 101&GPE
SEGMENT-WORD-TYPE&LABEL Xxxx&GPE
SEGMENT-WORD-CLUSTER&LABEL Xxxx&GPE
SOURCE-E-WIKI-TAG-MATCH 1
SOURCE-E-STANFORD-TAG-MATCH 0
SOURCE-E-WIKI-GLOBAL-TAG-MATCH 1
SOURCE-E-POSSIBLE-GLOBAL 1
SOURCE-E-ALL-TAG-MATCH 0
SOURCE-W-FWA-TAG & LABEL GPE & GPE
SOURCE-W-BWA-TAG & LABEL GPE & GPE
SCORE-SOURCE-E-WIKI-TAG-MATCH -0.009
SCORE-SOURCE-E-GLOBAL-TAG-MATCH -0.009
SCORE-SOURCE-E-STANFORD-TAG-MATCH -1
Table 4: Features with example values.
We look at performance using four sets of fea-
tures: (i) Monolingual Wiki-tagger based, using
only the features in Group 1 (MONO); (ii) Bilingual
label match and Wiki-tagger based, using features
in Groups 1 and 3 (BI); (iii) Monolingual all, us-
ing features in Groups 1 and 2 (MONO-ALL), and
(iv) Bilingual all, using all features (BI-ALL). Ad-
ditionally, we report performance of the full bilin-
gual model with all features, but when English can-
didate entities are generated only according to the
local+global Wiki-taggger (BI-ALL-WT).
The main results show that the full semi-CRF
model greatly outperforms the baseline projection
and Wiki-taggers. For Bulgarian, the F-measure of
the full model is 92.8 compared to the best base-
line result of 83.2. For Korean, the F-measure of the
semi-CRF is 91.2, more than 10 points higher than
the performance of the projection model.
Within the semi-CRF model, the contribution of
English sentence context was substantial, leading to
2.5 point increase in F-measure for Bulgarian (92.8
versus 90.3 F-measure), and 4.0 point increase for
Korean (91.2 versus 87.2).
The additional gain due to considering candidate
source entities generated from all English taggers
was 1.3 F-measure points for both language pairs
(comparing models BI-ALL and BI-ALL-WT).
If we restrict the semi-CRF to use only features
similar to the ones used by the projection model, we
still obtain performance much better than that of the
projection model: comparing BI to the projection
model, we see gains of 9.4 points for Bulgarian, and
4 points for Korean. This is due to the fact that the
semi-CRF is able to relax the assumption of one-to-
one correspondence between source and target enti-
ties, and can effectively combine information from
multiple source and target taggers.
We should note that the proposed method can only
tag foreign sentences in English-foreign sentence
pairs. The next step for this work is to train mono-
lingual NE taggers for the foreign languages, which
can work on text within or outside of Wikipedia.
Preliminary results show performance of over 80 F-
measure for such monolingual models.
6 Related Work
As discussed throughout the paper, our model builds
upon prior work on Wikipedia metadata-based NE
tagging (Richman and Schone, 2008) and cross-
lingual projection for named entities (Feng et al,
2004). Other interesting work on aligning named
entities in two languages is reported in (Huang and
Vogel, 2002; Moore, 2003).
Our direct semi-CRF tagging approach is related
to bilingual labeling models presented in previous
701
work (Burkett et al, 2010a; Smith and Smith, 2004;
Snyder and Barzilay, 2008). All of these models
jointly label aligned source and target sentences. In
contrast, our model is not concerned with tagging
English sentences but only tags foreign sentences in
the context of English sentences. Compared to the
joint log-linear model of Burkett et al (2010a), our
semi-CRF approach does not require enumeration of
n-best candidates for the English sentence and is not
limited to n-best candidates for the foreign sentence.
It enables the use of multiple unweighted and over-
lapping entity annotations on the English sentence.
7 Conclusions
In this paper we showed that using resources from
Wikipedia, it is possible to combine metadata-based
approaches and projection-based approaches for in-
ducing named entity annotations for foreign lan-
guages. We presented a direct semi-CRF tagging
model for labeling foreign sentences in parallel sen-
tence pairs, which outperformed projection by more
than 10 F-measure points for Bulgarian and Korean.
References
David Burkett, John Blitzer, and Dan Klein. 2010a.
Joint parsing and alignment with weakly synchronized
grammars. In Proceedings of NAACL.
David Burkett, Slav Petrov, John Blitzer, and Dan
Klein. 2010b. Learning better monolingual models
with unannotated bilingual text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 46?54, Uppsala, Sweden,
July. Association for Computational Linguistics.
Colin Cherry and Hisami Suzuki. 2009. Discrimina-
tive substring decoding for transliteration. In EMNLP,
pages 1066?1075.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600?609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A new
approach for English-Chinese named entity alignment.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing EMNLP, pages
372?379.
Jenny Finkel, Trond Grenager, and Christopher D. Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
ACL.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL, pages 127?133.
Robert C. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In EACL.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Alexander E. Richman and Patrick Schone. 2008.
Mining wiki resources for multilingual named entity
recognition. In ACL.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In In Advances in Neural Information Pro-
cessing Systems 17, pages 1185?1192.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
markov conditional random fields for information ex-
traction. In In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004).
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using English
to parse Korean. In EMNLP.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In HLT,
pages 403?411, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Crosslin-
gual propagation for morphological analysis. In AAAI.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In HLT.
702
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 406?411,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Non-linear Features for Machine Translation Using Gradient
Boosting Machines
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Byung-Gyu Ahn?
Johns Hopkins University
Baltimore, MD 21218
bahn@cs.jhu.edu
Abstract
In this paper we show how to auto-
matically induce non-linear features for
machine translation. The new features
are selected to approximately maximize
a BLEU-related objective and decompose
on the level of local phrases, which guar-
antees that the asymptotic complexity of
machine translation decoding does not in-
crease. We achieve this by applying gra-
dient boosting machines (Friedman, 2000)
to learn newweak learners (features) in the
form of regression trees, using a differen-
tiable loss function related to BLEU. Our
results indicate that small gains in perfor-
mance can be achieved using this method
but we do not see the dramatic gains ob-
served using feature induction for other
important machine learning tasks.
1 Introduction
The linear model for machine translation (Och and
Ney, 2002) has become the de-facto standard in
the field. Recently, researchers have proposed a
large number of additional features (TaroWatan-
abe et al, 2007; Chiang et al, 2009) and param-
eter tuning methods (Chiang et al, 2008b; Hop-
kins and May, 2011; Cherry and Foster, 2012)
which are better able to scale to the larger pa-
rameter space. However, a significant feature en-
gineering effort is still required from practition-
ers. When a linear model does not fit well, re-
searchers are careful to manually add important
feature conjunctions, as for example, (Daume? III
and Jagarlamudi, 2011; Clark et al, 2012). In the
related field of web search ranking, automatically
learned non-linear features have brought dramatic
improvements in quality (Burges et al, 2005; Wu
?This research was conducted during the author?s intern-
ship at Microsoft Research
et al, 2010). Here we adapt the main insights of
such work to the machine translation setting and
share results on two language pairs.
Some recent works have attempted to relax the
linearity assumption on MT features (Nguyen et
al., 2007), by defining non-parametric models on
complete translation hypotheses, for use in an n-
best re-ranking setting. In this paper we develop
a framework for inducing non-linear features in
the form of regression decision trees, which de-
compose locally and can be integrated efficiently
in decoding. The regression trees encode non-
linear feature combinations of the original fea-
tures. We build on the work by Friedman (2000)
which shows how to induce features to minimize
any differentiable loss function. In our applica-
tion the features are regression decision trees, and
the loss function is the pairwise ranking log-loss
from the PRO method for parameter tuning (Hop-
kins and May, 2011). Additionally, we show how
to design the learning process such that the in-
duced features are local on phrase-pairs and their
language model and reordering context, and thus
can be incorporated in decoding efficiently.
Our results using re-ranking on two language
pairs show that the feature induction approach can
bring small gains in performance. Overall, even
though the method shows some promise, we do
not see the dramatic gains that have been seen for
the web search ranking task (Wu et al, 2010). Fur-
ther improvements in the original feature set and
the induction algorithm, as well as full integration
in decoding are needed to potentially result in sub-
stantial performance improvements.
2 Feature learning using gradient
boosting machines
In the linear model for machine translation, the
scores of translation hypotheses are weighted
sums of a set of input features over the hypotheses.
406
Figure 1: A Bulgarian source sentence (meaning ?the
conference in Bulgaria?, together with a candidate transla-
tion. Local and global features for the translation hypoth-
esis are shown. f0=smoothed relative frequency estimate
of log p(s|t); f1=lexical weighting estimate of log p(s|t);
f2=joint count of the phrase-pair; f3=sum of language model
log-probabilities of target phrase words given context.
For a set of features f1(h), . . . , fL(h) and weights
for these features ?1, . . . , ?L, the hypothesis
scores are defined as: F (h) = ?l=1...L ?lfl(h).
In current state-of-the-art models, the features
fl(h) decompose locally on phrase-pairs (with
language model and reordering context) inside the
hypotheses. This enables hypothesis recombina-
tion during machine translation decoding, leading
to faster and more accurate search. As an exam-
ple, Figure 1 shows a Bulgarian source sentence
(spelled phonetically in Latin script) and a can-
didate translation. Two phrase-pairs are used to
compose the translation, and each phrase-pair has
a set of local feature function values. A mini-
mal set of four features is shown, for simplicity.
We can see that the hypothesis-level (global) fea-
ture values are sums of phrase-level (local) feature
values. The score of a translation given feature
weights ? can be computed either by scoring the
phrase-pairs and adding the scores, or by scoring
the complete hypothesis by computing its global
feature values. The local feature values do look at
some limited context outside of a phrase-pair, to
compute language model scores and re-ordering
scores; therefore we say that the features are de-
fined on phrase-pairs in context.
We start with such a state-of-the-art linear
model with decomposable features and show how
we can automatically induce additional features.
The new features are also locally decomposable,
so that the scores of hypotheses can be computed
as sums of phrase-level scores. The new local
phrase-level features are non-linear combinations
of the original phrase-level features.
Figure 2: Example of two decision tree features. The left
decision tree has linear nodes and the right decision tree has
constant nodes.
2.1 Form of induced features
We will use the example in Figure 1 to introduce
the form of the new features we induce and to give
an intuition of why such features might be useful.
The new features are expressed by regression de-
cision trees; Figure 2 shows two examples.
One intuition we might have is that, if a phrase
pair has been seen very few times in the training
corpus (for example, the first phrase pair P1 in the
Figure has been seen only one time f2 = 1), we
would like to trust its lexical weighting channel
model score f1 more than its smoothed relative-
frequency channel estimate f0. The first regres-
sion tree feature h1 in Figure 2 captures this in-
tuition. The feature value for a phrase-pair of
this feature is computed as follows: if f2 ?
2, then h1(f0, f1, f2, f3) = 2 ? f1; otherwise,
h1(f0, f1, f2, f3) = f1. The effect of this new
feature h1 is to boost the importance of the lexi-
cal weighting score for phrase-pairs of low joint
count. More generally, the regression tree fea-
tures we consider have either linear or constant
leaf nodes, and have up to 8 leaves. Deeper trees
can capture more complex conditions on several
input feature values. Each non-leaf node performs
a comparison of some input feature value to a
threshold and each leaf node (for linear nodes) re-
turns the value of some input feature multiplied
by some factor. For a given regression tree with
linear nodes, all leaf nodes are expressions of the
same input feature but have different coefficients
for it (for example, both leaf nodes of h1 return
affine functions of the input feature f1). A deci-
sion tree feature with constant-valued leaf nodes
is illustrated by the right-hand-side tree in Figure
2. For these decision trees, the leaf nodes contain
a constant, which is specific to each leaf. These
kinds of trees can effectively perform conjunctions
of several binary-valued input feature functions; or
they can achieve binning of real-values features to-
gether with conjunctions over binned values.
407
Having introduced the form of the new features
we learn, we now turn to the methodology for in-
ducing them. We apply the framework of gradient
boosting for decision tree weak learners (Fried-
man, 2000). To define the framework, we need
to introduce the original input features, the differ-
entiable loss function, and the details of the tree
growing algorithm. We discuss these in turn next.
2.2 Initial features
Our baseline MT system uses relative frequency
and lexical weighting channel model weights, one
or more language models, distortion penalty, word
count, phrase count, and multiple lexicalized re-
ordering weights, one for each distortion type. We
have around 15 features in this base feature set.
We further expand the input set of features to in-
crease the possibility that useful feature combi-
nations could be found by our feature induction
method. The large feature set contains around
190 features, including source and target word
count features, joint phrase count, lexical weight-
ing scores according to alternative word-alignment
model ran over morphemes instead of words, in-
dicator lexicalized features for insertion and dele-
tion of the top 15 words in each language, cluster-
based insertion and deletion indicators using hard
word clustering, and cluster based signatures of
phrase-pairs. This is the feature set we use as a
basis for weak learner induction.
2.3 Loss function
We use a pair-wise ranking log-loss as in the
PRO parameter tuning method (Hopkins and May,
2011). The loss is defined by comparing the model
scores of pairs of hypotheses hi and hj where
the BLEU score of the first hypothesis is greater
than the BLEU score of the second hypothesis by
a specified threshold. 1
We denote the sentences in a corpus as
s1, s2, . . . , sN . For each sentence sn, we de-
note the ordered selected pairs of hypotheses as
[hni1 , hnj1 ], . . . , [hniK , h
n
jK ]. The loss-function ? isdefined in terms of the hypothesis model scores
1In our implementation, for each sentence, we sample
10, 000 pairs of translations and accept a pair of transla-
tions for use with probability proportional to the BLEU score
difference, if that difference is greater than the threshold of
0.04. The top K = 100 or K = 300 hypothesis pairs with
the largest BLEU difference are selected for computation of
the loss. We compute sentence-level BLEUscores by add-?
smoothing of the match counts for computation of n-gram
precision. The ? and K parameters are chosen via cross-
validation.
1: F0(x) = argmin? ?(F (x, ?))
2: for m = 1toM do
3: yr = ?[??(F (x))?F (xr) ]F (x)=Fm?1(x), r =
1 . . . R
4: ?m = argmin?,?
?R
r=1[yr ? ?h(xi;?)]2
5: ?m = argmin? ?(Fm?1(x) + ?h(x;?m)
6: Fm(x) = Fm?1(x) + ?mh(x;?m)
7: end for
Figure 3: A gradient boosting algorithm for local
feature functions.
F (h) as follows: ?n=1...N
?
k=1...K log(1 +
eF (h
n
jk
)?F (hnik )).
The idea of the gradient boosting method is to
induce additional features by computing a func-
tional gradient of the target loss function and itera-
tively selecting the next weak learner (feature) that
is most parallel to the negative gradient. Since we
want to induce features such that the hypothesis
scores decompose locally, we need to formulate
our loss function as a function of local phrase-pair
in context scores. Having the model scores de-
compose locally means that the scores of hypothe-
ses F (h) decompose as F (h) = ?pr?h F (pr)),where by pr ? h we denote the enumeration over
phrase pairs in context that are parts of h. If xr de-
notes the input feature vector for a phrase-pair in
context pr, the score of this phrase-pair can be ex-
pressed as F (xr). Appendix A expresses the pair-
wise log-loss as a function of the phrase scores.
We are now ready to introduce the gradient
boosting algorithm, summarized in Figure 3. In
the first step of the algorithm, we start by set-
ting the phrase-pair in context scoring function
F0(x) as a linear function of the input feature val-
ues, by selecting the feature weights ? to min-
imize the PRO loss ?(F0(x)) as a function of
?. The initial scores have the form F0(x) =?
l=1...L ?lfl(x).This is equivalent to using the
(Hopkins and May, 2011) method of parameter
tuning for a fixed input feature set and a linear
model. We used LBFGS for the optimization in
Line 1. Then we iterate and induce a new de-
cision tree weak learner h(x;?m) like the exam-
ples in Figure 2 at each iteration. The parame-
ter vectors ?m encode the topology and parame-
ters of the decision trees, including which feature
value is tested at each node, what the compari-
son cutoffs are, and the way to compute the val-
ues at the leaf nodes. After a new decision tree
408
Language Train Dev-Train Dev-Select Test
Chs-En 999K NIST02+03 2K NIST05
Fin-En 2.2M 12K 2K 4.8K
Table 1: Data sets for the two language pairs Chinese-
English and Finnish-English.
Chs-En Fin-EnFeatures Tune Dev-Train Test Dev-Train Testbase MERT 31.3 30.76 49.8 51.31base PRO 31.1 31.16 49.7 51.56large PRO 31.8 31.44 49.8 51.77boost-global PRO 31.8 31.30 50.0 51.87boost-local PRO 31.8 31.44 50.1 51.95
Table 2: Results for the two language pairs using different
weight tuning methods and feature sets.
h(x;?m) is induced, it is treated as new feature
and a linear coefficient ?m for that feature is set
by minimizing the loss as a function of this pa-
rameter (Line 5). The new model scores are set as
the old model scores plus a weighted contribution
from the new feature (Line 6). At the end of learn-
ing, we have a linear model over the input features
and additional decision tree features. FM (x) =?
l=1...L ?lfl(x) +
?
m=1...M ?mh(x;?m). The
most time-intensive step of the algorithm is the se-
lection of the next decision tree h. This is done
by first computing the functional gradient of the
loss with respect to the phrase scores F (xr) at the
point of the current model scores Fm?1(xr). Ap-
pendix A shows a derivation of this gradient. We
then induce a regression tree using mean-square-
error minimization, setting the direction given by
the negative gradient as a target to be predicted us-
ing the features of each phrase-pair in context in-
stance. This is shown as the setting of the ?m pa-
rameters by mean-squared-error minimization in
Line 4 of the algorithm. The minimization is done
approximately by a standard greedy tree-growing
algorithm (Breiman et al, 1984). When we tune
weights to minimize the loss, such as the weights
? of the initial features, or the weights ?m of in-
duced learners, we also include an L2 penalty on
the parameters, to prevent overfitting.
3 Experiments
We report experimental results on two language
pairs: Chinese-English, and Finnish-English. Ta-
ble 1 summarizes statistics about the data. For
each language pair, we used a training set (Train)
for extracting phrase tables and language models,
a Dev-Train set for tuning feature weights and in-
ducing features, a Dev-Select set for selecting hy-
perparameters of PRO tuning and selecting a stop-
ping point and other hyperparameters of the boost-
ing method, and a Test set for reporting final re-
sults. For Chinese-English, the training corpus
consists of approximately one million sentence
pairs from the FBIS and HongKong portions of
the LDC data for the NIST MT evaluation and the
Dev-Train and Test sets are from NIST competi-
tions. The MT system is a phrasal system with a 4-
gram language model, trained on the Xinhua por-
tion of the English Gigaword corpus. The phrase
table has maximum phrase length of 7 words on
either side. For Finnish-English we used a data-
set from a technical domain of software manuals.
For this language pair we used two language mod-
els: one very large model trained on billions of
words, and another language model trained from
the target side of the parallel training set. We re-
port performance using the BLEU-SBP metric pro-
posed in (Chiang et al, 2008a). This is a vari-
ant of BLEU (Papineni et al, 2002) with strict
brevity penalty, where a long translation for one
sentence can not be used to counteract the brevity
penalty for another sentence with a short transla-
tion. Chiang et al (2008a) showed that this metric
overcomes several undesirable properties of BLEU
and has better correlation with human judgements.
In our experiments with different feature sets and
hyperparameters we observed more stable results
and better correlation of Dev-Train, Dev-Select,
and Test results using BLEU-SBP. For our exper-
iments, we first trained weights for the base fea-
ture sets described in Section 2.2 using MERT. We
then decoded the Dev-Train, Dev-Select, and Test
datasets, generating 500-best lists for each set. All
results in Table 2 report performance of re-ranking
on these 500-best lists using different feature sets
and parameter tuning methods.
The baseline (base feature set) performance us-
ing MERT and PRO tuning on the two language
pairs is shown on the first two lines. In line with
prior work, PRO tuning achieves a bit lower scores
on the tuning set but higher scores on the test set,
compared to MERT. The large feature set addi-
tionally contains over 170 manually specified fea-
tures, described in Section 2.2. It was infeasible
to run MERT training on this feature set. The test
set results using PRO tuning for the large set are
about a quarter of a BLEU-SBP point higher than
the results using the base feature set on both lan-
guage pairs. Finally, the last two rows show the
performance of the gradient boosting method. In
409
addition to learning locally decomposable features
boost-local, we also implemented boost-global,
where we are learning combinations of the global
feature values and lose decomposability. The fea-
tures learned by boost-global can not be com-
puted exactly on partial hypotheses in decoding
and thus this method has a speed disadvantage, but
we wanted to compare the performance of boost-
local and boost-global on n-best list re-ranking
to see the potential accuracy gain of the two meth-
ods. We see that boost-local is slightly better in
performance, in addition to being amenable to ef-
ficient decoder integration.
The gradient boosting results are mixed; for
Finnish-English, we see around .2 gain of the
boost-local model over the large feature set.
There is no improvement on Chinese-English, and
the boost-global method brings slight degrada-
tion. We did not see a large difference in perfor-
mance among models using different decision tree
leaf node types and different maximum numbers
of leaf nodes. The selected boost-local model
for FIN-ENU used trees with maximum of 2 leaf
nodes and linear leaf values; 25 new features were
induced before performance started to degrade
on the Dev-Select set. The induced features for
Finnish included combinations of language model
and channel model scores, combinations of word
count and channel model scores, and combina-
tions of channel and lexicalized reordering scores.
For example, one feature increases the contribu-
tion of the relative frequency channel score for
phrases with many target words, and decreases the
channel model contribution for shorter phrases.
The best boost-local model for Chs-Enu used
trees with a maximum of 2 constant-values leaf
nodes, and induced 24 new tree features. The fea-
tures effectively promoted and demoted phrase-
pairs in context based on whether an input fea-
ture?s value was smaller than a determined cutoff.
In conclusion, we proposed a new method to
induce feature combinations for machine transla-
tion, which do not increase the decoding complex-
ity. There were small improvements on one lan-
guage pair in a re-ranking setting. Further im-
provements in the original feature set and the in-
duction algorithm, as well as full integration in de-
coding are needed to result in substantial perfor-
mance improvements.
This work did not consider alternative ways
of generating non-linear features, such as taking
products of two or more input features. It would
be interesting to compare such alternatives to the
regression tree features we explored.
References
Leo Breiman, Jerome Friedman, Charles J. Stone, and
R.A. Olshen. 1984. Classification and Regression
Trees. Chapman and Hall.
Chris Burges, Tal Shaked, Erin Renshaw, Matt Deeds,
Nicole Hamilton, and Greg Hullender. 2005. Learn-
ing to rank using gradient descent. In ICML.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. InHLT-
NAACL.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of trans-
lation metrics for improved evaluation and efficient
algorithms. In EMNLP.
David Chiang, Yuval Marton, and Philp Resnik. 2008b.
Online large margin training of syntactic and struc-
tural translation features. In EMNLP.
D. Chiang, W. Wang, and K. Knight. 2009. 11,001
new features for statistical machine translation. In
NAACL.
Jonathan Clark, Alon Lavie, and Chris Dyer. 2012.
One system, many domains: Open-domain statisti-
cal machine translation via feature augmentation. In
AMTA.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In ACL.
Jerome H. Friedman. 2000. Greedy function approx-
imation: A gradient boosting machine. Annals of
Statistics, 29:1189?1232.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
Patrick Nguyen, Milind Mahajan, and Xiaodong He.
2007. Training non-parametric features for statis-
tical machine translation. In Second Workshop on
Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL.
TaroWatanabe, Jun Suzuki, Hajime Tsukuda, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In EMNLP.
410
QiangWu, Christopher J. Burges, Krysta M. Svore, and
Jianfeng Gao. 2010. Adapting boosting for infor-
mation retrieval measures. Information Retrieval,
13(3), June.
4 Appendix A: Derivation of derivatives
Here we express the loss as a function of phrase-
level in context scores and derive the derivative of
the loss with respect to these scores.
Let us number all phrase-pairs in context in
all hypotheses in all sentences as p1, . . . , pR and
denote their input feature vectors as x1, . . . ,xR.
We will use F (pr) and F (xr) interchange-
ably, because the score of a phrase-pair in
context is defined by its input feature vec-
tor. The loss ?(F (xr)) is expressed as follows:
?N
n=1
?K
k=1 log(1 + e
?
pr?hnjk
F (xr)?
?
pr?hnik
F (xr)).
Next we derive the derivatives of the loss
?(F (x)) with respect to the phrase scores. Intu-
itively, we are treating the scores we want to learn
as parameters for the loss function; thus the loss
function has a huge number of parameters, one
for each instance of each phrase pair in context in
each translation. We ask the loss function if these
scores could be set in an arbitrary way, what di-
rection it would like to move them in to be mini-
mized. This is the direction given by the negative
gradient.
Each phrase-pair in context pr occurs in exactly
one hypothesis h in one sentence. It is possible
that two phrase-pairs in context share the same set
of input features, but for ease of implementation
and exposition, we treat these as different train-
ing instances. To express the gradient with respect
to F (xr) we therefore need to focus on the terms
of the loss from a single sentence and to take into
account the hypothesis pairs [hj,k, hi,k] where the
left or the right hypothesis is the hypothesis h con-
taining our focus phrase pair pr. ??(F (x))?F (xr) is ex-pressed as:
= ?k:h=hik ?
e
?
pr?hnjk
F (xr)?
?
pr?hnik
F (xr)
1+e
?
pr?hnjk
F (xr)?
?
pr?hnik
F (xr)
+ ?k:h=hjk
e
?
pr?hnjk
F (xr)?
?
pr?hnik
F (xr)
1+e
?
pr?hnjk
F (xr)?
?
pr?hnik
F (xr)
Since in the boosting step we induce a deci-
sion tree to fit the negative gradient, we can see
that the feature induction algorithm is trying to in-
crease the scores of phrases that occur in better
hypotheses (the first hypothesis in each pair), and
it increases the scores more if weaker hypotheses
have higher advantage; it is also trying to decrease
the scores of phrases in weaker hypotheses that are
currently receiving high scores.
411
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 676?686,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Graph-based Semi-Supervised Learning of Translation Models from
Monolingual Data
Avneesh Saluja
?
Carnegie Mellon University
Pittsburgh, PA 15213, USA
avneesh@cs.cmu.edu
Hany Hassan, Kristina Toutanova, Chris Quirk
Microsoft Research
Redmond, WA 98502, USA
hanyh,kristout,chrisq@microsoft.com
Abstract
Statistical phrase-based translation learns
translation rules from bilingual corpora,
and has traditionally only used monolin-
gual evidence to construct features that
rescore existing translation candidates. In
this work, we present a semi-supervised
graph-based approach for generating new
translation rules that leverages bilingual
and monolingual data. The proposed tech-
nique first constructs phrase graphs using
both source and target language mono-
lingual corpora. Next, graph propaga-
tion identifies translations of phrases that
were not observed in the bilingual cor-
pus, assuming that similar phrases have
similar translations. We report results
on a large Arabic-English system and a
medium-sized Urdu-English system. Our
proposed approach significantly improves
the performance of competitive phrase-
based systems, leading to consistent im-
provements between 1 and 4 BLEU points
on standard evaluation sets.
1 Introduction
Statistical approaches to machine translation
(SMT) use sentence-aligned, parallel corpora to
learn translation rules along with their probabil-
ities. With large amounts of data, phrase-based
translation systems (Koehn et al, 2003; Chiang,
2007) achieve state-of-the-art results in many ty-
pologically diverse language pairs (Bojar et al,
2013). However, the limiting factor in the suc-
cess of these techniques is parallel data availabil-
ity. Even in resource-rich languages, learning re-
liable translations of multiword phrases is a chal-
lenge, and an adequate phrasal inventory is crucial
?
This work was done while the first author was interning
at Microsoft Research
for effective translation. This problem is exacer-
bated in the many language pairs for which par-
allel resources are either limited or nonexistent.
While parallel data is generally scarce, monolin-
gual resources exist in abundance and are being
created at accelerating rates. Can we use monolin-
gual data to augment the phrasal translations ac-
quired from parallel data?
The challenge of learning translations from
monolingual data is of long standing interest,
and has been approached in several ways (Rapp,
1995; Callison-Burch et al, 2006; Haghighi et
al., 2008; Ravi and Knight, 2011). Our work in-
troduces a new take on the problem using graph-
based semi-supervised learning to acquire trans-
lation rules and probabilities by leveraging both
monolingual and parallel data resources. On the
source side, labeled phrases (those with known
translations) are extracted from bilingual corpora,
and unlabeled phrases are extracted from mono-
lingual corpora; together they are embedded as
nodes in a graph, with the monolingual data de-
termining edge strengths between nodes (?2.2).
Unlike previous work (Irvine and Callison-Burch,
2013a; Razmara et al, 2013), we use higher order
n-grams instead of restricting to unigrams, since
our approach goes beyond OOV mitigation and
can enrich the entire translation model by using
evidence from monolingual text. This enhance-
ment alone results in an improvement of almost
1.4 BLEU points. On the target side, phrases ini-
tially consisting of translations from the parallel
data are selectively expanded with generated can-
didates (?2.1), and are embedded in a target graph.
We then limit the set of translation options for
each unlabeled source phrase (?2.3), and using
a structured graph propagation algorithm, where
translation information is propagated from la-
beled to unlabeled phrases proportional to both
source and target phrase similarities, we esti-
mate probability distributions over translations for
676
Source! Target!
el gato!
los gatos!
un gato! cat!
the cat! the cats!
a cat!
Target! Prob.!
the cat! 0.7!
cat! 0.15!
?! ?!
felino!
canino! el perro!
Target! Prob.!
canine! 0.6!
dog! 0.3!
?! ?!
Target! Prob.!
the cats! 0.8!
cats! 0.1!
?! ?!
Target! Prob.!
the dog! 0.9!
dog! 0.05!
?! ?!
canine!
dog!
the dog!
catlike!
Figure 1: Example source and target graphs used in our approach. Labeled phrases on the source side are black (with their
corresponding translations on the target side also black); unlabeled and generated (?2.1) phrases on the source and target sides
respectively are white. Labeled phrases also have conditional probability distributions defined over target phrases, which are
extracted from the parallel corpora.
the unlabeled source phrases (?2.4). The addi-
tional phrases are incorporated in the SMT sys-
tem through a secondary phrase table (?2.5). We
evaluated the proposed approach on both Arabic-
English and Urdu-English under a range of sce-
narios (?3), varying the amount and type of mono-
lingual corpora used, and obtained improvements
between 1 and 4 BLEU points, even when using
very large language models.
2 Generation & Propagation
Our goal is to obtain translation distributions for
source phrases that are not present in the phrase
table extracted from the parallel corpus. Both par-
allel and monolingual corpora are used to obtain
these probability distributions over target phrases.
We assume that sufficient parallel resources ex-
ist to learn a basic translation model using stan-
dard techniques, and also assume the availability
of larger monolingual corpora in both the source
and target languages. Although our technique ap-
plies to phrases of any length, in this work we con-
centrate on unigram and bigram phrases, which
provides substantial computational cost savings.
Monolingual data is used to construct separate
similarity graphs over phrases (word sequences),
as illustrated in Fig. 1. The source similarity graph
consists of phrase nodes representing sequences of
words in the source language. If a source phrase
is found in the baseline phrase table it is called a
labeled phrase: its conditional empirical probabil-
ity distribution over target phrases (estimated from
the parallel data) is used as the label, and is sub-
sequently never changed. Otherwise it is called an
unlabeled phrase, and our algorithm finds labels
(translations) for these unlabeled phrases, with the
help of the graph-based representation. The la-
bel space is thus the phrasal translation inventory,
and like the source side it can also be represented
in terms of a graph, initially consisting of target
phrase nodes from the parallel corpus.
For the unlabeled phrases, the set of possible
target translations could be extremely large (e.g.,
all target language n-grams). Therefore, we first
generate and fix a list of possible target transla-
tions for each unlabeled source phrase. We then
propagate by deriving a probability distribution
over these target phrases using graph propagation
techniques. Next, we will describe the generation,
graph construction and propagation steps.
2.1 Generation
The objective of the generation step is to popu-
late the target graph with additional target phrases
for all unlabeled source phrases, yielding the full
set of possible translations for the phrase. Prior to
generation, one phrase node for each target phrase
occurring in the baseline phrase table is added to
the target graph (black nodes in Fig. 1?s target
graph). We only consider target phrases whose
source phrase is a bigram, but it is worth noting
that the target phrases are of variable length.
The generation component is based on the ob-
servation that for structured label spaces, such as
translation candidates for source phrases in SMT,
even similar phrases have slightly different labels
(target translations). The exponential dependence
677
of the sizes of these spaces on the length of in-
stances is to blame. Thus, the target phrase inven-
tory from the parallel corpus may be inadequate
for unlabeled instances. We therefore need to en-
rich the target or label space for unknown phrases.
A na??ve way to achieve this goal would be to ex-
tract all n-grams, from n = 1 to a maximum n-
gram order, from the monolingual data, but this
strategy would lead to a combinatorial explosion
in the number of target phrases.
Instead, by intelligently expanding the target
space using linguistic information such as mor-
phology (Toutanova et al, 2008; Chahuneau et al,
2013), or relying on the baseline system to gener-
ate candidates similar to self-training (McClosky
et al, 2006), we can tractably propose novel trans-
lation candidates (white nodes in Fig. 1?s target
graph) whose probabilities are then estimated dur-
ing propagation. We refer to these additional can-
didates as ?generated? candidates.
To generate new translation candidates using
the baseline system, we decode each unlabeled
source bigram to generate its m-best translations.
This set of candidate phrases is filtered to include
only n-grams occurring in the target monolingual
corpus, and helps to prune passed-through OOV
words and invalid translations. To generate new
translation candidates using morphological infor-
mation, we morphologically segment words into
prefixes, stem, and suffixes using linguistic re-
sources. We assume that a morphological ana-
lyzer which provides context-independent analysis
of word types exists, and implements the functions
STEM(f ) and STEM(e) for source and target word
types. Based on these functions, source and target
sequences of words can be mapped to sequences
of stems. The morphological generation step adds
to the target graph all target word sequences from
the monolingual data that map to the same stem
sequence as one of the target phrases occurring in
the baseline phrase table. In other words, this step
adds phrases that are morphological variants of ex-
isting phrases, differing only in their affixes.
2.2 Graph Construction
At this stage, there exists a list of source bigram
phrases, both labeled and unlabeled, as well as a
list of target language phrases of variable length,
originating from both the phrase table and the gen-
eration step. To determine pairwise phrase similar-
ities in order to embed these nodes in their graphs,
we utilize the monolingual corpora on both the
source and target sides to extract distributional
features based on the context surrounding each
phrase. For a phrase, we look at the pwords before
and the p words after the phrase, explicitly distin-
guishing between the two sides, but not distance
(i.e., bag of words on each side). Co-occurrence
counts for each feature (context word) are accu-
mulated over the monolingual corpus, and these
counts are converted to pointwise mutual infor-
mation (PMI) values, as is standard practice when
computing distributional similarities. Cosine sim-
ilarity between two phrases? PMI vectors is used
for similarity, and we take only the k most simi-
lar phrases for each phrase, to create a k-nearest
neighbor similarity matrix for both source and tar-
get language phrases. These graphs are distinct,
in that propagation happens within the two graphs
but not between them.
While accumulating co-occurrence counts for
each phrase, we also maintain an inverted index
data structure, which is a mapping from features
(context words) to phrases that co-occur with that
feature within a window of p.
1
The inverted index
structure reduces the graph construction cost from
?(n
2
), by only computing similarities for a sub-
set of all possible pairs of phrases, namely other
phrases that have at least one feature in common.
2.3 Candidate Translation List Construction
As mentioned previously, we construct and fix
a set of translation candidates, i.e., the label set
for each unlabeled source phrase. The probabil-
ity distribution over these translations is estimated
through graph propagation, and the probabilities
of items outside the list are assumed to be zero.
We obtain these candidates from two sources:
2
1. The union of each unlabeled phrase?s la-
beled neighbors? labels, which represents the
set of target phrases that occur as transla-
tions of source phrases that are similar to
the unlabeled source phrase. For un gato in
Fig. 1, this source would yield the cat and
cat, among others, as candidates.
2. The generated candidates for the unlabeled
phrase ? the ones from the baseline system?s
1
The q most frequent words in the monolingual corpus
were removed as keys from this mapping, as these high en-
tropy features do not provide much information.
2
We also obtained the k-nearest neighbors of the transla-
tion candidates generated through these methods by utilizing
the target graph, but this had minimal impact.
678
decoder output, or from a morphological gen-
erator (e.g., a cat and catlike in Fig. 1).
The morphologically-generated candidates for a
given source unlabeled phrase are initially de-
fined as the target word sequences in the mono-
lingual data that have the same stem sequence
as one of the baseline?s target translations for a
source phrase which has the same stem sequence
as the unlabeled source phrase. These candidates
are scored using stem-level translation probabili-
ties, morpheme-level lexical weighting probabili-
ties, and a language model, and only the top 30
candidates are included.
After obtaining candidates from these two pos-
sible sources, the list is sorted by forward lexical
score, using the lexical models of the baseline sys-
tem. The top r candidates are then chosen for each
phrase?s translation candidate list.
In Figure 2 we provide example outputs of
our system for a handful of unlabeled source
phrases, and explicitly note the source of the trans-
lation candidate (?G? for generated, ?N? for labeled
neighbor?s label).
2.4 Graph Propagation
A graph propagation algorithm transfers label in-
formation from labeled nodes to unlabeled nodes
by following the graph?s structure. In some appli-
cations, a label may consist of class membership
information, e.g., each node can belong to one of
a certain number of classes. In our problem, the
?label? for each node is actually a probability dis-
tribution over a set of translation candidates (target
phrases). For a given node f , let e refer to a can-
didate in the label set for node f ; then in graph
propagation, the probability of candidate e given
source phrase f in iteration t + 1 is:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)P
t
(e|j) (1)
where the setN (f) contains the (labeled and unla-
beled) neighbors of node f , and T
s
(j|f) is a term
that captures how similar nodes f and j are. This
quantity is also known as the propagation proba-
bility, and its exact form will depend on the type
of graph propagation algorithm used. For our pur-
poses, node f is a source phrasal node, the set
N (f) refers to other source phrases that are neigh-
bors of f (restricted to the k-nearest neighbors as
in ?2.2), and the aim is to estimate P (e|f), the
probability of target phrase e being a phrasal trans-
lation of source phrase f .
A classic propagation algorithm that has been
suitably modified for use in bilingual lexicon in-
duction (Tamura et al, 2012; Razmara et al, 2013)
is the label propagation (LP) algorithm of Zhu et
al. (2003). In this case, T
s
(f, j) is chosen to be:
T
s
(j|f) =
w
s
f,j
P
j
0
2N (f)
w
s
f,j
0
(2)
where w
s
f,j
is the cosine similarity (as computed
in ?2.2) between phrase f and phrase j on side s
(the source side).
As evident in Eq. 2, LP only takes into account
source language similarity of phrases. To see this
observation more clearly, let us reformulate Eq. 1
more generally as:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)
X
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j) (3)
where H(j) is the translation candidate set for
source phrase j, and T
t
(e
0
|e) is the propagation
probability between nodes or phrases e and e
0
on the target side. We have simply replaced
P
t
(e|j) with
P
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j), defining it
in terms of j?s translation candidate list.
Note that in the original LP formulation the tar-
get side information is disregarded, i.e., T
t
(e
0
|e) =
1 if and only if e = e
0
and 0 otherwise. As a
result, LP is suboptimal for our needs, since it is
unable to appropriately handle generated transla-
tion candidates for the unlabeled phrases. These
translation candidates are usually not present as
translations for the labeled phrases (or for the la-
beled phrases that neighbor the unlabeled one in
question). When propagating information from
the labeled phrases, such candidates will obtain
no probability mass since e 6= e
0
. Thus, due to
the setup of the problem, LP naturally biases away
from translation candidates produced during the
generation step (?2.1).
2.4.1 Structured Label Propagation
The label set we are considering has a similarity
structure encoded by the target graph. How can
we exploit this structure in graph propagation on
the source graph? In Liu et al (2012), the authors
generalize label propagation to structured label
propagation (SLP) in an effort to work more el-
egantly with structured labels. In particular, the
definition of target similarity is similar to that of
source similarity:
T
t
(e
0
|e) =
w
t
e,e
0
P
e
00
2H(j)
w
t
e,e
00
(4)
679
Therefore, the final update equation in SLP is:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)
X
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j) (5)
With this formulation, even if e 6= e
0
, the simi-
larity T
t
(e
0
|e) as determined by the target phrase
graph will dictate propagation probability. We re-
normalize the probability distributions after each
propagation step to sum to one over the fixed list
of translation candidates, and run the SLP algo-
rithm to convergence.
3
2.5 Phrase-based SMT Expansion
After graph propagation, each unlabeled phrase
is labeled with a categorical distribution over
the set of translation candidates defined in ?2.3.
In order to utilize these newly acquired phrase
pairs, we need to compute their relevant features.
The phrase pairs have four log-probability fea-
tures with two likelihood features and two lexical
weighting features. In addition, we use a sophis-
ticated lexicalized hierarchical reordering model
(HRM) (Galley and Manning, 2008) with five fea-
tures for each phrase pair.
We utilize the graph propagation-estimated for-
ward phrasal probabilities P(e|f) as the forward
likelihood probabilities for the acquired phrases;
to obtain the backward phrasal probability for a
given phrase pair, we make use of Bayes? Theo-
rem:
P(f |e) =
P(e|f)P(f)
P(e)
where the marginal probabilities of source and tar-
get phrases e and f are obtained from the counts
extracted from the monolingual data. The baseline
system?s lexical models are used for the forward
and backward lexical scores. The HRM probabil-
ities for the new phrase pairs are estimated from
the baseline system by backing-off to the average
values for phrases with similar length.
3 Evaluation
We performed an extensive evaluation to exam-
ine various aspects of the approach along with
overall system performance. Two language pairs
were used: Arabic-English and Urdu-English. The
Arabic-English evaluation was used to validate the
decisions made during the development of our
3
Empirically within a few iterations and a wall-clock time
of less than 10 minutes in total.
method and also to highlight properties of the
technique. With it, in ?3.2 we first analyzed the
impact of utilizing phrases instead of words and
SLP instead of LP; the latter experiment under-
scores the importance of generated candidates. We
also look at how adding morphological knowledge
to the generation process can further enrich per-
formance. In ?3.3, we then examined the effect of
using a very large 5-gram language model train-
ing on 7.5 billion English tokens to understand the
nature of the improvements in ?3.2. The Urdu to
English evaluation in ?3.4 focuses on how noisy
parallel data and completely monolingual (i.e., not
even comparable) text can be used for a realistic
low-resource language pair, and is evaluated with
the larger language model only. We also exam-
ine how our approach can learn from noisy parallel
data compared to the traditional SMT system.
Baseline phrasal systems are used both for com-
parison and for generating translation candidates
for unlabeled phrases as described in ?2.1. The
baseline is a state-of-the-art phrase-based system;
we perform word alignment using a lexicalized
hidden Markov model, and then the phrase ta-
ble is extracted using the grow-diag-final
heuristic (Koehn et al, 2003). The 13 baseline
features (2 lexical, 2 phrasal, 5 HRM, and 1 lan-
guage model, word penalty, phrase length feature
and distortion penalty feature) were tuned using
MERT (Och, 2003), which is also used to tune
the 4 feature weights introduced by the secondary
phrase table (2 lexical and 2 phrasal, other fea-
tures being shared between the two tables). For
all systems, we use a distortion limit of 4. We use
case-insensitive BLEU (Papineni et al, 2002) to
evaluate translation quality.
3.1 Datasets
Bilingual corpus statistics for both language pairs
are presented in Table 2. For Arabic-English, our
training corpus consisted of 685k sentence pairs
from standard LDC corpora
4
. The NIST MT06
and MT08 Arabic-English evaluation sets (com-
bining the newswire and weblog domains for both
sets), with four references each, were used as
tuning and testing sets respectively. For Urdu-
English, the training corpus was provided by the
LDC for the NIST Urdu-English MT evaluation,
and most of the data was automatically acquired
from the web, making it quite noisy. After fil-
tering, there are approximately 65k parallel sen-
4
LDC2007T08 and LDC2008T09
680
Parameter Description Value
m m-best candidate list size when bootstrapping candidates in generation stage. 100
p Window size on each side when extracting features for phrases. 2
q Filter the q most frequent words when storing the inverted index data structure for graph construction.
Both source and target sides share the same value.
25
k Number of neighbors stored for each phrase for both source and target graphs. This parameter controls
the sparsity of the graph.
500
r Maximum size of translation candidate list for unlabeled phrases. 20
Table 1: Parameters, explanation of their function, and value chosen.
tences; these were supplemented by an additional
100k dictionary entries. Tuning and test data con-
sisted of the MT08 and MT09 evaluation corpora,
once again a mixture of news and web text.
Corpus Sentences Words (Src)
Ar-En Train 685,502 17,055,168
Ar-En Tune (MT06) 1,664 33,739
Ar-En Test (MT08) 1,360 42,472
Ur-En Train 165,159 1,169,367
Ur-En Tune (MT08) 1,864 39,925
Ur-En Test (MT09) 1,792 39,922
Table 2: Bilingual corpus statistics for the Arabic-English
and Urdu-English datasets used.
Table 3 contains statistics for the monolingual
corpora used in our experiments. From these cor-
pora, we extracted all sentences that contained at
least one source or target phrase match to com-
pute features for graph construction. For the Ara-
bic to English experiments, the monolingual cor-
pora are taken from the AFP Arabic and English
Gigaword corpora and are of a similar date range
to each other (1994-2010), rendering them compa-
rable but not sentence-aligned or parallel.
Corpus Sentences Words
Ar Comparable 10.2m 290m
En I Comparable 29.8m 900m
Ur Noisy Parallel 470k 5m
En II Noisy Parallel 470k 4.7m
Ur Non-Comparable 7m 119m
En II Non-Comparable 17m 510m
Table 3: Monolingual corpus statistics for the Arabic-English
and Urdu-English evaluations. The monolingual corpora can
be sub-divided into comparable, noisy parallel, and non-
comparable components. En I refers to the English side of
the Arabic-English corpora, and En II to the English side of
the Urdu-English corpora.
For the Urdu-English experiments, completely
non-comparable monolingual text was used for
graph construction; we obtained the Urdu side
through a web-crawler, and a subset of the AFP
Gigaword English corpus was used for English. In
addition, we obtained a corpus from the ELRA
5
,
which contains a mix of parallel and monolingual
data; based on timestamps, we extracted a compa-
rable English corpus for the ELRA Urdu monolin-
gual data to form a roughly 470k-sentence ?noisy
parallel? set. We used this set in two ways: ei-
ther to augment the parallel data presented in Table
2, or to augment the non-comparable monolingual
data in Table 3 for graph construction.
For the parameters introduced throughout the
text, we present in Table 1 a reminder of their in-
terpretation as well as the values used in this work.
3.2 Experimental Variations
In our first set of experiments, we looked at the im-
pact of choosing bigrams over unigrams as our ba-
sic unit of representation, along with performance
of LP (Eq. 2) compared to SLP (Eq. 4). Re-
call that LP only takes into account source sim-
ilarity; since the vast majority of generated can-
didates do not occur as labeled neighbors? labels,
restricting propagation to the source graph dras-
tically reduces the usage of generated candidates
as labels, but does not completely eliminate it. In
these experiments, we utilize a reasonably-sized
4-gram language model trained on 900m English
tokens, i.e., the English monolingual corpus.
Table 4 presents the results of these variations;
overall, by taking into account generated candi-
dates appropriately and using bigrams (?SLP 2-
gram?), we obtained a 1.13 BLEU gain on the
test set. Using unigrams (?SLP 1-gram?) actu-
ally does worse than the baseline, indicating the
importance of focusing on translations for sparser
bigrams. While LP (?LP 2-gram?) does reason-
ably well, its underperformance compared to SLP
underlines the importance of enriching the trans-
lation space with generated candidates and han-
dling these candidates appropriately.
6
In ?SLP-
5
ELRA-W0038
6
It is relatively straightforward to combine both unigrams
and bigrams in one source graph, but for experimental clarity
we did not mix these phrase lengths.
681
HalfMono?, we use only half of the monolingual
comparable corpora, and still obtain an improve-
ment of 0.56 BLEU points, indicating that adding
more monolingual data is likely to improve the
system further. Interestingly, biasing away from
generated candidates using all the monolingual
data (?LP 2-gram?) performs similarly to using
half the monolingual corpora and handling gener-
ated candidates properly (?SLP-HalfMono?).
BLEU
Setup Tune Test
Baseline 39.33 38.09
SLP 1-gram 39.47 37.85
LP 2-gram 40.75 38.68
SLP 2-gram 41.00 39.22
SLP-HalfMono 2-gram 40.82 38.65
SLP+Morph 2-gram 41.02 39.35
Table 4: Results for the Arabic-English evaluation. The LP
vs. SLP comparison highlights the importance of target side
enrichment via translation candidate generation, 1-gram vs.
2-gram comparisons highlight the importance of emphasiz-
ing phrases, utilizing half the monolingual data shows sensi-
tivity to monolingual corpus size, and adding morphological
information results in additional improvement.
Additional morphologically generated candi-
dates were added in this experiment as detailed in
?2.3. We used a simple hand-built Arabic morpho-
logical analyzer that segments word types based
on regular expressions, and an English lexicon-
based morphological analyzer. The morphological
candidates add a small amount of improvement,
primarily by targeting genuine OOVs.
3.3 Large Language Model Effect
In this set of experiments, we examined if the
improvements in ?3.2 can be explained primar-
ily through the extraction of language model char-
acteristics during the semi-supervised learning
phase, or through orthogonal pieces of evidence.
Would the improvement be less substantial had we
used a very large language model?
To answer this question we trained a 5-gram
language model on 570M sentences (7.6B tokens),
with data from various sources including the Gi-
gaword corpus
7
, WMT and European Parliamen-
tary Proceedings
8
, and web-crawled data from
Wikipedia and the web. Only m-best generated
candidates from the baseline were considered dur-
ing generation, along with labeled neighbors? la-
bels.
7
LDC2011T07
8
http://www.statmt.org/wmt13/
BLEU
Setup Tune Test
Baseline+LargeLM 41.48 39.86
SLP+LargeLM 42.82 41.29
Table 5: Results with the large language model scenario. The
gains are even better than with the smaller language model.
Table 5 presents the results of using this lan-
guage model. We obtained a robust, 1.43-BLEU
point gain, indicating that the addition of the
newly induced phrases provided genuine transla-
tion improvements that cannot be compensated by
the language model effect. Further examination of
the differences between the two systems yielded
that most of the improvements are due to better
bigrams and trigrams, as indicated by the break-
down of the BLEU score precision per n-gram,
and primarily leverages higher quality generated
candidates from the baseline system. We analyze
the output of these systems further in the output
analysis section below (?3.5).
3.4 Urdu-English
In order to evaluate the robustness of these results
beyond one language pair, we looked at Urdu-
English, a low resource pair likely to benefit from
this approach. In this set of experiments, we used
the large language model in ?3.3, and only used
baseline-generated candidates. We experimented
with two extreme setups that differed in the data
assumed parallel, from which we built our base-
line system, and the data treated as monolingual,
from which we built our source and target graphs.
In the first setup, we use the noisy parallel
data for graph construction and augment the non-
comparable corpora with it:
? parallel: ?Ur-En Train?
? Urdu monolingual: ?Ur Noisy Parallel?+?Ur
Non-Comparable?
? English monolingual: ?En II Noisy Paral-
lel?+?En II Non-Comparable?
The results from this setup are presented as ?Base-
line? and ?SLP+Noisy? in Table 6. In the second
setup, we train a baseline system using the data in
Table 2, augmented with the noisy parallel text:
? parallel: ?Ur-En Train?+?Ur Noisy Paral-
lel?+?En II Noisy Parallel?
? Urdu monolingual: ?Ur Non-Comparable?
? English monolingual: ?En II Non-
Comparable?
682
!Ex Source Reference Baseline System 1 (Ar) !???#$#"! %$??" ! sending reinforcements strong reinforcements sending reinforcements (N) 2 (Ar)  !???$??'!+!! with extinction OOV with extinction (N) 3 (Ar) !???#?? ??? ! thwarts address  thwarted (N) 4 (Ar) !?? ???# ! was quoted as saying attributed to was quoted as saying (G) 5 (Ar) ????"! ??! $#??& ! abdalmahmood said he said abdul mahmood  mahmood said (G) 6 (Ar)  ?#"! ????? it deems OOV it deems (G) 7 (Ur) !?"! ?$ ! I am hopeful this hope I am hopeful (N) 8 (Ur) ??! $???$ ! to defend him to defend to defend himself (G) 9 (Ur) !??? ???? ! while speaking In the  in conversation (N) 
Figure 2: Nine example outputs of our system vs. the baseline highlighting the properties of our approach. Each example is
labeled (Ar) for Arabic source or (Ur) for Urdu source, and system candidates are labeled with (N) if the candidate unlabeled
phrase?s labeled neighbor?s label, or (G) if the candidate was generated.
The results from this setup are presented as ?Base-
line+Noisy? and ?SLP? in Table 6. The two setups
allow us to examine how effectively our method
can learn from the noisy parallel data by treating it
as monolingual (i.e., for graph construction), com-
pared to treating this data as parallel, and also ex-
amines the realistic scenario of using completely
non-comparable monolingual text for graph con-
struction as in the second setup.
BLEU
Setup Tune Test
Baseline 21.87 21.17
SLP+Noisy 26.42 25.38
Baseline+Noisy 27.59 27.24
SLP 28.53 28.43
Table 6: Results for the Urdu-English evaluation evaluated
with BLEU. All experiments were conducted with the larger
language model, and generation only considered the m-best
candidates from the baseline system.
In the first setup, we get a huge improvement of
4.2 BLEU points (?SLP+Noisy?) when using the
monolingual data and the noisy parallel data for
graph construction. Our method obtained much
of the gains achieved by the supervised baseline
approach that utilizes the noisy parallel data in
conjunction with the NIST-provided parallel data
(?Baseline+Noisy?), but with fewer assumptions
on the nature of the corpora (monolingual vs.
parallel). Furthermore, despite completely un-
aligned, non-comparable monolingual text on the
Urdu and English sides, and a very large language
model, we can still achieve gains in excess of
1.2 BLEU points (?SLP?) in a difficult evaluation
scenario, which shows that the technique adds a
genuine translation improvement over and above
na??ve memorization of n-gram sequences.
3.5 Analysis of Output
Figure 2 looks at some of the sample hypotheses
produced by our system and the baseline, along
with reference translations. The outputs produced
by our system are additionally annotated with the
origin of the candidate, i.e., labeled neighbor?s la-
bel (N) or generated (G).
The Arabic-English examples are numbered 1
to 5. The first example shows a source bigram un-
known to the baseline system, resulting in a sub-
optimal translation, while our system proposes the
correct translation of ?sending reinforcements?.
The second example shows a word that was an
OOV for the baseline system, while our system
got a perfect translation. The third and fourth ex-
amples represent bigram phrases with much bet-
ter translations compared to backing off to the
lexical translations as in the baseline. The fifth
Arabic-English example demonstrates the pitfalls
of over-reliance on the distributional hypothesis:
the source bigram corresponding to the name ?abd
almahmood? is distributional similar to another
named entity ?mahmood? and the English equiva-
lent is offered as a translation. The distributional
hypothesis can sometimes be misleading. The
sixth example shows how morphological informa-
tion can propose novel candidates: an OOV word
is broken down to its stem via the analyzer and
candidates are generated based on the stem.
The Urdu-English examples are numbered 7
to 9. In example 7, the bigram ?par umeed?
(corresponding to ?hopeful?) is never seen in the
baseline system, which has only seen ?umeed?
(?hope?). By leveraging the monolingual corpus
to understand the context of this unlabeled bigram,
we can utilize the graph structure to propose a syn-
tactically correct form, also resulting in a more flu-
ent and correct sentence as determined by the lan-
guage model. Examples 8 & 9 show cases where
the baseline deletes words or translates them into
more common words e.g., ?conversation? to ?the?,
while our system proposes reasonable candidates.
683
4 Related Work
The idea presented in this paper is similar in spirit
to bilingual lexicon induction (BLI), where a seed
lexicon in two different languages is expanded
with the help of monolingual corpora, primarily by
extracting distributional similarities from the data
using word context. This line of work, initiated
by Rapp (1995) and continued by others (Fung
and Yee, 1998; Koehn and Knight, 2002) (inter
alia) is limited from a downstream perspective, as
translations for only a small number of words are
induced and oftentimes for common or frequently
occurring ones only. Recent improvements to BLI
(Tamura et al, 2012; Irvine and Callison-Burch,
2013b) have contained a graph-based flavor by
presenting label propagation-based approaches us-
ing a seed lexicon, but evaluation is once again
done on top-1 or top-3 accuracy, and the focus is
on unigrams.
Razmara et al (2013) and Irvine and Callison-
Burch (2013a) conduct a more extensive evalua-
tion of their graph-based BLI techniques, where
the emphasis and end-to-end BLEU evaluations
concentrated on OOVs, i.e., unigrams, and not on
enriching the entire translation model. As with
previous BLI work, these approaches only take
into account source-side similarity of words; only
moderate gains (and in the latter work, on a sub-
set of language pairs evaluated) are obtained. Ad-
ditionally, because of our structured propagation
algorithm, our approach is better at handling mul-
tiple translation candidates and does not need to
restrict itself to the top translation.
Klementiev et al (2012) propose a method that
utilizes a pre-existing phrase table and a small
bilingual lexicon, and performs BLI using mono-
lingual corpora. The operational scope of their ap-
proach is limited in that they assume a scenario
where unknown phrase pairs are provided (thereby
sidestepping the issue of translation candidate
generation for completely unknown phrases), and
what remains is the estimation of phrasal proba-
bilities. In our case, we obtain the phrase pairs
from the graph structure (and therefore indirectly
from the monolingual data) and a separate gener-
ation step, which plays an important role in good
performance of the method. Similarly, Zhang and
Zong (2013) present a series of heuristics that are
applicable in a fairly narrow setting.
The notion of translation consensus, wherein
similar sentences on the source side are encour-
aged to have similar target language translations,
has also been explored via a graph-based approach
(Alexandrescu and Kirchhoff, 2009). Liu et al
(2012) extend this method by proposing a novel
structured label propagation algorithm to deal with
the generalization of propagating sets of labels
instead of single labels, and also integrated in-
formation from the graph into the decoder. In
fact, we utilize this algorithm in our propagation
step (?2.4). However, the former work operates
only at the level of sentences, and while the latter
does extend the framework to sub-spans of sen-
tences, they do not discover new translation pairs
or phrasal probabilities for new pairs at all, but
instead re-estimate phrasal probabilities using the
graph structure and add this score as an additional
feature during decoding.
The goal of leveraging non-parallel data in ma-
chine translation has been explored from several
different angles. Paraphrases extracted by ?pivot-
ing? via a third language (Callison-Burch et al,
2006) can be derived solely from monolingual
corpora using distributional similarity (Marton et
al., 2009). Snover et al (2008) use cross-lingual
information retrieval techniques to find potential
sentence-level translation candidates among com-
parable corpora. In this case, the goal is to
try and construct a corpus as close to parallel
as possible from comparable corpora, and is a
fairly different take on the problem we are look-
ing at. Decipherment-based approaches (Ravi and
Knight, 2011; Dou and Knight, 2012) have gen-
erally taken a monolingual view to the problem
and combine phrase tables through the log-linear
model during feature weight training.
5 Conclusion
In this work, we presented an approach that
can expand a translation model extracted from a
sentence-aligned, bilingual corpus using a large
amount of unstructured, monolingual data in both
source and target languages, which leads to im-
provements of 1.4 and 1.2 BLEU points over
strong baselines on evaluation sets, and in some
scenarios gains in excess of 4 BLEU points. In
the future, we plan to estimate the graph structure
through other learned, distributed representations.
Acknowledgments
The authors would like to thank Chris Dyer, Arul
Menezes, and the anonymous reviewers for their
helpful comments and suggestions.
684
References
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL-HLT ?09, pages 119?
127. Association for Computational Linguistics,
June.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 17?24, New York
City, USA, June. Association for Computational
Linguistics.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proc. of
EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
266?275. Association for Computational Linguis-
tics, July.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1, ACL ?98, pages 414?
420, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. EMNLP ?08, pages 848?856, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 262?270, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 518?523, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward sta-
tistical machine translation without parallel corpora.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 130?140, Avignon, France, April.
Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012.
Learning translation consensus with structured la-
bel propagation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ?12, pages
302?310, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?09, pages 381?390, Singapore, August. Association
for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
685
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, ACL ?95.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 12?
21, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Majid Razmara, Maryam Siahbani, Gholamreza Haf-
fari, and Anoop Sarkar. 2013. Graph propagation
for paraphrasing out-of-vocabulary words in statis-
tical machine translation. In Proceedings of the
51st of the Association for Computational Linguis-
tics, ACL-51, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 857?866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 24?36.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08:
HLT, pages 514?522, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1425?1434, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Laf-
ferty. 2003. Semi-supervised learning using gaus-
sian fields and harmonic functions. In Proceedings
of the Twentieth International Conference on Ma-
chine Learning, ICML ?03, pages 912?919.
686
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247?256,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Learning Discriminative Projections for Text Similarity Measures
Wen-tau Yih Kristina Toutanova John C. Platt Christopher Meek
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{scottyih,kristout,jplatt,meek}@microsoft.com
Abstract
Traditional text similarity measures consider
each term similar only to itself and do not
model semantic relatedness of terms. We pro-
pose a novel discriminative training method
that projects the raw term vectors into a com-
mon, low-dimensional vector space. Our ap-
proach operates by finding the optimal matrix
to minimize the loss of the pre-selected sim-
ilarity function (e.g., cosine) of the projected
vectors, and is able to efficiently handle a
large number of training examples in the high-
dimensional space. Evaluated on two very dif-
ferent tasks, cross-lingual document retrieval
and ad relevance measure, our method not
only outperforms existing state-of-the-art ap-
proaches, but also achieves high accuracy at
low dimensions and is thus more efficient.
1 Introduction
Measures of text similarity have many applications
and have been studied extensively in both the NLP
and IR communities. For example, a combination
of corpus and knowledge based methods have been
invented for judging word similarity (Lin, 1998;
Agirre et al, 2009). Similarity derived from a large-
scale Web corpus has been used for automatically
extending lists of typed entities (Vyas and Pantel,
2009). Judging the degree of similarity between
documents is also fundamental to classical IR prob-
lems such as document retrieval (Manning et al,
2008). In all these applications, the vector-based
similarity method is the most widely used. Term
vectors are first constructed to represent the origi-
nal text objects, where each term is associated with
a weight indicating its importance. A pre-selected
function operating on these vectors, such as cosine,
is used to output the final similarity score. This ap-
proach has not only proved to be effective, but is also
efficient. For instance, only the term vectors rather
than the raw data need to be stored. A pruned inverse
index can be built to support fast similarity search.
However, the main weakness of this term-vector
representation is that different but semantically re-
lated terms are not matched and cannot influence
the final similarity score. As an illustrative ex-
ample, suppose the two compared term-vectors
are: {purchase:0.4, used:0.3, automobile:0.2} and
{buy:0.3, pre-owned: 0.5, car: 0.4}. Even though
the two vectors represent very similar concepts, their
similarity score will be 0, for functions like cosine,
overlap or Jaccard. Such an issue is more severe
in cross-lingual settings. Because language vocab-
ularies typically have little overlap, term-vector rep-
resentations are completely inapplicable to measur-
ing similarity between documents in different lan-
guages. The general strategy to handle this prob-
lem is to map the raw representation to a common
concept space, where extensive approaches have
been proposed. Existing methods roughly fall into
three categories. Generative topic models like La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
assume that the terms are sampled by probabil-
ity distributions governed by hidden topics. Lin-
ear projection methods like Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) learn a projec-
tion matrix and map the original term-vectors to the
dense low-dimensional space. Finally, metric learn-
ing approaches for high-dimensional spaces have
247
also been proposed (Davis and Dhillon, 2008).
In this paper, we propose a new projection learn-
ing framework, Similarity Learning via Siamese
Neural Network (S2Net), to discriminatively learn
the concept vector representations of input text ob-
jects. Following the general Siamese neural network
architecture (Bromley et al, 1993), our approach
trains two identical networks concurrently. The in-
put layer corresponds to the original term vector
and the output layer is the projected concept vector.
Model parameters (i.e., the weights on the edges)
are equivalently the projection matrix. Given pairs
of raw term vectors and their labels (e.g., similar or
not), the model is trained by minimizing the loss of
the similarity scores of the output vectors. S2Net
is closely related to the linear projection and met-
ric learning approaches, but enjoys additional ad-
vantages over existing methods. While its model
form is identical to that of LSA, CCA and OPCA, its
objective function can be easily designed to match
the true evaluation metric of interest for the target
task, which leads to better performance. Compared
to existing high-dimensional metric learning meth-
ods, S2Net can learn from a much larger number
of labeled examples. These two properties are cru-
cial in helping S2Net outperform existing methods.
For retrieving comparable cross-lingual documents,
S2Net achieves higher accuracy than the best ap-
proach (OPCA) at a much lower dimension of the
concept space (500 vs. 2,000). In a monolingual
setting, where the task is to judge the relevance of
an ad landing page to a query, S2Net alo has the
best performance when compared to a number of ap-
proaches, including the raw TFIDF cosine baseline.
In the rest of the paper, we first survey some
existing work in Sec. 2, with an emphasis on ap-
proaches included in our experimental comparison.
We present our method in Sec. 3 and report on an
extensive experimental study in Sec. 4. Other re-
lated work is discussed in Sec. 5 and finally Sec. 6
concludes the paper.
2 Previous Work
In this section, we briefly review existing ap-
proaches for mapping high-dimensional term-
vectors to a low-dimensional concept space.
2.1 Generative Topic Models
Probabilistic Latent Semantic Analysis
(PLSA) (Hofmann, 1999) assumes that each
document has a document-specific distribution ?
over some finite number K of topics, where each
token in a document is independently generated
by first selecting a topic z from a multinomial
distribution MULTI(?), and then sampling a word
token from the topic-specific word distribution
for the chosen topic MULTI(?z). Latent Dirichlet
Allocation (LDA) (Blei et al, 2003) generalizes
PLSA to a proper generative model for documents
and places Dirichlet priors over the parameters
? and ?. In the experiments in this paper, our
implementation of PLSA is LDA with maximum a
posteriori (MAP) inference, which was shown to be
comparable to the current best Bayesian inference
methods for LDA (Asuncion et al, 2009).
Recently, these topic models have been general-
ized to handle pairs or tuples of corresponding doc-
uments, which could be translations in multiple lan-
guages, or documents in the same language that are
considered similar. For instance, the Poly-lingual
Topic Model (PLTM) (Mimno et al, 2009) is an
extension to LDA that views documents in a tu-
ple as having a shared topic vector ?. Each of the
documents in the tuple uses ? to select the topics
z of tokens, but could use a different (language-
specific) word-topic-distribution MULTI(?Lz ). Two
additional models, Joint PLSA (JPLSA) and Cou-
pled PLSA (CPLSA) were introduced in (Platt et al,
2010). JPLSA is a close variant of PLTM when doc-
uments of all languages share the same word-topic
distribution parameters, and MAP inference is per-
formed instead of Bayesian. CPLSA extends JPLSA
by constraining paired documents to not only share
the same prior topic distribution ?, but to also have
similar fractions of tokens assigned to each topic.
This constraint is enforced on expectation using pos-
terior regularization (Ganchev et al, 2009).
2.2 Linear Projection Methods
The earliest method for projecting term vectors into
a low-dimensional concept space is Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990). LSA
models all documents in a corpus using a n ?
d document-term matrix D and performs singular
248
value decomposition (SVD) on D. The k biggest
singular values are then used to find the d ? k pro-
jection matrix. Instead of SVD, LSA can be done
by applying eigen-decomposition on the correlation
matrix between terms C = DTD. This is very sim-
ilar to principal component analysis (PCA), where a
covariance matrix between terms is used. In prac-
tice, term vectors are very sparse and their means
are close to 0. Therefore, the correlation matrix is in
fact close to the covariance matrix.
To model pairs of comparable documents,
LSA/PCA has been extended in different ways. For
instance, Cross-language Latent Semantic Indexing
(CL-LSI) (Dumais et al, 1997) applies LSA to con-
catenated comparable documents from different lan-
guages. Oriented Principal Component Analysis
(OPCA) (Diamantaras and Kung, 1996; Platt et al,
2010) solves a generalized eigen problem by intro-
ducing a noise covariance matrix to ensure that com-
parable documents can be projected closely. Canon-
ical Correlation Analysis (CCA) (Vinokourov et al,
2003) finds projections that maximize the cross-
covariance between the projected vectors.
2.3 Distance Metric Learning
Measuring the similarity between two vectors can be
viewed as equivalent to measuring their distance, as
the cosine score has a bijection mapping to the Eu-
clidean distance of unit vectors. Most work on met-
ric learning learns a Mahalanobis distance, which
generalizes the standard squared Euclidean distance
by modeling the similarity of elements in different
dimensions using a positive semi-definite matrix A.
Given two vectors x and y, their squared Maha-
lanobis distance is: dA = (x ? y)TA(x ? y).
However, the computational complexity of learn-
ing a general Mahalanobis matrix is at least O(n2),
where n is the dimensionality of the input vectors.
Therefore, such methods are not practical for high
dimensional problems in the text domain.
In order to tackle this issue, special metric
learning approaches for high-dimensional spaces
have been proposed. For example, high dimen-
sion low-rank (HDLR) metric learning (Davis and
Dhillon, 2008) constrains the form of A = UUT ,
where U is similar to the regular projection ma-
trix, and adapts information-theoretic metric learn-
ing (ITML) (Davis et al, 2007) to learn U.
sim(vp,vq) 
1t
dtvp vq 
it
1c
kcjc
'tw
tw
Figure 1: Learning concept vectors. The output layer
consists of a small number of concept nodes, where the
weight of each node is a linear combination of all the
original term weights.
3 Similarity Learning via Siamese Neural
Network (S2Net)
Given pairs of documents with their labels, such as
binary or real-valued similarity scores, our goal is
to construct a projection matrix that maps the corre-
sponding term-vectors into a low-dimensional con-
cept space such that similar documents are close
when projected into this space. We propose a sim-
ilarity learning framework via Siamese neural net-
work (S2Net) to learn the projection matrix directly
from labeled data. In this section, we introduce its
model design and describe the training process.
3.1 Model Design
The network structure of S2Net consists of two lay-
ers. The input layer corresponds to the raw term vec-
tor, where each node represents a term in the original
vocabulary and its associated value is determined by
a term-weighting function such as TFIDF. The out-
put layer is the learned low-dimensional vector rep-
resentation that captures relationships among terms.
Similarly, each node of the output layer is an ele-
ment in the new concept vector. In this work, the
final similarity score is calculated using the cosine
function, which is the standard choice for document
similarity (Manning et al, 2008). Our framework
can be easily extended to other similarity functions
as long as they are differentiable.
The output of each concept node is a linear com-
249
bination of the weights of all the terms in the orig-
inal term vector. In other words, these two layers
of nodes form a complete bipartite graph as shown
in Fig. 1. The output of a concept node cj is thus
defined as:
tw?(cj) =
?
ti?V
?ij ? tw(ti) (1)
Notice that it is straightforward to add a non-linear
activation function (e.g., sigmoid) in Eq. (1), which
can potentially lead to better results. However, in
the current design, the model form is exactly the
same as the low-rank projection matrix derived by
PCA, OPCA or CCA, which facilitates comparison
to alternative projection methods. Using concise
matrix notation, let f be a raw d-by-1 term vector,
A = [?ij ]d?k the projection matrix. g = AT f is
thus the k-by-1 projected concept vector.
3.2 Loss Function and Training Procedure
For a pair of term vectors fp and fq, their similar-
ity score is defined by the cosine value of the corre-
sponding concept vectors gp and gq according to the
projection matrix A.
simA(fp, fq) =
gTp gq
||gp||||gq||
,
where gp = AT fp and gq = AT fq. Let ypq be
the true label of this pair. The loss function can
be as simple as the mean-squared error 12(ypq ?
simA(fp, fq))2. However, in many applications, the
similarity scores are used to select the closest text
objects given the query. For example, given a query
document, we only need to have the comparable
document in the target language ranked higher than
any other documents. In this scenario, it is more
important for the similarity measure to yield a good
ordering than to match the target similarity scores.
Therefore, we use a pairwise learning setting by con-
sidering a pair of similarity scores (i.e., from two
vector pairs) in our learning objective.
Consider two pairs of term vectors (fp1 , fq1) and
(fp2 , fq2), where the first pair has higher similarity.
Let ? be the difference of their similarity scores.
Namely, ? = simA(fp1 , fq1)? simA(fp2 , fq2). We
use the following logistic loss over ?, which upper-
bounds the pairwise accuracy (i.e., 0-1 loss):
L(?;A) = log(1 + exp(???)) (2)
Because of the cosine function, we add a scaling
factor ? that magnifies ? from [?2, 2] to a larger
range, which helps penalize more on the prediction
errors. Empirically, the value of ? makes no dif-
ference as long as it is large enough1. In the ex-
periments, we set the value of ? to 10. Optimizing
the model parameters A can be done using gradi-
ent based methods. We derive the gradient of the
whole batch and apply the quasi-Newton optimiza-
tion method L-BFGS (Nocedal and Wright, 2006)
directly. For a cleaner presentation, we detail the
gradient derivation in Appendix A. Given that the
optimization problem is not convex, initializing the
model from a good projection matrix often helps re-
duce training time and may lead to convergence to
a better local minimum. Regularization can be done
by adding a term ?2 ||A ? A0||
2 in Eq. (2), which
forces the learned model not to deviate too much
from the starting point (A0), or simply by early stop-
ping. Empirically we found that the latter is more
effective and it is used in the experiments.
4 Experiments
We compare S2Net experimentally with existing ap-
proaches on two very different tasks: cross-lingual
document retrieval and ad relevance measures.
4.1 Comparable Document Retrieval
With the growth of multiple languages on the Web,
there is an increasing demand of processing cross-
lingual documents. For instance, machine trans-
lation (MT) systems can benefit from training on
sentences extracted from parallel or comparable
documents retrieved from the Web (Munteanu and
Marcu, 2005). Word-level translation lexicons can
also be learned from comparable documents (Fung
and Yee, 1998; Rapp, 1999). In this cross-lingual
document retrieval task, given a query document in
one language, the goal is to find the most similar
document from the corpus in another language.
4.1.1 Data & Setting
We followed the comparable document retrieval
setting described in (Platt et al, 2010) and evalu-
ated S2Net on the Wikipedia dataset used in that pa-
per. This data set consists of Wikipedia documents
1Without the ? parameter, the model still outperforms other
baselines in our experiments, but with a much smaller gain.
250
in two languages, English and Spanish. An article
in English is paired with a Spanish article if they
are identified as comparable across languages by the
Wikipedia community. To conduct a fair compari-
son, we use the same term vectors and data split as in
the previous study. The numbers of document pairs
in the training/development/testing sets are 43,380,
8,675 and 8,675, respectively. The dimensionality
of the raw term vectors is 20,000.
The models are evaluated by using each English
document as query against all documents in Span-
ish and vice versa; the results from the two direc-
tions are averaged. Performance is evaluated by two
metrics: the Top-1 accuracy, which tests whether
the document with the highest similarity score is the
true comparable document, and the Mean Recipro-
cal Rank (MRR) of the true comparable.
When training the S2Net model, all the compara-
ble document pairs are treated as positive examples
and all other pairs are used as negative examples.
Naively treating these 1.8 billion pairs (i.e., 433802)
as independent examples would make the training
very inefficient. Fortunately, most computation in
deriving the batch gradient can be reused via com-
pact matrix operations and training can still be done
efficiently. We initialized the S2Net model using the
matrix learned by OPCA, which gave us the best per-
formance on the development set2.
Our approach is compared with most methods
studied in (Platt et al, 2010), including the best per-
forming one. For CL-LSI, OPCA, and CCA, we in-
clude results from that work directly. In addition, we
re-implemented and improved JPLSA and CPLSA
by changing three settings: we used separate vocab-
ularies for the two languages as in the Poly-lingual
topic model (Mimno et al, 2009), we performed 10
EM iterations for folding-in instead of only one, and
we used the Jensen-Shannon distance instead of the
L1 distance. We also attempted to apply the HDLR
algorithm. Because this algorithm does not scale
well as the number of training examples increases,
we used 2,500 positive and 2,500 negative docu-
ment pairs for training. Unfortunately, among all the
2S2Net outperforms OPCA when initialized from a random
or CL-LSI matrix, but with a smaller gain. For example, when
the number of dimensions is 1000, the MRR score of OPCA
is 0.7660. Starting from the CL-LSI and OPCA matrices, the
MRR scores of S2Net are 0.7745 and 0.7855, respectively.
Figure 2: Mean reciprocal rank versus dimension for
Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
hyper-parameter settings we tested, HDLR could not
outperform its initial model, which was the OPCA
matrix. Therefore we omit these results.
4.1.2 Results
Fig. 2 shows the MRR performance of all meth-
ods on the development set, across different dimen-
sionality settings of the concept space. As can be
observed from the figure, higher dimensions usually
lead to better results. In addition, S2Net consistently
performs better than all other methods across differ-
ent dimensions. The gap is especially large when
projecting input vectors to a low-dimensional space,
which is preferable for efficiency. For instance, us-
ing 500 dimensions, S2Net aleady performs as well
as OPCA with 2000 dimensions.
Table 1 shows the averaged Top-1 accuracy and
MRR scores of all methods on the test set, where
the dimensionality for each method is optimized on
the development set (Fig. 2). S2Net clearly outper-
forms all other methods and the difference in terms
of accuracy is statistically significant3.
4.2 Ad Relevance
Paid search advertising is the main revenue source
that supports modern commercial search engines.
To ensure satisfactory user experience, it is impor-
tant to provide both relevant ads and regular search
3We use the unpaired t-test with Bonferroni correction and
the difference is considered statistically significant when the p-
value is less than 0.01.
251
Algorithm Dimension Accuracy MRR
S2Net 2000 0.7447 0.7973
OPCA 2000 0.7255 0.7734
CCA 1500 0.6894 0.7378
CPLSA 1000 0.6329 0.6842
JPLSA 1000 0.6079 0.6604
CL-LSI 5000 0.5302 0.6130
Table 1: Test results for comparable document retrieval
in Wikipedia. Results of OPCA, CCA and CL-LSI are
from (Platt et al, 2010).
results. Previous work on ad relevance focuses on
constructing appropriate term-vectors to represent
queries and ad-text (Broder et al, 2008; Choi et al,
2010). In this section, we extend the work in (Yih
and Jiang, 2010) and show how S2Net can exploit
annotated query?ad pairs to improve the vector rep-
resentation in this monolingual setting.
4.2.1 Data & Tasks
The ad relevance dataset we used consists of
12,481 unique queries randomly sampled from the
logs of the Bing search engine. For each query, a
number of top ranked ads are selected, which results
in a total number of 567,744 query-ad pairs in the
dataset. Each query-ad pair is manually labeled as
same, subset, superset or disjoint. In our experi-
ment, when the task is a binary classification prob-
lem, pairs labeled as same, subset, or superset are
considered relevant, and pairs labeled as disjoint are
considered irrelevant. When pairwise comparisons
are needed in either training or evaluation, the rele-
vance order is same > subset = superset > disjoint.
The dataset is split into training (40%), validation
(30%) and test (30%) sets by queries.
Because a query string usually contains only a few
words and thus provides very little content, we ap-
plied the same web relevance feedback technique
used in (Broder et al, 2008) to create ?pseudo-
documents? to represent queries. Each query in our
data set was first issued to the search engine. The
result page with up to 100 snippets was used as the
pseudo-document to create the raw term vectors. On
the ad side, we used the ad landing pages instead
of the short ad-text. Our vocabulary set contains
29,854 words and is determined using a document
frequency table derived from a large collection of
Web documents. Only words with counts larger than
a pre-selected threshold are retained.
How the data is used in training depends on the
model. For S2Net, we constructed preference pairs
in the following way. For the same query, each rel-
evant ad is paired with a less relevant ad. The loss
function from Eq. (2) encourages achieving a higher
similarity score for the more relevant ad. For HDLR,
we used a sample of 5,000 training pairs of queries
and ads, as it was not able to scale to more train-
ing examples. For OPCA, CCA, PLSA and JPLSA,
we constructed a parallel corpus using only rele-
vant pairs of queries and ads, as the negative exam-
ples (irrelevant pairs of queries and ads) cannot be
used by these models. Finally, PCA and PLSA learn
the models from all training queries and documents
without using any relevance information.
We tested S2Net and other methods in two differ-
ent application scenarios. The first is to use the ad
relevance measure as an ad filter. When the similar-
ity score between a query and an ad is below a pre-
selected decision threshold, this ad is considered ir-
relevant to the query and will be filtered. Evaluation
metrics used for this scenario are the ROC analysis
and the area under the curve (AUC). The second one
is the ranking scenario, where the ads are selected
and ranked by their relevance scores. In this sce-
nario, the performance is evaluated by the standard
ranking metric, Normalized Discounted Cumulative
Gain (NDCG) (Jarvelin and Kekalainen, 2000).
4.2.2 Results
We first compare different methods in their AUC
and NDCG scores. TFIDF is the basic term vec-
tor representation with the TFIDF weighting (tf ?
log(N/df)). It is used as our baseline and also as
the raw input for S2Net, HDLR and other linear pro-
jection methods. Based on the results on the devel-
opment set, we found that PCA performs better than
OPCA and CCA. Therefore, we initialized the mod-
els of S2Net and HDLR using the PCA matrix. Ta-
ble 2 summarizes results on the test set. All models,
except TFIDF, use 1000 dimensions and their best
configuration settings selected on the validation set.
TFIDF is a very strong baseline on this monolin-
gual ad relevance dataset. Among all the methods
we tested, at dimension 1000, only S2Net outper-
forms the raw TFIDF cosine measure in every eval-
uation metric, and the difference is statistically sig-
252
AUC NDCG@1 NDCG@3 NDCG@5
S2Net 0.892 0.855 0.883 0.901
TFIDF 0.861 0.825 0.854 0.876
HDLR 0.855 0.826 0.856 0.877
CPLSA 0.853 0.845 0.872 0.890
PCA 0.848 0.815 0.847 0.870
OPCA 0.844 0.817 0.850 0.872
JPLSA 0.840 0.838 0.864 0.883
CCA 0.836 0.820 0.852 0.874
PLSA 0.835 0.831 0.860 0.879
Table 2: The AUC and NDCG scores of the cosine sim-
ilarity scores on different vector representations. The di-
mension for all models except TFIDF is 1000.
 0.3 0.4 0.5 0.6 0.7 0.8 0.9
 0.05  0.1  0.15  0.2  0.25True-Positive Rate False-Positive RateThe ROC Curves S2NetTFIDFHDLRCPLSA
Figure 3: The ROC curves of S2Net, TFIDF, HDLR and
CPLSA when the similarity scores are used as ad filters.
nificant4. In contrast, both CPLSA and HDLR have
higher NDCG scores but lower AUC values, and
OPCA/CCA perform roughly the same as PCA.
When the cosine scores of these vector represen-
tations are used as ad filters, their ROC curves (fo-
cusing on the low false-positive region) are shown
in Fig. 3. It can be clearly observed that the similar-
ity score computed based on vectors derived from
S2Net indeed has better quality, compared to the
raw TFIDF representation. Unfortunately, other ap-
proaches perform worse than TFIDF and their per-
formance in the low false-positive region is consis-
tent with the AUC scores.
Although ideally we would like the dimensional-
ity of the projected concept vectors to be as small
4For AUC, we randomly split the data into 50 subsets and
ran a paired-t test between the corresponding AUC scores. For
NDCG, we compared the DCG scores per query of the com-
pared models using the paired-t test. The difference is consid-
ered statistically significant when the p-value is less than 0.01.
as possible for efficient processing, the quality of
the concept vector representation usually degrades
as well. It is thus interesting to know the best trade-
off point between these two variables. Table 3 shows
the AUC and NDCG scores of S2Net at different di-
mensions, as well as the results achieved by TFIDF
and PCA, HDLR and CPLSA at 1000 dimensions.
As can be seen, S2Net surpasses TFIDF in AUC at
dimension 300 and keeps improving as the dimen-
sionality increases. Its NDCG scores are also con-
sistently higher across all dimensions.
4.3 Discussion
It is encouraging to find that S2Net achieves strong
performance in two very different tasks, given that
it is a conceptually simple model. Its empirical suc-
cess can be attributed to two factors. First, it is flex-
ible in choosing the loss function and constructing
training examples and is thus able to optimize the
model directly for the target task. Second, it can
be trained on a large number of examples. For ex-
ample, HDLR can only use a few thousand exam-
ples and is not able to learn a matrix better than its
initial model for the task of cross-lingual document
retrieval. The fact that linear projection methods
like OPCA/CCA and generative topic models like
JPLSA/CPLSA cannot use negative examples more
effectively also limits their potential.
In terms of scalability, we found that methods
based on eigen decomposition, such as PCA, OPCA
and CCA, take the least training time. The complex-
ity is decided by the size of the covariance matrix,
which is quadratic in the number of dimensions. On
a regular eight-core server, it takes roughly 2 to 3
hours to train the projection matrix in both experi-
ments. The training time of S2Net scales roughly
linearly to the number of dimensions and training
examples. In each iteration, performing the projec-
tion takes the most time in gradient derivation, and
the complexity is O(mnk), where m is the num-
ber of distinct term-vectors, n is the largest number
of non-zero elements in the sparse term-vectors and
k is the dimensionality of the concept space. For
cross-lingual document retrieval, when k = 1000,
each iteration takes roughly 48 minutes and about 80
iterations are required to convergence. Fortunately,
the gradient computation is easily parallelizable and
further speed-up can be achieved using a cluster.
253
TFIDF HDLR CPLSA PCA S2Net100 S2Net300 S2Net500 S2Net750 S2Net1000
AUC 0.861 0.855 0.853 0.848 0.855 0.879 0.880 0.888 0.892
NDCG@1 0.825 0.826 0.845 0.815 0.843 0.852 0.856 0.860 0.855
NDCG@3 0.854 0.856 0.872 0.847 0.871 0.879 0.881 0.884 0.883
NDCG@5 0.876 0.877 0.890 0.870 0.890 0.897 0.899 0.902 0.901
Table 3: The AUC and NDCG scores of S2Net at different dimensions. PCA, HDLR & CPLSA (at dimension 1000)
along with the raw TFIDF representation are used for reference.
5 Related Work
Although the high-level design of S2Net follows the
Siamese architecture (Bromley et al, 1993; Chopra
et al, 2005), the network construction, loss func-
tion and training process of S2Net are all differ-
ent compared to previous work. For example, tar-
geting the application of face verification, Chopra
et al (2005) used a convolutional network and de-
signed a contrastive loss function for optimizing a
Eucliden distance metric. In contrast, the network
of S2Net is equivalent to a linear projection ma-
trix and has a pairwise loss function. In terms of
the learning framework, S2Net is closely related to
several neural network based approaches, including
autoencoders (Hinton and Salakhutdinov, 2006) and
finding low-dimensional word representations (Col-
lobert and Weston, 2008; Turian et al, 2010). Archi-
tecturally, S2Net is also similar to RankNet (Burges
et al, 2005), which can be viewed as a Siamese neu-
ral network that learns a ranking function.
The strategy that S2Net takes to learn from la-
beled pairs of documents can be analogous to the
work of distance metric learning. Although high
dimensionality is not a problem to algorithms like
HDLR, it suffers from a different scalability issue.
As we have observed in our experiments, the al-
gorithm can only handle a small number of simi-
larity/dissimilarity constraints (i.e., the labeled ex-
amples), and is not able to use a large number of
examples to learn a better model. Empirically, we
also found that HDLR is very sensitive to the hyper-
parameter settings and its performance can vary sub-
stantially from iteration to iteration.
Other than the applications presented in this pa-
per, concept vectors have shown useful in traditional
IR tasks. For instance, Egozi et al (2008) use ex-
plicit semantic analysis to improve the retrieval re-
call by leveraging Wikipedia. In a companion pa-
per, we also demonstrated that various topic mod-
els including S2Net can enhance the ranking func-
tion (Gao et al, 2011). For text categorization, simi-
larity between terms is often encoded as kernel func-
tions embedded in the learning algorithms, and thus
increase the classification accuracy. Representative
approaches include latent semantic kernels (Cris-
tianini et al, 2002), which learns an LSA-based ker-
nel function from a document collection, and work
that computes term-similarity based on the linguis-
tic knowledge provided by WordNet (Basili et al,
2005; Bloehdorn and Moschitti, 2007).
6 Conclusions
In this paper, we presented S2Net, a discrimina-
tive approach for learning a projection matrix that
maps raw term-vectors to a low-dimensional space.
Our learning method directly optimizes the model
so that the cosine score of the projected vectors can
become a reliable similarity measure. The strength
of this model design has been shown empirically in
two very different tasks. For cross-lingual document
retrieval, S2Net significantly outperforms OPCA,
which is the best prior approach. For ad selection
and filtering, S2Net alo outperforms all methods we
compared it with and is the only technique that beats
the raw TFIDF vectors in both AUC and NDCG.
The success of S2Net is truly encouraging, and
we would like to explore different directions to fur-
ther enhance the model in the future. For instance, it
will be interesting to extend the model to learn non-
linear transformations. In addition, since the pairs of
text objects being compared often come from differ-
ent distributions (e.g., English documents vs. Span-
ish documents or queries vs. pages), learning two
different matrices instead of one could increase the
model expressivity. Finally, we would like to apply
S2Net to more text similarity tasks, such as word
similarity and entity recognition and discovery.
254
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, June.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In UAI.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics via
kernel-based learning. In CoNLL.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:993?1022.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Combined syntactic and semantic kernels for text clas-
sification. In ECIR, pages 307?318.
Andrei Z. Broder, Peter Ciccolo, Marcus Fontoura,
Evgeniy Gabrilovich, Vanja Josifovski, and Lance
Riedel. 2008. Search advertising using web relevance
feedback. In CIKM, pages 1013?1022.
Jane Bromley, James W. Bentz, Le?on Bottou, Isabelle
Guyon, Yann LeCun, Cliff Moore, Eduard Sa?ckinger,
and Roopak Shah. 1993. Signature verification us-
ing a ?Siamese? time delay neural network. Interna-
tional Journal Pattern Recognition and Artificial Intel-
ligence, 7(4):669?688.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
ICML.
Y. Choi, M. Fontoura, E. Gabrilovich, V. Josifovski,
M. Mediano, and B. Pang. 2010. Using landing pages
for sponsored search ad selection. In WWW.
Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005.
Learning a similarity metric discriminatively, with ap-
plication to face verification. In Proceedings of CVPR-
2005, pages 539?546.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In ICML.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2002. Latent semantic kernels. Journal of Intelligent
Information Systems, 18(2?3):127?152.
Jason V. Davis and Inderjit S. Dhillon. 2008. Struc-
tured metric learning for high dimensional problems.
In KDD, pages 195?203.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and
Inderjit S. Dhillon. 2007. Information-theoretic met-
ric learning. In ICML.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Konstantinos I. Diamantaras and S.Y. Kung. 1996. Prin-
cipal Component Neural Networks: Theory and Appli-
cations. Wiley-Interscience.
Susan T. Dumais, Todd A. Letsche, Michael L. Littman,
and Thomas K. Landauer. 1997. Automatic cross-
linguistic information retrieval using latent seman-
tic indexing. In AAAI-97 Spring Symposium Series:
Cross-Language Text and Speech Retrieval.
Ofer Egozi, Evgeniy Gabrilovich, and Shaul Markovitch.
2008. Concept-based feature generation and selection
for information retrieval. In AAAI.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING-ACL.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical Report MS-
CIS-09-16, University of Pennsylvania.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In SIGIR.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504?507, July.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In SIGIR ?99, pages 50?57.
K. Jarvelin and J. Kekalainen. 2000. Ir evaluation meth-
ods for retrieving highly relevant documents. In SI-
GIR, pages 41?48.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL 98.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Pres.
David Mimno, Hanna W. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In EMNLP.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the ACL, pages 519?526.
255
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation of
text via cross-language correlation analysis. In NIPS-
15.
Vishnu Vyas and Patrick Pantel. 2009. Semi-automatic
entity set refinement. In NAACL ?09, pages 290?298.
Wen-tau Yih and Ning Jiang. 2010. Similarity models
for ad relevance measures. In MLOAD - NIPS 2010
Workshop on online advertising.
Appendix A. Gradient Derivation
The gradient of the loss function in Eq. (2) can be
derived as follows.
?L(?,A)
?A
=
??
1 + exp(???)
??
?A
??
?A
=
?
?A
simA(fp1 , fq1)?
?
?A
simA(fp2 , fq2)
?
?A
simA(fp, fq) =
?
?A
cos(gp,gq),
where gp = AT fp and gq = AT fq are the projected
concept vectors of fq and fq. The gradient of the
cosine score can be further derived in the following
steps.
cos(gp,gq) =
gTp gq
?gp??gq?
?Ag
T
p gq = (?AA
T fp)gq + (?AA
T fq)gp
= fpgTq + fqg
T
p
?A
1
?gp?
= ?A(g
T
p gp)
? 12
= ?
1
2
(gTp gp)
? 32?A(g
T
p gp)
= ?(gTp gp)
? 32 fpgTp
?A
1
?gq?
= ?(gTq gq)
? 32 fqgTq
Let a, b, c be gTp gq, 1/?gp? and 1/?gq?, respec-
tively.
?A
gTp gq
?gp??gq?
= ? abc3fqgTq ? acb
3fpgTp
+ bc(fpgTq + fqg
T
p )
256
