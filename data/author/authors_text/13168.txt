Coling 2010: Poster Volume, pages 189?196,
Beijing, August 2010
Unsupervised cleansing of noisy text
Danish Contractor
IBM India Software Labs
dcontrac@in.ibm.com
Tanveer A. Faruquie
IBM Research India
ftanveer@in.ibm.com
L. Venkata Subramaniam
IBM Research India
lvsubram@in.ibm.com
Abstract
In this paper we look at the problem of
cleansing noisy text using a statistical ma-
chine translation model. Noisy text is pro-
duced in informal communications such
as Short Message Service (SMS), Twit-
ter and chat. A typical Statistical Ma-
chine Translation system is trained on par-
allel text comprising noisy and clean sen-
tences. In this paper we propose an un-
supervised method for the translation of
noisy text to clean text. Our method has
two steps. For a given noisy sentence, a
weighted list of possible clean tokens for
each noisy token are obtained. The clean
sentence is then obtained by maximizing
the product of the weighted lists and the
language model scores.
1 Introduction
Noisy unstructured text data is found in informal
settings such as Short Message Service (SMS),
online chat, email, social message boards, news-
group postings, blogs, wikis and web pages. Such
text may contain spelling errors, abbreviations,
non-standard terminology, missing punctuation,
misleading case information, as well as false
starts, repetitions, and special characters.
We define noise in text as any kind of difference
between the surface form of a coded representa-
tion of the text and the correct text. The SMS ?u
kno whn is d last train of delhi metro? is noisy
because several of the words are not spelled cor-
rectly and there are grammar mistakes. Obviously
the person who wrote this message intended to
write exactly what is there in the SMS. But still it
is considered noisy because the message is coded
using non-standard spellings and grammar.
Current statistical machine translation (SMT)
systems rely on large parallel and monolingual
training corpora to produce high quality transla-
tions (Brown et al, 1993). Most of the large paral-
lel corpora available comprise newswire data that
include well formed sentences. Even when web
sources are used to train a SMT system, noisy por-
tions of the corpora are eliminated (Imamura et
al., 2003) (Imamura and Sumita, 2002) (Khadivi
and Ney, 2005). This is because it is known that
noise in parallel corpora results in incorrect train-
ing of models thus degrading the performance.
We are not aware of sufficiently large paral-
lel datasets comprising noisy and clean sentences.
In fact, even dictionaries comprising of noisy to
clean mappings in one language are very limited
in size.
With the increase in noisy text data generated
in various social communication media, cleans-
ing of such text has become necessary. The lack
of noisy parallel datasets means that this prob-
lem cannot be tackled in the traditional SMT way,
where translation models are learned based on the
parallel dataset. Consider the problem of translat-
ing a noisy English sentence e to a clean English
sentence h. SMT imagines that e was originally
conceived in clean English which when transmit-
ted over the noisy channel got corrupted and be-
came a noisy English sentence. The objective of
SMT is to recover the original clean sentence.
189
The goal of this paper is to analyze how noise
can be tackled. We present techniques to trans-
late noisy text sentences e to clean text sentences
h. We show that it is possible to clean noisy text
in an unsupervised fashion by incorporating steps
to construct ranked lists of possible clean English
tokens and then searching for the best clean sen-
tence. Of course as we will show for a given noisy
sentence, several clean sentences are possible. We
exploit the statistical machine learning paradigm
to let the decoder pick the best alternative from
these possible clean options to give the final trans-
lation for a given noisy sentence.
The rest of the paper is organized as follows.
In section 2 we state our contributions and give
an overview of our approach. In Section 3 we
describe the theory behind clean noisy text using
MT. In Section 4 we explain how we use a weigh-
ing function and a plain text dictionary of clean
tokens to guess possible clean English language
tokens. Section 5 describes our system along with
our results. We have given an analysis of the kind
of noise present in our data set in section 5.2
2 Our Approach
In this paper we describe an unsupervised method
to clean noisy text. We formulate the text cleans-
ing problem in the machine translation framework
using translation model 1 (Brown et al, 1993).
We clean the text using a pseudo-translation
model of clean and noisy words along with a lan-
guage model trained using a large monolingual
corpus. We use a decoder to search for the best
clean sentence for a noisy sentence using these
models.
We generate scores for the pseudo translation
model using a weighing function for each token in
an SMS and use these scores along with language
model probabilities to hypothesize the best clean
sentence for a given noisy SMS. Our approach can
be summarized in the following steps:
? Tokenize noisy SMS S into n tokens s1, s2 ...
sn. For each SMS token si create a weighted
list based on a weighing function. These lists
along with their scores corresponds to the
translation probabilities of the SMT transla-
tion model.
? Use the lists generated in the step above
along with clean text language model scores,
in a decoder to hypothesize the best clean
sentence
? At the end of the search choose the highest
scoring sentence as the clean translation of
the noisy sentence
In the above approach we do not learn the trans-
lation model but emulate the translation model
during decoding by analyzing the noise of the to-
kens in the input sentence.
3 Noisy sentence translation
Statistical Translation models were invented by
Brown, et al(Brown et al, 1993) and are based
on the source-channel paradigm of communica-
tion theory. Consider the problem of translating a
noisy sentence e to a clean sentence h. We imag-
ine that e was originally conceived cleanly which
when transmitted over the noisy communication
channel got corrupted and became a noisy sen-
tence. The goal is to get back the original clean
sentence from the noisy sentence. This can be ex-
pressed mathematically as
h? = argmax
h
Pr(h|e)
By Bayes? Theorem
h? = argmax
h
Pr(e|h)Pr(h)
Conceptually, the probability distribution
P (e|h) is a table which associates a probability
score with every possible pair of clean and noisy
sentences (e, h). Every noisy sentence e is a
candidate translation of a given clean sentence h.
The goodness of the translation h? e is given by
the probability score of the pair (e, h). Similarly,
Pr(h) is a table which associates a probability
score with every possible clean sentence h and
measures how well formed the sentence h is.
It is impractical to construct these tables exactly
by examining individual sentences (and sentence
pairs) since the number of conceivable sentences
in any language is countably infinite. Therefore,
the challenge in Statistical Machine Translation
is to construct approximations to the probability
190
distributions P (e|h) and Pr(h) that give an ac-
ceptable quality of translation. In the next section
we describe a model which is used to approximate
P (e|h).
3.1 IBM Translation Model 2
IBM translation model 2 is a generative model,
i.e., it describes how a noisy sentence e could be
stochastically generated given a clean sentence h.
It works as follows:
? Given a clean sentence h of length l, choose
the length (m) for the noisy sentence from a
distribution (m|l).
? For each position j = 1, 2, . . .m in the noisy
string, choose a position aj in the clean string
from a distribution a(aj |j, l,m). The map-
ping a = (a1, a2, . . . , am) is known as align-
ment between the noisy sentence e and the
clean sentence h. An alignment between e
and h tells which word of e is the corrupted
version of the corresponding word of h.
? For each j = 1, 2, . . .m in the noisy string,
choose an noisy word ej according to the dis-
tribution t(ej |haj ).
It follows from the generative model that prob-
ability of generating e = e1e2 . . . em given h =
h1h2 . . . hl with alignment a = (a1, a2, . . . , am)
is
Pr(e, a|h) = (m|l)
m?
j=1
t(ej |haj )a(aj |j,m, l).
It can be easily seen that a sentence e could be
produced from h employing many alignments and
therefore, the probability of generating e given
h is the sum of the probabilities of generating
e given h under all possible alignments a, i.e.,
Pr(e|h) =?a Pr(e, a|h). Therefore,
Pr(e|h) =
(m|l)
l?
a1=0
..
l?
am=0
m?
j=1
t(ej |haj )a(aj |j,m, l).
The above expression can be rewritten as follows:
Pr(e|h) = (m|l)
m?
j=1
l?
i=0
t(ej |hi)a(i|j,m, l).
Typical statistical machine translation systems
use large parallel corpora to learn the transla-
tion probabilities (Brown et al, 1993). Tradi-
tionally such corpora have consisted of news ar-
ticles and other well written articles. Therefore
in theory P (e|h) should be constructed by ex-
amining sentence pairs of clean and noisy sen-
tences. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training.
Aligned parallel corpora for noisy sentence is
difficult to obtain. This lack of data for a lan-
guage and the domain dependence of noise makes
it impractical to construct corpus from which
P (e|h) can be learnt automatically. This leads
to difficulty in learning P (e|h). Fortunately the
alignment between clean and noisy sentences are
monotonic in nature hence we assume a uniform
distribution for a(i|j,m, l) held fixed at (l+1)?1.
This is equivalent to model 1 of IBM translation
model. The translation models t(ej |haj ) can be
thought of as a ranked list of noisy words given
a clean word. In section 4.2 we show how this
ranked list can be constructed in an unsupervised
fashion.
3.2 Language Model
The problem of estimating the sentence forma-
tion distribution Pr(h) is known as the lan-
guage modeling problem. The language mod-
eling problem is well studied in literature par-
ticularly in the context of speech recognition.
Typically, the probability of a n-word sentence
h = h1h2 . . . hn is modeled as Pr(h) =
Pr(h1|H1)Pr(h2|H2) . . . P r(hn|Hn), where Hi
is the history of the ith word hi. One of the most
popular language models is the n-gram model
(Brown et al, 1993) where the history of a word
consists o f the word and the previous n?1 words
in the sentence, i.e., Hi = hihi?1 . . . hi?n+1. In
our application we use a smoothed trigram model.
3.3 Decoding
The problem of searching for a sentence h which
minimizes the product of translation model prob-
191
ability and the language model probability is
known as the decoding problem. The decoding
problem has been proved to be NP-complete even
when the translation model is IBM model 1 and
the language model is bi-gram (K Knight., 1999).
Effective suboptimal search schemes have been
proposed (F. Jelinek, 1969), (C. Tillman et al,
1997).
4 Pseudo Translation Model
In order to be able to exploit the SMT paradigm
we first construct a pseudo translation model. The
first step in this direction is to create noisy token
to clean token mapping. In order to process the
noisy input we first have to map noisy tokens in
noisy sentence, Se, to the possible correct lexical
representations. We use a similarity measure to
map the noisy tokens to their clean lexical repre-
sentations .
4.1 Similarity Measure
For a term te ? De, where De is a dictionary of
possible clean tokens, and token si of the noisy
input Se, the similarity measure ?(te, si) between
them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si) if te and si share
same starting
character
0 otherwise
(1)
where LCSRatio(te, si) = length(LCS(te,si))length(te) and
LCS(te, si) is the Longest common subsequence
between te and si. The intuition behind this mea-
sure is that people typically type the first few char-
acters of a word in an SMS correctly. This way we
limit the possible variants for a particular noisy to-
ken.
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is
the ratio of the length of their LCS and the length
of the longer string. Since in the SMS scenario,
the dictionary term will always be longer than the
SMS token, the denominator of LCSR is taken as
the length of the dictionary term.
The EditDistanceSMS (Figure 1) compares
the Consonant Skeletons (Prochasson et al, 2007)
of the dictionary term and the SMS token. If the
Levenshtein distance between consonant skele-
tons is small then ?(te, si) will be high. The intu-
ition behind using EditDistanceSMS can be ex-
plained through an example. Consider an SMS
token ?gud? whose most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a re-
sult the similarity measure between ?gud? and
?good? will be higher than that of ?gud? and
?guided?. Higher the LCSRatio and lower the
EditDistanceSMS , higher will be the similarity
measure. Hence, for a given SMS token ?byk?,
the similarity measure of word ?bike? is higher
than that of ?break?.
In the next section we show how we use
this similarity measure to construct ranked lists.
Ranked lists of clean tokens have also been used
in FAQ retrieval based on noisy queries (Kothari
et al, 2009).
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 1: EditDistanceSMS
4.2 List Creation
For a given noisy input string Se, we tokenize it
on white space and replace any occurrence of dig-
its to their string based form (e.g. 4get, 2day) to
get a series of n tokens s1, s2, . . . , sn. A list Lei
is created for each token si using terms in a dic-
192
hv u cmplted ure prj rprt
d ddline fr sbmission of d rprt hs bn xtnded
i wil be lte by 20 mns
d docs shd rech u in 2 days
thnk u for cmg 2 d prty
Figure 2: Sample SMS queries
tionary De consisting of clean english words. A
term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (2)
Heuristics are applied to boost scores of some
words based on positional properties of characters
in noisy and clean tokens. The scores of the fol-
lowing types of tokens are boosted:
1. Tokens that are a substring of a dictionary
words from the first character.
2. Tokens having the same first and last charac-
ter as a dictionary word.
3. Token that are dictionary words themselves
(clean text).
The threshold value ? is determined experimen-
tally. Thus we select only the top scoring possible
clean language tokens to construct the sentence.
Once the list are constructed the similarity mea-
sure along with the language model scores is used
by the decoding algorithm to find the best possi-
ble English sentence. It is to be noted that these
lists are constructed at decoding time since they
depend on the noisy surface forms of words in the
input sentence.
5 Experiments
To evaluate our system we used a set of 800 noisy
English SMSes sourced from the publicly avail-
able National University of Singapore SMS cor-
pus1 and a collection of SMSes available from the
Indian Institute of Technology, Kharagpur. The
SMSes are a collection of day-to-day SMS ex-
changes between different users. We manually
1http://wing.comp.nus.edu.sg/downloads/smsCorpus
Figure 3: System implementation
BLEU scores 1-gram 2-gram 3-gram 4-gram
Noisy text 40.96 63.7 45.1 34.5 28.3
Cleaned text 53.90 77.5 58.7 47.4 39.5
Table 1: BLEU scores
generated a cleaned english version of our test set
to use as a reference.
The noisy SMS tokens were used to generate
clean text candidates as described in section 4.2.
The dictionary De used for our experiments was a
plain text list of 25,000 English words. We cre-
ated a tri-gram language model using a collec-
tion of 100,000 clean text documents. The docu-
ments were a collection of articles on news, sport-
ing events, literature, history etc. For decoding
we used Moses2, which is an open source decoder
for SMT (Hoang et al, 2008), (Koehn et al,
2007). The noisy SMS along with clean candi-
date token lists, for each SMS token and language
model probabilities were used by Moses to hy-
pothesize the best clean english output for a given
noisy SMS. The language model and translation
models weights used by Moses during the decod-
ing phase, were adjusted manually after some ex-
perimentation.
We used BLEU (Bilingual evaluation under-
study) and Word error rate (WER) to evaluate the
performance of our system. BLEU is used to
2http://www.statmt.org/moses/
193
Figure 4: Comparison of BLEU scores
establish similarity between a system translated
and human generated reference text. A noisy
SMS ideally has only one possible clean transla-
tion and all human evaluators are likely to provide
the same translation. Thus, BLEU which makes
use of n-gram comparisons between reference and
system generated text, is very useful to measure
the accuracy of our system. As shown in Fig 4
, our system reported significantly higher BLEU
scores than unprocessed noisy text.
The word error rate is defined as
WER = S +D + IN (3)
where S is the number of substitutions, D is the
number of the deletions, I is the number of the in-
sertions and N is the number of words in the refer-
ence The WER can be thought of as an execution
of the Levenstein Edit distance algorithm at the
token level instead of character level.
Fig 5 shows a comparison of the WER. Sen-
tences generated from our system had 10 % lower
WER as compared to the unprocessed noisy sen-
tences. In addition, the sentences generated by our
system match a higher number of tokens (words)
with the reference sentences, as compared to the
noisy sentences.
5.1 System performance
Unlike standard MT system when P (e|h) is pre-
computed during the training time, list generation
in our system is dynamic because it depends on
the noisy words present in the input sentence. In
this section we evaluate the computation time for
list generation along with the decoding time for
finding the best list. We used an Intel Core 2
Duo 2.2 GHz processor with 3 GB DDR2 RAM
Figure 5: Word error rates
Figure 6: Execution time slices
to implement our system. As shown in Fig 6 the
additional computation involving list creation etc
takes up 56% (90 milliseconds) of total translation
time. 43% of the total execution time is taken by
the decoder, while I/O operations take only 1% of
the total execution time. The decoder execution
time slices reported above exclude the time taken
to load the language model. Moses took approxi-
mately 10 seconds to load our language model.
5.2 Measuring noise level in SMS queries
The noise in the collected SMS corpus can be cat-
egorized as follows
1. Removal of characters : The commonly ob-
served patterns include deletion of vowels
(as in ?msg? for ?message?), deletion of re-
peated character (as in ?happy? for ?hapy?)
and truncation (as in ?tue? for ?tuesday?)
Type of Noise % of Total Noisy Tokens
Deletion of Characters 48%
Phonetic Substitution 33%
Abbreviations 5%
Dialectical Usage 4%
Deletion of Words 1.2%
Table 2: Measure of Types of SMS Noise
194
Clean (Reference) text Noisy text Output text
Perplexity 19.61 34.56 21.77
Table 3: Perplexity for Reference, Noisy Cleaned
SMS
2. Phonetic substitution: For example, ?2? for
?to? or ?too?, ?lyf?? for ?life?, ?lite? for
?light? etc.
3. Abbreviation: Some frequently used abbre-
viations are ?tb? for ?text back?, ?lol? for
?laughs out loud?, ?AFAICT? for ?as far as
i can tell? etc.
4. Dialectal and informal usage: Often multiple
words are combined into a single token fol-
lowing certain dialectal conventions. For ex-
ample, ?gonna? is used for ?going to?, ?aint?
is used for ?are not?, etc.
5. Deletion of words: Function words (e.g. ar-
ticles) and pronouns are commonly deleted.
?I am reading the book? for example may be
typed as ?readin bk?.
Table 2 lists statistics on these noise types from
101 SMSes selected at random from our data set.
The average length of these SMSes was 13 words.
Out of the total number of words in the SMSes,
52% were non standard words. Table 2 lists the
statistics for the types of noise present in these non
standard words.
Measuring character level perplexity can be an-
other way of estimating noise in the SMS lan-
guage.The perplexity of a LM on a corpus gives
an indication of the average number of bits needed
per n-gram to encode the corpus. Noise results
in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits
are needed to encode these improbable n-grams
which results in increased perplexity.
We built a character-level language model (LM)
using a document collection (vocabulary size is
20K) and computed the perplexity of the language
model on the noisy and the cleaned SMS test-set
and the SMS reference data.
From Table 3 we can see the difference in per-
plexity for noisy and clean SMS data. Large per-
plexity values for the SMS dataset indicates a high
level of noise. The perplexity evaluation indicates
that our method is able to remove noise from the
input queries as given by the perplexity and is
close to the human correct reference corpus whose
perplexity is 19.61.
6 Conclusion
We have presented an inexpensive, unsupervised
method to clean noisy text. It does not require
the use of a noisy to clean language parallel cor-
pus for training. We show how a simple weigh-
ing function based on observed heuristics and a
vocabulary file can be used to shortlist clean to-
kens. These tokens and their weights are used
along with language model scores, by a decoder
to select the best clean language sentence.
References
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of tex-
ting language. International Journal on Document
Analysis and Recognition.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. In Proceedings of AAAI
Workshop on Enhanced Messaging.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of COLING-ACL.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS :
Evaluation et bilan quantitatif. In Actes de TALN,
Toulouse, France.
Catherine Kobus, Francois Yvon and Geraldine
Damnati. 2008. Normalizing SMS: Are two
metaphors better than one? In Proceedings of COL-
ING, Manchester.
Sreangsu Acharya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, Evan Herbst 2007.
Moses: Open source toolkit for statistical machine
195
translation. In Proceedings of ACL, Demonstration
Session .
Peter F. Brown, Vincent J.Della Pietra, Stephen A.
Della Pietra, Robert. L. Mercer 1993. The Math-
ematics of Statistical Machine Translation: Parame-
ter Estimation Computational Linguistics.
I. D. Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message
services. In Proceedings of ICDAR.
S. Khadivi and H. Ney. 2005. Automatic filtering of
bilingual corpora for statistical machine translation.
In Proceedings of NLDB, pages 263?274, 2005.
K. Imamura and E. Sumita. 2002. Bilingual corpus
cleaning focusing on translation literality. In In Pro-
ceedings of ICSLP.
K. Imamura, E. Sumita, and Y. Matsumoto. 2003. Au-
tomatic construction of machine translation knowl-
edge using translation literalness. In In Proceedings
of EACL.
K. Knight, 1999. Decoding complexity in word re-
placement translation models. Computational Lin-
guistics.
F. Jelinek, 1969. A fast sequential decoding algorithm
using a stack. IBM Journal of Research and Devel-
opment.
C. Tillman, S. Vogel, H. Ney, and A. Zubiaga. 1997.
A DP-based search using monotone alignments in
statistical translation. In Proceedings of ACL.
Hieu Hoang, Philipp Koehn. 2008. Design of the
Moses decoder for statistical machine translation.
In Proceedings of ACL Workshop on Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing.
Govind Kothari, Sumit Negi, Tanveer A. Faruquie,
Venkatesan T. Chakraverthy, L. Venkata Subrama-
niam. 2009. SMS based interface for FAQ retrieval,
In In Proceedings of ACL-IJCNLP
196
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87?96,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Handling Noisy Queries In Cross Language FAQ Retrieval
Danish Contractor Govind Kothari Tanveer A. Faruquie
L. Venkata Subramaniam Sumit Negi
IBM Research India
Vasant Kunj, Institutional Area
New Delhi, India
{dcontrac,govkotha,ftanveer,lvsubram,sumitneg}@in.ibm.com
Abstract
Recent times have seen a tremendous growth
in mobile based data services that allow peo-
ple to use Short Message Service (SMS) to
access these data services. In a multilin-
gual society it is essential that data services
that were developed for a specific language
be made accessible through other local lan-
guages also. In this paper, we present a ser-
vice that allows a user to query a Frequently-
Asked-Questions (FAQ) database built in a lo-
cal language (Hindi) using Noisy SMS En-
glish queries. The inherent noise in the SMS
queries, along with the language mismatch
makes this a challenging problem. We handle
these two problems by formulating the query
similarity over FAQ questions as a combina-
torial search problem where the search space
consists of combinations of dictionary varia-
tions of the noisy query and its top-N transla-
tions. We demonstrate the effectiveness of our
approach on a real-life dataset.
1 Introduction
There has been a tremendous growth in the number
of new mobile subscribers in the recent past. Most
of these new subscribers are from developing coun-
tries where mobile is the primary information de-
vice. Even for users familiar with computers and the
internet, the mobile provides unmatched portability.
This has encouraged the proliferation of informa-
tion services built around SMS technology. Several
applications, traditionally available on Internet, are
now being made available on mobile devices using
SMS. Examples include SMS short code services.
Short codes are numbers where a short message in
a predesignated format can be sent to get specific
information. For example, to get the closing stock
price of a particular share, the user has to send a
message IBMSTOCKPR. Other examples are search
(Schusteritsch et al, 2005), access to Yellow Page
services (Kopparapu et al, 2007), Email 1, Blog 2 ,
FAQ retrieval 3 etc. The SMS-based FAQ retrieval
services use human experts to answer SMS ques-
tions.
Recent studies have shown that instant messag-
ing is emerging as the preferred mode of commu-
nication after speech and email.4 Millions of users
of instant messaging (IM) services and short mes-
sage service (SMS) generate electronic content in a
dialect that does not adhere to conventional gram-
mar, punctuation and spelling standards. Words are
intentionally compressed by non-standard spellings,
abbreviations and phonetic transliteration are used.
Typical question answering systems are built for use
with languages which are free from such errors. It
is difficult to build an automated question answer-
ing system around SMS technology. This is true
even for questions whose answers are well docu-
mented like in a Frequently-Asked-Questions (FAQ)
database. Unlike other automatic question answer-
ing systems that focus on searching answers from
a given text collection, Q&A archive (Xue et al,
2008) or the Web (Jijkoun et al, 2005), in a FAQ
database the questions and answers are already pro-
1http://www.sms2email.com/
2http://www.letmeparty.com/
3http://www.chacha.com/
4http://www.whyconverge.com/
87
Figure 1: Sample SMS queries with Hindi FAQs
vided by an expert. The main task is then to iden-
tify the best matching question to retrieve the rel-
evant answer (Sneiders, 1999) (Song et al, 2007).
The high level of noise in SMS queries makes this a
difficult problem (Kothari et al, 2009). In a multi-
lingual setting this problem is even more formidable.
Natural language FAQ services built for users in one
language cannot be accessed in another language.
In this paper we present a FAQ-based question an-
swering system over a SMS interface that solves this
problem for two languages. We allow the FAQ to be
in one language and the SMS query to be in another.
Multi-lingual question answering and information
retrieval has been studied in the past (Sekine and
Grishman, 2003)(Cimiano et al, 2009). Such sys-
tems resort to machine translation so that the search
can be performed over a single language space. In
the two language setting, it involves building a ma-
chine translation system engine and using it such
that the question answering system built for a sin-
gle language can be used.
Typical statistical machine translation systems
use large parallel corpora to learn the translation
probabilities (Brown et al, 2007). Traditionally
such corpora have consisted of news articles and
other well written articles. Since the translation sys-
tems are not trained on SMS language they perform
very poorly when translating noisy SMS language.
Parallel corpora comprising noisy sentences in one
language and clean sentences in another language
are not available and it would be hard to build such
large parallel corpora to train a machine translation
system. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training. Such data is extremely hard
to create. Unsupervised techniques require huge
amounts of SMS data to learn mappings of non-
standard words to their corresponding conventional
form (Acharyya et al, 2009).
Removal of noise from SMS without the use of
parallel data has been studied but the methods used
are highly dependent on the language model and the
degree of noise present in the SMS (Contractor et
al., 2010). These systems are not very effective if
the SMSes contain grammatical errors (or the sys-
tem would require large amounts of training data in
the language model to be able to deal with all pos-
sible types of noise) in addition to misspellings etc.
Thus, the translation of a cleaned SMS, into a second
language, will not be very accurate and it would not
give good results if such a translated SMS is used to
query an FAQ collection.
Token based noise-correction techniques (such as
those using edit-distance, LCS etc) cannot be di-
rectly applied to handle the noise present in the SMS
query. These noise-correction methods return a list
of candidate terms for a given noisy token (E.g.
?gud? ? > ?god?,?good?,?guide? ) . Considering all
these candidate terms and their corresponding trans-
lations drastically increase the search space for any
multi-lingual IR system. Also , naively replacing the
noisy token in the SMS query with the top matching
candidate term gives poor performance as shown by
our experiments. Our algorithm handles these and
related issues in an efficient manner.
In this paper we address the challenges arising
when building a cross language FAQ-based ques-
tion answering system over an SMS interface. Our
method handles noisy representation of questions in
a source language to retrieve answers across target
languages. The proposed method does not require
hand corrected data or an aligned corpus for explicit
SMS normalization to mitigate the effects of noise.
It also works well with grammatical noise. To the
best of our knowledge we are the first to address
issues in noisy SMS based cross-language FAQ re-
trieval. We propose an efficient algorithm that can
handle noise in the form of lexical and semantic cor-
ruptions in the source language.
2 Problem formulation
Consider an input SMS Se in a source language
e. We view Se as a sequence of n tokens Se =
s1, s2, . . . , sn. As explained in the introduction, the
input is bound to have misspellings and other lexical
and semantic distortions. Also let Qh denote the set
88
of questions in the FAQ corpus of a target language
h. Each question Qh ? Qh is also viewed as a se-
quence of tokens. We want to find the question Q?h
from the corpus Qh that best matches the SMS Se.
The matching is assisted by a source dictionary
De consisting of clean terms in e constructed from
a general English dictionary and a domain dictio-
nary of target language Dh built from all the terms
appearing in Qh. For a token si in the SMS in-
put, term te in dictionary De and term th in dictio-
nary Dh we define a cross-lingual similarity mea-
sure ?(th, te, si) that measures the extent to which
term si matches th using the clean term te. We con-
sider th a cross lingual variant of si if for any te the
cross language similarity measure ?(th, te, si) > .
We denote this as th ? si.
We define a weight function ?(th, te, si) using the
cross lingual similarity measure and the inverse doc-
ument frequency (idf) of th in the target language
FAQ corpus. We also define a scoring function to as-
sign a score to each question in the corpusQh using
the weight function. Consider a question Qh ? Qh.
For each token si, the scoring function chooses the
term from Qh having the maximum weight using
possible clean representations of si; then the weight
of the n chosen terms are summed up to get the
score. The score measures how closely the question
in FAQ matches the noisy SMS string Se using the
composite weights of individual tokens.
Score(Qh) =
n?
i=1
max
th?Qh,te?De & th?si
?(th, te, si)
Our goal is to efficiently find the question Q?h having
the maximum score.
3 Noise removal from queries
In order to process the noisy SMS input we first have
to map noisy tokens in Se to the possible correct lex-
ical representations. We use a similarity measure to
map the noisy tokens to their clean lexical represen-
tations.
3.1 Similarity Measure
For a term te ? De and token si of the SMS input
Se, the similarity measure ?(te, si) between them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si)
if te and si share
same starting
character *
0 otherwise
(1)
Where LCSRatio(te, si) =
length(LCS(te,si))
length(te)
and LCS(te, si)
is the Longest common subsequence between te and si.
* The intuition behind this measure is that people typically type the
first few characters of a word in an SMS correctly. This way we limit
the possible variants for a particular noisy token
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is the
ratio of the length of their LCS and the length of the
longer string. Since in the SMS scenario, the dictio-
nary term will always be longer than the SMS token,
the denominator of LCSRatio is taken as the length
of the dictionary term.
The EditDistanceSMS (Figure 2) compares the
Consonant Skeletons (Prochasson et al, 2007) of the
dictionary term and the SMS token. If the Leven-
shtein distance between consonant skeletons is small
then ?(te, si) will be high. The intuition behind us-
ing EditDistanceSMS can be explained through
an example. Consider an SMS token ?gud? whose
most likely correct form is ?good?. The longest
common subsequence for ?good? and ?guided? with
?gud? is ?gd?. Hence the two dictionary terms
?good? and ?guided? have the same LCSRatio of 0.5
w.r.t ?gud?, but the EditDistanceSMS of ?good?
is 1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result the
similarity measure between ?gud? and ?good? will
be higher than that of ?gud? and ?guided?. Higher
the LCSRatio and lower the EditDistanceSMS ,
higher will be the similarity measure. Hence, for
a given SMS token ?byk?, the similarity measure of
word ?bike? is higher than that of ?break?.
4 Cross lingual similarity
Once we have potential candidates which are the
likely disambiguated representations of the noisy
89
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 2: EditDistanceSMS
term, we map these candidates to appropriate terms
in the target language. We use a statistical dictionary
to achieve this cross lingual mapping.
4.1 Statistical Dictionary
In order to build a statistical dictionary we use
the statistical translation model proposed in (Brown
et al, 2007). Under IBM model 2 the transla-
tion probability of source language sentence e? =
{t1e, . . . , t
j
e, . . . , t
m
e } and a target language sentence
h? = {t1h, . . . , t
i
h, . . . , t
l
e} is given by
Pr(h?|e?) = ?(l|m)
l?
i=1
m?
j=0
?(tih|t
j
e)a(j|i,m, l).
(2)
Here the word translation model ?(th|te) gives the
probability of translating the source term to target
term and the alignment model a(j|i,m, l) gives the
probability of translating the source term at position
i to a target position j. This model is learnt using an
aligned parallel corpus.
Given a clean term tie in source language we get
all the corresponding terms T = {t1h, . . . , t
k
h, . . .}
from the target language such that word translation
probability ?(tkh|t
i
e) > ?. We rank these terms ac-
cording to the probability given by the word trans-
lation model ?(th|te) and consider only those tar-
get terms that are part of domain dictionary i.e.
tkh ? D
h.
4.2 Cross lingual similarity measure
For each term si in SMS input query, we find all
the clean terms te in source dictionary De for which
similarity measure ?(te, si) > ?. For each of these
term te, we find the cross lingual similar terms Tte
using the word translation model. We compute the
cross lingual similarity measure between these terms
as
?(si, te, th) = ?(te, si).?(th, te) (3)
The measure selects those terms in target lan-
guage that have high probability of being translated
from a noisy term through one or more valid clean
terms.
4.3 Cross lingual similarity weight
We combine the idf and the cross lingual similarity
measure to define the cross lingual weight function
?(th, te, si) as
?(th, te, si) = ?(th, te, si).idf(th) (4)
By using idf we give preference to terms that are
highly discriminative. This is necessary because
queries are distinguished from each other using in-
formative words. For example for a given noisy
token ?bck? if a word translation model produces
a translation output ?wapas? (as in came back) or
?peet? or ?qamar? (as in back pain) then idf will
weigh ?peet? more as it is relatively more discrim-
inative compared to ?wapas? which is used fre-
quently.
5 Pruning and matching
In this section we describe our search algorithm and
the preprocessing needed to find the best question
Q?h for a given SMS query.
5.1 Indexing
Our algorithm operates at a token level and its corre-
sponding cross lingual variants. It is therefore nec-
essary to be able to retrieve all questions Qhth that
contain a given target language term th. To do this
efficiently we index the questions in FAQ corpus us-
ing Lucene5. Each question in FAQ is treated as a
document. It is tokenized using whitespace as de-
limiter before indexing.
5http://lucene.apache.org/java/docs/
90
The cross lingual similarity weight calculation re-
quires the idf for a given term th. We query on this
index to determine the number of documents f that
contain th. The idf of each term in Dh is precom-
puted and stored in a hashtable with th as the key.
The cross lingual similarity measure calculation re-
quires the word translation probability for a given
term te. For every te in dictionary De, we store
Tte in a hashmap that contains a list of terms in the
target language along with their statistically deter-
mined translation probability ?(th|te) > ?, where
th ? Dh.
Since the query and the FAQs use terms from dif-
ferent languages, the computation of IDF becomes a
challenge (Pirkola, 1998) (Oard et al, 2007). Prior
work uses a bilingual dictionary for translations for
calculating the IDF. We on the other hand rely on
a statistical dictionary that has translation probabil-
ities. Applying the method suggested in the prior
work on a statistical dictionary leads to errors as the
translations may themselves be inaccurate.
We therefore calculate IDFs for target language
term (translation) and use it in the weight measure
calculation. The method suggested by Oard et al
(Oard et al, 2007) is more useful in retrieval tasks
for multiple documents, while in our case we need
to retrieve a specific document (FAQ).
5.2 List Creation
Given an SMS input string Se, we tokenize it on
white space and replace any occurrence of digits to
their string based form (e.g. 4get, 2day) to get a se-
ries of n tokens s1, s2, . . . , sn. A list Lei is created
for each token si using terms in the monolingual dic-
tionary De. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop word.
A term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (5)
The threshold value ? is determined experimen-
tally. For every te ? Lei we retrieve Tte and then
retrieve the idf scores for every th ? Tte . Using the
word translation probabilities and the idf score we
compute the cross lingual similarity weight to create
a new list Lhi . A term th is included in the list only
if
?(th|te) > 0.1 (6)
This probability cut-off is used to prevent poor
quality translations from being included in the list.
If more than one term te has the same transla-
tion th, then th can occur more than once in a given
list. If this happens, then we remove repetitive oc-
currences of th and assign it a weight equal to the
maximum weight amongst all occurrences in the list,
multiplied by the number of times it occurs. The
terms th in Lhi are sorted in decreasing order of their
similarity weights. Henceforth, the term ?list? im-
plies a sorted list.
For example given a SMS query ?hw mch ds it cst
to stdy in india? as shown in Fig. 3, for each token
we create a list of possible correct dictionary words
by dictionary look up. Thus for token ?cst? we get
dictionary words lik ?cost, cast, case, close?. For
each dictionary word we get a set of possible words
in Hindi by looking at statistical translation table.
Finally we merged the list obtained to get single list
of Hindi words. The final list is ranked according to
their similarity weights.
5.3 Search algorithm
Given Se containing n tokens, we create n sorted
lists Lh1 , L
h
2 , . . . , L
h
n containing terms from the do-
main dictionary and sorted according to their cross
lingual weights as explained in the previous section.
A naive approach would be to query the index using
each term appearing in all Lhi to build a Collection
set C of questions. The best matching question Q?h
will be contained in this collection. We compute the
score of each question in C using Score(Q) and the
question with highest score is treated as Q?h. How-
ever the naive approach suffers from high runtime
cost.
Inspired by the Threshold Algorithm (Fagin et
al., 2001) we propose using a pruning algorithm
that maintains a much smaller candidate set C of
questions that can potentially contain the maximum
scoring question. The algorithm is shown in Fig-
ure 4. The algorithm works in an iterative manner.
In each iteration, it picks the term that has maxi-
mum weight among all the terms appearing in the
lists Lh1 , L
h
2 , . . . , L
h
n. As the lists are sorted in the
descending order of the weights, this amounts to
picking the maximum weight term amongst the first
terms of the n lists. The chosen term th is queried to
find the set Qth . The set Qth is added to the candi-
91
Figure 3: List creation
date set C. For each question Q ? Qth , we compute
its score Score(Q) and keep it along with Q. After
this the chosen term th is removed from the list and
the next iteration is carried out. We stop the iterative
process when a thresholding condition is met and fo-
cus only on the questions in the candidate set C. The
thresholding condition guarantees that the candidate
set C contains the maximum scoring question Q?h.
Next we develop this thresholding condition.
Let us consider the end of an iteration. Sup-
pose Q is a question not included in C. At
best, Q will include the current top-most tokens
Lh1 [1], L
h
2 [1], . . . , L
h
n[1] from every list. Thus, the
upper bound UB on the score of Q is
Score(Q) ?
n?
i=0
?(Lhi [1]).
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set C
cannot be the maximum scoring question. Thus, the
condition ?Q? ? UB? serves as the termination cri-
terion. At the end of each iteration, we check if the
termination condition is satisfied and if so, we can
stop the iterative process. Then, we simply pick the
question in C having the maximum score and return
it.
Procedure Search Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?h.
Begin
?si, construct Lei for which ?(si, te) > 
// Li lists variants of si
Construct lists Lh1 , L
h
2 , . . . , L
h
n //(see Section 5.2).
// Lhi lists cross lingual variants of si in decreasing
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(L
h
i [1])
t?h = L
h
j? [1]
// t?h is the term having maximum weight among
// all terms appearing in the n lists.
Delete t?h from the list L
h
j? .
Retrieve Qt?h using the index
// Qt?h : the set of all questions in Q
h
//having the term t?h
For each Q ? Qt?h
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(L
h
i [1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 4: Search Algorithm with Pruning
6 Experiments
To evaluate our system we used noisy English SMS
queries to query a collection of 10, 000 Hindi FAQs.
These FAQs were collected from websites of vari-
ous government organizations and other online re-
sources. These FAQs are related to railway reser-
vation, railway enquiry, passport application and
health related issues. For our experiments we asked
6 human evaluators, proficient in both English and
Hindi, to create English SMS queries based on the
general topics that our FAQ collection dealt with.
We found 60 SMS queries created by the evaluators,
had answers in our FAQ collection and we desig-
nated these as the in-domain queries. To measure
the effectiveness of our system in handling out of
domain queries we used a total of 380 SMSes part of
which were taken from the NUS corpus (How et al,
92
whch metro statn z nr pragati maidan ?
dus metro goes frm airpot 2 new delhi rlway statn?
is dere any special metro pas 4 delhi uni students?
whn is d last train of delhi metro?
whr r d auto stands N delhi?
Figure 5: Sample SMS queries
2005) and the rest from the ?out-of-domain? queries
created by the human evaluators. Thus the total SMS
query data size was 440. Fig 5 shows some of the
sample queries.
Our objective was to retrieve the correct Hindi
FAQ response given a noisy English SMS query. A
given English SMS query was matched against the
list of indexed FAQs and the best matching FAQ was
returned by the Pruning Algorithm described in Sec-
tion 5. A score of 1 was assigned if the retrieved
answer was indeed the response to the posed SMS
query else we assigned a score of 0. In case of out
of domain queries a score of 1 was assigned if the
output was NULL else we assigned a score of 0.
6.1 Translation System
We used the Moses toolkit (Koehn et al, 2007) to
build an English-Hindi statistical machine transla-
tion system. The system was trained on a collec-
tion of 150, 000 English and Hindi parallel sentences
sourced from a publishing house. The 150, 000 sen-
tences were on a varied range of subjects such as
news, literature, history etc. Apart from this the
training data also contained an aligned parallel cor-
pus of English and Hindi FAQs. The FAQs were
collected from government websites on topics such
as health, education, travel services etc.
Since an MT system trained solely on a collection
of sentences would not be very accurate in translat-
ing questions, we trained the system on an English-
Hindi parallel question corpus. As it was difficult
to find a large collection of parallel text consisting
of questions, we created a small collection of par-
allel questions using 240 FAQs and multiplied them
to create a parallel corpus of 50, 000 sentences. This
set was added to the training data and this helped fa-
miliarize the language model and phrase tables used
by the MT systems to questions. Thus in total the
MT system was trained on a corpus of 200, 000 sen-
tences.
Experiment 1 and 2 form the baseline against
which we evaluated our system. For our experi-
ments the lexical translation probabilities generated
by Moses toolkit were used to build the word trans-
lation model. In Experiment 1 the threshold ? de-
scribed in Equation 5 is set to 1. In Experiment 2
and 3 this is set to 0.5. The Hindi FAQ collection
was indexed using Lucene and a domain dictionary
Dh was created from the Hindi words in the FAQ
collection.
6.2 System Evaluation
We perform three sets of experiments to show how
each stage of the algorithm contributes in improving
the overall results.
6.2.1 Experiment 1
For Experiment 1 the threshold ? in Equation 5
is set to 1 i.e. we consider only those tokens in the
query which belong to the dictionary. This setup il-
lustrates the case when no noise handling is done.
The results are reported in Figure 6.
6.2.2 Experiment 2
For Experiment 2 the noisy SMS query was
cleaned using the following approach. Given a noisy
token in the SMS query it?s similarity (Equation 1)
with each word in the Dictionary is calculated. The
noisy token is replaced with the Dictionary word
with the maximum similarity score. This gives us
a clean English query.
For each token in the cleaned English SMS query,
we create a list of possible Hindi translations of the
token using the statistical translation table. Each
Hindi word was assigned a weight according to
Equation 4. The Pruning algorithm in Section 5 was
then applied to get the best matching FAQ.
6.2.3 Experiment 3
In this experiment, for each token in the noisy En-
glish SMS we obtain a list of possible English vari-
ations. For each English variation a corresponding
set of Hindi words from the statistical translation ta-
ble was obtained. Each Hindi word was assigned
a weight according to Equation 4. As described in
Section 5.2, all Hindi words obtained from English
variations of a given SMS token are merged to create
93
Experiment 1 Experiment 2 Experiment 3
MRR Score 0.41 0.68 0.83
Table 1: MRR Scores
F1 Score
Expt 1 (Baseline 1) 0.23
Expt 2 (Baseline 2) 0.68
Expt 3 (Proposed Method) 0.72
Table 2: F1 Measure
a list of Hindi words sorted in terms of their weight.
The Pruning algorithm as described in Section 5 was
then applied to get the best matching FAQ.
We evaluated our system using two different cri-
teria. We used MRR (Mean reciprocal rank) and
the best matching accuracy. Mean reciprocal rank
is used to evaluate a system by producing a list of
possible responses to a query, ordered by probabil-
ity of correctness. The reciprocal rank of a query
response is the multiplicative inverse of the rank of
the first correct answer. The mean reciprocal rank
is the average of the reciprocal ranks of results for a
sample of queries Q.
MRR = 1/|Q|
Q?
i=1
1/ranki (7)
Best match accuracy can be considered as a spe-
cial case of MRR where the size of the ranked list is
1. As the SMS based FAQ retrieval system will be
used via mobile phones where screen size is a ma-
jor constraint it is crucial to have the correct result
on the top. Hence in our settings the best match ac-
curacy is a more relevant and stricter performance
evaluation measure than MRR.
Table 1 compares the MRR scores for all three
experiments. Our method reports the highest MRR
of 0.83. Figure 6 shows the performance using the
strict evaluation criterion of the top result returned
being correct.
We also experimented with different values of
the threshold for Score(Q) (Section 5.3). The ROC
curve for various threshold is shown in Figure 7. The
result for both in-domain and out-of-domain queries
for the three experiments are shown in Figure 6 for
Score(Q) = 8. The F1 Score for experiments 1, 2 and
3 are shown in Table 2.
Figure 6: Comparison of results
Figure 7: ROC Curve for Score(Q)
6.3 Measuring noise level in SMS queries
In order to quantify the level of noise in the col-
lected SMS data, we built a character-level language
model(LM) using the questions in the FAQ data-set
(vocabulary size is 70) and computed the perplexity
of the language model on the noisy and the cleaned
SMS test-set. The perplexity of the LM on a cor-
pus gives an indication of the average number of bits
needed per n-gram to encode the corpus. Noise re-
Cleaned SMS Noisy SMS
English FAQ collection
bigram 16.64 55.19
trigram 9.75 69.41
Table 3: Perplexity for Cleaned and Noisy SMS
94
sults in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits are
needed to encode these improbable n-grams which
results in increased perplexity. From Table 3 we can
see the difference in perplexity for noisy and clean
SMS data for the English FAQ data-set. Large per-
plexity values for the SMS dataset indicates a high
level of noise.
For each noisy SMS query e.g. ?hw 2 prvnt ty-
phd? we manually created a clean SMS query ?how
to prevent typhoid?. A character level language
model using the questions in the clean English FAQ
dataset was created to quantify the level of noise in
our SMS dataset. We computed the perplexity of the
language model on clean and noisy SMS queries.
7 Conclusion
There has been a tremendous increase in information
access services using SMS based interfaces. How-
ever, these services are limited to a single language
and fail to scale for multilingual QA needs. The
ability to query a FAQ database in a language other
than the one for which it was developed is of great
practical significance in multilingual societies. Au-
tomatic cross-lingual QA over SMS is challenging
because of inherent noise in the query and the lack
of cross language resources for noisy processing. In
this paper we present a cross-language FAQ retrieval
system that handles the inherent noise in source lan-
guage to retrieve FAQs in a target language. Our sys-
tem does not require an end-to-end machine transla-
tion system and can be implemented using a sim-
ple dictionary which can be static or constructed
statistically using a moderate sized parallel corpus.
This side steps the problem of building full fledged
translation systems but still enabling the system to
be scaled across multiple languages quickly. We
present an efficient algorithm to search and match
the best question in the large FAQ corpus of tar-
get language for a noisy input question. We have
demonstrated the effectiveness of our approach on a
real life FAQ corpus.
References
Sreangsu Acharyya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition, pp. 175-184.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING-ACL, pp. 33-40.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, Robert. L. Mercer 1993. The Mathematics of
Statistical Machine Translation: Parameter Estimation
Computational Linguistics, pp. 263-311.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. AAAI Workshop on En-
hanced Messaging.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of texting
language. International Journal on Document Analy-
sis and Recognition, pp. 157-174.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, Steffen Staab. 2009. Explicit versus latent con-
cept models for cross-language information retrieval.
In Proceeding of IJCAI, pp. 1513-1518.
Danish Contractor, Tanveer A. Faruquie, L. Venkata Sub-
ramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceeding of COLING 2010: Posters, pp.
189-196.
R. Fagin, A. Lotem, and M. Naor. 2001. Optimal aggre-
gation algorithms for middleware. In Proceedings of
the 20th ACM SIGMOD-SIGACT-SIGART symposium
on Principles of database systems, pp. 102-113.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In M. J. Smith and G. Salvendy (Eds.) Proc. of
Human Computer Interfaces International,Lawrence
Erlbaum Associates
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management,CIKM, pp.
76-83.
Catherine Kobus, Francois Yvon and Grraldine Damnati.
2008. Normalizing SMS: Are two metaphors better
than one? In Proceedings of COLING, pp. 441-448.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst 2007. Moses:
Open source toolkit for statistical machine translation.
Annual Meeting of the Association for Computation
Linguistics (ACL), Demonstration Session .
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Interface
95
to Yellow Pages Directory. In Proceedings of the 4th
international conference on mobile technology, appli-
cations, and systems and the 1st international sympo-
sium on Computer human interaction in mobile tech-
nology, pp. 558-563 .
Govind Kothari, Sumit Negi, Tanveer Faruquie, Venkat
Chakravarthy and L V Subramaniam 2009. SMS
based Interface for FAQ Retrieval. Annual Meeting
of the Association for Computation Linguistics (ACL).
I. D. Melamed. 1999. Bitext maps and alignment via pat-
tern recognition. Computational Linguistics, pp. 107-
130.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS : Eval-
uation et bilan quantitatif. In Actes de TALN, pp. 123-
132.
Douglas W. Oard, Funda Ertunc. 2002. Translation-
Based Indexing for Cross-Language Retrieval In Pro-
ceedings of the ECIR, pp. 324-333.
A. Pirkola 1998. The Effects of Query Structure
and Dictionary Setups in Dictionary-Based Cross-
Language Information Retrieval SIGIR ?98: Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval , pp. 55-63.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message ser-
vices. In Proceedings of the 9th International Confer-
ence on Document Analysis and Recognition, pp. 83-
87.
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Designing
the User Experience for Google SMS. In Proceedings
of ACM SIGCHI, pp. 1777-1780.
Satoshi Sekine, Ralph Grishman. 2003. Hindi-English
cross-lingual question-answering system. ACM Trans-
actions on Asian Language Information Processing,
pp. 181-192.
E. Sneiders. 1999. Automated FAQ Answering: Contin-
ued Experience with Shallow Language Understand-
ing Question Answering Systems. Papers from the
1999 AAAI Fall Symposium. Technical Report FS-99-
02, AAAI Press, pp. 97-107.
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007. Ques-
tion similarity calculation for FAQ answering. In Pro-
ceeding of SKG 07, pp. 298-301.
X. Xue, J. Jeon, and W.B Croft. 2008. Retrieval Models
for Question and Answer Archives. In Proceedings of
SIGIR, pp. 475-482.
96
