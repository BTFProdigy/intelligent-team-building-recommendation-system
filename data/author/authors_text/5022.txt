Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 25?31,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The LexALP Information System: Term Bank and Corpus for Multi-lingual Legal Terminology Consolidated 
  Verena Lyding, Elena Chiocchetti EURAC research Viale Druso 1, 39100 Bozen/Bolzano - Italy  forename.name@eurac.edu 
Gilles S?rasset, Francis Brunet-Manquat GETA-CLIPS IMAG BP 53, 38041 Grenoble cedex 9 - France forename.name@imag.fr     Abstract Standard techniques used in multilingual terminology management fail to describe legal terminologies as they are bound to different legal systems and terms do not share a common meaning. In the LexALP project, we use a technique defined for general lexical databases to achieve cross language interoperability between lan-guages of the Alpine Convention. In this paper we present the methodology and tools developed for the collection, de-scription and harmonisation of the legal terminology of spatial planning and sus-tainable development in the four lan-guages of the countries of the Alpine Space. 1 Introduction The aim of the LexALP project is to harmo-nise the terminology used by the Alpine Conven-tion, both for internal purposes and for commu-nication among the member states. The Alpine Convention is an international treaty signed by all states of the Alpine territory (France, Monaco, Switzerland Liechtenstein, Austria, Germany, Italy and Slovenia) for the protection of land-scape and sustainable development of this moun-tain area1. The member states speak four differ-ent languages, namely French, German, Italian, and Slovene and have different legal systems and traditions. Hence arises the need for a systematization and uniformation of terminology and clear trans-lation equivalence in all four languages. For this reason, the project intends to provide all                                                 1 cf. also http: www.alpenkonvention.org  
stakeholders and the wider public with an infor-mation system which combines three main com-ponents, a terminology data base, a multilingual corpus and the relative bibliographic data base. In this way the manually revised, elaborated and validated (harmonised) quadrilingual information on the legal terminology (i.e. complete termino-logical entries) will be closely interacting with a facility to dynamically search for additional con-texts in a relevant set of legal texts in all lan-guages and for all main legal systems involved. 2 Multilingual legal information system The information system for the terminology of the Alpine Convention, with a specific focus on spatial planning and sustainable development, will give the possibility to search for relevant terms and their (harmonised or rejected) translations in all 4 official languages of the Alpine Convention in the first module, the term bank. Next to retrieving synonyms and translation equivalents within each legal system, the user will be provided with a representative context and a valid definition of the concept under consideration. Source information will be provided for each text field in the terminological entry. Via a link from the terminological data base to the second module, the corpus facility, the information system will give the possibility to search the corpus for further contexts. Finally, both term bank and corpus will be in-teracting with a third module, the bibliographic database, so as to allow retrieving full informa-tion on text excepts cited in the term bank and to store important meta data on corpus documents. 
25
3 Terminological data 3.1 Data categories and motivations The data categories present in the terminology database allow entering and organising relevant information on the concept under analysis. The term bank interface allows entering of the fol-lowing terminological data categories: denomi-nation/term, definition, context, note, sources (text fields), grammatical information to the term, harmonisation status, processing status, geo-graphical usage, frequency and domain, accord-ing to the appositely elaborated domain classifi-cation structure2 (pull down menus). Again by means of pull-down menus the terminologist will be able to signal to the users which terms are al-ready processed (i.e. checked by legal experts), harmonised or rejected and - most important - to which legal system they belong (the menu geo-graphical usage allows to specify this informa-tion). Furthermore it is possible to specify syno-nyms, short forms, abbreviations etc. in the ter-minological entry and, if necessary, link them to the relative full information already present in the term bank (however, no direct access to these linked data is possible, this must be done via the search interface). Finally, the terminologist is given the possibility of writing general com-ments to the entry. At the very end of one lan-guage entry the terminologist can decide whether to release the data to the public (by clicking on the button ?finish?) or keep it for further fine-tuning (button ?update?). Each term is created in its ?language volume? and described by means of all necessary informa-tion. As soon as one or all equivalents in the other languages are available too, the single en-tries can be linked to each other with the help of an axie (see detailed description below). Searches can be done for all languages or on a user-defined selection of source and target lan-guages. Presently the database allows global searching in all text fields and filtering by source, author, date of creation, as well as by axie name and ids. Results can be displayed in full form, as a short list of terms only or in XML. Some ex-port/import functions are granted. As the term bank serves mainly the scope of diffusing harmonised terminology, the four trans-lation equivalents (validated by a group of ex-perts) are displayed together, whereas rejected synonyms are displayed separately for each search language. In this way the user may well                                                 2 See also 4.1  
look for a non validated synonym and find it in the database but be warned as to which is the preferred term and its harmonised equivalents in the other languages. Figure 1 shows such a situa-tion where the French rejected term ?transport intra-alpin? is linked to the harmonised term ?trafic intra-alpin?.  
 Figure 1: A set of Alpine Convention terms and their relations 3.2 Monolingual data The LexALP term bank consists in 5 volumes for French, German, Italian, Slovene and English (no data is being entered for this fifth language at the moment), which contain the term descrip-tions. The set of data categories is represented in an XML structure that follows a common schema.  <entry id="fra.trafic_intra-alpin.1010743.e"        lang="fra"        legalSystem="AC"        process_status="FINALISED"        status="HARMONISED">   <term>trafic intra-alpin</term>   <grammar>n.m.</grammar>   <domain>Transport</domain>   <usage frequency="common"          geographical-code="INT"          technical="false"/>   <relatedTerm      isHarmonised="false"     relationToTerm="Synonym"     termref="fra.transport_intra-alpin?"/>   <relatedTerm      isHarmonised="false"     relationToTerm="Synonym"     termref="fra.circulation_intra-?"/>   <definition>     [T]rafic constitu? de trajets ayant leur      point de d?part et/ou d'arriv?e ?      l'int?rieur de l'espace alpin.   </definition>   <source>Prot. Transp., art. 2 </source>   <context url="http://www...">     Des projets routiers ? grand d?bit pour      le trafic intra-alpin peuvent ?tre      r?alis?s, si [...].   </context> </entry> Figure 2: XML form of the term ?trafic intra-alpin? Each entry represents a unique term/meaning. Terms with the same denomination, but belong-
26
ing to different legal systems have, de facto, dif-ferent meanings. Hence, different entries are cre-ated. Terms with different denominations but conveying the same ?meaning? (concept) are also represented using different entries3. In this case, the entries are linked through a synonymy rela-tion. Figure 2 shows the XML structure of the French term ?trafic intra-alpin?, as defined in the Alpine Convention. The term entry is associated to a unique identifier used to establish relations between volume entries. The example term belongs to the Alpine Con-vention legal system4 (code AC). The entry also bears the information on its status (harmonised or rejected) and its processing status (to be proc-essed, provisionally processed or finalised). In addition, a definition (along with its source) and a context may be given. The definition and context should be extracted from a legal text, which must be identified in the source field. 3.3 Achieving language/legal system interoperability As the project deals with several different le-gal terms, standard techniques used in multilin-gual terminology management need to be adapted to the peculiarities of the specialised language of the law. Indeed, terms in different languages are (generally) defined according to different legal systems and these legal systems cannot be changed. Hence, it is not possible to define a common ?meaning? that could be used as a pivot for language interoperability5. In this respect, legal terminology is closer to general lexicography than to standard terminology. In order to achieve language/legal system interoperability we had several options that are used in general lexicography.  Using a set of bilingual dictionaries is not an option here, as we have to deal with at least 16                                                 3 Variants, acronyms, etc. are not considered as dif-ferent denominations. 4 Strictly speaking, the Alpine Convention does not constitute a legal system per se. 5 Consider for instance the difference between the Italian and the Austrian concepts of journalists? pro-fessional confidentiality. Whereas the Redaktionsge-heimnis explicitly underlines that the journalist can refuse to witness in court in order to keep the profes-sional secret, in Italy the segreto giornalistico must obligatorily be lifted on a judge?s request. The two concepts have overlapping meanings in the two states, however, they diverge greatly with respect to the be-haviour in court.  
language/legal system couples (with alpine Con-vention and EU levels, but without taking into account regional levels). Moreover, such a solu-tion will not reflect the multilingual aspect of the Alpine Convention or the Swiss legal system. Finally, building bilingual volumes between the French and Italian legal systems is far beyond the objectives of the LexALP project. Another solution would be to use an ?Eu-rowordnet like? approach (Vossen, 1998) where a specific language/legal system is used as a pivot and elements of the other systems are linked by equivalent or near-equivalent links. As such an approach artificially puts a language in the pivot position, it generally leads to an ?eth-nocentric? view of the other languages. The ad-vantage being that the architecture uses the bilin-gual competence of lexicographers to achieve multilingualism.  In this project, we chose to use ?interlingual acceptions? (a.k.a. axies) as defined in (S?rasset, 1994) to represent such complex contrastive phenomena as generally described in general lexicography work. In this approach, each ?term meaning? is associated to an interlingual accep-tion (or axie). These axies are used to achieve interoperability as a pivot linking terms of differ-ent languages bearing the same meaning. However, as we are dealing with legal terms (bound to different legal systems), it is generally not possible to find terms in different languages that bear the same meaning. In fact such terms can only be found in the Alpine Convention (which is considered as a legal system expressed in all the considered languages). Hence, we use these terms to achieve interoperability between languages. In this aspect, we are close to Eu-rowordnet?s approach as we use a specific legal system as a pivot, but in our case the pivot itself is generally a quadrilingual set of entries.  These harmonised Alpine Convention terms are linked through an interlingual acception. An axie is a place holder for relations. Each interlin-gual acception may be linked to several term en-tries in the languages volumes through termref elements and to other interlingual acceptions through axieref elements, as illustrated in Figure 3. <axie id="axi..1011424.e">  <termref    idref="ita.traffico_intraalpino.1010654.e"    lang="ita"/>  <termref    idref="fra.trafic_intra-alpin.1010743.e"    lang="fra"/>  <termref   idref="deu.inneralpiner_Verkehr.1011065.e"  
27
  lang="deu"/>  <termref    idref="slo.znotrajalpski_promet.1011132.e"    lang="slo"/>  <axieref idref=""/>  <misc></misc> </axie> Figure 3: XML form of the interlingual acception illustrated Figure 1 The termref relation establishes a direct translation relation between these harmonised equivalents. Then, national legal terms are indi-rectly linked to Alpine Convention terms through the axieref relation as illustrated in Figure 4. 
 Figure 4: An example French term, linked to a quadrilingual Alpine Convention Term. 4 Corpus 4.1 Corpus content The corpus comprises around 3000 legal documents of eight legal systems (Germany, It-aly, France, Switzerland, Austria, Slovenia, European law  and international law with the specific framework of the Alpine Convention,) (see table 1).   AT CH DE FR IT SI AC EU INT 612 119 62 613 490 213 38 791 149 Table 1: Corpus documents for each legal system Documents of the supranational level are pro-vided in up to four languages (subject to avail-ability). National legislation is generally added in the national language (monolingual documents) and in case of Switzerland (multilingual docu-ments) in the three official languages of that na-tion (French, German and Italian). The documents are selected by legal experts of the respective legal systems following prede-fined criteria: ? entire documents (no single paragraphs or excerpts etc.); ? strong relevance to the subjects ?spatial planning and sustainable development? as described in art. 9 of the relative Alpine Convention Protocol; 
? primary sources of the law for every sys-tem at national and international/EU level, i.e. normative texts only (laws, codes etc.);  ? latest amendments and versions of all legislation (at time of collection: June ? August 2005); ? terminological relevance. Each document is classified according to the following (bibliographical) categories: full title, short title, abbreviation, legal system, language, legal hierarchy, legal text type, subfield (1, 2 and 3), official date, official number, published in official journal (date, number, page), ? The bib-liographical information of all documents is stored in a database and can at any time be con-sulted by the user. The subfields have been elaborated and se-lected by a team of legal experts, taking into ac-count the classification specificities followed by the Alpine Convention and the need to classify texts from several different legal systems accord-ing to one common structure. For this reason, the legal experts have subdivided the fields spatial planning and sustainable development into 5 main areas, in accordance with the Alpine Con-vention Protocol dealing with these subjects and subsequently adopted an EU-based model for further subdividing the 5 main topics in such a way that all countries involved could classify their selected documents under a maximum of 3 main items, the first of which must be indicated obligatorily. This classification allows an easy selection of all subsets of documents according to subject field. 
 Figure 5: Example of document classification 
28
<header   lang="ita"  creator="X"  created="Fri Feb 17 10:45:15 CET 2006"> <h.title>  Legge_regionale_25974.14_87.txt </h.title>  <bibID>  17658 </bibID> </header> Figure 6: XML-header of corpus documents <text id="17658"> <body id="17658.b"> <div type="intro" id="17658.b.i"> <p id="17658.b.i.p1"> <title id="17658.b.i.p1.ti1"> LEGGE REGIONALE 15/05/1987, N. 014  Disciplina dell' esercizio [?] di fauna selvatica.  </title> </p> </div> <div type="section" id="17658.b.c0.se1"> <p id="17658.b.c0.se1.p1"> <title id="17658.b.c0.se1.p1.ti1"> Art. 1 </title> </p> <p id="17658.b.c0.se1.p2"> <s id="17658.b.c0.se1.p2.s1"> 1. Sull' intero territorio  regionale la caccia selettiva  per qualita', [?] </s> <s id="17658.b.c0.se1.p2.s2"> a) capriolo: dal 15 maggio al  15 gennaio;  </s> <s id="17658.b.c0.se1.p2.s3"> b) cinghiale: dal 15 giugno  al 15 gennaio;  </s> </p> <p id="17658.b.c0.se1.p3"> <s id="17658.b.c0.se1.p3.s1"> 2. E' ammesso l' uso [?]  </s> </p> </div> </body> </text> Figure 7: XML-structure of corpus document 4.2 Structural organization of corpus data Collected in raw text format (one file for each legal text) the documents are first transformed into XML-structured files and in a second step inserted into the database.  The XML-annotation is done in compliance with the Corpus Encoding Standard for XML (XCES) 6 . Slightly simplified, the provided schema7 serves to add structural information to the documents. Each text is segmented into sub-sections like: preamble, chapter, section, para-                                                6 http://www.cs.vassar.edu/XCES/ 7 http://www.cs.vassar.edu/XCES/schema/xcesDoc.xsd 
graph, title and sentence. Furthermore, a link to the classification data (bibliographic data base) is inserted and, in case of multilingual documents, alignment is done at sentence level. The XML-annotated documents hold all the information needed for the insertion into the cor-pus database, such as structural mark-up and bib-liographical information. The full text documents are transformed into sets of database entries, which can be imported into the database. 4.3 Technical organization of corpus data Following the bistro approach as realized for the Corpus Ladin dl?Eurac (CLE) (Streiter et al 2004) the corpus data is stored in a relational database (PostgreSQL). The information present in the XML-annotated documents is distributed among four main tables: document_info, cor-pus_words, corpus_structure, corpus_alignment.   The four tables can be described as follows: document_info: This table holds the meta-information about the documents; each category (like full title, short title, abbreviation, legal sys-tem, language, etc.) is represented by a separate column. For each legal document one entry (one row) with unique identification number is added to the table. These identification numbers are cited in the XML-header of the corpus docu-ments. corpus_words: This table holds the actual text of the collected documents. Instead of stor-ing entire paragraphs as it was done during the creation of CLE, for this corpus a different ap-proach is being tested. Every annotated text is split into an indexed sequence of words, starting with counter one. Once inserted into the database a text is stored as a set of tuples composed of word, position in text and document id (as a ref-erence to the document information).  corpus_structure: This table holds all infor-mation about the internal structure of the docu-ments. Titles, sentences, paragraphs etc. are stored by indicating starting and ending point of the section. For each segment a tuple of segment type, segment id, starting point (indicated by the index of the first word), ending point (indicated by the index of the last word) and document id is added. corpus_alignment: This table defines the alignment of multilingual documents. By provid-ing one column for each language the texts are aligned via the document ids or via the ids of single segments.  
29
The tables are interconnected by explicitly stated references. That means that the columns of one table refer to the values of a certain column of another table. As shown in figure 8 all tables hold a column document_id that refers to the document id of the table document_info. Fur-thermore, the table corpus_structure holds refer-ences to the column position of the table cor-pus_words.  
 Figure 8: Interconnection of tables 5 Searching the corpus Due to the fine-grained classification (see section 4.1) and the structural mark-up (see section 4.2) of all corpus documents, corpus searches can be restricted in the following ways: ? by specifying a subset of corpus docu-ments over which the search should be carried out (e.g. all documents of legal system CH with language French); ? by choosing the type of unit to be dis-played (whole paragraphs <p>, sentences <s>, titles <title>, ?); ? by searching for whole words only (ex-act match) or parts of words (fuzzy match); ? by restricting the number of hits to be displayed at a time. For searches in multilingual documents it will be possible to search for aligned segments, specify-ing search word as well as target translation. For example, the user could search for all alignments of German-Italian sentences that contain the word Umweltschutz translated as tutela ambien-tale (and not with protezione dell?ambiente). Figure 9 shows a simple interface for searching monolingual documents. 
 Figure 9: Example search over monolingual documents 6 Interaction term bank and corpus Term bank and corpus are independent compo-nents which together form the LexALP Informa-tion System. The interaction between corpus and term bank will concern in particular 1) corpus segments used as contexts and definitions in the termino-logical entries, 2) short source references in the term bank (and the associated sets of biblio-graphical information) and 3) legal terms. 6.1 Entering data into term bank When adding citations to a term bank entry, the relative bibliographic information will automati-cally be counterchecked with the contents of the bibliographical database. In case the information about the cited document is already present in the DB, a link to the term bank can be added. Oth-erwise the terminologist is asked to provide all information about the new source to the biblio-graphic database and later create the link. Next to static contexts and definitions present for each terminological entry, each entry will show a button for the dynamic creation of con-texts. Hitting the button will start a context search in the corpus and return all sentences con-taining the term under consideration. 6.2 Searching the corpus When searching the corpus the user will have the opportunity to highlight terms present in the term bank. In the same way standardised or rejected terms can be brought out. Via a link it will then 
30
be possible to directly access the term bank entry for the term found in the corpus. In general each corpus segment is linked to the full set of bibliographic information of the document that the segment is part of. Accessing the source information will lead the user to a de-tailed overview as shown in figure 4. 7 Conclusion In this paper, we have presented the LexALP information system, used to collect, describe and harmonise the terminology used by the Alpine Convention and to link it with national legal ter-minology of the alpine Convention?s member states. Even if we currently give a specific focus on spatial planning and sustainable development, the project is not restricted to these fields and the methodology and tools developed can be adapted to legal terminology of other fields.  In this paper we also proposed a solution to the encoding of multilingual legal terminologies in a context where standard techniques used in multilingual terminology management usually fail. The terminology developed and the corpus used for its development will be accessible on-line for the stakeholders and the wider public through the LexALP information system. 
Acknowledgements The LexALP research project started in Janu-ary 2005 thanks to the funds granted by the IN-TERREG IIIB ?Alpine Space? Programme, a Community Initiative Programme funded by the European Regional Development Fund. References Gilles S?rasset. 1994. Interlingual Lexical Organisa-tion for Multilingual Lexical Databases in NADIA.  In Makoto Nagao, editor, COLING-94, volume 1, pages 278?282, August. Streiter, O., Stuflesser, M. & Ties, I. (2004). CLE, an aligned Tri-lingual Ladin-Italian-German Corpus. Corpus Design and Interface, LREC 2004, Work-shop on "First Steps for Language Documentation of Minority Languages: Computational Linguistic Tools for Morphology, Lexicon and Corpus Com-pilation" Lisbon, May 24, 2004. Vossen, Piek. 1998. Introduction to EuroWordNet. In Nancy Ide, Daniel Greenstein, and Piek Vossen, editors, Special Issue on EuroWordNet, Computers and the Humanities, 32(2-3): 73-89. Wright, Sue Ellen 2001. Data Categories for Termi-nology Management. In Sue Ellen Wright & Gerhard Budin, editors, Handbook of Terminology Management, volume 2, pages 552-569.  
31
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 44?48,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Visualising Linguistic Evolution in Academic Discourse
Verena Lyding
European Academy of Bolzano-Bozen
verena.lyding@eurac.edu
Ekaterina Lapshinova-Koltunski
Saarland University
e.lapshinova@mx.uni-saarland.de
Stefania Degaetano-Ortlieb
Saarland University
s.degaetano@mx.uni-saarland.de
Henrik Dittmann
European Academy of Bolzano-Bozen
henrik.dittmann@eurac.edu
Christopher Culy
The University of Tu?bingen
christopher.culy@uni-tuebingen.de
Abstract
The present paper describes procedures to
visualise diachronic language changes in
academic discourse to support analysis.
These changes are reflected in the distri-
bution of different lexico-grammatical fea-
tures according to register. Findings about
register differences are relevant for both lin-
guistic applications (e.g., discourse analysis
and translation studies) and NLP tasks (no-
tably automatic text classification).
1 Introduction
The present paper describes procedures to visu-
alise diachronic language changes in academic
discourse with the aim to facilitate analysis
and interpretation of complex data. Diachronic
changes are reflected by linguistic features of reg-
isters under analysis. Registers are patterns of lan-
guage according to use in context, cf. (Halliday
and Hasan, 1989).
To analyse register change, we extract lexico-
grammatical features from a diachronic corpus of
academic English, and visualise our extraction re-
sults with Structured Parallel Coordinates (SPC),
a tool for the visualisation of structured multidi-
mensional data, cf. (Culy et al, 2011).
Our approach is based on the inspection and
comparison of how different features change over
time and registers. The major aim is to deter-
mine and describe tendencies of features, which
might become rarer, more frequent or cluster in
new ways. The amount and complexity of the in-
terrelated data, which is obtained for nine disci-
plines in two time periods (see section 2) makes
the analysis more difficult.
Structured Parallel Coordinates provide a tool
for the compact visual presentation of complex
data. The visualisation of statistical values for
different linguistic features laid out over time and
register supports data analysis as tendencies be-
come apparent. Furthermore, interactive features
allow for taking different views on the data and
focussing on interesting aspects.
2 Data to Analyse
2.1 Features and theoretical background
When defining lexico-grammatical features, we
refer to Systemic Functional Linguistics (SFL)
and register theory, e.g., (Quirk, 1985), (Halliday
and Hasan, 1989) and (Biber, 1995), which are
concerned with linguistic variation according to
contexts of use, typically distinguishing the three
contextual variables of field, tenor and mode of
discourse. Particular settings of these variables
are associated with the co-occurrences of certain
lexico-grammatical features, creating distinctive
registers (e.g., the language of linguistics in aca-
demic discourse). We also consider investiga-
tions of recent language change, observed, e.g.,
by (Mair, 2006), who analyses changes in prefer-
ences of lexico-grammatical selection in English
in the 1960s vs. the 1990s.
As a case study, we show an analysis of
modal verbs (falling into the contextual variable
of tenor), which we group according to (Biber,
1999) into three categories of meaning that rep-
resent three features: obligation, permission and
volition (see Table 1).
2.2 Resources
The selected features are extracted from SciTex,
cf. (Degaetano et al, 2012) and (Teich and
44
categories of meanings (feature) realisation
obligation/necessity (obligaton) can, could, may, etc.
permission/possibility/ability (permission) must, should, etc.
volition/prediction (volition) will, would, shall, etc.
Table 1: Categories of modal meanings for feature extraction
Fankhauser, 2010), an English corpus which con-
tains full English scientific journal articles from
nine disciplines (see Figure 1). The corpus covers
two time periods: the 1970/early 1980s (SaSci-
Tex) and the early 2000s (DaSciTex), and in-
cludes ca. 34 million tokens. Our focus is espe-
cially on the subcorpora representing contact reg-
isters, i.e. registers emerged out of register con-
tact, in our case with computer science: computa-
tional linguistics (B1), bioinformatics (B2), digi-
tal construction (B3), and microelectronics (B4).
COMPUTER
SCIENCE
(A)
LINGUISTICS
(C1)
CO
M
PU
TA
TI
O
NA
L
LI
NG
UI
ST
IC
S
(B
1)
BIOLOGY
(C2)
B
IO-
INFO
RM
ATICS
(B2)
ELECTRICAL
ENGINEERING
(C4)
M
ICRO-
E
LECTRO
NICS
(B4)
MECHANICAL
ENGINEERING
(C3)
DI
G
IT
AL
CO
NS
TR
UC
TI
O
N
(B
3)
Figure 1: Scientific disciplines in the SciTex corpus
SciTex is annotated1 with information on to-
ken, lemma, part-of-speech and sentence bound-
ary, as well as further information on text bound-
ary, register information, etc., and can be queried
in form of regular expressions by the Corpus
Query Processor (CQP), cf. (Evert, 2005).
2.3 Feature Extraction and Analysis
To extract the above described features for the two
time slices (1970/80s and 2000s) and for all nine
registers of SciTex, we elaborate queries, which
include both lexical (based on token and lemma
information) and grammatical (based on part-of-
speech or sentence boundary information) con-
straints.
1Annotations were obtained by means of a dedicated pro-
cessing pipeline (Kermes, 2011).
Annotations on the register information allow
us to sort the extracted material according to spe-
cific subcorpora. This enables the analysis of fea-
tures possibly involved in creating distinctive reg-
isters. Comparing differences and/or common-
alities in the distribution of features for A-B-C
triples of subcorpora (e.g., A-computer science,
B1-computational linguistics, C1-linguistics, cf.
Figure 1), we analyse whether the contact disci-
plines (B-subcorpora) are more similar to com-
puter science (A-subcorpus), the discipline of ori-
gin (C-subcorpus) or distinct from both (A and C).
The two time periods in SciTex (70/80s vs. 2000s)
enable a diachronic analysis. A more fine-grained
diachronic analysis is also possible with the infor-
mation on the publication year annotated in the
corpus.
3 Analysing language changes with SPC
3.1 SPC visualisation
Structured Parallel Coordinates (Culy et al, 2011)
are a specialisation of the Parallel Coordinates
visualisation (cf. (d?Ocagne, 1885), (Inselberg,
1985), (Inselberg, 2009)) for representing mul-
tidimensional data using a two-dimensional dis-
play. Parallel Coordinates place data on vertical
axes, with the axes lined up horizontally. Each
axis represents a separate data dimension and can
hold either categorical or numerical data. Data
points on different axes are related which is indi-
cated by colored lines connecting all data items
belonging to one record.
Targeted to the application to language data,
SPC additionally provide for ordered characteris-
tics of data within and across data dimensions. In
the n-grams with frequencies/KWIC2 implemen-
tations of SPC, ordered axes represent the linear
ordering of words in text.
In our analysis of language change based on
linguistic features, we are interested in two di-
rections of changes across data sets that can be
represented by ordering: changes over time and
2www.eurac.edu/linfovis
45
changes across registers, e.g., from linguistics and
computer science to computational linguistics.
3.2 Adjustments to SPC
For the analysis of linguistic features with SPC,
we start off with the n-grams with frequencies im-
plementation. In analyzing just two time dimen-
sions the ordered aspect of SPC is not as crucial
and a similar analysis could have been done with
Parallel Coordinates. However, the setup of n-
grams with frequencies conveniently provides us
with the combination of categorical and numerical
data dimensions in one display but separated visu-
ally. For our diachronic register analysis, we cre-
ate a subcorpus comparison application where the
feature under analysis as well as some of the cor-
pus data are placed on the unordered categorical
axes, and frequencies for the two time periods are
placed on ordered axes with numerical scales. As
shown in Figure 2 below, unordered dimensions
are followed by ordered dimensions, the inverse
situation to n-grams with frequencies. To visu-
ally support the categorical nature of data on the
first three axes, SPC was adjusted to display the
connecting lines in discrete colors instead of the
default color scale shading from red to blue. To
improve the comparability of values on numerical
axes, a function for switching between compara-
ble and individual scales was added that applies to
all axes right of the separating red line. Figure 2
and 3 present numerical values as percentages on
comparable scales scaled to 100.
3.3 Interactive features for analysis
SPC provide a number of interactive features that
support data analysis. To highlight and accentuate
selected parts of the data, an axis can be put into
focus and parts of axes can be selected. Lines are
colored according to the axis under focus, and fil-
ters apply to the selected portions of axes, with the
other data rendered in gray. Users can switch be-
tween discrete colors and scaled coloring of con-
necting lines. The scales of numerical axes can be
adjusted interactively, as described above. Hover-
ing over a determined connecting line brings it out
as a slightly wider line and gives a written sum-
mary of the values of that record.
4 Interpreting Visualisation Results
Visualised structures provided by SPC supply us
with information on development tendencies, and
thus, deliver valuable material for further interpre-
tation of language variation across registers and
time.
To analyse the frequencies of modal meanings
(see Table 1) for A-B-C triples of subcorpora, we
use the subcorpus comparison option of SPC. The
interactive functionality of SPC allows us to focus
on different aspects and provides us with dynam-
ically updated versions of the visualisation.
First, by setting focus on the axis of modal
meanings, the visualisation in Figure 2 shows di-
achronic changes of the modal meanings from the
1970/80s to the early 2000s. In both time periods
the permission (blue) meaning is most prominent
and has considerably increased over time. The
volition (green) and obligation (orange) meanings
are less prominent and we can observe a decrease
of volition and a very slight decrease of obliga-
tion.
Second, by setting the axis of the registers into
focus and selecting the disciplines one by one, we
can explore whether there are changes in the use
of modal meanings between the A register, the
contact registers (B), and the respective C regis-
ters. In Figure 3, for example, computer science
and biology have been selected (gray shaded) on
the ?disciplines? axis. For this selection, the struc-
tures starting from the ?registers? axis represent
(1) computer science (blue) being the A regis-
ter, (2) biology (green) from the C registers, and
(3) bioinformatics (orange) from the B registers
as the corresponding contact register. In terms
of register changes, Figure 3 shows that bioin-
formatics differs in the development tendencies
(a) of permission from biology and computer sci-
ence (less increase than the former, more increase
than the latter) and (b) of obligation from biology
(decrease for biology, whereas nearly stable for
bioinformatics and computer science).
5 Conclusion and Future Work
The results described above show that Structured
Parallel Coordinates provides us with a means for
the interactive inspection of complex data sets fa-
cilitating our diachronic register analysis. The vi-
sualisation allows to gain an overview and detect
tendencies by accomodating a complex set of data
in one display (nine registers over two time peri-
ods for three meanings).
The interactive features of SPC give the possi-
bility to put different aspects of the data into fo-
46
Figure 2: Modal meanings in SciTex in the 1970/80s and 2000s
Figure 3: Modal meanings in computer science (A-subcorpus; blue), bioinformatics (from B-subcorpus; orange)
and biology (from C-subcorpus; green)
47
cus, and thus to successively zoom into specific
subsets of the data for detailed analyses. In this
way, we can determine general tendencies (e.g.,
increase of permission over time) or provide de-
tailed analyses for certain linguistic features and
registers by selecting subparts of the data and by
highlighting different data dimensions (e.g., com-
paring changes between different registers).
Future work comprises to use the data obtained
from the corpus to feed several different SPC vi-
sualisations. For example, the data presented in
Figure 2 can also be layed out to place values for
registers instead of values for time periods on the
numerical axes.
Future analyses will focus on inspecting fur-
ther tendencies in the feature development for the
three contextual variables mentioned in 2.1, e.g.,
verb valency patterns for field or conjunctive re-
lations expressing cohesion for mode. We also
aim at analysing several linguistic features at the
same time to possibly detect feature sets involved
in register variation of contact registers. Addition-
ally, a more fine-grained diachronic analysis ac-
cording to the publication years, which are anno-
tated in the corpus, might also prove to be useful.
From a technical point of view, the issue with
fully overlapping lines being displayed in one
color only will be tackled by experimenting with
semi-transparent or stacked lines. Furthermore,
SPC should in the future be expanded by a func-
tion for restructuring the underlying data to cre-
ate different layouts. This could also include the
merging of axes with categorical values (e.g., axes
registers and disciplines in Figure 2 above). Fur-
thermore on each data dimension a ?summary?
category could be introduced that would repre-
sent the sum of all individual values, and would
provide an extra point of reference for the analy-
sis. For interactive data analysis, support could be
provided to select data items based on crossings
or declination of their connecting lines.
References
Douglas Biber. 1995. Dimensions of Register Varia-
tion. A Cross-linguistic Comparison. Cambridge:
Cambridge University Press.
Douglas Biber. 1999. Longman Grammar of Spoken
and Written English. Harlow: Pearson ESL.
Chris Culy, Verena Lyding, and Henrik Dittmann.
2011. Structured Parallel Coordinates: a visualiza-
tion for analyzing structured language data. In Pro-
ceedings of the 3rd International Conference on
Corpus Linguistics, CILC-11, April 6-9, 2011, Va-
lencia, Spain, 485?493.
Stefania Degaetano-Ortlieb, Hannah Kermes, Ekate-
rina Lapshinova-Koltunski and Elke Teich. 2012.
SciTex ? A Diachronic Corpus for Analyzing the
Development of Scientific Registers. In: Paul Ben-
nett, Martin Durrell, Silke Scheible & Richard J.
Whitt (eds.), New Methods in Historical Corpus
Linguistics. CLIP, Vol. 2, Narr: Tu?bingen.
Stefan Evert. 2005. The CQP Query Language Tuto-
rial. IMS, Universita?t Stuttgart.
M.A.K. Halliday and Ruqaiya Hasan. 1989. Language,
context and text: Aspects of language in a social
semiotic perspective. OUP.
Alfred Inselberg. 2009. Parallel Coordinates: VISUAL
Multidimensional Geometry and its Applications.
New York: Springer.
Alfred Inselberg. 1985. The plane with parallel coor-
dinates. The Visual Computer 1(2), pp. 69?91.
Hannah Kermes. 2011. Automatic corpus creation.
Manual. Institute of Applied Linguistics, Transla-
tion and Interpreting, Universita?t des Saarlandes,
Saarbru?cken.
Christian Mair. 2006. Twentieth-Century English: His-
tory, Variation and Standardization. Cambridge:
Cambridge University Press.
Maurice d?Ocagne. 1885. Coordonne?es Paralle`les et
Axiales: Me?thode de transformation ge?ome?trique
et proce?de? nouveau de calcul graphique de?duits de
la conside?ration des coordonne?es paralle`lles. Paris:
Gauthier-Villars.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech
and Jan Svartvik. 1985. A comprehensive grammar
of the English language. Harlow: Longman
Elke Teich and Peter Fankhauser. 2010. Exploring a
corpus of scientific texts using data mining. In:
Gries S., S. Wulff and M. Davies (eds), Corpus-
linguistic applications - Current studies, new direc-
tions. Rodopi, Amsterdam and New York, pp. 233?
247.
48
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 36?43,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
The PAIS
`
A Corpus of Italian Web Texts
Verena Lyding
?
verena.lyding@eurac.edu
Egon Stemle
?
egon.stemle@eurac.edu
Claudia Borghetti
?
claudia.borghetti@unibo.it
Marco Brunello
?
marcobrunello84@gmail.com
Sara Castagnoli
?
s.castagnoli@unibo.it
Felice Dell?Orletta
?
felice.dellorletta@ilc.cnr.it
Henrik Dittmann
?
henrik.dittmann@bordet.be
Alessandro Lenci
?
alessandro.lenci@ling.unipi.it
Vito Pirrelli
?
vito.pirrelli@ilc.cnr.it
Abstract
PAIS
`
A is a Creative Commons licensed,
large web corpus of contemporary Italian.
We describe the design, harvesting, and
processing steps involved in its creation.
1 Introduction
This paper provides an overview of the PAIS
`
A cor-
pus of Italian web texts and an introductory de-
scription of the motivation, procedures and facili-
ties for its creation and delivery.
Developed within the PAIS
`
A project, the cor-
pus is intended to meet the objective to help over-
come the technological barriers that still prevent
web users from making use of large quantities of
contemporary Italian texts for language and cul-
tural education, by creating a comprehensive and
easily accessible corpus resource of Italian.
The initial motivation of the initiative stemmed
from the awareness that any static repertoire of
digital data, however carefully designed and de-
veloped, is doomed to fast obsolescence, if con-
tents are not freely available for public usage, con-
tinuously updated and checked for quality, incre-
mentally augmented with new texts and annota-
tion metadata for intelligent indexing and brows-
ing. These requirements brought us to design a
resource that was (1) freely available and freely
re-publishable, (2) comprehensively covering con-
temporary common language and cultural content
and (3) enhanced with a rich set of automatically-
annotated linguistic information to enable ad-
vanced querying and retrieving of data. On top
?
EURAC Research Bolzano/Bozen, IT
?
University of Bologna, IT
?
University of Leeds, UK
?
Institute of Computational Linguistics ?Antonio Zam-
polli? - CNR, IT
?
Institut Jules Bordet, BE
?
University of Pisa, IT
of that, we set out to develop (4) a dedicated in-
terface with a low entry barrier for different target
groups. The end result of this original plan repre-
sents an unprecedented digital language resource
in the Italian scenario.
The main novelty of the PAIS
`
A web corpus is
that it exclusively draws on Creative Commons li-
censed data, provides advanced linguistic annota-
tions with respect to corpora of comparable size
and corpora of web data, and invests in a carefully
designed query interface, targeted at different user
groups. In particular, the integration of richly an-
notated language content with an easily accessible,
user-oriented interface makes PAIS
`
A a unique and
flexible resource for language teaching.
2 Related Work
The world wide web, with its inexhaustible
amount of natural language data, has become an
established source for efficiently building large
corpora (Kilgarriff and Grefenstette, 2003). Tools
are available that make it convenient to bootstrap
corpora from the web based on mere seed term
lists, such as the BootCaT toolkit (Baroni and
Bernardini, 2004). The huge corpora created by
the WaCky project (Baroni et al., 2009) are an ex-
ample of such an approach.
A large number of papers have recently been
published on the harvesting, cleaning and pro-
cessing of web corpora.
1
However, freely avail-
able, large, contemporary, linguistically anno-
tated, easily accessible web corpora are still miss-
ing for many languages; but cf. e.g. (G?en?ereux
et al., 2012) and the Common Crawl Foundations
(CCF) web crawl
2
.
1
cf. the Special Interest Group of the Association for
Computational Linguistics on Web as Corpus (SIGWAC)
http://sigwac.org.uk/
2
CCF produces and maintains a repository of web crawl
data that is openly accessible: http://commoncrawl.
org/
36
3 Corpus Composition
3.1 Corpus design
PAIS
`
A aimed at creating a comprehensive corpus
resource of Italian web texts which adheres to the
criteria laid out in section 1. For these criteria to
be fully met, we had to address a wide variety of
issues covering the entire life-cycle of a digital text
resource, ranging from robust algorithms for web
navigation and harvesting, to adaptive annotation
tools for advanced text indexing and querying and
user-friendly accessing and rendering online inter-
faces customisable for different target groups.
Initially, we targeted a size of 100M tokens, and
planned to automatically annotate the data with
lemma, part-of-speech, structural dependency, and
advanced linguistic information, using and adapt-
ing standard annotation tools (cf. section 4). In-
tegration into a querying environment and a dedi-
cated online interface were planned.
3.2 Licenses
A crucial point when planning to compile a cor-
pus that is free to redistribute without encounter-
ing legal copyright issues is to collect texts that are
in the public domain or at least, have been made
available in a copyleft regime. This is the case
when the author of a certain document decided to
share some rights (copy and/or distribute, adapt
etc.) on her work with the public, in a way that
end users do not need to ask permission to the cre-
ator/owner of the original work. This is possible
by employing licenses other than the traditional
?all right reserved? copyright, i.e. GNU, Creative
Commons etc., which found a wide use especially
on the web. Exploratory studies (Brunello, 2009)
have shown that Creative Commons licenses are
widely employed throughout the web (at least on
the Italian webspace), enough to consider the pos-
sibility to build a large corpus from the web ex-
clusively made of documents released under such
licenses.
In particular, Creative Commons provides five
basic ?baseline rights?: Attribution (BY), Share
Alike (SA), Non Commercial (NC), No Deriva-
tive Works (ND). The licenses themselves are
composed of at least Attribution (which can be
used even alone) plus the other elements, al-
lowing six different combinations:
3
(1) Attribu-
tion (CC BY), (2) Attribution-NonCommercial
3
For detailed descriptions of each license see http://
creativecommons.org/licenses/
(CC BY-NC), (3) Attribution-ShareAlike (CC BY-
SA), (4) Attribution-NoDerivs (CC BY-ND), (5)
Attribution-NonCommercial-ShareAlike (CC BY-
NC-SA), and (6) Attribution-NonCommercial-
NoDerivs (CC BY-NC-ND).
Some combinations are not possible because
certain elements are not compatible, e.g. Share
Alike and No Derivative Works. For our purposes
we decided to discard documents released with the
two licenses containing the No Derivative Works
option, because our corpus is in fact a derivative
work of collected documents.
3.3 The final corpus
The corpus contains approximately 388,000 docu-
ments from 1,067 different websites, for a total of
about 250M tokens. All documents contained in
the PAIS
`
A corpus date back to Sept./Oct. 2010.
The documents come from several web sources
which, at the time of corpus collection, provided
their content under Creative Commons license
(see section 3.2 for details). About 269,000 texts
are from Wikimedia Foundation projects, with
approximately 263,300 pages from Wikipedia,
2380 pages from Wikibooks, 1680 pages from
Wikinews, 740 pages from Wikiversity, 410 pages
from Wikisource, and 390 Wikivoyage pages.
The remaining 119,000 documents come
from guide.supereva.it (ca. 19,000),
italy.indymedia.org (ca. 10,000) and
several blog services from more than another
1,000 different sites (e.g. www.tvblog.it
(9,088 pages), www.motoblog.it (3,300),
www.ecowebnews.it (3,220), and
www.webmasterpoint.org (3,138).
Texts included in PAIS
`
A have an average length
of 683 words, with the longest text
4
counting
66,380 running tokens. A non exhaustive list of
average text lengths by source type is provided in
table 1 by way of illustration.
The corpus has been annotated for lemma, part-
of-speech and dependency information (see sec-
tion 4.2 for details). At the document level, the
corpus contains information on the URL of origin
and a set of descriptive statistics of the text, includ-
ing text length, rate of advanced vocabulary, read-
ability parameters, etc. (see section 4.3). Also,
each document is marked with a unique identifier.
4
The European Constitution from wikisource.org:
http://it.wikisource.org/wiki/Trattato_
che_adotta_una_Costituzione_per_l?Europa
37
Document source Avg text length
PAIS
`
A total 683 words
Wikipedia 693 words
Wikibooks 1844 words
guide.supereva.it 378 words
italy.indymedia.it 1147 words
tvblog.it 1472 words
motoblog.it 421 words
ecowebnews.it 347 words
webmasterpoint.org 332 words
Table 1: Average text length by source
The annotated corpus adheres to the stan-
dard CoNLL column-based format (Buchholz and
Marsi, 2006), is encoded in UTF-8.
4 Corpus Creation
4.1 Collecting and cleaning web data
The web pages for PAIS
`
A were selected in two
ways: part of the corpus collection was made
through CC-focused web crawling, and another
part through a targeted collection of documents
from specific websites.
4.1.1 Seed-term based harvesting
At the time of corpus collection (2010), we used
the BootCaT toolkit mainly because collecting
URLs could be based on the public Yahoo! search
API
5
, including the option to restrict search to CC-
licensed pages (including the possibility to specify
even the particular licenses). Unfortunately, Ya-
hoo! discontinued the free availability of this API,
and BootCaT?s remaining search engines do not
provide this feature.
An earlier version of the corpus was collected
using the tuple list originally employed to build
itWaC
6
. As we noticed that the use of this list, in
combination with the restriction to CC, biased the
final results (i.e. specific websites occurred very
often as top results) , we provided as input 50,000
medium frequent seed terms from a basic Italian
vocabulary list
7
, in order to get a wider distribu-
tion of search queries, and, ultimately, of texts.
As introduced in section 3.2, we restricted the
selection not just to Creative Commons-licensed
5
http://developer.yahoo.com/boss/
6
http://wacky.sslmit.unibo.it/doku.
php?id=seed_words_and_tuples
7
http://ppbm.paravia.it/dib_lemmario.
php
texts, but specifically to those licenses allowing
redistribution: namely, CC BY, CC BY-SA, CC
BY-NC-SA, and CC BY-NC.
Results were downloaded and automatically
cleaned with the KrdWrd system, an environment
for the unified processing of web content (Steger
and Stemle, 2009).
Wrongly CC-tagged pages were eliminated us-
ing a black-list that had been manually populated
following inspection of earlier corpus versions.
4.1.2 Targeted
In September 2009, the Wikimedia Foundation de-
cided to release the content of their wikis under
CC BY-SA
8
, so we decided to download the large
and varied amount of texts made available through
the Italian versions of these websites. This was
done using the Wikipedia Extractor
9
on official
dumps
10
of Wikipedia, Wikinews, Wikisource,
Wikibooks, Wikiversity and Wikivoyage.
4.2 Linguistic annotation and tools
adaptation
The corpus was automatically annotated with
lemma, part-of-speech and dependency infor-
mation, using state-of-the-art annotation tools
for Italian. Part-of-speech tagging was per-
formed with the Part-Of-Speech tagger described
in Dell?Orletta (2009) and dependency-parsed by
the DeSR parser (Attardi et al., 2009), using Mul-
tilayer Perceptron as the learning algorithm. The
systems used the ISST-TANL part-of-speech
11
and dependency tagsets
12
. In particular, the pos-
tagger achieves a performance of 96.34% and
DeSR, trained on the ISST-TANL treebank con-
sisting of articles from newspapers and period-
icals, achieves a performance of 83.38% and
87.71% in terms of LAS (labelled attachment
score) and UAS (unlabelled attachment score) re-
spectively, when tested on texts of the same type.
However, since Gildea (2001), it is widely ac-
knowledged that statistical NLP tools have a drop
of accuracy when tested against corpora differing
from the typology of texts on which they were
trained. This also holds true for PAIS
`
A: it contains
8
Previously under GNU Free Documentation License.
9
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
10
http://dumps.wikimedia.org/
11
http://www.italianlp.it/docs/
ISST-TANL-POStagset.pdf
12
http://www.italianlp.it/docs/
ISST-TANL-DEPtagset.pdf
38
lexical and syntactic structures of non-canonical
languages such as the language of social media,
blogs, forum posts, consumer reviews, etc. As re-
ported in Petrov and McDonald (2012), there are
multiple reasons why parsing the web texts is dif-
ficult: punctuation and capitalization are often in-
consistent, there is a lexical shift due to increased
use of slang and technical jargon, some syntactic
constructions are more frequent in web text than
in newswire, etc.
In order to overcome this problem, two main ty-
pologies of methods and techniques have been de-
veloped: Self-training (McClosky et al., 2006) and
Active Learning (Thompson et al., 1999).
For the specific purpose of the NLP tools adap-
tation to the Italian web texts, we adopted two dif-
ferent strategies for the pos-tagger and the parser.
For what concerns pos-tagging, we used an active
learning approach: given a subset of automatically
pos-tagged sentences of PAIS
`
A, we selected the
ones with the lowest likelihood, where the sen-
tence likelihood was computed as the product of
the probabilities of the assignments of the pos-
tagger for all the tokens. These sentences were
manually revised and added to the training corpus
in order to build a new pos-tagger model incor-
porating some new knowledge from the target do-
main.
For what concerns parsing, we used a self-
training approach to domain adaptation described
in Dell?Orletta et al. (2013), based on ULISSE
(Dell?Orletta et al., 2011). ULISSE is an unsu-
pervised linguistically-driven algorithm to select
reliable parses from a collection of dependency
annotated texts. It assigns to each dependency
tree a score quantifying its reliability based on a
wide range of linguistic features. After collect-
ing statistics about selected features from a cor-
pus of automatically parsed sentences, for each
newly parsed sentence ULISSE computes a reli-
ability score using the previously extracted feature
statistics. From the top of the parses (ranked ac-
cording to their reliability score) different pools of
parses were selected to be used for training. The
new training contains the original training set as
well as the new selected parses which include lex-
ical and syntactic characteristics specific of the tar-
get domain (Italian web texts). The parser trained
on this new training set improves its performance
when tested on the target domain.
We used this domain adaptation approach for
the following three main reasons: a) it is unsuper-
vised (i.e. no need for manually annotated training
data); b) unlike the Active Learning approach used
for pos-tagging, it does not need manual revision
of the automatically parsed samples to be used for
training; c) it was previously tested on Italian texts
with good results (Dell?Orletta et al., 2013).
4.3 Readability analysis of corpus documents
For each corpus document, we calculated several
text statistics indicative of the linguistic complex-
ity, or ?readability? of a text.
The applied measures include, (1) text length in
tokens, that is the number of tokens per text, (2)
sentences per text, that is a sentence count, and (3)
type-token ratio indicated as a percentage value.
In addition, we calculated (4) the advanced vo-
cabulary per text, that is a word count of the text
vocabulary which is not part of the the basic Ital-
ian vocabulary (?vocabolario di base?) for written
texts, as defined by De Mauro (1991)
13
, and (5)
the Gulpease Index (?Indice Gulpease?) (Lucisano
and Piemontese, 1988), which is a measure for the
readability of text that is based on frequency rela-
tions between the number of sentences, words and
letters of a text.
All values are encoded as metadata for the cor-
pus. Via the PAIS
`
A online interface, they can
be employed for filtering documents and building
subcorpora. This facility was implemented with
the principal target group of PAIS
`
A users in mind,
as the selection of language examples according
to their readability level is particularly relevant for
language learning and teaching.
4.4 Attempts at text classification for genre,
topic, and function
Lack of information about the composition of cor-
pora collected from the web using unsupervised
methods is probably one of the major limitations
of current web corpora vis-`a-vis more traditional,
carefully constructed corpora, most notably when
applications to language teaching and learning are
envisaged. This also holds true for PAIS
`
A, es-
13
The advanced vocabulary was calculated on the ba-
sis of a word list consisting of De Mauro?s ?vocabolario
fondamentale? (http://it.wikipedia.org/wiki/
Vocabolario_fondamentale) and ?vocabolario
di alto uso? (http://it.wikipedia.org/wiki/
Vocabolario_di_alto_uso), together with high
frequent function words not contained in those two lists.
39
pecially for the harvested
14
subcorpus that was
downloaded as described in section 4.1. We there-
fore carried out some experiments with the ulti-
mate aim to enrich the corpus with metadata about
text genre, topic and function, using automated
techniques.
In order to gain some insights into the com-
position of PAIS
`
A, we first conducted some man-
ual investigations. Drawing on existing literature
on web genres (e.g. (Santini, 2005; Rehm et al.,
2008; Santini et al., 2010)) and text classification
according to text function and topic (e.g. (Sharoff,
2006)), we developed a tentative three-fold taxon-
omy to be used for text classification. Following
four cycles of sample manual annotation by three
annotators, categories were adjusted in order to
better reflect the nature of PAIS
`
A?s web documents
(cf. (Sharoff, 2010) about differences between do-
mains covered in the BNC and in the web-derived
ukWaC). Details about the taxonomy are provided
in Borghetti et al. (2011). Then, we started to
cross-check whether the devised taxonomy was
indeed appropriate to describe PAIS
`
A?s composi-
tion by comparing its categories with data result-
ing from the application of unsupervised methods
for text classification.
Interesting insights have emerged so far re-
garding the topic category. Following Sharoff
(2010), we used topic modelling based on La-
tent Dirichlet Allocation for the detection of top-
ics: 20 clusters/topics were identified on the ba-
sis of keywords (the number of clusters to re-
trieve is a user-defined parameter) and projected
onto the manually defined taxonomy. This re-
vealed that most of the 20 automatically iden-
tified topics could be reasonably matched to
one of the 8 categories included in the tax-
onomy; exceptions were represented by clus-
ters characterised by proper nouns and gen-
eral language words such bambino/uomo/famiglia
(?child?/?man?/?family?) or credere/sentire/sperare
(?to believe?/?feel?/?hope?), which may in fact be
indicative of genres such as diary or personal com-
ment (e.g. personal blog). Only one of the cate-
gories originally included in the taxonomy ? natu-
ral sciences ? was not represented in the clusters,
which may indicate that there are few texts within
PAIS
`
A belonging to this domain. One of the ma-
14
In fact, even the nature of the targeted texts is not pre-
cisely defined: for instance, Wikipedia articles can actually
encompass a variety of text types such as biographies, intro-
ductions to academic theories etc. (Santini et al., 2010, p. 15)
jor advantages of topic models is that each corpus
document can be associated ? to varying degrees ?
to several topics/clusters: if encoded as metadata,
this information makes it possible not only to fil-
ter texts according to their prevailing domain, but
also to represent the heterogeneous nature of many
web documents.
5 Corpus Access and Usage
5.1 Corpus distribution
The PAIS
`
A corpus is distributed in two ways: it is
made available for download and it can be queried
via its online interface. For both cases, no restric-
tions on its usage apply other than those defined
by the Creative Commons BY-NC-SA license. For
corpus download, both the raw text version and the
annotated corpus in CoNLL format are provided.
The PAIS
`
A corpus together with all project-
related information is accessible via the project
web site at http://www.corpusitaliano.it
5.2 Corpus interface
The creation of a dedicated open online interface
for the PAIS
`
A corpus has been a declared primary
objective of the project.
The interface is aimed at providing a power-
ful, effective and easy-to-employ tool for mak-
ing full use of the resource, without having to go
through downloading, installation or registration
procedures. It is targeted at different user groups,
particularly language learners, teachers, and lin-
guists. As users of PAIS
`
A are expected to show
varying levels of proficiency in terms of language
competence, linguistic knowledge, and concern-
ing the use of online search tools, the interface
has been designed to provide four separate search
components, implementing different query modes.
Initially, the user is directed to a basic keyword
search that adopts a ?Google-style? search box.
Single search terms, as well as multi-word combi-
nations or sequences can be searched by inserting
them in a simple text box.
The second component is an advanced graph-
ical search form. It provides elaborated search
options for querying linguistic annotation layers
and allows for defining distances between search
terms as well as repetitions or optionally occurring
terms. Furthermore, the advanced search supports
regular expressions.
The third component emulates a command-line
search via the powerful CQP query language of
40
the Open Corpus Workbench (Evert and Hardie,
2011). It allows for complex search queries in
CQP syntax that rely on linguistic annotation lay-
ers as well as on metadata information.
Finally, a filter interface is presented in a fourth
component. It serves the purpose of retriev-
ing full-text corpus documents based on keyword
searches as well as text statistics (see section 4.3).
Like the CQP interface, the filter interface is also
supporting the building of temporary subcorpora
for subsequent querying.
By default, search results are displayed as
KWIC (KeyWord In Context) lines, centred
around the search expression. Each search hit can
be expanded to its full sentence view. In addition,
the originating full text document can be accessed
and its source URL is provided.
Based on an interactive visualisation for depen-
dency graphs (Culy et al., 2011) for each search
result a graphical representations of dependency
relations together with the sentence and associated
lemma and part-of-speech information can be gen-
erated (see Figure 1).
Figure 1: Dependency diagram
Targeted at novice language learners of Italian,
a filter for automatically restricting search results
to sentences of limited complexity has been in-
tegrated into each search component. When ac-
tivated, search results are automatically filtered
based on a combination of the complexity mea-
sures introduced in section 4.3.
5.3 Technical details
The PAIS
`
A online interface has been developed in
several layers: in essence, it provides a front-end
to the corpus as indexed in Open Corpus Work-
bench (Evert and Hardie, 2011). This corpus
query engine provides the fundamental search ca-
pabilities through the CQP language. Based on
the CWB/Perl API that is part of the Open Corpus
Workbench package, a web service has been de-
veloped at EURAC which exposes a large part of
the CQP language
15
through a RESTful API.
16
The four types of searches provided by the on-
line interface are developed on top of this web ser-
vice. The user queries are translated into CQP
queries and passed to the web service. In many
cases, such as the free word order queries in the
simple and advanced search forms, more than one
CQP query is necessary to produce the desired
result. Other functionalities implemented in this
layer are the management of subcorpora and the
filtering by complexity. The results returned by
the web service are then formatted and presented
to the user.
The user interface as well as the mechanisms
for translation of queries from the web forms into
CQP have been developed server-side in PHP.
The visualizations are implemented client-side in
JavaScript and jQuery, the dependency graphs
based on the xLDD framework (Culy et al., 2011).
5.4 Extraction of lexico-syntactic information
PAIS
`
A is currently used in the CombiNet project
?Word Combinations in Italian ? Theoretical and
descriptive analysis, computational models, lexi-
cographic layout and creation of a dictionary?.
17
The project goal is to study the combinatory prop-
erties of Italian words by developing advanced
computational linguistics methods for extracting
distributional information from PAIS
`
A.
In particular, CombiNet uses a pattern-based
approach to extract a wide range of multiword
expressions, such as phrasal lexemes, colloca-
tions, and usual combinations. POS n-grams
are automatically extracted from PAIS
`
A, and then
ranked according to different types of associa-
tion measures (e.g., pointwise mutual informa-
tion, log-likelihood ratios, etc.). Extending the
LexIt methodology (Lenci et al., 2012), CombiNet
also extracts distributional profiles from the parsed
layer of PAIS
`
A, including the following types of
information:
1. syntactic slots (subject, complements, modi-
15
To safeguard the system against malicious attacks, secu-
rity measures had to be taken at several of the layers, which
unfortunately also make some of the more advanced CQP fea-
tures inaccessible to the user.
16
Web services based on REST (Representational State
Transfer) principles employ standard concepts such as a URI
and standard HTTP methods to provide an interface to func-
tionalities on a remote host.
17
3-year PRIN(2010/2011)-project, coordination by Raf-
faele Simone ? University of Rome Tre
41
fiers, etc.) and subcategorization frames;
2. lexical sets filling syntactic slots (e.g. proto-
typical subjects of a target verb);
3. semantic classes describing selectional pref-
erences of syntactic slots (e.g. the direct obj.
of mangiare/?to eat? typically selects nouns
referring to food, while its subject selects an-
imate nouns); semantic roles of predicates.
The saliency and typicality of combinatory pat-
terns are weighted by means of different statisti-
cal indexes and the resulting profiles will be used
to define a distributional semantic classification of
Italian verbs, comparable to the one elaborated in
the VerbNet project (Kipper et al., 2008).
6 Evaluation
We performed post-crawl evaluations on the data.
For licensing, we analysed 200,534 pages that
were originally collected for the PAIS
`
A corpus,
and only 1,060 were identified as containing no
CC license link (99.95% with CC mark-up). Then,
from 10,000 randomly selected non-CC-licensed
Italian pages 15 were wrongly identified as CC li-
censed containing CC mark-up (0.15% error). For
language identification we checked the harvested
corpus part with the CLD2 toolkit
18
, and > 99%
of the data was identified as Italian.
The pos-tagger has been adapted to peculiari-
ties of the PAIS
`
A web texts, by manually correct-
ing sample annotation output and re-training the
tagger accordingly. Following the active learning
approach as described in section 4.2 we built a new
pos-tagger model based on 40.000 manually re-
vised tokens. With the new model, we obtained
an improvement in accuracy of 1% on a test-set
of 5000 tokens extracted from PAIS
`
A. Final tag-
ger accuracy reached 96.03%.
7 Conclusion / Future Work
In this paper we showed how a contemporary and
free language resource of Italian with linguistic
annotations can be designed, implemented and de-
veloped from the web and made available for dif-
ferent types of language users.
Future work will focus on enriching the cor-
pus with metadata by means of automatic clas-
sification techniques, so as to make a better as-
sessment of corpus composition. A multi-faceted
18
Compact Language Detection 2, http://code.
google.com/p/cld2/
approach combining linguistic features extracted
from texts (content/function words ratio, sentence
length, word frequency, etc.) and information
extracted from document URLs (e.g., tags like
?wiki?, ?blog?) might be particularly suitable for
genre and function annotation.
Metadata annotation will enable more advanced
applications of the corpus for language teaching
and learning purposes. In this respect, existing
exemplifications of the use of the PAIS
`
A inter-
face for language learning and teaching (Lyding et
al., 2013) could be followed by further pedagogi-
cal proposals as well as empowered by dedicated
teaching guidelines for the exploitation of the cor-
pus and its web interface in the class of Italian as
a second language.
In a more general perspective, we envisage
a tighter integration between acquisition of new
texts, automated text annotation and development
of lexical and language learning resources allow-
ing even non-specialised users to carve out and
develop their own language data. This ambitious
goal points in the direction of a fully-automatised
control of the entire life-cycle of open-access Ital-
ian language resources with a view to address an
increasingly wider range of potential demands.
Acknowledgements
The three years PAIS
`
A project
19
, concluded in
January 2013, received funding from the Italian
Ministry of Education, Universities and Research
(MIUR)
20
, by the FIRB program (Fondo per gli
Investimenti della Ricerca di Base)
21
.
References
G. Attardi, F. Dell?Orletta, M. Simi, and J. Turian.
2009. Accurate dependency parsing with a stacked
multilayer perceptron. In Proc. of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
M. Baroni and S. Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In Proc.
of LREC 2004, pages 1313?1316. ELDA.
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed
19
An effort of four Italian research units: University of
Bologna, CNR Pisa, University of Trento and European
Academy of Bolzano/Bozen.
20
http://www.istruzione.it/
21
http://hubmiur.pubblica.istruzione.
it/web/ricerca/firb
42
web-crawled corpora. Journal of LRE, 43(3):209?
226.
C. Borghetti, S. Castagnoli, and M. Brunello. 2011. I
testi del web: una proposta di classificazione sulla
base del corpus pais`a. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica.,
pages 147?170. Carocci, Roma.
M. Brunello. 2009. The creation of free linguistic cor-
pora from the web. In I. Alegria, I. Leturia, and
S. Sharoff, editors, Proc. of the Fifth Web as Corpus
Workshop (WAC5), pages 9?16. Elhuyar Fundazioa.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
Tenth Conf. Comput. Nat. Lang. Learn., number
June in CoNLL-X ?06, pages 149?164. Association
for Computational Linguistics.
C. Culy, V. Lyding, and H. Dittmann. 2011. xldd:
Extended linguistic dependency diagrams. In Proc.
of the 15th International Conference on Information
Visualisation IV2011, pages 164?169, London, UK.
T. De Mauro. 1991. Guida all?uso delle parole. Edi-
tori Riuniti, Roma.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2011.
Ulisse: an unsupervised algorithm for detecting re-
liable dependency parses. In Proc. of CoNLL 2011,
Conferences on Natural Language Learning, Port-
land, Oregon.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2013.
Unsupervised linguistically-driven reliable depen-
dency parses detection and self-training for adapta-
tion to the biomedical domain. In Proc. of BioNLP
2013, Workshop on Biomedical NLP, Sofia.
F. Dell?Orletta. 2009. Ensemble system for part-of-
speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
S. Evert and A. Hardie. 2011. Twenty-first century
corpus workbench: Updating a query architecture
for the new millennium. In Proc. of the Corpus Lin-
guistics 2011, Birmingham, UK.
M. G?en?ereux, I. Hendrickx, and A. Mendes. 2012.
A large portuguese corpus on-line: Cleaning and
preprocessing. In PROPOR, volume 7243 of Lec-
ture Notes in Computer Science, pages 113?120.
Springer.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333?347.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of english verbs.
Journal of LRE, 42:21?40.
A. Lenci, G. Lapesa, and G. Bonansinga. 2012. Lexit:
A computational resource on italian argument struc-
ture. In N. Calzolari, K. Choukri, T. Declerck,
M. U?gur Do?gan, B. Maegaard, J. Mariani, J. Odijk,
and S. Piperidis, editors, Proc. of LREC 2012, pages
3712?3718, Istanbul, Turkey, May. ELRA.
P. Lucisano and M. E. Piemontese. 1988. Gulpease:
una formula per la predizione della difficolt dei testi
in lingua italiana. Scuola e citt`a, 39(3):110?124.
V. Lyding, C. Borghetti, H. Dittmann, L. Nicolas, and
E. Stemle. 2013. Open corpus interface for italian
language learning. In Proc. of the ICT for Language
Learning Conference, 6th Edition, Florence, Italy.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL 2006, ACL, Sydney.
S. Petrov and R. McDonald. 2012. Overview of the
2012 shared task on parsing the web. In Proc. of
SANCL 2012, First Workshop on Syntactic Analysis
of Non-Canonical Language, Montreal.
G. Rehm, M. Santini, A. Mehler, P. Braslavski,
R. Gleim, A. Stubbe, S. Symonenko, M. Tavosanis,
and V. Vidulin. 2008. Towards a reference corpus of
web genres for the evaluation of genre identification
systems. In Proc. of LREC 2008, pages 351?358,
Marrakech, Morocco.
M. Santini, A. Mehler, and S. Sharoff. 2010. Riding
the Rough Waves of Genre on the Web. Concepts
and Research Questions. In A. Mehler, S. Sharoff,
and M. Santini, editors, Genres on the Web: Compu-
tational Models and Empirical Studies., pages 3?33.
Springer, Dordrecht.
M. Santini. 2005. Genres in formation? an ex-
ploratory study of web pages using cluster analysis.
In Proc. of the 8th Annual Colloquium for the UK
Special Interest Group for Computational Linguis-
tics (CLUK05), Manchester, UK.
S. Sharoff. 2006. Creating General-Purpose Corpora
Using Automated Search Engine Queries. In M. Ba-
roni and S. Bernardini, editors, Wacky! Working
Papers on the Web as Corpus, pages 63?98. Gedit,
Bologna.
S. Sharoff. 2010. Analysing similarities and differ-
ences between corpora. In 7th Language Technolo-
gies Conference, Ljubljana.
J. M. Steger and E. W. Stemle. 2009. KrdWrd ? The
Architecture for Unified Processing of Web Content.
In Proc. Fifth Web as Corpus Work., Donostia-San
Sebastian, Basque Country.
C. A. Thompson, M. E. Califf, and R. J. Mooney. 1999.
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML99, the Six-
teenth International Conference on Machine Learn-
ing, San Francisco, CA.
43
