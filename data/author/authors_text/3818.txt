Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304?313,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Selecting Sentences for Answering Complex Questions
Yllias Chali
University of Lethbridge
4401 University Drive
Lethbridge, Alberta, Canada, T1K 3M4
chali@cs.uleth.ca
Shafiq R. Joty
University of British Columbia
2366 Main Mall
Vancouver, B.C. Canada V6T 1Z4
rjoty@cs.ubc.ca
Abstract
Complex questions that require inferencing
and synthesizing information from multiple
documents can be seen as a kind of topic-
oriented, informative multi-document summa-
rization. In this paper, we have experimented
with one empirical and two unsupervised
statistical machine learning techniques: k-
means and Expectation Maximization (EM),
for computing relative importance of the sen-
tences. However, the performance of these ap-
proaches depends entirely on the feature set
used and the weighting of these features. We
extracted different kinds of features (i.e. lex-
ical, lexical semantic, cosine similarity, ba-
sic element, tree kernel based syntactic and
shallow-semantic) for each of the document
sentences in order to measure its importance
and relevancy to the user query. We used a
local search technique to learn the weights of
the features. For all our methods of generating
summaries, we have shown the effects of syn-
tactic and shallow-semantic features over the
bag of words (BOW) features.
1 Introduction
After having made substantial headway in factoid
and list questions, researchers have turned their at-
tention to more complex information needs that can-
not be answered by simply extracting named enti-
ties (persons, organizations, locations, dates, etc.)
from documents. For example, the question: ?De-
scribe the after-effects of cyclone Sidr-Nov 2007 in
Bangladesh? requires inferencing and synthesizing
information from multiple documents. This infor-
mation synthesis in NLP can be seen as a kind of
topic-oriented, informative multi-document summa-
rization, where the goal is to produce a single text as
a compressed version of a set of documents with a
minimum loss of relevant information.
In this paper, we experimented with one em-
pirical and two well-known unsupervised statisti-
cal machine learning techniques: k-means and EM
and evaluated their performance in generating topic-
oriented summaries. However, the performance of
these approaches depends entirely on the feature set
used and the weighting of these features. We ex-
tracted different kinds of features (i.e. lexical, lexi-
cal semantic, cosine similarity, basic element, tree
kernel based syntactic and shallow-semantic) for
each of the document sentences in order to measure
its importance and relevancy to the user query. We
have used a gradient descent local search technique
to learn the weights of the features. Traditionally,
information extraction techniques are based on the
BOW approach augmented by language modeling.
But when the task requires the use of more com-
plex semantics, the approaches based on only BOW
are often inadequate to perform fine-level textual
analysis. Some improvements on BOW are given
by the use of dependency trees and syntactic parse
trees (Hirao et al, 2004), (Punyakanok et al, 2004),
(Zhang and Lee, 2003), but these, too are not ade-
quate when dealing with complex questions whose
answers are expressed by long and articulated sen-
tences or even paragraphs. Shallow semantic rep-
resentations, bearing a more compact information,
could prevent the sparseness of deep structural ap-
proaches and the weakness of BOW models (Mos-
chitti et al, 2007). Attempting an application of
304
syntactic and semantic information to complex QA
hence seems natural, as pinpointing the answer to a
question relies on a deep understanding of the se-
mantics of both. In more complex tasks such as
computing the relatedness between the query sen-
tences and the document sentences in order to gen-
erate query-focused summaries (or answers to com-
plex questions), to our knowledge no study uses tree
kernel functions to encode syntactic/semantic infor-
mation. For all our methods of generating sum-
maries (i.e. empirical, k-means and EM), we have
shown the effects of syntactic and shallow-semantic
features over the BOW features.
This paper is organized as follows: Section 2 fo-
cuses on the related work, Section 3 describes how
the features are extracted, Section 4 discusses the
scoring approaches, Section 5 discusses how we re-
move the redundant sentences before adding them
to the summary, Section 6 describes our experimen-
tal study. We conclude and give future directions in
Section 7.
2 Related Work
Researchers all over the world working on query-
based summarization are trying different direc-
tions to see which methods provide the best re-
sults. The LexRank method addressed in (Erkan
and Radev, 2004) was very successful in generic
multi-document summarization. A topic-sensitive
LexRank is proposed in (Otterbacher et al, 2005).
As in LexRank, the set of sentences in a document
cluster is represented as a graph, where nodes are
sentences and links between the nodes are induced
by a similarity relation between the sentences. Then
the system ranked the sentences according to a ran-
dom walk model defined in terms of both the inter-
sentence similarities and the similarities of the sen-
tences to the topic description or question.
The summarization methods based on lexical
chain first extract the nouns, compound nouns and
named entities as candidate words (Li et al, 2007).
Then using WordNet, the systems find the semantic
similarity between the nouns and compound nouns.
After that, lexical chains are built in two steps: 1)
Building single document strong chains while dis-
ambiguating the senses of the words and, 2) build-
ing multi-chain by merging the strongest chains of
the single documents into one chain. The systems
rank sentences using a formula that involves a) the
lexical chain, b) keywords from query and c) named
entities.
(Harabagiu et al, 2006) introduce a new paradigm
for processing complex questions that relies on a
combination of (a) question decompositions; (b) fac-
toid QA techniques; and (c) Multi-Document Sum-
marization (MDS) techniques. The question decom-
position procedure operates on a Marcov chain, by
following a random walk with mixture model on a
bipartite graph of relations established between con-
cepts related to the topic of a complex question and
subquestions derived from topic-relevant passages
that manifest these relations. Decomposed questions
are then submitted to a state-of-the-art QA system
in order to retrieve a set of passages that can later be
merged into a comprehensive answer by a MDS sys-
tem. They show that question decompositions using
this method can significantly enhance the relevance
and comprehensiveness of summary-length answers
to complex questions.
There are approaches that are based on probabilis-
tic models (Pingali et al, 2007) (Toutanova et al,
2007). (Pingali et al, 2007) rank the sentences based
on a mixture model where each component of the
model is a statistical model:
Score(s) = ??QIScore(s)+(1??)?QFocus(s,Q)
Where, Score(s) is the score for sentence s. Query-
independent score (QIScore) and query-dependent score
(QFocus) are calculated based on probabilistic models.
(Toutanova et al, 2007) learns a log-linear sentence rank-
ing model by maximizing three metrics of sentence good-
ness: (a) ROUGE oracle, (b) Pyramid-derived, and (c)
Model Frequency. The scoring function is learned by fit-
ting weights for a set of feature functions of sentences
in the document set and is trained to optimize a sentence
pair-wise ranking criterion. The scoring function is fur-
ther adapted to apply to summaries rather than sentences
and to take into account redundancy among sentences.
There are approaches in ?Recognizing Textual Entail-
ment?, ?Sentence Alignment? and ?Question Answering?
that use syntactic and/or semantic information in order to
measure the similarity between two textual units. (Mac-
Cartney et al, 2006) use typed dependency graphs (same
as dependency trees) to represent the text and the hypoth-
esis. Then they try to find a good partial alignment be-
tween the typed dependency graphs representing the hy-
pothesis and the text in a search space of O((m + 1)n)
305
where hypothesis graph contains n nodes and a text graph
contains m nodes. (Hirao et al, 2004) represent the sen-
tences using Dependency Tree Path (DTP) to incorporate
syntactic information. They apply String Subsequence
Kernel (SSK) to measure the similarity between the DTPs
of two sentences. They also introduce Extended String
Subsequence Kernel (ESK) to incorporate semantics in
DTPs. (Kouylekov and Magnini, 2005) use the tree edit
distance algorithms on the dependency trees of the text
and the hypothesis to recognize the textual entailment.
According to this approach, a text T entails a hypothesis
H if there exists a sequence of transformations (i.e. dele-
tion, insertion and substitution) applied to T such that
we can obtain H with an overall cost below a certain
threshold. (Punyakanok et al, 2004) represent the ques-
tion and the sentence containing answer with their depen-
dency trees. They add semantic information (i.e. named
entity, synonyms and other related words) in the depen-
dency trees. They apply the approximate tree matching
in order to decide how similar any given pair of trees are.
They also use the edit distance as the matching criteria in
the approximate tree matching. All these methods show
the improvement over the BOW scoring methods.
Our Basic Element (BE)-based feature used the depen-
dency tree to extract the BEs (i.e. head-modifier-relation)
and ranked the BEs based on their log-likelihood ratios.
For syntactic feature, we extracted the syntactic trees for
the sentence as well as for the query using the Charniak
parser and measured the similarity between the two trees
using the tree kernel function. We used the ASSERT se-
mantic role labeler system to parse the sentence as well
as the query semantically and used the shallow seman-
tic tree kernel to measure the similarity between the two
shallow-semantic trees.
3 Feature Extraction
The sentences in the document collection are analyzed
in various levels and each of the document-sentences is
represented as a vector of feature-values. The features
can be divided into several categories:
3.1 Lexical Features
3.1.1 N-gram Overlap
N-gram overlap measures the overlapping word se-
quences between the candidate sentence and the query
sentence. With the view to measure the N-gram
(N=1,2,3,4) overlap scores, a query pool and a sentence
pool are created. In order to create the query (or sentence)
pool, we took the query (or document) sentence and cre-
ated a set of related sentences by replacing its important
words1 by their first-sense synonyms. For example given
1hence forth important words are the nouns, verbs, adverbs
and adjectives
a stemmed document-sentence: ?John write a poem?, the
sentence pool contains: ?John compose a poem?, ?John
write a verse form? along with the given sentence. We
measured the recall based n-gram scores for a sentence P
using the following formula:
n-gramScore(P) = maxi(maxj N-gram(si, qj))
N-gram(S,Q) =
?
gramn?S
Countmatch(gramn)
?
gramn?S
Count(gramn)
Where, n stands for the length of the n-gram (n =
1, 2, 3, 4) and Countmatch (gramn) is the number
of n-grams co-occurring in the query and the candi-
date sentence, qj is the jth sentence in the query
pool and si is the ith sentence in the sentence pool
of sentence P .
3.1.2 LCS, WLCS and Skip-Bigram
A sequence W = [w1, w2, ..., wn] is a subse-
quence of another sequence X = [x1, x2, ..., xm], if
there exists a strict increasing sequence [i1, i2, ..., ik]
of indices of X such that for all j =
1, 2, ..., k we have xij = wj . Given two sequences,
S1 and S2, the Longest Common Subsequence
(LCS) of S1 and S2 is a common subsequence with
maximum length. The longer the LCS of two sen-
tences is, the more similar the two sentences are.
The basic LCS has a problem that it does not dif-
ferentiate LCSes of different spatial relations within
their embedding sequences (Lin, 2004). To improve
the basic LCS method, we can remember the length
of consecutive matches encountered so far to a reg-
ular two dimensional dynamic program table com-
puting LCS. We call this weighted LCS (WLCS)
and use k to indicate the length of the current con-
secutive matches ending at words xi and yj . Given
two sentences X and Y, the WLCS score of X and
Y can be computed using the similar dynamic pro-
gramming procedure as stated in (Lin, 2004). We
computed the LCS and WLCS-based F-measure fol-
lowing (Lin, 2004) using both the query pool and the
sentence pool as in the previous section.
Skip-bigram is any pair of words in their sentence
order, allowing for arbitrary gaps. Skip-bigram mea-
sures the overlap of skip-bigrams between a candi-
date sentence and a query sentence. Following (Lin,
2004), we computed the skip bi-gram score using
both the sentence pool and the query pool.
306
3.1.3 Head and Head Related-words Overlap
The number of head words common in between
two sentences can indicate how much they are rel-
evant to each other. In order to extract the heads
from the sentence (or query), the sentence (or query)
is parsed by Minipar 2 and from the dependency
tree we extract the heads which we call exact head
words. For example, the head word of the sentence:
?John eats rice? is ?eat?.
We take the synonyms, hyponyms and hyper-
nyms3 of both the query-head words and the
sentence-head words and form a set of words which
we call head-related words. We measured the exact
head score and the head-related score as follows:
ExactHeadScore =
?
w1?HeadSet
Countmatch(w1)
?
w1?HeadSet
Count(w1)
HeadRelatedScore =
?
w1?HeadRelSet
Countmatch(w1)
?
w1?HeadRelSet
Count(w1)
Where HeadSet is the set of head words in the sen-
tence and Countmatch is the number of matches
between the HeadSet of the query and the sen-
tence. HeadRelSet is the set of synonyms, hy-
ponyms and hypernyms of head words in the sen-
tence and Countmatch is the number of matches
between the head-related words of the query and the
sentence.
3.2 Lexical Semantic Features
We form a set of words which we call QueryRe-
latedWords by taking the important words from the
query, their first-sense synonyms, the nouns? hy-
pernyms/hyponyms and important words from the
nouns? gloss definitions.
Synonym overlap measure is the overlap be-
tween the list of synonyms of the important words
extracted from the candidate sentence and the
QueryRelatedWords. Hypernym/hyponym overlap
measure is the overlap between the list of hypernyms
and hyponyms of the nouns extracted from the sen-
tence and the QueryRelatedWords, and gloss overlap
measure is the overlap between the list of important
words that are extracted from the gloss definitions
of the nouns of the sentence and the QueryRelated-
Words.
2http://www.cs.ualberta.ca/ lindek/minipar.htm
3hypernym and hyponym levels are restricted to 2 and 3 re-
spectively
3.3 Statistical Similarity Measures
Statistical similarity measures are based on the
co-occurance of similar words in a corpus. We
have used two statistical similarity measures:
1. Dependency-based similarity measure and 2.
Proximity-based similarity measure.
Dependency-based similarity measure uses the
dependency relations among words in order to mea-
sure the similarity. It extracts the dependency triples
then uses statistical approach to measure the similar-
ity. Proximity-based similarity measure is computed
based on the linear proximity relationship between
words only. It uses the information theoretic defini-
tion of similarity to measure the similarity.
We used the data provided by Dr. Dekang Lin4.
Using the data, one can retrieve most similar words
for a given word. The similar words are grouped into
clusters. Note that, for a word there can be more than
one cluster. Each cluster represents the sense of the
word and its similar words for that sense.
For each query word, we extract all of its clus-
ters from the data. Now, in order to determine the
right cluster for a query word, we measure the over-
lap score between the QueryRelatedWords and the
clusters of words. The hypothesis is that, the cluster
that has more words common with the QueryRelat-
edWords is the right cluster. We chose the cluster for
a word which has the highest overlap score.
Once we get the clusters for the query words, we
measured the overlap between the cluster words and
the sentence words as follows:
Measure =
?
w1?SenWords
Countmatch(w1)
?
w1?SenWords
Count(w1)
Where, SenWords is the set of important words ex-
tracted from the sentence and Countmatch is the number
of matches between the sentence words and the clusters
of similar words of the query words.
3.4 Graph-based Similarity Measure
In LexRank (Erkan and Radev, 2004), the concept of
graph-based centrality is used to rank a set of sentences,
in producing generic multi-document summaries. A sim-
ilarity graph is produced for the sentences in the docu-
ment collection. In the graph, each node represents a
sentence. The edges between the nodes measure the co-
sine similarity between the respective pair of sentences.
The degree of a given node is an indication of how much
important the sentence is. Once the similarity graph is
4http://www.cs.ualberta.ca/ lindek/downloads.htm
307
constructed, the sentences are then ranked according to
their eigenvector centrality. To apply LexRank to query-
focused context, a topic-sensitive version of LexRank is
proposed in (Otterbacher et al, 2005). We followed a
similar approach in order to calculate this feature. The
score of a sentence is determined by a mixture model of
the relevance of the sentence to the query and the similar-
ity of the sentence to other high-scoring sentences.
3.5 Syntactic and Semantic Features:
So far, we have included the features of type Bag of
Words (BOW). The task like query-based summarization
that requires the use of more complex syntactic and se-
mantics, the approaches with only BOW are often inade-
quate to perform fine-level textual analysis. We extracted
three features that incorporate syntactic/semantic infor-
mation.
3.5.1 Basic Element (BE) Overlap Measure
The ?head-modifier-relation? triples, extracted from
the dependency trees are considered as BEs in our exper-
iment. The triples encode some syntactic/semantic infor-
mation and one can quite easily decide whether any two
units match or not- considerably more easily than with
longer units (Zhou et al, 2005). We used the BE package
distributed by ISI5 to extract the BEs for the sentences.
Once we get the BEs for a sentence, we computed the
Likelihood Ratio (LR) for each BE following (Zhou et
al., 2005). Sorting BEs according to their LR scores pro-
duced a BE-ranked list. Our goal is to generate a sum-
mary that will answer the user questions. The ranked
list of BEs in this way contains important BEs at the top
which may or may not be relevant to the user questions.
We filter those BEs by checking whether they contain any
word which is a query word or a QueryRelatedWords (de-
fined in Section 3.2). The score of a sentence is the sum
of its BE scores divided by the number of BEs in the sen-
tence.
3.5.2 Syntactic Feature
Encoding syntactic structure is easier and straight for-
ward. Given a sentence (or query), we first parse it into
a syntactic tree using a syntactic parser (i.e. Charniak
parser) and then we calculate the similarity between the
two trees using the tree kernel defined in (Collins and
Duffy, 2001).
3.5.3 Shallow-semantic Feature
Though introducing BE and syntactic information
gives an improvement on BOW by the use of depen-
dency/syntactic parses, but these, too are not adequate
when dealing with complex questions whose answers
are expressed by long and articulated sentences or even
5BE website:http://www.isi.edu/ cyl/BE
Figure 1: Example of semantic trees
paragraphs. Shallow semantic representations, bearing a
more compact information, could prevent the sparseness
of deep structural approaches and the weakness of BOW
models (Moschitti et al, 2007).
Initiatives such as PropBank (PB) (Kingsbury and
Palmer, 2002) have made possible the design of accurate
automatic Semantic Role Labeling (SRL) systems like
ASSERT (Hacioglu et al, 2003). For example, consider
the PB annotation:
[ARG0 all][TARGET use][ARG1 the french
franc][ARG2 as their currency]
Such annotation can be used to design a shallow se-
mantic representation that can be matched against other
semantically similar sentences, e.g.
[ARG0 the Vatican][TARGET use][ARG1 the Italian
lira][ARG2 as their currency]
In order to calculate the semantic similarity between
the sentences, we first represent the annotated sentence
using the tree structures like Figure 1 which we call Se-
mantic Tree (ST). In the semantic tree, arguments are re-
placed with the most important word-often referred to as
the semantic head.
The sentences may contain one or more subordinate
clauses. For example the sentence, ?the Vatican, located
wholly within Italy uses the Italian lira as their currency.?
gives the STs as in Figure 2. As we can see in Fig-
ure 2(A), when an argument node corresponds to an en-
tire subordinate clause, we label its leaf with ST, e.g.
the leaf of ARG0. Such ST node is actually the root of
the subordinate clause in Figure 2(B). If taken separately,
such STs do not express the whole meaning of the sen-
tence, hence it is more accurate to define a single struc-
ture encoding the dependency between the two predicates
as in Figure 2(C). We refer to this kind of nested STs as
STNs.
Note that, the tree kernel (TK) function defined in
(Collins and Duffy, 2001) computes the number of com-
mon subtrees between two trees. Such subtrees are sub-
ject to the constraint that their nodes are taken with all
or none of the children they have in the original tree.
308
Figure 2: Two STs composing a STN
Though, this definition of subtrees makes the TK func-
tion appropriate for syntactic trees but at the same time
makes it not well suited for the semantic trees (ST) de-
fined above. For instance, although the two STs of Fig-
ure 1 share most of the subtrees rooted in the ST node,
the kernel defined above computes only one match (ST
ARG0 TARGET ARG1 ARG2) which is not useful.
The critical aspect of the TK function is that the pro-
ductions of two evaluated nodes have to be identical to
allow the match of further descendants. This means that
common substructures cannot be composed by a node
with only some of its children as an effective ST represen-
tation would require. (Moschitti et al, 2007) solve this
problem by designing the Shallow Semantic Tree Kernel
(SSTK) which allows to match portions of a ST. We fol-
lowed the similar approach to compute the SSTK.
4 Ranking Sentences
In this section, we describe the scoring techniques in de-
tail.
4.1 Learning Feature-weights: A Local Search
Strategy
In order to fine-tune the weights of the features, we used
a local search technique with simulated annealing to find
the global maximum. Initially, we set al the feature-
weights, w1, ? ? ? , wn, as equal values (i.e. 0.5) (see Al-
gorithm 1). Based on the current weights we score the
sentences and generate summaries accordingly. We eval-
uate the summaries using the automatic evaluation tool
ROUGE (Lin, 2004) (described in Section 6) and the
ROUGE value works as the feedback to our learning
loop. Our learning system tries to maximize the ROUGE
score in every step by changing the weights individually
by a specific step size (i.e. 0.01). That means, to learn
weight wi, we change the value of wi keeping all other
weight values (wj?j 6=i) stagnant. For each weight wi,
the algorithm achieves the local maximum of ROUGE
value. In order to find the global maximum we ran this
algorithm multiple times with different random choices
of initial values (i.e. simulated annealing).
Input: Stepsize l, Weight Initial Value v
Output: A vector ~w of learned weights
Initialize the weight values wi to v.
for i? 1 to n do
rg1 = rg2 = prev = 0
while (true) do
scoreSentences(~w)
generateSummaries()
rg2 = evaluateROUGE()
if rg1 ? rg2 then
prev = wi
wi+ = l
rg1 = rg2
else
break
end
end
end
return ~w
Algorithm 1: Tuning weights using Local Search
technique
Once we have learned the feature-weights, our empir-
ical method computes the final scores for the sentences
using the formula:
scorei = ~xi. ~w (1)
Where, ~xi is the feature vector for i-th sentence, ~w is
the weight vector and scorei is the score of i-th sentence.
4.2 K-means Learning
We start with a set of initial cluster centers and go through
several iterations of assigning each object to the cluster
whose center is closest. After all objects have been as-
signed, we recompute the center of each cluster as the
centroid or mean (?) of its members.
Once we have learned the means of the clusters using
the k-means algorithm, our next task is to rank the sen-
tences according to a probability model. We have used
Bayesian model in order to do so. Bayes? law says:
P (qk|~x,?) =
p(~x|qk,?)P (qk|?)
?K
k=1 p(~x|qk,?)p(qk|?)
(2)
where qk is a class, ~x is a feature vector repre-
senting a sentence and ? is the parameter set of all
class models. We set the weights of the clusters as
equiprobable (i.e. P (qk|?) = 1/K). We calculated
309
p(x|qk,?) using the gaussian probability distribu-
tion. The gaussian probability density function (pdf)
for the d-dimensional random variable ~x is given by:
p(?,?)(~x) =
e
?1
2 (~x??)
T??1(~x??)
?
2pi
d?
det(?)
(3)
where ?, the mean vector and ?, the covariance
matrix are the parameters of the gaussian distribu-
tion. We get the means (?) from the k-means algo-
rithm and we calculate the covariance matrix using
the unbiased covariance estimation:
?? =
1
N ? 1
N?
i=1
(xj ? ?j)(xi ? ?i)
T (4)
4.3 EM Learning
EM is an iterative two step procedure:
1. Expectation-step and 2. Maximization-step.
In the expectation step, we compute expected values
for the hidden variables hi,j which are cluster mem-
bership probabilities. Given the current parameters,
we compute how likely an object belongs to any
of the clusters. The maximization step computes
the most likely parameters of the model given the
cluster membership probabilities. The data-points
are considered to be generated by a mixture model
of k-gaussians of the form:
P (~x) =
k?
i=1
P (C = i)P (~x|?i,?i) (5)
Where the total likelihood of model ? with k
components given the observed data points, X =
x1, ? ? ? , xn is:
L(?|X) =
n?
i=1
k?
j=1
P (C = j)P (xi|?j)
=
n?
i=1
k?
j=1
wjP (xi|?j ,?j)
?
n?
i=1
log
k?
j=1
wjP (xi|?j ,?j)
where P is the probability density function (i.e.
eq 3). ?j and ?j are the mean and covariance ma-
trix of component j, respectively. Each component
contributes a proportion, wj , of the total population,
such that:
?K
j=1wj = 1.
However, a significant problem with the EM al-
gorithm is that it converges to a local maximum
of the likelihood function and hence the quality of
the result depends on the initialization. In order
to get good results from using random starting val-
ues, we can run the EM algorithm several times
and choose the initial configuration for which we
get the maximum log likelihood among all con-
figurations. Choosing the best one among several
runs is very computer intensive process. So, to im-
prove the outcome of the EM algorithm on gaus-
sian mixture models it is necessary to find a better
method of estimating initial means for the compo-
nents. To achieve this aim we explored the widely
used ?k-means? algorithm as a cluster (means) find-
ing method. That means, the means found by k-
means clustering above will be utilized as the initial
means for EM and we calculate the initial covari-
ance matrices using the unbiased covariance estima-
tion procedure (eq:4).
Once the sentences are clustered by EM al-
gorithm, we filter out the sentences which are
not query-relevant by checking their probabilities,
P (qr|xi,?) where, qr denotes the cluster ?query-
relevant?. If for a sentence xi, P (qr|xi,?) > 0.5
then xi is considered to be query-relevant.
Our next task is to rank the query-relevant sen-
tences in order to include them in the summary. This
can be done easily by multiplying the feature vector
~xi with the weight vector ~w that we learned by the
local search technique (eq:1).
5 Redundancy Checking
When many of the competing sentences are included
in the summary, the issue of information overlap be-
tween parts of the output comes up, and a mecha-
nism for addressing redundancy is needed. There-
fore, our summarization systems employ a final level
of analysis: before being added to the final output,
the sentences deemed to be important are compared
to each other and only those that are not too simi-
lar to other candidates are included in the final an-
swer or summary. Following (Zhou et al, 2005), we
modeled this by BE overlap between an intermedi-
ate summary and a to-be-added candidate summary
310
sentence. We call this overlap ratio R, where R is
between 0 and 1 inclusively. Setting R = 0.7 means
that a candidate summary sentence, s, can be added
to an intermediate summary, S, if the sentence has a
BE overlap ratio less than or equal to 0.7.
6 Experimental Evaluation
6.1 Evaluation Setup
We used the main task of Document Understanding
Conference (DUC) 2007 for evaluation. The task
was: ?Given a complex question (topic description)
and a collection of relevant documents, the task is to
synthesize a fluent, well-organized 250-word sum-
mary of the documents that answers the question(s)
in the topic.?
NIST assessors developed topics of interest to
them and choose a set of 25 documents relevant
(document cluster) to each topic. Each topic and its
document cluster were given to 4 different NIST as-
sessors. The assessor created a 250-word summary
of the document cluster that satisfies the information
need expressed in the topic statement. These multi-
ple ?reference summaries? are used in the evaluation
of summary content.
We carried out automatic evaluation of our sum-
maries using ROUGE (Lin, 2004) toolkit, which
has been widely adopted by DUC for automatic
summarization evaluation. It measures summary
quality by counting overlapping units such as the
n-grams (ROUGE-N), word sequences (ROUGE-L
and ROUGE-W) and word pairs (ROUGE-S and
ROUGE-SU) between the candidate summary and
the reference summary. ROUGE parameters were
set as the same as DUC 2007 evaluation setup.
One purpose of our experiments is to study the
impact of different features for complex question
answering task. To accomplish this, we generated
summaries for the topics of DUC 2007 by each of
our seven systems defined as below:
The LEX system generates summaries based on
only lexical features: n-gram (n=1,2,3,4), LCS,
WLCS, skip bi-gram, head, head synonym. The
LSEM system considers only lexical semantic
features: synonym, hypernym/hyponym, gloss,
dependency-based and proximity-based similarity.
The COS system generates summary based on the
graph-based method. The SYS1 system considers
all the features except the BE, syntactic and seman-
tic features. The SYS2 system considers all the fea-
tures except the syntactic and semantic features. The
SYS3 considers all the features except the semantic
and the ALL6 system generates summaries taking
all the features into account.
6.2 Evaluation Results
Table 17 to Table 3, Table 4 to Table 6 and Table 7 to
Table 9 show the evaluation measures for k-means,
EM and empirical approaches respectively. As Ta-
ble 1 shows, in k-means, SYS2 gets 0-21%, SYS3
gets 4-32% and ALL gets 3-36% improvement in
ROUGE-2 scores over the SYS1 system. We get best
ROUGE-W (Table 2) scores for SYS2 (i.e. includ-
ing BE) but SYS3 and ALL do not perform well in
this case. SYS2 improves the ROUGE-W F-score by
1% over SYS1. We do not get any improvement in
ROUGE-SU (Table 3) scores when we include any
kind of syntactic/semantic structures.
The case is different for EM and empirical ap-
proaches. Here, in every case we get a significant
amount of improvement when we include the syn-
tactic and/or semantic features. For EM (Table 4 to
Table 6), the ratio of improvement in F-scores over
SYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 2-
24% for ALL. In our empirical approach (Table 7
to Table 9), SYS2, SYS3 and ALL improve the F-
scores by 3-11%, 7-15% and 8-19% over SYS1 re-
spectively. These results clearly indicate the positive
impact of the syntactic/semantic features for com-
plex question answering task.
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.074 0.077 0.086 0.075 0.075 0.078 0.077
P 0.081 0.084 0.093 0.081 0.098 0.107 0.110
F 0.078 0.080 0.089 0.078 0.085 0.090 0.090
Table 1: ROUGE-2 measures in k-means learning
Table 10 shows the F-scores of the ROUGE mea-
sures for one baseline system, the best system in
DUC 2007 and our three scoring techniques con-
sidering all features. The baseline system gener-
6SYS2, SYS3 and ALL systems show the impact of BE,
syntactic and semantic features respectively
7R stands for Recall, P stands for Precision and F stands for
F-score
311
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.098 0.097 0.101 0.099 0.101 0.097 0.097
P 0.195 0.194 0.200 0.237 0.233 0.241 0.237
F 0.130 0.129 0.134 0.140 0.141 0.139 0.138
Table 2: ROUGE-W measures in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.131 0.127 0.139 0.136 0.135 0.135 0.135
P 0.155 0.152 0.162 0.176 0.171 0.174 0.174
F 0.142 0.139 0.150 0.153 0.151 0.152 0.152
Table 3: ROUGE-SU in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.089 0.080 0.087 0.085 0.085 0.089 0.091
P 0.096 0.087 0.094 0.092 0.095 0.116 0.138
F 0.092 0.083 0.090 0.088 0.090 0.101 0.109
Table 4: ROUGE-2 measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.103 0.096 0.101 0.102 0.101 0.102 0.101
P 0.205 0.193 0.200 0.203 0.218 0.222 0.223
F 0.137 0.128 0.134 0.136 0.138 0.139 0.139
Table 5: ROUGE-W measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.146 0.128 0.138 0.143 0.144 0.145 0.144
P 0.171 0.153 0.162 0.168 0.177 0.186 0.185
F 0.157 0.140 0.149 0.154 0.159 0.163 0.162
Table 6: ROUGE-SU measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.086 0.080 0.087 0.087 0.090 0.095 0.099
P 0.093 0.087 0.094 0.094 0.112 0.115 0.116
F 0.089 0.083 0.090 0.090 0.100 0.104 0.107
Table 7: ROUGE-2 in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.102 0.096 0.101 0.102 0.102 0.104 0.105
P 0.203 0.193 0.200 0.204 0.239 0.246 0.247
F 0.135 0.128 0.134 0.137 0.143 0.147 0.148
Table 8: ROUGE-W in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.144 0.129 0.138 0.145 0.146 0.149 0.150
P 0.169 0.153 0.162 0.171 0.182 0.195 0.197
F 0.155 0.140 0.150 0.157 0.162 0.169 0.170
Table 9: ROUGE-SU in empirical approach
ates summaries by returning all the leading sen-
tences (up to 250 words) in the ?TEXT ? field of
the most recent document(s). It shows that the em-
pirical approach outperforms the other two learning
techniques and EM performs better than k-means al-
gorithm. EM improves the F-scores over k-means
by 0.7-22.5%. Empirical approach improves the F-
scores over k-means and EM by 5.9-20.2% and 3.5-
6.5% respectively. Comparing with the DUC 2007
participants our systems achieve top scores and for
some ROUGE measures there is no statistically sig-
nificant difference between our system and the best
DUC 2007 system.
System ROUGE-
1
ROUGE-
2
ROUGE-
W
ROUGE-
SU
Baseline 0.335 0.065 0.114 0.113
Best 0.438 0.122 0.153 0.174
k-means 0.390 0.090 0.138 0.152
EM 0.399 0.109 0.139 0.162
Empirical 0.413 0.107 0.148 0.170
Table 10: F-measures for different systems
7 Conclusion and Future Work
Our experiments show the following: (a) our ap-
proaches achieve promising results, (b) empirical
approach outperforms the other two learning and
EM performs better than the k-means algorithm for
this particular task, and (c) our systems achieve bet-
ter results when we include BE, syntactic and se-
mantic features.
In future, we have the plan to decompose the com-
plex questions into several simple questions before
measuring the similarity between the document sen-
tence and the query sentence. We expect that by de-
composing complex questions into the sets of sub-
questions that they entail, systems can improve the
average quality of answers returned and achieve bet-
ter coverage for the question as a whole.
312
References
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. In Proceedings of Neural Informa-
tion Processing Systems, pages 625?632, Vancouver,
Canada.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2003. Shallow Semantic Parsing Using
Support Vector Machines. In Technical Report TR-
CSLR-2003-03, University of Colorado.
S. Harabagiu, F. Lacatusu, and A. Hickl. 2006. Answer-
ing complex questions with random walk models. In
Proceedings of the 29th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 220 ? 227. ACM.
T. Hirao, , J. Suzuki, H. Isozaki, and E. Maeda. 2004.
Dependency-based sentence alignment for multiple
document summarization. In Proceedings of Coling
2004, pages 446?452, Geneva, Switzerland. COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of the international con-
ference on Language Resources and Evaluation, Las
Palmas, Spain.
M. Kouylekov and B. Magnini. 2005. Recognizing
textual entailment with tree edit distance algorithms.
In Proceedings of the PASCAL Challenges Workshop:
Recognising Textual Entailment Challenge.
J. Li, L. Sun, C. Kit, and J. Webster. 2007. A Query-
Focused Multi-Document Summarizer Based on Lex-
ical Chains. In Proceedings of the Document Under-
standing Conference, Rochester. NIST.
C. Y. Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
Workshop on Text Summarization Branches Out, Post-
Conference Workshop of Association for Computa-
tional Linguistics, pages 74?81, Barcelona, Spain.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C. D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, page 4148, New
York, USA.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Seman-
tic Kernels for Question/Answer Classificaion. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 776?783, Prague,
Czech Republic. ACL.
J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Us-
ing Random Walks for Question-focused Sentence Re-
trieval. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 915?922,
Vancouver, Canada.
P. Pingali, Rahul K., and V. Varma. 2007. IIIT Hyder-
abad at DUC 2007. In Proceedings of the Document
Understanding Conference, Rochester. NIST.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping de-
pendencies trees: An application to question answer-
ing. In Proceedings of AI & Math, Florida, USA.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
Summarization System: Microsoft Research at DUC
2007 . In proceedings of the Document Understanding
Conference, Rochester. NIST.
D. Zhang and W. S. Lee. 2003. A Language Mod-
eling Approach to Passage Question Answering. In
Proceedings of the Twelfth Text REtreival Conference,
pages 489?495, Gaithersburg, Maryland.
L. Zhou, C. Y. Lin, and E. Hovy. 2005. A BE-based
Multi-dccument Summarizer with Query Interpreta-
tion. In Proceedings of Document Understanding
Conference, Vancouver, B.C., Canada.
313
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 329?332,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Do Automatic Annotation Techniques Have Any Impact on Supervised
Complex Question Answering?
Yllias Chali
University of Lethbridge
Lethbridge, AB, Canada
chali@cs.uleth.ca
Sadid A. Hasan
University of Lethbridge
Lethbridge, AB, Canada
hasan@cs.uleth.ca
Shafiq R. Joty
University of British Columbia
Vancouver, BC, Canada
rjoty@cs.ubc.ca
Abstract
In this paper, we analyze the impact of
different automatic annotation methods on
the performance of supervised approaches
to the complex question answering prob-
lem (defined in the DUC-2007 main task).
Huge amount of annotated or labeled
data is a prerequisite for supervised train-
ing. The task of labeling can be ac-
complished either by humans or by com-
puter programs. When humans are em-
ployed, the whole process becomes time
consuming and expensive. So, in order
to produce a large set of labeled data we
prefer the automatic annotation strategy.
We apply five different automatic anno-
tation techniques to produce labeled data
using ROUGE similarity measure, Ba-
sic Element (BE) overlap, syntactic sim-
ilarity measure, semantic similarity mea-
sure, and Extended String Subsequence
Kernel (ESSK). The representative super-
vised methods we use are Support Vec-
tor Machines (SVM), Conditional Ran-
dom Fields (CRF), Hidden Markov Mod-
els (HMM), and Maximum Entropy (Max-
Ent). Evaluation results are presented to
show the impact.
1 Introduction
In this paper, we consider the complex question
answering problem defined in the DUC-2007 main
task
1
. We focus on an extractive approach of sum-
marization to answer complex questions where a
subset of the sentences in the original documents
are chosen. For supervised learning methods,
huge amount of annotated or labeled data sets are
obviously required as a precondition. The deci-
sion as to whether a sentence is important enough
1
http://www-nlpir.nist.gov/projects/duc/duc2007/
to be annotated can be taken either by humans or
by computer programs. When humans are em-
ployed in the process, producing such a large la-
beled corpora becomes time consuming and ex-
pensive. There comes the necessity of using au-
tomatic methods to align sentences with the in-
tention to build extracts from abstracts. In this
paper, we use ROUGE similarity measure, Basic
Element (BE) overlap, syntactic similarity mea-
sure, semantic similarity measure, and Extended
String Subsequence Kernel (ESSK) to automati-
cally label the corpora of sentences (DUC-2006
data) into extract summary or non-summary cat-
egories in correspondence with the document ab-
stracts. We feed these 5 types of labeled data into
the learners of each of the supervised approaches:
SVM, CRF, HMM, and MaxEnt. Then we exten-
sively investigate the performance of the classi-
fiers to label unseen sentences (from 25 topics of
DUC-2007 data set) as summary or non-summary
sentence. The experimental results clearly show
the impact of different automatic annotation meth-
ods on the performance of the candidate super-
vised techniques.
2 Automatic Annotation Schemes
Using ROUGE Similarity Measures ROUGE
(Recall-Oriented Understudy for Gisting Evalua-
tion) is an automatic tool to determine the qual-
ity of a summary using a collection of measures
ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W
and ROUGE-S which count the number of over-
lapping units such as n-gram, word-sequences,
and word-pairs between the extract and the ab-
stract summaries (Lin, 2004). We assume each
individual document sentence as the extract sum-
mary and calculate its ROUGE similarity scores
with the corresponding abstract summaries. Thus
an average ROUGE score is assigned to each sen-
tence in the document. We choose the top N sen-
tences based on ROUGE scores to have the label
329
+1 (summary sentences) and the rest to have the
label ?1 (non-summary sentences).
Basic Element (BE) Overlap Measure We ex-
tract BEs, the ?head-modifier-relation? triples for
the sentences in the document collection using BE
package 1.0 distributed by ISI
2
. The ranked list
of BEs sorted according to their Likelihood Ra-
tio (LR) scores contains important BEs at the top
which may or may not be relevant to the abstract
summary sentences. We filter those BEs by check-
ing possible matches with an abstract sentence
word or a related word. For each abstract sen-
tence, we assign a score to every document sen-
tence as the sum of its filtered BE scores divided
by the number of BEs in the sentence. Thus, ev-
ery abstract sentence contributes to the BE score
of each document sentence and we select the top
N sentences based on average BE scores to have
the label +1 and the rest to have the label ?1.
Syntactic Similarity Measure In order to cal-
culate the syntactic similarity between the abstract
sentence and the document sentence, we first parse
the corresponding sentences into syntactic trees
using Charniak parser
3
(Charniak, 1999) and then
we calculate the similarity between the two trees
using the tree kernel (Collins and Duffy, 2001).
We convert each parenthesis representation gener-
ated by Charniak parser to its corresponding tree
and give the trees as input to the tree kernel func-
tions for measuring the syntactic similarity. The
tree kernel of two syntactic trees T
1
and T
2
is ac-
tually the inner product of the two m-dimensional
vectors, v(T
1
) and v(T
2
):
TK(T
1
, T
2
) = v(T
1
).v(T
2
)
The TK (tree kernel) function gives the simi-
larity score between the abstract sentence and the
document sentence based on the syntactic struc-
ture. Each abstract sentence contributes a score to
the document sentences and the top N sentences
are selected to be annotated as +1 and the rest as
?1 based on the average of similarity scores.
Semantic Similarity Measure Shallow seman-
tic representations, bearing a more compact infor-
mation, can prevent the sparseness of deep struc-
tural approaches and the weakness of BOW mod-
els (Moschitti et al, 2007). To experiment with
semantic structures, we parse the corresponding
2
BE website:http://www.isi.edu/ cyl/BE
3
available at ftp://ftp.cs.brown.edu/pub/nlparser/
sentences semantically using a Semantic Role La-
beling (SRL) system like ASSERT
4
. ASSERT is
an automatic statistical semantic role tagger, that
can annotate naturally occuring text with semantic
arguments. We represent the annotated sentences
using tree structures called semantic trees (ST).
Thus, by calculating the similarity between STs,
each document sentence gets a semantic similarity
score corresponding to each abstract sentence and
then the topN sentences are selected to be labeled
as +1 and the rest as ?1 on the basis of average
similarity scores.
Extended String Subsequence Kernel (ESSK)
Formally, ESSK is defined as follows (Hirao et al,
2004):
K
essk
(T,U) =
d
?
m=1
?
t
i
?T
?
u
j
?U
K
m
(t
i
, u
j
)
K
m
(t
i
, u
j
) =
{
val(t
i
, u
j
) if m = 1
K
?
m?1
(t
i
, u
j
) ? val(t
i
, u
j
)
Here, K
?
m
(t
i
, u
j
) is defined below. t
i
and u
j
are the nodes of T and U , respectively. Each node
includes a word and its disambiguated sense. The
function val(t, u) returns the number of attributes
common to the given nodes t and u.
K
?
m
(t
i
, u
j
) =
{
0 if j = 1
?K
?
m
(t
i
, u
j?1
) +K
??
m
(t
i
, u
j?1
)
Here ? is the decay parameter for the number
of skipped words. We choose ? = 0.5 for this
research. K
??
m
(t
i
, u
j
) is defined as:
K
??
m
(t
i
, u
j
) =
{
0 if i = 1
?K
??
m
(t
i?1
, u
j
) +K
m
(t
i?1
, u
j
)
Finally, the similarity measure is defined after
normalization as below:
sim
essk
(T,U) =
K
essk
(T,U)
?
K
essk
(T, T )K
essk
(U,U)
Indeed, this is the similarity score we assign to
each document sentence for each abstract sentence
and in the end, top N sentences are selected to
be annotated as +1 and the rest as ?1 based on
average similarity scores.
3 Experiments
Task Description The problem definition at
DUC-2007 was: ?Given a complex question (topic
description) and a collection of relevant docu-
ments, the task is to synthesize a fluent, well-
organized 250-word summary of the documents
4
available at http://cemantix.org/assert
330
that answers the question(s) in the topic?. We con-
sider this task and use the five automatic annota-
tion methods to label each sentence of the 50 doc-
ument sets of DUC-2006 to produce five differ-
ent versions of training data for feeding the SVM,
HMM, CRF and MaxEnt learners. We choose the
top 30% sentences (based on the scores assigned
by an annotation scheme) of a document set to
have the label +1 and the rest to have ?1. Unla-
beled sentences of 25 document sets of DUC-2007
data are used for the testing purpose.
Feature Space We represent each of the
document-sentences as a vector of feature-values.
We extract several query-related features and
some other important features from each sen-
tence. We use the features: n-gram overlap,
Longest Common Subsequence (LCS), Weighted
LCS (WLCS), skip-bigram, exact word overlap,
synonym overlap, hypernym/hyponym overlap,
gloss overlap, Basic Element (BE) overlap, syn-
tactic tree similarity measure, position of sen-
tences, length of sentences, Named Entity (NE),
cue word match, and title match (Edmundson,
1969).
Supervised Systems For SVM we use second
order polynomial kernel for the ROUGE and
ESSK labeled training. For the BE, syntactic, and
semantic labeled training third order polynomial
kernel is used. The use of kernel is based on the
accuracy we achieved during training. We apply
3-fold cross validation with randomized local-grid
search for estimating the value of the trade-off pa-
rameter C. We try the value of C in 2
i
following
heuristics, where i ? {?5,?4, ? ? ? , 4, 5} and set
C as the best performed value 0.125 for second
order polynomial kernel and default value is used
for third order kernel. We use SVM
light 5
pack-
age for training and testing in this research. In case
of HMM, we apply the Maximum Likelihood Esti-
mation (MLE) technique by frequency counts with
add-one smoothing to estimate the three HMM
parameters: initial state probabilities, transition
probabilities and emission probabilities. We use
Dr. Dekang Lin?s HMM package
6
to generate
the most probable label sequence given the model
parameters and the observation sequence (unla-
beled DUC-2007 test data). We use MALLET-0.4
NLP toolkit
7
to implement the CRF. We formu-
5
http://svmlight.joachims.org/
6
http://www.cs.ualberta.ca/
?
lindek/hmm.htm
7
http://mallet.cs.umass.edu/
late our problem in terms of MALLET?s Simple-
Tagger class which is a command line interface to
the MALLET CRF class. We modify the Simple-
Tagger class in order to include the provision for
producing corresponding posterior probabilities of
the predicted labels which are used later for rank-
ing sentences. We build the MaxEnt system using
Dr. Dekang Lin?s MaxEnt package
8
. To define the
exponential prior of the ? values in MaxEnt mod-
els, an extra parameter ? is used in the package
during training. We keep the value of ? as default.
Sentence Selection The proportion of important
sentences in the training data will differ from the
one in the test data. A simple strategy is to rank
the sentences in a document, then select the top N
sentences. In SVM systems, we use the normal-
ized distance from the hyperplane to each sample
to rank the sentences. Then, we choose N sen-
tences until the summary length (250 words for
DUC-2007) is reached. For HMM systems, we
use Maximal Marginal Relevance (MMR) based
method to rank the sentences (Carbonell et al,
1997). In CRF systems, we generate posterior
probabilities corresponding to each predicted label
in the label sequence to measure the confidence of
each sentence for summary inclusion. Similarly
for MaxEnt, the corresponding probability values
of the predicted labels are used to rank the sen-
tences.
Evaluation Results The multiple ?reference
summaries? given by DUC-2007 are used in the
evaluation of our summary content. We evalu-
ate the system generated summaries using the au-
tomatic evaluation toolkit ROUGE (Lin, 2004).
We report the three widely adopted important
ROUGE metrics in the results: ROUGE-1 (uni-
gram), ROUGE-2 (bigram) and ROUGE-SU (skip
bi-gram). Figure 1 shows the ROUGE F-measures
for SVM, HMM, CRF and MaxEnt systems. The
X-axis containing ROUGE, BE, Synt (Syntactic),
Sem (Semantic), and ESSK stands for the annota-
tion scheme used. The Y-axis shows the ROUGE-
1 scores at the top, ROUGE-2 scores at the bottom
and ROUGE-SU scores in the middle. The super-
vised systems are distinguished by the line style
used in the figure.
From the figure, we can see that the ESSK la-
beled SVM system is having the poorest ROUGE -
1 score whereas the Sem labeled system performs
8
http://www.cs.ualberta.ca/
?
lindek/downloads.htm
331
Figure 1: ROUGE F-scores for different supervised systems
best. The other annotation methods? impact is al-
most similar here in terms of ROUGE-1. Ana-
lyzing ROUGE-2 scores, we find that the BE per-
forms the best for SVM, on the other hand, Sem
achieves top ROUGE-SU score. As for the two
measures Sem annotation is performing the best,
we can typically conclude that Sem annotation is
the most suitable method for the SVM system.
ESSK works as the best for HMM and Sem la-
beling performs the worst for all ROUGE scores.
Synt and BE labeled HMMs perform almost simi-
lar whereas ROUGE labeled system is pretty close
to that of ESSK. Again, we see that the CRF per-
forms best with the ESSK annotated data in terms
of ROUGE -1 and ROUGE-SU scores and Sem
has the highest ROUGE-2 score. But BE and Synt
labeling work bad for CRF whereas the ROUGE
labeling performs decently. So, we can typically
conclude that ESSK annotation is the best method
for the CRF system. Analyzing further, we find
that ESSK works best for MaxEnt and BE label-
ing is the worst for all ROUGE scores. We can
also see that ROUGE, Synt and Sem labeled Max-
Ent systems perform almost similar. So, from this
discussion we can come to a conclusion that SVM
system performs best if the training data uses se-
mantic annotation scheme and ESSK works best
for HMM, CRF and MaxEnt systems.
4 Conclusion and Future Work
In the work reported in this paper, we have per-
formed an extensive experimental evaluation to
show the impact of five automatic annotation
methods on the performance of different super-
vised machine learning techniques in confronting
the complex question answering problem. Experi-
mental results show that Sem annotation is the best
for SVM whereas ESSK works well for HMM,
CRF and MaxEnt systems. In the near future,
we plan to work on finding more sophisticated ap-
proaches to effective automatic labeling so that we
can experiment with different supervised methods.
References
Jaime Carbonell, Yibing Geng, and Jade Goldstein.
1997. Automated query-relevant summarization and
diversity-based reranking. In IJCAI-97 Workshop on
AI in Digital Libraries, pages 12?19, Japan.
Eugene Charniak. 1999. A Maximum-Entropy-
Inspired Parser. In Technical Report CS-99-12,
Brown University, Computer Science Department.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
Neural Information Processing Systems, pages 625?
632, Vancouver, Canada.
Harold P. Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM, 16(2):264?
285.
Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, and Eisaku
Maeda. 2004. Dependency-based sentence align-
ment for multiple document summarization. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 446?452.
Chin-Yew Lin. 2004. ROUGE: A Package for Au-
tomatic Evaluation of Summaries. In Proceed-
ings of Workshop on Text Summarization Branches
Out, Post-Conference Workshop of Association for
Computational Linguistics, pages 74?81, Barcelona,
Spain.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
Syntactic and Shallow Semantic Kernels for Ques-
tion/Answer Classificaion. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 776?783, Prague, Czech
Republic. ACL.
332
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 9?12,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving the Performance of the Random Walk Model for Answering
Complex Questions
Yllias Chali and Shafiq R. Joty
University of Lethbridge
4401 University Drive
Lethbridge, Alberta, Canada, T1K 3M4
{chali,jotys}@cs.uleth.ca
Abstract
We consider the problem of answering com-
plex questions that require inferencing and
synthesizing information from multiple doc-
uments and can be seen as a kind of topic-
oriented, informative multi-document summa-
rization. The stochastic, graph-based method
for computing the relative importance of tex-
tual units (i.e. sentences) is very successful
in generic summarization. In this method,
a sentence is encoded as a vector in which
each component represents the occurrence fre-
quency (TF*IDF) of a word. However, the
major limitation of the TF*IDF approach is
that it only retains the frequency of the words
and does not take into account the sequence,
syntactic and semantic information. In this pa-
per, we study the impact of syntactic and shal-
low semantic information in the graph-based
method for answering complex questions.
1 Introduction
After having made substantial headway in factoid
and list questions, researchers have turned their at-
tention to more complex information needs that can-
not be answered by simply extracting named en-
tities like persons, organizations, locations, dates,
etc. Unlike informationally-simple factoid ques-
tions, complex questions often seek multiple differ-
ent types of information simultaneously and do not
presupposed that one single answer could meet al
of its information needs. For example, with complex
questions like ?What are the causes of AIDS??, the
wider focus of this question suggests that the sub-
mitter may not have a single or well-defined infor-
mation need and therefore may be amenable to re-
ceiving additional supporting information that is rel-
evant to some (as yet) undefined informational goal.
This type of questions require inferencing and syn-
thesizing information from multiple documents. In
Natural Language Processing (NLP), this informa-
tion synthesis can be seen as a kind of topic-oriented,
informative multi-document summarization, where
the goal is to produce a single text as a compressed
version of a set of documents with a minimum loss
of relevant information.
Recently, the graph-based method (LexRank) is
applied successfully to generic, multi-document
summarization (Erkan and Radev, 2004). A topic-
sensitive LexRank is proposed in (Otterbacher et al,
2005). In this method, a sentence is mapped to a vec-
tor in which each element represents the occurrence
frequency (TF*IDF) of a word. However, the major
limitation of the TF*IDF approach is that it only re-
tains the frequency of the words and does not take
into account the sequence, syntactic and semantic
information thus cannot distinguish between ?The
hero killed the villain? and ?The villain killed the
hero?. The task like answering complex questions
that requires the use of more complex syntactic and
semantics, the approaches with only TF*IDF are of-
ten inadequate to perform fine-level textual analysis.
In this paper, we extensively study the impact
of syntactic and shallow semantic information in
measuring similarity between the sentences in the
random walk model for answering complex ques-
tions. We argue that for this task, similarity mea-
sures based on syntactic and semantic information
performs better and can be used to characterize the
9
relation between a question and a sentence (answer)
in a more effective way than the traditional TF*IDF
based similarity measures.
2 Graph-based Random Walk Model for
Text Summarization
In (Erkan and Radev, 2004), the concept of graph-
based centrality is used to rank a set of sentences,
in producing generic multi-document summaries. A
similarity graph is produced where each node repre-
sents a sentence in the collection and the edges be-
tween nodes measure the cosine similarity between
the respective pair of sentences. Each sentence is
represented as a vector of term specific weights. The
term specific weights in the sentence vectors are
products of term frequency (tf) and inverse docu-
ment frequency (idf). The degree of a given node
is an indication of how much important the sentence
is. To apply LexRank to query-focused context, a
topic-sensitive version of LexRank is proposed in
(Otterbacher et al, 2005). The score of a sentence is
determined by a mixture model:
p(s|q) = d?
rel(s|q)
?
z?C rel(z|q)
+ (1? d)
?
?
v?C
sim(s, v)
?
z?C sim(z, v)
? p(v|q) (1)
Where, p(s|q) is the score of a sentence s given a
question q, is determined as the sum of its relevance
to the question (i.e. rel(s|q)) and the similarity to
other sentences in the collection (i.e. sim(s, v)).
The denominators in both terms are for normaliza-
tion. C is the set of all sentences in the collection.
The value of the parameter d which we call ?bias?,
is a trade-off between two terms in the equation and
is set empirically. We claim that for a complex task
like answering complex questions where the related-
ness between the query sentences and the document
sentences is an important factor, the graph-based
random walk model of ranking sentences would per-
form better if we could encode the syntactic and se-
mantic information instead of just the bag of word
(i.e. TF*IDF) information in calculating the similar-
ity between sentences. Thus, our mixture model for
answering complex questions is:
p(s|q) = d? TREESIM(s, q) + (1? d)
?
?
v?C
TREESIM(s, v)? p(v|q) (2)
Figure 1: Example of semantic trees
Where TREESIM(s,q) is the normalized syntactic
(and/or semantic) similarity between the query (q)
and the document sentence (s) and C is the set of
all sentences in the collection. In cases where the
query is composed of two or more sentences, we
compute the similarity between the document sen-
tence (s) and each of the query-sentences (qi) then
we take the average of the scores.
3 Encoding Syntactic and Shallow
Semantic Structures
Encoding syntactic structure is easier and straight
forward. Given a sentence (or query), we first parse
it into a syntactic tree using a syntactic parser (i.e.
Charniak parser) and then we calculate the similarity
between the two trees using the general tree kernel
function (Section 4.1).
Initiatives such as PropBank (PB) (Kingsbury and
Palmer, 2002) have made possible the design of
accurate automatic Semantic Role Labeling (SRL)
systems like ASSERT (Hacioglu et al, 2003). For
example, consider the PB annotation:
[ARG0 all][TARGET use][ARG1 the french
franc][ARG2 as their currency]
Such annotation can be used to design a shallow
semantic representation that can be matched against
other semantically similar sentences, e.g.
[ARG0 the Vatican][TARGET use][ARG1 the
Italian lira][ARG2 as their currency]
In order to calculate the semantic similarity be-
tween the sentences, we first represent the annotated
sentence using the tree structures like Figure 1 which
we call Semantic Tree (ST). In the semantic tree, ar-
guments are replaced with the most important word-
often referred to as the semantic head.
The sentences may contain one or more subordi-
nate clauses. For example the sentence, ?the Vati-
can, located wholly within Italy uses the Italian lira
10
Figure 2: Two STs composing a STN
as their currency.? gives the STs as in Figure 2. As
we can see in Figure 2(A), when an argument node
corresponds to an entire subordinate clause, we la-
bel its leaf with ST , e.g. the leaf of ARG0. Such ST
node is actually the root of the subordinate clause
in Figure 2(B). If taken separately, such STs do not
express the whole meaning of the sentence, hence it
is more accurate to define a single structure encod-
ing the dependency between the two predicates as in
Figure 2(C). We refer to this kind of nested STs as
STNs.
4 Syntactic and Semantic Kernels for Text
4.1 Tree Kernels
Once we build the trees (syntactic or semantic),
our next task is to measure the similarity be-
tween the trees. For this, every tree T is rep-
resented by an m dimensional vector v(T ) =
(v1(T ), v2(T ), ? ? ? vm(T )), where the i-th element
vi(T ) is the number of occurrences of the i-th tree
fragment in tree T . The tree fragments of a tree are
all of its sub-trees which include at least one produc-
tion with the restriction that no production rules can
be broken into incomplete parts.
Implicitly we enumerate all the possible tree frag-
ments 1, 2, ? ? ? ,m. These fragments are the axis
of this m-dimensional space. Note that this could
be done only implicitly, since the number m is ex-
tremely large. Because of this, (Collins and Duffy,
2001) defines the tree kernel algorithm whose com-
putational complexity does not depend on m. We
followed the similar approach to compute the tree
kernel between two syntactic trees.
4.2 Shallow Semantic Tree Kernel (SSTK)
Note that, the tree kernel (TK) function defined in
(Collins and Duffy, 2001) computes the number of
common subtrees between two trees. Such subtrees
are subject to the constraint that their nodes are taken
with all or none of the children they have in the orig-
inal tree. Though, this definition of subtrees makes
the TK function appropriate for syntactic trees but
at the same time makes it not well suited for the se-
mantic trees (ST) defined in Section 3. For instance,
although the two STs of Figure 1 share most of the
subtrees rooted in the ST node, the kernel defined
above computes no match.
The critical aspect of the TK function is that the
productions of two evaluated nodes have to be iden-
tical to allow the match of further descendants. This
means that common substructures cannot be com-
posed by a node with only some of its children as
an effective ST representation would require. Mos-
chitti et al (2007) solve this problem by designing
the Shallow Semantic Tree Kernel (SSTK) which
allows to match portions of a ST. We followed the
similar approach to compute the SSTK.
5 Experiments
5.1 Evaluation Setup
The Document Understanding Conference (DUC)
series is run by the National Institute of Standards
and Technology (NIST) to further progress in sum-
marization and enable researchers to participate in
large-scale experiments. We used the DUC 2007
datasets for evaluation.
We carried out automatic evaluation of our sum-
maries using ROUGE (Lin, 2004) toolkit, which
has been widely adopted by DUC for automatic
summarization evaluation. It measures summary
quality by counting overlapping units such as the
n-gram (ROUGE-N), word sequences (ROUGE-L
and ROUGE-W) and word pairs (ROUGE-S and
ROUGE-SU) between the candidate summary and
the reference summary. ROUGE parameters were
set as the same as DUC 2007 evaluation setup. All
the ROUGE measures were calculated by running
ROUGE-1.5.5 with stemming but no removal of
stopwords. The ROUGE run-time parameters are:
ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2
-m -l 250 -a
11
The purpose of our experiments is to study the
impact of the syntactic and semantic representation
for complex question answering task. To accomplish
this, we generate summaries for the topics of DUC
2007 by each of our four systems defined as below:
(1) TF*IDF: system is the original topic-sensitive
LexRank described in Section 2 that uses the simi-
larity measures based on tf*idf.
(2) SYN: system measures the similarity between
the sentences using the syntactic tree and the gen-
eral tree kernel function defined in Section 4.1.
(3) SEM: system measures the similarity between
the sentences using the shallow semantic tree and
the shallow semantic tree kernel function defined in
Section 4.2.
(4) SYNSEM: system measures the similarity be-
tween the sentences using both the syntactic and
shallow semantic trees and their associated kernels.
For each sentence it measures the syntactic and se-
mantic similarity with the query and takes the aver-
age of these measures.
5.2 Evaluation Results
The comparison between the systems in terms of
their F-scores is given in Table 1. The SYN system
improves the ROUGE-1, ROUGE-L and ROUGE-
W scores over the TF*IDF system by 2.84%, 0.53%
and 2.14% respectively. The SEM system im-
proves the ROUGE-1, ROUGE-L, ROUGE-W, and
ROUGE-SU scores over the TF*IDF system by
8.46%, 6.54%, 6.56%, and 11.68%, and over the
SYN system by 5.46%, 5.98%, 4.33%, and 12.97%
respectively. The SYNSEM system improves the
ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-
SU scores over the TF*IDF system by 4.64%,
1.63%, 2.15%, and 4.06%, and over the SYN sys-
tem by 1.74%, 1.09%, 0%, and 5.26% respectively.
The SEM system improves the ROUGE-1, ROUGE-
L, ROUGE-W, and ROUGE-SU scores over the
SYNSEM system by 3.65%, 4.84%, 4.32%, and
7.33% respectively which indicates that including
syntactic feature with the semantic feature degrades
the performance.
6 Conclusion
In this paper, we have introduced the syntactic and
shallow semantic structures and discussed their im-
Systems ROUGE 1 ROUGE L ROUGE W ROUGE SU
TF*IDF 0.359458 0.334882 0.124226 0.130603
SYN 0.369677 0.336673 0.126890 0.129109
SEM 0.389865 0.356792 0.132378 0.145859
SYNSEM 0.376126 0.340330 0.126894 0.135901
Table 1: ROUGE F-scores for different systems
pacts in measuring the similarity between the sen-
tences in the random walk framework for answer-
ing complex questions. Our experiments suggest the
following: (a) similarity measures based on the syn-
tactic tree and/or shallow semantic tree outperforms
the similarity measures based on the TF*IDF and (b)
similarity measures based on the shallow semantic
tree performs best for this problem.
References
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. In Proceedings of Neural Informa-
tion Processing Systems, pages 625?632, Vancouver,
Canada.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2003. Shallow Semantic Parsing Using
Support Vector Machines. In Technical Report TR-
CSLR-2003-03, University of Colorado.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of the international con-
ference on Language Resources and Evaluation, Las
Palmas, Spain.
C. Y. Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
Workshop on Text Summarization Branches Out, Post-
Conference Workshop of Association for Computa-
tional Linguistics, pages 74?81, Barcelona, Spain.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Seman-
tic Kernels for Question/Answer Classificaion. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 776?783, Prague,
Czech Republic. ACL.
J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Us-
ing Random Walks for Question-focused Sentence Re-
trieval. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 915?922,
Vancouver, Canada.
12
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 476?479,
Prague, June 2007. c?2007 Association for Computational Linguistics
UofL: Word Sense Disambiguation Using Lexical Cohesion 
Yllias Chali  
        Department of Computer Science 
University of Lethbridge  
Lethbridge, Alberta, Canada, T1K 3M4 
chali@cs.uleth.ca 
Shafiq R. Joty 
Department of Computer Science 
University of Lethbridge  
Lethbridge, Alberta, Canada, T1K 3M4 
jotys@cs.uleth.ca 
 
 
Abstract 
One of the main challenges in the applica-
tions (i.e.: text summarization, question an-
swering, information retrieval, etc.) of 
Natural Language Processing is to deter-
mine which of the several senses of a word 
is used in a given context. The problem is 
phrased as ?Word Sense Disambiguation 
(WSD)? in the NLP community. This paper 
presents the dictionary based disambigua-
tion technique that adopts the assumption 
of one sense per discourse in the context of 
SemEval-2007 Task 7: ?Coarse-grained 
English all-words?.  
1 Introduction 
Cohesion can be defined as the way certain words 
or grammatical features of a sentence can connect 
it to its predecessors (and successors) in a text. 
(Halliday and Hasan, 1976) defined cohesion as 
?the set of possibilities that exist in the language 
for making text hang together?. Cohesion occurs 
where the interpretation of some element in the 
discourse is dependent on that of another. For ex-
ample, an understanding of the reference of a pro-
noun (i.e.: he, she, it, etc.) requires to look back to 
something that has been said before. Through this 
cohesion relation, two text clauses are linked to-
gether. 
Cohesion is achieved through the use in the text 
of semantically related terms, reference, ellipse and 
conjunctions (Barzilay and Elhadad, 1997). Among 
the different cohesion-building devices, the most 
easily identifiable and the most frequent type is 
lexical cohesion. Lexical cohesion is created by 
using semantically related words (repetitions, 
synonyms, hypernyms, hyponyms, meronyms and 
holonyms, glosses, etc.)  
Our technique used WordNet (Miller, 1990) as 
the knowledge source to find the semantic relations 
among the words in a text. We assign weights to 
the semantic relations. The technique can be de-
composed into two steps: (1) building a representa-
tion of all possible senses of the words and (2) dis-
ambiguating the words based on the highest score.  
The remainder of this paper is organized as fol-
lows. In the next section, we review previous work. 
In Section 3, we define the semantic relations and 
their weights. Section 4 presents our two step pro-
cedure for WSD. We conclude with the evaluation. 
2 Previous Work 
Lexical Chaining is the process of connecting se-
mantically related words, creating a set of chains 
that represent different threads of cohesion through 
the text (Galley and McKeown, 2003). This inter-
mediate representation of text has been used in 
many natural language processing applications, 
including automatic summarization (Barzilay and 
Elhadad, 1997; Silber and McCoy, 2003), informa-
tion retrieval (Al-Halimi and Kazman, 1998), and 
intelligent spell checking (Hirst and St-Onge, 
1998). 
Morris and Hirst (1991) at first proposed a man-
ual method for computing lexical chains and first 
computational model of lexical chains was intro-
duced by Hirst and St-Onge (1997). This linear-
time algorithm, however, suffers from inaccurate 
WSD, since their greedy strategy immediately dis-
ambiguates a word as it is first encountered. Later 
476
research (Barzilay and Elhadad, 1997) significantly 
alleviated this problem at the cost of a worse run-
ning time (quadratic); computational inefficiency is 
due to their processing of many possible combina-
tions of word senses in the text in order to decide 
which assignment is the most likely. Silber and 
McCoy (2003) presented an efficient linear-time 
algorithm to compute lexical chains, which models 
Barzilay?s approach, but nonetheless has inaccura-
cies in WSD. 
More recently, Galley and McKeown (2003) 
suggested an efficient chaining method that sepa-
rated WSD from the actual chaining. It performs 
the WSD before the construction of the chains. 
They showed that it could achieve more accuracy 
than the earlier ones. Our method follows the simi-
lar technique with some new semantic relations 
(i.e.: gloss, holonym, meronym). 
3 Semantic Relations 
We used WordNet2.11 (Miller, 1990) and eXtended 
WordNet (Moldovan and Mihalcea, 2001) as our 
knowledge source to find the semantic relations 
among the words in a context.  We assigned a 
weight to each semantic relation. The relations and 
their scores are summarized in the table 1. 
4 System Overview 
The global architecture of our system is shown in 
Figure 1. Each of the modules of the system is de-
scribed below. 
4.1 Context Processing 
Context-processing involves preprocessing the con-
texts using several tools.  We have used the follow-
ing tools:  
Extracting the main text: This module extracts 
the context of the target word from the source xml 
document removing the unnecessary tags and 
makes the context ready for further processing. 
 
Sentence Splitting, Text Stemming and 
Chunking: This module splits the context into sen-
tences, then stems out the words and chunks those. 
We used OAK systems 2  (Sekine, 2002) for this 
purpose.  
                                                 
1
 http://wordnet.princeton.edu/ 
2
 http://nlp.cs.nyu.edu/oak/ 
 
Candidate Words Extraction: This module ex-
tracts the candidate words (for task 7: noun, verb, 
adjective and adverb) from the chunked text. 
4.2 All Sense Representation 
Each candidate word is expanded to all of its 
senses. We created a hash representation to identify 
all possible word representations, motivated from 
Galley and McKeown (2003). Each word sense is 
inserted into the hash entry having the index value 
equal to its synsetID. For example, athlete and jock 
are inserted into the same hash entry (Figure 2). 
 
 
 
Figure 2.  Hash indexed by synsetID 
 
On insertion of the candidate sense into the hash 
we check to see if there exists an entry into the in-
dex value, with which the current word sense has 
one of the above mentioned relations. No disam-
biguation is done at this point; the only purpose is 
to build a representation used in the next stage of 
the algorithm. This representation can be shown as 
a disambiguation graph (Galley and McKeown, 
2003) where the nodes represent word instances 
with their WordNet senses and weighted edges 
connecting the senses of two different words repre-
sent semantic relations (Figure: 3). 
 
 
 
Figure 3. Partial Disambiguation graph, Bass has 
two senses, 1. Food related 2. Music instrument 
related sense. The instrument sense dominates over 
the fish sense as it has more relations (score) with 
the other words in the context. 
Athlete Jock 
Gymnast 
09675378 
10002518 
???
 
Hypernym/ 
Hyponym 
    
Bass   
Instrument sense 
sound 
property  
      Food sense 
Pitch 
Fish 
ground 
bass 
477
4.3 Sense Disambiguation 
We use the intermediate representation (disam-
biguation graph) to perform the WSD. We sum the 
weight of all edges leaving the nodes under their 
different senses. The one sense with the highest 
score is considered the most probable sense. For 
example in fig: 3 Bass is connected with three 
words: Pitch, ground bass and sound property by 
its instrument sense and with one word: Fish by its 
Food sense. For this specific example all the se-
mantic relations are of Hyponym/Hypernym type 
(score 0.33). So we get the score as in table 2.  
In case of tie between two or more senses, we 
select the one sense that comes first in WordNet, 
since WordNet orders the senses of a word by de-
creasing order of frequency. 
  
Sense Mne-
monic  
Score Disambigu-
ated Sense 
4928349 Musical 
Instru-
ment 
3*0.33
=0.99 
7672239 Fish or 
Food 
0.33 
Musical In-
strument 
(4928349) 
 
Table 2.  Score of the senses of word ?Bass? 
 
 
 
Relation Definition Example Weight 
Repetition Same occurrences of the word Weather is great in Atlanta. Florida is 
having a really bad weather. 
1 
Synonym Words belonging to the same syn-
set in WordNet 
Not all criminals are outlaws. 1 
Hypernym 
and Hypo-
nym 
Y is a hypernym of X if X is a 
(kind of) Y And 
X is a hyponym of Y if X is a (kind 
of) Y. 
Peter bought a computer. It was a Dell 
machine. 
0.33 
Holonym 
And 
Meronym 
Y is a holonym of X if X is a part 
of Y And  
X is a meronym of Y if X is a part 
of Y 
The keyboard of this computer is not 
working. 
0.33 
Gloss Definition and/or example sen-
tences for a synset. 
Gloss of word ?dormitory? is  
{a college or university building con-
taining living quarters for students} 
0.33 
 
      Table 1: The relations and their associated weights 
 
 
 
 
 
                               
   Figure 1: Overview of WSD System 
 
 
Context  
Processing 
Sense Disam-
biguation 
 
All Sense Represen-
tation 
Disambiguated  
Sense 
Candidate words 
Extraction 
Source Con-
text 
Chunked Text 
478
             
5 Evaluation 
In SemEval-2007, we participated in Task 7: 
?Coarse-grained English all-words?. The evalua-
tion of our system is given below: 
 
Cases Precision Recall F1-measure 
Average 0.52592 0.48744 0.50595 
Best 0.61408 0.59239 0.60304 
Worst 0.44375 0.41159 0.42707 
 
6 Conclusion 
In this paper, we presented briefly our WSD sys-
tem in the context of SemEval 2007 Task 7. Along 
with normal WordNet relations, our method also 
included additional relations such as repetition and 
gloss using semantically enhanced tool, eXtended 
WordNet. After disambiguation, the intermediate 
representation (disambiguation graph) can be used 
to build the lexical chains which in tern can be used 
as an intermediate representation for other NLP 
applications such as text summarization, question 
answering, text clustering. This method (summing 
edge weights in selecting the right sense) of WSD 
before constructing the chain (Gallery and McKe-
own, 2003) outperforms the earlier methods of 
Barzilay and Elhadad (1997) and Silber and 
McCoy (2003) but this method is highly dependent 
on the lexical cohesion among words in a context. 
So the length of context is an important factor for 
our system to achieve good performance. For the 
task the context given for a tagged word was not so 
large to capture the semantic relations among 
words. This may be the one of the reasons for 
which our system could not achieve one of the best 
results. 
 
References 
Barzilay, R. and Elhadad, M.  1997. Using Lexical 
Chains for Text Summarization. In Proceedings 
of the 35th Annual Meeting of the Association 
for Computational Linguistics and the 8th Euro-
pean Chapter Meeting of the Association for 
Computational Linguistics, Workshop on Intel-
ligent Scalable Test Summarization, pages 10-
17, Madrid. 
 
Chali, Y. and Kolla, M. 2004. Summarization 
techniques at DUC 2004. In Proceedings of the 
Document Understanding Conference, pages 
105 -111, Boston. NIST. 
 
Galley, M. and McKeown, K. 2003. Improving 
Word Sense Disambiguation in Lexical Chain-
ing. In Proceedings of the 18th International 
Joint Conference on Artificial Intelligence 
(IJCAI?03), pages 1486-1488, Acapulco, Mex-
ico. 
 
Halliday M. and Hasan R. 1976. Cohesion in Eng-
lish. Longman, London. 
 
Harabagiu S. and Moldovan D. 1998. WordNet: 
An Electronic Lexical Database, chapter Knowl-
edge Processing on an Extended WordNet. MIT 
press. 
 
Hirst G. and St-Onge D.  1997. Lexical Chains as 
representation of context for the detection and 
correction of malapropisms. In Christiane Fell-
baum, editor, WordNet: An Electronic Lexical 
Database and Some of its Applications. MIT 
Press, pages 305-332. 
 
Morris J. and Hirst. G. 1991, Lexical Cohesion 
Computed by Thesaural Relations as an Indica-
tor of the Structure of Text .Computational Lin-
guistics, 17(1):21-48. 
 
Silber H.G. and McCoy K.F. 2002. Efficiently Com-
puted Lexical Chains As an Intermediate Representa-
tion for Automatic Text Summarization. Computa-
tional Linguistics, 28(4):487-496. 
479
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 193?202, Dublin, Ireland, August 23-29 2014.
A Study of using Syntactic and Semantic Structures
for Concept Segmentation and Labeling
Iman Saleh
?
, Shafiq Joty, Llu??s M
`
arquez,
Alessandro Moschitti, Preslav Nakov
ALT Research Group
Qatar Computing Research Institute
{sjoty,lmarquez,amoschitti,pnakov}
@qf.org.qa
Scott Cyphers, Jim Glass
MIT CSAIL
Cambridge, Massachusetts 02139
USA
{cyphers,glass}@mit.edu
Abstract
This paper presents an empirical study on using syntactic and semantic information for Concept
Segmentation and Labeling (CSL), a well-known component in spoken language understand-
ing. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We
perform extensive experimentation by comparing different tree-based kernels with a variety of
representations of the available linguistic information, including semantic concepts, words, POS
tags, shallow and full syntax, and discourse trees. The results show that the structured representa-
tion with the semantic concepts yields significant improvement over the base CSL parser, much
larger compared to learning with an explicit feature vector representation. We also show that
shallow syntax helps improve the results and that discourse relations can be partially beneficial.
1 Introduction
Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms,
or, equivalently, database queries, which can then be used to satisfy the user?s information needs. This
process is known as Concept Segmentation and Labeling (CSL): it maps utterances into meaning repre-
sentations based on semantic constituents. The latter are basically sequences of semantic entities, often
referred to as concepts, attributes or semantic tags. Traditionally, grammar-based methods have been
used for CSL, but more recently machine learning approaches to semantic structure computation have
been shown to yield higher accuracy. However, most previous work did not exploit syntactic/semantic
structures of the utterances, and the state-of-the-art is represented by conditional models for sequence la-
beling, such as Conditional Random Fields (Lafferty et al., 2001) trained with simple morphological and
lexical features. In our study, we measure the impact of syntactic and discourse structures by also com-
bining them with innovative features. In the following subsections, we present the application context
for our CSL task and then we outline the challenges and the findings of our research.
1.1 Semantic parsing for the ?restaurant? domain
We experiment with the dataset of McGraw et al. (2012), containing spoken and typed questions about
restaurants, which are to be answered using a database of free text such as reviews, categorical data such
as names and locations, and semi-categorical data such as user-reported cuisines and amenities.
Semantic parsing, in the form of sequential segmentation and labeling, makes it easy to convert spoken
and typed questions such as ?cheap lebanese restaurants in doha with take out? into database queries.
First, a language-specific semantic parser tokenizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in] [
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to the segments, with the option to possibly relabel mis-
labeled segments; at this point, discourse history may be incorporated as well.
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
?
Iman Saleh (iman.saleh@fci-cu.edu.eg) is affiliated to Faculty of Computers and Information, Cairo University.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
193
Finally, a database query is formed from the list of labels and values, and is then executed against the
database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed.
{$and [{cuisine:"lebanese"}, {city:"doha"}, {price:"low"}, {amenity:"carry out"}]}
1.2 Related work on CSL
Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were
word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein
and Hastie, 1997; Santaf?e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other genera-
tive models were applied, which model the joint probability of a word sequence and a concept sequence,
as well as discriminative models, which directly model a conditional probability over the concepts in the
input text.
Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied
stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local
syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al.
(1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty
et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an
approach for CSL that is specific to query understanding for web applications. A general survey of CSL
approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on
shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview.
Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels
for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used
explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins,
2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al.,
2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL
hypotheses using structures built on top of concepts, words and features that are simpler than those
studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar
to ours, as it models the extraction of semantics as a reranking task using string kernels.
1.3 Syntactic and semantic structures for CSL
The related work has highlighted that automatic CSL is mostly based on powerful machine learning al-
gorithms and simple feature representations based on word and tag n-grams. In this paper, we study the
impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and
discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts
derived by a local model, where the hypotheses are represented as trees enriched with semantic con-
cepts similarly to (Dinarelli et al., 2011). These tree-based structures can capture dependencies between
sentence constituents and concepts. However, extracting features from them is rather difficult as their
number is exponentially large. Thus, we rely on structural kernels (e.g., see (Moschitti, 2006)) for au-
tomatically encoding tree fragments, which represent syntactic and semantic dependencies from words
and concepts, and we train the reranking functions with Support Vector Machines (e.g., see (Joachims,
1999)). Additionally, we experiment with several types of kernels and newly designed feature vectors.
We test our models on the above-mentioned Restaurant domain. The results show that (i) the basic
CRF model, in fact semi-CRF (see below), is very accurate, achieving more than 83% in F
1
-score, which
indicates that improving over the semi-CRF approach is very hard; (ii) the upper-bound performance
of the reranking approach is very high as well, i.e., the correct annotation is generated in the first 100
hypotheses in 98.72% of the cases; (iii) our feature vectors show improvement only when all feature
groups are used together; otherwise, we only observe marginal improvement; (iv) structural kernels yield
a 10% relative error reduction from the semi-CRF baseline, which is more than double the feature vector
result; (v) syntactic information significantly improves on the best model, but only when using shallow
syntax; and finally, (vi) although, discourse structures provide good improvement over the semi-CRF
model, they perform lower than shallow syntax (thus, a valuable use of discourse features is still an open
problem that we plan to pursue in future work).
194
2 CSL reranking
Reranking is based on a list of N annotation hypotheses, which are generated and sorted by probability
using local classifiers. Then a reranker, typically a meta-classifier, tries to select the best hypothesis from
the list. The reranker can exploit global information, and, specifically, the dependencies between the
different concepts that are made available by the local model. We use semi-CRF as our local model since
it yields the highest accuracy in CSL (when using a single model), and preference reranking with kernel
machines to rerank the N hypotheses generated by the semi-CRF.
2.1 Basic parser using semi-CRF
We use a semi-Markov CRF (Sarawagi and Cohen, 2004), or semi-CRF, a variation of a linear-chain
CRF (Lafferty et al., 2001), to produce the N -best list of labeled segment hypotheses that serve as the
input to reranking. In a linear-chain CRF, with a sequence of tokens x and labels y, we approximate
p(y|x) as a product of factors of the form p(y
i
|y
i?1
, x), which corresponds to features of the form
f
j
(y
i?1
, y
i
, i, x), where i iterates over the token/label positions. This supports a Viterbi search for the
approximateN best values of y. WithM label values, if for each label y
m
we know the bestN sequences
of labels y
1
, y
2
, . . . , y
i?1
= y
m
, then we can use p(y
i
|y
i?1
, x) to get the probability for extending each
path by each possible label y
i
= y
?
m
. Then for each label y
?
m
, we will have MN paths and scores, one
from each of the paths of length i? 1 ending with y
m
. For each y
?
m
, we pick the N best extended paths.
With semi-CRF, we want a labeled segmentation s rather than a sequence of labels. Each segment
s
i
= (y
i
, t
i
, u
i
) has a label y
i
as well as a starting and ending token position for the segment, t
i
and
u
i
respectively, where u
i
+ 1 = t
i+1
. We approximate p(s|x), with factors of the form p(s
i
|s
i?1
, x),
which we simplify to p(y
i
, u
i
|y
i?1
, t
i
), so features take the form f
j
(y
i?1
, y
i
, t
i
, u
i
), i.e., they can use the
previous segment?s label and the current segment?s label and endpoints. The Viterbi search is extended
to search for a pair of label and segment end. Whereas for M labels we kept track of MN paths, we
must keep track of MLN paths, where L is the maximum segment length.
We use token n-gram features relative to the segment boundaries, n-grams within the segment, token
regular expression and lexicon features within a segment. Each of these features also includes the labels
of the previous and current segment, and the segment length.
2.2 Preference reranking with kernel machines
Preference reranking (PR) uses a classifier C of pairs of hypotheses ?H
i
, H
j
?, which decides if H
i
is
better thanH
j
. Given each training question Q, positive and negative examples are generated for training
the classifier. We adopt the following approach for example generation: the pairs ?H
1
, H
i
? constitute
positive examples, where H
1
has the lowest error rate with respect to the gold standard among the
hypotheses for Q, and vice versa, ?H
i
, H
1
? are considered as negative examples. At testing time, given
a new question Q
?
, C classifies all pairs ?H
i
, H
j
? generated from the annotation hypotheses of Q
?
: a
positive classification is a vote for H
i
, otherwise the vote is for H
j
. Also, the classifier score can be used
as a weighted vote. H
k
are then ranked according to the number (sum) of the (weighted) votes they get.
We build our reranker with kernel machines. The latter, e.g., SVMs, classify an input object o using
the following function: C(o) =
?
i
?
i
y
i
K(o, o
i
), where ?
i
are model parameters estimated from the
training data, o
i
are support objects and y
i
are the labels of the support objects. K(?, ?) is a kernel
function, which computes the scalar product between the two objects in an implicit vector space. In the
case of the reranker, the objects o are ?H
i
, H
j
?, and the kernel is defined as follow:
K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) = S(H
1
, H
?
1
) + S(H
2
, H
?
2
)? S(H
1
, H
?
2
)? S(H
2
, H
?
1
).
Our reranker also includes traditional feature vectors in addition to the trees. Therefore, we define each
hypothesis H as a tuple ?T,~v? composed of a tree T and a feature vector ~v. We then define a structural
kernel (similarity) between two hypotheses H and H
?
as follows: S(H,H
?
) = S
TK
(T, T
?
) + S
v
(~v,~v
?
),
where S
TK
is one of the tree kernel functions defined in Section 3.1, and S
v
is a kernel over feature
vectors (see Section 3.3), e.g., linear, polynomial, gaussian, etc.
195
(a) Basic Tree (BT). (b) Discourse Tree (DT).
(c) Shallow Syntactic Tree (ShT).
(d) Syntactic Tree (ST).
(e) BT with POS (BTP).
Figure 1: Syntactic/semantic trees. The numeric semantic tagset is defined in Table 7.
3 Structural kernels for semantic parsing
In this section, we briefly describe the kernels we use in S(H,H
?
) for preference reranking. We engineer
them by combining three aspects: (i) different types of existing tree kernels, (ii) new syntactic/semantic
structures for representing CSL, and (iii) new feature vectors.
3.1 Tree kernels
Structural kernels, e.g., tree and sequence kernels, measure the similarity between two structures in terms
of their shared substructures. One interesting aspect is that these kernels correspond to a scalar product
in the fragment space, where each substructure is a feature. Therefore, they can be used in the training
and testing algorithms of kernel machines (see Section 2.2). Below, we briefly describe different types of
kernels we tested in our study, which are made available in the SVM-Light-TK toolkit (Moschitti, 2006).
Subtree Kernel (K0) is one of the simplest tree kernels, as it only generates complete subtrees, i.e., tree
fragments that, given any arbitrary starting node, necessarily include all its descendants.
Syntactic Tree Kernel (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob-
jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot
be separated from their parents. In other words, substructures are composed of atomic building blocks
corresponding to nodes, along with all of their direct children. In the case of a syntactic parse tree, these
are complete production rules for the associated parser grammar.
Syntactic Tree Kernel + BOW (K2) extends ST by allowing leaf nodes to be part of the feature space.
The leaves of the trees correspond to words, i.e., we allow bag-of-words (BOW).
Partial Tree Kernel (K3) can be effectively applied to both constituency and dependency parse trees.
It generates all possible connected tree fragments, e.g., sibling nodes can be also separated and be part
of different tree fragments. In other words, a fragment is any possible tree path from whose nodes other
tree paths can depart. Thus, it can generate a very rich feature space.
Sequence Kernel (K4) is the traditional string kernel applied to the words of a sentence. In our case, we
apply it to the sequence of concepts.
3.2 Semantic/syntactic structures
As mentioned before, tree kernels allow us to compute structural similarities between two trees without
explicitly representing them as feature vectors. For the CSL task, we experimented with a number of tree
representations that incorporate different levels of syntactic and semantic information.
To capture the structural dependencies between the semantic tags, we use a basic tree (Figure 1a)
where the words of a sentence are tagged with their semantic tags. More specifically, the words in the
sentence constitute the leaves of the tree, which are in turn connected to the pre-terminals containing the
semantic tags in BIO notation (?B?=begin, ?I?=inside, ?O?=outside). The BIO tags are then generalized
in the upper level, and so on. The basic tree does not include any syntactic information.
196
However, part-of-speech (POS) and phrasal information could be informative for both segmentation
and labeling in semantic parsing. To incorporate this information, we use two extensions of the basic
tree: one that includes the POS tags of the words (Figure 1e), and another one that includes both POS
tags and syntactic chunks (Figure 1c). The POS tags are children of the semantic tags, whereas the
chunks (i.e., phrasal information) are included as parents of the semantic tags.
We also experiment with full syntactic trees (Figure 1d) to see the impact of deep syntactic informa-
tion. The semantic tags are attached to the pre-terminals (i.e., POS tags) in the syntactic tree. We use the
Stanford POS tagger and syntactic parser and the Twitter NLP tool
1
for the shallow trees.
A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example,
the first clause ?along my route tell me the next steak house? is elaborated by the second clause ?that is
within a mile?. The relations by which clauses in a text are linked are called coherence relations (e.g.,
Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide addi-
tional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse
structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates
discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson,
1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a
nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is.
3.3 New features
In order to compare to the structured representation, we also devoted significant effort towards engineer-
ing a set of features to be used in a flat feature-vector representation; they can be used in isolation or in
combination with the kernel-based approach (as a composite kernel using a linear combination):
CRF-based: these include the basic features used to train the initial semi-CRF model (cf. Section 2.1).
n-gram based: we collected 3- and 4-grams of the output label sequence at the level of concepts, with
artificial tags inserted to identify the start (?S?) and end (?E?) of the sequence.
2
Probability-based: two features computing the probability of the label sequence as an average of the
probabilities at the word level p(l
i
|w
i
) (i.e., assuming independence between words). The unigram prob-
abilities are estimated by frequency counts using maximum likelihood in two ways: (i) from the complete
100-best list of hypotheses; (ii) from the training set (according to the gold standard annotation).
DB-based: a single feature encoding the number of results returned from the database when constructing
a query using the conjunction of all semantic segments in the hypothesis. Three possible values are
considered by using a threshold t: 0 (if the query result is void), 1 (if the number of results is in [1, t]),
and 2 (if the number of results is greater than t). In our case, t is empirically set to 10,000.
4 Experiments
The experiments aim at investigating which structures, and thus which linguistic models and combination
with other models, are the most appropriate for our reranker. We first calculate the oracle accuracy in
order to compute an upper bound of the reranker. Then we present experiments with the feature vectors,
tree kernels, and representations of linguistic information introduced in the previous sections.
4.1 Experimental setup
In our experiments, we use questions annotated with semantic tags in the restaurant domain,
3
which were
collected by McGraw et al. (2012) through crowdsourcing on Amazon Mechanical Turk.
4
We split the
dataset into training, development and test sets. Table 1 shows statistics about the dataset and about the
size of the parts we used for training, development and testing (see the semi-CRF line).
We subsequently split the training data randomly into ten folds. We generated the N -best lists on
the training set in a cross-validation fashion, i.e., iteratively training on nine folds and annotating the
remaining fold. We computed the 100-best hypotheses for each example.
1
Available from http://nlp.stanford.edu/software/index.shtml and https://github.com/aritter/twitter nlp, respectively.
2
For instance, if the output sequence is Other-Rating-Other-Amenity the 3-gram patterns would be: S-Other-Rating, Other-
Rating-Other, Rating-Other-Amenity, and Other-Amenity-E.
3
http://www.sls.csail.mit.edu/downloads/restaurant
4
We could not use the datasets used by Dinarelli et al. (2011), because they use French and Italian corpora for which there
are no reliable syntactic and discourse parsers.
197
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 28,482 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
-score for N -best lists
of different lengths.
We used the development set to experiment and tune the hyper-parameters of the reranking model. The
results on the development set presented in Section 4.2 were obtained by semi-CRF and reranking models
learned on the training set. The results on the test set were obtained by models trained on the training
plus development sets. Similarly, the N -best lists for the development and test sets were generated using
a single semi-CRF model trained on the training set and the training+development sets, respectively.
Each generated hypothesis is represented using a semantic tree and a feature vector (explained in
Section 3) and two extra features accounting for (i) the semi-CRF probability of the hypothesis, and
(ii) the hypothesis reciprocal rank in the N -best list. SVM-Light-TK
5
is used to train the reranker with
a combination of tree kernels and feature vectors (Moschitti, 2006; Joachims, 1999). Although we
tried several parameters on the validation set, we observed that the default values yielded the highest
results. Thus, we used the default c (trade-off) and tree kernel parameters and a linear kernel for the
feature vectors. Table 1 shows the sizes of the train, the development and the test sets used for the
semi-CRF as well as the number of pairs generated for the reranker. As a baseline, we picked the best-
scored hypothesis in the list, according to the semi-CRF tagger. The evaluation measure used in all
the experiments is the harmonic mean of precision and recall, i.e., the F
1
-score (van Rijsbergen, 1979),
computed at the token level and micro-averaged over the different semantic types.
6
We used paired t-test
to measure the statistical significance of the improvements: we split the test set into 31 equally-sized
samples and performed t-tests based on the F
1
-scores of different models on the resulting samples.
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
-score for N -best lists of different lengths, i.e., which
can be achieved by picking the best candidate of the N -best list for various values of N . We can see that
going to 5-best increases the oracle F
1
-score by almost ten points absolute. Going down to 10-best only
adds 2.5 extra F
1
points absolute, and a 100-best list adds 3.5 F
1
points more to yield a respectable F
1
-
score of 98.72. This high result can be explained considering that the size of the complete hypothesis set
is smaller than 100 for most questions. Thus, we can conclude that theN -best lists do include many good
options and do offer quite a large space for potential improvement. We can further observe that going to
5-best lists offers a good balance between the length of the list and the possibility to improve F
1
-score:
generally, we do not want too long N -best lists since they slow down computation and also introduce
more opportunities to make the wrong choice for a reranker (since there are just more candidates to
choose from). In our experiments with larger N , we observed improvements only for 10 and only on the
development set; thus, we will focus on 5-best lists in our experiments below.
K0 K1 K2 K3 K4
Dev 84.21 82.92 83.07 85.07 83.78
Test 84.08 83.19 83.20 84.61 82.93
Table 3: Results for using different tree kernels on the basic tree (BT) representation.
Choosing the best tree kernel. We first select the most appropriate tree kernel to limit the number
of experiment variables. Table 3 shows the results of different tree kernels using the basic tree (BT)
representation (see Figure 1a). We can observe that for both the development set and the test set, kernel
K3 (see Section 3.1) yields the highest F
1
-score.
Impact of feature vectors. Table 4 presents the results for the feature vector experiments in terms
of F
1
-scores and relative error reductions (row RER). The first column shows the baseline, when no
reranking is used; the following four columns contain the results when using vectors including different
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
?Other? is not considered a semantic type, thus ?Other? tokens are not included in the F
1
calculation.
198
Baseline n-grams CRF features Count DB ProbBased AllFeat
Dev 83.86 83.79 83.96 83.80 83.86 83.87 84.49
RER -0.4 0.6 -0.4 0.0 0.0 3.9
Test 83.03 82.90 83.44 82.90 83.01 83.09 83.86
RER -0.7 2.4 -0.7 -0.1 0.3 4.8
Table 4: Feature vector experiments: F
1
score and relative error reduction (in %).
Combining AllFeat and
Baseline BT BTP ShT ST AllFeat +BT +ShT +ShT +BT
Dev 83.86 85.07 85.41 85.06 84.30 84.49 85.57 85.58 85.33
RER 7.5 9.6 7.4 2.8 3.9 10.6 10.7 9.1
Test 83.03 84.61 84.63 84.07 83.81 83.86 84.67 84.79 84.76
RER 9.3 9.4 6.1 4.5 4.8 9.6 10.2 10.2
p.v. 0.00049 0.0002 0.012 0.032 0.00018 0.00028 0.00004 0.000023
Table 5: Tree kernel experiments: F
1
-score, relative error reduction (in %), and p-values.
kinds of features: (i) n-gram features, (ii) all features used by the semi-CRF, (iii) count features, and
(iv) database (DB) features. In each case, we include two additional features: the semi-CRF score
(i.e., the probability) and the reciprocal rank of the hypothesis in the N -best list. Among (i)?(iv), only
the semi-CRF features seem to help; the rest either show no improvements or degrade the performance.
However, putting all these features together (AllFeat) yields sizable gains in terms of F
1
-score and a
relative error reduction of 4-5%; the improvement is statistically significant, and it is slightly larger on
the test dataset compared to the development dataset.
Impact of structural kernels and combinations. Table 5 shows the results when experimenting with
various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with
part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We
can see that the basic tree works rather well, yielding +1.6 F
1
-score on the test dataset, but adding POS
information can help a bit more, especially for the tuning dataset. Interestingly, the syntactic tree kernels,
ShT and ST, perform worse than BT and BTP, especially on the test dataset. The last three columns in the
table show the results when we combine the AllFeat feature vector (see Table 4) with BT and ShT. We can
see that combining AllFeat with ShT works better, on both development and test sets, than combining it
with BT or with both ShT and BT. Also note the big jump in performance from AllFeat to AllFeat+ShT.
Overall, we can conclude that shallow syntax has a lot to offer over AllFeat, and it is preferable over BT
in the combination with AllFeat. The improvements reported in Tables 5 and 6 are statistically significant
when compared to the semi-CRF baseline as shown by the p.v. (value) row. Moreover, the improvement
of AllFeat + ShT over BT is also statistically significant (p.v.<0.05).
Combining AllFeat and
Baseline DS +DS +DS +BT +DS +ShT
Dev 83.86 84.61 85.14 85.43 85.46
RER 4.7 7.9 9.7 9.9
Test 83.03 84.38 84.55 84.63 84.67
RER 7.9 8.9 9.4 9.6
p.v. 0.0005 0.0001 0.00066 0.00015
Table 6: Experiments with discourse kernels: F
1
score, relative error reduction (in %), and p-values.
Discourse structure. Finally, Table 6 shows the results for the discourse tree kernel (DS), which we
designed and experimented with for the first time in this paper. We see that DS yields sizable improve-
ments over the baseline. We also see that further gains can be achieved by combining DS with AllFeat,
and also with BT and ShT, the best combination being AllFeat+DS+ShT (see last column). However,
comparing to Table 5, we see that it is better to use just AllFeat+ShT and leave DS out. We would like
to note though that the discourse parser produced non-trivial trees for only 30% of the hypotheses (due
to the short, simple nature of the questions); in the remaining cases, it probably hurt rather than helped.
We conclude that discourse structure has clear potential, but how to make best use of it, especially in the
case of short simple questions, remains an open question that deserves further investigation.
199
Tag ID Other Rating Restaurant Amenity Cuisine Dish Hours Location Price
0 Other 8260 35 43 110 15 19 55 113 9
1 Rating 29 266 0 14 3 6 0 0 8
2 Restaurant 72 6 657 20 19 15 0 5 0
3 Amenity 117 9 10 841 27 27 7 12 7
4 Cuisine 36 2 12 26 543 44 3 1 0
5 Dish 23 0 4 20 33 324 1 4 0
6 Hours 61 0 1 2 6 1 426 9 1
7 Location 104 1 14 20 2 1 1 1457 0
8 Price 22 1 0 7 0 2 0 1 204
Table 7: Confusion matrix for the output of the best performing system.
4.3 Error analysis and discussion
Table 7 shows the confusion matrix for our best-performing model AllFeat+ShT (rows = gold standard
tags; columns = system predicted tags). Given the good results of the semantic parser, the numbers in the
diagonal are clearly dominating the weight of the matrix. The largest errors correspond to missed (first
column) and over-generated (first row) entity tokens. Among the proper confusions between semantic
types, Dish and Cuisine tend to mislead each other most. This is due to the fact that these two tags
are semantically similar, thus making them hard to distinguish. We can also notice that it is difficult to
identify Amenity correctly, and the model mistakenly tags many other tags as Amenity. We looked into
some examples to further investigate the errors. Our findings are as follow:
Inaccuracies and inconsistencies in human annotations. Since the annotations were done in Me-
chanical Turk, they have many inaccuracies and inconsistencies. For example, the word good with
exactly the same sense was tagged as both Other and Rating by the Turkers in the following examples:
Gold: [
Other
any good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Model: [
Other
any] [
Rating
good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Gold: [
Other
any place] [
Location
along the road] [
Other
has a] [
Rating
good] [
Dish
beer] [
Other
selection that also serves] ...
Requires lexical semantics and more coverage. In some cases our model fails to generalize well. For
instance, it fails to correctly tag establishments and tameles for the following examples. This suggests
that we need to consider other forms of semantic information, e.g., distributional and compositional
semantics computed from large corpora and/or using Web resources such as Wikipedia.
Gold: [
Other
any] [
Location
dancing establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Model: [
Other
any] [
Amenity
dancing] [
Other
establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Gold: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Dish
tameles] [
Amenity
special today]
Model: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Amenity
tameles] [
Other
special] [
Hours
today]
5 Conclusions
We have presented a study on the usage of syntactic and semantic structured information for improved
Concept Segmentation and Labeling (CSL). Our approach is based on reranking a set of N -best se-
quences generated by a state-of-the-art semi-CRF model for CSL. The syntactic and semantic informa-
tion was encoded in tree-based structures, which we used to train a reranker with kernel-based Support
Vector Machines. We empirically compared several variants of syntactic/semantic structured representa-
tions and kernels, including also a vector of manually engineered features.
The first and foremost conclusion from our study is that structural kernels yield significant improve-
ment over the strong baseline system, with a relative error reduction of ?10%. This more than doubles
the improvement when using the explicit feature vector. Second, we observed that shallow syntactic
information also improves results significantly over the best model. Unfortunately, the results obtained
using full syntax and discourse trees are not so clear. This is probably explained by the fact that user
queries are rather short and linguistically not very complex. We also observed that the upper bound per-
formance for the reranker still leaves large room for improvement. Thus, it remains to be seen whether
some alternative kernel representations can be devised to make better use of discourse and other syntac-
tic/semantic information. Also, we think that some innovative features based on analyzing the results
obtained from our database (or the Web) when querying with the segments represented in each hypothe-
ses have the potential to improve the results. All these concerns will be addressed in future work.
200
Acknowledgments
This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Re-
search Institute (QCRI) within the Qatar Foundation in collaboration with MIT. It is part of the Interactive
sYstems for Answer Search (Iyas) project.
References
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 263?270, Philadelphia, PA, USA.
Renato De Mori, Dilek Hakkani-T?ur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken
language understanding: a survey. IEEE Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2011. Discriminative reranking for spoken lan-
guage understanding. IEEE Transactions on Audio, Speech and Language Processing, 20(2):526?539.
Ruifang Ge and Raymond Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the
21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL?06, pages 263?270, Sydney, Australia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Thorsten Joachims. 1999. Advances in kernel methods. In Bernhard Sch?olkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Making Large-scale Support Vector Machine Learning Practical, pages 169?184,
Cambridge, MA, USA. MIT Press.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level dis-
course analysis. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.
Rohit Kate and Raymond Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL ?06, pages 913?920, Sydney, Australia.
Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proceedings
of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,
HLT-EMNLP ?05, pages 507?514, Vancouver, British Columbia, Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 189?
196, Ann Arbor, MI, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, ICML ?01, pages 282?289, Williamstown, MA, USA.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Llu??s M`arquez, Xavier Carreras, Kenneth Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An
Introduction to the Special Issue. Computational Linguistics, 34(2):145?159.
Ian McGraw, Scott Cyphers, Panupong Pasupat, Jingjing Liu, and Jim Glass. 2012. Automating crowd-supervised
learning for spoken language systems. In Proceedings of 13th Annual Conference of the International Speech
Communication Association, INTERSPEECH ?12, Portland, OR, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and Robert Ingria. 1994. Statistical language processing using
hidden understanding models. In Proceedings of the workshop on Human Language Technology, HLT ?94,
pages 278?282, Morristown, NJ, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint
inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June.
201
Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Machine Learning, ECML?06, pages 318?329, Berlin, Hei-
delberg. Springer-Verlag.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2012. Structural reranking models for named entity recognition.
Intelligenza Artificiale, 6(2):177?190.
Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of
direct translation models. In Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing, volume 1, pages 189?192, Seattle, WA, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee. 1991. Stochastic representation of conceptual structure in
the ATIS task. In Proceedings of the Workshop on Speech and Natural Language, HLT ?91, pages 121?124,
Pacific Grove, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language
understanding. In Proceedings of 8th Annual Conference of the International Speech Communication Associa-
tion, INTERSPEECH ?07, pages 1605?1608, Antwerp, Belgium.
Yigal Dan Rubinstein and Trevor Hastie. 1997. Discriminative vs informative learning. In Proceedings of the
Third International Conference on Knowledge Discovery and Data Mining, KDD ?97, pages 49?53, Newport
Beach, CA, USA.
Guzm?an Santaf?e, Jose Lozano, and Pedro Larra?naga. 2007. Discriminative vs. generative learning of Bayesian
network classifiers. Lecture Notes in Computer Science, 4724:453?464.
Sunita Sarawagi and William Cohen. 2004. Semi-Markov conditional random fields for information extraction.
In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS ?04, pages
1185?1192, Vancouver, British Columbia, Canada.
Stephanie Seneff. 1989. TINA: A probabilistic syntactic parser for speech understanding systems. In Proceedings
of the Workshop on Speech and Natural Language, HLT ?89, pages 168?178, Philadelphia, PA, USA.
Manfred Stede. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan and
Claypool Publishers.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworth.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Jakub Szymanski. 2009. Semi-supervised learning of semantic
classes for query understanding: from the web and for the web. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, CIKM ?09, pages 37?46, New York, NY, USA.
202
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 388?398,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Exploiting Conversation Structure in Unsupervised Topic Segmentation for
Emails
Shafiq Joty and Giuseppe Carenini and Gabriel Murray and Raymond T. Ng
{rjoty, carenini, gabrielm, rng}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
Abstract
This work concerns automatic topic segmen-
tation of email conversations. We present a
corpus of email threads manually annotated
with topics, and evaluate annotator reliabil-
ity. To our knowledge, this is the first such
email corpus. We show how the existing topic
segmentation models (i.e., Lexical Chain Seg-
menter (LCSeg) and Latent Dirichlet Alloca-
tion (LDA)) which are solely based on lex-
ical information, can be applied to emails.
By pointing out where these methods fail and
what any desired model should consider, we
propose two novel extensions of the models
that not only use lexical information but also
exploit finer level conversation structure in a
principled way. Empirical evaluation shows
that LCSeg is a better model than LDA for
segmenting an email thread into topical clus-
ters and incorporating conversation structure
into these models improves the performance
significantly.
1 Introduction
With the ever increasing popularity of emails and
web technologies, it is very common for people to
discuss issues, events, agendas or tasks by email.
Effective processing of the email contents can be
of great strategic value. In this paper, we study
the problem of topic segmentation for emails, i.e.,
grouping the sentences of an email thread into a
set of coherent topical clusters. Adapting the stan-
dard definition of topic (Galley et al, 2003) to con-
versations/emails, we consider a topic is something
about which the participant(s) discuss or argue or
express their opinions. For example, in the email
thread shown in Figure 1, according to the major-
ity of our annotators, participants discuss three top-
ics (e.g., ?telecon cancellation?, ?TAG document?,
and ?responding to I18N?). Multiple topics seem to
occur naturally in social interactions, whether syn-
chronous (e.g., chats, meetings) or asynchronous
(e.g., emails, blogs) conversations. In multi-party
chat (Elsner and Charniak, 2008) report an average
of 2.75 discussions active at a time. In our email cor-
pus, we found an average of 2.5 topics per thread.
Topic segmentation is often considered a pre-
requisite for other higher-level conversation analy-
sis and applications of the extracted structure are
broad, encompassing: summarization (Harabagiu
and Lacatusu, 2005), information extraction and or-
dering (Allan, 2002), information retrieval (Dias et
al., 2007), and intelligent user interfaces (Dredze et
al., 2008). While extensive research has been con-
ducted in topic segmentation for monologues (e.g.,
(Malioutov and Barzilay, 2006), (Choi et al, 2001))
and synchronous dialogs (e.g., (Galley et al, 2003),
(Hsueh et al, 2006)), none has studied the problem
of segmenting asynchronous multi-party conversa-
tions (e.g., email). Therefore, there is no reliable an-
notation scheme, no standard corpus, and no agreed-
upon metrics available. Also, it is our key hypothe-
sis that, because of its asynchronous nature, and the
use of quotation (Crystal, 2001), topics in an email
thread often do not change in a sequential way. As a
result, we do not expect models which have proved
successful in monologue or dialog to be as effective
when they are applied to email conversations.
Our contributions in this paper aim to remedy
388
these problems. First, we present an email corpus
annotated with topics and evaluate annotator agree-
ment. Second, we adopt a set of metrics to mea-
sure the local and global structural similarity be-
tween two annotations from the work on multi-party
chat disentanglement (Elsner and Charniak, 2008).
Third, we show how the two state-of-the-art topic
segmentation methods (i.e., LCSeg and LDA) which
are solely based on lexical information and make
strong assumptions on the resulting topic models,
can be effectively applied to emails, by having them
to consider, in a principled way, a finer level struc-
ture of the underlying conversations. Experimen-
tal results show that both LCSeg and LDA benefit
when they are extended to consider the conversa-
tional structure. When comparing the two methods,
we found that LCSeg is better than LDA and this
advantage is preserved when they are extended to
incorporate conversational structure.
2 Related Work
Three research areas are directly related to our study:
a) text segmentation models, b) probabilistic topic
models, and c) extracting and representing the con-
versation structure of emails.
Topic segmentation has been extensively studied
both for monologues and dialogs. (Malioutov and
Barzilay, 2006) uses the minimum cut model to seg-
ment spoken lectures (i.e., monologue). They form a
weighted undirected graph where the vertices repre-
sent sentences and the weighted links represent the
similarity between sentences. Then the segmenta-
tion problem can be solved as a graph partitioning
problem, where the assumption is that the sentences
in a segment should be similar, while sentences in
different segments should be dissimilar. They op-
timize the ?normalized cut? criterion to extract the
segments. In general, the minimization of the nor-
malized cut criterion is NP-complete. However, the
linearity constraint on text segmentation for mono-
logue allows them to find an exact solution in poly-
nomial time. In our extension of LCSeg, we use
a similar method to consolidate different segments;
however, in our case the linearity constraint is ab-
sent. Therefore, we approximate the optimal solu-
tion by spectral clustering (Shi and Malik, 2000).
Moving to the task of segmenting dialogs, (Galley
et al, 2003) first proposed the lexical chain based
unsupervised segmenter (LCSeg) and a supervised
segmenter for segmenting meeting transcripts. Their
supervised approach uses C4.5 and C4.5 rules binary
classifiers with lexical and conversational features
(e.g., cue phrase, overlap, speaker, silence, and lex-
ical cohesion function). Their supervised approach
performs significantly better than LCSeg. (Hsueh
et al, 2006) follow the same approaches as (Galley
et al, 2003) on both manual transcripts and ASR
output of meetings. They perform segmentation at
both coarse (topic) and fine (subtopic) levels. For
the topic level, they achieve similar results as (Gal-
ley et al, 2003), with the supervised approach out-
performing LCSeg. However, for the subtopic level,
LCSeg performs significantly better than the super-
vised one. In our work, we show how LCSeg per-
forms when applied to the temporal ordering of the
emails in a thread. We also propose its extension to
leverage the finer conversation structure of emails.
The probabilistic generative topic models, such
as LDA and its variants (e.g., (Blei et al, 2003),
(Steyvers and Griffiths, 2007)), have proven to be
successful for topic segmentation in both mono-
logue (e.g., (Chen et al, 2009)) and dialog (e.g.,
(Georgescul et al, 2008)). (Purver et al, 2006) uses
a variant of LDA for the tasks of segmenting meet-
ing transcripts and extracting the associated topic
labels. However, their approach for segmentation
does not perform better than LCSeg. In our work,
we show how the general LDA performs when ap-
plied to email conversations and describe how it can
be extended to exploit the conversation structure of
emails.
Several approaches have been proposed to cap-
ture an email conversation . Email programs (e.g.,
Gmail, Yahoomail) group emails into threads using
headers. However, our annotations show that top-
ics change at a finer level of granularity than emails.
(Carenini et al, 2007) present a method to capture an
email conversation at the finer level by analyzing the
embedded quotations in emails. A fragment quota-
tion graph (FQG) is generated, which is shown to be
beneficial for email summarization. In this paper, we
show that topic segmentation models can also bene-
fit significantly from this fine conversation structure
of email threads.
389
3 Corpus and Evaluation Metrics
There are no publicly available email corpora anno-
tated with topics. Therefore, the first step was to
develop our own corpus. We have annotated the
BC3 email corpus (Ulrich et al, 2008) with top-
ics1. The BC3 corpus, previously annotated with
sentence level speech acts, meta sentence, subjectiv-
ity, extractive and abstractive summaries, is one of a
growing number of corpora being used for email re-
search. The corpus contains 40 email threads from
the W3C corpus2. It has 3222 sentences and an av-
erage of 5 emails per thread.
3.1 Topic Annotation
Topic segmentation in general is a nontrivial and
subjective task (Hsueh et al, 2006). The conver-
sation phenomenon called ?Schism? makes it even
more challenging for conversations. In schism a
new conversation takes birth from an existing one,
not necessarily because of a topic shift but because
some participants refocus their attention onto each
other, and away from whoever held the floor in the
parent conversation and the annotators can disagree
on the birth of a new topic (Aoki et al, 2006). In the
example email thread shown in Figure 1, a schism
takes place when people discuss about ?responding
to I18N?. All the annotators do not agree on the fact
that the topic about ?responding to I18N? swerves
from the one about ?TAG document?. The annota-
tors can disagree on the number of topics (i.e., some
are specific and some are general), and on the topic
assignment of the sentences3. To properly design an
effective annotation manual and procedure we per-
formed a two-phase pilot study before carrying out
the actual annotation. For the pilot study we picked
five email threads randomly from the corpus. In the
first phase of the pilot study we selected five uni-
versity graduate students to do the annotation. We
then revised our instruction manual based on their
feedback and the source of disagreement found. In
1The BC3 corpus had already been annotated for email sum-
marization, speech act recognition and subjectivity detection.
This new annotation with topics will be also made publicly
available at http://www.cs.ubc.ca/labs/lci/bc3.html
2http://research.microsoft.com/en-
us/um/people/nickcr/w3c-summary.html
3The annotators also disagree on the topic labels, however
in this work we are not interested in finding the topic labels.
the second phase we tested with a university postdoc
doing the annotation.
For the actual annotation we selected three com-
puter science graduates who are also native speakers
of English. They annotated 39 threads of the BC3
corpus4. On an average they took seven hours to an-
notate the whole dataset.
BC3 contains three human written abstract sum-
maries for each email thread. With each email thread
the annotators were also given an associated human
written summary to give a brief overview of the cor-
responding conversation. The task of finding topics
was carried out in two phases. In the first phase, the
annotators read the conversation and the associated
summary and list the topics discussed. They spec-
ify the topics by a short description (e.g., ?meeting
agenda?, ?location and schedule?) which provides a
high-level overview of the topic. The target number
of topics and the topic labels were not given in ad-
vance and they were instructed to find as many top-
ics as needed to convey the overall content structure
of the conversation.
In the second phase the annotators identify the
most appropriate topic for each sentence. However,
if a sentence covers more than one topic, they were
asked to label it with all the relevant topics according
to their order of relevance. If they find any sentence
that does not fit into any topic, they are told to label
those as the predefined topic ?OFF-TOPIC?. Wher-
ever appropriate they were also asked to make use of
two other predefined topics: ?INTRO? and ?END?.
INTRO (e.g., ?hi?, ?hello?) signifies the section (usu-
ally at the beginning) of an email that people use to
begin their email. Likewise, END (e.g., ?Cheers?,
?Best?) signifies the section (usually at the end) that
people use to end their email. The annotators car-
ried out the task on paper. We created the hierar-
chical thread view (?reply to? relation) using ?TAB?s
(indentation) and each participant?s name is printed
in a different color as in Gmail.
Table 1 shows some basic statistics computed on
the three annotations of the 39 email threads5. On
4The annotators in the pilot and in the actual study were dif-
ferent so we could reuse the threads used in pilot study. How-
ever, one thread on which the pilot annotators agree fully, was
used as an example in the instruction manual. This gives 39
threads left for the actual study.
5We got 100% agreement on the two predefined topics ?IN-
390
average we have 26.3 sentences and 2.5 topics per
thread. A topic contains an average of 12.6 sen-
tences. The average number of topics active at a
time is 1.4. The average entropy is 0.94 and cor-
responds (as described in detail in the next section)
to the granularity of the annotation. These statistics
(number of topics and topic density) indicate that the
dataset is suitable for topic segmentation.
Mean Max Min
Number of sentences 26.3 55 13
Number of topics 2.5 7 1
Avg. topic length 12.6 35 3
Avg. topic density 1.4 3.1 1
Entropy 0.94 2.7 0
Table 1: Corpus statistics of human annotations
Metrics Mean Max Min
1-to-1 0.804 1 0.31
lock 0.831 1 0.43
m-to-1 0.949 1 0.61
Table 2: Annotator agreement in the scale of 0 to 1
3.2 Evaluation Metrics
In this section we describe the metrics used to com-
pare different human annotations and system?s out-
put. As different annotations (or system?s output)
can group sentences in different number of clusters,
metrics widely used in classification, such as the ?
statistic, are not applicable. Again, our problem of
topic segmentation for emails is not sequential in na-
ture. Therefore, the standard metrics widely used in
sequential topic segmentation for monologues and
dialogs, such as Pk and WindowDiff(WD), are
also not applicable. We adopt the more appropri-
ate metrics 1-to-1, lock and m-to-1, introduced re-
cently by (Elsner and Charniak, 2008). The 1-to-1
metric measures the global similarity between two
annotations. It pairs up the clusters from the two
annotations in a way that maximizes (globally) the
total overlap and then reports the percentage of over-
lap. lock measures the local agreement within a con-
TRO? and ?END?. In all our computation (i.e., statistics, agree-
ment, system?s input) we excluded the sentences marked as ei-
ther ?INTRO? or ?END?
text of k sentences. To compute the loc3 metric for
the m-th sentence in the two annotations, we con-
sider the previous 3 sentences: m-1, m-2 and m-3,
and mark them as either ?same? or ?different? de-
pending on their topic assignment. The loc3 score
between two annotations is the mean agreement on
these ?same? or ?different? judgments, averaged over
all sentences. We report the agreement found in 1-
to-1 and lock in Table 2. In both of the metrics we
get high agreement, though the local agreement (av-
erage of 83%) is little higher than the global agree-
ment (average of 80%).
If we consider the topic of a randomly picked sen-
tence as a random variable then its entropy measures
the level of detail in an annotation. If the topics are
evenly distributed then the uncertainty (i.e., entropy)
is higher. It also increases with the increase of the
number of topics. Therefore, it is a measure of how
specific an annotator is and in our dataset it varies
from 0 6 to 2.7. To measure how much the annota-
tors agree on the general structure we use the m-to-1
metric. It maps each of the source clusters to the
single target cluster with which it gets the highest
overlap, then computes the total percentage of over-
lap. This metric is asymmetrical and not a measure
to be optimized7, but it gives us some intuition about
specificity (Elsner and Charniak, 2008). If one an-
notator divides a cluster into two clusters then, the
m-to-1 metric from fine to coarse is 1. In our corpus
by mapping from fine to coarse we get an m-to-1
average of 0.949.
4 Topic Segmentation Models
Developing automatic tools for segmenting an email
thread is challenging. The example email thread in
Figure 1 demonstrates why. We use different col-
ors and fonts to represent sentences of different top-
ics8. One can notice that email conversations are
different from written monologues (e.g., newspaper)
and dialogs (e.g., meeting, chat) in various ways.
As a communication media Email is distributed (un-
like face to face meeting) and asynchronous (unlike
60 uncertainty happens when there is only one topic found
7hence we do not use it to compare our models.
82 of the 3 annotators agree on this segmentation. Green rep-
resents topic 1 (?telecon cancellation?), orange indicates topic 2
(?TAG document?) and magenta represents topic 3 (?responding
to I18N?)
391
chat), meaning that different people from different
locations can collaborate at different times. There-
fore, topics in an email thread may not change in
sequential way. In the example, we see that topic 1
(i.e., ?telecon cancellation?) is revisited after some
gaps.
The headers (i.e., subjects) do not convey much
information and are often misleading. In the exam-
ple thread, participants use the same subject (i.e.,
20030220 telecon) but they talk about ?responding
to I18N? and ?TAG document? instead of ?telecon
cancellation?. Writing style varies among partici-
pants, and many people tend to use informal, short
and ungrammatical sentences. These properties of
email limit the application of techniques that have
been successful in monologues and dialogues.
LDA and LCSeg are the two state-of-the-art mod-
els for topic segmentation of multi-party conversa-
tion (e.g., (Galley et al, 2003), (Hsueh et al, 2006),
(Georgescul et al, 2008)). In this section, at first we
describe how the existing models of topic segmen-
tation can be applied to emails. We then point out
where these methods fail and propose extensions of
these basic models for email conversations.
4.1 Latent Dirichlet Allocation (LDA)
Our first model is the probabilistic LDA model
(Steyvers and Griffiths, 2007). This model relies on
the fundamental idea that documents are mixtures of
topics, and a topic is a multinomial distribution over
words. The generative topic model specifies the fol-
lowing distribution over words within a document:
P (wi) =
T?
j=1
P (wi|zi = j)P (zi = j)
Where T is the number of topics. P (wi|zi = j) is
the probability of word wi under topic j and P (zi =
j) is the probability that jth topic was sampled for
the ith word token. We refer the multinomial dis-
tributions ?(j) = P (w|zi = j) and ?(d) = P (z)
as topic-word distribution and document-topic dis-
tribution respectively. (Blei et al, 2003) refined this
basic model by placing a Dirichlet (?) prior on ?.
(Griffiths and Steyvers, 2003) further refined it by
placing a Dirichlet (?) prior on ?. The inference
problem is to find ? and ? given a document set.
Variational EM has been applied to estimate these
two parameters directly. Instead of estimating ? and
?, one can also directly estimate the posterior distri-
bution over z = P (zi = j|wi) (topic assignments
for words). One efficient estimation technique uses
Gibbs sampling to estimate this distribution.
This framework can be directly applied to an
email thread by considering each email as a doc-
ument. Using LDA we get z = P (zi = j|wi)
(i.e., topic assignments for words). By assuming the
words in a sentence occur independently we can esti-
mate the topic assignments for sentences as follows:
P (zi = j|sk) =
?
wi?sk
P (zi = j|wi)
where, sk is the kth sentence for which we can
assign the topic by: j? = argmaxjP (zi = j|sk).
4.2 Lexical Chain Segmenter (LCSeg)
Our second model is the lexical chain based seg-
menter LCSeg, (Galley et al, 2003). LCSeg as-
sumes that topic shifts are likely to occur where
strong term repetitions start and end9. LCSeg at first
computes ?lexical chains? for each non-stop word
based on word repetitions. It then ranks the chains
according to two measures: ?number of words in the
chain? and ?compactness of the chain?. The more
compact (in terms of number of sentences) and the
more populated chains get higher scores.
The algorithm then works with two adjacent anal-
ysis windows, each of a fixed size k which is em-
pirically determined. For each sentence boundary,
LCSeg computes the cosine similarity (or lexical co-
hesion function) at the transition between the two
windows. Low similarity indicates low lexical cohe-
sion, and a sharp change signals a high probability
of an actual topic boundary. This method is similar
to TextTiling (Hearst, 1997) except that the similar-
ity is computed based on the scores of the ?lexical
chains? instead of ?term counts?. In order to apply
LCSeg on email threads we arrange the emails based
on their temporal relation (i.e., arrival time) and ap-
ply the LCSeg algorithm to get the topic boundaries.
9One can also consider other lexical semantic relations (e.g.,
synonym, hypernym, hyponym) in lexical chaining. However,
Galley et al, (Galley et al, 2003) uses only repetition relation
as previous research results (e.g., (Choi, 2000)) account only
for repetition.
392
From: Brian To: rdf core Subject: 20030220 telecon Date: Tue Feb 17 13:52:15 
 	
		
	

Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 904?915, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Novel Discriminative Framework for Sentence-Level Discourse Analysis
Shafiq Joty and Giuseppe Carenini and Raymond T. Ng
{rjoty, carenini, rng}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
Abstract
We propose a complete probabilistic discrim-
inative framework for performing sentence-
level discourse analysis. Our framework com-
prises a discourse segmenter, based on a bi-
nary classifier, and a discourse parser, which
applies an optimal CKY-like parsing algo-
rithm to probabilities inferred from a Dynamic
Conditional Random Field. We show on two
corpora that our approach outperforms the
state-of-the-art, often by a wide margin.
1 Introduction
Automatic discourse analysis has been shown to
be critical in several fundamental Natural Lan-
guage Processing (NLP) tasks including text gener-
ation (Prasad et al2005), summarization (Marcu,
2000b), sentence compression (Sporleder and Lap-
ata, 2005) and question answering (Verberne et al
2007). Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988), one of the most influential
theories of discourse, posits a tree representation of
a discourse, known as a Discourse Tree (DT), as
exemplified by the sample DT shown in Figure 1.
The leaves of a DT correspond to contiguous atomic
text spans, also called Elementary Discourse Units
(EDUs) (three in the example). The adjacent EDUs
are connected by a rhetorical relation (e.g., ELAB-
ORATION), and the resulting larger text spans are
recursively also subject to this relation linking. A
span linked by a rhetorical relation can be either
a NUCLEUS or a SATELLITE depending on how
central the message is to the author. Discourse anal-
ysis in RST involves two subtasks: (i) breaking the
text into EDUs (known as discourse segmentation)
and (ii) linking the EDUs into a labeled hierarchical
tree structure (known as discourse parsing).
Figure 1: Discourse structure of a sentence in RST-DT.
Previous studies on discourse analysis have been
quite successful in identifying what machine learn-
ing approaches and what features are more useful for
automatic discourse segmentation and parsing (Sori-
cut and Marcu, 2003; Subba and Eugenio, 2009; du-
Verle and Prendinger, 2009). However, all the pro-
posed solutions suffer from at least one of the fol-
lowing two key limitations: first, they make strong
independence assumptions on the structure and the
labels of the resulting DT, and typically model the
construction of the DT and the labeling of the rela-
tions separately; second, they apply a greedy, sub-
optimal algorithm to build the structure of the DT.
In this paper, we propose a new sentence-level
discourse parser that addresses both limitations. The
crucial component is a probabilistic discriminative
parsing model, expressed as a Dynamic Conditional
Random Field (DCRF) (Sutton et al2007). By
representing the structure and the relation of each
discourse tree constituent jointly and by explicitly
capturing the sequential and hierarchical dependen-
cies between constituents of a discourse tree, our
DCRF model does not make any independence as-
sumption among these properties. Furthermore, our
904
parsing model supports a bottom-up parsing algo-
rithm which is non-greedy and provably optimal.
The discourse parser assumes that the input text
has been already segmented into EDUs. As an addi-
tional contribution of this paper, we propose a novel
discriminative approach to discourse segmentation
that not only achieves state-of-the-art performance,
but also reduces the time and space complexities by
using fewer features. Notice that the combination
of our segmenter with our parser forms a complete
probabilistic discriminative framework for perform-
ing sentence-level discourse analysis.
Our framework was tested in a series of experi-
ments. The empirical evaluation indicates that our
approach to discourse parsing outperforms the state-
of-the-art by a wide margin. Moreover, we show this
to be the case on two very different genres: news ar-
ticles and instructional how-to-do manuals.
In the rest of the paper, after discussing related
work, we present our discourse parser. Then, we
describe our segmenter. The experiments and the
corpora we used are described next, followed by a
discussion of the key results and some error analysis.
2 Related work
Automatic discourse analysis has a long history;
see (Stede, 2011) for a detailed overview. Sori-
cut and Marcu (2003) present the publicly available
SPADE1 system that comes with probabilistic mod-
els for sentence-level discourse segmentation and
parsing based on lexical and syntactic features de-
rived from the lexicalized syntactic tree of a sen-
tence. Their parsing algorithm finds the most proba-
ble DT for a sentence, where the probabilities of the
constituents are estimated by their parsing model.
A constituent (e.g., ATTRIBUTION-NS[(1,2),3] in
Figure 1) in a DT has two components, first, the la-
bel denoting the relation and second, the structure
indicating which spans are being linked by the rela-
tion. The nuclearity statuses of the spans are built
into the relation labels (e.g., NS[(1,2),3] means that
span (1,2) is the NUCLEUS and it comes before
span 3 which is the SATELLITE). SPADE is limited
in several ways. It makes an independence assump-
tion between the label and the structure while mod-
eling a constituent, and it ignores the sequential and
1http://www.isi.edu/licensed-sw/spade/
hierarchical dependencies between the constituents
in the parsing model. Furthermore, SPADE relies
only on lexico-syntactic features, and it follows a
generative approach to estimate the model param-
eters for the segmentation and the parsing models.
SPADE was trained and tested on the RST-DT cor-
pus (Carlson et al2002), which contains human-
annotated discourse trees for news articles.
Subsequent research addresses the question of
how much syntax one really needs in discourse
analysis. Sporleder and Lapata (2005) focus on
discourse chunking, comprising the two subtasks
of segmentation and non-hierarchical nuclearity as-
signment. More specifically, they examine whether
features derived via part of speech (POS) and chunk
taggers would be sufficient for these purposes. Their
results on RST-DT turn out to be comparable to
SPADE without using any features from the syntac-
tic tree. Later, Fisher and Roark (2007) demonstrate
over 4% absolute ?performance gain? in segmenta-
tion, by combining the features extracted from the
syntactic tree with the ones derived via taggers. Us-
ing quite a large number of features in a binary log-
linear model they achieve the state-of-the-art seg-
mentation performance on the RST-DT test set.
On the different genre of instructional manuals,
Subba and Eugenio (2009) propose a shift-reduce
parser that relies on a classifier to find the appro-
priate relation between two text segments. Their
classifier is based on Inductive Logic Programming
(ILP), which learns first-order logic rules from a
large set of features including the linguistically rich
compositional semantics coming from a semantic
parser. They show that the compositional seman-
tics improves the classification performance. How-
ever, their discourse parser implements a greedy ap-
proach (hence not optimal) and their classifier disre-
gards the sequence and hierarchical dependencies.
Using RST-DT, Hernault et al2010) present
the HILDA system that comes with a segmenter
and a parser based on Support Vector Machines
(SVMs). The segmenter is a binary SVM classi-
fier which relies on the same lexico-syntactic fea-
tures used in SPADE, but with more context. The
discourse parser builds a DT iteratively utilizing two
SVM classifiers in each iteration: (i) a binary classi-
fier decides which of the two adjacent spans to link,
and (ii) a multi-class classifier then connects the se-
905
lected spans with the appropriate relation. They use
a very large set of features in their parser. How-
ever, taking a radically-greedy approach, they model
structure and relations separately, and ignore the se-
quence dependencies in their models.
Recently, there has been an explosion of interest
in Conditional Random Fields (CRFs) (Lafferty et
al., 2001) for solving structured output classification
problems, with many successful applications in NLP
including syntactic parsing (Finkel et al2008), syn-
tactic chunking (Sha and Pereira, 2003) and dis-
course chunking (Ghosh et al2011) in Penn Dis-
course Treebank (Prasad et al2008). CRFs being a
discriminative approach to sequence modeling (i.e.,
directly models the conditional p(y|x,?)), have sev-
eral advantages over its generative counterparts such
as Hidden Markov Models (HMMs) and Markov
Random Fields (MRFs), which first model the joint
p(y, x|?), then infer the conditional p(y|x,?)). Key
advantages include the ability to incorporate arbi-
trary overlapping local and global features, and the
ability to relax strong independence assumptions. It
has been advocated that CRFs are generally more
accurate since they do not ?waste effort? modeling
complex distributions (i.e., p(x)) that are not rele-
vant for the target task (Murphy, 2012).
3 The Discourse Parser
Assuming that a sentence is already segmented into
a sequence of EDUs e1, e2, . . . en manually or by an
automatic segmenter (see Section 4), the discourse
parsing problem is to decide which spans to con-
nect (i.e., structure of the DT) and which relations
(i.e., labels of the internal nodes) to use in the pro-
cess of building the hierarchical DT. To build the
DTs effectively, a common assumption is that they
are binary trees (Soricut and Marcu, 2003; duVerle
and Prendinger, 2009). That is, multi-nuclear re-
lations (e.g., LIST, JOINT, SEQUENCE) involving
more than two EDUs are mapped to a hierarchi-
cal right-branching binary tree. For example, a flat
LIST (e1, e2, e3, e4) is mapped to a right-branching
binary tree LIST (e1, LIST (e2, LIST (e3, e4))).
Our discourse parser has two components. The
first component, the parsing model, assigns a proba-
bility to every possible DT. The second component,
the parsing algorithm, finds the most probable DT
among the candidate discourse trees.
3.1 Parsing Model
A DT can be represented as a set of constituents
of the form R[i,m, j], which denotes a rhetorical
relation R that holds between the span containing
EDUs i through m, and the span containing EDUs
m+1 through j. For example, the DT in Figure 1
can be written as {ELABORATION-NS[1,1,2],
ATTRIBUTION-NS[1,2,3]}. Notice that a rela-
tion R also indicates the nuclearity assignments
of the spans being connected, which can be one
of NUCLEUS-SATELLITE (NS), SATELLITE-
NUCLEUS (SN) and NUCLEUS-NUCLEUS (NN).
Given the model parameters ? and a candi-
date DT T , for all the constituents c in T , our
parsing model estimates the conditional probabil-
ity P (c|C,?), which specifies the joint probabil-
ity of the relation R and the structure [i,m, j]
associated with the constituent c, given that c
has a set of sub-constituents C. For instance,
for the DT shown in Figure 1, our model
would estimate P (R?[1, 1, 2]|?), P (R?[2, 2, 3]|?),
P (R?[1, 2, 3]|R??[1, 1, 2],?) etc. for all R? and R??
ranging on the set of relations. In what follows we
describe our probabilistic parsing model to compute
all these conditional probabilities P (c|C,?). We
will demonstrate how our approach not only models
the structure and the relation jointly, but it also cap-
tures linear sequence dependencies and hierarchical
dependencies between constituents of a DT.
Our novel parsing model is the Dynamic Condi-
tional Random Field (DCRF) (Sutton et al2007)
shown in Figure 2. A DCRF is a generalization
of linear-chain CRFs to represent complex interac-
tion between labels, such as when performing mul-
tiple labeling tasks on the same sequence. The ob-
served nodes Wj in the figure are the text spans.
A text span can be either an EDU or a concatena-
tion of a sequence of EDUs. The structure nodes
Sj?{0, 1} in the figure represent whether text spans
Wj?1 and Wj should be connected or not. The re-
lation nodes Rj?{1 . . .M} denote the discourse re-
lation between spans Wj?1 and Wj , given that M is
the total number of relations in our relation set. No-
tice that we now model the structure and the relation
jointly and also take the sequential dependencies be-
tween adjacent constituents into consideration.
906
Figure 2: A Dynamic CRF as a discourse parsing model.
We can obtain the conditional probabilities of
the constituents (i.e., P (c|C,?)) of all candidate
DTs for a sentence by applying the DCRF pars-
ing model recursively at different levels, and by
computing the posterior marginals of the relation-
structure pairs. To illustrate, consider the example
sentence in Figure 1 where we have three EDUs
e1, e2 and e3. The DCRF model for the first
level is shown in Figure 3(a), where the (observed)
EDUs are the spans in the span sequence. Given
this model, we obtain the probabilities of the con-
stituents R[1, 1, 2] and R[2, 2, 3] by computing the
posterior marginals P (R2, S2=1|e1, e2, e3,?) and
P (R3, S3=1|e1, e2, e3,?), respectively. At the sec-
ond level (see Figure 3(b)), there are two possi-
ble span sequences (e1:2, e3) and (e1, e2:3). In the
first sequence, EDUs e1 and e2 are linked into
a larger span, and in the second one, EDUs e2
and e3 are connected into a larger span. We ap-
ply our DCRF model to the two possible span se-
quences and obtain the probabilities of the con-
stituents R[1, 2, 3] and R[1, 1, 3] by computing
the posterior marginals P (R3, S3=1|e1:2, e3,?) and
P (R2:3, S2:3=1|e1, e2:3,?), respectively.
Figure 3: DCRF model applied to the sequences at differ-
ent levels in the example in Fig. 1. (a) A sequence at the
first level (b) Two possible sequences at the second level.
To further clarify the process, let us as-
sume that the sentence contains four EDUs
e1, e2, e3 and e4. At the first level (Fig-
ure 4(a)), there is only one possible span se-
quence to which we apply our DCRF model.
We obtain the probabilities of the constituents
R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4] by computing the
posterior marginals P (R2, S2=1|e1, e2, e3, e4,?),
P (R3, S3=1|e1, e2, e3, e4,?) and P (R4, S4=1|e1,
e2, e3, e4,?), respectively. At the second level
(Figure 4(b)), there are three possible sequences
(e1:2, e3, e4), (e1, e2:3, e4) and (e1, e2, e3:4). When
the DCRF model is applied to the sequence
(e1:2, e3, e4), we obtain the probabilities of the
constituent R[1, 2, 3] by computing the posterior
marginal P (R3, S3=1|e1:2, e3, e4,?). Likewise, the
posterior marginals P (R2:3, S2:3=1|e1, e2:3, e4,?)
and P (R4, S4=1|e1, e2:3, e4,?) in the DCRF model
applied to the sequence (e1, e2:3, e4) represents
the probabilities of the constituents R[1, 1, 3]
and R[2, 3, 4], respectively. Similarly, we at-
tain the probabilities of the constituent R[2, 2, 4]
from the DCRF model applied to the sequence
(e1, e2, e3:4) by computing the posterior marginal
P (R3:4, S3:4=1|e1, e2, e3:4,?). At the third level
(Figure 4(c)), there are three possible sequences
(e1:3, e4), (e1, e2:4) and (e1:2, e3:4), to which we ap-
ply our model and acquire the probabilities of the
constituents R[1, 3, 4], R[1, 1, 4] and R[1, 2, 4] by
computing their respective posterior marginals.
Figure 4: DCRF model applied to the sequences at differ-
ent levels of a discourse tree. (a) A sequence at the first
level, (b) Three possible sequences at the second level,
(c) Two possible sequences at the third level.
Our DCRF model is designed using MALLET
(McCallum, 2002). In order to avoid overfitting we
regularize the DCRF model with l2 regularization
and learn the model parameters using the limited-
memory BFGS (L-BFGS) fitting algorithm. Since
exact inference can be intractable in DCRF models,
907
we perform approximate inference (to compute the
posterior marginals) using tree-based reparameteri-
zation (Wainwright et al2002).
3.1.1 Features Used in the Parsing Model
Crucial to parsing performance is the set of fea-
tures used, as summarized in Table 1. Note that
these features are defined on two consecutive spans
Wj?1 and Wj of a span sequence. Most of the fea-
tures have been explored in previous studies. How-
ever, we improve some of these as explained below.
Organizational features encode useful informa-
tion about the surface structure of a sentence as
shown by (duVerle and Prendinger, 2009). We mea-
sure the length of the spans in terms of the number of
EDUs and tokens in it. However, in order to better
adjust to the length variations, rather than comput-
ing their absolute numbers in a span, we choose to
measure their relative numbers with respect to their
total numbers in the sentence. For example, in a sen-
tence containing three EDUs, a span containing two
of these EDUs will have a relative EDU number of
0.67. We also measure the distances of the spans
from the beginning and to the end of the sentence in
terms of the number of EDUs.
8 organizational features
Relative number of EDUs in span 1 and span 2.
Relative number of tokens in span 1 and span 2.
Distances of span 1 in EDUs to the beginning and to the end.
Distances of span 2 in EDUs to the beginning and to the end.
8 N-gram features
Beginning and end lexical N-grams in span 1.
Beginning and end lexical N-grams in span 2.
Beginning and end POS N-grams in span 1.
Beginning and end POS N-grams in span 2.
5 dominance set features
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two text spans.
2 contextual features
Previous and next feature vectors.
2 substructure features
Root nodes of the left and right rhetorical subtrees.
Table 1: Features used in the DCRF parsing model.
Discourse connectives (e.g., because, but), when
present, signal rhetorical relations between two text
segments (Knott and Dale, 1994; Marcu, 2000a).
However, previous studies (e.g., Hernault et al
(2010), Biran and Rambow (2011)) suggest that an
empirically acquired lexical N-gram dictionary is
more effective than a fixed list of connectives, since
this approach is domain independent and capable
of capturing non-lexical cues such as punctuations.
To build the lexical N-gram dictionary empirically
from the training corpus we consider the first and
last N tokens (N?{1, 2}) of each span and rank
them according to their mutual information2 with
the two labels, Structure and Relation. Intuitively,
the most informative cues are not only the most fre-
quent, but also the ones that are indicative of the la-
bels in the training data (Blitzer, 2008). In addition
to the lexical N-grams we also encode POS tags of
the first and last N tokens (N?{1, 2}) as features.
Figure 5: A discourse segmented lexicalized syntactic
tree. Boxed nodes form the dominance set D.
Dominance set extracted from the Discourse Seg-
mented Lexicalized Syntactic Tree (DS-LST) (Sori-
cut and Marcu, 2003) has been shown to be a very
effective feature in SPADE. Figure 5 shows the DS-
LST for our running example (see Figure 1 and 3).
In a DS-LST, each EDU except the one with the root
node must have a head node NH that is attached to
an attachment node NA residing in a separate EDU.
A dominance set D (shown at the bottom of Figure 5
for our example) contains these attachment points of
the EDUs in a DS-LST. In addition to the syntactic
and lexical information of the head and attachment
nodes, each element in D also represents a domi-
nance relationship between the EDUs involved. The
EDU with NA dominates the EDU with NH . In or-
2In contrast, HILDA ranks the N-grams by frequencies.
908
der to extract dominance set features for two consec-
utive spans ei:j and ej+1:k, we first compute D from
the DS-LST of the sentence. We then extract the
element from D that holds across the EDUs j and
j + 1. In our running example, for the spans e1 and
e2 (Figure 3(a)), the relevant dominance set element
is (1, efforts/NP)>(2, to/S). We encode the syntac-
tic labels and lexical heads of NH and NA and the
dominance relationship (i.e., which of the two spans
is dominating) as features in our model.
We also incorporate more contextual information
by including the above features computed for the
neighboring span pairs in the current feature vector.
We incorporate hierarchical dependencies be-
tween constituents in a DT by means of the sub-
structure features. For the two adjacent spans ei:j
and ej+1:k, we extract the roots of the rhetorical
subtrees spanning over ei:j (left) and ej+1:k (right).
In our example (see Figure 1 and Figure 3 (b)),
the root of the rhetorical subtree spanning over e1:2
is ELABORATION-NS. However, this assumes the
presence of a labeled DT which is not the case when
we apply the parser to a new sentence. This problem
can be easily solved by looping twice through build-
ing the model and the parsing algorithm (described
below). We first build the model without considering
the substructure features. Then we find the optimal
DT employing our parsing algorithm. This interme-
diate DT will now provide labels for the substruc-
tures. Next we can build a new, more accurate model
by including the substructure features, and run again
the parsing algorithm to find the final optimal DT.
3.2 Parsing Algorithm
Our parsing model above assigns a conditional prob-
ability to every possible DT constituent for a sen-
tence, the job of the parsing algorithm is to find the
most probable DT. Formally, this can be written as,
DT ? = argmax DTP (DT |?)
Our discourse parser implements a probabilistic
CKY-like bottom-up algorithm for computing the
most likely parse of a sentence using dynamic pro-
gramming; see (Jurafsky and Martin, 2008) for a
description. Specifically, with n number of EDUs
in a sentence, we use the upper-triangular por-
tion of the n ? n Dynamic Programming Table
(DPT). The cell [i, j] in the DPT represents the
span containing EDUs i through j and stores the
probability of a constituent R[i,m, j], where m =
argmax i?k?jP (R[i, k, j]).
In contrast to HILDA which implements a greedy
algorithm, our approach finds a DT that is glob-
ally optimal. Our approach is also different from
SPADE?s implementation. SPADE first finds the
tree structure that is globally optimal, then it assigns
the most probable relations to the internal nodes.
More specifically, the cell [i, j] in SPADE?s DPT
stores the probability of a constituent R[i,m, j],
where m = argmax i?k?jP ([i, k, j]). Disregard-
ing the relation label R while building the DPT, this
approach may find a tree that is not globally optimal.
4 The Discourse Segmenter
Our discourse parser above assumes that the input
sentences have been already segmented into EDUs.
Since it has been shown that discourse segmentation
is a primary source of inaccuracy for discourse pars-
ing (Soricut and Marcu, 2003), we have developed
our own segmenter, that not only achieves state-of-
the-art performance as shown later, but also reduces
the time complexity by using fewer features.
Our segmenter implements a binary classifier to
decide for each word (except the last word) in a sen-
tence, whether to put an EDU boundary after that
word. We use a Logistic Regression (LR) (i.e., dis-
criminative) model with l2 regularization and learn
the model parameters using the L-BFGS algorithm,
which gives quadratic convergence rate. To avoid
overfitting, we use 5-fold cross validation to learn
the regularization strength parameter from the train-
ing data. We also use a simple bagging technique
(Breiman, 1996) to deal with the sparsity of bound-
ary tags. Note that, our first attempt at this task im-
plemented a linear-chain CRF model to capture the
sequence dependencies between the tags in a dis-
criminative way. However, the binary LR classifier,
using the same features, not only outperforms the
CRF model, but also reduces the space complexity.
4.1 Features Used in the Segmentation Model
Our set of features for discourse segmentation are
mostly inspired from previous studies but used in a
novel way as we describe below.
Our first subset of features which we call SPADE
features, includes the lexico-syntactic patterns ex-
909
tracted from the lexicalized syntactic tree for the
given sentence. These features replicates the fea-
tures used in SPADE, but used in a discriminative
way. To decide on an EDU boundary after a token
wk, we find the lowest constituent in the lexicalized
syntactic tree that spans over tokens wi . . . wj such
that i?k<j. The production that expands this con-
stituent in the tree and its different variations, form
the feature set. For example in Figure 5, the produc-
tion NP(efforts)?PRP$(its)NNS(efforts)?S(to) and
its different variations depending on whether they
include the lexical heads and how many non-
terminals (up to two) to consider before and after
the potential EDU boundary (?), are used to de-
termine the existence of a boundary after the word
efforts (see (Fisher and Roark, 2007) for details).
SPADE uses these features in a generative way,
meaning that, it inserts an EDU boundary if the rela-
tive frequency (i.e., Maximum Likelihood Estimate
(MLE)) of a potential boundary given the production
in the training corpus is greater than 0.5. If the pro-
duction has not been observed frequently enough, it
uses its other variations to perform further smooth-
ing. In contrast, we compute the MLE estimates for
a production and its other variations, and use those
as features with/without binarizing the values.
Shallow syntactic parse (or Chunk) and POS tags
have been shown to possess valuable cues for dis-
course segmentation (Fisher and Roark, 2007). For
example, it is less likely that an EDU boundary oc-
curs within a chunk. We, therefore, annotate the to-
kens of a sentence with chunk and POS tags by a
state-of-the-art tagger3 and encode these as features.
EDUs are normally multi-word strings. Thus, a
token near the beginning or end of a sentence is un-
likely to be the end of a segment. Therefore, for each
token we include its relative position in the sentence
and distances to the beginning and end as features.
It is unlikely that two consecutive tokens are
tagged with EDU boundaries. We incorporate con-
textual information for a token by including the
above features computed for its neighboring tokens.
We also experimented with different N-gram
(N?{1, 2, 3}) features extracted from the token se-
quence, POS sequence and chunk sequence. How-
ever, since such features did not improve the seg-
3http://cogcomp.cs.illinois.edu/page/software
mentation accuracy on the development set, they
were excluded from our final set of features.
5 Experiments
5.1 Corpora
To demonstrate the generality of our model, we ex-
periment with two different genres. First, we use the
standard RST-DT corpus (Carlson et al2002) that
contains discourse annotations for 385 Wall Street
Journal news articles from the Penn Treebank (Mar-
cus et al1994). Second, we use the Instructional
corpus developed by Subba and Eugenio (2009) that
contains discourse annotations for 176 instructional
how-to-do manuals on home-repair.
The RST-DT corpus is partitioned into a training
set of 347 documents (7673 sentences) and a test set
of 38 documents (991 sentences), and 53 documents
(1208 sentences) have been (doubly) annotated by
two human annotators, based on which we compute
the human agreement. We use the human-annotated
syntactic trees from Penn Treebank to train SPADE
in our experiments using RST-DT as done in (Sori-
cut and Marcu, 2003). We extracted a sentence-level
DT from a document-level DT by finding the subtree
that exactly spans over the sentence. By our count,
7321 sentences in the training set, 951 sentences
in the test set and 1114 sentences in the doubly-
annotated set have a well-formed DT in RST-DT.
The Instructional corpus contains 3430 sentences in
total, out of which 3032 have a well-formed DT.
This forms our sentence-level corpora for discourse
parsing. However, the existence of a well-formed
DT in not a necessity for discourse segmentation,
therefore, we do not exclude any sentence in our dis-
course segmentation experiments.
5.2 Experimental Setup
We perform our experiments on discourse pars-
ing in RST-DT with the 18 coarser relations (see
Figure 6) defined in (Carlson and Marcu, 2001)
and also used in SPADE and HILDA. By attach-
ing the nuclearity statuses (i.e., NS, SN, NN) to
these relations we get 39 distinct relations4. Our
experiments on the Instructional corpus consider
the same 26 primary relations (e.g., GOAL:ACT,
CAUSE:EFFECT, GENERAL-SPECIFIC) used in
4Not all relations take all the possible nuclearity statuses.
910
(Subba and Eugenio, 2009) and also treat the re-
versals of non-commutative relations as separate re-
lations. That is, PREPARATION-ACT and ACT-
PREPARATION are two different relations. Attach-
ing the nuclearity statuses to these relations gives 70
distinct relations in the Instructional corpus.
We use SPADE as our baseline model and apply
the same modifications to its default setting as de-
scribed in (Fisher and Roark, 2007), which delivers
improved performance. Specifically, in testing, we
replace the Charniak parser (Charniak, 2000) with a
more accurate reranking parser (Charniak and John-
son, 2005). We use the reranking parser in all our
models to generate the syntactic trees. This parser
was trained on the sections of the Penn Treebank not
included in the test set. For a fair comparison, we ap-
ply the same canonical lexical head projection rules
(Magerman, 1995; Collins, 2003) to lexicalize the
syntactic trees as done in SPADE and HILDA. Note
that, all the previous works described in Section 2,
report their models? performance on a particular test
set of a specific corpus. To compare our results with
the previous studies, we test our models on those
specific test sets. In addition, we show more general
performance based on 10-fold cross validation.
5.3 Parsing based on Manual Segmentation
First, we present the results of our discourse parser
based on manual segmentation. The parsing perfor-
mance is assessed using the unlabeled (i.e., span)
and labeled (i.e., nuclearity, relation) precision, re-
call and F-score as described in (Marcu, 2000b, page
143). For brevity, we report only the F-scores in Ta-
ble 2. Notice that, our parser (DCRF) consistently
outperforms SPADE (SP) on the RST-DT test set5.
Especially, on relation labeling, which is the hardest
among the three tasks, we get an absolute F-score
improvement of 9.5%, which represents a relative
error rate reduction of 29.3%. Our F-score of 77.1
in relation labeling is also close to the human agree-
ment (i.e., F-score of 83.0) on the doubly-annotated
data. Our results on the RST-DT test set are con-
sistent with the mean scores over 10-folds, when we
perform 10-fold cross validation on RST-DT.
The improvement is even larger on the Instruc-
tional corpus, where we compare our mean results
5The improvements are statistically significant (p < 0.01).
over 10-folds with the results reported in Subba and
Eugenio (S&E) (2009) on a test set6, giving ab-
solute F-score improvements of 4.8%, 15.5% and
10.6% in span, nuclearity and relations, respectively.
Our parser reduces the errors by 67.6%, 54.6% and
28.6% in span, nuclearity and relations, respectively.
RST-DT Instructional
Test set 10-fold Doubly S&E 10-fold
Scores SP DCRF DCRF Human ILP DCRF
Span 93.5 94.6 93.7 95.7 92.9 97.7
Nuc. 85.8 86.9 85.2 90.4 71.8 87.2
Rel. 67.6 77.1 75.4 83.0 63.0 73.6
Table 2: Parsing results using manual segmentation.
If we compare the performance of our model on
the two corpora, we see that our model is more accu-
rate in finding the right tree structure (see Span) on
the Instructional corpus. This may be due to the fact
that sentences in the Instructional domain are rela-
tively short and contain fewer EDUs than sentences
in the News domain, thus making it easier to find
the right tree structure. However, when we compare
the performance on the relation labeling task, we ob-
serve a decrease on the Instructional corpus. This
may be due to the small amount of data available for
training and the imbalanced distribution of a large
number of discourse relations in this corpus.
To analyze the features, Table 3 presents the pars-
ing results on the RST-DT test set using different
subsets of features. Every new subset of features
appears to improve the accuracy. More specifically,
when we add the organizational features with the
dominance set features (see S2), we get about 2%
absolute improvement in nuclearity and relations.
With N-gram features (S3), the gain is even higher;
6% in relations and 3.5% in nuclearity, demonstrat-
ing the utility of the N-gram features. This is con-
sistent with the findings of (duVerle and Prendinger,
2009; Schilder, 2002). Including the Contextual fea-
tures (S4), we get further 3% and 2.2% improve-
ments in nuclearity and relations, respectively. No-
tice that, adding the substructure features (S5) does
not help much in sentence-level parsing, giving only
6Subba and Eugenio (2009) report their results based on an
arbitrary split between a training set and a test set. We asked the
authors for their particular split. However, since we could not
obtain that information, we compare our model?s performance
based on 10-fold cross validation with their reported results.
911
an improvement of 0.8% in relations. Therefore, one
may choose to avoid using this computationally ex-
pensive feature in time-constrained scenarios. How-
ever, in the future, it will be interesting to see its im-
portance in document-level parsing with large trees.
Scores S1 S2 S3 S4 S5
Span 91.3 92.1 93.3 94.6 94.6
Nuclearity 78.2 80.3 83.8 86.8 86.9
Relation 66.2 68.1 74.1 76.3 77.1
Table 3: Parsing results based on manual segmentation
using different subsets of features on RST-DT test set.
Feature subsets S1 = {Dominance set}, S2 = {Dominance
set, Organizational}, S3 = {Dominance set, Organiza-
tional, N-gram}, S4 = {Dominance set, Organizational,
N-gram, Contextual}, S5 (all) = {Dominance set, Orga-
nizational, N-gram, Contextual, Substructure}.
5.4 Evaluation of the Discourse Segmenter
We evaluate the segmentation accuracy with respect
to the intra-sentential segment boundaries following
(Fisher and Roark, 2007). Specifically, if a sen-
tence contains n EDUs, which corresponds to n? 1
intra-sentence segment boundaries, we measure the
model?s ability to correctly identify these n ? 1
boundaries. Human agreement for this task is quite
high (F-score of 98.3) on RST-DT.
Table 4 shows the results of different models in
(P)recision, (R)ecall, and (F)-score on the two cor-
pora. We compare our model?s (LR) results with
HILDA (HIL), SPADE (SP) and the results reported
in Fisher and Roark (F&R) (2007) on the RST-DT
test set. HILDA gives the weakest performance7.
Our results are also much better than SPADE8, with
an absolute F-score improvement of 4.9%, and com-
parable to the results of F&R, even though we use
fewer features. Furthermore, we perform 10-fold
cross validation on both corpora and compare with
SPADE. However, SPADE does not come with a
training module for its segmenter. We reimple-
mented this module and verified it on the RST-DT
test set. Due to the lack of human-annotated syntac-
tic trees in the Instructional corpus, we train SPADE
in this corpus using the syntactic trees produced
7Note that, the high segmentation accuracy reported in (Her-
nault et al2010) is due to a less stringent evaluation metric.
8The improvements are statistically significant (p<2.4e-06)
by the reranking parser. Our model delivers abso-
lute F-score improvements of 3.8% and 8.1% on the
RST-DT and the Instructional corpora, respectively,
which is statistically significant in both cases (p <
3.0e-06). However, when we compare our results on
the two corpora, we observe a substantial decrease in
performance on the Instructional corpus. This could
be due to a smaller amount of data in this corpus and
the inaccuracies in the syntactic parser and taggers,
which are trained on news articles.
RST-DT Instructional
Test Set 10-fold 10-fold 10-fold
HIL SP F&R LR SP LR SP LR
P 77.9 83.8 91.3 88.0 83.7 87.5 65.1 73.9
R 70.6 86.8 89.7 92.3 86.2 89.9 82.8 89.7
F 74.1 85.2 90.5 90.1 84.9 88.7 72.8 80.9
Table 4: Segmentation results of different models.
5.5 Parsing based on Automatic Segmentation
In order to evaluate our full system, we feed our
discourse parser the output of our discourse seg-
menter. Table 5 shows the F-score results. We com-
pare our results with SPADE on the RST-DT test set.
We achieve absolute F-score improvements of 3.6%,
3.4% and 7.4% in span, nuclearity and relation, re-
spectively. These improvements are statistically sig-
nificant (p<0.001). Our system, therefore, reduces
the errors by 15.5%, 11.4%, and 17.6% in span, nu-
clearity and relations, respectively. These results are
also consistent with the mean results over 10-folds.
RST-DT Instructional
Test set 10-fold 10-fold
Scores SPADE DCRF DCRF DCRF
Span 76.7 80.3 78.7 71.9
Nuclearity 70.2 73.6 72.2 64.3
Relation 58.0 65.4 64.2 54.8
Table 5: Parsing results using automatic segmentation.
For the Instructional corpus, the last column of
Table 5 shows the mean 10-fold cross validation re-
sults. We cannot compare with S&E because no re-
sults were reported using an automatic segmenter.
However, it is interesting to observe how much our
full system is affected by an automatic segmenter
on both RST-DT and the Instructional corpus (see
Table 2 and Table 5). Nevertheless, taking into ac-
count the segmentation results in Table 4, this is
912
not surprising because previous studies (Soricut and
Marcu, 2003) have already shown that automatic
segmentation is the primary impediment to high ac-
curacy discourse parsing. This demonstrates the
need for a more accurate segmentation model in the
Instructional genre. A promising future direction
would be to apply effective domain adaptation meth-
ods (e.g., easyadapt (Daume, 2007)) to improve
the segmentation performance in the Instructional
domain by leveraging the rich data in RST-DT.
5.6 Error Analysis and Discussion
The results in Table 2 suggest that given a manually
segmented discourse, our sentence-level discourse
parser finds the unlabeled (i.e., span) discourse tree
and assigns the nuclearity statuses to the spans at a
performance level close to human annotators. We,
therefore, look more closely into the performance of
our parser on the hardest task of relation labeling.
Figure 6 shows the confusion matrix for the rela-
tion labeling task using manual segmentation on the
RST-DT test set. The relation labels are ordered ac-
cording to their frequency in the RST-DT training
set and represented by their initial letters. For exam-
ple, EL represents ELABORATION and CA repre-
sents CAUSE. In general, errors can be explained by
two different phenomena acting together: (i) the fre-
quency of the relations in the training data, and (ii)
the semantic (or pragmatic) similarity between the
relations. The most frequent relations (e.g., ELAB-
ORATION) tend to confuse the less frequent ones
(e.g., SUMMARY), and the relations which are se-
mantically similar (e.g., CAUSE, EXPLANATION)
confuse each other, making it hard to distinguish for
the computational models. Notice that, the confu-
sions caused by JOINT appears to be high consid-
ering its frequency. The confusion between JOINT
and TEMPORAL may be due to the fact that both of
these coarser relations9 contain finer relations (i.e.,
list in JOINT and sequence in TEMPORAL), which
are semantically similar, as pointed out by Carlson
and Marcu (2001). The confusion between JOINT
and BACKGROUND may be explained by their dif-
ferent (semantic vs. pragmatic) interpretation in the
RST theory (Stede, 2011, page 85).
9JOINT is actually not a relation, but is characterized by
juxtaposition of two EDUs without a relation.
Figure 6: Confusion matrix for the relation labels on
the RST-DT test set. Y-axis represents true and X-axis
represents predicted labels. The relation labels are TOPIC-
COMMENT, EVALUATION, SUMMARY, MANNER-MEANS,
COMPARISON, EXPLANATION, CONDITION, TEMPORAL,
CAUSE, ENABLEMENT, BACKGROUND, CONTRAST, JOINT,
SAME-UNIT, ATTRIBUTION, ELABORATION.
Based on these observations we will pursue two
ways to improve our discourse parser. We need a
more robust (e.g., bagging) method to deal with the
imbalanced distribution of relations, along with a
better representation of semantic knowledge. For
example, compositional semantics (Subba and Eu-
genio, 2009) and subjectivity (Somasundaran, 2010)
can be quite relevant for identifying relations.
6 Conclusion
In this paper, we have described a complete prob-
abilistic discriminative framework for performing
sentence-level discourse analysis. Experiments indi-
cate that our approach outperforms the state-of-the-
art on two corpora, often by a wide margin.
In ongoing work, we plan to generalize our
DCRF-based parser to multi-sentential text and also
verify to what extent parsing and segmentation can
be jointly performed. A longer term goal is to extend
our framework to also work with graph structures
of discourse, as recommended by several recent dis-
course theories (Wolf and Gibson, 2005). Once we
achieve similar performance on graph structures, we
will perform extrinsic evaluation to determine their
relative utility for various NLP tasks.
Acknowledgments
We are grateful to G. Murray, J. CK Cheung, the 3
reviewers and the NSERC CGS-D award.
913
References
Or Biran and Owen Rambow. 2011. Identifying Justifi-
cations in Written Dialogs by Classifying Text as Ar-
gumentative. Int. J. Semantic Computing, 5(4):363?
381.
J. Blitzer, 2008. Domain Adaptation of Natural Lan-
guage Processing Systems. PhD thesis, University of
Pennsylvania.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140, August.
L. Carlson and D. Marcu. 2001. Discourse Tagging Ref-
erence Manual. Technical Report ISI-TR-545, Univer-
sity of Southern California Information Sciences Insti-
tute.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 173?
180, NJ, USA. ACL.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 1st North American
Chapter of the Association for Computational Linguis-
tics Conference, pages 132?139, Seattle, Washington.
ACL.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics, 29(4):589?637, December.
H. Daume. 2007. Frustratingly Easy Domain Adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
256?263, Prague, Czech Republic. ACL.
D. duVerle and H. Prendinger. 2009. A Novel Discourse
Parser based on Support Vector Machine Classifica-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 665?673, Suntec, Singapore.
ACL.
J. Finkel, A. Kleeman, and C. Manning. 2008. Efficient,
Feature-based, Conditional Random Field Parsing. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 959?967,
Columbus, Ohio, USA. ACL.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmenta-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
488?495, Prague, Czech Republic. ACL.
S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli.
2011. Shallow Discourse Parsing with Conditional
Random Fields. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1071?1079, Chiang Mai, Thailand. AFNLP.
H. Hernault, H. Prendinger, D. duVerle, and M. Ishizuka.
2010. HILDA: A Discourse Parser Using Support
Vector Machine Classification. Dialogue and Dis-
course, 1(3):1?33.
D. Jurafsky and J. Martin, 2008. Speech and Language
Processing, chapter 14. Prentice Hall.
A. Knott and R. Dale. 1994. Using Linguistic Phenom-
ena to Motivate a Set of Coherence Relations. Dis-
course Processes, 18(1):35?62.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the Eighteenth International Conference on Ma-
chine Learning, pages 282?289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
D. Magerman. 1995. Statistical Decision-tree Mod-
els for Parsing. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 276?283, Cambridge, Massachusetts. ACL.
W. Mann and S. Thompson. 1988. Rhetorical Structure
Theory: Toward a Functional Theory of Text Organi-
zation. Text, 8(3):243?281.
D. Marcu. 2000a. The Rhetorical Parsing of Unrestricted
Texts: A Surface-based Approach. Computational
Linguistics, 26:395?448.
D. Marcu. 2000b. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press, Cambridge,
MA, USA.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313?330.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
K. Murphy. 2012. Machine Learning A Probabilistic
Perspective (Forthcoming, August 2012). MIT Press,
Cambridge, MA, USA.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and
B. Webber. 2005. The Penn Discourse TreeBank as a
Resource for Natural Language Generation. In Pro-
ceedings of the Corpus Linguistics Workshop on Us-
ing Corpora for Natural Language Generation, pages
25?32, Birmingham, U.K.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The Penn Discourse
TreeBank 2.0. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 2961?2968, Marrakech, Mo-
rocco. ELRA.
914
F. Schilder. 2002. Robust Discourse Parsing via Dis-
course Markers, Topicality and Position. Natural Lan-
guage Engineering, 8(3):235?255, June.
F. Sha and F. Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, pages 134?141, Ed-
monton, Canada. ACL.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing Using Syntactic and Lexical Informa-
tion. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 149?156, Edmonton, Canada.
ACL.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 257?264, Vancouver, British
Columbia, Canada. ACL.
M. Stede. 2011. Discourse Processing. Synthesis Lec-
tures on Human Language Technologies. Morgan And
Claypool Publishers, November.
R. Subba and B. Di Eugenio. 2009. An Effective
Discourse Parser that Uses Rich Linguistic Informa-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 566?574, Boulder, Colorado. ACL.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting Se-
quence Data. Journal of Machine Learning Research
(JMLR), 8:693?723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings of
the 30th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 735?736, Amsterdam, The Netherlands. ACM.
M. Wainwright, T. Jaakkola, and A. Willsky. 2002. Tree-
based Reparameterization for Approximate Inference
on Loopy Graphs. In Advances in Neural Information
Processing Systems 14, pages 1001?1008. MIT Press.
F. Wolf and E. Gibson. 2005. Representing Discourse
Coherence: A Corpus-Based Study. Computational
Linguistics, 31:249?288, June.
915
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214?220,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning to Differentiate Better from Worse Translations
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez
Alessandro Moschitti Preslav Nakov Massimo Nicosia
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa
Abstract
We present a pairwise learning-to-rank
approach to machine translation evalua-
tion that learns to differentiate better from
worse translations in the context of a given
reference. We integrate several layers
of linguistic information encapsulated in
tree-based structures, making use of both
the reference and the system output simul-
taneously, thus bringing our ranking closer
to how humans evaluate translations. Most
importantly, instead of deciding upfront
which types of features are important, we
use the learning framework of preference
re-ranking kernels to learn the features au-
tomatically. The evaluation results show
that learning in the proposed framework
yields better correlation with humans than
computing the direct similarity over the
same type of structures. Also, we show
our structural kernel learning (SKL) can
be a general framework for MT evaluation,
in which syntactic and semantic informa-
tion can be naturally incorporated.
1 Introduction
We have seen in recent years fast improvement
in the overall quality of machine translation (MT)
systems. This was only possible because of the
use of automatic metrics for MT evaluation, such
as BLEU (Papineni et al., 2002), which is the de-
facto standard; and more recently: TER (Snover et
al., 2006) and METEOR (Lavie and Denkowski,
2009), among other emerging MT evaluation met-
rics. These automatic metrics provide fast and in-
expensive means to compare the output of differ-
ent MT systems, without the need to ask for hu-
man judgments each time the MT system has been
changed.
As a result, this has enabled rapid develop-
ment in the field of statistical machine translation
(SMT), by allowing to train and tune systems as
well as to track progress in a way that highly cor-
relates with human judgments.
Today, MT evaluation is an active field of re-
search, and modern metrics perform analysis at
various levels, e.g., lexical (Papineni et al., 2002;
Snover et al., 2006), including synonymy and
paraphrasing (Lavie and Denkowski, 2009); syn-
tactic (Gim?enez and M`arquez, 2007; Popovi?c
and Ney, 2007; Liu and Gildea, 2005); semantic
(Gim?enez and M`arquez, 2007; Lo et al., 2012);
and discourse (Comelles et al., 2010; Wong and
Kit, 2012; Guzm?an et al., 2014; Joty et al., 2014).
Automatic MT evaluation metrics compare the
output of a system to one or more human ref-
erences in order to produce a similarity score.
The quality of such a metric is typically judged
in terms of correlation of the scores it produces
with scores given by human judges. As a result,
some evaluation metrics have been trained to re-
produce the scores assigned by humans as closely
as possible (Albrecht and Hwa, 2008). Unfortu-
nately, humans have a hard time assigning an ab-
solute score to a translation. Hence, direct hu-
man evaluation scores such as adequacy and flu-
ency, which were widely used in the past, are
now discontinued in favor of ranking-based eval-
uations, where judges are asked to rank the out-
put of 2 to 5 systems instead. It has been shown
that using such ranking-based assessments yields
much higher inter-annotator agreement (Callison-
Burch et al., 2007).
While evaluation metrics still produce numeri-
cal scores, in part because MT evaluation shared
tasks at NIST and WMT ask for it, there has also
been work on a ranking formulation of the MT
evaluation task for a given set of outputs. This
was shown to yield higher correlation with human
judgments (Duh, 2008; Song and Cohn, 2011).
214
Learning automatic metrics in a pairwise set-
ting, i.e., learning to distinguish between two al-
ternative translations and to decide which of the
two is better (which is arguably one of the easiest
ways to produce a ranking), emulates closely how
human judges perform evaluation assessments in
reality. Instead of learning a similarity function
between a translation and the reference, they learn
how to differentiate a better from a worse trans-
lation given a corresponding reference. While the
pairwise setting does not provide an absolute qual-
ity scoring metric, it is useful for most evaluation
and MT development scenarios.
In this paper, we propose a pairwise learning
setting similar to that of Duh (2008), but we extend
it to a new level, both in terms of feature represen-
tation and learning framework. First, we integrate
several layers of linguistic information encapsu-
lated in tree-based structures; Duh (2008) only
used lexical and POS matches as features. Second,
we use information about both the reference and
two alternative translations simultaneously, thus
bringing our ranking closer to how humans rank
translations. Finally, instead of deciding upfront
which types of features between hypotheses and
references are important, we use a our structural
kernel learning (SKL) framework to generate and
select them automatically.
The structural kernel learning (SKL) framework
we propose consists in: (i) designing a struc-
tural representation, e.g., using syntactic and dis-
course trees of translation hypotheses and a refer-
ences; and (ii) applying structural kernels (Mos-
chitti, 2006; Moschitti, 2008), to such representa-
tions in order to automatically inject structural fea-
tures in the preference re-ranking algorithm. We
use this method with translation-reference pairs
to directly learn the features themselves, instead
of learning the importance of a predetermined set
of features. A similar learning framework has
been proven to be effective for question answer-
ing (Moschitti et al., 2007), and textual entailment
recognition (Zanzotto and Moschitti, 2006).
Our goals are twofold: (i) in the short term, to
demonstrate that structural kernel learning is suit-
able for this task, and can effectively learn to rank
hypotheses at the segment-level; and (ii) in the
long term, to show that this approach provides a
unified framework that allows to integrate several
layers of linguistic analysis and information and to
improve over the state-of-the-art.
Below we report the results of some initial ex-
periments using syntactic and discourse structures.
We show that learning in the proposed framework
yields better correlation with humans than apply-
ing the traditional translation?reference similarity
metrics using the same type of structures. We
also show that the contributions of syntax and dis-
course information are cumulative. Finally, de-
spite the limited information we use, we achieve
correlation at the segment level that outperforms
BLEU and other metrics at WMT12, e.g., our met-
ric would have been ranked higher in terms of cor-
relation with human judgments compared to TER,
NIST, and BLEU in the WMT12 Metrics shared
task (Callison-Burch et al., 2012).
2 Kernel-based Learning from Linguistic
Structures
In our pairwise setting, each sentence s in
the source language is represented by a tuple
?t
1
, t
2
, r?, where t
1
and t
2
are two alternative
translations and r is a reference translation. Our
goal is to develop a classifier of such tuples that
decides whether t
1
is a better translation than t
2
given the reference r.
Engineering features for deciding whether t
1
is
a better translation than t
2
is a difficult task. Thus,
we rely on the automatic feature extraction en-
abled by the SKL framework, and our task is re-
duced to choosing: (i) a meaningful structural rep-
resentation for ?t
1
, t
2
, r?, and (ii) a feature func-
tion ?
mt
that maps such structures to substruc-
tures, i.e., our feature space. Since the design
of ?
mt
is complex, we use tree kernels applied
to two simpler structural mappings ?
M
(t
1
, r) and
?
M
(t
2
, r). The latter generate the tree representa-
tions for the translation-reference pairs (t
1
, r) and
(t
2
, r). The next section shows such mappings.
2.1 Representations
To represent a translation-reference pair (t, r), we
adopt shallow syntactic trees combined with RST-
style discourse trees. Shallow trees have been
successfully used for question answering (Severyn
and Moschitti, 2012) and semantic textual sim-
ilarity (Severyn et al., 2013b); while discourse
information has proved useful in MT evaluation
(Guzm?an et al., 2014; Joty et al., 2014). Com-
bined shallow syntax and discourse trees worked
well for concept segmentation and labeling (Saleh
et al., 2014a).
215
DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
VP NP-REL NP VP-REL o-REL o-REL
RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
not to give them the time to think . "
VP NP-REL NP VP-REL o-REL o-REL
TO-REL `` VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
to " give them no time to think . "
a) Hypothesis
b) Reference DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
Bag-of-words relations 
rela
tion
 pro
pag
atio
n di
rect
ion
Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.
Figure 1 shows two example trees combining
discourse, shallow syntax and POS: one for a
translation hypothesis (top) and the other one for
the reference (bottom). To build such structures,
we used the Stanford POS tagger (Toutanova et
al., 2003), the Illinois chunker (Punyakanok and
Roth, 2001), and the discourse parser
1
of (Joty et
al., 2012; Joty et al., 2013).
The lexical items constitute the leaves of the
tree. The words are connected to their respec-
tive POS tags, which are in turn grouped into
chunks. Then, the chunks are grouped into el-
ementary discourse units (EDU), to which the
nuclearity status is attached (i.e., NUCLEUS or
SATELLITE). Finally, EDUs and higher-order dis-
course units are connected by discourse relations
(e.g., DIS:ELABORATION).
2.2 Kernels-based modeling
In the SKL framework, the learning objects are
pairs of translations ?t
1
, t
2
?. Our objective is to
automatically learn which pair features are impor-
tant, independently of the source sentence. We
achieve this by using kernel machines (KMs) over
two learning objects ?t
1
, t
2
?, ?t
?
1
, t
?
2
?, along with
an explicit and structural representation of the
pairs (see Fig. 1).
1
The discourse parser can be downloaded from
http://alt.qcri.org/tools/
More specifically, KMs carry out learning using
the scalar product
K
mt
(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) = ?
mt
(t
1
, t
2
) ??
mt
(t
?
1
, t
?
2
),
where ?
mt
maps pairs into the feature space.
Considering that our task is to decide whether
t
1
is better than t
2
, we can conveniently rep-
resent the vector for the pair in terms of the
difference between the two translation vectors,
i.e., ?
mt
(t
1
, t
2
) = ?
K
(t
1
) ? ?
K
(t
2
). We can
approximate K
mt
with a preference kernel PK to
compute this difference in the kernel space K:
PK(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) (1)
= K(t
1
)? ?
K
(t
2
)) ? (?
K
(t
?
1
)? ?
K
(t
?
2
))
= K(t
1
, t
?
1
) +K(t
2
, t
?
2
)?K(t
1
, t
?
2
)?K(t
2
, t
?
1
)
The advantage of this is that now K(t
i
, t
?
j
) =
?
K
(t
i
) ? ?
K
(t
?
j
) is defined between two transla-
tions only, and not between two pairs of transla-
tions. This simplification enables us to map trans-
lations into simple trees, e.g., those in Figure 1,
and then to apply them tree kernels, e.g., the Par-
tial Tree Kernel (Moschitti, 2006), which carry out
a scalar product in the subtree space.
We can further enrich the representation ?
K
, if
we consider all the information available to the
human judges when they are ranking translations.
That is, the two alternative translations along with
their corresponding reference.
216
In particular, let r and r
?
be the references for
the pairs ?t
1
, t
2
? and ?t
?
1
, t
?
2
?, we can redefine all
the members of Eq. 1, e.g., K(t
1
, t
?
1
) becomes
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
, r), ?
M
(t
?
1
, r
?
))
+ PTK(?
M
(r, t
1
), ?
M
(r
?
, t
?
1
)),
where ?
M
maps a pair of texts to a single tree.
There are several options to produce the bitext-
to-tree mapping for ?
M
. A simple approach is
to only use the tree corresponding to the first ar-
gument of ?
M
. This leads to the basic model
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
), ?
M
(t
?
1
)) +
PTK(?
M
(r), ?
M
(r
?
)), i.e., the sum of two tree
kernels applied to the trees constructed by ?
M
(we
previously informally mentioned it).
However, this simple mapping may be ineffec-
tive since the trees within a pair, e.g., (t
1
, r), are
treated independently, and no meaningful features
connecting t
1
and r can be derived from their
tree fragments. Therefore, we model ?
M
(r, t
1
) by
using word-matching relations between t
1
and r,
such that connections between words and con-
stituents of the two trees are established using
position-independent word matching. For exam-
ple, in Figure 1, the thin dashed arrows show the
links connecting the matching words between t
1
and r. The propagation of these relations works
from the bottom up. Thus, if all children in a con-
stituent have a link, their parent is also linked.
The use of such connections is essential as it en-
ables the comparison of the structural properties
and relations between two translation-reference
pairs. For example, the tree fragment [ELABORA-
TION [SATELLITE]] from the translation is con-
nected to [ELABORATION [SATELLITE]] in the
reference, indicating a link between two entire dis-
course units (drawn with a thicker arrow), and pro-
viding some reliability to the translation
2
.
Note that the use of connections yields a graph
representation instead of a tree. This is problem-
atic as effective models for graph kernels, which
would be a natural fit to this problem, are not cur-
rently available for exploiting linguistic informa-
tion. Thus, we simply use K, as defined above,
where the mapping ?
M
(t
1
, r) only produces a tree
for t
1
annotated with the marker REL represent-
ing the connections to r. This marker is placed on
all node labels of the tree generated from t
1
that
match labels from the tree generated from r.
2
Note that a non-pairwise model, i.e., K(t
1
, r), could
also be used to match the structural information above, but
it would not learn to compare it to a second pair (t
2
, r).
In other words, we only consider the trees en-
riched by markers separately, and ignore the edges
connecting both trees.
3 Experiments and Discussion
We experimented with datasets of segment-level
human rankings of system outputs from the
WMT11 and the WMT12 Metrics shared tasks
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012): we used the WMT11 dataset for training
and the WMT12 dataset for testing. We focused
on translating into English only, for which the
datasets can be split by source language: Czech
(cs), German (de), Spanish (es), and French (fr).
There were about 10,000 non-tied human judg-
ments per language pair per dataset. We scored
our pairwise system predictions with respect to
the WMT12 human judgments using the Kendall?s
Tau (? ), which was official at WMT12.
Table 1 presents the ? scores for all metric vari-
ants introduced in this paper: for the individual
language pairs and overall. The left-hand side of
the table shows the results when using as sim-
ilarity the direct kernel calculation between the
corresponding structures of the candidate transla-
tion and the reference
3
, e.g., as in (Guzm?an et al.,
2014; Joty et al., 2014). The right-hand side con-
tains the results for structured kernel learning.
We can make the following observations:
(i) The overall results for all SKL-trained metrics
are higher than the ones when applying direct sim-
ilarity, showing that learning tree structures is bet-
ter than just calculating similarity.
(ii) Regarding the linguistic representation, we see
that, when learning tree structures, syntactic and
discourse-based trees yield similar improvements
with a slight advantage for the former. More in-
terestingly, when both structures are put together
in a combined tree, the improvement is cumula-
tive and yields the best results by a sizable margin.
This provides positive evidence towards our goal
of a unified tree-based representation with multi-
ple layers of linguistic information.
(iii) Comparing to the best evaluation metrics
that participated in the WMT12 Metrics shared
task, we find that our approach is competitive and
would have been ranked among the top 3 partici-
pants.
3
Applying tree kernels between the members of a pair to
generate one feature (for each different kernel function) has
become a standard practice in text similarity tasks (Severyn et
al., 2013b) and in question answering (Severyn et al., 2013a).
217
Similarity Structured Kernel Learning
Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all
1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198
2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184
3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183
4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231
Table 1: Kendall?s (? ) correlation with human judgements on WMT12 for each language pair.
Furthermore, our result (0.237) is ahead of the
correlation obtained by popular metrics such as
TER (0.217), NIST (0.214) and BLEU (0.185) at
WMT12. This is very encouraging and shows the
potential of our new proposal.
In this paper, we have presented only the first
exploratory results. Our approach can be easily
extended with richer linguistic structures and fur-
ther combined with some of the already existing
strong evaluation metrics.
Testing
Train cs-en de-en es-en fr-en all
1 cs-en 0.210 0.204 0.217 0.204 0.209
2 de-en 0.196 0.251 0.203 0.202 0.213
3 es-en 0.218 0.204 0.240 0.223 0.221
4 fr-en 0.203 0.218 0.224 0.223 0.217
5 all 0.231 0.258 0.226 0.232 0.237
Table 2: Kendall?s (? ) on WMT12 for cross-
language training with DIS+SYN.
Note that the results in Table 1 were for train-
ing on WMT11 and testing on WMT12 for each
language pair in isolation. Next, we study the im-
pact of the choice of training language pair. Ta-
ble 2 shows cross-language evaluation results for
DIS+SYN: lines 1-4 show results when training on
WMT11 for one language pair, and then testing for
each language pair of WMT12.
We can see that the overall differences in perfor-
mance (see the last column: all) when training on
different source languages are rather small, rang-
ing from 0.209 to 0.221, which suggests that our
approach is quite independent of the source lan-
guage used for training. Still, looking at individ-
ual test languages, we can see that for de-en and
es-en, it is best to train on the same language; this
also holds for fr-en, but there it is equally good
to train on es-en. Interestingly, training on es-en
improves a bit for cs-en.
These somewhat mixed results have motivated
us to try tuning on the full WMT11 dataset; as line
5 shows, this yielded improvements for all lan-
guage pairs except for es-en. Comparing to line
4 in Table 1, we see that the overall Tau improved
from 0.231 to 0.237.
4 Conclusions and Future Work
We have presented a pairwise learning-to-rank ap-
proach to MT evaluation, which learns to differen-
tiate good from bad translations in the context of
a given reference. We have integrated several lay-
ers of linguistic information (lexical, syntactic and
discourse) in tree-based structures, and we have
used the structured kernel learning to identify rel-
evant features and learn pairwise rankers.
The evaluation results have shown that learning
in the proposed SKL framework is possible, yield-
ing better correlation (Kendall?s ? ) with human
judgments than computing the direct kernel sim-
ilarity between translation and reference, over the
same type of structures. We have also shown that
the contributions of syntax and discourse informa-
tion are cumulative, indicating that this learning
framework can be appropriate for the combination
of different sources of information. Finally, de-
spite the limited information we used, we achieved
better correlation at the segment level than BLEU
and other metrics in the WMT12 Metrics task.
In the future, we plan to work towards our long-
term goal, i.e., including more linguistic informa-
tion in the SKL framework and showing that this
can help. This would also include more semantic
information, e.g., in the form of Brown clusters or
using semantic similarity between the words com-
posing the structure calculated with latent seman-
tic analysis (Saleh et al., 2014b).
We further want to show that the proposed
framework is flexible and can include information
in the form of quality scores predicted by other
evaluation metrics, for which a vector of features
would be combined with the structured kernel.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
218
References
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1?27.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ?07, pages 136?158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64, Edin-
burgh, Scotland, UK.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 10?51, Montr?eal, Canada.
Elisabet Comelles, Jes?us Gim?enez, Llu??s M`arquez,
Irene Castell?on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 333?
338, Uppsala, Sweden.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, WMT
?08, pages 191?194, Columbus, Ohio, USA.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ?07,
pages 256?264, Prague, Czech Republic.
Francisco Guzm?an, Shafiq Joty, Llu??s M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ?14, pages 687?
698, Baltimore, Maryland, USA.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, pages 486?496, Sofia,
Bulgaria.
Shafiq Joty, Francisco Guzm?an, Llu??s M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ?14, pages 402?408, Balti-
more, Maryland, USA.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25?32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ?12, pages 243?252,
Montr?eal, Canada.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ?07, pages 776?783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of 17th European Conference on Ma-
chine Learning and the 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML/PKDD ?06, pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ?08,
pages 253?262, Napa Valley, California, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovi?c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ?07, pages 48?55, Prague, Czech Republic.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Advances in
Neural Information Processing Systems 14, NIPS
?01, pages 995?1001, Vancouver, Canada.
219
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014a. A study of using syntactic and se-
mantic structures for concept segmentation and la-
beling. In Proceedings of the 25th International
Conference on Computational Linguistics, COLING
?14, pages 193?202, Dublin, Ireland.
Iman Saleh, Alessandro Moschitti, Preslav Nakov,
Llu??s M`arquez, and Shafiq Joty. 2014b. Semantic
kernels for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?14, Doha, Qatar.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 741?750, Portland, Oregon, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?13, pages 75?83, Sofia,
Bulgaria.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning semantic textual sim-
ilarity with structural representations. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ?13, pages 714?718, Sofia, Bulgaria.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ?06, Cambridge, Massachusetts, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ?11, pages
123?129, Edinburgh, Scotland, UK.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, HLT-NAACL ?03, pages 173?180, Ed-
monton, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1060?1068, Jeju Island, Korea.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, COLING-ACL
?06, pages 401?408, Sydney, Australia.
220
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 436?442,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semantic Kernels for Semantic Parsing
Iman Saleh
Faculty of Computers and Information
Cairo University
iman.saleh@fci-cu.edu.eg
Alessandro Moschitti, Preslav Nakov,
Llu??s M
`
arquez, Shafiq Joty
ALT Research Group
Qatar Computing Research Institute
{amoschitti,pnakov,lmarquez,sjoty}@qf.org.qa
Abstract
We present an empirical study on the use
of semantic information for Concept Seg-
mentation and Labeling (CSL), which is
an important step for semantic parsing.
We represent the alternative analyses out-
put by a state-of-the-art CSL parser with
tree structures, which we rerank with a
classifier trained on two types of seman-
tic tree kernels: one processing structures
built with words, concepts and Brown
clusters, and another one using semantic
similarity among the words composing the
structure. The results on a corpus from the
restaurant domain show that our semantic
kernels exploiting similarity measures out-
perform state-of-the-art rerankers.
1 Introduction
Spoken Language Understanding aims to inter-
pret user utterances and to convert them to logical
forms or, equivalently, to database queries, which
can then be used to satisfy the user?s information
needs. This process is known as Concept Segmen-
tation and Labeling (CSL), also called semantic
parsing in the speech community: it maps utter-
ances into meaning representations based on se-
mantic constituents. The latter are basically word
sequences, often referred to as concepts, attributes
or semantic tags. CSL makes it easy to convert
spoken questions such as ?cheap lebanese restau-
rants in doha with take out? into database queries.
First, a language-specific semantic parser tok-
enizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in]
[
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to
the segments, with the option to possibly relabel
mislabeled segments:
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
Finally, a database query is formed from the list
of labels and values, and is then executed against
the database, e.g., MongoDB; a backoff mecha-
nism may be used if the query has not succeeded.
{$and [{cuisine:"lebanese"},{city:"doha"},
{price:"low"},{amenity:"carry out"}]}
The state-of-the-art of CSL is represented by
conditional models for sequence labeling such as
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) trained with simple morphological and
lexical features. The basic CRF model was im-
proved by means of reranking (Moschitti et al.,
2006; Dinarelli et al., 2012) using structural ker-
nels (Moschitti, 2006). Although these meth-
ods exploited sentence structure, they did not use
syntax at all. More recently, we applied shal-
low syntactic structures and discourse parsing with
slightly better results (Saleh et al., 2014). How-
ever, the most obvious models for semantic pars-
ing, i.e., rerankers based on semantic structural
kernels (Bloehdorn and Moschitti, 2007b), had not
been applied to semantic structures yet.
In this paper, we study the impact of semantic
information conveyed by Brown Clusters (BCs)
(Brown et al., 1992) and semantic similarity, while
also combining them with innovative features. We
use reranking, similarly to (Saleh et al., 2014),
to select the best hypothesis annotated with con-
cepts predicted by a local model. The competing
hypotheses are represented as innovative trees en-
riched with the semantic concepts and BC labels.
The trees can capture dependencies between sen-
tence constituents, concepts and BCs. However,
extracting explicit features from them is rather
difficult as their number is exponentially large.
Thus, we rely on (i) Support Vector Machines
(Joachims, 1999) to train the reranking functions
and on (ii) structural kernels (Moschitti, 2010;
Moschitti, 2012; Moschitti, 2013) to automatically
encode tree fragments that represent syntactic and
semantic dependencies from words and concepts.
436
(a) Semantic Kernel Structure (SKS)
(b) SKS with Brown Clusters
Figure 1: CSL structures: standard and with Brown Clusters.
We further apply a semantic kernel (SK),
namely the Smoothed Partial Tree Kernel (Croce
et al., 2011), which uses the lexical similarity be-
tween the tree nodes, while computing the sub-
structure space. This is the first time that SKs are
applied to reranking hypotheses. This (i) makes
the global sentence structure along with concepts
available to the learning algorithm, and (ii) enables
computing the similarity between lexicals in syn-
tactic patterns that are enriched by concepts.
We tested our models on the Restaurant do-
main. Our results show that: (i) The basic CRF
parser, which uses semi-Markov CRF, or semi-
CRF (Sarawagi and Cohen, 2004), is already very
accurate; it achieves F
1
scores over 83%, mak-
ing any further improvement very hard. (ii) The
upper-bound performance of the reranker is very
high as well, i.e., the correct annotation is gen-
erated in the list of the first 100 hypotheses in
98.72% of the cases. (iii) SKs significantly im-
prove over the semi-CRF baseline and our pre-
vious state-of-the-art reranker exploiting shallow
syntactic patterns (Saleh et al., 2014), as shown
by extensive comparisons using several systems.
(iv) Making BCs effective requires a deeper study.
2 Related Work
One of the early approaches to CSL was that
of Pieraccini et al. (1991), where the word se-
quences and concepts were modeled using Hid-
den Markov Models (HMMs) as observations and
hidden states, respectively. Generative models
were exploited by Seneff (1989) and Miller et
al. (1994), who used stochastic grammars for
CSL. Other discriminative models followed such
preliminary work, e.g., (Rubinstein and Hastie,
1997; Santaf?e et al., 2007; Raymond and Riccardi,
2007). CRF-based models are considered to be the
state of the art in CSL (De Mori et al., 2008).
Another relevant line of research are the seman-
tic kernels, i.e., kernels that use lexical similarity
between features. One of the first that applyed
LSA was (Cristianini et al., 2002), whereas (Bloe-
hdorn et al., 2006; Basili et al., 2006) used Word-
Net. Semantic structural kernels of the type we
use in this paper were first introduced in (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b). The most advanced model based on
tree kernels, which we also use in this paper, is the
Smoothed PTK (Croce et al., 2011).
3 Reranking for CSL
Reranking is applied to a list of N annotation hy-
potheses, which are generated and sorted by the
probability to be globally correct as estimated us-
ing local classifiers or global classifiers that only
use local features. Then, a reranker, typically a
meta-classifier, tries to select the best hypothe-
sis from the list. The reranker can exploit global
information, and specifically, the dependencies
between the different concepts, which are made
available by the local model. We use semi-CRFs
for the local model as they yield the highest ac-
curacy in CSL (when using a single model) and
preference reranking for the global reranker.
3.1 Preference Reranking (PR)
PR uses a classifier C, which takes a pair of hy-
potheses ?H
i
, H
j
? and decides whether H
i
is bet-
ter than H
j
. Given a training question Q, posi-
tive and negative examples are built for training
the classifier. Let H
1
be the hypothesis with the
lowest error rate with respect to the gold standard
among all hypotheses generated for question Q.
We adopt the following approach for example gen-
eration: the pairs ?H
1
, H
i
? (i = 2, 3, . . . , N ) are
positive examples, while ?H
i
, H
1
? are considered
negative.
437
At testing time, given a new question Q
?
, C clas-
sifies all pairs ?H
i
, H
j
? generated from the anno-
tation hypotheses of Q
?
: a positive classification is
a vote for H
i
, otherwise the vote is for H
j
, where
the classifier score can be used as a weighted vote.
H
k
are then ranked according to the number (sum)
of the votes (weighted by score) they receive.
We build our reranker with SVMs using the
following kernel: K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) =
?(?H
1
, H
2
?) ? ?(?H
?
1
, H
?
2
?) ,
(
?(H
1
) ?
?(H
2
)
)
?
(
?(H
?
1
) ? ?(H
?
2
)
)
= ?(H
1
)?(H
?
1
) +
?(H
2
)?(H
?
2
) ? ?(H
1
)?(H
?
2
) ? ?(H
2
)?(H
?
1
) =
S(H
1
, H
?
1
) + S(H
2
, H
?
2
) ? S(H
1
, H
?
2
) ?
S(H
2
, H
?
1
). We consider H as a tuple ?T,~v? com-
posed of a tree T and a feature vector ~v. Then, we
define S(H,H
?
) = S
TK
(T, T
?
)+S
v
(~v,~v
?
), where
S
TK
computes one of the tree kernel functions
defined in 3.2 and 3.3; and S
v
is a kernel (see 3.4),
e.g., linear, polynomial, Gaussian, etc.
3.2 Tree kernels (TKs)
TKs measure the similarity between two structures
in terms of the number of substructures they share.
We use two types of tree kernels: (i) Partial Tree
Kernel (PTK), which can be effectively applied
to both constituency and dependency parse trees
(Moschitti, 2006). It generates all possible con-
nected tree fragments, e.g., sibling nodes can be
also separated and can be part of different tree
fragments: a fragment is any possible tree path,
and other tree paths are allowed to depart from its
nodes. Thus, it can generate a very rich feature
space. (ii) The smoothed PTK or semantic kernel
(SK) (Croce et al., 2011), which extends PTK by
allowing soft matching (i.e., via similarity compu-
tation) between nodes associated with different but
related lexical items. The node similarity can be
derived from manually annotated resources, e.g.,
WordNet or Wikipedia, as well as using corpus-
based clustering approaches, e.g., latent semantic
analysis (LSA), as we do in this paper.
3.3 Semantic structures
Tree kernels allow us to compute structural simi-
larities between two trees; thus, we engineered a
special structure for the CSL task. In order to cap-
ture the structural dependencies between the se-
mantic tags,
1
we use a basic tree (see for exam-
ple Figure 1a), where the words of a sentence are
tagged with their semantic tags.
1
They are associated with the following IDs: 0-Other,
1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6-
Hours, 7-Location, and 8-Price.
More specifically, the words in the sentence
constitute the leaves of the tree, which are in
turn connected to the pre-terminals containing
the semantic tags in BIO notation (?B?=begin,
?I?=inside, ?O?=outside). The BIO tags are then
generalized in the upper level, and joined to the
Root node. Additionally, part-of-speech (POS)
tags
2
are added to each word by concatenating
it with the string ?::L?, where L is the first let-
ter of the POS-tags of the words, e.g., along, my
and route, receive i, p and n, which are the first
letters of the POS-tags IN, PRN and NN, respec-
tively. SK applied to the above structure can gen-
erate powerful semantic patterns such as [Root
[4-Cuisine [similar to(stake house)]][7-Loc [simi-
lar to(within a mile)]]], e.g., for correctly labeling
new clauses like Pizza Parlor in three kilometers.
The BC labels, represented as cluster IDs, are sim-
ply added as siblings of words as shown in Fig. 1b.
3.4 Feature Vectors
For the sake of comparison, we also devoted
some effort towards engineering a set of features
to be used in a flat feature-vector representation.
These features can be used in isolation to learn
the reranking function, or in combination with the
kernel-based approach (as a composite kernel us-
ing a linear combination). They belong to the fol-
lowing four categories: (i) CRF-based: these in-
clude the basic features used to train the initial
semi-CRF model; (ii) n-gram based: we collected
3- and 4-grams of the output label sequence at
the level of concepts, with artificial tags inserted
to identify the start (?S?) and end (?E?) of the se-
quence.
3
(iii) Probability-based, computing the
probability of the label sequence as an average of
the probabilities at the word level in the N -best
list; and (iv) DB-based: a single feature encoding
the number of results returned from the database
when constructing a query using the conjunction
of all semantic segments in the hypothesis.
4 Experiments
The experiments aim at investigating the role of
feature vectors, PTK, SK and BCs in reranking.
We first describe the experimental setting and then
we move into the analysis of the results.
2
We use the Stanford tagger (Toutanova et al., 2003).
3
For instance, if the output sequence is Other-Rating-
Other-Amenity the 3-gram patterns would be: S-Other-
Rating, Other-Rating-Other, Rating-Other-Amenity, and
Other-Amenity-E.
438
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 7,000 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
4.1 Experimental setup
Dataset. In our experiments, we used questions
annotated with semantic tags, which were col-
lected through crowdsourcing on Amazon Me-
chanical Turk and made available
4
by McGraw et
al. (2012). We split the dataset into training, de-
velopment and test sets. Table 1 shows the num-
ber of examples and example pairs we used for
the semi-CRF and the reranker, respectively. We
subsequently split the training data randomly into
10 folds. We used cross-validation, i.e., iteratively
training with 9 folds and annotating the remaining
fold, in order to generate the N -best lists of hy-
potheses for the entire training dataset. We com-
puted the 100-best hypotheses for each example.
We then used the development dataset to test and
tune the hyper-parameters of our reranking model.
The results on the development set, which we will
present in Section 4.2 below, were obtained us-
ing semi-CRF and reranking models trained on the
training set.
Data representation. Each hypothesis is repre-
sented by a semantic tree, a feature vector (ex-
plained in Section 3), and two extra features:
(i) the semi-CRF probability of the hypothesis,
and (ii) its reciprocal rank in the N -best list.
Learning algorithm. We used the SVM-Light-
TK
5
to train the reranker with a combination of
tree kernels and feature vectors (Moschitti, 2006;
Joachims, 1999). We used the default parameters
and a linear kernel for the feature vectors. As a
baseline, we picked the best-scoring hypothesis in
the list, i.e., the output by the regular semi-CRF
parser. The setting is exactly the same as that de-
scribed in (Saleh et al., 2014).
Evaluation measure. In all experiments, we used
the harmonic mean of precision and recall (F
1
)
(van Rijsbergen, 1979), computed at the token
level and micro-averaged across the different se-
mantic types.
6
4
http://groups.csail.mit.edu/sls/downloads/restaurant/
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
We do not consider ?Other? to be a semantic type; thus,
we did not include it in the F
1
calculation.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
score for N -best lists.
Brown Clusters. Clustering groups of similar
words together provides a way of generalizing
them. In this work, we explore the use of Brown
clusters (Brown et al., 1992) in both feature vec-
tors and tree kernels. The Brown clustering al-
gorithm uses an n-gram class model. It first as-
signs each word to a distinct cluster, and then it
merges different clusters in a bottom-up fashion.
The merge step is done in a way that minimizes the
loss in average mutual information between clus-
ters. The outcome is hierarchical clustering, which
we use in our reranking algorithm. To create the
Brown clusters, we used the Yelp dataset of re-
views.
7
It contains 335,022 reviews about 15,585
businesses; 5,575 of the businesses and 233,839 of
the reviews are restaurant-related. This dataset is
very similar to the dataset of queries about restau-
rants we use in our experiments.
Similarity matrix for SK. We compute the lexi-
cal similarity for SK by applying LSA (Furnas et
al., 1988) to Tripadvisor data. The dataset and the
exact procedure for creating the LSA matrix are
described in (Castellucci et al., 2013; Croce and
Previtali, 2010).
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
score for N -best lists of different lengths, i.e., the
F
1
that is achieved by picking the best candidate
in the N -best list for various values of N . Con-
sidering 5-best lists yields an increase in oracle F
1
of almost ten absolute points. Going up to 10-best
lists only adds 2.5 extra F
1
points. The complete
100-best lists add 3.5 extra F
1
points, for a total
of 98.72. This very high value is explained by the
fact that often the total number of different anno-
tations for a given question is smaller than 100. In
our experiments, we will focus on 5-best lists.
Baseline accuracy. We computed F
1
for the semi-
CRF model on both the development and the test
sets, obtaining 83.86 and 83.03, respectively.
Learning Curves. The semantic information in
terms of BCs or semantic similarity derived by
LSA can have a major impact in case of data
scarcity. Therefore, we trained our reranking mod-
els with increasing sizes of training data.
7
http://www.yelp.com/dataset challenge/
439
Development set
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?PTK+BC	 ? PTK+all	 ?PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Test set
74	 ?
76	 ?
78	 ?
80	 ?
82	 ?
84	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?
PTK+BC	 ? PTK+all	 ?
PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Figure 2: Learning curves for different reranking models on the development and on the testing sets.
The first two graphs in Fig. 2 show the plots
on the development set whereas the last two are
computed on the test set. The reranking models
reported are Baseline, PTK, PTK+BC, PTK+all
(features), PTK+BC+all, SK, SK+BC, SK+all and
SK+BC+all.
8
We can see that: (i) PTK alone, i.e.,
without semantic information, has the lowest ac-
curacy; (ii) BCs do not improve significantly any
model; (iii) SK almost always achieves the high-
est accuracy; (iv) PTK+all (i.e., the model also us-
ing features) improves on PTK, but its accuracy
is lower than for any model using SK, i.e., us-
ing semantic similarity; and (v) all features pro-
vide an initial boost to SK, but as soon as the data
increases, their impact decreases.
5 Conclusion and Future Work
In summary, the learning curves clearly show the
good generalization ability of SK, which improve
the CRF baseline using little data (?3,000). The
semantic kernel significantly improves over the
semi-CRF baseline and our previous state-of-the-
art reranker exploiting shallow syntactic patterns
(Saleh et al., 2014), which corresponds to PTK+all
in the above comparison.
8
Models are split between 2 plots in order to ease reading.
The improvement falls between 1-2 absolute
percent points. This is remarkable as (i) it corre-
sponds to ?10% relative error reduction, and (ii)
the state-of-the-art baseline system is very difficult
to beat, as confirmed by the low impact of tradi-
tional features and BCs. Although the latter can
generalize over concepts and words, their use is
not straightforward, resulting in no improvement.
In the future, we plan to investigate the use of
semantic similarity from distributional and other
sources (Mihalcea et al., 2006; Pad?o and Lapata,
2007), e.g., Wikipedia (Strube and Ponzetto, 2006;
Mihalcea and Csomai, 2007), Wiktionary (Zesch
et al., 2008), WordNet (Pedersen et al., 2004;
Agirre et al., 2009), FrameNet, VerbNet (Shi and
Mihalcea, 2005), BabelNet (Navigli and Ponzetto,
2010), and LSA, and for different domains.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation. We would like to
thank Danilo Croce, Roberto Basili and Giuseppe
Castellucci for helping and providing us with the
similarity matrix for the semantic kernels.
440
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Boulder, Colorado, June.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica (Slovenia),
30(2):163?172.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Advances in Information Retrieval
- Proceedings of the 29th European Conference on
Information Retrieval (ECIR 2007), pages 307?318,
Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In Proceedings of the 16th ACM Conference on
Information and Knowledge Management (CIKM
2007), pages 861?864, Lisbon, Portugal.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), pages 808?812, Hong Kong.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. UNITOR: Combining
Syntactic and Semantic Kernels for Twitter Senti-
ment Analysis. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
369?374, Atlanta, Georgia, USA.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent Semantic Kernels. Journal
of Intelligent Information Systems, 18(2):127?152.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: An empirical investigation. In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 7?16,
Uppsala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1034?1046,
Edinburgh, Scotland, UK.
Renato De Mori, Frederic B?echet, Dilek Hakkani-T?ur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken Language Understanding. IEEE
Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative reranking for
spoken language understanding. IEEE Transac-
tions on Audio, Speech and Language Processing,
20(2):526?539.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proceedings of the 11th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval (SIGIR ?88),
pages 465?480, New York, USA.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289, Williamstown, MA, USA.
Ian McGraw, Scott Cyphers, Panupong Pasupat,
Jingjing Liu, and Jim Glass. 2012. Automating
crowd-supervised learning for spoken language sys-
tems. In Proceedings of the 13th Annual Conference
of the International Speech Communication Asso-
ciation (INTERSPEECH 2012), pages 2473?2476,
Portland, OR, USA.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment (CIKM 2007), pages 233?242, Lisbon, Portu-
gal.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st National Conference on Artificial In-
telligence - Volume 1 (AAAI 2006), pages 775?780,
Boston, MA, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and
Robert Ingria. 1994. Statistical Language Process-
ing using Hidden Understanding Models. In Pro-
ceedings of the workshop on Human Language Tech-
nology (HLT 1994), pages 278?282, Plainsboro, NJ,
USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
441
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 61?68, New York City, USA.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML 2006), pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2010. Kernel engineering
for fast and easy design of natural language ap-
plications. In Coling 2010: Kernel Engineering
for Fast and Easy Design of Natural Language
Applications?Tutorial notes, pages 1?91, Beijing,
China.
Alessandro Moschitti. 2012. State-of-the-art kernels
for natural language processing. In Tutorial Ab-
stracts of ACL 2012, page 2, Jeju Island, Korea.
Alessandro Moschitti. 2013. Kernel-based learning to
rank with syntactic and semantic structures. In Tu-
torial abstracts of the 36th Annual ACM SIGIR Con-
ference, page 1128, Dublin, Ireland.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the association for computational linguistics
(ACL 2010), pages 216?225, Uppsala, Sweden.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the
relatedness of concepts. In HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee.
1991. Stochastic Representation of Conceptual
Structure in the ATIS Task. In Proceedings of the
Fourth Joint DARPA Speech and Natural Language
Workshop, pages 121?124, Los Altos, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proceedings
of the 8th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2007), pages 1605?1608, Antwerp, Bel-
gium, August.
Y. Dan Rubinstein and Trevor Hastie. 1997. Discrimi-
native vs Informative Learning. In Proceedings of
the Third International Conference on Knowledge
Discovery and Data Mining (KDD-1997), pages 49?
53, Newport Beach, CA, USA.
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014. A study of using syntactic and seman-
tic structures for concept segmentation and labeling.
In Proceedings of the 25th International Conference
on Computational Linguistics, COLING ?14, pages
193?202, Dublin, Ireland.
G. Santaf?e, J.A. Lozano, and P. Larra?naga. 2007.
Discriminative vs. Generative Learning of Bayesian
Network Classifiers. In Proceedings of the 9th Euro-
pean Conference on Symbolic and Quantitative Ap-
proaches to Reasoning with Uncertainty (ECSQARU
2007), pages 453?546, Hammamet, Tunisia.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004), Vancouver, British
Columbia, Canada.
Stephanie Seneff. 1989. TINA: A Probabilistic Syn-
tactic Parser for Speech Understanding Systems.
In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing (ICASSP-
89), pages 711?714, Glasgow, UK.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Computational Lin-
guistics and Intelligent Text Processing, pages 100?
111. Springer Berlin Heidelberg.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI?06), pages
1419?1424, Boston, Massachusetts, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 173?180, Edmon-
ton, Canada.
Cornelis J. van Rijsbergen. 1979. Information
Retrieval. Butterworth-Heinemann Newton, MA,
USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd National Con-
ference on Artificial Intelligence (AAAI?08), pages
861?866, Chicago, Illinois,USA.
442
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2049?2060,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Discriminative Reranking of Discourse Parses Using Tree Kernels
Shafiq Joty and Alessandro Moschitti
ALT Research Group
Qatar Computing Research Institute
{sjoty,amoschitti}@qf.org.qa
Abstract
In this paper, we present a discrimina-
tive approach for reranking discourse trees
generated by an existing probabilistic dis-
course parser. The reranker relies on tree
kernels (TKs) to capture the global depen-
dencies between discourse units in a tree.
In particular, we design new computa-
tional structures of discourse trees, which
combined with standard TKs, originate
novel discourse TKs. The empirical evalu-
ation shows that our reranker can improve
the state-of-the-art sentence-level parsing
accuracy from 79.77% to 82.15%, a rel-
ative error reduction of 11.8%, which in
turn pushes the state-of-the-art document-
level accuracy from 55.8% to 57.3%.
1 Introduction
Clauses and sentences in a well-written text are
interrelated and exhibit a coherence structure.
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) represents the coherence struc-
ture of a text by a labeled tree, called discourse
tree (DT) as shown in Figure 1. The leaves cor-
respond to contiguous clause-like units called ele-
mentary discourse units (EDUs). Adjacent EDUs
and larger discourse units are hierarchically con-
nected by coherence relations (e.g., ELABORA-
TION, CAUSE). Discourse units connected by a re-
lation are further distinguished depending on their
relative importance: nuclei are the core parts of the
relation while satellites are the supportive ones.
Conventionally, discourse analysis in RST in-
volves two subtasks: (i) discourse segmentation:
breaking the text into a sequence of EDUs, and
(ii) discourse parsing: linking the discourse units
to form a labeled tree. Despite the fact that dis-
course analysis is central to many NLP appli-
cations, the state-of-the-art document-level dis-
course parser (Joty et al., 2013) has an f -score
of only 55.83% using manual discourse segmen-
tation on the RST Discourse Treebank (RST-DT).
Although recent work has proposed rich lin-
guistic features (Feng and Hirst, 2012) and pow-
erful parsing models (Joty et al., 2012), discourse
parsing remains a hard task, partly because these
approaches do not consider global features and
long range structural dependencies between DT
constituents. For example, consider the human-
annotated DT (Figure 1a) and the DT generated by
the discourse parser of Joty et al. (2013) (Figure
1b) for the same text. The parser makes a mistake
in finding the right structure: it considers only e
3
as the text to be attributed to e
2
, where all the text
spans from e
3
to e
6
(linked by CAUSE and ELAB-
ORATION) compose the statement to be attributed.
Such errors occur because existing systems do not
encode long range dependencies between DT con-
stituents such as those between e
3
and e
4?6
.
Reranking models can make the global struc-
tural information available to the system as fol-
lows: first, a base parser produces several DT
hypotheses; and then a classifier exploits the en-
tire information in each hypothesis, e.g., the com-
plete DT with its dependencies, for selecting the
best DT. Designing features capturing such global
properties is however not trivial as it requires the
selection of important DT fragments. This means
selecting subtree patterns from an exponential fea-
ture space. An alternative approach is to implicitly
generate the whole feature space using tree kernels
(TKs) (Collins and Duffy, 2002; Moschitti, 2006).
In this paper, we present reranking models for
discourse parsing based on Support Vector Ma-
chines (SVMs) and TKs. The latter allows us
to represent structured data using the substructure
space thus capturing structural dependencies be-
tween DT constituents, which is essential for ef-
fective discourse parsing. Specifically, we made
the following contributions. First, we extend the
2049
Topic-Comment
Attribution
Cause
Elaboration
Elaboration
e
e
e
e
e e
2
3
4
5 6
1
(a) A human-annotated discourse tree.
Background
Attribution
Cause
Elaboration
Elaboration
e
e e e
e e
2 3 4
5 6
1
(b) A discourse tree generated by Joty et al. (2013).
Figure 1: Example of human-annotated and system-generated discourse trees for the text [what?s more,]
e
1
[he believes]
e
2
[seasonal swings in the auto industry this year aren?t occurring at the same time in the past,]
e
3
[because of production and pric-
ing differences]
e
4
[that are curbing the accuracy of seasonal adjustments]
e
5
] [built into the employment data.]
e
6
Horizontal
lines indicate text segments; satellites are connected to their nuclei by curved arrows.
existing discourse parser
1
(Joty et al., 2013) to
produce a list of k most probable parses for each
input text, with associated probabilities that define
the initial ranking.
Second, we define a set of discourse tree ker-
nels (DISCTK) based on the functional composi-
tion of standard TKs with structures representing
the properties of DTs. DISCTK can be used for
any classification task involving discourse trees.
Third, we use DISCTK to define kernels for
reranking and use them in SVMs. Our rerankers
can exploit the complete DT structure using TKs.
They can ascertain if portions of a DT are compat-
ible, incompatible or simply not likely to coexist,
since each substructure is an exploitable feature.
In other words, problematic DTs are expected to
be ranked lower by our reranker.
Finally, we investigate the potential of our ap-
proach by computing the oracle f -scores for both
document- and sentence-level discourse parsing.
However, as demonstrated later in Section 6, for
document-level parsing, the top k parses often
miss the best parse. For example, the oracle f -
scores for 5- and 20-best document-level parsing
are only 56.91% and 57.65%, respectively. Thus
the scope of improvement for the reranker is rather
narrow at the document level. On the other hand,
the oracle f -score for 5-best sentence-level dis-
course parsing is 88.09%, where the base parser
(i.e., 1-best) has an oracle f -score of 79.77%.
Therefore, in this paper we address the following
two questions: (i) how far can a reranker improve
the parsing accuracy at the sentence level? and
(ii) how far can this improvement, if at all, push
the (combined) document-level parsing accuracy?
To this end, our comparative experiments on
1
Available from http://alt.qcri.org/tools/
RST-DT show that the sentence-level reranker can
improve the f -score of the state-of-the-art from
79.77% to 82.15%, corresponding to a relative
error reduction of 11.8%, which in turn pushes
the state-of-the-art document-level f -score from
55.8% to 57.3%, an error reduction of 3.4%.
In the rest of the paper, after introducing the TK
technology in Section 2, we illustrate our novel
structures, and how they lead to the design of
novel DISCTKs in Section 3. We present the k-
best discourse parser in Section 4. In Section 5, we
describe our reranking approach using DISCTKs.
We report our experiments in Section 6. We briefly
overview the related work in Section 7, and finally,
we summarize our contributions in Section 8.
2 Kernels for Structural Representation
Tree kernels (Collins and Duffy, 2002; Shawe-
Taylor and Cristianini, 2004; Moschitti, 2006) are
a viable alternative for representing arbitrary sub-
tree structures in learning algorithms. Their ba-
sic idea is that kernel-based learning algorithms,
e.g., SVMs or perceptron, only need the scalar
product between the feature vectors representing
the data instances to learn and classify; and kernel
functions compute such scalar products in an effi-
cient way. In the following subsections, we briefly
describe the kernel machines and three types of
tree kernels (TKs), which efficiently compute the
scalar product in the subtree space, where the vec-
tor components are all possible substructures of
the corresponding trees.
2.1 Kernel Machines
Kernel Machines (Cortes and Vapnik, 1995), e.g.,
SVMs, perform binary classification by learning
a hyperplane H(~x) = ~w ? ~x + b = 0, where
2050
Chapter 2. Background 15
a
b
c e
g
)
a
b
c e
g
b
c e
g c e
Figure 2.4: A tree (left) and all of its proper subtrees (right).
a
b
c e
g
)
a
b
c e
g
a
b g
c e
g
b
c e
Figure 2.5: A tree (left) and all of its subset trees (right).
Proper Subtree A proper subtree t
i
comprises node v
i
along with all of its de-
scendants (see figure 2.4 for an example of a tree along with all its proper subtrees).
Subset Tree A subset tree is a subtree for which the following constraint is sat-
isfied: either all of the children of a node belong to the subset tree or none of them.
The reason for adding such a constraint can be understood by considering the fact
that subset trees were defined for measuring the similarity of parse trees in natural
language applications. In that context a node along with all of its children represent
a grammar production. Figure 2.5 gives an example of a tree along with some of its
subset trees.
Figure 2: A tree with its STK subtrees; STK
b
also includes
leaves as features.
~x ? R
n
is the feature vector representation of an
object o ? O to be classified and ~w ? R
n
and
b ? R are parameters learned from the training
data. One can train such machines in the dual
space by rewri ing the model parameter ~w as a l n-
ear combination of training examples, i.e., ~w =
?
i=1..l
y
i
?
i
~x
i
, where y
i
is equal to 1 for positive
exam les and ?1 for negative example , ?
i
? R
+
and ~x
i
?i ? {1, .., l} are the training instances.
Then, we can use the data object o
i
? O directly
in the hyperplane equation considering their map-
ping function ? : O ? R
n
, as follows: H(o) =
?
i=1..l
y
i
?
i
~x
i
?~x+b =
?
i=1..l
y
i
?
i
?(o
i
) ??(o)+
b =
?
i=1..l
y
i
?
i
K(o
i
, o) + b, where the product
K(o
i
, o) = ??(o
i
) ? ?(o)? is the kernel function
(e.g., TK) associated with the mapping ?.
2.2 Tree Kernels
Convolution TKs compute the number of com-
mon tree fragments between two trees T
1
and T
2
without explicitly considering the whole fragment
space. A TK function over T
1
and T
2
is defined as:
TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
),
where N
T
1
and N
T
2
are the sets of the nodes of
T
1
and T
2
, respectively, and ?(n
1
, n
2
) is equal
to the number of common fragments rooted in
the n
1
and n
2
nodes.
2
The computation of ?
function depends on the shape of fragments,
conversely, a different ? determines the richness
of the kernel space and thus different tree kernels.
In the following, we briefly describe two existing
and well-known tree kernels. Please see several
tutorials on kernels (Moschitti, 2013; Moschitti,
2012; Moschitti, 2010) for more details.
3
Syntactic Tree Kernels (STK) produce fragments
such that each of their nodes includes all or none
of its children. Figure 2 shows a tree T and its
three fragments (do not consider the single nodes)
in the STK space on the left and right of the ar-
2
To get a similarity score between 0 and 1, it is
common to apply a normalization in the kernel space,
i.e.
TK(T
1
,T
2
)
?
TK(T
1
,T
1
)?TK(T
2
,T
2
)
.
3
Tutorials notes available at http://disi.unitn.
it/moschitti/
14 Chapter 2. Background
a
b
c e
g
2 3
31
Figure 2.2: A positional Tree. The number over an arc represents the position of
the node with respect to its parent.
a
b
c e
g
)
a
b
c e
g
a
b g
c e
g a
gb
c e
a
b
c e
a
b
e
Figure 2.3: A tree (left) and some of its subtrees (right).
node. The maximum out-degree of a tree is the highest index of all the nodes of the
tree. The out-degree of a node for an ordered tree corresponds to the number of its
children. The depth of a node v
i
with respect to one of its ascendants v
j
is defined
as the number of nodes comprising the path from v
j
to v
i
. When not specified, the
node with respect to the depth is computed, is the root.
A tree can be decomposed in many types of substructures.
Subtree A subtree t is a subset of nodes in the tree T , with corresponding edges,
which forms a tree. A subtree rooted at node v
i
will be indicated with t
i
, while a
subtree rooted at a generic node v will be indicated by t(v). When t is used in a
context where a node is expected, t refers to the root node of the subtree t. The
set of subtrees of a tree will be indicated by N
T
. When clear from the context N
T
may refer to specific type of subtrees. Figure 2.3 gives an example of a tree together
with its subtrees. Various types of subtrees can be defined for a tree T .
Figure 3: A tree with its PTK fragments.
row, respectively. STK(T ,T ) counts the number
of co mon fragments, which in this case is the
umber of subtrees of T , i.e., three. In the figure,
we also show three single nodes, c, e, and g, i.e.,
the leaves of T , which are computed by a vari-
ant of the kernel, that we call STK
b
. The com-
putati nal complexity of STK is O(|N
T
1
||N
T
2
|),
but the average running time tends to be linear
(i.e. O(|N
T
1
| + |N
T
2
|)) for syntactic trees (Mos-
chitti, 2006).
Partial Tree Kernel (PTK) generates a richer set
of tree fragments. Given a target tree T , PTK
can generate any subset of connected nodes of T ,
whose edges are in T . For example, Figure 3
shows a tree with its nine fragments including all
single nodes (i.e., the leaves of T ). PTK is more
general than STK as its fragments can include any
subsequence of children of a target node. The time
complexity of PTK is O(p?
2
|N
T
1
||N
T
2
|), where
p is the largest subsequence of children that one
wants to consider and ? is the maximal out-degree
observed in the two trees. However, the average
running time again tends to be linear for syntactic
trees (Moschitti, 2006).
3 Discourse Tree Kernels (DISCTK)
Engineering features that can capture the depen-
dencies between DT constituents is a difficult task.
In principle, any dependency between words, rela-
tions and structures (see Figure 1) can be an im-
portant feature for discourse parsing. This may
lead to an exponential number of features, which
makes the feature engineering process very hard.
The standard TKs described in the previous sec-
tion serve as a viable option to get useful sub-
tree features automatically. However, the defini-
tion of the input to a TK, i.e., the tree represent-
ing a training instance, is extremely important as
it implicitly affects the subtree space generated by
the TK, where the target learning task is carried
out. This can be shown as follows. Let ?
M
()
be a mapping from linguistic objects o
i
, e.g., a
discourse parse, to a meaningful tree T
i
, and let
?
TK
() be a mapping into a tree kernel space us-
2051
(a) JRN
(b) SRN
Figure 4: DISCTK trees: (a) Joint Relation-Nucleus (JRN), and (b) Split Relation Nucleus (SRN).
ing one of the TKs described in Section 2.2, i.e.,
TK(T
1
, T
2
) = ?
TK
(T
1
) ? ?
TK
(T
2
). If we apply
TK to the objects o
i
transformed by ?
M
(), we
obtain TK(?
M
(o
1
), ?
M
(o
2
)) = ?
TK
(?
M
(o
1
)) ?
?
TK
(?
M
(o
2
))=
(
?
TK
??
M
)
(o
1
)?
(
?
TK
??
M
)
(o
2
)
= DiscTK(o
1
, o
2
), which is a new kernel
4
in-
duced by the mapping ?
DiscTK
=
(
?
TK
? ?
M
)
.
We define two different mappings ?
M
to trans-
form the discourse parses generated by the base
parser into two different tree structures: (i) the
Joint Relation-Nucleus tree (JRN), and (ii) the
Split Relation Nucleus tree (SRN).
3.1 Joint Relation-Nucleus Tree (JRN)
As shown in Figure 4a, JRN is a direct mapping
of the parser output, where the nuclearity statuses
(i.e., satellite or nucleus) of the connecting nodes
are attached to the relation labels.
5
For example,
the root BACKGROUND
Satellite?Nucleus
in Figure
4a denotes a Background relation between a satel-
lite discourse unit on the left and a nucleus unit on
the right. Text spans (i.e., EDUs) are represented
as sequences of Part-of-Speech (POS) tags con-
nected to the associated words, and are grouped
under dummy SPAN nodes. We experiment with
two lexical variations of the trees: (i) All includes
all the words in the EDU, and (ii) Bigram includes
only the first and last two words in the EDU.
When JRN is used with the STK kernel, an ex-
ponential number of fragments are generated. For
example, the upper row of Figure 5 shows two
4
People interested in algorithms may like it more design-
ing a complex algorithm to compute
(
?
TK
??
M
)
. However,
the design of ?
M
is conceptually equivalent and more effec-
tive from an engineering viewpoint.
5
This is a common standard followed by the parsers.
smallest (atomic) fragments and one subtree com-
posed of two atomic fragments. Note that much
larger structures encoding long range dependen-
cies are also part of the feature space. These frag-
ments can reveal if the discourse units are orga-
nized in a compatible way, and help the reranker
to detect the kind of errors shown earlier in Fig-
ure 1b. However, one problem with JRN repre-
sentation is that since the relation nodes are com-
posed of three different labels, the generated sub-
trees tend to be sparse. In the following, we de-
scribe SRN that attempts to solve this issue.
3.2 Split Relation Nucleus Tree (SRN)
SRN is not very different from JRN as shown in
Figure 4b. The only difference is that instead of
attaching the nuclearity statuses to the relation la-
bels, in this representation we assign them to their
respective discourse units. When STK kernel is
applied to SRN it again produces an exponential
number of fragments. For example, the lower row
of Figure 5 shows two atomic fragments and one
subtree composed of two atomic fragments. Com-
paring the two examples in Figure 5, it is easy
to understand that the space of subtrees extracted
from SRN is less sparse than that of JRN.
Note that, as described in Secion 2.2, when the
PTK kernel is applied to JRN and SRN trees, it can
generate a richer feature space, e.g., features that
are paths containing relation labels (e.g., BACK-
GROUND - CAUSE - ELABORATION or ATTRIBU-
TION - CAUSE - ELABORATION).
4 Generation of k-best Discourse Parses
In this section we describe the 1-best discourse
parser of Joty et al. (2013), and how we extend
2052
Figure 5: Fragments from JRN in Figure 4a (upper row) and SRN in Figure 4b (lower row).
it to k-best discourse parsing.
Joty et al. (2013) decompose the problem of
document-level discourse parsing into two stages
as shown in Figure 6. In the first stage, the intra-
sentential discourse parser produces discourse
subtrees for the individual sentences in a docu-
ment. Then the multi-sentential parser combines
the sentence-level subtrees and produces a DT for
the document. Both parsers have the same two
components: a parsing model and a parsing al-
gorithm. The parsing model explores the search
space of possible DTs and assigns a probability to
every possible DT. Then the parsing algorithm se-
lects the most probable DT(s). While two separate
parsing models are employed for intra- and multi-
sentential parsing, the same parsing algorithm is
used in both parsing conditions. The two-stage
parsing exploits the fact that sentence boundaries
correlate very well with discourse boundaries. For
example, more than 95% of the sentences in RST-
DT have a well-formed discourse subtree in the
full document-level discourse tree.
The choice of using two separate models for
intra- and multi-sentential parsing is well justified
for the following two reasons: (i) it has been ob-
served that discourse relations have different dis-
tributions in the two parsing scenarios, and (ii) the
models could independently pick their own infor-
mative feature sets. The parsing model used for
intra-sentential parsing is a Dynamic Conditional
Random Field (DCRF) (Sutton et al., 2007) shown
in Figure 7. The observed nodes U
j
at the bottom
layer represent the discourse units at a certain level
of the DT; the binary nodes S
j
at the middle layer
predict whether two adjacent units U
j?1
and U
j
should be connected or not; and the multi-class
nodes R
j
at the top layer predict the discourse
relation between U
j?1
and U
j
. Notice that the
model represents the structure and the label of a
DT constituent jointly, and captures the sequential
dependencies between the DT constituents. Since
the chain-structured DCRF model does not scale
up to multi-sentential parsing of long documents,
Model
A lgorithmSentences segmentedinto EDUs
Document-leveldiscourse treeModel
A lgorithm
Multi-sentential parserIntra-sentential parser
Figure 6: The two-stage document-level discourse parser
proposed by Joty et al. (2013).
U UU U U
2
2
2
3 j t-1 t
SS S S S
R R R R R
3
3 j
j t-1
t-1 t
U1
t
Relation sequenceStructuresequence
 Unit sequence at level i 
Figure 7: The intra-sentential parsing model.
the multi-sentential parsing model is a CRF which
breaks the chain structure of the DCRF model.
The parsing models are applied recursively at
different levels of the DT in their respective pars-
ing scenarios (i.e., intra- and multi-sentential),
and the probabilities of all possible DT con-
stituents are obtained by computing the posterior
marginals over the relation-structure pairs (i.e.,
P (R
j
, S
j
=1|U
1
, ? ? ? , U
t
,?), where ? are model
parameters). These probabilities are then used in
a CKY-like probabilistic parsing algorithm to find
the globally optimal DT for the given text.
Let U
b
x
and U
e
x
denote the beginning and
end EDU Ids of a discourse unit U
x
, and
R[U
b
i
, U
e
m
, U
e
j
] refers to a coherence relation
R that holds between the discourse unit con-
taining EDUs U
b
i
through U
e
m
and the unit
containing EDUs U
e
m
+1 through U
e
j
. Given n
discourse units, the parsing algorithm uses the
upper-triangular portion of the n?n dynamic
programming table A, where cell A[i, j] (for
i < j) stores:
A[i, j] = P (r
?
[U
b
i
, U
e
m
?
, U
e
j
]), where
(m
?
, r
?
) = argmax
i?m<j ; R
P (R[U
b
i
, U
e
m
, U
e
j
])?
A[i,m]?A[m+ 1, j] (1)
2053
1 1 2
2 2
3
B
r
1
r
3
r
2
r
2
r
3
r
4
C
r
2
r
1
e
1
e
2
r
4
e
3
e
4
Figure 8: The B and C dynamic programming tables (left), and the corresponding discourse tree (right).
In addition to A, which stores the probability of
the most probable constituents of a DT, the pars-
ing algorithm also simultaneously maintains two
other tables B and C for storing the best structure
(i.e., U
e
m
?
) and the relations (i.e., r
?
) of the corre-
sponding DT constituents, respectively. For exam-
ple, given 4 EDUs e
1
? ? ? e
4
, the B and C tables at
the left side in Figure 8 together represent the DT
shown at the right. More specifically, to generate
the DT, we first look at the top-right entries in the
two tables, and find B[1, 4] = 2 and C[1, 4] = r
2
,
which specify that the two discourse units e
1:2
and
e
3:4
should be connected by the relation r
2
(the
root in the DT). Then, we see how EDUs e
1
and
e
2
should be connected by looking at the entries
B[1, 2] and C[1, 2], and find B[1, 2] = 1 and
C[1, 2] = r
1
, which indicates that these two units
should be connected by the relation r
1
(the left
pre-terminal). Finally, to see how EDUs e
3
and e
4
should be linked, we look at the entriesB[3, 4] and
C[3, 4], which tell us that they should be linked by
the relation r
4
(the right pre-terminal).
It is straight-forward to generalize the above al-
gorithm to produce k most probable DTs. When
filling up the dynamic programming tables, rather
than storing a single best parse for each subtree,
we store and keep track of k-best candidates si-
multaneously. More specifically, each cell in the
dynamic programming tables (i.e., A, B and C)
should now contain k entries (sorted by their prob-
abilities), and for each such entry there should be a
back-pointer that keeps track of the decoding path.
The algorithm works in polynomial time. For
n discourse units and M number of relations, the
1-best parsing algorithm has a time complexity of
O(n
3
M) and a space complexity of O(n
2
), where
the k-best version has a time and space complexi-
ties ofO(n
3
Mk
2
log k) andO(n
2
k), respectively.
There are cleverer ways to reduce the complexity
(e.g., see (Huang and Chiang, 2005) for three such
ways). However, since the efficiency of the algo-
rithm did not limit us to produce k-best parses for
larger k, it was not a priority in this work.
5 Kernels for Reranking Discourse Trees
In Section 3, we described DISCTK, which essen-
tially can be used for any classification task involv-
ing discourse trees. For example, given a DT, we
can use DISCTK to classify it as correct vs. in-
correct. However, such classification is not com-
pletely aligned to our purpose, since our goal is
to select the best (i.e., the most correct) DT from
k candidate DTs; i.e., a ranking task. We adopt
a preference reranking technique as described in
(Moschitti et al., 2006; Dinarelli et al., 2011).
5.1 Preference Reranker
Preference reranking (PR) uses a classifier C of
pairs of hypotheses ?h
i
, h
j
?, which decides if h
i
(i.e., a candidate DT in our case) is better than
h
j
. We generate positive and negative examples to
train the classifier using the following approach.
The pairs ?h
1
, h
i
? constitute positive examples,
where h
1
has the highest f -score accuracy on the
Relation metric (to be described in Section 6) with
respect to the gold standard among the candidate
hypotheses, and vice versa, ?h
i
, h
1
? are considered
as negative examples. At test time, C classifies all
pairs ?h
i
, h
j
? generated from the k-best hypothe-
ses. A positive decision is a vote for h
i
, and a neg-
ative decision is a vote for h
j
. Also, the classifier
score can be used as a weighted vote. Hypotheses
are then ranked according to the number (sum) of
the (weighted) votes they get.
6
We build our reranker using simple SVMs.
7
6
As shown by Collins and Duffy (2002), only the classifi-
cation of k hypotheses (paired with the empty one) is needed
in practice, thus the complexity is only O(k).
7
Structural kernels, e.g., TKs, cannot be used in more ad-
vanced algorithms working in structured output spaces, e.g.,
SVM
struct
. Indeed, to our knowledge, no one could suc-
cessfully find a general and exact solution for the argmax
equation, typically part of such advanced models, when struc-
tural kernels are used. Some approximate solutions for sim-
ple kernels, e.g., polynomial or gaussian kernels, are given in
(Joachims and Yu, 2009), whereas (Severyn and Moschitti,
2011; Severyn and Moschitti, 2012) provide solutions for
using the cutting-plane algorithm (which requires argmax
computation) with structural kernels but in binary SVMs.
2054
Since in our problem a pair of hypotheses ?h
i
, h
j
?
constitutes a data instance, we now need to define
the kernel between the pairs. However, notice that
DISCTK only works on a single pair.
Considering that our task is to decide whether
h
i
is better than h
j
, it can be convenient to
represent the pairs in terms of differences be-
tween the vectors of the two hypotheses, i.e.,
?
K
(h
i
)? ?
K
(h
j
), where K (i.e., DISCTK) is de-
fined between two hypotheses (not on two pairs
of hypotheses). More specifically, to compute
this difference implicitly, we can use the follow-
ing kernel summation: PK(?h
1
, h
2
?, ?h
?
1
, h
?
2
?) =
(?
K
(h
1
) ? ?
K
(h
2
)) ? (?
K
(h
?
1
) ? ?
K
(h
?
2
)) =
K(h
1
, h
?
1
)+K(h
2
, h
?
2
)?K(h
1
, h
?
2
)?K(h
2
, h
?
1
).
In general, Preference Kernel (PK) works well
because it removes many identical features by tak-
ing differences between two huge implicit TK-
vectors. In our reranking framework, we also in-
clude traditional feature vectors in addition to the
trees. Therefore, each hypothesis h is represented
as a tuple ?T,~v? composed of a tree T and a fea-
ture vector ~v. We then define a structural kernel
(i.e., similarity) between two hypotheses h and
h
?
as follows: K(h, h
?
) = DiscTK(T, T
?
) +
FV (~v,~v
?
), where DISCTK maps the DTs T and
T
?
to JRN or SRN and then applies STK, STK
b
or
PTK defined in Sections 2.2 and 3, and FV is a
standard kernel, e.g., linear, polynomial, gaussian,
etc., over feature vectors (see next section).
5.2 Feature Vectors
We also investigate the impact of traditional
(i.e., not subtree) features for reranking discourse
parses. Our feature vector comprises two types of
features that capture global properties of the DTs.
Basic Features. This set includes eight global
features. The first two are the probability and
the (inverse) rank of the DT given by the base
parser. These two features are expected to help
the reranker to perform at least as good as the base
parser. The other six features encode the structural
properties of the DT, which include depth of the
DT, number of nodes connecting two EDUs (i.e.,
SPANs in Figure 4), number of nodes connecting
two relational nodes, number of nodes connecting
a relational node and an EDU, number of nodes
that connects a relational node as left child and an
EDU as right child, and vice versa.
Relation Features. We encode the relations in
the DT as bag-of-relations (i.e., frequency count).
This will allow us to assess the impact of a flat rep-
resentation of the DT. Note that more important
relational features would be the subtree patterns
extracted from the DT. However, they are already
generated by TKs in a simpler way. See (Pighin
and Moschitti, 2009; Pighin and Moschitti, 2010)
for a way to extract the most relevant features from
a model learned in the kernel space.
6 Experiments
Our experiments aim to show that reranking of
discourse parses is a promising research direction,
which can improve the state-of-the-art. To achieve
this, we (i) compute the oracle accuracy of the k-
best parser, (ii) test different kernels for reranking
discourse parses by applying standard kernels to
our new structures, (iii) show the reranking perfor-
mance using the best kernel for different number
of hypotheses, and (iv) show the relative impor-
tance of features coming from different sources.
6.1 Experimental Setup
Data. We use the standard RST-DT corpus (Carl-
son et al., 2002), which comes with discourse an-
notations for 385 articles (347 for training and 38
for testing) from the Wall Street Journal. We ex-
tracted sentence-level DTs from a document-level
DT by finding the subtrees that exactly span over
the sentences. This gives 7321 and 951 sentences
in the training and test sets, respectively. Follow-
ing previous work, we use the same 18 coarser re-
lations defined by Carlson and Marcu (2001).
We create the training data for the reranker in a
5-fold cross-validation fashion.
8
Specifically, we
split the training set into 5 equal-sized folds, and
train the parsing model on 4 folds and apply to the
rest to produce k most probable DTs for each text.
Then we generate and label the pairs (by compar-
ing with the gold) from the k most probable trees
as described in Section 5.1. Finally, we merge the
5 labeled folds to create the full training data.
SVM Reranker. We use SVM-light-TK to train
our reranking models,
9
which enables the use
of tree kernels (Moschitti, 2006) in SVM-light
(Joachims, 1999). We build our new kernels for
reranking exploiting the standard built-in TK func-
tions, such as STK, STK
b
and PTK. We applied
8
Note that our earlier experiments with a 2-fold cross vali-
dation process yielded only 50% of our current improvement.
9
http://disi.unitn.it/moschitti/Tree-Kernel.htm
2055
a linear kernel to standard feature vectors as it
showed to be the best on our development set.
Metrics. The standard procedure to evaluate dis-
course parsing performance is to compute Pre-
cision, Recall and f -score of the unlabeled and
labeled metrics proposed by Marcu (2000b).
10
Specifically, the unlabeled metric Span measures
how accurate the parser is in finding the right
structure (i.e., skeleton) of the DT, while the la-
beled metrics Nuclearity and Relation measure the
parser?s ability to find the right labels (nuclearity
and relation) in addition to the right structure. Op-
timization of the Relation metric is considered to
be the hardest and the most desirable goal in dis-
course parsing since it gives aggregated evaluation
on tree structure and relation labels. Therefore,
we measure the oracle accuracy of the k-best dis-
course parser based on the f -scores of the Relation
metric, and our reranking framework aims to op-
timize the Relation metric.
11
Specifically, the ora-
cle accuracy for k-best parsing is measured as fol-
lows: ORACLE =
?
N
i=1
max
k
j=1
f?score
r
(g
i
,h
j
i
)
N
, where
N is the total number of texts (sentences or docu-
ments) evaluated, g
i
is the gold DT annotation for
text i, h
j
i
is the j
th
parse hypothesis generated by
the k-best parser for text i, and f -score
r
(g
i
, h
j
i
) is
the f -score accuracy of hypothesis h
j
i
on the Re-
lation metric. In all our experiments we report the
f -scores of the Relation metric.
6.2 Oracle Accuracy
Table 1 presents the oracle scores of the k-
best intra-sentential parser PAR-S on the standard
RST-DT test set. The 1-best result corresponds
to the accuracy of the base parser (i.e., 79.77%).
The 2-best shows dramatic oracle-rate improve-
ment (i.e., 4.65% absolute), suggesting that the
base parser often generates the best tree in its
top 2 outputs. 5-best increases the oracle score
to 88.09%. Afterwards, the increase in accuracy
slows down, achieving, e.g., 90.37% and 92.57%
at 10-best and 20-best, respectively.
The results are quite different at the document
level as Table 2 shows the oracle scores of the k-
best document-level parser PAR-D.
12
The results
10
Precision, Recall and f -score are the same when the dis-
course parser uses manual discourse segmentation. Since all
our experiments in this paper are based on manual discourse
segmentation, we only report the f -scores.
11
It is important to note that optimizing Relation metric
may also result in improved Nuclearity scores.
12
For document-level parsing, Joty et al. (2013) pro-
k 1 2 5 10 15 20
PAR-S 79.77 84.42 88.09 90.37 91.74 92.57
Table 1: Oracle scores as a function of k of k-best sentence-
level parses on RST-DT test set.
k 1 2 5 10 15 20
PAR-D 55.83 56.52 56.91 57.23 57.54 57.65
Table 2: Oracle scores as a function of k of k-best
document-level parses on RST-DT test set.
suggest that the best tree is often missing in the
top k parses, and the improvement in oracle-rate is
very little as compared to the sentence-level pars-
ing. The 2-best and the 5-best improve over the
base accuracy by only 0.7% and 1.0%, respec-
tively. The improvement becomes even lower for
larger k. For example, the gain from 20-best to
30-best parsing is only 0.09%. This is not sur-
prising because generally document-level DTs are
big with many constituents, and only a very few
of them change from k-best to k+1-best parsing.
These small changes do not contribute much to
the overall f -score accuracy.
13
In summary, the
results in Tables 1 and 2 demonstrate that a k-best
reranker can potentially improve the parsing accu-
racy at the sentence level, but may not be a suit-
able option for improving parsing at the document
level. In the following, we report our results for
reranking sentence-level discourse parses.
6.3 Performance of Different DISCTKs
Section 3 has pointed out that different DISCTKs
can be obtained by specifying the TK type (e.g.,
STK, STK
b
, PTK) and the mapping ?
M
(i.e.,
JRN, SRN) in the overall kernel function
(
?
TK
?
?
M
)
(o
1
)?
(
?
TK
??
M
)
(o
2
). Table 3 reports the per-
formance of such model compositions using the 5-
best hypotheses on the RST-DT test set. Addition-
ally, it also reports the accuracy for the two ver-
sions of JRN and SRN, i.e., Bigram and All. From
these results, we can note the following.
Firstly, the kernels generally perform better on
Bigram than All lexicalization. This suggests that
using all the words from the text spans (i.e., EDUs)
produces sparse models.
pose two approaches to combine intra- and multi-sentential
parsers, namely 1S-1S (1 Sentence-1 Subtree) and Sliding
window. In this work we extend 1S-1S to k-best document-
level parser PAR-D since it is not only time efficient but it
also achieves better results on the Relation metric.
13
Note that Joty et al. (2012; 2013) report lower f -scores
both at the sentence level (i.e., 77.1% as opposed to our
79.77%) and at the document level (i.e., 55.73% as opposed
to our 55.83%). We fixed a crucial bug in their (1-best) pars-
ing algorithm, which accounts for the improved performance.
2056
?TK
? ?
M
JRN SRN
Bigram All Bigram All
STK 81.28 80.04 82.15 80.04
STK
b
81.35 80.28 82.18 80.25
PTK 81.63 78.50 81.42 78.25
Table 3: Reranking performance of different discourse tree
kernels on different representations.
Secondly, while the tree kernels perform sim-
ilarly on the JRN representation, STK performs
significantly better (p-value < 0.01) than PTK
on SRN.
14
This result is interesting as it pro-
vides indications of the type of DT fragments use-
ful for improving parsing accuracy. As pointed
out in Section 2.2, PTK includes all features
generated by STK, and additionally, it includes
fragments whose nodes can have any subsets of
the children they have in the original DT. Since
this does not improve the accuracy, we speculate
that complete fragments, e.g., [CAUSE [ATTRI-
BUTION][ELABORATION]] are more meaningful
than the partial ones, e.g., [CAUSE [ATTRIBU-
TION]] and [CAUSE [ELABORATION]], which
may add too much uncertainty on the signature
of the relations contained in the DT. We verified
this hypothesis by running an experiment with
PTK constraining it to only generate fragments
whose nodes preserve all or none of their children.
The accuracy of such fragments approached the
ones of STK, suggesting that relation information
should be used as a whole for engineering features.
Finally, STK
b
is slightly (but not significantly)
better than STK suggesting that the lexical infor-
mation is already captured by the base parser.
Note that the results in Table 3 confirms many
other experiments we carried out on several devel-
opment sets. For any run: (i) STK always performs
as well as STK
b
, (ii) STK is always better than
PTK, and (iii) SRN is always better than JRN. In
what follows, we show the reranking performance
based on STK applied to SRN with Bigram.
6.4 Insights on DISCTK-based Reranking
Table 4 reports the performance of our reranker
(RR) in comparison with the oracle (OR) accuracy
for different values of k, where we also show the
corresponding relative error rate reduction (ERR)
with respect to the baseline. To assess the general-
ity of our approach, we evaluated our reranker on
both the standard test set and the entire training set
using 5-fold cross validation.
15
14
Statistical significance is verified using paired t-test.
15
The reranker was trained on 4 folds and tested on the rest
Baseline Basic feat. + Rel. feat. + Tree
79.77 79.84 79.81 82.15
Table 5: Comparison of features from different sources for
5-best discourse reranking.
(Joty et al., 2013) With Reranker
PAR-D 55.8 57.3
Table 6: Document-level parsing results with 5-best
sentence-level discourse reranker.
We note that: (i) the best result on the standard
test set is 82.15% for k = 4 and 5, which gives
an ERR of 11.76%, and significantly (p-value <
0.01) outperforms the baseline, (ii) the improve-
ment is consistent when we move from standard
test set to 5-folds, (iii) the best result on the 5-folds
is 80.86 for k = 6, which is significantly (p-value
< 0.01) better than the baseline 78.57, and gives
an ERR of 11.32%. We also experimented with
other values of k in both training and test sets (also
increasing k only in the test set), but we could not
improve over our best result. This suggests that
outperforming the baseline (which in our case is
the state of the art) is rather difficult.
16
In this respect, we also investigated the im-
pact of traditional ranking methods based on fea-
ture vectors, and compared it with our TK-based
model. Table 5 shows the 5-best reranking accu-
racy for different feature subsets. The Basic fea-
tures (Section 5.2) alone do not significantly im-
prove over the Baseline. The only relevant fea-
tures are the probability and the rank of each hy-
pothesis, which condense all the information of
the local model (TKs models always used them).
Similarly, adding the relations as bag-of-
relations in the vector (Rel. feat.) does not pro-
vide any gain, whereas the relations encoded in
the tree fragments (Tree) gives improvement. This
shows the importance of using structural depen-
dencies for reranking discourse parses.
Finally, Table 6 shows that if we use our
sentence-level reranker in the document-level
parser of Joty et al. (2013), the accuracy of the lat-
ter increases from 55.8% to 57.3%, which is a sig-
nificant improvement (p < 0.01), and establishes
a new state-of-the-art for document-level parsing.
6.5 Error Analysis
We looked at some examples where our reranker
failed to identify the best DT. Unsurprisingly, it
16
The human agreement on sentence-level parsing is 83%.
2057
Standard test set 5-folds (average)
k=1 k=2 k=3 k=4 k=5 k=6 k=1 k=2 k=3 k=4 k=5 k=6
RR 79.77 81.08 81.56 82.15 82.15 82.11 78.57 79.76 80.28 80.68 80.80 80.86
ERR - 6.48 8.85 11.76 11.76 11.57 - 5.88 8.45 10.43 11.02 11.32
OR 79.77 84.42 86.55 87.68 88.09 88.75 78.57 83.20 85.13 86.49 87.35 88.03
Table 4: Reranking performance (RR) in comparison with oracle (OR) accuracy for different values of k on the standard
testset and 5-folds of RST-DT. Second row shows the relative error rate reduction (ERR).
happens many times for small DTs containing
only two or three EDUs, especially when the re-
lations are semantically similar. Figure 9 presents
such a case, where the reranker fails to rank the
DT with Summary ahead of the DT with Elabo-
ration. Although we understand that the reranker
lacks enough structural context to distinguish the
two relations in this example, we expected that in-
cluding the lexical items (e.g., (CFD)) in our DT
representation could help. However, similar short
parenthesized texts are also used to elaborate as
in Senate Majority Leader George Mitchell (D.,
Maine), where the text (D., Maine) (i.e., Democrat
from state Maine) elaborates its preceding text.
This confuses our reranker. We also found er-
ror examples where the reranker failed to distin-
guish between Background and Elaboration, and
between Cause and Elaboration. This suggests
that we need rich semantic representation of the
text to improve our reranker further.
7 Related Work
Early work on discourse parsing applied hand-
coded rules based on discourse cues and surface
patterns (Marcu, 2000a). Supervised learning was
first attempted by Marcu (2000b) to build a shift-
reduce discourse parser. This work was then con-
siderably improved by Soricut and Marcu (2003).
They presented probabilistic generative models for
sentence-level discourse parsing based on lexico-
syntactic patterns. Sporleder and Lapata (2005)
investigated the necessity of syntax in discourse
analysis. More recently, Hernault et al. (2010)
presented the HILDA discourse parser that itera-
tively employs two SVM classifiers in pipeline to
build a DT in a greedy way. Feng and Hirst (2012)
improved the HILDA parser by incorporating rich
linguistic features, which include lexical seman-
tics and discourse production rules.
Joty et al. (2013) achieved the best prior results
by (i) jointly modeling the structure and the la-
bel of a DT constituent, (ii) performing optimal
rather than greedy decoding, and (iii) discriminat-
ing between intra- and multi-sentential discourse
parsing. However, their model does not con-
Same-UnitSummary
begins trading today.On the Big Board, Crawford & Co., Atlanta, (CFD)
Elaboration
Figure 9: An error made by our reranker.
sider long range dependencies between DT con-
stituents, which are encoded by our kernels. Re-
garding the latter, our work is surely inspired by
(Collins and Duffy, 2002), which uses TK for syn-
tactic parsing reranking or in general discrimina-
tive reranking, e.g., (Collins and Koo, 2005; Char-
niak and Johnson, 2005; Dinarelli et al., 2011).
However, such excellent studies do not regard
discourse parsing, and in absolute they achieved
lower improvements than our methods.
8 Conclusions and Future Work
In this paper, we have presented a discriminative
approach for reranking discourse trees generated
by an existing discourse parser. Our reranker uses
tree kernels in SVM preference ranking frame-
work to effectively capture the long range struc-
tural dependencies between the constituents of a
discourse tree. We have shown the reranking per-
formance for sentence-level discourse parsing us-
ing the standard tree kernels (i.e., STK and PTK)
on two different representations (i.e., JRN and
SRN) of the discourse tree, and compare it with
the traditional feature vector-based approach. Our
results show that: (i) the reranker improves only
when it considers subtree features computed by
the tree kernels, (ii) SRN is a better representation
than JRN, (iii) STK performs better than PTK for
reranking discourse trees, and (iv) our best result
outperforms the state-of-the-art significantly.
In the future, we would like to apply our
reranker to the document-level parses. However,
this will require a better hypotheses generator.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
2058
References
Lynn Carlson and Daniel Marcu. 2001. Discourse Tag-
ging Reference Manual. Technical Report ISI-TR-
545, University of Southern California Information
Sciences Institute.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002. RST Discourse Treebank (RST-
DT) LDC2002T07. Linguistic Data Consortium,
Philadelphia.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL?05, pages 173?180, NJ, USA. ACL.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Comput.
Linguist., 31(1):25?70, March.
Corinna Cortes and Vladimir Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273?297.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2011. Discriminative Reranking for
Spoken Language Understanding. IEEE Transac-
tions on Audio, Speech and Language Processing
(TASLP), 20:526539.
Vanessa Feng and Graeme Hirst. 2012. Text-level Dis-
course Parsing with Rich Linguistic Features. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?12,
pages 60?68, Jeju Island, Korea. ACL.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1?33.
Liang Huang and David Chiang. 2005. Better K-
best Parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Thorsten Joachims and Chun-Nam John Yu. 2009.
Sparse Kernel SVMs via Cutting-Plane Training.
Machine Learning, 76(2-3):179?193. ECML.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. In Advances in Kernel Methods
- Support Vector Learning.
Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea. ACL.
Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, Sofia, Bulgaria. ACL.
William Mann and Sandra Thompson. 1988. Rhetor-
ical Structure Theory: Toward a Functional Theory
of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 2000a. The Rhetorical Parsing of Un-
restricted Texts: A Surface-based Approach. Com-
putational Linguistics, 26:395?448.
Daniel Marcu. 2000b. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press,
Cambridge, MA, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic Role Labeling via Tree Ker-
nel Joint Inference. In Proceedings of the Tenth
Conference on Computational Natural Language
Learning (CoNLL-X), pages 61?68, New York City,
June. Association for Computational Linguistics.
Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In 17th European Conference on Machine
Learning, pages 318?329. Springer.
Alessandro Moschitti. 2010. Kernel Engineering for
Fast and Easy Design of Natural Language Applica-
tions. In COLING (Tutorials), pages 1?91.
Alessandro Moschitti. 2012. State-of-the-Art Kernels
for Natural Language Processing. In Tutorial Ab-
stracts of ACL 2012, page 2, Jeju Island, Korea, July.
Association for Computational Linguistics.
Alessandro Moschitti. 2013. Kernel-based Learning
to Rank with Syntactic and Semantic Structures. In
SIGIR, page 1128.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse Engineering of Tree Kernel Feature Spaces. In
EMNLP, pages 111?120.
Daniele Pighin and Alessandro Moschitti. 2010. On
Reverse Feature Engineering of Syntactic Tree Ker-
nels. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 223?233, Uppsala, Sweden, July. Association
for Computational Linguistics.
Aliaksei Severyn and Alessandro Moschitti. 2011.
Fast Support Vector Machines for Structural Ker-
nels. In ECML/PKDD (3), pages 175?190.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Fast Support Vector Machines for Convolution Tree
Kernels. Data Min. Knowl. Discov., 25(2):325?357.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
2059
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL?03, pages 149?156,
Edmonton, Canada. ACL.
Caroline Sporleder and Mirella Lapata. 2005. Dis-
course Chunking and its Application to Sentence
Compression. In Proceedings of the conference
on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT-
EMNLP?05, pages 257?264, Vancouver, British
Columbia, Canada. ACL.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic Conditional Ran-
dom Fields: Factorized Probabilistic Models for La-
beling and Segmenting Sequence Data. Journal of
Machine Learning Research (JMLR), 8:693?723.
2060
Proceedings of NAACL-HLT 2013, pages 179?189,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Towards Topic Labeling with Phrase Entailment and Aggregation
Yashar Mehdad Giuseppe Carenini Raymond T. Ng Shafiq Joty
Department of Computer Science, University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{mehdad, carenini, rng, rjoty}@cs.ubc.ca
Abstract
We propose a novel framework for topic la-
beling that assigns the most representative
phrases for a given set of sentences cover-
ing the same topic. We build an entailment
graph over phrases that are extracted from the
sentences, and use the entailment relations to
identify and select the most relevant phrases.
We then aggregate those selected phrases by
means of phrase generalization and merging.
We motivate our approach by applying over
conversational data, and show that our frame-
work improves performance significantly over
baseline algorithms.
1 Introduction
Given text segments about the same topic written in
different ways (i.e., language variability), topic la-
beling deals with the problem of automatically gen-
erating semantically meaningful labels for those text
segments. The potential of integrating topic label-
ing as a prerequisite for higher-level analysis has
been reported in several areas, such as summariza-
tion (Harabagiu and Lacatusu, 2010; Kleinbauer et
al., 2007; Dias et al, 2007), information extraction
(Allan, 2002) and conversation visualization (Liu
et al, 2012). Moreover, the huge amount of tex-
tual data generated everyday specifically in conver-
sations (e.g., emails and blogs) calls for automated
methods to analyze and re-organize them into mean-
ingful coherent clusters.
Table 1 shows an example of two human written
topic labels for a topic cluster collected from a blog1,
1http://slashdot.org
Text: a: Where do you think the term ?Horse laugh? comes
from?
b: And that rats also giggled when tickled.
c: My hypothesis- if an animal can play, it can ?laugh? or at
least it is familiar with the concept of ?laughing?.
Many animals play. There are various sorts of humour though.
Some involve you laughing because your brain suddenly made
a lots of unexpected connections.
Possible extracted phrases: animals play, rats have, laugh,
Horse laugh, rats also giggle, rats
Human-authored topic labels: animals which laugh, animal
laughter
Table 1: Topic labeling example.
and possible phrases that can be extracted from the
topic cluster using different approaches. This ex-
ample demonstrates that although most approaches
(Mei et al, 2007; Lau et al, 2011; Branavan et al,
2007) advocate extracting phrase-level topic labels
from the text segments, topically related text seg-
ments do not always contain one keyword or key
phrase that can capture the meaning of the topic. As
shown in this example, such labels do not exist in the
original text and cannot be extracted using the exist-
ing probabilistic models (e.g., (Mei et al, 2007)).
The same problem can be observed with many other
examples. This suggests the idea of aggregating and
generating topic labels, instead of simply extracting
them, as a challenging scenario for this field of re-
search.
Moreover, to generate a label for a topic we have
to be able to capture the overall meaning of a topic.
However, most current methods disregard semantic
relations, in favor of statistical models of word dis-
tributions and frequencies. This calls for the integra-
179
tion of semantic models for topic labeling.
Towards the solution of the mentioned problems,
in this paper we focus on two novel contributions:
1. Phrase aggregation. We propose to generate
topic labels using the extracted information by pro-
ducing the most representative phrases for each text
segment. We perform this task in two steps. First,
we generalize some lexically diverse concepts in
the extracted phrases. Second, we aggregate and
generate new phrases that can semantically imply
more than one original extracted phrase. For ex-
ample, the phrase ?rats also giggle? and ?horse
laugh? should be merged into a new phrase ?animals
laugh?. Although our method is still relying on ex-
tracting phrases, we move beyond current extractive
approaches, by generating new phrases through gen-
eralization and aggregation of the extracted ones.
2. Building a multidirectional entailment graph
over the extracted phrases to identify and select the
relevant information. We set such problem as an
application-oriented variant of the Textual Entail-
ment (TE) recognition task (Dagan and Glickman,
2004), to identify the information that are seman-
tically equivalent, novel, or more informative with
respect to the content of the others. In this way, we
prune the redundant and less informative text por-
tions (e.g., phrases), and produce semantically in-
formed phrases for the generation phase. In the case
of the example in Table 1, we eliminate phrases such
as ?rats have?, ?rats? and ?laugh? while keeping
?animal play?, ?Horse laugh? and ?rats also gig-
gle?.
The experimental results over conversational data
sets show that, in all cases, our approach outper-
forms other models significantly. Although conver-
sational data are known to be challenging (Carenini
et al, 2011), we choose to test our method on con-
versations because this is a genre in which topic
modeling is critically needed, as conversations lack
the structure and organization of, for instance, edited
monologues. The results indicate that our frame-
work is sufficiently robust to deal with topic labeling
in less structured, informal genres (when compared
with edited monologues). As an additional result of
our experiments, we show that the identification and
selection phase using semantic relations (entailment
graph) is a necessary step to perform the final step
(i.e., the phrase aggregation).
2 Topic Labeling Framework
Each topic cluster contains the sentences that can
semantically represent a topic. The task of cluster-
ing the sentences into a set of coherent topic clus-
ters is called topic segmentation (Joty et al, 2011),
which is out of the scope of this paper. Our goal is to
generate an understandable label (i.e., a sequence of
words) that could capture the semantic of the topic,
and distinguish a topic from other topics (based on
definition of a good topic label by (Mei et al, 2007)),
given a set of topic clusters. Among possible choices
of word sequences as topic labels, in order to balance
the granularity, we set phrases as valid topic labels.
Extract all
Filter/select
Entailment
Identify
Phrase extraction Entailment Graph
Generalize
Merge
Phraseaggregation? ? ?- -
1Figure 1: Topic labeling framework.
As shown in Figure 1, our framework consists of
three main components that we describe in more de-
tails in the following sections.
2.1 Phrase extraction
We tokenize and preprocess each cluster in the col-
lection of topic clusters with lemmas, stems, part-of-
speech tags, sense tags and chunks. We also extract
n-grams up to length 5 which do not start or end with
a stop word. In this phase, we do not include any
frequency count feature in our candidate extraction
pipeline. Once we have built the candidates pool,
the next step is to identify a subset containing the
most significant of those candidates. Since most top
systems in key phrase extraction use supervised ap-
proaches, we follow the same method (Kim et al,
2010b; Medelyan et al, 2008; Frank et al, 1999).
Initially, we consider a set of features used in the
other systems to determine whether a phrase is likely
to be a key phrase. However, since our dataset is
conversational (more details in Section 3), and the
text segments are not long, we aim for a classifier
with high recall. Thus, we only use TFxIDF (Salton
and McGill, 1986), position of the first occurrence
(Frank et al, 1999) and phrase length as our fea-
tures. We merge the training and test data released
180
for SemEval-2010 Task #5 (Kim et al, 2010b),
which consists of 244 scientific articles and 3705
key phrases, to train a Naive Bayes classifier in or-
der to learn a supervised model. We then apply our
model to extract the candidate phrases from the col-
lected candidates pool.
As a further step, to increase the coverage (re-
call) of our extracted phrases and to reduce the num-
ber of very short phrases (frequent keywords), we
choose the chunks containing any of the extracted
keywords. We add those chunks to our extracted
phrases and eliminate the associated keywords.
2.2 Entailment graph
So far, we have extracted a pool of key phrases from
each topic cluster. Many such phrases include re-
dundant information which are semantically equiv-
alent but vary in lexical choices. By identifying the
semantic relations between the phrases we can dis-
cover the information in one phrase that is seman-
tically equivalent, novel, or more/less informative
with respect to the content of the other phrase.
We set this problem as a variant of the Textual
Entailment (TE) recognition task (Mehdad et al,
2010b; Adler et al, 2012; Berant et al, 2011). We
build an entailment graph for each topic cluster,
where nodes are the extracted phrases and edges are
the entailment relations between nodes. Given two
phrases (ph1 and ph2), we aim at identifying and
handling the following cases:
i) ph1 and ph2 express the same meaning (bidirec-
tional entailment). In such cases one of the phrases
should be eliminated;
ii) ph1 is more informative than ph2 (unidirectional
entailment). In such cases, the entailing phrase
should replace or complement the entailed one;
iii) ph1 contains facts that are not present in ph2,
and vice-versa (the ?unknown? cases in TE par-
lance). In such cases, both phrases should remain.
Figure 2 shows how entailment relations can help
in selecting the phrases by removing the redun-
dant and less informative ones. For example, the
phrase ?animals laugh? entails ?rats giggle?, ?Horse
laugh? and ?Mice chuckle?,2 but not ?Animals play?.
2Assuming that ?animals laugh? is interpreted as ?all ani-
mals laugh?.
rats
giggle
Horse
laugh
laugh
rats
Mice
chuckle
animals
laugh
Animals
playx
x
1
Figure 2: Building an entailment graph over phrases. Ar-
rows and ?x? represent the entailment direction and un-
known cases respectively.
So we can keep ?animals laugh? and ?Animals play?
and eliminate others. In this way, TE-based phrase
identification method can be designed to distinguish
meaning-preserving variations from true divergence,
regardless of lexical choices and structures.
Similar to previous approaches in TE (e.g., (Be-
rant et al, 2011; Mehdad et al, 2010b; Mehdad et
al., 2010a)), we use supervised method. To train and
build the entailment graph, we perform the follow-
ing three steps.
2.2.1 Training set collection
In the last few years, TE corpora have been cre-
ated and distributed in the framework of several
evaluation campaigns, including the Recognizing
Textual Entailment (RTE) Challenge3 and Cross-
lingual textual entailment for content synchroniza-
tion4 (Negri et al, 2012). However, such datasets
cannot directly support our application. Specifi-
cally, our entailment graph is built over the extracted
phrases (with max. length of 5 tokens per phrase),
while the RTE datasets are composed of longer sen-
tences and paragraphs (Bentivogli et al, 2009; Negri
et al, 2011).
In order to collect a dataset which is more similar
to the goal of our entailment framework, we decide
to select a subset of the sixth and seventh RTE chal-
lenge main task (i.e., RTE within a Corpus). Our
3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
4http://www.cs.york.ac.uk/semeval-2013/task8/
181
dataset choice is based on the following reasons: i)
the length of sentence pairs in RTE6 and RTE7 is
shorter than the others, and ii) RTE6 and RTE7 main
task datasets are originally created for summariza-
tion purpose which is closer to our work. We sort
the RTE6 and RTE7 dataset pairs based on the sen-
tence length and choose the first 2000 samples with
a equal number of positive and negative examples.
The average length of words in our training data is
6.7 words. There are certainly some differences be-
tween our training set and our phrases. However, the
collected training samples was the closest available
dataset to our purpose.
2.2.2 Feature representation and training
Working at the phrase level imposes another con-
straint. Phrases are short and in terms of syntactic
structure, they are not as rich as sentences. This lim-
its our features to the lexical level. Lexical mod-
els, on the other hand, are less computationally ex-
pensive and easier to implement and often deliver a
strong performance for RTE (Sammons et al, 2011).
Our entailment decision criterion is based on
similarity scores calculated with a phrase-to-phrase
matching process. Each example pair of phrases
(ph1 and ph2) is represented by a feature vector,
where each feature is a specific similarity score esti-
mating whether ph1 entails ph2.
We compute 18 similarity scores for each pair of
phrases. In order to adapt the similarity scores to the
entailment score, we normalize the similarity scores
by the length of ph2 (in terms of lexical items), when
checking the entailment direction from ph1 to ph2.
In this way, we can check the portion of informa-
tion/facts in ph2 which is covered by ph1.
The first 5 scores are computed based on the exact
lexical overlap between the phrases: word overlap,
edit distance, ngram-overlap, longest common sub-
sequence and Lesk (Lesk, 1986). The other scores
were computed using lexical resources: Word-
Net (Fellbaum, 1998), VerbOcean (Chklovski and
Pantel, 2004), paraphrases (Denkowski and Lavie,
2010) and phrase matching (Mehdad et al, 2011).
We used WordNet to compute the word similarity
as the least common subsumer between two words
considering the synonymy-antonymy, hypernymy-
hyponymy, and meronymy relations. Then, we cal-
culated the sentence similarity as the sum of the sim-
ilarity scores of the word pairs in Text and Hypothe-
sis, normalized by the number of words in Hypothe-
sis. We also use phrase matching features described
in (Mehdad et al, 2011) which consists of phrasal
matching at the level on ngrams (1 to 5 grams). The
rationale behind using different entailment features
is that combining various scores will yield a better
model (Berant et al, 2011).
To combine the entailment scores and optimize
their relative weights, we train a Support Vector Ma-
chine binary classifier, SVMlight (Joachims, 1999),
over an equal number of positive and negative exam-
ples. This results in an entailment model with 95%
accuracy over 2-fold and 5-fold cross-validation,
which further proves the effectiveness of our fea-
ture set for this lexical entailment model. The reason
that we gained a very high accuracy is because our
selected sentences are a subset of RTE6 and RTE7
with a shorter length (less number of words) which
makes the entailment recognition task much easier
than recognizing entailment between paragraphs or
complex long sentences.
2.2.3 Graph edge labeling
We set the edge labeling problem as a two-way
classification task. Two-way classification casts
multidirectional entailment as a unidirectional prob-
lem, where each pair is analyzed checking for en-
tailment in both directions (Mehdad et al, 2012). In
this condition, each original test example is correctly
classified if both pairs originated from it are cor-
rectly judged (?YES-YES? for bidirectional,?YES-
NO? and ?NO-YES? for unidirectional entailment
and ?NO-NO? for unknown cases). Two-way clas-
sification represents an intuitive solution to capture
multidimensional entailment relations. Moreover,
since our training examples are labeled with binary
judgments, we are not able to train a three-way clas-
sifier.
2.2.4 Identification and selection
Assigning all entailment relations between the ex-
tracted phrase pairs, we are aiming at identifying
relevant phrases and eliminating the redundant (in
terms of meaning) and less informative ones. In or-
der to perform this task we follow a set of rules based
on the graph edge labels. Note that since entailment
182
# Merging patterns
1 merge ( cw11(CPOS=[N|V |J]) ..w1n , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n and w22..w2n
E.g. merge ( challenging situation , challenging problem ) = challenging situation and problem
2 merge ( w11..cw1n(CPOS=[N|V |J]) , w21..cw2n(CPOS=[N|V |J]) ) = w11..w1n?1 and w21..w2n
E.g. merge ( wet Mars , warm Mars ) = wet and warm Mars
3 merge ( w11..cw1n(CPOS=[N|V |J]) , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n w22..w2n
E.g. merge ( interesting story , story continues ) = interesting story continues
4 merge ( cw11(CPOS=[N|V |J]) ..w1n , w21..cw2n(CPOS=[N|V |J]) ) = w21..w2n w12..w1n
E.g. merge ( LHC shutting down , details about LHC ) = details about LHC shutting down
5 merge ( w11Cpos , cw12(CPOS=[N|V |J]) , w13Cpos , w21Cpos , cw22(CPOS=[N|V |J]) , w23Cpos ) = w11 and w21 w22 w23
and w13
E.g. merge ( technology grow fast , media grow exponentially ) = technology and media grow exponentially and fast
Table 2: Phrase merging patterns.
is a transitive relation, our entailment graph is transi-
tive i.e., if entail(ph1,ph2) and entail(ph2,ph3) then
entail(ph1,ph3) (Berant et al, 2011).
Rule 1) If there is a chain of entailing nodes, we
keep the one which is in the root of the chain and
eliminate others (e.g. ?animals laugh? in Figure 2);
Rule 2) Among the nodes that are connected
with bidirectional entailment (semantically equiva-
lent nodes) we keep only the one with more outgoing
bidirectional and unidirectional entailment relations,
respectively;
Rule 3) Among the nodes that are connected with
unknown entailment (novel information with respect
to others) we keep the ones with no incoming entail-
ment relation (e.g., ?Animals play? in Figure 2).
Although deleting might be harsh, in our current
framework, we only rely on the performance of an
entailment model which gives us a yes/no entailment
decision. In future, we are planning to improve our
entailment graph by weighting the edges. In this
way, we can take advantage of the weights to make
a more conservative decision in pruning the entail-
ment chains.
2.3 Phrase aggregation
Once we have identified and selected the informa-
tive phrases, the generation of topic labels can be
done in two steps. First, we generalize the phrases
containing the concepts that are lexically connected.
Second, we merge the phrases with a set of hand
written linguistically motivated patterns.
2.3.1 Phrase generalization
In this step, we generalize phrases that contain
concepts which are lexically connected. For this
purpose, we search in phrases for different words
with the same part-of-speech and sense tag. Then,
we find the link between those words in WordNet. If
they are connected and the shortest path connecting
them is less than 3 (estimated over the development
set), we replace both by their common parent in the
WordNet. In the case that they belong to the same
synset, we can replace one by another. Note that we
limit our search to nouns and verbs. For example,
?rat? and ?horse? can be replaced by ?animal?, or
?giggle? and ?chuckle? can be replaced by ?laugh?.
The motivation behind the generalization step is to
enrich the common terms between the phrases in fa-
vor of increasing the chance that they could merge
to a single phrase. This also helps to move beyond
the limitation of original lexical choices.
2.3.2 Phrase merging
The goal is to merge the phrases that are con-
nected, and to generate a human readable phrase that
contains more information than a single extracted
phrase. Several approaches have been proposed to
aggregate and merge sentences in Natural Language
Generation (NLG) (e.g. (Barzilay and Lapata, 2006;
Cheng and Mellish, 2000)), however most of them
use syntactic structure of the sentences. To merge
phrases at the lexical level, we set few common lin-
guistically motivated aggregation patterns such as:
simple conjunction, and conjunction via shared par-
ticipants (Reiter and Dale, 2000).
Table 2 demonstrates the merging patterns, where
wij is the jth word (or segment) in phrase i, cw
is the common word (or segment) in both phrases
and CPOS is the common part-of-speech tag of
the corresponding word. To illustrate, pattern 1
183
looks for the first segment of each phrase (wi1).
If they are same (cwi1) and share the same POS
tag (CPOS), then we aggregate the first phrase
(w11..w1n) and the second phrase removing the first
element (w22..w2n) by using the connective ?and?.
For instance, the aggregation of ?animals laugh? and
?animals play? results in ?animals laugh and play?.
The rest of the patterns follow the same logic and for
the sake of brevity we avoid illustrating each pattern.
These patterns are among the most common domain
and application independent methods by which two
phrases/sentences can be aggregated, as described in
the NLG literature (Reiter and Dale, 2000).
In our aggregation pipeline, we group the phrases
based on their lexical overlap (number of common
words). The merging process is conducted over each
group in descending order (larger number of words
in common), in order to increase the chance of merg-
ing rules application. Then, we perform the merg-
ing over the resulting generated phrases from each
group. If our phrases cannot be merged (i.e., do not
match merging patterns), we select them as labels
for the topic cluster.
3 Datasets and Evaluation Metrics
3.1 Datasets
To verify the effectiveness of our approach, we ex-
periment with two different conversational datasets.
Our interest in dealing with conversational texts de-
rives from two reasons. First, the huge amount of
textual data generated everyday in these conversa-
tions validates the need of text analysis frameworks
to process such conversational texts effectively. Sec-
ond, conversational texts pose challenges to the tra-
ditional techniques, including redundancies, disflu-
encies, higher language variabilities and ill-formed
sentence structure (Liu et al, 2011).
Our conversational datasets are from two differ-
ent asynchronous media: email and blog. For email,
we use the dataset presented in (Joty et al, 2010),
where three individuals annotated the publicly avail-
able BC3 email corpus (Ulrich et al, 2008) with top-
ics. The corpus contains 40 email threads (or conver-
sations) at an average of 5 emails per thread. On av-
erage it has 26.3 sentences and 2.5 topics per thread.
A topic has an average length of 12.6 sentences. In
total, the three annotators found 269 topics in a cor-
pus of 1,024 sentences.
There are no publicly available blog corpora an-
notated with topics. For this study, we build our
own blog corpus containing 20 blog conversations of
various lengths from Slashdot, each annotated with
topics by three human annotators.5 The number of
comments per conversation varies from 30 to 101
with an average of 60.3 and the number of sentences
per conversation varies from 105 to 430 with an av-
erage of 220.6. The annotators first read a conversa-
tion and list the topics discussed in the conversation
by a short description (e.g., Game contents or size,
Bugs or faults) which provides a high-level overview
of the topic. Then, they assign the most appropriate
topic to each sentence in the conversation. The short
high-level descriptions of the topics serve as refer-
ence (or gold) topic labels in our experiments. The
target number of topics was not given in advance and
the annotators were instructed to find as many topics
as needed to convey the overall content structure of
the conversation. The annotators found 5 to 23 top-
ics per conversation with an average of 10.77. The
number of sentences per topic varies from 11.7 to
61.2 with an average of 27.16. In total, the three
annotators found 512 topics in our blog corpus con-
taining 4,411 sentences overall.
Note that our annotators performed topic segmen-
tation and labeling independently. In the email cor-
pus, the three annotators found 100, 77 and 92 top-
ics respectively (269 in total), and in the blog corpus,
they found 251, 119 and 192 topics respectively (562
in total). For the evaluation, there is a single gold
standard per topic written by each annotator. Table
1 shows a case in which two annotators selected the
same topical cluster and so we have two labels for
the same cluster.
3.2 Evaluation metrics
Traditionally, key phrase extraction is evaluated us-
ing precision, recall and f-measure based on exact
matches on all the extracted key phrases with gold
standards for a given text. However, as claimed
by (Kim et al, 2010a), this approach is not flexible
enough as it ignores the near-misses. Moreover, in
the case of topic labeling, most of the human written
5The new blog corpus annotated with topics will be made
publicly available for research purposes.
184
topic labels cannot be found in the text. Recently,
(Kim et al, 2010a) evaluated the utility of differ-
ent n-gram-based metrics for key phrase extraction
and showed that the metric R-precision correlates
most with human judgments. R-precision normal-
izes the approximate matching score by the maxi-
mum number of words in the reference and candi-
date phrases. Since this penalize our aggregation
phase, where the phrases tend to be longer than orig-
inal extracted phrase, we decide to use R-f1 as our
evaluation metric which considers length of both ref-
erence and candidate phrases.
R?precision = 1k
k?
i=1
overlap(candi, ref)
#words(candi)
R?recall = 1k
k?
i=1
overlap(candi, ref)
#words(ref)
R?f1 = 2 ?R?precision ?R?recall(R?precision + R?recall)
The metric described above only considers word
overlap and ignores other semantic relations (e.g.,
synonymy, hypernymy) between words. However,
annotators write labels of their own and may use
words that are not directly from the conversation but
are semantically related. Therefore, we propose to
also use another variant of R-f1 that incorporates se-
mantic relation between words. To calculate the Se-
mantic R-f1, we count the number of overlaps not
only when they have the same form, but also when
they are connected in WordNet with a synonymy,
hypernymy, hyponymy and entailment relation.
Its worth noting that the generalizations phase and
the evaluation method are completely independent.
In the generalization step, we try to generalize the
phrases which are automatically extracted from the
text segments. While, in the evaluation, we compare
the human written gold standards with the system
output. Therefore, using WordNet in the generaliza-
tion step does not bias the results in the evaluation.
4 Experiments and Results
4.1 Experimental settings
We conduct our experiments over the blog and email
datasets described in Section 3.1, after eliminating
the development set from the test datasets. In our ex-
periments, the development set was used for the pat-
tern extraction and the shortest path threshold con-
necting the words in Wordnet in the generalization
phase. Our test dataset consists of 461 topics (i.e.,
clusters and their associated topic labels) from 20
blog conversations and 242 topics from 40 email
conversations.
For preprocessing our dataset we use OpenNLP6
for tokenization, part-of-speech tagging and chunck-
ing. For sense disambiguation, we use the extended
gloss overlap measure with the window size of 5,
developed by (Pedersen et al, 2005). We also apply
Snowball algorithm (Porter, 2001) for stemming.
We compare our approach with two strong base-
lines. The first baseline Freq-BL ranks the words
according to their frequencies and select the top 5
candidates applying Maximum Marginal Relevance
algorithm (Carbonell and Goldstein, 1998) using
the same pre- and post-processing as the work by
(Mihalcea and Tarau, 2004). The second baseline
Lead-BL, ranks the words based on their relevance
to the leading sentences.7 The ranking criteria is
log(tfw,Lt + 1)? log(tfw,t + 1), where tfw,Lt and
tfw,t are the number of times word w appears in a
set of leading sentences Lt and topic cluster t, re-
spectively (Allan, 2002). The log expressions, as the
ranking criterion, assign more weights to the words
in the topic segment, that also appear in the leading
sentences. This is because topics tend to be intro-
duced in the first few sentences of a topical cluster.
We also measure the performance of our framework
at each step in order to compare the effectiveness of
each phase independently or in combination.
4.2 Results
We evaluate the performance of different models us-
ing the metrics R-f1 and Semantic R-f1 (Sem-R-f1),
described in Section 3.2. Table 4 shows the results
in percentage for different models. The results show
that our framework outperforms the baselines signif-
6http://opennlp.sourceforge.net/
7The key intuitions for this baseline is the leading sentences
of a topic cluster carry the most informative clues for the topic
labels. Based on our development set, when we consider the
first three sentences, the coverage of content words that appear
in human labeled topics are 39% and 49% for blog and email,
respectively.
185
Blog Email
Human-authored system generated Human-authored system generated
Shutting down the LHC story about the LHC shutting down (#3) How it affects coding it screws my coding
typical shutdown and upgrade times typical and scheduled shutdown (#2) Opinions and preferences of tools opinion about what tools
MARS was warm and wet 3B years ago Mars was warm and wet early history (#3) white on black for disabled users white text on black background (#3)
Moon Treaty and outer space treaty Moon and Outer Space Treaty (#2) Contact with Steven email to Steven Pemberton (#3)
Table 3: Successful examples of human-authored and system generated labels for blog and email datasets. The number
near some examples refers to the aggregation patterns in Table 2.
Models
R-f1 Sem-R-f1
blog email blog email
Lead-BL 13.5 14.0 34.5 30.1
Freq-BL 15.3 13.1 34.7 29.1
Extraction-BL 13.9 16.0 31.6 33.2
Entailment 12.2 15.6 30.8 33.3
Extraction+Aggregation 15.1 18.5 35.5 37.6
Extraction+Entailment+
Aggregation 17.9 20.4 38.7 41.6
Table 4: Results for candidate topic labels on blog and
email corpora.
icantly8 in both datasets.
On the blog corpus, our key phrase extraction
method (Extraction-BL) fails to beat the other base-
lines (Lead-BL and Freq-BL) in majority of cases
(except R-f1 for Lead-BL). However, in the email
dataset, it improves the performance over both base-
lines in both evaluation metrics. This might be due
to the shorter topic clusters (in terms of number of
sentences) in email corpus which causes a smaller
number of phrases to be extracted.
We also observe the effectiveness of the aggre-
gation phase. In all cases, there is a significant
improvement (p < 0.05) after applying the ag-
gregation phase over the extracted phrases (Extrac-
tion+Aggregation).
Note that there is no improvement over the ex-
traction phase after the entailment (Entailment row).
This is mainly due to the fact that the entailment
phase filters the equivalent phrases. This affects the
results negatively when such filtered phrases share
many common words with our human-authored
phrases. However, the results improve more sig-
nificantly (p < 0.01) when the aggregation is con-
ducted after the entailment. This demonstrates that,
the combination of these two steps are beneficial for
topic labeling over conversational datasets.
In addition, the differences between the results us-
8The statistical significance tests was calculated by approx-
imate randomization described in (Yeh, 2000).
ing R-f1 and Sem-R-f1 metrics suggests the need for
more flexible automatic evaluation methods for this
task. Moreover, although the same trend of improve-
ment is observed in blog and email corpora, the dif-
ferences between their performance suggest the in-
vestigation of specialized methods for various con-
versational modalities.
0 100 200 300 400 500 600
0
0.2
0.4
0.6
0.8
1
Blog
Se
m-
R-
f1
Ext
Ext+Ent
Ext+Agg
Ext+Ent+Agg
1
Figure 3: Sem-R-f1 results distribution after each phase
of our pipeline for blog corpus. The x-axis represents the
examples sorted based on their Sem-R-f1 score.
To further analyze the performance, in Figure 3,
we show the Sem-R-f1 results distribution for our
blog dataset.9 We can observe that the aggrega-
tion after the entailment phase (bold curve) clearly
increase the number of correct labels, while such
improvement can be only achieved when the en-
tailment relations is used to identify the relevant
phrases. This further highlights the need of seman-
tics in this task. Comparing both datasets, this ef-
fect is more dominant in blogs. We believe that this
is due to the length of topic clusters. Presumably,
building an entailment graph over a greater pool of
9For brevity?s sake we do not show the email dataset graph.
186
original phrases is more effective to filter the redun-
dant information and identify the relevant phrases.
5 Discussion
After analyzing the results and through manual veri-
fication of some cases, we observe that our approach
led to some interestingly successful examples. Table
3 shows few generated labels and the human written
topics for such cases.
In general, given that the results are expressed in
percentage, it appears that the performance is still
far from satisfactory level. This leaves an interesting
challenge for the research community to tackle.
However, this is not always due to the weakness
of our proposed model. We have identified three
different system independent sources of error:10
Type 1: Abstractive human-authored labels: the
nature of our method is based on extraction (with
the exception of our simple generalization phase)
and in many cases the human-written labels cannot
be extracted from the text and require more complex
generalizations. In fact, only 9.81% of the labels
in blog and 12.74% of the labels in email appear
verbatim in their respective conversations. For
example:
Human-authored label: meeting schedule and location
Generated phrases: meeting, Boston area, mid October
Type 2: Evaluation methods: in this work, we
proposed a semantic method to evaluate our system.
However, the current evaluation methods fail to
capture the meaning. For example:
Human-authored label: Food choices
Generated phrase: I would ask what people want to eat
Type 3: Subjective topic labels: often is not easy
for human to agree on one label for a topic cluster.11
For example:
Human-authored label 1: Member introduction
Human-authored label 2: Bio of Len
Generated phrases: own intro, Len Kasday, chair
In light of this analysis, we conclude that a more
comprehensive evaluation method (e.g., human eval-
uation) could better reveal the potential of our sys-
10There are many examples of such cases, however for
brevity we just mention one example for each type.
11The mean R-precision agreements computed based on one-
to-one mappings of the topic clusters are 20.22 and 36.84 on
blog and email data sets, respectively.
tem in dealing with topic labeling, specially on con-
versational data.
6 Conclusion
In this paper, we study the problem of automatic
topic labeling, and propose a novel framework to la-
bel topic clusters with meaningful readable phrases.
Within such framework, this paper makes two main
contributions. First, in contrast with most current
methods based on fully extractive models, we pro-
pose to aggregate topic labels by means of gener-
alizing and merging techniques. Second, beyond
current approaches which disregard semantic infor-
mation, we integrate semantics by means of build-
ing textual entailment graphs over the topic clusters.
To achieve our objectives, we successfully applied
our framework over two challenging conversational
datasets. Coherent results on both datasets demon-
strate the potential of our approach in dealing with
topic labeling task.
Future work will address both the improvement of
our aggregation phase and ranking the output candi-
date phrases for each topic cluster. On one hand,
we plan to accommodate more sophisticated NLG
techniques for the aggregation and generation phase.
Incorporating a better source of prior knowledge in
the generalization phase (e.g., YAGO or DBpedia) is
also an interesting research direction towards a bet-
ter phrase aggregation step. On the other hand, we
plan to apply a ranking strategy to select the top can-
didate phrases generated by our framework.
Acknowledgments
We would like to thank the anonymous reviewers
and Frank Tompa for their valuable comments and
suggestions to improve the paper, and the NSERC
Business Intelligence Network for financial support.
Yashar Mehdad also would like to acknowledge the
early discussions on the related topics with Matteo
Negri.
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application to
the health-care domain. In Proceedings of the ACL
2012 System Demonstrations, ACL ?12, pages 79?84,
187
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Allan. 2002. Topic detection and tracking: event-
based information organization.
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, HLT-NAACL ?06, pages 359?366, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In In
Proc Text Analysis Conference (TAC09.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
SRK Branavan, Pawan Deshpande, and Regina Barzilay.
2007. Generating a table-of-contents. In ACL, vol-
ume 45, page 544.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?98, pages 335?336, New York, NY, USA.
ACM.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for mining and summarizing text con-
versations.
Hua Cheng and Chris Mellish. 2000. Capturing the in-
teraction between aggregation and text planning in two
generation systems. In In Proceedings of the 1st In-
ternational Natural Language Generation Conference,
186193, Mitzpe.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 33?40, Barcelona, Spain,
July. Association for Computational Linguistics.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: improved evaluation
support for five target languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 339?342,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Gae?l Dias, Elsa Alves, and Jose? Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text summa-
rization and passage retrieval: an exhaustive evalua-
tion. In Proceedings of the 22nd national conference
on Artificial intelligence - Volume 2, AAAI?07, pages
1334?1339. AAAI Press.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceedings
of the Sixteenth International Joint Conference on Ar-
tificial Intelligence, IJCAI ?99, pages 668?673, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Sanda Harabagiu and Finley Lacatusu. 2010. Us-
ing topic themes for multi-document summarization.
ACM Trans. Inf. Syst., 28(3):13:1?13:47, July.
T. Joachims. 1999. Making large-scale svm learning
practical. LS8-Report 24, Universita?t Dortmund, LS
VIII-Report.
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and
Raymond T. Ng. 2010. Exploiting conversation struc-
ture in unsupervised topic segmentation for emails.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Shafiq Joty, Gabriel Murray, and Raymond T. Ng. 2011.
Supervised topic segmentation of email conversations.
In In ICWSM11. AAAI.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2010a. Evaluating n-gram based evaluation metrics
for automatic keyphrase extraction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 572?580, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010b. Semeval-2010 task 5: Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, SemEval ?10, pages 21?26, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Thomas Kleinbauer, Stephanie Becker, and Tilman
Becker. 2007. Combining multiple information lay-
ers for the automatic generation of indicative meeting
abstracts. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, ENLG
?07, pages 151?154, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jey Han Lau, Karl Grieser, David Newman, and Timothy
Baldwin. 2011. Automatic labelling of topic models.
In ACL, pages 1536?1545.
188
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, pages 24?26, New York, NY,
USA. ACM.
F. Liu, Y. Liu, C. Busso, S. Harabagiu, and V. Ng. 2011.
Identifying the Gist of Conversational Text: Automatic
Keyword Extraction and Summarization. Ph.D. thesis,
THE UNIVERSITY OF TEXAS AT DALLAS.
Shixia Liu, Michelle X. Zhou, Shimei Pan, Yangqiu
Song, Weihong Qian, Weijia Cai, and Xiaoxiao Lian.
2012. Tiara: Interactive, topic-based visual text sum-
marization and analysis. ACM Trans. Intell. Syst.
Technol., 3(2):25:1?25:28, February.
O. Medelyan, I.H. Witten, and D. Milne. 2008. Topic
indexing with wikipedia. In Proceedings of the AAAI
WikiAI workshop.
Y. Mehdad, A. Moschitti, and F.M. Zanzotto. 2010a.
Syntactic semantic structures for textual entailment
recognition. Association for Computational Linguis-
tics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010b. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 321?324, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ?12, pages 120?124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, KDD ?07, pages 490?499, New York, NY,
USA. ACM.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of EMNLP-04and
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, July.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: cross-lingual textual entailment
for content synchronization. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the main con-
ference and the shared task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, SemEval ?12, pages 399?407, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. Pedersen, S. Banerjee, and S. Patwardhan. 2005.
Maximizing Semantic Relatedness to Perform Word
Sense Disambiguation. Research Report UMSI
2005/25, University of Minnesota Supercomputing In-
stitute, March.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems.
Gerard Salton and Michael J. McGill. 1986. Introduction
to modern information retrieval.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth.
2011. Recognizing Textual Entailment. In Daniel M.
Bikel and Imed Zitouni, editors, Multilingual Natu-
ral Language Applications: From Theory to Practice.
Prentice Hall, Jun.
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for su-
pervised email summarization.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics -
Volume 2, COLING ?00, pages 947?953, Stroudsburg,
PA, USA. Association for Computational Linguistics.
189
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 486?496,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combining Intra- and Multi-sentential Rhetorical Parsing for
Document-level Discourse Analysis
Shafiq Joty?
sjoty@qf.org.qa
Qatar Computing Research Institute
Qatar Foundation
Doha, Qatar
Giuseppe Carenini, Raymond Ng, Yashar Mehdad
{carenini, rng, mehdad}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, Canada
Abstract
We propose a novel approach for develop-
ing a two-stage document-level discourse
parser. Our parser builds a discourse tree
by applying an optimal parsing algorithm
to probabilities inferred from two Con-
ditional Random Fields: one for intra-
sentential parsing and the other for multi-
sentential parsing. We present two ap-
proaches to combine these two stages of
discourse parsing effectively. A set of
empirical evaluations over two different
datasets demonstrates that our discourse
parser significantly outperforms the state-
of-the-art, often by a wide margin.
1 Introduction
Discourse of any kind is not formed by inde-
pendent and isolated textual units, but by related
and structured units. Discourse analysis seeks
to uncover such structures underneath the surface
of the text, and has been shown to be benefi-
cial for text summarization (Louis et al, 2010;
Marcu, 2000b), sentence compression (Sporleder
and Lapata, 2005), text generation (Prasad et al,
2005), sentiment analysis (Somasundaran, 2010)
and question answering (Verberne et al, 2007).
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988), one of the most influential the-
ories of discourse, represents texts by labeled hier-
archical structures, called Discourse Trees (DTs),
as exemplified by a sample DT in Figure 1. The
leaves of a DT correspond to contiguous Elemen-
tary Discourse Units (EDUs) (six in the exam-
ple). Adjacent EDUs are connected by rhetori-
cal relations (e.g., Elaboration, Contrast), form-
ing larger discourse units (represented by internal
?This work was conducted at the University of British
Columbia, Vancouver, Canada.
nodes), which in turn are also subject to this re-
lation linking. Discourse units linked by a rhetori-
cal relation are further distinguished based on their
relative importance in the text: nucleus being the
central part, whereas satellite being the peripheral
one. Discourse analysis in RST involves two sub-
tasks: discourse segmentation is the task of identi-
fying the EDUs, and discourse parsing is the task
of linking the discourse units into a labeled tree.
While recent advances in automatic discourse
segmentation and sentence-level discourse parsing
have attained accuracies close to human perfor-
mance (Fisher and Roark, 2007; Joty et al, 2012),
discourse parsing at the document-level still poses
significant challenges (Feng and Hirst, 2012) and
the performance of the existing document-level
parsers (Hernault et al, 2010; Subba and Di-
Eugenio, 2009) is still considerably inferior com-
pared to human gold-standard. This paper aims
to reduce this performance gap and take discourse
parsing one step further. To this end, we address
three key limitations of existing parsers as follows.
First, existing discourse parsers typically model
the structure and the labels of a DT separately
in a pipeline fashion, and also do not consider
the sequential dependencies between the DT con-
stituents, which has been recently shown to be crit-
ical (Feng and Hirst, 2012). To address this limi-
tation, as the first contribution, we propose a novel
document-level discourse parser based on proba-
bilistic discriminative parsing models, represented
as Conditional Random Fields (CRFs) (Sutton et
al., 2007), to infer the probability of all possible
DT constituents. The CRF models effectively rep-
resent the structure and the label of a DT con-
stituent jointly, and whenever possible, capture the
sequential dependencies between the constituents.
Second, existing parsers apply greedy and sub-
optimal parsing algorithms to build the DT for a
document. To cope with this limitation, our CRF
models support a probabilistic bottom-up parsing
486
But he added:
"Some people use the purchasers?
 index as a leading indicator, some use it as a coincident indicator. But the thing it?s supposed to measure -- manufacturing strength --
it missed altogether last month." <P>Elaboration
Same-UnitContrast
Contrast
Attribution
(1)
(2) (3)
(4) (5)
(6)
Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The
second sentence has a well-formed discourse tree, but the first sentence does not have one.
algorithm which is non-greedy and optimal.
Third, existing discourse parsers do not dis-
criminate between intra-sentential (i.e., building
the DTs for the individual sentences) and multi-
sentential parsing (i.e., building the DT for the
document). However, we argue that distinguish-
ing between these two conditions can result in
more effective parsing. Two separate parsing
models could exploit the fact that rhetorical re-
lations are distributed differently intra-sententially
vs. multi-sententially. Also, they could indepen-
dently choose their own informative features. As
another key contribution of our work, we devise
two different parsing components: one for intra-
sentential parsing, the other for multi-sentential
parsing. This provides for scalable, modular and
flexible solutions, that can exploit the strong cor-
relation observed between the text structure (sen-
tence boundaries) and the structure of the DT.
In order to develop a complete and robust dis-
course parser, we combine our intra-sentential
and multi-sentential parsers in two different ways.
Since most sentences have a well-formed dis-
course sub-tree in the full document-level DT (for
example, the second sentence in Figure 1), our first
approach constructs a DT for every sentence us-
ing our intra-sentential parser, and then runs the
multi-sentential parser on the resulting sentence-
level DTs. However, this approach would disre-
gard those cases where rhetorical structures vio-
late sentence boundaries. For example, consider
the first sentence in Figure 1. It does not have a
well-formed sub-tree because the unit containing
EDUs 2 and 3 merges with the next sentence and
only then is the resulting unit merged with EDU
1. Our second approach, in an attempt of dealing
with these cases, builds sentence-level sub-trees
by applying the intra-sentential parser on a sliding
window covering two adjacent sentences and by
then consolidating the results produced by over-
lapping windows. After that, the multi-sentential
parser takes all these sentence-level sub-trees and
builds a full rhetorical parse for the document.
While previous approaches have been tested on
only one corpus, we evaluate our approach on
texts from two very different genres: news articles
and instructional how-to-do manuals. The results
demonstrate that our contributions provide con-
sistent and statistically significant improvements
over previous approaches. Our final result com-
pares very favorably to the result of state-of-the-art
models in document-level discourse parsing.
In the rest of the paper, after discussing related
work in Section 2, we present our discourse pars-
ing framework in Section 3. In Section 4, we de-
scribe the intra- and multi-sentential parsing com-
ponents. Section 5 presents the two approaches
to combine the two stages of parsing. The exper-
iments and error analysis, followed by future di-
rections are discussed in Section 6. Finally, we
summarize our contributions in Section 7.
2 Related work
The idea of staging document-level discourse
parsing on top of sentence-level discourse parsing
was investigated in (Marcu, 2000a; LeThanh et al,
2004). These approaches mainly rely on discourse
markers (or cues), and use hand-coded rules to
build DTs for sentences first, then for paragraphs,
and so on. However, often rhetorical relations
are not explicitly signaled by discourse markers
(Marcu and Echihabi, 2002), and discourse struc-
tures do not always correspond to paragraph struc-
tures (Sporleder and Lascarides, 2004). Therefore,
rather than relying on hand-coded rules based on
discourse markers, recent approaches employ su-
pervised machine learning techniques with a large
set of informative features.
Hernault et al, (2010) presents the publicly
available HILDA parser. Given the EDUs in a doc-
487
Elaboration Joint AttributionSame-Unit Contrast Explanation
0
5
10
15
20
25
30 Multi-sentential
Intra-sentential
Figure 2: Distributions of six most frequent relations in
intra-sentential and multi-sentential parsing scenarios.
ument, HILDA iteratively employs two Support
Vector Machine (SVM) classifiers in pipeline to
build the DT. In each iteration, a binary classifier
first decides which of the adjacent units to merge,
then a multi-class classifier connects the selected
units with an appropriate relation label. They eval-
uate their approach on the RST-DT corpus (Carl-
son et al, 2002) of news articles. On a different
genre of instructional texts, Subba and Di-Eugenio
(2009) propose a shift-reduce parser that relies on
a classifier for relation labeling. Their classifier
uses Inductive Logic Programming (ILP) to learn
first-order logic rules from a set of features includ-
ing compositional semantics. In this work, we ad-
dress the limitations of these models (described in
Section 1) introducing our novel discourse parser.
3 Our Discourse Parsing Framework
Given a document with sentences already seg-
mented into EDUs, the discourse parsing prob-
lem is determining which discourse units (EDUs
or larger units) to relate (i.e., the structure), and
how to relate them (i.e., the labels or the discourse
relations) in the resulting DT. Since we already
have an accurate sentence-level discourse parser
(Joty et al, 2012), a straightforward approach to
document-level parsing could be to simply apply
this parser to the whole document. However this
strategy would be problematic because of scalabil-
ity and modeling issues. Note that the number of
valid trees grows exponentially with the number
of EDUs in a document.1 Therefore, an exhaus-
tive search over the valid trees is often unfeasible,
even for relatively small documents.
For modeling, the problem is two-fold. On the
one hand, it appears that rhetorical relations are
distributed differently intra-sententially vs. multi-
sententially. For example, Figure 2 shows a com-
parison between the two distributions of six most
1For n + 1 EDUs, the number of valid discourse trees is
actually the Catalan number Cn.
model
AlgorithmSentences segmented into EDUs
Document-level
 discourse tree
Intra-sententialparser Multi-sententialparser
model
Algorithm
Figure 3: Discourse parsing framework.
frequent relations on a development set containing
20 randomly selected documents from RST-DT.
Notice that relations Attribution and Same-Unit
are more frequent than Joint in intra-sentential
case, whereas Joint is more frequent than the other
two in multi-sentential case. On the other hand,
different kinds of features are applicable and in-
formative for intra-sentential vs. multi-sentential
parsing. For example, syntactic features like dom-
inance sets (Soricut and Marcu, 2003) are ex-
tremely useful for sentence-level parsing, but are
not even applicable in multi-sentential case. Like-
wise, lexical chain features (Sporleder and Las-
carides, 2004), that are useful for multi-sentential
parsing, are not applicable at the sentence level.
Based on these observations, our discourse
parsing framework comprises two separate mod-
ules: an intra-sentential parser and a multi-
sentential parser (Figure 3). First, the intra-
sentential parser produces one or more discourse
sub-trees for each sentence. Then, the multi-
sentential parser generates a full DT for the doc-
ument from these sub-trees. Both of our parsers
have the same two components: a parsing model
assigns a probability to every possible DT, and
a parsing algorithm identifies the most probable
DT among the candidate DTs in that scenario.
While the two models are rather different, the
same parsing algorithm is shared by the two mod-
ules. Staging multi-sentential parsing on top of
intra-sentential parsing in this way allows us to ex-
ploit the strong correlation between the text struc-
ture and the DT structure as explained in detail in
Section 5. Before describing our parsing models
and the parsing algorithm, we introduce some ter-
minology that we will use throughout the paper.
Following (Joty et al, 2012), a DT can be for-
mally represented as a set of constituents of the
form R[i,m, j], referring to a rhetorical relation
R between the discourse unit containing EDUs i
through m and the unit containing EDUs m+1
through j. For example, the DT for the sec-
ond sentence in Figure 1 can be represented as
488
{Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}.
Notice that a relation R also specifies the nuclear-
ity statuses of the discourse units involved, which
can be one of Nucleus-Satellite (NS), Satellite-
Nucleus (SN) and Nucleus-Nucleus (NN).
4 Parsing Models and Parsing Algorithm
The job of our intra-sentential and multi-sentential
parsing models is to assign a probability to each
of the constituents of all possible DTs at the sen-
tence level and at the document level, respectively.
Formally, given the model parameters ?, for each
possible constituent R[i,m, j] in a candidate DT
at the sentence or document level, the parsing
model estimates P (R[i,m, j]|?), which specifies
a joint distribution over the label R and the struc-
ture [i,m, j] of the constituent.
4.1 Intra-Sentential Parsing Model
Recently, we proposed a novel parsing model
for sentence-level discourse parsing (Joty et
al., 2012), that outperforms previous approaches
by effectively modeling sequential dependencies
along with structure and labels jointly. Below we
briefly describe the parsing model, and show how
it is applied to obtain the probabilities of all possi-
ble DT constituents at the sentence level.
Figure 4 shows the intra-sentential parsing
model expressed as a Dynamic Conditional Ran-
dom Field (DCRF) (Sutton et al, 2007). The ob-
served nodes Uj in a sequence represent the dis-
course units (EDUs or larger units). The first layer
of hidden nodes are the structure nodes, where
Sj?{0, 1} denotes whether two adjacent discourse
units Uj?1 and Uj should be connected or not.
The second layer of hidden nodes are the relation
nodes, with Rj?{1 . . .M} denoting the relation
between two adjacent unitsUj?1 andUj , whereM
is the total number of relations in the relation set.
The connections between adjacent nodes in a hid-
den layer encode sequential dependencies between
the respective hidden nodes, and can enforce con-
straints such as the fact that a Sj= 1 must not fol-
low a Sj?1= 1. The connections between the two
hidden layers model the structure and the relation
of a DT (sentence-level) constituent jointly.
To obtain the probability of the constituents
of all candidate DTs for a sentence, we apply
the parsing model recursively at different levels
of the DT and compute the posterior marginals
over the relation-structure pairs. To illustrate the
U UU U U
2
2
2
3 j t-1 t
SS S S S
R R R R R
3
3 j
j t-1
t-1 t
Unit sequenceat level i
Structure sequence
Relationsequence
U1
t
Figure 4: A chain-structured DCRF as our intra-
sentential parsing model.
process, let us assume that the sentence contains
four EDUs. At the first (bottom) level, when all
the units are the EDUs, there is only one possible
unit sequence to which we apply our DCRF
model (Figure 5(a)). We compute the posterior
marginals P (R2, S2=1|e1, e2, e3, e4,?), P (R3,
S3=1|e1, e2, e3, e4,?) and P (R4, S4=1|e1, e2, e3,
e4,?) to obtain the probability of the con-
stituents R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4],
respectively. At the second level, there are
three possible unit sequences (e1:2, e3, e4),
(e1,e2:3, e4) and (e1,e2,e3:4). Figure 5(b) shows
their corresponding DCRFs. The posterior
marginals P (R3, S3=1|e1:2,e3,e4,?), P (R2:3
S2:3=1|e1,e2:3,e4,?), P (R4, S4=1|e1,e2:3,e4,?)
and P (R3:4, S3:4=1|e1,e2,e3:4,?) computed from
the three sequences correspond to the probability
of the constituents R[1, 2, 3], R[1, 1, 3], R[2, 3, 4]
and R[2, 2, 4], respectively. Similarly, we attain
the probability of the constituents R[1, 1, 4],
R[1, 2, 4] and R[1, 3, 4] by computing their
respective posterior marginals from the three
possible sequences at the third (top) level.
e 1 e e2
2
2
3
S S3
R R3
(a)
e 1e
S
R
1:2 3
3
3
e e
S
R
2:3
2:3
(b)
2:3e4
S4
R4
e4
S4
R4
e4
S4
R4
1e e
S
R
2
2
2 e3:4
S3:4
R3:4
1 e
S
R
1:3 4
4
4
e e
S
R
2:4
2:4
(c)
2:4 ee
S
R
1:2e
3:4
3:4
3:4
(i) (ii)
(iii)(i) (ii) (iii)
Figure 5: Our parsing model applied to the sequences at
different levels of a sentence-level DT. (a) Only possible se-
quence at the first level, (b) Three possible sequences at the
second level, (c) Three possible sequences at the third level.
At this point what is left to be explained is
how we generate all possible sequences for a
given number of EDUs in a sentence. Algorithm
1 demonstrates how we do that. More specifi-
cally, to compute the probabilities of each DT con-
489
stituent R[i, k, j], we need to generate sequences
like (e1, ? ? ? , ei?1, ei:k, ek+1:j , ej+1, ? ? ? , en) for
1 ? i ? k < j ? n. In doing so, we may
generate some duplicate sequences. Clearly, the
sequence (e1, ? ? ? , ei?1, ei:i, ei+1:j , ej+1, ? ? ? , en)
for 1 ? i ? k < j < n is already considered
for computing the probability of R[i+ 1, j, j+ 1].
Therefore, it is a duplicate sequence that we ex-
clude from our list of all possible sequences.
Input: Sequence of EDUs: (e1, e2, ? ? ? , en)
Output: List of sequences: L
for i = 1? n? 1 do
for j = i+ 1? n do
if j == n then
for k = i? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
else
for k = i+ 1? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
end
end
end
Algorithm 1: Generating all possible sequences
for a sentence with n EDUs.
Once we obtain the probability of all possible
DT constituents, the discourse sub-trees for the
sentences are built by applying an optimal prob-
abilistic parsing algorithm (Section 4.4) using one
of the methods described in Section 5.
4.2 Multi-Sentential Parsing Model
Given the discourse units (sub-trees) for all the
sentences of a document, a simple approach to
build the rhetorical tree of the document would be
to apply a new DCRF model, similar to the one
in Figure 4 (with different parameters), to all the
possible sequences generated from these units to
infer the probability of all possible higher-order
constituents. However, the number of possible se-
quences and their length increase with the number
of sentences in a document. For example, assum-
ing that each sentence has a well-formed DT, for
a document with n sentences, Algorithm 1 gener-
ates O(n3) sequences, where the sequence at the
bottom level has n units, each of the sequences at
the second level has n-1 units, and so on. Since
the model in Figure 4 has a ?fat? chain structure,
U Ut-1 t
S
R t
Adjacent Unitsat level i
Structure
Relation
t
Figure 6: A CRF as a multi-sentential parsing model.
we could use forwards-backwards algorithm for
exact inference in this model (Sutton and McCal-
lum, 2012). However, forwards-backwards on a
sequence containing T units costs O(TM2) time,
where M is the number of relations in our rela-
tion set. This makes the chain-structured DCRF
model impractical for multi-sentential parsing of
long documents, since learning requires to run in-
ference on every training sequence with an overall
time complexity of O(TM2n3) per document.
Our model for multi-sentential parsing is shown
in Figure 6. The two observed nodes Ut?1 and
Ut are two adjacent discourse units. The (hidden)
structure node S?{0, 1} denotes whether the two
units should be connected or not. The hidden node
R?{1 . . .M} represents the relation between the
two units. Notice that like the previous model, this
is also an undirected graphical model. It becomes
a CRF if we directly model the hidden (output)
variables by conditioning its clique potential (or
factor) ? on the observed (input) variables:
P (Rt, St|x,?) = 1Z(x,?)?(Rt, St|x,?) (1)
where x represents input features extracted from
the observed variables Ut?1 and Ut, and Z(x,?)
is the partition function. We use a log-linear rep-
resentation of the factor:
?(Rt, St|x,?) = exp(?T f(Rt, St, x)) (2)
where f(Rt, St, x) is a feature vector derived from
the input features x and the labels Rt and St, and
? is the corresponding weight vector. Although,
this model is similar in spirit to the model in Fig-
ure 4, we now break the chain structure, which
makes the inference much faster (i.e., complex-
ity of O(M2)). Breaking the chain structure also
allows us to balance the data for training (equal
number instances with S=1 and S=0), which dra-
matically reduces the learning time of the model.
We apply our model to all possible adjacent
units at all levels for the multi-sentential case, and
490
compute the posterior marginals of the relation-
structure pairs P (Rt, St=1|Ut?1, Ut,?) to obtain
the probability of all possible DT constituents.
4.3 Features Used in our Parsing Models
Table 1 summarizes the features used in our pars-
ing models, which are extracted from two adjacent
unitsUt?1 andUt. Since most of these features are
adopted from previous studies (Joty et al, 2012;
Hernault et al, 2010), we briefly describe them.
Organizational features include the length of
the units as the number of EDUs and tokens.
It also includes the distances of the units from
the beginning and end of the sentence (or text in
the multi-sentential case). Text structural fea-
tures indirectly capture the correlation between
text structure and rhetorical structure by counting
the number of sentence and paragraph boundaries
in the units. Discourse markers (e.g., because, al-
though) carry informative clues for rhetorical re-
lations (Marcu, 2000a). Rather than using a fixed
list of discourse markers, we use an empirically
learned lexical N-gram dictionary following (Joty
et al, 2012). This approach has been shown to
be more robust and flexible across domains (Bi-
ran and Rambow, 2011; Hernault et al, 2010). We
also include part-of-speech (POS) tags for the be-
ginning and end N tokens in a unit.
8 Organizational features Intra & Multi-Sentential
Number of EDUs in unit 1 (or unit 2).
Number of tokens in unit 1 (or unit 2).
Distance of unit 1 in EDUs to the beginning (or to the end).
Distance of unit 2 in EDUs to the beginning (or to the end).
4 Text structural features Multi-Sentential
Number of sentences in unit 1 (or unit 2).
Number of paragraphs in unit 1 (or unit 2).
8 N-gram features N?{1, 2, 3} Intra & Multi-Sentential
Beginning (or end) lexical N-grams in unit 1.
Beginning (or end) lexical N-grams in unit 2.
Beginning (or end) POS N-grams in unit 1.
Beginning (or end) POS N-grams in unit 2.
5 Dominance set features Intra-Sentential
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two units.
8 Lexical chain features Multi-Sentential
Number of chains start in unit 1 and end in unit 2.
Number of chains start (or end) in unit 1 (or in unit 2).
Number of chains skipping both unit 1 and unit 2.
Number of chains skipping unit 1 (or unit 2).
2 Contextual features Intra & Multi-Sentential
Previous and next feature vectors.
2 Substructure features Intra & Multi-Sentential
Root nodes of the left and right rhetorical sub-trees.
Table 1: Features used in our parsing models.
Lexico-syntactic features dominance sets
(Soricut and Marcu, 2003) are very effective for
intra-sentential parsing. We include syntactic
labels and lexical heads of head and attachment
nodes along with their dominance relationship
as features. Lexical chains (Morris and Hirst,
1991) are sequences of semantically related words
that can indicate topic shifts. Features extracted
from lexical chains have been shown to be useful
for finding paragraph-level discourse structure
(Sporleder and Lascarides, 2004). We compute
lexical chains for a document following the ap-
proach proposed in (Galley and McKeown, 2003),
that extracts lexical chains after performing word
sense disambiguation. Following (Joty et al,
2012), we also encode contextual and rhetorical
sub-structure features in our models. The rhetori-
cal sub-structure features incorporate hierarchical
dependencies between DT constituents.
4.4 Parsing Algorithm
Given the probability of all possible DT con-
stituents in the intra-sentential and multi-sentential
scenarios, the job of the parsing algorithm is to
find the most probable DT for that scenario. Fol-
lowing (Joty et al, 2012), we implement a prob-
abilistic CKY-like bottom-up algorithm for com-
puting the most likely parse using dynamic pro-
gramming. Specifically, with n discourse units,
we use the upper-triangular portion of the n?n
dynamic programming table D. Given Ux(0) and
Ux(1) are the start and end EDU Ids of unit Ux:
D[i, j] = P (R[Ui(0), Uk(1), Uj(1)]) (3)
where, k = argmax
i?p?j
P (R[Ui(0), Up(1), Uj(1)]).
Note that, in contrast to previous studies on
document-level parsing (Hernault et al, 2010;
Subba and Di-Eugenio, 2009; Marcu, 2000b),
which use a greedy algorithm, our approach finds
a discourse tree that is globally optimal.
5 Document-level Parsing Approaches
Now that we have presented our intra-sentential
and our multi-sentential parsers, we are ready to
describe how they can be effectively combined to
perform document-level discourse analysis. Re-
call that a key motivation for a two-stage parsing is
that it allows us to capture the correlation between
text structure and discourse structure in a scalable,
modular and flexible way. Below we describe two
different approaches to model this correlation.
491
5.1 1S-1S (1 Sentence-1 Sub-tree)
A key finding from several previous studies on
sentence-level discourse analysis is that most sen-
tences have a well-formed discourse sub-tree in
the full document-level DT (Joty et al, 2012;
Fisher and Roark, 2007). For example, Figure 7(a)
shows 10 EDUs in 3 sentences (see boxes), where
the DTs for the sentences obey their respective
sentence boundaries. The 1S-1S approach aims to
maximally exploit this finding. It first constructs
a DT for every sentence using our intra-sentential
parser, and then it provides our multi-sentential
parser with the sentence-level DTs to build the
rhetorical parse for the whole document.
1     2  3S 1
8  9   10S 34   5      6   7S 2
1    2   3S 1
8   9    10S 34   5    6    7S 2(a) (b)
???
Figure 7: Two possible DTs for three sentences.
5.2 Sliding Window
While the assumption made by 1S-1S clearly sim-
plifies the parsing process, it totally ignores the
cases where discourse structures violate sentence
boundaries. For example, in the DT shown in Fig-
ure 7(b), sentence S2 does not have a well-formed
sub-tree because some of its units attach to the
left (4-5, 6) and some to the right (7). Vliet and
Redeker (2011) call these cases as ?leaky? bound-
aries. Even though less than 5% of the sentences
have leaky boundaries in RST-DT, in other corpora
this can be true for a larger portion of the sen-
tences. For example, we observe over 12% sen-
tences with leaky boundaries in the Instructional
corpus of (Subba and Di-Eugenio, 2009). How-
ever, we notice that in most cases where discourse
structures violate sentence boundaries, its units are
merged with the units of its adjacent sentences, as
in Figure 7(b). For example, this is true for 75%
cases in our development set containing 20 news
articles from RST-DT and for 79% cases in our
development set containing 20 how-to-do manuals
from the Instructional corpus. Based on this obser-
vation, we propose a sliding window approach.
In this approach, our intra-sentential parser
works with a window of two consecutive sen-
tences, and builds a DT for the two sentences. For
example, given the three sentences in Figure 7, our
intra-sentential parser constructs a DT for S1-S2
and a DT for S2-S3. In this process, each sentence
in a document except the first and the last will be
associated with two DTs: one with the previous
sentence (say DTp) and one with the next (say
DTn). In other words, for each non-boundary sen-
tence, we will have two decisions: one from DTp
and one from DTn. Our parser consolidates the
two decisions and generates one or more sub-trees
for each sentence by checking the following three
mutually exclusive conditions one after another:
? Same in both: If the sentence has the same (in
terms of both structure and labels) well-formed
sub-tree in both DTp and DTn, we take this sub-
tree for the sentence. For example, in Figure 8(a),
S2 has the same sub-tree in the two DTs, i.e. a DT
for S1-S2 and a DT for S2-S3. The two decisions
agree on the DT for the sentence.
? Different but no cross: If the sentence has a
well-formed sub-tree in both DTp and DTn, but
the two sub-trees vary either in structure or in la-
bels, we pick the most probable one. For example,
consider the DT for S1-S2 in Figure 8(a) and the
DT for S2-S3 in Figure 8(b). In both cases S2 has
a well-formed sub-tree, but they differ in structure.
We pick the sub-tree which has the higher proba-
bility in the two dynamic programming tables.
1     2  3S1 8  9   10S34   5      6   7S2
1    2   3S1 8   9    10S3
4   5    6    7S2
(a)
(c)
8  9   10S 34   5    6     7S2 (b)
4   5    6    7S2(i) (ii)
4   5      6   7S2
Figure 8: Extracting sub-trees for S2.
? Cross: If either or both of DTp and DTn seg-
ment the sentence into multiple sub-trees, we pick
the one with more sub-trees. For example, con-
sider the two DTs in Figure 8(c). In the DT for
S1-S2, S2 has three sub-trees (4-5,6,7), whereas
in the DT for S2-S3, it has two (4-6,7). So, we ex-
tract the three sub-trees for S2 from the first DT. If
the sentence has the same number of sub-trees in
both DTp and DTn, we pick the one with higher
probability in the dynamic programming tables.
At the end, the multi-sentential parser takes all
these sentence-level sub-trees for a document, and
builds a full rhetorical parse for the document.
492
6 Experiments
6.1 Corpora
While previous studies on document-level parsing
only report their results on a particular corpus, to
show the generality of our method, we experiment
with texts from two very different genres. Our
first corpus is the standard RST-DT (Carlson et
al., 2002), which consists of 385 Wall Street Jour-
nal articles, and is partitioned into a training set
of 347 documents and a test set of 38 documents.
53 documents, selected from both sets were anno-
tated by two annotators, based on which we mea-
sure human agreement. In RST-DT, the original 25
rhetorical relations defined by (Mann and Thomp-
son, 1988) are further divided into a set of 18
coarser relation classes with 78 finer-grained rela-
tions. Our second corpus is the Instructional cor-
pus prepared by (Subba and Di-Eugenio, 2009),
which contains 176 how-to-do manuals on home-
repair. The corpus was annotated with 26 informa-
tional relations (e.g., Preparation-Act, Act-Goal).
6.2 Experimental Setup
We experiment with our discourse parser on the
two datasets using our two different parsing ap-
proaches, namely 1S-1S and the sliding window.
We compare our approach with HILDA (Hernault
et al, 2010) on RST-DT, and with the ILP-based
approach of (Subba and Di-Eugenio, 2009) on the
Instructional corpus, since they are the state-of-
the-art on the respective genres. On RST-DT, the
standard split was used for training and testing
purposes. The results for HILDA were obtained
by running the system with default settings on the
same inputs we provided to our system. Since we
could not run the ILP-based system of (Subba and
Di-Eugenio, 2009) (not publicly available) on the
Instructional corpus, we report the performances
presented in their paper. They used 151 documents
for training and 25 documents for testing. Since
we did not have access to their particular split,
we took 5 random samples of 151 documents for
training and 25 documents for testing, and report
the average performance over the 5 test sets.
To evaluate the parsing performance, we use
the standard unlabeled (i.e., hierarchical spans)
and labeled (i.e., nuclearity and relation) preci-
sion, recall and F-score as described in (Marcu,
2000b). To compare with previous studies, our
experiments on RST-DT use the 18 coarser rela-
tions. After attaching the nuclearity statuses (NS,
SN, NN) to these relations, we get 41 distinct re-
lations. Following (Subba and Di-Eugenio, 2009)
on the Instructional corpus, we use 26 relations,
and treat the reversals of non-commutative rela-
tions as separate relations. That is, Goal-Act and
Act-Goal are considered as two different relations.
Attaching the nuclearity statuses to these relations
gives 76 distinct relations. Analogous to previous
studies, we map the n-ary relations (e.g., Joint)
into nested right-branching binary relations.
6.3 Results and Error Analysis
Table 2 presents F-score parsing results for our
parsers and the existing systems on the two cor-
pora.2 On both corpora, our parser, namely, 1S-1S
(TSP 1-1) and sliding window (TSP SW), outper-
form existing systems by a wide margin (p<7.1e-
05).3 On RST-DT, our parsers achieve absolute
F-score improvements of 8%, 9.4% and 11.4%
in span, nuclearity and relation, respectively, over
HILDA. This represents relative error reductions
of 32%, 23% and 21% in span, nuclearity and rela-
tion, respectively. Our results are also close to the
upper bound, i.e. human agreement on this corpus.
On the Instructional genre, our parsers deliver
absolute F-score improvements of 10.5%, 13.6%
and 8.14% in span, nuclearity and relations, re-
spectively, over the ILP-based approach. Our
parsers, therefore, reduce errors by 36%, 27% and
13% in span, nuclearity and relations, respectively.
If we compare the performance of our parsers
on the two corpora, we observe higher results
on RST-DT. This can be explained in at least
two ways. First, the Instructional corpus has a
smaller amount of data with a larger set of rela-
tions (76 when nuclearity attached). Second, some
frequent relations are (semantically) very similar
(e.g., Preparation-Act, Step1-Step2), which makes
it difficult even for the human annotators to distin-
guish them (Subba and Di-Eugenio, 2009).
Comparison between our two models reveals
that TSP SW significantly outperforms TSP 1-1
only in finding the right structure on both corpora
(p<0.01). Not surprisingly, the improvement is
higher on the Instructional corpus. A likely ex-
planation is that the Instructional corpus contains
more leaky boundaries (12%), allowing the sliding
2Precision, Recall and F-score are the same when manual
segmentation is used (see Marcu, (2000b), page 143).
3Since we did not have access to the output or to the sys-
tem of (Subba and Di-Eugenio, 2009), we were not able to
perform a significance test on the Instructional corpus.
493
RST-DT Instructional
Metrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW
Span 74.68 82.47* 82.74*? 88.70 70.35 79.67 80.88?
Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10
Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58
Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA
(with p<7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by ?.
T-CT-OT-CMM-MCMPEVSUCNDENCATEEXBACOJOS-UATEL
T-C  T-O  T-CM      M-M     CMP  EV  SU     CND  EN     CA  TE    EX     BA   CO      JO   S-U   AT     EL10732111227114121291309359
00010203133350012722
0000100000001018532
 0020121007936757108
0000000302112332000
0001300102901921001
0001320004112121008
000000000070431001
000010001305111006
00000000242210000014
000000800000001000
0000100221010122010
0001010000011110000
000040000000020000
000000000000000000
001000000000000000
020000000000000001
000000000000000000
Figure 9: Confusion matrix for relation labels on the
RST-DT test set. Y-axis represents true and X-axis repre-
sents predicted relations. The relations are Topic-Change
(T-C), Topic-Comment (T-CM), Textual Organization (T-
O), Manner-Means (M-M), Comparison (CMP), Evaluation
(EV), Summary (SU), Condition (CND), Enablement (EN),
Cause (CA), Temporal (TE), Explanation (EX), Background
(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-
tion (AT) and Elaboration (EL).
window approach to be more effective in finding
those, without inducing much noise for the labels.
This clearly demonstrates the potential of TSP SW
for datasets with even more leaky boundaries e.g.,
the Dutch (Vliet and Redeker, 2011) and the Ger-
man Potsdam (Stede, 2004) corpora.
Error analysis reveals that although TSP SW
finds more correct structures, a corresponding im-
provement in labeling relations is not present be-
cause in a few cases, it tends to induce noise from
the neighboring sentences for the labels. For ex-
ample, when parsing was performed on the first
sentence in Figure 1 in isolation using 1S-1S, our
parser rightly identifies the Contrast relation be-
tween EDUs 2 and 3. But, when it is considered
with its neighboring sentences by the sliding win-
dow, the parser labels it as Elaboration. A promis-
ing strategy to deal with this and similar problems
that we plan to explore in future, is to apply both
approaches to each sentence and combine them by
consolidating three probabilistic decisions, i.e. the
one from 1S-1S and the two from sliding window.
To further analyze the errors made by our parser
on the hardest task of relation labeling, Figure 9
presents the confusion matrix for TSP 1-1 on the
RST-DT test set. The relation labels are ordered
according to their frequency in the RST-DT train-
ing set. In general, the errors are produced by two
different causes acting together: (i) imbalanced
distribution of the relations, and (ii) semantic sim-
ilarity between the relations. The most frequent
relation Elaboration tends to mislead others es-
pecially, the ones which are semantically similar
(e.g., Explanation, Background) and less frequent
(e.g., Summary, Evaluation). The relations which
are semantically similar mislead each other (e.g.,
Temporal:Background, Cause:Explanation).
These observations suggest two ways to im-
prove our parser. We would like to employ a more
robust method (e.g., ensemble methods with bag-
ging) to deal with the imbalanced distribution of
relations, along with taking advantage of a richer
semantic knowledge (e.g., compositional seman-
tics) to cope with the errors caused by semantic
similarity between the rhetorical relations.
7 Conclusion
In this paper, we have presented a novel discourse
parser that applies an optimal parsing algorithm
to probabilities inferred from two CRF models:
one for intra-sentential parsing and the other for
multi-sentential parsing. The two models exploit
their own informative feature sets and the distribu-
tional variations of the relations in the two parsing
conditions. We have also presented two novel ap-
proaches to combine them effectively. Empirical
evaluations on two different genres demonstrate
that our approach yields substantial improvement
over existing methods in discourse parsing.
Acknowledgments
We are grateful to Frank Tompa and the anony-
mous reviewers for their comments, and the
NSERC BIN and CGS-D for financial support.
494
References
O. Biran and O. Rambow. 2011. Identifying Justi-
fications in Written Dialogs by Classifying Text as
Argumentative. International Journal of Semantic
Computing, 5(4):363?381.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
V. Feng and G. Hirst. 2012. Text-level Discourse Pars-
ing with Rich Linguistic Features. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, ACL ?12, pages 60?68,
Jeju Island, Korea. Association for Computational
Linguistics.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmen-
tation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, ACL
?07, pages 488?495, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Pro-
ceedings of the 18th International Joint Conference
on Artificial Intelligence, IJCAI ?07, pages 1486?
1488, Acapulco, Mexico.
H. Hernault, H. Prendinger, D. duVerle, and
M. Ishizuka. 2010. HILDA: A Discourse Parser
Using Support Vector Machine Classification. Dia-
logue and Discourse, 1(3):1?33.
S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel
Discriminative Framework for Sentence-Level Dis-
course Analysis. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 904?
915, Jeju Island, Korea. Association for Computa-
tional Linguistics.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating Discourse Structures for Written Texts.
In Proceedings of the 20th international confer-
ence on Computational Linguistics, COLING ?04,
Geneva, Switzerland. Association for Computa-
tional Linguistics.
A. Louis, A. Joshi, and A. Nenkova. 2010. Discourse
Indicators for Content Selection in Summarization.
In Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
SIGDIAL ?10, pages 147?156, Tokyo, Japan. Asso-
ciation for Computational Linguistics.
W. Mann and S. Thompson. 1988. Rhetorical Struc-
ture Theory: Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?02, pages
368?375. Association for Computational Linguis-
tics.
D. Marcu. 2000a. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-based Approach. Compu-
tational Linguistics, 26:395?448.
D. Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press,
Cambridge, MA, USA.
J. Morris and G. Hirst. 1991. Lexical Cohesion
Computed by Thesaural Relations as an Indicator
of Structure of Text. Computational Linguistics,
17(1):21?48.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki,
and B. Webber. 2005. The Penn Discourse Tree-
Bank as a Resource for Natural Language Gener-
ation. In Proceedings of the Corpus Linguistics
Workshop on Using Corpora for Natural Language
Generation, pages 25?32, Birmingham, U.K.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology, NAACL-HLT ?03, pages 149?
156, Edmonton, Canada. Association for Computa-
tional Linguistics.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 257?264, Van-
couver, British Columbia, Canada. Association for
Computational Linguistics.
C. Sporleder and A. Lascarides. 2004. Combining Hi-
erarchical Clustering and Machine Learning to Pre-
dict High-Level Discourse Structure. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Geneva, Switzer-
land. Association for Computational Linguistics.
M. Stede. 2004. The Potsdam Commentary Corpus.
In Proceedings of the ACL-04 Workshop on Dis-
course Annotation, Barcelona. Association for Com-
putational Linguistics.
R. Subba and B. Di-Eugenio. 2009. An Effective Dis-
course Parser that Uses Rich Linguistic Information.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT-NAACL ?09, pages 566?574, Boul-
der, Colorado. Association for Computational Lin-
guistics.
495
C. Sutton and A. McCallum. 2012. An Introduction
to Conditional Random Fields. Foundations and
Trends in Machine Learning, 4(4):267?373.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting
Sequence Data. Journal of Machine Learning Re-
search (JMLR), 8:693?723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 735?736, Amsterdam, The Nether-
lands. ACM.
N. Vliet and G. Redeker. 2011. Complex Sentences as
Leaky Units in Discourse Parsing. In Proceedings
of Constraints in Discourse, Agay-Saint Raphael,
September.
496
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687?698,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Using Discourse Structure Improves Machine Translation Evaluation
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,pnakov}@qf.org.qa
Abstract
We present experiments in using dis-
course structure for improving machine
translation evaluation. We first design
two discourse-aware similarity measures,
which use all-subtree kernels to compare
discourse parse trees in accordance with
the Rhetorical Structure Theory. Then,
we show that these measures can help
improve a number of existing machine
translation evaluation metrics both at the
segment- and at the system-level. Rather
than proposing a single new metric, we
show that discourse information is com-
plementary to the state-of-the-art evalu-
ation metrics, and thus should be taken
into account in the development of future
richer evaluation metrics.
1 Introduction
From its foundations, Statistical Machine Transla-
tion (SMT) had two defining characteristics: first,
translation was modeled as a generative process at
the sentence-level. Second, it was purely statisti-
cal over words or word sequences and made lit-
tle to no use of linguistic information. Although
modern SMT systems have switched to a discrim-
inative log-linear framework, which allows for ad-
ditional sources as features, it is generally hard to
incorporate dependencies beyond a small window
of adjacent words, thus making it difficult to use
linguistically-rich models.
Recently, there have been two promising re-
search directions for improving SMT and its eval-
uation: (a) by using more structured linguistic
information, such as syntax (Galley et al, 2004;
Quirk et al, 2005), hierarchical structures (Chi-
ang, 2005), and semantic roles (Wu and Fung,
2009; Lo et al, 2012), and (b) by going beyond
the sentence-level, e.g., translating at the docu-
ment level (Hardmeier et al, 2012).
Going beyond the sentence-level is important
since sentences rarely stand on their own in a
well-written text. Rather, each sentence follows
smoothly from the ones before it, and leads into
the ones that come afterwards. The logical rela-
tionship between sentences carries important in-
formation that allows the text to express a meaning
as a whole beyond the sum of its separate parts.
Note that sentences can be made of several
clauses, which in turn can be interrelated through
the same logical relations. Thus, in a coherent text,
discourse units (sentences or clauses) are logically
connected: the meaning of a unit relates to that of
the previous and the following units.
Discourse analysis seeks to uncover this coher-
ence structure underneath the text. Several formal
theories of discourse have been proposed to de-
scribe the coherence structure (Mann and Thomp-
son, 1988; Asher and Lascarides, 2003; Webber,
2004). For example, the Rhetorical Structure The-
ory (Mann and Thompson, 1988), or RST, repre-
sents text by labeled hierarchical structures called
Discourse Trees (DTs), which can incorporate sev-
eral layers of other linguistic information, e.g.,
syntax, predicate-argument structure, etc.
Modeling discourse brings together the above
research directions (a) and (b), which makes it an
attractive goal for MT. This is demonstrated by the
establishment of a recent workshop dedicated to
Discourse in Machine Translation (Webber et al,
2013), collocated with the 2013 annual meeting of
the Association of Computational Linguistics.
The area of discourse analysis for SMT is still
nascent and, to the best of our knowledge, no
previous research has attempted to use rhetorical
structure for SMT or machine translation evalua-
tion. One possible reason could be the unavailabil-
ity of accurate discourse parsers. However, this
situation is likely to change given the most recent
advances in automatic discourse analysis (Joty et
al., 2012; Joty et al, 2013).
687
We believe that the semantic and pragmatic in-
formation captured in the form of DTs (i) can help
develop discourse-aware SMT systems that pro-
duce coherent translations, and (ii) can yield bet-
ter MT evaluation metrics. While in this work we
focus on the latter, we think that the former is also
within reach, and that SMT systems would bene-
fit from preserving the coherence relations in the
source language when generating target-language
translations.
In this paper, rather than proposing yet another
MT evaluation metric, we show that discourse
information is complementary to many existing
evaluation metrics, and thus should not be ignored.
We first design two discourse-aware similarity
measures, which use DTs generated by a publicly-
available discourse parser (Joty et al, 2012); then,
we show that they can help improve a number of
MT evaluation metrics at the segment- and at the
system-level in the context of the WMT11 and the
WMT12 metrics shared tasks (Callison-Burch et
al., 2011; Callison-Burch et al, 2012).
These metrics tasks are based on sentence-level
evaluation, which arguably can limit the benefits
of using global discourse properties. Fortunately,
several sentences are long and complex enough to
present rich discourse structures connecting their
basic clauses. Thus, although limited, this setting
is able to demonstrate the potential of discourse-
level information for MT evaluation. Furthermore,
sentence-level scoring (i) is compatible with most
translation systems, which work on a sentence-by-
sentence basis, (ii) could be beneficial to mod-
ern MT tuning mechanisms such as PRO (Hop-
kins and May, 2011) and MIRA (Watanabe et al,
2007; Chiang et al, 2008), which also work at
the sentence-level, and (iii) could be used for re-
ranking n-best lists of translation hypotheses.
2 Related Work
Addressing discourse-level phenomena in ma-
chine translation is relatively new as a research di-
rection. Some recent work has looked at anaphora
resolution (Hardmeier and Federico, 2010) and
discourse connectives (Cartoni et al, 2011; Meyer,
2011), to mention two examples.
1
However, so
far the attempts to incorporate discourse-related
knowledge in MT have been only moderately suc-
cessful, at best.
1
We refer the reader to (Hardmeier, 2012) for an in-depth
overview of discourse-related research for MT.
A common argument, is that current automatic
evaluation metrics such as BLEU are inadequate
to capture discourse-related aspects of translation
quality (Hardmeier and Federico, 2010; Meyer et
al., 2012). Thus, there is consensus that discourse-
informed MT evaluation metrics are needed in or-
der to advance research in this direction. Here we
suggest some simple ways to create such metrics,
and we also show that they yield better correlation
with human judgments.
The field of automatic evaluation metrics for
MT is very active, and new metrics are contin-
uously being proposed, especially in the context
of the evaluation campaigns that run as part of
the Workshops on Statistical Machine Transla-
tion (WMT 2008-2012), and NIST Metrics for
Machine Translation Challenge (MetricsMATR),
among others. For example, at WMT12, 12 met-
rics were compared (Callison-Burch et al, 2012),
most of them new.
There have been several attempts to incorpo-
rate syntactic and semantic linguistic knowledge
into MT evaluation. For instance, at the syn-
tactic level, we find metrics that measure the
structural similarity between shallow syntactic se-
quences (Gim?enez and M`arquez, 2007; Popovic
and Ney, 2007) or between constituency trees (Liu
and Gildea, 2005). In the semantic case, there are
metrics that exploit the similarity over named en-
tities and predicate-argument structures (Gim?enez
and M`arquez, 2007; Lo et al, 2012).
In this work, instead of proposing a new metric,
we focus on enriching current MT evaluation met-
rics with discourse information. Our experiments
show that many existing metrics can benefit from
additional knowledge about discourse structure.
In comparison to the syntactic and semantic ex-
tensions of MT metrics, there have been very few
attempts to incorporate discourse information so
far. One example are the semantics-aware metrics
of Gim?enez and M`arquez (2009) and Comelles et
al. (2010), which use the Discourse Representa-
tion Theory (Kamp and Reyle, 1993) and tree-
based discourse representation structures (DRS)
produced by a semantic parser. They calculate the
similarity between the MT output and references
based on DRS subtree matching, as defined in (Liu
and Gildea, 2005), DRS lexical overlap, and DRS
morpho-syntactic overlap. However, they could
not improve correlation with human judgments, as
evaluated on the MetricsMATR dataset.
688
Compared to the previous work, (i) we use a
different discourse representation (RST), (ii) we
compare discourse parses using all-subtree ker-
nels (Collins and Duffy, 2001), (iii) we evaluate
on much larger datasets, for several language pairs
and for multiple metrics, and (iv) we do demon-
strate better correlation with human judgments.
Wong and Kit (2012) recently proposed an
extension of MT metrics with a measure of
document-level lexical cohesion (Halliday and
Hasan, 1976). Lexical cohesion is achieved using
word repetitions and semantically similar words
such as synonyms, hypernyms, and hyponyms.
For BLEU and TER, they observed improved
correlation with human judgments on the MTC4
dataset when linearly interpolating these metrics
with their lexical cohesion score. Unlike their
work, which measures lexical cohesion at the
document-level, here we are concerned with co-
herence (rhetorical) structure, primarily at the
sentence-level.
3 Our Discourse-Based Measures
Our working hypothesis is that the similarity be-
tween the discourse structures of an automatic and
of a reference translation provides additional in-
formation that can be valuable for evaluating MT
systems. In particular, we believe that good trans-
lations should tend to preserve discourse relations.
As an example, consider the three discourse
trees (DTs) shown in Figure 1: (a) for a reference
(human) translation, and (b) and (c) for transla-
tions of two different systems on the WMT12 test
dataset. The leaves of a DT correspond to con-
tiguous atomic text spans, called Elementary Dis-
course Units or EDUs (three in Figure 1a). Ad-
jacent spans are connected by certain coherence
relations (e.g., Elaboration, Attribution), forming
larger discourse units, which in turn are also sub-
ject to this relation linking. Discourse units linked
by a relation are further distinguished based on
their relative importance in the text: nuclei are
the core parts of the relation while satellites are
supportive ones. Note that the nuclearity and re-
lation labels in the reference translation are also
realized in the system translation in (b), but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis. We ar-
gue that existing metrics that only use lexical and
syntactic information cannot distinguish well be-
tween (b) and (c).
In order to develop a discourse-aware evalua-
tion metric, we first generate discourse trees for
the reference and the system-translated sentences
using a discourse parser, and then we measure the
similarity between the two discourse trees. We de-
scribe these two steps below.
3.1 Generating Discourse Trees
In Rhetorical Structure Theory, discourse analysis
involves two subtasks: (i) discourse segmentation,
or breaking the text into a sequence of EDUs, and
(ii) discourse parsing, or the task of linking the
units (EDUs and larger discourse units) into la-
beled discourse trees. Recently, Joty et al (2012)
proposed discriminative models for both discourse
segmentation and discourse parsing at the sen-
tence level. The segmenter uses a maximum en-
tropy model that achieves state-of-the-art accuracy
on this task, having an F
1
-score of 90.5%, while
human agreement is 98.3%.
The discourse parser uses a dynamic Condi-
tional Random Field (Sutton et al, 2007) as a pars-
ing model in order to infer the probability of all
possible discourse tree constituents. The inferred
(posterior) probabilities are then used in a proba-
bilistic CKY-like bottom-up parsing algorithm to
find the most likely DT. Using the standard set
of 18 coarse-grained relations defined in (Carlson
and Marcu, 2001), the parser achieved an F
1
-score
of 79.8%, which is very close to the human agree-
ment of 83%. These high scores allowed us to de-
velop successful discourse similarity metrics.
2
3.2 Measuring Similarity
A number of metrics have been proposed to mea-
sure the similarity between two labeled trees, e.g.,
Tree Edit Distance (Tai, 1979) and Tree Kernels
(Collins and Duffy, 2001; Moschitti and Basili,
2006). Tree kernels (TKs) provide an effective
way to integrate arbitrary tree structures in kernel-
based machine learning algorithms like SVMs.
In the present work, we use the convolution TK
defined in (Collins and Duffy, 2001), which effi-
ciently calculates the number of common subtrees
in two trees. Note that this kernel was originally
designed for syntactic parsing, where the subtrees
are subject to the constraint that their nodes are
taken with either all or none of the children. This
constraint of the TK imposes some limitations on
the type of substructures that can be compared.
2
The discourse parser is freely available from
http://alt.qcri.org/tools/
689
ElaborationROOT
SPAN NucleusAttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human) translation.
	

 	
		Proceedings of the SIGDIAL 2013 Conference, pages 117?121,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Dialogue Act Recognition in
Synchronous and Asynchronous Conversations
Maryam Tavafi?, Yashar Mehdad?, Shafiq Joty?, Giuseppe Carenini?, Raymond Ng?
?Department of Computer Science, University of British Columbia, Vancouver, Canada
?Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
?{tavafi, mehdad, carenini, rng}@cs.ubc.ca ?sjoty@qf.org.qa
Abstract
In this work, we study the effectiveness of
state-of-the-art, sophisticated supervised
learning algorithms for dialogue act mod-
eling across a comprehensive set of differ-
ent spoken and written conversations in-
cluding: emails, forums, meetings, and
phone conversations. To this aim, we com-
pare the results of SVM-multiclass and
two structured predictors namely SVM-
hmm and CRF algorithms. Extensive em-
pirical results, across different conversa-
tional modalities, demonstrate the effec-
tiveness of our SVM-hmm model for di-
alogue act recognition in conversations.
1 Introduction
Revealing the underlying conversational struc-
ture in dialogues is important for detecting the
human social intentions in spoken conversations
and in many applications including summariza-
tion (Murray, 2010), dialogue systems and di-
alogue games (Carlson, 1983) and flirt detec-
tion (Ranganath, 2009). As an additional example,
Ravi and Kim (2007) show that dialogue acts can
be used for analyzing the interaction of students in
educational forums.
Recently, there have been increasing interests
for dialogue act (DA) recognition in spoken and
written conversations, which include meetings,
phone conversations, emails and blogs. However,
most of the previous works are specific to one of
these domains. There are potentially useful fea-
tures and algorithms for each of these domains,
but due to the underlying similarities between
these types of conversations, we aim to identify a
domain-independent DA modeling approach that
can achieve good results across all types of con-
versations. Such a domain-independent dialogue
act recognizer makes it possible to automatically
recognize dialogue acts in a wide variety of con-
versational data, as well as in conversations span-
ning multiple domains/modalities; for instance a
conversation that starts in a meeting and then con-
tinues via email.
While previous work in DA modeling has fo-
cused on studying only one (Carvalho, 2005;
Shrestha, 2004; Ravi, 2007; Ferschke, 2012; Kim,
2010a; Sun, 2012) or, in a few cases, a couple of
conversational domains (Jeong, 2009; Joty, 2011),
in this paper, we analyze the performance of su-
pervised DA modeling on a comprehensive set
of different spoken and written conversations that
includes: emails, forums, meetings, and phone
conversations. More specifically, we compare
the performance of three state-of-the-art, sophis-
ticated machine learning algorithms, which in-
clude SVM-multiclass and two structured predic-
tors SVM-hmm and Conditional Random Fields
(CRF) for DA modeling. We present an exten-
sive set of experiments studying the effectiveness
of DA modeling on different types of conversa-
tions such as emails, forums, meeting, and phone
discussions. The experimental results show that
the SVM-hmm algorithm outperforms other su-
pervised algorithms across all datasets.
2 Related Work
There have been several studies on supervised
dialogue act (DA) modeling. To the best of
our knowledge, none of them compare the per-
formance of DA recognition on different syn-
chronous (e.g., meeting and phone) and asyn-
chronous (e.g., email and forum) conversations.
Most of the works analyze DA modeling in a spe-
cific domain. Carvalho and Cohen (2005) propose
classifying emails into their dialogue acts accord-
ing to two ontologies for nouns and verbs. The
ontologies are used for determining the speech
acts of each single email with verb-noun pairs.
Shrestha and McKeown (2004) also study the
117
problem of DA modeling in email conversations
considering the two dialogue acts of question and
answer. Likewise, Ravi and Kin (2007) present
a DA recognition method for detecting questions
and answers in educational discussions. Ferschke
et al (2012) apply DA modeling to Wikipedia dis-
cussions to analyze the collaborative process of
editing Wikipedia pages. Kim et al (2010a) study
the task of supervised classification of dialogue
acts in one-to-one online chats in the shopping do-
main.
All these previous studies focus on DA recog-
nition in one or two domains, and do not sys-
tematically analyze the performance of different
dialog act modeling approaches on a compre-
hensive set of conversation domains. As far as
we know, the present work is the first that pro-
poses domain-independent supervised DA model-
ing techniques, and analyzes their effectiveness on
different modalities of conversations.
3 Dialogue Act Recognition
3.1 Conversational structure
Adjacent utterances in a conversation have a
strong correlation in terms of their dialogue acts.
As an example, if speaker 1 asks a question to
speaker 2, it is a high probability that the next ut-
terance of the conversation would be an answer
from speaker 2. Therefore, the conversational
structure is a paramount factor that should be taken
into account for automatic DA modeling. The con-
versational structure differs in spoken and written
discussions. In spoken conversations, the discus-
sion between the speakers is synchronized. The
speakers hear each other?s ideas and then state
their opinions. So the temporal order of the ut-
terances can be considered as the conversational
structure in these types of conversations. How-
ever, in written conversations such as email and
forum, authors contribute to the discussion in dif-
ferent order, and sometimes they do not pay atten-
tion to the content of previous posts. Therefore,
the temporal order of the conversation cannot be
used as the conversational structure in these do-
mains, and appropriate techniques should be used
to extract the underlying structure in these conver-
sations.
To this aim, when reply links are available in
the dataset, we use them to capture the conversa-
tion structure. To obtain a conversational structure
that is often even more refined than the reply links,
we build the Fragment Quotation Graph. To this
end, we follow the procedure proposed by Joty et
al. (2011) to extract the graph structure of a thread.
3.2 Features
In defining the feature set, we have two primary
criteria, being domain independent and effective-
ness in previous works. Lexical features such as
unigrams and bigrams have been shown to be use-
ful for the task of DA modeling in previous stud-
ies (Sun, 2012; Ferschke, 2012; Kim, 2010a; Ravi,
2007; Carvalho, 2005). In addition, unigrams have
been shown to be the most effective among the
two. So, as the lexical feature, we include the fre-
quency of unigrams in our feature set.
Moreover, length of the utterance is another
beneficial feature for DA recognition (Ferschke,
2012; Shrestha, 2004; Joty, 2011), which we add
to our feature set. The speaker of an utterance
has shown its utility for recognizing speech acts
(Sun, 2012; Kim, 2010a; Joty, 2011). Sun and
Morency (2012) specifically employ a speaker-
adaptation technique to demonstrate the effective-
ness of this feature for DA modeling. We also
include the relative position of a sentence in a
post for DA modeling since most of previous stud-
ies (Ferschke, 2012; Kim, 2010a; Joty, 2011)
prove the efficiency of this feature.
3.3 Algorithms
Since most top performing DA models use su-
pervised approaches (Carvalho, 2005; Shrestha,
2004; Ravi, 2007; Ferschke, 2012; Kim, 2010a),
to analyze the performance of DA modeling on a
comprehensive set of different spoken and written
conversations, we compare the state-of-the-art su-
pervised algorithms.
We employ three state-of-the-art, sophisticated
supervised learning algorithms:
SVM-hmm predicts labels for the examples
in a sequence (Tsochantaridis, 2004). This
approach uses the Viterbi algorithm to find the
highest scoring tag sequence for a given obser-
vation sequence. Being a Hidden Markov Model
(HMM), the model makes the Markov assump-
tion, which means that the label of a particular
example is assigned only by considering the
label of the previous example. This approach is
considered an SVM because the parameters of the
model are trained discriminatively to separate the
label of sequences by a large margin.
118
CRF is a probabilistic framework to label and
segment sequence data (Lafferty, 2001). The
main advantage of CRF over HMM is that it re-
laxes the assumption of conditional independence
of observed data. HMM is a generative model
that assigns a joint distribution over label and
observation sequences. Whereas, CRF defines the
conditional probability distribution over label se-
quences given a particular observation sequence.
SVM-multiclass is a generalization of binary
SVM to a multiclass predictor (Crammer, 2001).
The SVM-multiclass does not consider the
sequential dependency between the examples.
4 Corpora
Gathering conversational corpora for DA model-
ing is an expensive and time-consuming task. Due
to the privacy issues, there are few available con-
versational datasets.
For asynchronous conversations, we use avail-
able corpora for email and forum discussions. For
synchronous domains we employ available cor-
pora in multi-party meeting and phone conversa-
tions.
BC3 (Email): As the labeled dataset for email
conversations, we use BC3 (Ulrich, 2008), which
contains 40 threads from W3C corpus. The
BC3 corpus is annotated with twelve domain-
independent dialogue acts, which are mainly
adopted from the MRDA tagset, and it has been
used in several previous works (e.g., (Joty, 2011)).
CNET (Forum): As the labeled forum dataset,
we use the available CNET corpus, which is an-
notated with eleven domain-independent dialogue
acts in a post-level (Kim et al 2010b). This corpus
consists of 320 threads and a total of 1332 posts,
which are mostly from technical forums.
MRDA (Meeting): ICSI-MRDA dataset is
used as labeled data for meeting conversation,
which contains 75 meetings with 53 unique speak-
ers (Shriberg, 2004). The ICSI-MRDA dataset re-
quires one general tag per sentence followed by
variable number of specific tags. There are 11
general tags and 39 specific tags in the annotation
scheme. We reduce their tagset to the eleven gen-
eral tags to be consistent with the other datasets.
SWBD (Phone): In addition to multi-party
meeting conversations, we also report our experi-
mental results on Switchboard-DAMSL (SWBD),
which is a large-scale corpus containing telephone
speech (Jurafsky, 1997). This corpus is annotated
with the SWBD-DAMSL tagset, which consists of
220 tags. We use the mapping table presented by
Jeong (2009) to reduce the tagset to 16 domain-
independent dialogue acts.
All the available corpora are annotated with di-
alogue acts at the sentence-level. The only excep-
tion is the CNET forum dataset, on which we ap-
ply DA classification at the post-level.
5 Experiments and Results
5.1 Experimental settings
In our experiments, we use the SVM-hmm1 and
SVM-multiclass2 packages developed with the
SVM-light software. We use the Mallet package3
for the CRF algorithm. The results of supervised
classifications are compared to the baseline, which
is the majority class of each dataset. We apply
5-fold cross-validation for the supervised learn-
ing methods to each dataset, and compare the re-
sults of different methods using micro-averaged
and macro-averaged accuracies.
5.2 Results
Table 1 shows the results of supervised classifi-
cation on different conversation modalities. We
observe that SVM-hmm and CRF classifiers out-
perform SVM-multiclass classifier in all conversa-
tional domains. Both SVM-hmm and CRF classi-
fiers consider the sequential structure of conversa-
tions, while this is ignored in the SVM-multiclass
classifier. This shows that the sequential structure
of the conversation is beneficial independently of
the conversational modality. We can also observe
that the SVM-hmm algorithm results in the highest
performance in all datasets. As shown in (Altun,
2003), generalization performace of SVM-hmm
is superior to CRF. This superiority also applies
to the DA modeling task across all the conversa-
tional modalities. However, as it was investigated
by Keerthi and Sundararajan (2007), the discrep-
ancy in the performance of these methods may
arise from different feature functions that these
two methods use, and they might perform simi-
larly when they use the same feature functions.
Comparing the results across different datasets,
we can also note that the largest improvement
of SVM-hmm and CRF is on the SWBD, the
1http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html
2http://svmlight.joachims.org/svm_multiclass.html
3http://mallet.cs.umass.edu
119
Corpus Baseline SVM-multiclass SVM-hmm CRFMicro Macro Micro Macro Micro Macro Micro Macro
BC3 69.56 8.34 73.57 (4.01) 8.34 (0) 77.75 (8.19) 18.20 (9.86) 72.18 (2.62) 14.9 (6.56)
CNET 36.75 9.09 34.8 (-1.95) 9.3 (0.21) 58.7 (21.95) 17.1 (8.01) 40.3 (3.55) 11.5 (2.41)
MRDA 66.47 9.09 66.47 (0) 9.09 (0) 80.5 (14.03) 32.4 (23.31) 77.8 (11.33) 22.9 (13.81)
SWBD 46.44 6.25 46.5 (0.06) 6.25 (0) 74.32 (27.88) 30.13 (23.88) 73.04 (26.6) 24.05 (17.8)
Table 1: Results of supervised DA modeling; columns are micro-averaged and macro-averaged accura-
cies with difference with baseline in parentheses.
phone conversation dataset. Moreover, super-
vised DA recognition on synchronous conversa-
tions achieves a better performance than on asyn-
chronous conversations. We can argue that this is
due to the less complex sequential structure of syn-
chronous conversations. A lower macro-averaged
accuracy in asynchronous conversations (i.e., fo-
rums and emails) can be justified in the same way.
By looking at the results in asynchronous con-
versations, we observe a larger improvement of
micro-averaged accuracy over the CNET corpus.
This might be due to two reasons: i) the DA tagsets
in both corpora are different (i.e., no overlap in
tagsets); and ii) the conversational structure in fo-
rums and emails is different.
5.3 Discussion
We analyze the strengths and weakness of super-
vised DA modeling with SVM-hmm in different
conversations individually.
BC3: SVM-hmm succeeds in classifying most
of the statement and yes-no question speech acts in
the BC3 corpus. However, it does not show a high
accuracy for classifying polite mechanisms such
as ?thanks? and ?regards?. Through the error anal-
ysis, we observed that in most of these cases the
error arose from the voting algorithm. Moreover,
the improvement of supervised DA modeling on
the BC3 corpus is smaller than the other datasets.
This may suggest that email conversation is a chal-
lenging domain for DA recognition.
CNET: The inventory of dialogue acts in the
CNET dataset can be considered as two groups of
question and answer dialogue acts, and we would
need more sophisticated features in order to clas-
sify the posts into the fine-grained dialogue acts.
The SVM-hmm succeeds in predicting the labels
of question-question and answer-answer dialogue
acts, but it performs poorly for the other labels.
The improvement of DA modeling over the base-
line is significant for this dataset. To further im-
prove the performance, a hierarchical DA classifi-
cation can be applied. In this way, the posts would
be classified into question and non-question dia-
logue acts in the first level.
MRDA: SVM-hmm performs well for pre-
dicting the classes of statement, floor holder,
backchannel, and wh-question. Floor holders and
backchannels are mostly the short utterances such
as ?ok?, ?um?, and ?so?, and we believe the length
and unigrams features are very effective for pre-
dicting these dialogue acts. On the other hand,
SVM-hmm fails in predicting the other types of
questions such as rhetorical questions and open-
ended questions by classifying them as statements.
Arguably by adding more sophisticated features
such as POS tags, SVM-hmm would perform bet-
ter for classifying these speech acts.
SWBD: The improvement of supervised DA
recognition on the SWBD is higher than the other
domains. Supervised DA classification correctly
predicts most of the classes of statement, reject re-
sponse, wh-question, and backchannel. However,
SVM-hmm cannot predict some specific dialogue
acts of phone conversations such as self-talk and
signal-non-understanding. There are a few utter-
ances in the corpus with these dialogue acts, and
most of them are classified as statements.
6 Conclusion and Future Work
We have studied the effectiveness of sophisticated
supervised learning algorithms for DA modeling
across a comprehensive set of different spoken and
written conversations. Through an extensive ex-
periment, we have shown that our proposed SVM-
hmm algorithm with the domain-independent fea-
ture set can achieve high results on different syn-
chronous and asynchronous conversations.
In future, we will incorporate other lexical and
syntactic features in our supervised framework.
We also plan to augment our feature set with
domain-specific features like prosodic features for
spoken conversations. We will also investigate the
performance of our domain-independent approach
in a semi-supervised framework.
120
References
Congkai Sun and Louise-Philippe Morency. 2012. Di-
alogue Act Recognition using Reweighted Speaker
Adaptation. 13th Annual SIGdial Meeting on Dis-
course and Dialogue.
Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL labeling project
coder?s manual, draft 13. Technical report, Univ. of
Colorado Institute of Cognitive Science.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI Meet-
ing Recorder Dialog Act (MRDA) Corpus. HLT-
NAACL SIGDIAL Workshop.
Gabriel Murray, Giuseppe Carenini, and Raymond T.
Ng. 2010. Generating and validating abstracts of
meeting conversations: a user study. INLG?10.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. Proceedings of the 21st Inter-
national Conference on Machine Learning (ICML).
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for
supervised email summarization. EMAIL?08 Work-
shop. AAAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Intl. Conf. on Machine Learning.
Koby Crammer and Yoram Singer. 2001. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search.
Lari Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. Proceedings of the 20th Biennial Int. Conf. on
Computational Linguistics.
Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee.
2009. The Semi-supervised speech act recognition
in emails and forums. Proceedings of the 2009 Conf.
Empirical Methods in Natural Language Processing.
Oliver Ferschke, Iryna Gurevych, and Yevgen Cheb-
otar. 2012. Behind the Article: Recognizing Dia-
log Acts in Wikipedia Talk Pages. Proceedings of
the 13th Conference of the European Chapter of the
ACL.
Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.
2009. Its not you, its me: Detecting flirting and its
misperception in speed-dates. EMNLP-09.
S. S. Keerthi and S. Sundararajan. 2007. CRF versus
SVM-Struct for sequence labeling. Technical report,
Yahoo Research.
Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in
asynchronous conversations. IJCAI.
Su N. Kim, Lawrence Cavedon, and Timothy Baldwin.
2010a. Classifying dialogue acts in one-on-one live
chats. EMNLP?10.
Su N. Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, CoNLL ?10.
Sujith Ravi and Jihie Kim. 2007. Profiling student
interactions in threaded discussions with speech act
classifiers. AIED?07, LA, USA.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email "speech acts".
Proceedings of the 31st Annual Int. ACM SIGIR
Conf. on Research and Development in Information
Retrieval.
Yasemin Altun and Ioannis Tsochantaridis and Thomas
Hofmann. 2003. Hidden Markov Support Vector
Machines. Proceedings of the 20th International
Conference on Machine Learning.
7 Appendix A. Frequency of Dialogue
Acts in the Corpora
Tag Dialogue Acts Email(BC3) Forum(CNET) Meeting(MRDA) Phone(SWBD)A Accept response 2.07% ? ? 6.96%AA Acknowledge and appreciate 1.24% ? ? 2.12%AC Action motivator 6.09% ? ? 0.38%P Polite mechanism 6.97% ? ? 0.12%QH Rhetorical question 0.75% ? 0.34% 0.25%QO Open-ended question 1.32% ? 0.17% 0.3%QR Or/or-clause question 1.10% ? ? 0.2%QW Wh-question 2.29% ? 1.63% 0.95%QY Yes-no question 6.75% ? 4.75% 2.62%R Reject response 1.06% ? ? 1.03%S Statement 69.56% ? 66.47% 46.44%U Uncertain response 0.79% ? ? 0.15%Z Hedge ? ? ? 11.55%B Backchannel ? ? 14.44% 26.62%D Self-talk ? ? ? 0.1%C Signal-non-understanding ? ? ? 0.14%FH Floor holder ? ? 7.96% ?FG Floor grabber ? ? 2.96% ?H Hold ? ? 0.76% ?QRR Or clause after yes-no question ? ? 0.38% ?QR Or question ? ? 0.2% ?QQ Question-question ? 27.92% ? ?QA Question-add ? 11.67% ? ?QCN Question-confirmation ? 3.89% ? ?QCC Question-correction ? 0.36% ? ?AA Answer-answer ? 36.75% ? ?AD Answer-add ? 8.84% ? ?AC Answer-confirmation ? 0.36% ? ?RP Reproduction ? 0.71% ? ?AO Answer-objection ? 1.07% ? ?RS Resolution ? 7.78% ? ?O Other ? 0.71% ? ?
Table 2: Dialogue act categories and their relative
frequency.
Table 2 indicates the dialogue acts of each cor-
pus and their relative frequencies in that dataset.
The table shows that the distribution of dialogue
acts in the datasets are not balanced. Most of the
utterances in the datasets are labeled as statements.
Consequently, during the classification step, most
of the utterances are labeled as the statement dia-
logue act. This always affects the performance of
a classifier in dealing with low frequency classes.
A possible approach to tackle this problem is to
cluster the correlative dialogue acts into the same
group and apply a DA modeling approach in a hi-
erarchical manner.
121
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 45?52,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Interactive Exploration of Asynchronous Conversations: Applying a
User-centered Approach to Design a Visual Text Analytic System
Enamul Hoque, Giuseppe Carenini
{enamul,carenini}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, Canada
Shafiq Joty
sjoty@qf.org.qa
Qatar Computing Research Institute
Qatar Foundation
Doha, Qatar
Abstract
Exploring an online conversation can be
very difficult for a user, especially when
it becomes a long complex thread. We fol-
low a human-centered design approach to
tightly integrate text mining methods with
interactive visualization techniques to sup-
port the users in fulfilling their informa-
tion needs. The resulting visual text ana-
lytic system provides multifaceted explo-
ration of asynchronous conversations. We
discuss a number of open challenges and
possible directions for further improve-
ment including the integration of interac-
tive human feedback in the text mining
loop, applying more advanced text analy-
sis methods with visualization techniques,
and evaluating the system with real users.
1 Introduction
With the rapid adoption of Web-based social me-
dia, asynchronous online conversations are be-
coming extremely common for supporting com-
munication and collaboration. An asynchronous
conversation such as a blog may start with a news
article or an editorial opinion, and later generate a
long and complex thread as comments are added
by the participants (Carenini et al., 2011). Con-
sider a scenario, where a reader opens a blog con-
versation about Obama?s healthcare policy. The
reader wants to know why people are supporting
or opposing ObamaCare. However, since some
related discussion topics like student loan and job
recession are introduced, the reader finds it hard
to keep track of the comments about ObamaCare,
which end up being buried in the long discussion.
This may lead to an information overload problem,
where the reader gets overwhelmed, starts to skip
comments, and eventually leaves the conversation
without satisfying her information needs (Jones et
al., 2004).
How can we support the user in performing this
and similar information seeking tasks? Arguably,
supporting this task requires tight integration be-
tween Natural Language Processing (NLP) and in-
formation visualization (InfoVis) techniques, but
what specific text analysis methods should be ap-
plied? What metadata of the conversation could be
useful to the user? How this data should be visual-
ized to the user? And even more importantly, how
NLP and InfoVis techniques should be effectively
integrated? Our hypothesis is that to answer these
questions effectively, we need to apply human-
centered design methodologies originally devised
for generic InfoVis (e.g., (Munzner, 2009; Sedl-
mair et al., 2012)). Starting from an analysis of
user behaviours and needs in the target conversa-
tional domain, such methods help uncover useful
task and data abstractions that can guide system
design. On the one hand, task and data abstrac-
tions can characterize the type of information that
needs to be extracted from the conversation; on the
other hand, they can inform the design of the vi-
sual encodings and interaction techniques. More
tellingly, as both the NLP and the InfoVis compo-
nents of the resulting system refer to a common set
of task and data abstractions, they are more likely
to be consistent and synergistic.
We have explored this hypothesis in developing
ConVis, a visual analytic system to support the in-
teractive analysis of blog conversations. In the first
part of the paper, we describe the development of
ConVis, from characterizing the domain of blogs,
its users, tasks and data, to designing and imple-
menting specific NLP and InfoVis techniques in-
formed by our user-centered design. In the second
part of the paper, starting from an informal evalu-
ation of Convis and a comprehensive literature re-
view, we discuss several ideas on howConVis (and
similar systems) could be further improved and
tested. These include the integration of interac-
tive human feedback in the text mining techniques
45
(which are based on Machine Learning), the cou-
pling of even more advanced NLP methods with
the InfoVis techniques, and the challenges in run-
ning evaluations of ConVis and similar interfaces.
2 Related Work
While in the last decade, NLP and InfoVis meth-
ods have been investigated to support the user in
making sense of conversational data, most of this
work has been limited in several ways.
For example, earlier works on visualizing
asynchronous conversations primarily investigated
how to reveal the thread structure of a conversation
using tree visualization techniques, such as using
a mixed-model visualization to show both chrono-
logical sequence and reply relationships (Venolia
and Neustaedter, 2003), thumbnail metaphor using
a sequence of rectangles (Wattenberg and Millen,
2003; Kerr, 2003), and radial tree layout (Pascual-
Cid and Kaltenbrunner, 2009). However, such vi-
sualizations did not focus on analysing the actual
content (i.e., the text) of the conversations, which
is something that according to our user-centred de-
sign users are very interested in.
On the other hand, text mining approaches
that perform content analysis of the conversations,
such as finding primary themes (or topics) within
conversations (Sack, 2000; Dave et al., 2004), or
visualizing the content evolution over time (Wei et
al., 2010; Vi?egas et al., 2006), often did not derive
their visual encodings and interactive techniques
from task and data abstractions based on a detailed
analysis of specific user needs and requirements in
the target domains.
Furthermore, more on the technical side, the
text analysis methods employed by these ap-
proaches are not designed to exploit the spe-
cific characteristics of asynchronous conversations
(e.g., use of quotation). Recently, (Joty et al.,
2013b) has shown that topic segmentation and la-
beling models are more accurate when these spe-
cific characteristics are taken into account. The
methods presented in (Joty et al., 2013b) are
adopted in ConVis.
In general, to the best of our knowledge, no
previous work has applied user-centred design to
tightly integrate text mining methods with interac-
tive visualization in the domain of asynchronous
conversations.
3 Domains and User Activities
Conversational domains: The phenomenal adop-
tion of novel Web-based social media has lead to
the rise of textual conversations in many different
modalities. While email remains a fundamental
way of communicating for most people, other con-
versational modalities such as blogs, microblogs
(e.g., Twitter) and discussion fora have quickly be-
come widely popular. Since the nature of data and
tasks may vary significantly from one domain to
the other, rather than trying to build an one-size-
fit-all interface, we follow a design methodology
that is driven by modeling the tasks and usage
characteristics in a specific domain.
In this work, we focus on blogs, where people
can express their thoughts and engage in online
discussions. Due to the large number of comments
with complex thread structure (Joty et al., 2013b),
mining and visualizing blog conversations can be-
come a challenging problem. However, the visual-
ization can be effective for other threaded discus-
sions (e.g., news stories, Youtube comments).
Users: As shown in Table 1, blog users can be
categorized into two groups based on their activ-
ities: (a) participants who already contributed to
the conversations, and (b) non-participants who
wish to join the conversations or analyze the con-
versations. Depending on different user groups the
tasks might vary as well, something that needs to
be taken into account in the design process.
For example, imagine a participant who has ex-
pressed her opinion about a major political issue.
After some time, she may become interested to
know what comments were made supporting or
opposing her opinion, and whether those com-
ments require a reply right away. On the contrary,
a non-participant, who is interested in joining the
ongoing conversation on that particular political
issue, may want to decide whether and how she
should contribute by quickly skimming through a
long thread of blog comments. Another group of
users may include the analysts, a policy maker for
instance, who does not wish to join the conversa-
tion, but may want to make an informed decision
based on a summary of arguments used to support
or oppose the political issue.
Once the conversation becomes inactive (i.e.,
no further comments are added), still a distinction
may remain between the activities of participants
and non-participants on tasks (see Table 1). In our
work, we have initially concentrated on supporting
46
User
types
Ongoing conver-
sation
Inactive/past conver-
sation
Participant Already joined the
conversation (wants
to get updated and
possibly make new
comments)
Wants to delve into
the past conversations
and re-examine what
was discussed, what
she commented on,
what other people
replied, etc.
Non-
participant
Potential partici-
pant (wants to join
the conversation)
Analyst (wants to
analyze the ongo-
ing conversation,
but does not intend
to join)
Wants to analyze and
gain insight about the
past conversation.
Table 1: User categorization for asynchronous
conversation.
the non-participant?s activity on an inactive con-
versation (as opposed to an ongoing conversation).
4 Designing ConVis: From Tasks to NLP
and InfoVis Techniques
We now briefly describe our design approach for
integrating text mining techniques with interactive
visualization in ConVis. We first characterize the
domain of blogs and perform the data and tasks
abstraction according to the nested model of de-
sign study (Munzner, 2009). We then mine the
data as appeared to be essential from that data and
task analysis, followed by iteratively refining the
design of ConVis that aims to effectively support
the identified blog reading tasks (A more detailed
analysis of the task abstractions and visual design
is provided in (Hoque and Carenini, 2014)).
4.1 Tasks
To understand the blog reading tasks, we re-
viewed the literature focusing on why and how
people read blogs. From the analysis, we
found that the primary goals of reading blogs in-
clude information seeking, fact checking, guid-
ance/opinion seeking, and political surveillance
(Kaye, 2005). People may also read blogs to con-
nect to their communities of interest (Dave et al.,
2004; Mishne, 2006), or just for fun/ enjoyment
(Baumer et al., 2008; Kaye, 2005).
Some studies have also revealed interesting be-
havioural patterns of blog readers. For example,
people often look for variety of opinions and have
tendencies to switch from one topic to another
quickly (Singh et al., 2010; Munson and Resnick,
2010). In addition, they often exhibit exploratory
behaviour, i.e., they quickly skim through a few
posts about a topic before delving deeper into its
details (Zinman, 2011). Therefore, the interface
should facilitate open-ended exploration, by pro-
viding navigational cues that help the user to seek
interesting comments.
From the analyses of primary goals of blog
reading, we compile a list of tasks and the asso-
ciated data variables that one would wish to visu-
alize for these tasks. These tasks can be framed
as a set of questions, for instance, ?what do peo-
ple say about topic X??, ?how other people?s view-
points differ from my current viewpoint on topic
X??, ?what are some interesting/funny comments
to read?? We then identify the primary data vari-
ables involved in these tasks and their abstract
types. For instance, most of these questions in-
volve topics discussed and sentiments expressed
in the conversation. Note that some questions may
additionally require to know people-centric infor-
mation and relate such information to the visual-
ization design. We also identify a set of meta-
data to be useful cues for navigating a conversa-
tion (the position of the comments, thread struc-
ture, and comment length) (Narayan and Cheshire,
2010; Baumer et al., 2008). We choose to encode
the position of the comments (ordinal) as opposed
to their timestamps (quantitative); since the exact
timestamp of a comment is less important to users
than its chronological position with respect to the
other comments (Baumer et al., 2008).
4.2 Text Analysis
Since most of the blog reading tasks we identi-
fied involved topics and sentiments expressed in
the conversation, we applied both topic modeling
and sentiment analysis on a given conversation.
In topic modeling, we group the sentences of a
blog conversation into a number of topical clusters
and label each cluster by assigning a short infor-
mative topic descriptor (i.e., a keyphrase). To find
the topical clusters and their associated labels, we
apply the topic segmentation and labeling models
recently proposed by (Joty et al., 2013b) for asyn-
chronous conversations, and successfully evalu-
ated on email and blog datasets. More specifically,
for topic segmentation, we use their best unsu-
pervised topic segmentation model LCSeg+FQG,
which extends the generic lexical cohesion based
topic segmenter (LCSeg) (Galley et al., 2003)
47
Figure 1: A snapshot of ConVis showing a blog conversation from Slashdot, where the user has hovered
the mouse over a topic element (?major army security?) that highlights the connecting visual links, brush-
ing the related authors(right), and providing visual prominence to the related comments in the Thread
Overview (middle).
to consider a fine-grain conversational structure
of the conversation, i.e., the Fragment Quotation
Graph (FQG) (Carenini et al., 2007). The FQG
captures the reply relations between text frag-
ments, which are extracted by analyzing the actual
body of the comments, thus provides a finer rep-
resentation of the conversation than the reply-to
structure. Similarly, the topic labels are found by
using their best unsupervised graph-based rank-
ing model (i.e., BiasedCorank) that extracts rep-
resentative keyphrases for each topical segment
by combining informative clues from initial sen-
tences of the segment and the fine-grain conversa-
tional structure, i.e., the FQG.
For sentiment analysis, we apply the Seman-
tic Orientation CALculator (SO-CAL) (Taboada
et al., 2011), which is a lexicon-based approach
(i.e., unsupervised) for determining sentiment of
a text. Its performance is consistent across vari-
ous domains and on completely unseen data, thus
making a suitable tool for our purpose. We define
five different polarity intervals (-2 to +2), and for
each comment we count how many sentences fall
in any of these polarity intervals to compute the
polarity distribution for that comment.
While designing and implementing ConVis, we
have been mainly working with blog conversations
from two different sources: Slashdot
1
?a technol-
ogy related blog site, and Daily Kos
2
? a political
analysis blog site.
1
http://slashdot.org
2
http://www.dailykos.com
4.3 Designing Interactive Visualization
Upon identifying the tasks and data variables, we
design the visual encoding and user interactions.
Figure 1 shows an initial prototype of ConVis.
3
It is designed as an overview + details interface,
since it has been found to be more effective for
text comprehension tasks than other approaches
such as zooming and focus+context (Cockburn et
al., 2008). The overview consists of what was dis-
cussed by whom (i.e., topics and authors) and a
visual summary of the whole conversation (i.e.,
the Thread Overview), while the detailed view
represents the actual conversation. The Thread
Overview visually represents each comment of
the discussion as a horizontal stacked bar, where
each stacked bar encodes three different meta-
data (comment length, position of the comment
in the thread, and depth of the comment within
the thread). To express the sentiment distribution
within a comment, the number of sentences that
belong to a particular sentiment orientation is in-
dicated by the width of each cell within a stacked
bar. A set of five diverging colors was used to vi-
sualize this distribution in a perceptually meaning-
ful order, ranging from purple (highly negative) to
orange (highly positive). Thus, the distribution of
colors in the Thread Overview can help the user to
perceive the kind of conversation they are going to
deal with. For example, if the Thread Overview is
3
https://www.cs.ubc.ca/cs-research/lci/research-
groups/natural-language-processing/ConVis.html
48
mostly in strong purple color, then the conversa-
tion has many negative comments.
The primary facets of the conversations, namely
topics and authors are presented in a circular
layout around the Thread Overview. Both top-
ics and authors are positioned according to their
chronological order in the conversation starting
from the top, allowing the user to understand how
the conversation evolves as the discussion pro-
gresses. The font size of facet items helps the
user to quickly identify what are the mostly dis-
cussed themes and who are the most dominant
participants within a conversation. Finally, the
facet elements are connected to their correspond-
ing comments in the Thread Overview via subtle
curved links indicating topic-comment-author re-
lationships. While a common way to relate various
elements in multiple views is synchronized visual
highlighting, we choose visual links to connect
related entities. This was motivated by the find-
ings that users can locate visually linked elements
in complex visualizations more quickly and with
greater subjective satisfaction than plain highlight-
ing (Steinberger et al., 2011). Finally, the Conver-
sation View displays the actual text of the com-
ments in the discussion as a scrollable list. At
the left side of each comment, the following meta-
data are presented: title, author name, photo, and a
stacked bar representing the sentiment distribution
(mirrored from Thread Overview).
Exploring Conversations: ConVis sup-
ports multi-faceted exploration of conversations
through a set of lightweight interactions (Lam,
2008) that can be easily triggered without causing
drastic modifications to the visual encoding. The
user can explore interesting topics/ authors by
hovering the mouse on them, which highlights
the connecting curved links and related comments
in the Thread Overview (see Figure 1). As such,
one can quickly understand how multiple facet
elements are related, which is useful for the tasks
that require the user to interpret the relationships
between facets. If the reader becomes further
interested in specific topic/ author, she can
subsequently click on it, resulting in drawing a
thick vertical outline next to the corresponding
comments in the Thread Overview. Such outlines
are also mirrored in the Conversation View.
Moreover, the user can select multiple facet items
(for instance a topic and an author) to quickly
understand who said about what topics.
Besides exploring by the topics/ authors, the
reader can browse individual comments by hover-
ing and clicking on them in the Thread Overview,
that causes to highlight its topic and scrolling to
the relevant comment in the Conversation View.
Thus, the user can easily locate the comments that
belong to a particular topic and/or author. More-
over, the keyphrases of the relevant topic and sen-
timents are highlighted in the Conversation View
upon selection, providing more details on demand
about what makes a particular comment positive/
negative or how it is related to a particular topic.
5 Further Challenges and Directions
After implementing the prototype, we ran an infor-
mal evaluation (Lam et al., 2012) with five target
users (age range 18 to 24, 2 female) to evaluate
the higher levels of the nested model (Munzner,
2009), where the aim was to collect anecdotal ev-
idence that the system met its design goals. The
participants? feedback from our evaluation sug-
gests that ConVis can help the user to identify
the topics and opinions expressed in the conver-
sation; supporting the user in exploring comments
of interest, even if they are buried near the end of
the thread. We also identified further challenges
from the observations and participants feedback.
Based on our experience and literature review, we
provide potential directions to address these chal-
lenges as we describe below.
5.1 Human in the Loop: Interactive Topic
Revision
Although the topic modeling method we applied
enhances the accuracy over traditional methods
for non-conversational text, the informal evalua-
tion reveals that still the extracted topics may not
always match user?s information need. In some
cases, the results of topic modeling can mismatch
with the reference set of topics/ concepts described
by human (Chuang et al., 2013). Even the in-
terpretations of topics can vary among people ac-
cording to expertise and the current task in hand.
In fact, during topic annotations by human experts,
there was considerable disagreement on the num-
ber of topics and on the assignment of sentences
to topic clusters (Joty et al., 2013b). Depending
on user?s mental model and current tasks, the topic
modeling results may require to be more specific
in some cases, and more generic in other cases. As
such, the topic model needs to be revised based
49
on user feedback to better support her analysis
tasks. Thus, our goal is to support a human-in-
the-loop topic modeling for asynchronous conver-
sations via interactive visualization.
There have been some recent works for incorpo-
rating user supervision in probabilistic topic mod-
els (e.g., Latent Dirichlet Allocation (LDA)) by
adding constraints in the form of must-link and
cannot-link (Andrzejewski et al., 2009; Hu et al.,
2011), or in the form of a one-to-one mapping be-
tween LDA?s latent topics and user tags (Ramage
et al., 2009). The feedback from users has been
also integrated through visualizations, that steers a
semi-supervised topic model (Choo et al., 2013).
In contrast to the above-mentioned methods that
are designed for generic documents, we are fo-
cusing on how our topic modeling approach that
is specific to asynchronous conversations, can be
steered by the end-users. We are planning to com-
bine a visual interface for expressing the user?s in-
tention via a set of actions, and a semi-supervised
version of the topic model that can be iteratively
refined from such user actions.
A set of possible topic revision operations are
shown in Figure 2. Splitting a topic into further
sub-topics can be useful when the user wants to
explore the conversation at a finer-topic granular-
ity (Figure 2(a)). A merging operation serves the
opposite purpose, i.e., when the user wants to ana-
lyze the conversation at a coarser topic granularity
(Figure 2(b)). Together, these two operations are
intended to help the user in dynamically changing
the granularity levels of different topics.
Since each topic is currently represented by a
set of keyphrases, they can also be effectively
used to revise the topic model. Consider an ex-
ample, where the sentences related to two dif-
ferent keyphrases, namely ?Obama health policy?
and ?job recession? are grouped together under the
same topic. The user may realize that the sen-
tences related to ?job recession? should have been
separated from its original topic into a new one
(Figure 2(c)). Finally, topic assignment modifi-
cation can be performed, when the domain ex-
pert believes that a group of sentences are wrongly
grouped/clustered (Figure 2(d)) by the system.
In order to design the interactive visualization
and algorithms for incorporating user feedback, a
number of open questions need to be answered.
Some of these questions are related to the user re-
quirement analysis of the problem domain, e.g.,
(a) Split (b) Merge
(c) Create topic by a
keyphrase
(d) Topic assignment
modification
Figure 2: Four different possible user actions for
topic revision
what are the tasks for exploring asynchronous con-
versation that require the introduction of user feed-
back to refine the topic model? What data should
be shown to the user to help her decide what topic
refinement actions are appropriate?
In terms of designing the set of interaction tech-
niques, the aim is to define a minimum set of
model refinement operations, and allowing the
user to express these operations from the visual
interface in a way that enhances the ability to pro-
vide feedback. A domain expert could possibly
express these operations through the direct manip-
ulation method (e.g., dragging a topic node over
another). A related open question is: how can we
minimize the cognitive load associated with inter-
preting the modeling results and deciding the next
round of topic revision operations?
From the algorithmic perspective, the most cru-
cial challenge seems to be devising an efficient
semi-supervised method in the current graph-
based topic segmentation and labeling framework
(Joty et al., 2013b). It needs to be fast enough to
respond to the user refinement actions and update
results in an acceptable period of time. In addition,
determining the number of topics is a challenging
problem when running the initial model and when
splitting a topic further.
5.2 Coupling Advanced NLP Methods with
Interactive Visualizations
In light of the informal evaluation, we also investi-
gate how current NLP methods are supporting the
tasks we identified and what additional methods
could be incorporated? For example, one of the
crucial data variable in most of the tasks is opin-
ion. However, during the evaluation two users did
50
not find the current sentiment analysis sufficient
enough in revealing whether a comment is sup-
porting/ opposing a preceding one. It seems that
opinion seeking tasks (e.g., ?why people were sup-
porting or opposing an opinion??) would require
the reader to know the argumentation flow within
the conversation, namely the rhetorical structure
of each comment (Joty et al., 2013a) and how
these structures are linked to each other.
An early work (Yee and Hearst, 2005) at-
tempted to organize the comments using a tree-
map like layout, where the parent comment is
placed on top as a text block and the space below
the parent node is divided between supporting and
opposing statements. We plan to follow this idea
in ConVis, but incorporating a higher level dis-
course relation analysis of the conversations and
detecting controversial topics.
Incorporating additional complex text analysis
results into the visualization may require us to re-
visit some of the higher levels of the nested model,
i.e., data abstraction and visual encoding. It may
impose further tradeoffs for visual encoding; for
instance how can we visually represent the argu-
mentation structure within a conversation? How
can we represent such structure, while preserv-
ing the data already found to be useful such as
topic and thread structure? How can we represent
that a topic is controversial? Besides text analysis
results, some additional facets can become more
useful to the participants (e.g., moderation scores,
named entities), while an existing facet being less
useful. In such cases, allowing the user to dynam-
ically change the facets of interest can be useful.
5.3 Evaluation in the Wild
While controlled experiments allow us to mea-
sure the user performance on specific tasks for the
given interface, they may not accurately capture
real world uses scenario (Lam et al., 2012). In this
context, an ecologically valid evaluation of Con-
Vis would be to allow the users to use the system
to read their own conversations of interest over an
extended period of time. Such longitudinal study
would provide valuable insights regarding the util-
ity of the interface.
Evaluating the topic refinement approach for
asynchronous conversation can be even more chal-
lenging. An initial approach could be to formu-
late some quantitative evaluation metrics, that help
us understand whether the iterative feedback from
the user would improve the resultant topic model
in terms of agreement with the reference set of
topics described by human annotators. However,
such approach would not capture the subjective
differences of the users in interpreting the topic
model. It would be more interesting to see, how
much users would actually care about providing
the feedback to refine the model in a real world
scenario? What refinement operations would be
performed more often? Would these operations
eventually support the user to perform some anal-
ysis tasks more effectively?
6 Conclusions
Understanding the user behaviours, needs, and re-
quirements in the target domain is critical in ef-
fectively combining NLP and InfoVis techniques.
In this paper, we apply a visualization design
method (Munzner, 2009) to identify what infor-
mation should be mined from the conversation as
well as how the visual encoding and interaction
techniques should be designed. We claim that the
NLP and the InfoVis components of the resulting
system, ConVis, are more consistent and better in-
tegrated, because they refer to a common set of
task and data abstractions. In future work, we aim
to explore a set of open challenges that were moti-
vated by an initial informal evaluation of ConVis.
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proc. Conf.
on Machine Learning, pages 25?32.
Eric Baumer, Mark Sueyoshi, and Bill Tomlinson.
2008. Exploring the role of the reader in the activity
of blogging. In Proc. of CHI, pages 1111?1120.
G. Carenini, R. T. Ng, and X. Zhou. 2007. Summariz-
ing Email Conversations with Clue Words. In Proc.
conf. on World Wide Web, pages 91?100.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for Mining and Summarizing Text
Conversations. Morgan Claypool.
Jaegul Choo, Changhyun Lee, Chandan K Reddy, and
Haesun Park. 2013. Utopian: User-driven topic
modeling based on interactive nonnegative matrix
factorization. IEEE Trans. Visualization & Comp.
Graphics, 19(12):1992?2001.
Jason Chuang, Sonal Gupta, Christopher Manning, and
Jeffrey Heer. 2013. Topic model diagnostics: As-
sessing domain relevance via topical alignment. In
Proc. Conf. on Machine Learning, pages 612?620.
51
Andy Cockburn, Amy Karlson, and Benjamin B Bed-
erson. 2008. A review of overview+ detail, zoom-
ing, and focus+ context interfaces. ACM Computing
Surveys (CSUR), 41(1):2.
Kushal Dave, Martin Wattenberg, and Michael Muller.
2004. Flash forums and forumreader: navigating a
new kind of large-scale online discussion. In Proc.
ACM Conf. on CSCW, pages 232?241.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proc. of
ACL, pages 562?569.
Enamul Hoque and Giuseppe Carenini. 2014. ConVis:
A visual text analytic system for exploring blog con-
versations. (Computer Graphic Forum (to appear)).
Yuening Hu, Jordan Boyd-Graber, and Brianna Sati-
noff. 2011. Interactive topic modeling. In Proc. of
ACL.
Quentin Jones, Gilad Ravid, and Sheizaf Rafaeli. 2004.
Information overload and the message dynamics
of online interaction spaces: A theoretical model
and empirical exploration. Information Systems Re-
search, 15(2):194?210.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013a. Combining intra-and
multi-sentential rhetorical parsing for document-
level discourse analysis. In Proc. of ACL.
Shafiq Joty, Giuseppe Carenini, and Raymond T Ng.
2013b. Topic segmentation and labeling in asyn-
chronous conversations. Journal of Artificial Intelli-
gence Research, 47:521?573.
B. K. Kaye. 2005. Web side story: An exploratory
study of why weblog users say they use weblogs.
AEJMC Annual Conf.
Bernard Kerr. 2003. Thread arcs: An email thread
visualization. In IEEE Symposium on Information
Visualization, pages 211?218.
H. Lam, E. Bertini, P. Isenberg, C. Plaisant, and
S. Carpendale. 2012. Empirical studies in infor-
mation visualization: Seven scenarios. IEEE Trans.
Visualization & Comp. Graphics, 18(9):1520?1536.
Heidi Lam. 2008. A framework of interaction costs
in information visualization. IEEE Trans. Visualiza-
tion & Comp. Graphics, 14(6):1149?1156.
Gilad Mishne. 2006. Information access challenges in
the blogspace. In Workshop on Intelligent Informa-
tion Access (IIIA).
Sean A Munson and Paul Resnick. 2010. Presenting
diverse political opinions: how and how much. In
Proc. of CHI, pages 1457?1466.
Tamara Munzner. 2009. A nested model for visualiza-
tion design and validation. IEEE Trans. Visualiza-
tion & Comp. Graphics, 15(6):921?928.
S. Narayan and C. Cheshire. 2010. Not too long to
read: The tldr interface for exploring and navigating
large-scale discussion spaces. In Hawaii Conf. on
System Sciences (HICSS), pages 1?10.
V?ctor Pascual-Cid and Andreas Kaltenbrunner. 2009.
Exploring asynchronous online discussions through
hierarchical visualisation. In IEEE Conf. on Infor-
mation Visualization, pages 191?196.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D Manning. 2009. Labeled LDA: A su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proc. of EMNLP, pages 248?
256.
Warren Sack. 2000. Conversation map: an interface
for very-large-scale conversations. Journal of Man-
agement Information Systems, 17(3):73?92.
Michael Sedlmair, Miriah Meyer, and Tamara Mun-
zner. 2012. Design study methodology: reflections
from the trenches and the stacks. IEEE Trans. Visu-
alization & Comp. Graphics, 18(12):2431?2440.
Param Vir Singh, Nachiketa Sahoo, and Tridas
Mukhopadhyay. 2010. Seeking variety: A dynamic
model of employee blog reading behavior. Available
at SSRN 1617405.
Markus Steinberger, Manuela Waldner, Marc Streit,
Alexander Lex, and Dieter Schmalstieg. 2011.
Context-preserving visual links. IEEE Trans. Visu-
alization & Comp. Graphics, 17(12):2249?2258.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Gina Danielle Venolia and Carman Neustaedter. 2003.
Understanding sequence and reply relationships
within email conversations: a mixed-model visual-
ization. In Proc. of CHI, pages 361?368.
Fernanda B Vi?egas, Scott Golder, and Judith Donath.
2006. Visualizing email content: portraying rela-
tionships from conversational histories. In Proc. of
CHI, pages 979?988.
Martin Wattenberg and David Millen. 2003. Conver-
sation thumbnails for large-scale discussions. In ex-
tended abstracts on CHI, pages 742?743.
Furu Wei, Shixia Liu, Yangqiu Song, Shimei Pan,
Michelle X Zhou, Weihong Qian, Lei Shi, Li Tan,
and Qiang Zhang. 2010. Tiara: a visual exploratory
text analytic system. In Proc. ACM Conf. on Knowl-
edge Discovery and Data Mining, pages 153?162.
Ka-Ping Yee and Marti Hearst. 2005. Content-
centered discussion mapping. Online Deliberation
2005/DIAC-2005.
Aaron Robert Zinman. 2011. Me, myself, and my hy-
perego: understanding people through the aggrega-
tion of their digital footprints. Ph.D. thesis, MIT.
52
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402?408,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
DiscoTK: Using Discourse Structure for Machine Translation Evaluation
Shafiq Joty Francisco Guzm
?
an Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{sjoty,fguzman,lmarquez,pnakov}@qf.org.qa
Abstract
We present novel automatic metrics for
machine translation evaluation that use
discourse structure and convolution ker-
nels to compare the discourse tree of an
automatic translation with that of the hu-
man reference. We experiment with five
transformations and augmentations of a
base discourse tree representation based
on the rhetorical structure theory, and we
combine the kernel scores for each of them
into a single score. Finally, we add other
metrics from the ASIYA MT evaluation
toolkit, and we tune the weights of the
combination on actual human judgments.
Experiments on the WMT12 and WMT13
metrics shared task datasets show corre-
lation with human judgments that outper-
forms what the best systems that partici-
pated in these years achieved, both at the
segment and at the system level.
1 Introduction
The rapid development of statistical machine
translation (SMT) that we have seen in recent
years would not have been possible without au-
tomatic metrics for measuring SMT quality. In
particular, the development of BLEU (Papineni
et al., 2002) revolutionized the SMT field, al-
lowing not only to compare two systems in a
way that strongly correlates with human judg-
ments, but it also enabled the rise of discrimina-
tive log-linear models, which use optimizers such
as MERT (Och, 2003), and later MIRA (Watanabe
et al., 2007; Chiang et al., 2008) and PRO (Hop-
kins and May, 2011), to optimize BLEU, or an ap-
proximation thereof, directly. While over the years
other strong metrics such as TER (Snover et al.,
2006) and Meteor (Lavie and Denkowski, 2009)
have emerged, BLEU remains the de-facto stan-
dard, despite its simplicity.
Recently, there has been steady increase in
BLEU scores for well-resourced language pairs
such as Spanish-English and Arabic-English.
However, it was also observed that BLEU-like n-
gram matching metrics are unreliable for high-
quality translation output (Doddington, 2002;
Lavie and Agarwal, 2007). In fact, researchers al-
ready worry that BLEU will soon be unable to dis-
tinguish automatic from human translations.
1
This
is a problem for most present-day metrics, which
cannot tell apart raw machine translation output
from a fully fluent professionally post-edited ver-
sion thereof (Denkowski and Lavie, 2012).
Another concern is that BLEU-like n-gram
matching metrics tend to favor phrase-based SMT
systems over rule-based systems and other SMT
paradigms. In particular, they are unable to cap-
ture the syntactic and semantic structure of sen-
tences, and are thus insensitive to improvement
in these aspects. Furthermore, it has been shown
that lexical similarity is both insufficient and not
strictly necessary for two sentences to convey
the same meaning (Culy and Riehemann, 2003;
Coughlin, 2003; Callison-Burch et al., 2006).
The above issues have motivated a large amount
of work dedicated to design better evaluation met-
rics. The Metrics task at the Workshop on Ma-
chine Translation (WMT) has been instrumental in
this quest. Below we present QCRI?s submission
to the Metrics task of WMT14, which consists of
the DiscoTK family of discourse-based metrics.
In particular, we experiment with five different
transformations and augmentations of a discourse
tree representation, and we combine the kernel
scores for each of them into a single score which
we call DISCOTK
light
. Next, we add to the com-
bination other metrics from the ASIYA MT eval-
uation toolkit (Gim?enez and M`arquez, 2010), to
produce the DISCOTK
party
metric.
1
This would not mean that computers have achieved hu-
man proficiency; it would rather show BLEU?s inadequacy.
402
Finally, we tune the relative weights of the met-
rics in the combination using human judgments
in a learning-to-rank framework. This proved
to be quite beneficial: the tuned version of the
DISCOTK
party
metric was the best performing
metric in the WMT14 Metrics shared task.
The rest of the paper is organized as follows:
Section 2 introduces our basic discourse metrics
and the tree representations they are based on.
Section 3 describes our metric combinations. Sec-
tion 4 presents our experiments and results on
datasets from previous years. Finally, Section 5
concludes and suggests directions for future work.
2 Discourse-Based Metrics
In our recent work (Guzm?an et al., 2014), we used
the information embedded in the discourse-trees
(DTs) to compare the output of an MT system to
a human reference. More specifically, we used
a state-of-the-art sentence-level discourse parser
(Joty et al., 2012) to generate discourse trees for
the sentences in accordance with the Rhetorical
Structure Theory (RST) of discourse (Mann and
Thompson, 1988). Then, we computed the simi-
larity between DTs of the human references and
the system translations using a convolution tree
kernel (Collins and Duffy, 2001), which efficiently
computes the number of common subtrees. Note
that this kernel was originally designed for syntac-
tic parsing, and the subtrees are subject to the con-
straint that their nodes are taken with all or none
of their children, i.e., if we take a direct descen-
dant of a given node, we must also take all siblings
of that descendant. This imposes some limitations
on the type of substructures that can be compared,
and motivates the enriched tree representations ex-
plained in subsections 2.1?2.4.
The motivation to compare discourse trees, is
that translations should preserve the coherence re-
lations. For example, consider the three discourse
trees (DTs) shown in Figure 1. Notice that the
Attribution relation in the reference translation is
also realized in the system translation in (b) but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis.
In (Guzm?an et al., 2014), we have shown that
discourse structure provides additional informa-
tion for MT evaluation, which is not captured by
existing metrics that use lexical, syntactic and se-
mantic information; thus, discourse should be con-
sidered when developing new rich metrics.
Here, we extend our previous work by devel-
oping metrics that are based on new representa-
tions of the DTs. In the remainder of this section,
we will focus on the individual DT representations
that we will experiment with; then, the following
section will describe the metric combinations and
tuning used to produce the DiscoTK metrics.
2.1 DR-LEX
1
Figure 2a shows our first representation of the DT.
The lexical items, i.e., words, constitute the leaves
of the tree. The words in an Elementary Discourse
Unit (EDU) are grouped under a predefined tag
EDU, to which the nuclearity status of the EDU
is attached: nucleus vs. satellite. Coherence re-
lations, such as Attribution, Elaboration, and En-
ablement, between adjacent text spans constitute
the internal nodes of the tree. Like the EDUs, the
nuclearity statuses of the larger discourse units are
attached to the relation labels. Notice that with
this representation the tree kernel can easily be ex-
tended to find subtree matches at the word level,
i.e., by including an additional layer of dummy
leaves as was done in (Moschitti et al., 2007). We
applied the same solution in our representations.
2.2 DR-NOLEX
Our second representation DR-NOLEX (Figure 2b)
is a simple variation of DR-LEX
1
, where we ex-
clude the lexical items. This allows us to measure
the similarity between two translations in terms of
their discourse structures alone.
2.3 DR-LEX
2
One limitation of DR-LEX
1
and DR-NOLEX is that
they do not separate the structure, i.e., the skele-
ton, of the tree from its labels. Therefore, when
measuring the similarity between two DTs, they
do not allow the tree kernel to give partial credit
to subtrees that differ in labels but match in their
structures. DR-LEX
2
, a variation of DR-LEX
1
, ad-
dresses this limitation as shown in Figure 2c. It
uses predefined tags SPAN and EDU to build the
skeleton of the tree, and considers the nuclearity
and/or relation labels as properties (added as chil-
dren) of these tags. For example, a SPAN has two
properties, namely its nuclearity and its relation,
and an EDU has one property, namely its nucle-
arity. The words of an EDU are placed under the
predefined tag NGRAM.
403
Elaboration ROOT
SPANNucleus AttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human-written) translation.
AttributionROOT
SPANSatellite SPANNucleus
In Germany voices , the ECB should be the lender of last resort .
(b) A higher quality (system-generated) translation.
SPANROOT
In Germany the ECB should be for the creditors of last resort .
(c) A lower quality (system-generated) translation.
Figure 1: Three discourse trees for the translations of a source sentence: (a) the reference, (b) a higher
quality automatic translation, and (c) a lower quality automatic translation.
2.4 DR-LEX
1.1
and DR-LEX
2.1
Although both DR-LEX
1
and DR-LEX
2
allow the
tree kernel to find matches at the word level, the
words are compared in a bag-of-words fashion,
i.e., if the trees share a common word, the ker-
nel will find a match regardless of its position in
the tree. Therefore, a word that has occurred in
an EDU with status Nucleus in one tree could be
matched with the same word under a Satellite in
the other tree. In other words, the kernel based
on these representations is insensitive to the nu-
clearity status and the relation labels under which
the words are matched. DR-LEX
1.1
, an exten-
sion of DR-LEX
1
, and DR-LEX
2.1
, an extension
of DR-LEX
2
, are sensitive to these variations at
the lexical level. DR-LEX
1.1
(Figure 2d) and DR-
LEX
2.1
(Figure 2e) propagate the nuclearity sta-
tuses and/or the relation labels to the lexical items
by including three more subtrees at the EDU level.
3 Metric Combination and Tuning
In this section, we describe our Discourse Tree
Kernel (DiscoTK) metrics. We have two main
versions: DISCOTK
light
, which combines the five
DR-based metrics, and DISCOTK
party
, which fur-
ther adds the Asiya metrics.
3.1 DISCOTK
light
In the previous section, we have presented several
discourse tree representations that can be used to
compare the output of a machine translation sys-
tem to a human reference. Each representation
stresses a different aspect of the discourse tree.
In order to make our estimations more robust,
we propose DISCOTK
light
, a metric that takes ad-
vantage of all the previous discourse representa-
tions by linearly interpolating their scores. Here
are the processing steps needed to compute this
metric:
(i) Parsing: We parsed each sentence in order to
produce discourse trees for the human references
and for the outputs of the systems.
(ii) Tree enrichment/simplification: For each
sentence-level discourse tree, we generated the
five different tree representations: DR-NOLEX,
DR-LEX
1
, DR-LEX
1.1
, DR-LEX
2
, DR-LEX
2.1
.
(iii) Estimation: We calculated the per-sentence
similarity scores between tree representations of
the system hypothesis and the human reference
using the extended convolution tree kernel as de-
scribed in the previous section. To compute the
system-level similarity scores, we calculated the
average sentence-level similarity; note that this en-
sures that our metric is ?the same? at the system
and at the segment level.
(iv) Normalization: In order to make the scores of
the different representations comparable, we per-
formed a min?max normalization
2
for each met-
ric and for each language pair.
(v) Combination: Finally, for each sentence, we
computed DISCOTK
light
as the average of the
normalized similarity scores of the different repre-
sentations. For system-level experiments, we per-
formed linear interpolation of system-level scores.
2
Where x
?
= (x?min)/(max?min).
404
	


 

