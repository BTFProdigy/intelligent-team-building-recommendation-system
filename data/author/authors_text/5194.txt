Syntactic Features and Word Similarity for Supervised Metonymy
Resolution
Malvina Nissim
ICCS, School of Informatics
University of Edinburgh
mnissim@inf.ed.ac.uk
Katja Markert
ICCS, School of Informatics
University of Edinburgh and
School of Computing
University of Leeds
markert@inf.ed.ac.uk
Abstract
We present a supervised machine learning
algorithm for metonymy resolution, which
exploits the similarity between examples
of conventional metonymy. We show
that syntactic head-modifier relations are
a high precision feature for metonymy
recognition but suffer from data sparse-
ness. We partially overcome this problem
by integrating a thesaurus and introduc-
ing simpler grammatical features, thereby
preserving precision and increasing recall.
Our algorithm generalises over two levels
of contextual similarity. Resulting infer-
ences exceed the complexity of inferences
undertaken in word sense disambiguation.
We also compare automatic and manual
methods for syntactic feature extraction.
1 Introduction
Metonymy is a figure of speech, in which one ex-
pression is used to refer to the standard referent of
a related one (Lakoff and Johnson, 1980). In (1),1
?seat 19? refers to the person occupying seat 19.
(1) Ask seat 19 whether he wants to swap
The importance of resolving metonymies has
been shown for a variety of NLP tasks, e.g., ma-
chine translation (Kamei and Wakao, 1992), ques-
tion answering (Stallard, 1993) and anaphora reso-
lution (Harabagiu, 1998; Markert and Hahn, 2002).
1(1) was actually uttered by a flight attendant on a plane.
In order to recognise and interpret the metonymy
in (1), a large amount of knowledge and contextual
inference is necessary (e.g. seats cannot be ques-
tioned, people occupy seats, people can be ques-
tioned). Metonymic readings are also potentially
open-ended (Nunberg, 1978), so that developing a
machine learning algorithm based on previous ex-
amples does not seem feasible.
However, it has long been recognised that many
metonymic readings are actually quite regular
(Lakoff and Johnson, 1980; Nunberg, 1995).2 In (2),
?Pakistan?, the name of a location, refers to one of
its national sports teams.3
(2) Pakistan had won the World Cup
Similar examples can be regularly found for many
other location names (see (3) and (4)).
(3) England won the World Cup
(4) Scotland lost in the semi-final
In contrast to (1), the regularity of these exam-
ples can be exploited by a supervised machine learn-
ing algorithm, although this method is not pursued
in standard approaches to regular polysemy and
metonymy (with the exception of our own previous
work in (Markert and Nissim, 2002a)). Such an al-
gorithm needs to infer from examples like (2) (when
labelled as a metonymy) that ?England? and ?Scot-
land? in (3) and (4) are also metonymic. In order to
2Due to its regularity, conventional metonymy is also known
as regular polysemy (Copestake and Briscoe, 1995). We use the
term ?metonymy? to encompass both conventional and uncon-
ventional readings.
3All following examples are from the British National Cor-
pus (BNC, http://info.ox.ac.uk/bnc).
Scotland
subj-of subj-of
win lose
context reduction
Pakistan
Scotland-subj-of-losePakistan-subj-of-win
similarity
semantic class
head similarity
role similarity
Pakistan
had won the World Cup lost in the semi-finalScotland
Figure 1: Context reduction and similarity levels
draw this inference, two levels of similarity need to
be taken into account. One concerns the similarity of
the words to be recognised as metonymic or literal
(Possibly Metonymic Words, PMWs). In the above
examples, the PMWs are ?Pakistan?, ?England? and
?Scotland?. The other level pertains to the similar-
ity between the PMW?s contexts (?<subject> (had)
won the World Cup? and ?<subject> lost in the
semi-final?). In this paper, we show how a machine
learning algorithm can exploit both similarities.
Our corpus study on the semantic class of lo-
cations confirms that regular metonymic patterns,
e.g., using a place name for any of its sports teams,
cover most metonymies, whereas unconventional
metonymies like (1) are very rare (Section 2). Thus,
we can recast metonymy resolution as a classifica-
tion task operating on semantic classes (Section 3).
In Section 4, we restrict the classifier?s features to
head-modifier relations involving the PMW. In both
(2) and (3), the context is reduced to subj-of-win.
This allows the inference from (2) to (3), as they
have the same feature value. Although the remain-
ing context is discarded, this feature achieves high
precision. In Section 5, we generalize context simi-
larity to draw inferences from (2) or (3) to (4). We
exploit both the similarity of the heads in the gram-
matical relation (e.g., ?win? and ?lose?) and that of
the grammatical role (e.g. subject). Figure 1 illus-
trates context reduction and similarity levels.
We evaluate the impact of automatic extraction of
head-modifier relations in Section 6. Finally, we dis-
cuss related work and our contributions.
2 Corpus Study
We summarize (Markert and Nissim, 2002b)?s an-
notation scheme for location names and present an
annotated corpus of occurrences of country names.
2.1 Annotation Scheme for Location Names
We identify literal, metonymic, and mixed readings.
The literal reading comprises a locative (5)
and a political entity interpretation (6).
(5) coral coast of Papua New Guinea
(6) Britain?s current account deficit
We distinguish the following metonymic patterns
(see also (Lakoff and Johnson, 1980; Fass, 1997;
Stern, 1931)). In a place-for-people pattern,
a place stands for any persons/organisations associ-
ated with it, e.g., for sports teams in (2), (3), and (4),
and for the government in (7).4
(7) a cardinal element in Iran?s strategy when
Iranian naval craft [...] bombarded [...]
In a place-for-event pattern, a location
name refers to an event that occurred there (e.g., us-
ing the word Vietnam for the Vietnam war). In a
place-for-product pattern a place stands for
a product manufactured there (e.g., the word Bor-
deaux referring to the local wine).
The category othermet covers unconventional
metonymies, as (1), and is only used if none of the
other categories fits (Markert and Nissim, 2002b).
We also found examples where two predicates are
involved, each triggering a different reading.
(8) they arrived in Nigeria, hitherto a leading
critic of the South African regime
In (8), both a literal (triggered by ?arriving in?)
and a place-for-people reading (triggered by
?leading critic?) are invoked. We introduced the cat-
egory mixed to deal with these cases.
2.2 Annotation Results
Using Gsearch (Corley et al, 2001), we randomly
extracted 1000 occurrences of country names from
the BNC, allowing any country name and its variants
listed in the CIA factbook5 or WordNet (Fellbaum,
4As the explicit referent is often underspecified, we intro-
duce place-for-people as a supertype category and we
evaluate our system on supertype classification in this paper. In
the annotation, we further specify the different groups of people
referred to, whenever possible (Markert and Nissim, 2002b).
5http://www.cia.gov/cia/publications/
factbook/
1998) to occur. Each country name is surrounded by
three sentences of context.
The 1000 examples of our corpus have been inde-
pendently annotated by two computational linguists,
who are the authors of this paper. The annotation
can be considered reliable (Krippendorff, 1980) with
95% agreement and a kappa (Carletta, 1996) of .88.
Our corpus for testing and training the algorithm
includes only the examples which both annotators
could agree on and which were not marked as noise
(e.g. homonyms, as ?Professor Greenland?), for a
total of 925. Table 1 reports the reading distribution.
Table 1: Distribution of readings in our corpus
reading freq %
literal 737 79.7
place-for-people 161 17.4
place-for-event 3 .3
place-for-product 0 .0
mixed 15 1.6
othermet 9 1.0
total non-literal 188 20.3
total 925 100.0
3 Metonymy Resolution as a Classification
Task
The corpus distribution confirms that metonymies
that do not follow established metonymic patterns
(othermet) are very rare. This seems to be the
case for other kinds of metonymies, too (Verspoor,
1997). We can therefore reformulate metonymy res-
olution as a classification task between the literal
reading and a fixed set of metonymic patterns that
can be identified in advance for particular semantic
classes. This approach makes the task comparable to
classic word sense disambiguation (WSD), which is
also concerned with distinguishing between possible
word senses/interpretations.
However, whereas a classic (supervised) WSD
algorithm is trained on a set of labelled instances
of one particular word and assigns word senses to
new test instances of the same word, (supervised)
metonymy recognition can be trained on a set of
labelled instances of different words of one seman-
tic class and assign literal readings and metonymic
patterns to new test instances of possibly different
words of the same semantic class. This class-based
approach enables one to, for example, infer the read-
ing of (3) from that of (2).
We use a decision list (DL) classifier. All features
encountered in the training data are ranked in the DL
(best evidence first) according to the following log-
likelihood ratio (Yarowsky, 1995):
Log
(
Pr(reading
i
|feature
k
)
?
j 6=i
Pr(reading
j
|feature
k
)
)
We estimated probabilities via maximum likeli-
hood, adopting a simple smoothing method (Mar-
tinez and Agirre, 2000): 0.1 is added to both the de-
nominator and numerator.
The target readings to be distinguished are
literal, place-for-people,place-for-
event, place-for-product, othermet and
mixed. All our algorithms are tested on our an-
notated corpus, employing 10-fold cross-validation.
We evaluate accuracy and coverage:
Acc = # correct decisions made
# decisions made
Cov = # decisions made
# test data
We also use a backing-off strategy to the most fre-
quent reading (literal) for the cases where no
decision can be made. We report the results as ac-
curacy backoff (Acc
b
); coverage backoff is always
1. We are also interested in the algorithm?s perfor-
mance in recognising non-literal readings. There-
fore, we compute precision (P ), recall (R), and F-
measure (F ), where A is the number of non-literal
readings correctly identified as non-literal (true pos-
itives) and B the number of literal readings that are
incorrectly identified as non-literal (false positives):
P = A/(A + B)
R = A
#non-literal examples in the test data
F = 2PR/(R + P )
The baseline used for comparison is the assign-
ment of the most frequent reading literal.
4 Context Reduction
We show that reducing the context to head-modifier
relations involving the Possibly Metonymic Word
achieves high precision metonymy recognition.6
6In (Markert and Nissim, 2002a), we also considered local
and topical cooccurrences as contextual features. They con-
stantly achieved lower precision than grammatical features.
Table 2: Example feature values for role-of-head
role-of-head (r-of-h) example
subj-of-win England won the World Cup (place-for-people)
subjp-of-govern Britain has been governed by . . . (literal)
dobj-of-visit the Apostle had visited Spain (literal)
gen-of-strategy in Iran?s strategy . . . (place-for-people)
premod-of-veteran a Vietnam veteran from Rhode Island (place-for-event)
ppmod-of-with its border with Hungary (literal)
Table 3: Role distribution
role freq #non-lit
subj 92 65
subjp 6 4
dobj 28 12
gen 93 20
premod 94 13
ppmod 522 57
other 90 17
total 925 188
We represent each example in our corpus by a sin-
gle feature role-of-head, expressing the grammat-
ical role of the PMW (limited to (active) subject,
passive subject, direct object, modifier in a prenom-
inal genitive, other nominal premodifier, dependent
in a prepositional phrase) and its lemmatised lexi-
cal head within a dependency grammar framework.7
Table 2 shows example values and Table 3 the role
distribution in our corpus.
We trained and tested our algorithm with this fea-
ture (hmr).8 Results for hmr are reported in the
first line of Table 5. The reasonably high precision
(74.5%) and accuracy (90.2%) indicate that reduc-
ing the context to a head-modifier feature does not
cause loss of crucial information in most cases. Low
recall is mainly due to low coverage (see Problem 2
below). We identified two main problems.
Problem 1. The feature can be too simplistic, so
that decisions based on the head-modifier relation
can assign the wrong reading in the following cases:
? ?Bad? heads: Some lexical heads are semanti-
cally empty, thus failing to provide strong evi-
dence for any reading and lowering both recall
and precision. Bad predictors are the verbs ?to
have? and ?to be? and some prepositions such
as ?with?, which can be used with metonymic
(talk with Hungary) and literal (border with
Hungary) readings. This problem is more se-
rious for function than for content word heads:
precision on the set of subjects and objects is
81.8%, but only 73.3% on PPs.
? ?Bad? relations: The premod relation suffers
from noun-noun compound ambiguity. US op-
7We consider only one link per PMW, although cases like (8)
would benefit from including all links the PMW participates in.
8The feature values were manually annotated for the follow-
ing experiments, adapting the guidelines in (Poesio, 2000). The
effect of automatic feature extraction is described in Section 6.
eration can refer to an operation in the US (lit-
eral) or by the US (metonymic).
? Other cases: Very rarely neglecting the remain-
ing context leads to errors, even for ?good?
lexical heads and relations. Inferring from the
metonymy in (4) that ?Germany? in ?Germany
lost a fifth of its territory? is also metonymic,
e.g., is wrong and lowers precision.
However, wrong assignments (based on head-
modifier relations) do not constitute a major problem
as accuracy is very high (90.2%).
Problem 2. The algorithm is often unable to make
any decision that is based on the head-modifier re-
lation. This is by far the more frequent problem,
which we adress in the remainder of the paper. The
feature role-of-head accounts for the similarity be-
tween (2) and (3) only, as classification of a test in-
stance with a particular feature value relies on hav-
ing seen exactly the same feature value in the train-
ing data. Therefore, we have not tackled the infer-
ence from (2) or (3) to (4). This problem manifests
itself in data sparseness and low recall and coverage,
as many heads are encountered only once in the cor-
pus. As hmr?s coverage is only 63.1%, backoff to a
literal reading is required in 36.9% of the cases.
5 Generalising Context Similarity
In order to draw the more complex inference from
(2) or (3) to (4) we need to generalise context sim-
ilarity. We relax the identity constraint of the orig-
inal algorithm (the same role-of-head value of the
test instance must be found in the DL), exploiting
two similarity levels. Firstly, we allow to draw infer-
ences over similar values of lexical heads (e.g. from
subj-of-win to subj-of-lose), rather than over iden-
tical ones only. Secondly, we allow to discard the
Table 4: Example thesaurus entries
lose[V]: win
1
0.216, gain
2
0.209, have
3
0.207, ...
attitude[N]:stance
1
0.181, behavior
2
0.18, ..., strategy
17
0.128
lexical head and generalise over the PMW?s gram-
matical role (e.g. subject). These generalisations al-
low us to double recall without sacrificing precision
or increasing the size of the training set.
5.1 Relaxing Lexical Heads
We regard two feature values r-of-h and r-of-h? as
similar if h and h? are similar. In order to capture the
similarity between h and h? we integrate a thesaurus
(Lin, 1998) in our algorithm?s testing phase. In Lin?s
thesaurus, similarity between words is determined
by their distribution in dependency relations in a
newswire corpus. For a content word h (e.g., ?lose?)
of a specific part-of-speech a set of similar words ?
h
of the same part-of-speech is given. The set mem-
bers are ranked in decreasing order by a similarity
score. Table 4 reports example entries.9
Our modified algorithm (relax I) is as follows:
1. train DL with role-of-head as in hmr; for each test in-
stance observe the following procedure (r-of-h indicates
the feature value of the test instance);
2. if r-of-h is found in the DL, apply the corresponding rule
and stop;
2? otherwise choose a number n ? 1 and set i = 1;
(a) extract the ith most similar word h
i
to h from the
thesaurus;
(b) if i > n or the similarity score of h
i
< 0.10, assign
no reading and stop;
(b?) otherwise: if r-of-h
i
is found in the DL, apply cor-
responding rule and stop; if r-of-h
i
is not found in
the DL, increase i by 1 and go to (a);
The examples already covered by hmr are clas-
sified in exactly the same way by relax I (see Step
2). Let us therefore assume we encounter the test
instance (4), its feature value subj-of-lose has not
been seen in the training data (so that Step 2 fails
and Step 2? has to be applied) and subj-of-win is in
the DL. For all n ? 1, relax I will use the rule for
subj-of-win to assign a reading to ?Scotland? in (4)
as ?win? is the most similar word to ?lose? in the
thesaurus (see Table 4). In this case (2b?) is only
9In the original thesaurus, each ?
h
is subdivided into clus-
ters. We do not take these divisions into account.
0 10 20 30 40 50
Thesaurus Iterations (n)
0.1 0.1
0.2 0.2
0.3 0.3
0.4 0.4
0.5 0.5
0.6 0.6
0.7 0.7
0.8 0.8
0.9 0.9
R
es
ul
ts
Precision
Recall
F-Measure
Figure 2: Results for relax I
applied once as already the first iteration over the
thesaurus finds a word h
1
with r-of-h
1
in the DL.
The classification of ?Turkey? with feature value
gen-of-attitude in (9) required 17 iterations to find
a word h
17
(?strategy?; see Example (7)) similar to
?attitude?, with r-of-h
17
(gen-of-strategy) in the DL.
(9) To say that this sums up Turkey?s attitude as
a whole would nevertheless be untrue
Precision, recall and F-measure for n ?
{1, ..., 10, 15, 20, 25, 30, 40, 50} are visualised in
Figure 2. Both precision and recall increase with
n. Recall more than doubles from 18.6% in hmr
to 41% and precision increases from 74.5% in hmr
to 80.2%, yielding an increase in F-measure from
29.8% to 54.2% (n = 50). Coverage rises to 78.9%
and accuracy backoff to 85.1% (Table 5).
Whereas the increase in coverage and recall is
quite intuitive, the high precision achieved by re-
lax I requires further explanation. Let S be the set
of examples that relax I covers. It consists of two
subsets: S1 is the subset aleady covered by hmr and
its treatment does not change in relax I, yielding the
same precision. S2 is the set of examples that re-
lax I covers in addition to hmr. The examples in S2
consist of cases with highly predictive content word
heads as (a) function words are not included in the
thesaurus and (b) unpredictive content word heads
like ?have? or ?be? are very frequent and normally
already covered by hmr (they are therefore members
of S1). Precision on S2 is very high (84%) and raises
the overall precision on the set S.
Cases that relax I does not cover are mainly due
to (a) missing thesaurus entries (e.g., many proper
Table 5: Results summary for manual annotation.
For relax I and combination we report best results
(50 thesaurus iterations).
algorithm Acc Cov Acc
b
P R F
hmr .902 .631 .817 .745 .186 .298
relax I .877 .789 .851 .802 .410 .542
relax II .865 .903 .859 .813 .441 .572
combination .894 .797 .870 .814 .510 .627
baseline .797 1.00 .797 n/a .000 n/a
names or alternative spelling), (b) the small num-
ber of training instances for some grammatical roles
(e.g. dobj), so that even after 50 thesaurus iterations
no similar role-of-head value could be found that is
covered in the DL, or (c) grammatical roles that are
not covered (other in Table 3).
5.2 Discarding Lexical Heads
Another way of capturing the similarity between (3)
and (4), or (7) and (9) is to ignore lexical heads and
generalise over the grammatical role (role) of the
PMW (with the feature values as in Table 3: subj,
subjp, dobj, gen, premod, ppmod). We therefore de-
veloped the algorithm relax II.
1. train decision lists:
(a) DL1 with role-of-head as in hmr
(b) DL2 with role;
for each test instance observe the following procedure (r-
of-h and r are the feature values of the test instance);
2. if r-of-h is found in the DL1, apply the corresponding rule
and stop;
2? otherwise, if r is found in DL2, apply the corresponding
rule.
Let us assume we encounter the test instance
(4), subj-of-lose is not in DL1 (so that Step 2 fails
and Step 2? has to be applied) and subj is in DL2.
The algorithm relax II will assign a place-for-
people reading to ?Scotland?, as most subjects in
our corpus are metonymic (see Table 3).
Generalising over the grammatical role outper-
forms hmr, achieving 81.3% precision, 44.1% re-
call, and 57.2% F-measure (see Table 5). The algo-
rithm relax II also yields fewer false negatives than
relax I (and therefore higher recall) since all sub-
jects not covered in DL1 are assigned a metonymic
reading, which is not true for relax I.
5.3 Combining Generalisations
There are several ways of combining the algorithms
we introduced. In our experiments, the most suc-
cessful one exploits the facts that relax II performs
better than relax I on subjects and that relax I per-
forms better on the other roles. Therefore the algo-
rithm combination uses relax II if the test instance
is a subject, and relax I otherwise. This yields the
best results so far, with 87% accuracy backoff and
62.7% F-measure (Table 5).
6 Influence of Parsing
The results obtained by training and testing our clas-
sifier with manually annotated grammatical relations
are the upper bound of what can be achieved by us-
ing these features. To evaluate the influence pars-
ing has on the results, we used the RASP toolkit
(Briscoe and Carroll, 2002) that includes a pipeline
of tokenisation, tagging and state-of-the-art statisti-
cal parsing, allowing multiple word tags. The toolkit
also maps parse trees to representations of gram-
matical relations, which we in turn could map in a
straightforward way to our role categories.
RASP produces at least partial parses for 96% of
our examples. However, some of these parses do
not assign any role of our roleset to the PMW ?
only 76.9% of the PMWs are assigned such a role
by RASP (in contrast to 90.2% in the manual anno-
tation; see Table 3). RASP recognises PMW sub-
jects with 79% precision and 81% recall. For PMW
direct objects, precision is 60% and recall 86%.10
We reproduced all experiments using the auto-
matically extracted relations. Although the relative
performance of the algorithms remains mostly un-
changed, most of the resulting F-measures are more
than 10% lower than for hand annotated roles (Ta-
ble 6). This is in line with results in (Gildea and
Palmer, 2002), who compare the effect of man-
ual and automatic parsing on semantic predicate-
argument recognition.
7 Related Work
Previous Approaches to Metonymy Recognition.
Our approach is the first machine learning algorithm
to metonymy recognition, building on our previous
10We did not evaluate RASP?s performance on relations that
do not involve the PMW.
Table 6: Results summary for the different algo-
rithms using RASP. For relax I and combination
we report best results (50 thesaurus iterations).
algorithm Acc Cov Acc
b
P R F
hmr .884 .514 .812 .674 .154 .251
relax I .841 .666 .821 .619 .319 .421
relax II .820 .769 .823 .621 .340 .439
combination .850 .672 .830 .640 .388 .483
baseline .797 1.00 .797 n/a .000 n/a
work (Markert and Nissim, 2002a). The current ap-
proach expands on it by including a larger number
of grammatical relations, thesaurus integration, and
an assessment of the influence of parsing. Best F-
measure for manual annotated roles increased from
46.7% to 62.7% on the same dataset.
Most other traditional approaches rely on hand-
crafted knowledge bases or lexica and use vi-
olations of hand-modelled selectional restrictions
(plus sometimes syntactic violations) for metonymy
recognition (Pustejovsky, 1995; Hobbs et al, 1993;
Fass, 1997; Copestake and Briscoe, 1995; Stallard,
1993).11 In these approaches, selectional restric-
tions (SRs) are not seen as preferences but as ab-
solute constraints. If and only if such an absolute
constraint is violated, a non-literal reading is pro-
posed. Our system, instead, does not have any a
priori knowledge of semantic predicate-argument re-
strictions. Rather, it refers to previously seen train-
ing examples in head-modifier relations and their la-
belled senses and computes the likelihood of each
sense using this distribution. This is an advantage as
our algorithm also resolved metonymies without SR
violations in our experiments. An empirical compar-
ison between our approach in (Markert and Nissim,
2002a)12 and an SRs violation approach showed that
our approach performed better.
In contrast to previous approaches (Fass, 1997;
Hobbs et al, 1993; Copestake and Briscoe, 1995;
Pustejovsky, 1995; Verspoor, 1996; Markert and
Hahn, 2002; Harabagiu, 1998; Stallard, 1993), we
use a corpus reliably annotated for metonymy for
evaluation, moving the field towards more objective
11(Markert and Hahn, 2002) and (Harabagiu, 1998) en-
hance this with anaphoric information. (Briscoe and Copes-
take, 1999) propose using frequency information besides syn-
tactic/semantic restrictions, but use only a priori sense frequen-
cies without contextual features.
12Note that our current approach even outperforms (Markert
and Nissim, 2002a).
evaluation procedures.
Word Sense Disambiguation. We compared our
approach to supervised WSD in Section 3, stressing
word-to-word vs. class-to-class inference. This al-
lows for a level of abstraction not present in standard
supervised WSD. We can infer readings for words
that have not been seen in the training data before,
allow an easy treatment of rare words that undergo
regular sense alternations and do not have to anno-
tate and train separately for every individual word to
treat regular sense distinctions.13
By exploiting additional similarity levels and inte-
grating a thesaurus we further generalise the kind of
inferences we can make and limit the size of anno-
tated training data: as our sampling frame contains
553 different names, an annotated data set of 925
samples is quite small. These generalisations over
context and collocates are also applicable to stan-
dard WSD and can supplement those achieved e.g.,
by subcategorisation frames (Martinez et al, 2002).
Our approach to word similarity to overcome data
sparseness is perhaps most similar to (Karov and
Edelman, 1998). However, they mainly focus on the
computation of similarity measures from the train-
ing data. We instead use an off-the-shelf resource
without adding much computational complexity and
achieve a considerable improvement in our results.
8 Conclusions
We presented a supervised classification algorithm
for metonymy recognition, which exploits the simi-
larity between examples of conventional metonymy,
operates on semantic classes and thereby enables
complex inferences from training to test examples.
We showed that syntactic head-modifier relations
are a high precision feature for metonymy recogni-
tion. However, basing inferences only on the lex-
ical heads seen in the training data leads to data
sparseness due to the large number of different lex-
ical heads encountered in natural language texts. In
order to overcome this problem we have integrated
a thesaurus that allows us to draw inferences be-
13Incorporating knowledge about particular PMWs (e.g., as
a prior) will probably improve performance, as word idiosyn-
cracies ? which can still exist even when treating regular sense
distinctions ? could be accounted for. In addition, knowledge
about the individual word is necessary to assign its original se-
mantic class.
tween examples with similar but not identical lex-
ical heads. We also explored the use of simpler
grammatical role features that allow further gener-
alisations. The results show a substantial increase in
precision, recall and F-measure. In the future, we
will experiment with combining grammatical fea-
tures and local/topical cooccurrences. The use of
semantic classes and lexical head similarity gener-
alises over two levels of contextual similarity, which
exceeds the complexity of inferences undertaken in
standard supervised word sense disambiguation.
Acknowledgements. The research reported in this
paper was supported by ESRC Grant R000239444.
Katja Markert is funded by an Emmy Noether Fel-
lowship of the Deutsche Forschungsgemeinschaft
(DFG). We thank three anonymous reviewers for
their comments and suggestions.
References
E. Briscoe and J. Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proc. of LREC, 2002,
pages 1499?1504.
T. Briscoe and A. Copestake. 1999. Lexical rules in
constraint-based grammar. Computational Linguis-
tics, 25(4):487?526.
J. Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
A. Copestake and T. Briscoe. 1995. Semi-productive
polysemy and sense extension. Journal of Semantics,
12:15?67.
S. Corley, M. Corley, F. Keller, M. Crocker, and S.
Trewin. 2001. Finding syntactic structure in unparsed
corpora: The Gsearch corpus query system. Comput-
ers and the Humanities, 35(2):81?94.
D. Fass. 1997. Processing Metaphor and Metonymy.
Ablex, Stanford, CA.
C. Fellbaum, ed. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, Mass.
D. Gildea and M. Palmer. 2002. The necessity of parsing
for predicate argument recognition. In Proc. of ACL,
2002, pages 239?246.
S. Harabagiu. 1998. Deriving metonymic coercions
from WordNet. In Workshop on the Usage of WordNet
in Natural Language Processing Systems, COLING-
ACL, 1998, pages 142?148.
J. R. Hobbs, M. E. Stickel, D. E. Appelt, and P. Martin.
1993. Interpretation as abduction. Artificial Intelli-
gence, 63:69?142.
S. Kamei and T. Wakao. 1992. Metonymy: Reassess-
ment, survey of acceptability and its treatment in ma-
chine translation systems. In Proc. of ACL, 1992,
pages 309?311.
Y. Karov and S. Edelman. 1998. Similarity-based
word sense disambiguation. Computational Linguis-
tics, 24(1):41-59.
K. Krippendorff. 1980. Content Analysis: An Introduc-
tion to Its Methodology. Sage Publications.
G. Lakoff and M. Johnson. 1980. Metaphors We Live By.
Chicago University Press, Chicago, Ill.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. of International Conference on
Machine Learning, Madison, Wisconsin.
K. Markert and U. Hahn. 2002. Understanding
metonymies in discourse. Artificial Intelligence,
135(1/2):145?198.
K. Markert and M. Nissim. 2002a. Metonymy resolu-
tion as a classification task. In Proc. of EMNLP, 2002,
pages 204?213.
Katja Markert and Malvina Nissim. 2002b. Towards a
corpus annotated for metonymies: the case of location
names. In Proc. of LREC, 2002, pages 1385?1392.
D. Martinez and E. Agirre. 2000. One sense per collo-
cation and genre/topic variations. In Proc. of EMNLP,
2000.
D. Martinez, E. Agirre, and L. Marquez. 2002. Syntactic
features for high precision word sense disambiguation.
In Proc. of COLING, 2002.
G. Nunberg. 1978. The Pragmatics of Reference. Ph.D.
thesis, City University of New York, New York.
G. Nunberg. 1995. Transfers of meaning. Journal of
Semantics, 12:109?132.
M. Poesio, 2000. The GNOME Annotation Scheme Man-
ual. University of Edinburgh, 4th version. Available
from http://www.hcrc.ed.ac.uk/?gnome.
J. Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, Mass.
D. Stallard. 1993. Two kinds of metonymy. In Proc. of
ACL, 1993, pages 87?94.
G. Stern. 1931. Meaning and Change of Meaning.
Go?teborg: Wettergren & Kerbers Fo?rlag.
C. Verspoor. 1996. Lexical limits on the influence of
context. In Proc. of CogSci, 1996, pages 116?120.
C. Verspoor. 1997. Conventionality-governed logical
metonymy. In H. Bunt et al, editors, Proc. of IWCS-2,
1997, pages 300?312.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL, 1995, pages 189?196.
  	
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 825?832
Manchester, August 2008
From Words to Senses: A Case Study of Subjectivity Recognition
Fangzhong Su
School of Computing
University of Leeds, UK
fzsu@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds, UK
markert@comp.leeds.ac.uk
Abstract
We determine the subjectivity of word
senses. To avoid costly annotation, we
evaluate how useful existing resources es-
tablished in opinion mining are for this
task. We show that results achieved with
existing resources that are not tailored to-
wards word sense subjectivity classifica-
tion can rival results achieved with super-
vision on a manually annotated training
set. However, results with different re-
sources vary substantially and are depen-
dent on the different definitions of subjec-
tivity used in the establishment of the re-
sources.
1 Introduction
In recent years, subjectivity analysis and opinion
mining have attracted considerable attention in the
NLP community. Unlike traditional information
extraction and document classification tasks which
usually focus on extracting facts or categorizing
documents into topics (e.g., ?sports?, ?politics?,
?medicine?), subjectivity analysis focuses on de-
termining whether a language unit (such as a word,
sentence or document) expresses a private state,
opinion or attitude and, if so, what polarity is ex-
pressed, i.e. a positive or negative attitude.
Inspired by Esuli and Sebastiani (2006) and
Wiebe and Mihalcea (2006), we explore the auto-
matic detection of the subjectivity of word senses,
in contrast to the more frequently explored task
of determining the subjectivity of words (see Sec-
tion 2). This is motivated by many words being
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
subjectivity-ambiguous, i.e. having both subjec-
tive and objective senses, such as the word positive
with its two example senses given below.
1
(1) positive, electropositive?having a positive electric
charge;?protons are positive? (objective)
(2) plus, positive?involving advantage or good; ?a plus
(or positive) factor? (subjective)
Subjectivity labels for senses add an additional
layer of annotation to electronic lexica and allow to
group many fine-grained senses into higher-level
classes based on subjectivity/objectivity. This can
increase the lexica?s usability. As an example,
Wiebe and Mihalcea (2006) prove that subjectiv-
ity information for WordNet senses can improve
word sense disambiguation tasks for subjectivity-
ambiguous words (such as positive). In addition,
Andreevskaia and Bergler (2006) show that the
performance of automatic annotation of subjectiv-
ity at the word level can be hurt by the presence of
subjectivity-ambiguous words in the training sets
they use. Moreover, the prevalence of different
word senses in different domains also means that
a subjective or an objective sense of a word might
be dominant in different domains; thus, in a sci-
ence text positive is likely not to have a subjective
reading. The annotation of words as subjective and
objective or positive and negative independent of
sense or domain does not capture such distinctions.
In this paper, we validate whether word sense
subjectivity labeling can be achieved with existing
resources for subjectivity analysis at the word and
sentence level without creating a dedicated, man-
ually annotated training set of WordNet senses la-
beled for subjectivity.
2
We show that such an ap-
1
All examples in this paper are from WordNet 2.0.
2
We use a subset of WordNet senses that are manually an-
notated for subjectivity as test set (see Section 3).
825
proach ? even using a simple rule-based unsu-
pervised algorithm ? can compete with a stan-
dard supervised approach and also compares well
to prior research on word sense subjectivity label-
ing. However, success depends to a large degree
on the definition of subjectivity used in the estab-
lishment of the prior resources.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Sec-
tion 3 introduces our human annotation scheme
for word sense subjectivity and also shows that
subjectivity-ambiguous words are frequent. Sec-
tion 4 describes our proposed classification algo-
rithms in detail. Section 5 presents the experimen-
tal results and evaluation, followed by conclusions
and future work in Section 6.
2 Related Work
There has been extensive research in opinion min-
ing at the document level, for example on product
and movie reviews (Pang et al, 2002; Pang and
Lee, 2004; Dave et al, 2003; Popescu and Etzioni,
2005). Several other approaches focus on the
subjectivity classification of sentences (Kim and
Hovy, 2005; Kudo and Matsumoto, 2004; Riloff
and Wiebe, 2003). They often build on the pres-
ence of subjective words in the sentence to be clas-
sified.
Closer to our work is the large body of work
on the automatic, context-independent classifica-
tion of words according to their polarity, i.e as pos-
itive or negative (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003; Kim and Hovy,
2004; Takamura et al, 2005). They use either
co-occurrence patterns in corpora or dictionary-
based methods. Many papers assume that sub-
jectivity recognition, i.e. separating subjective
from objective words, has already been achieved
prior to polarity recognition and test against word
lists containing subjective words only (Hatzivas-
siloglou and McKeown, 1997; Takamura et al,
2005). However, Kim and Hovy (2004) and An-
dreevskaia and Bergler (2006) also address the
classification into subjective/objective words and
show this to be a potentially harder task than po-
larity classification with lower human agreement
and automatic performance.
There are only two prior approaches address-
ing word sense subjectivity or polarity classifi-
cation. Esuli and Sebastiani (2006) determine
the polarity of word senses in WordNet, distin-
guishing among positive, negative and objective.
They expand a small, manually determined seed
set of strongly positive/negative WordNet senses
by following WordNet relations and use the result-
ing larger training set for supervised classification.
The resulting labeled WordNet gives three scores
for each sense, representing the positive, negative
and objective score respectively. However, there
is no evaluation as to the accuracy of their ap-
proach. They then extend their work (Esuli and
Sebastiani, 2007) by applying the Page Rank algo-
rithm to ranking the WordNet senses in terms of
how strongly a sense possesses a given semantic
property (e.g., positive or negative).
Wiebe and Mihalcea (2006) label word senses
in WordNet as subjective or objective. They use a
method relying on distributional similarity as well
as an independent, large manually annotated opin-
ion corpus (MPQA) (Wiebe et al, 2005) for deter-
mining subjectivity. One of the disadvantages of
their algorithm is that it is restricted to senses that
have distributionally similar words in the MPQA
corpus, excluding 23.2% of their test data from au-
tomatic classification.
3 Human Annotation of Word Sense
Subjectivity and Polarity
In contrast to other researchers (Hatzivassiloglou
and McKeown, 1997; Takamura et al, 2005), we
do not see polarity as a category that is depen-
dent on prior subjectivity assignment and therefore
applicable to subjective senses only. We follow
Wiebe and Mihalcea (2006) in that we see subjec-
tive expressions as private states ?that are not open
to objective observation or verification?. This in-
cludes direct references to emotions, beliefs and
judgements (such as anger, criticise) as well as ex-
pressions that let a private state be inferred, for
example by referring to a doctor as a quack. In
contrast, polarity refers to positive or negative as-
sociations of a word or sense. Whereas there is a
dependency in that most subjective senses have a
relatively clear polarity, polarity can be attached to
objective words/senses as well. For example, tu-
berculosis is not subjective ? it does not describe
a private state, is objectively verifiable and would
not cause a sentence containing it to carry an opin-
ion, but it does carry negative associations for the
vast majority of people.
We therefore annotate subjectivity of word
senses similarly to Wiebe and Mihalcea (2006),
826
distinguishing between subjective (S), objective
(O) or both (B). Both is used if a literal and
metaphoric sense of a word are collapsed into one
WordNet synset or if a WordNet synset contains
both opinionated and objective expressions (such
as bastard and illegitimate child in Ex. 3 below).
We expand their annotation scheme by also an-
notating polarity, using the labels positive (P), neg-
ative (N) and varied (V). The latter is used when a
sense?s polarity varies strongly with the context,
such as Example 8 below, where we would ex-
pect uncompromising to be a judgement but this
judgement will be positive or negative depending
on what a person is uncompromising about. To
avoid prevalence of personalised associations, an-
notators were told to only annotate polarity for
subjective senses, as well as objective senses that
carry a strong association likely to be shared by
most people at least in Western culture (such as the
negative polarity for words referring to diseases
and crime). Other objective senses would receive
the label O:NoPol.
Therefore, we have 7 sub categories in total:
O:NoPol, O:P, O:N, S:P, S:N, S:V, and B. The
notation before and after the colon represents the
subjectivity and polarity label respectively. We list
some annotated examples below.
(3) bastard, by-blow, love child, illegitimate child, illegiti-
mate, whoreson? the illegitimate offspring of unmar-
ried parents (B)
(4) atrophy?undergo atrophy; ?Muscles that are not used
will atrophy? (O:N)
(5) guard, safety, safety device?a device designed to pre-
vent injury (O:P)
(6) nasty, awful?offensive or even (of persons) mali-
cious;?in a nasty mood?;?a nasty accident?; ?a nasty
shock? (S:N)
(7) happy?enjoying or showing or marked by joy or plea-
sure or good fortune; ?a happy smile?;?spent many
happy days on the beach?; ?a happy marriage? (S:P)
(8) uncompromising, inflexible?not making concessions;
?took an uncompromising stance in the peace talks?
(S:V)
As far as we are aware, this is the first annotation
scheme for both subjectivity and polarity of word
senses. We believe both are relevant for opinion
extraction: subjectivity for finding and analysing
directly expressed opinions, and polarity for ei-
ther classifying these further or extracting objec-
tive words that, however, serve to ?colour? a text
or present bias rather than explicitly stated opin-
ions. Su and Markert (2008) describe the annota-
tion scheme and agreement study in full.
3.1 Agreement Study
We used the Micro-WNOp corpus containing 1105
WordNet synsets to test our annotation scheme.
3
The Micro-WNOp corpus is representative of the
part-of-speech distribution in WordNet.
Two annotators (both near-native English speak-
ers) independently annotated 606 synsets of the
Micro-WNOp corpus for subjectivity and polarity.
One annotator is the second author of this paper
whereas the other is not a linguist. The overall
agreement using all 7 categories is 84.6%, with a
kappa of 0.77, showing high reliability for a dif-
ficult pragmatic task. This at first seems at odds
with the notion of sentiment as a fuzzy category as
expressed in (Andreevskaia and Bergler, 2006) but
we believe is due to three factors:
? The annotation of senses instead of words
splits most subjectivity-ambiguous words
into several senses, removing one source of
annotation difficulty.
? The annotation of senses in a dictionary pro-
vided the annotators with sense descriptions
in form of Wordnet glosses as well as re-
lated senses, providing more information than
a pure word annotation task.
? The split of subjectivity and polarity annota-
tion made the task clearer and the annotation
of only very strong connotations for objective
word senses ?de-individualized? the task.
As in this paper we are only interested in subjec-
tivity recognition, we collapse S:V, S:P, and S:N
into a single label S and O:NoPol, O:P, and O:N
into a single label O. Label B remains unchanged.
For this three-way annotation overall percentage
agreement is 90.1%, with a kappa of 0.79.
3.2 Gold Standard
After cases with disagreement were negotiated be-
tween the two annotators, a gold standard annota-
tion was agreed upon. Our test set consists of this
agreed set as well as the remainder of the Micro-
WNOp corpus annotated by one of the annotators
alone after agreement was established. This set is
available for research purposes at http://www.
comp.leeds.ac.uk/markert/data.
3
The corpus has originally been annotated by the
providers (Esuli and Sebastiani, 2007) with scores for posi-
tive, negative and objective/no polarity, thus a mixture of sub-
jectivity and polarity annotation. We re-annotated the corpus
with our annotation scheme.
827
How many words are subjectivity-ambiguous?
As the number of senses increases with word fre-
quency, we expect rare words to be less likely
to be subjectivity-ambiguous than frequent words.
The Micro-WNOp corpus contains relatively fre-
quent words so we will get an overestimation of
subjective-ambiguous word types from this cor-
pus, though not necessarily of word tokens. It in-
cludes 298 different words with all their synsets
in WordNet 2.0. Of all words, 97 (32.5%) are
subjectivity-ambiguous, a substantial number.
4 Algorithms
In this section, we present experiments using five
different resources as training sets or clue sets for
this task. The first is the Micro-WNOp corpus with
our own dedicated word sense subjectivity anno-
tation which is used in a standard supervised ap-
proach as training and test set via 10-fold cross-
validation. This technique presupposes a man-
ual annotation effort tailored directly to our task
to provide training data. As it is costly to create
such training sets, we investigate whether exist-
ing resources such as two different subjective sen-
tence lists (Section 4.2) and two different subjec-
tive word lists (Section 4.3) can be adapted to pro-
vide training data or clue sets although they do not
provide any information about word senses. All
resources are used to create training data for su-
pervised approaches; the subjective word lists are
also used in a simple rule-based unsupervised ap-
proach.
All algorithms were tested on the Micro-WNOp
corpus by comparing to the human gold stan-
dard annotation. However, we excluded all senses
with the label both from Micro-WNOp for test-
ing the automatic algorithms, resulting in a final
1061 senses, with 703 objective and 358 subjec-
tive senses. We also compare all algorithms to a
baseline of always assigning the most frequent cat-
egory (objective) to each sense, which results in an
overall accuracy of 66.3%.
4.1 Standard Supervised Approach: 10-fold
Cross-validation (CV) on Micro-WNOp
We use 10-fold cross validation for training and
testing on the annotated synsets in the Micro-
WNOp corpus. We applied a Naive Bayes clas-
sifier,
4
using the following three types of features:
4
We also experimented with KNN, Maximum Entropy,
Rocchio and SVM algorithms and overall Naive Bayes per-
Lexical Features: These are unigrams in the
glosses. We use a bag-of-words approach and filter
out stop words.
As glosses are usually quite short, using a bag-
of-word feature representation will result in high-
dimensional and sparse feature vectors, which of-
ten deteriorate classification performance. In order
to address this problem to some degree, we also ex-
plored other features which are available as train-
ing and test instances are WordNet synsets.
Part-of-Speech (POS) Features: each sense
gets its POS as a feature (adjective, noun, verb or
adverb).
Relation Features: WordNet relations are good
indicators for determining subjectivity as many
of them are subjectivity-preserving. For exam-
ple, if sense A is subjective, then its antonym
sense B is likely to be subjective. We employ
8 relations here?antonym, similar-to, derived-
from, attribute, also-see, direct-hyponym, direct-
hypernym, and extended-antonym. Each relation
R leads to 2 features that describe for a sense A
how many links of that type it has to synsets in the
subjective or the objective training set respectively.
Finally, we represent the feature weights
through a TF*IDF measure.
Considering the size of WordNet (115,424
synsets inWordNet 2.0), the labeled Micro-WNOp
corpus is small. Therefore, the question arises
whether it is possible to adapt other data sources
that provide subjectivity information to our task.
4.2 Sentence Collections: Movie and MPQA
It is reasonable to cast word sense subjectivity
classification as a sentence classification task, with
the glosses that WordNet provides for each sense
as the sentences to be classified. Then we can in
theory feed any collection of annotated subjective
and objective sentences as training data into our
classifier while the annotated Micro-WNOp cor-
pus is used as test data. We experimented with two
different available data sets to test this assumption.
Movie-domain Subjectivity Data Set (Movie):
Pang and Lee (2004) used a collection of labeled
subjective and objective sentences in their work
on review classification.
5
The data set contains
5000 subjective sentences, extracted from movie
reviews collected from the Rotten Tomatoes web
formed best.
5
Available at http://www.cs.cornell.edu/
People/pabo/movie-review-data/
828
site.
6
The 5000 objective sentences were col-
lected from movie plot summaries from the In-
ternet Movie Database (IMDB). The assumption
is that all the snippets from the Rotten Tomatoes
pages are subjective (as they come from a review
site), while all the sentences from IMDB are ob-
jective (as they focus on movie plot descriptions).
The MPQA Corpus contains news articles
manually annotated at the phrase level for opin-
ions, their polarity and their strength. The cor-
pus (Version 1.2) contains 11,112 sentences. We
convert it into a corpus of subjective and objective
sentences following exactly the approach in (Riloff
et al, 2003; Riloff and Wiebe, 2003) and obtain
6127 subjective and 4985 objective sentences re-
spectively. Basically any sentence that contains at
least one strong subjective annotation at the phrase
level is seen as a subjective sentence.
We again use a Naive Bayes algorithm with lex-
ical unigram features. Note that part-of-speech
and relation features are not applicable here as the
training set consists of corpus sentences, notWord-
Net synsets.
4.3 Word Lists: General Inquirer and
Subjectivity List
Several word lists annotated for subjectivity or po-
larity such as the General Inquirer (GI)
7
or the sub-
jectivity clues list (SL) collated by Janyce Wiebe
and her colleagues
8
are available.
The General Inquirer (GI) was developed by
Philip Stone and colleagues in the 1960s. It con-
centrates on word polarity. Here we make the
simple assumption that both positive and negative
words in the GI list are subjective clues whereas
all other words are objective.
The Subjectivity Lexicon (SL) centers on
subjectivity so that it is ideally suited for our
task. It provides fine-grained information for each
clue, such as part-of-speech, subjectivity strength
(strong/weak), and prior polarity (positive, nega-
tive, or neutral). For example, object(verb) is a
subjective clue whereas object(noun) is objective.
Regarding strength, the adjective evil is marked as
strong subjective whereas the adjective exposed is
marked as a weak subjective clue.
Both lexica do not include any information
about word senses and therefore cannot be used
directly for subjectivity assignment at the sense
6
http://www.rottentomatoes.com/
7
http://www.wjh.harvard.edu/
?
inquirer/
8
http://www.cs.pitt.edu/mpqa/
level. For example, at least one sense of any
subjectivity-ambiguous word will be labeled incor-
rectly if we just adopt a word-based label. In addi-
tion, these lists are far from complete: compared to
the over 100,000 synsets in WordNet, GI contains
11,788 words marked for polarity (1915 positive,
2291 negative and 7582 no-polarity words) and the
SL list contains about 8,000 subjective words.
Still, it is a reasonable assumption that any gloss
that contains several subjective words indicates a
subjective sense overall. This intuition is strength-
ened by the characteristics of glosses. They nor-
mally are short and concise without a complex syn-
tactic structure, thus the occurrence of subjective
words in such a short string is likely to indicate
a subjective sense overall. This contrasts, for ex-
ample, with sentences in newspapers where one
clause might express an opinion, whereas other
parts of the sentence are objective.
Therefore, for the rule-based unsupervised
algorithm we lemmatized and POS-tagged the
glosses in the Micro-WNOp test set. Then we
compute a subjectivity score S for each synset by
summing up the weight values of all subjectivity
clues in its gloss. If S is equal or higher than an
agreed threshold T, then the synset is classified as
subjective, otherwise as objective. For the GI lexi-
con, all subjectivity clues have the same weight 1,
whereas for the SL list we assign a weight value
2 to strongly subjective clues and 1 to weakly
subjective clues. We experimented with several
thresholds T and report here the results for the best
thresholds, which were 2 for SL and 4 for the GI
word list. The corresponding methods are called
Rule-SL and Rule-GI.
This approach does not allow us to easily inte-
grate relational WordNet features. It might also
suffer from the incompleteness of the lexica and
the fact that it has to make decisions for bor-
derline cases (at the value of the threshold set).
We therefore explored instead to generate larger,
more reliable training data consisting of Word-
Net synsets from the word lists. To achieve this,
we assign a subjectivity score S as above to all
WordNet synsets (excluding synsets in the test set).
If S is higher or equal to a threshold T
1
it is added
to the subjective training set, if it is lower or equal
to T
2
it is added to the objective training set. This
allows us to choose quite clear thresholds so that
borderline cases with a score between T
1
and T
2
are not in the training set. It also allows to use part-
829
of-speech and relational features as the training set
then consists of WordNet synsets. In this way,
we can automatically generate (potentially noisy)
training data of WordNet senses marked for sub-
jectivity without annotating any WordNet senses
manually for subjectivity.
We experimented with several different thresh-
old sets but we found that they actually have a min-
imal impact on the final results. We report here the
best results for a threshold T
1
of 4 and T
2
of 2 for
the SL lexicon and of 3 and 1 respectively for the
GI word list.
5 Experiments and Evaluation
We measure the classification performance with
overall accuracy as well as precision, recall and
balanced F-score for both categories (objective and
subjective). All results are summarised in Table 1.
Results are compared to the baseline of majority
classification using a McNemar test at the signifi-
cance level of 5%.
5.1 Experimental Results
Table 1 shows that SL
?
performs best among all
the methodologies. All CV, Rule-SL and SL meth-
ods significantly beat the baseline. In addition, if
we compare the results of methods with and with-
out additional parts-of-speech and WordNet rela-
tion features, we see a small but consistent im-
provement when we use additional features. It is
also worthwhile to expand the rule-based unsuper-
vised method into a method for generating train-
ing data and use additional features as SL
?
signifi-
cantly outperforms Rule-SL.
5.2 Discussion
Word Lists. Surprisingly, using SL greatly outper-
forms GI, regardless of whether we use the super-
vised or unsupervised method or whether we use
lexical features only or the other features as well.
9
There are several reasons for this. First, the GI
lexicon is annotated for polarity, not subjectivity.
More specifically, words that we see as objective
but with a strong positive or negative association
(such as words for crimes) and words that we see
as subjective are annotated with the same polar-
ity label in the GI lexicon. Therefore, the GI def-
inition of subjectivity does not match ours. Also,
9
This pattern is repeated for all threshold combinations,
which are not reported here.
the GI lexicon does not operate with a clearly ex-
pressed polarity definition, leading to conflicting
annotations and casting doubt on its widespread
use in the opinion mining community as a gold
standard (Turney and Littman, 2003; Takamura et
al., 2005; Andreevskaia and Bergler, 2006). For
example, amelioration is seen as non-polar in GI
but improvement is annotated with positive polar-
ity. Second, in contrast to SL, GI does not consider
different parts-of-speech of a word and subjectiv-
ity strength (strong/weak subjectivity). Third, GI
contains many fewer subjective clues than SL.
Sentence Data. When using the Movie dataset
and MPQA corpus as training data, the results
are not satisfactory. We first checked the purity
of these two datasets to see whether they are too
noisy. For this purpose, we used a naive Bayes
algorithm with unigram features and conducted a
10-fold cross validation experiment on recognizing
subjective/objective sentences within the Movie
dataset and MPQA independently. Interestingly,
the accuracy for the Movie dataset and MPQA cor-
pus achieved 91% and 76% respectively. Consid-
ering that they are balanced datasets with a most
frequent category baseline of about 50%, this ac-
curacy is high, especially for the Movie dataset.
However, again the subjectivity definition in the
Movie corpus does not seem to match ours. Re-
call that we see a word sense or a sentence as sub-
jective if it expresses a private state (i.e., emotion,
opinion, sentiment, etc.), and objective otherwise.
Inspecting the movie data set, we found that in-
deed the sentences included in its subjective set
would mostly be seen as subjective in our sense
as well as they contain opinions about the movie
such as it desperately wants to be a wacky , screw-
ball comedy , but the most screwy thing here is how
so many talented people were convinced to waste
their time. It is also true that the sentences (plot
descriptions) in its ?objective? data set relatively
rarely contain opinions about the movie. How-
ever, they still contain other opinionated content
like opinions and emotions of the characters in the
movie such as the obsession of a character with
John Lennon in the beatles fan is a drama about
Albert, a psychotic prisoner who is a devoted fan
of John Lennon and the beatles. Since the data
set?s definition of subjective sentences is closer
to ours than the one for objective sentences, we
conducted a one-class learning approach (Li and
Liu, 2003) using Movie?s subjective sentences as
830
Table 1: Results
Method Subjective Objective Accuracy
Precision Recall F-score Precision Recall F-score
Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3%
CV 65.2% 52.8% 58.3% 78.1% 85.6% 81.7% 74.6%?
CV
?
69.5% 55.3% 61.6% 79.4% 87.6% 83.3% 76.7%?
Movie 43.8% 60.1% 50.6% 74.9% 60.7% 67.1% 60.5%
MPQA 44.5% 78.5% 56.8% 82.1% 50.1% 62.2% 59.7%
GI 50.4% 39.4% 44.2% 72.2% 80.2% 76.0% 66.4%
GI
?
54.5% 33.5% 41.5% 71.7% 85.8% 78.1% 68.1%
SL 64.3% 62.8% 63.6% 81.3% 82.2% 81.8% 75.7%?
SL
?
66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%?
Rule-GI 38.5% 5.6% 9.8% 66.5% 95.4% 78.4% 65.1%
Rule-SL 59.7% 70.4% 64.6% 83.4% 75.8% 79.4% 74.0%?
1
CV, GI and SL correspond to methods using lexical features only.
2
CV
?
, GI
?
and SL
?
correspond to methods using a feature combination of lexical,
part-of-speech, and WordNet relations.
3
? indicates results significantly better than the baseline.
the only training data. The algorithm
10
combines
Expectation Maximization and Naive Bayes algo-
rithms, and we used randomly extracted 50,000
unlabeled synsets in WordNet as the necessary un-
labeled data. This approach achieves an accuracy
of 69.4% on Micro-WNOp, which is significantly
better than the baseline.
The subjectivity definition in the MPQA corpus
is quite close to ours. However, our mapping from
its phrase annotation to sentence annotation might
be too coarse-grained as many sentences in the cor-
pus span several clauses containing both opinions
and factual description. We assume that this is pos-
sibly also the reason why its purity is lower than
in the Movie dataset. We therefore experimented
again with a one-class learning approach using just
the subjective phrases in MPQA as training data.
The accuracy does improve to 67.6% but is still
not significantly higher than the baseline.
5.3 Comparison to Prior Approaches
Esuli and Sebastiani (2006) make their labeled
WordNet SentiWordNet 1.0 publically available.
11
Recall that they actually use polarity classification:
however, as there is a dependency between po-
larity and subjectivity classification for subjective
senses, we map their polarity scores to our subjec-
tivity labels as follows. If the sum of positive and
10
Available at http://www.cs.uic.edu/?liub/LPU/.
11
Available at http://sentiwordnet.isti.cnr.
it/
negative scores of a sense in SentiWordNet is more
than or equal to 0.5, then it is subjective and other-
wise objective.
12
Using this mapping, it achieves
an accuracy of 75.3% on the Micro-WNOp cor-
pus, compared to our gold standard. Therefore our
methods CV
?
and SL
?
perform slightly better than
theirs, although the improvement is not significant.
The task definition in Wiebe and Mihal-
cea (2006) is much more similar to ours but they
use different annotated test data, which is not pub-
lically available, so an exact comparison is not pos-
sible. Both data sets, however, seem to include rel-
atively frequent words. One disadvantage of their
method is that it is not applicable to all WordNet
senses as it is dependent on distributionally sim-
ilar words being available in the MPQA. Thus,
23% of their test data is excluded from evaluation,
whereas our methods can be used on any WordNet
sense. They measure precision and recall for sub-
jective senses in a precision/recall curve: Precision
is about 48/9% at a recall of 60% for subjective
senses whereas our best SL
?
method has a preci-
sion of 66% at about the same recall. Although
this suggests better performance of our method, it
is not possible to draw final conclusions from this
comparison due to the data set differences.
12
We experimented with slightly different mappings but
this mapping gave SentiWordNet the best possible result.
There is a relatively large number of cases with a 0.5/0.5 split
in SentiWordNet, making it hard to decide between subjective
and objective senses.
831
6 Conclusion and Future Work
We proposed different ways of extracting training
data and clue sets for word sense subjectivity label-
ing from existing opinion mining resources. The
effectiveness of the resulting algorithms depends
greatly on the generated training data, more specif-
ically on the different definitions of subjectivity
used in resource creation. However, we were able
to show that at least one of these methods (based
on the SL word list) resulted in a classifier that per-
formed on a par with a supervised classifier that
used dedicated training data developed for this task
(CV). Thus, it is possible to avoid any manual an-
notation for the subjectivity classification of word
senses.
Our future work will explore new methodolo-
gies in feature representation by importing more
background information (e.g., syntactic informa-
tion). Furthermore, our current method of integrat-
ing the rich relation information in WordNet (us-
ing them as standard features) does not use joint
classification of several senses. Instead, we think
it will be more promising to use the relations to
construct graphs for semi-supervised graph-based
learning of word sense subjectivity. In addition, we
will also explore whether the derived sense labels
improve applications such as sentence classifica-
tion and clustering WordNet senses.
References
Andreevskaia, Alina and Sabine Bergler. 2006. Min-
ing WordNet for Fuzzy Sentiment: Sentiment Tag
Extraction from WordNet Glosses. Proceedings of
EACL?06.
Dave, Kushal, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
Proceedings of WWW?03.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource for
Opinion Mining. Proceedings of LREC?06.
Esuli, Andrea and Fabrizio Sebastiani. 2007. PageR-
anking WordNet Synsets: An application to Opinion
Mining. Proceedings of ACL?07.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. Proceedings of ACL?97.
Kim, Soo-Min and Eduard Hovy. 2004. Determining
the Sentiment of Opinions. Proceedings of COL-
ING?04.
Kim, Soo-Min and Eduard Hovy. 2005. Automatic
Detection of Opinion Bearing Words and Sentences.
Proceedings of ICJNLP?05.
Kudo, Taku and Yuji Matsumoto. 2004. A Boosting
Algorithm for Classification of Semi-structured Text.
Proceedings of EMNLP?04.
Li, Xiaoli and Bing Liu. 2003. Learning to classify
text using positive and unlabeled data. Proceedings
of IJCAI?03.
Pang, Bo and Lillian Lee. 2004. A Sentiment Edu-
cation: Sentiment Analysis Using Subjectivity sum-
marization Based on Minimum Cuts. Proceedings of
ACL?04.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification us-
ing Machine Learning Techniques. Proceedings of
EMNLP?02.
Popescu, Ana-Maria and Oren Etzioni. 2003. Ex-
tracting Product Fatures and Opinions from Reviews
Proceedings of EMNLP?05.
Riloff, Ellen, JanyceWiebe, and TheresaWilson. 2003.
Learning Subjective Nouns using Extraction Pattern
Bootstrapping. Proceedings of CoNLL?03
Riloff, Ellen and Janyce Wiebe. 2003. Learning Ex-
traction Patterns for Subjective Expressions. Pro-
ceedings of EMNLP?03.
Su, Fangzhong and Katja Markert. 2008. Elicit-
ing Subjectivity and Polarity Judgements on Word
Senses. Proceedings of Coling?08 workshop of Hu-
man Judgements in Computational Linguistics.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting Semantic Orientations of
Words using Spin Model. Proceedings of ACL?05.
Turney, Peter and Michael Littman. 2003. Measuring
Praise and Criticism: Inference of Semantic Orien-
tation from Association. ACM Transaction on Infor-
mation Systems.
Wiebe, Janyce and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation.
832
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 628?637,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Comparison of Windowless and Window-Based Computational
Association Measures as Predictors of Syntagmatic Human Associations
Justin Washtell
School of Computing
University of Leeds
washtell@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds
markert@comp.leeds.ac.uk
Abstract
Distance-based (windowless) word asso-
cation measures have only very recently
appeared in the NLP literature and their
performance compared to existing win-
dowed or frequency-based measures is
largely unknown. We conduct a large-
scale empirical comparison of a variety of
distance-based and frequency-based mea-
sures for the reproduction of syntagmatic
human assocation norms. Overall, our
results show an improvement in the pre-
dictive power of windowless over win-
dowed measures. This provides support
to some of the previously published the-
oretical advantages and makes window-
less approaches a promising avenue to
explore further. This study also serves
as a first comparison of windowed meth-
ods across numerous human association
datasets. During this comparison we
also introduce some novel variations of
window-based measures which perform as
well as or better in the human association
norm task than established measures.
1 Introduction
Automatic discovery of semantically associated
words has attracted a large amount of attention in
the last decades and a host of computational asso-
ciation measures have been proposed to deal with
this task (see Section 2). These measures tradi-
tionally rely on the co-ocurrence frequency of two
words in a corpus to estimate a relatedness score.
There has been a recent emergence of distance-
based language modelling techiques in NLP (Sav-
icki and Hlavacova, 2002; Terra and Clarke, 2004)
in which the number of tokens separating words
is the essential quantity. While some of this work
has considered distance-based alternatives to con-
ventional association measures (Hardcastle, 2005;
Washtell, 2009), there has been no principled em-
pirical evaluation of these measures as predictors
of human association. We remedy this by conduct-
ing a thorough comparison of a wide variety of
frequency-based and distance-based measures as
predictors of human association scores as elicited
in several different free word association tasks.
In this work we focus on first-order associ-
ation measures as predictors of syntagmatic as-
sociations. This is in contrast to second and
higher-order measures which are better predictors
of paradigmatic associations, or word similarity.
The distinction between syntagmatic and paradig-
matic relationship types is neither exact nor mu-
tually exclusive, and many paradigmatic relation-
ships can be observed syntagmatically in the text.
Roughly in keeping with (Rapp, 2002), we hereby
regard paradigmatic assocations as those based
largely on word similarity (i.e. including those
typically classed as synonyms, antonyms, hyper-
nyms, hyponyms etc), whereas syntagmatic as-
sociations are all those words which strongly in-
voke one another yet which cannot readily be
said to be similar. Typically these will have an
identifiable semantic or grammatical relationship
(meronym/holonym: stem ? flower, verb/object:
eat ? food etc), or may have harder-to-classify top-
ical or idiomatic relationships (family ? Christmas,
rock ? roll).
We will show in Section 3.2 that syntagmatic
relations by themselves constitute a substantial
25-40% of the strongest human responses to cue
words. Although the automatic detection of these
assocations in text has received less attention
than that of paradigmatic associations, they are
nonetheless important in applications such as the
resolution of bridging anaphora (Vieira and Poe-
sio, 2000).
1
Furthermore, first-order associations
1
where for example resolving my house ? the windows to
the windows of my house can be aided by the knowledge that
windows are often (syntagmatically) associated with houses.
628
are often the basis of higher-order vector word-
space models used for predicting paradigmatic
relationships: i.e. through the observation of
words which share similar sets of syntagmatic as-
sociations. Therefore improvements made at the
level we are concerned with may reasonably be
expected to carry through to applications which
hinge on the identification of paradigmatic rela-
tionships.
After a discussion of previous work in Sec-
tion 2, we formulate the exact association mea-
sures and parameter settings which we compare
in Section 3, where we also introduce the corpora
and human association sets used. Then, by using
evaluations similar to those described in (Baroni
et al, 2008) and by Rapp (2002), we show that
the best distance-based measures correlate better
overall with human association scores than do the
best window based configurations (see Section 4),
and that they also serve as better predictors of the
strongest human associations (see Section 5).
2 Related Work
Measures based on co-ocurrence frequency.
The standard way of estimating the syntagmatic
association of word pairs in a corpus is to ex-
amine the frequency of their co-occurence, and
then usually to compare this to some expected fre-
quency. There are a host of measures which ex-
ist for this purpose. After raw co-occurrence fre-
quency, the simplest and most prevalent in the
literature is Pointwise Mutual Information, fa-
mously used by Church (1989) (as the associa-
tion ratio). This is defined as the log of the ra-
tio of the observed co-occurrence frequency to the
frequency expected under independence. More
sophisticated and statistically-informed measures
include t-Score, z-Score, Chi-Squared and Log-
Likelihood (see Evert (2005) for a thorough re-
view).
All of these measures have in common that they
require co-occurrence frequency to be specified,
and therefore require some definition of a region
within which to count co-occurrences. This re-
gion might be the entirety of a document at one
extreme, or a bigram at the other. A versatile and
hugely popular generalised approach is therefore
to consider a ??window?? of w words, where w can
be varied to suit the application. Unsurprisingly,
it has been found that this is a parameter which
can have a significant impact upon performance
(Yarowsky and Florian, 2002; Lamjiri et al, 2004;
Wang, 2005). While choosing an optimum win-
dow size for an application is often subject to
trial and error, there are some generally recog-
nized trade-offs between small versus large win-
dows, such as the impact of data-sparseness, and
the nature of the associations retrieved (Church
and Hanks, 1989; Church and Hanks, 1991; Rapp,
2002)
Measures based on distance between words in
the text. The idea of using distance as an al-
ternative to frequency for modelling language has
been touched upon in recent literature (Savicki and
Hlavacova, 2002; Terra and Clarke, 2004; Hard-
castle, 2005). Washtell (2009) showed that it is
possible to build distance-based analogues of ex-
isting syntagmatic association measures, by using
the notions of mean and expected distance rather
than of frequency. These measures have certain
theoretical qualities - notably scale-independence
and relative resilience to data-sparseness - which
might be expected to provide gains in tasks such
as the reproduction of human association norms
from corpus data. The specific measure introduced
by Washtell, called Co-Dispersion, is based upon
an established biogeographic dispersion measure
(Clark and Evans, 1954). We provide a thor-
ough empirical investigation of Co-Dispersion and
some of its derivatives herein.
Measures based on syntactic relations. Sev-
eral researchers (Lin, 1998; Curran, 2003; Pado
and Lapata, 2007) have used word space models
based on grammatical relationships for detecting
and quantifying (mostly paradigmatic) word asso-
ciations. In this paper, we will not use syntactic
relation measures for two main reasons. Firstly
these depend on the availability of parsers, which
is not a given for many languages. Secondly, this
may not be the most pertinent approach for pre-
dicting human free associations, in which certain
observed relationsips can be hard to express in
terms of syntactic relationships.
3 Methodology
Similar to (Rapp, 2002; Baroni et al, 2008, among
others), we use comparison to human assocation
datasets as a test bed for the scores produced by
computational association measures. An alterna-
tive might be to validate scores against those de-
rived from a structured resource such as WordNet.
629
Table 1: Human association datasets
Name Origin Cues Respondents
Kent Kent & Rosanoff (1910) 100 ? 1000
Minnesota Russell & Jenkins (1954) 100 ? 1000
EAT Kiss et al(1973) 8400 100
Florida Nelson et al(1980) 5019 ? 140
However, relatedness measures for WordNet are
many and varied and are themselves the subject of
evaluation (Pedersen et al, 2004). Although hu-
man association datasets have their own peculiari-
ties, they do at least provide some kind of definite
Gold Standard. Yet another alternative might be to
incorporate our computational association scores
into an application (such as anaphora resolution),
and measure the performance of that, but noise
from other submodules would complicate evalu-
ation. We leave such extensions to possible future
work.
We use evaluations similar to those used before
(Rapp, 2002; Pado and Lapata, 2007; Baroni et
al., 2008, among others). However, whereas most
existing studies use only one dataset, or hand-
selected parts thereof, we aim to evaluate mea-
sures across four different human datasets. In this
way we hope to get as unbiased a picture as possi-
ble.
3.1 Association data
The datasets used are listed in Table 1. While
the exact experimental conditions may differ, the
datasets used were all elicited using the same ba-
sic methodology: by presenting individual words
(cues) to a number of healthy human subjects and
asking in each case for the word that is most imme-
diately or strongly evoked. An association score
can then be derived for each cue/response pair in a
dataset by dividing the number of participants pro-
viding a given response by the number who were
presented with the cue word. In Table 1, respon-
dents refers to the number of people from whom
a response was solicited for each cue word in a
study (this is not to be confused with the number
of unique responses).
Of these four datasets, one (Kent & Rosanoff)
appears not to have been previously used in any
peer-reviewed study of corpus-derived lexical as-
sociation. It is worth noting that some of these
datasets are quite dated, which might affect corre-
lations with corpus-derived scores, as culture and
contemporary language have a fundamental im-
pact upon the associations humans form (White
and Abrams, 2004).
3.2 Frequency of Syntagmatic Associations
To verify that strong human associations do in-
clude a large number of syntagmatic associations,
we manually annotated all pairs consisting of
a cue and its strongest human response in the
Minnesota and Kent datasets as expressing ei-
ther a syntagmatic or a paradigmatic relationship.
The overall set to be annotated consisted of 200
pairs.
Annotators were given short (half-page) guide-
lines on syntagmatic and paradigmatic assoca-
tions, stating that very similar items (including
hyponyms/hypernyms) as well as antonyms were
to be judged as paradigmatic whereas words that
do not fulfil this criterion are to be judged as
syntagmatic. The two annotators were the au-
thors of this paper (one native and one near-native
speaker). After independent annotation, agree-
ment was measured at a percentage agreement of
91/93% and a kappa of 0.80/0.82 for Minnesota
and Kent, respectively. Therefore, the distinction
can be made with high reliability.
Overall, 27/39% of the human responses
were syntagmatic in the Kent/Minnesota datasets,
showing that syntagmatic relations make up a
large proportion of even the strongest human as-
sociations.
3.3 Corpora
We use two randomized subsets of the British Na-
tional Corpus (BNC), a representative 100 million
word corpus of British English (Burnard, 1995):
one 10 million word sample, and a 1 million word
sample. A vocabulary of approximately 33,000
word types was used. The selected words included
approximately 24,000 word types comprising all
cue and target words from the multiple sets of hu-
man association norms to be used in this study. To
these were added a top-cut of the most frequent
words in the BNC, until the total of 33,000 word
types was reached. The resultant set included ap-
630
proximately the 24,000 most common word types
in the BNC, with the remaining 9000 words types
therefore comprising relatively uncommon words
taken from the human associative responses.
The words included in the vocabulary ac-
counted for over 94.5% of tokens in the corpus.
Although statistics for the remaining word types
in the BNC were not gathered, their correspond-
ing tokens were left in the corpus so that these
could be properly accounted for when calculating
distances and window spans.
In order to maximize matching between word
types in the corpus and association norms, all
words in both were normalized by converting to
lower-case and removing hyphens and periods.
Words consisting entirely of numerals, or numer-
als and punctuation, and all ?phrasal? associa-
tive responses (those containing spaces) were dis-
carded. The 33,000 word count was satisfied after
making these normalizations.
In order to maximize the variety of the language
in the samples, the subsets were built from ap-
proximately the first 2000 words only of each ran-
domly selected document from the BNC (a similar
strategy to that used in constructing the 1 million
word Brown Corpus). Both a 10 million word and
a 1 million word sample were constructed in this
fashion, allowing us to also examine the effects of
varying corpus size and content.
3.4 Association measures used
3.4.1 Frequency-based measures
In the following, x is the cue word and y a (possi-
ble) response word. Therefore p(x) is the proba-
bility of observing x, and p(x?) refers to the prob-
ability of not observing x.
Pointwise Mutual Information (hereonin PMI)
was introduced in Section 2. For ranking word
pairs, we can neglect the usual logarithm.
PMI =
p(x, y)
p(x)p(y)
PMI is infamous for its tendency to attribute very
high association scores to pairs involving low fre-
quency words, as the denominator is small in such
cases, even though the evidence for association in
such cases is also small. This can result in some
unlikely associations. There exist a number of al-
ternative measures which factor in the amount of
evidence to give an estimate of the significance of
association. One popular and statistically appeal-
ing such measure is Log-Likelihood (LL) (Dun-
ning, 1993). LL works on a similar principle to
PMI but considers the ratio of the observed to ex-
pected co-occurrence frequencies for all contin-
gencies (i.e. including those where the words do
not co-occur). LL, as it most frequently appears in
the literature, is not actually a measure of positive
association: it also responds to significant negative
association. Therefore LL is arguably not suited to
the task in hand. Krenn & Evert (2001) experiment
with one-tailed variants of LL and Chi-Squared
measures, although they do not define these vari-
ants. Here, we construct a one-tailed variant of LL
by simply reversing the signs of the terms which
respond to negative association.
LL
1tail
= p(x, y) log
p(x, y)
p(x)p(y)
? p(x, y?) log
p(x, y?)
p(x)p(y?)
? p(x?, y) log
p(x?, y)
p(x?)p(y)
+ p(x?, y?) log
p(x?, y?)
p(x?)p(y?)
LL does not have a clear analogue amongst
the distance-based measures (introduced in Sec-
tion 3.4.2), whereas PMI for instance does. We
therefore construct variants of PMI and other mea-
sures which take the amount of evidence into ac-
count in a way which can be directly reproduced
in the distance domain. For this we borrow from
Sackett (2001) who asserts that, all other things
being equal, statistical significance is proportional
to the square root of the sample size. There are a
number of ways one might quantify sample size.
We take a consistent approach across the various
distance-based and frequency-based measures: we
assume sample size to be equivalent to the lesser of
the frequencies of the two words as this represents
the total number of words available for pairing,
with fewer observed pairs therefore being consid-
ered to constitute negative evidence.
PMI
sig
=
?
min(p(x), p(y))
p(x, y)
p(x)p(y)
All of the above measures are symmetric. Human
associative responses however are not (Michel-
bacher et al, 2007): a person?s tendency to give
the response because to the cue why does not nec-
essarily reflect their tendency to give the response
why to the cue because.
2
A simple asymmetric as-
sociation measure is conditional probability (CP)
2
This notion of assymmetry is not to be confused with
631
- the probability of observing the response, given
that the cue has already occurred.
CP = p(y|x) =
p(x, y)
p(x)
CP suffers from the fact that it does not account
at all for the general frequency of the response
word. It therefore tends to favour very frequent
words, such as function words. An obvious so-
lution would be to divide CP by the frequency of
the response word, however this merely results in
PMI which is symmetric. By multiplying CP with
PMI (and taking the root, to simplify) we obtain a
measure which is asymmetric yet does not overtly
favour frequent response words.
3
We refer to this
herein as Semi-Conditional Information (SCI).
SCI =
p(x, y)
p(x)
?
p(y)
We also explore variants of both CP and SCI with
the additional significance correction presented for
PMI
sig
. These can be easily inferred from the for-
mulae above.
3.4.2 Distance-based Measures
Co-Dispersion (herein CD), introduced by
Washtell (2009), is defined as the ratio of the
mean observed distance to the expected distance,
where the expected distance is derived from
the frequency of the more frequent word type.
Distance refers to the number of tokens separat-
ing an occurrence of one word and the nearest
occurrence of another word. Pairs spanning an
intervening occurrence of either word type or a
document boundary are not considered. Note that
here we specify only the generalised mean M , as
we wish to keep the specific choice of mean as a
parameter to be explored,
CD =
1/max(p(x), p(y))
M(dist
xy1
. . . dist
xyn
)
that of direction in the text. While the two may correlate, one
can find ample counter-examples: jerky triggers beef more
strongly than beef triggers jerky.
3
Note that Wettler & Rapp (1993) introduced a more gen-
eral asymmetric measure for predicting human associations,
by employing an exponent parameter to p(y). Our formuli-
sation is equivalent to their measure with an exponent of 0.5,
whereas they found an exponent of 0.66 to be most effective
in their empirical study. Exponents of 0 and 1 result in CP
and PMI respectively.
where dist
xyi
is i
th
observed distance between
some occurrence of word type x and its nearest
preceding or following occurrence of word type
y, and n is the total number of such distances ob-
served (being at most equal to the frequency of the
rarer word).
In cases where many occurrences of the less
frequent word were not able to be paired, raw
CD gives midleading results. This is because un-
pairable words themselves provide useful nega-
tive evidence which CD ignores. A more ap-
propriate measure can be formed in which the
mean distance is calculated using the frequency of
the less frequent word, regardless of whether this
many distances were actually observed. This gives
us Neutrally-Weighted Co-Dispersion (NWCD).
Note that for convenience, we keep the standard
definition of the mean and introduce a correction
factor instead.
NWCD =
n
min(p(x), p(y))
1/max(p(x), p(y))
M(dist
xy1
. . . dist
xyn
)
An asymmetric association measure can be
formed in a similar manner. Instead of calculat-
ing the mean using the frequency of the less fre-
quent word as described above, we explicitly use
the frequency of the cue word (which in some
cases may actually exceed the number of dis-
tances observed). This gives us Cue-Weighted Co-
Dispersion (CWCD).
CWCD =
n
p(x)
1/max(p(x), p(y))
M(dist
xy1
. . . dist
xyn
)
(1)
In addition to these measures, we also ex-
plore significance-corrected forms NWCD
sig
and
CWCD
sig
, by introducing the same sample size
term employed by PMI
sig
, CP
sig
and SCI
sig
.
Again, these can readily be inferred from the ex-
isting formulae in the above two sections.
3.5 Co-occurrence Parameters
For frequency-based co-occurrence statistics, the
principle parameter is the window size. We will
use five window sizes separated by a constant scal-
ing factor, chosen so as to span those most com-
monly encountered in the literature, with some ex-
tension towards the upper end. We use w to rep-
resent this parameter, with w = 2 implying a win-
dow size of +/-2. The parameter values explored
632
are w = 2, w = 10, w = 50, w = 250 and
w = 1250. We examine such large window sizes
so as to give a fairer comparison with the distance
approach which is not bounded by a window, and
in acknowledgement of the fact that the entire doc-
ument as context has been used with some success
in other application areas (most notably informa-
tion retrieval).
For distance-based statistics, the principle pa-
rameter is the function via which the various ob-
served distances between tokens are reduced to a
single mean value. In this investigation we will ex-
plore five means. These are the power means with
exponents (which herein we refer to as m) rang-
ing from -2 to +2. These give us the quadratic
mean or RMS (m = 2), the arithmetic mean
(m = 1), the geometric mean (m = 0), the har-
monic mean (m = ?1), and the inverse quadratic
mean (m = ?2).
4 Task I: Correlations on word pairs
One of the ESSLLI Workshop shared tasks (Ba-
roni et al, 2008) required the evaluation of cor-
relation between a small, manually selected sub-
set of human cue-response scores from the EAT
dataset and automatic scores for the same word
pairs. Here, tather than focusing on word pairs
which meet certain grammatical and frequency
criteria we test on all pairs. For the EAT and
Florida datasets, this amounts to many tens of
thousands of cue-response pairs. Although this
makes the task of correlation harder, it means we
can attribute a great deal of statistical significance
to the results and make our observations as general
as possible.
4.1 Evaluation Measures, Upper Bounds and
Baselines
For evaluating agreement between corpus-derived
associations and human associations, we use
Spearman?s Rank correlation. This is appropri-
ate because we are primarily interested in the rel-
ative ranking of word pair associations (in order
to predict particularly strong responses, for exam-
ple). Although some studies have used Pearson?s
correlation, the various association measures ex-
plored here are not linear within each another and
it would be inappropriate to evaluate them under
the assumption of a linear relationship with the hu-
man norms.
Two of the human datasets, Kent and
Minnesota, though collected independently, are
based on the same set of 100 cue words established
by Kent (1910). Therefore by performing a rank
correlation of these two datasets with one another,
(each of which was produced by pooling the re-
sponses of some 1000 people) we can get a useful
upper-bound for correlations: if a computer-based
system were to exceed this upper-bound in corre-
lations with either dataset, then we would need to
suspect it of over-fitting.
As a baseline, we use the corpus frequency of
the response word. The simple assumption is that
the more frequent a word is, the more likely it is
to appear as a human response independent of the
cue given. This is also the simplest formulation
which does not assign equal scores to the various
possible responses, and which is therefore capable
of producing a rank-list of predictions.
4.2 Task I Results
Figure 1 shows the Spearman?s rank correlation
co-efficients across all paramaterisations of all as-
sociation measures (frequency-based on the left,
and distance-based on the right), with each human
dataset, for the 10 million word corpus. Embold-
ened are the best performing windowed and win-
dowless configurations for each dataset. The dif-
ference of these figures over the baseline is highly
significant (p < 0.0001 in most cases). The panels
to the right show summary statistics for these fig-
ures, and for the 1 million word corpus (for which
full figures are not included owing to space limita-
tions). These statistics include the performance of
the baseline, where relevant the estimated upper-
bound (see Section 4.1), and the difference in per-
formance of the distance-based method over the
window-based. The accuracy and error figures are
based on the co-efficients of determination (r
2
)
and are expressed both as a relative improvement
in accuracy (how much closer (r
2
) is to 1 under the
distance-based approach) and reduction in error
(how much further r
2
is from zero). Also the sig-
nificance of the difference in the r values is given.
4.3 Discussion
The two-way Spearman?s rank correlations be-
tween the Kent and Minesota datasets sug-
gested an upper bound of r = 0.4. In theory,
a large proportion of this agreement is accounted
for by paradigmatic associations which we are
not likely to fully reproduce with these first-order
measures. By this standard, the general levels of
633
Figure 1: Correlations for window-based and windowless measures on a 10 million word corpus
correlation seen here (for these datasets r = 0.235
and r = 0.239 respectively) seem very reasonable.
What is immediately clear from Figure 1 is that,
for the range of parameters tested here, we see
a relatively small but statistically significant im-
provement across four of the five datasets when
adopting the distance-based approach.
The correlations are unsurprisingly lower across
the board for the much smaller 1 million word cor-
pus. Here, the best distance-based measure statis-
tically significantly outperforms the best window-
based one (with a significance level of p <
0.0001) on one out of four datasets, while the dif-
ferences are not great enough to be considered sta-
tistically significant on the other three datasets.
There is therefore some evidence that the bene-
fits observed with the larger corpus hold in the
presence of limited data, which is in support of
the general theory that distance-based methods
capture more information from the corpus at the
co-occurrence level (Washtell, 2009). It remains
clear, however, that no method is presently a sub-
stitute for using a larger corpus.
In terms of optimum configurations, we find
that for the frequency-based approach with the
larger corpus, a window size of around +/-10 to
+/-50 words more or less consistently produces the
best results, irrespective of association the mea-
sure. Interestingly on the small corpus the ten-
dency appears to be towards a somewhat larger
window size than with the larger corpus. This
may be related to the larger windows? increased
resilience to data-sparseness. Somewhat surpris-
ingly, we also see that our assymmetric associa-
tion measures SCI and SCI
sig
perform the best
overall amongst the windowed measures, largely
irrespective of the window or corpus, size.
In the large corpus, the best distance-based
measure is the asymmetric CWCD, with the sig-
nificance corrected measure CWCD
sig
showing
greater strength in the small corpus: perhaps,
again, for its improved reliability in the presence
of very low-frequency data. The optimum mean
for the distance-based parameterisations is some-
where around m = ?1 (the harmonic) to m = 0
(the geometric). We find this unsurprising as the
typical distribution of inter-word distances in a
corpus is heavily skewed towards the smaller dis-
tances - indeed even a random corpus exhibits this
characteristic with the distances following a geo-
metric distribution.
5 Task II: Agreement with strongest
human associations
The correlation evalation presented considers all
word pairs present in the human datasets. How-
ever, human association norms tend to contain
a very long tail of hapax legomena - responses
which were given by only one individual. Such
responses are extremely difficult for corpus-based
634
association measures to predict, and given that
there is so little consensus amongst human respon-
dents over these items, it is probably not partic-
ularly useful to do so. Rather, it might be most
useful to predict common or majority human re-
sponses.
5.1 Evaluation measure and Upper Bound
For the strongest human response to each cue
in the human datasets, its rank was calculated
amongst all 33, 000 possible responses to that
cue, according to each association measure and
parameterisation. Where there were tied scores
for various responses, a median rank was assigned.
As a rough upper bound, we would be impressed
by a computer system which was able to predict
the most popular human response as often as a
randomly selected individual in the human exper-
iments happened to chose the most popular re-
sponse.
5.2 Task II Results
Figure 2 illustrates the range of computational as-
sociation scores attributed to only the strongest
human responses. The position of the strongest
human response to each cue word, within the
computationally-ranked lists of all possible re-
sponses, is plotted on the y-axis. For each asso-
ciation measure the points are ordered from best
to worst along the x-axis. In the ideal case there-
fore, the most popular human response for ev-
ery cue word would appear at rank 1 amongst the
computer-generated responses, resulting in a hori-
zonal line at y=1. Generally speaking therefore,
the smaller the area above a line the better the per-
formance of a measure.
Three summary statistics can be derived from
Figure 2:
1) The number of most popular human re-
sponses that are correctly predicted by a measure
is indicated by the x-position at which its line de-
parts from y=1. This can be seen to be around 11%
for CWCD
sig
and is zero for the two best PMI
parameterizations, with other illustrated measures
performing intermediately.
2) The width of the flat horizontal tails at the op-
posite corner of the figure indicate the proportion
of the cue words for which a measure was unable
to differentiate the strongest human response from
the large contingent of zero association scores re-
sulting from unobservable co-occurrences. This
tail is non-existent for CWCD
sig
, but afflicts some
25% and 62% of cue words under the two best
PMI parameterizations, again with other illus-
trated measures performing intermediately.
3) The median rank of the most popular human
response for each measure can be read of on the
y-axis at the horizontal mid-point (indicated by a
feint vertical line).
Figure 2: Agreement of computational measures
with strongest human responses
Figure 3: Relative agreement of computational
measures with strongest human responses
The results shown are for the Kent dataset, and
are highly typical. Included in the figure are
the three frequency-based configurations with the
highest median rank: SCI
sig
at window sizes w =
10 and w = 50, and standard LL at w = 10. Three
635
other frequency-based configurations are included
for contrast. Also included is the single window-
less configuration with the highest median rank -
in this case CWCD
sig
using the harmonic mean.
Several other windowless configurations (notably
CWCD and the nearby means) and had very simi-
lar profiles.
Figure 3 shows the magnitude of the difference
in the ranking of each of the same 100 strong hu-
man cue/response pairs, between the best window-
less versus best windowed method. Points above
the axis represent those cue/response pairs which
the windowless method ranked more highly, and
vice-versa. The points have been ordered on the
x-axis according the the cue word frequency.
5.3 Discussion
Noteworthy, studying Figure 2, is the great sen-
sitivity of the frequency-based measures to the
window size parameter. There exists a cut-off
point, linked to window size, beyond which the
frequency-based measures are unable to make
any differentiation between the desired human re-
sponse and a large portion of the 33, 000 candidate
responses. This is almost certainly due to a lack
of evidence in the presence of very low frequency
words. Log-Likelihood performs somewhat better
in this respect, as it takes negative information into
account.
Although the distance-based approach follows
the same general trend as the other measures, it
is nonetheless able to generate a distinct non-zero
association score for every strong human response
and overall it aptly ranks them more highly. A
larger number these responses are actually ranked
first (i.e. successfully predicted) by the distance-
based approach. In fact this number is compara-
ble to, and sometimes exceeds, the upper-bound
of 10% implied by taking the average proportion
of human respondents who give the most popular
response to a given cue.
Whilst Figure 2 showed that overall the win-
dowless method fairs better, on a per-cue basis
(Figure 3) things are a little more interesting: For
a little over a third of cue-words the windowed
method actually appears to perform somewhat bet-
ter. For the majority however, the windowless ap-
proach performs considerably better (note that the
y-axis scale is logarithmic). It can also be seen
that the difference between the methods is most
pronounced for low frequency cue words, with re-
sponses to some cues exhibiting a relative ranking
of around one-hundred times lower for the win-
dowed method. This further supports the theory
that the windowless methods are better able to ex-
ploit sparse data.
6 Conclusions and Future work
This paper presented the first empirical compar-
ison of window-based and the relatively recently
introduced windowless association measures, us-
ing their ability to reproduce human association
scores as a testbed. We show that the best win-
dowless measures are always at least as good as
the best window-based measures, both when it
comes to overall correlation with human associ-
ation scores and predicting the strongest human
response. In addition, for several human associ-
ation sets, they perform significantly better. Al-
though not all parameter settings and corpus sizes
could be explored, we conclude that it is worth-
while investigating windowless association mea-
sures further. As a side-benefit, we have also in-
troduced new variants of existing frequency-based
association measures and shown them to perform
as well as or better than their existing counterparts.
Although these measures were semi-principled in
their construction, a deeper understanding of why
they work so well is needed. This may in turn lead
to the construction of superior windowless mea-
sures.
In our own future work, we are especially in-
terested in using higher-order windowless associa-
tion measures for retrieving paradigmatic relations
as well as exploring their use in various NLP ap-
plications.
7 Acknowledgements
We would like to extend sincere thanks to Rein-
hard Rapp for providing us with the Minnesota
dataset in digital form, and additional thanks to
Eric Atwell for his support.
References
M. Baroni, S. Evert, and A. Lenci, editors. 2008. Esslli
Workshop on Distributional Lexical Semantics.
L. Burnard, 1995. Users? Reference Guide, British Na-
tional Corpus. British National Corpus Consortium,
Oxford, England.
K. Church and P. Hanks. 1989. Word association
norms, mutual information, and lexicography. In
Proc. of ACL-89, pages 76?83.
636
K. Church and P. Hanks. 1991. Word association
norms, mutual information and lexicography. Com-
putational Linguistics, 16(1):22?29.
P. Clark and F.C. Evans. 1954. Distance to near-
est neighbor as a measure of spatial relationships in
populations. Ecology, 35:445?453.
J. Curran. 2003. From distributional to semantic simi-
larity. Ph.D. thesis, University of Edinburgh.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19:61?74.
S. Evert. 2005. The Statistics of Word Cooccurrences:
Word Pairs and Collocations. Ph.D. thesis, Insti-
tut fr maschinelle Sprachverarbeitung, University of
Stuttgart.
D. Hardcastle. 2005. Using the distributional hypothe-
sis to derive coocurrence scores from the British Na-
tional Corpus. In Proc. of Corpus Linguistics.
J. Jenkins. 1970. The 1952 Minnesota word associa-
tion norms. In L. Postman and G. Keppel, editors,
Norms of word associations, pages 1?38. Academic
press.
G. Kent and A. Rosanoff. 1910. A study of association
in insanity. Amer. J. of Insanity, pages 317?390.
G. Kiss, C. Armstrong, R. Milroy, and J. Piper. 1973.
An associative thesaurus of English and its computer
analysis. In A. Aitken, R. Bailey, and N. Hamilton-
Smith, editors, The Computer and Literary Studies.
Edinburgh University Press.
B. Krenn and S. Evert. 2001. Cam we do better than
frequency? a case study on extracting pp-verb collo-
cations. In Proc. of the ACL Workshop on Colloca-
tions.
A. Lamjiri, O. El Demerdash, and L. Kosseim. 2004.
Simple features for statistical word sense disam-
biguation. In Proc. of SENSEVAL-2004.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL-98.
Lukas Michelbacher, Stefan Evert, and Hinrich
Sch?utze. 2007. Asymmetric association measures.
In Proc. of RANLP-2007.
D. Nelson, C. McEvoy, J. Walling, and J. Wheeler.
1980. The University of South Florida homograph
norms. Behaviour Research Methods and Instru-
mentation, 12:16?37.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of the 21
st
National Conference
on Artificial Intelligence; 2004.
R. Rapp. 2002. The computation of word associa-
tions: comparing syntagmatic and paradigmatic ap-
proaches. In Proc of COLING 2002.
D.L. Sackett. 2001. Why randomized controlled tri-
als fail but needn?t: 2. failure to employ physiolog-
ical statistics, or the only formula a clinician-trialist
is every likely to need (or understand). Canadian
Medical Association Journal, 165(9):1226?1237.
P. Savicki and J. Hlavacova. 2002. Measures of word
commonness. Journal of Quantitative Linguistcs,
9(3):215?231.
E. Terra and C. Clarke. 2004. Fast computation of
lexical affinity models. In Proc of COLING 2004.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4), De-
cember.
X. Wang, 2005. Robust Utilization of Context in Word
Sense Disambiguation, chapter Modeling and Using
Context, pages 529?541. Springer Lecture Notes in
Computer Science.
J. Washtell. 2009. Co-dispersion: A windowless ap-
proach to lexical association. In Proc. of EACL-
2009.
M. Wettler and R. Rapp. 1993. Computation of word
associations based on the co-ocurrences of words in
large corpora. In Proc. of the First Workshop on Very
Large Corpora.
K. White and L. Abrams. 2004. Free associations and
dominance ratings of homophones for young and
older adults. Behaviour Research Methods, Instru-
ments and Computers, 36:408?420.
D. Yarowsky and R Florian. 2002. Evaluating sense
disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
637
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 628?635, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Recognising Textual Entailment with Logical Inference
Johan Bos
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
jbos@inf.ed.ac.uk
Katja Markert
School of Computing
University of Leeds
Woodhouse Lane
Leeds, LS2 9JT
markert@comp.leeds.ac.uk
Abstract
We use logical inference techniques for
recognising textual entailment. As the
performance of theorem proving turns
out to be highly dependent on not read-
ily available background knowledge, we
incorporate model building, a technique
borrowed from automated reasoning, and
show that it is a useful robust method to
approximate entailment. Finally, we use
machine learning to combine these deep
semantic analysis techniques with simple
shallow word overlap; the resulting hy-
brid model achieves high accuracy on the
RTE testset, given the state of the art. Our
results also show that the different tech-
niques that we employ perform very dif-
ferently on some of the subsets of the RTE
corpus and as a result, it is useful to use the
nature of the dataset as a feature.
1 Introduction
Recognising textual entailment (RTE) is the task to
find out whether some text T entails a hypothesis H.
This task has recently been the focus of a challenge
organised by the PASCAL network in 2004/5.1 In
Example 1550 H follows from T whereas this is not
the case in Example 731.
1All examples are from the corpus released as part of
the RTE challenge. It is downloadable from http://www.
pascal-network.org/Challenges/RTE/. The exam-
ple numbers have also been kept. Each example is marked for
entailment as TRUE if H follows from T and FALSE otherwise.
The dataset is described in Section 4.1.
Example: 1550 (TRUE)
T: In 1998, the General Assembly of the Nippon Sei Ko
Kai (Anglican Church in Japan) voted to accept female
priests.
H: The Anglican church in Japan approved the ordination
of women.
Example: 731 (FALSE)
T: The city Tenochtitlan grew rapidly and was the center
of the Aztec?s great empire.
H: Tenochtitlan quickly spread over the island, marshes,
and swamps.
The recognition of textual entailment is without
doubt one of the ultimate challenges for any NLP
system: if it is able to do so with reasonable accu-
racy, it is clearly an indication that it has some thor-
ough understanding of how language works. Indeed,
recognising entailment bears similarities to Turing?s
famous test to assess whether machines can think,
as access to different sources of knowledge and the
ability to draw inferences seem to be among the pri-
mary ingredients for an intelligent system. More-
over, many NLP tasks have strong links to entail-
ment: in summarisation, a summary should be en-
tailed by the text; paraphrases can be seen as mutual
entailment between T and H; in IE, the extracted in-
formation should also be entailed by the text.
In this paper, we discuss two methods for recog-
nising textual entailment: a shallow method relying
mainly on word overlap (Section 2), and deep se-
mantic analysis, using state-of-the-art off-the-shelf
inference tools, namely a theorem prover and a
model builder (Section 3). These tools rely on Dis-
course Representation Structures for T and H as well
as lexical and world knowledge. To our knowledge,
few approaches to entailment currently use theorem
provers and none incorporate model building (see
628
Section 5 for a discussion of related work).
Both methods are domain-independent to increase
transferrability and have not been tailored to any par-
ticular test suite. In Section 4 we test their accuracy
and robustness on the RTE datasets as one of the few
currently available datasets for textual inference. We
also combine the two methods in a hybrid approach
using machine learning. We discuss particularly the
following questions:
? Can the methods presented improve signifi-
cantly over the baseline and what are the per-
formance differences between them? Does the
hybrid system using both shallow and deep se-
mantic analysis improve over the individual use
of these methods?
? How far does deep semantic analysis suffer
from a lack of lexical and world knowledge and
how can we perform logical inference in the
face of potentially large knowledge gaps?
? How does the design of the test suite affect per-
formance? Are there subsets of the test suite
that are more suited to any particular textual en-
tailment recognition method?
2 Shallow Semantic Features
We use several shallow surface features to model the
text, hypothesis and their relation to each other.
Most importantly, we expect some dependency
between surface string similarity of text and hypoth-
esis and the existence of entailment. Our string sim-
ilarity measure uses only a form of extended word
overlap between text and hypothesis, taking into ac-
count equality of words, synonymy and morpholog-
ical derivations. WordNet (Fellbaum, 1998) is used
as the knowledge source for synonymy and deriva-
tions. The exact procedure is as follows:
Both text and hypothesis are tokenised and lem-
matised. A lemma l1 in the hypothesis is said
to be related to a lemma l2 in the text iff l1 and
l2 are equal, belong to the same WordNet synset
(e.g., ?murder? and ?slay?), are related via WordNet
derivations (e.g. ?murder? and ?murderer?) or are
related via a combination of synonymy and deriva-
tions (e.g. ?murder? via ?murderer? to ?liquidator?).
No word sense disambiguation is performed and all
synsets for a particular lemma are considered.
In addition, each lemma in the hypothesis is as-
signed its inverse document frequency, accessing the
Web as corpus via the GoogleAPI, as its weight.
This standard procedure allows us to assign more
importance to less frequent words.
The overlap measure wnoverlap between text
and hypothesis is initialised as zero. Should a lemma
in the hypothesis be related to a lemma in the text,
its weight is added to wnoverlap, otherwise it is
ignored. In the end wnoverlap is normalised by
dividing it by the sum of all weights of the lemmas
in the hypothesis. This ensures that wnoverlap is
always a real number between 0 and 1 and also en-
sures independence of the length of the hypothesis.
Apart from wnoverlap we take into account
length (as measured by number of lemmas) of text
and hypothesis, because in most of the observed
cases for true entailments the hypothesis is shorter
than the text as it contains less information. This is
covered by three numerical features measuring the
length of the text, of the hypothesis and the relative
length of hypothesis with regard to the text.
3 Deep Semantic Analysis
3.1 Semantic Interpretation
We use a robust wide-coverage CCG-parser (Bos et
al., 2004) to generate fine-grained semantic repre-
sentations for each T/H-pair. The semantic represen-
tation language is a first-order fragment of the DRS-
language used in Discourse Representation Theory
(Kamp and Reyle, 1993), conveying argument struc-
ture with a neo-Davidsonian analysis and including
the recursive DRS structure to cover negation, dis-
junction, and implication. Consider for example:
Example: 78 (FALSE)
T: Clinton?s new book is not big seller here.
H: Clinton?s book is a big seller.
drs(T):
x1 x2 x3
book(x1)
book(x2)
?
x1=x2
clinton(x3)
of(x1,x3)
?
e4 x5
big(x5)
seller(x5)
be(e4)
agent(e4,x1)
patient(e4,x5)
loc(e4,here)
drs(H):
x1 x2 e3 x4
book(x1)
clinton(x2)
of(x1,x2)
big(x4)
seller(x4)
be(e3)
agent(e3,x1)
patient(e3,x4)
629
Proper names and definite descriptions are treated
as anaphoric, and bound to previously introduced
discourse referents if possible, otherwise accommo-
dated. Some lexical items are specified as presup-
position triggers. An example is the adjective ?new?
which has a presuppositional reading, as shown by
the existence of two different ?book? entities in
drs(T). Scope is fully specified.
To check whether an entailment holds or not, we
use two kinds of automated reasoning tools: Vam-
pire, a theorem prover (Riazanov and Voronkov,
2002), and Paradox, a model builder (Claessen and
So?rensson, 2003). Both tools are developed to deal
with inference problems stated in first-order logic.
We use the standard translation from DRS to first-
order logic (Kamp and Reyle, 1993) to map our se-
mantic representation onto the format required by
the inference tools.
3.2 Theorem Proving
Given a T/H pair, a theorem prover can be used to
find answers to the following conjectures:
1. T implies H (shows entailment)
2. T+H are inconsistent (shows no entailment)
Assume that the function DRS denotes the DRS cor-
responding to T or H, and FOL the function that
translates a DRS into first-order logic. Then, if the
theorem prover manages to find a proof for
FOL(DRS(T))?FOL(DRS(H)) (A)
we know that we are dealing with a true entailment.
In addition, to use a theorem prover to detect incon-
sistencies in a T/H pair, we give it:
?FOL(DRS(T);DRS(H)) (B)
If the theorem prover returns a proof for (B), we
know that T and H are inconsistent and T definitely
doesn?t entail H (assuming that T and H are them-
selves consistent).
Examples The theorem prover will find that T im-
plies H for the following examples:
Example: 1005 (TRUE)
T: Jessica Litman, a law professor at Michigan?s Wayne
State University, has specialized in copyright law and
Internet law for more than 20 years.
H: Jessica Litman is a law professor.
Example: 1977 (TRUE)
T: His family has steadfastly denied the charges.
H: The charges were denied by his family.
Example: 898 (TRUE)
T: After the war the city was briefly occupied by the Allies
and then was returned to the Dutch.
H: After the war, the city was returned to the Dutch.
Example: 1952 (TRUE)
T: Crude oil prices soared to record levels.
H: Crude oil prices rise.
These examples show how deep semantic analy-
sis deals effectively with apposition, active-passive
alternation, coordination, and can integrate lexical
knowledge.
The RTE dataset only contains a few inconsistent
T/H pairs. Even although Example 78 might look
like a case in point, it is not inconsistent: It would
be if the T in the example would have been Clinton?s
new book is not a big seller. The addition of the
adverb here makes T+H consistent.
3.3 Background Knowledge
The theorem prover needs background knowledge
to support its proofs. Finding a proof for Example
1952 above is only possible if the theorem prover
knows that soaring is a way of rising.
How does it know this? Because in addi-
tion to the information from T and H alone, we
also supply relevant background knowledge in the
form of first-order axioms. Instead of giving just
FOL(DRS(T);DRS(H)) to the theorem prover, we sup-
ply it with (BK ? FOL(DRS(T);DRS(H))) where BK
is short for the relevant background knowledge.
We generate background knowledge using three
kinds of sources: generic knowledge, lexical knowl-
edge, and geographical knowledge. Axioms for
generic knowledge cover the semantics of posses-
sives, active-passive alternation, and spatial knowl-
edge. There are about 20 different axioms in the cur-
rent system and these are the only manually gener-
ated axioms. An example is
?e?x?y(event(e)?agent(e,x)?in(e,y)?in(x,y))
which states that if an event is located in y, then so
is the agent of that event.
Lexical knowledge is created automatically from
WordNet. A hyponymy relation between two
630
synsets A and B is converted into ?x(A(x)?B(x)).
Two synset sisters A and B are translated into
?x(A(x)? ?B(x)). Here the predicate symbols
from the DRS are mapped to WordNet synsets using
a variant of Lesk?s WSD algorithm (Manning and
Schuetze, 1999). Examples 78 and 1952 would be
supported by knowledge similar to:
?x(clinton(x)?person(x)) ?x(book(x)?artifact(x))
?x(artifact(x)? ?person(x)) ?x(soar(x)?rise(x))
Finally, axioms covering geographical knowledge
about capitals, countries and US states are extracted
automatically from the CIA factbook. An example:
?x?y(paris(x)?france(y)?in(x,y))
3.4 Model Building
While theorem provers are designed to prove that a
formula is a theorem (i.e., that the formula is true in
any model), they are generally not good at deciding
that a formula is not a theorem. Model builders are
designed to show that a formula is true in at least one
model. To exploit these complementary approaches
to inference, we use both a theorem prover and a
model builder for any inference problem: the theo-
rem prover attempts to prove the input whereas the
model builder simultaneously tries to find a model
for the negation of the input. If the model builder
finds a model for
?FOL(DRS(T))?FOL(DRS(H)) (= ?A)
we know that there can?t be a proof for its negation
(hence no entailment). And if the model builder is
able to generate a model for
FOL(DRS(T);DRS(H)) (= ?B)
we know that T and H are consistent (maybe entail-
ment). (In practice, this is also a good way to termi-
nate the search for proofs or models: if the theorem
prover finds a proof for ??, we can halt the model
builder to try and find a model for ? (because there
won?t be one), and vice versa.)
Another attractive property of a model builder is
that it outputs a model for its input formula (only of
course if the input is satisfiable). A model is here
the logical notion of a model, describing a situation
in which the input formula is true. Formally, a model
is a pair ?D,F ? where D is the set of entities in the
domain, and F a function mapping predicate sym-
bols to sets of domain members. For instance, the
model returned for fol(drs(T)) in Example 78 is one
where the domain consists of three entities (domain
size = 3):
D = {d1,d2,d3} F(loc) = {}
F(book) = {d1,d2} F(seller) = {}
F(clinton) = {d3} F(be) = {}
F(of) = {(d1,d3)} F(agent) = {}
F(big) = {} F(patient) = {}
Model builders like Paradox generate finite mod-
els by iteration. They attempt to create a model for
domain size 1. If they fail, they increase the domain
size and try again, until either they find a model or
their resources run out. Thus, although there are in-
finitely many models satisfying fol(drs(T)), model
builders generally build a model with a minimal do-
main size. (For more information on model building
consult (Blackburn and Bos, 2005)).
3.5 Approximating Entailment
In an ideal world we calculate all the required back-
ground knowledge and by either finding a proof or
a countermodel, decide how T and H relate with re-
spect to entailment. However, it is extremely hard
to acquire all the required background knowledge.
This is partly due to the limitations of word sense
disambiguation, the lack of resources like WordNet,
and the lack of general knowledge in a form suitable
for automatic inference tasks.
To introduce an element of robustness into our ap-
proach, we use the models as produced by the model
builders to measure the ?distance? from an entail-
ment. The intuition behind it is as follows. If H
is entailed by T, the model for T+H is not informa-
tive compared to the one for T, and hence does not
introduce new entities. Put differently, the domain
size for T+H would equal the domain size of T. In
contrast, if T does not entail H, H normally intro-
duce some new information (except when it contains
negated information), and this will be reflected in
the domain size of T+H, which then is larger than
the domain size of T. It turns out that this difference
between the domain sizes is a useful way of measur-
ing the likelihood of entailment. Large differences
are mostly not entailments, small differences mostly
are. Consider the following example:
631
Example: 1049 (TRUE)
T: Four Venezuelan firefighters who were traveling to a
training course in Texas were killed when their sport
utility vehicle drifted onto the shoulder of a highway
and struck a parked truck.
H: Four firefighters were killed in a car accident.
Although this example is judged as a true entail-
ment, Vampire doesn?t find a proof because it lacks
the background knowledge that one way of causing a
car accident is to drift onto the shoulder of the high-
way and strike something. It generates a model with
domain size 11 for fol(drs(T)), and a model with do-
main size 12 for fol((drs(T);drs(H))). The absolute
difference in domain sizes is small, and therefore
likely to indicate an entailment. Apart from the ab-
solute difference we also compute the difference rel-
ative to the domain size. For the example above the
relative domain size yields 1/12 = 0.083.
The domain size only tells us something about the
number of entities used in a model?not about the
number of established relations between the model?s
entities. Therefore, we also introduce the notion of
model size. The model size is defined here by count-
ing the number of all instances of two-place relations
(and three-place relations, if there are any) in the
model, and multiplying this with the domain size.
For instance, the following model
D = {d1,d2,d3}
F(cat) = {d1,d2}
F(john) = {d3}
F(of) = {(d1,d3)}
F(like) = {(d3,d1),(d3,d2)}
has a domain size of 3 and 3 instantiated two-place
relations, yielding a model size of 3 ? 3 = 9.
3.6 Deep Semantic Features
Given our approach to deep semantic analysis,
we identified eight features relevant for recognis-
ing textual entailment. The theorem prover pro-
vides us with two features: entailed determin-
ing whether T implies H, and inconsistent
determining whether T together with H is incon-
sistent. The model builder gives us six features:
domainsize and modelsize for T+H as well
as the absolute and relative difference between the
sizes of T and T+H, both for the size of the domains
(domainsizeabsdif, domainsizereldif)
and the size of the models (modelsizeabsdif,
modelsizereldif).
4 Experiments
There are not many test suites available for textual
inference. We use throughout this section the dataset
made available as part of the RTE challenge.
4.1 Dataset Design and Evaluation Measures
The organisers released a development set of 567
sentence pairs and a test set of 800 sentence pairs.
In both sets, 50% of the sentence pairs were anno-
tated as TRUE and 50% as FALSE, leading to a 50%
most frequent class baseline for automatic systems.
The examples are further distinguished according to
the way they were designed via a so-called Task
variable. For examples marked CD (Comparable
Documents), sentences with high lexical overlap in
comparable news articles were selected, whereas the
hypotheses of examples marked QA (Question An-
swering) were formed by translating questions from
e.g., TREC into statements. The other subsets are IE
(Information extraction), MT (Machine Translation)
RC (Reading Comprehension), PP (Paraphrase Ac-
quisition) and IR (Information Retrieval). The dif-
ferent examples and subsets cover a wide variety of
different aspects of entailment, from incorporation
of background knowledge to lexical to syntactic en-
tailment and combinations of all these. For a more
exhaustive description of dataset design we refer the
reader to (Dagan et al, 2005).
4.2 Experiment 1: Human Upper bound
To establish a human upper bound as well as inves-
tigate the validity of the datasets issued, one of the
authors annotated all 800 examples of the test set
for entailment, using the short RTE annotation rules.
The annotation was performed before the release of
the gold standard annotation for the test set and was
therefore independent of the organisers? annotation.
The organisers? and the author?s annotation yielded
a high percentage agreement of 95.25%. However,
33% of the originally created examples were already
filtered out of the corpus before release by the organ-
isers because of agreement-related problems. There-
fore we expect that human agreement on textual en-
tailment in general is rather lower.
632
4.3 Decision trees for entailment recognition
We expressed each example pair as a feature vector,
using different subsets of the features described in
Section 2 and Section 3 for each experiment. We
then trained a decision tree for classification into
TRUE and FALSE entailment on the development
set, using the Weka machine learning tool (Witten
and Frank, 2000), and tested on the test set. Apart
from a classification, Weka also computes a confi-
dence value for each decision, dependent on the leaf
in the tree that the classified example falls into: if the
leaf covers x examples in the training set, of which y
examples are classified wrongly, then the error rate
is y/x and the confidence value is 1? y/x.
Our evaluation measures are accuracy (acc) as
the percentage of correct judgements as well as
confidence-weighted average score (cws), which re-
wards the system?s ability to assign a higher confi-
dence score to correct judgements than wrong ones
(Dagan et al, 2005): after the n judgements are
sorted in decreasing order by their confidence value,
the following measure is computed:
cws =
1
n
n?
i=1
#correct-up-rank-i
i
All evaluation measures are computed over the
whole test set as well as on the 7 different subsets
(CD, IE, etc.). The results are summarised in Ta-
ble 1. We also computed precision, recall and F-
measure for both classes TRUE and FALSE and will
discuss the results in the text whenever of interest.
Experiment 2: Shallow Features In this experi-
ment only the shallow features (see Section 2) were
used. The overall accuracy of 56.9% is significantly
higher than the baseline.2
Column 2 in Table 1 shows that this decent per-
formance is entirely due to excellent performance
on the CD subset. (Recall that the CD set was de-
signed explicitly with examples with high lexical
overlap in mind.) In addition, the method overes-
timates the number of true entailments, achieving a
Recall of 0.926 for the class TRUE, but a precision
of only 0.547 on the same class. In contrast, it has
2We used the z-test for the difference between two propor-
tions to measure whether the difference in accuracy between
two algorithms or an algorithm and the baseline is statistically
significant at the 5% level.
good precision (0.761) but low recall (0.236) for the
FALSE class. Thus, there is a correspondence be-
tween low word overlap and FALSE examples (see
Example 731 in the Introduction, where important
words in the hypothesis like ?swamps? or ?marshes?
are not matched in the text); high overlap, however,
is normally necessary but not sufficient for TRUE
entailment (see also Example 78 in Section 3).
Experiment 3: Strict entailment To test the po-
tential of entailment as discovered by theorem prov-
ing alone, we now use only the entailment and
inconsistent features. As to be expected, the
decision tree shows that, if a proof for T implies H
has been found, the example should be classified as
TRUE, otherwise as FALSE.3 The precision (0.767)
for the class TRUE is reasonably high: if a proof
is found, then an entailment is indeed very likely.
However, recall is very low (0.058) as only 30 proofs
were found on the test set (for some examples see
Section 3). This yields an F-measure of only 0.10
for the TRUE class. Due to the low recall, the over-
all accuracy of the system (0.52, see Table 1) is not
significantly higher than the baseline.
Thus, this feature behaves in the opposite way
to shallow lexical overlap and overgenerates the
FALSE class. Missing lexical and background
knowledge is the major cause for missing proofs.
Experiment 4: Approximating entailment As
discussed in Section 3.5 we now try to compensate
for missing knowledge and improve recall for TRUE
entailments by approximating entailment with the
features that are furnished by the model builder.
Thus, Experiment 4 uses all eight deep semantic
analysis features, including the features capturing
differences in domain- and modelsizes. The recall
for the TRUE class indeed jumps to 0.735. Al-
though, unavoidably, the FALSE class suffers, the
resulting overall accuracy (0.562, see Column 4 in
Table 1) is significantly higher than when using the
features provided by the theorem prover alone (as
in Experiment 3). The confidence weighted score
also rises substantially from 0.548 to 0.608. The
approximation achieved can be seen in the different
treatment of Example 1049 (see Section 3.5) in Ex-
periments 3 and 4. In Experiment 3, this example
3The inconsistent feature was not used by the decision
tree as very few examples were covered by that feature.
633
Table 1: Summary of Results for Experiments 1 to 6
Exp 1: Human 2: Shallow 3: Strict 4: Deep 5: Hybrid 6: Hybrid+Task
Task acc cws acc cws acc cws acc cws acc cws acc cws
CD 0.967 n/a 0.827 0.881 0.547 0.617 0.713 0.787 0.700 0.790 0.827 0.827
IE 0.975 n/a 0.508 0.503 0.542 0.622 0.533 0.616 0.542 0.639 0.542 0.627
MT 0.900 n/a 0.500 0.515 0.500 0.436 0.592 0.596 0.525 0.512 0.533 0.581
QA 0.961 n/a 0.531 0.557 0.461 0.422 0.515 0.419 0.569 0.520 0.577 0.531
RC 0.979 n/a 0.507 0.502 0.557 0.638 0.457 0.537 0.507 0.587 0.557 0.644
PP 0.920 n/a 0.480 0.467 0.540 0.581 0.520 0.616 0.560 0.667 0.580 0.619
IR 0.922 n/a 0.511 0.561 0.489 0.421 0.567 0.503 0.622 0.569 0.611 0.561
all 0.951 n/a 0.569 0.624 0.520 0.548 0.562 0.608 0.577 0.632 0.612 0.646
is wrongly classified as FALSE as no proof can be
found; in Experiment 4, it is correctly classified as
TRUE due to the small difference between domain-
and modelsizes for T and T+H.
There is hardly any overall difference in accuracy
between the shallow and the deep classifier. How-
ever, it seems that the shallow classifier in its current
form has very little potential outside of the CD sub-
set whereas the deep classifier shows a more promis-
ing performance for several subsets.
Experiment 5: Hybrid classification As shallow
and deep classifiers seem to perform differently on
differently designed datasets, we hypothesized that a
combination of these classifiers should bring further
improvement. Experiment 5 therefore used all shal-
low and deep features together. However, the overall
performance of this classifier (see Column 5 in Ta-
ble 1) is not significantly better than either of the
separate classifiers. Closer inspection of the results
reveals that, in comparison to the shallow classifier,
the hybrid classifier performs better or equally on all
subsets but CD. In comparison to the deep classifier
in Column 4, the hybrid classifier performs equally
well or better on all subsets apart from MT. Over-
all, this means more robust performance of the hy-
brid classifier over differently designed datasets and
therefore more independence from dataset design.
Experiment 6: Dependency on dataset design
As Eperiment 5 shows, simple combination of meth-
ods, while maybe more robust, will not necessar-
ily raise overall performance if the system does not
know when to apply which method. To test this hy-
pothesis further we integrated the subset indicator
as a feature with the values CD, IE, MT, RC, IR,
PP, QA into our hybrid system. Indeed, the resulting
overall accuracy (0.612) is significantly better than
either shallow or deep system alone. Note that us-
ing both a combination of methodologies and the
subset indicator is necessary to improve on individ-
ual shallow and deep classifiers for this corpus. We
integrated the subset indicator also into the shallow
and deep classifier by themselves, yielding classi-
fiers Shallow+Task and Deep+Task, with no or only
very small changes in accuracy (these figures are not
included in Table 1).
5 Related Work
Our shallow analysis is similar to the IDF models
proposed by (Monz and de Rijke, 2003; Saggion et
al., 2004). We have expanded their approach by us-
ing other shallow features regarding text length.
The basic idea of our deep analysis, using a de-
tailed semantic analysis and first-order inference,
goes back to (Blackburn and Bos, 2005). It is sim-
ilar to some of the recent approaches that were pro-
posed in the context of the PASCAL RTE workshop,
i.e. using the OTTER theorem prover (Akhmatova,
2005; Fowler et al, 2005), using EPILOG (Bayer et
al., 2005), or abduction (Raina et al, 2005).
None of these systems, however, incorporate
model building as a central part of the inference
mechanism. We have shown that solely relying on
theorem proving is normally insufficient due to low
recall, and that using model builders is a promising
way to approximate entailment.
Results of other approaches to determining tex-
tual entailment indicate that it is an extremely hard
634
task. The aforementioned RTE workshop revealed
that participating systems reached accuracy figures
ranging between 0.50 and 0.59 and cws scores be-
tween 0.50 and 0.69 (Dagan et al, 2005). Com-
paring this with our own results (accuracy 0.61 and
cws 0.65) shows how well our systems performs on
the same data set. This is partly due to our hy-
brid approach which is more robust across different
datasets.
6 Conclusions
Relying on theorem proving as a technique for de-
termining textual entailment yielded high precision
but low recall due to a general lack of appropriate
background knowledge. We used model building as
an innovative technique to surmount this problem to
a certain extent. Still, it will be unavoidable to incor-
porate automatic methods for knowledge acquisition
to increase the performance of our approach. Future
work will be directed to the acquisition of targeted
paraphrases that can be converted into background
knowledge in the form of axioms.
Our hybrid approach combines shallow analysis
with both theorem proving and model building and
achieves high accuracy scores on the RTE dataset
compared to other systems that we are aware of.
The results for this approach also indicate that (a)
the choice of entailment recognition methods might
have to vary according to the dataset design and/or
application and (b) that a method that wants to
achieve robust performance across different datasets
might need the integration of several different entail-
ment recognition methods as well as an indicator of
design methodology or application.
Thus, although test suites establish a controlled
way of assessing textual entailment detection sys-
tems, the importance of being able to predict textual
entailment in NLP might be better justified using
task-based evaluation. This can be achieved by in-
corporating them in QA or summarisation systems.
Acknowledgements We would like to thank Mirella La-
pata and Malvina Nissim as well as three anonymous review-
ers for their comments on this paper. We are also grateful to
Valentin Jijkoun and Bonnie Webber for discussion and Steve
Clark and James Curran for help on using the CCG-parser.
References
E. Akhmatova. 2005. Textual entailment resolution via
atomic propositions. In PASCAL. Proc. of the First
Challenge Workshop. Recognizing Textual Entailment.
S. Bayer, J. Burger, L. Ferro, J. Henderson, and A. Yeh.
2005. Mitre?s submission to the eu pascal rte chal-
lenge. In PASCAL. Proc. of the First Challenge Work-
shop. Recognizing Textual Entailment.
P. Blackburn and J. Bos. 2005. Representation and In-
ference for Natural Language. A First Course in Com-
putational Semantics. CSLI.
J. Bos, S. Clark, M. Steedman, J. Curran, and J. Hocken-
maier. 2004. Wide-coverage semantic representations
from a CCG parser. In Proc of COLING.
K. Claessen and N. So?rensson. 2003. New techniques
that improve mace-style model finding. In Model
Computationa - Principles, Algorithms, Applications
(Cade-19 Workshop), Miami, Florida.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pas-
cal recognising textual entailment challenge. In PAS-
CAL. Proc. of the First Challenge Workshop. Recog-
nizing Textual Entailment.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, Mass.
A. Fowler, B. Hauser, D. Hodges, I. Niles, A. Novis-
chi, and J. Stephan. 2005. Applying cogex to recog-
nize textual entailment. In PASCAL. Proc. of the First
Challenge Workshop. Recognizing Textual Entailment.
H. Kamp and U. Reyle. 1993. From Discourse to Logic.
Introduction to Modeltheoretic Semantics of Natural
Language, Formal Logic and Discourse Representa-
tion Theory. Kluwer, Dordrecht, Netherlands.
C. Manning and H. Schuetze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
C. Monz and M. de Rijke. 2003. Light-weight entail-
ment checking for computational semantics. In Proc.
of ICOS-3.
R. Raina, A.Y. Ng, and C. Manning. 2005. Robust tex-
tual inference via learning and abductive reasoning. In
Proc. of AAAI 2005.
A. Riazanov and A. Voronkov. 2002. The design and
implementation of Vampire. AI Comm., 15(2-3).
H. Saggion, R. Gaizauskas, M. Hepple, I. Roberts, and
M Greenwood. 2004. Exploring the performance of
boolean retrieval strategies for open domain question
answering. In Proc. of the IR4QA Workshop at SIGIR.
I. H. Witten and E. Frank. 2000. Data Mining: Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann, San Diego, CA.
635
Comparing Knowledge Sources for Nominal
Anaphora Resolution
Katja Markert?
University of Leeds
Malvina Nissim?
University of Edinburgh
We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora
and definite noun phrase coreference. Specifically, we compare an algorithm that relies on links
encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora
by means of shallow lexico-semantic patterns. As corpora we use the British National Corpus
(BNC), as well as the Web, which has not been previously used for this task. Our results
show that (a) the knowledge encoded in WordNet is often insufficient, especially for anaphor?
antecedent relations that exploit subjective or context-dependent knowledge; (b) for other-
anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite NP
coreference, the Web-based method yields results comparable to those obtained using WordNet
over the whole data set and outperforms the WordNet-based method on subsets of the data
set; (d) in both case studies, the BNC-based method is worse than the other methods because
of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge
gap often encountered in anaphora resolution and handled examples with context-dependent
relations between anaphor and antecedent. Because it is inexpensive and needs no hand-modeling
of lexical knowledge, it is a promising knowledge source to integrate into anaphora resolution
systems.
1. Introduction
Most work on anaphora resolution has focused on pronominal anaphora, often
achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube,
Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an
F-measure of 82.8% for personal pronouns, respectively. Less attention has been paid
to nominal anaphors with full lexical heads, which cover a variety of phenomena, such
as coreference (Example (1)), bridging (Clark 1975; Example (2)), and comparative
anaphora (Examples (3?4)).1
? School of Computing, University of Leeds, Woodhouse Lane, LS2 9JT Leeds, UK. E-mail:
markert@comp.leeds.ac.uk.
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, EH8 9LW Edinburgh, UK. E-mail:
mnissim@inf.ed.ac.uk.
1 In all examples presented in this article, the anaphor is typed in boldface and the correct antecedent in
italics. The abbreviation in parentheses at the end of each example specifies the corpus from which the
example is taken: WSJ stands for the Wall Street Journal, Penn Treebank, release 2; BNC stands for British
National Corpus (Burnard 1995), and MUC-6 for the combined training/test set for the coreference task
of the Sixth Message Understanding Conference (Hirschman and Chinchor 1997).
Submission received: 15 December 2003; revised submission received: 21 November 2004; accepted for
publication: 19 March 2005.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
(1) The death of Maxwell, the British publishing magnate whose empire
collapsed in ruins of fraud, and who was the magazine?s publisher, gave the
periodical a brief international fame. (BNC)
(2) [. . . ] you don?t have to undo the jacket to get to the map?particularly
important when it?s blowing a hooley. There are elasticated adjustable
drawcords on the hem, waist and on the hood. (BNC)
(3) In addition to increasing costs as a result of greater financial exposure for
members, these measures could have other, far-reaching repercussions.
(WSJ)
(4) The ordinance, in Moon Township, prohibits locating a group home for the
handicapped within a mile of another such facility. (WSJ)
In Example (1), the definite noun phrase (NP) the periodical corefers with the
magazine.2 In Example (2), the definite NP the hood can be felicitously used because
a related entity has already been introduced by the NP the jacket, and a part-of
relation between the two entities can be established. Examples (3)?(4) are instances
of other-anaphora. Other-anaphora are a subclass of comparative anaphora (Halliday
and Hasan 1976; Webber et al 2003) in which the anaphoric NP is introduced by
a lexical modifier (such as other, such, and comparative adjectives) that specifies
the relationship (such as set-complement, similarity and comparison) between the
entities invoked by anaphor and antecedent. For other-anaphora, the modifiers
other or another provide a set-complement to an entity already evoked in the
discourse model. In Example (3), the NP other, far-reaching repercussions refers to
a set of repercussions excluding increasing costs and can be paraphrased as other
(far-reaching) repercussions than (increasing) costs. Similarly, in Example (4), the NP
another such facility refers to a group home which is not identical to the specific (planned)
group home mentioned before.
A large and diverse amount of lexical or world knowledge is usually necessary
to understand anaphors with full lexical heads. For the examples above, we need
the knowledge that magazines are periodicals, that hoods are parts of jackets,
that costs can be or can be viewed as repercussions of an event, and that institutional
homes are facilities. Therefore, many resolution systems that handle these phenomena
(Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie
2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on
hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical
hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has
given strong indications that such resources are insufficient for the entire range of
full NP anaphora. Additionally, we discuss some serious methodological problems
that arise when fixed ontologies are used that have been encountered by previous
researchers and/or us: the costs of building, maintaining and mining ontolo-
gies; domain-specific and context-dependent knowledge; different ways of encoding
information; and sense ambiguity.
2 In this article, we restrict the notion of definite NPs to NPs modified by the article ?the.?
3 These systems also use surface-level features (such as string matching), recency, and grammatical
constraints. In this article, we concentrate on the lexical and semantic knowledge employed.
368
Markert and Nissim Knowledge Sources for Anaphora Resolution
In Section 3, we discuss an alternative to the manual construction of knowledge
bases, which we call the corpus-based approach. A number of researchers (Hearst
1992; Berland and Charniak 1999, among others) have suggested that knowledge
bases be enhanced via (semi)automatic knowledge extraction from corpora, and such
enhanced knowledge bases have also been used for anaphora resolution, specifically
for bridging (Poesio et al 2002; Meyer and Dale 2002). Building on our previous work
(Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two
ways. First, we suggest using the Web for anaphora resolution instead of the smaller-
size, but less noisy and more balanced, corpora used previously, making available
a huge additional source of knowledge.4 Second, we do not induce a fixed lexical
knowledge base from the Web but use shallow lexicosyntactic patterns and their Web
frequencies for anaphora resolution on the fly. This allows us to circumvent some of the
above-mentioned methodological problems that occur with any fixed ontology, whether
constructed manually or automatically.
The core of this article consists of an empirical comparison of these different
sources of lexical knowledge for the task of antecedent selection or antecedent ranking
in anaphora resolution. We focus on two types of full NP anaphora: other-anaphora
(Section 4) and definite NP coreference (Section 5).5 In both case studies, we compare
an algorithm that relies mainly on the frequencies of lexico-syntactic patterns in corpora
(both the Web and the BNC) with an algorithm that relies mainly on a fixed ontology
(WordNet 1.7.1). We specifically address the following questions:
1. Can the shortcomings of using a fixed ontology that have been stipulated
by previous research on definite NPs be confirmed in our coreference
study? Do they also hold for other-anaphora, a phenomenon less studied
so far?
2. How does corpus-based knowledge acquisition compare to using
manually constructed lexical hierarchies in antecedent selection? And is
the use of the Web an improvement over using smaller, but manually
controlled, corpora?
3. To what extent is the answer to the previous question dependent on the
anaphoric phenomenon addressed?
In Section 6 we discuss several aspects of our findings that still need elaboration
in future work. Specifically, our work is purely comparative and regards the different
lexical knowledge sources in isolation. It remains to be seen how the results carry
forward when the knowledge sources interact with other features (for example,
grammatical preferences). A similar issue concerns the integration of the methods
into anaphoricity determination in addition to antecedent selection. Additionally,
future work should explore the contribution of different knowledge sources for yet
other anaphora types.
4 There is a growing body of research that uses the Web for NLP. As we concentrate on anaphora resolution
in this article, we refer the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as the
December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other
NLP tasks.
5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to the
entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and
antecedent are identical.
369
Computational Linguistics Volume 31, Number 3
2. The Knowledge Gap and Other Problems for Lexico-semantic Resources
A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio
2000; Harabagiu, Bunescu, and Maiorano 2001; Strube, Rapp, and Mueller 2002;
Modjeska 2002; Gardent, Manuelian, and Kow 2003) point to the importance of lexical
and world knowledge for the resolution of full NP anaphora and the lack of such
knowledge in existing ontologies (Section 2.1). In addition to this knowledge gap, we
summarize other, methodological problems with the use of ontologies in anaphora
resolution (Section 2.2).
2.1 The Knowledge Gap for Nominal Anaphora with Full Lexical Heads
In the following, we discuss previous studies on the automatic resolution of coreference,
bridging and comparative anaphora, concentrating on work that yields insights into the
use of lexical and semantic knowledge.
2.1.1 Coreference. The prevailing current approaches to coreference resolution are
evaluated on MUC-style (Hirschman and Chinchor 1997) annotated text and treat
pronominal and full NP anaphora, named-entity coreference, and non-anaphoric
coreferential links that can be stipulated by appositions and copula. The performance of
these approaches on definite NPs is often substantially worse than on pronouns and/or
named entities (Connolly, Burger, and Day 1997; Strube, Rapp, and Mueller 2002; Ng
and Cardie 2002b; Yang et al 2003). For example, for a coreference resolution algorithm
on German texts, Strube, Rapp, and Mueller (2002) report an F-measure of 33.9% for
definite NPs that contrasts with 82.8% for personal pronouns.
Several reasons for this performance difference have been established. First,
whereas pronouns are mostly anaphoric in written text, definite NPs do not have
to be so, inducing the problem of whether a definite NP is anaphoric in addition to
determining an antecedent from among a set of potential antecedents (Fraurud 1990;
Vieira and Poesio 2000).6 Second, the antecedents of definite NP anaphora can occur at
considerable distance from the anaphor, whereas antecedents to pronominal anaphora
tend to be relatively close (Preiss, Gasperin, and Briscoe 2004; McCoy and Strube 1999).
An automatic system can therefore more easily restrict its antecedent set for pronominal
anaphora.
Third, it is in general believed that pronouns are used to refer to entities in focus,
whereas entities that are not in focus are referred to by definite descriptions (Hawkins
1978; Ariel 1990; Gundel, Hedberg, and Zacharski 1993), because the head nouns of
anaphoric definite NPs provide the reader with lexico-semantic knowledge. Antecedent
accessibility is therefore additionally restricted via semantic compatibility and does not
need to rely on notions of focus or salience to the same extent as for pronouns. Given this
lexical richness of common noun anaphors, many resolution algorithms for coreference
have incorporated manually controlled lexical hierarchies, such as WordNet. They use,
for example, a relatively coarse-grained notion of semantic compatibility between a few
high-level concepts in WordNet (Soon, Ng, and Lim 2001), or more detailed hyponymy
and synonymy links between anaphor and antecedent head nouns (Vieira and Poesio
6 A two-stage process in which the first stage identifies anaphoricity of the NP and the second the
antecedent for anaphoric NPs (Uryupina 2003; Ng 2004) can alleviate this problem. In this article, we
focus on the second stage, namely, antecedent selection.
370
Markert and Nissim Knowledge Sources for Anaphora Resolution
2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b, among others).
However, several researchers have pointed out that the incorporated information is
still insufficient. Harabagiu, Bunescu, and Maiorano (2001) (see also Kameyama 1997)
report that evaluation of previous systems has shown that ?more than 30% of the
missed coreference links are due to the lack of semantic consistency information
between the anaphoric noun and its antecedent noun? (page 59). Vieira and Poesio
(2000) report results on anaphoric definite NPs in the WSJ that stand in a synonymy
or hyponymy relation to their antecedents (as in Example (1)). Using WordNet links
to retrieve the appropriate knowledge proved insufficient, as only 35.0% of synonymy
relations and 56.0% of hyponymy relations needed were encoded in WordNet as direct
or inherited links.7 The semantic knowledge used might also not necessarily improve
on string matching: Soon, Ng, and Lim (2001) final, automatically derived decision tree
does not incorporate their semantic-compatibility feature and instead relies heavily on
string matching and aliasing, thereby leaving open how much information in a lexical
hierarchy can improve over string matching.
In this article, we concentrate on this last of the three problems (insufficient lexical
knowledge). We investigate whether the knowledge gap for definite NP coreference
can be overcome by using corpora as knowledge sources as well as whether the
incorporation of lexical knowledge sources improves over simple head noun matching.
2.1.2 Comparative Anaphora. Modjeska (2002)?one of the few computational studies
on comparative anaphora?shows that lexico-semantic knowledge plays a larger role
than grammatical salience for other-anaphora. In this article, we show that the semantic
knowledge provided via synonymy and hyponymy links in WordNet is insufficient
for the resolution of other-anaphora, although the head of the antecedent is normally
a synonym or hyponym of the head of the anaphor in other-anaphora (Section 4.4).8
2.1.3 Bridging. Vieira and Poesio (2000) report that 62.0% of meronymy relations (see
Example (2)) needed for bridging resolution in their corpus were not encoded in
WordNet. Gardent, Manuelian, and Kow (2003) identified bridging descriptions in a
French corpus, of which 187 (52%) exploited meronymic relations. Almost 80% of these
were not found in WordNet. Hahn, Strube, and Markert (1996) report experiments on
109 bridging cases from German information technology reports, using a hand-crafted,
domain-specific knowledge base of 449 concepts and 334 relations. They state that 42
(38.5%) links between anaphor and antecedents were missing in their knowledge base,
a high proportion given the domain-specific task. In this article, we will not address
bridging, although we will discuss the extension of our work to bridging in Section 6.
2.2 Methodological Problems for the Use of Ontologies in Anaphora Resolution
Over the years, several major problems have been identified with the use of ontologies
for anaphora resolution. In the following we provide a summary of the different issues
raised, using the examples in the Introduction.
7 Whenever we refer to ?hyponymy/meronymy (relations/links)? in WordNet, we include both direct and
inherited links.
8 From this point on, we will often use the terms anaphor and antecedent instead of head of anaphor and head
of antecedent if the context is non-ambiguous.
371
Computational Linguistics Volume 31, Number 3
2.2.1 Problem 1: Knowledge Gap. As discussed above, even in large ontologies the
lack of knowledge can be severe, and this problem increases for non-hyponymy rela-
tions. None of the examples in Section 1 are covered by synonymy, hyponymy, or
meronymy links in WordNet; for example, hoods are not encoded as parts of jackets,
and homes are not encoded as a hyponym of facilities. In addition, building, extending,
and maintaining ontologies by hand is expensive.
2.2.2 Problem 2: Context-Dependent Relations. Whereas the knowledge gap might
be reduced as (semi)automatic efforts to enrich ontologies become available (Hearst
1992; Berland and Charniak 1999; Poesio et al 2002), the second problem is intrinsic to
fixed context-independent ontologies: How much and which knowledge should they
include? Thus, Hearst (1992) raises the issue of whether underspecified, context- or
point-of-view-dependent hyponymy relations (like the context-dependent link between
costs and repercussions in Example (3)) should be included in a fixed ontology, in
addition to universally true hyponymy relations. Some other hyponymy relations that
we encountered in our studies whose inclusion in ontologies is debatable are age:(risk)
factor, coffee:export, pilots:union, country:member.
2.2.3 Problem 3: Information Encoding. Knowledge might be encoded in many
different ways in a lexical hierarchy, and this can pose a problem for anaphora resolution
(Humphreys et al 1997; Poesio, Vieira, and Teufel 1997). For example, although
magazine and periodical are not linked in WordNet via synonymy/hyponymy, the gloss
records magazine as a periodic publication. Thus, the desired link might be derived
through the analysis of the gloss together with derivation of periodical from periodic.
However, such extensive mining of the ontology (as performed, e.g., by Harabagiu,
Bunescu, and Maiorano [2001]) can be costly. In addition, different information sources
must be weighed (e.g., is a hyponymy link preferred over a gloss inclusion?) and
combined (should hyponyms/hyperonyms/sisters of gloss expressions be considered
recursively?). Extensive combinations also increase the risk of false positives.9
2.2.4 Problem 4: Sense Proliferation. Using all senses of anaphor and potential an-
tecedents in the search for relations might yield a link between an incorrect antecedent
candidate and the anaphor due to an inappropriate sense selection. On the other hand,
considering only the most frequent sense for anaphor and antecedent (as is done in
Soon, Ng, and Lim [2001]) might lead to wrong antecedent assignment if a minority
sense is intended in the text. So, for example, the most frequent sense of hood in
WordNet is criminal, whereas the sense used in Example (2) is headdress. The alterna-
tives are either weighing senses according to different domains or a more costly sense
disambiguation procedure before anaphora resolution (Preiss 2002).
3. The Alternative: Corpus-Based Knowledge Extraction
There have been a considerable number of efforts to extract lexical relations from
corpora in order to build new knowledge sources and enrich existing ones without time-
9 Even without extensive mining, this risk can be high: Vieira and Poesio (2000) report a high number of
false positives for one of their data sets, although they use only WordNet-encoded links.
372
Markert and Nissim Knowledge Sources for Anaphora Resolution
consuming hand-modeling. This includes the extraction of hyponymy and synonymy
relations (Hearst 1992; Caraballo 1999, among others) as well as meronymy (Berland
and Charniak 1999; Meyer 2001).10 One approach to the extraction of instances of a
particular lexical relation is the use of patterns that express lexical relations structurally
explicitly in a corpus (Hearst 1992; Berland and Charniak 1999; Caraballo 1999; Meyer
2001), and this is the approach we focus on here. As an example, the pattern NP1 and
other NP2 usually expresses a hyponymy/similarity relation between the hyponym
NP1 and its hypernym NP2 (Hearst 1992), and it can therefore be postulated that two
noun phrases that occur in such a pattern in a corpus should be linked in an ontology via
a hyponymy link. Applications of the extracted relations to anaphora resolution are less
frequent. However, Poesio et al (2002) and Meyer and Dale (2002) have used patterns
for the corpus-based acquisition of meronymy relations: these patterns are subsequently
exploited for bridging resolution.
Although automatic acquisition can help bridge the knowledge gap (see Prob-
lem 1 in Section 2.2.1), the incorporation of the acquired knowledge into a fixed
ontology yields other problems. Most notably, it has to be decided which knowl-
edge should be included in ontologies, because pattern-based acquisition will
also find spurious, subjective and context-dependent knowledge (see Problem 2 in
Section 2.2.2). There is also the problem of pattern ambiguity, since patterns do
not necessarily have a one-to-one correspondence to lexical relations (Meyer 2001).
Following our work in Markert, Nissim, and Modjeska (2003), we argue that for the
task of antecedent ranking, these problems can be circumvented by not constructing
a fixed ontology at all. Instead, we use the pattern-based approach to find lexical
relationships holding between anaphor and antecedent in corpora on the fly. For
instance, in Example (3), we do not need to know whether costs are always repercus-
sions (and should therefore be linked via hyponymy in an ontology) but only that
they are more likely to be viewed as repercussions than the other antecedent candidates.
We therefore adapt the pattern-based approach in the following way for antecedent
selection.
Step 1: Relation Identification. We determine which lexical relation usu-
ally holds between anaphor and antecedent head nouns for a partic-
ular anaphoric phenomenon. For example, in other-anaphora, a hyponymy/
similarity relation between anaphor and antecedent is exploited (homes are
facilities) or stipulated by the context (costs are viewed as repercussions).
Step 2: Pattern Selection. We select patterns that express this lexical relation
structurally explicitly. For example, the pattern NP1 and other NP2 usually
expresses hyponymy/similarity relations between the hyponym NP1 and its
hypernym NP2 (see above).
Step 3: Pattern Instantiation. If the lexical relation between anaphor and
antecedent head nouns is strong, then it is likely that the anaphor and
antecedent also frequently co-occur in the selected explicit patterns. We
extract all potential antecedents for each anaphor and instantiate the explicit
10 There is also a long history in the extraction of other lexical knowledge, which is also potentially useful
for anaphora resolution, for example, of selectional restrictions/preferences. In this article we focus on
the lexical relations that can hold between antecedent and anaphor head nouns.
373
Computational Linguistics Volume 31, Number 3
for all anaphor/antecedent pairs. In Example (4) the pattern NP1 and
other NP2 can be instantiated with ordinances and other facilities, Moon
Township and other facilities, homes and other facilities, handicapped
and other facilities, and miles and other facilities.11
Step 4: Antecedent Assignment. The instantiation of a pattern can be searched
in any corpus to determine its frequency. We follow the rationale that the
most frequent of these instantiated patterns determines the most likely
antecedent. Therefore, should the head noun of an antecedent candidate
and the anaphor co-occur in a pattern although they do not stand in the
lexical relationship considered (because of pattern ambiguity, noise in the
corpus, or spurious occurrences), this need not prove a problem as long
as the correct antecedent candidate co-occurs more frequently with the
anaphor.
As the patterns can be elaborate, most manually controlled and linguistically processed
corpora are too small to determine the pattern frequencies reliably. Therefore, the size
of the corpora used in some previous approaches leads to data sparseness (Berland and
Charniak 1999), and the extraction procedure can therefore require extensive smoothing.
Thus as a further extension, we suggest using the largest corpus available, the Web,
in the above procedure. The instantiation for the correct antecedent homes and other
facilities in Example (4), for instance, does not occur at all in the BNC but yields over
1,500 hits on the Web.12 The competing instantiations (listed in Step 3) yield 0 hits in the
BNC and fewer than 20 hits on the Web.
In the remainder of this article, we present two comparative case studies on
coreference and other-anaphora that evaluate the ontology- and corpus-based ap-
proaches in general and our extensions in particular.
4. Case Study I: Other-Anaphora
We now describe our first case study for antecedent selection in other-anaphora.
4.1 Corpus Description and Annotation
We use Modjeska?s (2003) annotated corpus of other-anaphors from the WSJ. All
examples in this section are from this corpus. Modjeska restricts the notion of other-
anaphora to anaphoric NPs with full lexical heads modified by other or another
(Examples (3)?(4)), thereby excluding idiomatic non-referential uses (e.g., on the other
hand), reciprocals such as each other, ellipsis, and one-anaphora. The excluded cases
either are non-anaphoric or do not have a full lexical head and would therefore re-
quire a mostly non-lexical approach to resolution. Modjeska?s corpus also excludes
11 These simplified instantiations serve as an example; for final instantiations, see Section 4.5.1.
12 This search and all searches for the Web experiments in Section 4 were executed on August 29, 2003. All
Web searches for Section 5 were executed August 27, 2004.
374
Markert and Nissim Knowledge Sources for Anaphora Resolution
other-anaphors with structurally available antecedents: In list contexts such as
Example (5), the antecedent is normally given as the left conjunct of the list:
(5) [. . .] AZT can relieve dementia and other symptoms in children [. . .]
A similar case is the construction Xs other than Ys. For a computational treatment of
other-NPs with structural antecedents, see Bierner (2001).
The original corpus collected and annotated by Modjeska (2003) contains 500
instances of other-anaphors with NP antecedents in a five-sentence window. In this
study we use the 408 (81.6%) other-anaphors in the corpus that have NP antecedents
within a two-sentence window (the current or previous sentence).13 An antecedent
candidate is manually annotated as correct if it is the latest mention of the entity to
which the anaphor provides the set complement. The tag lenient was used to annotate
previous mentions of the same entity. In Example (6), all other bidders refers to all bidders
excluding United Illuminating Co., whose latest mention is it. In this article, lenient
antecedents are underlined. All other potential antecedents (e.g., offer in Example (6)),
are called distractors.
(6) United Illuminating Co. raised its proposed offer to one it valued at $2.29
billion from $2.19 billion, apparently topping all other bidders.
The antecedent can be a set of separately mentioned entities, like May and July in
Example (7). For such split antecedents (Modjeska 2003), the latest mention of each set
member is annotated as correct, so that there can be more than one correct antecedent to
an anaphor.14
(7) The May contract, which also is without restraints, ended with a gain of
0.45 cent to 14.26 cents. The July delivery rose its daily permissible limit of
0.50 cent a pound to 14.00 cent, while other contract months showed
near-limit advances.
4.2 Antecedent Extraction and Preprocessing
For each anaphor, all previously occurring NPs in the two-sentence window were
automatically extracted exploiting the WSJ parse trees. NPs containing a possessive NP
modifier (e.g., Spain?s economy) were split into a possessor phrase (Spain) and a possessed
entity (Spain?s economy).15 Modjeska (2003) identifies several syntactic positions that
cannot serve as antecedents of other-anaphors. We automatically exclude only NPs
preceding an appositive other-anaphor from the candidate antecedent set. In ?Mary
Elizabeth Ariail, another social-studies teacher,? the NP Mary Elizabeth Ariail cannot
13 We concentrate on this majority of cases to focus on the comparison of different sources of lexical
knowledge without involving discourse segmentation or focus tracking. In Section 5 we expand the
window size to allow equally high coverage for definite NP coreference.
14 The occurrence of split antecedents also motivated the distinction between correct and lenient
antecedents in the annotation. Anaphors with split antecedents have several antecedent candidates
annotated as correct. All other anaphors have only one antecedent candidate annotated as correct, with
previous mentions of the same entity marked as lenient.
15 We thank Natalia Modjeska for the extraction and for making the resulting sets of candidate antecedents
available to us.
375
Computational Linguistics Volume 31, Number 3
be the antecedent of another social-studies teacher as the two phrases are coreferential and
cannot provide a set complement to each other.
The resulting set of potential NP antecedents for an anaphor ana (with a unique
identifier anaid) is called Aanaid.16 The final number of extracted antecedents for the
whole data set is 4,272, with an average of 10.5 antecedent candidates per anaphor.
After extraction, all modification was eliminated, and only the rightmost noun of
compounds was retained, as modification results in data sparseness for the corpus-
based methods, and compounds are often not recorded in WordNet.
For the same reasons we automatically resolved named entities (NEs). They
were classified into the ENAMEX MUC-7 categories (Chinchor 1997) PERSON,
ORGANIZATION and LOCATION, using the software ANNIE (GATE2; http://gate.ac.
uk). We then automatically obtained more-fine-grained distinctions for the NE cate-
gories LOCATION and ORGANIZATION, whenever possible. We classified LOCATIONS
into COUNTRY, (US) STATE, CITY, RIVER, LAKE, and OCEAN in the following way. First,
small gazetteers for these subcategories were extracted from the Web. Second, if an
entity marked as LOCATION by ANNIE occurred in exactly one of these gazetteers
(e.g., Texas in the (US) STATE gazetteer) it received the corresponding specific label;
if it occurred in none or in several of the gazetteers (e.g., Mississippi occurred in
both the state and the river gazetteer), then the label was left at the LOCATION level.
We further classified an ORGANIZATION entity by using its internal makeup as follows.
We extracted all single-word hyponyms of the noun organization from WordNet and
used the members of this set, OrgSet, as the target categories for the fine-grained
distinctions. If an entity was classified by ANNIE as ORGANIZATION and it had
an element <ORG> of OrgSet as its final lemmatized word (e.g., Deutsche Bank) or
contained the pattern <ORG> of (for example, Bank of America), it was subclassified
as <ORG> (here, BANK). In cases of ambiguity, again, no subclassification was carried
out. No further distinctions were developed for the category PERSON. We used regular
expression matching to classify numeric and time entities into DAY, MONTH, and YEAR
as well as DOLLAR or simply NUMBER. This subclassification of the standard cate-
gories provides us with additional lexical information for antecedent selection. Thus, in
Example (8), for instance, a finer-grained classification of South Carolina into STATE
provides more useful information than resolving both South Carolina and Greenville
County as LOCATION only:
(8) Use of Scoring High is widespread in South Carolina and common in
Greenville County. . . . Experts say there isn?t another state in the country
where . . .
Finally, all antecedent candidates and anaphors were lemmatized. The procedure of
extraction and preprocessing results in the following antecedent sets and anaphors
for Examples (3) and (4): A3 = {..., addition, cost, result, exposure, member, measure} and
ana = repercussion and A4 = {..., ordinance, Moon Township [= location], home, handicapped,
mile} and ana = facility.
Table 1 shows the distribution of antecedent NP types in the other-anaphora data
set.17 NE resolution is clearly important as 205 of 468 (43.8%) of correct antecedents
are NEs.
16 In this article the anaphor ID corresponds to the example numbers.
17 Note that there are more correct antecedents than anaphors because the data include split antecedents.
376
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 1
Distribution of antecedent NP types in the other-anaphora data set.
Correct Lenient Distractors All
Pronouns 49 19 329 397
Named entities 205 56 806 1,067
Common nouns 214 104 2,490 2,808
Total 468 179 3,625 4,272
4.3 Evaluation Measures and Baselines
For each anaphor, each algorithm selects at most one antecedent as the correct one. If
this antecedent provides the appropriate set complement to the anaphor (i.e., is marked
in the gold standard as correct or lenient), the assignment is evaluated as correct.18
Otherwise, it is evaluated as wrong. We use the following evaluation measures: Precision
is the number of correct assignments divided by the number of assignments, recall is the
number of correct assignments divided by the number of anaphors, and F-measure is
based on equal weighting of precision and recall. In addition, we also give the coverage
of each algorithm as the number of assignments divided by the number of anaphors.
This last measure is included to indicate how often the algorithm has any knowledge to
go on, whether correct or false. For algorithms in which the coverage is 100%, precision,
recall, and F-measure all coincide.
We developed two simple rule-based baseline algorithms. The first, a recency-based
baseline (baselineREC), always selects the antecedent candidate closest to the anaphor.
The second (baselineSTR) takes into account that the lemmatized head of an other-
anaphor is sometimes the same as that of its antecedent, as in the pilot?s claim . . . other
bankruptcy claims. For each anaphor, baselineSTR string-compares its last (lemmatized)
word with the last (lemmatized) word of each of its potential antecedents. If the strings
match, the corresponding antecedent is chosen as the correct one. If several antecedents
produce a match, the baseline chooses the most recent one among them. If no antecedent
produces a match, no antecedent is assigned.
We tested two variations of this baseline.19 The algorithm baselineSTRv1 uses
only the original antecedents for string matching, disregarding named-entity res-
olution. If string-comparison returns no match, a back-off version (baselineSTR?v1)
chooses the antecedent closest to the anaphor among all antecedent candidates, thereby
yielding a 100% coverage. The second variation (baselineSTRv2) uses the replacements
for named entities for string matching; again a back-off version (baselineSTR?v2) uses
a recency back-off. This baseline performs slightly better, as now cases such as that in
Example (8) (South Carolina . . . another state, in which South Carolina is resolved to STATE)
can also be resolved. The results of all baselines are summarized in Table 2. Results of
the 100% coverage backoff algorithms are indicated by Precision? in all tables. The sets
of anaphors covered by the string-matching baselines baselineSTRv1 and baselineSTRv2
18 This does not hold for anaphors with split antecedents, for which all antecedents marked as correct need
to be found in order to provide the complete set complement. Therefore, all our algorithms? assignments
in these cases are evaluated as wrong, as they select at most one antecedent.
19 Different versions of the same prototype algorithm are indicated via an index of v1, v2, . . . . The general
prototype algorithm is referred to without indices.
377
Computational Linguistics Volume 31, Number 3
Table 2
Overview of the results for all baselines for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
baselineREC 1.000 0.178 0.178 0.178 0.178
baselineSTRv1 0.282 0.686 0.194 0.304 0.333
baselineSTRv2 0.309 0.698 0.216 0.329 0.350
will be called StrSetv1 and StrSetv2, respectively. These sets do not include the cases
assigned by the recency back-off in baselineSTR?v1 and baselineSTR
?
v2.
For our WordNet and corpus-based algorithms we additionally deleted pronouns
from the antecedent sets, since they are lexically not very informative and are also
not encoded in WordNet. This removes 49 (10.5%) of the 468 correct antecedents
(see Table 1); however, we can still resolve some of the anaphors with pronoun
antecedents if they also have a lenient non-pronominal antecedent, as in Example (6).
After pronoun deletion, the total number of antecedents in our data set is 3,875
for 408 anaphors, of which 419 are correct antecedents, 160 are lenient, and 3,296
are distractors.
4.4 Wordnet as a Knowledge Source for Other-Anaphora Resolution
4.4.1 Descriptive Statistics. As most antecedents are hyponyms or synonyms of their
anaphors in other-anaphora, for each anaphor ana, we look up which elements of its
antecedent set Aanaid are hyponyms/synonyms of ana in WordNet, considering all
senses of anaphor and candidate antecedent. In Example (4), for example, we look
up whether ordinance, Moon Township, home, handicapped, and mile are hyponyms or
synonyms of facility in WordNet. Similarly, in Example (9), we look up whether Will
Quinlan [= PERSON], gene, and risk are hyponyms/synonyms of child.
(9) Will Quinlan had not inherited a damaged retinoblastoma supressor gene
and, therefore, faced no more risk than other children . . .
As proper nouns (e.g., Will Quinlan) are often not included in WordNet, we also
look up whether the NE category of an NE antecedent is a hyponym/synonym of
the anaphor (e.g., whether person is a synonym/hyponym of child) and vice versa
(e.g., whether child is a synonym/hyponym of person). This last inverted look-up
is necessary, as the NE category of the antecedent is often too general to preserve
the normal hyponymy relationship to the anaphor. Indeed, in Example (9), it is the
inverted look-up that captures the correct hyponymy relation between person and
child. If the single look-up for common nouns or any of the three look-ups for
proper nouns is successful, we say that a hyp/syn relation between candidate
antecedent and anaphor holds in WordNet. Note that each noun in WordNet
stands in a hyp/syn relation to itself. Table 3 summarizes how many correct/
lenient antecedents and distractors stand in a hyp/syn relation to their anaphor in
WordNet.
Correct/lenient antecedents stand in a hyp/syn relation to their anaphor sig-
nificantly more often than distractors do (p < 0.001, t-test). The use of WordNet
hyponymy/synonymy relations to distinguish between correct/lenient antecedents
and distractors is therefore plausible. However, Table 3 also shows two limitations
378
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 3
Descriptive statistics for WordNet hyp/syn relations for other-anaphora.
Hyp/syn relation No hyp/syn relation Total
to anaphor to anaphor
Correct antecedents 180 (43.0%) 239 (57.0%) 419 (100%)
Lenient antecedents 68 (42.5%) 92 (57.5%) 160 (100%)
Distractors 296 (9.0%) 3,000 (91.0%) 3,296 (100%)
All antecedents 544 (14.0%) 3,331 (86.0%) 3,875 (100%)
of relying on WordNet in resolution algorithms. First, 57% of correct and lenient
antecedents are not linked via a hyp/syn relation to their anaphor in WordNet.
This will affect coverage and recall (see also Section 2.2.1). Examples from our data
set that are not covered are home:facility, cost:repercussion, age:(risk) factor, pension:benefit,
coffee:export, and pilot(s):union, including both missing universal hyponymy links and
context-stipulated ones. Second, the raw frequency (296) of distractors that stand
in a hyp/syn relation to their anaphor is higher than the combined raw frequency
for correct/lenient antecedents (248) that do so, which can affect precision. This is
due to both sense proliferation (Section 2.2.4) and anaphors that require more than just
lexical knowledge about antecedent and anaphor heads to select a correct antecedent
over a distractor. In Example (10), the distractor product stands in a hyp/syn relation-
ship to the anaphor commodity and?disregarding other factors?is a good antecedent
candidate.20
(10) . . . the move is designed to more accurately reflect the value of products
and to put steel on a more equal footing with other commodities.
4.4.2 The WordNet-Based Algorithm. The WordNet-based algorithm resolves each
anaphor ana to a hyponym or synonym in Aanaid, if possible. If several antecedent
candidates are hyponyms or synonyms of ana, it uses a tiebreaker based on string
match and recency. When no candidate antecedent is a hyponym or synonym of
ana, string match and recency can be used as a possible back-off.21 String compari-
son for tiebreaker and back-off can again use the original or the replaced anteced-
ents, yielding two versions, algoWNv1 (original antecedents) and algoWNv2 (replaced
antecedents).
The exact procedure for the version algoWNv1 given an anaphor ana is as follows:22
(i) for each antecedent a in Aanaid, look up whether a hyp/syn relation
between a and ana holds in WordNet; if this is the case, push a into a set
Ahyp/synanaid ;
20 This problem is not WordNet-specific but affects all algorithms that rely on lexical knowledge only.
21 Because each noun is a synonym of itself, anaphors in StrSetv1/StrSetv2 that do have a string-matching
antecedent candidate will already be covered by the WordNet look-up prior to back-off in almost all
cases: Back-off string matching will take effect only if the anaphor/antecedent head noun is not in
WordNet at all. Therefore, the described back-off will most of the time just amount to a recency back-off.
22 The algorithm algoWNv2 follows the same procedure apart from the variation in string matching.
379
Computational Linguistics Volume 31, Number 3
(ii) if Ahyp/synanaid contains exactly one element, choose this element and stop;
(iii) otherwise, if Ahyp/synanaid contains more than one element, string-compare
each antecedent in Ahyp/synanaid with ana (using original antecedents only).
If exactly one element of Ahyp/synanaid matches ana, select this one and stop;
if several match ana, select the closest to ana within these matching
antecedents and stop; if none match, select the closest to ana within
Ahyp/synanaid and stop;
(iv) otherwise, if Ahyp/synanaid is empty, make no assignment and stop.
The back-off algorithm algoWN?v1 uses baselineSTR
?
v1 as a back-off (iv?) if no antecedent
can be assigned:
(iv?) otherwise, if Ahyp/synanaid is empty, use baselineSTR?v1 to assign an
antecedent to ana and stop;
Both algoWNv1 and algoWNv2 achieved the same results, namely, a coverage of
65.2%, precision of 56.8%, and recall of 37.0%, yielding an F-measure of 44.8%.
The low coverage and recall confirm our predictions in Section 4.4.1. Using backoff
algoWN?v1/algoWN
?
v2 achieves a coverage of 100% and a precision/recall/F-measure
of 44.4%.
4.5 Corpora as Knowledge Sources for Other-Anaphora Resolution
In Section 3 we suggested the use of shallow lexico-semantic patterns for obtaining
anaphor?antecedent relations from corpora. In our first experiment we use the Web,
which with its approximately 8,058M pages23 is the largest corpus available to the
NLP community. In our second experiment we use the same technique on the BNC,
a smaller (100 million words) but virtually noise-free and balanced corpus of contem-
porary English.
4.5.1 Pattern Selection and Instantiation. The list-context Xs and other Ys explicitly
expresses a hyponymy/synonymy relationship with X being hyponyms/synonyms
of Y (see also Example (5) and [Hearst 1992]). This is only one of the possible
structures that express hyponymy/synonymy. Others involve such, including, and
especially (Hearst 1992) or appositions and coordination. We derive our patterns from the
list-context because it corresponds relatively unambigously to hyponymy/synonymy
relations (in contrast to coordination, which often links sister concepts instead of a
hyponym and its hyperonym, as in tigers and lions, or even completely unrelated
concepts). In addition, it is quite frequent (for example, and other occurs more
frequently on the Web than such as and other than). Future work has to explore
which patterns have the highest precision and/or recall and how different patterns
can be combined effectively without increasing the risk of false positives (see also
Section 2.2.3).
23 Google (http://www.google.com), estimate from November 2004.
380
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 4
Patterns and instantiations for other-anaphora.
Common-noun patterns Common-noun instantiations
W1: (N1{sg} OR N1{pl}) and other N2{pl} WIc1: (home OR homes) and other facilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B1: (...) and D* other A* N2{pl} BIc1: (home OR homes) and D* other A* facilities
Proper-noun patterns Proper-noun instantiations
W1: (N1{sg} OR N1{pl}) and other N2{pl} WIp1: (person OR persons) and other children
WIp2: (child OR children) and other persons
W2: N1 and other N2{pl} WIp3: Will Quinlan and other children
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B1: (...) and D* other A* N2{pl} BIp1: (person OR persons) and D* other A* children
BIp2: (child OR children) and D* other A* persons
B2: N1 and D* other A* N2{pl} BIp3: Will Quinlan and D* other A* children
Web. For the Web algorithm (algoWeb), we use the following pattern:24
(W1) (N1{sg} OR N1{pl}) and other N2{pl}
Given an anaphor ana and a common-noun antecedent candidate a in Aanaid, we
instantiate (W1) by substituting a for N1 and ana for N2. An instantiated pattern for
Example (4) is (home OR homes) and other facilities (WIc1 in Table 4).
25 This pattern
instantiation is parallel to the WordNet hyp/syn relation look-up for common nouns.
For NE antecedents we instantiate (W1) by substituting the NE category of the
antecedent for N1, and ana for N2. An instantiated pattern for Example (9) is (person
OR persons) and other children (WIp1 in Table 4). In this instantiation, N1 (person) is not
a hyponym of N2 (child); instead N2 is a hyponym of N1 (see the discussion on inverted
queries in Section 4.4.1). Therefore, we also instantiate (W1) by substituting ana for N1
and the NE type of the antecedent for N2 (WI
p
2 in Table 4). Finally, for NE antecedents,
we use an additional pattern:
(W2) N1 and other N2{pl}
which we instantiate by substituting the original NE antecedent for N1 and
ana for N2 (WI
p
3 in Table 4). The three instantiations for NEs are parallel to the three
hyp/syn relation look-ups in the WordNet experiment in Section 4.4.1. We submit
these instantiations as queries to the Google search engine, making use of the Google
API technology.
BNC. For BNC patterns and instantiations, we exploit the BNC?s part-of-speech tagging.
On the one hand, we restrict the instantiation of N1 and N2 to nouns to avoid noise,
and on the other hand, we allow occurrence of modification to improve coverage. We
24 In all patterns and instantiations in this article, OR is the boolean operator, N1 and N2 are variables, and
and and other are constants.
25 All common-noun instantiations are marked by a superscript c and all proper-noun instantiations by a
superscript p.
381
Computational Linguistics Volume 31, Number 3
therefore extend (W1) and (W2) to the patterns (B1) and (B2).26 An instantiation for
(B1), for example, also matches ?homes and the other four facilities.? Otherwise the
instantiations are produced parallel to the Web (see Table 4). We search the instantiations
in the BNC using the IMS Corpus Query Workbench (Christ 1995).
(B1) (N1{sg} OR N1{pl}) and D* other A* N2{pl}
(B2) N1 and D* other A* N2{pl}
For both algoWeb and algoBNC, each antecedent candidate a in Aanaid is assigned a
score. The procedure, using the notation for the Web, is as follows. We obtain the raw
frequencies of all instantiations in which a occurs (WIc1 for common nouns, or WI
p
1, WI
p
2,
and WIp3 for proper names) from the Web, yielding freq(WI
c
1), or freq(WI
p
1 ), freq(WI
p
2 ),
and freq(WIp3 ). The maximum WMa over these frequencies is the score associated with
each antecedent (given an anaphor ana), which we will also simply refer to as the
antecedent?s Web score. For the BNC, we call the corresponding maximum score BMa
and refer to it as the antecedent?s BNC score. This simple maximum score is biased
toward antecedent candidates whose head nouns occur more frequently overall. In a
previous experiment we used mutual information to normalize Web scores (Markert,
Nissim, and Modjeska 2003). However, the results achieved with normalized and
non-normalized scores showed no significant difference. Other normalization methods
might yield significant improvements over simple maximum scoring and can be
explored in future work.
4.5.2 Descriptive Statistics. Table 5 gives descriptive statistics for the Web and BNC
score distributions for correct/lenient antecedents and distractors, including the mini-
mum and maximum score, mean score and standard deviation, median, and number of
zero scores, scores of one, and scores greater than one.
Web scores resulting from simple pattern-based search produce on average signif-
icantly higher scores for correct/lenient antecedents (mean: 2,416.68/807.63; median:
68/68.5) than for distractors (mean: 290.97; median: 1). Moreover, the method produces
significantly fewer zero scores for correct/lenient antecedents (19.6%/22.5%) than for
distractors (42.3%).27 Therefore the pattern-based Web method is a good candidate for
distinguishing correct/lenient antecedents and distractors in anaphora resolution. In
addition, the median for correct/lenient antecedents is relatively high (68/68.5), which
ensures a relatively large amount of data upon which to base decisions. Only 19.6%
of correct antecedents have scores of zero, which indicates that the method might
have high coverage (compared to the missing 57% of hyp/syn relations for correct
antecedents in WordNet; Section 4.4).
Although the means of the BNC score distributions of correct/lenient antecedents
are significantly higher than that of the distractors, this is due to a few outliers; more
interestingly, the median for the BNC score distributions is zero for all antecedent
groups. This will affect precision for a BNC-based algorithm because of the small
amount of data decisions are based on. In addition, although the number of zero scores
26 The star operator indicates zero or more occurrences of a variable. The variable D can be instantiated by
any determiner; the variable A can be instantiated by any adjective or cardinal number.
27 Difference in means was calculated via a t-test; for medians we used chi-square, and for zero counts a
t-test for proportions. The significance level used was 5%.
382
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 5
Descriptive statistics for Web scores and BNC scores for other-anaphora.
Min?Max Mean SD Med 0 scores 1 scores scores > 1
All possible antecedents (Total: 3,875)
BNC 0?22 0.07 0.60 0 3,714 (95.8%) 109 (2.8%) 52 (1.4%)
Web 0?283,000 542.15 8,352.46 2 1,513 (39.0%) 270 (7.0%) 2,092 (54.0%)
Correct antecedents (Total: 419)
BNC 0?22 0.32 1.62 0 360 (85.9%) 39 (9.3%) 20 (4.8%)
Web 0?283,000 2,416.68 15,947.93 68 82 (19.6%) 11 (2.6%) 326 (77.8%)
Lenient antecedents (Total: 160)
BNC 0?4 0.21 0.62 0 139 (86.9%) 13 (8.1%) 8 (5.0%)
Web 0?8,840 807.63 1,718.13 68.5 36 (22.5%) 3 (1.9%) 121 (75.6%)
Distractors (Total: 3,296)
BNC 0?6 0.03 0.25 0 3,215 (97.5%) 57 (1.7%) 24 (0.8%)
Web 0?283,000 290.97 7,010.07 1 1,395 (42.3%) 256 (7.8%) 1,645 (49.9%)
for correct/lenient antecedents (85.9%/86.9%) is significantly lower than for distractors
(97.5%), the number of zero scores is well above 80% for all antecedent groups. Thus,
the coverage and recall of a BNC-based algorithm will be very low. Although the
BNC scores are in general much lower than Web scores and although the Web scores
distinguish better between correct/lenient antecedents and distractors, we observe that
Web and BNC scores still correlate significantly, with correlation coefficients between
0.20 and 0.35, depending on antecedent group.28
To summarize, the pattern-based method yields correlated results on different
corpora, but it is expected to depend on large corpora to be really successful.
4.5.3 The Corpus-Based Algorithms. The prototype Web-based algorithm resolves each
anaphor ana to the antecedent candidate in Aanaid with the highest Web score above zero.
If several potential antecedents achieve the same Web score, it uses a tiebreaker based on
string match and recency. If no antecedent candidate achieves a Web score above zero,
string match and recency can be used as a back-off. String comparison for tiebreaker and
back-off can again use the original or the replaced antecedents, yielding two versions,
algoWebv1 (original antecedents) and algoWebv2 (replaced antecedents).
The exact procedure for the version algoWebv1 for an anaphor ana is as follows:29
(i) for each antecedent a in Aanaid, compute its Web score WMa. Compute
the maximum WM of all Web scores over all antecedents in Aanaid. If WMa
is equal to WM and bigger than zero, push a into a set AWManaid;
28 Correlation significance was measured by both a t-test for the correlation coefficient and also by the
nonparametric paired Kendall rank correlation test, both yielding significance at the 1% level.
29 The algorithm algoWebv2 follows the same basic procedure apart from the variation regarding
original/replaced antecedents in string matching.
383
Computational Linguistics Volume 31, Number 3
(ii) if AWManaid contains exactly one element, select this element and stop;
(iii) otherwise, if AWManaid contains more than one element, string-compare
each antecedent in AWManaid with ana (using original antecedents). If exactly
one element of AWManaid matches ana, select this one and stop; if several match
ana, select the closest to ana within these matching antecedents and stop; if
none match, select the closest to ana within AWManaid and stop;
(iv) otherwise, if AWManaid is empty, make no assigment and stop.
The back-off algorithm algoWeb?v1 uses baselineSTR
?
v1 as a back-off (iv?) if no antecedent
can be assigned (parallel to the back-off in algoWN?v1):
(iv?) otherwise, if AWManaid is empty, use baselineSTR?v1 to assign an antecedent
to ana and stop;
algoWebv1 and algoWebv2 can overrule string matching for anaphors in StrSetv1/StrSetv2.
This happens when the Web score of an antecedent candidate that does not match
the anaphor is higher than the Web scores of matching antecedent candidates. In
particular, there is no guarantee that matching antecedent candidates are included
in AWManaid. In that respect, algoWebv1 and algoWebv2 differ from the corresponding
WordNet alorithms: Matching antecedent candidates are always synonyms of the
anaphor (as each noun is a synonym of itself) and therefore always included in Ahyp/synanaid .
Therefore the WordNet alorithms can be seen as a direct extension of baselineSTR;
that is, they achieve the same results as the string-matching baseline on the sets
StrSetv1/StrSetv2.
Given the high precision of baselineSTR, we might want to exclude the possibility
that the Web algorithms overrule string matching. Instead we can use string matching
prior to Web scoring, use the Web scores only when there are no matching antecedent
candidates, and use recency as the final back-off. This variation then achieves the same
results on the sets StrSetv1/StrSetv2 as the WordNet alorithms and the string-matching
baselines. In combination with the possibility of using original or replaced antecedents
for string matching this yields four algorithm variations overall (see Table 6). The
results (see Table 7) do not show any significant differences according to the variation
explored.
The BNC-based algorithms follow the same procedures as the Web-based algo-
rithms, using the BNC scores instead of Web scores. The results (see Table 8) are
disappointing because of data sparseness (see above). No variation yields considerable
improvement over baselineSTRv2 in the final precision?; in fact, in most cases the varia-
Table 6
Properties of the variations for the corpus-based algorithms for other-anaphora.
Replaced/original antecedent Overrule string matching?
v1 original yes
v2 replaced yes
v3 original no
v4 replaced no
384
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 7
Web results for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
algoWebv1 0.950 0.520 0.495 0.507 0.512
algoWebv2 0.950 0.518 0.493 0.505 0.509
algoWebv3 0.958 0.534 0.512 0.523 0.519
algoWebv4 0.961 0.538 0.517 0.527 0.524
Table 8
BNC results for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
algoBNCv1 0.210 0.488 0.103 0.170 0.355
algoBNCv2 0.210 0.488 0.103 0.170 0.360
algoBNCv3 0.417 0.618 0.257 0.363 0.370
algoBNCv4 0.419 0.626 0.262 0.369 0.375
tions just apply a string-matching baseline, either as a back-off or prior to checking BNC
scores, depending on the variation used.
4.6 Discussion and Error Analysis
The performances of the best versions of all algorithms for other-anaphora are summa-
rized in Table 9.
4.6.1 Algorithm Comparison. Algorithms are compared on their final precision? using
two tests throughout this article. We used a t-test to measure the difference between two
algorithms in the proportion of correctly resolved anaphors. However, there are many
examples which are easy (for example, string-matching examples) and that therefore
most or all algorithms will resolve correctly, as well as many that are too hard for all
algorithms. Therefore, we also compare two algorithms using McNemar?s test, which
only relies on the part of the data set in which the algorithms do not give the same
answer.30 If not otherwise stated, all significance claims hold at the 5% level for both the
t-test and McNemar?s test.
The algorithm baselineSTR significantly outperforms baselineREC in precision?,
showing that the ?same predicate match? is quite accurate even though not very
frequent (coverage is only 30.9%). The WordNet-based and Web-based algorithms
achieve a final precision that is significantly better than the baselines? as well as
algoBNC?s. Most interestingly, the Web-based algorithms significantly outperform the
WordNet-based algorithms, confirming our predictions based on the descriptive statis-
tics. The Web approach, for example, resolves Examples (3), (4), (6), and (11) (which
WordNet could not resolve) in addition to Examples (8) and (9), which both the Web
and WordNet alorithms could resolve.
30 We thank an anonymous reviewer for suggesting the use of McNemar?s test for this article.
385
Computational Linguistics Volume 31, Number 3
Table 9
Overview of the results for the best algorithms for other-anaphora.
Algorithm Coverage Precision Recall F-measure Precision?
baselineREC 1.000 0.178 0.178 0.178 0.178
baselineSTRv2 0.309 0.698 0.216 0.329 0.350
algoBNCv4 0.419 0.626 0.262 0.369 0.375
algoWNv2 0.652 0.568 0.370 0.448 0.444
algoWebv4 0.961 0.538 0.517 0.527 0.524
As expected, the WordNet-based algorithms suffer from the problems discussed
in Section 2.2. In particular, Problem 1 proved to be quite severe, as algoWN achieved
a coverage of only 65.2%. Missing links in WordNet alo affect precision if a good
distractor has a link to the anaphor in WordNet, whereas the correct antecedent does
not (Example (10)). Missing links are both universal relations that should be included
in an ontology (such as home:facility) and context-dependent links (e.g., age:(risk) factor,
costs:repercussions; see Problem 2 in Section 2.2.2). Further mining of WordNet beyond
following hyponymy/synonymy links might alleviate Problem 1 but is more costly
and might lead to false positives (Problem 3). To a lesser degree, the WordNet alo-
rithms also suffer from sense proliferation (Problem 4), as all senses of both anaphor
and antecedent candidates were considered. Therefore, some hyp/syn relations based
on a sense not intended in the text were found, leading to wrong-antecedent selection
and lowering precision. In Example (11), for instance, there is no hyponymy link be-
tween the head noun of the correct antecedent (question) and the head noun of the
anaphor (issue), whereas there is a hyponymy link between issue and person = [Mr.
Dallara] (using the sense of issue as offspring) as well as a synonymy link between number
and issue. While in this case considering the most frequent sense of the anaphor issue as
indicated in WordNet would help, this would backfire in other cases in our data set in
which issue is mostly used in the minority sense of stock, share. Obviously, prior word
sense disambiguation would be the most principled but also a more costly solution.
(11) While Mr. Dallara and Japanese officials say the question of investors
access to the U.S. and Japanese markets may get a disproportionate
share of the public?s attention, a number of other important economic
issues [. . . ]
The Web-based method does not suffer as much from these problems. The linguistically
motivated patterns we use reduce long-distance dependencies between anaphor and
antecedent to local dependencies. By looking up these patterns on the Web we make
use of a large amount of data that is very likely to encode strong semantic links
via these local dependencies and to do so frequently. This holds both for universal
hyponymy relations (addressing Problem 1) and relations that are not necessarily to
be included in an ontology (addressing Problem 2). The problem of whether to include
subjective and context-dependent relations in an ontology (Problem 2) is circumvented
by using Web scores only in comparison to Web scores of other antecedent candidates.
In addition, the Web-based algorithm needs no hand-processing or hand-modeling
whatsoever, thereby avoiding the manual effort of building ontologies. Moreover, the
local dependencies we use reduce the need for prior word sense disambiguation (Prob-
lem 4), as the anaphor and the antecedent constrain each other?s sense within the
386
Markert and Nissim Knowledge Sources for Anaphora Resolution
Figure 1
Decision tree for error classification.
context of the pattern. Furthermore, the Web scores are based on frequency, which
biases the Web-based algorithms toward frequent senses as well as sense pairs that
occur together frequently. Thus, the Web algorithm has no problem resolving issue
to question in Example (11) because of the high frequency of the query question OR
questions and other issues. Problem 3 is still not addressed, however, as any corpus can
encode the same semantic relations via different patterns. Combining patterns might
therefore yield problems similar to those presented by combining information sources
in an ontology.
Our pattern-based method, though, seems to work on very large corpora only.
Unlike the Web-based algorithms, the BNC-based ones make use of POS tagging
and observe sentence boundaries, thus reducing the noise intrinsic to an unprocessed
corpus like the Web. Moreover, the instantiations used in algoBNC allow for modifi-
cation to occur (see Table 4), thus increasing chances of a match. Nevertheless, the
BNC-based algorithms performed much worse than the Web-based ones: Only 4.2% of
all pattern instantiations were found in the BNC, yielding very low coverage and recall
(see Table 5).
4.6.2 Error Analysis. Although the Web algorithms perform best, algoWEBv4 still incurs
194 errors (47.6% of 408). Because in several cases there is more than one reason for a
wrong assignment, we use the decision tree in Figure 1 for error classification. By using
this decision tree, we can, for example, exclude from further analysis those cases that
none of the algorithms could resolve because of their intrinsic design.
As can be seen in Table 10, quite a large number of errors result from deleting
pronouns as well as not dealing with split antecedents (44 cases, or 22.7% of all mis-
takes).31 Out of these 44, 30 involve split antecedents. In 19 of these 30 cases, one
of the several correct antecedents has indeed been chosen by our algorithm, but all
the correct antecedents need to be found to allow for the resolution to be counted as
correct.
Given the high number of NE antecedents in our corpus (43.8% of correct, 25%
of all antecedents; see Table 1), NE resolution is crucial. In 11.3% of the cases, the
algorithm selects a distractor instead of the correct antecedent because the NER module
31 Percentages of errors are rounded to the first decimal; rounding errors account for the coverage of 99.9%
of errors instead of 100%.
387
Computational Linguistics Volume 31, Number 3
Table 10
Occurrences of error types for the best other-anaphora algorithm algoWebv4.
Error type Number of cases Percentage of cases
Design 44 22.7
Named entity 22 11.3
String matching 19 9.8
Zero score 48 24.7
Tiebreaker 13 6.7
Other 48 24.7
Total 194 99.9
either leaves the correct antecedent unresolved (which could then lead to very few or
zero hits in Google) or resolves the named entity to the wrong NE category. String
matching is a minor cause of errors (under 10%). This is because, apart from its being
generally reliable, there is also a possible string match only in just about 30% of the cases
(see Table 2).
Many mistakes, instead, occur because other-anaphora can express heavily context-
dependent and very unconventional relations, such as the description of dolls as winners
in Example (12).
(12) Coleco bounced back with the introduction of the Cabbage Patch
dolls. [. . . ] But as the craze died, Coleco failed to come up with another
winner. [. . . ]
In such cases, the relation between the anaphor and antecedent head nouns is not
frequent enough to be found in a corpus even as large as the Web.32 This is mir-
rored in the high percentage of zero-score errors (24.7% of all mistakes). Although the
Web algorithm suffers from a knowledge gap to a smaller degree than WordNet,
there is still a substantial number of cases in which we cannot find the right lexical
relation.
Errors of type other are normally due to good distractors that achieve higher Web
scores than the correct antecedent. A common reason is that the wished-for relation
is attested but rare and therefore other candidates yield higher scores. This is simi-
lar to zero-score errors. Furthermore, the elimination of modification, although useful to
reduce data sparseness, can sometimes lead to the elimination of information that
could help disambiguate among several candidate antecedents. Lastly, lexical informa-
tion, albeit crucial and probably more important than syntactic information (Modjeska
2002), is not sufficient for the resolution of other-anaphora. The integration of other
features, such as grammatical function, NP form, and discourse structure, could prob-
ably help when very good distractors cannot be ruled out by purely lexical methods
(Example (10)). The integration of the Web feature in a machine-learning algorithm
using several other features has yielded good results (Modjeska, Markert, and Nissim
2003).
32 Using different or simply more patterns might yield some hits for anaphor?antecedent pairs that return a
zero score when instantiated in the pattern we use in this article.
388
Markert and Nissim Knowledge Sources for Anaphora Resolution
5. Case Study II: Definite NP Coreference
The Web-based method we have described outperforms WordNet as a knowledge
source for antecedent selection in other-anaphora resolution. However, it is not clear
how far the method and the achieved comparative results generalize to other kinds of
full NP anaphora. In particular, we are interested in the following questions:
 Is the knowledge gap encountered in WordNet for other-anaphora equally
severe for other kinds of full NP anaphora? A partial (mostly affirmative)
answer to this is given by previous researchers, who put the knowledge
gap for coreference at 30?50% and for bridging at 38?80%, depending on
language, domain, and corpus (see Section 2).
 Do the Web-based method and the specific search patterns we use
generalize to other kinds of anaphora?
 Do different anaphoric phenomena require different lexical knowledge
sources?
As a contribution, we investigate the performance of the knowledge sources discussed
for other-anaphora in the resolution of coreferential NPs with full lexical heads,
concentrating on definite NPs (see Example (1)). The automatic resolution of such
anaphors has been the subject of quite significant interest in the past years, but results
are much less satisfactory than those obtained for the resolution of pronouns (see
Section 2).
The relation between the head nouns of coreferential definite NPs and their
antecedents is again, in general, one of hyponymy or synonymy, making an extension
of our approach feasible. However, other-anaphors are especially apt at conveying
context-specific or subjective information by forcing the reader via the other-expression
to accommodate specific viewpoints. This might not hold for definite NPs.33
5.1 Corpus Collection
We extracted definite NP anaphors and their candidate antecedents from the MUC-6
coreference corpus, including both the original training and test material, for a total
of 60 documents. The documents were automatically preprocessed in the following
way: All meta-information about each document indicated in XML (such as WSJ cat-
egory and date) was discarded, and the headline was included and counted as one
sentence. Whenever headlines contained three dashes, everything after the dashes was
discarded.
We then converted the MUC coreference chains into an anaphor?antecedent anno-
tation concentrating on anaphoric definite NPs. All definite NPs which are in, but not
at the beginning of, a coreference chain are potential anaphors. We excluded definite
NPs with proper noun heads (such as the United States) from this set, since these do
not depend on an antecedent for interpretation and are therefore not truly anaphoric.34
We also excluded appositives, which provide coreference structurally and are therefore
33 We thank an anonymous reviewer for pointing out that this role for coreference is more likely to be
provided by demonstratives than definite NPs.
34 Proper-noun heads are approximated by capitalization in the exclusion procedure.
389
Computational Linguistics Volume 31, Number 3
not anaphoric. Otherwise, we strictly followed the MUC annotation for coreference in
our extraction, although it is not entirely consistent and not necessarily comprehensive
(van Deemter and Kibble 2000). This extraction method yielded a set of 565 anaphoric
definite NPs.
For each extracted anaphor in a coreference chain C we regard the NP in C that is
closest to the anaphor as the correct antecedent, whereas all other previous mentions
in C are regarded as lenient. NPs that occur before the anaphor but are not marked as
being in the same coreference chain are distractors. Since anaphors with split antecedents
are not annotated in MUC, anaphors cannot have more than one correct antecedent. In
Example (13), the NPs with the head nouns Pact, contract, and settlement are marked
as coreferent in MUC: In our annotation, the settlement is an anaphor with a correct
antecedent headed by contract and a lenient antecedent Pact. Other NPs prior to the
anaphor (e.g., Canada or the IWA-Canada union) are distractors.35
(13) Forest Products Firms Tentatively Agree On Pact in Canada. A group of
large British Columbia forest products companies has reached a tentative,
three-year labor contract with about 18,000 members of the IWA-Canada union,
. . .The settlement involves . . .
With respect to other-anaphora, we expanded our window size from two to five sen-
tences (the current and the four previous sentences) and excluded all anaphors with
no correct or lenient antecedent within this window size, thus yielding a final set of 477
anaphors (84.4% of 565). This larger window size is motivated by the fact that a window
size of two would cover only 62.3% of all anaphors (352 out 565).
5.2 Antecedent Extraction, Preprocessing, and Baselines
All NPs prior to the anaphor within the five-sentence window were extracted
as antecedent candidates.36 We further processed anaphors and antecedents as in
Case Study I (see Section 4.2): Modification was stripped and all NPs were lemmatized.
In this experiment, named entities were resolved using Curran and Clark?s (2003) NE
tagger rather than GATE.37 The identified named entities were further subclassified into
finer-grained entities, as described for Case Study I.
The final number of extracted antecedents for the whole data set of 477 anaphors is
14,233, with an average of 29.84 antecedent candidates per anaphor. This figure is much
higher than the average number of antecedent candidates for other-anaphors (10.5)
because of the larger window size used. The data set includes 473 correct antecedents,
803 lenient antecedents, and 12,957 distractors. Table 11 shows the distribution of NP
types for correct and lenient antecedents and for distractors.
There are fewer correct antecedents (473) than anaphors (477) because the MUC
annotation also includes anaphors whose antecedent is not an NP but, for exam-
ple, a nominal modifier in a compound. Thus, in Example (14), the bankruptcy code
is annotated in MUC as coreferential to bankruptcy-law, a modifier in bankruptcy-law
protection.
35 All examples in the coreference study are from the MUC-6 corpus.
36 This extraction was conducted manually, to put this study on an equal footing with Case Study I. It
presupposes perfect NP chunking. A further discussion of this issue can be found in Section 6.
37 Curran and Clark?s (2003) tagger was not available to us during the first case study. Both NE taggers are
state-of-the-art taggers trained on newspaper text.
390
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 11
Distribution of antecedent NP types for definite NP anaphora.
Correct Lenient Distractors All
Pronouns 70 145 1,078 1,293
Named entities 123 316 3,108 3,547
Common nouns 280 342 8,771 9,133
Total 473 803 12,957 14,233
(14) All legal proceedings against Eastern, a unit of Texas Air Corp., were put
on hold when Eastern filed for bankruptcy-law protection March 9. . . . If it
doesn?t go quickly enough, the judge said he may invoke a provision of
the bankruptcy code [. . . ]
In our scheme we extract the bankruptcy code as anaphoric but our method of extract-
ing candidate antecedents does not include bankruptcy-law. Therefore, there are four
anaphors in our data set with no correct/lenient antecedent extracted. These cannot be
resolved by any of the suggested approaches.
We use the same evaluation measures as for other-anaphora as well as the same
significance tests for precision?. We also use the same baseline variations baselineREC,
baselineSTRv1, and baselineSTRv2 (see Table 12 and cf. Table 2). The recency baseline per-
forms worse than for other-anaphora. String matching improves dramatically on simple
recency. It also seems to be more relevant than for our other-anaphora data set, achieving
higher coverage, precision, and recall. This confirms the high value of string matching
that has been assigned to coreference resolution by previous researchers (Soon, Ng, and
Lim 2001; Strube, Rapp, and Mueller 2002, among others).
As the MUC data set does not include split antecedents, an anaphor ana usually
agrees in number with its antecedent. Therefore, we also explored variations of all
algorithms that as a first step delete from Aanaid all candidate antecedents that do not
agree in number with ana.38 The algorithms then proceed as usual. Algorithms that
use number checking are marked with an additional n in the subscript. Using number
checking leads to small but consistent gains for all baselines.
As in Case Study I, we deleted pronouns for the WordNet- and corpus-based meth-
ods, thereby removing 70 of 473 (14.8%) of correct antecedents (see Table 11). After
pronoun deletion, the total number of antecedents in our data set is 12,940 for 477
anaphors, of which 403 are correct antecedents, 658 are lenient antecedents, and 11,879
are distractors.
38 The number feature can have the values singular, plural, or unknown. All NE antecedent candidates
received the value singular, as this was by far the most common occurrence in the data set. Information
about the grammatical number of anaphors and common-noun antecedent candidates was calculated
and retained as additional information during the lemmatization process. If lemmatization to both a
plural and a singular noun (as determined by WordNet and CELEX) was possible (for example, the word
talks could be lemmatized to talk or talks), the value unknown was used. An anaphor and an antecedent
candidate were said to agree in number if they had the same value or if at least one of the two values was
unknown.
391
Computational Linguistics Volume 31, Number 3
Table 12
Overview of the results for all baselines for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
baselineREC 1.000 0.031 0.031 0.031 0.031
baselineSTRv1 0.637 0.803 0.511 0.625 0.532
baselineSTRv2 0.717 0.775 0.555 0.647 0.570
With number checking
baselineRECn 1.000 0.086 0.086 0.086 0.086
baselineSTRv1n 0.614 0.833 0.511 0.634 0.549
baselineSTRv2n 0.694 0.809 0.562 0.664 0.591
5.3 WordNet for Antecedent Selection in Definite NP Coreference
We hypothesize that again most antecedents are hyponyms or synonyms of their
anaphors in definite NP coreference (see Examples (1) and (13)). Therefore we use the
same look-up for hyp/syn relations that was used for other-anaphora (see Section 4.4),
including the specifications for common noun and proper name look-ups. Parallel to
Table 3, Table 13 summarizes how many correct and lenient antecedents and distractors
stand in a hyp/syn relation to their anaphor in WordNet.
As already observed for other-anaphora, correct and lenient antecedents stand in a
hyp/syn relation to their anaphor significantly more often than distractors do (t-test,
p < 0.001). Hyp/syn relations in WordNet might be better at capturing the relation
between antecedent and anaphors for definite NP coreference than for other-anaphora:39
A higher percentage of correct and lenient antecedents of definite NP coreference
(71.96%/67.78%) stand in a hyp/syn relation to their anaphors than is the case for
other-anaphora (43.0%/42.5%). At the same time, though, there is no difference in the
percentage of distractors that stand in a hyp/syn relation to their anaphors (9% for other-
anaphora, 8.80% for definite NP coreference). For our WordNet alorithms, this is likely
to translate directly into higher coverage and recall and potentially into higher precision
than in Case Study I. Still, about 30% of correct antecedents are not in a hyp/syn
relation to their anaphor in the current case study, confirming results by Harabagiu,
Bunescu, and Maiorano (2001), who also look at MUC-style corpora.40 This gap, though,
is alleviated by a quite high number of lenient antecedents, whose resolution can make
up for a missing link between anaphor and correct antecedent.41
The WordNet-based algorithms are defined exactly as in Section 4.4, with the
additional two algorithms that include number checking. Results are summarized in
Table 14.
All variations of the WordNet alorithms perform significantly better than the
corresponding versions of the string-matching baseline (i.e., algoWNv1 is better than
baselineSTRv1, . . . , algoWNv2n is better than baselineSTRv2n), showing that they add
39 Some of this difference might be due to the corpus used instead of the phenomenon as such.
40 Harabagiu, Bunescu, and Maiorano (2001) include all common-noun coreference links in their countings,
whereas we concentrate on definite NPs only, so that the results are not exactly the same.
41 The possibility of resolving to lenient antecedents follows a similar approach as that of Ng and Cardie
(2002b), who suggest a ?best-first? coreference resolution approach instead of a ?most recent first?
approach.
392
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 13
Descriptive statistics for WordNet hyp/syn relations on the coreference data set.
Hyp/syn relation to anaphor No hyp/syn relation Total
Correct antecedents 290 (71.96%) 113 (28.04%) 403 (100%)
Lenient antecedents 446 (67.78%) 212 (32.22%) 658 (100%)
Distractors 1,046 (8.80%) 10,833 (91.20%) 11,879 (100%)
All antecedents 1,782 (13.77%) 11,158 (86.23%) 12,940 (100%)
additional lexical knowledge to string matching. As expected from the descriptive
statistics discussed above, the results are better than those obtained by the WordNet
algorithms for other-anaphora, even if we disregard the additional morphosyntactic
number constraint.
5.4 The Corpus-Based Approach for Definite NP Coreference
Following the assumption that most antecedents are hyponyms or synonyms of
their anaphors in definite NP coreference, we use the same list-context pattern and
instantiations that were used for other-anaphora, allowing us to evaluate whether they
are transferrable. The corpora we use are again the Web and the BNC.
As with other-anaphora, the Web scores do well in distinguishing between cor-
rect/lenient antecedents and distractors, with significantly higher means/medians for
correct/lenient antecedents (median 472/617 vs. 2 for distractors), as well as signifi-
cantly fewer zero scores (8% for correct/lenient vs. 41% for distractors). This indicates
transferability of the Web-based approach to coreference. Compared to other-anaphora,
the number of zero-scores is lower for correct/lenient antecedent types, so that we
expect better overall results, similar to our expectations for the WordNet alorithm.
The BNC scores can also distinguish between correct/lenient antecedents and
distractors, since the number of zero scores for correct/lenient antecedents (68.98%/
58.05%) is significantly lower than for distractors (96.97%). Although more than
50% of correct/lenient antecedents receive a zero score, there are fewer zero scores
than for other-anaphora (for which more than 80% of correct/lenient antecedents re-
ceived zero scores). However, BNC scores are again in general much lower than Web
scores, as measured by means, medians, and zero scores. Nevertheless, Web scores
and BNC scores correlate significantly, with the correlations reaching higher coeffi-
Table 14
Overview of the results for all WordNet alorithms for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
algoWNv1 0.874 0.715 0.625 0.666 0.631
algoWNv2 0.874 0.724 0.633 0.676 0.639
With number checking
algoWNv1n 0.866 0.734 0.635 0.681 0.648
algoWNv2n 0.866 0.751 0.649 0.697 0.662
393
Computational Linguistics Volume 31, Number 3
cients (0.53 to 0.65, depending on antecedent group) than they did in the case study for
other-anaphora.
The corpus-based algorithms for coreference resolution are parallel to those
described for other-anaphora and are marked by the same subscripts. The variations that
include number checking are again marked by a subscript n. Tables 15 and 16 report the
results for all the Web and BNC algorithms, respectively.
5.5 Discussion and Error Analysis
5.5.1 Algorithm Comparison. Using the original or the replaced antecedent for string
matching (versions v1 vs. v2, v1n vs. v2n, v3 vs. v4, and v3n vs. v4n) never results
in interesting differences in any of the approaches discussed. Also, number matching
provides consistent improvements. Therefore, from this point on, our discussion will
disregard those variations, that use original antecedents only (v1, v1n, v3, and v3n) as
well as algorithms that do not use number matching (v2, v4). We will also concentrate
on the final precision? of the full-coverage algorithms. The set of anaphors that are cov-
ered by the best string-matching baseline, prior to recency back-off, will again be denoted
by StrSetv2n. Again, both a t-test and McNemar?s test will be used, when statements
about significance are made.
The results for the string-matching baselines and for the lexical methods are higher
for definite coreferential NPs than for other-anaphora. This is largely a result of the
higher number of string-matching antecedent/anaphor pairs in coreference, the higher
precision of string matching, and to a lesser degree, the lower number of unusual
redescriptions.
Similar to the results for other-anaphora, the WordNet-based algorithms beat the
corresponding baselines. The first striking result is that the Web algorithm variation
algoWebv2n, which relies only on the highest Web scores and is therefore allowed
to overrule string matching, does not outperform the corresponding string-matching
baseline baselineSTRv2n and performs significantly worse than the corresponding
WordNet alorithm algoWNv2n. This contrasts with the results for other-anaphora. When
the results were examined in detail, it emerged that for a considerable number of
anaphors in StrSetv2n, the highest Web score was indeed achieved by a distractor with
a high-frequency head noun when the correct or lenient antecedent could be instead
found by a simple string match to the anaphor. This problem is much more severe than
Table 15
Overview of the results for all Web algorithms for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
algoWebv1 0.994 0.561 0.558 0.559 0.562
algoWebv2 0.994 0.553 0.549 0.550 0.554
algoWebv3 0.998 0.674 0.673 0.673 0.673
algoWebv4 0.998 0.679 0.677 0.678 0.677
With number checking
algoWebv1n 0.992 0.613 0.608 0.610 0.612
algoWebv2n 0.992 0.607 0.602 0.604 0.606
algoWebv3n 0.996 0.705 0.702 0.703 0.703
algoWebv4n 0.996 0.716 0.713 0.714 0.713
394
Markert and Nissim Knowledge Sources for Anaphora Resolution
Table 16
Overview of the results for all BNC algorithms for coreference.
Algorithm Coverage Precision Recall F-measure Precision?
algoBNCv1 0.438 0.559 0.245 0.341 0.524
algoBNCv2 0.438 0.559 0.245 0.341 0.526
algoBNCv3 0.769 0.749 0.576 0.651 0.589
algoBNCv4 0.777 0.757 0.589 0.663 0.599
With number checking
algoBNCv1n 0.411 0.612 0.251 0.356 0.562
algoBNCv2n 0.411 0.622 0.256 0.369 0.570
algoBNCv3n 0.753 0.769 0.579 0.661 0.610
algoBNCv4n 0.761 0.785 0.597 0.678 0.627
for other-anaphora because of (1) the larger window size that includes more distractors
and (2) the higher a priori precision of the string-matching baseline, which means that
overruling string matching leads to wrong results more frequently. Typical examples
involve named-entity recognition and inverted queries. Thus, in Example (15), the
anaphor the union is coreferent with the first occurrence of the union, a case easily
resolved by string matching. However, the distractor organization [= Chrysler Canada]
achieves a higher Web score, because of the score of the inverted query union OR unions
and other organizations.42
(15) [. . . ] The union struck Chrysler Canada Tuesday after rejecting a company
offer on pension adjustments. The union said the size of the adjustments
was inadequate.
Several potential solutions exist to this problem, such as normalization of Web scores
or penalizing of inverted queries. The solution we have adopted in algoWebv4n is to
use Web scores only after string matching, thereby making the Web-based approach
more comparable to the WordNet approach. Therefore, baselineSTRv2n, algoWebv4n, and
algoWNv2n (as well as algoBNCv4n) all coincide in their decisions for anaphors in StrSetv2n
and only differ in the decisions made for anaphors that do not have a matching
antecedent candidate. Indeed, algoWebv4n performs significantly better than the base-
lines at the 1% level, and results rise from a precision? of 60.6% for algoWebv2n to 71.3%
for algoWebv4n. It also significantly outperforms the best BNC results, thus showing that
overcoming data sparseness is more important than working with a controlled, tagged,
and representative corpus. Furthermore, shows better performance than WordNet in the
final algorithm variation (71.3% vs. 66.2%).43 According to results of a t-test, however,
this last difference is not significant. McNemar?s test, concentrating on the part of the
data in which the methods differ, shows instead significance at the 1% level.
Indeed, one of the problems in comparing algorithm results for coreference is that
such a large number of anaphors are covered by simple string matching, leaving only
42 Remember that this problem does not affect the WordNet-based algorithm, which always achieves the
same results as the string-matching baseline on StrSetv2n. Both the correct antecedent and the organization
[= Chrysler Canada] distractor stand in a hyp/syn relation to the anaphor, and then string matching is
used as a tiebreaker.
43 In general, the WordNet methods achieve higher precision, with the Web method achieving higher recall.
395
Computational Linguistics Volume 31, Number 3
a small data set on which the lexical methods can differ. Thus, StrSetv2n contains 331 of
477 cases (268 of which are assigned correctly by baselineStrv2n), so that improvements by
the other methods are confined to the set of the remaining 146 anaphors. Of these 146,
baselineStr?v2n assigns the correct antecedent to 13 (8.9%) anaphors by using a recency
back-off, the best WordNet method to 55 anaphors (37.67%), and the best Web method
to 72 anaphors (49.31%). Therefore the Web-based method is a better complement to
string matching than WordNet, which is reflected in the results of McNemar?s test.
Anaphor?antecedent relations that were not covered in WordNet but that did not prove
a problem for the Web algorithm were again both general hyponymy relations, such as
retailer:organization, bill:legislation and month:time, and more subjective relations like (wage)
cuts:concessions and legislation:attack.
5.5.2 Error Analysis. The best-performing Web-based algorithm, algoWebv4n, still selects
the wrong antecedent for a given anaphor in 137 of 477 cases (28.7%). Again, we use
the decision tree in Figure 1 to classify errors. Design errors now do not include split
antecedents but do include errors that occur because the condition of number agreement
was violated, pronoun deletion errors, and the four cases in which the antecedent is a
non-NP antecedent and therefore not extracted in the first place (see Section 5.1 and
Example (14)). Table 17 reports the frequency of each error type.
Differently from other-anaphora, the design and NE errors together account for
under 15% of the mistakes. Also rare are zero-score errors (only 8%). When compared
to the number of zero-score errors in other anaphora (24.7%), this low figure suggests
that other-anaphora is more prone to exploit rare, unusual, and context-dependent
redescriptions than full NP coreference. Nevertheless, it is yet possible to find non-
standard redescriptions in coreference as well which yield zero scores, such as the use
of transaction to refer to move in Example (16).
(16) Conseco Inc., in a move to generate about $200 million in tax deductions,
said it induced five of its top executives to exercise stock options to
purchase about 3.6 million common shares of the financial-services
concern. As a result of the transaction, . . .
Much more substantial is the weight of errors due to string matching, tiebreaker deci-
sions, and the presence of good distractors (the main reason for errors of type other),
which together account for over three-quarters of all mistakes.
String matching is quite successful for coreference (baselineSTRv2n covers nearly
70% of the cases with a precision of 80.9%). However, because algoWebv4n never over-
Table 17
Occurrences of error types for the best coreference algorithm algoWebv4n.
Error type Number of cases Percentage of cases
Design 12 8.7
Named entity 7 5.1
String matching 33 24.1
Zero scores 11 8.0
Tiebreaker 34 24.8
Other 40 29.2
Total 137 99.9
396
Markert and Nissim Knowledge Sources for Anaphora Resolution
rules string matching, the errors of baselineSTRv2n are preserved here and account for
24.1% of all mistakes.44 Tiebreaker errors are quite frequent too (24.8%), as our far-from-
sophisticated tiebreaker was needed in nearly half of the cases (224 times; 47.0%).
The remaining errors (29.2%) are due to the presence of good distractors that score
higher than the correct/lenient antecedent. In Example (17), for instance, a distractor
with a higher Web score (comment) prevents the algorithm from selecting the correct
antecedent (investigation) for the anaphor the inquiry.
(17) Mr. Adams couldn?t be reached for comment. Though the investigation has
barely begun, persons close to the board said Messrs. Lavin and Young
will get a ?hard look? as to whether they were involved, and are both
considered a ?natural focus? of the inquiry.
Example (18) shows how stripping modification might have eliminated information
crucial to identifying the correct antecedent: Only the head process was retained of the
anaphor arbitration process, so that the surface link between anaphor and antecedent
(arbitration) was lost and the distractor securities industry, reduced to industry, was
instead selected.
(18) The securities industry has favored arbitration because it keeps brokers and
dealers out of court. But consumer advocates say customers sometimes
unwittingly sign away their right to sue. ?We don?t necessarily have a beef
with the arbitration process,? says Martin Meehan, [. . . ]
6. Open Issues
6.1 Preprocessing and Prior Assumptions
Our algorithms build on two main preprocessing assumptions. First, we assume perfect
base-NP chunking and expect results to be lower with automatic chunking. Neverthe-
less, since automatic chunking will affect all algorithms in the same way, we do expect
comparative results to stand. We are not, however, dependent on full parsing, as no
parsing-dependent grammatical features are used by the algorithms.
Second, the anaphoricity of the definite NPs in Case Study II has de facto been
manually determined, as we restrict our study to antecedent selection for the NPs
that are marked in the MUC corpus as coreferent. One of the reasons why pronoun res-
olution has been more successful than definite NP resolution is that whereas pronouns
are mostly anaphoric, definite NPs do not have to be so (see Section 2). In fact, it has been
argued by several researchers that an anaphora resolution algorithm should proceed to
antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng
2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a two-
stage process which we also follow in this article. Although recent work on automatic
anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our
algorithms will perform worse when building on non-manually determined anaphors.
Future work will explore the extent of such a decrease in performance.
44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors.
397
Computational Linguistics Volume 31, Number 3
6.2 Directions for Improvement
All algorithms we have described can be considered blueprints for more complex
versions. Specifically, the WordNet-based algorithms could be improved by exploiting
information encoded in WordNet beyond explicitly encoded links (glosses could be
mined, too, for example; see also Harabagiu, Bunescu, and Maiorano [2001]). The Web-
based algorithms could similarly benefit from the exploration of different patterns and
their combination, as well as from using non-pattern-based approaches for hyponymy
detection (Shinzato and Torisawa 2004). In addition, we have evaluated the contribution
of lexical resources in isolation rather than within a more sophisticated system that
integrates additional non-lexical features. It is unclear whether integrating such knowl-
edge sources in a full-resolution system might even out the differences between the
Web-based and the WordNet-based algorithms or exacerbate them. Modjeska, Markert,
and Nissim (2003) included a feature based on Web scores in a naive Bayes model
for other-anaphora resolution that also used grammatical features and showed that
the addition of the Web feature yielded an 11.4-percentage-point improvement over
using a WordNet-based feature. This gives some indication that additional grammatical
features might not be able to compensate fully for the knowledge gap encountered in
WordNet.
6.3 Extension to Yet Other Anaphora Types
Using the Web for antecedent selection in anaphora resolution is novel and needs
further study for other types of full NP anaphora than the ones studied in this article.
If an anaphora type exploits hyponymy/synonymy relationships between anaphor and
antecedent head nouns, it can in principle be treated with the exact same pattern we
used in this article. This holds, for example, for demonstratives and such-anaphors. The
latter, in particular, are similar to other-anaphora in that they establish a comparison
between the entity they invoke and that invoked by the antecedent and are also easily
used to accommodate subjective viewpoints. They should therefore benefit especially
from not relying wholly on standard taxonomic links.
Different patterns can be developed for anaphora types that build on non-
hyponymy relations. For example, bridging exploits meronymy and/or causal rela-
tions (among others). Therefore, patterns that express ?part-of? links, for example,
such as X of Y and genitives, would be appropriate. Indeed, these patterns have been
recently used in Web search for antecedent selection for bridging anaphora by Poesio
et al (2004). They compare accuracy in antecedent selection for a method that inte-
grates Web hits and focusing techniques with a method that uses WordNet and fo-
cusing, achieving comparable results for both methods. This strenghtens our hypothesis
that antecedent selection for full NP anaphora without hand-modeled lexical knowl-
edge has become feasible.
7. Conclusions
We have explored two different ways of exploiting lexical knowledge for antecedent
selection in other-anaphora and definite NP coreference. Specifically, we have compared
a hand-crafted and -structured source of information such as WordNet and a simple
and inexpensive pattern-based method operating on corpora. As corpora we have used
the BNC and also suggested the Web as the biggest corpus available.
398
Markert and Nissim Knowledge Sources for Anaphora Resolution
We confirmed results by other researchers that show that a substantial number of
lexical links often exploited in coreference are not included in WordNet. We have also
shown the presence of an even more severe knowledge gap for other-anaphora (see also
Question 1 in Section 1). Largely because of this knowledge gap, the novel Web-based
method that we proposed proved better than WordNet at resolving other-anaphora.
Although the gains for coreference are not as high, the Web-based method improves
more substantially on string-matching techniques for coreference than WordNet does
(see the success rate beyond StrSetv2n for coreference, Section 5.5). In both studies, the
Web-based method clearly outperformed the BNC-based one. This shows that, for our
tasks, overcoming data sparseness was more important than working with a manually
controlled, virtually noise-free, but relatively small corpus, which addresses Question 2
in Section 1: Corpus-induced knowledge can indeed rival and even outperform the
knowledge obtained via lexical hierarchies, as long as the corpus is large enough.
Corpus-based methods can therefore be a very useful complement to resolution al-
gorithms for languages for which hand-crafted taxonomies have not yet been created
but for which large corpora do exist. In answer to Question 3 in Section 1, our results
suggest that different anaphoric phenomena suffer in varying degrees from missing
knowledge and that the Web-based method performs best when used to deal with
phenomena that standard taxonomy links do not capture that easily or that frequently
exploit subjective and context-dependent knowledge.
In addition, the Web-based method that we propose does not suffer from some
of the intrinsic limitations of ontologies, specifically, the problem of what knowledge
should be included (see Section 2.2). It is also inexpensive and does not need any
postprocessing of the Web pages returned or any hand-modeling of lexical knowledge.
To summarize, antecedent selection for other-anaphora and definite NP coreference
without hand-crafted lexical knowledge is feasible. This might also be the case for yet
other full NP anaphora types with similar properties?an issue that we will explore in
future work.
Acknowledgments
We especially thank Natalia Modjeska for
providing us with her annotated corpus of
other-anaphors as well as with the extracted
and partially preprocessed sets of candidate
antecedents for Case Study I. She also
collaborated on previous related work on
other-anaphora (Markert, Nissim, and
Modjeska 2003; Modjeska, Markert, and
Nissim 2003) on which this article builds. We
would also like to thank Johan Bos, James
Curran, Bonnie Webber, and four
anonymous reviewers for helpful comments,
which allowed us to greatly improve this
article. Malvina Nissim was partially
supported by Scottish Enterprise
Stanford-Link Grants R36766 (Paraphrase
Generation) and R36759 (SEER).
References
Ariel, Mira. 1990. Accessing Noun Phrase
Antecedents. Routledge, London and
New York.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large corpora.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 57?64, Providence, RI.
Bierner, Gann. 2001. Alternative phrases
and natural language information
retrieval. In Proceedings of the 39th
Annual Meeting of the Association for
Computational Linguistics, pages 58?65,
Toulouse, France.
Burnard, Lou, 1995. Users? Reference Guide,
British National Corpus. British National
Corpus Consortium, Oxford.
Caraballo, Sharon. 1999. Automatic
acquisition of a hypernym-labelled noun
hierarchy from text. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 120?126,
Providence, RI.
Chinchor, Nancy. 1997. MUC-7 named entity
task definition. In Proceedings of the Seventh
Conference on Message Understanding,
Washington, DC.
399
Computational Linguistics Volume 31, Number 3
Christ, Oliver, 1995. The XKWIC User Manual.
Institute for Computational Linguistics,
University of Stuttgart.
Clark, Herbert H. 1975. Bridging. In
Proceedings of the Conference on Theoretical
Issues in Natural Language Processing,
pages 169?174, Cambridge, MA.
Connolly, Dennis, John D. Burger, and
David S. Day. 1997. A machine learning
approach to anaphoric reference. In
Daniel Jones and Harold Somers, editors,
New Methods in Language Processing.
University College London Press, London,
pages 133?144.
Curran, James and Stephen Clark. 2003.
Language independent NER using
a maximum entropy tagger. In
Proceedings of the Seventh Conference on
Natural Language Learning (CoNLLO3),
pages 164?167, Edmonton, Alberta,
Canada.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fraurud, Kari. 1990. Definiteness and the
processing of NPs in natural discourse.
Journal of Semantics, 7:395?433.
Gardent, Claire, Helene Manuelian, and
Eric Kow. 2003. Which bridges for
bridging definite descriptions? In
Proceedings of the EACL 2003 Workshop
on Linguistically Interpreted Corpora,
pages 69?76, Budapest.
Grefenstette, Gregory. 1999. The WWW as a
resource for example-based MT tasks. In
Proceedings of ASLIB?99: Translating and the
Computer 21, London.
Gundel, Jeanette, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and the
form of referring expressions in discourse.
Language, 69(2):274?307.
Hahn, Udo, Michael Strube, and Katja
Markert. 1996. Bridging textual ellipses. In
Proceedings of the 16th International
Conference on Computational Linguistics,
pages 496?501, Copenhagen.
Halliday, Michael A. K. and Ruqaiya Hasan.
1976. Cohesion in English. Longman,
London.
Harabagiu, Sanda. 1997. WordNet-Based
Inference of Textual Context, Cohesion and
Coherence. Ph.D. thesis, University of
Southern California.
Harabagiu, Sanda, Razvan Bunescu, and
Steven J. Maiorano. 2001. Text and
knowledge mining for coreference
resolution. In Proceedings of the Second
Conference of the North American Chapter of
the ACL, pages 55?62, Pittsburgh.
Hawkins, John A. 1978. Definiteness and
Indefiniteness. Croom Helm, London.
Hearst, Marti. 1992. Automatic acquisition of
hyponyms from large text corpora. In
Proceedings of the 14th International
Conference on Computational Linguistics,
Nantes, France.
Hirschman, Lynette and Nancy Chinchor.
1997. MUC-7 coreference task definition.
In Proceedings of the Seventh Conference on
Message Understanding, Washington, DC.
Humphreys, Kevin, Robert Gaizauskas,
Saliha Azzam, Chris Huyck, Brian
Mitchell, and Hamish Cunningham. 1997.
University of Sheffield: Description of the
LaSie-II system as used for MUC-7. In
Proceedings of the Seventh Message
Understanding Conference (MUC-7),
Washington, DC.
Kameyama, Megumi. 1997. Recognizing
referential links: An information extraction
perspective. In Proceedings of the ACL-1997
Workshop on Operational Factors in Practical,
Robust Anaphora Resolution for Unrestricted
Texts, pages 46?53, Madrid.
Keller, Frank and Maria Lapata. 2003. Using
the Web to obtain frequencies for unseen
bigrams. Computational Linguistics,
29(3):459?484.
Kennedy, Christopher and Branimir
Boguraev. 1996. Anaphora for everyone:
Pronominal anaphora resolution without a
parser. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 113?118, Copenhagen.
Markert, Katja, Malvina Nissim, and
Natalia N. Modjeska. 2003. Using the
Web for nominal anaphora resolution. In
Robert Dale, Kees van Deemter, and
Ruslan Mitkov, editors, Proceedings of the
EACL Workshop on the Computational
Treatment of Anaphora, pages 39?46,
Budapest.
McCoy, Kathleen and Michael Strube. 1999.
Generating anaphoric expressions:
Pronoun or definite description? In ACL-99
Workshop on the Relation of
Discourse/Dialogue Structure and Reference,
pages 63?71, College Park, MD.
Meyer, Ingrid. 2001. Extracting
knowledge-rich contexts for
terminography. In Didier Bourigault,
Christian Jacquemin, and Marie-Claude
L?Homme, editors, Recent Advances in
Computational Terminology. John Benjamins,
Amsterdam, pages 279?301.
Meyer, Josef and Robert Dale. 2002.
Mining a corpus to support associative
anaphora resolution. In Proceedings of
400
Markert and Nissim Knowledge Sources for Anaphora Resolution
the Fourth International Conference on
Discourse Anaphora and Anaphor Resolution,
Lisbon.
Mitkov, Ruslan. 1998. Robust pronoun
resolution with limited knowledge. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 869?879,
Montreal.
Modjeska, Natalia N. 2002. Lexical and
grammatical role constraints in resolving
other-anaphora. In Proceedings of DAARC
2002, pages 129?134, Lisbon.
Modjeska, Natalia N. 2003. Resolving
other-anaphora. Ph.D. thesis, School of
Informatics, University of Edinburgh.
Modjeska, Natalia N., Katja Markert, and
Malvina Nissim. 2003. Using the Web in
machine learning for other-anaphora
resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 176?183,
Sapporo, Japan.
Ng, Vincent. 2004. Learning noun phrase
anaphoricity to improve coreference
resolution: Issues in representation and
optimization. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 151?158,
Barcelona.
Ng, Vincent and Claire Cardie. 2002a.
Identifying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. In Proceedings of the 19th
International Conference on Computational
Linguistics, pages 730?736, Taipei.
Ng, Vincent and Claire Cardie. 2002b.
Improving machine learning approaches
to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
Philadelphia.
Poesio, Massimo, Tomonori Ishikawa, Sabine
Schulte im Walde, and Renata Vieira. 2002.
Acquiring lexical knowledge for anaphora
resolution. In Proceedings of the Third
International Conference on Language
Resources and Evaluation, pages 1220?1224,
Las Palmas, Canary Islands.
Poesio, Massimo, Rahul Mehta, Axel
Maroudas, and Janet Hitzeman. 2004.
Learning to resolve bridging references. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 143?150, Barcelona.
Poesio, Massimo, Renata Vieira, and Simone
Teufel. 1997. Resolving bridging references
in unrestricted text. In Ruslan Mitkov,
editor, Proceedings of the ACL Workshop on
Operational Factors in Robust Anaphora
Resolution, pages 1?6, Madrid.
Preiss, Judita. 2002. Anaphora resolution
with word sense disambiguation. In
Proceedings of SENSEVAL-2, pages 143?146,
Philadelphia.
Preiss, Judita, Caroline Gasperin, and Ted
Briscoe. 2004. Can anaphoric definite
descriptions be replaced by pronouns? In
Proceedings of the Fourth International
Conference on Language Resources and
Evaluation, pages 1499?1502, Lisbon.
Shinzato, Keiji and Kentaro Torisawa. 2004.
Acquiring hyponymy relations from Web
documents. In Proceedings of the Conference
of the North American Chapter of the ACL,
pages 73?80, Boston.
Soon, Wee Meng, Hwee Tou Ng Ng, and
Daniel Chung Yung Lim. 2001. A machine
learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521?544.
Strube, Michael, Stefan Rapp, and
Christoph Mueller. 2002. The influence
of minimum edit distance on reference
resolution. In Proceedings of the 2002
Conference on Empirical Methods in Natural
Language Processing, pages 312?319,
Philadelphia.
Uryupina, Olga. 2003. High-precision
identification of discourse new and unique
noun phrases. In Proceedings of the ACL
2003 Student Workshop, pages 80?86,
Sapporo, Japan.
van Deemter, Kees and Rodger Kibble. 2000.
On coreferring: Coreference in MUC and
related annotation schemes. Computational
Linguistics, 26(4):615?662.
Vieira, Renata and Massimo Poesio. 2000. An
empirically-based system for processing
definite descriptions. Computational
Linguistics, 26(4):539?593.
Webber, Bonnie, Matthew Stone, Aravind
Joshi, and Alistair Knott. 2003. Anaphora
and discourse structure. Computational
Linguistics, 29(4):545?587.
Yang, Xiaofeng, Guodong Zhou, Jian Su,
and Chew Lim Tan. 2003. Coreference
resolution using competition learning
approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics, pages 176?183, Sapporo, Japan.
401

Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Subjectivity Recognition on Word Senses via Semi-supervised Mincuts
Fangzhong Su
School of Computing
University of Leeds
fzsu@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds
markert@comp.leeds.ac.uk
Abstract
We supplement WordNet entries with infor-
mation on the subjectivity of its word senses.
Supervised classifiers that operate on word
sense definitions in the same way that text
classifiers operate on web or newspaper texts
need large amounts of training data. The re-
sulting data sparseness problem is aggravated
by the fact that dictionary definitions are very
short. We propose a semi-supervised mini-
mum cut framework that makes use of both
WordNet definitions and its relation structure.
The experimental results show that it outper-
forms supervised minimum cut as well as stan-
dard supervised, non-graph classification, re-
ducing the error rate by 40%. In addition, the
semi-supervised approach achieves the same
results as the supervised framework with less
than 20% of the training data.
1 Introduction
There is considerable academic and commercial in-
terest in processing subjective content in text, where
subjective content refers to any expression of a pri-
vate state such as an opinion or belief (Wiebe et
al., 2005). Important strands of work include the
identification of subjective content and the determi-
nation of its polarity, i.e. whether a favourable or
unfavourable opinion is expressed.
Automatic identification of subjective content of-
ten relies on word indicators, such as unigrams
(Pang et al, 2002) or predetermined sentiment lex-
ica (Wilson et al, 2005). Thus, the word positive
in the sentence ?This deal is a positive development
for our company.? gives a strong indication that
the sentence contains a favourable opinion. How-
ever, such word-based indicators can be misleading
for two reasons. First, contextual indicators such as
irony and negation can reverse subjectivity or po-
larity indications (Polanyi and Zaenen, 2004). Sec-
ond, different word senses of a single word can ac-
tually be of different subjectivity or polarity. A typ-
ical subjectivity-ambiguous word, i.e. a word that
has at least one subjective and at least one objec-
tive sense, is positive, as shown by the two example
senses given below.1
(1) positive, electropositive?having a positive electric
charge;?protons are positive? (objective)
(2) plus, positive?involving advantage or good; ?a
plus (or positive) factor? (subjective)
We concentrate on this latter problem by automat-
ically creating lists of subjective senses, instead
of subjective words, via adding subjectivity labels
for senses to electronic lexica, using the exam-
ple of WordNet. This is important as the prob-
lem of subjectivity-ambiguity is frequent: We (Su
and Markert, 2008) find that over 30% of words
in our dataset are subjectivity-ambiguous. Informa-
tion on subjectivity of senses can also improve other
tasks such as word sense disambiguation (Wiebe
and Mihalcea, 2006). Moreover, Andreevskaia and
Bergler (2006) show that the performance of auto-
matic annotation of subjectivity at the word level can
be hurt by the presence of subjectivity-ambiguous
words in the training sets they use.
1All examples in this paper are from WordNet 2.0.
1
We propose a semi-supervised approach based on
minimum cut in a lexical relation graph to assign
subjectivity (subjective/objective) labels to word
senses.2 Our algorithm outperforms supervised min-
imum cuts and standard supervised, non-graph clas-
sification algorithms (like SVM), reducing the error
rate by up to 40%. In addition, the semi-supervised
approach achieves the same results as the supervised
framework with less than 20% of the training data.
Our approach also outperforms prior approaches to
the subjectivity recognition of word senses and per-
forms well across two different data sets.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Section 3
describes our proposed semi-supervised minimum
cut framework in detail. Section 4 presents the ex-
perimental results and evaluation, followed by con-
clusions and future work in Section 5.
2 Related Work
There has been a large and diverse body of research
in opinion mining, with most research at the text
(Pang et al, 2002; Pang and Lee, 2004; Popescu and
Etzioni, 2005; Ounis et al, 2006), sentence (Kim
and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff
et al, 2003; Yu and Hatzivassiloglou, 2003) or word
(Hatzivassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kim and Hovy, 2004; Takamura et
al., 2005; Andreevskaia and Bergler, 2006; Kaji and
Kitsuregawa, 2007) level. An up-to-date overview is
given in Pang and Lee (2008).
Graph-based algorithms for classification into
subjective/objective or positive/negative language
units have been mostly used at the sentence and
document level (Pang and Lee, 2004; Agarwal and
Bhattacharyya, 2005; Thomas et al, 2006), instead
of aiming at dictionary annotation as we do. We
also cannot use prior graph construction methods
for the document level (such as physical proxim-
ity of sentences, used in Pang and Lee (2004)) at
the word sense level. At the word level Taka-
mura et al (2005) use a semi-supervised spin model
for word polarity determination, where the graph
2It can be argued that subjectivity labels are maybe rather
more graded than the clear-cut binary distinction we assign.
However, in Su and Markert (2008a) as well as Wiebe and Mi-
halcea (2006) we find that human can assign the binary distinc-
tion to word senses with a high level of reliability.
is constructed using a variety of information such
as gloss co-occurrences and WordNet links. Apart
from using a different graph-based model from ours,
they assume that subjectivity recognition has already
been achieved prior to polarity recognition and test
against word lists containing subjective words only.
However, Kim and Hovy (2004) and Andreevskaia
and Bergler (2006) show that subjectivity recogni-
tion might be the harder problem with lower human
agreement and automatic performance. In addition,
we deal with classification at the word sense level,
treating also subjectivity-ambiguous words, which
goes beyond the work in Takamura et al (2005).
Word Sense Level: There are three prior ap-
proaches addressing word sense subjectivity or po-
larity classification. Esuli and Sebastiani (2006) de-
termine the polarity (positive/negative/objective) of
word senses in WordNet. However, there is no eval-
uation as to the accuracy of their approach. They
then extend their work (Esuli and Sebastiani, 2007)
by applying the Page Rank algorithm to rank the
WordNet senses in terms of how strongly a sense
possesses a given semantic property (e.g., positive
or negative). Apart from us tackling subjectivity
instead of polarity, their Page Rank graph is also
constructed focusing on WordNet glosses (linking
glosses containing the same words), whereas we
concentrate on the use of WordNet relations.
Both Wiebe and Mihalcea (2006) and our prior
work (Su and Markert, 2008) present an annota-
tion scheme for word sense subjectivity and algo-
rithms for automatic classification. Wiebe and Mi-
halcea (2006) use an algorithm relying on distribu-
tional similarity and an independent, large manually
annotated opinion corpus (MPQA) (Wiebe et al,
2005). One of the disadvantages of their algorithm is
that it is restricted to senses that have distributionally
similar words in the MPQA corpus, excluding 23%
of their test data from automatic classification. Su
and Markert (2008) present supervised classifiers,
which rely mostly on WordNet glosses and do not
effectively exploit WordNet?s relation structure.
3 Semi-Supervised Mincuts
3.1 Minimum Cuts: The Main Idea
Binary classification with minimum cuts (Mincuts)
in graphs is based on the idea that similar items
2
should be grouped in the same cut. All items in the
training/test data are seen as vertices in a graph with
undirected weighted edges between them specifying
how strong the similarity/association between two
vertices is. We use minimum s-t cuts: the graph con-
tains two particular vertices s (source, corresponds
to subjective) and t (sink, corresponds to objective)
and each vertex u is connected to s and t via a
weighted edge that can express how likely u is to
be classified as s or t in isolation.
Binary classification of the vertices is equivalent
to splitting the graph into two disconnected subsets
of all vertices, S and T with s ? S and t ? T .
This corresponds to removing a set of edges from
the graph. As similar items should be in the same
part of the split, the best split is one which removes
edges with low weights. In other words, a minimum
cut problem is to find a partition of the graph which
minimizes the following formula, where w(u, v) ex-
presses the weight of an edge between two vertices.
W (S, T ) = ?
u?S,v?T
w(u, v)
Globally optimal minimum cuts can be found in
polynomial time and near-linear running time in
practice, using the maximum flow algorithm (Pang
and Lee, 2004; Cormen et al, 2002).
3.2 Why might Semi-supervised Minimum
Cuts Work?
We propose semi-supervised mincuts for subjectiv-
ity recognition on senses for several reasons.
First, our problem satisfies two major conditions
necessary for using minimum cuts. It is a bi-
nary classification problem (subjective vs. objective
senses) as is needed to divide the graph into two
components. Our dataset alo lends itself naturally
to s-t Mincuts as we have two different views on the
data. Thus, the edges of a vertex (=sense) to the
source/sink can be seen as the probability of a sense
being subjective or objective without taking similar-
ity to other senses into account, for example via con-
sidering only the sense gloss. In contrast, the edges
between two senses can incorporate the WordNet re-
lation hierarchy, which is a good source of similar-
ity for our problem as many WordNet relations are
subjectivity-preserving, i.e. if two senses are con-
nected via such a relation they are likely to be both
subjective or both objective.3 An example here is
the antonym relation, where two antonyms such as
good?morally admirable and evil, wicked?morally
bad or wrong are both subjective.
Second, Mincuts can be easily expanded into
a semi-supervised framework (Blum and Chawla,
2001). This is essential as the existing labeled
datasets for our problem are small. In addition,
glosses are short, leading to sparse high dimensional
vectors in standard feature representations. Also,
WordNet connections between different parts of the
WordNet hierarchy can also be sparse, leading to
relatively isolated senses in a graph in a supervised
framework. Semi-supervised Mincuts allow us to
import unlabeled data that can serve as bridges to
isolated components. More importantly, as the unla-
beled data can be chosen to be related to the labeled
and test data, they might help pull test data to the
right cuts (categories).
3.3 Formulation of Semi-supervised Mincuts
The formulation of our semi-supervised Mincut for
sense subjectivity classification involves the follow-
ing steps, which we later describe in more detail.
1. We define two vertices s (source) and t (sink),
which correspond to the ?subjective? and ?ob-
jective? category, respectively. Following the
definition in Blum and Chawla (2001), we call
the vertices s and t classification vertices, and
all other vertices (labeled, test, and unlabeled
data) example vertices. Each example vertex
corresponds to one WordNet sense and is con-
nected to both s and t via a weighted edge. The
latter guarantees that the graph is connected.
2. For the test and unlabeled examples, we see
the edges to the classification vertices as the
probability of them being subjective/objective
disregarding other example vertices. We use a
supervised classifier to set these edge weights.
For the labeled training examples, they are con-
nected by edges with a high constant weight to
the classification vertices that they belong to.
3. WordNet relations are used to construct the
edges between two example vertices. Such
3See Kamps et al (2004) for an early indication of such
properties for some WordNet relations.
3
edges can exist between any pair of example
vertices, for example between two unlabeled
examples.
4. After graph construction we then employ a
maximum-flow algorithm to find the minimum
s-t cuts of the graph. The cut in which the
source vertex s lies is classified as ?subjective?,
and the cut in which the sink vertex t lies is ?ob-
jective?.
We now describe the above steps in more detail.
Selection of unlabeled data: Random selection
of unlabeled data might hurt the performance of
Mincuts, as they might not be related to any sense in
our training/test data (denoted by A). Thus a basic
principle is that the selected unlabeled senses should
be related to the training/test data by WordNet rela-
tions. We therefore simply scan each sense inA, and
collect all senses related to it via one of the WordNet
relations in Table 1. All such senses that are not in
A are collected in the unlabeled data set.
Weighting of edges to the classification ver-
tices: The edge weight to s and t represents how
likely it is that an example vertex is initially put in
the cut in which s (subjective) or t (objective) lies.
For unlabeled and test vertices, we use a supervised
classifier (SVM4) with the labeled data as training
data to assign the edge weights. The SVM is also
used as a baseline and its features are described in
Section 4.3. As we do not wish the Mincut to re-
verse labels of the labeled training data, we assign a
high constant weight of 5 to the edge between a la-
beled vertex and its corresponding classification ver-
tex, and a low weight of 0.01 to the edge to the other
classification vertex.
Assigning weights to WordNet relations: We
connect two vertices that are linked by one of
the ten WordNet relations in Table 1 via an edge.
Not all WordNet relations we use are subjectivity-
preserving to the same degree: for example, hy-
ponyms (such as simpleton) of objective senses
(such as person) do not have to be objective. How-
ever, we aim for high graph connectivity and we
can assign different weights to different relations
4We employ LIBSVM, available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm/. Linear kernel and prob-
ability estimates are used in this work.
to reflect the degree to which they are subjectivity-
preserving. Therefore, we experiment with two
methods of weight assignment. Method 1 (NoSL)
assigns the same constant weight of 1.0 to all Word-
Net relations.
Method 2 (SL) reflects different degrees of pre-
serving subjectivity. To do this, we adapt an un-
supervised method of generating a large noisy set
of subjective and objective senses from our previ-
ous work (Su and Markert, 2008). This method
uses a list of subjective words (SL)5 to classify each
WordNet sense with at least two subjective words
in its gloss as subjective and all other senses as ob-
jective. We then count how often two senses re-
lated via a given relation have the same or a dif-
ferent subjectivity label. The weight is computed
by #same/(#same+#different). Results are listed in
Table 1.
Table 1: Relation weights (Method 2)
Method #Same #Different Weight
Antonym 2,808 309 0.90
Similar-to 6,887 1,614 0.81
Derived-from 4,630 947 0.83
Direct-Hypernym 71,915 8,600 0.89
Direct-Hyponym 71,915 8,600 0.89
Attribute 350 109 0.76
Also-see 1,037 337 0.75
Extended-Antonym 6,917 1,651 0.81
Domain 4,387 892 0.83
Domain-member 4,387 892 0.83
Example graph: An example graph is shown in
Figure 1. The three example vertices correspond
to the senses religious?extremely scrupulous and
conscientious, scrupulous?having scruples; aris-
ing from a sense of right and wrong; principled;
and flicker, spark, glint?a momentary flash of light
respectively. The vertex ?scrupulous? is unlabeled
data derived from the vertex ?religious?(a test item)
by the relation ?similar-to?.
4 Experiments and Evaluation
4.1 Datasets
We conduct the experiments on two different gold
standard datasets. One is the Micro-WNOp corpus,
5Available at http://www.cs.pitt.edu/mpqa
4
scrupulous
religious
subjective objective
flicker
0.24 0.76
0.83 0.17
0.16 0.84
0.81similar-to
Figure 1: Graph of Word Senses
which is representative of the part-of-speech distri-
bution in WordNet 6. It includes 298 words with
703 objective and 358 subjective WordNet senses.
The second one is the dataset created by Wiebe
and Mihalcea (2006).7 It only contains noun and
verb senses, and includes 60 words with 236 ob-
jective and 92 subjective WordNet senses. As the
Micro-WNOp set is larger and also contains adjec-
tive and adverb senses, we describe our results in
more detail on that corpus in the Section 4.3 and
4.4. In Section 4.5, we shortly discuss results on
Wiebe&Mihalcea?s dataset.
4.2 Baseline and Evaluation
We compare to a baseline that assigns the most
frequent category objective to all senses, which
achieves an accuracy of 66.3% and 72.0% onMicro-
WNOp and Wiebe&Mihalcea?s dataset respectively.
We use the McNemar test at the significance level of
5% for significance statements. All evaluations are
carried out by 10-fold cross-validation.
4.3 Standard Supervised Learning
We use an SVM classifier to compare our proposed
semi-supervised Mincut approach to a reasonable
6Available at http://www.comp.leeds.ac.uk/
markert/data. This dataset was first used with a different
annotation scheme in Esuli and Sebastiani (2007) and we also
used it in Su and Markert (2008).
7Available at http://www.cs.pitt.edu/?wiebe/
pubs/papers/goldstandard.total.acl06.
baseline.8 Three different feature types are used.
Lexical Features (L): a bag-of-words representa-
tion of the sense glosses with stop word filtering.
Relation Features (R): First, we use two features
for each of the ten WordNet relations in Table 1, de-
scribing how many relations of that type the sense
has to senses in the subjective or objective part of the
training set, respectively. This provides a non-graph
summary of subjectivity-preserving links. Second,
we manually collected a small set (denoted by
SubjSet) of seven subjective verb and noun senses
which are close to the root in WordNet?s hypernym
tree. A typical example element of SubjSet is psy-
chological feature ?a feature of the mental life of a
living organism, which indicates subjectivity for its
hyponyms such as hope ? the general feeling that
some desire will be fulfilled. A binary feature de-
scribes whether a noun/verb sense is a hyponym of
an element of SubjSet.
Monosemous Feature (M): for each sense, we
scan if a monosemous word is part of its synset. If
so, we further check if the monosemous word is col-
lected in the subjective word list (SL). The intuition
is that if a monosemous word is subjective, obvi-
ously its (single) sense is subjective. For example,
the sense uncompromising, inflexible?not making
concessions is subjective, as ?uncompromising? is
a monosemous word and also in SL.
We experiment with different combinations of
features and the results are listed in Table 2, prefixed
by ?SVM?. All combinations perform significantly
better than the more frequent category baseline and
similarly to the supervised Naive Bayes classifier
(see S&M in Table 2) we used in Su and Mark-
ert (2008). However, improvements by adding more
features remain small.
In addition, we compare to a supervised classifier
(see Lesk in Table 2) that just assigns each sense
the subjectivity label of its most similar sense in
the training data, using Lesk?s similarity measure
from Pedersen?s WordNet similarity package9. We
use Lesk as it is one of the few measures applicable
across all parts-of-speech.
8This SVM is also used to provide the edge weights to the
classification vertices in the Mincut approach.
9Available at http://www.d.umn.edu/?tpederse/
similarity.html.
5
Table 2: Results of SVM and Mincuts with different settings of feature
Method Subjective Objective Accuracy
Precision Recall F-score Precision Recall F-score
Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3%
S&M 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%
Lesk 65.6% 50.3% 56.9% 77.5% 86.6% 81.8% 74.4%
SVM-L 69.6% 37.7% 48.9% 74.3% 91.6% 82.0% 73.4%
L-SL 82.0% 43.3% 56.7% 76.7% 95.2% 85.0% 77.7%
L-NoSL 80.8% 43.6% 56.6% 76.7% 94.7% 84.8% 77.5%
SVM-LM 68.9% 42.2% 52.3% 75.4% 90.3% 82.2% 74.1%
LM-SL 83.2% 44.4% 57.9% 77.1% 95.4% 85.3% 78.2%
LM-NoSL 83.6% 44.1% 57.8% 77.1% 95.6% 85.3% 78.2%
SVM-LR 68.4% 45.3% 54.5% 76.2% 89.3% 82.3% 74.5%
LR-SL 82.7% 65.4% 73.0% 84.1% 93.0% 88.3% 83.7%
LR-NoSL 82.4% 65.4% 72.9% 84.0% 92.9% 88.2% 83.6%
SVM-LRM 69.8% 47.2% 56.3% 76.9% 89.6% 82.8% 75.3%
LRM-SL 85.5% 65.6% 74.2% 84.4% 94.3% 89.1% 84.6%
LRM-NoSL 84.6% 65.9% 74.1% 84.4% 93.9% 88.9% 84.4%
1 L, R and M correspond to the lexical, relation and monosemous features respectively.
2 SVM-L corresponds to using lexical features only for the SVM classifier. Likewise, SVM-
LRM corresponds to using a combination for lexical, relation, and monosemous features
for the SVM classifier.
3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier,
and subjective list (SL) to infer the weight of WordNet relations. Likewise, LM-NoSL
corresponds to the Mincut algorithm that uses lexical and monosemous features for the
SVM, and predefined constants for WordNet relations (without subjective list).
4.4 Semi-supervised Graph Mincuts
Using our formulation in Section 3.3, we import
3,220 senses linked by the ten WordNet relations to
any senses in Micro-WNOp as unlabeled data. We
construct edge weights to classification vertices us-
ing the SVM discussed above and use WordNet re-
lations for links between example vertices, weighted
by either constants (NoSL) or via the method illus-
trated in Table 1 (SL). The results are also summa-
rized in Table 2. Semi-supervised Mincuts always
significantly outperform the corresponding SVM
classifiers, regardless of whether the subjectivity list
is used for setting edge weights. We can also see
that we achieve good results without using any other
knowledge sources (setting LR-NoSL).
The example in Figure 1 explains why semi-
supervised Mincuts outperforms the supervised ap-
proach. The vertex ?religious? is initially assigned
the subjective/objective probabilities 0.24/0.76 by
the SVM classifier, leading to a wrong classification.
However, in our graph-based Mincut framework, the
vertex ?religious? might link to other vertices (for
example, it links to the vertex ?scrupulous? in the
unlabeled data by the relation ?similar-to?). The
mincut algorithm will put vertices ?religious? and
?scrupulous? in the same cut (subjective category) as
this results in the least cost 0.93 (ignoring the cost of
assigning the unrelated sense of ?flicker?). In other
words, the edges between the vertices are likely to
correct some initially wrong classification and pull
the vertices into the right cuts.
In the following we will analyze the best mini-
mum cut algorithm LRM-SL in more detail. We
measure its accuracy for each part-of-speech in the
Micro-WNOp dataset. The number of noun, adjec-
tive, adverb and verb senses in Micro-WNOp is 484,
265, 31 and 281, respectively. The result is listed
in Table 3. The significantly better performance of
semi-supervised mincuts holds across all parts-of-
speech but the small set of adverbs, where there is
no significant difference between the baseline, SVM
and the Mincut algorithm.
6
Table 3: Accuracy for Different Part-Of-Speech
Method Noun Adjective Adverb Verb
Baseline 76.9% 61.1% 77.4% 72.6%
SVM 81.4% 63.4% 83.9% 75.1%
Mincut 88.6% 78.9% 77.4% 84.0%
We will now investigate how LRM-SL performs
with different sizes of labeled and unlabeled data.
All learning curves are generated via averaging 10
learning curves from 10-fold cross-validation.
Performance with different sizes of labeled data:
we randomly generate subsets of labeled data A1,
A2... An, and guarantee that A1 ? A2... ? An.
Results for the best SVM (LRM) and the best min-
imum cut (LRM-SL) are listed in Table 4, and the
corresponding learning curve is shown in Figure 2.
As can be seen, the semi-supervised Mincuts is
consistently better than SVM. Moreover, the semi-
supervised Mincut with only 200 labeled data items
performs even better than SVM with 954 training
items (78.9% vs 75.3%), showing that our semi-
supervised framework allows for a training data re-
duction of more than 80%.
Table 4: Accuracy with different sizes of labeled data
# labeled data SVM Mincuts
100 69.1% 72.2%
200 72.6% 78.9%
400 74.4% 82.7%
600 75.5% 83.7%
800 76.0% 84.1%
900 75.6% 84.8%
954 (all) 75.3% 84.6%
Performance with different sizes of unlabeled
data: We propose two different settings.
Option1: Use a subset of the ten relations to
generate the unlabeled data (and edges between
example vertices). For example, we first use
{antonym, similar-to} only to obtain a unlabeled
dataset U1, then use a larger subset of the relations
like {antonym, similar-to, direct-hyponym, direct-
hypernym} to generate another unlabeled dataset
U2, and so forth. Obviously, Ui is a subset of Ui+1.
Option2: Use all the ten relations to generate the
unlabeled data U . We then randomly select subsets
of U , such as subset U1, U2 and U3, and guarantee
that U1 ? U2 ? U3 ? . . . U .
 68
 71
 74
 77
 80
 83
 86
 89
 100  200  300  400  500  600  700  800  900 1000
A
c
c
u
r
a
c
y
(
%
)
Size of Labeled Data
Mincuts
SVM
Figure 2: Learning curve with different sizes of labeled
data
The results are listed in Table 5 and Table 6 re-
spectively. The corresponding learning curves are
shown in Figure 3. We see that performance im-
proves with the increase of unlabeled data. In addi-
tion, the curves seem to converge when the size of
unlabeled data is larger than 3,000. From the results
in Tabel 5 one can also see that hyponymy is the re-
lation accounting for the largest increase.
Table 6: Accuracy with different sizes of unlabeled data
(random selection)
# unlabeled data Accuracy
0 75.9%
200 76.5%
500 78.6%
1000 80.2%
2000 82.8%
3000 84.0%
3220 84.6%
Furthermore, these results also show that a super-
vised mincut without unlabeled data performs only
on a par with other supervised classifiers (75.9%).
The reason is that if we exclude the unlabeled data,
there are only 67 WordNet relations/edges between
senses in the small Micro-WNOp dataset. In con-
trast, the use of unlabeled data adds more edges
(4,586) to the graph, which strongly affects the
graph cut partition (see also Figure 1).
4.5 Comparison to Prior Approaches
In our previous work (Su and Markert, 2008), we re-
port 76.9% as the best accuracy on the same Micro-
7
Table 5: Accuracy with different sizes of unlabeled data from WordNet relation
Relation # unlabeled data Accuracy
{?} 0 75.3%
{similar-to} 418 79.1%
{similar-to, antonym} 514 79.5%
{similar-to, antonym, direct-hypernym, direct-
hyponym}
2,721 84.4%
{similar-to, antonym, direct-hypernym, direct-
hyponym, also-see, extended-antonym}
3,004 84.4%
{similar-to, antonym, direct-hypernym, direct-
hyponym, also-see, extended-antonym, derived-from,
attribute, domain, domain-member}
3,220 84.6%
 75
 77
 79
 81
 83
 85
 87
 89
 0  500  1000  1500  2000  2500  3000  3500
A
c
c
u
r
a
c
y
(
%
)
Size of Unlabeled Data
Option1
Option2
Figure 3: Learning curve with different sizes of unlabeled
data
WNOp dataset used in the previous sections, using a
supervised Naive Bayes (S&M in Tabel 2). Our best
result from Mincuts is significantly better at 84.6%
(see LRM-SL in Table 2).
For comparison to Wiebe and Mihalcea (2006),
we use their dataset for testing, henceforth called
Wiebe (see Section 4.1 for a description). Wiebe
and Mihalcea (2006) report their results in precision
and recall curves for subjective senses, such as a pre-
cision of about 55% at a recall of 50% for subjective
senses. Their F-score for subjective senses seems to
remain relatively static at 0.52 throughout their pre-
cision/recall curve.
We run our best Mincut LRM-SL algorithm with
two different settings on Wiebe. Using Micro-
WNOp as training set and Wiebe as test set, we
achieve an accuracy of 83.2%, which is similar to the
results on the Micro-WNOp dataset. At the recall of
50% we achieve a precision of 83.6% (in compari-
son to their precision of 55% at the same recall). Our
F-score is 0.63 (vs. 0.52).
To check whether the high performance is just due
to our larger training set, we also conduct 10-fold
cross-validation on Wiebe. The accuracy achieved
is 81.1% and the F-score 0.56 (vs. 0.52), suggesting
that our algorithm performs better. Our algorithm
can be used on all WordNet senses whereas theirs is
restricted to senses that have distributionally similar
words in the MPQA corpus (see Section 2). How-
ever, they use an unsupervised algorithm i.e. they
do not need labeled word senses, although they do
need a large, manually annotated opinion corpus.
5 Conclusion and Future Work
We propose a semi-supervised minimum cut algo-
rithm for subjectivity recognition on word senses.
The experimental results show that our proposed ap-
proach is significantly better than a standard super-
vised classification framework as well as a super-
vised Mincut. Overall, we achieve a 40% reduction
in error rates (from an error rate of about 25% to an
error rate of 15%). To achieve the results of standard
supervised approaches with our model, we need less
than 20% of their training data. In addition, we com-
pare our algorithm to previous state-of-the-art ap-
proaches, showing that our model performs better
on the same datasets.
Future work will explore other graph construc-
tion methods, such as the use of morphological re-
lations as well as thesaurus and distributional sim-
ilarity measures. We will also explore other semi-
supervised algorithms.
8
References
Alekh Agarwal and Pushpak Bhattacharyya. 2005. Sen-
timent Analysis: A new Approach for Effective Use
of Linguistic Knowledge and Exploiting Similarities
in a Set of Documents to be Classified. Proceedings of
ICON?05.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for Fuzzy Sentiment: Sentiment Tag Extrac-
tion from WordNet Glosses. Proceedings of EACL?06.
Avrim Blum and Shuchi Chawla. 2001. Learning from
Labeled and Unlabeled Data using Graph Mincuts.
Proceedings of ICML?01.
Thomas Cormen, Charles Leiserson, Ronald Rivest and
Clifford Stein. 2002. Introduction to Algorithms.
Second Edition, the MIT Press.
Kushal Dave, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
Proceedings of WWW?03.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A Publicly Available Lexical Resource for Opin-
ion Mining. Proceedings of LREC?06.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageRank-
ing WordNet Synsets: An application to Opinion Min-
ing. Proceedings of ACL?07.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. Proceedings of ACL?97.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. Proceedings of
EMNLP?07.
Japp Kamps, Maarten Marx, Robert Mokken, and
Maarten de Rijke. 2004. Using WordNet to Measure
Semantic Orientation of Adjectives. Proceedings of
LREC?04.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
Sentiment of Opinions. Proceedings of COLING?04.
Soo-Min Kim and Eduard Hovy. 2005. Automatic De-
tection of Opinion BearingWords and Sentences. Pro-
ceedings of ICJNLP?05.
Taku Kudo and Yuji Matsumoto. 2004. A Boosting
Algorithm for Classification of Semi-structured Text.
Proceedings of EMNLP?04.
Iadh Ounis, Maarten de Rijke, Craig Macdonald, Gilad
Mishne and Ian Soboroff. 2006. Overview of the
TREC-2006 Blog Track. Proceedings of TREC?06.
Bo Pang and Lillian Lee. 2004. A Sentiment Education:
Sentiment Analysis Using Subjectivity summarization
Based on Minimum Cuts. Proceedings of ACL?04.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification us-
ing Machine Learning Techniques. Proceedings of
EMNLP?02.
Livia Polanyi and Annie Zaenen. 2004. Contextual Va-
lence Shifters. Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text: The-
ories and Applications.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing Product Features and Opinions from Reviews Pro-
ceedings of EMNLP?05.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning Subjective Nouns using Extraction Pattern
Bootstrapping. Proceedings of CoNLL?03
Fangzhong Su and Katja Markert. 2008. From Words
to Senses: A Case Study in Subjectivity Recognition.
Proceedings of COLING?08.
Fangzhong Su and Katja Markert. 2008a. Elicit-
ing Subjectivity and Polarity Judgements on Word
Senses. Proceedings of COLING?08 workshop of Hu-
man Judgements in Computational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting Semantic Orientations of Words us-
ing Spin Model. Proceedings of ACL?05.
Matt Thomas, Bo Pang and Lilian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. Proceedings of
EMNLP?06.
Peter Turney. 2002. Thumbs up or Thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. Proceedings of ACL?02.
Peter Turney and Michael Littman. 2003. Measuring
Praise and Criticism: Inference of Semantic Orienta-
tion from Association. ACM Transaction on Informa-
tion Systems.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
Answering Opinion Questions: Separating Facts from
Opinions and Identifying the Polarity of Opinion Sen-
tences. Proceedings of EMNLP?03.
Janyce Wiebe and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity in
Phrase-Level Sentiment Analysis. Proceedings of
HLT/EMNLP?05.
9
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 36?41,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 08: Metonymy Resolution at SemEval-2007
Katja Markert
School of Computing
University of Leeds, UK
markert@comp.leeds.ac.uk
Malvina Nissim
Dept. of Linguistics and Oriental Studies
University of Bologna, Italy
malvina.nissim@unibo.it
Abstract
We provide an overview of the metonymy
resolution shared task organised within
SemEval-2007. We describe the problem,
the data provided to participants, and the
evaluation measures we used to assess per-
formance. We also give an overview of the
systems that have taken part in the task, and
discuss possible directions for future work.
1 Introduction
Both word sense disambiguation and named entity
recognition have benefited enormously from shared
task evaluations, for example in the Senseval, MUC
and CoNLL frameworks. Similar campaigns have
not been developed for the resolution of figurative
language, such as metaphor, metonymy, idioms and
irony. However, resolution of figurative language is
an important complement to and extension of word
sense disambiguation as it often deals with word
senses that are not listed in the lexicon. For exam-
ple, the meaning of stopover in the sentence He saw
teaching as a stopover on his way to bigger things
is a metaphorical sense of the sense ?stopping place
in a physical journey?, with the literal sense listed
in WordNet 2.0 but the metaphorical one not being
listed.1 The same holds for the metonymic reading
of rattlesnake (for the animal?s meat) in Roast rat-
tlesnake tastes like chicken.2 Again, the meat read-
1This example was taken from the Berkely Master Metaphor
list (Lakoff and Johnson, 1980) .
2From now on, all examples in this paper are taken from the
British National Corpus (BNC) (Burnard, 1995), but Ex. 23.
ing of rattlesnake is not listed in WordNet whereas
the meat reading for chicken is.
As there is no common framework or corpus for
figurative language resolution, previous computa-
tional works (Fass, 1997; Hobbs et al, 1993; Barn-
den et al, 2003, among others) carry out only small-
scale evaluations. In recent years, there has been
growing interest in metaphor and metonymy resolu-
tion that is either corpus-based or evaluated on larger
datasets (Martin, 1994; Nissim and Markert, 2003;
Mason, 2004; Peirsman, 2006; Birke and Sarkaar,
2006; Krishnakamuran and Zhu, 2007). Still, apart
from (Nissim and Markert, 2003; Peirsman, 2006)
who evaluate their work on the same dataset, results
are hardly comparable as they all operate within dif-
ferent frameworks.
This situation motivated us to organise the first
shared task for figurative language, concentrating on
metonymy. In metonymy one expression is used to
refer to the referent of a related one, like the use of
an animal name for its meat. Similarly, in Ex. 1,
Vietnam, the name of a location, refers to an event (a
war) that happened there.
(1) Sex, drugs, and Vietnam have haunted Bill
Clinton?s campaign.
In Ex. 2 and 3, BMW, the name of a company, stands
for its index on the stock market, or a vehicle manu-
factured by BMW, respectively.
(2) BMW slipped 4p to 31p
(3) His BMW went on to race at Le Mans
The importance of resolving metonymies has been
shown for a variety of NLP tasks, such as ma-
36
chine translation (Kamei and Wakao, 1992), ques-
tion answering (Stallard, 1993), anaphora resolution
(Harabagiu, 1998; Markert and Hahn, 2002) and
geographical information retrieval (Leveling and
Hartrumpf, 2006).
Although metonymic readings are, like all figu-
rative readings, potentially open ended and can be
innovative, the regularity of usage for word groups
helps in establishing a common evaluation frame-
work. Many other location names, for instance, can
be used in the same fashion as Vietnam in Ex. 1.
Thus, given a semantic class (e.g. location), one
can specify several regular metonymic patterns (e.g.
place-for-event) that instances of the class are likely
to undergo. In addition to literal readings, regu-
lar metonymic patterns and innovative metonymic
readings, there can also be so-called mixed read-
ings, similar to zeugma, where both a literal and a
metonymic reading are evoked (Nunberg, 1995).
The metonymy task is a lexical sample task for
English, consisting of two subtasks, one concentrat-
ing on the semantic class location, exemplified by
country names, and another one concentrating on or-
ganisation, exemplified by company names. Partici-
pants had to automatically classify preselected coun-
try/company names as having a literal or non-literal
meaning, given a four-sentence context. Addition-
ally, participants could attempt finer-grained inter-
pretations, further specifying readings into prespec-
ified metonymic patterns (such as place-for-event)
and recognising innovative readings.
2 Annotation Categories
We distinguish between literal, metonymic, and
mixed readings for locations and organisations. In
the case of a metonymic reading, we also specify
the actual patterns. The annotation categories were
motivated by prior linguistic research by ourselves
(Markert and Nissim, 2006), and others (Fass, 1997;
Lakoff and Johnson, 1980).
2.1 Locations
Literal readings for locations comprise locative
(Ex. 4) and political entity interpretations (Ex. 5).
(4) coral coast of Papua New Guinea.
(5) Britain?s current account deficit.
Metonymic readings encompass four types:
- place-for-people a place stands for any per-
sons/organisations associated with it. These can be
governments (Ex. 6), affiliated organisations, incl.
sports teams (Ex. 7), or the whole population (Ex. 8).
Often, the referent is underspecified (Ex. 9).
(6) America did once try to ban alcohol.
(7) England lost in the semi-final.
(8) [. . . ] the incarnation was to fulfil the
promise to Israel and to reconcile the world
with God.
(9) The G-24 group expressed readiness to pro-
vide Albania with food aid.
- place-for-event a location name stands for an
event that happened in the location (see Ex. 1).
- place-for-product a place stands for a product
manufactured in the place, as Bordeaux in Ex. 10.
(10) a smooth Bordeaux that was gutsy enough
to cope with our food
- othermet a metonymy that does not fall into any
of the prespecified patterns, as in Ex. 11, where New
Jersey refers to typical local tunes.
(11) The thing about the record is the influ-
ences of the music. The bottom end is very
New York/New Jersey and the top is very
melodic.
When two predicates are involved, triggering a dif-
ferent reading each (Nunberg, 1995), the annotation
category is mixed. In Ex. 12, both a literal and a
place-for-people reading are involved.
(12) they arrived in Nigeria, hitherto a leading
critic of [. . . ]
2.2 Organisations
The literal reading for organisation names describes
references to the organisation in general, where an
organisation is seen as a legal entity, which consists
of organisation members that speak with a collec-
tive voice, and which has a charter, statute or defined
aims. Examples of literal readings include (among
others) descriptions of the structure of an organisa-
tion (see Ex. 13), associations between organisations
(see Ex. 14) or relations between organisations and
products/services they offer (see Ex. 15).
37
(13) NATO countries
(14) Sun acquired that part of Eastman-Kodak
Cos Unix subsidary
(15) Intel?s Indeo video compression hardware
Metonymic readings include six types:
- org-for-members an organisation stands for
its members, such as a spokesperson or official
(Ex. 16), or all its employees, as in Ex. 17.
(16) Last February IBM announced [. . . ]
(17) It?s customary to go to work in black or
white suits. [. . . ] Woolworths wear them
- org-for-event an organisation name is used to re-
fer to an event associated with the organisation (e.g.
a scandal or bankruptcy), as in Ex. 18.
(18) the resignation of Leon Brittan from Trade
and Industry in the aftermath of Westland.
- org-for-product the name of a commercial or-
ganisation can refer to its products, as in Ex. 3.
- org-for-facility organisations can also stand for
the facility that houses the organisation or one of its
branches, as in the following example.
(19) The opening of a McDonald?s is a major
event
- org-for-index an organisation name can be used
for an index that indicates its value (see Ex. 2).
- othermet a metonymy that does not fall into any
of the prespecified patterns, as in Ex. 20, where Bar-
clays Bank stands for an account at the bank.
(20) funds [. . . ] had been paid into Barclays
Bank.
Mixed readings exist for organisations as well.
In Ex. 21, both an org-for-index and an org-for-
members pattern are invoked.
(21) Barclays slipped 4p to 351p after confirm-
ing 3,000 more job losses.
2.3 Class-independent categories
Apart from class-specific metonymic readings, some
patterns seem to apply across classes to all names. In
the SemEval dataset, we annotated two of them.
object-for-name all names can be used as mere
signifiers, instead of referring to an object or set of
objects. In Ex. 22, both Chevrolet and Ford are used
as strings, rather than referring to the companies.
(22) Chevrolet is feminine because of its sound
(it?s a longer word than Ford, has an open
vowel at the end, connotes Frenchness).
object-for-representation a name can refer to a
representation (such as a photo or painting) of the
referent of its literal reading. In Ex. 23, Malta refers
to a drawing of the island when pointing to a map.
(23) This is Malta
3 Data Collection and Annotation
We used the CIA Factbook3 and the Fortune 500
list as sampling frames for country and company
names respectively. All occurrences (including plu-
ral forms) of all names in the sampling frames were
extracted in context from all texts of the BNC, Ver-
sion 1.0. All samples extracted are coded in XML
and contain up to four sentences: the sentence in
which the country/company name occurs, two be-
fore, and one after. If the name occurs at the begin-
ning or end of a text the samples may contain less
than four sentences.
For both the location and the organisation subtask,
two random subsets of the extracted samples were
selected as training and test set, respectively. Before
metonymy annotation, samples that were not under-
stood by the annotators because of insufficient con-
text were removed from the datsets. In addition, a
sample was also removed if the name extracted was
a homonym not in the desired semantic class (for ex-
ample Mr. Greenland when annotating locations).4
For those names that do have the semantic class
location or organisation, metonymy anno-
tation was performed, using the categories described
in Section 2. All training set annotation was carried
out independently by both organisers. Annotation
was highly reliable with a kappa (Carletta, 1996) of
3https://www.cia.gov/cia/publications/
factbook/index.html
4Given that the task is not about standard Named Entity
Recognition, we assume that the general semantic class of the
name is already known.
38
Table 1: Reading distribution for locations
reading train test
literal 737 721
mixed 15 20
othermet 9 11
obj-for-name 0 4
obj-for-representation 0 0
place-for-people 161 141
place-for-event 3 10
place-for-product 0 1
total 925 908
Table 2: Reading distribution for organisations
reading train test
literal 690 520
mixed 59 60
othermet 14 8
obj-for-name 8 6
obj-for-representation 1 0
org-for-members 220 161
org-for-event 2 1
org-for-product 74 67
org-for-facility 15 16
org-for-index 7 3
total 1090 842
.88/.89 for locations/organisations.5 As agreement
was established, annotation of the test set was car-
ried out by the first organiser. All cases which were
not entirely straightforward were then independently
checked by the second organiser. Samples whose
readings could not be agreed on (after a reconcil-
iation phase) were excluded from both training and
test set. The reading distributions of training and test
sets for both subtasks are shown in Tables 1 and 2.
In addition to a simple text format including only
the metonymy annotation, we provided participants
with several linguistic annotations of both training
and testset. This included the original BNC tokeni-
sation and part-of-speech tags as well as manually
annotated dependency relations for each annotated
name (e.g. BMW subj-of-slip for Ex. 2).
4 Submission and Evaluation
Teams were allowed to participate in the location
or organisation task or both. We encouraged super-
vised, semi-supervised or unsupervised approaches.
Systems could be tailored to recognise
metonymies at three different levels of granu-
5The training sets are part of the already available Mascara
corpus for metonymy (Markert and Nissim, 2006). The test sets
were newly created for SemEval.
larity: coarse, medium, or fine, with an increasing
number and specification of target classification
categories, and thus difficulty. At the coarse level,
only a distinction between literal and non-literal was
asked for; medium asked for a distinction between
literal, metonymic and mixed readings; fine needed
a classification into literal readings, mixed readings,
any of the class-dependent and class-independent
metonymic patterns (Section 2) or an innovative
metonymic reading (category othermet).
Systems were evaluated via accuracy (acc) and
coverage (cov), allowing for partial submissions.
acc = # correct predictions# predictions cov =
# predictions
# samples
For each target category c we also measured:
precisionc = # correct assignments of c# assignments of c
recallc = # correct assignments of c# dataset instances of c
fscorec = 2precisioncrecallcprecisionc+recallc
A baseline, consisting of the assignment of the most
frequent category (always literal), was used for each
task and granularity level.
5 Systems and Results
We received five submissions (FUH, GYDER,
up13, UTD-HLT-CG, XRCE-M). All tackled
the location task; three (GYDER, UTD-HLT-CG,
XRCE-M) also participated in the organisation task.
All systems were full submissions (coverage of 1)
and participated at all granularity levels.
5.1 Methods and Features
Out of five teams, four (FUH, GYDER, up13,
UTD-HLT-CG) used supervised machine learning,
including single (FUH,GYDER, up13) as well
as multiple classifiers (UTD-HLT-CG). A range
of learning paradigms was represented (including
instance-based learning, maximum entropy, deci-
sion trees, etc.). One participant (XRCE-M) built a
hybrid system, combining a symbolic, supervised
approach based on deep parsing with an unsuper-
vised distributional approach exploiting lexical in-
formation obtained from large corpora.
Systems up13 and FUH used mostly shallow fea-
tures extracted directly from the training data (in-
cluding parts-of-speech, co-occurrences and collo-
39
cations). The other systems made also use of syn-
tactic/grammatical features (syntactic roles, deter-
mination, morphology etc.). Two of them (GYDER
and UTD-HLT-CG) exploited the manually anno-
tated grammatical roles provided by the organisers.
All systems apart from up13 made use of exter-
nal knowledge resources such as lexical databases
for feature generalisation (WordNet, FrameNet,
VerbNet, Levin verb classes) as well as other cor-
pora (the Mascara corpus for additional training ma-
terial, the BNC, and the Web).
5.2 Performance
Tables 3 and 4 report accuracy for all systems.6 Ta-
ble 5 provides a summary of the results with lowest,
highest, and average accuracy and f-scores for each
subtask and granularity level.7
The task seemed extremely difficult, with 2 of the
5 systems (up13,FUH) participating in the location
task not beating the baseline. These two systems re-
lied mainly on shallow features with limited or no
use of external resources, thus suggesting that these
features might only be of limited use for identify-
ing metonymic shifts. The organisers themselves
have come to similar conclusions in their own ex-
periments (Markert and Nissim, 2002). The sys-
tems using syntactic/grammatical features (GYDER,
UTD-HLT-CG, XRCE-M) could improve over the
baseline whether using manual annotation or pars-
ing. These systems also made heavy use of feature
generalisation. Classification granularity had only a
small effect on system performance.
Only few of the fine-grained categories could be
distinguished with reasonable success (see the f-
scores in Table 5). These include literal readings,
and place-for-people, org-for-members, and org-for-
product metonymies, which are the most frequent
categories (see Tables 1 and 2). Rarer metonymic
targets were either not assigned by the systems
at all (?undef? in Table 5) or assigned wrongly
6Due to space limitations we do not report precision, recall,
and f-score per class and refer the reader to each system de-
scription provided within this volume.
7The value ?undef? is used for cases where the system did
not attempt any assignment for a given class, whereas the value
?0? signals that assignments were done, but were not correct.
8Please note that results for the FUH system are slightly dif-
ferent than those presented in the FUH system description pa-
per. This is due to a preprocessing problem in the FUH system
that was fixed only after the run submission deadline.
Table 5: Overview of scores
base min max ave
LOCATION-coarse
accuracy 0.794 0.754 0.852 0.815
literal-f 0.849 0.912 0.888
non-literal-f 0.344 0.576 0.472
LOCATION-medium
accuracy 0.794 0.750 0.848 0.812
literal-f 0.849 0.912 0.889
metonymic-f 0.331 0.580 0.476
mixed-f 0.000 0.083 0.017
LOCATION-fine
accuracy 0.794 0.741 0.844 0.801
literal-f 0.849 0.912 0.887
place-for-people-f 0.308 0.589 0.456
place-for-event-f 0.000 0.167 0.033
place-for-product-f 0.000 undef 0.000
obj-for-name-f 0.000 0.667 0.133
obj-for-rep-f undef undef undef
othermet-f 0.000 undef 0.000
mixed-f 0.000 0.083 0.017
ORGANISATION-coarse
accuracy 0.618 0.732 0.767 0.746
literal-f 0.800 0.825 0.810
non-literal-f 0.572 0.652 0.615
ORGANISATION-medium
accuracy 0.618 0.711 0.733 0.718
literal-f 0.804 0.825 0.814
metonymic-f 0.553 0.604 0.577
mixed-f 0.000 0.308 0.163
ORGANISATION-fine
accuracy 0.618 0.700 0.728 0.713
literal-f 0.808 0.826 0.817
org-for-members-f 0.568 0.630 0.608
org-for-event-f 0.000 undef 0.000
org-for-product-f 0.400 0.500 0.458
org-for-facility-f 0.000 0.222 0.141
org-for-index-f 0.000 undef 0.000
obj-for-name-f 0.250 0.800 0.592
obj-for-rep-f undef undef undef
othermet-f 0.000 undef 0.000
mixed-f 0.000 0.343 0.135
(low f-scores). An exception is the object-for-
name pattern, which XRCE-M and UTD-HLT-CG
could distinguish with good success. Mixed read-
ings also proved problematic since more than one
pattern is involved, thus limiting the possibilities
of learning from a single training instance. Only
GYDER succeeded in correctly identifiying a variety
of mixed readings in the organisation subtask. No
systems could identify unconventional metonymies
correctly. Such poor performance is due to the non-
regularity of the reading by definition, so that ap-
proaches based on learning from similar examples
alone cannot work too well.
40
Table 3: Accuracy scores for all systems for all the location tasks.8
task ? / system ? baseline FUH UTD-HLT-CG XRCE-M GYDER up13
LOCATION-coarse 0.794 0.778 0.841 0.851 0.852 0.754
LOCATION-medium 0.794 0.772 0.840 0.848 0.848 0.750
LOCATION-fine 0.794 0.759 0.822 0.841 0.844 0.741
Table 4: Accuracy scores for all systems for all the organisation tasks
task ? / system ? baseline UTD-HLT-CG XRCE-M GYDER
ORGANISATION-coarse 0.618 0.739 0.732 0.767
ORGANISATION-medium 0.618 0.711 0.711 0.733
ORGANISATION-fine 0.618 0.711 0.700 0.728
6 Concluding Remarks
There is a wide range of opportunities for future fig-
urative language resolution tasks. In the SemEval
corpus the reading distribution mirrored the actual
distribution in the original corpus (BNC). Although
realistic, this led to little training data for several
phenomena. A future option, geared entirely to-
wards system improvement, would be to use a strat-
ified corpus, built with different acquisition strate-
gies like active learning or specialised search proce-
dures. There are also several options for expand-
ing the scope of the task, for example to a wider
range of semantic classes, from proper names to
common nouns, and from lexical samples to an all-
words task. In addition, our task currently covers
only metonymies and could be extended to other
kinds of figurative language.
Acknowledgements
We are very grateful to the BNC Consortium for let-
ting us use and distribute samples from the British
National Corpus, version 1.0.
References
J.A. Barnden, S.R. Glasbey, M.G. Lee, and A.M. Walling-
ton. 2003. Domain-transcending mappings in a system for
metaphorical reasoning. In Proc. of EACL-2003, 57-61.
J. Birke and A Sarkaar. 2006. A clustering approach for the
nearly unsupervised recognition of nonliteral language. In
Proc. of EACL-2006.
L. Burnard, 1995. Users? Reference Guide, British National
Corpus. BNC Consortium, Oxford, England.
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22:249-254.
D. Fass. 1997. Processing Metaphor and Metonymy. Ablex,
Stanford, CA.
S. Harabagiu. 1998. Deriving metonymic coercions from
WordNet. In Workshop on the Usage of WordNet in Natural
Language Processing Systems, COLING-ACL ?98, 142-148,
Montreal, Canada.
J.R. Hobbs, M.E. Stickel, D.E. Appelt, and P. Martin. 1993.
Interpretation as abduction. Artificial Intelligence, 63:69-
142.
S. Kamei and T. Wakao. 1992. Metonymy: Reassessment, sur-
vey of acceptability and its treatment in machine translation
systems. In Proc. of ACL-92, 309-311.
S. Krishnakamuran and X. Zhu. 2007. Hunting elusive
metaphors using lexical resources. In NAACL 2007 Work-
shop on Computational Approaches to Figurative Language.
G. Lakoff and M. Johnson. 1980. Metaphors We Live By.
Chicago University Press, Chicago, Ill.
J. Leveling and S. Hartrumpf. 2006. On metonymy recogni-
tion for gir. In Proceedings of GIR-2006: 3rd Workshop on
Geographical Information Retrieval.
K. Markert and U. Hahn. 2002. Understanding metonymies in
discourse. Artificial Intelligence, 135(1/2):145?198.
K. Markert and M. Nissim. 2002. Metonymy resolution as a
classification task. In Proc. of EMNLP-2002, 204-213.
K. Markert and M. Nissim. 2006. Metonymic proper names: A
corpus-based account. In A. Stefanowitsch, editor, Corpora
in Cognitive Linguistics. Vol. 1: Metaphor and Metonymy.
Mouton de Gruyter, 2006.
J. Martin. 1994. Metabank: a knowledge base of
metaphoric language conventions. Computational Intelli-
gence, 10(2):134-149.
Z. Mason. 2004. Cormet: A computational corpus-based con-
ventional metaphor extraction system. Computational Lin-
guistics, 30(1):23-44.
M. Nissim and K. Markert. 2003. Syntactic features and word
similarity for supervised metonymy resolution. In Proc. of
ACL-2003, 56-63.
G. Nunberg. 1995. Transfers of meaning. Journal of Seman-
tics, 12:109-132.
Y Peirsman. 2006. Example-based metonymy recognition for
proper nouns. In Student Session of EACL 2006.
D. Stallard. 1993. Two kinds of metonymy. In Proc. of ACL-
93, 87-94.
41
Using the Web in Machine Learning for Other-Anaphora Resolution
Natalia N. Modjeska
School of Informatics
University of Edinburgh and
Department of Computer Science
University of Toronto
natalia@cs.utoronto.ca
Katja Markert
School of Computing
University of Leeds and
School of Informatics
University of Edinburgh
markert@inf.ed.ac.uk
Malvina Nissim
School of Informatics
University of Edinburgh
mnissim@inf.ed.ac.uk
Abstract
We present a machine learning frame-
work for resolving other-anaphora. Be-
sides morpho-syntactic, recency, and se-
mantic features based on existing lexi-
cal knowledge resources, our algorithm
obtains additional semantic knowledge
from the Web. We search the Web via
lexico-syntactic patterns that are specific
to other-anaphors. Incorporating this in-
novative feature leads to an 11.4 percent-
age point improvement in the classifier?s
F -measure (25% improvement relative to
results without this feature).
1 Introduction
Other-anaphors are referential NPs with the mod-
ifiers ?other? or ?another? and non-structural an-
tecedents:1
(1) An exhibition of American design and architec-
ture opened in September in Moscow and will
travel to eight other Soviet cities.
(2) [. . . ] the alumni director of a Big Ten university
?I?d love to see sports cut back and so would a
lot of my counterparts at other schools, [. . . ]?
(3) You either believe Seymour can do it again or
you don?t. Beside the designer?s age, other
risk factors for Mr. Cray?s company include
the Cray-3?s [. . . ] chip technology.
1All examples are from the Wall Street Journal; the correct
antecedents are in italics and the anaphors are in bold font.
In (1), ?eight other Soviet cities? refers to a set of So-
viet cities excluding Moscow, and can be rephrased
as ?eight Soviet cities other than Moscow?. In (2),
?other schools? refers to a set of schools excluding
the mentioned Big Ten university. In (3), ?other risk
factors for Mr. Cray?s company? refers to a set of
risk factors excluding the designer?s age.
In contrast, in list-contexts such as (4), the an-
tecedent is available both anaphorically and struc-
turally, as the left conjunct of the anaphor.2
(4) Research shows AZT can relieve dementia and
other symptoms in children [. . . ]
We focus on cases such as (1?3).
Section 2 describes a corpus of other-anaphors.
We present a machine learning approach to other-
anaphora, using a Naive Bayes (NB) classifier (Sec-
tion 3) with two different feature sets. In Section 4
we present the first feature set (F1) that includes
standard morpho-syntactic, recency, and string com-
parison features. However, there is evidence that,
e.g., syntactic features play a smaller role in resolv-
ing anaphors with full lexical heads than in pronom-
inal anaphora (Strube, 2002; Modjeska, 2002). In-
stead, a large and diverse amount of lexical or
world knowledge is necessary to understand exam-
ples such as (1?3), e.g., that Moscow is a (Soviet)
city, that universities are informally called schools
in American English and that age can be viewed as
a risk factor. Therefore we add lexical knowledge,
which is extracted from WordNet (Fellbaum, 1998)
and from a Named Entity (NE) Recognition algo-
rithm, to F1.
2Antecedents are also available structurally in constructions
?other than?, e.g., ?few clients other than the state?. For a com-
putational treatment of ?other? with structural antecedents see
(Bierner, 2001).
The algorithm?s performance with this feature set
is encouraging. However, the semantic knowledge
the algorithm relies on is not sufficient for many
cases of other-anaphors (Section 4.2). Many expres-
sions, word senses and lexical relations are miss-
ing from WordNet. Whereas it includes Moscow
as a hyponym of city, so that the relation between
anaphor and antecedent in (1) can be retrieved, it
does not include the sense of school as university,
nor does it allow to infer that age is a risk factor.
There have been efforts to extract missing lexical
relations from corpora in order to build new knowl-
edge sources and enrich existing ones (Hearst, 1992;
Berland and Charniak, 1999; Poesio et al, 2002).3
However, the size of the used corpora still leads
to data sparseness (Berland and Charniak, 1999)
and the extraction procedure can therefore require
extensive smoothing. Moreover, some relations
should probably not be encoded in fixed context-
independent ontologies at all. Should, e.g., under-
specified and point-of-view dependent hyponymy
relations (Hearst, 1992) be included? Should age,
for example, be classified as a hyponym of risk fac-
tor independent of context?
Building on our previous work in (Markert et al,
2003), we instead claim that the Web can be used
as a huge additional source of domain- and context-
independent, rich and up-to-date knowledge, with-
out having to build a fixed lexical knowledge base
(Section 5). We describe the benefit of integrating
Web frequency counts obtained for lexico-syntactic
patterns specific to other-anaphora as an additional
feature into our NB algorithm. This feature raises
the algorithm?s F -measure from 45.5% to 56.9%.
2 Data Collection and Preparation
We collected 500 other-anaphors with NP an-
tecedents from the Wall Street Journal corpus (Penn
Treebank, release 2). This data sample excludes
several types of expressions containing ?other?: (a)
list-contexts (Ex. 4) and other-than contexts (foot-
note 2), in which the antecedents are available struc-
turally and thus a relatively unsophisticated proce-
dure would suffice to find them; (b) idiomatic and
discourse connective ?other?, e.g., ?on the other
3In parallel, efforts have been made to enrich WordNet by
adding information in glosses (Harabagiu et al, 1999).
hand?, which are not anaphoric; and (c) reciprocal
?each other? and ?one another?, elliptic phrases e.g.
?one X . . . the other(s)? and one-anaphora, e.g., ?the
other/another one?, which behave like pronouns and
thus would require a different search method. Also
excluded from the data set are samples of other-
anaphors with non-NP antecedents (e.g., adjectival
and nominal pre- and postmodifiers and clauses).
Each anaphor was extracted in a 5-sentence con-
text. The correct antecedents were manually an-
notated to create a training/test corpus. For each
anaphor, we automatically extracted a set of po-
tential NP antecedents as follows. First, we ex-
tracted all base NPs, i.e., NPs that contain no further
NPs within them. NPs containing a possessive NP
modifier, e.g., ?Spain?s economy?, were split into a
possessor phrase, ?Spain?, and a possessed entity,
?economy?. We then filtered out null elements and
lemmatised all antecedents and anaphors.
3 The Algorithm
We use a Naive Bayes classifier, specifically the im-
plementation in the Weka ML library.4
The training data was generated following the
procedure employed by Soon et al (2001) for
coreference resolution. Every pair of an anaphor
and its closest preceding antecedent created a pos-
itive training instance. To generate negative train-
ing instances, we paired anaphors with each of the
NPs that intervene between the anaphor and its an-
tecedent. This procedure produced a set of 3,084
antecedent-anaphor pairs, of which 500 (16%) were
positive training instances.
The classifier was trained and tested using 10-fold
cross-validation. We follow the general practice of
ML algorithms for coreference resolution and com-
pute precision (P), recall (R), and F-measure (F ) on
all possible anaphor-antecedent pairs.
As a first approximation of the difficulty of our
task, we developed a simple rule-based baseline al-
gorithm which takes into account the fact that the
lemmatised head of an other-anaphor is sometimes
the same as that of its antecedent, as in (5).
4http://www.cs.waikato.ac.nz/ml/weka/.
We also experimented with a decision tree classifier, with
Neural Networks and Support Vector Machines with Sequential
Minimal Optimization (SMO), all available from Weka. These
classifiers achieved worse results than NB on our data set.
Table 1: Feature set F1
Type Feature Description Values
Gramm NP FORM Surface form (for all NPs) definite, indefinite, demonstrative, pronoun,
proper name, unknown
Match RESTR SUBSTR Does lemmatized antecedent string contain lemma-
tized anaphor string?
yes, no
Syntactic GRAM FUNC Grammatical role (for all NPs) subject, predicative NP, dative object, direct
object, oblique, unknown
Syntactic SYN PAR Anaphor-antecedent agreement with respect to
grammatical function
yes, no
Positional SDIST Distance between antecedent and anaphor in sen-
tences
1, 2, 3, 4, 5
Semantic SEMCLASS Semantic class (for all NPs) person, organization, location, date, money,
number, thing, abstract, unknown
Semantic SEMCLASS AGR Anaphor-antecedent agreement with respect to se-
mantic class
yes, no, unknown
Semantic GENDER AGR Anaphor-antecedent agreement with respect to gen-
der
same, compatible, incompatible, unknown
Semantic RELATION Type of relation between anaphor and antecedent same-predicate, hypernymy, meronymy,
compatible, incompatible, unknown
(5) These three countries aren?t completely off the
hook, though. They will remain on a lower-
priority list that includes other countries [. . . ]
For each anaphor, the baseline string-compares its
last (lemmatised) word with the last (lemmatised)
word of each of its possible antecedents. If the
words match, the corresponding antecedent is cho-
sen as the correct one. If several antecedents pro-
duce a match, the baseline chooses the most re-
cent one among them. If string-comparison returns
no antecedent, the baseline chooses the antecedent
closest to the anaphor among all antecedents. The
baseline assigns ?yes? to exactly one antecedent per
anaphor. Its P, R and F -measure are 27.8%.
4 Naive Bayes without the Web
First, we trained and tested the NB classifier with
a set of 9 features motivated by our own work on
other-anaphora (Modjeska, 2002) and previous ML
research on coreference resolution (Aone and Ben-
nett, 1995; McCarthy and Lehnert, 1995; Soon et
al., 2001; Ng and Cardie, 2002; Strube et al, 2002).
4.1 Features
A set of 9 features, F1, was automatically acquired
from the corpus and from additional external re-
sources (see summary in Table 1).
Non-semantic features. NP FORM is based on the
POS tags in the Wall Street Journal corpus and
heuristics. RESTR SUBSTR matches lemmatised
strings and checks whether the antecedent string
contains the anaphor string. This allows to resolve
examples such as ?one woman ringer . . . another
woman?. The values for GRAM FUNC were approxi-
mated from the parse trees and Penn Treebank anno-
tation. The feature SYN PAR captures syntactic par-
allelism between anaphor and antecedent. The fea-
ture SDIST measures the distance between anaphor
and antecedent in terms of sentences.5
Semantic features. GENDER AGR captures agree-
ment in gender between anaphor and antecedent,
gender having been determined using gazetteers,
kinship and occupational terms, titles, and Word-
Net. Four values are possible: ?same?, if both NPs
have same gender; ?compatible?, if antecedent and
anaphor have compatible gender, e.g., ?lawyer . . .
other women?; ?incompatible?, e.g., ?Mr. Johnson
. . . other women?; and ?unknown?, if one of the
NPs is undifferentiated, i.e., the gender value is ?un-
known?. SEMCLASS: Proper names were classified
using ANNIE, part of the GATE2 software package
(http://gate.ac.uk). Common nouns were
looked up in WordNet, considering only the most
frequent sense of each noun (the first sense in Word-
Net). In each case, the output was mapped onto one
of the values in Table 1. The SEMCLASS AGR fea-
5We also experimented with a feature MDIST that measures
intervening NP units. This feature worsened the overall perfor-
mance of the classifier.
ture compares the semantic class of the antecedent
with that of the anaphor NP and returns ?yes? if
they belong to the same class; ?no?, if they belong
to different classes; and ?unknown? if the seman-
tic class of either the anaphor or antecedent has not
been determined. The RELATION between other-
anaphors and their antecedents can partially be de-
termined by string comparison (?same-predicate?)6
or WordNet (?hypernymy? and ?meronymy?). As
other relations, e.g. ?redescription? (Ex. (3), cannot
be readily determined on the basis of the information
in WordNet, the following values were used: ?com-
patible?, for NPs with compatible semantic classes,
e.g., ?woman . . . other leaders?; and ?incompati-
ble?, e.g., ?woman . . . other economic indicators?.
Compatibility can be defined along a variety of pa-
rameters. The notion we used roughly corresponds
to the root level of the WordNet hierarchy. Two
nouns are compatible if they have the same SEM-
CLASS value, e.g., ?person?. ?Unknown? was used
if the type of relation could not be determined.
4.2 Results
Table 2 shows the results for the Naive Bayes clas-
sifier using F1 in comparison to the baseline.
Table 2: Results with F1
Features P R F
baseline 27.8 27.8 27.8
F1 51.7 40.6 45.5
Our algorithm performs significantly better than the
baseline.7 While these results are encouraging, there
were several classification errors.
Word sense ambiguity is one of the reasons for
misclassifications. Antecedents were looked up in
WordNet for their most frequent sense for a context-
independent assignment of the values of semantic
class and relations. However, in many cases either
the anaphor or antecedent or both are used in a sense
that is ranked as less frequent in Wordnet. This
might even be a quite frequent sense for a specific
corpus, e.g., the word ?issue? in the sense of ?shares,
stocks? in the WSJ. Therefore there is a strong inter-
6Same-predicate is not really a relation. We use it when the
head noun of the anaphor and antecedent are the same.
7We used a t-test with confidence level 0.05 for all signifi-
cance tests.
action between word sense disambiguation and ref-
erence resolution (see also (Preiss, 2002)).
Named Entity resolution is another weak link.
Several correct NE antecedents were classified as
?antecedent=no? (false negatives) because the NER
module assigned the wrong class to them.
The largest class of errors is however due to insuf-
ficient semantic knowledge. Problem examples can
roughly be classified into five partially overlapping
groups: (a) examples that suffer from gaps in Word-
Net, e.g., (2); (b) examples that require domain-,
situation-specific, or general world knowledge, e.g.,
(3); (c) examples involving bridging phenomena
(sometimes triggered by a metonymic or metaphoric
antecedent or anaphor), e.g., (6); (d) redescriptions
and paraphrases, often involving semantically vague
anaphors and/or antecedents, e.g., (7) and (3); and
(e) examples with ellipsis, e.g., (8).
(6) The Justice Department?s view is shared by
other lawyers [. . . ]
(7) While Mr. Dallara and Japanese officials say
the question of investors? access to the U.S.
and Japanese markets may get a disproportion-
ate share of the public?s attention, a number of
other important economic issues will be on
the table at next week?s talks.
(8) He sees flashy sports as the only way the last-
place network can cut through the clutter of ca-
ble and VCRs, grab millions of new viewers
and tell them about other shows premiering a
few weeks later.
In (6), the antecedent is an organization-for-people
metonymy. In (7), the question of investors? access
to the U.S. and Japanese markets is characterized as
an important economic issue. Also, the head ?is-
sues? is lexically uninformative to sufficiently con-
strain the search space for the antecedent. In (8), the
antecedent is not the flashy sports, but rather flashy
sport shows, and thus an important piece of infor-
mation is omitted. Alternatively, the antecedent is a
content-for-container metonymy.
Overall, our approach misclassifies antecedents
whose relation to the other-anaphor is based on sim-
ilarity, property-sharing, causality, or is constrained
to a specific domain. These relation types are not ?
and perhaps should not be ? encoded in WordNet.
5 Naive Bayes with the Web
With its approximately 3033M pages8 the Web is
the largest corpus available to the NLP community.
Building on our approach in (Markert et al, 2003),
we suggest using the Web as a knowledge source
for anaphora resolution. In this paper, we show how
to integrate Web counts for lexico-syntactic patterns
specific to other-anaphora into our ML approach.
5.1 Basic Idea
In the examples we consider, the relation between
anaphor and antecedent is implicitly expressed, i.e.,
anaphor and antecedent do not stand in a structural
relationship. However, they are linked by a strong
semantic relation that is likely to be structurally ex-
plicitly expressed in other texts. We exploit this in-
sight by adopting the following procedure:
1. In other-anaphora, a hyponymy/similarity rela-
tion between the lexical heads of anaphor and
antecedent is exploited or stipulated by the con-
text,9 e.g. that ?schools? is an alternative term
for universities in Ex. (2) or that age is viewed
as a risk factor in Ex. (3).
2. We select patterns that structurally explicitly
express the same lexical relations. E.g., the list-
context NP
1
and other NP
2
(as Ex. (4))
usually expresses hyponymy/similarity rela-
tions between the hyponym NP
1
and its hyper-
nym NP
2
(Hearst, 1992).
3. If the implicit lexical relationship between
anaphor and antecedent is strong, it is likely
that anaphor and antecedent also frequently
cooccur in the selected explicit patterns. We
instantiate the explicit pattern for all anaphor-
antecedent pairs. In (2) the pattern NP
1
and other NP
2
is instantiated with e.g.,
counterparts and other schools, sports
and other schools and universities and
other schools.10 These instantiations can be
8http://www.searchengineshowdown.com/
stats/sizeest.shtml, data from March 2003.
9In the Web feature context, we will often use
?anaphor/antecedent? instead of the more cumbersome
?lexical heads of the anaphor/antecedent?.
10These simplified instantiations serve as an example and are
neither exhaustive nor the final instantiations we use; see Sec-
tion 5.3.
searched in any corpus to determine their fre-
quencies. The rationale is that the most fre-
quent of these instantiated patterns is a good
clue for the correct antecedent.
4. As the patterns can be quite elaborate, most
corpora will be too small to determine the cor-
responding frequencies reliably. The instantia-
tion universities and other schools, e.g.,
does not occur at all in the British National Cor-
pus (BNC), a 100M words corpus of British
English.11 Therefore we use the largest corpus
available, the Web. We submit all instantiated
patterns as queries making use of the Google
API technology. Here, universities and
other schools yields over 700 hits, whereas
the other two instantiations yield under 10 hits
each. High frequencies do not only occur
for synonyms; the corresponding instantiation
for the correct antecedent in Ex. (3) age and
other risk factors yields over 400 hits on
the Web and again none in the BNC.
5.2 Antecedent Preparation
In addition to the antecedent preparation described
in Section 2, further processing is necessary. First,
pronouns can be antecedents of other-anaphors but
they were not used as Web query input as they are
lexically empty. Second, all modification was elim-
inated and only the rightmost noun of compounds
was kept, to avoid data sparseness. Third, using pat-
terns containing NEs such as ?Will Quinlan? in (9)
also leads to data sparseness (see also the use of NE
recognition for feature SEMCLASS).
(9) [. . . ] Will Quinlan had not inherited a damaged
retinoblastoma supressor gene and, therefore,
faced no more risk than other children [. . . ]
We resolved NEs in two steps. In addition
to GATE?s classification into ENAMEX and NU-
MEX categories, we used heuristics to automati-
cally obtain more fine-grained distinctions for the
categories LOCATION, ORGANIZATION, DATE and
MONEY, whenever possible. No further distinc-
tions were made for the category PERSON. We
classified LOCATIONS into COUNTRY, (US) STATE,
CITY, RIVER, LAKE and OCEAN, using mainly
11http://info.ox.ac.uk/bnc
Table 3: Patterns and Instantiations for other-anaphora
ANTECEDENT PATTERN INSTANTIATIONS
common noun (O1): (N
1
fsgg OR N
1
fplg) and other N
2
fplg Ic
1
: ?(university OR universities) and other schools?
proper name (O1): (N
1
fsgg OR N
1
fplg) and other N
2
fplg Ip
1
: ?(person OR persons) and other children?
Ip
2
: ?(child OR children) and other persons?
(O2): N
1
and other N
2
fplg Ip
3
: ?Will Quinlan and other children?
gazetteers.12 If an entity classified by GATE as
ORGANIZATION contained an indication of the or-
ganization type, we used this as a subclassifica-
tion; therefore ?Bank of America? is classified as
BANK. For DATE and MONEY entities we used
simple heuristics to classify them further into DAY,
MONTH, YEAR as well as DOLLAR.
From now on we call A the list of possible an-
tecedents and ana the anaphor. For (2), this list
is A
2
=fcounterpart, sport, universityg (the pronoun
?I? has been discarded) and ana
2
=school. For (9),
they are A
9
=frisk, gene, person [=Will Quinlan]g
and ana
9
=child.
5.3 Queries and Scoring Method
We use the list-context pattern:13
(O1) (N
1
fsgg OR N
1
fplg) and other N
2
fplg
For common noun antecedents, we instantiate the
pattern by substituting N
1
with each possible an-
tecedent from set A, and N
2
with ana, as normally
N
1
is a hyponym of N
2
in (O1), and the antecedent
is a hyponym of the anaphor. An instantiated pat-
tern for Ex. (2) is (university OR universities)
and other schools (Ic
1
in Table 3).14
For NE antecedents we instantiate (O1) by substi-
tuting N
1
with the NE category of the antecedent,
and N
2
with ana. An instantiated pattern for
Example (9) is (person OR persons) and other
children (Ip
1
in Table 3). In this instantiation, N
1
(?person?) is not a hyponym of N
2
(?child?), instead
N
2
is a hyponym of N
1
. This is a consequence of
the substitution of the antecedent (?Will Quinlan?)
12They were extracted from the Web. Small gazetteers, con-
taining in all about 500 entries, are sufficient. This is the only
external knowledge collected for the Web feature.
13In all patterns in this paper, ?OR? is the boolean operator,
?N
1
? and ?N
2
? are variables, all other words are constants.
14Common noun instantiations are marked by a superscript
?c? and proper name instantiations by a superscript ?p?.
with its NE category (?person?); such an instanti-
ation is not frequent, since it violates standard re-
lations within (O1). Therefore, we also instantiate
(O1) by substituting N
1
with ana, and N
2
with the
NE type of the antecedent (Ip
2
in Table 3). Finally,
for NE antecedents, we use an additional pattern:
(O2) N
1
and other N
2
fplg
which we instantiate by substituting N
1
with the
original NE antecedent and N
2
with ana (Ip
3
in Ta-
ble 3).
Patterns and instantiations are summarised in Ta-
ble 3. We submit these instantiations as queries to
the Google search engine.
For each antecedent ant in A we obtain the raw
frequencies of all instantiations it occurs in (Ic
1
for
common nouns, or I
p
1
, I
p
2
, I
p
3
for proper names) from
the Web, yielding freq(Ic
1
), or freq(I
p
1
), freq(I
p
2
)
and freq(Ip
3
). We compute the maximum M
ant
over these frequencies for proper names. For com-
mon nouns M
ant
corresponds to freq(Ic
1
). The in-
stantiation yielding M
ant
is then called Imax
ant
.
Our scoring method takes into account the indi-
vidual frequencies of ant and ana by adapting mu-
tual information. We call the first part of Imax
ant
(e.g. ?university OR universities?, or ?child OR chil-
dren?) X
ant
, and the second part (e.g. ?schools?
or ?persons?) Y
ant
. We compute the probability of
Imax
ant
, X
ant
and Y
ant
, using Google to determine
freq(X
ant
) and freq(Y
ant
).
Pr(Imax
ant
) =
M
ant
number of GOOGLE pages
Pr(X
ant
) =
freq(X
ant
)
number of GOOGLE pages
Pr(Y
ant
) =
freq(Y
ant
)
number of GOOGLE pages
We then compute the final score MI
ant
.
MI
ant
= log
Pr(Imax
ant
)
Pr(X
ant
)Pr(Y
ant
)
5.4 Integration into ML Framework and
Results
For each anaphor, the antecedent in A with the
highest MI
ant
gets feature value ?webfirst?.15 All
other antecedents (including pronouns) get the fea-
ture value ?webrest?. We chose this method instead
of e.g., giving score intervals for two reasons. First,
since score intervals are unique for each anaphor,
it is not straightforward to incorporate them into a
ML framework in a consistent manner. Second, this
method introduces an element of competition be-
tween several antecedents (see also (Connolly et al,
1997)), which the individual scores do not reflect.
We trained and tested the NB classifier with the
feature set F1, plus the Web feature. The last row
in Table 4 shows the results. We obtained a 9.1 per-
centage point improvement in precision (an 18% im-
provement relative to the F1 feature set) and a 12.8
percentage point improvement in recall (32% im-
provement relative to F1), which amounts to an 11.4
percentage point improvement in F -measure (25%
improvement relative to F1 feature set). In particu-
lar, all the examples in this paper were resolved.
Our algorithm still misclassified several an-
tecedents. Sometimes even the Web is not large
enough to contain the instantiated pattern, espe-
cially when this is situation or speaker specific. An-
other problem is the high number of NE antecedents
(39.6%) in our corpus. While our NER module is
quite good, any errors in NE classification lead to
incorrect instantiations and thus to incorrect classi-
fications. In addition, the Web feature does not yet
take into account pronouns (7.43% of all correct and
potential antecedents in our corpus).
6 Related Work and Discussion
Modjeska (2002) presented two hand-crafted algo-
rithms, SAL and LEX, which resolve the anaphoric
references of other-NPs on the basis of grammati-
cal salience and lexical information from WordNet,
respectively. In our own previous work (Markert et
15If several antecedents have the highest MI
ant
they all get
value ?webfirst?.
Table 4: Results with F1 and F1+Web
Features P R F
baseline 27.8 27.8 27.8
F1 51.7 40.6 45.5
F1+Web 60.8 53.4 56.9
al., 2003) we presented a preliminary symbolic ap-
proach that uses Web counts and a recency-based
tie-breaker for resolution of other-anaphora and
bridging descriptions. (For another Web-based sym-
bolic approach to bridging see (Bunescu, 2003).)
The approach described in this paper is the first ma-
chine learning approach to other-anaphora. It is
not directly comparable to the symbolic approaches
above for two reasons. First, the approaches dif-
fer in the data and the evaluation metrics they used.
Second, our algorithm does not yet constitute a
full resolution procedure. As the classifier oper-
ates on the whole set of antecedent-anaphor pairs,
more than one potential antecedent for each anaphor
can be classified as ?antecedent=yes?. This can
be amended by e.g. incremental processing. Also,
the classifier does not know that each other-NP is
anaphoric and therefore has an antecedent. (This
contrasts with e.g. definite NPs.) Thus, it can clas-
sify all antecedents as ?antecedent=no?. This can be
remedied by using a back-off procedure, or a compe-
tition learning approach (Connolly et al, 1997). Fi-
nally, the full resolution procedure will have to take
into account other factors, e.g., syntactic constraints
on antecedent realization.
Our approach is the first ML approach to any kind
of anaphora that integrates the Web. Using the Web
as a knowledge source has considerable advantages.
First, the size of the Web almost eliminates the prob-
lem of data sparseness for our task. For this rea-
son, using the Web has proved successful in sev-
eral other fields of NLP, e.g., machine translation
(Grefenstette, 1999) and bigram frequency estima-
tion (Keller et al, 2002). In particular, (Keller et al,
2002) have shown that using the Web handles data
sparseness better than smoothing. Second, we do
not process the returned Web pages in any way (tag-
ging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poe-
sio et al, 2002). Third, the linguistically motivated
patterns we use reduce long-distance dependencies
between anaphor and antecedent to local dependen-
cies. By looking up these patterns on the Web we
obtain semantic information that is not and perhaps
should not be encoded in an ontology (redescrip-
tions, vague relations, etc.). Finally, these local de-
pendencies also reduce the need for prior word sense
disambiguation, as the anaphor and the antecedent
constrain each other?s sense within the context of the
pattern.
7 Conclusions
We presented a machine learning approach to other-
anaphora, which uses a NB classifier and two sets
of features. The first set consists of standard
morpho-syntactic, recency, and semantic features
based on WordNet. The second set alo incorpo-
rates semantic knowledge obtained from the Web via
lexico-semantic patterns specific to other-anaphora.
Adding this knowledge resulted in a dramatic im-
provement of 11.4% points in the classifier?s F -
measure, yielding a final F -measure of 56.9%.
To our knowledge, we are the first to integrate a
Web feature into a ML framework for anaphora reso-
lution. Adding this feature is inexpensive, solves the
data sparseness problem, and allows to handle ex-
amples with non-standard relations between anaphor
and antecedent. The approach is easily applicable to
other anaphoric phenomena by developing appropri-
ate lexico-syntactic patterns (Markert et al, 2003).
Acknowledgments
Natalia N.Modjeska is supported by EPSRC grant
GR/M75129; Katja Markert by an Emmy Noether
Fellowship of the Deutsche Forschungsgemen-
schaft. We thank three anonymous reviewers for
helpful comments and suggestions.
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proc. of ACL?95, pages 122?129.
M. Berland and E. Charniak. 1999. Finding parts in very
large corpora. In Proc. of ACL?99, pages 57?64.
G. Bierner. 2001. Alternative phrases and natural lan-
guage information retrieval. In Proc. of ACL?01.
R. Bunescu. 2003. Associative anaphora resolution: A
Web-based approach. In R. Dale, K. van Deemter, and
R. Mitkov, editors, Proc. of the EACL Workshop on the
Computational Treatment of Anaphora.
D. Connolly, J. D. Burger, and D. S. Day. 1997. A
machine learning approach to anaphoric reference. In
Daniel Jones and Harold Somers, editors, New Meth-
ods in Language Processing, pages 133?144. UCL
Press, London.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. The MIT Press.
G. Grefenstette. 1999. The WWW as a resource for
example-based MT tasks. In Proc. of ASLIB?99 Trans-
lating and the Computer 21, London.
S. Harabagiu, G. Miller, and D. Moldovan. 1999. Word-
net 2 - a morphologically and semantically enhanced
resource. In Proc. of SIGLEX-99, pages 1?8.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of COLING-92.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using the
Web to overcome data sparseness. In Proc. of EMNLP
2002, pages 230?237.
K. Markert, M. Nissim, and N. N. Modjeska. 2003.
Using the Web for nominal anaphora resolution. In
R. Dale, K. van Deemter, and R. Mitkov, editors, Proc.
of the EACL Workshop on the Computational Treat-
ment of Anaphora, pages 39?46.
J. F. McCarthy and W. G. Lehnert. 1995. Using decision
trees for coreference resolution. In Proc. of IJCAI-95,
pages 1050?1055.
N. N. Modjeska. 2002. Lexical and grammatical role
constraints in resolving other-anaphora. In Proc. of
DAARC 2002, pages 129?134.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
ACL?02, pages 104?111.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Viera. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proc. of LREC 2002, pages
1220?1224.
J. Preiss. 2002. Anaphora resolution with word sense
disambiguation. In Proc. of SENSEVAL-2, pages 143?
146.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
M. Strube, S. Rapp, and C. Mu?ller. 2002. The influence
of minimum edit distance on reference resolution. In
Proc. of EMNLP 2002, pages 312?319.
M. Strube. 2002. NLP approaches to reference resolu-
tion. Tutorial notes, ACL?02.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736?747,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Modelling Discourse Relations for Arabic
Amal Alsaif
University of Leeds
Leeds, UK
LS2 9JT
amalalsaif@yahoo.co.uk
Katja Markert
University of Leeds
Leeds, UK
LS2 9JT
markert@comp.leeds.ac.uk
Abstract
We present the first algorithms to automat-
ically identify explicit discourse connectives
and the relations they signal for Arabic text.
First we show that, for Arabic news, most
adjacent sentences are connected via explicit
connectives in contrast to English, making the
treatment of explicit discourse connectives for
Arabic highly important. We also show that
explicit Arabic discourse connectives are far
more ambiguous than English ones, making
their treatment challenging. In the second
part of the paper, we present supervised al-
gorithms to address automatic discourse con-
nective identification and discourse relation
recognition. Our connective identifier based
on gold standard syntactic features achieves
almost human performance. In addition, an
identifier based solely on simple lexical and
automatically derived morphological and POS
features performs with high reliability, essen-
tial for languages that do not have high-quality
parsers yet. Our algorithm for recognizing dis-
course relations performs significantly better
than a baseline based on the connective sur-
face string alone and therefore reduces the am-
biguity in explicit connective interpretation.
1 Introduction
The automatic detection of discourse relations, such
as causal, contrast or temporal relations, is useful for
many applications such as automatic summarization
(Marcu, 2000), question answering (Girju, 2003),
sentiment analysis (Somasundaran et al, 2008) and
readability assessment (Pitler and Nenkova, 2008).
This task has recently seen renewed interest due to
the growing availability of large-scale corpora anno-
tated for discourse relations, such as the Penn Dis-
course Treebank (Prasad et al, 2008a).
In the Penn Discourse Treebank (PDTB), lo-
cal discourse relations (also called senses) such as
CAUSAL or CONTRAST are annotated. They hold
between two text segments (so-called arguments)
that express abstract entities such as events, facts and
propositions. Annotated discourse relations can be
signalled explicitly by so-called discourse connec-
tives (Marcu, 2000; Webber et al, 1999; Prasad et
al., 2008a) or hold implicitly between adjacent sen-
tences in the same paragraph, i.e. are not signalled
by a specific surface string. In Ex. 1, the connec-
tive while indicates an explicit CONTRAST between
the attitudes of John and Richard. In Ex. 2, the con-
nective while indicates an explicit TEMPORAL rela-
tion. In Ex. 3, an implicit CAUSAL relation between
the first and second sentence holds. We indicate dis-
course connectives and the two arguments they re-
late via annotated square brackets.
(1) [John liked adventure,]Arg2 [ while]DC[Richard
was cautious]Arg2
(2) [The children were crying
loudly]Arg1[while]DC,[their mother was
cooking]Arg2
(3) [I cannot eat any dessert.]Arg1 [I have eaten far
too much already.]Arg2
Although similar corpora for other languages are
being developed such as for Hindi (Prasad et al,
2008b), Turkish (Zeyrek and Webber, 2008), Chi-
nese (Xue, 2005) and, by ourselves, for Arabic (Al-
736
Saif and Markert, 2010), efforts in the automated
recognition of discourse connectives, arguments and
relations have so far almost exclusively centered on
English.
In contrast we present the first models for dis-
course relations for Arabic, focusing on explicit con-
nectives. This focus is partially justified by the fact
that this first study for a new language should cen-
ter on the superficially more straightforward case
and that no annotations for implicit relations are yet
available for Arabic. More importantly, however,
we make two essential claims (Section 4). Firstly,
Arabic discourse connectives are more ambiguous
than their English counterparts, i.e cases such as
while which can signal different relations dependent
on context (see Example 1 and 2) are far more fre-
quent. This makes their treatment more challenging.
Secondly, discourse relations between adjacent sen-
tences in Arabic tend to be expressed via an explicit
connective, at least for the news genre, i.e. cases
such as Example 3 are rarer. This makes the treat-
ment of explicit connectives essential.
We tackle two tasks for explicit Arabic connec-
tives in this paper, which are further discussed in
Section 2. Discourse connective recognition needs
to distinguish between discourse usage of potential
connectives and non-discourse usage (such as the
use of while as a noun). We show in Section 5 that
we can distinguish discourse- and non-discourse us-
age for potential connectives in Arabic with very
high reliability, even without parsed data, a fact that
is important for languages with fewer high quality
NLP tools available. We then present an algorithm
for relation identification in Section 6 that shows
small but significant gains over assigning the most
frequent relation for each connective. We discuss
future work and conclude in Section 7.
2 The Tasks
The handling of explicit connectives can be split into
three tasks (Pitler and Nenkova, 2009). The first task
of discourse connective recognition distinguishes
between the discourse usage and non-discourse us-
age of potential connectives. Whereas some poten-
tial connectives such as the Arabic connective 	???
/lkn/but almost always have discourse usage, this is
not true for all potential connectives.1 Thus, the dis-
course usage of Arabic ?J. 	?P /rg?bh/desire needs to
be distinguished from its use as a noun. Conjunc-
tions such as ? /w/and,?@ /a?w/or can have discourse
usage or just conjoin two non-abstract entities as in
?PA? ? Q?? /?mr w sa?rh/Omar and Sarah.
The second task is discourse connective interpre-
tation where a discourse connective in context is as-
signed a discourse relation. Again, some connec-
tives are largely unambiguous in this respect. For
example, 	??? /lkn/but signals almost always a CON-
TRAST relation. However, there are connectives
where this is not the case, such as 	Y 	J? /mnd?/since
which has a CAUSAL and a TEMPORAL sense.
The third task is argument identification which
identifies the arguments? position and extent. In this
paper we tackle Task 1 and Task 2 for Arabic in a
supervised machine learning framework.
3 Related work
Annotated Discourse Corpora and Linguistic
Background. Discourse relations are widely stud-
ied in theoretical linguistics (Halliday and Hasan,
1976; Hobbs, 1985), where also different relation
taxonomies have been derived (Hobbs, 1985; Knott
and Sanders, 1998; Mann and Thompson, 1988;
Marcu, 2000). Different inventories have been used
in English corpora annotated for discourse relations
(Hobbs, 1985; Prasad et al, 2008a; Carlson et al,
2002) which also differ in other respects (such as
whether they prescribe a tree structure for discourse
annotation). However, the annotation level of ex-
isting Arabic corpora has not yet included the dis-
course layer, making our work the first to address
this problem for Arabic on a larger scale.
Automatic discourse parsing: explicit relations.
There is no work on discourse connective recog-
nition, interpretation and argument assignment for
Arabic, so that we break entirely new ground here.
However, the two tasks we explore (discourse con-
nective recognition and discourse connective disam-
biguation) have been tackled for English.2 (Pitler
1Arabic examples contain in order: the Arabic right-to-left
script, the transliteration (standards ISO/R 233 and DIN 31635)
and the English translation (if possible).
2There is also substantial work on argument identification
(Wellner and Pustejovski, 2007; Elwell and Baldridge, 2008)
737
and Nenkova, 2009) use gold standard syntactic fea-
tures as well as the connective surface string in a
supervised model for discourse connective recogni-
tion. They achieve very high results with this ap-
proach. We will (i) show that similar features work
well for Arabic (ii) take into account Arabic-specific
morphological properties that improve results fur-
ther and (iii) present a robust version of this ap-
proach that does not rely on full parsing or gold stan-
dard syntactic annotations.
With regard to discourse connective interpreta-
tion, (Miltsakaki et al, 2005) concentrate on disam-
biguating the three connectives since, while, when
only, using a very small set of features indicating
tense and temporal markers in arguments. They
achieve good improvements over a ?most frequent
relation per connective? baseline. A more compre-
hensive study on all discourse connectives in the
PDTB (Pitler et al, 2008; Pitler and Nenkova, 2009)
reveals that most connectives are not ambiguous in
English. Using syntactic features of the connec-
tive, they achieve only a very small improvement
over a ?most frequent relation per connective base-
line? for which significance tests are not given. We
will show that for Arabic, discourse connectives are
more highly ambiguous with regard to the relations
they convey. We will present a supervised learning
model that uses a wider feature set and that achieves
small but significant improvements over the most
frequent relation per connective baseline.
Automatic discourse parsing: implicit relations.
Implicit relations have excited substantial interest
for English. This includes work in the frame-
work of RST (Soricut and Marcu, 2003; duVerle
and Prendinger, 2009; Marcu and Echihabi, 2002),
SDRT (Baldridge and Lascarides, 2005), Graph-
Bank (Wellner et al, 2006), the PDTB (Blair-
Goldensohn et al, 2007; Pitler et al, 2009; Lin
et al, 2009; Wang et al, 2010; Zhou et al,
2010; Louis and Nenkova, 2010) or framework-
independent (Sporleder and Lascarides, 2008).3 The
task is challenging as implicits behave substantially
differently from explicits (Sporleder and Lascarides,
but we do not discuss this work in depth here.
3Some work does not make the distinction between implicit
and explicit and/or treats them in a joint framework (Soricut and
Marcu, 2003; Wellner et al, 2006; Wang et al, 2010).
2008) and often need world knowledge (Lin et
al., 2009). However, features/approaches that have
shown improvement over a baseline are word pairs
(Sporleder and Lascarides, 2008), production rules
and syntactic trees (Wang et al, 2010; Lin et al,
2009) as well as language modelling (Zhou et al,
2010). As we only deal with explicit connectives
this work is not directly comparable to ours, al-
though we do explore some of the suggested features
for improving explicit connective disambiguation.
4 An Arabic Discourse Corpus
We annotate news articles from the Arabic Penn
Treebank (Part 1 v2.0) (Maamouri and Bies, 2004)
for explicitly marked discourse relations. This is the
first discourse-annotated corpus for Arabic, whose
initial development stages we have described in (Al-
Saif and Markert, 2010). We summarize this previ-
ous work and extend it by including agreement stud-
ies for arguments in Sections 4.1 and 4.2. In Sec-
tions 4.3, 4.4 and 4.5. we then present a corpus study
on the corpus which shows our major claim as to the
importance and high levels of ambiguity of Arabic
discourse connectives.
4.1 Annotation Principles
We overall follow the annotation principles in the
Penn Discourse Treebank for explicit connectives
(for example, arguments can occur at any distance
from the connectives). The relation set we use is
a more coarse-grained version of the PDTB rela-
tions with two relations added ? BACKGROUND
and SIMILARITY ? that we found in our Arabic
news texts. The final, hierarchically organized, re-
lation set of 17 discourse relations is shown in Fig 1.
Further adaptations necessary for Arabic are the
inclusion of clitics as connectives such as ? /l/for, H.
/b/by,with and 	? /f/then . In addition, differently to
English, prepositions were included as connectives
as these are frequently used to express discourse re-
lations in Arabic. In these cases, normally argument
2 is the so-called Al-Masdar.4 Typical examples are
???? /ws. wl/arrival from the verb ??? /ws. /to ar-
rive and ???Am? /mh. a?wlh/attempt from the verb ??Ag
4The medieval Arabic grammar schools, the Basra and Kufa,
debated whether the noun (almasdar) or the verb is the most
basic element of language (Ryding, 2005).
738
Figure 1: Discourse relations for Arabic
/h. a?wl/to try. Al-Masdar is formed using morpho-
logical patterns well-known in the Arabic grammat-
ical tradition: major Arabic grammars list around 60
patterns although some other references also claim
that the patterns are many more as well as more un-
predictable (Abdl al latif et al, 1997; Wright, 2008;
Ryding, 2005). Al-Masdar forms do not fit into one
grammatical or morphological category in English:
they might correspond to a gerund, a nominalization
or a noun which is not a nominalization. Some ex-
amples are listed in Table 1.
Table 1: A list of Al-MaSdar patterns, examples and their
English correspondence
Root Pattern MaSdar Translation
iJ.? /sbh. ??A? 	? /f?alh ?kAJ.? /sbah. h swimming
	
Y
	
?
	
K /nfd? ?J
? 	?K /tf?yl 	YJ
 	? 	JK /tnfyd? execution
?
	
?X /df? ?A? 	? /f??al ?A 	?X /dfa?? defence
?P 	P /zr? ??A? 	? /f??alh ??@P 	P /zra??h agriculture
H. Qk /h. rb ?? 	? /f?l H. Qk /h. rb war
An example of Al-MaSdar as argument of a dis-
course relation is Ex. 4, where 	?J
?J. K/tblyg?/informing
is the Al-MaSdar form of 	??K. /blg?/inform.
(4) 	?@Y? 	? 	?? 	?J
?J. J

?]DC[?] Arg1[

??Q??? @ 	Q?Q? ??@ A
	
JJ.?
	
X]
Arg2[

?J
??
?Q? @

??Q??? @

?

KA

K?
[d?hbna? ?la? mrkz al-s?rt.t.]Arg1[l]DC[ltblyg? ?n fqda?n
wt?a??iq als?rkh alrsmyh]Arg2
[We went to the police station]Arg1 [for]DC [in-
forming about the loss of the company?s official
documents.]Arg2
4.2 Agreement Studies
The occurrences of a precompiled list of 107 po-
tential discourse connectives were annotated inde-
pendently by 2 native Arabic speakers on 537 news
texts. Agreement was measured for the distinction
of discourse vs. non-discourse usage, relation as-
signment and argument assignment.
Agreement for the classification tasks of dis-
course connective recognition and relation assign-
ment was measured using kappa (Siegel and Castel-
lan, 1956). Argument agreement was measured by
agr, a directional measure (Wiebe et al, 2005). It
measures the word overlap between the text spans
of two judges (ann1 and ann2). agr(ann1||ann2)
measures the proportion of words ann1 annotated
that were also annotated by ann2.
agr(ann1||ann2) = |ann1 matching ann2||ann1|
Discourse connective recognition proved to be
highly reliable with percentage agreement of 0.95
and a kappa of 0.88 on the 23,331 occurrences of
the 107 potential discourse connectives. 5586 of the
potential connectives were agreed on by both anno-
tators to have discourse usage and agreement for re-
lations and argument assignment was measured on
these. As shown in Table 2, kappa on all 17 relations
was low with 0.57 ? it turned out that this was due
to the frequent, almost rhetorical use of the connec-
tive ? /w/and at the beginning of paragraphs, which
is a genre convention for Arabic news that normally
does not convey a specific discourse relation. Disre-
garding such occurrences of ? /w/and, kappa rises to
good agreement: 0.69 for fine-grained relations and
0.75 when measuring agreement between the 4 ma-
jor relations EXPANSION, CONTINGENCY, COM-
PARISON and TEMPORAL.
Argument agreement on the 5586 agreed connec-
tives is shown in Table 3. We report high word over-
lap via agr (over 90%) for Arg2, which is the ar-
gument syntactically attached to the connective, and
lesser but still substantial agreement for Arg1.
739
Table 2: Inter-annotator reliability for discourse relation
assignment
All connectives (5586)
Observed agreement 0.66
Kappa 0.57
Class level
Observed agreement 0.8
Kappa 0.67
Connectives excluding ? /w/and at BOP (3500)
Observed agreement 0.74
Kappa 0.69
Class level
Observed agreement 0.71
Kappa 0.75
Agreed disc. conn 5586
Arg1 Arg2
a) exact match
exact match =1 2361 (42%) 3803 (68%)
exact match =0 699 (13%) 18 (0.3%)
partial match 2526 (45%) 1765 (32%)
b) agr metric
agr(ann1||ann2) 78% 93%
agr(ann2||ann1) 74% 93%
Avr (agr) 76% 93%
Table 3: Inter-annotator reliability for arguments Arg1
and Arg2, using two different measurements (a) exact
match (b) agr
4.3 Gold standard
We produced a unified gold standard. First, we auto-
matically corrected easily made annotator mistakes.
With regard to argument extent, we automatically
corrected mistakes such as the erroneous inclusion
of punctuation marks at the end of clauses/sentences
or not including all obligatory complements in a
verb phrase argument. The latter relied on the syn-
tactic annotation in the ATB. Second, with regard to
discourse relation assignment, we automatically as-
signed EXPANSION.CONJUNCTION to all disagreed
instances of ? /w/and at BOP.5 A further disam-
biguation study is necessary for ? /w/and at BOP,
which is beyond the scope of this paper.
Finally, an adjudicator not initially involved in an-
notation reconciled the remaining disagreements at
5Other instances of ? /w/and are not treated this way.
all levels and included annotations for 5 new po-
tential discourse connective types not in our initial
connective list but commented on by the annotators
during annotation. 3 news files were removed from
the corpus ? they contained no actual news reports
but just a list of headlines.
The final discourse treebank we use has 6328 an-
notated explicit connectives in 534 files. 68 connec-
tive types were found, rising to 80 connective types
if we include all modified forms of a connective as
distinct types such as 	?? ? 	?Q?AK. /ba?lrg?m mn, 	?@ ? 	?P
/rg?m a?n as modified forms of ? 	?P /rg?m/although.
Most discourse connectives were only annotated
with a single relation but 5% were annotated with
two or more relations (as also allowed in the PDTB).
These statistics are summarised in Table 4.
Files 534
Total tagged tokens 126,046
(125KB)
Sentences 3607
Paragraphs 3312
Discourse connectives (tokens) 6328
Distinct connective (types) 68
including modifed form con-
nectives
80
Clitic discourse connectives (to-
kens)
4779
(76%)
Non-clitic discourse connec-
tives (tokens)
1549
(24%)
Relations types (17 single, 38
combined)
55
Single relations (tokens) 6039
(95%)
Combined relations (tokens) 289 (5%)
Table 4: Statistics of the final gold standard corpus
4.4 Importance of explicitly signalled relations
We compared the number of relations between 2
adjacent sentences that were explicitly signalled in
English vs. the ones that were explicitly signalled
in Arabic, using the PDTB and our corpus (both
containing texts of the news genre). Out of a total
44,470 adjacent sentence pairs in the PDTB, 5355
740
(12%) were linked by an explicit connective.6 In
contrast, out of the 3073 adjacent sentence pairs in
our corpus, 2140 (70%) were linked by an explicit
connective, 948 (30%) were linked via non-wa con-
nectives. Thus, for our corpus, modeling of explicit
connectives is primary: intrasentential relations tend
to be marked by connectives anyway in both English
and Arabic, and our corpus shows that this is true for
most local intersentential relations as well.
4.5 Ambiguity for Arabic discourse connectives
We investigate the ambiguity of Arabic connectives
with regard to their sense at class level (4 relations)
as well as the more fine-grained level (all 17 rela-
tions). We restrict our investigation to the connec-
tive occurrences that were annotated with a single
relation (6039 tokens) and also exclude ? /w/and at
the beginning of paragraph, leaving 3813 tokens.7
Of 80 connective types, 52 were unambiguous at the
class level and 47 at the fine-grained level. However,
many of the most frequent connectives are highly
ambiguous. If we just assign the most frequent read-
ing to each of the 3813 connectives, we achieve an
accuracy of 82.7% at the class-level and 74.3% at
the more fine-grained level for relation assignment,
leaving a substantial error margin. This contrasts
with the English PDTB, where at the class-level 92%
can be achieved with this simple method and 85% at
the second-level.8
5 Discourse Connective Recognition
We distinguished discourse vs. non-discourse usage
for all potential connectives in the 534 gold stan-
dard files. As headers and footers in the news files
never contained true discourse connectives, we dis-
regarded these, leaving 20,312 potential discourse
connectives of which 6328 are actual connectives.
6Connections between subclauses or phrases in different,
adjacent sentences were included in the count.
7We automatically assigned CONJUNCTION to many occur-
rences of ? /w/and at BOP (Section 4.3) so that it is not sensible
to include these occurrences in a study of human-assigned am-
biguity.
8The second level in the PDTB with its 16 relations corre-
sponds approximately to our fine-grained inventory. This com-
parison can only be appropriate due to slight differences in the
lower-grained relation inventory.
5.1 Features
Apart from the surface string of the potential con-
nective Conn, we use the following features. Fea-
tures are either extracted from raw files tokenized
by white space only (M2) or from raw files tok-
enized by white space and tagged by the Stanford
tagger9 (Models M3, M4) or from the Arabic Tree-
bank (ATB) gold standard part-of-speech and parse
annotation (models M5-M9). The syntactic features
(Syn) are inspired by (Pitler and Nenkova, 2009).
Lexical/POS patterns of surrounding words, clitic
features and Al-Masdar are novel.
Surface Features (SConn). These include the po-
sition of the potential connective (sentence-initial,
medial or final). The type of the potential connective
is Simple when the potential connective is a single
token not attached to other tokens, PotClitic when
it is attached. Potential connectives containing more
than one token have MoreThanToken type.
Models where we use ATB or automated tagging
(M3-M9) distinguish further between potential cli-
tics that are assigned a POS and ones that are not.
Models that use ATB annotation also distinguish
between potential connectives that correspond to a
phrase in the ATB (MorethanToken Phrase) and
the ones that do not (MorethanToken NonPhrase).
Lexical features of surrounding words (Lex).
We encode the surface strings of the three words
before and after the connective, recording posi-
tion. These features are especially useful for lan-
guages where no accurate parser or tagger is avail-
able as lexical patterns can capture discourse and
non-discourse usage. For instance, if a potential
connective is followed by 	?@ /a?n/ it most likely has
a discourse function (see Ex. 5).
(5) DC[ ? ] Arg1[ ?A?PBAK. @?K. A?
 	?@] 	????
 ?A 	??B@ 	?@
@??A
	
JK
 ?? @
	
X @

??@PY?@ ?C
	
g Arg2[?A?	J?AK. @?Q? ?

	
?@]
@YJ
k.
[ a?n a?la?t.fa?l ymkn [a?n ys. a?bwa? ba?la?rha?-
q]Arg1[w]DC[a?n ys??rwa? ba?ln??as]Arg2 h?la?l a?ldra?sh
a?d?a? lm yna?mwa? g?yda?
[Children might be tired]Arg1 [and]DC [feel
sleepy]Arg2 during school time if they did not sleep
well
9http://nlp.stanford.edu/software/tagger.shtml
741
Part of Speech features (POS). We include
the pos tag of the potential connective via the
ATB/Stanford Tagger. For potential connectives that
consist of more than one token, we combined its
ordered POS tags. Thus, the potential connective
?Ag ?


	
? /fy h. a?l/in case with its tags (fy PREP)(Hal
NOUN)) will receive the pos PREP#NOUN. If a po-
tential connective does not receive a separate POS
tag in the ATB/tagger, the value ?NONE? is as-
signed. This allows to distinguish clitics from let-
ters at the start of a word. We also record the
POS of the three words before/after the connective
(ATB/Stanford Tagger). Similar to lexical patterns,
these can capture discourse and non-discourse us-
age. For instance, if a potential connective is soon
followed by a modal, it is more likely to have a dis-
course function.
Syntactic category of related phrases (Syn). We
record the syntactic category of the parent of the po-
tential connective in ATB. For example, it is rare
that cases where the parent of the potential connec-
tive is an adjective phrase, correspond to discourse-
usage. A typical example of a non-discourse usage
of ? /w/and ( ??J
?g. ? ?Q
J.? ??PY?? @ /a?lmdrsh kbyrh wg?mylh/ the school is very large and beautiful) illus-
trates this. Unlike English, parents in Arabic often
are noun phrases as nominalisations are frequent ar-
guments of prepositional connectives. We also en-
code the Left sibling category and right sibling cat-
egory of the connective. For discourse connectives,
the right sibling is normally S, SBAR, VP or an NP
(if the connective is a preposition).
Al-Masdar feature. Potential connectives fol-
lowed by Al-Masdar are more likely to have dis-
course usage (see Section 4.1). Especially preposi-
tions with discourse usage are normally attached to
Al-masdar such as in ?KXAj?? /lmh. a?dt?h/for contacting
or Z @Qk. AK. /ba?g?ra??/by processing. Al-Masdar informa-
tion is not included in the ATB so we constructed a
binary Al-Masdar feature from (tagged) text by ex-
amining the first noun after the potential connective.
We developed an algorithm to judge such a noun as
Al-Masdar or not. This algorithm uses a stemmer
for Arabic and then determines whether the stem is
al-Masdar by a combination of surface-based rules
to check whether the stem corresponds to one of the
known Al-Masdar patterns.
5.2 Results and Discussion
We used the implementation JRip of the rule-
based classifier Ripper in the machine learning tool
WEKA with its default settings. We used 10-fold
cross-validation throughout. Significance tests are
reported using the McNemar test at the significance
level of 1%. A most frequent category baseline
would assign all potential connectives as not connec-
tive, achieving an accuracy of 68.9% as only 6328 of
our potential 20,312 connectives actually have dis-
course usage. We built several models using differ-
ent features. The results are shown in Table 5.
A simple model M1 that only uses the connective
string improves significantly over the baseline with
75.7% accuracy but a kappa of only 0.48, showing
that this is not a reliable strategy. Models M2-M4
do not rely on gold standard annotation or parsing
(in contrast to the models for English in (Pitler and
Nenkova, 2009)). Using only surface and lexical
features that can be extracted from white-space to-
kenized raw files in addition to the connective string
(M2), gains a substantial improvement over using
the connective string alone. This is further improved
by using POS tags of connectives and surrounding
words with an automatic tagger (M3) and by includ-
ing the Al-Masdar feature (M4), thus making good
use of the morphological properties of Arabic. All
differences are statistically significant (M1 < M2 <
M3 < M4). The final model is reliable (kappa 0.70),
an encouraging result given the absence of parsing
and important for resource-scarce languages.
With ATB gold standard tokenisation, tagging and
parsing, our models (not surprisingly) improve fur-
ther showing the same pattern of (M1 < M5 < M6 <
M7) with all differences being significant. The final
best model achieves highly reliable results (accuracy
92.4%, kappa 0.82). We also conclude that syntac-
tic features are more useful than lexical patterns as
model M8 (syntax with no lexical patterns) achieves
equally good results as M6. Our models also man-
age to generalise well over individual connectives.
If we leave out the connective string (M9), we still
achieve a highly reliable result.
6 Discourse Relation Recognition
When disambiguating the relation that discourse
connectives signal, we assume that the arguments of
742
Features Acurr K
Baseline (not conn) 68.9 0
M1 Conn only 75.7 0.48
Tokenization by white space + auto tagger
M2 Conn+ SConn+Lex 85.6 0.62
M3 Conn+ SConn+Lex+POS 87.6 0.69
M4 Conn+SConn+Lex+POS+Masdar 88.5 0.70
ATB-based features
M5 Conn+SConn+Lex 86.2 0.65
M6 Conn+SConn+Lex+Syn/POS 91.2 0.79
M7 Conn+SConn+Lex+Syn/POS+Masdar 92.4 0.82
M8 Conn+SConn+Syn 91.2 0.79
M9 SConn+Lex+Syn+Masdar 91.2 0.79
Table 5: Performance of different models for identifying discourse connectives.
the connective are known. This is well-established
for PDTB relation recognition (Wang et al, 2010;
Lin et al, 2009; Miltsakaki et al, 2005). Our mod-
els predict single relations on two datasets: (i) all
instances of connectives signalling single relations
(Set All, 6039 instances) (2) all instances apart from
the connective ? /w/and at beginning of paragraph
as they are affected by the auto-correction process
(Set no-wa-atBOP, 3813 instances). We use 10-fold
cross-validation and JRip as well as a McNemar test
at the 5% level for significance tests.
6.1 Features
Whereas some of the features we use have been used
for English implicit relation recognition (Lin et al,
2009; Wang et al, 2010; Pitler et al, 2009) , they
are new for Arabic and not widely used for explicit
connectives. All features are extracted from the ATB
gold standard parses.
Connective features. This includes the connec-
tive string Conn. In addition, we also use the sur-
face connective features and POS of connective de-
scribed in Section 5. We also use the syntactic path
to the connective as a novel feature.
Words and POS of arguments. The words and
pos tags of the first three words in Arg1 and
Arg2 are used to catch patterns in arguments.
For example, when the first word of Arg2 is
Y

? /qd/might/may or 	?A? /ka?n/had, the relation is
likely to be EXPANSION.BACKGROUND or EXPAN-
SION.CONJUNCTION. We also measure word over-
lap between the arguments, hoping to catch relations
such as COMPARISON.SIMILARITY.
Masdar. This feature states whether the first or
second word in Arg 2 is an Al-Masdar. Many prepo-
sitional connectives followed by an Al-Masdar indi-
cate a CONTINGENCY.CAUSE relation (see Ex. 4)
Tense and Negation. Each argument is assigned
its tense as one of perfect, imperfect, future or none.
We also indicate whether the tense of Arg1 or 2 are
the same and whether a negation is part of Arg 1
or 2. Inspired by (Miltsakaki et al, 2005), we stip-
ulate that tense is useful for recognizing temporal
and causal relations. For example, the arguments of
the relation TEMPORAL.SYNCHRONOUS are likely
to have the same tense. In contrast, arg1 tense is
more likely to be prior to arg2 tense for TEMPO-
RAL.ASYNCHRONOUS and CAUSE relations.
Length, Distance and Order Features. We use
the length of arguments (in words), word distance
between a connective and its arguments (-1; for ar-
guments in order Arg1 Conn Arg2 Arg1), tree dis-
tance of connective and arguments (0 if connective
and an argument are in the same tree) and a bi-
nary feature of whether Arg1 and Arg2 are in dif-
ferent sentences. A nominal feature encodes one of
the three orders Arg1 Conn Arg2, Conn Arg2 Arg1
and Arg1 Conn Arg2 Arg1, the latter being fre-
quent in Arabic for TEMPORAL.ASYNCHRONOUS
relations.
Argument Parent. We record the syntactic par-
ent of each Argument. However, not every argu-
743
ment corresponds to a complete tree in the ATB ?
in these cases we extract the category of the parent
shared by the first and last word in the argument.
Production Rules. We use all non-lexical produc-
tion rules that occur more than 10 times in the argu-
ments as binary features. This was inspired by (Lin
et al, 2009) who use production rules to good effect
for implicit relations in English.
6.2 Results
Table 6 shows the results for fine-grained (17 rela-
tions) classification. The baseline of assigning the
most frequent relation EXPANSION.CONJUNCTION
to every connective performs with an accuracy of
52.5% on Set All and 35% on set no-wa-atBOP. If
we use a model that relies on the discourse connec-
tive alone (M1) we achieve results of 77.2%/74.3%,
respectively. As noted in Section 4.5 this is substan-
tially lower than what the same model can achieve
for English. Including connective and argument fea-
tures (apart from production rules) in M2, leads to a
small but significant improvement.10 Further incor-
poration of production rules does not improve the
results (M3). In Table 7, we show the results at the
class-level (4 relations). Here using additional fea-
tures over the connective string does not lead to sig-
nificant improvements.
6.3 Discussion and Error Analysis
We concentrate our discussion on fine-grained clas-
sification excluding wa at BOP.
Our improvements in M2 over the connective-
only classifier (M1) are in two main areas. First, our
model performs generalisation, i.e. outputs some
rules that do not use the connective string at all.
These achieve a somewhat surprising improvement
of M2 over M1 for unambiguous connectives which
are too rare to classify via the connective string. In
those cases, they either (i) have not been seen in the
training data before and are therefore not classifiable
when seen first time in the test set or (ii) have been
10Our corpus includes some texts on similar topics where
some sentences are (almost) repeated in different texts. To
investigate whether our improvements are due to this repeti-
tion, we also performed an experiment excluding all repeated
instances of feature vectors from the corpus. The results are
almost the same and, most importantly, M2 again improves sig-
nificantly over M1.
Ref Features Acc K
All connectives (6039)
Baseline (CONJUNCTION) 52.5 0
M1 Conn only (1) 77.2 0.60
M2 Conn+Conn f+ Arg f (37) 78.8 0.66
M3 Conn+Conn f+ Arg f+ Pro-
duction rules (1237)
78.3 0.65
excluding wa at BOP (3813)
Baseline (CONJUNCTION) 35 0
M1 Conn only (1) 74.3 0.65
M2 Conn+Conn f+ Arg f (37) 77 0.69
M3 Conn+Conn f+ Arg f+ Pro-
duction rules (1237)
76.7 0.69
Table 6: Performance of different models for identifying
fine-grained discourse relations on two datasets.
Ref Features Acc K
All connectives (6039)
Baseline (EXPANSION) 62.4 0
M1 Conn only (1) 88.7 0.78
M2 Conn+Conn f+ Arg f (37) 88.7 0.78
excluding wa at BOP (3813)
Baseline (EXPANSION) 41.8 0
M1 Conn only (1) 82.7 0.74
M2 Conn+Conn f+ Arg f (37) 83.5 0.75
Table 7: Performance of different models for identifying
class-level discourse relations on two datasets.
seen in the training data too rarely for the rule-based
classifier to develop a rule judged to be more re-
liable than the default EXPANSION.CONJUNCTION
classification. Our data includes 47 unambiguous
connective types, accounting for 574 of the 3813
tokens. 30 of these 47 types are so rare that we
found mistakes in the connective-only classification,
including B@ /a?la?/except (2), I. ?? /?qb(2), A?? A? /t.a?-
lma?(2), ? 	?QK. /brg?m(1). For 14 of these 30 connec-
tives, model M2 was able to use generalised rules
to improve relation assignment.11 These rules in-
volve mainly connective surface and POS features.
Thus, sentence-start adverbials consisting of more
than one token such as 	?@ YJ
K. /byd a?n(6) and 	?@ Q
 	?
/g?yr a?n(6) were correctly classified as CONTRAST.
11For the other 16 connectives neither of the models was able
to classify them correctly.
744
This advantage of our model over the connective-
only model might disappear if in a larger corpus
more instances of those connectives are found
and are still unambiguous. Therefore, we are
more interested in how our classifier does on
truly ambiguous connectives (33 connective types
accounting for 3239 tokens of 3813 overall to-
kens). We conducted a separate significance test
on ambiguous connectives only and found that
M2 improves over M1 classification significantly
at the 1% level. How well we do on individual
connectives depends on their frequency and on their
level of ambiguity. If connectives are ambiguous
and of low frequency (?? /lw, A? 	? @ /a?nma?, ?Ag /h. a?l/)
both M1 and M2 do perform badly on them. If
connectives are frequent (10 or more occurrences)
and have relatively low ambiguity (majority reading
accounts for more than 70% of instances), the
overall performance of M1 and M2 with regard to
accuracy is also similar, often both using just the
connective string. On the other hand, if connectives
are frequent and have high ambiguity (i.e. no such
clear majority reading), then M2 normally improves
(often substantially) on M1. Examples of such
connectives are A?? /kma?, A?J
 	? /fyma?and QK@ /a?t?r.
Most of the successful rules use tense in some form,
either via part of speech of verbs or via comparing
the tense in the two arguments. This, for example,
led to a successful recognition of all 9 instances of
Similarity in the connective kmA (whose major-
ity relation is Expansion.Conjunction in 40 out
of 65 occurrences). The connective 	? /f/then is dis-
tinguished into EXPANSION.EXEMPLIFICATION,
CONTINGENCY.CAUSE.RESULT and CONTIN-
GENCY.CAUSE.REASON readings, depending on
the lexemes around it, the parents of its arguments,
and whether its argument 2 is tensed or not. Thus,
nontensed arguments are most often nominalisations
leading to a reason reading, whereas a verb phrase
as argument 2 and a sentence as argument 1 often is
a result reading. However, it is worth reporting that
in cases of very high ambiguity, M2 is still far from
perfect such as for connectives f 	? /fand QK@ /a?t?r.
Some improvements again come from gener-
alised rules: there are some very high-coverage
and high precision generalised rules that reduce
dependency on the connective string. For example,
clitic prepositions (such as ? /l/for) can without
any further information be clearly classified as
Contingency.Cause.Reason.NonPragmatic
covering 494 occurrences with only 26 mistakes.
These are cases where the following argument is
normally Al-Masdar.
Our analysis leads us to the following strategy
for follow-on work. First of all, a larger corpus is
necessary to get more examples for low frequency
connectives. Secondly, experiments with different
classifiers are worthwhile to conduct to see how our
improvements generalise. Third, the most mileage
is in further improvements on frequent, ambiguous
connectives such as 	? /f, 	Y 	J? /mnd?and ?@ /a?w. This
can be achieved with, on the one hand, training
connective-specific classifiers on larger data sets but
will, on the other hand, also need a wider feature
base. From our corpus study, we think that lexico-
semantic features such as word pairs and seman-
tic classes of verbal/nominalised arguments are the
most promising.
7 Conclusions and Future Work
We have presented the first study on the automatic
detection and disambiguation of Arabic discourse
connectives. A corpus study showed that these
are highly frequent and more ambiguous than their
English counterparts. Our automatic algorithms
achieve very good results on discourse connective
identification, using Arabic morphological proper-
ties to good effect. It is particular promising that we
do not need parsed data to identify discourse usage
of potential connectives reliably. Our algorithm for
discourse connective interpretation beats the chal-
lenging baseline of assigning the most frequent re-
lation per connective. In future, we will explore fur-
ther features for connective disambiguation as well
as connective-specific classification, combined with
semi-supervised algorithms to alleviate data sparse-
ness. We will also develop algorithms for argument
identification.
Acknowledgments
Amal Al-Saif is supported by a PhD scholarship from
the Imam Muhammad Ibn Saud University, Saudi Arabia.
We thank the British Academy for additional funding for
the annotation study via Grant SG51944. Also thank you
to the 3 anonymous reviewers for their comments.
745
References
M. Abdl al latif, A. Umar, and M. Zahran. 1997. Alnhw
AlAsAsi. Dar Alfker Al-Arabi, Cairo, Egypt.
A. AlSaif and K. Markert. 2010. The leeds arabic dis-
course treebank: Annotating discourse connectives for
arabic. In Language Resources and Evaluation Con-
ference (LREC).
J. Baldridge and A. Lascarides. 2005. Probabilistic head-
driven parsing for discourse structure. In Proc. Of
Conll 2005.
S. Blair-Goldensohn, K McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic rela-
tion models. In Proc. of HLT-NAACL 2007.
L. Carlson, D. Marcu, and M. Okurewski. 2002. Rst
discourse treebank. Linguistic Data Consortium.
D. duVerle and H. Prendinger. 2009. A novel discourse
parser based on support vector machine classification.
In Proc. of ACL 2009.
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In Proc. of the International Conference on
Semantic Computing.
R. Girju. 2003. Automatic detection of causal relations
for questions answering. In Proc. of the ACL 2003
Workshop on Multilingual Summarisation and Ques-
tion Answering, pages 76?83.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in En-
glish. Longman London.
J.R. Hobbs. 1985. On the coherence and structure of
discourse. Center for the Study of Language and In-
formation, Stanford, Calif.
A. Knott and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmatics,
30(2):135?175.
Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing implicit
discourse relations in the penn discourse treebank. In
Proc. of EMNLP 2009, pages 343?351.
A. Louis and A. Nenkova. 2010. Creating local coher-
ence: An empirical assessment. In Proc. of NAACL
2010.
M. Maamouri and A. Bies. 2004. Developing an Ara-
bic treebank: Methods, guidelines, procedures, and
tools. In Proceedings of the Workshop on Computa-
tional Approaches to Arabic Script-based Languages
(COLING), Geneva.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text organi-
zation. Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc. of
ACL 2002.
D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. MIT Press.
E. Miltsakaki, N. Dinesh, R. Prasad, A. Joshi, and
B. Webber. 2005. Experiments on sense annotation
and sense disambiguation of discourse connectives. In
Proc. of the Workshop on Treebanks and Linguistic
Theories.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proc. of EMNLP 2008, pages 186?195.
E. Pitler and A. Nenkova. 2009. Using syntax to dis-
ambiguate explicit discourse connectives. In Proc of
ACL-IJCNLP 2009 (Short Papers), pages 13?16.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse
relations. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING
2008), Manchester, UK, August.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of ACL-IJCNLP 2009, pages 683?691.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The Penn discourse
treebank 2.0. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC 2008).
R. Prasad, S. Husain, D.M. Sharma, and A. Joshi. 2008b.
Towards an Annotated Corpus of Discourse Relations
in Hindi. In The Third International Joint Conference
on Natural Language Processing, pages 7?12. Cite-
seer.
K.C. Ryding. 2005. A reference grammar of modern
standard Arabic. Cambridge Univ Pr.
S. Siegel and N.J. Castellan. 1956. Nonparametric
statistics for the behavioral sciences. McGraw-Hill
New York.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse-level opinion interpretation. In Proc. of
Coling 2008.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proc of HLT-NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using auto-
matically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineering,
14:369?416.
W. Wang, J. Su, and C. Tan. 2010. Kernel-based dis-
course relation recognition with temporal ordering in-
formation. In Proc. of ACL 2010, pages 710?719.
B. Webber, A. Knott, M. Stone, and A. Joshi. 1999.
Discourse relations: A structural and presuppositional
account using lexicalised TAG. In Proceedings of
746
the 37th annual meeting of the Association for Com-
putational Linguistics on Computational Linguistics,
page 48. Association for Computational Linguistics.
B. Wellner and J. Pustejovski. 2007. Automatically
identifying the arguments of discourse connectives. In
Proc. of EMNLP 2007, pages 92?101.
B. Wellner, J. Pustejovski, A. Havasi, A. Rumshisky, and
R. Suair. 2006. Classification of discourse coherence
relations: An exploratory study using multiple knowl-
edge sources. In Proc. of SIGDIAL2006.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation.
W. Wright. 2008. A grammar of the Arabic language.
Bibliobazaar.
Nianwen Xue. 2005. Annotating discourse connectives
in the chinese treebank. In CorpusAnno ?05: Proceed-
ings of the Workshop on Frontiers in Corpus Annota-
tions II, pages 84?91, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
D. Zeyrek and B. Webber. 2008. A discourse resource
for turkish: Annotating discourse connectives in the
metu corpus. Proceedings of IJCNLP-2008. Hyder-
abad, India.
Z. Zhou, Y. Xu, Z. Niu, M. Lan, . Su, and Tan. C. 2010.
Predicting discourse connectives for implicit discourse
relation recognition. In Proc. of Coling 2010, pages
1507?1514.
747
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 183?193, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Local and Global Context
for Supervised and Unsupervised Metonymy Resolution
Vivi Nastase
HITS gGmbH
Heidelberg, Germany
vivi.nastase@h-its.org
Alex Judea
University of Stuttgart
Stuttgart, Germany
alexander.judea@ims.uni-stuttgart.de
Katja Markert
University of Leeds
Leeds, UK
K.Markert@leeds.ac.uk
Michael Strube
HITS gGmbH
Heidelberg, Germany
michael.strube@h-its.org
Abstract
Computational approaches to metonymy res-
olution have focused almost exclusively on
the local context, especially the constraints
placed on a potentially metonymic word by
its grammatical collocates. We expand such
approaches by taking into account the larger
context. Our algorithm is tested on the data
from the metonymy resolution task (Task 8) at
SemEval 2007. The results show that incorpo-
ration of the global context can improve over
the use of the local context alone, depending
on the types of metonymies addressed. As a
second contribution, we move towards unsu-
pervised resolution of metonymies, made fea-
sible by considering ontological relations as
possible readings. We show that such an unsu-
pervised approach delivers promising results:
it beats the supervised most frequent sense
baseline and performs close to a supervised
approach using only standard lexico-syntactic
features.
1 Introduction
With the exception of explicit tasks in metonymy
and metaphor analysis, computational treatment of
language relies on the assumption that the texts to be
processed have a literal interpretation. This contrasts
with the fact that figurative expressions are com-
mon in language, as exemplified by the metonymy
in the excerpt from a Wikipedia article in Exam-
ple 1 and another in Example 2 from the SemEval
2007 metonymy resolution task (Markert and Nis-
sim, 2009).
(1) In the gold medal game, Canada defeated the
American team 2-0 to win their third consecu-
tive gold.
(2) This keyword is only required when your rela-
tional database is Oracle.
The defeating in Example 1 will not be done
by the country as such, but by a team represent-
ing the country in a sporting event. Hence, in a
metonymy a potentially metonymic expression or
word (here Canada) stands for a conceptually re-
lated entity (here, people of Canada). In the sec-
ond Example, a company name (Oracle) stands for
a product (database) developed by the company.
Metonymy resolution can be important for a
variety of tasks. Textual entailment may need
metonymy resolution (Bentivogli et al2007): for
example, we would like to be able to induce from
Example 1 the hypothesis
The Canadian team won . . . .
Leveling and Hartrumpf (2008) show that
metonymy recognition on location proper names
helps geographical information retrieval by ex-
cluding metonymically used place names from
consideration (such as Example 1 or the use of
Vietnam for the Vietnam war). Metonymies also fre-
quently interact with anaphora resolution (Nunberg,
1995; Markert and Hahn, 2002), as in Example 1
where the metonymic use of Canada is referred to
by a plural pronoun afterward (their).
Metonymies can be quite regular: company
names can be used for their management or their
products, country names can be used for associated
sports teams. Following from this, the currently
183
prevalent set-up for metonymy resolution ? as in
the SemEval 2007 task ? provides a manually com-
piled list of frequent readings or metonymic patterns
such as organization-for-product for pre-
specified semantic classes (such as organizations) as
well as annotated examples for these patterns so that
systems can then treat metonymy resolution as a (su-
pervised) word sense disambiguation task. How-
ever, this approach needs novel, manual provision
of readings as well as annotated examples for each
new semantic class.
In contrast, we will see readings as relations be-
tween the potentially metonymic word (PMW) and
other concepts in a large concept network, a priori
allowing all possible relations as readings. We base
this approach on the observation that metonymic
words stand in for concepts that they are related
with ? e.g. the part for the whole, the company
for the product. These readings are obtained on
the fly and are therefore independent of manually
provided, preclassified interpretations or semantic
classes, leading eventually to the possibility of un-
supervised metonymy resolution. We achieve this
by first linking a PMW to an article in Wikipedia.
Then we extract from a large concept network de-
rived from Wikipedia the relations surrounding the
PMW.
As there will be (many) more than one such rela-
tion, these need to be ranked or scored. We achieve
this in a probabilistic framework where we condi-
tion the probability of a relation on the context of
the PMW. This ranking showcases our second major
innovation in that the flexibility of our framework al-
lows us to incorporate a wider context than in most
prior approaches. Let us consider the indications for
metonymic readings and its interpretation in Exam-
ple 1, on the one hand, and Example 2, on the other
hand. In Example 1, the grammatical relation to the
verb defeat and the verb?s selectional preferences in-
dicate the metonymy. We will call all such grammat-
ically related words and the grammatical relations
the local context of the PMW. Such types of local
context have been used by most prior approaches
(Pustejovsky, 1991; Hobbs et al1993; Fass, 1991;
Nastase and Strube, 2009, among others). However,
Example 2 shows that the local context can be am-
biguous or often weak, such as the verb to be. In
these examples, the wider context (database, key-
word) is a better indication for a metonymy but has
not been satisfactorily integrated in prior approaches
(see Section 2). We here call all words surround-
ing the PMW but not grammatically related to it the
global context.
In our approach we integrate both the local and
the global context in our probabilistic framework.
For the local context, we compute the selectional
preferences for the words related to the PMW from a
corpus of English Wikipedia articles and generalize
them in the Wikipedia concept network, thus (auto-
matically) providing a set of abstractions ? general
concepts in the network that capture the semantic
classes required by the local context. In the next
step we compute probabilities of the global con-
text surrounding the PMWs under each (locally re-
quired) abstraction, and combine this with the se-
lectional preferences of the grammatically related
words. That we can integrate local and global con-
text in one probabilistic but also knowledge-based
framework is possible because we combine two de-
scriptions of meaning ? ontological and distribu-
tional ? by exploiting different sources of informa-
tion in Wikipedia (category-article hierarchy and ar-
ticle texts).
We compute the probabilities of the relations (=
readings) between the concept corresponding to the
PMW and its directly related concepts. These can
be used either (i) as additional features in a super-
vised approach or (ii) directly for unsupervised res-
olution. We do both in this paper and show that (i)
the supervised approach using both local and global
context can outperform one using just local con-
text, dependent on the semantic class studied and
(ii) that an unsupervised approach ? although lower
than the supervised one ? outperforms the super-
vised most frequent reading baseline and performs
close to a standard supervised model with the basic
set of lexico-syntactic features (Nissim and Markert,
2005).
2 Related Work
The word sense disambiguation setting for
metonymy resolution as developed by Nissim
and Markert (2005) and used for the SemEval 2007
task (Markert and Nissim, 2009) uses a small, pre-
specified number of frequently occurring readings.
184
The approaches building on this work (Farkas et
al., 2007; Nicolae et al2007, among others) are
supervised, mostly using shallow surface features
as well as grammatical relations.1 Most effective
in the SemEval task as summarized in Markert
and Nissim (2009) has been the local, grammatical
context, with the two systems relying on the global
context or the local/global context in a BOW model
(Leveling, 2007; Poibeau, 2007) not outperforming
the most frequent reading baseline. We believe
that might be due to the lack of a link between the
local and global context in these approaches ? in
our work, we condition the global context on the
abstractions and selectional preferences yielded by
the local context and achieve better results.
Lapata (2003), Shutova (2009) as well as Roberts
and Harabagiu (2011) deal with the issue of logical
metonymy, where the participant stands in for the
full event: e.g. Mary enjoyed the book., where book
stands in for reading the book, and this missing event
(reading) can be inferred from a corpus. Utiyama
et al2000), Lapata (2003) propose a probabilis-
tic model for finding the correct interpretation of
such metonymies in an unsupervised manner. How-
ever, these event type metonymies differ from the
problem dealt with in our paper and the SemEval
2007 task in that their recognition (i.e. their distinc-
tion from literal occurrences) is achieved simply by
grammatical patterns (a noun instead of a gerund or
to-infinitive following the verb) and the problem is
limited to interpretation.
Our view of relations in a concept network being
the interpretations of metonymies is strongly remi-
niscent of older work in metonymy resolution such
as Hobbs et al1993), Fass (1991), Markert and
Hahn (2002) or the use of a generative lexicon and
its relations in Pustejovsky (1991), which also are
unsupervised. However, these approaches lacked
scalability due to the use of small hand-modeled
knowledge bases which our use of a very large
Wikipedia-derived ontology overcomes. In addition,
most of these approaches (Fass, 1991; Hobbs et al
1993; Pustejovsky, 1991; Harabagiu, 1998) rely on
the view that metonymies violate selectional restric-
tions in their immediate, local context, usually those
1Brun et al2007) is semi-supervised but again relies on the
local grammatical context.
imposed by the verbs on their arguments. As can
be seen in the Example 2, this misses metonymies
which do not violate selectional restrictions. Nas-
tase and Strube (2009) use more flexible proba-
bilistic selectional preferences instead of strict con-
straint violations as well as WordNet as a larger tax-
onomy but are also restricted to the local context.
Markert and Hahn (2002) do propose a treatment of
metonymies that takes into account the larger dis-
course in the form of anaphoric relations between
a metonymy and the prior context. However, they
constrain discourse integration to potential PMWs
that are definite NPs and the context to few previous
noun phrases. In addition, their framework uses a
strict rule-based ranking of competing readings that
cannot be easily extended.
The work presented here also relies on a con-
cept network, built automatically from Wikipedia.
This resource provides us with links between enti-
ties in the text, and also a variety of ontological re-
lations for the PMW, that will allow us to identify a
wide variety of metonymic interpretations. Our ap-
proach combines information from the concept net-
work with automatically acquired selectional prefer-
ences as well as a possibility to combine in a prob-
abilistic framework the influence of the local and
global context on the interpretation of a potentially
metonymic word.
3 The Approach
The approach we present takes into account both
the local, grammatical, context and the larger textual
context of a potentially metonymic word. Figure 1
presents a graphical representation of our approach.
On the one hand, the word/term to be interpreted
(the potentially metonymic word/term ? PMW) is
mapped onto a concept in the concept network (Sec-
tion 3.3), which gives us access to the conceptual
relations (Ri) between the PMW and other concepts
(cx ? CRi). On the other hand, any word w gram-
matically related to the PMW via a grammatical re-
lation r provides us with semantic restrictions on the
interpretation of the PMW, namely preferred seman-
tic classes Aj (we call them abstractions) and a se-
lectional preference score.2 These are automatically
2We restrict the grammatical context that provides selec-
tional preferences to verbs or adjectives grammatically related
185
A 1 A 2
1R kR
A n
12c
14c
11c
13c 1n?1c
1nc
k1c k2
c
k4ck3c
kmc
km?1c w1w2w3
wl
w
r
...
...
PMW
p(Ri|Aj) p(Aj|Cont,w,r)
Global context
...
... ...
...
... ...
Figure 1: Metonymy resolution using selectional preferencesAj derived from local contextw and r, semantic relations
Ri to the PMW from a concept network, and the global context surrounding a term to be interpreted
acquired by using a corpus of Wikipedia articles and
a repository of encyclopedic knowledge (presented
in Section 3.1), as described in detail in 3.2. Because
the abstractions Aj and the PMW?s related concepts
(cx) come from the same structured resource, we
can compute the probabilities for each Ri given the
grammatically related word w and the grammatical
relation r. The global context can also easily be
added to the computation, as the probability of each
word in the context relative to an abstraction Aj can
be computed through the resource?s is a hierarchy
and its link to Wikipedia articles. This is detailed in
Section 3.4.
3.1 A concept network obtained from
Wikipedia
We use a Wikipedia article dump (January 2011)
which provided over 3.5 million English articles,
interconnected through a hierarchy of categories
and hyperlinks. This partly structured repository
is transformed into a large-scale multilingual con-
cept network, whose nodes are concepts correspond-
ing to articles or categories in Wikipedia (Nastase
et al2010). Concepts in this network are con-
nected through a variety of semantic relations (e.g.
is a, member of, nationality) derived from category
names and infoboxes. The version of WikiNet used
to the PMW.
had 3,707,718 nodes and 49,931,266 relation in-
stances of 494 types, and is freely available3.
WikiNet is used here as a concept inventory,
and its links and structure to generalize more spe-
cific concepts identified in texts to general concepts.
The fact that nodes in WikiNet correspond to arti-
cles/categories in Wikipedia is used to link article
texts in Wikipedia to general concepts, for the pur-
pose of computing various probability scores (de-
tailed in Section 3.4).
3.2 Selectional preferences and abstractions
To compute selectional preferences we use the set of
English Wikipedia articles, which describe specific
concepts. Wikipedia contributors are encouraged to
insert hyperlinks, which link important terms in an
article to the corresponding articles. A hyperlink
consists of two parts, the actual link (i.e. a URL)
and a phrase to appear in the text. Hyperlinks then
constitute a bridge from the textual level to the con-
ceptual level without the need for word sense dis-
ambiguation. We exploit these links to gather con-
cept arguments for verbs and adjectives, and gen-
eralize these using the concept network built from
Wikipedia.
The corpus of Wikipedia articles was first en-
riched with hyperlinks, making the ?one sense per
3http://www.h-its.org/english/research/
nlp/download/wikinet.php
186
Algorithm 1 computeSelPrefs(G,WkN)
Input: G ? grammatical relation triples
WkN ? WikiNet
M ? maximum number of generalization steps
Output: ?
? = {}
for all (w, r) such that (c, r, w) ? G do
S = {(c, f)|f is the frequency of (c, r, w) in G}
?w,r = S
mdl = MDL(?w,r,S)
for all i = 1,M do
?? = abstract(S,WkN)
mdl?? = MDL(??,S)
if mdl?? < mdl then
?w,r = ??
? = {?w,r} ? ?
return ?
Algorithm 2 MDL(?,S)
Input: ? = {(c, f)} ? a scored list of concepts
S ? the set of observations (concept collocates)
Output: MDL(?,S)
?? =< f1, ..., fn >; (ci, fi) ? ?
remove {(c, f) ? ?|f = 1} // parameter description
length :
L(??|?) = |?|?12 ? log(|S|) // data description length :
for all (c, f) ? ? do
L(S|?, ??) = L(S|?, ??) + f ? log( fhyponyms(c)?|?| )
return L(??|?)? L(S|?, ??)
Algorithm 3 abstract(S,WkN)
Input: S = {(c, f)|(w,R, c) ? G}
WkN ? WikiNet
Output: S ?
S ? = {}
for c|(c, ) ? S do
while c has only one is a link do
c = c?, (c, is a, c?) ?WkN
C = {(c?, c)|(c, is a, c?) ?WkN}
for (c?, c) ? C do
if (c?, f ?) ? S ? then
replace (c?, f ?) with (c?, f ? + f|C| ), (c, f) ? S
in S ?
else
S ?? = {(c?, f)}, (c, f) ? S
// Remove hyponyms.
for all {(c, c?) ? S ?|(c?, is a, c) ?WkN} do
// update frequency f of c
fc = fc + fc? , f ? S
delete c?
return S ?
discourse? assumption ? a phrase that appears as-
sociated with a hyperlink once in the article body
will be associated with the same hyperlink through-
out the article (this applies to the article title as well,
which is not hyperlinked in the article itself). This
new version of the corpus was then split into sen-
tences, and those without hyperlinks were removed.
The remaining 18 million sentences were parsed
with a parallelized version of Ensemble4 (Surdeanu
and Manning, 2010), and we extracted G, the set of
all grammatical relations of the type (verb, depen-
dency, hyperlink) and (adjective, dependency, hy-
perlink), with the hyperlinks resolved to their cor-
responding node (concept) in the network ( |G| =
1,578,413 triples). For each verb and adjective in the
extracted collocations, and for each of their depen-
dency relations, their collocates were generalized in
the network defined by the hypernym/hyponym re-
lations in WikiNet following a method similar to the
Minimum Description Length principle (Li and Abe,
1998).
Essentially, we aimed to determine a small set of
(more general) concepts that describe the set of col-
locates for a word w and grammatical relation r.
Starting from the concept collocates gathered, we
go upwards following WikiNet?s is a links, and for
each node found that covers at least N concept col-
locates (N is a parameter, N=2 in the experiments
presented here), the MDL score of the node is com-
puted (Algorithm 2). We place a limit M on the
number of upward steps in the hierarchy (M=3 in
our experiments). The disjoint set of nodes that has
the lowest overall MDL score is chosen (?), and for
each node in this cut (which we call abstraction),
we compute the selectional preference score, based
on the number of concepts it dominates.
As an example, for the verb defeat, the corpus
leads to collocations such as5:
defeat
nsubj
Earle Page (10357) ? 8, Manuela Maleeva
(1092361) ? 7, New York Yankees
(10128601) ? 5, Tommy Haas (1118005)
? 5, . . .
obj
4http://www.surdeanu.name/mihai/
ensemble/
5The format is:
Article name (Article Id) ? frequency.
187
New York Yankees (10128601) ? 9, Oak-
land Athletics (11641124) ? 6, Phoenix
Suns (11309373) ? 4, Jason Suttie
(10080653) ? 3, Ravana (100234) ? 3, . . .
Determining abstractions and selectional prefer-
ences leads to the following information6:
defeat
nsubj
Martial artists (118977183) ? 0.5, Person
(219599) ? 0.3518, Interest (146738) ?
0.037, . . .
obj
Video games (9570081) ? 0.25, British
games (24489088) ? 0.25, Person (219599)
? 0.1445, Interest (146738) ? 0.1341, . . .
3.3 Linking the PMW to the concept network
In our environment, linking the PMW to the con-
cept network is equivalent to finding its correspond-
ing concept in our ontology, WikiNet. We see this
corresponding concept as the literal reading of the
PMW. Doing so is a non-trivial task (see the Cross-
Lingual Link Discovery task at NTCIR-9 (Tang et
al., 2011) and the Cross-Lingual Entity Linking task
? part of the Knowledge Base Population track ? at
TAC 20117). In our particular setting, where we use
the metonymy data from SemEval 2007, the domain
of the PMW is well defined: locations and compa-
nies, respectively. Using these constraints, finding
the corresponding Wikipedia articles is much sim-
plified, by using the category hierarchy and con-
straining the concepts to fall under the Geography
and Companies categories respectively. When mul-
tiple options are present, we find instead a matching
disambiguation page. In this case we pick the article
that is listed first on this disambiguation page. On
a manually checked random sample, the accuracy of
the approach was 100% (on a sample of 100 PMWs).
3.4 Scoring conceptual relations with local and
global context
We work under the assumption that the concept cor-
responding to the PMW is related to the possible in-
terpretations through a semantic relation, in particu-
lar one that is captured in the concept network. After
6The format is:
Concept name (Concept Id) ? selectional preference score.
7http://nlp.cs.qc.cuny.edu/kbp/2011/
countries : Administrator of, Architect of,
Based in, Built in, Continent, ...
companies : Association, Brand, Company, Dis-
tributed by, Executive of, ...
Table 1: Example conceptual relations
establishing the connection to the resource by link-
ing the PMW to the concept cPMW corresponding to
its literal interpretation (see Section 3.3), we extract
the relations in which it is involved (Ri, i = 1, k),
and the concepts it is connected to through these re-
lations (CRi = {cx|(cPMWRicx)}). Table 1 shows
examples of conceptual relations extracted for com-
panies and countries.
We are interested in computing the likelihood of
a conceptual relation being the correct interpreta-
tion of a PMW, given its local and global context
p(Ri|Cont, w, r).
3.4.1 The local context
The local context considered in this work are all
grammatically related verbs and adjectives w and
their associated grammatical relation r. The gram-
matical analysis (see Section 3.2) provides the set of
abstractions corresponding to the grammatically re-
lated word w and grammatical relation r: Aj , j =
1, n. Remember that these are local context con-
straints on the interpretation of the PMW.
Through the knowledge resource used we can es-
tablish and quantify connections between each cx
and Aj , and thus between eachRi and Aj :
p(Ri|Aj) =
?
x?CRi
p(cx|Aj)(3)
where p(cx|Aj) is the probability of concept cx un-
der abstraction Aj , which is computed based on the
semantic relations in WikiNet:
p(cx|Aj) =
?
H
?
hi?H
p(hi|hi+1)
whereH is in turn each path from cx toAj following
is a links in WikiNet, starting with cx (i.e. h0 = cx)
and ending in Aj . p(hi|hi+1) is the probability of
the child node hi given its ancestor hi+1. Within this
work we assume a uniform probability distribution
in each node:
188
p(hi|hi+1) =
1
|descendants(hi+1)|
Through this, it is straightforward that
?
cx p(cx|Aj) = 1 when cx ranges over all
concepts subsumed by Aj , and is thus a valid
probability distribution.
3.4.2 The global context
The abstractions obtained before are concepts.
We extract all nodes in the network subsumed
by these concepts, and their corresponding articles
in Wikipedia (if they have one). This produces
?abstraction-specific? article sets, based on which
we compute the probability of the global context of
a PMW for each abstraction. We are interested in
the probability of an abstraction, given the context
and the word w and grammatical relation r, which
we compute as:
p(Aj |Cont, w, r) =
p(Cont|Aj , w, r) ? p(Aj , w, r)
p(Cont, w, r)
which, considering that p(Cont, w, r) is the same
for a given context, we approximate as
p(Aj |Cont) ? p(Cont|Aj) ? p(Aj , w, r)
p(Aj , w, r) = p(Aj |w, r)?p(w, r), and we approxi-
mate it through the computed selectional preference
p(Aj |w, r), since p(w, r) is constant for a given ex-
ample to analyze.
p(Cont|Aj , w, r) =
n?
j=1
p(Cont|Aj)p(Aj |w, r)
=
n?
j=1
(
m?
l=1
p(wl|Aj))p(Aj |w, r)
where Cont is the global context consisting of m
words wl, l = 1,m.8
8The global context therefore could be all words in a text
or all words in a sentence or any other token-based definition
in our framework. As the SemEval 2007 data gives metonymic
examples in a three-sentence context we use all the words in the
3 sentences as our global context.
p(wl|Aj) =
count(wl,Aj)
|Aj |
where Aj is the set of articles subsumed by abstrac-
tion Aj , and count(wl,Aj) is the number of times
word wl appears in the article collection Aj .
3.4.3 Putting it all together
This enables us now to compute p(Ri|Cont, w, r)
based on the formulas 3, 4:
p(Ri|Cont, w, r) =
n?
j=1
(p(Ri|Aj)?p(Aj |Cont, w, r))
4 Experiments
The computed probabilities for each conceptual re-
lation (= potential readings) of the PMW in the con-
cept network can be used as features in a supervised
framework or directly as an unsupervised prediction,
returning the most likely conceptual relation given
the context as the required reading.
Although the latter is our ultimate goal, to allow
comparison with related work from the metonymy
resolution task (Task 8) at SemEval 2007, we first
investigate the supervised set-up. We then simulate
the unsupervised setting in Section 4.3.
4.1 Data
We use the data from the metonymy resolution task
(Task 8) at SemEval 2007. It consists of training and
test data for country and company names which are
potentially metonymic. Table 2 shows the statistics
of the data, and the possible interpretations for the
PMWs. The training-test division was achieved ran-
domly so that the test data can have metonymic read-
ings for which no training data exists, showing again
the limitations of a supervised approach of prespec-
ified readings.
Grammatical features The features used by Nis-
sim and Markert (2005), and commonly used for
the supervised classification of metonymy readings
(Markert and Nissim, 2009):
? grammatical role of PMW (subj, obj, ...);
? lemmatized head/modifier of PMW (announce,
say, ...);
189
reading train test
locations 925 908
literal 737 721
mixed 15 20
othermet 9 11
obj-for-name 0 4
obj-for-representation 0 0
place-for-people 161 141
place-for-event 3 10
place-for-product 0 1
organizations 1090 842
literal 690 520
mixed 59 60
othermet 14 8
obj-for-name 8 6
obj-for-representation 1 0
org-for-members 220 161
org-for-event 2 1
org-for-product 74 67
org-for-facility 15 16
org-for-index 7 3
Table 2: Statistics for the Task 8 data
? determiner of PMW (def, indef, bare, demonst,
other, ...);
? grammatical number of PMW (sg, pl);
? number of grammatical roles in which the
PMW appears in its current context;
? number of words in PMW.
All these features can be extracted from the gram-
matically annotated and POS tagged data provided
by the organizers.
The annotations provided are dependency rela-
tions, many of which contain a preposition as an ar-
gument (e.g. (to, pp, UK) from the example ... the
visit to the UK of ...). Such relations are not infor-
mative, but together with the head that dominates the
prepositional complement (e.g. visit to) they may be.
Because of this, we process the provided annotations
and add wherever possible to the simple prepositions
the head of their subsuming constituent. This would
change the above mentioned dependency to (visit,
prep-to, UK).
Semantic relations as features To evaluate the
proposed approach we use the PMW?s conceptual
relations as features. The feature values are the
p(Ri|Cont, w, r) scores.
For the ?countries? portion of the data this adds
109 semantic relation features, and for companies
29 features. Table 1 showed examples of these new
features.
4.2 Supervised learning
We use the SMO classifier in the WEKA machine
learning toolkit (Witten and Frank, 2000) with its
standard settings, training on the SemEval 2007
(Task 8) training set.
Table 3 shows the results of various configura-
tions on the test data, in comparison with a most
frequent reading baseline (assigning literal to all
PMWs) as well as a system M&N that shows the re-
sults computed using only the features proposed by
Nissim and Markert (2005). In addition, we com-
pare to the best results9 at SemEval 2007 (SEmax)
and Nastase and Strube (2009) (N09). Nastase and
Strube (2009) added WordNet supersenses as fea-
tures, and their values are selectional preferences
computed with reference to WordNet. These are
similar to our abstractions, which in our approach
serve to link the local and the global context to the
ontological relations, but do not appear as features.
Our system SP shows the results obtained us-
ing the M&N features plus the conceptual relation
features conditioned on both local and global con-
text whereas SPlocal and SPglobal use conceptual
relations conditioned on local (p(Aj |Cont, w, r) ?
p(Aj |w, r)) or global context (p(Aj |Cont, w, r) ?
p(Aj |Cont) =
?n
j=1(
?m
l=1 p(wl|Aj))) only.
While the differences in overall accuracies are
small, there are significant differences in classifying
individual classes, as shown in Tables 4 ? 510, where
the distrib. column shows the class distribution in
the test data. It is interesting to note that, in our set-
ting, the global context is more useful than the local
9We show the best result for each category, not necessarily
from the overall best performing system. This holds for Tables
4 and 5 as well.
10The detailed results for previous approaches are reproduced
from (Nastase and Strube, 2009). We include only the classes
that have a non-zero F-score for at least one of the presented
approaches.
190
task ? method? baseline SEmax N09 M&N SP SPlocal SPglobal SPunsup
LOCATION-COARSE 79.4 85.2 86.1 83.4 85.8 83.0 85.0 81.6
LOCATION-MEDIUM 79.4 84.8 85.9 82.3 85.7 82.7 84.6 81.5
LOCATION-FINE 79.4 84.4 85.0 81.3 84.7 82.1 83.8 81.0
ORGANIZATION-COARSE 61.8 76.7 74.9 74.0 77.0 76.4 76.8 67.8
ORGANIZATION-MEDIUM 61.8 73.3 72.4 69.4 74.6 74.0 74.4 66.3
ORGANIZATION-FINE 61.8 72.8 71.0 68.5 72.8 71.9 72.7 65.3
Table 3: Accuracy scores
task ? method? distrib. SEmax N09 SP
LOCATION-COARSE
literal 79.4 91.2 91.6 91.4
non-literal 20.6 57.6 59.1 58.5
LOCATION-MEDIUM
literal 79.4 91.2 91.6 91.4
metonymic 18.4 58.0 61.5 61.6
mixed 2.2 8.3 16 9.1
LOCATION-FINE
literal 79.4 91.2 91.6 91.4
place-for-people 15.5 58.9 61.7 61.1
place-for-event 1.1 16.7 0 0
obj-for-name 0.4 66.7 0 0
mixed 2.2 8.3 16 9.1
Table 4: Fine-grained results for each classification task
for countries (F-scores)
one for resolving metonymies. Combining local and
global evidence improves over both, indicating that
the information they provide is not redundant.
For companies the difference is small in terms of
accuracy, but in classification of individual classes
the difference in performance is higher, but because
of the small data size not statistically significant.
Countries in WikiNet have a high number of sur-
rounding relations, because they are used as cat-
egorization criteria for professionals, for example,
which generates fine-grained relations such as Ad-
ministrator of, Ambassador of, Chemist of .... Such
a fine grained distinction between different profes-
sions for people in a country is not necessary, or in-
deed, desirable, for the metonymy resolution task.
The results show that despite this shortcoming, the
results are on par with the state-of-the-art, but in fu-
ture work we plan to explore the task of relation gen-
eralization and its impact on the current task.
task ? method? distrib. SEmax N09 SP
ORGANIZATION-COARSE
literal 61.8 82.5 81.4 82.7
non-literal 38.2 65.2 61.6 65.5
ORGANIZATION-MEDIUM
literal 61.8 82.5 81.4 82.7
metonymic 31.0 60.4 58.7 63.1
mixed 7.2 30.8 26.8 27.4
ORGANIZATION-FINE
literal 61.8 82.6 81.4 82.7
org-for-members 19.1 63.0 59.7 66.5
org-for-product 8.0 50.0 44.4 35.0
org-for-facility 2.0 22.2 36.3 45.5
org-for-name 0.7 80.0 58.8 44.4
mixed 7.2 34.3 27.1 27.4
Table 5: Fine-grained results for each classification task
for companies (F-scores)
4.3 Simulating unsupervised metonymy
resolution
In an unsupervised metonymy resolution approach,
we would assign as interpretation the conceptual re-
lation whose probability given the PMW, global and
local contexts is highest. To simulate then the un-
supervised metonymy resolution task, we make the
relation features (used in the supervised approach)
binary, where for each instance the relation that has
highest probability has the value 1, the others 0.
Using only the relation features simulates an un-
supervised approach ? this set-up learns a map-
ping between the relations used as features and
the metonymy classes in the data used. Column
SPUnsup in Table 3 shows the results obtained in
this configuration. As expected the results are lower,
but still close to the supervised method when using
only grammatical features (M&N) for the location
191
setting. The results also significantly beat the base-
line (apart from the Location-Fine setting). One fea-
ture that contributes greatly to the results, especially
for the company semantic class, is the grammatical
role of the PMW, but we could not incorporate this
in the unsupervised setting.
The results in the simulated unsupervised set-
ting indicate that relations are a viable substitute
for manually provided classes in an unsupervised
framework, while leaving space for improvement.
5 Conclusion
We have explored the usage of local and global con-
text for the task of metonymy resolution in a prob-
abilistic framework. The global context has been
rarely used for the task of determining the intended
reading of a potentially metonymic word (PMW)
in context. We rely on automatically computed se-
lectional preferences, extracted from a corpus of
Wikipedia articles, and generalized based on a con-
cept network also extracted from Wikipedia. De-
spite relying on automatically derived resources, the
presented approach produces results on-a-par with
current state-of-the-art systems. The method de-
scribed here is also a step towards the unsupervised
resolution of metonymic words in context, by tak-
ing into account knowledge about the concept cor-
responding to the literal interpretation of the PMW,
and its relations to other concepts. This frame-
work would also allow for exploring the metonymy
resolution phenomena in various languages (since
Wikipedia and WikiNet are multilingual), and inves-
tigate whether the same relations apply or different
languages have different metonymic patterns.
Acknowledgments
Katja Markert is the recipient of an Alexander-von-
Humboldt Fellowship for Experienced Researchers.
This work was financially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853) and the
Klaus Tschirra Foundation. We thank the review-
ers for the helpful comments, and Helga Kra?mer-
Houska for additional support for conference partic-
ipation.
References
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2007. Building textual entailment specialized data
sets: A methodology for isolating linguistic phenom-
ena relevant to inference. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, La Valetta, Malta, 17?23 May 2010.
Caroline Brun, Maud Ehrmann, and Guillaume Jacquet.
2007. XRCE-M: A hybrid system for named en-
tity metonymy resolution. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-1), Prague, Czech Republic, 23?24 June
2007, pages 488?491.
Richa?rd Farkas, Eszter Simon, Gyo?rgy Szarvas, and
Da?niel Varga. 2007. GYDER: Maxent metonymy res-
olution. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23?24 June 2007, pages 161?164.
Dan C. Fass. 1991. met?: A method for discriminating
metonomy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Sanda M. Harabagiu. 1998. Deriving metonymic co-
ercions from WordNet. In Proceedings of the Work-
shop on the Usage of WordNet in Natural Language
Systems, Montral, Quebec, Canada, 16 August, 1998,
pages 142?148.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1-2):69?142.
Maria Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan, 7?12 July 2003,
pages 545?552.
Johannes Leveling and Sven Hartrumpf. 2008. On
metonymy recognition for geographic information re-
trieval. International Journal of Geographical Infor-
mation Science, 22(3):289?299.
Johannes Leveling. 2007. FUH (FernUniversita?t in Ha-
gen): Metonymy recognition using different kinds of
context for a memory-based learner. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-1), Prague, Czech Republic, 23?24
June 2007, pages 153?156.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Katja Markert and Udo Hahn. 2002. Metonymies in dis-
course. Artificial Intelligence, 135(1/2):145?198.
Katja Markert and Malvina Nissim. 2009. Data and
models for metonymy resolution. Language Re-
sources and Evaluation, 43(2):123?138.
192
Vivi Nastase and Michael Strube. 2009. Combining
collocations, lexical and encyclopedic knowledge for
metonymy resolution. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, Singapore, 6-7 August 2009, pages
910?918.
Vivi Nastase, Michael Strube, Benjamin Bo?rschinger,
Ca?cilia Zirn, and Anas Elghafari. 2010. WikiNet:
A very large scale multi-lingual concept network.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, La Valetta,
Malta, 17?23 May 2010.
Cristina Nicolae, Gabriel Nicolae, and Sanda Harabagiu.
2007. UTD-HLT-CG: Semantic architecture for
metonymy resolution and classification of nominal re-
lations. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-1), Prague,
Czech Republic, 23?24 June 2007, pages 454?459.
Malvina Nissim and Katja Markert. 2005. Learning to
buy a Renault and talk to BMW: A supervised ap-
proach to conventional metonymy. In Proceedings of
the 6th International Workshop on Computational Se-
mantics, Tilburg, Netherlands, January 12-14, 2005.
Geoffrey Nunberg. 1995. Transfers of meaning. Journal
of Semantics, 12(1):109?132.
Thierry Poibeau. 2007. Up13: Knowledge-poor meth-
ods (sometimes) perform poorly. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations (SemEval-1), Prague, Czech Republic, 23?24
June 2007, pages 418?421.
James Pustejovsky. 1991. The generative lexicon. Com-
putational Linguistics, 17(4):209?241.
Kirk Roberts and Sanda M. Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detection
of argument coercions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, UK, 27-29 July 2011,
pages 980?990.
Ekaterina Shutova. 2009. Sense-based interpretation of
logical metonymy using a statistical method. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference on
Natural Language Processing, Singapore, 2?7 August
2009, pages 1?9.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble Models for Dependency Parsing: Cheap and
Good? In Proceedings of Human Language Tech-
nologies 2010: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, Cal., 2?4 June 2010, pages 649?
652.
Ling-Xiang Tang, Shlomo Geva, Andrew Trotman, Yue
Xu, and Kelly Y. Itakura. 2011. Overview of the
NTCIR-9 crosslink task: Cross-lingual link discovery.
In Proceedings of the 9th NII Test Collection for IR
Systems Workshop meeting ? NTCIR-9 Tokyo, Japan,
6?9 December 2011.
Masao Utiyama, Masaki Murata, and Hitoshi Isahara.
2000. A statistical approach to the processing
of metonymy. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics,
Saarbru?cken, Germany, 31 July ? 4 August 2000,
pages 885?891.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Diego, CA.
193
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 814?820,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Cascading Collective Classification for Bridging Anaphora Recognition
Using a Rich Linguistic Feature Set
Yufang Hou1, Katja Markert2, Michael Strube1
1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
Recognizing bridging anaphora is difficult due
to the wide variation within the phenomenon,
the resulting lack of easily identifiable surface
markers and their relative rarity. We develop
linguistically motivated discourse structure,
lexico-semantic and genericity detection fea-
tures and integrate these into a cascaded mi-
nority preference algorithm that models bridg-
ing recognition as a subtask of learning fine-
grained information status (IS). We substan-
tially improve bridging recognition without
impairing performance on other IS classes.
1 Introduction
In bridging or associative anaphora (Clark, 1975;
Prince, 1981; Gundel et al, 1993), the antecedent
and anaphor are not coreferent but are linked via a
variety of contiguity relations.1 In Example 1, the
phrases a resident, the stairs and the lobby are bridg-
ing anaphors with the antecedent One building.2
(1) One building was upgraded to red status while peo-
ple were taking things out, and a resident called up the
stairs to his girlfriend, telling her to keep sending things
down to the lobby.
Bridging is an important problem as it affects lin-
guistic theory and applications alike. For exam-
ple, without bridging resolution, entity coherence
between the first and second coordinated clause in
1We exclude comparative anaphora where anaphor and an-
tecedent are in a similarity/exclusion relation, indicated by ana-
phor modifiers such as other or similar (Modjeska et al, 2003).
2Examples are from OntoNotes (Weischedel et al, 2011).
Bridging anaphora are set in boldface; antecedents in italics.
Example 1 cannot be established. This is a prob-
lem both for coherence theories such as Centering
(Grosz et al, 1995) (where bridging is therefore in-
corporated as an indirect realization of previous en-
tities) as well as applications relying on entity co-
herence modelling, such as readability assessment
or sentence ordering (Barzilay and Lapata, 2008).
Full bridging resolution needs (i) recognition that
a bridging anaphor is present and (ii) identification
of the antecedent and contiguity relation. In re-
cent work, these two tasks have been tackled sep-
arately, with bridging recognition handled as part of
information status (IS) classification (Markert et al,
2012; Cahill and Riester, 2012; Rahman and Ng,
2012). Each mention in a text gets assigned one IS
class that describes its accessibility to the reader at
a given point in a text, bridging being one possible
class. We stay within this framework.
Bridging recognition is a difficult task, so that we
had to report very low results on this IS class in pre-
vious work (Markert et al, 2012). This is due to the
phenomenon?s variety, leading to a lack of clear sur-
face features for recognition. Instead, we formulate
in this paper novel discourse structure and lexico-
semantic features as well as features that distinguish
bridging from generics (see Section 3). In addition,
making up between 5% and 20% of definite descrip-
tions (Gardent and Manue?lian, 2005; Caselli and
Prodanof, 2006) and around 6% of all NPs (Mark-
ert et al, 2012), bridging is still less frequent than
many other IS classes and recognition of minority
classes is well known to be more difficult. We there-
fore use a cascaded classification algorithm to ad-
dress this problem (Omuya et al, 2013).
814
2 Related Work
Most bridging research concentrates on antecedent
selection only (Poesio and Vieira, 1998; Poesio et
al., 2004a; Markert et al, 2003; Lassalle and De-
nis, 2011; Hou et al, 2013), assuming that bridg-
ing recognition has already been performed. Previ-
ous work on recognition is either limited to definite
NPs based on heuristics evaluated on small datasets
(Hahn et al, 1996; Vieira and Poesio, 2000), or
models it as a subtask of learning fine-grained IS
(Rahman and Ng, 2012; Markert et al, 2012; Cahill
and Riester, 2012). Results within this latter frame-
work for bridging have been mixed: We reported in
Markert et al (2012) low results for bridging in writ-
ten news text whereas Rahman and Ng (2012) re-
port high results for the four subcategories of bridg-
ing annotated in the Switchboard dialogue corpus by
Nissim et al (2004). We believe this discrepancy to
be due to differences in corpus size and genre as well
as in bridging definition. Bridging in Switchboard
includes non-anaphoric, syntactically linked part-of
and set-member relationships (such as the building?s
lobby), as well as comparative anaphora, the latter
being marked by surface indicators such as other,
another etc. Both types are much easier to identify
than anaphoric bridging cases.3 In addition, many
non-anaphoric lexical cohesion cases have been an-
notated as bridging in Switchbard as well.
We also separate bridging recognition and ante-
cedent selection. One could argue that a joint model
is more attractive as potential antecedents such as
building ?trigger? subsequent bridging cases such as
stairs (Example 1). However, bridging can be indi-
cated by referential patterns without world knowl-
edge about the anaphor/antecedent NPs, as the non-
sense example 2 shows: the wug is clearly a bridging
anaphor although we do not know the antecedent.4
(2) The blicket couldn?t be connected to the dax. The
wug failed.
Similarly, Clark (1975) distinguishes between
bridging via necessary, probable and inducible
parts/roles and argues that only in the first and
maybe the second case the antecedent triggers the
3See also the high results for our specific category for com-
parative anaphora (Markert et al, 2012).
4We thank an anonymous reviewer for pointing this out.
bridging anaphor in the sense that we already spon-
taneously think of the anaphor when we read the an-
tecedent. Also, bridging recognition on its own can
be valuable for applications: for example, prosody is
influenced by IS status without needing antecedent
knowledge (Baumann and Riester, 2013).
3 Characterizing Bridging Anaphora for
Automatic Recognition
3.1 Properties of bridging anaphora
Bridging anaphors are rarely marked by surface fea-
tures. Indeed, even the common practice (Vieira and
Poesio, 2000; Lassalle and Denis, 2011; Cahill and
Riester, 2012) to limit bridging to definite NPs does
not seem to be correct: We report in previous work
(Hou et al, 2013) that less than 40% of the bridg-
ing anaphora in our corpus are definites. Instead,
bridging is diverse with regard to syntactic form
and function: bridging anaphora can be definite NPs
(Examples 4 and 6), indefinite NPs (Example 5) or
bare NPs (Examples 3, 8 and 9). The only frequent
syntactic property shared is that bridging NPs tend
to have a simple internal structure with regards to
modification. Bridging is also easily confused with
generics: friends is used as bridging anaphor in Ex-
ample 9 but generically in Example 10.
(3) . . . meat . . . The Communists froze prices instead.
(4) . . . the fund?s building . . . The budget was only
$400,000.
(5) . . . employees . . . A food caterer stashed stones in the
false bottom of a milk pail.
(6) . . . his truck . . . The farmer at the next truck shouts,
?Wheat!?
(7) . . . the firms . . . Crime was the reason that 26% re-
ported difficulty recruiting personnel and that 19% said
they were considering moving.
(8) . . . the company . . . His father was chairman and
chief executive until his death in an accident five years
ago.
(9) . . . Josephine Baker . . . Friends pitched in.
(10) Friends are part of the glue that holds life and faith
together.
Bridging anaphora can have almost limitless varia-
tion. However, we observe that bridging anaphors
are often licensed because of discourse structure
815
Markert et al (2012) local feature set
f1 FullPrevMention (b) f2 FullPreMentionTime (n)
f3 PartialPreMention (b) f4 ContentWordPreMention (b)
f5 Determiner (n) f6 NPtype (n)
f7 NPlength (int) f8 GrammaticalRole (n)
f9 NPNumber (n) f10 PreModByCompMarker (b)
f11 SemanticClass (n)
Markert et al (2012) relational feature set
f12 HasChild (r) f13 Precedes (r)
Table 1: Markert et al?s (2012) feature set, b indi-
cates binary, n nominal, r relational features.
and/or lexical or world knowledge. With regard to
discourse structure, Grosz et al (1995) observe that
bridging is often needed to establish entity coher-
ence between two adjacent sentences (Examples 1,
2, 4, 5, 6, 7 and 9). With regard to lexical and world
knowledge, relational noun phrases (Examples 3, 4,
8 and 9), building parts (Example 1), set member-
ship elements (Example 7), or, more rarely, tem-
poral/spatial modification (Example 6) may favor a
bridging reading. Motivated by these observations,
we develop discourse structure and lexico-semantic
features indicating bridging anaphora as well as fea-
tures designed to separate genericity from bridging.
3.2 Features
In Markert et al (2012) we classify eight fine-
grained IS categories for NPs in written text: old,
new and 6 mediated categories (syntactic, world-
Knowledge, bridging, comparative, aggregate and
function). This feature set (Table 1, f1-f13) works
well to identify old, new and several mediated cate-
gories. However, it fails to recognize most bridging
anaphora which we try to remedy in this work by
including more diverse features.
Discourse structure features (Table 2, f1-f3).
Bridging occurs frequently in sentences where oth-
erwise there would no entity coherence to previous
sentences/clauses (see Grosz et al (1995) and Poe-
sio et al (2004b) for discussions about bridging, en-
tity coherence and centering transitions in the Cen-
tering framework). This is especially true for topic
NPs (Halliday and Hasan, 1976) in such sentences.
We follow these insights by identifying coherence
gap sentences (see Examples 1, 4, 5, 6, 7, 9 and also
2): a sentence has a coherence gap (f1) if it has none
new local features for bridging
discourse f1 IsCoherenceGap (b)
structure f2 IsSentFirstMention (b)
f3 IsDocFirstMention (b)
semantics f4 IsWordNetRelationalNoun (b)
f5 IsInquirerRoleNoun (b)
f6 IsBuildingPart (b)
f7 IsSetElement (b)
f8 PreModSpatialTemporal (b)
f9 IsYear (b)
f10 PreModifiedByCountry (b)
generic f11 AppearInIfClause (b)
NP f12 VerbPosTag (l)
features f13 IsFrequentGenericNP (b)
f14 WorldKnowledgeNP (l)
f15 PreModByGeneralQuantifier (b)
other features f16 Unigrams (l)
f17 BridgingHeadNP (l)
f18 HasChildNP (b)
new features for other mediated categories
aggregate f19 HasChildCoordination (r)
function f20 DependOnChangeVerb (b)
worldKnowledge f21 IsFrequentProperName (b)
Table 2: New feature set, l indicates lexical features.
of the following three coherence elements: (1) entity
coreference to previous sentences, as approximated
via string match or presence of pronouns, (2) com-
parative anaphora approximated by mentions modi-
fied via a small set of comparative markers (see also
Table 1, f10 PreModByCompMarker), or (3) proper
names. We approximate the topic of a sentence via
the first mention (f2).
f3 models that bridging anaphors do not appear
at the beginning of a text.
Semantic features (Table 2, f4-f10). In contrast
to generic patterns, our semantic features capture
lexical properties of nouns that make them more
likely to be the head of a bridging NP. We create
f4-f8 to capture four kinds of bridging anaphora.
Lo?bner (1985) distinguishes between relational
nouns that take on at least one obligatory semantic
role (such as friend) and sortal nouns. It is likely that
relational nouns are more frequently used as bridg-
ing than sortal nouns (see Examples 3, 4, 8 and 9).
We extract a list containing around 4,000 relational
nouns from WordNet and a list containing around
500 nouns that specify professional roles from the
General Inquirer lexicon (Stone et al, 1966), then
determine whether the NP head appears in these lists
816
or not (f4 and f5). The obligatory semantic role for
a relational noun can of course also be filled NP in-
ternally instead of anaphorically and we use the fea-
tures f10 (for instances such as the Egyptian presi-
dent) and f18 (for complex NPs that are likely to fill
needed roles NP internally) to address this.
Because part-of relations are typical bridging re-
lations (see Example 1 and Clark (1975)), we use f6
to determine whether the NP is a part of the building
or not, using again a list extracted from Inquirer.
f7 is used to identify set membership bridging
cases (see Example 7), by checking whether the
NP head is a number or indefinite pronoun (such as
none, one, some) or modified by each, one. How-
ever, not all numbers are bridging cases (such as
1976) and we use f9 to exclude such cases.
Lassalle and Denis (2011) note that some bridging
anaphors are indicated by spatial or temporal modi-
fications (see Example 6). We use f8 to detect this
by compiling 20 such adjectives from Inquirer.
Features to detect generic nouns (Table 2, f11-
f15). Generic NPs (Example 10) are easily con-
fused with bridging anaphora. Inspired by Reiter
and Frank (2010) who build on linguistic research,
we develop features (f11-f15) to exclude generics.
First, hypothetical entities are likely to refer to
generic entities (Mitchell et al, 2002), We approx-
imate this by determining whether the NP appears
in an if-clause (f11). Also the clause tense and
mood may play a role to decide genericity (Reiter
and Frank, 2010). This is often reflected by the main
verb of a clause, so we extract its POS tag (f12).
Some NPs are commonly used generically, such
as children, men, or the dollar. The ACE-2 corpus
(distinct from our corpus) contains generic annota-
tion . We collect all NPs from ACE-2 that are always
used generically (f13). We also try to learn NPs that
are uniquely identifiable without further description
or anaphoric links such as the sun or the pope. We
do this by extracting common nouns which are an-
notated as worldKnowledge from the training part of
our corpus5 and use these as lexical features (f14).
Finally, motivated by the ACE-2 annotation
guidelines, we identify six quantifiers that may in-
dicate genericity, such as all, no, neither (f15).
5This list varies for each run of our algorithm in 10-fold
cross validation.
Other features for bridging (Table 2, f16-f18).
Following Rahman and Ng (2012), we use unigrams
(f16). We also extract heads of bridging anaphors
from the training data as lexical features (f17) to
learn typical nouns used for bridging that we did not
cover in lexicon extraction (f4 to f6).
Feature f18 models that bridging anaphora most
often have a simple internal structure and usually do
not contain any other NPs.
Features for other IS categories (Table 2, f19-
f21). We propose three features to improve other
IS categories. In the relational feature f19, we sep-
arate coordination parent-child from other parent-
child relations to help with the class aggregate. f20
determines whether a number is the object of an in-
crease/decrease verb (using a list extracted from In-
quirer) and therefore likely to be the IS class func-
tion. Frequent proper names are more likely to be
hearer old and hence of the class worldKnowledge.
f21 extracts proper names that occur in at least 100
documents in the Tipster corpus to approximate this.
4 Experiments and Results
Experimental setup. We perform experiments on
the corpus provided in Markert et al (2012)6. It con-
sists of 50 texts taken from the WSJ portion of the
OntoNotes corpus (Weischedel et al, 2011) with al-
most 11,000 NPs annotated for information status
including 663 bridging NPs and their antecedents.
All experiments are performed via 10-fold cross-
validation on documents. We use gold standard
mentions and the OntoNotes named entity and syn-
tactic annotation layers for feature extraction.
Reimplemented baseline system (rbls). rbls uses
the same features as Markert et al (2012) (Table 1)
but replaces the local decision tree classifier with
LibSVM as we will need to include lexical features.
rbls + Table 2 feature set (rbls+newfeat). Based
on rbls, all the new features from Table 2 are added.
Cascading minority preference system (cmps).
Minority classes such as bridging suffer during stan-
dard multi-class classification. Inspired by Omuya
6http://www.h-its.org/nlp/download/
isnotes.php
817
collective cascade + collective
markert 12 rbls rbls+newfeat cmps cmps?newfeat
R P F R P F R P F R P F R P F
old 84.1 85.2 84.6 84.6 85.5 85.1 84.4 86.0 85.2 82.2 87.2 84.7 78.9 89.5 83.8
med/worldKnowledge 60.6 70.0 65.0 65.9 69.6 67.7 67.4 77.3 72.0 67.2 77.2 71.9 67.5 66.7 67.1
med/syntactic 75.7 80.1 77.9 77.8 81.2 79.4 82.2 81.9 82.0 81.6 82.5 82.0 73.9 81.7 77.6
med/aggregate 43.1 55.8 48.7 47.9 58.0 52.5 64.5 79.5 71.2 63.5 77.9 70.0 46.9 60.0 52.7
med/function 35.4 53.5 48.7 33.8 56.4 42.3 67.7 72.1 69.8 67.7 72.1 69.8 41.5 50.0 45.4
med/comparative 81.4 82.0 81.7 81.8 82.5 82.1 81.8 82.1 82.0 86.6 78.2 82.2 86.2 78.7 82.3
med/bridging 12.2 41.7 18.9 10.7 36.6 16.6 19.3 39.0 25.8 44.9 39.8 42.2 31.8 23.9 27.3
new 87.7 73.3 79.8 87.5 74.8 80.7 86.5 76.1 81.0 83.0 78.1 80.5 82.4 76.1 79.1
acc 76.8 77.6 78.9 78.6 75.0
Table 3: Experimental results
et al (2013), we develop a cascading minority pref-
erence system for fine-grained IS classification. For
the five minority classes (function, aggregate, com-
parative, bridging and worldKnowledge) that each
make up less than the expected 18 of the data set, we
develop five binary classifiers with LibSVM7 using
all features from Tables 1 and 2 and apply them in
order from rarest to more frequent category. When-
ever a minority classifier predicts true, this class is
assigned. When all minority classifiers say false, we
back off to the multiclass rbls+newfeat system.
cmps ? Table 2 feature set (cmps?newfeat). To
test the effect of using the minority preference sys-
tem without additional features, we employ a cmps
system with baseline features from Table 1 only.
Results and Discussion (Table 3). Our novel
features in rbls+newfeat show improvements for
worldKnowledge, aggregate and function as well as
bridging categories compared to both baseline sys-
tems, although the performance for bridging is still
low. In addition, the overall accuracy is significantly
better than the two baseline systems (at the level of
1% using McNemar?s test). Using the cascaded mi-
nority preference system cmps in addition improves
bridging results substantially while the performance
on other categories does not worsen. The algorithm
needs both our novel feature classes as well as cas-
caded modelling to achieve this improvement as the
comparison to cmps?newfeat shows: the latter low-
ers overall accuracy as it tends to overgenerate rare
7Parameter against data imbalance is set according to the
ratio between positive and negative instances in the training set.
classes (including bridging) with low precision if the
features are not strong enough. Our novel features
(addressing linguistic properties of bridging) and the
cascaded algorithm (addressing data sparseness) ap-
pear to be complementary.
To look at the impact of features in our best sys-
tem, we performed an ablation study. Lexical fea-
tures as well as semantic ones have the most impact.
Discourse structure and genericity information fea-
tures have less of an impact. We believe the latter to
be due to noise involved in extracting these features
(such as approximating coreference for the coher-
ence gap feature) as well as genericity recognition
still being in its infancy (Reiter and Frank, 2010).
5 Conclusions
This paper aims to recognize bridging anaphora in
written text. We develop discourse structure, lexico-
semantic and genericity features based on linguis-
tic intuition and corpus research. By using a cas-
cading minority preference system, we show that
our approach outperforms the bridging recognition
in Markert et al (2012) by a large margin without
impairing the performance on other IS classes.
Acknowledgements. Yufang Hou is funded by a PhD
scholarship from the Research Training Group Coher-
ence in Language Processing at Heidelberg University.
Katja Markert receives a Fellowship for Experienced Re-
searchers by the Alexander-von-Humboldt Foundation.
We thank HITS gGmbH for hosting Katja Markert and
funding the annotation.
818
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Stefan Baumann and Arndt Riester. 2013. Coreference,
lexical givenness and prosody in German. Lingua.
Accepted.
Aoife Cahill and Arndt Riester. 2012. Automatically ac-
quiring fine-grained information status distinctions in
German. In Proceedings of the SIGdial 2012 Confer-
ence: The 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, Seoul, Korea, 5?6
July 2012, pages 232?236.
Tommaso Caselli and Irina Prodanof. 2006. Annotat-
ing bridging anaphors in Italian: In search of reliabil-
ity. In Proceedings of the 5th International Conference
on Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006.
Herbert H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, Cambridge, Mass., June 1975, pages 169?
174.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Udo Hahn, Michael Strube, and Katja Markert. 1996.
Bridging textual ellipses. In Proceedings of the 16th
International Conference on Computational Linguis-
tics, Copenhagen, Denmark, 5?9 August 1996, vol-
ume 1, pages 496?501.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. London, U.K.: Longman.
Yufang Hou, Katja Markert, and Michael Strube. 2013.
Global inference for bridging anaphora resolution. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, Georgia, 9?14 June 2013, pages 907?917.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridging
resolution in French. In Proceedings of the 8th Dis-
course Anaphora and Anaphor Resolution Colloquium
(DAARC 2011), Faro, Algarve, Portugal, 6?7 October
2011, pages 35?46.
Sebastian Lo?bner. 1985. Definites. Journal of Seman-
tics, 4:279?326.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop on
the Computational Treatment of Anaphora. Budapest,
Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju Is-
land, Korea, 8?14 July 2012, pages 795?804.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstain, Lisa Ferro, and Beth
Sundheim. 2002. ACE-2 Version 1.0. LDC2003T11,
Philadelphia, Penn.: Linguistic Data Consortium.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, Sydney, Australia, 22?23 July 2006, pages
94?012.
Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen
Rambow. 2013. Improving the quality of minority
class identification in dialog act tagging. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Atlanta, Geor-
gia, 9?14 June 2013, pages 802?807.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004a. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio, Rosemary Stevenson, Barbara Di Euge-
nio, and Janet Hitzeman. 2004b. Centering: A para-
metric theory and its instantiations. Computational
Linguistics, 30(3). 309-363.
819
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Altaf Rahman and Vincent Ng. 2012. Learning the fine-
grained information status of discourse entities. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France, 23?27 April 2012, pages 798?
807.
Nils Reiter and Anette Frank. 2010. Identifying generic
noun phrases. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010, pages 40?49.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and Cambridge Computer Asso-
ciates. 1966. General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press, Cambridge,
Mass.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
820
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2082?2093,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Rule-Based System for Unrestricted Bridging Resolution:
Recognizing Bridging Anaphora and Finding Links to Antecedents
Yufang Hou
1
, Katja Markert
2
, Michael Strube
1
1
Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2
School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
Bridging resolution plays an important
role in establishing (local) entity coher-
ence. This paper proposes a rule-based
approach for the challenging task of unre-
stricted bridging resolution, where bridg-
ing anaphors are not limited to defi-
nite NPs and semantic relations between
anaphors and their antecedents are not re-
stricted to meronymic relations. The sys-
tem consists of eight rules which target
different relations based on linguistic in-
sights. Our rule-based system significantly
outperforms a reimplementation of a pre-
vious rule-based system (Vieira and Poe-
sio, 2000). Furthermore, it performs better
than a learning-based approach which has
access to the same knowledge resources
as the rule-based system. Additionally,
incorporating the rules and more features
into the learning-based system yields a mi-
nor improvement over the rule-based sys-
tem.
1 Introduction
Bridging resolution recovers the various non-
identity relations between anaphora and an-
tecedents. It plays an important role in establish-
ing entity coherence in a text. In Example 1, the
links between the bridging anaphors (The five as-
tronauts and touchdown) and the antecedent (The
space shuttle Atlantis) establish (local) entity co-
herence.
1
(1) The space shuttle Atlantis landed at a desert
air strip at Edwards Air Force Base, Calif.,
ending a five-day mission that dispatched
the Jupiter-bound Galileo space probe. The
1
Examples are from OntoNotes (Weischedel et al., 2011).
Bridging anaphora are typed in boldface; antecedents in ital-
ics.
five astronauts returned to Earth about three
hours early because high winds had been pre-
dicted at the landing site. Fog shrouded the
base before touchdown.
Bridging or associative anaphora has been
widely discussed in the linguistic literature (Clark,
1975; Prince, 1981; Gundel et al., 1993;
L?obner, 1998). Poesio and Vieira (1998) and
Bunescu (2003) include cases where antecedent
and anaphor are coreferent but do not share the
same head noun (different-head coreference). We
follow our previous work (Hou et al., 2013b) and
restrict bridging to non-coreferential cases. We
also exclude comparative anaphora (Modjeska et
al., 2003).
Bridging resolution includes two subtasks: (1)
recognizing bridging anaphors and (2) finding the
correct antecedent among candidates. In recent
empirical work, these two subtasks have been
tackled separately: (Markert et al., 2012; Cahill
and Riester, 2012; Rahman and Ng, 2012; Hou et
al., 2013a) handle bridging recognition as part of
information status (IS) classification, while (Poe-
sio et al., 1997; Poesio et al., 2004; Markert et
al., 2003; Lassalle and Denis, 2011; Hou et al.,
2013b) concentrate on antecedent selection only,
assuming that bridging recognition has already
been performed. One exception is Vieira and Poe-
sio (2000). They propose a rule-based system for
processing definite NPs. However, they include
different-head coreference into bridging. They re-
port results for the whole anaphora resolution but
do not report results for bridging resolution only.
Another exception is R?osiger and Teufel (2014).
They apply a coreference resolution system with
several additional semantic features to find bridg-
ing links in scientific text where bridging anaphors
are limited to definite NPs. They report prelim-
inary results using the CoNLL scorer. However,
we think the coreference resolution system and the
evaluation metric for coreference resolution are
2082
not suitable for bridging resolution since bridging
is not a set problem.
Another vein of research for bridging resolu-
tion focuses on formal semantics. Asher and Las-
carides (1998) and Cimiano (2006) model bridg-
ing by integrating discourse structure and seman-
tics from a formal semantics viewpoint. However,
the implementation of such a theoretical frame-
work is beyond the current capabilities of NLP
since it depends heavily on commonsense entail-
ment.
In this paper, we propose a rule-based system
for unrestricted bridging resolution. The system
consists of eight rules which we carefully design
based on linguistic intuitions, i.e., how the nature
of bridging is reflected by various lexical, syntac-
tic and semantic features. We evaluate our rule-
based system on a corpus where bridging is reli-
ably annotated. We find that our rule-based sys-
tem significantly outperforms a reimplementation
of a previous rule-based system (Vieira and Poe-
sio, 2000). We further notice that our rule-based
system performs better than a learning-based ap-
proach which has access to the same knowledge
resources as the rule-based system. Surprisingly,
incorporating the rules and more features into the
learning-based approach only yields a minor im-
provement over the rule-based system. We ob-
serve that diverse bridging relations and relatively
small-scale data for each type of relations make
generalization difficult for the learning-based ap-
proach. This work is ? to the best of our
knowledge ? the first system recognizing bridging
anaphora and finding links to antecedents for unre-
stricted phenomenon where bridging anaphors are
not limited to definite NPs and semantic relations
between anaphors and their antecedents are not re-
stricted to meronymic relations.
2 Data
All the data used throughout the paper come
from the ISNotes corpus
2
released by Hou et al.
(2013b). This corpus contains around 11,000 NPs
annotated for information status including 663
bridging NPs and their antecedents in 50 texts
taken from the WSJ portion of the OntoNotes cor-
pus (Weischedel et al., 2011). ISNotes is reli-
ably annotated for bridging: for bridging anaphor
recognition, ? is over 60 for all three possible an-
2
http://www.h-its.org/english/research/nlp/download/
isnotes.php
notator pairings (? is over 70 for two expert anno-
tators); for selecting bridging antecedents, agree-
ment is around 80% for all annotator pairings.
It is notable that bridging anaphors in ISNotes
are not limited to definite NPs as in previous work
(Poesio et al., 1997; Poesio et al., 2004; Lassalle
and Denis, 2011). Table 1 shows the bridging
Bridging Anaphors 663
Non-determiner 44.9%
Definite 38.5%
Indefinite 15.4%
Other-determiner 1.2%
Table 1: Bridging anaphora distribution w.r.t. de-
terminers in ISNotes.
anaphora distribution with regard to determiners in
ISNotes: only around 38% of bridging anaphors
are definite NPs (NPs modified by the); 15.4%
of bridging anaphors are modified by determiners
such as a, an or one which normally indicate in-
definite NPs. Most bridging anaphors (43%) are
not modified by any determiners, such as touch-
down in Example 1. A small fraction of bridging
anaphors (1.2%) are modified by other determin-
ers, such as demonstratives.
The semantic relations between anaphor and
antecedent in the corpus are extremely diverse:
only 14% of anaphors have a part-of/attribute-
of relation with the antecedent (see Example 2)
and only 7% of anaphors stand in a set relation-
ship to the antecedent (see Example 3). 79%
of anaphors have ?other? relations with their an-
tecedents (without further distinction), including
encyclopedic relations such as The space shut-
tle Atlantis-The five astronauts (see Example 1)
as well as context-specific relations such as The
space shuttle Atlantis-touchdown (Example 1).
(2) At age eight, Josephine Baker was sent by
her mother to a white women?s house to do
chores in exchange for meals and a place to
sleep ? a place in the basement with coal.
(3) This creates several problems. One is that
there are not enough police to satisfy small
businesses.
In ISNotes, bridging anaphora with distant an-
tecedents are common when the antecedent is the
global focus of a document. 29% of the anaphors
in the corpus have antecedents that are three or
more sentences away.
2083
Bridging resolution is an extremely challeng-
ing task in ISNotes. In contrast with surface clues
for coreference resolution, there are no clear sur-
face clues for bridging resolution. In Example 4,
the bridging anaphor low-interest disaster loans
associates to the antecedent the Carolinas and
Caribbean, whereas in Example 5 the NP loans is
a generic use. In Example 6, the bridging anaphor
The opening show associates to the antecedent
Mancuso FBI, whereas the NP the show is coref-
erent with its antecedent Mancuso FBI.
(4) The $2.85 billion measure comes on top of
$1.1 billion appropriated after Hugo stuck
the Carolinas and Caribbean last month, and
these totals don?t reflect the additional benefit
of low-interest disaster loans.
(5) Many states already have Enterprise Zones
and legislation that combines tax incentives,
loans, and grants to encourage investment in
depressed areas.
(6) Over the first few weeks, Mancuso FBI has
sprung straight from the headlines. The
opening show featured a secretary of defense
designate accused of womanizing (a la John
Tower).
. . .
Most of all though, the show is redeemed
by the character of Mancuso.
Our previous work on bridging resolution on
this corpus only focuses on its subtasks. In
Hou et al. (2013a) we model bridging anaphora
recognition as a subtask of learning fine-grained
information status. We report an F-measure
of 0.42 for bridging anaphora recognition. In
Hou et al. (2013b) we propose a joint inference
framework for antecedent selection by exploring
Markov logic networks. We report an accuracy
of 0.41 for antecedent selection given gold bridg-
ing anaphora. In this paper, we aim to solve these
two substasks together, i.e., recognizing bridging
anaphora and finding links to antecedents.
3 Method
In this section, we describe our rule-based system
for unrestricted bridging resolution. We choose
ten documents randomly from the corpus as the
development set. Then we carefully design rules
for finding ?bridging links? among all NPs in a
document based on the generalizations of bridg-
ing in the linguistic literature as well as our in-
spections of bridging annotations in the develop-
ment set. The system consists of two components:
bridging link prediction and post processing.
3.1 Bridging Link Prediction
The bridging link prediction component consists
of eight rules. L?obner (1985; 1998) interprets
bridging anaphora as a particular kind of func-
tional concept, which in a given situation assign
a necessarily unique correlate to a (implicit) pos-
sessor argument. He distinguishes between rela-
tional nouns (e.g. parts terms, kinship terms, role
terms) and sortal nouns and points out that rela-
tional nouns are more frequently used as bridg-
ing anaphora than sortal nouns. Rule1 to Rule4 in
our system aim to resolve such relational nouns.
We design Rule5 and Rule6 to capture set bridg-
ing. Finally, Rule7 and Rule8 are motivated by
previous work on implicit semantic role labeling
(Laparra and Rigau, 2013) which focuses on few
predicates.
For all NPs in a document, each rule r is applied
separately to predict a set of potential bridging
links. Every rule has its own constraints on bridg-
ing anaphora and antecedents respectively. Bridg-
ing anaphors are diverse with regard to syntactic
form and function: they can be modified by def-
inite or indefinite determiners (Table 1), further-
more they can take the subject (e.g. Example 3
and Example 6) or other positions (e.g. Example
2 and Example 4) in sentences. The only fre-
quent syntactic property shared is that bridging
anaphors most often have a simple internal struc-
ture concerning modification. Therefore we first
create an initial list of potential bridging anaphora
A which excludes NPs which have a complex syn-
tactic structure. An NP is added to A if it does
not contain any other NPs and do not have modifi-
cations strongly indicating comparative NPs (such
as other symptoms)
3
. Since head match is a strong
indicator of coreference anaphora for definite NPs
(Vieira and Poesio, 2000; Soon et al., 2001), we
further exclude definite NPs from A if they have
the same head as a previous NP. Then a set of
potential bridging anaphors A
r
is chosen from A
based on r?s constraints on bridging anaphora. Fi-
nally, for each potential bridging anaphor ana ?
3
A small list of 10 markers such as such, another . . . and
the presence of adjectives or adverbs in the comparative form
are used to predict comparative NPs.
2084
Ar
, a single best antecedent ante from a list of
candidate NPs (C
ana
) is chosen if the rule?s con-
straint on antecedents is applied successfully.
Every rule has its own scope to form the
antecedent candidate set C
ana
. Instead of using
a static sentence window to construct the list of
antecedent candidates like most previous work for
resolving bridging anaphora (Poesio et al., 1997;
Markert et al., 2003; Poesio et al., 2004; Lassalle
and Denis, 2011), we use the development set
to estimate the proper scope for each rule. The
scope is influenced by the following factors: (1)
the nature of the target bridging link (e.g., set
bridging is a local coherence phenomenon where
the antecedent often occurs in the same or up
to two sentences prior to the anaphor); and (2)
the strength of the rule?s constraint to select the
correct antecedent (e.g., in Rule8, the ability
to select the correct antecedent decreases with
increasing the scope to contain more antecedent
candidates). In the following, we describe the mo-
tivation for each rule and their constraints in detail.
Rule1: building part NPs. To capture typical
part-of bridging (see Example 2), we extract a
list of 45 nouns which specify building parts (e.g.
room or roof ) from the General Inquirer lexicon
(Stone et al., 1966). A common noun phrase from
A is added to A
r1
if: (1) its head appears in the
building part list; and (2) it does not contain any
nominal pre-modifications. Then for each poten-
tial bridging anaphor ana ? A
r1
, the NP with
the strongest semantic connectivity to the potential
anaphor ana among all NPs preceding ana from
the same sentence as well as from the previous two
sentences is predicted to be the antecedent.
The semantic connectivity of an NP to a po-
tential anaphor is measured via the hit counts of
the preposition pattern query (anaphor preposi-
tion NP) in big corpora
4
. An initial effort to ex-
tract partOf relations using WordNet yields low
recall on the development set. Therefore we use
semantic connectivity expressed by prepositional
patterns (e.g. the basement of the house) to cap-
ture underlying semantic relations. Such syntactic
patterns are also explored in Poesio et al. (2004) to
resolve meronymy bridging.
4
We use Gigaword (Parker et al., 2011) with automatic
POS tag and NP chunk information.
Rule2: relative person NPs. This rule is used
to capture the bridging relation between a relative
(e.g. The husband) and its antecedent (e.g. She).
A list of 110 such relative nouns is extracted from
WordNet. However, some relative nouns are fre-
quently used generically instead of being bridging,
such as children. To exclude such cases, we com-
pute the argument taking ratio ? for an NP using
NomBank (Meyers et al., 2004). For each NP, ? is
calculated via its head frequency in the NomBank
annotation divided by the head?s total frequency
in the WSJ corpus in which the NomBank anno-
tation is conducted. The value of ? reflects how
likely an NP is to take arguments. For instance,
the value of ? is 0.90 for husband but 0.31 for
children. To predict bridging anaphora more ac-
curately, a conservative constraint is used. An NP
from A is added to A
r2
if: (1) its head appears in
the relative person list; (2) its argument taking ra-
tio ? is bigger than 0.5; and (3) it does not contain
any nominal or adjective pre-modifications. Then
for each potential bridging anaphor ana ? A
r2
,
the closest non-relative person NP among all NPs
preceding ana from the same sentence as well as
from the previous two sentences is chosen as its
antecedent.
Rule3: GPE job title NPs. In news articles, it is
common that a globally salient geo-political entity
(hence GPE, e.g. Japan or U.S.) is introduced in
the beginning, then later a related job title NP (e.g.
officials or the prime minister) is used directly
without referring to this GPE explicitly. To resolve
such bridging cases accurately, we compile a list
of twelve job titles which are related to GPEs (e.g.
mayor or official). An NP from A is added to A
r3
if its head appears in this list and does not have a
country pre-modification (e.g. the Egyptian pres-
ident). Then for each potential bridging anaphor
ana ? A
r3
, the most salient GPE NP among all
NPs preceding ana is predicted as its antecedent.
We use the NP?s frequency in the whole document
to measure its salience throughout the paper. In
case of a tie, the closest one is chosen to be the
predicted antecedent.
Rule4: role NPs. Compared to Rule3, Rule4
is designed to resolve more general role NPs to
their implicit possessor arguments. We extract a
list containing around 100 nouns which specify
professional roles from WordNet (e.g. chairman,
president or professor). An NP from A is added to
2085
Ar4
if its head appears in this list. Then for each
potential bridging anaphor ana ? A
r4
, the most
salient proper name NP which stands for an orga-
nization among all NPs preceding ana from the
same sentence as well as from the previous four
sentences is chosen as its antecedent (if such an
NP exists). Recency is again used to break ties.
Rule5: percentage NPs. In set bridging as
shown in Example 7, the anaphor (Seventeen per-
cent) is indicated by a percentage expression from
A, which is often in the subject position. The an-
tecedent (the firms) is predicted to be the closest
NP which modifies another percentage NP via the
preposition ?of? among all NPs occurring in the
same or up to two sentences prior to the potential
anaphor.
(7) 22% of the firms said employees or owners
had been robbed on their way to or from
work. Seventeen percent reported their cus-
tomers being robbed.
Rule6: other set member NPs. In set bridg-
ing, apart from percentage expressions, numbers
or indefinite pronouns are also good indicators for
bridging anaphora. For such cases, the anaphor
is predicted if it is: (1) a number expression (e.g.
One in Example 3) or an indefinite pronoun(e.g.
some, as shown in Example 8) from A; and (2) a
subject NP. The antecedent is predicted to be the
closest NP among all plural, subject NPs preced-
ing the potential anaphor from the same sentence
as well as from the previous two sentences (e.g.
Reds and yellows in Example 8). If such an NP
does not exist, the closest NP among all plural, ob-
ject NPs preceding the potential anaphor from the
same sentence as well as from the previous two
sentences is chosen to be the predicted antecedent
(e.g. several problems in Example 3).
(8) Reds and yellows went about their business
with a kind of measured grimness. Some
frantically dumped belongings into pillow-
cases.
Rule7: argument-taking NPs I. Laparra and
Rigau (2013) found that different instances of the
same predicate in a document likely maintain the
same argument fillers. Here we follow this as-
sumption but apply it to nouns and their nomi-
nal modifiers only: different instances of the same
noun predicate likely maintain the same argument
fillers indicated by nominal modifiers. First, a
common noun phrase from A is added to A
r7
if:
(1) its argument taking ratio ? is bigger than 0.5;
(2) it does not contain any nominal or adjective
pre-modifications; and (3) it is not modified by in-
definite determiners
5
which usually introduce new
discourse referents (Hawkins, 1978). Then for
each potential bridging anaphor ana ? A
r7
, we
choose the antecedent by performing the follow-
ing steps:
1. We take ana?s head lemma form ana
h
and collect all its syntactic modifications in
the document. We consider nominal pre-
modification, possessive modification as well
as prepositional post-modification. All real-
izations of these modifications which precede
ana form the antecedent candidates setC
ana
.
2. We choose the most recent NP from C
ana
as the predicted antecedent for the potential
bridging anaphor ana.
In Example 9, we first predict the two occur-
rences of residents as bridging anaphors. Since
in the text, other occurrences of the lemma ?res-
ident? are modified by ?Marina? (supported by
Marina residents) and ?buildings? (supported by
some residents of badly damaged buildings), we
collect all NPs whose syntactic head is ?Ma-
rina? or ?buildings? in C
ana
(i.e. Marina, badly
damaged buildings and buildings with substan-
tial damage). Then among all NPs in C
ana
, the
most recent NP is chosen to be the antecedent (i.e.
buildings with substantial damage).
(9) She finds the response of Marina residents to
the devastation of their homes ?incredible?.
. . .
Out on the streets, some residents of badly
damaged buildings were allowed a 15 minute
scavenger hunt through their possessions.
. . .
After being inspected, buildings with sub-
stantial damage were color - coded.
Green allowed residents to re-enter; red
allowed residents one last entry to gather
everything they could within 15 minutes.
Rule8: argument-taking NPs II. Prince (1992)
found that discourse-old entities are more likely
5
We compile a list of 17 such determiners, such as a, an
or one.
2086
to be represented by NPs in subject position.
Although she could not draw a similar conclu-
sion when collapsing Inferrable (= bridging) with
Discourse-old Nonpronominal, we find that in the
development set, an argument-taking NP in the
subject position is a good indicator for bridging
anaphora (e.g. participants in Example 10). A
common noun phrase from A is collected in A
r8
if: (1) its argument taking ratio ? is bigger than
0.5; (2) it does not contain any nominal or adjec-
tive pre-modifications; and (3) it is in the subject
position. Semantic connectivity again is used as
the criteria to choose the antecedent: for each po-
tential bridging anaphor ana ? A
r8
, the NP with
the strongest semantic connectivity to ana among
all NPs preceding ana from the same sentence as
well as from the previous two sentences is pre-
dicted to be the antecedent.
(10) Initial steps were taken at Poland?s first in-
ternational environmental conference, which
I attended last month. . . . While Polish data
have been freely available since 1980, it was
no accident that participants urged the free
flow of information.
3.2 Post-processing
In the bridging link prediction component, each
rule is applied separately. To resolve the conflicts
between different rules (e.g., two rules predict dif-
ferent antecedents for the same potential anaphor),
a post processing step is applied. We first order
the rules according to their precision for predicting
bridging pairs (i.e., recognizing bridging anaphors
and finding links to antecedents) in the develop-
ment set. When a conflict happens, the rule with
the highest order has the priority to decide the an-
tecedent. Table 2 summarizes the rules described
in Section 3.1, the numbers in square brackets in
the first column indicate the order of the rules. Ta-
ble 3 shows the precisions of bridging anaphora
recognition and bridging pairs prediction for each
rule in the development set. Firing rate is the
proportion of bridging links predicted by rule r
among all predicted links.
4 Experiments and Results
4.1 Experimental Setup
We conduct all experiments on the ISNotes cor-
pus. We use the OntoNotes named entity and syn-
tactic annotations to extract features. Ten doc-
uments containing 113 bridging anaphors from
the ISNotes corpus are set as the development set
to estimate parameters for the rule-based system.
The remaining 40 documents are used as the test
set. In order to compare the results of different
systems directly, we evaluate all systems on the
test set.
4.2 Evaluation Metric
In ISNotes, bridging is annotated mostly between
an NP (anaphor) and an entity (antecedent)
6
, so
that a bridging anaphor could have multiple links
to different instantiations of the same entity (entity
information is based on the Ontonotes coreference
annotation). For bridging resolution, we use an
evaluation metric based on bridging anaphors in-
stead of all links between bridging anaphors and
their antecedent instantiations. A link predicted by
the system is counted as correct if it recognizes the
bridging anaphor correctly and links the anaphor
to any instantiation of the right antecedent entity
preceding the anaphor.
In the evaluation metric, recall is calculated
via the number of the correct links predicted by
the system (one unique link per each predicted
anaphor) divided by the total number of the gold
bridging anaphors, precision is calculated via the
number of the correct links predicted by the sys-
tem divided by the total links predicted by the sys-
tem.
4.3 A Learning-based Approach
To compare our rule-based system (hence ruleSys-
tem, described in Section 3) with other ap-
proaches, we implement a learning-based system
for unrestricted bridging resolution. We adapt the
pairwise model which is widely used in corefer-
ence resolution (Soon et al., 2001). Similar to
the rule-based system, we first create an initial list
of possible bridging anaphora A
ml
with one more
constraint. The purpose is to exclude as many ob-
vious non-bridging anaphoric NPs from the list
as possible. An NP is added to A
ml
if: (1) it
does not contain any other NPs; (2) it is not mod-
ified by pre-modifications which strongly indicate
comparative NPs; and (3) it is not a pronoun or a
proper name. Then for each NP a ? A
ml
, a list
of antecedent candidates C
a
is created by includ-
ing all NPs preceding a from the same sentence
6
There are a few cases where bridging is annotated be-
tween an NP and a non-NP antecedent (e.g. verbs or clauses).
2087
antecedent
rule anaphor antecedent
candidates scope
rule1 [2] building part NPs the NP with the strongest semantic connectivity to the two
potential anaphor
rule2 [5] relative person NPs the closest person NP which is not a relative NP two
rule3 [6] GPE job title NPs the most salient GPE NP all
rule4 [7] role NPs the most salient organization NP four
rule5 [1] percentage NPs the closest NP which modifies another percentage NP two
via the preposition ?of?
rule6 [3] other set member NPs the closest subject, plural NP; two
otherwise the closest object, plural NP
rule7 [4] argument-taking NPs I the closest NP whose head is an unfilled role of the potential all
anaphor (such a role is predicted via syntactic modifications of NPs
which have the same head as the potential anaphor)
rule8 [8] argument-taking NPs II the NP with the strongest semantic connectivity to the two
potential anaphor
Table 2: Rules for unrestricted bridging resolution. Antecedent candidates scope are verified in the
development set: ?all? represents all NPs preceding the potential anaphor from the whole document,
?four? NPs occurring in the same or up to four sentences prior to the potential anaphor, ?two? NPs
occurring in the same or up to two sentences prior to the potential anaphor.
anaphora recognition bridging pairs prediction
rule anaphora
precision precision
firing rate
rule1 [2] building part NPs 75.0% 50.0% 6.1%
rule2 [5] relative person NPs 69.2% 46.2% 6.1%
rule3 [6] GPE job title NPs 52.6% 44.7% 19.4%
rule4 [7] role NPs 61.7% 32.1% 28.6%
rule5 [1] percentage NPs 100.0% 100.0% 2.6%
rule6 [3] other set member NPs 66.7% 46.7% 7.8%
rule7 [4] argument-taking NPs I 53.8% 46.4% 6.1%
rule8 [8] argument-taking NPs II 64.5% 25.0% 25.5%
Table 3: Precision of bridging anaphora recognition and bridging pairs prediction for each rule in the
development set. The numbers in square brackets in the first column indicate the order of the rules.
as well as from the previous two sentences
7
. We
create a pairwise instance (a, c) for every c ? C
a
.
We also add extra pairwise instances from the pre-
diction of ruleSystem to the learning-based sys-
tem. In the decoding stage, the best first strat-
egy (Ng and Cardie, 2002) is used to predict the
bridging links. Specifically, for each a ? A
ml
, we
predict the bridging link to be the most confident
pair (a, c
ante
) among all instances with the posi-
tive prediction. We use SVM
light
to conduct the
experiments
8
. All experiments are conducted via
10-fold cross-validation on the whole corpus
9
.
7
In ISNotes, 71% of NP antecedents occur in the same
or up to two sentences prior to the anaphor. Initial experi-
ments show that increasing the window size more than two
sentences decreases the performance.
8
To deal with data imbalance, the SVM
light
parameter
is set according to the ratio between positive and negative
instances in the training set.
9
To compare the learning-based approach to the rule-
based system described in Section 3 directly, we report the
mlSystem ruleFeats We provide mlSys-
tem ruleFeats with the same knowledge resources
as the rule-based system. All rules from the
rule-based system are incorporated into mlSys-
tem ruleFeats as the features.
mlSystem ruleFeats + atomFeats We augment
mlSystem ruleFeats with more features from our
previous work (Markert et al., 2012; Hou et al.,
2013a; Hou et al., 2013b) on bridging anaphora
recognition and antecedent selection. Some of
these features overlap with the atomic features
used in the rule-based system.
Table 4 shows all the features we use for rec-
ognizing bridging anaphora. ??? indicates the re-
sources are used in the rule-based system. We ap-
ply them to the first element a of a pairwise in-
stance (a, c). Markert et al. (2012) and Hou et
results of learning-based approaches on the same test set as
the rule-based system.
2088
Markert et al. local feature set
f1 FullPrevMention (b) ? f2 FullPreMentionTime (n) f3 PartialPreMention (b)
f4 ContentWordPreMention (b) f5 Determiner (n) ? f6 NPtype (n) ?
f7 NPlength (int) f8 GrammaticalRole (n) ? f9 NPNumber (n) ?
f10 PreModByCompMarker (b) ?
Hou et al. local feature set
features to identify bridging anaphora
f1 IsCoherenceGap (b) f2 IsSentFirstMention (b) f3 IsDocFirstMention (b)
f4 IsWordNetRelationalNoun (b) ? f5 IsInquirerRoleNoun (b) f6 IsBuildingPart (b) ?
f7 IsSetElement (b) ? f8 PreModSpatialTemporal (b) f9 IsYearExpression (b)
f10 PreModifiedByCountry (b) ? f11 AppearInIfClause (b) f12 VerbPosTag (l)
f13 IsFrequentGenericNP (b) f14 WorldKnowledgeNP (l) f15 Unigrams (l)
f16 PreModByGeneralQuantifier (b) f17 BridgingHeadNP (l) f18 HasChildNP (b) ?
features to identify function and worldKnowledge NPs
f20 DependOnChangeVerb (b) f21 IsFrequentProperName (b)
Table 4: Features for bridging anaphora recognition from Markert et al. (2012) and Hou et al. (2013a).
?b? indicates binary, ?n? nominal, ?l? lexical features, ??? resources used in the rule-based system.
Group Feature Value
semantic f1 preposition pattern ? the normalized hit counts of the preposition pattern query
a prep. c (e.g. participants of the conference) in big corpora
f2 verb pattern the normalized hit counts of the verb pattern query c verb
a
or
verb
a
c in big corpora (for set bridging in Example 7, the
pattern query is the firms reported)
f3 WordNet partOf whether a partOf relation holds between a and c in WordNet
f4 semantic class ? 16 classes, e.g. location, organization, GPE, rolePerson,
relativePerson, product, date, money, percent
salience f5 document span the normalized value of the span of text in which c is mentioned
f6 utterance distance the sentence distance between a and c
f7 local first mention whether c is the first mention within the previous five sentences
f8 global first mention whether c is the first mention in the whole document
syntactic f9 isSameHead whether a and c share the same head
& (exclude coreferent antecedent candidates)
lexical f10 isWordOverlap whether a is prenominally modified by the head of c (for
bridging where the anaphor is a compound noun, such as
the mine-mine security)
f11 isCoArgument whether subject c and object a are dependent on the same verb
(the subject can not be the bridging antecedent of the object
in the same clause)
f12 WordNet distance the inverse value of the shortest path length between a and c
in WordNet
Table 5: Features for antecedent selection from Hou et al. (2013b). ??? indicates resources used in the
rule-based system.
al. (2013a) classify eight fine-grained information
status (IS) categories for NPs: old, new and 6
mediated categories (syntactic, worldKnowledge,
bridging, comparative, aggregate and function).
Features from Markert et al. (2012) work well to
identify old, new and several mediated categories
but fail to recognize most bridging anaphora. Hou
et al. (2013a) remedy this by adding discourse
structure features (f1-f3), semantic features (f4-
f10) and features to detect generic nouns (f11-
2089
Feature Value
for anaphor candidate a
f1 preModByNominal whether a contains any nominal pre-modifications
f2 preModByAdj whether a contains any adjective modifications
f3 isGPEJobTitle whether a is a job title about GPE (e.g. mayor or official)
f4 isArgumentTakingNP whether the argument taking ratio of a is bigger than 0.5
for antecedent candidate c
f5 fullMentionTime the normalized value of the frequency of c in the whole document
for pairwise instance (a, c)
f6 word distance the token distance between a and c
Table 6: Additional atomic features from the rule-based system.
f14 and f16).
Table 5 shows all features we use for selecting
antecedents for bridging anaphora. ??? indicates
the resources that are used in the rule-based sys-
tem. These features are from Hou et al. (2013b)?s
local pairwise model. They try to model: (1) the
semantic relations between bridging anaphors and
their antecedents (f1 to f4); (2) the salience of
an antecedent from different perspectives (f5 to
f8); and (3) the syntactic and lexical constraints
between anaphor and antecedent (f9 to f12).
Apart from the features shown in Table
4 and Table 5, we further enrich mlSys-
tem ruleFeats+atomFeats with additional atomic
features used in the rule-based system (Table 6).
mlSystem atomFeats Based on mlSys-
tem ruleFeats+atomFeats, the rule features
from the rule-based system are removed.
4.4 Baseline
We also reimplement the rule-based system from
Vieira and Poesio (2000) as a baseline. The origi-
nal algorithm focuses on processing definite NPs.
It classifies four categories for the definite NPs:
discourse new, direct anaphora (same-head coref-
erent anaphora), lenient bridging and Unknown.
This algorithm also finds antecedents for NPs
which belong to direct anaphora or lenient bridg-
ing.
Since Vieira and Poesio (2000) include
different-head coreference into their lenient
bridging category, we further divide their le-
nient bridging category into two subcategories:
different-head coreference and bridging. Figure
1 shows the details of the division after failing
to classify an NP as discourse new or direct
anaphora. For more details about the whole
system, see Vieira and Poesio (2000). We then
apply this slightly revised algorithm to process
all NPs in the initial list of potential bridging
anaphoraA from ruleSystem (described in Section
3.1).
4.5 Results and Discussion
Table 7 shows the results on the same test set of
different approaches for unrestricted bridging res-
olution. The results reveal the difficulty of the
task, when evaluating on a realistic scenario with-
out constraints on types of bridging anaphora and
bridging relations.
Both our rule-based system and all learning-
based approaches significantly outperform the
baseline at p < 0.01 (randomization test). The
low recall in baseline is predictable, since it
only considers meronymy bridging and compound
noun anaphors whose head is prenominally mod-
ified by the antecedent head. (e.g. the state?
state gasoline taxes). Under the same features,
the learning-based approach (mlSystem ruleFeats)
performs slightly worse than the rule-based sys-
tem (ruleSystem) with regard to the F-score.
R P F
baseline 2.9 13.3 4.8
ruleSystem 11.9 42.9 18.6
mlSystem ruleFeats 12.1 35.0 18.0
mlSystem ruleFeats+atomFeats 16.7 21.2 18.7
mlSystem atomFeats 20.5 10.1 13.5
Table 7: Experimental results for the baseline, the
rule-based system and the learning-based systems.
Surprisingly, incorporating rich features
into the learning-based approach (mlSys-
tem ruleFeats+atomFeats) does not yield much
improvement over the rule-based system (with an
2090
Figure 1: Vieria & Poesio?s (2000) original algorithm for processing definite NPs. We further divide
their lenient bridging category into two subcategories: 2.1 Different-head coreference and 2.2 Bridging.
F-score of 18.7 in mlSystem ruleFeats+atomFeats
compared to 18.6 in ruleSystem). We suppose that
the learning-based system generalizes poorly with
only atomic features in Table 4, Table 5 and Table
6. Results on mlSystem atomFeats support our
assumption: the F-score drops considerably after
removing the rule features. Although ISNotes is
a reasonably sized corpus for bridging compared
to previous work, diverse bridging relations,
especially lots of context specific relations such
as pachinko-devotees or palms-the thieves, lead
to relatively small-scale training data for each
type of relation. Therefore it is difficult for the
learning-based approach to learn effective rules to
predict bridging links.
However, all learning-based systems tend to
have higher recall but lower precision compared
to the rule-based system. This suggests that the
learning-based systems are ?greedy? to predict
bridging links. A close look at these links in
mlSystem atomFeats indicates that the learning-
based system predicts more correct bridging
anaphors but fails to find the correct antecedents.
In fact, lots of those ?half? correct links sound
reasonable without the specific context, such as
the story-readers (gold bridging link: this novel-
readers) or the executive director?s office-the
desks (gold bridging link: the fund?s building-the
desks).
5 Conclusions
We proposed a rule-based approach for un-
restricted bridging resolution where bridging
anaphora are not limited to definite NPs and the
relations between anaphor and antecedent are not
restricted to meronymic relations. We designed
eight rules to resolve bridging based on linguis-
tic intuition. Our rule-based system performs bet-
ter than a learning-based approach which has ac-
cess to the same knowledge resources as the rule-
based system. Particularly, the learning-based sys-
tem enriched with more features does not yield
much improvement over the rule-based system.
We speculate that the learning-based system could
benefit from more training data. Furthermore, bet-
ter methods to model the semantics of the specific
context need to be explored in the future.
This work is ? to our knowledge ? the first
bridging resolution system that handles the unre-
stricted phenomenon in a realistic setting.
Acknowledgements
We thank Renata Vieira for excavating part of her
source code for us. We also thank the reviewers
for their helpful comments. Yufang Hou is funded
by a PhD scholarship from the Research Training
Group Coherence in Language Processing at Hei-
delberg University. This work has been partially
funded by the Klaus Tschira Foundation.
2091
References
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15:83?113.
Razvan Bunescu. 2003. Associative anaphora resolu-
tion: A Web-based approach. In Proceedings of the
EACL 2003 Workshop on The Computational Treat-
ment of Anaphora, Budapest, Hungary, 14 April,
2003, pages 47?52.
Aoife Cahill and Arndt Riester. 2012. Automati-
cally acquiring fine-grained information status dis-
tinctions in German. In Proceedings of the SIGdial
2012 Conference: The 13th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
Seoul, Korea, 5?6 July 2012, pages 232?236.
Philipp Cimiano. 2006. Ingredients of a first-order ac-
count of bridging. In Proceedings of the 5th Inter-
national Workshop on Inference in Computational
Semantics, Buxton, U.K., 20?21 April 2006, pages
139?144.
Herbert H. Clark. 1975. Bridging. In Proceedings
of the Conference on Theoretical Issues in Natu-
ral Language Processing, Cambridge, Mass., June
1975, pages 169?174.
Jeanette K. Gundel, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and the form
of referring expressions in discourse. Language,
69:274?307.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: A study in reference and grammaticality pre-
diction. Humanities Press, Atlantic Highlands, N.J.
Yufang Hou, Katja Markert, and Michael Strube.
2013a. Cascading collective classification for bridg-
ing anaphora recognition using a rich linguistic fea-
ture set. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Wash., 18?21 October 2013, pages 814?
820.
Yufang Hou, Katja Markert, and Michael Strube.
2013b. Global inference for bridging anaphora res-
olution. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, 9?14 June 2013, pages
907?917.
Egoitz Laparra and German Rigau. 2013. ImpAr: A
deterministic algorithm for implicit semantic role la-
belling. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, Sofia, Bulgaria, 4?9 August 2013, pages 1180?
1189.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridg-
ing resolution in French. In Proceedings of the 8th
Discourse Anaphora and Anaphor Resolution Col-
loquium (DAARC 2011), Faro, Algarve, Portugal, 6?
7 October 2011, pages 35?46.
Sebastian L?obner. 1985. Definites. Journal of Seman-
tics, 4:279?326.
Sebastian L?obner. 1998. Definite associative
anaphora. Unpublished Manuscript, Heinrich-
Heine-Universit?at D?usseldorf.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop
on the Computational Treatment of Anaphora. Bu-
dapest, Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju
Island, Korea, 8?14 July 2012, pages 795?804.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishaman. 2004. Annotating noun ar-
gument structure for NomBank. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, Lisbon, Portugal, 26?28
May 2004, pages 803?806.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning
for other-anaphora resolution. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing, Sapporo, Japan, 11?12 July
2003, pages 176?183.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadel-
phia, Penn., 7?12 July 2002, pages 104?111.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. LDC2011T07.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging references in unrestricted
text. In Proceedings of the ACL Workshop on Oper-
ational Factors in Practical, Robust Anaphora Res-
olution for Unrestricted Text, Madrid, Spain, July
1997, pages 1?6.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridg-
ing references. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, 21?26 July 2004, pages
143?150.
Ellen F. Prince. 1981. Towards a taxonomy of given-
new information. In P. Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press, New York,
N.Y.
2092
Ellen F. Prince. 1992. The ZPG letter: Subjects, defi-
niteness, and information-status. In W.C. Mann and
S.A. Thompson, editors, Discourse Description. Di-
verse Linguistic Analyses of a Fund-Raising Text,
pages 295?325. John Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2012. Learning the
fine-grained information status of discourse entities.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Avignon, France, 23?27 April 2012,
pages 798?807.
Ina R?osiger and Simone Teufel. 2014. Resolving
coreference and associative noun phrases in scien-
tific text. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, 26?30 April 2014,
pages 44?55.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and Cambridge Computer Asso-
ciates. 1966. General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press, Cambridge,
Mass.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
2093
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 357?360,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Word Sense Subjectivity for Cross-lingual Lexical Substitution
Fangzhong Su
School of Computing
University of Leeds, UK
scsfs@leeds.ac.uk
Katja Markert
School of Computing
University of Leeds, UK
scskm@leeds.ac.uk
Abstract
We explore the relation between word sense
subjectivity and cross-lingual lexical substitu-
tion, following the intuition that good substi-
tutions will transfer a word?s (contextual) sen-
timent from the source language into the target
language. Experiments on English-Chinese
lexical substitution show that taking a word?s
subjectivity into account can indeed improve
performance. We also show that just using
word sense subjectivity can perform as well
as integrating fully-fledged fine-grained word
sense disambiguation for words which have
both subjective and objective senses.
1 Introduction
Cross-lingual lexical substitution has been proposed
as a Task at SemEval-2010.1 Given a target word
and its context in a source language (like English),
the goal is to provide correct translations for that
word in a target language (like Chinese). The trans-
lations must fit the given context.
In this paper, we explore the relation between the
sentiment of the used word in the source language
and translation choice in the target language, focus-
ing on English as the source and Chinese as the tar-
get language. Our work is motivated by the intuition
that most good word translations will be sentiment-
invariant, i.e. if a source word is used in a subjec-
tive (opinion-carrying) sense it will be often trans-
lated with a subjective sense in the target language
whereas if it used in an objective sense, it will be
1http://lit.csci.unt.edu/index.php/
Semeval_2010
translated with an objective sense. As an exam-
ple, consider the two words positive and collaborate
with example senses from WordNet 2.0 below.
(1) positive?greater than zero; ?positive numbers?
(objective)
(2) plus, positive?involving advantage or good; ?a
plus (or positive) factor? (subjective)
(3) collaborate, join forces, cooperate?work together
on a common enterprise of project; ?We joined
forces with another research group?(objective)
(4) collaborate?cooperate as a traitor; (subjective)
In most cases, if the word positive is used in
the sense ?greater than zero? (objective) in an
English context, the corresponding Chinese trans-
lation is ? ff?; if ?involving advantage or
good?(subjective) is used, its Chinese translations
are ??4ff,?ff?. Similarly, for the word collab-
orate, the sense ?work together on a common en-
terprise of project? (objective) corresponds to ??
?,?? in Chinese translation, and ?cooperate as
a traitor? (subjective) corresponds to ?(, H
?r?. Therefore, subjectivity information should
be effective for improving lexical translation for
what we previously (Su and Markert, 2008) termed
subjectivity-ambiguous words, i.e. words with both
subjective and objective senses such as positive and
collaborate above.
We therefore incorporate subjectivity word sense
disambiguation (SWSD) as defined in Akkaya et
al. (2009) into lexical substitution. SWSD is a
binary classification task that decides in context
whether a word occurs with one of its subjective or
one of its objective senses. In contrast to standard
357
multi-class Word Sense Disambiguation (WSD), it
uses a coarse-grained sense inventory that allows to
achieve higher accuracy than WSD and therefore in-
troduces less noise when embedded in another task
such as word translation. For example, the accuracy
reported in Akkaya et al (2009) for SWSD is over
20% higher than for standard WSD. Coarse-grained
senses are also easier to annotate, so getting train-
ing data for learning is less arduous. On the mi-
nus side, SWSD can only be useful for subjectivity-
ambiguous words. However, we showed (Su and
Markert, 2008) that subjectivity-ambiguity is fre-
quent (around 30% of common words).
2 Related Work
McCarthy and Navigli (2007) organized a monolin-
gual English lexical substitution task in Semeval-
2007, i.e finding English substitutions for an English
target word. Mihalcea et al organize an English-
Spanish lexical substitution task in SemEval-2010.
Approaches to lexical substitution in the past com-
petitions did not use sentiment features.
Independent of these lexical substitution tasks, the
connection between word senses and word transla-
tion has been explored in Chan et al (2007) and
Carpuat and Wu (2007), who predict the probabil-
ities of a target word being translated as an item in
a ?sense inventory?, where the sense inventory is a
list of possible translations. They then incorporate
these probabilities into machine translation. How-
ever, they do not consider sentiment explicitly.
Subjectivity at the word sense level has been
discussed by (Wiebe and Mihalcea, 2006; Su and
Markert, 2008; Akkaya et al, 2009). Wiebe and
Mihalcea (2006) and Su and Markert (2008) both
show that this is a well-defined concept via human
annotation as well as automatic recognition. Akkaya
et al (2009) show that subjectivity word sense dis-
ambiguation (SWSD) can boost the performance of
a sentiment analysis system. None of these paper
considers the impact of word sense subjectivity on
cross-lingual lexical substitution.
3 Methodology
3.1 Task and Dataset
We constructed an English-Chinese lexical substi-
tution gold standard by translating the English tar-
get words in the SENSEVAL 2 and SENSEVAL 3
lexical sample training and test sets into Chinese.
We choose the SENSEVAL datasets as they are rel-
atively domain-independent and also because we
can use them for our SWSD/WSD subtasks as well.
The translation is carried out by two native Chinese
speakers with a good command of English. First,
candidate Chinese translations (denoted by T) of the
English target words are provided from the on-line
English-Chinese dictionary iciba2, which is com-
posed of more than 150 different English-Chinese
dictionaries. To reduce annotation bias, the order
of the Senseval sentences is randomized. The an-
notators then independently assign the most fitting
Chinese translation(s) (from T) for the English tar-
get words in the given Senseval sentences. For the
agreement study, different Chinese translations (for
example, ?%? and ??? of the word author-
ity) that are actually synonyms are merged. The
observed agreement between the two annotators is
86.7%. Finally, the two annotators discuss the dis-
agreed examples together, leading to a gold stan-
dard.
Since we evaluate how word sense subjectivity
affects cross-lingual lexical substitution, we lim-
ited our study to the SENSEVAL words that are
subjectivity-ambiguous. Therefore, following the
annotation schemes in (Su and Markert, 2008;
Wiebe and Mihalcea, 2006), all senses of all target
words in SENSEVAL 2&3 are annotated by a near-
native English speaker as subjective orobjective.
This annotator was not involved in the English to
Chinese translation. We also discard subjectivity-
ambiguous words if its subjective or objective senses
do not appear in both training and test set. In total we
collect 28 subjectivity-ambiguous words. Their En-
glish example sentences and translations yield 2890
training sentence pairs and 1444 test sentence pairs.
3.2 Algorithms
For the English-Chinese lexical substitution task, we
first develop a basic system (called B) to assign Chi-
nese translations to the target English words in con-
text. This system uses only standard contextual fea-
tures from the English sentences (see Section 3.3).
We then add word sense subjectivity information to
2http://www.iciba.com
358
the basic system (see Section 3.4). We also compare
including word sense subjectivity to the inclusion of
full fine-grained sense information (Section 3.5).
All systems are supervised classifiers trained on
the SENSEVAL training data and evaluated on the
SENSEVAL test data for each of the 28 words. We
employ an SVM classifier from the libsvm pack-
age3 with a linear kernel.
3.3 Common Features
In the basic system B, we adopt features which are
commonly used in WSD or lexical translation.
Surrounding Words: Lemmatized bag of words
with stop word filtering.
Part-of-Speech (POS): The POS of the neigh-
bouring words of the target word. We extract POS
tag of the 3 words to the right and left together with
position information.
Collocation: The neighbouring words of the tar-
get word. We extract 4 lemmatized words to the
right and left, together with position information.
Syntactic Relations: We employ the MaltParser4
for dependency parsing and extract 4 features: the
head word of the target word, POS of the head word,
the dependency relation between head word and tar-
get word, and the relative position (left or right) of
the head word to the target word.
3.4 Subjectivity Features
We add a feature that incorporates whether the origi-
nal English word is used subjectively or objectively.
For an upper bound, we use the SENSEVAL gold
standard sense annotation (gold-subj), mapped onto
binary subjective/objective labels. For a more re-
alistic assessment, we use SWSD to derive the sub-
jectivity sense label automatically (auto-subj) using
standard supervised binary SVMs and the features in
Section 3.3 on the SENSEVAL data.
3.5 Sense Features
We compare using subjectivity information to using
full fine-grained word sense information, incorpo-
rating a feature that specifies the exact word sense
of the target word to be translated. This setting
3http://www.csie.ntu.edu.tw/?cjlin/
libsvm
4http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
also compares the SENSEVAL gold standard (gold-
senses) and automatically predicted sense informa-
tion (auto-senses), the latter via supervised multi-
class learning on the SENSEVAL dataset.
4 Experiments and Evaluation
For the English-Chinese lexical substitution task, we
evaluate 6 different methods: Baseline (assign the
most frequent translation to all examples), B (use
common features), B+gold subj (incorporate gold
standard word sense subjectivity), B+gold sense (in-
corporate gold standard sense), B+auto subj (incor-
porate automatically predicted word sense subjectiv-
ity), and B+auto sense (incorporate automatically
predicted fine-grained senses). We measure lexical
substitution accuracy on the SENSEVAL test data by
comparing to the human gold standard annotation
(see Section 3.1). Results are listed in Table 1.
Results. Table 1 shows that our standard lexical
substitution system B improves strongly (near 11%
average accuracy gain) over the most frequent trans-
lation baseline. Incorporating sense subjectivity as
in B+gold subj leads to a further strong improve-
ment, confirming our hypothesis that word sense
subjectivity can improve lexical substitution. Incor-
porating fine-grained senses B+gold senses yields
only a slightly higher gain, showing that a coarse-
grained subjective/objective classification might be
sufficient for subjectivity-ambiguous words for aid-
ing translation. In addition, the small gain using
fine-grained senses might disappear in practice as
automatic WSD is a more challenging task than
SWSD: in our experiment, B+auto sense performs
worse than B+auto subj. The current improve-
ment of B+auto subj over B is significant (McNe-
mar test at the 5% level). The difference between
the actual performance of word sense subjectivity
and its potential as exemplified in B+gold subj is,
obviously, caused by imperfect performance of the
SWSD component, mostly due to a distributional
bias in the SENSEVAL training data, with few ex-
amples for rarer senses of the target words.
For some words (such as authority and stress),
the additional sense subjectivity feature does not im-
prove lexical substitution, even when gold standard
labels are used. There are two main reasons for this.
First, one candidate Chinese translation might cover
359
Table 1: Accuracy of lexical substitution with different
different feature settings
Word Subjectivity
of Senses
Baseline Basic
(B)
B+gold
subj
B+gold
senses
B+auto
subj
B+auto
senses
authority 3-S 4-O 50.5% 70.3% 70.3% 84.6% 70.3% 79.1%
blind 2-S 1-O 87.0% 88.9% 94.4% 94.4% 88.9% 88.9%
cool 3-S 3-O 46.0% 46.0% 68.0% 68.0% 58.0% 48.0%
dyke 1-S 1-O 89.3% 89.3% 92.9% 92.9% 89.3% 89.3%
fatigue 1-S 2-O 1-B 80.0% 80.0% 82.5% 85.0% 82.5% 82.5%
fine 5-S 4-O 78.5% 78.5% 90.8% 80.0% 80.0% 78.5%
nature 1-S 3-O 1-B 53.3% 62.2% 73.3% 71.1% 64.4% 62.2%
oblique 1-S 1-O 65.5% 75.9% 86.2% 89.7% 79.3% 79.3%
sense 3-S 2-O 47.5% 67.5% 77.5% 77.5% 75.0% 72.5%
simple 2-S 2-O 1-B 71.2% 71.2% 75.8% 74.2% 72.7% 71.2%
stress 3-S 2-O 92.1% 92.1% 92.1% 92.1% 92.1% 92.1%
collaborate 1-S 1-O 90.0% 90.0% 93.3% 93.3% 93.3% 90.0%
drive 3-S 5-O 1-B 51.4% 78.4% 89.2% 86.5% 83.8% 78.4%
play 4-S 13-O 1-B 23.3% 40.0% 48.3% 56.7% 41.7% 43.3%
see 7-S 11-O 30.9% 36.8% 58.8% 61.8% 42.6% 38.2%
strike 3-S 10-O 1-B 20.5% 27.3% 43.2% 45.5% 29.5% 38.6%
treat 2-S 4-O 36.4% 61.4% 65.9% 81.8% 56.8% 65.9%
wander 1-S 2-O 1-B 79.2% 81.3% 83.3% 83.3% 81.3% 81.3%
work 2-S 9-O 2-B 56.8% 56.8% 75.0% 75.0% 63.6% 61.4%
appear 1-S 2-O 42.7% 63.4% 80.2% 90.8% 65.6% 66.4%
express 2-S 2-O 81.5% 81.5% 90.7% 88.9% 83.3% 81.5%
hot 3-S 4-O 1-B 85.0% 85.0% 85.0% 85.0% 85.0% 85.0%
image 3-S 4-O 56.7% 83.6% 94.0% 92.5% 85.1% 79.1%
interest 2-S 4-O 1-B 38.7% 73.1% 84.9% 88.2% 74.2% 71.0%
judgment 4-S 3-O 46.9% 65.6% 78.1% 75.0% 68.8% 62.5%
miss 3-S 5-O 50.0% 63.3% 70.0% 66.7% 63.3% 60.0%
solid 4-S 10-O 40.0% 40.0% 44.0% 48.0% 44.0% 44.0%
watch 3-S 4-O 86.3% 86.3% 90.2% 88.2% 86.3% 86.3%
AVERAGE 57.4% 68.5% 77.9% 80.2% 70.7% 70.1%
both subjective and objective uses of the word. For
example, both the objective sense (?physics force
that produces strain on a physical body?) and sub-
jective senses (?difficulty that causes worry or emo-
tional emotional tension? and ? a state of mental
or emotional strain or suspense? ) of stress are of-
ten translated as ???? in Chinese. Second, in
some cases, subjectivity word sense disambiguation
is too coarse-grained and finer-grained WSD is ac-
tually necessary. For example, the subjective usages
of authority in SENSEVAL examples are often trans-
lated as ?;[, %?, ?g&? or ??&? (called
List-S), and objective usages are often translated
as ??, ??,???,??, ?? or ??, 1O?
(called List-O). In this case, word sense subjectivity
might help to distinguish List-S from List-O, but
not among the candidate translations within a single
list.
5 Discussion
We tackle cross-lingual lexical substitution as a su-
pervised task, using sets of manual translations for a
target word as training data even for baseline system
B. However, we do not necessarily need dedicated
human translated data as we could also use existing
parallel texts in which the target word occurs. There-
fore, we think that a supervised approach to lexical
substitution is feasible. However, we do need addi-
tional monolingual sense-tagged data in the source
language for incorporating our word sense subjec-
tivity features.5 Although a disadvantage, more and
more sense-tagged data does become available (such
as OntoNotes). We also only need tagging at a
coarse-grained sense level, which is much easier to
create than fine-grained data.
6 Conclusion and Future Work
We investigate the relation between word sense sub-
jectivity and cross-lingual lexical substitution. The
experimental results show that incorporating word
sense subjectivity into a standard supervised classi-
fication model yields a significantly better perfor-
mance for an English-Chinese lexical substitution
task. We also compare the effect of sense subjec-
tivity to the effect of fine-grained sense informa-
tion on lexical substitution. The differences be-
tween the two methods turn out to be small, mak-
ing a case for the ?easier?, coarse-grained SWSD
over WSD for subjectivity-ambiguous words. Fu-
ture work will widen the study by (i) looking at a
wider range of words and languages, (ii) improv-
ing automatic SWSD results for better application
and (iii) integrating unsupervised subjectivity fea-
tures into cross-lingual lexical substitution.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity Word Sense Disambiguation. Proceed-
ings of EMNLP?09.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. Proceedings of EMNLP?07.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. Proceedings of ACL?07.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. Pro-
ceedings of SemEval-2007.
Fangzhong Su and Katja Markert. 2008. From Words
to Senses: A Case Study in Subjectivity Recognition.
Proceedings of COLING?08.
Janyce Wiebe and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
5In our case, this is the same data as the data the lexical
substitution algorithms are trained on, but this is not mandatory.
360
Proceedings of NAACL-HLT 2013, pages 907?917,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Global Inference for Bridging Anaphora Resolution
Yufang Hou1, Katja Markert2, Michael Strube1
1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
We present the first work on antecedent se-
lection for bridging resolution without restric-
tions on anaphor or relation types. Our model
integrates global constraints on top of a rich
local feature set in the framework of Markov
logic networks. The global model improves
over the local one and both strongly outper-
form a reimplementation of prior work.
1 Introduction
Identity coreference is a relatively well understood
and well-studied instance of entity coherence. How-
ever, entity coherence can rely on more complex,
lexico-semantic, frame or encyclopedic relations
than identity. Anaphora linking distinct entities or
events this way are called bridging or associative
anaphora and have been widely discussed in the lin-
guistic literature (Clark, 1975; Prince, 1981; Gundel
et al, 1993).1 In Example 1, the phrases the win-
dows, the carpets and walls can be felicitously used
because they are semantically related via a part-of
relation to their antecedent the Polish center.2
(1) . . . as much as possible of the Polish center will
be made from aluminum, steel and glass recycled
from Warsaw?s abundant rubble. . . . The windows
will open. The carpets won?t be glued down and
walls will be coated with non-toxic finishes.
1Poesio and Vieira (1998) include cases where antecedent
and anaphor are coreferent but do not share the same head noun.
We restrict bridging to non-coreferential cases. We also exclude
comparative anaphora (Modjeska et al, 2003)
2Examples are from OntoNotes (Weischedel et al, 2011).
Bridging anaphora are typed in boldface; antecedents in italics.
Bridging is frequent amounting to between 5%
(Gardent and Manue?lian, 2005) and 20% (Caselli
and Prodanof, 2006) of definite descriptions (both
studies limited to NPs starting with the or non-
English equivalents). Bridging resolution is needed
to fill gaps in entity grids based on coreference only
(Barzilay and Lapata, 2008). Example 1 does not ex-
hibit any coreferential entity coherence. Coherence
can only be established when the bridging anaphora
are resolved. Bridging resolution may also be im-
portant for textual entailment (Mirkin et al, 2010).
Bridging resolution can be divided into two tasks,
recognizing that a bridging anaphor is present and
finding the correct antecedent among a list of candi-
dates. These two tasks have frequently been handled
in a pipeline with most research concentrating on an-
tecedent selection only. We also handle only the task
of antecedent selection.
Previous work on antecedent selection for bridg-
ing anaphora is restricted. It makes strong untested
assumptions about bridging anaphora types or rela-
tions, limiting it to definite NPs (Poesio and Vieira,
1998; Poesio et al, 2004; Lassalle and Denis, 2011)
or to part-of relations between anaphor and an-
tecedent (Poesio et al, 2004; Markert et al, 2003;
Lassalle and Denis, 2011). We break new ground
by considering all relations and anaphora/antecedent
types and show that the variety of bridging anaphora
is much higher than reported previously.
Following work on coreference resolution, we ap-
ply a local pairwise model (Soon et al, 2001) for an-
tecedent selection. We then develop novel semantic,
syntactic and salience features for this task, show-
ing strong improvements over one of the best known
907
prior models (Poesio et al, 2004).
However, this local model classifies each
anaphor-antecedent candidate pair in isolation.
Thus, it neglects that bridging anaphora referring to
a single antecedent often occur in clusters (see Ex-
ample 1). It also neglects that once an entity is an
antecedent for a bridging anaphor it is more likely to
be used again as antecedent. In addition, such local
models construct the list of possible antecedent can-
didates normally relying on a window size constraint
to restrict the set of candidates: is the window too
small, we miss too many correct antecedents; is it
too large, we include so many incorrect antecedents
as to lead to severe data imbalance in learning.
To remedy these flaws we change to a global
Markov logic model that allows us to:
? model constraints that certain anaphora are
likely to share the same antecedent;
? model the global semantic connectivity of a
salient potential antecedent to all anaphora in a
text;
? consider the union of potential antecedents for
all anaphora instead of a static window-sized
constraint.
We show that this global model with the same lo-
cal features but enhanced with global constraints im-
proves significantly over the local model.
2 Related Work
Prior corpus-linguistic studies on bridging are be-
set by three main problems. First, reliability is not
measured or low (Fraurud, 1990; Poesio, 2003; Gar-
dent and Manue?lian, 2005; Riester et al, 2010).3
Second, annotated corpora are small (Poesio et al,
2004; Korzen and Buch-Kromann, 2011). Third,
they are often based on strong untested assumptions
about bridging anaphora types, antecedent types or
relations, such as limiting it to definite NP anaphora
(Poesio and Vieira, 1998; Poesio et al, 2004; Gar-
dent and Manue?lian, 2005; Caselli and Prodanof,
2006; Riester et al, 2010; Lassalle and Denis,
2011), to NP antecedents (all prior work) or to part-
3Although the overall information status scheme in Riester
et al (2010) achieved high agreement, their confusion matrix
shows that the anaphoric bridging category (BRI) is frequently
confused with other categories so that the two annotators agreed
on only less than a third of bridging anaphors.
of relations between anaphor and antecedent (Mark-
ert et al, 2003; Poesio et al, 2004). In our own
work (Markert et al, 2012) we established a corpus
that circumvents these problems, i.e. human bridg-
ing recognition was reliable, it contains a medium
number of bridging cases that allows generalisable
statistics and we did not limit bridging anaphora or
antecedents according to their syntactic type or re-
lations between them. However, we only discussed
human agreement on bridging recognition in Mark-
ert et al (2012), disregarding antecedent annotation.
We also did not discuss the different types of bridg-
ing in the corpus. We will remedy this in Section 3.
Automatic work on bridging distinguishes be-
tween recognition (Vieira and Poesio, 2000; Rah-
man and Ng, 2012; Cahill and Riester, 2012; Mark-
ert et al, 2012) and antecedent selection. Work on
antecedent selection suffers from focusing on sub-
problems, e.g. only part-of bridging (Poesio et al,
2004; Markert et al, 2003) or definite NP anaphora
(Lassalle and Denis, 2011). Most relevant for us is
Lassalle and Denis (2011) who restrict anaphora to
definite descriptions but have no other restrictions
on relations or antecedent NPs (in a French corpus)
with an accuracy of 23%. Also the evaluation set-
up is sometimes not clear: The high results in Poe-
sio et al (2004) cannot be used for comparison as
they test unrealistically: they distinguish only be-
tween the correct antecedent and one or three false
candidates (baseline of 50% for the former). They
also restrict the phenomenon to part-of relations.
There is a partial overlap between bridging and
implicit noun roles (Ruppenhofer et al, 2010).
However, work on implicit noun roles is mostly
focused on few predicates (e.g. Gerber and Chai
(2012)). We consider all bridging anaphors in run-
ning text. The closest work to ours interpreting im-
plicit role filling as anaphora resolution is Silberer
and Frank (2012).
3 Corpus for Bridging: An Overview
We use the dataset we created in Markert et al
(2012) with almost 11,000 NPs annotated for infor-
mation status including 663 bridging NPs and their
antecedents in 50 texts taken from the WSJ portion
of the OntoNotes corpus (Weischedel et al, 2011).
Bridging anaphora can be any noun phrase. They
908
are not limited to definite NPs as in previous work.
In contrast to Nissim et al (2004), antecedents are
annotated and can be noun phrases, verb phrases or
even clauses. Our bridging annotation is also not
limited with regards to semantic relations between
anaphor and antecedent.
In Markert et al (2012) we achieved high agree-
ment for the overall information status annotation
scheme between three annotators (? between 75 and
80, dependent on annotator pairs) as well as for all
subcategories, including bridging (? over 60 for all
annotator pairings, over 70 for two expert annota-
tors). Here, we add the following new results:
? Agreement for selecting bridging antecedents
was around 80% for all annotator pairings.
? Surprisingly, only 255 of the 663 (38%) bridg-
ing anaphors are definite NPs, which calls into
question the strategy of prior approaches to limit
themselves to these types of bridging.
? NPs are the most frequent antecedents by far
with only 42 of 663 (6%) bridging anaphora hav-
ing a non-NP antecedent (mostly verb phrases).
? Bridging is a relatively local phenomenon with
71% of NP antecedents occurring in the same or
up to 2 sentences prior to the anaphor. However,
farther away antecedents are common when the
antecedent is the global focus of a document.
? The semantic relations between anaphor and an-
tecedent are extremely diverse with only 92 of
663 (14%) anaphors having a part-of/attribute-
of antecedent (see Example 1) and only 45 (7%)
anaphors standing in a set relationship to the an-
tecedent (see Example 2). This contrasts with
Gardent and Manue?lian?s (2005) finding that
52% of bridging cases had meronymic relations.
We find many different types of relations in our
corpus, including encyclopedic relations such as
restaurant ? the waiter as well as, frequently,
relational person nouns as bridging anaphors
such as friend, husband, president.
? There are only a few cases of bridging where
surface cues may indicate the antecedent. First,
some bridging anaphors are modified by a small
number of adjectives that have more than one
role filler, with the bridging relation often being
temporal or spatial sequence between two enti-
ties of the same semantic type as in Example 3
(see also Lassalle and Denis (2011) for a dis-
cussion of such cases). Second, some anaphors
are compounds where the nominal premodifier
matches the antecedent head as in Example 4.
(2) Still employees do occasionally try to smuggle
out a gem or two. One man wrapped several dia-
monds in the knot of his tie. Another poked a hole
in the heel of his shoe. None made it past the body
searches . . .
(3) His truck is parked across the field . . . The
farmer at the next truck shouts . . .
(4) . . . it doesn?t make the equipment needed to
produce those chips. And IBM worries that the
Japanese will take over that equipment market.
4 Models for Bridging Resolution
4.1 Pairwise mention-entity model
The pairwise model is widely used in coreference
resolution (Soon et al, 2001). We adapt it for bridg-
ing resolution4: Given an anaphor mention m and
the set of antecedent candidate entities Em which
appear before m, we create a pairwise instance
(m, e) for every e ? Em. A binary decision whether
m is bridged to e is made for each instance (m, e)
separately. A post-processing step to choose one an-
tecedent is necessary (closest first or best first are
common strategies). This model causes three prob-
lems for bridging resolution: First, the ratio between
positive and negative instances is 1 to 17 even if only
antecedent candidates from the current and the im-
mediately preceding two sentences are considered.
The ratio will be even worse with a larger win-
dow size. Therefore, usually a fixed window size is
used restricting the set of candidates. This, however,
causes a second problem: antecedents which are be-
yond the window cannot be found. In our data, only
81% of NP antecedents appear within the previous 5
sentences, and only 71% of NP antecedents appear
within the previous 2 sentences. The third problem
is a shortcoming of the pairwise model itself: deci-
sions are made for each instance separately, ignoring
4Different from coreference, we treat an anaphor as a men-
tion and an antecedent as an entity. The anaphor is the first
mention of the corresponding entity in the document.
909
relations between instances. We resolve these prob-
lems by employing a global model based on Markov
logic networks.
4.2 Markov Logic Networks
Bridging can be considered a document global phe-
nomenon, where globally salient entities are pre-
ferred as antecedents and two or more anaphors hav-
ing the same antecedent should be related or similar.
Motivated by this observation, we explore Markov
logic networks (Domingos and Lowd, 2009, MLNs)
to model bridging resolution on the global discourse
level.
MLNs are a powerful representation for joint
inference with uncertainty. An MLN consists
of a set of pairs (Fi, wi), where Fi is a formula
in first-order logic and wi is its associated real
numbered weight. It can be viewed as a template for
constructing Markov networks. Given different sets
of constants, an MLN will produce different ground
Markov networks which may vary in size but have
the same structure and parameters. For a ground
Markov network, the probability distribution over
possible worlds x is given by
P (X = x) = 1Z exp
(
?
i
wini(x)
)
(1)
where ni(x) is the number of true groundings of Fi
in x. The normalization factor Z is the partition
function.
MLNs have been applied to many NLP tasks and
achieved good performance by leveraging rich re-
lations among objects (Poon and Domingos, 2008;
Meza-Ruiz and Riedel, 2009; Fahrni and Strube,
2012, inter alia). We use thebeast5 to learn weights
for the formulas and to perform inference. thebeast
employs cutting plane inference (Riedel, 2008) to
improve the accuracy and efficiency of MAP infer-
ence for Markov logic.
With MLNs, we model bridging resolution glob-
ally on the discourse level: given the set M of all
anaphors and sets of local antecedent candidates Em
for each anaphor m ? M , we select antecedents for
all anaphors from E =?m?M Em at the same time.
Table 1 shows the hidden predicates and formulas
used. Each formula is associated with a weight. The
5http://code.google.com/p/thebeast
polarity of the weights is indicated by the leading
+ or ?. The weight value (except for hard con-
straints) is learned from training data. For some for-
mulas the final weight consists of a learned weight
w multiplied by a score d (e.g. inverse distance be-
tween antecedent and anaphor). In these cases the
final weight for a formula in a ground Markov net-
work does not just depend on the respective formula,
but also on the specific constants. We indicate such
combined weights by the term w ? d.
We tackle the previously mentioned problems of
the pairwise model: (1) We construct hard con-
straints to specify that each anaphor has at most
one antecedent entity (Table 1: f1) and that the an-
tecedent must precede the anaphor (f2). This elim-
inates the need for the post-processing step in the
pairwise model. (2) We select the antecedent en-
tity for each anaphor from the antecedent candidate
entities pool E which alleviates the missing true
antecedent problem in the pairwise model. Based
on (1) and (2), MLNs allow us to express relations
between anaphor-anaphor and anaphor-antecedent
pairs ((m,n) or (m,e)) on the global discourse level
improving accuracy by performing joint inference.
5 Features
5.1 Local features
5.1.1 Poesio et al?s feature set
Table 2 shows the feature set proposed by Poesio et
al. (2004) for part-of bridging. Google distance is
the inverse value of Google hit counts for the ofPat-
tern query (e.g. the windows of the center). Word-
Net distance is the inverse value of the shortest path
length between an anaphor and an antecedent candi-
date among all synset combinations. These features
are supposed to capture the meronymy relation be-
tween anaphor and antecedent. The other ones mea-
sure the salience of the antecedent candidate.
Group Feature Value
lexical Google distance numeric
WordNet distance numeric
salience utterance distance numeric
local first mention boolean
global first mention boolean
Table 2: Poesio et al?s feature set
910
Hidden predicates
p1 isBridging(m, e)
p2 hasSameAntecedent (m,n)
Formulas
Hard constraints
f1 ?m ? M : |e ? E : isBridging(m, e)| ? 1
f2 ?m ? M?e ? E : hasPairDistance(e,m, d) ? d < 0 ? ?isBridging(m, e)
f3 ?m,n ? M : m 6= n ? hasSameAntecedent (m,n)
? hasSameAntecedent (n,m)
f4 ?m,n, l ? M : m 6= n ?m 6= l ? n 6= l ? hasSameAntecedent (m,n)
? hasSameAntecedent (n, l) ? hasSameAntecedent (m, l)
f5 ?m,n ? M?e ? E : m 6= n ? hasSameAntecedent (m,n) ? isBridging(m, e)
? isBridging(n, e)
f6 ?m,n ? M?e ? E : m 6= n ? isBridging(m, e) ? isBridging(n, e)
? hasSameAntecedent (m,n)
Discourse level formulas
f7 + (w) ?m ? M?e ? E : predictedGlobalAnte(e) ? hasPairDistance(e,m, d)
? d > 0 ? isBridging(m, e)
f8 + (w) ?m,n ? M conjunction(m,n) ? hasSameAntecedent (m,n)
f9 + (w) ?m,n ? M sameHead(m,n) ? hasSameAntecedent (m,n)
f10 + (w) ?m,n ? M similarTo(m,n) ? hasSameAntecedent (m,n)
f11 + (w) ?m ? M?e ? E : hasSemanticClass (m, ?rolePerson?)
? hasSemanticClass(e, ?org|gpe?) ? hasPairDistance(e,m, d) ? d > 0
? isBridging(m, e)
f12 + (w ? d) ?m ? M?e ? E : hasSemanticClass (m, ?relativePerson?)
? hasSemanticClass(e, ?otherPerson?) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
f13 + (w ? d) ?m ? M?e ? E : hasSemanticClass (m, ?date?)
? hasSemanticClass(e, ?date?) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
Local formulas
f14 + (w) ?m ? M ?e ? Em : isTopRelativeRankPrepPattern (m, e) ? isBridging(m, e)
f15 + (w) ?m ? M ?e ? Em : isTopRelativeRankVerbPattern(m, e) ? isBridging(m, e)
f16 + (w ? d) ?m ? M ?e ? Em : isPartOf (m, e) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
f17 + (w) ?m ? M ?e ? Em : isTopRelativeRankDocSpan (m, e) ? isBridging(m, e)
f18 ? (w) ?m ? M ?e ? Em : isSameHead(m, e) ? isBridging(m, e)
f19 + (w) ?m ? M ?e ? Em : isPremodOverlap(m, e) ? isBridging(m, e)
f20 ? (w) ?m ? M ?e ? Em : isCoArgument(m, e) ? isBridging(m, e)
Table 1: Hidden predicates and formulas used for bridging resolution (m,n, l represent mentions, M the set of bridging
anaphora mentions in the whole document, e the antecedent candidate entity, Em the set of local antecedent candidate
entities for m, and E =
?
m?M Em )
911
5.1.2 Other features
Since Poesio et al (2004) deal exclusively with
meronymy bridging, we have to extend the fea-
ture set to capture more diverse relations between
anaphor and antecedent. All numeric features in Ta-
ble 3 are normalized among all antecedent candi-
dates of one anaphor. For anaphor mi and its an-
tecedent candidates Emi (eij ? Emi), the numeric
score for pair {mi, eik} is Sik. Then the value
NormSik for this pair is normalized (set to values
between 0 and 1) as below:
NormSik =
Sik ?minj Sij
maxj Sij ?minj Sij
(2)
A second variant of numeric features tells whether
the score of an anaphor-antecedent candidate pair is
the highest among all pairs for this anaphor.
Group Feature Value
semantic feat1 preposition pattern numeric
feat2 verb pattern numeric
feat3 WordNet partOf boolean
feat4 semantic class nominal
salience feat5 document span numeric
surface feat6 isSameHead boolean
feat7 isPremodOverlap boolean
syntactic feat8 isCoArgument boolean
Table 3: Local features we developed
Preposition pattern (feat1). The ofPattern pro-
posed by Poesio et al (2004) is useful for part-of
and attribute-of relations but cannot cover all bridg-
ing relations (such as sanctions against a country).
We extend the ofPattern to a generalised preposition
pattern by using the Gigaword (Parker et al, 2011)
and the Tipster (Harman and Liberman, 1993) cor-
pora (both automatically POS tagged and NP chun-
ked for improving query match precision).
First, we extract the three most highly associ-
ated prepositions for each anaphor. Then for each
anaphor-antecedent candidate pair, we use their head
words to create the query ?anaphor preposition an-
tecedent?. To improve recall, we take lowercase,
uppercase, singular and plural forms of the head
word into account, and replace proper names by
fine-grained named entity types (using a gazetteer).
All raw hit counts are converted into the Dunning
Root Loglikelihood association measure,6 then nor-
malized using Formula 2 within all antecedent can-
didates of one anaphor.
Verb pattern (feat2). A set-membership rela-
tion between anaphor and antecedent is often hard
to capture by the preposition pattern because the
anaphor often has no common noun head (see Ex-
ample 2 in Section 3). Hence, we measure the com-
patibility of the antecedent candidates with the verb
the anaphor depends on.
First, we hypothesise that anaphors whose lexi-
cal head is a pronoun or a number are potential set
bridging cases and then extract the verb the anaphor
depends on. In example 2, for the set anaphor An-
other, poked is the verb. Then for each antecedent
candidate, subject-verb or verb-object queries are
applied to the Web 1T 5-gram corpus (Brants and
Franz, 2006). In this case, employees poked and di-
amonds poked are example queries. The hit counts
are transformed into PMI and all pairs for one
anaphor are normalized as described in Formula 2.
WordNet partOf relation (feat3). To capture
part-of bridging, we extract whether the anaphor is
part of the antecedent candidate in WordNet. To im-
prove recall, we use hyponym information of the
antecedent. If an antecedent e is a hypernym of x
and an anaphor m is a meronym of x, then m is a
meronym of e.
Semantic class (feat4). The anaphor and the an-
tecedent candidate are assigned one of 16 coarse-
grained semantic classes, e.g. location, organiza-
tion, GPE, roleperson, relativePerson, otherPerson7,
product, language, NORP (nationalities, religious
or political groups) and several classes for numbers
(such as date, money or percent).
Salience feature (feat5). Salient entities are pre-
ferred as antecedents. We capture salience super-
ficially by computing the ?antecedent document
span? of an antecedent candidate. We compute the
6http://tdunning.blogspot.de/2008/03/
surprise-and-coincidence.html
7We use WordNet to extract lists for rolePerson (persons like
president or teacher playing a role in an organization) and rela-
tivePerson (persons like father or son indicating that they have
a relation with another person). Persons not in these two lists
are counted as otherPerson.
912
span of text (measured in sentences) in which the
antecedent candidate entity is mentioned. This is di-
vided by the number of sentences in the whole doc-
ument. This score is normalized using Formula 2 for
all antecedent candidates of one anaphor.
Surface features (feat6-feat7). isSameHead
(feat6) checks whether antecedent candidates have
the same head as the anaphor: this is rarely the
case in bridging anaphora (except in some cases
of set bridging and spatial/temporal sequence, see
Example 3) and can therefore be used to exclude
antecedent candidates. isPremodOverlap (feat7)
determines the antecedent for compound noun
anaphors whose head is prenominally modified by
the antecedent head (see Example 4).
Syntactic feature (feat8) The isCoArgument fea-
ture is based on the intuition that the subject can-
not be the bridging antecedent of the object in
the same clause. This feature excludes (some)
close antecedent candidates. In Example 4, the an-
tecedent candidate the Japanese isCoArgument with
the anaphor that equipment market.
5.2 Global features for MLNs
f1-f13 in Table 1 are discourse level constraints.
All antecedent candidates come from the antecedent
candidates pool E in the whole document.
Global salience (Table 1: f3-f10). The salience
feature in the pairwise model only measures the
salience for candidates within the local window.
However, globally salient antecedents are preferred
even if they are far away from the anaphor. We
model this from two perspectives:
f7 models the preference for globally salient an-
tecedents, which we derive for each document. For
m ? M and e ? E, let score(m, e) be the prepo-
sition pattern score for pair (m,e). Calculate pattern
semantic salience score esal for each e ? E as
esal =
?
m?M
score(m, e) (3)
If e appears in the title and also has the highest
pattern semantic salience score esal among all e in
E, then e is the predicted globally salient antecedent
for this document. Note that global salience here is
based on semantic connectivity to all anaphors in the
document and that not every document has a glob-
ally salient antecedent.
f3-f6 and f8-f10 model that similar or related
anaphors in one document are likely to have the
same antecedent. To make the ground Markov net-
work more sparse for more efficient inference, we
add the hidden predicate (p2) and hard constraints
(f3-f6) specifying relations among similar/related
anaphors m, n and l (reflexivity and transitivity).
Formulas f8-f10 explore three different ways (syn-
tactic and semantic) to compute the similarity be-
tween two anaphors. In f10, we use SVMlight (simi-
larity scores from WordNet plus sentence distance as
features) to predict whether two anaphors not shar-
ing the same head are similar or not.
Frequent bridging relations (Table 1: f11-f13).
Three common bridging relations are restricted by
semantic class of anaphor and antecedent (see also
Section 3). It is worth noting that in formula f11
(modeling that a role person mention like presi-
dent or chairman prefers organization or GPE an-
tecedents), we do not penalize the antecedents far
away from the anaphor. In formula f12 (modeling
that a relativePerson mention such as mother or hus-
band prefers close person antecedents) and f13, we
prefer close antecedents by including the distance
between antecedent and anaphor into the weights.
MLN formulation of local features (Table 1: f14-
f20). Corresponding to features of the pairwise
model (Table 3) ? we exclude only semantic class
as this is modelled globally via features f11-f13.
These local features are only used for an anaphor m
and its local antecedent candidate e from Em.
6 Experiments and Results
6.1 Experimental setup
We perform experiments on our gold standard cor-
pus via 10-fold cross-validation on documents. We
use gold standard mentions, true coreference infor-
mation, and the OntoNotes named entity and syntac-
tic annotation layers for feature extraction.
6.2 Improved baseline
We reimplement the algorithm from Poesio et al
(2004) as baseline. Since they did not explain
913
whether they used the mention-mention or mention-
entity model, we assume they treated antecedents as
entities and use a 2 and 5 sentence window for can-
didates8. Since the GoogleAPI is not available any
more, we use the Web 1T 5-gram corpus (Brants and
Franz, 2006) to extract the Google distance feature.
We improve it by taking all information about en-
tities via coreference into account as well as by re-
placing proper names. All other features (Table 2
in Section 5.1.1) are extracted as Poesio et al did.
A Naive Bayes classifier with standard settings in
WEKA (Witten and Frank, 2005) is used. In order
to evaluate their model in the more realistic setting
of our experiment, we apply the best first strategy to
select the antecedent for each anaphor.
6.3 Pairwise models
Pairwise model I: We use the preposition pattern
feature (feat1) plus Poesio et al?s salience features
(Table 2). We use a 2 sentence window as it per-
formed on a par with the 5 sentence window in the
baseline. We replace Naive Bayes with SVMlight
because it can deal better with imbalanced data9.
Pairwise model II: Based on Pairwise model I.
Local features feat2-feat8 from Table 2 are added.
Pairwise model III: Based on Pairwise model II.
We apply a more advanced antecedent candidate se-
lection strategy, which allows to include 77% of NP
antecedents compared to 71% in Pairwise model II.
For each anaphor, we add the top k salient enti-
ties measured through the length of the coreference
chains (k is set to 10%) as additional antecedent can-
didates. For potential set anaphors (as automatically
determined by pronoun or number heads), singu-
lar antecedent candidates are filtered out. We com-
piled a small set of adjectives (using FrameNet and
thesauri) that indicate spatial or temporal sequences
(see Example 3). For anaphors modified by such ad-
jectives we consider only antecedent candidates that
have the same semantic class as the anaphor.
8They use a 5 sentence window, because all antecedents in
their corpus are within the previous 5 sentences.
9The SVMlight parameter is set according to the ratio be-
tween positive and negative instances in the training set.
6.4 MLN models
MLN model I: MLN system using local formu-
las f1-f2 and f14-f20. The same strategy as in
Pairwise model III is used to select local antecedent
candidates Em for each anaphor m.
MLN model II: Based on MLN model I, all for-
mulas in Table 1 are used.
6.5 Results
Table 4 shows the comparison of our models to base-
lines. Significance tests are conducted using McNe-
mar?s test on overall accuracy at the level of 1%.
acc
improved baseline 2 sent. + NB 18.85
5 sent. + NB 18.40
pairwise model pairwise model I 29.11
pairwise model II 33.94
pairwise model III 36.35
MLN model MLN model I 35.60
MLN model II 41.32
Table 4: Results for MLN models compared to pairwise
models and baselines.
MLN model II, which is inspired by the linguis-
tic observation that globally salient entities are pre-
ferred as antecedents, performs significantly better
than all other systems. The gains come from three
aspects. First, by selecting the antecedent for each
anaphor from the antecedent candidate pool E in the
whole document 91% of NP antecedents are acces-
sible compared to 77% in pairwise model III. Sec-
ond, we leverage semantics and salience by using
local formulas and discourse level formulas. Lo-
cal formulas are used to capture semantic relations
for bridging pairs as well as surface and syntactic
constraints. Global formulas resolve several bridg-
ing anaphors together, often to a globally salient an-
tecedent beyond the local window. Third, the model
allows us to express specific relations among bridg-
ing anaphors and their antecedents (f11-f13).
However, our pairwise model I already outper-
forms improved baselines by about 10%, which sug-
gests that our preposition pattern feature can capture
more diverse semantic relations. The continuous im-
provements shown in pairwise model II and pair-
wise model III verify the contribution of our other
914
features and advanced antecedent candidate selec-
tion strategy. pairwise model III would become too
complex if we tried to integrate discourse level for-
mulas f7, f11-f13 into antecedent candidate selec-
tion. MLN model II solves this task elegantly.
6.6 Discussion and error analysis
We analyse our best model (MLN model II) and
compare it to the best local one (pairwise model III).
Anaphors with long distance antecedents are
harder to resolve. Table 5 shows the compari-
son of correctly resolved anaphors with regard to
anaphor-antecedent distance. We can see that the
global model is equal or better to the local model
for all anaphor types but that the difference is espe-
cially large for anaphora with antecedents that are
3 or more sentences away due to the use of global
salience and accessibility of possible antecedents
beyond a fixed window-size.
# pairs MLN II pairwise III
sent. distance
0 175 48.57 45.14
1 260 34.62 35
2 90 47.78 43.33
?3 158 35.44 16.46
Table 5: Comparison of the percentage of correctly re-
solved anaphors with regard to anaphor-antecedent dis-
tance. Significance tests are conducted using McNemar?s
test at the level of 1%.
We now distinguish between ?sibling anaphors?
(anaphors that share an antecedent with other bridg-
ing anaphors) and ?non-siblings? (anaphors that do
not share an antecedent with any other anaphor).
The performance of our MLN model II is 54%
on sibling anaphors but only 24% on non-sibling
anaphors. This shows that our use of global salience
and links between related anaphors does indeed help
to capture the behaviour of sibling anaphors.
However, our global model is good at predicting
the right antecedent for sibling anaphors where the
antecedent is globally salient but not as good for sib-
ling anaphors where the (shared) antecedent is a lo-
cally salient subtopic. Thus, in the future we need
to model equivalent constraints for local salience
of antecedents, taking into account topic segmen-
tation/shifts to improve over the 54% for sibling
anaphors.
The semantic knowledge we employ is still in-
sufficient. Typical cases where we have problems
are: (i) cases with very context-specific bridging re-
lations. For example, in one text about the stealing
of Sago Palms in California we found the thieves
as a bridging anaphor with the antecedent palms,
which is not a very usual semantic link. (ii) more
frequently, we have cases where several good an-
tecedents from a semantic perspective can be found.
For example, two laws are discussed and a later
anaphor the veto could be the veto of either bills.
Integration of the wider context apart from the two
NPs is necessary in these cases. This includes the se-
mantics of modification, whereas we currently con-
sider only head noun knowledge. An example is that
the anaphor the local council would preferably be
interpreted as the council of a village instead of the
council of a state due to the occurrence of local.
Finally, 6% of the anaphors in our corpus have a
non-NP antecedent. These cases are not correctly
resolved in our current model as we only extract NP
phrases as potential candidate antecedents.
7 Conclusions
We provide the first reasonably sized and reliably
annotated English corpus for bridging resolution. It
covers a diverse set of relations between anaphor and
antecedent as well as all anaphor/antecedent types.
We developed novel semantic, syntactic and salience
features based on linguistic intuition. Inspired by
the observation that salient entities are preferred as
antecedents, we implemented a global model for an-
tecedent selection within the framework of Markov
logic networks. We show that our global model sig-
nificantly outperforms other local models and base-
lines. This work is ? to our knowledge ? the first
bridging resolution algorithm that tackles the unre-
stricted phenomenon in a real setting.
Acknowledgements. Yufang Hou is funded by a PhD
scholarship from the Research Training Group Coher-
ence in Language Processing at Heidelberg University.
Katja Markert receives a Fellowship for Experienced Re-
searchers by the Alexander-von-Humboldt Foundation.
We thank HITS gGmbH for hosting Katja Markert and
funding the annotation. We thank our colleague Angela
Fahrni for advice on using Markov logic networks.
915
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. LDC2006T13, Philadelphia, Penn.: Lin-
guistic Data Consortium.
Aoife Cahill and Arndt Riester. 2012. Automatically ac-
quiring fine-grained information status distinctions in
German. In Proceedings of the SIGdial 2012 Confer-
ence: The 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, Seoul, Korea, 5?6
July 2012, pages 232?236.
Tommaso Caselli and Irina Prodanof. 2006. Annotat-
ing bridging anaphors in Italian: In search of reliabil-
ity. In Proceedings of the 5th International Conference
on Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006.
Herbert H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, Cambridge, Mass., June 1975, pages 169?
174.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan Claypool Publishers.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with Markov logic. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics,
Mumbai, India, 8?15 December 2012, pages 815?832.
Kari Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Seman-
tics, 7:395?433.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Matthew Gerber and Joyce Chai. 2012. Semantic role
labeling of implicit arguments for nominal predicates.
Computational Linguistics, 38(4):756?798.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Donna Harman and Mark Liberman. 1993. TIPSTER
Complete. LDC93T3A, Philadelphia, Penn.: Linguis-
tic Data Consortium.
Iorn Korzen and Matthias Buch-Kromann. 2011.
Anaphoric relations in the Copenhagen dependency
treebanks. In S. Dipper and H. Zinsmeister, edi-
tors, Corpus-based Investigations of Pragmatic and
Discourse Phenomena, volume 3 of Bochumer Lin-
guistische Arbeitsberichte, pages 83?98. University of
Bochum, Bochum, Germany.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridging
resolution in French. In Proceedings of the 8th Dis-
course Anaphora and Anaphor Resolution Colloquium
(DAARC 2011), Faro, Algarve, Portugal, 6?7 October
2011, pages 35?46.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop on
the Computational Treatment of Anaphora. Budapest,
Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju Is-
land, Korea, 8?14 July 2012, pages 795?804.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In Proceedings of Human Language
Technologies 2009: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Boulder, Col., 31 May ? 5 June
2009, pages 155?163.
Shachar Mirkin, Ido Dagan, and Sebastian Pado?. 2010.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010, pages 1209?
1219.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. LDC2011T07.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio. 2003. Associate descriptions and
salience: A preliminary investigation. In Proceedings
916
of the EACL Workshop on the Computational Treat-
ment of Anaphora. Budapest, Hungary, 14 April 2003,
pages 31?38.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25?27 October 2008, pages 650?
659.
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Altaf Rahman and Vincent Ng. 2012. Learning the fine-
grained information status of discourse entities. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France, 23?27 April 2012, pages 798?
807.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for Markov logic. In Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, Helsinki, Finland, 9?12 July 2008,
pages 468?475.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A recursive annotation scheme for referential informa-
tion status. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
La Valetta, Malta, 17?23 May 2010, pages 717?722.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking events and their participants
in discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), Up-
psala, Sweden, 15?16 July 2010, pages 45?50.
Carina Silberer and Anette Frank. 2012. Casting
implicit role linking as an anaphora resolution task.
In Proceedings of STARSEM 2012: The First Joint
Conference on Lexical and Computational Semantics,
Montre?al, Que?bec, Canada, 7?8 June 2012, pages 1?
10.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, Cal., 2nd edition.
917
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 749?759,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fine-grained Genre Classification using Structural Learning Algorithms
Zhili Wu
Centre for Translation Studies
University of Leeds, UK
z.wu@leeds.ac.uk
Katja Markert
School of Computing
University of Leeds, UK
scskm@leeds.ac.uk
Serge Sharoff
Centre for Translation Studies
University of Leeds, UK
s.sharoff@leeds.ac.uk
Abstract
Prior use of machine learning in genre
classification used a list of labels as clas-
sification categories. However, genre
classes are often organised into hierar-
chies, e.g., covering the subgenres of fic-
tion. In this paper we present a method
of using the hierarchy of labels to improve
the classification accuracy. As a testbed
for this approach we use the Brown Cor-
pus as well as a range of other corpora, in-
cluding the BNC, HGC and Syracuse. The
results are not encouraging: apart from the
Brown corpus, the improvements of our
structural classifier over the flat one are
not statistically significant. We discuss the
relation between structural learning per-
formance and the visual and distributional
balance of the label hierarchy, suggesting
that only balanced hierarchies might profit
from structural learning.
1 Introduction
Automatic genre identification (AGI) can be
traced to the mid-1990s (Karlgren and Cutting,
1994; Kessler et al, 1997), but this research be-
came much more active in recent years, partly be-
cause of the explosive growth of the Web, and
partly because of the importance of making genre
distinctions in NLP applications. In Information
Retrieval, given the large number of web pages on
any given topic, it is often difficult for the users
to find relevant pages that are in the right genre
(Vidulin et al, 2007). As for other applications,
the accuracy of many tasks, such as machine trans-
lation, POS tagging (Giesbrecht and Evert, 2009)
or identification of discourse relations (Webber,
2009) relies of defining the language model suit-
able for the genre of a given text. For example,
the accuracy of POS tagging reaching 96.9% on
newspaper texts drops down to 85.7% on forums
(Giesbrecht and Evert, 2009), i.e., every seventh
word in forums is tagged incorrectly.
This interest in genres resulted in a prolifer-
ation of studies on corpus development of web
genres and comparison of methods for AGI. The
two corpora commonly used for this task are KI-
04 (Meyer zu Eissen and Stein, 2004) and San-
tinis (Santini, 2007). The best results reported for
these corpora (with 10-fold cross-validation) reach
84.1% on KI-04 and 96.5% accuracy on Santinis
(Kanaris and Stamatatos, 2009). In our research
(Sharoff et al, 2010) we produced even better re-
sults on these two benchmarks (85.8% and 97.1%,
respectively). However, this impressive accuracy
is not realistic in vivo, i.e., in classifying web
pages retrieved as a result of actual queries. One
reason comes from the limited number of genres
present in these two collections (eight genres in
KI-04 and seven in Santinis). As an example, only
front pages of online newspapers are listed in San-
tinis, but not actual newspaper articles, so once an
article is retrieved, it cannot be assigned to any
class at all. Another reason why the high accu-
racy is not useful concerns the limited number of
sources in each collection, e.g., all FAQs in Santi-
nis come from either a website with FAQs on hur-
ricanes or another one with tax advice. In the end,
a classifier built for FAQs on this training data re-
lies on a high topic-genre correlation in this par-
ticular collection and fails to spot any other FAQs.
There are other corpora, which are more diverse
in the range of their genres, such as the fifteen
genres of the Brown Corpus (Kuc?era and Fran-
cis, 1967) or the seventy genres of the BNC (Lee,
2001), but because of the number of genres in
them and the diversity of documents within each
genre, the accuracy of prior work on these collec-
tions is much less impressive. For example, Karl-
gren and Cutting (1994) using linear discriminant
analysis achieve an accuracy of 52% without us-
749
ing cross-validation (the entire Brown Corpus was
used as both the test set and training set), with the
accuracy improving to 65% when the 15 genres
are collapsed into 10, and to 73% with only 4 gen-
res (Figure 1). This result suggests the importance
of the hierarchy of genres. Firstly, making a deci-
sion on higher levels might be easier than on lower
levels (fiction or non-fiction rather than science
fiction or mystery). Secondly, we might be able
to improve the accuracy on lower levels, by taking
into account the relevant position of each node in
the hierarchy (distinguishing between reportage
or editorial becomes easier when we know they
are safely under the category of press).
Figure 1: Hierarchy of Brown corpus.
This paper explores a way of using information on
the hierarchy of labels for improving fine-grained
genre classification. To the best of our knowl-
edge, this is the first work presenting structural
genre classification and distance measures for gen-
res. In Section 2 we present a structural reformula-
tion of Support Vector Machines (SVMs) that can
take similarities between different genres into ac-
count. This formulation necessitates the develop-
ment of distance measures between different gen-
res in a hierarchy, of which we present three dif-
ferent types in Section 3, along with possible esti-
mation procedures for these distances. We present
experiments with these novel structural SVMs and
distance measures on three different corpora in
Section 4. Our experiments show that structural
SVMs can outperform the non-structural standard.
However, the improvement is only statistically sig-
nificant on the Brown corpus. In Section 5 we
investigate potential reasons for this, including
the (im)balance of different genre hierarchies and
problems with our distance measures.
2 Structural SVMs
Discriminative methods are often used for clas-
sification, with SVMs being a well-performing
method in many tasks (Boser et al, 1992;
Joachims, 1999). Linear SVMs on a flat list of
labels achieve high efficiency and accuracy in text
classification when compared to nonlinear SVMs
or other state-of-the-art methods. As for structural
output learning, a few SVM-based objective func-
tions have been proposed, including margin for-
mulation for hierarchical learning (Dekel et al,
2004) or general structural learning (Joachims
et al, 2009; Tsochantaridis et al, 2005). But many
implementations are not publicly available, and
their scalability to real-life text classification tasks
is unknown. Also they have not been applied to
genre classification.
Our formulation can be taken as a special in-
stance of the structural learning framework in
(Tsochantaridis et al, 2005). However, they con-
centrate on more complicated label structures as
for sequence alignment or parsing. They proposed
two formulations, slack-rescaling and margin-
rescaling, claiming that margin-rescaling has two
disadvantages. First, it potentially gives signifi-
cant weight to output values that might not be eas-
ily confused with the target values, because every
increase in the loss increases the required margin.
However, they did not provide empirical evidence
for this claim. Second, margin rescaling is not
necessarily invariant to the scaling of the distance
matrix. We still used margin-rescaling because it
allows us to use the sequential dual method for
large-scale implementation (Keerthi et al, 2008),
which is not applicable to the slack-rescaling for-
mulation. For web page classification we will
need fast processing. In addition, we performed
model calibration to address the second disadvan-
tage (distance matrix invariance).
Let x be a document and wm a weight vector
associated with the genre class m in a corpus with
k genres at the most fine-grained level. The pre-
dicted class is the class achieving the maximum
inner product between x and the weight vector for
the class, denoted as,
argmax
m
wTmx,?m. (1)
750
Accurate prediction requires that when a docu-
ment vector is multiplied with the weight vector
associated with its own class, the resulting inner
product should be larger than its inner products
with a weight vector for any other genre class m.
This helps us to define criteria for weight vectors.
Let xi be the i?th training document, and yi its
genre label. For its weight vector wyi , the inner
productwTyixi should be larger than all other prod-
ucts wTmxi, that is,
wTyixi ?w
T
mxi ? 0,?m. (2)
To strengthen the constraints, the zero value on the
right hand side of the inequality for the flat SVM
can be replaced by a positive value, corresponding
to a distance measure h(yi,m) between two genre
classes, leading to the following constraint:
wTyixi ?w
T
mxi ? h(yi,m),?m. (3)
To allow feasible models, in real scenarios such
constraints can be violated, but the degree of vio-
lation is expected to be small. For each document,
the maximum violation in the k constraints is of
interest, as given by the following loss term:
Lossi = max
m
{h(yi,m)?wTyixi +w
T
mxi}. (4)
Adding up all loss terms over all training docu-
ments, and further introducing a term to penalize
large values in the weight vectors, we have the
following objective function (C is a user-specified
nonnegative parameter).
min
m,i
:
1
2
k?
m=1
wTmwm + C
p?
i=1
Lossi. (5)
Efficient methods can be derived by borrowing the
sequential dual methods in (Keerthi et al, 2008)
or other optimization techniques (Crammer and
Singer, 2002).
3 Genre Distance Measures
The structural SVM (Section 2) requires a dis-
tance measure h between two genres. We can
derive such distance measures from the genre
hierarchy in a way similar to word similarity
measures that were invented for lexical hierar-
chies such as WordNet (see (Pedersen et al,
2007) for an overview). In the following,
we will first shortly summarise path-based and
information-based measures for similarity. How-
ever, information-based measures are based on
the information content of a node in a hierarchy.
Whereas the information content of a word or con-
cept in a lexical hierarchy has been well-defined
(Resnik, 1995), it is less clear how to estimate
the information content of a genre label. We will
therefore discuss several different ways of estimat-
ing information content of nodes in a genre hierar-
chy.
3.1 Distance Measures based on Path Length
If genre labels are organised into a tree (Figure 1),
one of the simplest ways to measure distance be-
tween two genre labels (= tree nodes) is path
length (h(a, b)plen):
f(a, LCS(a, b)) + f(b, LCS(a, b)), (6)
where a and b are two nodes in the tree,
LCS(a, b) is their Least Common Subsumer, and
f(a, LCS(a, b)) is the number of levels passed
through when traversing from a to the ancestral
node LCS(a, b). In other words, the distance
counts the number of edges traversed from nodes a
to b in the tree. For example, the distance between
Learned and Misc in Figure 1 would be 3.
As an alternative, the maximum path length
h(a, b)pmax to their least common subsumer can
be used to reduce the range of possible values:
max{f(a, LCS(a, b)), f(b, LCS(a, b))}. (7)
The Leacock & Chodorow similarity measure
(Leacock and Chodorow, 1998) normalizes the
path length measure (6) by the maximum number
of nodes D when traversing down from the root.
s(a, b)plsk = ?log((h(a, b)plen + 1)/2D). (8)
To convert it into a distance measure, we can
invert it h(a, b)plsk = 1/s(a, b)plsk.
Other path-length based measures include the
Wu & Palmer Similarity (Wu and Palmer, 1994).
s(a, b)pwupal =
2f(R,LCS(a, b))
(f(R, a) + f(R, b))
, (9)
where R describes the hierarchy?s root node. Here
similarity is proportional to the shared path from
the root to the least common subsumer of two
nodes. Since the Wu & Palmer similarity is always
between [0 1), we can convert it into a distance
measure by h(a, b)pwupal = 1? s(a, b)pwupal.
751
3.2 Distance Measures based on Information
Content
Path-based distance measures work relatively well
on balanced hierarchies such as the one in Figure 1
but fail to treat hierarchies with different levels
of granularity well. For lexical hierarchies, as a
result, several distance measures based on infor-
mation content have been suggested where the in-
formation content of a concept c in a hierarchy is
measured by (Resnik, 1995)
IC(c) = ?log(
freq(c)
freq(root)
). (10)
The frequency freq of a concept c is the sum of
the frequency of the node c itself and the frequen-
cies of all its subnodes. Since the root may be a
dummy concept, its frequency is simply the sum
of the frequencies of all its subnodes. The simi-
larity between two nodes can then be defined as
the information content of their least common sub-
sumer:
s(a, b)resk = IC(LCS(a, b)). (11)
If two nodes just share the root as their subsumer,
their similarity will be zero. To convert 11 into a
distance measure, it is possible to add a constant 1
to it before inverting it, as given by
h(a, b)resk = 1/(s(a, b)resk + 1). (12)
Several other similarity measures have been pro-
posed based on the Resnik similarity such as the
one by (Lin, 1998):
s(a, b)lin =
2IC(LCS(a, b))
IC(a) + IC(b)
. (13)
Again to avoid the effect of zero similarity when
defining the Lin?s distance we use:
h(a, b)lin = 1/(s(a, b)lin + 1). (14)
(Jiang and Conrath, 1997) directly define Jiang?s
distance (h(a, b)jng):
IC(a) + IC(b)? 2IC(LCS(a, b)). (15)
3.2.1 Information Content of Genre Labels
The notion of information content of a genre is not
straightforward. We use two ways of measuring
the frequency freq of a genre, depending on its
interpretation.
Genre Frequency based on Document Occur-
rence. We can interpret the ?frequency? of a
genre node simply as the number of all documents
belonging to that genre (including any of its sub-
genres). Unfortunately, there are no estimates for
genre frequencies on, for example, a representa-
tive sample of web documents. Therefore, we ap-
proximate genre frequencies from the document
frequencies (dfs) in the training sets used in clas-
sification. Note that (i) for balanced class distribu-
tions this information will not be helpful and (ii)
that this is a relatively poor substitute for an esti-
mation on an independent, representative corpus.
Genre Frequency based on Genre Labels. We
can also use the labels/names of the genre nodes
as the unit of frequency estimation. Then, the
frequency of a genre node is the occurrence fre-
quency of its label in a corpus plus the occurrence
frequencies of the labels of all its subnodes. Note
that there is no direct correspondence between this
measure and the document frequency of a genre:
measuring the number of times the potential genre
label poem occurs in a corpus is not in any way
equivalent to the number of poems in that corpus.
However, the measure is still structurally aware
as frequencies of labels of subnodes are included,
i.e. a higher level genre label will have higher
frequency (and lower information content) than a
lower level genre label.1
For label frequency estimation, we manually
expand any label abbreviations (such as "newsp"
for BNC genre labels), delete stop words and func-
tion words and then use two search methods. For
the search method word we simply search the fre-
quency of the genre label in a corpus, using three
different corpora (the BNC, Brown and Google
web search). As for the BNC and Brown cor-
pus some labels are very rarely mentioned, we for
these two corpora use also a search method gram
where all character 5-grams within the genre label
are searched for and their frequencies aggregated.
3.3 Terminology
Algorithms are prefixed by the kind of distance
measure they employ ? IC for Information con-
tent and p for path-based). If the measure is infor-
1Obviously when using this measure we rely on genre la-
bels which are meaningful in the sense that lower level labels
were chosen to be more specific and therefore probably rarer
terms in a corpus. The measure could not possibly be use-
ful on a genre hierarchy that would give random names to its
genres such as genre 1.
752
mation content based the specific measure is men-
tioned next, such as lin. The way for measuring
genre frequency is indicated last with df for mea-
suring via document frequency and word/gram
when measured via frequency of genre labels. If
frequencies of genre labels are used, the corpus
for counting the occurrence of genre labels is also
indicated via brown, bnc or the Web as estimated
by Google hit counts gg. Standard non-structural
SVMs are indicated by flat.
4 Experiments
4.1 Datasets
We use four genre-annotated corpora for genre
classification: the Brown Corpus (Kuc?era and
Francis, 1967), BNC (Lee, 2001), HGC (Stubbe
and Ringlstetter, 2007) and Syracuse (Crowston
et al, 2009). They have a wide variety of genre
labels (from 15 in the Brown corpus to 32 genres
in HGC to 70 in the BNC to 292 in Syracuse), and
different types of hierarchies.
4.2 Evaluation Measures
We use standard classification accuracy (Acc) on
the most fine-grained level of target categories in
the genre hierarchy.
In addition, given a structural distance H , mis-
classifications can be weighted based on the dis-
tance measure. This allows us to penalize incor-
rect predictions which are further away in the hi-
erarchy (such as between government documents
and westerns) more than "close" mismatches (such
as between science fiction and westerns). For-
mally, given the classification confusion matrix M
then each Mab for a 6= b contains the number
of class a documents that are misclassified into
class b. To achieve proper normalization in giv-
ing weights to misclassified entries, we can redis-
tribute a total weight k ? 1 to each row of H pro-
portionally to its values, where k is the number
of genres. That is, given g the row summation
of H , we define a weight matrix Q by normal-
izing the rows of H in a way given by Qab =
(k ? 1)hab/ga, a 6= b. We further assign a unit
value to the diagonal of Q. Then it is possible to
construct a structurally-aware measure (S-Acc):
S-Acc =
?
a
Maa/
?
a,b
MabQab. (16)
4.3 Experimental Setup
We compare structural SVMs using all path-based
and information-content based measures (see also
Section 3.3). As a baseline we use the accuracy
achieved by a standard "flat" SVM.
We use 10-fold (randomised) cross validation
throughout. In each fold, for each genre class 10%
of documents are used for testing. For the re-
maining 90%, a portion of 10% are sampled for
parameter tuning, leaving 80% for training. In
each round the validation set is used to help de-
termine the best C associated with Equation (5)
based on the validation accuracy from the candi-
date list 0.0001, 0.0005, 0.001, 0.005, 0.01,
0.05, 0.1, 0.5, 1. Note via this experiment setup,
all methods are tuned to their best performance.
For any algorithm comparison, we use a McNe-
mar test with the significance level of 5% as rec-
ommended by (Dietterich, 1998).
4.4 Features
The features used for genre classification are char-
acter 4-grams for all algorithms, i.e. each docu-
ment is represented by a binary vector indicating
the existence of each character 4-gram. We used
character n-grams because they are very easy to
extract, language-independent (no need to rely on
parsing or even stemming), and they are known
to have the best performance in genre classifica-
tion tasks (Kanaris and Stamatatos, 2009; Sharoff
et al, 2010).
4.5 Brown Corpus Results
The Brown Corpus has 500 documents and is or-
ganized in a hierarchy with a depth of 3. It
contains 15 end-level genres. In one experiment
in (Karlgren and Cutting, 1994) the subgenres un-
der fiction are grouped together, leading to 10 gen-
res to classify.
Results on 10-genre Brown Corpus. A stan-
dard flat SVM achieves an accuracy of 64.4%
whereas the best structural SVM based on Lin?s
information content distance measure (IC-lin-
word-bnc) achieves 68.8% accuracy, significantly
better at the 1% level. The result is also signif-
icantly better than prior work on the Brown cor-
pus in (Karlgren and Cutting, 1994) (who use the
whole corpus as test as well as training data). Ta-
ble 1 summarizes the best performing measures
that all outperform the flat SVM at the 1% level.
753
Table 1: Brown 10-genre Classification Results.
Method Accuracy
Karlgren and Cutting, 1994 65 (Training)
Flat SVM 64.40
SSVM(IC-lin-word-bnc) 68.80
SSVM(IC-lin-word-br) 68.60
SSVM(IC-lin-gram-br) 67.80
Figure 2 provides the box plots of accuracy scores.
The dashed boxes indicate that the distance mea-
sures perform significantly worse than the best
performing IC-lin-word-bnc at the bottom. The
solid boxes indicate the corresponding measures
are statistically comparable to the IC-lin-word-bnc
in terms of the mean accuracy they can achieve.
50 55 60 65 70 75 80
IC?lin?word?bncIC?lin?word?br
IC?jng?dfpwupal
IC?lin?gram?brIC?resk?word?bnc
IC?resk?word?ggplen
IC?resk?dfIC?lin?gram?bnc
IC?resk?gram?brIC?lin?df
IC?resk?gram?bncIC?resk?word?br
IC?lin?word?ggplsk
pmaxIC?jng?word?br
IC?jng?word?bncflat
IC?jng?gram?bncIC?jng?gram?br
IC?jng?word?gg
Accuracy
Figure 2: Accuracy on Brown Corpus (10 genres).
Results on 15-genre Brown Corpus. We per-
form experiments on all 15 genres on the end level
of the Brown corpus. The increase of genre classes
leads to reduced classification performance. In our
experiment, the flat SVM achieves an accuracy of
52.40%, and the structural SVM using path length
measure achieves 55.40%, a difference significant
at the 5% level. The structural SVMs using infor-
mation content measures IC-lin-gram-bnc and IC-
resk-word-br also perform equally well. In addi-
tion, we improve on the training accuracy of 52%
reported in (Karlgren and Cutting, 1994).
We are also interested in structural accuracy (S-
Acc) to see whether the structural SVMs make
fewer "big" mistakes. Table 2 shows a cross com-
parison of structural accuracy. Each row shows
how accurate the corresponding method is un-
der the structural accuracy criteria given in the
column. The ?no-struct? column corresponds to
vanilla accuracy. It is natural to expect each di-
agonal entry of the numeric table to be the high-
est, since the respective method is optimised for
its own structural distance. However, in our case,
Lin?s information content measure and the plen
measure perform well under any structural ac-
curacy evaluation measure and outperform flat
SVMs.
4.6 Other Corpora
In spite of the promising results on the Brown
Corpus, structural SVMs on other corpora (BNC,
HGC, Syracuse) did not show considerable im-
provement.
HGC contains 1330 documents divided into 32
approximately equally frequent classes. Its hierar-
chy has just two levels. Standard accuracy for the
best performing structural methods on HGC is just
the same as for flat SVM (69.1%), with marginally
better structural accuracy (for example, 71.39 vs.
71.04%, using a path-length based structural ac-
curacy). The BNC corpus contains 70 genres and
4053 documents. The number of documents per
class ranges from 2 to 501. The accuracy of SSVM
is also just comparable to flat SVM (73.6%). The
Syracuse corpus is a recently developed large col-
lection of 3027 annotated webpages divided into
292 genres (Crowston et al, 2009). Focusing only
on genres containing 15 or more examples, we ar-
rived at a corpus of 2293 samples and 52 genres.
Accuracy for flat (53.3%) and structural SVMs
(53.7%) are again comparable.
5 Discussion
Given that structural learning can help in topical
classification tasks (Tsochantaridis et al, 2005;
Dekel et al, 2004), the lack of success on genres
is surprising. We now discuss potential reasons for
this lack of success.
5.1 Tree Depth and Balance
Our best results were achieved on the Brown cor-
pus, whose genre tree has at least three attractive
properties. Firstly, it has a depth greater than 2,
i.e. several levels are distinguished. Secondly,
it seems visually balanced: branches from root
to leaves (or terminals) are of pretty much equal
length; branching factors are similar, for exam-
ple ranging between 2 and 6 for the last level of
branching. Thirdly, the number of examples at
754
Table 2: Structural Accuracy on Brown 15-genre Classification.
Method no-struct (=typical accuracy) IC-lin-gram-bnc plen IC-resk-word-br IC-jng-word-gg
flat 52.40 55.34 60.60 58.91 52.19
IC-lin-gram-bnc 55.00 58.15 63.59 61.83 53.85
plen 55.40 58.74 64.51 62.61 54.27
IC-resk-word-br 55.00 58.24 63.96 62.08 54.08
IC-jng-word-gg 46.00 49.00 54.89 53.01 52.58
each leaf node is roughly comparable (distribu-
tional balance).
The other hierarchies violate these properties to
a large extent. Thus, the genres in HGC are al-
most represented by a flat list with just one extra
level over 32 categories. Similarly, the vast ma-
jority of genres in the Syracuse corpus are also
organised in two levels only. Such flat hierar-
chies do not offer much scope to improve over a
completely flat list. There are considerably more
levels in the BNC for some branches, e.g., writ-
ten/national/broadsheet/arts, but many other gen-
res are still only specified to the second level of
its hierarchy, e.g., written/adverts. In addition, the
BNC is also distributionally imbalanced, i.e. the
number of documents per class varies from 2 to
501 documents.
To test our hypothesis, we tried to skew the
Brown genre tree in two ways. First, we kept the
tree relatively balanced visually and distribution-
ally but flattened it by removing the second layer
Press, Misc, Non-Fiction, Fiction from the hierar-
chy, leaving a tree with only two layers. Second,
we skewed the visual and distributional balance of
the tree by collapsing its three leaf-level genres un-
der Press, and the two under non-fiction, leading to
12 genres to classify (cf. Figure 1).
30 35 40 45 50 55 60 65 70
IC?resk?word?bncIC?resk?gram?bnc
IC?resk?word?brIC?lin?gram?bnc
plenpwupal
IC?lin?word?brIC?resk?word?gg
IC?lin?dfIC?lin?word?bnc
IC?lin?gram?brIC?jng?df
flatIC?resk?df
plskIC?resk?gram?br
pmaxIC?lin?word?gg
IC?jng?gram?bncIC?jng?gram?br
IC?jng?word?brIC?jng?word?bnc
IC?jng?word?gg
Accuracy
Figure 3: Accuracy on flattened Brown Corpus (15
genres).
35 40 45 50 55 60 65 70 75
IC?resk?word?brIC?resk?gram?bnc
pmaxIC?resk?gram?br
IC?resk?dfIC?lin?word?bnc
pwupalplen
IC?resk?word?bncplsk
IC?lin?gram?brflat
IC?lin?word?brIC?lin?df
IC?lin?gram?bncIC?jng?gram?br
IC?jng?dfIC?resk?word?gg
IC?lin?word?ggIC?jng?gram?bnc
IC?jng?word?brIC?jng?word?bnc
IC?jng?word?gg
Accuracy
Figure 4: Accuracy on skewed Brown Corpus (12
genres).
As expected, the structural methods on either
skewed or flattened hierarchies are not signifi-
cantly better than the flat SVM. For the flattened
hierarchy of 15 leaf genres the maximal accuracy
is 54.2% vs. 52.4% for the flat SVM (Figure 3), a
non-significant improvement. Similarly, the max-
imal accuracy on the skewed 12-genre hierarchy
is 58.2% vs. 56% (see also Figure 4), again a not
significant improvement.
To measure the degree of balance of a tree,
we introduce two tree balance scores based on
entropy. First, for both measures we extend all
branches to the maximum depth of the tree. Then
level by level we calculate an entropy score, ei-
ther according to how many tree nodes at the next
level belong to a node at this level (denoted as
vb: visual balance), or according to how many
end level documents belong to a node at this level
(denoted as db: distribution balance). To make
trees with different numbers of internal nodes
and leaves more comparable, the entropy score
at each level is normalized by the maximal en-
tropy achieved by a tree with uniform distribution
of nodes/documents, which is simply?log(1/N),
where N denotes the number of nodes at the corre-
755
sponding level. Finally, the entropy scores for all
levels are averaged. It can be shown that any per-
fect N-ary tree will have the largest visual balance
score of 1. If in addition its nodes at each level
contain the same number of documents, the distri-
bution balance score will reach the maximum, too.
Table 3 shows the balance scores for all the cor-
pora we use. The first two rows for the Brown cor-
pus have both large visual balance and distribution
balance scores. As shown earlier, for those two se-
tups the structural SVMs perform better than the
flat approach. In contrast, for the tree hierarchies
of Brown that we deformed or flattened, and also
BNC and Syracuse, either or both of the two bal-
ance scores tend to be lower, and no improvement
has been obtained over the flat approach. This
may indicate that a further exploration of the rela-
tion between tree balance and the performance of
structural SVMs is warranted. However, high vi-
sual balance and distribution scores do not neces-
sarily imply high performance of structural SVMs,
as very flat trees are also visually very balanced.
As an example, HGC has a high visual balance
score due to a shallow hierarchy and a high distri-
butional balance score due to a roughly equal num-
ber of documents contained in each genre. How-
ever, HGC did not benefit from structural learning
as it is also a very shallow hierarchy; therefore we
think that a third variable depth also needs to be
taken into account.
A similar observation on the importance of
well-balanced hierarchies comes from a recent
Pascal challenge on large scale hierarchical text
classification,2 which shows that some flat ap-
proaches perform competitively in topic classifi-
cation with imbalanced hierarchies. However, the
participants do not explore explicitly the relation
between tree balance and performance.
Other methods for measuring tree balance
(some of which are related to ours) are used in
the field of phylogenetic research (Shao and Sokal,
1990) but they are only applicable to visual bal-
ance. In addition, the methods they used often
provide conflicting results on which trees are con-
sidered as balanced (Shao and Sokal, 1990).
5.2 Distance Measures
We also scrutinise our distance measures as these
are crucial for the structural approach. We no-
tice that simple path length based measures per-
2http://lshtc.iit.demokritos.gr/
Table 3: Tree Balance Scores
Corpus depth vb db
Brown (10 genres) 3 0.9115 0.9024
Brown (15 genres) 3 0.9186 0.9083
Brown (15, flattened) 2 0.9855 0.8742
Brown (12, skewed) 3 0.8747 0.8947
HGC (32) 2 0.9562 0.9570
BNC (70) 4 0.9536 0.8039
Syracuse (52) 3 0.9404 0.8634
form well overall; again for the Brown corpus
this is probably due to its balanced hierarchy
which makes path length appropriate. There are
other probable reasons why information content
based measures do not perform better than path-
length based ones. When measured via docu-
ment frequency in a corpus we do not have suffi-
ciently large, representative genre-annotated cor-
pora to hand. When measured via genre label
frequency, we run into at least two problems.
Firstly, as mentioned in Section 3.2.1 genre la-
bel frequency does not have to correspond to class
frequency of documents. Secondly, the labels
used are often abbreviations (e.g. W_institut_doc,
W_newsp_brdsht_nat_social in BNC Corpus),
underspecified (other, misc, unclassified) or a col-
lection of phrases (e.g. belles letters, etc. in
Brown). This made search for frequency very ap-
proximate and also loosens the link between label
and content.
We investigated in more depth how well the dif-
ferent distance measures are aligned. We adapt
the alignment measure between kernels (Cristian-
ini et al, 2002), to investigate how close the dis-
tance matrices are. For two distance matrices H1
and H2, their alignment A(H1, H2) is defined as:
< H1, H2 >F?
< H1, H1 >F , < H2, H2 >F
, (17)
where < H1, H2 >F=
?k
i,j H1(gi, gj)H2(gi, gj)
which is the total sum of the entry-wise products
between the two distance matrices. Figure 5 shows
several distance matrices on the (original) 15 genre
Brown corpus. The plen matrix has clear blocks
for the super genres press, informative, imagina-
tive, etc. The IC-lin-gram-bnc matrix refines dis-
tances in the blocks, due to the introduction of in-
formation content. It keeps an alignment score that
is over 0.99 (the maximum is 1.00) toward the plen
matrix, and still has visible block patterns. How-
ever, the IC-jng-word-bnc significantly adjusts the
756
distance entries, has a much lower alignment score
with the plen matrix, and doesn?t reveal appar-
ent blocks. This partially explains the bad perfor-
mance of the Jiang distance measure on the Brown
corpus (see Section 4). The diagrams also show
the high closeness between the best performing IC
measure and the simple path length based mea-
sure.
plen
Informative Imaginative
Press
Misc
nonfiction
IC?lin?gram?bnc (0.98376)
Informative Imaginative
Press
Misc
nonfiction
plsk (0.96061)
Informative Imaginative
Press
Misc
nonfiction
IC?jng?word?bnc (0.92993)
Informative Imaginative
Press
Misc
nonfiction
Figure 5: Distance Matrices on Brown. Values in
bracket is the alignment with the plen matrix
An alternative to structural distance measures
would be distance measures between the gen-
res based on pairwise cosine similarities between
them. To assess this, we aggregated all character
4-gram training vectors of each genre and calcu-
lated standard cosine similarities. Note that these
similarities are based on the documents only and
do not make use of the Brown hierarchy at all. Af-
ter converting the similarities to distance, we plug
the distance matrix into our structural SVM. How-
ever, accuracy on the Brown corpus (15 genres)
was almost the same as for a flat SVM. Inspecting
the distance matrix visually, we determined that
the cosine similarity could clearly distinguish be-
tween Fiction and Non-Fiction texts but not be-
tween any other genres. This also indicates that
the genre structural hierarchy clearly gives infor-
mation not present in the simple character 4-gram
features we use. For a more detailed discussion
of the problems of the currently prevalently used
character n-grams as features for genre classifica-
tion, we refer the reader to (Sharoff et al, 2010).
6 Conclusions
In this paper, we have evaluated structural learn-
ing approaches to genre classification using sev-
eral different genre distance measures. Although
we were able to improve on non-structural ap-
proaches for the Brown corpus, we found it hard to
improve over flat SVMs on other corpora. As po-
tential reasons for this negative result, we suggest
that current genre hierarchies are either not of suf-
ficient depth or are visually or distributionally im-
balanced. We think further investigation into the
relationship between hierarchy balance and struc-
tural learning is warranted. Further investigation
is also needed into the appropriateness of n-gram
features for genre identification as well as good
measures of genre distance.
In the future, an important task would be the re-
finement or unsupervised generation of new hier-
archies, using information theoretic or data-driven
approaches. For a full assessment of hierarchical
learning for genre classification, the field of genre
studies needs a testbed similar to the Reuters or 20
Newsgroups datasets used in topic-based IR with a
balanced genre hierarchy and a representative cor-
pus of reliably annotated webpages.
With regard to algorithms, we are also inter-
ested in other formulations for structural SVMs
and their large-scale implementation as well as the
combination of different distance measures, for
example in ensemble learning.
Acknowledgements
We would like to thank the authors of each corpus
collection, who invested a lot of effort into produc-
ing them. We are also grateful to Google Inc. for
supporting this research via their Google Research
Awards programme.
References
Boser, B. E., Guyon, I. M., and Vapnik, V. N.
(1992). A training algorithm for optimal mar-
gin classifiers. In COLT ?92: Proceedings of
the fifth annual workshop on Computational
learning theory, pages 144?152, New York,
NY, USA. ACM.
Crammer, K. and Singer, Y. (2002). On the algo-
rithmic implementation of multiclass kernel-
based vector machines. J. Mach. Learn. Res.,
2:265?292.
Cristianini, N., Shawe-Taylor, J., and Kandola, J.
(2002). On kernel target algnment. In Pro-
ceedings of the Neural Information Process-
757
ing Systems, NIPS?01, pages 367?373. MIT
Press.
Crowston, K., Kwasnik, B., and Rubleske, J.
(2009). Problems in the use-centered de-
velopment of a taxonomy of web genres.
In Mehler, A., Sharoff, S., and Santini,
M., editors, Genres on the Web: Com-
putational Models and Empirical Studies.
Springer, Berlin/New York.
Dekel, O., Keshet, J., and Singer, Y. (2004).
Large margin hierarchical classification. In
ICML ?04: Proceedings of the twenty-first in-
ternational conference on Machine learning,
page 27, New York, NY, USA. ACM.
Dietterich, T. G. (1998). Approximate statistical
tests for comparing supervised classification
learning algorithms. Neural Computation,
10:1895?1923.
Giesbrecht, E. and Evert, S. (2009). Part-of-
Speech (POS) Tagging - a solved task? An
evaluation of POS taggers for the Web as
corpus. In Proceedings of the Fifth Web
as Corpus Workshop (WAC5), pages 27?35,
Donostia-San Sebasti?n.
Jiang, J. J. and Conrath, D. W. (1997). Semantic
similarity based on corpus statistics and lexi-
cal taxonomy. CoRR, cmp-lg/9709008.
Joachims, T. (1999). Making large-scale SVM
learning practical. In Sch?lkopf, B., Burges,
C., and Smola, A., editors, Advances in
Kernel Methods ? Support Vector Learning,
pages 41?56. MIT Press.
Joachims, T., Finley, T., and Yu, C.-N. (2009).
Cutting-plane training of structural svms.
Machine Learning, 77(1):27?59.
Kanaris, I. and Stamatatos, E. (2009). Learning to
recognize webpage genres. Information Pro-
cessing and Management, 45:499?512.
Karlgren, J. and Cutting, D. (1994). Recogniz-
ing text genres with simple metrics using dis-
criminant analysis. In Proc. of the 15th. Inter-
national Conference on Computational Lin-
guistics (COLING 94), pages 1071 ? 1075,
Kyoto, Japan.
Keerthi, S. S., Sundararajan, S., Chang, K.-W.,
Hsieh, C.-J., and Lin, C.-J. (2008). A se-
quential dual method for large scale multi-
class linear svms. In KDD ?08: Proceeding of
the 14th ACM SIGKDD international confer-
ence on Knowledge discovery and data min-
ing, pages 408?416, New York, NY, USA.
ACM.
Kessler, B., Nunberg, G., and Sch?tze, H. (1997).
Automatic detection of text genre. In Pro-
ceedings of the 35th ACL/8th EACL, pages
32?38.
Kuc?era, H. and Francis, W. N. (1967). Computa-
tional analysis of present-day American En-
glish. Brown University Press, Providence.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for
word sense identification, pages 305?332. In
C. Fellbaum (Ed.), MIT Press.
Lee, D. (2001). Genres, registers, text types, do-
mains, and styles: clarifying the concepts
and navigating a path through the BNC jun-
gle. Language Learning and Technology,
5(3):37?72.
Lin, D. (1998). An information-theoretic defini-
tion of similarity. In ICML ?98: Proceed-
ings of the Fifteenth International Confer-
ence on Machine Learning, pages 296?304,
San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Meyer zu Eissen, S. and Stein, B. (2004). Genre
classification of web pages. In Proceedings
of the 27th German Conference on Artificial
Intelligence, Ulm, Germany.
Pedersen, T., Pakhomov, S. V. S., Patwardhan, S.,
and Chute, C. G. (2007). Measures of seman-
tic similarity and relatedness in the biomed-
ical domain. J. of Biomedical Informatics,
40(3):288?299.
Resnik, P. (1995). Using information content to
evaluate semantic similarity in a taxonomy.
In IJCAI?95: Proceedings of the 14th inter-
national joint conference on Artificial intel-
ligence, pages 448?453, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
758
Santini, M. (2007). Automatic Identification of
Genre in Web Pages. PhD thesis, University
of Brighton.
Shao, K.-T. and Sokal, R. R. (1990). Tree balance.
Systematic Zoology, 39(3):266?276.
Sharoff, S., Wu, Z., and Markert, K. (2010). The
Web library of Babel: evaluating genre col-
lections. In Proc. of the Seventh Language
Resources and Evaluation Conference, LREC
2010, Malta.
Stubbe, A. and Ringlstetter, C. (2007). Recogniz-
ing genres. In Santini, M. and Sharoff, S.,
editors, Proc. Towards a Reference Corpus of
Web Genres.
Tsochantaridis, I., Joachims, T., Hofmann, T., and
Altun, Y. (2005). Large margin methods
for structured and interdependent output vari-
ables. J. Mach. Learn. Res., 6:1453?1484.
Vidulin, V., Lu?trek, M., and Gams, M. (2007).
Using genres to improve search engines. In
Proc. Towards Genre-Enabled Search En-
gines: The Impact of NLP. RANLP-07.
Webber, B. (2009). Genre distinctions for dis-
course in the Penn TreeBank. In Proc the
47th Annual Meeting of the ACL, pages 674?
682.
Wu, Z. and Palmer, M. (1994). Verbs seman-
tics and lexical selection. In Proceedings of
the 32nd annual meeting on Association for
Computational Linguistics, pages 133?138,
Morristown, NJ, USA. Association for Com-
putational Linguistics.
759
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 795?804,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Collective Classification for Fine-grained Information Status
Katja Markert1,2, Yufang Hou2, Michael Strube2
1 School of Computing, University of Leeds, UK, scskm@leeds.ac.uk
2 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
Abstract
Previous work on classifying information sta-
tus (Nissim, 2006; Rahman and Ng, 2011)
is restricted to coarse-grained classification
and focuses on conversational dialogue. We
here introduce the task of classifying fine-
grained information status and work on writ-
ten text. We add a fine-grained information
status layer to the Wall Street Journal portion
of the OntoNotes corpus. We claim that the
information status of a mention depends not
only on the mention itself but also on other
mentions in the vicinity and solve the task by
collectively classifying the information status
of all mentions. Our approach strongly outper-
forms reimplementations of previous work.
1 Introduction
Speakers present already known and yet to be es-
tablished information according to principles re-
ferred to as information structure (Prince, 1981;
Lambrecht, 1994; Kruijff-Korbayova? and Steedman,
2003, inter alia). While information structure af-
fects all kinds of constituents in a sentence, we here
adopt the more restricted notion of information sta-
tus which concerns only discourse entities realized
as noun phrases, i.e. mentions1. Information status
(IS henceforth) describes the degree to which a dis-
course entity is available to the hearer with regard to
the speaker?s assumptions about the hearer?s knowl-
edge and beliefs (Nissim et al, 2004). Old men-
tions are known to the hearer and have been referred
1Since not all noun phrases are referential, we call noun
phrases which carry information status mentions.
to previously. Mediated mentions have not been
mentioned before but are also not autonomous, i.e.,
they can only be correctly interpreted by reference
to another mention or to prior world knowledge. All
other mentions are new.
IS can be beneficial for a number of NLP tasks,
though the results have been mixed. Nenkova et
al. (2007) used IS as a feature for generating pitch
accent in conversational speech. As IS is restricted
to noun phrases, while pitch accent can be assigned
to any word in an utterance, the experiments were
not conclusive. For determining constituent order of
German sentences, Cahill and Riester (2009) incor-
porate features modeling IS to good effect. Rahman
and Ng (2011) showed that IS is a useful feature for
coreference resolution.
Previous work on learning IS (Nissim, 2006; Rah-
man and Ng, 2011) is restricted in several ways.
It deals with conversational dialogue, in particular
with the corpus annotated by Nissim et al (2004).
However, many applications that can profit from IS
concentrate on written texts, such as summariza-
tion. For example, Siddharthan et al (2011) show
that solving the IS subproblem of whether a per-
son proper name is already known to the reader im-
proves automatic summarization of news. There-
fore, we here model IS in written text, creating a
new dataset which adds an IS layer to the already
existing comprehensive annotation in the OntoNotes
corpus (Weischedel et al, 2011). We also report
the first results on fine-grained IS classification by
modelling further distinctions within the category
of mediated mentions, such as comparative and
bridging anaphora (see Examples 1 and 2, re-
795
spectively).2 Fine-grained IS is a prerequisite to
full bridging/comparative anaphora resolution, and
therefore necessary to fill gaps in entity grids (Barzi-
lay and Lapata, 2008) based on coreference only.
Thus, Examples 1 and 2 do not exhibit any corefer-
ential entity coherence but coherence can be estab-
lished when the comparative anaphor others is re-
solved to others than freeway survivor Buck Helm,
and the bridging anaphor the streets is resolved to
the streets of Oranjemund, respectively.
(1) the condition of freeway survivor Buck
Helm . . . , improved, hospital officials said.
Rescue crews, however, gave up hope that
others would be found.
(2) Oranjemund, the mine headquarters, is a
lonely corporate oasis of 9,000 residents.
Jackals roam the streets at night . . .
We approach the challenge of modeling IS via
collective classification, using several novel linguis-
tically motivated features. We reimplement Nissim?s
(2006) and Rahman and Ng?s (2011) approaches as
baselines and show that our approach outperforms
these by a large margin for both coarse- and fine-
grained IS classification.
2 Related Work
IS annotation schemes and corpora. We en-
hance the approach in Nissim et al (2004) in two
major ways (see also Section 3.1). First, compar-
ative anaphora are not specifically handled in Nis-
sim et al (2004) (and follow-on work such as Ritz
et al (2008) and Riester et al (2010)), although
some of them might be included in their respective
bridging subcategories. Second, we apply the
annotation scheme reliably to a new genre, namely
news. This is a non-trivial extension: Ritz et al
(2008) applied a variation of the Nissim et al (2004)
scheme to a small set of 220 NPs in a German
news/commentary corpus but found that reliability
then dropped significantly to the range of ? = 0.55
to 0.60. They attributed this to the higher syntac-
tic complexity and semantic vagueness in the com-
mentary corpus. Riester et al (2010) annotated a
2All examples in this paper are from the OntoNotes cor-
pus. The mention in question is typed in boldface; antecedents,
where applicable, are displayed in italics.
German news corpus marginally reliable (? = 0.66)
for their overall scheme but their confusion ma-
trix shows even lower reliability for several subcate-
gories, most importantly deixis and bridging.
While standard coreference corpora do not con-
tain IS annotation, some corpora annotated for
bridging are emerging (Poesio, 2004; Korzen and
Buch-Kromann, 2011) but they are (i) not annotated
for comparative anaphora or other IS categories, (ii)
often not tested for reliability or reach only low reli-
ability, (iii) often very small (Poesio, 2004).
To the best of our knowledge, we therefore
present the first English corpus reliably annotated
for a wide range of IS categories as well as full
anaphoric information for three main anaphora types
(coreference, bridging, comparative).
Automatic recognition of IS. Vieira and Poesio
(2000) describe heuristics for processing definite de-
scriptions in news text. As their approach is re-
stricted to definites, they only analyse a subset of
the mentions we consider carrying IS. Siddharthan
et al (2011) also concentrate on a subproblem of IS
only, namely the hearer-old/hearer-new distinctions
for person proper names.
Nissim (2006) and Rahman and Ng (2011) both
present algorithms for IS detection on Nissim et
al.?s (2004) Switchboard corpus. Both papers treat
IS classification as a local classification problem
whereas we look at dependencies between the IS
status of different mentions, leading to collective
classification. In addition, they only distinguish the
three main categories old, mediated and new.
Finally, we work on news corpora which poses dif-
ferent problems from dialogue.
Anaphoricity determination (Ng, 2009; Zhou and
Kong, 2009) identifies many or most old men-
tions. However, no distinction between mediated
and new mentions is made. Most approaches to
bridging resolution (Meyer and Dale, 2002; Poe-
sio et al, 2004) or comparative anaphora (Mod-
jeska et al, 2003; Markert and Nissim, 2005)
address only the selection of the antecedent for
the bridging/comparative anaphor, not its recogni-
tion. Sasano and Kurohashi (2009) do also tackle
bridging recognition, but they depend on language-
specific non-transferrable features for Japanese.
796
3 Corpus Creation
3.1 Annotation Scheme
Our scheme follows Nissim et al (2004) in dis-
tinguishing three major IS categories old, new
and mediated. A mention is old if it is ei-
ther coreferential with an already introduced entity
or a generic or deictic pronoun. We follow the
OntoNotes (Weischedel et al, 2011) definition of
coreference to be able to integrate our annotations
with it. This definition includes coreference with
noun phrase as well as verb phrase antecedents3 .
Mediated refers to entities which have not yet
been introduced in the text but are inferrable via
other mentions or are known via world knowl-
edge. We distinguish the following six subcate-
gories: The category mediated/comparative
comprises mentions compared via either a contrast
or similarity to another one (see Example 1). This
category is novel in our scheme. We also in-
clude a category mediated/bridging (see Ex-
amples 2, 3 and 4). Bridging anaphora can be
any noun phrase and are not limited to definite NPs
as in Poesio et al (2004), Gardent and Manue?lian
(2005), Riester et al (2010). In contrast to Nissim
et al (2004), antecedents for both comparative and
bridging categories are annotated and can be noun
phrases, verb phrases or even clauses. The category
mediated/knowledge is inspired by the hearer-
old distinction introduced by Prince (1992) and cov-
ers entities generally known to the hearer. It includes
many proper names, such as Poland.4 Mentions that
are syntactically linked via a possessive relation or a
PP modification to other, old or mediated men-
tions fall into the type mediated/synt (see Ex-
amples 5 and 6).5 With no change to Nissim et al?s
scheme, coordinated mentions where at least one el-
ement in the conjunction is old or mediated are
covered by the category mediated/aggregate,
and mentions referring to a value of a previously
mentioned function by the type mediated/func.
All other mentions are annotated as new, includ-
3In contrast to Nissim et al (2004), but in accordance with
OntoNotes, we do not consider generics for coreference.
4This class corresponds roughly to Nissim et al?s (2004)
mediated/general.
5This class expands Nissim et al?s (2004) poss category
that only considers possessives but not PP modification.
ing most generics as well as newly introduced, spe-
cific mentions such as Example 7.
(3) Initial steps were taken at Poland?s first en-
vironmental conference, which I attended
last month. . . . it was no accident that par-
ticipants urged the free flow of information
(4) The Bakersfield supermarket went out of
business last May. The reason was . . .
(5) One Washington couple sold their liquor
store
(6) the main artery into San Francisco
(7) the owner was murdered by robbers
3.2 Agreement Study
We carried out an agreement study with 3 annota-
tors, of which Annotator A was the scheme devel-
oper and first author of this paper. All texts used
were from the Wall Street Journal (WSJ) portion of
OntoNotes. There were no restrictions on which
texts to include apart from (i) exclusion of letters
to the editor as they contain cross-document links
and (ii) a preference for longer texts with potentially
richer discourse structure.
Mentions were automatically preselected for the
annotators using the gold-standard syntactic annota-
tion.6 The existing coreference annotation was auto-
matically carried over to the IS task by marking all
mentions in a coreference chain (apart from the first
mention in the chain) as old. The annotation task
consisted of marking all mentions for their IS (old,
mediated or new) as well as marking mediated
subcategories (see Section 3.1) and the antecedents
for comparative and bridging anaphora.
The scheme was developed on 9 texts, which were
also used for training the annotators. Inter-annotator
agreement was measured on 26 new texts, which in-
cluded 5905 pre-marked potential mentions. The an-
notations of 1499 of these were carried over from
OntoNotes, leaving 4406 potential mentions for an-
notation and agreement measurement. In addition to
6Some non-mentions such as idioms could not be filtered
out via the syntactic annotation and had to be excluded during
human annotation.
797
A-B A-C B-C
Overall Percentage coarse 87.5 86.3 86.5
Overall ? coarse 77.3 75.2 74.7
Overall Percentage fine 86.6 85.3 85.7
Overall ? fine 80.1 77.7 77.3
Table 1: Agreement Results
A-B A-C B-C
? Non-mention 81.5 78.9 86.0
? Old 80.5 83.2 79.3
? New 76.6 74.0 74.3
? Mediated/Knowledge 82.1 78.4 74.1
? Mediated/Synt 88.4 87.8 87.6
? Mediated/Aggregate 87.0 85.4 86.0
? Mediated/Func 6.0 83.2 6.9
? Mediated/Comp 81.8 78.3 81.2
? Mediated/Bridging 70.8 60.6 62.3
Table 2: Agreement Results for individual categories
percentage agreement, we measured Cohen?s ? (Art-
stein and Poesio, 2008) between all 3 possible anno-
tator pairings. We also report single-category agree-
ment for each category, where all categories but one
are merged and then ? is computed as usual. Table 1
shows agreement results for the overall scheme at
the coarse-grained (4 categories: non-mention, old,
new, mediated) and the fine-grained level (9 cate-
gories: non-mention, old, new and the 6 mediated
subtypes). The results show that the scheme is over-
all reliable, with not too many differences between
the different annotator pairings.7
Table 2 shows the individual category agreement
for all 9 categories. We achieve high reliability for
most categories.8 Particularly interesting is the fact
that hearer-old entities (mediated/knowledge)
can be identified reliably although all annotators had
substantially different backgrounds. The reliabil-
ity of the category bridging is more annotator-
dependent, although still higher, sometimes con-
siderably, than other previous attempts at bridg-
7Often, annotation is considered highly reliable when ? ex-
ceeds 0.80 and marginally reliable when between 0.67 and 0.80
(Carletta, 1996). However, the interpretation of ? is still under
discussion (Artstein and Poesio, 2008).
8The low reliability of the rare category func, when involv-
ing Annotator B, was explained by Annotator B forgetting about
this category after having used it once. Pair A-C achieved high
reliability (? 83.2 for pair A-C).
ing annotation (Poesio et al, 2004; Gardent and
Manue?lian, 2005; Riester et al, 2010).
3.3 Gold Standard
Our final gold standard corpus consists of 50 texts
from the WSJ portion of the OntoNotes corpus-
The corpus will be made publically available as
OntoNotes annotation layer via http://www.
h-its.org/nlp/download.
Disagreements in the 35 texts used for annota-
tor training (9 texts) and testing (26 texts) were re-
solved via discussion between the annotators. An
additional 15 texts were annotated by Annotator A.
Finally, Annotator A carried out consistency checks
over all texts. ? The gold standard includes 10,980
true mentions (see Table 3).
Texts 50
Mentions 10,980
old 3237
coref 3,143
generic deictic pr 94
mediated 3,708
world knowledge 924
syntactic 1,592
aggregate 211
func 65
comparative 253
bridging 663
new 4,035
Table 3: Gold Standard Distribution
4 Features
In this Section, we describe both the local as well as
the relational features we use.
4.1 Features for Local Classification
We use the following local features, including the
features in Nissim (2006) and Rahman and Ng
(2011) to be able to gauge how their systems fare on
our corpus and as a comparison point for our novel
collective classification approach.
The features developed by Nissim (2006) are
shown in Table 4. Nissim shows clearly that
these features are useful for IS classification.
Thus, subjects are more likely to be old as as-
sumed by, e.g., centering theory (Grosz et al,
798
Feature Value
full prev mention {yes, no, NA}9
mention time {first, second, more}
partial prev mention {yes, no, NA}
determiner {bare, def, dem, indef, poss, NA}
NP type {pronoun, common, proper, other}
NP length numeric
grammatical role {subject, subjpass, pp, other}
Table 4: Nissim?s (2006) feature set
1995). Also, previously unmentioned proper names
are more likely to be hearer-old and therefore
mediated/knowledge, although their exact sta-
tus will depend on how well known a particular
proper name is.
Rahman and Ng (2011) add all unigrams appear-
ing in any mention in the training set as features.
They also integrated (via a convolution tree-kernel
SVM (Collins and Duffy, 2001)) partial parse trees
that capture the generalised syntactic context of a
mention e and include the mention?s parent and sib-
ling nodes without lexical leaves. However, they use
no structure underneath the mention node e itself,
assuming that ?any NP-internal information has pre-
sumably been captured by the flat features?.
To these feature sets, we add a small set of other
local features otherlocal. These track partial previ-
ous mentions by also counting partial previous men-
tion time as well as the previous mention of con-
tent words only. We also add a mention?s number as
one of singular, plural or unknown, and whether the
mention is modified by an adjective. Another feature
encapsulates whether the mention is modified by a
comparative marker, using a small set of 10 markers
such as another, such, similar . . . and the presence
of adjectives or adverbs in the comparative. Finally,
we include the mention?s semantic class as one of 12
coarse-grained classes, including location, organisa-
tion, person and several classes for numbers (such as
date, money or percent).
4.2 Relations for Collective Classification
Both Nissim (2006) and Rahman and Ng (2011)
classify each mention individually in a standard su-
pervised ML setting, not considering potential de-
pendencies between the IS categories of different
9We changed the value of ?full prev mention? from ?nu-
meric? to {yes, no, NA}.
mentions. However, collective or joint classifica-
tion has made substantial impact in other NLP tasks,
such as opinion mining (Pang and Lee, 2004; Soma-
sundaran et al, 2009), text categorization (Yang et
al., 2002; Taskar et al, 2002) and the related task of
coreference resolution (Denis and Baldridge, 2007).
We investigate two types of relations between men-
tions that might impact on IS classification.
Syntactic parent-child relations. Two media-
ted subcategories account for accessibility via syn-
tactic links to another old or mediated men-
tion: mediated/synt is used when at least one
child of a mention is mediated or old, with child
relations restricted to pre- or postnominal posses-
sives as well as PP children in our scheme (see Sec-
tion 3.1). mediated/aggregate is for coordi-
nations in which at least one of the children is old
or mediated. In these two cases, a mention?s
IS depends directly on the IS of its children. We
therefore link a mention m1 to a mention m2 via a
hasChild relation if (i) m2 is a possessive or prepo-
sitional modification ofm1, or (ii)m1 is a coordina-
tion and m2 is one of its children.
Using such a relational feature catches two birds
with one stone: firstly, it integrates the internal struc-
ture of a mention into the algorithm, which Rah-
man and Ng (2011) ignore; secondly, it captures de-
pendencies between parent and child classification,
which would not be possible if we integrated the in-
ternal structure via flat features or additional tree
kernels. We hypothesise that the higher syntactic
complexity of our news genre (14.5% of all men-
tions are mediated/synt) will make this feature
highly effective in distinguishing between new and
mediated categories.
Syntactic precedence relations. IS is said to in-
fluence word order (Birner and Ward, 1998; Cahill
and Riester, 2009) and this fact has been exploited
in work on generation (Prevost, 1996; Filippova and
Strube, 2007; Cahill and Riester, 2009). Therefore,
we integrate dependencies between the IS classifica-
tion of mentions in precedence relations.
m1 precedes m2 if (i) m1 and m2 are in the same
clause, allowing for trace subjects in gerund and in-
finitive constructions, (ii) m1 and m2 are dependent
on the same verb or noun, allowing for interven-
ing nodes via modal, auxiliary, gerund and infinitive
799
constructions, (iii) m1 is neither a child nor a parent
of m2, and (iv) m1 occurs before m2.
For Example 8 (slightly simplified) we extract the
precedence relations shown in Table 5.
(8) She was sent by her mother to a white
woman?s house to do chores in exchange for
meals and a place to sleep.
(She)old >p (her mother)med/synt
(She)old >p (a white-woman?s house)new
(She)old >p (chores)new
(She)old >p (exchange .....sleep)new
(her mother)med/synt >p (a white woman?s house)new
(chores)new >p (exchange . . . sleep)new
(meals)new >p (a place to sleep)new
Table 5: Precedence Relations for Example 8. She is a
trace subject for do.
Proper names behave differently from common
nouns. For example, they can occur at many differ-
ent places in the clause when functioning as spatial
or temporal scene-setting elements, such as In New
York. We therefore exclude all precedence relations
where one element of the pair is a proper name.
We extract 2855 precedence relations. Table 6
shows the statistics on precedence with the first men-
tion in a pair in rows and the second in columns. Me-
diated and new mentions indeed rarely precede old
mentions, so that precedence should improve sepa-
rating of old vs other mentions.
old mediated new
old 136 387 519
mediated 88 357 379
new 85 291 613
Table 6: Precedence relations in our corpus
5 Experiments
5.1 Experimental Setup
We use our gold standard corpus (see Section 3.3)
via 10-fold cross-validation on documents for all ex-
periments. Following Nissim (2006) and Rahman
and Ng (2011), we perform all experiments on gold
standard mentions and use the human WSJ syntac-
tic annotation for feature extraction, when neces-
sary. For the extraction of semantic class, we use
OntoNotes entity type annotation for proper names
and an automatic assignment of semantic class via
WordNet hypernyms for common nouns.
Coarse-grained versions of all algorithms distin-
guish only between the three old, mediated,
new categories. Fine-grained versions distinguish
between the categories old, the six mediated
subtypes, and new. We report overall accuracy as
well as precision, recall and F-measure per category.
Significance tests are conducted using McNemar?s
test on overall algorithm accuracy, at the level of 1%.
5.2 Local Classifiers
We reimplemented the algorithms in Nissim (2006)
and Rahman and Ng (2011) as comparison base-
lines, using their feature and algorithm choices. Al-
gorithm Nissim is therefore a decision tree J48 with
standard settings in WEKA with the features in Ta-
ble 4. Algorithm RahmanNg is an SVM with a com-
posite kernel and one-vs-all training/testing (toolkit
SVMLight). They use the features in Table 4 plus
unigram and tree kernel features, described in Sec-
tion 4.1. We add our additional set of otherlocal
features to both baseline algorithms (yielding Nis-
sim+ol and RahmanNg+ol) as they aim specifically
at improving fine-grained classification.
5.3 Collective Classification
For incorporating our inter-mention links, we use a
variant of Iterative Collective classification (ICA),
which has shown good performance over a variety
of tasks (Lu and Getoor, 2003) and has been used
in NLP for example for opinion mining (Somasun-
daran et al, 2009). ICA is normally faster than
Gibbs sampling and ? in initial experiments ? did
not yield significantly different results from it.
ICA initializes each mention with its most likely
IS, according to the local classifier and features. It
then iterates a relational classifier, which uses both
local and relational features (our hasChild and pre-
cedes features) taking IS assignments to neighbour-
ing mentions into account. We use the exist aggre-
gator to define the dependence between mentions.
We use NetKit (Macskassy and Provost, 2007)
with its standard ICA settings for collective infer-
ence, as it allows direct comparison between local
and collective classification. The relational classi-
fiers are always exactly the same classifiers as the
800
local collective
Nissim+ol Nissim+olNissim Nissim+ol
+hasChild +hasChild+precedes
R P F R P F R P F R P F
Coarse
old 82.2 86.4 84.2 81.2 88.6 84.8 81.7 88.6 85.0 80.9 89.1 84.8
mediated 51.9 60.2 55.7 57.8 64.6 61.0 68.4 77.4 72.6 68.8 76.9 72.6
new 74.2 63.6 68.5 78.4 67.3 72.4 87.7 75.1 80.9 87.9 75.0 80.9
acc 69.0 72.3 79.4 79.4
Fine
old 84.0 83.3 83.6 85.0 83.9 84.5 84.3 84.7 84.5 84.1 85.2 84.6
med/knowledge 61.3 60.0 60.6 61.0 69.5 65.0 62.3 70.0 65.9 60.6 70.0 65.0
med/synt 37.2 59.7 45.8 44.7 60.0 51.3 76.8 81.4 79.0 75.7 80.1 77.9
med/agg 26.0 42.0 32.2 20.4 38.4 26.6 42.6 55.9 48.4 43.1 55.8 48.7
med/func 0.0 NA NA 32.3 65.6 43.3 33.8 53.7 41.5 35.4 53.5 48.7
med/comp 0.4 7.70 0.7 79.0 82.6 80.0 80.6 82.9 81.8 81.4 82.0 81.7
med/bridging 6.6 26.2 10.6 8.9 30.9 13.8 9.6 34.4 15.1 12.2 41.7 18.9
new 82.6 61.0 70.2 82.7 65.1 72.8 88.0 74.0 80.4 87.7 73.3 79.8
acc 66.6 70.0 77.0 76.8
Table 7: Collective classification compared to Nissim?s local classifier. Best performing algorithms are bolded.
local ones with the relational features added: thus, if
the local classifier is a tree kernel SVM so is the rela-
tional one. One problem when using the SVM Tree
kernel as relational classifier is that it allows only for
binary classification so that we need to train several
binary networks in a one-vs-all paradigm (see also
(Rahman and Ng, 2011)), which will not be able to
use the multiclass dependencies of the relational fea-
tures to optimum effect.
5.4 Results
Table 7 shows the comparison of collective classifi-
cation to local classification, using Nissim?s frame-
work and features, and Table 8 the equivalent table
for Rahman and Ng?s approach.
The improvements using the additional local fea-
tures over the original local classifiers are sta-
tistically significant in all cases. In particu-
lar, the inclusion of semantic classes improves
mediated/knowledge and mediated/func,
and comparative anaphora are recognised highly re-
liably via a small set of comparative markers.
The hasChild relation leads to significant im-
provement in accuracy over local classification in
all cases, showing the value of collective clas-
sification. The improvement here is centered
on the categories of mediated/synt (for both
cases) and mediated/aggregate (for Nis-
sim+ol+hasChild) as well as their distinction from
new.10 It is also interesting that collective clas-
sification with a concise feature set and a sim-
ple decision tree as used in Nissim+ol+hasChild,
performs equally well as RahmanNg+ol+hasChild,
which uses thousands of unigram and tree features
and a more sophisticated local classifier. It also
shows more consistent improvements over all fine-
grained classes.
The precedes relation does not lead to any fur-
ther improvement. We investigated several varia-
tions of the precedence link, such as restricting it
to certain grammatical relations, taking into account
definiteness or NP type but none of them led to
any improvement. We think there are two reasons
for this lack of success. First, the precedence of
mediated vs. new mentions does not follow a
clear order and is therefore not a very predictive fea-
ture (see Table 6). At first, this seems to contradict
studies such as Cahill and Riester (2009) that find
a variety of precedences according to information
status. However, many of the clearest precedences
they find are more specific variants of the old >p
mediated or old >p new precedence or they
are preferences at an even finer level than the one we
annotate, including for example the identification of
generics. Second, the clear old >p mediated
10For RhamanNg+ol+hasChild, the aggregate class suf-
fers from collective classification. We hypothesise that this is
an artefact of the one-vs-all training/testing for rare categories.
801
local collective
RahmanNg+ol RahmanNg+olRahmanNg RahmanNg+ol
+hasChild +hasChild+precedes
R P F R P F R P F R P F
Coarse
old 81.3 90.1 85.5 82.6 91.4 86.8 83.5 87.8 85.6 82.9 87.2 85.0
mediated 61.4 68.6 64.8 61.5 71.9 66.3 66.7 79.5 72.6 64.8 76.7 70.3
new 82.1 69.9 75.5 84.9 70.1 76.8 89.0 74.9 81.3 86.9 73.5 79.6
acc 74.9 76.3 79.8 78.3
Fine
old 85.1 87.0 86.0 85.6 87.9 86.7 85.3 87.4 86.3 85.8 87.5 86.4
med/knowledge 65.8 67.2 66.5 64.8 72.6 68.5 67.1 69.6 68.3 64.7 73.2 68.7
med/synt 55.8 72.1 62.9 55.8 72.6 63.1 79.8 78.1 78.9 79.8 78.1 78.9
med/agg 29.9 75.9 42.9 29.9 75.9 42.9 17.1 53.7 25.9 14.2 49.2 22.1
med/func 27.7 38.3 32.1 38.5 69.4 49.5 40.0 44.1 42.0 40.0 40.0 40.0
med/comp 25.3 86.5 39.1 76.7 82.2 79.3 74.3 62.7 68.0 74.3 62.7 68.0
med/bridging 10.6 44.6 17.1 9.0 47.2 15.2 1.0 15.2 2.0 1.0 13.7 1.9
new 87.3 66.3 75.4 89.0 67.8 77.0 89.2 74.6 81.2 89.2 74.6 81.2
acc 72.6 74.6 77.5 77.4
Table 8: Collective classification compared to Rahman and Ng?s local classifier. Best performing algorithms are
bolded.
and old >p new preferences are partially already
captured by the local features, especially the gram-
matical role, as, for example, subjects are often both
old as well as early on in a sentence.
With regard to fine-grained classification, many
categories including comparative anaphora, are
identified quite reliably, especially in the multiclass
classification setting (Nissim+ol+hasChild). Bridg-
ing seems to be the by far most difficult category
to identify with final best F-measures still very low.
Most bridging mentions do not have any clear inter-
nal structure or external syntactic contexts that sig-
nal their presence. Instead, they rely more on lexi-
cal and world knowledge for recognition. Unigrams
could potentially encapsulate some of this lexical
knowledge but ? without generalization ? are too
sparse for a relatively rare category such as bridg-
ing (6% of all mentions) to perform well. The diffi-
culty of bridging recognition is an important insight
of this paper as it casts doubt on the strategy in pre-
vious research to concentrate almost exclusively on
antecedent selection (see Section 2).
6 Conclusions
We presented a new approach to information sta-
tus classification in written text, for which we also
provide the first reliably annotated English language
corpus. Based on linguistic intuition, we define fea-
tures for classifying mentions collectively. We show
that our collective classification approach outper-
forms the state-of-the-art in coarse-grained IS classi-
fication by about 10% (Nissim, 2006) and 5% (Rah-
man and Ng, 2011) accuracy. The gain is almost
entirely due to improvements in distinguishing be-
tween new and mediatedmentions. For the latter,
we also report the ? to our knowledge ? first fine-
grained IS classification results.
Since the work reported in this paper relied ? fol-
lowing Nissim (2006) and Rahman and Ng (2011)
? on gold standard mentions and syntactic anno-
tations, we plan to perform experiments with pre-
dicted mentions as well. We also have to im-
prove the recognition of bridging, ideally combining
recognition and antecedent selection for a complete
resolution component. In addition, we plan to inte-
grate IS resolution with our coreference resolution
system (Cai et al, 2011) to provide us with a more
comprehensive discourse processing system.
Acknowledgements. Katja Markert received a Fel-
lowship for Experienced Researchers by the Alexander-
von-Humboldt Foundation and Yufang Hou is funded by
a PhD scholarship from the Research Training GroupCo-
herence in Language Processing at Heidelberg Univer-
sity. We thank the Heidelberg Institute for Theoretical
Studies for hosting Katja Markert and funding the anno-
tation study, and the annotators for their diligent work.
802
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Betty J. Birner and Gregory Ward. 1998. Information
Status and NoncanonicalWord Order in English. John
Benjamins, Amsterdam, The Netherlands.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing, Singapore, 2?7 August 2009,
pages 817?825.
Jie Cai, ?Eva Mu?jdricza-Maydt, and Michael Strube.
2011. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, Vancouver, B.C.,
Canada, 3?8 December, 2001, pages 625?632, Cam-
bridge, Mass. MIT Press.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Rochester, N.Y., 22?27 April
2007, pages 236?243.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pages 320?327.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Iorn Korzen and Matthias Buch-Kromann. 2011.
Anaphoric relations in the Copenhagen dependency
treebanks. In S. Dipper and H. Zinsmeister, edi-
tors, Corpus-based Investigations of Pragmatic and
Discourse Phenomena, volume 3 of Bochumer Lin-
guistische Arbeitsberichte, pages 83?98. University of
Bochum, Bochum, Germany.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003.
Discourse and information structure. Journal of Logic,
Language and Information. Special Issue on Dis-
cource and Information Structure, 12(3):149?259.
Knud Lambrecht. 1994. Information Structure and Sen-
tence Form. Cambridge, U.K.: Cambridge University
Press.
Qing Lu and Lise Getoor. 2003. Link-based classifica-
tion. In Proceedings of the 20th International Confer-
ence on Machine Learning, Washington, D.C., 21?24
August 2003, pages 496?503.
Sofus A. Macskassy and Foster Provost. 2007. Classi-
fication in networked data: A toolkit and a univariate
case study. Journal of Machine Learning Research,
8:935?983.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?401.
Josef Meyer and Robert Dale. 2002. Mining a corpus to
support associative anaphora resolution. In Proceed-
ings of the 4th International Conference on Discourse
Anaphora and Anaphor Resolution, Lisbon, Portugal,
18?20 September, 2002.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Ani Nenkova, Jason Brenier, Anubha Kothari, Sasha Cal-
houn, LauraWhitton, David Beaver, and Dan Jurafsky.
2007. To memorize or to predict: Prominence labeling
in conversational speech. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Rochester, N.Y., 22?27 April
2007, pages 9?16.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings of
Human Language Technologies 2009: The Conference
of the North American Chapter of the Association for
Computational Linguistics, Boulder, Col., 31 May ? 5
June 2009, pages 575?583.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
803
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, Sydney, Australia, 22?23 July 2006, pages
94?012.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, 21?26 July 2004, pages
272?279.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio. 2004. The MATE/GNOME proposals
for anaphoric annotation, revisited. In Proceedings of
the 5th SIGdial Workshop on Discourse and Dialogue,
Cambridge, Mass., 30 April ? 1 May 2004, pages 154?
162.
Scott Prevost. 1996. An information structural approach
to spoken language generation. In Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics, Santa Cruz, Cal., 24?27 June 1996,
pages 294?301.
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Ellen F. Prince. 1992. The ZPG letter: Subjects,
definiteness, and information-status. In W.C. Mann
and S.A. Thompson, editors, Discourse Description.
Diverse Linguistic Analyses of a Fund-Raising Text,
pages 295?325. John Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2011. Learning the in-
formation status of noun phrases in spoken dialogues.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland, U.K., 27?29 July 2011, pages 1069?1080.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A recursive annotation scheme for referential informa-
tion status. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
La Valetta, Malta, 17?23 May 2010, pages 717?722.
Julia Ritz, Stefanie Dipper, and Michael Go?tze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the
6th International Conference on Language Resources
and Evaluation, Marrakech, Morocco, 26 May ? 1
June 2008, pages 2137?2142.
Ryohei Sasano and Sadao Kurohashi. 2009. A prob-
abilistic model for associative anaphora resolution.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1455?1464.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2011. Information status distinctions and re-
ferring expressions: An empirical study of references
to people in news summaries. Computational Linguis-
tics, 37(4):811?842.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, Singapore, 6?7 August 2009.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Proceedings of the 18th Conference on Uncertainty
in Artificial Intelligence, Edmonton, Alberta, Canada,
1-4 August 2002, pages 485?492.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
Yiming Yang, Sea?n Slattery, and Rayid Ghani. 2002. A
study of approaches to hypertext categorization. Jour-
nal of Intelligent Information Systems, 18(2-3):219?
241.
Guodong Zhou and Fang Kong. 2009. Global learning of
noun phrase anaphoricity in coreference resolution via
label propagation. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Singapore, 6?7 August 2009, pages 978?986.
804
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 42?50
Manchester, August 2008
Eliciting Subjectivity and Polarity Judgements on Word Senses
Fangzhong Su
School of Computing
University of Leeds
fzsu@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds
markert@comp.leeds.ac.uk
Abstract
There has been extensive work on elicit-
ing human judgements on the sentiment
of words and the resulting annotated word
lists have frequently been used for opin-
ion mining applications in Natural Lan-
guage Processing (NLP). However, this
word-based approach does not take differ-
ent senses of a word into account, which
might differ in whether and what kind
of sentiment they evoke. In this paper,
we therefore introduce a human annotation
scheme for judging both the subjectivity
and polarity of word senses. We show that
the scheme is overall reliable, making this
a well-defined task for automatic process-
ing. We also discuss three issues that sur-
faced during annotation: the role of anno-
tation bias, hierarchical annotation (or un-
derspecification) and bias in the sense in-
ventory used.
1 Introduction
Work in psychology, linguistics and computational
linguistics has explored the affective connotations
of words via eliciting human judgements (see
Section 2 for an in-depth review). Two impor-
tant parameters in determining affective meaning
that have emerged are subjectivity and polarity.
Subjectivity identification focuses on determining
whether a language unit (such as a word, sentence
or document) is subjective, i.e. whether it ex-
presses a private state, opinion or attitude, or is
factual. Polarity identification focuses on whether
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
a language unit has a positive or negative connota-
tion.
Word lists that result from such studies would,
for example tag good or positive as a positive
word, bad as negative and table as neither. Such
word lists have frequently been used in natural lan-
guage processing applications, such as the auto-
matic identification of a review as favourable or
unfavourable (Das and Chen, 2001). However,
the word-based annotation conducted so far is at
least partially unreliable. Thus Andreevskaia and
Bergler (2006) find only a 78.7% agreement on
subjectivity/polarity tags between two widely used
word lists. One problem they identify is that word-
based annotation does not take different senses
of a word into account. Thus, many words are
subjectivity-ambiguous or polarity-ambiguous, i.e.
have both subjective and objective or both posi-
tive and negative senses, such as the words posi-
tive and catch with corresponding example senses
given below.
1
(1) positive, electropositive?having a positive electric
charge;?protons are positive? (objective)
(2) plus, positive?involving advantage or good; ?a plus (or
positive) factor? (subjective)
(3) catch?a hidden drawback; ?it sounds good but what?s
the catch?? (negative)
(4) catch, match?a person regarded as a good matrimonial
prospect (positive)
Inspired by Andreeivskaia and Bergler (2006)
and Wiebe and Mihalcea (2006), we therefore ex-
plore the subjectivity and polarity annotation of
word senses instead of words. We hypothesize
that annotation at the sense level might eliminate
one possible source of disagreement for subjectiv-
ity/polarity annotation and will therefore hopefully
lead to higher agreement than at the word level.
1
All examples in this paper are from WordNet 2.0.
42
An additional advantage for practical purposes is
that subjectivity labels for senses add an additional
layer of annotation to electronic lexica and can
therefore increase their usability. As an example,
Wiebe and Mihalcea (2006) prove that subjectiv-
ity information for WordNet senses can improve
word sense disambiguation tasks for subjectivity-
ambiguous words (such as positive). In addition,
Andreevskaia and Bergler (2006) show that the
performance of automatic annotation of subjectiv-
ity at the word level can be hurt by the presence of
subjectivity-ambiguous words in the training sets
they use. A potential disadvantage for annotation
at the sense level is that it is dependent on a lexical
resource for sense distinctions and that an annota-
tion scheme might have to take idiosyncracies of
specific resources into account or, ideally, abstract
away from them.
In this paper, we investigate the reliability
of manual subjectivity labeling of word senses.
Specifically, we mark up subjectivity/attitude (sub-
jective, objective, and both) of word senses as well
as polarity/connotation (positive, negative and no
polarity). To the best of our knowledge, this is
the first annotation scheme for judging both sub-
jectivity and polarity of word senses. We test its
reliability on the WordNet sense inventory. Over-
all, the experimental results show high agreement,
confirming our hypothesis that agreement at sense
level might be higher than at the word level. The
annotated sense inventory will be made publically
available to other researchers at http://www.
comp.leeds.ac.uk/markert/data.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous related work.
Section 3 describes our human annotation scheme
for word sense subjectivity and polarity in detail.
Section 4 presents the experimental results and
evaluation. We also discuss the problems of bias in
the annotation scheme, the impact of hierarchical
organization or underspecification on agreement as
well as problems with bias in WordNet sense de-
scriptions. Section 5 compares our annotation to
the annotation of a different scheme, followed by
conclusions and future work in Section 6.
2 Related Work
Osgood et al (1957) proposed semantic differ-
ential to measure the connotative meaning of
concepts. They conducted a factor analysis of
large collections of semantic differential scales and
pointed out three referring attitudes that people use
to evaluate words and phrases?evaluation (good-
bad), potency (strong-weak), and activity (active-
passive). Also, they showed that these three di-
mensions of affective meaning are cross-cultural
universals from a study on dozens of cultures (Os-
good et al, 1975). This work has spawned a con-
siderable amount of linguistic and psychological
work in affect analysis on the word level. In psy-
chology both the Affective Norms for English
Words (ANEW) project as well as the Magel-
lan project focus on collecting human judgements
on affective meanings of words, roughly follow-
ing Osgood?s scheme. In the ANEW project they
collected numerical ratings of pleasure (equivalent
to our term polarity), arousal, and dominance for
1000 English terms (Bradley and Lang, 2006) and
in Magellan they collected cross-cultural affective
meanings (including polarity) in a wide variety of
countries such as the USA, China, Japan, and Ger-
many (Heise, 2001). Both projects concentrate on
collecting a large number of ratings on a large va-
riety of words: there is no principled evaluation of
agreement.
The more linguistically oriented projects of the
General Inquirer (GI) lexicon
2
and the Ap-
praisal framework
3
also provide word lists anno-
tated for affective meanings but judgements seem
to be currently provided by one researcher only.
Especially the General Enquirer which contains
11788 words marked for polarity (1915 positive,
2291 negative and 7582 no-polarity words) seems
to use a relatively ad hoc definition of polarity.
Thus, for example amelioration is marked as no-
polarity whereas improvement is marked as posi-
tive.
The projects mentioned above center on subjec-
tivity analysis on words and therefore are not good
at dealing with subjectivity or polarity-ambiguous
words as explained in the Introduction. Work that
like us concentrates on word senses includes ap-
proaches where the subjectivity labels are automat-
ically assigned such as WordNet-Affect (Strap-
parava and Valitutti, 2004), which is a subset of
WordNet senses with semi-automatically assigned
affective labels (such as emotion, mood or be-
haviour). In a first step, they manually collect
an affective word list and a list of synsets which
contain at least one word in this word list. Fine-
2
Available at http://www.wjh.harvard.edu/ inquirer/
3
Available at http://www.grammatics.com/appraisal/
43
grained affect labels are assigned to these synsets
by the resource developers. Then they automati-
cally expand the lists by employing WordNet re-
lations which they consider to reliably preserve
the involved labels (such as similar-to, antonym,
derived-from, pertains-to, and attribute). Our work
differs from theirs in three respects. First, they
focus on their semi-automatic procedure, whereas
we are interested in human judgements. Second,
they use a finer-grained set of affect labels. Third,
they do not provide agreement results for their an-
notation. Similarly, SentiWordNet
4
is a resource
with automatically determined polarity of word
senses in WordNet (Esuli and Sebastiani, 2006),
produced via bootstrapping from a small manually
determined seed set. Each synset has three scores
assigned, representing the positive, negative and
neutral score respectively. No human annotation
study is conducted.
There are only two human annotation studies
on subjectivity of word senses as far as we are
aware. Firstly, theMicro-WNOp corpus is a list of
about 1000 WordNet synsets annotated by Cerini
et al (2007) for polarity. The raters manually as-
signed a triplet of numerical scores to each sense
which represent the strength of positivity, negativ-
ity, and neutrality respectively. Their work dif-
fers from us in two main aspects. First, they fo-
cus on polarity instead of subjectivity annotation
(see Section 3 for a discussion of the two con-
cepts). Second, they do not use absolute categories
but give a rating between 0 and 1 to each synset?
thus a synset could have a non-zero rating on both
negativity and positivity. They also do not report
on agreement results. Secondly, Wiebe and Mi-
halcea (2006) mark up WordNet senses as subjec-
tive, objective or both with good agreement. How-
ever, we expand their annotation scheme with po-
larity annotation. In addition, we hope to annotate
a larger set of word senses.
3 Human Judgements on Word Sense
Subjectivity and Polarity
We follow Wiebe and Mihalcea (2006) in that
we see subjective expressions as private states
?that are not open to objective observation or ver-
ification? and in that annotators distinguish be-
tween subjective (S), objective (O) and both sub-
jective/objective (B) senses.
4
Available at http://sentiwordnet.isti.cnr.it/
Polarity refers to positive or negative connota-
tions associated with a word or sense. In contrast
to other researchers (Hatzivassiloglou and McKe-
own, 1997; Takamura et al, 2005), we do not see
polarity as a category that is dependent on prior
subjectivity assignment and therefore applicable to
subjective senses only. Whereas there is a depen-
dency in that most subjective senses have a rel-
atively clear polarity, polarity can be attached to
objective words/senses as well. For example, tu-
berculosis is not subjective ? it does not describe
a private state, is objectively verifiable and would
not cause a sentence containing it to carry an opin-
ion, but it does carry negative associations for the
vast majority of people. We allow for the polarity
categories positive (P), negative (N), varying (V)
or no-polarity (NoPol).
Overall we combine these annotations into 7
categories?S:N, S:P, S:V, B, O:N, O:P, and
O:NoPol, which are explained in detail in the sub-
sequent sections. Figure 1 gives an overview of the
hierarchies over all categories.
As can be seen in Figure 1, our annotation
scheme allows for hierarchical annotation, i.e. it is
possible to only annotate for subjectivity or polar-
ity. This can be necessary to achieve higher agree-
ment by merging categories or to concentrate in
specific applications on only one aspect.
3.1 Subjectivity
3.1.1 Subjective Senses
Subjective senses include several categories,
which can be expressed by nouns, verbs, adjec-
tives or adverbs. Firstly, we include emotions.
Secondly, we include judgements, assessments and
evaluations of behaviour as well as aesthetic as-
sessments of individuals, natural objects and arte-
facts. Thirdly, mental states such as doubts, beliefs
and speculations are also covered by our definition.
This grouping follows relatively closely the def-
inition of attitudinal positioning in the Appraisal
scheme (which has, however, only been used on
words, not on word senses before).
These types of subjectivity can be expressed via
direct references to an emotion or mental state (see
Example 5 or 8 below) as well as by expressive
subjective elements (Wiebe and Mihalcea, 2006).
Expressive subjective elements contain judgemen-
tal references to objects or events. Thus, pontifi-
cate in Example 6 below is a reference to a speech
event that always judges it negatively; beautiful as
44
word sense
subjective(S) both(B) objective(O)
negative positive varying/context-depedent(S:V) strong negativeconnotation(O:N) no strongconnotation(O:NoPol) strong positiveconnotation(O:P)(S:N) (S:P)
Figure 1: Overview of the hierarchies over all categories
in Example 7 below is a positive judgement.
(5) angry?feeling or showing anger; ?angry at the
weather; ?angry customers; an angry silence? (emotion)
(6) pontificate?talk in a dogmatic and pompous manner;
?The new professor always pontificates? (assessment of
behaviour)
(7) beautiful?aesthetically pleasing (aesthetic assess-
ment)
(8) doubt, uncertainty, incertitude, dubiety, doubtfulness,
dubiousness?the state of being unsure of something
(mental state)
3.1.2 Objective Senses
Objective senses refer to persons, objects, ac-
tions, events or states without an inherent emotion
or judgement or an expression of a mental state.
Examples are references to individuals via named
entities (see Example 9) or non-judgemental refer-
ences to artefacts, persons, animals, plants, states
or events (see Example 10 and 11). If a sentence
contains an opinion, it is not normally due to the
presence of this word sense and the sense often
expresses objectively verifiable states or events.
Thus, Example 12 is objective as we can verify
whether there is a war going on. In addition, a sen-
tence containing this sense of war does not neces-
sarily express an opinion.
(9) Einstein, Albert Einstein ? physicist born in Germany
who formulated the special theory of relativity and the
general theory of relativity; Einstein also proposed that
light consists of discrete quantized bundles of energy
(later called photons) (1879-1955) (named entity)
(10) lawyer, attorney ? a professional person authorized to
practice law; conducts lawsuits or gives legal advice
(non-judgemental reference to person)
(11) alarm clock, alarm ? a clock that wakes sleeper at preset
time (non-judgemental reference to object)
(12) war, warfare ? the waging of armed conflict against an
enemy; ?thousands of people were killed in the war?
(non-judgemental reference to event)
3.1.3 Both
In rare cases, a sense can be both subjective and
objective (denoted by B). The following are the
two most frequent cases. First, a WordNet sense
might conflate a private state meaning and an ob-
jective meaning of a word in the gloss description.
Thus, in Example 13 we have the objective literal
use of the word tarnish mentioned such as tarnish
the silver, which does not express a private state.
However, it also includes a metaphorical use of
tarnish as in tarnish a reputation, which implicitly
expresses a negative attitude.
(13) tarnish, stain, maculate, sully, defile?make dirty or
spotty, as by exposure to air; also used metaphorically;
?The silver was tarnished by the long exposure to the
air?; ?Her reputation was sullied after the affair with a
married man?
The second case includes the inclusion of near-
synonyms (Edmonds, 1999) which differs on sen-
timent in the same synset list. Thus in Example
14, the term alcoholic is objective as it is not nec-
essarily judgemental, whereas the other words in
the synset such as soaker or souse are normally in-
sults and therefore subjective.
(14) alcoholic, alky, dipsomaniac, boozer, lush, soaker,
souse?a person who drinks alcohol to excess habitu-
ally
3.2 Polarity
3.2.1 Polarity of Subjective Senses
The polarity of a subjective sense can be positive
(Category S:P), negative (S:N), or varying, depen-
dent on context or individual preference (S:V). The
definitions of these three categories are as follows.
? S:P is assigned to private states that express
a positive attitude, emotion or judgement (see
Example 7).
? S:N is assigned to private states that express a
negative attitude, emotion or judgement (see
Example 5, 6 and 8).
? S:V is used for senses where the polarity is
varying by context or user. For example, it is
45
likely that you give an opinion about some-
body if you call him aloof; however, only
context can determine whether this is positive
or negative (see Example 15).
(15) aloof, distant, upstage?remote in manner; ?stood apart
with aloof dignity?; ?a distant smile?; ?he was upstage
with strangers? (S:V)
3.2.2 Polarity of Objective Senses
There are many senses that are objective but
have strong negative or positive connotations. For
example, war describes in many texts an objec-
tive state (?He fought in the last war?) but still
has strong negative connotations. In many (but not
all) cases the negative or positive associations are
mentioned in the WordNet gloss. Therefore, we
can determine three polarity categories for objec-
tive senses:
? O:NoPol Objective with no strong, generally
shared connotations (see Example 9, 10, 11
and 16).
? O:P Objective senses with strong positive
connotations. These refer to senses that do
not describe or express a mental state, emo-
tion or judgement but whose presence in a
text would give it a strong feel-good flavour
(see Example 17).
? O:N Objective senses with strong negative
connotations. These are senses that do not
describe or express an emotion or judgement
but whose presence in a text would give it a
negative flavour (see Example 12). Another
example is (18): you can verify objectively
whether a liquor was diluted, but it is nor-
mally associated negatively.
(16) above?appearing earlier in the same text; ?flaws in the
above interpretation? (O:NoPol)
(17) remedy, curative, cure ? a medicine or therapy that
cures disease or relieve pain (O:P)
(18) adulterate, stretch, dilute, debase?corrupt, debase, or
make impure by adding a foreign or inferior substance;
often by replacing valuable ingredients with inferior
ones; ?adulterate liquor? (O:N)
We only allow positive and negative annotations
for objective senses if we expect strong connota-
tions that are shared among most people (in West-
ern culture). Thus, for example war, diseases and
crimes can relatively safely be predicted to have
shared negative connotations. In contrast, a sense
like the one of alarm clock in Example 11 might
have negative connotations for late risers but it
would be annotated as O:NoPol in our scheme. We
are interested in strong shared connotations as the
presence of such ?loaded? terms can partially in-
dicate bias in a text. In addition, such objective
senses are likely to give rise to figurative subjec-
tive senses (see Example 18).
4 Experiments and Evaluation
This section describes the experimental setup for
our annotation experiments, presents reliability re-
sults and discusses the benefits of the use of a hier-
archical annotation scheme as well as the problems
of bias in the annotation scheme, annotator prefer-
ences and bias in the sense inventory.
4.1 Dataset and Annotation Procedure
The dataset used in our annotation scheme is the
Micro-WNOp corpus
5
, which contains all senses
of 298 words in WordNet 2.0. We used it as it is
representative of WordNet with respect to its part-
of-speech distribution and includes synsets of rel-
atively frequent words, including a wide variety of
subjective senses. It contains 1105 synsets in total,
divided into three groups common (110 synset),
group1 (496 synsets) and group2 (499 synsets).
We used common as the training set for the anno-
tators and tested annotation reliability on group1.
Annotation was performed by two annotators.
Both are fluent English speakers; one is a compu-
tational linguist whereas the other is not in linguis-
tics. All annotation was carried out independently
and without discussion during the annotation pro-
cess. The annotators were furnished with guide-
line annotations with examples for each category.
Annotators saw the full synset, including all syn-
onyms, glosses and examples.
4.2 Agreement Study
Training. The two annotators first annotated the
common group for training. Observed agreement
on the training data is 83.6%, with a kappa (Co-
hen, 1960) of 0.76. Although this looks overall
quite good, several categories are hard to identify,
for example B and S:V, as can be seen in the con-
fusion matrix below (Table 1) with Annotator 1 in
columns and Annotator 2 in the rows.
Testing. Problem cases were discussed between
the annotators and a larger study on group 1 as test
5
Available at http://www.unipv.it/wnop/micrownop.tgz
46
Table 1: Confusion matrix for the training data
B S:N S:P S:V O:NoPol O:N O:P total
B 1 0 0 0 2 0 0 3
S:N 0 13 0 0 0 2 0 15
S:P 0 0 8 1 1 0 0 10
S:V 1 1 0 13 6 0 0 21
O:NoPol 1 0 0 0 50 0 0 51
O:N 0 0 0 0 2 4 0 6
O:P 0 0 1 0 0 0 3 4
total 3 14 9 14 61 6 3 110
data was carried out. Table 2 shows the confusion
matrix for all 7 categories.
Table 2: Confusion matrix on the test set
B S:N S:P S:V O:NoPol O:N O:P total
B 7 2 0 2 0 0 0 11
S:N 0 41 1 0 0 0 0 42
S:P 0 0 65 4 0 0 2 71
S:V 0 0 7 17 3 0 0 27
O:NoPol 9 1 2 6 253 5 8 284
O:N 0 14 0 2 0 25 0 41
O:P 1 0 5 0 1 0 13 20
total 17 58 80 31 257 30 23 496
The observed agreement is 84.9% and the kappa
is 0.77. This is good agreement for a relatively
subjective task. However, there is no improve-
ment over agreement in training although an ad-
ditional clarification phase of the training material
took place between training and testing.
We also computed single category kappa in or-
der to estimate which categories proved the most
difficult. Single category-kappa concentrates on
one target category and conflates all other cate-
gories into one non-target category and measures
agreement between the two resulting categories.
The results showed that S:N (0.80), S:P (0.84)
and O:NoPol (0.86) were highly reliable with less
convincing results for B (0.49), S:V (0.56), O:N
(0.68), and O:P (0.59). B is easily missed dur-
ing annotation (see Example 19), S:V is easily con-
fused with several other categories (Example 20),
whereas O:N is easily confused with O:NoPol and
S:N (Example 21); andO:P is easily confused with
O:NoPol and S:P (Example 22).
(19) antic, joke, prank, trick, caper, put-on?a ludicrous
or grotesque act done for fun and amusement (B vs
O:NoPol)
(20) humble?marked by meekness or modesty; not arro-
gant or prideful; ?a humble apology? (S:V vs S:P)
(21) hot?recently stolen or smuggled; ?hot merchandise?;
?a hot car? (O:N vs O:NoPol)
(22) profit, gain?the advantageous quality of being benefi-
cial (S:P vs O:P)
Our annotation scheme also needs testing on an
even larger data set as a few categories such as B
and O:P occur relatively rarely.
4.3 The Effect of Hierarchical Annotation
As mentioned above, our annotation scheme al-
lows us to consider the subjectivity or polarity dis-
tinction individually, leaving the full categoriza-
tion underspecified.
Subjectivity Distinction Only. For subjectivity
distinctions we collapse S:V, S:P and S:N into a
single label S (subjective) and O:NoPol, O:N and
O:P into a single label O (objective). B remains
unchanged. The resulting confusion matrix on the
test set is in Table 3.
Table 3: Confusion matrix for Subjectivity
B S O total
B 7 4 0 11
S 0 135 5 140
O 10 30 305 345
total 17 169 310 496
Observed agreement is 90.1% and kappa is 0.79.
Single category kappa is 0.49 for B, 0.82 for S and
0.80 for O. As B is a very rare category (less than
5% of items), this is overall an acceptable level
of distinction with excellent reliability for the two
main categories.
Polarity Distinction Only. We collapse O:N
and S:N into a single category N (negative) and
O:P and S:P into P (positive), leaving the other
categories intact. This results in 5 categories B,
S:V/V, NoPol, N and P. The resulting confusion
matrix is in Table 4.
Table 4: Confusion matrix for Polarity
B N P V NoPol total
B 7 2 0 2 0 11
N 0 80 1 2 0 83
P 1 0 85 4 1 91
V 0 0 7 17 3 27
NoPol 9 6 10 6 253 284
total 17 88 103 31 257 496
Observed agreement is 89.1% and kappa is 0.83.
Single category kappa is as follows: B (0.49), N
(0.92), P (0.85), V (0.56), and NoPol (0.86). This
means all categories but B and V (together about
10% of items) are reliably identifiable.
Overall we show that both polarity and sub-
jectivity identification of word senses can be re-
liably annotated and are well-defined tasks for
automatic classification. Specifically the per-
47
centage agreement of about 90% for word sense
polarity/subjectivity identification is substantially
higher than the one of 78% reported in An-
dreeivskaia and Bergler (2006). Agreement for
polarity-only is significantly higher than for the
full annotation scheme, showing the value of hi-
erarchical annotation. We believe hierarchical an-
notation is also appropriate for this task, as sub-
jectivity and polarity are linked but still separate
concepts. Thus, a researcher might want to mainly
focus on explicitly expressed opinions as exempli-
fied by subjectivity, whereas another can also focus
on opinion bias in a text as expressed by loaded
words of positive or negative polarity.
4.4 Bias in Annotation Performance, Sense
Inventory and Annotation Guidelines
Why do annotators assign different labels to some
senses? Three main aspects are responsible for
non-spurious disagreement.
Firstly, individual perspective or bias played a
role. For example, Annotator 2 was more inclined
to give positive or negative polarity labels than An-
notator 1 as can be seen in Table 4, where Anno-
tator 2 assigned 103 positive and 88 negative la-
bels,whereas Annotator 1 assigned only 91 posi-
tive and 83 negative labels.
Secondly, the WordNet sense inventory con-
flates near-synonyms which just differ in sentiment
properties (see Section 3.1.3 and Example 14). Al-
though the labels B and S:V were specifically cre-
ated in the annotation scheme to address this prob-
lem, these cases still proved confusing to annota-
tors and do not readily lead to consistent annota-
tion.
Thirdly, WordNet sometimes includes a conno-
tation bias either in its glosses or in its hierarchical
organization. Here we use the word connotation
bias for the inclusion of connotations that seem
highly controversial. Thus, in Example 23, the
WordNet gloss for Iran evokes negative connota-
tions by mentioning allegations of terrorism.
6
In
Example 24 skinhead is a hyponym of bully, giv-
ing strong negative connotations for all skinheads.
Although the annotation scheme explicitly encour-
ages annotators to disregard especially such con-
troversial connotations as in Example 23 such ex-
amples can still confuse annotators and show that
word sense annotation is to a certain degree depen-
6
Note that this was part of WordNet 2.0 and has been re-
moved in WordNet 2.1.
dent on the sense inventory used.
(23) Iran, Islamic Republic of Iran, Persia?a theocratic is-
lamic republic in the Middle East in western Asia; Iran
was the core of the ancient empire that was known
as Persia until 1935; rich in oil; involved in state-
sponsored terrorism
(24) skinhead ?? bully, tough, hooligan, ruffian, rough-
neck, rowdy, yob, yobo, yobbo
Some of our good reliability performance might
be due to one particular instance of bias in the an-
notation guidelines. We strongly advised annota-
tors to only annotate positive or negative polarity
for objective senses when strong, shared connota-
tions are expected,
7
thereby ?de-individualising?
the task of polarity annotation. This introduces
a bias towards the category NoPol for objective
senses. We also did not allow varying polarity for
objective senses, instructing annotators that such
polarity would be unclear and should be annotated
as NoPol as not being a strong shared connotation.
It can of course be questioned whether the intro-
duction of such a bias is good or not. It helps
agreement but might reduce the usefulness of the
annotation as individual connotations are not an-
notated for objective senses. However, to consider
more individual connotations needs an annotation
effort with a much larger number of annotators to
arrive at a profile of polarity connotations over a
larger population. We leave this for future work.
Our current framework is comprehensive for sub-
jectivity as well as polarity for subjective senses.
4.5 Gold Standard
After discussion between the two annotators, a
gold standard annotation was agreed upon. Our
data set consists of this agreed set as well as the re-
mainder of the Micro-WNOp corpus (group2) an-
notated by one of the annotators alone after agree-
ment was established.
How many words are subjectivity-ambiguous or
polarity-ambiguous, i.e. how much information
do we gain by annotating senses over annotating
words? As the number of senses increases with
word frequency, we expect rare words to be less
likely to be subjectivity-ambiguous than frequent
words. The Micro-WNOp corpus contains rela-
tively frequent words so we will get an overesti-
mation of subjectivity-ambiguous word types from
this corpus, though not necessarily of word tokens.
Of all 298 words, 97 (32.5%) are subjectivity-
ambiguous, a substantial number. Fewer words are
7
See Section 3.2.2 for justification.
48
polarity-ambiguous: only 10 words have at least
one positive and one negatively annotated sense
with a further 44 words having at least one sub-
jective sense with varying polarity (S:V). This sug-
gests that subjective and objective uses of the same
word are more frequent than reverses in emotional
orientation.
5 Comparison to Original Polarity
Annotation (Cerini et al)
We can compare the reliability of our own annota-
tion scheme with the original (polarity) annotation
in the Micro-WNOp corpus. Cerini et al (2007)
do not present agreement figures but as their cor-
pus is publically available we can easily compute
reliability. Recall that each synset has a triplet of
numerical scores between 0 and 1 each: positiv-
ity, negativity and neutrality, which is not explic-
itly annotated but derived as 1 ? (positivity +
negativity). Subjectivity in our sense (existence
of a private state) is not annotated.
The ratings of three annotators are available for
Group 1 and of two annotators for Group 2. We
measured the Pearson correlation coefficient be-
tween each annotator pair for both groups for both
negativity and positivity scoring. As correlation
can be high without necessarily high agreement
on absolute values, we also computed a variant of
kappa useful for numerical ratings, namely alpha
(Artstein and Poesio, 2005), which gives weight
to degrees of disagreement. Thus, a disagreement
between two scores would be weighted as the ab-
solute value of score1 ? score2. The results are
listed in Table 5.
Table 5: Reliability of original annotation on
Micro-WNOp
dataset raters score type correlation alpha
Group 1 1 and 2 negative 83.7 64.9
Group 1 1 and 3 negative 86.4 71.8
Group 1 2 and 3 negative 82.5 56.9
Group 1 1 and 2 positive 80.5 60.9
Group 1 1 and 3 positive 87.8 74.9
Group 1 2 and 3 positive 78.2 57.5
Group 2 1 and 2 negative 95.9 90.7
Group 2 1 and 2 positive 92.2 84.9
Correlation between the annotators is high.
However, Rater 2 (in Group1) still behaves differ-
ently from the other two raters, giving consistently
higher or lower scores overall, leading to low al-
pha. Thus, we can conclude that Group 2 is much
more reliably annotated than Group 1 and that es-
pecially Rater 2 in Group 1 is an outlier in this
(small) set of raters. This also shows that work
with several annotators is valuable and should be
conducted for our scheme as well.
6 Conclusion and Future Work
We elicit human judgements on the subjectivity
and polarity of word senses. To the best of our
knowledge, this is the first such annotation scheme
for both categories. We detail the definitions for
each category and measure the reliability of the an-
notation. The experimental results show that when
using all 7 categories, only 3 categories (S:N, S:P,
and O:NoPol) are reliable while the reliability of
the other 4 categories is not high. We also show
that this is improved by the virtue of hierarchical
annotation and that the general tasks of subjectivity
and polarity annotation on word senses are there-
fore well-defined. Moreover, we also discuss the
effect of different kinds of bias on our approach.
In future we will refine the guidelines for the
more difficult categories, including more detailed
advice on how to deal with sense inventory bias.
We will also perform larger-scale annotation exer-
cises with more annotators as the latter is necessary
to deal with more individualised polarity connota-
tions. In addition, we will use the data to test learn-
ing methods for the automatic detection of subjec-
tivity and polarity properties of word senses.
References
Andreevskaia, Alina and Sabine Bergler. 2006. Min-
ing WordNet for Fuzzy Sentiment: Sentiment Tag
Extraction from WordNet Glosses. Proceedings of
EACL?06.
Artstein, Ron and Massimo Poesio. 2005.
Kappa
3
=alpha(or beta). Technical Report CSM-
437, University of Essex.
Bradley, Margaret and Peter Lang. 1999. Affective
Norms for EnglishWords (ANEW): Stimuli, Instruc-
tion Manual and Affective Ratings Technical report
C-1, the Center for Research in Psychophysiology,
University of Florida. .
Cerini, Sabrina, Valentina Compagnoni, Alice Demon-
tis, Maicol Formentelli, and Caterina Gandini. 2007.
Micro-WNOp: A Gold Standard for the Evaluation
of Automatically Compiled Lexical Resources for
Opinion Mining. Language resources and linguis-
tic theory: Typology, second language acquisition,
English linguistics.
49
Cohen, Jacob. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psychological
Measurement, Vol.20, No.1.
Das, Sanjiv and Mike Chen. 2001. Yahoo! for Ama-
zon: Extracting Market Sentiment from Stock Mes-
sage Boards. Proceedings of APFA?01.
Edmonds, Philip. 1999. Semantic Representations
Of Near-Synonyms For Automatic Lexical Choice.
PhD thesis, University of Toronto.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource for
Opinion Mining. Proceedings of LREC?06.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. Proceedings of ACL?97.
Heise, David. 2001. Project Magellan: Collecting
Cross-culture Affective Meanings via the Internet.
Electronic Journal of Sociology.
Osgood, Charles, William May, and Murray Miron.
1975. Cross-cultural Universals of Affective Mean-
ing. University of Illinois Press.
Osgood, Charles, George Suci, and Percy Tannenbaum.
1957. The Measurment of Meaning. University of
Illinois Press.
Strapparava, Carlo and Alessandro Valitutti. 2004.
WordNet-Affect: an Affective Extension of Word-
Net. Proceedings of LREC?04.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting Semantic Orientations of
Words using Spin Model. Proceedings of ACL?05.
Wiebe, Janyce and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation.
50
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39?47,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semi-supervised Graph-based Genre Classification for Web Pages
Noushin Rezapour Asheghi
School of Computing
University of Leeds
scs5nra@leeds.ac.uk
Katja Markert
L3S Research Center
Leibniz Universit?at Hannover
and School of Computing
University of Leeds
markert@l3s.de
Serge Sharoff
School of Modern
Languages and Cultures
University of Leeds
s.sharoff@leeds.ac.uk
Abstract
Until now, it is still unclear which set of
features produces the best result in au-
tomatic genre classification on the web.
Therefore, in the first set of experiments,
we compared a wide range of content-
based features which are extracted from
the data appearing within the web pages.
The results show that lexical features such
as word unigrams and character n-grams
have more discriminative power in genre
classification compared to features such
as part-of-speech n-grams and text statis-
tics. In a second set of experiments,
with the aim of learning from the neigh-
bouring web pages, we investigated the
performance of a semi-supervised graph-
based model, which is a novel technique
in genre classification. The results show
that our semi-supervised min-cut algo-
rithm improves the overall genre classifi-
cation accuracy. However, it seems that
some genre classes benefit more from this
graph-based model than others.
1 Introduction
In Automatic Genre Identification (AGI), docu-
ments are classified based on their genres rather
than their topics or subjects. Genre classes such
as editorial, interview, news and blog which are
recognizable by their distinct purposes, can be on
any topic. The most important application of AGI
could be in Information Retrieval. If a user could
use the search engine to retrieve web pages from
a specific genre such as news articles, reviews
or blogs, search results could be more beneficial.
With the aim of enhancing search engines, AGI
has attracted a lot of attention (see Section 2).
In this paper, we investigate two important open
questions in AGI. The first question is: what set
of features produces the best result in genre clas-
sification on the web? The drawbacks of exist-
ing genre-annotated web corpora (low inter-coder
agreement; false correlations between topic and
genre classes) resulted in researchers? doubt on the
outcomes of classification models based on these
corpora (Sharoff et al., 2010). Therefore, in order
to answer this question, we perform genre classi-
fication with a wide range of features on a reli-
able and source diverse genre-annotated web cor-
pus. The second question that we investigate in
this paper is: could we exploit the graph structure
of the web to increase genre classification accu-
racy? With the aim of learning from the neigh-
bouring web pages, we investigated the perfor-
mance of a semi-supervised graph-based model,
which is a novel technique in genre classification.
The remainder of this paper is structured as fol-
lows. After reviewing related work in Section 2,
we compare different supervised genre classifica-
tion models based on various lexical, POS-based
and text statistics features in Section 3. Section 4
describes our semi-supervised graph-based classi-
fication experiment, where we use the multi-class
min-cut algorithm as a novel technique in genre
classification. Section 5 concludes the findings
and discusses future work.
2 Related Work
There has been a considerable body of research
in AGI. In previous studies on automatic genre
classification of web pages, various types of fea-
tures such as common words (Stamatatos et al.,
2000), function words (Argamon et al., 1998),
word unigrams (Freund et al., 2006), character
n-grams (Kanaris and Stamatatos, 2007), part-of-
speech tags (Karlgren and Cutting, 1994) , part-
of-speech trigrams (Argamon et al., 1998; San-
tini, 2007), document statistics (e.g. average sen-
tence length, average word length and type/token
ratio) (Finn and Kushmerick, 2006; Kessler et
39
al., 1997), HTML tags (e.g. (Santini, 2007))
have been explored. However, researchers con-
ducted genre classification experiments with dif-
ferent features on different corpora with differ-
ent sets of genre labels. As a result, it is dif-
ficult to compare them. This motivated Sharoff
et al. (2010) to examine a wide range of word-
based, character-based and POS-based features on
the existing genre-annotated corpora. They re-
ported that word unigrams and character 4-grams
outperform other features in genre classification.
However, they concluded that the results cannot
be trusted because of two main reasons. First,
some of these collections exhibit low inter-coder
agreement and any results based on unreliable data
could be misleading. Second, the spurious cor-
relation between topic and genre classes in some
of these corpora was one of the reasons for some
of the very impressive results reported by Sharoff
et al. (2010). These good results were achieved
by detecting topics rather than genres of individ-
ual texts. A similar point was made by Petrenz
and Webber (2010) who examined the impact of
topic change on the performance of AGI systems.
They showed that a shift in topic can have a mas-
sive impact on genre classification models which
are based on lexical features such as word uni-
grams or character n-grams. Therefore, the ques-
tion which set of features produces the best result
in automatic genre classification on the web is still
an open question. In order to investigate this ques-
tion, we perform genre classification with a wide
range of features on a reliable and topically diverse
dataset. Section 3.1 describes the dataset and the
experimental setup.
Most of the current works in the field of AGI
concentrated on extracting features from the con-
tent of the documents and classify them by em-
ploying a standard supervised algorithm. How-
ever, on the web there are other sources of
information which can be utilized to improve
genre classification of web pages. For instance,
the web has a graph structure and web pages
are connected via hyper-links. These connec-
tions could be exploited to improve genre clas-
sification. Various graph-based classification al-
gorithms have been proposed to improve topic
classification for web pages, such as the re-
laxation labelling algorithm (Chakrabarti et al.,
1998), iterative classification algorithm (Lu and
Getoor, 2003), Markov logic networks (Crane
and McDowell, 2012), random graph walk (Lin
and Cohen, 2010) and weighted-vote relational
neighbour algorithm (Macskassy and Provost,
2007). These classification algorithms which uti-
lize hyper-link connections between web pages
to construct graphs, outperformed the classifiers
which are solely based on textual content of the
web pages for topic classification. Such connected
data presents opportunities for boosting the perfor-
mance of genre classification too.
Graph-based web page classification presented
in studies such as (Crane and McDowell, 2012;
Lu and Getoor, 2003; Macskassy and Provost,
2007) on the WebKB dataset (CRAVEN, 1998)
could be considered as genre classification as op-
posed to topic classification. The WebKB dataset
contains web pages from four computer science
departments categorised into seven classes: stu-
dent, faculty, staff, department, course, project
and other. However, this dataset is very specific
to the academic domain with low coverage for
the web overall, whereas we examine graph-based
learning for automatic genre classification of web
pages on a much more general dataset with pop-
ular genre classes such as news, blog and edito-
rial. Moreover, the graph-based algorithms used
on the WebKB dataset are all supervised and were
performed on a very clean and noise free dataset
which was achieved by removing the class other.
Class other contains all the web pages which do
not belong to any other predefined classes. How-
ever, our experiment is in a semi-supervised man-
ner which is a much more realistic scenario on the
web, because it is highly unlikely that for each
web page, we have genre labels for all its neigh-
bouring web pages as well. Therefore, we per-
form our experiment on a very noisy dataset where
neighbouring web pages could belong to none of
our predefined genre classes. Section 4 describes
our semi-supervised graph-based classification ex-
periment, where we use a multi-class min-cut al-
gorithm as a novel technique in genre classifica-
tion.
3 Content-based Classification
3.1 Dataset and Experimental Setup
Petrenz and Webber (2010) and Sharoff et
al. (2010) emphasize that the impact of topic on
genre classification should be eliminated or con-
trolled. In order to avoid the influence of topic on
genre classification, some researchers (e.g. (Sta-
40
Number of # of pages from
Genre the same website Fleiss?s ?
web pages websites max min med
Personal Homepage (php) 304 288 9 1 1 0.858
Company/ Business Homepage (com) 264 264 1 1 1 0.713
Educational Organization Homepage (edu) 299 299 1 1 1 0.953
Personal Blog /Diary (blog) 244 215 9 1 1 0.812
Online Shop (shop) 292 209 23 1 1 0.830
Instruction/ How to (instruction) 231 142 15 1 1 0.871
Recipe 332 116 8 1 1 0.971
News 330 127 12 1 1 0.801
Editorial 310 69 11 1 3 0.877
Conversational Forum (forum) 280 106 11 1 1 0.951
Biography (bio) 242 190 15 1 1 0.905
Frequently Asked Questions (faq) 201 140 8 1 1 0.915
Review 266 179 15 1 1 0.880
Story 184 24 38 1 7 0.953
Interview 185 154 11 1 1 0.905
Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi et
al., 2014). To save space, in this paper we use the abbreviation of genre labels which are specified after
the genre names.
matatos et al., 2000) and (Argamon et al., 1998))
use only topic independent features such as com-
mon words or function words in genre classifica-
tion. However, neither of these features are exclu-
sive to genre classification. Function words and
common words are used in authorship classifica-
tion (e.g. (Argamon et al., 2007)) because they can
capture the style of the authors without being in-
fluenced by the topics of the texts. On the other
hand, word unigrams are a popular document rep-
resentation in topic classification. If we want these
models to capture the genre of documents without
being influenced by their topics or the style of their
authors, we must eliminate the influence of these
factors on genre classification by keeping them
constant across the genre classes in the training
data. That means all the documents in the train-
ing set should be about the same topic and written
by the same person. However, constructing such a
dataset is practically impossible for genre classes
on the web. The other more practical solution to
this problem would be to collect data from various
topics and sources in order to minimize the im-
pact of these factors on genre classification. For
that reason, we (Asheghi et al., 2014) created a
web genre annotated corpus which is reliable (with
Fleiss?s kappa (Fleiss, 1971) equal to 0.874) and
source diverse. We tried to reduce the influence
of topic, the writing style of the authors as well as
the design of the websites on genre classification
by collecting data from various sources and top-
ics. The corpus consists of 3964 web pages from
2522 different websites, distributed across 15 gen-
res (Table 1).
Moreover, we prepared two versions of the
corpus: the original text and the main text cor-
pora. First, we converted web pages to plain
text by removing HTML markup using the Krd-
Wrd tool (Steger and Stemle, 2009). This re-
sulted in the original text corpus which contains
individual web pages with all the textual elements
present on them. Moreover, in order to investigate
the influence of boilerplate parts (e.g. advertise-
ments, headers, footers, template materials, navi-
gation menus and lists of links) of the web pages
on genre classification, we removed the boilerplate
parts and extracted the main text of each web page
using the justext tool
1
. This resulted in the cre-
ation of the main text corpus. This is the first time
that the performance of genre classification mod-
els is compared on both the original and the main
text of the web pages.
Since the outputs of the justext tool for 518 of
the web pages were empty files, the main text cor-
pus has fewer pages. However, the main text cor-
pus still has a balanced distribution with a rela-
tively large number of web pages per category. Ta-
ble 2 compares the number of web pages in the two
versions of the corpus. For all the experiments we
use this corpus via 10-fold cross-validation on the
web pages. Also, in order to minimize the effect
of factors such as topic, the writing style of the au-
thors and the design of the websites even further,
we ensured that all the web pages from the same
website are in the same fold. Many, if not all of the
previous studies in automatic genre classification
on the web ignored this essential step when divid-
ing the data into folds. For machine learning, we
1
http://code.google.com/p/justext/
41
Number of web pages in corpora
Genre Original text Main text
php 304 221
com 264 190
edu 299 191
blog 244 242
shop 292 221
instruction 231 229
recipe 332 243
news 330 320
editorial 310 307
forum 280 251
bio 242 242
faq 201 160
review 266 262
story 184 184
interview 185 183
Table 2: Number of web pages in individual genre
classes in both original text and main text corpora.
chose Support Vector Machines (SVM) because
it has been shown by other researchers in AGI
(e.g. (Santini, 2007)) that SVM produces better or
at least similar results compared to other machine
learning algorithms. We used the one-versus-one
multi-class SVM implemented in Weka
2
with the
default setting. All the experiments are carried out
on both the original text and the main text corpora.
3.2 Features
In order to compare the performance of differ-
ent lexical and structural features used in previous
work, we reimplemented the following published
approaches to AGI: function words (Argamon et
al., 1998), part-of-speech n-grams (Santini, 2007),
word unigrams (Freund et al., 2006) and charac-
ter 4-grams binary representation (Sharoff et al.,
2010). We also explored the discriminative power
of other features such as readability features (Pitler
and Nenkova, 2008), HTML tags
3
and named en-
tity tags in genre classification (Table 3). This is
the first time that some of these features such as
average depth of syntax trees and entity coherence
features (Barzilay and Lapata, 2008) are used for
genre classification. To set a base-line, we used
a list of genre names (e.g. news, editorial, in-
terview, review) as features. We used two differ-
ent feature representations: binary and normalized
frequency. In the binary representation of a doc-
ument, the value for each feature is either one or
zero which represents the presence or the absence
of each feature respectively. In the normalized fre-
2
http://www.cs.waikato.ac.nz/ml/weka/
3
http://www.w3schools.com/tags/ref byfunc.asp
quency representation of a document, the value for
each feature is the frequency of that feature which
is normalized by the length of the document.
For extracting lexical features, we tokenized
each document using the Stanford tokenizer (in-
cluded as part of the Stanford part of speech tag-
ger (Toutanova et al., 2003)) and converted all the
tokens to lower case. For extracting POS tags
and named entity tags, we used the Stanford max-
imum entropy tagger
4
and the Stanford Named
Entity Recognizer
5
respectively. For extracting
some of the readability features such as average
parse tree height and average number of noun and
verb phrases per sentences, we used the Stanford
Parser (Klein and Manning, 2003). However, web
pages must be cleaned before they can be fed to
a parser, because parsers cannot handle tables and
list of links. Therefore, we only used the main
text of each web page as an input to the parser.
For web pages for which the justext tool produced
empty files, we treated these features as missing
values. Moreover, we used the Brown Coherence
Toolkit
6
to construct the entity grid for each web
page and computed the probability of each entity
transition type. This tool needs the parsed version
of the text as an input. Therefore, for web pages
for which the justext tool produced empty files, we
also treated these features as missing values.
3.3 Results and Discussion
Table 4 shows the result of the different feature
sets listed in the previous section on both the orig-
inal text and the main text corpora. At first glance,
we see that the results of genre classification on
the original text corpus are higher than the main
text corpus. This shows that boiler plates contain
valuable information which helps genre classifica-
tion.
Moreover, the results show that binary repre-
sentation of word unigrams is the best performing
feature set when we use the whole text of the web
pages. However, on the main text corpus, charac-
ter 4-grams outperform other features. This con-
firms the results reported in (Sharoff et al., 2010).
The results also highlight that the performance of
POS-based features are much less accurate than
that of textual features such as word unigrams and
character n-grams. The results also show that the
combination of word unigrams, text statistics and
4
http://nlp.stanford.edu/software/tagger.shtml
5
http://nlp.stanford.edu/software/CRF-NER.shtml
6
http://www.cs.brown.edu/ melsner/manual.html
42
Category Features
Token features number of tokens and number of types
normalized frequency of punctuation marks and currency characters
Named entity tags normalized frequency of tags: time, location, organization, person, money, date
average parse tree height
average sentence length and word length
Readability features standard deviation of sentence length and of word length
average number of syllables per word
type/token ratio
average number of noun phrases and verb phrases per sentence
entity coherence features (Barzilay and Lapata, 2008)
HTML tags normalized frequency of tags for: sections / style, formatting, programming,
visual features such as forms, images, lists and tables
Table 3: List of text statistics features explored in this paper
part of speech features resulted in improving genre
classification accuracy (compared to the accuracy
achieved by word unigrams alone), for both origi-
nal and main text corpora. However, while the im-
provement for the main text corpus is statistically
significant
7
, there is no significant difference be-
tween these two models for the original corpus.
Surprisingly, adding part of speech 3-grams to the
word unigrams features decreased the genre clas-
sification accuracy in both original and main text
corpora. The reason could be that the model is
over-fitted on the training data and as a result, it
performs poorly on the test data. Therefore, com-
bining various features will not always improve
the performance of the classification task. More-
over, for extracting POS-based features and some
of the text statistics features we rely on tools such
as part-of-speech taggers and parsers whose per-
formance varies for different genres. Even the best
part-of-speech taggers and parsers are error prone
and cannot be trusted on new unseen genres.
4 Graph-based Classification
Until now we extracted features only from the con-
tent of the web pages. However, other sources
of information such as the connections and the
link patterns between the web pages could be ex-
ploited to improve genre classification. The under-
lying assumption of this approach is that a page is
more likely to be connected to pages with the same
genre category. For example, if the neighbouring
web pages of a particular web page are labelled
as shop, it is more likely that this web page is a
shop too, whereas, it is highly unlikely that it is a
news or editorial. This property (i.e. entities with
similar labels are more likely to be connected) is
known as homophily (Sen et al., 2008). We hy-
7
McNemar test at the significance level of 5%
pothesis that homophily exists for genre classes
and it can help us to improve genre classifica-
tion on the web. In this paper, we use a semi-
supervised graph-based algorithm namely, multi-
class min-cut, which is a novel approach in genre
classification. This algorithm, which is a collec-
tive classification method, considers the class la-
bels of all the web pages within a graph.
4.1 Multi-class Min-cut: The Main Idea
The Min-cut classification algorithm originally
proposed by Blum and Chawla (2001) is based
on the idea that linked entities have a tendency
to belong to the same class. In other words, it
is based on the homophily assumption. There-
fore, it should be able to improve genre classifica-
tion on the web if our hypothesis holds. However,
this technique is a binary classification algorithm,
whereas, we have a multi-class problem. Unfor-
tunately, multi-class min-cut is NP-hard and there
is no exact solution for it. Nevertheless, Ganchev
and Pereira (2007) proposed a multi-class exten-
sion to Blum and Chawla (2001)?s min-cut algo-
rithm by encoding a multi-class min-cut problem
as an instance of metric labelling. Kleinberg and
Tardos (1999; 2002) introduced metric labelling
for the first time. The main idea of metric labelling
for web page classification can be described as fol-
lows:
Assume we have a weighted and undirected
graph G = (V,E) where each vertex v ? V is a
web page and the edges represent the hyper-links
between the web pages. The task is to classify
these web pages into a set of labels L. This task can
be denoted as a function f : V ? L. In order to
do this labelling task in an optimal way, we need to
minimize two different types of costs. First, there
is a non-negative cost c(v, l) for assigning label l
43
Feature set Original text Main text
genre names bin 57.39 29.02
genre name nf 38.29 14.16
function words bin 65.71 55.57
function words nf 74.95 66.86
word unigrams bin 89.32 76.61
word unigrams nf 85.21 74.91
character 4-grams bin 87.96 78.88
POS-3grams bin 73.18 61.23
POS-3grams nf 70.28 57.83
POS-2grams bin 64.10 54.91
POS-2grams nf 68.94 60.76
POS nf 60.14 54.64
text statistics 55.47 59.17
word unigrams bin + text statistics 89.48 78.09
word uni-grams bin + text statistics + POS nf 89.63 78.24
word uni-grams bin + POS 3-grams bin 88.14 75.59
Table 4: Classification accuracy of different features in genre classification. bin and nf refer to the use of
binary and normalized frequency representation of the features respectively.
to web page v. Second, if two web pages v
1
and v
2
are connected together with an edge e with weight
w
e
, we need to pay a cost of w
e
? d(f(v
1
), f(v
2
))
where d(., .) denotes distance between the two la-
bels. A big distance value between labels indicates
less similarity between them. Therefore, the total
cost of labelling task f is:
(1)
E(f) =
?
v?V
c(v, f(v)) +
?
e=(v
1
,v
2
)?E
w
e
? d(f(v
1
), f(v
2
))
Kleinberg and Tardos (1999; 2002) developed
an algorithm for minimizing E(f). However,
their algorithm uses linear programming which is
impractical for large data (Boykov et al., 2001).
In a separate study for metric labelling problems,
Boykov et al. (2001) have developed a multi-way
min-cut algorithm to minimize E(f). This algo-
rithm is very fast and can be applied to large-scale
problems with good performance (Boykov et al.,
2001).
4.2 Selection of unlabelled data
A web page w has different kind of neighbours on
the web such as parents, children, siblings, grand
parents and grand children which are mainly dif-
ferentiated based on the distance to the target web
page as well as the direction of the links (Qi and
Davison, 2009). Since the identification of chil-
dren of a web page (i.e. web pages which have
Cosine # of unlabelled Average # of
similarity web pages neighbours
? 0 103,372 40.65
? 0.1 98,824 39.08
? 0.2 87,834 34.23
? 0.3 70,602 26.46
? 0.4 50,232 17.52
? 0.5 28,437 8.62
? 0.6 13,919 3.77
? 0.7 7,241 1.86
? 0.8 3,772 0.98
? 0.9 1,732 0.44
Table 5: Number of unlabelled web pages with
different cosine similarity thresholds. The last col-
umn shows the average number of neighbours per
labelled page.
direct links from the target web page) is a straight-
forward task as their URLs can be extracted from
the HTML version of the target web page, in this
study, we explore the effect of the target web
pages? children on genre classification. Therefore,
in this experiment, by neighbouring web pages we
mean the web pages? children. In order to collect
the neighbouring web pages, for every web page in
the data set, we extracted all its out-going URLs
and downloaded them as unlabelled data. How-
ever, using all these neighbouring pages could
hurt the genre classification accuracy because web
pages are noisy (e.g. links to advertisements) and
some neighbours could have different genres than
the target page. In order to control the negative im-
pact of such neighbours, we could preselect a sub-
set of neighbours whose content are close enough
to the target page. To implement this idea, we
44
computed the cosine similarity between the web
page w and its neighbouring web pages and used
different threshold to select the neighbours. If u is
a neighbour of w and
??
u and
??
w are the represen-
tative feature vectors of these two web pages re-
spectively, we could compute the cosine similarity
between these two web pages using the following
formula:
cos(
??
w ,
??
u ) =
??
w ?
??
u
?
??
w ??
??
u ?
=
?
n
i=1
w
i
? u
i
?
?
n
i=1
(w
i
)
2
?
?
?
n
i=1
(u
i
)
2
(2)
where n is the number of the dimensions of the
vectors and w
i
is the value of the ith dimension
of the vector
??
w . Since the word unigrams bi-
nary representation model yields the best result for
content-based genre classification, we used this
representation of web pages to construct their fea-
ture vectors. Table 5 shows the number of unla-
belled data and the average number of neighbours
per labelled web page for different cosine similar-
ity thresholds.
4.3 Formulation of Semi-supervised
Multi-class Min-cuts
The formulation of semi-supervised multi-class
min-cut for genre classification involves the fol-
lowing steps:
1. We built the weighted and undirected graph
G = (V,E) where vertices are the web pages
(labelled and unlabelled) and the edges rep-
resent the hyper-links between the web pages
and set the weights to 1.
2. For training nodes, set the cost of the correct
label to zero and all other labels to a large
constant.
3. For test nodes and unlabelled nodes, we set
the cost of each label using a supervised clas-
sifier (SVM) using the following formula:
c(w, l) = 1? p
l
(w) (3)
where c(w, l) is the cost of label l for web
page w and p
l
(w) is the probability of w be-
longing to the label l which is computed by a
supervised SVM using word unigrams binary
representation of the web pages.
4. Set d(i, j), which denotes the distance be-
tween two labels i and j, to 1 if i 6= j and
zero otherwise.
5. Employ Boykov et al. (2001) algorithm to
find the minimum total cost using multiway
min-cut algorithm.
4.4 Results and Discussion
We divided the labelled data into 10 folds again
ensuring that all the web pages from the same
websites are in the same fold. We used 8 folds
for training, one fold for validation and one fold
for testing. We learnt the best cosine similar-
ity threshold using validation data and then eval-
uated it on the test data. Tables 6 and 7 illus-
trate the results of the multi-class min-cut algo-
rithm and the content-based algorithm (both using
word unigrams as features) respectively. The re-
sults show that the multi-class min-cut algorithm
significantly outperforms
8
the content-based clas-
sifier for the cosine similarity equal or greater than
0.8 which was chosen on the validation data. It
must be noted that the result of the multi-class
min-cut algorithm when we used all the neigh-
bouring pages was much lower than the content-
based algorithm due to noise. The results also
shows that some genre classes such as news, edito-
rial, blog, interview and instruction benefited more
than other genre classes from the neighbouring
web pages. Genre categories with improved re-
sults are shown in bold in Table 6. The homophily
property of these genre categories was the reason
behind this improvement. For example, the fact
that a news article is more likely to be linked to
other news articles, whereas, an editorial is more
likely to be linked to other editorials, helped us to
differentiate these two categories further. On the
other hand, we observe no improvement or even
decrease in F-measure for some genre categories
such as frequently asked questions, forums and
company home pages. Two reasons could have
contributed to these results. First, the homophily
property might not exist for these categories. Sec-
ond, the homophily property holds for these cate-
gories, however, in order to benefit from this prop-
erty, we need to examine other neighbours of the
target web pages such as parents, siblings, grand
parents, grand children or even more distant neigh-
8
McNemar test at the significance level of 5%
45
class Recall Precision F1-measure
php 0.928 0.850 0.887
forum 0.925 0.977 0.951
review 0.895 0.832 0.862
news 0.897 0.798 0.845
com 0.897 0.891 0.894
shop 0.860 0.965 0.910
instruction 0.870 0.914 0.892
recipe 0.994 0.991 0.993
blog 0.889 0.879 0.884
bio 0.905 0.948 0.926
editorial 0.800 0.932 0.861
faq 0.902 0.841 0.870
edu 0.957 0.963 0.960
story 0.902 0.943 0.922
interview 0.870 0.809 0.839
overall accuracy = 90.11%
Table 6: Recall, Precision and F-measure for multi-
class min-cut genre classification.
class Recall Precision F1-measure
php 0.938 0.798 0.862
forum 0.943 0.974 0.958
review 0.872 0.859 0.866
news 0.894 0.782 0.835
com 0.920 0.874 0.897
shop 0.849 0.950 0.897
instruction 0.866 0.889 0.877
recipe 0.988 0.988 0.988
blog 0.865 0.841 0.853
bio 0.884 0.926 0.905
editorial 0.765 0.926 0.837
faq 0.866 0.879 0.872
edu 0.950 0.969 0.959
story 0.864 0.941 0.901
interview 0.827 0.785 0.805
overall accuracy = 88.98%
9
Table 7: Recall, Precision and F-measure for content-based
genre classification using word unigrams feature set
bours.
5 Conclusions and Future work
In the first set of experiments, we compared
a diverse range of content-based features in
genre classification using a reliable and source
diverse genre-annotated corpus. The evaluation
shows that lexical features outperformed all
other features. Source diversity of the corpus
minimized the influence of topic, authorship and
web page design on genre classification. In the
second experiment, we significantly improved the
genre classification result using a semi-supervised
min-cut algorithm by employing the children of
the target web pages as unlabelled data. The
results of this method which takes advantage of
the graph structure of the web shows that some
genre classes benefit more than others from the
neighbouring web pages. The homophily property
of genre categories such as news, blogs and edi-
torial was the reason behind the improvement of
genre classification in this experiment. In future
work, we would like to examine the effect of other
types of neighbours on genre classification of
web pages and experiment with other graph-based
algorithms.
References
Shlomo Argamon, Moshe Koppel, and Galit Avneri.
1998. Routing documents according to style. In
9
Please note that in this experiment we had less training
data because we used 8 folds for training, one fold for valida-
tion and one fold for testing. As a result, the accuracy of word
unigrams is slightly lower than the result reported in Table 4.
First international workshop on innovative informa-
tion systems, pages 85?92. Citeseer.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional
lexical features. Journal of the American Society
for Information Science and Technology, 58(6):802?
822.
Noushin Rezapour Asheghi, Serge Sharoff, and Katja
Markert. 2014. Designing and evaluating a reliable
corpus of web genres via crowd-sourcing. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC?14).
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Avrim Blum and Shuchi Chawla. 2001. Learning from
labeled and unlabeled data using graph mincuts. In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 19?26. Morgan
Kaufmann Publishers Inc.
Yuri Boykov, Olga Veksler, and Ramin Zabih. 2001.
Fast approximate energy minimization via graph
cuts. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 23(11):1222?1239.
Soumen Chakrabarti, Byron Dom, and Piotr Indyk.
1998. Enhanced hypertext categorization using hy-
perlinks. In ACM SIGMOD Record, volume 27,
pages 307?318. ACM.
Robert Crane and Luke McDowell. 2012. Investigat-
ing markov logic networks for collective classifica-
tion. In ICAART (1), pages 5?15.
M CRAVEN. 1998. Learning to extract symbolic
knowledge from the world wide web. In Proc. of the
15th National Conference on Artificial Intelligence
(AAAI-98).
46
Aidan Finn and Nicholas Kushmerick. 2006. Learning
to classify documents according to genre. Journal
of the American Society for Information Science and
Technology, 57(11):1506?1518.
J.L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378.
L. Freund, C.L.A. Clarke, and E.G. Toms. 2006. To-
wards genre classification for ir in the workplace.
In Proceedings of the 1st international conference
on Information interaction in context, pages 30?36.
ACM.
Kuzman Ganchev and Fernando Pereira. 2007. Trans-
ductive structured classification through constrained
min-cuts. TextGraphs-2: Graph-Based Algorithms
for Natural Language Processing, page 37.
Ioannis Kanaris and Efstathios Stamatatos. 2007.
Webpage genre identification using variable-length
character n-grams. In Tools with Artificial Intelli-
gence, 2007. ICTAI 2007. 19th IEEE International
Conference on, volume 2, pages 3?10. IEEE.
J. Karlgren and D. Cutting. 1994. Recognizing text
genres with simple metrics using discriminant anal-
ysis. In Proceedings of the 15th conference on Com-
putational linguistics-Volume 2, pages 1071?1075.
B. Kessler, G. Numberg, and H. Schutze. 1997. Au-
tomatic detection of text genre. In Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 32?38.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Jon Kleinberg and Eva Tardos. 1999. Approximation
algorithms for classification problems with pairwise
relationships: Metric labeling and markov random
fields. In focs, page 14. Published by the IEEE Com-
puter Society.
Jon Kleinberg and Eva Tardos. 2002. Approximation
algorithms for classification problems with pairwise
relationships: Metric labeling and markov random
fields. Journal of the ACM (JACM), 49(5):616?639.
Frank Lin and William W Cohen. 2010. Semi-
supervised classification of network data using very
few labels. In Advances in Social Networks Analysis
and Mining (ASONAM), 2010 International Confer-
ence on, pages 192?199. IEEE.
Q. Lu and L. Getoor. 2003. Link-based classification
using labeled and unlabeled data. The Continuum
from Labeled to Unlabeled Data in Machine Learn-
ing & Data Mining, page 88.
Sofus A Macskassy and Foster Provost. 2007. Classifi-
cation in networked data: A toolkit and a univariate
case study. The Journal of Machine Learning Re-
search, 8:935?983.
P. Petrenz and B. Webber. 2010. Stable classification
of text genres. Computational Linguistics, (Early
Access):1?9.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 186?195.
Xiaoguang Qi and Brian D Davison. 2009. Web page
classification: Features and algorithms. ACM Com-
puting Surveys (CSUR), 41(2):12.
Marina Santini. 2007. Automatic identification of
genre in web pages. Ph.D. thesis, University of
Brighton.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise
Getoor, Brian Galligher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
magazine, 29(3):93.
S. Sharoff, Z. Wu, and K. Markert. 2010. The web li-
brary of babel: evaluating genre collections. In Pro-
ceedings of the Seventh Conference on International
Language Resources and Evaluation, pages 3063?
3070.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2000. Text genre detection using com-
mon word frequencies. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 808?814.
Johannes M. Steger and Egon W. Stemle. 2009. Krd-
Wrd ? architecture for unified processing of web
content.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180.
47
