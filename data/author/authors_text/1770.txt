Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 755?763, Prague, June 2007. c?2007 Association for Computational Linguistics
What Can Syntax-based MT Learn from Phrase-based MT?
Steve DeNeefe and Kevin Knight
Information Sciences Institute
The Viterbi School of Engineering
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{sdeneefe,knight}@isi.edu
Wei Wang and Daniel Marcu
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
{wwang,dmarcu}@languageweaver.com
Abstract
We compare and contrast the strengths
and weaknesses of a syntax-based machine
translation model with a phrase-based ma-
chine translation model on several levels.
We briefly describe each model, highlight-
ing points where they differ. We include a
quantitative comparison of the phrase pairs
that each model has to work with, as well
as the reasons why some phrase pairs are
not learned by the syntax-based model. We
then evaluate proposed improvements to the
syntax-based extraction techniques in light
of phrase pairs captured. We also compare
the translation accuracy for all variations.
1 Introduction
String models are popular in statistical machine
translation. Approaches include word substitution
systems (Brown et al, 1993), phrase substitution
systems (Koehn et al, 2003; Och and Ney, 2004),
and synchronous context-free grammar systems (Wu
and Wong, 1998; Chiang, 2005), all of which train
on string pairs and seek to establish connections be-
tween source and target strings. By contrast, ex-
plicit syntax approaches seek to directly model the
relations learned from parsed data, including models
between source trees and target trees (Gildea, 2003;
Eisner, 2003; Melamed, 2004; Cowan et al, 2006),
source trees and target strings (Quirk et al, 2005;
Huang et al, 2006), or source strings and target trees
(Yamada and Knight, 2001; Galley et al, 2004).
It is unclear which of these important pursuits will
best explain human translation data, as each has ad-
vantages and disadvantages. A strength of phrase
models is that they can acquire all phrase pairs con-
sistent with computed word alignments, snap those
phrases together easily by concatenation, and re-
order them under several cost models. An advan-
tage of syntax-based models is that outputs tend to
be syntactically well-formed, with re-ordering influ-
enced by syntactic context and function words intro-
duced to serve specific syntactic purposes.
A great number of MT models have been re-
cently proposed, and other papers have gone over the
expressive advantages of syntax-based approaches.
But it is rare to see an in-depth, quantitative study
of strengths and weaknesses of particular models
with respect to each other. This is important for a
scientific understanding of how these models work
in practice. Our main novel contribution is a com-
parison of phrase-based and syntax-based extraction
methods and phrase pair coverage. We also add to
the literature a new method of improving that cover-
age. Additionally, we do a careful study of several
syntax-based extraction techniques, testing whether
(and how much) they affect phrase pair coverage,
and whether (and how much) they affect end-to-end
MT accuracy. The MT accuracy tests are needed
because we want to see the individual effects of par-
ticular techniques under the same testing conditions.
For this comparison, we choose a previously estab-
lished statistical phrase-based model (Och and Ney,
2004) and a previously established statistical string-
to-tree model (Galley et al, 2004). These two mod-
els are chosen because they are the basis of two of
the most successful systems in the NIST 2006 MT
755
evaluation1.
2 Phrase-based Extraction
The Alignment Template system (ATS) described by
Och and Ney (2004) is representative of statistical
phrase-based models. The basic unit of translation
is the phrase pair, which consists of a sequence of
words in the source language, a sequence of words
in the target language, and a vector of feature val-
ues which describe this pair?s likelihood. Decod-
ing produces a string in the target language, in or-
der, from beginning to end. During decoding, fea-
tures from each phrase pair are combined with other
features (e.g., re-ordering, language models) using a
log-linear model to compute the score of the entire
translation.
The ATS phrase extraction algorithm learns these
phrase pairs from an aligned, parallel corpus.
This corpus is conceptually a list of tuples of
<source sentence, target sentence, bi-directional
word alignments> which serve as training exam-
ples, one of which is shown in Figure 1.
Figure 1: a phrase-based training example
For each training example, the algorithm identi-
fies and extracts all pairs of <source sequence, tar-
get sequence> that are consistent with the align-
ments. It does this by first enumerating all source-
side word sequences up to a length limit L, and for
each source sequence, it identifies all target words
aligned to those source words. For example, in Fig-
ure 1, for the source phrase ? 	  , the target
words it aligns to are felt, obliged, and do.
These words, and all those between them, are the
proposed target phrase. If no words in the proposed
target phrase align to words outside of the source
phrase, then this phrase pair is extracted.
The extraction algorithm can also look to the left
and right of the proposed target phrase for neighbor-
ing unaligned words and extracts phrases. For ex-
ample, for the phrase pair ? 	  ? felt obliged,
1http://www.nist.gov/speech/tests/mt/
mt06eval official results.html
the word to is a neighboring unaligned word. It
constructs new target phrases by adding on con-
secutive unaligned words in both directions, and
extracts those in new pairs, too (e.g., ? 	  ?
felt obliged to). For efficiency reasons, imple-
mentations often skip this step.
Figure 2 shows the complete set of phrase pairs
up to length 4 that are extracted from the Figure 1
training example. Notice that no extracted phrase
pair contains the character ?. Because of the align-
ments, the smallest legal phrase pair, ? ? 	  
? i felt obliged to do my, is beyond the size
limit of 4, so it is not extracted in this example.
? ? felt
? 	  ? felt obliged
? 	   ? felt obliged to do
	  ? obliged
	   ? obliged to do
 ? do
 P ? part
 P ? ? part
 P ? . ? part .
? . ? .
. ? .
Figure 2: phrases up to length 4 extracted from the
example in Figure 1
Phrase pairs are extracted over the entire train-
ing corpus. Due to differing alignments, some
phrase pairs that cannot be learned from one exam-
ple may be learned from another. These pairs are
then counted, once for each time they are seen in a
training example, and these counts are used as the
basis for maximum likelihood probability features,
such as p(f |e) and p(e|f).
3 Syntax-based Extraction
The GHKM syntax-based extraction method for
learning statistical syntax-based translation rules,
presented first in (Galley et al, 2004) and expanded
on in (Galley et al, 2006), is similar to phrase-based
extraction in that it extracts rules consistent with
given word alignments. A primary difference is the
use of syntax trees on the target side, rather than se-
quences of words. The basic unit of translation is the
translation rule, consisting of a sequence of words
756
and variables in the source language, a syntax tree
in the target language having words or variables at
the leaves, and again a vector of feature values which
describe this pair?s likelihood. Translation rules can:
? look like phrase pairs with syntax decoration:
NPB(NNP(prime)
NNP(minister)
NNP(keizo)
NNP(obuchi))
? B ? ? ? D #
? carry extra contextual constraints:
VP(VBD(said)
x0:SBAR-C)
? ? x0
(according to this rule, ? can translate to
said only if some Chinese sequence to the
right of ? is translated into an SBAR-C)
? be non-constituent phrases:
VP(VBD(said)
SBAR-C(IN(that)
x0:S-C))
? ? x0
VP(VBD(pointed)
PRT(RP(out))
x0:SBAR-C)
? ? ? x0
? contain non-contiguous phrases, effectively
?phrases with holes?:
PP(IN(on)
NP-C(NPB(DT(the)
x0:NNP))
NN(issue))))
? ? x0 ?  ?
PP(IN(on)
NP-C(NPB(DT(the)
NN(issue))
x0:PP))
? ? x0 ?  ?
? be purely structural (no words):
S(x0:NP-C x1:VP)? x0 x1
? re-order their children:
NP-C(NPB(DT(the)
x0:NN)
PP(IN(of)
x1:NP-C))
? x1 { x0
Decoding with this model produces a tree in the
target language, bottom-up, by parsing the foreign
string using a CYK parser and a binarized rule set
(Zhang et al, 2006). During decoding, features from
each translation rule are combined with a language
model using a log-linear model to compute the score
of the entire translation.
The GHKM extractor learns translation rules from
an aligned parallel corpus where the target side has
been parsed. This corpus is conceptually a list of tu-
ples of <source sentence, target tree, bi-directional
word alignments> which serve as training exam-
ples, one of which is shown in Figure 3.
Figure 3: a syntax-based training example
For each training example, the GHKM extrac-
tor computes the set of minimally-sized translation
rules that can explain the training example while re-
maining consistent with the alignments. This is, in
effect, a non-overlapping tiling of translation rules
over the tree-string pair. If there are no unaligned
words in the source sentence, this is a unique set.
This set, ordered into a tree of rule applications, is
called the derivation tree of the training example.
Unlike the ATS model, there are no inherent size
limits, just the constraint that the rules be as small
as possible for the example.
Ignoring the unaligned ? for the moment, there
are seven minimal translation rules that are extracted
from the example in Figure 3, as shown in Fig-
ure 4. Notice that rule 6 is rather large and applies
to a very limited syntactic context. The only con-
stituent node that covers both i and my is the S,
so the rule rooted at S is extracted, with variables
for every branch below this top constituent that can
be explained by other rules. Note also that to be-
757
comes a part of this rule naturally. If the alignments
were not as constraining (e.g., if my was unaligned),
then instead of this one big rule many smaller rules
would be extracted, such as structural rules (e.g.,
VP(x0:VBD x1:VP-C)? x0 x1) and function word in-
sertion rules (e.g., VP(TO(to) x0:VP-C)? x0).
1. VBD(felt)? ?
2. VBN(obliged)? 	 
3. VB(do)? 
4. NN(part)?  P
5. PERIOD(.)? .
6. S(NP-C(NPB(PRP(I)))
VP(x0:VBD
VP-C(x1:VBN
SG-C(VP(TO(to)
VP-C(x2:VB
NP-C(NPB(PRP$(my)
x3:NN)))))))
x4:PERIOD)? ? x0 x1 x2 x3 x4
7. TOP(x0:S)? x0
Figure 4: rules extracted from training example
We ignored unaligned source words in the exam-
ple above. Galley et al (2004) attach the unaligned
source word to the highest possible location, in our
example, the S. Thus it is extracted along with our
large rule 6, changing the target language sequence
to ?? x0 x1 x2 x3 ? x4?. This treatment still re-
sults in a unique derivation tree no matter how many
unaligned words are present.
In Galley et al (2006), instead of a unique deriva-
tion tree, the extractor computes several derivation
trees, each with the unaligned word added to a dif-
ferent rule such that the data is still explained. For
example, for the tree-string pair in Figure 3, ?
could be added not only to rule 6, but alternatively
to rule 4 or 5, to make the new rules:
NN(part)?  P ?
PERIOD(.)? ? .
This results in three different derivations, one
with the ? character in rule 4 (with rules 5 and 6
as originally shown), another with the ? character
in rule 5 (with rules 4 and 6 as originally shown),
and lastly one with the ? character in rule 6 (with
rules 4 and 5 as originally shown) as in the origi-
nal paper (Galley et al, 2004). In total, ten different
rules are extracted from this training example.
As with ATS, translation rules are extracted and
counted over the entire training corpus, a count of
one for each time they appear in a training example.
These counts are used to estimate several features,
including maximum likelihood probability features
for p(etree, fwords|ehead), p(ewords|fwords), and
p(fwords|ewords).
4 Differences in Phrasal Coverage
Both the ATS model and the GHKM model extract
linguistic knowledge from parallel corpora, but each
has fundamentally different constraints and assump-
tions. To compare the models empirically, we ex-
tracted phrase pairs (for the ATS model) and transla-
tion rules (for the GHKM model) from parallel train-
ing corpora described in Table 1. The ATS model
was limited to phrases of length 10 on the source
side, and length 20 on the target side. A super-
set of the parallel data was word aligned by GIZA
union (Och and Ney, 2003) and EMD (Fraser and
Marcu, 2006). The English side of training data was
parsed using an implementation of Collins? model 2
(Collins, 2003).
Chinese Arabic
Document IDs LDC2003E07 LDC2004T17
LDC2003E14 LDC2004T18
LDC2005T06 LDC2005E46
# of segments 329,031 140,511
# of words in foreign corpus 7,520,779 3,147,420
# of words in English corpus 9,864,294 4,067,454
Table 1: parallel corpora used to train both models
Table 2 shows the total number of GHKM rules
extracted, and a breakdown of the different kinds
of rules. Non-lexical rules are those whose source
side is composed entirely of variables ? there are
no source words in them. Because of this, they
potentially apply to any sentence. Lexical rules
(their counterpart) far outnumber non-lexical rules.
Of the lexical rules, a rule is considered a phrasal
rule if its source side and the yield of its target
side contain exactly one contiguous phrase each, op-
tionally with one or more variables on either side
of the phrase. Non-phrasal rules include structural
rules, re-ordering rules, and non-contiguous phrases.
These rules are not easy to directly compare to any
phrase pairs from the ATS model, so we do not focus
on them here.
Phrasal rules can be directly compared to ATS
phrase pairs, the easiest way being to discard the
758
Statistic Chinese Arabic
total translation rules 2,487,110 662,037
non-lexical rules 110,066 15,812
lexical rules 2,377,044 646,225
phrasal rules 1,069,233 406,020
distinct GHKM-derived phrase pairs 919,234 352,783
distinct corpus-specific
GHKM-derived phrase pairs 203,809 75,807
Table 2: a breakdown of how many rules the
GHKM extraction algorithm produces, and how
many phrase pairs can be derived from them
syntactic context and look at the phrases contained
in the rules. The second to last line of Table 2 shows
the number of phrase pairs that can be derived from
the above phrasal rules. The number of GHKM-
derived phrase pairs is lower than the number of
phrasal rules because some rules represent the same
phrasal translation, but with different syntactic con-
texts. The last line of Table 2 shows the subset of
phrase pairs that contain source phrases found in our
development corpus.
Table 3 compares these corpus-specific GHKM-
derived phrase pairs with the corpus-specific ATS
phrase pairs. Note that the number of phrase pairs
derived from the GHKM rules is less than the num-
ber of phrase pairs extracted by ATS. Moreover, only
slightly over half of the phrase pairs extracted by the
ATS model are common to both models. The lim-
its and constraints of each model are responsible for
this difference in contiguous phrases learned.
Source of phrase pairs Chinese Arabic
GHKM-derived 203,809 75,807
ATS 295,537 133,576
Overlap between models 160,901 75,038
GHKM only 42,908 769
ATS only 134,636 58,538
ATS-useful only 1,994 2,199
Table 3: comparison of corpus-specific phrase pairs
from each model
GHKM learns some contiguous phrase pairs that
the phrase-based extractor does not. Only a small
portion of these are due to the fact that the GHKM
model has no inherent size limit, while the phrase
based system has limits. More numerous are cases
where unaligned English words are not added to an
ATS phrase pair while GHKM adopts them at a syn-
tactically motivated location, or where a larger rule
contains mostly syntactic structure but happens to
have some unaligned words in it. For example, con-
sider Figure 5. Because basic and will are un-
aligned, ATS will learn no phrase pairs that translate
to these words alone, though they will be learned as
a part of larger phrases.
Figure 5: Situation where GHKM is able to learn
rules that translate into basic and will, but ATS
is not
GHKM, however, will learn several phrasal rules
that translate to basic, based on the syntactic con-
text
NPB(x0:DT
JJ(basic)
x1:NN)
? x0  x1
NPB(x0:DT
JJ(basic)
x1:NN)
? x0  ? ? x1
NPB(x0:DT
JJ(basic)
x1:NN)
? x0 ? ? x1
and one phrasal rule that translates into will
VP(MD(will)
x0:RB
x1:VP-C)
? x0 ? ? x1
The quality of such phrases may vary. For example,
the first translation of  (literally: ?one? or ?a?) to
basic above is a phrase pair of poor quality, while
the other two for basic and one for will are ar-
guably reasonable.
However, Table 3 shows that ATS was able to
learn many more phrase pairs that GHKM was not.
Even more significant is the subset of these missing
phrase pairs that the ATS decoder used in its best2
2i.e. highest scoring
759
translation of the corpus. According to the phrase-
based system these are the most ?useful? phrase
pairs and GHKM could not learn them. Since this is
a clear deficiency, we will focus on analyzing these
phrase pairs (which we call ATS-useful) and the rea-
sons they were not learned.
Table 4 shows a breakdown, categorizing each of
these missing ATS-useful phrase pairs and the rea-
sons they were not able to be learned. The most
common reason is straightforward: by extracting
only the minimally-sized rules, GHKM is unable to
learn many larger phrases that ATS learns. If GHKM
can make a word-level analysis, it will do that, at
the expense of a phrase-level analysis. Galley et
al. (2006) propose one solution to this problem and
Marcu et al (2006) propose another, both of which
we explore in Sections 5.1 and 5.2.
Category of missing ATS-useful phrase pairs Chinese Arabic
Not minimal 1,320 1,366
Extra target words in GHKM rules 220 27
Extra source words in GHKM rules 446 799
Other (e.g. parse failures) 8 7
Total missing useful phrase pairs 1,994 2,199
Table 4: reasons that ATS-useful phrase pairs could
not be extracted by GHKM as phrasal rules
The second reason is that the GHKM model is
sometimes forced by its syntactic constraints to in-
clude extra words. Sometimes this is only target lan-
guage words, and this is often useful ? the rules are
learning to insert these words in their proper context.
But most of the time, source language words are also
forced to be part of the rule, and this is harmful ? it
makes the rules less general. This latter case is often
due to poorly aligned target language words (such as
the ? in our Section 3 rule extraction example), or
unaligned words under large, flat constituents.
Another factor here: some of the phrase pairs are
learned by both systems, but GHKM is more specific
about the context of use. This can be both a strength
and a weakness. It is a strength when the syntactic
context helps the phrase to be used in a syntactically
correct way, as in
VP(VBD(said)
x0:SBAR-C)
? ? x0
where the syntax rule requires a constituent of type
SBAR-C. Conversely its weakness is seen when the
context is too constrained. For example, ATS can
easily learn the phrase
 ? ? prime minister
and is then free to use it in many contexts. But
GHKM learns 45 different rules, each that translate
this phrase pair in a unique context. Figure 6 shows
a sampling. Notice that though many variations are
present, the decoder is unable to use any of these
rules to produce certain noun phrases, such as ?cur-
rent Japanese Prime Minister Shinzo Abe?, because
no rule has the proper number of English modifiers.
NPB(NNP(prime) NNP(minister) x0:NNP)? x0  ?
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(x0:JJ NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(NNP(prime) NNP(minister) x0:NNP)?  ? x0
NPB(NNP(prime) NNP(minister))?  ?
NPB(NNP(prime) NNP(minister) x0:NNP x1:NNP)? x0 x1  ?
NPB(x0:DT x1:JJ JJ(prime) NN(minister))? x0 x1  ?
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
Figure 6: a sampling of the 45 rules that translate
 ? to prime minister
5 Coverage Improvements
Each of the models presented so far has advantages
and disadvantages. In this section, we consider ideas
that make up for deficiencies in the GHKM model,
drawing our inspiration from the strong points of the
ATS model. We then measure the effects of each
idea empirically, showing both what is gained and
the potential limits of each modification.
5.1 Composed Rules
Galley et al (2006) proposed the idea of composed
rules. This removes the minimality constraint re-
quired earlier: any two or more rules in a parent-
child relationship in the derivation tree can be com-
bined to form a larger, composed rule. This change
is similar in spirit to the move from word-based to
phrase-based MT models, or parsing with a DOP
model (Bod et al, 2003) rather than a plain PCFG.
Because this results in exponential variations, a
size limit is employed: for any two or more rules
to be allowed to combine, the size of the resulting
rule must be at most n. The size of a rule is de-
fined as the number of non-part-of-speech, non-leaf
760
constituent labels in a rule?s target tree. For exam-
ple, rules 1-5 shown in Section 3 have a size of 0,
and rule 6 has a size of 10. Composed rules are ex-
tracted in addition to minimal rules, which means
that a larger n limit always results in a superset of
the rules extracted when a smaller n value is used.
When n is set to 0, then only minimal rules are ex-
tracted. Table 5 shows the growth in the number of
rules extracted for several size limits.
Size limit (n) Chinese Arabic
0 (minimal) 2,487,110 662,037
2 12,351,297 2,742,513
3 26,917,088 4,824,928
4 55,781,061 8,487,656
Table 5: increasing the size limit of composed rules
significantly increases the number of rules extracted
In our previous analysis, the main reason that
GHKM did not learn translations for ATS-useful
phrase pairs was due to its minimal-only approach.
Table 6 shows the effect that composed rule extrac-
tion has on the total number of ATS-useful phrases
missing. Note that as the allowed size of composed
rule increases, we are able to extract an greater per-
centage of the missing ATS-useful phrase pairs.
Size limit (n) Chinese Arabic
0 (minimal) 1,994 2,199
2 1,478 1,528
3 1,096 1,210
4 900 1,041
Table 6: number of ATS-useful phrases still missing
when using GHKM composed rule extraction
Unfortunately, a comparison of Tables 5 and 6 in-
dicates that the number of ATS-useful phrase pairs
gained is growing at a much slower rate than the total
number of rules. From a practical standpoint, more
rules means more processing work and longer de-
coding times, so there are diminishing returns from
continuing to explore larger size limits.
5.2 SPMT Model 1 Rules
An alternative for extracting larger rules called
SPMT model 1 is presented by Marcu et al (2006).
Though originally presented as a separate model,
the method of rule extraction itself builds upon the
minimal GHKM method just as composed rules do.
For each training example, the method considers all
source language phrases up to length L. For each of
these phrases, it extracts the smallest possible syn-
tax rule that does not violate the alignments. Ta-
ble 7 shows that this method is able to extract rules
that cover useful phrases, and can be combined with
size 4 composed rules to an even better effect. Since
there is some overlap in these methods, when com-
bining the two methods we eliminate any redundant
rules.
Method Chinese Arabic
composed alone (size 4) 900 1,041
SPMT model 1 alone 676 854
composed + SPMT model 1 663 835
Table 7: ATS-useful phrases still missing after dif-
ferent non-minimal methods are applied
Note that having more phrasal rules is not the only
advantage of composed rules. Here, combining both
composed and SPMT model 1 rules, our gain in use-
ful phrases is not very large, but we do gain addi-
tional, larger syntax rules. As discussed in (Galley
et al, 2006), composed rules also allow the learning
of more context, such as
ADJP(ADVP(RB(far)
CC(and)
RB(away)
x0:JJ)
? ? ? x0
This rule is not learned by SPMT model 1 because
it is not the smallest rule that can explain the phrase
pair, but it is still valuable for its syntactic context.
5.3 Restructuring Trees
Table 8 updates the causes of missing ATS-useful
phrase pairs. Most are now caused by syntactic con-
straints, thus we need to address these in some way.
GHKM translation rules are affected by large,
flat constituents in syntax trees, as in the prime
minister example earlier. One way to soften this
constraint is to binarize the trees, so that wide con-
stituents are broken down into multiple levels of tree
structure. The approach we take here is head-out bi-
narization (Wang et al, 2007), where any constituent
with more than two children is split into partial con-
stituents. The children to the left of the head word
761
Category of ATS-useful phrase pairs Chinese Arabic
Too large 12 9
Extra target words in GHKM rules 218 27
Extra source words in GHKM rules 424 792
Other (e.g. parse failures) 9 7
Total missing useful phrase pairs 663 835
Table 8: reasons that ATS-useful phrase pairs are
still not extracted as phrasal rules, with composed
and SPMT model 1 rules in place
are binarized one direction, while the children to
the right are binarized the other direction. The top
node retains its original label (e.g. NPB), while the
new partial constituents are labeled with a bar (e.g.
NPB). Figure 7 shows an example.
Figure 7: head-out binarization in the target lan-
guage: S, NPB, and VP are binarized according to
the head word
Table 9 shows the effect of binarization on phrasal
coverage, using both composed and SPMT rules. By
eliminating some of the syntactic constraints we al-
low more freedom, which allows increased phrasal
coverage, but generates more rules.
Category of missing ATS-useful phrase pairs Chinese Arabic
Too large 16 12
Extra target words in GHKM rules 123 12
Extra source words in GHKM rules 307 591
Other (e.g. parse failures) 12 7
Total missing useful phrase pairs 458 622
Table 9: reasons that ATS-useful phrase pairs still
could not be extracted as phrasal rules after bina-
rization
6 Evaluation of Translations
To evaluate translation quality of each of these mod-
els and methods, we ran the ATS decoder using its
extracted phrase pairs and the syntax-based decoder
using all the rule sets mentioned above. Table 10 de-
scribes the development and test datasets used, along
with four references for measuring BLEU. Tun-
ing was done using Maximum BLEU hill-climbing
(Och, 2003). Features used for the ATS system were
the standard set. For the syntax-based translation
system, we used a similar set of features.
# of lines
Dataset Chinese Arabic
Development set NIST 2002 MT eval 925 696
(sentences < 47 tokens)
Test set NIST 2003 MT eval 919 663
Table 10: development and test corpora
Table 11 shows the case-insensitive NIST BLEU4
scores for both our development and test decod-
ings. The BLEU scores indicate, first of all, that
the syntax-based system is much stronger in trans-
lating Chinese than Arabic, in comparison to the
phrase-based system. Also, the ideas presented here
for improving phrasal coverage generally improve
the syntax-based translation quality. In addition,
composed rules are shown to be helpful as com-
pared to the minimal runs. This is true even when
SPMT model 1 is added, which indicates that the
size 4 composed rules bring more than just improved
phrasal coverage.
Chinese Arabic
Experiment Dev Test Dev Test
Baseline ATS 34.94 32.83 50.46 50.52
Baseline GHKM (minimal only) 38.02 37.67 49.34 49.99
GHKM composed size 2 40.24 39.75 50.76 50.94
GHKM composed size 3 40.95 40.44 51.56 51.48
GHKM composed size 4 41.36 40.69 51.60 51.71
GHKM minimal + SPMT model 1 39.78 39.16 50.17 51.27
GHKM composed + SPMT model 1 42.04 41.07 51.73 51.53
With binarization 42.17 41.26 52.50 51.79
Table 11: evaluation results (reported in case-
insensitive NIST BLEU4)
7 Conclusions
Both the ATS model for phrase-based machine
translation and the GHKM model for syntax-based
machine translation are state-of-the-art methods.
Each extraction method has strengths and weak-
nesses as compared to the other, and there are sur-
prising differences in phrasal coverage ? neither is
merely a superset of the other. We have shown that
it is possible to gain insights from the strengths of
the phrase-based extraction model to increase both
762
the phrasal coverage and translation accuracy of the
syntax-based model.
However, there is still room for improvement in
both models. For syntax models, there are still holes
in phrasal coverage, and other areas are needing
progress, such as decoding efficiency. For phrase-
based models, incorporating syntactic knowledge
and constraints may lead to improvements as well.
8 Acknowledgments
The authors wish to acknowledge our colleagues at
ISI, especially David Chiang, for constructive criti-
cism on an early draft of this document, and several
reviewers for their detailed comments which helped
us make the paper stronger. We are also grateful to
Jens-So?nke Vo?ckler for his assistance in setting up
an experimental pipeline, without which this work
would have been much more tedious and difficult.
This research was supported under DARPA Contract
No. HR0011-06-C-0022.
References
Rens Bod, Remko Scha, and Khalil Sima?an, editors. 2003.
Data-Oriented Parsing. CSLI Publications, University of
Chicago Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL 2005.
Michael Collins. 2003. Head-driven statistical models for nat-
ural language parsing. Computational Linguistics, 29(4).
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins. 2006.
A discriminative model for tree-to-tree translation. In Proc.
EMNLP 2006.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. ACL 2003.
Alexander Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In Proc. ACL 2006.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc. HLT-
NAACL 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proc. ACL 2006.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. ACL 2003, companion volume.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Sta-
tistical syntax-directed translation with extended domain of
locality. In Proc. AMTA 2006.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. HLT-NAACL 2003.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proc. EMNLP 2006.
I. Dan Melamed. 2004. Statistical machine translation by pars-
ing. In Proc. ACL 2004.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1).
Franz Josef Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics, 30.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. ACL 2003.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proc. ACL 2005.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing
syntax trees to improve syntax-based machine translation ac-
curacy. In Proc. EMNLP and CoNLL 2007.
Dekai Wu and Hongsing Wong. 1998. Machine translation
with a stochastic grammatical channel. In Proc. ACL 1998.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proc. ACL 2001.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proc. NAACL HLT 2006.
763
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610?619,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Decomposability of Translation Metrics
for Improved Evaluation and Efficient Algorithms
David Chiang and Steve DeNeefe
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
{chiang,sdeneefe}@isi.edu
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
Law Link
Singapore 117590
{chanys,nght}@comp.nus.edu.sg
Abstract
B??? is the de facto standard for evaluation
and development of statistical machine trans-
lation systems. We describe three real-world
situations involving comparisons between dif-
ferent versions of the same systems where one
can obtain improvements in B??? scores that
are questionable or even absurd. These situ-
ations arise because B??? lacks the property
of decomposability, a property which is also
computationally convenient for various appli-
cations. We propose a very conservative modi-
fication to B??? and a cross between B??? and
word error rate that address these issues while
improving correlation with human judgments.
1 Introduction
B??? (Papineni et al, 2002) was one of the first au-
tomatic evaluation metrics for machine translation
(MT), and despite being challenged by a number
of alternative metrics (Melamed et al, 2003; Baner-
jee and Lavie, 2005; Snover et al, 2006; Chan and
Ng, 2008), it remains the standard in the statistical
MT literature. Callison-Burch et al (2006) have sub-
jected B??? to a searching criticism, with two real-
world case studies of significant failures of corre-
lation between B??? and human adequacy/fluency
judgments. Both cases involve comparisons between
statistical MT systems and other translation meth-
ods (human post-editing and a rule-based MT sys-
tem), and they recommend that the use of B??? be
restricted to comparisons between related systems or
different versions of the same systems. In B????s de-
fense, comparisons between different versions of the
same system were exactly what B??? was designed
for.
However, we show that even in such situations,
difficulties with B??? can arise. We illustrate three
ways that properties of B??? can be exploited to
yield improvements that are questionable or even
absurd. All of these scenarios arose in actual prac-
tice and involve comparisons between different ver-
sions of the same statistical MT systems. They can
be traced to the fact that B??? is not decomposable
at the sentence level: that is, it lacks the property
that improving a sentence in a test set leads to an
increase in overall score, and degrading a sentence
leads to a decrease in the overall score. This prop-
erty is not only intuitive, but also computationally
convenient for various applications such as transla-
tion reranking and discriminative training. We pro-
pose a minimal modification to B??? that reduces
its nondecomposability, as well as a cross between
B??? and word error rate (WER) that is decompos-
able down to the subsentential level (in a sense to be
made more precise below). Both metrics correct the
observed problems and correlate with human judg-
ments better than B???.
2 The B??? metric
Let gk(w) be the multiset of all k-grams of a sentence
w. We are given a sequence of candidate translations
c to be scored against a set of sequences of reference
translations, {r j} = r1, . . . , rR:
c = c1, c2, c3, . . . , cN
r1 = r11, r
1
2, r
1
3, . . . , r
1
N
...
rR = rR1 , r
R
2 , r
R
3 , . . . , r
R
N
610
Then the B??? score of c is defined to be
B???(c, {r j}) =
4?
k=1
prk(c, {r
j})
1
4 ? bp(c, {r j}) (1)
where1
prk(c, {r
j}) =
?
i
????gk(ci) ?
?
j gk(r
j
i )
????
?
i |gk(ci)|
(2)
is the k-gram precision of c with respect to {r j}, and
bp(c, r), known as the brevity penalty, is defined as
follows. Let ?(x) = exp(1 ? 1/x). In the case of a
single reference r,
bp(c, r) = ?
(
min
{
1,
?
i |ci|
?
i |ri|
})
(3)
In the multiple-reference case, the length |ri| is re-
placed with an effective reference length, which can
be calculated in several ways.
? In the original definition (Papineni et al, 2002),
it is the length of the reference sentence whose
length is closest to the test sentence.
? In the NIST definition, it is the length of the
shortest reference sentence.
? A third possibility would be to take the average
length of the reference sentences.
The purpose of the brevity penalty is to prevent
a system from generating very short but precise
translations, and the definition of effective reference
length impacts how strong the penalty is. The NIST
definition is the most tolerant of short translations
and becomes more tolerant with more reference sen-
tences. The original definition is less tolerant but
has the counterintuitive property that decreasing the
length of a test sentence can eliminate the brevity
penalty. Using the average reference length seems
attractive but has the counterintuitive property that
1We use the following definitions about multisets: if X is a
multiset, let #X(a) be the number of times a occurs in X. Then:
|X| ?
?
a
#X(a)
#X?Y (a) ? min{#X(a), #Y (a)}
#X?Y (a) ? max{#X(a), #Y (a)}
an exact match with one of the references may not
get a 100% score. Throughout this paper we use the
NIST definition, as it is currently the definition most
used in the literature and in evaluations.
The brevity penalty can also be seen as a stand-
in for recall. The fraction
?
i |ci |?
i |ri |
in the definition of
the brevity penalty (3) indeed resembles a weak re-
call score in which every guessed item counts as a
match. However, with recall, the per-sentence score
|ci |
|ri |
would never exceed unity, but with the brevity
penalty, it can. This means that if a system generates
a long translation for one sentence, it can generate
a short translation for another sentence without fac-
ing a penalty. This is a serious weakness in the B???
metric, as we demonstrate below using three scenar-
ios, encountered in actual practice.
3 Exploiting the B??? metric
3.1 The sign test
We are aware of two methods that have been pro-
posed for significance testing with B???: bootstrap
resampling (Koehn, 2004b; Zhang et al, 2004) and
the sign test (Collins et al, 2005). In bootstrap re-
sampling, we sample with replacement from the test
set to synthesize a large number of test sets, and
then we compare the performance of two systems on
those synthetic test sets to see whether one is better
95% (or 99%) of the time. But Collins et al (2005)
note that it is not clear whether the conditions re-
quired by bootstrap resampling are met in the case of
B???, and recommend the sign test instead. Suppose
we want to determine whether a set of outputs c from
a test system is better or worse than a set of baseline
outputs b. The sign test requires a function f (bi, ci)
that indicates whether ci is a better, worse, or same-
quality translation relative to bi. However, because
B??? is not defined on single sentences, Collins et
al. use an approximation: for each i, form a compos-
ite set of outputs b? = {b1, . . . , bi?1, ci, bi+1, . . . , bN},
and compare the B??? scores of b and b?.
The goodness of this approximation depends on
to what extent the comparison between b and b? is
dependent only on bi and ci, and independent of the
other sentences. However, B??? scores are highly
context-dependent: for example, if the sentences in
b are on average  words longer than the reference
sentences, then ci can be as short as (N ? 1) words
611
shorter than ri without incurring the brevity penalty.
Moreover, since the ci are substituted in one at a
time, we can do this for all of the ci. Hence, c could
have a disastrously low B??? score (because of the
brevity penalty) yet be found by the sign test to be
significantly better than the baseline.
We have encountered this situation in practice:
two versions of the same system with B??? scores of
29.6 (length ratio 1.02) and 29.3 (length ratio 0.97),
where the sign test finds the second system to be sig-
nificantly better than the first (and the first system
significantly better than the second). Clearly, in or-
der for a significance test to be sensible, it should not
contradict the observed scores, and should certainly
not contradict itself. In the rest of this paper, except
where indicated, all significance tests are performed
using bootstrap resampling.
3.2 Genre-specific training
For several years, much statistical MT research has
focused on translating newswire documents. One
likely reason is that the DARPA TIDES program
used newswire documents for evaluation for several
years. But more recent evaluations have included
other genres such as weblogs and conversation. The
conventional wisdom has been that if one uses a
single statistical translation system to translate text
from several different genres, it may perform poorly,
and it is better to use several systems optimized sep-
arately for each genre.
However, if our task is to translate documents
from multiple known genres, but they are evaluated
together, the B??? metric allows us to use that fact
to our advantage. To understand how, notice that
our system has an optimal number of words that it
should generate for the entire corpus: too few and it
will be penalized by B????s brevity penalty, and too
many increases the risk of additional non-matching
k-grams. But these words can be distributed among
the sentences (and genres) in any way we like. In-
stead of translating sentences from each genre with
the best genre-specific systems possible, we can
generate longer outputs for the genre we have more
confidence in, while generating shorter outputs for
the harder genre. This strategy will have mediocre
performance on each individual genre (according to
both intuition and B???), yet will receive a higher
B??? score on the combined test set than the com-
bined systems optimized for each genre.
In fact, knowing which sentence is in which genre
is not even always necessary. In one recent task,
we translated documents from two different genres,
without knowing the genre of any given sentence.
The easier genre, newswire, also tended to have
shorter reference sentences (relative to the source
sentences) than the harder genre, weblogs. For ex-
ample, in one dataset, the newswire reference sets
had between 1.3 and 1.37 English words per Ara-
bic word, but the weblog reference set had 1.52 En-
glish words per Arabic word. Thus, a system that
is uniformly verbose across both genres will appor-
tion more of its output to newswire than to weblogs,
serendipitously leading to a higher score. This phe-
nomenon has subsequently been observed by Och
(2008) as well.
We trained three Arabic-English syntax-based
statistical MT systems (Galley et al, 2004; Galley
et al, 2006) using max-B??? training (Och, 2003):
one on a newswire development set, one on a we-
blog development set, and one on a combined devel-
opment set containing documents from both genres.
We then translated a new mixed-genre test set in two
ways: (1) each document with its appropriate genre-
specific system, and (2) all documents with the sys-
tem trained on the combined (mixed-genre) devel-
opment set. In Table 3, we report the results of both
approaches on the entire test dataset as well as the
portion of the test dataset in each genre, for both the
genre-specific and mixed-genre trainings.
The genre-specific systems each outperform the
mixed system on their own genre as expected, but
when the same results are combined, the mixed sys-
tem?s output is a full B??? point higher than the com-
bination of the genre-specific systems. This is be-
cause the mixed system produces outputs that have
about 1.35 English words per Arabic word on av-
erage: longer than the shortest newswire references,
but shorter than the weblog references. The mixed
system does worse on each genre but better on the
combined test set, whereas, according to intuition,
a system that does worse on the two subsets should
also do worse on the combined test set.
3.3 Word deletion
A third way to take advantage of the B??? metric
is to permit an MT system to delete arbitrary words
612
in the input sentence. We can do this by introduc-
ing new phrases or rules into the system that match
words in the input sentence but generate no output;
to these rules we attach a feature whose weight is
tuned during max-B??? training. Such rules have
been in use for some time but were only recently
discussed by Li et al (2008).
When we add word-deletion rules to our MT sys-
tem, we find that the B??? increases significantly
(Table 6, line 2). Figure 1 shows some examples
of deletion in Chinese-English translation. The first
sentence has a proper name,?<[[/maigesaisai
?Magsaysay?, which has been mistokenized into four
tokens. The baseline system attempts to translate the
first two phonetic characters as ?wheat Georgia,?
whereas the other system simply deletes them. On
the other hand, the second sentence shows how word
deletion can sacrifice adequacy for the sake of flu-
ency, and the third sentence shows that sometimes
word deletion removes words that could have been
translated well (as seen in the baseline translation).
Does B??? reward word deletion fairly? We note
two reasons why word deletion might be desirable.
First, some function words should truly be deleted:
for example, the Chinese particle?/de and Chinese
measure words often have no counterpart in English
(Li et al, 2008). Second, even content word deletion
might be helpful if it allows a more fluent translation
to be assembled from the remnants. We observe that
in the above experiment, word deletion caused the
absolute number of k-gram matches, and not just k-
gram precision, to increase for all 1 ? k ? 4.
Human evaluation is needed to conclusively de-
termine whether B??? rewards deletion fairly. But to
control for these potentially positive effects of dele-
tion, we tested a sentence-deletion system, which
is the same as the word-deletion system but con-
strained to delete all of the words in a sentence or
none of them. This system (Table 6, line 3) deleted
8?10% of its input and yielded a B??? score with
no significant decrease (p ? 0.05) from the base-
line system?s. Given that our model treats sentences
independently, so that it cannot move information
from one sentence to another, we claim that dele-
tion of nearly 10% of the input is a grave translation
deficiency, yet B??? is insensitive to it.
What does this tell us about word deletion? While
acknowledging that some word deletions can im-
prove translation quality, we suggest in addition that
because word deletion provides a way for the system
to translate the test set selectively, a behavior which
we have shown that B??? is insensitive to, part of
the score increase due to word deletion is likely an
artifact of B???.
4 Other metrics
Are other metrics susceptible to the same problems
as the B??? metric? In this section we examine sev-
eral other popular metrics for these problems, pro-
pose two of our own, and discuss some desirable
characteristics for any new MT evaluation metric.
4.1 Previous metrics
We ran a suite of other metrics on the above problem
cases to see whether they were affected. In none of
these cases did we repeat minimum-error-rate train-
ing; all these systems were trained using max-B???.
The metrics we tested were:
? METEOR (Banerjee and Lavie, 2005), version
0.6, using the exact, Porter-stemmer, andWord-
Net synonmy stages, and the optimized param-
eters ? = 0.81, ? = 0.83, ? = 0.28 as reported
in (Lavie and Agarwal, 2007).
? GTM (Melamed et al, 2003), version 1.4, with
default settings, except e = 1.2, following the
WMT 2007 shared task (Callison-Burch et al,
2007).
? M??S?? (Chan and Ng, 2008), more specifi-
cally M??S??n, which skips the dependency re-
lations.
On the sign test (Table 2), all metrics found sig-
nificant differences consistent with the difference in
score between the two systems. The problem related
to genre-specific training does not seem to affect the
other metrics (see Table 4), but they still manifest
the unintuitive result that genre-specific training is
sometimes worse than mixed-genre training. Finally,
all metrics but GTM disfavored both word deletion
and sentence deletion (Table 7).
4.2 Strict brevity penalty
A very conservative way of modifying the B??? met-
ric to combat the effects described above is to im-
613
(a) source 9]Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 727?736,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Synchronous Tree Adjoining Machine Translation
Steve DeNeefe and Kevin Knight
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
{sdeneefe,knight}@isi.edu
Abstract
Tree Adjoining Grammars have well-known
advantages, but are typically considered too
difficult for practical systems. We demon-
strate that, when done right, adjoining im-
proves translation quality without becoming
computationally intractable. Using adjoining
to model optionality allows general translation
patterns to be learned without the clutter of
endless variations of optional material. The
appropriate modifiers can later be spliced in as
needed.
In this paper, we describe a novel method
for learning a type of Synchronous Tree Ad-
joining Grammar and associated probabilities
from aligned tree/string training data. We in-
troduce a method of converting these gram-
mars to a weakly equivalent tree transducer
for decoding. Finally, we show that adjoining
results in an end-to-end improvement of +0.8
BLEU over a baseline statistical syntax-based
MTmodel on a large-scale Arabic/EnglishMT
task.
1 Introduction
Statistical MT has changed a lot in recent years.
We have seen quick progress from manually
crafted linguistic models to empirically learned
statistical models, from word-based models to
phrase-based models, and from string-based mod-
els to tree-based models. Recently there is a swing
back to incorporating more linguistic information
again, but this time linguistic insight carefully
guides the setup of empirically learned models.
Shieber (2007) recently argued that proba-
bilistic Synchronous Tree Adjoining Grammars
(Shieber and Schabes, 1990) have the right com-
bination of properties that satisfy both linguists
and empirical MT practitioners. So far, though,
most work in this area has been either more lin-
guistic than statistical (Abeille et al, 1990) or
statistically-based, but linguistically light (Nesson
et al, 2006).
Current tree-based models that integrate lin-
guistics and statistics, such as GHKM (Galley et
al., 2004), are not able to generalize well from
a single phrase pair. For example, from the data
in Figure 1, GHKM can learn rule (a) to translate
nouns with two pre-modifiers, but does not gener-
alize to learn translation rules (b) - (d) without the
optional adjective or noun modifiers. Likewise,
none of these rules allow extra material to be intro-
duced, e.g. ?Pakistan?s national defense minister?.
In large enough training data sets, we see many
examples of all the common patterns, but the rarer
patterns have sparse statistics or poor coverage.
NP
JJ
national
NN
defense
NN
minister
wzyr AldfAE AlwTnY
(a)
NP
JJ
1
NN
2
NN
3
? NN
3
NN
2
JJ
1
(b)
NP
NN
1
NN
2
? NN
2
NN
1
(c)
NP
JJ
1
NN
2
? NN
2
JJ
1
(d)
NP
NN
1
? NN
1
Figure 1: Rule (a) can be learned from this training
example. Arguably, the more general rules (b) -
(d) should also be learnable.
To mitigate this problem, the parse trees used
as training data for these systems can be binarized
(Wang et al, 2007). Binarization allows rules with
partial constituents to be learned, resulting in more
general rules, richer statistics, and better phrasal
coverage (DeNeefe et al, 2007), but no principled
required vs. optional decision has been made. This
method?s key weakness is that binarization always
keeps adjacent siblings together, so there is no way
to group the head with a required complement if
optional information intervenes between the two.
Furthermore, if all kinds of children are consid-
ered equally optional, then we have removed im-
portant syntactic constraints, which may end up
permitting too much freedom. In addition, spu-
rious alignments may limit the binarization tech-
727
nique?s effectiveness.
In this paper, we present a method of learning
a type of probabilistic Synchronous Tree Adjoin-
ing Grammar (STAG) automatically from a cor-
pus of word-aligned tree/string pairs. To learn this
grammar we use linguistic resources to make the
required vs. optional decision. We then directly
model the optionality in the translation rules by
learning statistics for the required parts of the rule
independently from the optional parts. We also
present a method of converting these rules into a
well-studied tree transducer formalism for decod-
ing purposes. We then show that modeling option-
ality using adjoining results in a statistically sig-
nificant BLEU gain over our baseline syntax-based
model with no adjoining.
2 Translation Model
2.1 Synchronous Tree Insertion Grammars
Tree Adjoining Grammars (TAG), introduced by
Joshi et al (1975) and Joshi (1985), allow inser-
tion of unbounded amounts of material into the
structure of an existing tree using an adjunction
operation. Usually they also include a substitution
operation, which has a ?fill in the blank? seman-
tics, replacing a substitution leaf node with a tree.
Figure 2 visually demonstrates TAG operations.
Shieber and Schabes (1990) offer a synchronous
version of TAG (STAG), allowing the construc-
tion of a pair of trees in lockstep fashion using the
TAG operations of substitution and adjunction on
tree pairs. To facilitate this synchronous behav-
ior, links between pairs of nodes in each tree pair
define the possible sites for substitution and ad-
junction to happen. One application of STAG is
machine translation (Abeille et al, 1990).
One negative aspect of TAG is the compu-
tational complexity: O(n6) time is required
for monolingual parsing (and thus decoding),
and STAG requires O(n12) for bilingual parsing
(which might be used for training the model di-
rectly on bilingual data). Tree Insertion Grammars
(TIG) are a restricted form of TAG that was in-
troduced (Schabes and Waters, 1995) to keep the
same benefits as TAG (adjoining of unbounded
material) without the computational complexity?
TIG parsing is O(n3). This reduction is due to a
limitation on adjoining: auxiliary trees can only
introduce tree material to the left or the right of
the node adjoined to. Thus an auxiliary tree can
be classified by direction as left or right adjoining.
adjunction
NP
DT
the
NP
NN?
NP
JJ? NP*
substitution substitution
NN
minister
JJ
defense
=?
NP
DT
the
NP
JJ
defense
NP
NN
minister
Figure 2: TAG grammars use substitution and ad-
junction operations to construct trees. Substitu-
tion replaces the substitution node (marked with
?) with another tree. Adjunction inserts an aux-
iliary tree?a special kind of tree fragment with a
foot node (marked with *)?into an existing tree at
a permitted non-terminal node. Note that in TAG,
adjunctions are permitted at any non-terminal with
the same label as the root and foot node of the
auxiliary tree, while in STAG adjunctions are re-
stricted to linked sites.
Nesson et al (2006) introduce a probabilis-
tic, synchronous variant of TIG and demonstrate
its use for machine translation, showing results
that beat both word-based and phrase-based MT
models on a limited-vocabulary, small-scale train-
ing and test set. Training the model uses an
O(n
6
) bilingual parsing algorithm, and decoding
is O(n3). Though this model uses trees in the for-
mal sense, it does not create Penn Treebank (Mar-
cus et al, 1993) style linguistic trees, but uses only
one non-terminal label (X) to create those trees us-
ing six simple rule structures.
The grammars we use in this paper share some
properties in common with those of Nesson et al
(2006) in that they are of the probabilistic, syn-
chronous tree-insertion variety. All pairs of sites
(both adjunction and substitution in our case) are
explicitly linked. Adjunction sites are restricted by
direction: at each linked site, the source and target
side each specify one allowed direction. The re-
sult is that each synchronous adjunction site can be
classified into one of four direction classes: {LR,
LL, RR, RL}. For example, LR means the source
side site only allows left adjoining trees and the
target side site only allows right adjoining trees.
There are several important differences between
our grammars and the ones of Nesson et al (2006):
Richer, Linguistic Trees: Our grammars have a
728
Penn Treebank-style linguistic tree on the En-
glish (target) side, and a hierarchical structure
using only a single non-terminal symbol (X)
on the source side. We believe this provides
the rich information needed in the target lan-
guage without over-constraining the model.
Substitution Sites/Non-lexical trees: We use
both substitution and adjunction (Nesson
et al (2006) only used adjunction) and do
not require all trees to contain lexical items
as is commonly done in TIG (Schabes and
Waters, 1995).
Single Adjunction/Multiple Sites: Each non-
terminal node in a tree may allow multiple
adjunction sites, but every site only allows at
most one adjunction,1 a common assumption
for TAG as specified in the Vijay-Shanker
(1987) definition.
Here are some examples of automatically
learned translation rules with interpretations of
how they work:
1. simple lexical rules for translating words or
phrases:
IN
without
??
X
AlA
interpretation: translate the Arabic word
?AlA? as the preposition ?without?
2. rules with substitution for translating phrases
with holes (substitution sites are designated
by an arrow and numeric subscript, e.g.
NP?
1
):
PP
PP
IN
of
NP?
1
??
X
X?
1
interpretation: insert ?of? to turn a noun
phrase into a prepositional phrase
3. simple adjoining rules for inserting optional
modifiers (adjoining sites are designated by
1An adjoined rule may itself have adjoining sites allowing
further adjunction.
an alphabetic subscript before or after a non-
terminal to indicate direction of adjoining,
e.g.
a
NP):
a
NP
JJ?
1
NP*
??
X
X* X
a
X?
1
interpretation: adjoin an adjective before a
noun in English but after in Arabic, and al-
lowing further adjoinings in those same di-
rections afterward
4. rules with multiple adjunction and substitu-
tion sites:
a
S
NP?
1 b
S
c
VP
d
VP
e
VBD?
2
NP?
3
??
X
a
X
X?
2
X
X?
1
e,b
X
d,c
X?
3
interpretation: translate an Arabic sentence in
VSO form into an English sentence in SVO
form, with multiple adjoining options
2.2 Generative Story
When we use these rules to translate from a for-
eign sentence f into an English sentence e, we
use several models together in a log-linear fash-
ion, but our primary model is a joint model of
P (e
tree
, f
tree
), which is our surrogate for directly
modeling P (e|f). This can be justified because
P (e|f) =
P (e,f)
P (f)
, and P (f) is fixed for a given
foreign sentence. Therefore:
argmax
e
P (e|f) = argmax
e
P (e, f)
? yield(argmax
e
tree
P (e
tree
, f
tree
))
? yield(argmax
e
tree
P (d
e
tree
,f
tree
))
where d
e
tree
,f
tree
is a derivation tree of rules that
generates e
tree
and f
tree
. In other words, e, the
highest probability translation of f , can be approx-
imated by taking the yield of the highest proba-
bility tree e
tree
that is a translation of the high-
est probability tree of f . This can further be ap-
proximated by the highest probability derivation
of rules translating between f and e via trees.
Now we define the probability of generating
d
e
tree
,f
tree
. Starting with an initial symbol pair
729
representing a rule with a single substitution site,2
?TOP?, X??, a tree pair can be generated by the
following steps:
1. For each substitution site s
i
in the current rule
r
1
:
(a) Choose with probability
P
sub
(r
2
|?label
L
(s
i
), label
R
(s
i
)?) a rule
r
2
having root node labels label
L
(s
i
)
and label
R
(s
i
) that match the left and
right labels at s
i
.
2. For each adjunction site s
i,r
1
in the current
rule r
1
:
(a) Choose with rule-specific probability
Pifadj(decisionadjoin|si,r
1
, r
1
) choose
whether or not to adjoin at the current
site s
i,r
1
.
(b) If we are adjoining at site
s
i,r
1
, choose with probability
P
adj
(r
2
|d, ?label
L
(s
i,r
1
), label
R
(s
i,r
1
)?)
a rule r
2
of direction class d having
root node labels label
L
(s
i,r
1
) and
label
R
(s
i,r
1
) that match the left and
right labels at s
i,r
1
.
3. Recursively process each of the added rules
For all substitution rules r
s
, adjoining rules r
a
,
and adjoining sites s
i,r
, the probability of a deriva-
tion tree using these rules is the product of all the
probabilities used in this process, i.e.:
P
deriv
=
?
r
s
(
P
sub
(r
s
|?root
L
(r
s
), root
R
(r
s
)?) ?
?
s
i,r
s
Pifadj(decisionadjoin|si,r
s
, r
s
)
)
?
?
r
a
(
P
adj
(r
a
|dir(r
a
), ?root
L
(r
a
), root
R
(r
a
)?) ?
?
s
i,r
a
Pifadj(decisionadjoin|si,r
a
, r
a
)
)
Note that while every new substitution site re-
quires an additional rule to be added, adjunction
sites may or may not introduce an additional rule
based on the rule-specific Pifadj probability. This
allows adjunction to represent linguistic optional-
ity.
2Here and in the following, we use site as shorthand for
synchronous site pair.
3 Learning the Model
Instead of using bilingual parsing to directly train
our model from strings as done by Nesson et al
(2006), we follow the method of Galley et al
(2004) by dividing the training process into steps.
First, we word align the parallel sentences and
parse the English (target) side. Then, we transform
the aligned tree/string training data into derivation
trees of minimal translation rules (Section 3.1). Fi-
nally, we learn our probability models P
sub
, Pifadj ,
and P
adj
by collecting counts over the derivation
trees (Section 3.2). This method is quick enough
to allow us to scale our learning process to large-
scale data sets.
3.1 Generating Derivation Trees and Rules
There are four steps in transforming the training
data into derivation trees and rules, the first two
operating only on the English parse tree itself:3
A. Marking Required vs. Optional. For each
constituent in the English parse tree, we mark chil-
dren as (H)ead, (R)equired, or (O)ptional elements
(see step (a) in Figure 3). The choice of head, re-
quired, or optional has a large impact on the gen-
erality and applicability of our grammar. If all
children are considered required, the result is the
same as the GHKM rules of Galley et al (2004)
and has the same problem?lots of low count,
syntactically over-constrained rules. Too many
optional children, on the other hand, allows un-
grammatical output. Our proposed model is a lin-
guistically motivated middle ground: we consider
the linguistic heads and complements selected by
Collins? (2003) rules to be required and all other
children to be optional.
B. Parse tree to TIG tree. Next, we re-
structure the English tree to form a TIG deriva-
tion where head and required elements are substi-
tutions, and optional elements are adjunctions (see
step (b) in Figure 3). To allow for adjoining be-
tween siblings under a constituent, we first do a
head-out binarization of the tree. This is followed
by excising4 any children marked as optional and
replacing them with an adjunction site, as shown
in Figure 4. Note that we excise a chain of op-
tional children as one site with each optional child
3These first two steps were inspired by the method Chiang
(2003) used to automatically extract a TIG from an English
parse tree.
4Excising is the opposite of adjoining: extracting out an
auxiliary rule from a tree to form two smaller trees.
730
SADVP , NP VP .
(a)
=?
S
ADVP
O
,
O
NP
R
VP
H
.
O
(b)
=?
S
ADVP , NP VP .
Figure 3: Parse tree to TIG transformation: (a) mark constituent children with (H)ead, (R)equired, and
(O)ptional, then (b) restructure the tree so that head and required elements are substitutions, while op-
tional elements are adjoined (shown with dotted lines).
NT
1
NT
1
ABC
NT
2
XYZ
=?
NT
1
ABC NT
1
NT
1
* NT
2
XYZ
NT
1
NT
3
XYZ
NT
1
NT
2
DEF
NT
1
ABC
=?
NT
1
NT
1
NT
1
NT
3
XYZ
NT
1
*
NT
2
DEF
NT
1
*
ABC
(a) excising one optional child (XYZ) (b) excising a series of optional children (DEF, then XYZ)
Figure 4: Two examples of excising auxiliary trees from a head-out binarized parse tree: (a) excising one
optional left branch, (b) excising a chain of optional branches in the same (right) direction into a series
of adjunctions. In both examples, the ?ABC? child is the head, while the other children are optional.
adjoined to the previous child, as in Figure 4(b).
C. Extracting rules and derivation trees. We
now have a TIG derivation tree, with each elemen-
tary tree attached to its parent by a substitution or
adjunction link. We can now extract synchronous
rules allowed by the alignments and syntactic con-
stituents. This can be done using a method in-
spired by the rule-extraction approach of Galley et
al. (2004), but instead of directly operating on the
parse tree we process the English TIG derivation
tree. In bottom-up fashion, we visit each elemen-
tary tree in the derivation, allowing a rule rooted
at this tree to be extracted if its words or those
of its descendants are aligned such that they are
the English side of a self-contained parallel phrase
(i.e., the foreign text of this phrase is not aligned to
English leaves outside of the set of descendants).
Otherwise, this elementary tree is rejoined with its
parent to form a larger elementary tree. At the end
of this process we have a new set of linked ele-
mentary trees which make up the English side of
the grammar, where each substitution or adjunc-
tion link becomes a substitution or adjunction site
in the synchronous grammar.
On the foreign side we start with the foreign text
of the self-contained parallel phrase and replace
any parts of this phrase covered by substituted or
adjoined children of the English side tree with sub-
stitution sites or adjunction site markers. From
this, we produce a tree with a simple, regular form
by placing all items under a root node labeled X.
In the case of more than one foreign word or sub-
stitution site, we introduce an intermediate level of
X-labeled non-terminals to allow for possible ad-
junction between elements, otherwise the adjoin-
ing sites attach to the single root node. We attach
all foreign-side adjoining sites to be left adjoining,
except on the right side of the right-hand child.
It is possible to have the head child tree on the
English side not aligned to anything, while the ad-
joined children are. This may lead to rules with no
foreign non-terminal from which to anchor the ad-
junctions, so in this case, we attach adjoined child
elementary trees starting from the head and mov-
ing out until we attach a some child with a non-
empty foreign side.
D. Generalizing rules. We need to clarify
what makes one rule distinct from another. Con-
sider the example in Figure 5, which shows se-
lected rules learned in the case of two different
noun phrases. If the noun phrase consists of just
a single noun, we learn rule (a), while if the noun
phrase also has an adjective, we learn rules (b) and
(c). Since adjoining the adjective is optional, we
731
consider rules (a) and (c) to be the same rule, the
latter with an adjoining seen, and the former with
the same adjoining not seen.
3.2 Statistical Models
Once we have the derivation trees and list of rules,
we learn our statistical models using maximum
likelihood estimation. By counting and normal-
izing appropriately over the entire corpus, we can
straightforwardly learn the P
sub
and P
adj
distribu-
tions. However, recall that in our model Pifadj is a
rule-specific probability, which makes it more dif-
ficult to estimate accurately. For common rules,
we see plenty of examples of adjoining, while for
other rules, we need to learn from only a handful
of examples. Smoothing and generalization are es-
pecially important for these low frequency cases.
Two options present themselves for how to esti-
mate adjoining:
(a) A joint model of adjoining. We assume that
adjoining decisions are made in combination
with each other, and so learn non-zero proba-
bilities only for adjoining combinations seen
in data
(b) An independent model of adjoining. We as-
sume adjoining decisions are made indepen-
dently, and learn a model for each adjoining
site separately
Option (a) may be sufficient for frequent rules,
and will accurately model dependencies between
different kinds of adjoining. However, it does not
allow us to generalize to unseen patterns of adjoin-
ing. Consider the low frequency situation depicted
in Figure 6, rules (d)-(f). We may have seen this
rule four times, once with adjoining site a, twice
with adjoining sites a and b, and once with a third
adjoining site c. The joint model will give a zero
probability to unseen patterns of adjoining, e.g. no
adjoining at any site or adjoining at site b alone.
Even if we use a discounting method to give a non-
zero probability to unseen cases, we still have no
way to distinguish one from another.
Option (b) allows us to learn reasonable esti-
mates for these missing cases by separating out
adjoining decisions and letting each speak for it-
self. To properly learn non-zero probabilities for
unseen cases5 we use add k smoothing (k = 1
2
).
5For example, low frequency rules may have always been
observed with a single adjoining pattern, and never without
adjoining.
A weakness of this approach still remains: ad-
joining is not a truly independent process, as we
observe empirically in the data. In real data, fre-
quent rules have many different observed adjoin-
ing sites (10 or 20 in some cases), many of which
represent already infrequent sites in combinations
never seen together. To reduce the number of in-
valid combinations produced, we only allow ad-
joinings to be used at the same time if they have
occurred together in the training data. This restric-
tion makes it possible to do less adjoining than ob-
served, but not more. For the example in Figure 6,
in addition to the observed patterns, we would also
allow site b to be used alone, and we would allow
no adjoinings, but we would not allow combina-
tions of site c with either a or b. Later, we will
see that this makes the decoding process more ef-
ficient.
Because both option (a) and (b) above have
strengths and weaknesses, we also explore a third
option which builds upon the strengths of each:
(c) A log-linear combination of the joint model
and independent model. We assume the prob-
ability has both a dependent and indepen-
dent element, and learn the relative weight
between them automatically
To help smooth this model we add two addi-
tional binary features: one indicating adjoining
patterns seen in data and one indicating previously
unseen patterns.
4 Decoding
To translate with these rules, we do a monolingual
parse using the foreign side of the rules (constrain-
ing the search using non-terminal labels from both
sides), while keeping track of the English side
string and structure for language modeling pur-
poses. This produces all valid derivations of rules
whose foreign side yield is the input string, from
which we simply choose the one with the high-
est log-linear model score. Though this process
could be done directly using a specialized parsing
algorithm, we note that these rules have weakly
equivalent counterparts in the Synchronous Tree
Substitution Grammar (STSG) and Tree-to-string
transducer (xLNTs6) worlds, such that each STIG
rule can be translated into one equivalent rule, plus
some helper rules to model the adjoin/no-adjoin
6xLNTs is shorthand for extended linear non-deleting top-
down tree-to-string transducer.
732
Case 1: Case 2:
NP
NN
health
AlSHp
? (a)
NP
NN?
1
??
X
X?
1
NP
JJ
national
NP
NN
defense
AldfAE AlwTnY
?
(b)
NP
JJ?
1
NP*
??
X
X* X
X?
1
(c)
a
NP
NN?
1
??
X
a
X?
1
Figure 5: Selected rules learned in two cases. Rule (a) and (c) are considered the same rule, where (c)
has the optional synchronous adjoining site marked with a. From these (limited) examples alone we
would infer that adjective adjoining happens half the time, and is positioned before the noun in English,
but after the noun in Arabic (thus the positioning of site a).
(d)
a
QP
b
IN?
1
??
a
X
b
X?
1
(e)
a
QP
IN?
1
??
a
X
X?
1
(f)
c
QP
IN?
1
??
X
c
X?
1
(seen once) (seen twice) (seen once)
Figure 6: For a low frequency rule, we may see only a few different adjoining patterns, but we want to
infer more.
decision. Conversion to a better known and ex-
plored formalism allows us to take advantage of
existing code and algorithms. Here we describe
the conversion process to xLNTs rules, though
conversion to STSG is similar.
Algorithm 1 describes the process of converting
one of our automatically learned STIG rules. On
each side of the rule, we traverse the tree in a top-
down, left-to-right order, recording words, substi-
tution sites, and adjoining sites in the order en-
countered (left adjoinings before the node?s chil-
dren and right adjoinings after). We make these
words and sites as the children under a single root
node. The substitution sites are given states made
up of a combination of their source and target la-
bels as are the roots of non-adjoining rules. Ad-
joining sites are labeled with a combination of the
rule id and a site id. Adjoining rule roots are la-
beled with a combination of the source and target
root labels and the direction class. To allow for the
adjoining/no-adjoining decision, two helper rules
are created for each adjoining site, their root state
a combination of the rule and site ids. One of these
rules has only epsilon leaf nodes (representing no
adjoining), while the other has leaf nodes and a
state that match with the corresponding adjoining
rule root (labeled with the site?s source and target
labels and the direction class).
For each rule, the algorithm generates one
main rule and pairs of helper rules to facilitate
adjoining/non-adjoining. For computational effi-
ciency reasons, our decoder supports neither ep-
silon rules nor non-binary rules. So we remove ep-
silons using an exponential expansion of the rules:
combine each main rule with an adjoining or non-
adjoining helper rule for each adjunction site, then
remove epsilon-only branches. For k adjunction
sites this could possibly results in 2k rules. But as
discussed previously (at the end of Section 3.2),
we only allow subsets of adjoining combinations
seen in training data, so this number is substan-
tially lower for large values of k.
5 Experiments
All experiments are trained with a subset (171,000
sentences or 4 million words) of the Arabic-
English training data from the constrained data
track of the NIST 2008 MT Evaluation, leav-
ing out LDC2004T18, LDC2007E07, and the UN
data. The training data is aligned using the LEAF
technique (Fraser and Marcu, 2007). The English
side of the training data is parsed with an imple-
mentation of Collins Model 2 (Collins, 2003)
then head-out binarized. The tuning data (1,178
sentences) and devtest data (1,298 sentences) are
733
Input: Synchronous TIG rule r with j adjoining sites, S ? T , where S and T are trees
Output: a weakly equivalent xLNTs rule S? ? t
1
. . . t
n
, where S? is a one-level tree, and 2 ? j
helper rules for adjoining
Run time: O(|S| + |T |)
begin
rules ? {}, lhs-state ? concat(?q?, get-root(S), get-root(T ))
site-and-word-list-s ? get-sites-and-words-in-order(S)
site-and-word-list-t ? get-sites-and-words-in-order(T )
if r is adjoining then lhs-state ? concat(lhs-state, get-adjoin-dir(S), get-adjoin-dir(T ))
lhs? construct-LHS(lhs-state, get-root(S), site-and-word-list-s)
rhs? construct-RHS(add-states(id(r), site-and-word-list-t))
add(rules, ?lhs ? rhs?) /* main rule */
foreach adjoining site i ? 1 . . . k do
lhs-state ? concat(?q?, id(r), i), rhs-state ? concat(?q?, lhs-root)
lhs-root ? concat(source-label(i), target-label(i), source-dir(i), target-dir(i))
lhs ? construct-LHS(lhs-state, lhs-root, lhs-root)
rhs ? construct-RHS({(rhs-state, lhs-root)})
rhs-eps ? construct-RHS(!)
add(rules, {?lhs ? rhs?, ?lhs ? rhs-eps?}) /* helper rules for site i */
return rules
end
function get-sites-and-words-in-order(node)
y ? {}
if node is substitution site or word then append site or word to y else
append left adjoining sites to y in outside-to-inside order
foreach child c of node do append result of get-yield(c) to y
append right adjoining sites to y in inside-to-outside order
return y
end
function add-states(ruld-id, node-list)
foreach substitution or adjunction site s
i
and in node-list do
if s
i
is substitution site then state = concat(?q?, source-site-label(s
i
), target-site-label(s
i
))
else state = concat(?q?, rule-id, i)
replace s
i
with (state, s
i
)
return modified node-list
end
Algorithm 1: Conversion from synchronous TIG rules to weakly equivalent xLNTs rules
BLEU
description DevTest NIST06
(1) baseline: all required (GHKM minimal, head-out binarized parse trees) 48.0 47.0
(2) joint adjoining prob model alone (only observed adjoining patterns) 48.0 46.6
(3) independent adjoining prob model alone (only observed adjoining patterns) 48.1 46.7
(4) independent adjoining prob model alone (with new adjoining patterns) 48.5 47.6
(5) independent model alone + features (adjoining pattern, direction) 48.4 47.7
(6) log-linear combination of joint & independent models + features 48.7 47.8
Table 1: End-to-end MT results show that the best adjoining model using a log-linear combination
of joint and independent models (line 6) outperforms the baseline (line 1) by +0.7 and +0.8 BLEU, a
statistically significant difference at the 95% confidence level.
734
made up of newswire documents drawn from the
NIST MT evaluation data from 2004, 2005, and
2006 (GALE part). We use the newswire docu-
ments from the NIST part of the 2006 evaluation
data (765 sentences) as a held-out test set.
We train our feature weights using max-BLEU
(Och, 2003) and decode with a CKY-based de-
coder that supports language model scoring di-
rectly integrated into the search.
In addition to P
sub
, P
adj
, and Pifadj , we
use several other features in our log-linear
model during decoding, including: lexical and
phrase-based translation probabilities, a model
similar to conditional probability on the trees
(P (f
tree
(rule)|e
tree
(rule))), a probability model
for generating the top tree non-terminal, a 5-gram
language model7, and target length bonus. We
also have several binary features?lexical rule,
rule with missing or spurious content words?and
several binary indicator features for specialized
rules: unknown word rules; name, number, and
date translation rules; and special fail-safe mono-
tone translation rules in case of parse failures and
extremely long sentences.
Table 1 shows the comparison between our
baseline model (minimal GHKM on head-out bi-
narized parse trees) and different models of ad-
joining, measured with case-insensitive, NIST-
tokenized BLEU (IBM definition). The top section
(lines 1?4) compares the joint adjoining probabil-
ity model to the independent adjoining probabil-
ity model and seen vs. unseen adjoining combi-
nations. While the joint model results in a BLEU
score at the same level as our baseline (line 2),
the independent model (line 4) improves BLEU by
+0.5 and +0.6, which are significant differences
at the 95% confidence level. Since with the in-
dependent model we introduce both new adjoin-
ing patterns and a different probability model for
adjoining (each site is independent), we also use
the independent model with only previously seen
adjoining patterns (line 3). The insignificant dif-
ference in BLEU between lines 2 and 3 leads us
to think that the new adjoining patterns are where
the improvement comes from, rather than the in-
dependent probability model alone.
We also test several other features and combi-
nations. First, we add binary features to indicate
a new adjoining combination vs. one previously
7The 5-gram LM was trained on 2 billion words of auto-
matically selected collections taken from the NIST 08 allow-
able data.
seen in data. We also add features to indicate the
direction class of adjoining to test if there is a sys-
tematic bias toward particular directions. These
features cause no significant difference in score
(line 5). We also add the joint-adjoining proba-
bility as a feature, allowing it to be combined in a
log-linear fashion with the independent probabil-
ity (line 6). This results in our best BLEU gain:
+0.7 and +0.8 over our non-adjoining baseline.
6 Conclusion
We have presented a novel method for learning
the rules and probabilities for a new statistical,
linguistically-informed, syntax-based MT model
that allows for adjoining. We have described a
method to translate using this model. And we have
demonstrated that linguistically-motivated adjoin-
ing improves the end-to-end MT results.
There are many potential directions for research
to proceed. One possibility is to investigate other
methods of making the required vs. optional de-
cision, either using linguistic resources such as
COMLEX or automatically learning the distinc-
tion using EM (as done for tree binarization by
Wang et al (2007)). In addition, most ideas pre-
sented here are extendable to rules with linguistic
trees on both sides (using insights from Lavie et
al. (2008)). Also worth investigating is the direct
integration of bilingual dictionaries into the gram-
mar (as suggested by Shieber (2007)). Lastly, rule
composition and different amounts of lexicaliza-
tion (Galley et al, 2006; Marcu et al, 2006; De-
Neefe et al, 2007) or context modeling (Marin?o et
al., 2006) have been successful with other mod-
els.
Acknowledgments
We thank David Chiang for suggestions about
adjoining models, Michael Pust and Jens-So?nke
Vo?ckler for developing parts of the experimen-
tal framework, and other colleagues at ISI for
their helpful input. We also thank the anony-
mous reviewers for insightful comments and sug-
gestions. This research is financially supported
under DARPA Contract No. HR0011-06-C-0022,
BBN subcontract 9500008412.
References
Anne Abeille, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized TAGs for machine trans-
lation. In Proc. COLING, volume 3.
735
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. Data-
Oriented Parsing.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4).
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. EMNLP-CoNLL.
Alexander Fraser and Daniel Marcu. 2007. Getting the
structure right for word alignment: LEAF. In Proc.
EMNLP-CoNLL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL.
Aravind K. Joshi, L. S. Levy, and M. Takahashi. 1975.
Tree adjunct grammars. Journal of Computer and
System Sciences, 10(1).
Aravind K. Joshi. 1985. How much context-
sensitivity is necessary for characterizing structural
descriptions?tree adjoining grammars. Natural
Language Processing?Theoretical, Computational,
and Psychological Perspectives.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. SSST.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2).
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4).
Rebecca Nesson, Stuart M. Shieber, and Alexander
Rush. 2006. Induction of probabilistic synchronous
tree-insertion grammars for machine translation. In
Proc. AMTA.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: A cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, 21(4).
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. COL-
ING.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Proc.
SSST Wkshp., NAACL-HLT.
Kumar Vijay-Shanker. 1987. A study of tree adjoining
grammars. Ph.D. thesis.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-basedma-
chine translation accuracy. In Proc. EMNLP and
CoNLL.
736
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 97?100, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Interactively Exploring a Machine Translation Model
Steve DeNeefe, Kevin Knight, and Hayward H. Chan
Information Sciences Institute and Department of Computer Science
The Viterbi School of Engineering, University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{sdeneefe,knight}@isi.edu, hhchan@umich.edu
Abstract
This paper describes a method of in-
teractively visualizing and directing the
process of translating a sentence. The
method allows a user to explore a model
of syntax-based statistical machine trans-
lation (MT), to understand the model?s
strengths and weaknesses, and to compare
it to other MT systems. Using this visual-
ization method, we can find and address
conceptual and practical problems in an
MT system. In our demonstration at ACL,
new users of our tool will drive a syntax-
based decoder for themselves.
1 Introduction
There are many new approaches to statistical ma-
chine translation, and more ideas are being sug-
gested all the time. However, it is difficult to deter-
mine how well a model will actually perform. Ex-
perienced researchers have been surprised by the ca-
pability of unintuitive word-for-word models; at the
same time, seemingly capable models often have se-
rious hidden problems ? intuition is no substitute
for experimentation. With translation ideas growing
more complex, capturing aspects of linguistic struc-
ture in different ways, it becomes difficult to try out
a new idea without a large-scale software develop-
ment effort.
Anyone who builds a full-scale, trainable trans-
lation system using syntactic information faces this
problem. We know that syntactic models often do
not fit the data. For example, the syntactic sys-
tem described in Yamada and Knight (2001) can-
not translate n-to-m-word phrases and does not al-
low for multi-level syntactic transformations; both
phenomena are frequently observed in real data. In
building a new syntax-based MT system which ad-
dresses these flaws, we wanted to find problems in
our framework as early as possible. So we decided
to create a tool that could help us answer questions
like:
1. Does our framework allow good translations
for real data, and if not, where does it get stuck?
2. How does our framework compare to exist-
ing state-of-the-art phrase-based statistical MT
systems such as Och and Ney (2004)?
The result is DerivTool, an interactive translation
visualization tool. It allows a user to build up a
translation from one language to another, step by
step, presenting the user with the myriad of choices
available to the decoder at each point in the pro-
cess. DerivTool simplifies the user?s experience of
exploring these choices by presenting only the de-
cisions relevant to the context in which the user is
working, and allowing the user to search for choices
that fit a particular set of conditions. Some previ-
ous tools have allowed the user to visualize word
alignment information (Callison-Burch et al, 2004;
Smith and Jahr, 2000), but there has been no cor-
responding deep effort into visualizing the decoding
experience itself. Other tools use visualization to aid
the user in manually developing a grammar (Copes-
take and Flickinger, 2000), while our tool visualizes
97
Starting with: ? ?0 ?
and applying the rule: NPB(DT(the) NNS(police)) ? ?0
we get: ? NPB(DT(the) NNS(police)) ?
If we then apply the rule: VBN(killed) ? ?
we get: ? NPB(DT(the) NNS(police)) VBN(killed)
Applying the next rule: NP-C(x0:NPB) ? x0
results in: ? NP-C(NPB(DT(the) NNS(police))) VBN(killed)
Finally, applying the rule: VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ? ? x1 x0
results in the final phrase: VP(VBD(was) VP-C(VBN(killed) PP(IN(by) NP-C(NPB(DT(the) NNS(police))))))
Table 1: By applying applying four rules, a Chinese verb phrase is translated to English.
the translation process itself, using rules from very
large, automatically learned rule sets. DerivTool can
be adapted to visualize other syntax-based MT mod-
els, other tree-to-tree or tree-to-string MT models, or
models for paraphrasing.
2 Translation Framework
It is useful at this point to give a brief descrip-
tion of the syntax-based framework that we work
with, which is based on translating Chinese sen-
tences into English syntax trees. Galley et al (2004)
describe how to learn hundreds of millions of tree-
transformation rules from a parsed, aligned Chi-
nese/English corpus, and Galley et al (submitted)
describe probability estimators for those rules. We
decode a new Chinese sentence with a method simi-
lar to parsing, where we apply learned rules to build
up a complete English tree hypothesis from the Chi-
nese string.
The rule extractor learns rules for many situations.
Some are simple phrase-to-phrase rules such as:
NPB(DT(the) NNS(police)) ? ?0
This rule should be read as follows: replace the Chi-
nese word ?0 with the noun phrase ?the police?.
Others rules can take existing tree fragments and
build upon them. For example, the rule
S(x0:NP-C x1:VP x2:.) ? x0 x1 x2
takes three parts of a sentence, a noun phrase (x0),
a verb phrase (x1), and a period (x2) and ties them
together to build a complete sentence. Rules also
can involve phrase re-ordering, as in
NPB(x0:JJ x1:NN) ? x1 x0
This rule builds an English noun phrase out of an
adjective (x0) and a noun (x1), but in the Chinese,
the order is reversed. Multilevel rules can tie several
of these concepts together; the rule
VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C)))
? ? x1 x0
takes a Chinese word ? and two English con-
stituents ? x1, a noun phrase, and x0, a past-
participle verb ? and translates them into a phrase
of the form ?was [verb] by [noun-phrase]?. Notice
that the order of the constituents has been reversed in
the resulting English phrase, and that English func-
tion words have been generated.
The decoder builds up a translation from the
Chinese sentence into an English tree by apply-
ing these rules. It follows the decoding-as-parsing
idea exemplified by Wu (1996) and Yamada and
Knight (2002). For example, the Chinese verb
phrase ? ?0 ? (literally, ?[passive] police
kill?) can be translated to English via four rules (see
Table 1).
3 DerivTool
In order to test whether good translations can be gen-
erated with rules learned by Galley et al (2004),
we created DerivTool as an environment for interac-
tively using these rules as a decoder would. A user
starts with a Chinese sentence and applies rules one
after another, building up a translation from Chinese
to English. After finishing the translation, the user
can save the trace of rule-applications (the deriva-
tion tree) for later analysis.
We now outline the typical procedure for a user
to translate a sentence with DerivTool. To start, the
user loads a set of sentences to translate and chooses
a particular one to work with. The tool then presents
the user with a window split halfway up. The top
98
Figure 1: DerivTool with a completed derivation.
half is the workspace where the user builds a transla-
tion. It initially displays only the Chinese sentence,
with each word as a separate node. The bottom half
presents a set of tabbed panels which allow the user
to select rules to build up the translation. See Fig-
ure 1 for a picture of the interface showing a com-
pleted derivation tree.
The most immediately useful panel is called Se-
lecting Template, which shows a grid of possible En-
glish phrasal translations for Chinese phrases from
the sentence. This phrase grid contains both phrases
learned in our extracted rules (e.g., ?the police?
from earlier) and phrases learned by the phrase-
based translation system (Och and Ney, 2004)1. The
user presses a grid button to choose a phrase to in-
clude in the translation. At this point, a frequency-
1The phrase-based system serves as a sparring partner. We
display its best decoding in the center of the screen. Note that
in Figure 1 its output lacks an auxiliary verb and an article.
ordered list of rules will appear; these rules trans-
late the Chinese phrase into the button-selected En-
glish phrase, and the user specifies which one to use.
Often there will be more than one rule (e.g., ?
may translate via the rule VBD(killed) ? ? or
VBN(killed) ? ?), and sometimes there are no
rules available. When there are no rules, the buttons
are marked in red, telling us that the phrase-based
system has access to this phrasal translation but our
learned syntactic rules did not capture it. Other but-
tons are marked green to represent translations from
the specialized number/name/date system, and oth-
ers are blue, indicating the phrases in the phrase-
based decoder?s best output. A purple button indi-
cates both red and blue, i.e., the phrase was cho-
sen by the phrase-based decoder but is unavailable
in our syntactic framework. This is a bad combina-
tion, showing us where rule learning is weak. The
99
remaining buttons are gray.
Once the user has chosen the phrasal rules re-
quired for translating the sentence, the next step is
to stitch these phrases together into a complete En-
glish syntax tree using more general rules. These are
found in another panel called Searching. This panel
allows a user to select a set of adjacent, top-level
nodes in the tree and find a rule that will connect
them together. It is commonly used for building up
larger constituents from smaller ones. For example,
if one has a noun-phrase, a verb-phrase, and a pe-
riod, the user can search for the rule that connects
them and builds an ?S? on top, completing the sen-
tence. The results of a search are presented in a list,
again ordered by frequency.
A few more features to note are: 1) loading and
saving your work at any point, 2) adding free-form
notes to the document (e.g. ?I couldn?t find a rule
that...?), and 3) manually typing rules if one cannot
be found by the above methods. This allows us to
see deficiencies in the framework.
4 How DerivTool Helps
First, DerivTool has given us confidence that our
syntax-based framework can work, and that the rules
we are learning are good. We have been able to
manually build a good translation for each sentence
we tried, both for short and long sentences. In fact,
there are multiple good ways to translate sentences
using these rules, because different DerivTool users
translate sentences differently. Ordering rules by
frequency and/or probability helps us determine if
the rules we want are also frequent and favored by
our model.
DerivTool has also helped us to find problems
with the framework and to see clearly how to fix
them. For example, in one of our first sentences
we realized that there was no rule for translat-
ing a date ? likewise for numbers, names, cur-
rency values, and times of day. Our phrase-based
system solves these problems with a specialized
date/name/number translator. Through the process
of manually typing syntactic transformation rules
for dates and numbers in DerivTool, it became clear
that our current date/name/number translator did not
provide enough information to create such syntac-
tic rules automatically. This sparked a new area of
research before we had a fully-functional decoder.
We also found that multi-word noun phrases, such
as ?Israeli Prime Minister Sharon? and ?the French
Ambassador?s visit? were often parsed in a way that
did not allow us to learn good translation rules.
The flat structure of the constituents in the syntax
tree makes it difficult to learn rules that are general
enough to be useful. Phrases with possessives also
gave particular difficulty due to the awkward mul-
tilevel structure of the parser?s output. We are re-
searching solutions to these problems involving re-
structuring the syntax trees before training.
Finally, our tool has helped us find bugs in our
system. We found many cases where rules we
wanted to use were unexpectedly absent. We eventu-
ally traced these bugs to our rule extraction system.
Our decoder would have simply worked around this
problem, producing less desirable translations, but
DerivTool allowed us to quickly spot the missing
rules.
5 Conclusion
We created DerivTool to test our MT framework
against real-world data before building a fully-
functional decoder. By allowing us to play the role
of a decoder and translate sentences manually, it has
given us insight into how well our framework fits
the data, what some of its weaknesses are, and how
it compares to other systems. We continue to use
it as we try out new rule-extraction techniques and
finish the decoding system.
References
Chris Callison-Burch, Colin Bannard and Josh Schroeder.
2004. Improved statistical translation through editing.
EAMT-2004 Workshop.
Ann Copestake and Dan Flickinger. 2000. An open source
grammar development environment and broad-coverage En-
glish grammar using HPSG. Proc. of LREC 2000.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? Proc. of NAACL-
HLT 2004.
Franz Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. Computational
Linguistics, 30(4).
Noah A. Smith and Michael E. Jahr. 2000. Cairo: An Align-
ment Visualization Tool. Proc. of LREC 2000.
Dekai Wu. 1996. A polynomial-time algorithm for statistical
machine translation. Proc. of ACL.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. Proc. of ACL.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-
based statistical MT. Proc. of ACL.
100
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 961?968,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Scalable Inference and Training of
Context-Rich Syntactic Translation Models
Michel Galley*, Jonathan Graehl?, Kevin Knight??, Daniel Marcu??,
Steve DeNeefe?, Wei Wang? and Ignacio Thayer?
*Columbia University
Dept. of Computer Science
New York, NY 10027
galley@cs.columbia.edu, {graehl,knight,marcu,sdeneefe}@isi.edu,
wwang@languageweaver.com, thayer@google.com
?University of Southern California
Information Sciences Institute
Marina del Rey, CA 90292
?Language Weaver, Inc.
4640 Admiralty Way
Marina del Rey, CA 90292
Abstract
Statistical MT has made great progress in the last
few years, but current translation models are weak
on re-ordering and target language fluency. Syn-
tactic approaches seek to remedy these problems.
In this paper, we take the framework for acquir-
ing multi-level syntactic translation rules of (Gal-
ley et al, 2004) from aligned tree-string pairs, and
present two main extensions of their approach: first,
instead of merely computing a single derivation that
minimally explains a sentence pair, we construct
a large number of derivations that include contex-
tually richer rules, and account for multiple inter-
pretations of unaligned words. Second, we pro-
pose probability estimates and a training procedure
for weighting these rules. We contrast different
approaches on real examples, show that our esti-
mates based on multiple derivations favor phrasal
re-orderings that are linguistically better motivated,
and establish that our larger rules provide a 3.63
BLEU point increase over minimal rules.
1 Introduction
While syntactic approaches seek to remedy word-
ordering problems common to statistical machine
translation (SMT) systems, many of the earlier
models?particularly child re-ordering models?
fail to account for human translation behavior.
Galley et al (2004) alleviate this modeling prob-
lem and present a method for acquiring millions
of syntactic transfer rules from bilingual corpora,
which we review below. Here, we make the fol-
lowing new contributions: (1) we show how to
acquire larger rules that crucially condition on
more syntactic context, and show how to com-
pute multiple derivations for each training exam-
ple, capturing both large and small rules, as well
as multiple interpretations for unaligned words;
(2) we develop probability models for these multi-
level transfer rules, and give estimation methods
for assigning probabilities to very large rule sets.
We contrast our work with (Galley et al, 2004),
highlight some severe limitations of probability
estimates computed from single derivations, and
demonstrate that it is critical to account for many
derivations for each sentence pair. We also use
real examples to show that our probability mod-
els estimated from a large number of derivations
favor phrasal re-orderings that are linguistically
well motivated. An empirical evaluation against
a state-of-the-art SMT system similar to (Och and
Ney, 2004) indicates positive prospects. Finally,
we show that our contextually richer rules provide
a 3.63 BLEU point increase over those of (Galley
et al, 2004).
2 Inferring syntactic transformations
We assume we are given a source-language (e.g.,
French) sentence f , a target-language (e.g., En-
glish) parse tree pi, whose yield e is a translation
of f , and a word alignment a between f and e.
Our aim is to gain insight into the process of trans-
forming pi into f and to discover grammatically-
grounded translation rules. For this, we need
a formalism that is expressive enough to deal
with cases of syntactic divergence between source
and target languages (Fox, 2002): for any given
(pi, f ,a) triple, it is useful to produce a derivation
that minimally explains the transformation be-
tween pi and f , while remaining consistent with a.
Galley et al (2004) present one such formalism
(henceforth ?GHKM?).
2.1 Tree-to-string alignments
It is appealing to model the transformation of pi
into f using tree-to-string (xRs) transducers, since
their theory has been worked out in an exten-
sive literature and is well understood (see, e.g.,
(Graehl and Knight, 2004)). Formally, transfor-
mational rules ri presented in (Galley et al, 2004)
are equivalent to 1-state xRs transducers mapping
a given pattern (subtree to match in pi) to a right
hand side string. We will refer to them as lhs(ri)
and rhs(ri), respectively. For example, some xRs
961
rules may describe the transformation of does not
into ne ... pas in French. A particular instance may
look like this:
VP(AUX(does), RB(not), x0:VB) ? ne, x0, pas
lhs(ri) can be any arbitrary syntax tree fragment.
Its leaves are either lexicalized (e.g. does) or vari-
ables (x0, x1, etc). rhs(ri) is represented as a se-
quence of target-language words and variables.
Now we give a brief overview of how such
transformational rules are acquired automatically
in GHKM.1 In Figure 1, the (pi, f ,a) triple is rep-
resented as a directed graph G (edges going down-
ward), with no distinction between edges of pi and
alignments. Each node of the graph is labeled with
its span and complement span (the latter in italic
in the figure). The span of a node n is defined by
the indices of the first and last word in f that are
reachable from n. The complement span of n is
the union of the spans of all nodes n? in G that
are neither descendants nor ancestors of n. Nodes
of G whose spans and complement spans are non-
overlapping form the frontier set F ? G.
What is particularly interesting about the fron-
tier set? For any frontier of graph G containing
a given node n ? F , spans on that frontier de-
fine an ordering between n and each other frontier
node n?. For example, the span of VP[4-5] either
precedes or follows, but never overlaps the span of
any node n? on any graph frontier. This property
does not hold for nodes outside of F . For instance,
PP[4-5] and VBG[4] are two nodes of the same
graph frontier, but they cannot be ordered because
of their overlapping spans.
The purpose of xRs rules in this framework is
to order constituents along sensible frontiers in G,
and all frontiers containing undefined orderings,
as between PP[4-5] and VBG[4], must be disre-
garded during rule extraction. To ensure that xRs
rules are prevented from attempting to re-order
any such pair of constituents, these rules are de-
signed in such a way that variables in their lhs can
only match nodes of the frontier set. Rules that
satisfy this property are said to be induced by G.2
For example, rule (d) in Table 1 is valid accord-
ing to GHKM, since the spans corresponding to
1Note that we use a slightly different terminology.
2Specifically, an xRs rule ri is extracted fromG by taking
a subtree ? ? pi as lhs(ri), appending a variable to each
leaf node of ? that is internal to pi, adding those variables to
rhs(ri), ordering them in accordance to a, and if necessary
inserting any word of f to ensure that rhs(ri) is a sequence of
contiguous spans (e.g., [4-5][6][7-8] for rule (f) in Table 1).
DT
CD
VBP
NNS
IN
NNP
NP
NNS
VBG
3
2
2
1
7-8
4
4
5
9
1
2
3
4
5
6
7
8
9
3 1-2,4
-9
2 1-9
2 1-9
1 2-9
7-8 1-5,9
4 1-9
4 1-9
5 1-4,7
-9
9 1-8
1-2 3-9
NP 7-8 1-5,9
NP 5 1-4, 7
-9
PP 4-5 1-4,7
-9
VP 4-5 1-3,7
-9
NP 4-8 1-3,9
VP 3-8 1-2,9
S 1-9 ?
7!
"#
$
%&
'(
)
*+
,
.
Thes
e
peop
le
inclu
de
astro
naut
s
com
ing
from
Fran
ce
..
7
-
Figure 1: Spans and complement-spans determine what
rules are extracted. Constituents in gray are members of the
frontier set; a minimal rule is extracted from each of them.
(a) S(x0:NP, x1:VP, x2:.) ? x0, x1, x2
(b) NP(x0:DT, CD(7), NNS(people)) ? x0, 7?
(c) DT(these) ??
(d) VP(x0:VBP, x1:NP) ? x0, x1
(e) VBP(include) ?-?
(f) NP(x0:NP, x1:VP) ? x1,?, x0
(g) NP(x0:NNS) ? x0
(h) NNS(astronauts) ??*,X
(i) VP(VBG(coming), PP(IN(from), x0:NP)) ?e?, x0
(j) NP(x0:NNP) ? x0
(k) NNP(France) ???
(l) .(.) ? .
Table 1: A minimal derivation corresponding to Figure 1.
its rhs constituents (VBP[3] and NP[4-8]) do not
overlap. Conversely, NP(x0:DT, x1:CD:, x2:NNS)
is not the lhs of any rule extractible from G, since
its frontier constituents CD[2] and NNS[2] have
overlapping spans.3 Finally, the GHKM proce-
dure produces a single derivation from G, which
is shown in Table 1.
The concern in GHKM was to extract minimal
rules, whereas ours is to extract rules of any arbi-
trary size. Minimal rules defined over G are those
that cannot be decomposed into simpler rules in-
duced by the same graph G, e.g., all rules in Ta-
ble 1. We call minimal a derivation that only con-
tains minimal rules. Conversely, a composed rule
results from the composition of two or more min-
imal rules, e.g., rule (b) and (c) compose into:
NP(DT(these), CD(7), NNS(people)) ??, 7?
3It is generally reasonable to also require that the root n
of lhs(ri) be part of F , because no rule induced by G can
compose with ri at n, due to the restrictions imposed on the
extraction procedure, and ri wouldn?t be part of any valid
derivation.
962
OR
NP
(x0
:NP
, x1
:V
P) 
!
x1
,!
, x0
VP
(x0
:VB
P, 
x1:
NP
) 
!
x0
 , x
1
S(x
0:N
P, 
x1:
VP
, x
2:.
) 
!
x0
 , x
1, x
2
NP
(x0
:DT
 CD
(7)
, N
NS
(pe
opl
e))
 
!
x0
, 7"
.(.)
 
!
.
DT
(th
ese
) 
!
#
VB
P(i
ncl
ude
) 
!
$%
&
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
  
!
'(
, x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
NP
(x0
:NN
S) 
!
x0
NP
(x0
:NN
S) 
!
!,
 x0
NP
(x0
:NN
P) 
!
x0
, !
NN
P(F
ran
ce)
 
!
)*
NN
S(a
stro
nau
ts) 
!
+,
, -
OR
OR N
NS
(as
tro
nau
ts) 
!!
,+
,,
 -
OR
NP
(x0
:NN
P) 
!
x0
NP
(x0
:NN
P) 
!
x0
NN
P(F
ran
ce)
 
!
)*
, !
NP
(x0
:NN
S) 
!
x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
co
min
g
fro
m
NN
S
IN
NN
P
NP
VP
NP
VB
G
PP
NP
7-8
5
7-8
5
7-8
4
4
5
4
5
6
7
8
4
4
4-5
4-5
4-8
NN
P(F
ran
ce)
 
!)
*,
 !
NP
(x0
:NN
P) 
!
x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m)
, 
x0:
NP
))  
!
'(
, x0
, !
NN
S(a
stro
nau
ts) 
!
! ,
 +
,,
 -
NP
(x0
:NN
S) 
!
! ,
 x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, !
, x0
(a)
(b)
-
'(
)*
!
+,
as
tro
na
uts
Fra
nce
Figure 2: (a) Multiple ways of aligning? to constituents in the tree. (b) Derivation corresponding to the parse tree in Figure 1,
which takes into account all alignments of? pictured in (a).
Note that these properties are dependent on G, and
the above rule would be considered a minimal rule
in a graph G? similar to G, but additionally con-
taining a word alignment between 7 and ?. We
will see in Sections 3 and 5 why extracting only
minimal rules can be highly problematic.
2.2 Unaligned words
While the general theory presented in GHKM ac-
counts for any kind of derivation consistent with
G, it does not particularly discuss the case where
some words of the source-language string f are
not aligned to any word of e, thus disconnected
from the rest of the graph. This case is highly fre-
quent: 24.1% of Chinese words in our 179 mil-
lion word English-Chinese bilingual corpus are
unaligned, and 84.8% of Chinese sentences con-
tain at least one unaligned word. The question is
what to do with such lexical items, e.g., ? in
Figure 2(a). The approach of building one mini-
mal derivation for G as in the algorithm described
in GHKM assumes that we commit ourselves to
a particular heuristic to attach the unaligned item
to a certain constituent of pi, e.g., highest attach-
ment (in the example, ? is attached to NP[4-8]
and the heuristic generates rule (f)). A more rea-
sonable approach is to invoke the principle of in-
sufficient reason and make no a priori assump-
tion about what is a ?correct? way of assigning
the item to a constituent, and return all derivations
that are consistent with G. In Section 4, we will
see how to use corpus evidence to give preference
to unaligned-word attachments that are the most
consistent across the data. Figure 2(a) shows the
six possible ways of attaching ? to constituents
of pi: besides the highest attachment (rule (f)),?
can move along the ancestors of France, since it is
to the right of the translation of that word, and be
considered to be part of an NNP, NP, or VP rule.
We make the same reasoning to the left: ? can
either start the NNS of astronauts, or start an NP.
Our account of all possible ways of consistently
attaching ? to constituents means we must ex-
tract more than one derivation to explain transfor-
mations in G, even if we still restrict ourselves to
minimal derivations (a minimal derivation for G
is unique if and only if no source-language word
in G is unaligned). While we could enumerate
all derivations separately, it is much more effi-
cient both in time and space to represent them as a
derivation forest, as in Figure 2(b). Here, the for-
est covers all minimal derivations that correspond
to G. It is necessary to ensure that for each deriva-
tion, each unaligned item (here ?) appears only
once in the rules of that derivation, as shown in
Figure 2 (which satisfies the property). That re-
quirement will prove to be critical when we ad-
dress the problem of estimating probabilities for
our rules: if we allowed in our example to spuri-
ously generate?s in multiple successive steps of
the same derivation, we would not only represent
the transformation incorrectly, but also ?-rules
would be disproportionately represented, leading
to strongly biased estimates. We will now see how
to ensure this constraint is satisfied in our rule ex-
traction and derivation building algorithm.
963
2.3 Algorithm
The linear-time algorithm presented in GHKM is
only a particular case of the more general one we
describe here, which is used to extract all rules,
minimal and composed, induced by G. Similarly
to the GHKM algorithm, ours performs a top-
down traversal of G, but differs in the operations
it performs at each node n ? F : we must explore
all subtrees rooted at n, find all consistent ways
of attaching unaligned words of f, and build valid
derivations in accordance to these attachments.
We use a table or-dforest[x, y, c] to store OR-
nodes, in which each OR-node can be uniquely
defined by a syntactic category c and a span [x, y]
(which may cover unaligned words of f). This ta-
ble is used to prevent the same partial derivation
to be followed multiple times (the in-degrees of
OR-nodes generally become large with composed
rules). Furthermore, to avoid over-generating un-
aligned words, the root and variables in each rule
are represented with their spans. For example, in
Figure 2(b), the second and third child of the top-
most OR-node respectively span across [4-5][6-8]
and [4-6][7-8] (after constituent reordering). In
the former case, ? will eventually be realized in
an NP, and in the latter case, in a VP.
The preprocessing step consists of assigning
spans and complement spans to nodes of G, in
the first case by a bottom-up exploration of the
graph, and in the latter by a top-down traversal.
To assign complement spans, we assign the com-
plement span of any node n to each of its children,
and for each of them, add the span of the child
to the complement span of all other children. In
another traversal of G, we determine the minimal
rule extractible from each node in F .
We explore all tree fragments rooted at n by
maintaining an open and a closed queue of rules
extracted from n (qo and qc). At each step, we
pick the smallest rule in qo, and for each of its
variable nodes, try to discover new rules (?succes-
sor rules?) by means of composition with minimal
rules, until a given threshold on rule size or maxi-
mum number of rules in qc is reached. There may
be more that one successor per rule, since we must
account for all possible spans than can be assigned
to non-lexical leaves of a rule. Once a threshold is
reached, or if the open queue is empty, we connect
a new OR-node to all rules that have just been ex-
tracted from n, and add it to or-dforest. Finally,
we proceed recursively, and extract new rules from
each node at the frontier of the minimal rule rooted
at n. Once all nodes of F have been processed, the
or-dforest table contains a representation encod-
ing only valid derivations.
3 Probability models
The overall goal of our translation system is to
transform a given source-language sentence f
into an appropriate translation e in the set E
of all possible target-language sentences. In a
noisy-channel approach to SMT, we uses Bayes?
theorem and choose the English sentence e? ? E
that maximizes:4
e? = argmax
e?E
{
Pr(e) ? Pr(f |e)
}
(1)
Pr(e) is our language model, and Pr(f |e) our
translation model. In a grammatical approach to
MT, we hypothesize that syntactic information
can help produce good translation, and thus
introduce dependencies on target-language syntax
trees. The function to optimize becomes:
e? = argmax
e?E
{
Pr(e) ?
?
pi??(e)
Pr(f |pi) ?Pr(pi|e)
}
(2)
?(e) is the set of all English trees that yield the
given sentence e. Estimating Pr(pi|e) is a prob-
lem equivalent to syntactic parsing and thus is not
discussed here. Estimating Pr(f |pi) is the task of
syntax-based translation models (SBTM).
Given a rule set R, our SBTM makes the
common assumption that left-most compositions
of xRs rules ?i = r1 ? ... ? rn are independent
from one another in a given derivation ?i ? ?,
where ? is the set of all derivations constructible
from G = (pi, f ,a) using rules of R. Assuming
that ? is the set of all subtree decompositions of pi
corresponding to derivations in ?, we define the
estimate:
Pr(f |pi) =
1
|?|
?
?i??
?
rj??i
p(rhs(rj)|lhs(rj)) (3)
under the assumption:
?
rj?R:lhs(rj)=lhs(ri)
p(rhs(rj)|lhs(rj)) = 1 (4)
It is important to notice that the probability
distribution defined in Equation 3 requires a
normalization factor (|?|) in order to be tight, i.e.,
sum to 1 over all strings fi ? F that can be derived
4We denote general probability distributions with Pr(?)
and use p(?) for probabilities assigned by our models.
964
Xa
Y b
a?
b?
c?c
(!,f 1
,a 1):
X
a
Y b
b?
a?
c?c
(!,f 2
,a 2):
Figure 3: Example corpus.
from pi. A simple example suffices to demonstrate
it is not tight without normalization. Figure 3
contains a sample corpus from which four rules
can be extracted:
r1: X(a, Y(b, c)) ? a?, b?, c?
r2: X(a, Y(b, c)) ? b?, a?, c?
r3: X(a, x0:Y) ? a?, x0
r4: Y(b, c) ? b?, c?
From Equation 4, the probabilities of r3 and r4
must be 1, and those of r1 and r2 must sum to
1. Thus, the total probability mass, which is dis-
tributed across two possible output strings a?b?c?
and b?a?c?, is: p(a?b?c?|pi) + p(b?a?c?|pi) = p1 +
p3 ? p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri)).
It is relatively easy to prove that the probabil-
ities of all derivations that correspond to a given
decomposition ?i ? ? sum to 1 (the proof is omit-
ted due to constraints on space). From this prop-
erty we can immediately conclude that the model
described by Equation 3 is tight.5
We examine two estimates p(rhs(r)|lhs(r)).
The first one is the relative frequency estimator
conditioning on left hand sides:
p(rhs(r)|lhs(r)) =
f(r)
?
r?:lhs(r?)=lhs(r) f(r
?)
(5)
f(r) represents the number of times rule r oc-
curred in the derivations of the training corpus.
One of the major negative consequences of
extracting only minimal rules from a corpus is
that an estimator such as Equation 5 can become
extremely biased. This again can be observed
from Figure 3. In the minimal-rule extraction of
GHKM, only three rules are extracted from the ex-
ample corpus, i.e. rules r2, r3, and r4. Let?s as-
sume now that the triple (pi, f1,a1) is represented
99 times, and (pi, f2,a2) only once. Given a tree
pi, the model trained on that corpus can generate
the two strings a?b?c? and b?a?c? only through two
derivations, r3 ? r4 and r2, respectively. Since
all rules in that example have probability 1, and
5If each tree fragment in pi is the lhs of some rule in R,
then we have |?| = 2n, where n is the number of nodes of
the frontier set F ? G (each node is a binary choice point).
given that the normalization factor |?| is 2, both
probabilities p(a?b?c?|pi) and p(b?a?c?|pi) are 0.5.
On the other hand, if all rules are extracted and
incorporated into our relative-frequency probabil-
ity model, r1 seriously counterbalances r2 and the
probability of a?b?c? becomes: 12 ?(
99
100+1) = .995
(since it differs from .99, the estimator remains bi-
ased, but to a much lesser extent).
An alternative to the conditional model of
Equation 3 is to use a joint model conditioning on
the root node instead of the entire left hand side:
p(r|root(r)) =
f(r)
?
r?:root(r?)=root(r) f(r
?)
(6)
This can be particularly useful if no parser or
syntax-based language model is available, and we
need to rely on the translation model to penalize
ill-formed parse trees. Section 6 will describe an
empirical evaluation based on this estimate.
4 EM training
In our previous discussion of parameter estima-
tion, we did not explore the possibility that one
derivation in a forest may be much more plau-
sible than the others. If we knew which deriva-
tion in each forest was the ?true? derivation, then
we could straightforwardly collect rule counts off
those derivations. On the other hand, if we had
good rule probabilities, we could compute the
most likely (Viterbi) derivations for each training
example. This is a situation in which we can em-
ploy EM training, starting with uniform rule prob-
abilities. For each training example, we would like
to: (1) score each derivation ?i as a product of the
probabilities of the rules it contains, (2) compute
a conditional probability pi for each derivation ?i
(conditioned on the observed training pair) by nor-
malizing those scores to add to 1, and (3) collect
weighted counts for each rule in each ?i, where
the weight is pi. We can then normalize the counts
to get refined probabilities, and iterate; the corpus
likelihood is guaranteed to improve with each it-
eration. While it is infeasible to enumerate the
millions of derivations in each forest, Graehl and
Knight (2004) demonstrate an efficient algorithm.
They also analyze how to train arbitrary tree trans-
ducers into two steps. The first step is to build a
derivation forest for each training example, where
the forest contains those derivations licensed by
the (already supplied) transducer?s rules. The sec-
ond step employs EM on those derivation forests,
running in time proportional to the size of the
965
Best minimal-rule derivation (Cm) p(r)
(a) S(x0:NP-C x1:VP x2:.) ? x0 x1 x2 .845
(b) NP-C(x0:NPB) ? x0 .82
(c) NPB(DT(the) x0:NNS) ? x0 .507
(d) NNS(gunmen) ??K .559
(e) VP(VBD(were) x0:VP-C) ? x0 .434
(f) VP-C(x0:VBN x1:PP) ? x1 x0 .374
(g) PP(x0:IN x1:NP-C) ? x0 x1 .64
(h) IN(by) ?? .0067
(i) NP-C(x0:NPB) ? x0 .82
(j) NPB(DT(the) x0:NN) ? x0 .586
(k) NN(police) ?f? .0429
(l) VBN(killed) ??? .0072
(m) .(.) ? . .981
.
 
The
gunm
enw
ere
killed
by
the
polic
e.
DT
VBD
VBN
DT
NN
NP
PP
VP-C
VPS
NNS
IN
NP
.
!"
#$
%
&'
Best composed-rule derivation (C4) p(r)
(o) S(NP-C(NPB(DT(the) NNS(gunmen))) x0:VP .(.)) ??K x0 . 1
(p) VP(VBD(were) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ?? x1 x0 0.00724
(q) NP-C(NPB(DT(the) NN(police))) ?f? 0.173
(r) VBN(killed) ??? 0.00719
Figure 4: Two most probable derivations for the graph on the right: the top table restricted to minimal rules; the bottom one,
much more probable, using a large set of composed rules. Note: the derivations are constrained on the (pi, f ,a) triple, and thus
include some non-literal translations with relatively low probabilities (e.g. killed, which is more commonly translated as{?).
rule nb. of nb. of deriv- EM-
set rules nodes time time
Cm 4M 192M 2 h. 4 h.
C3 142M 1255M 52 h. 34 h.
C4 254M 2274M 134 h. 60 h.
Table 2: Rules and derivation nodes for a 54M-word, 1.95M
sentence pair English-Chinese corpus, and time to build
derivations (on 10 cluster nodes) and run 50 EM iterations.
forests. We only need to borrow the second step
for our present purposes, as we construct our own
derivation forests when we acquire our rule set.
A major challenge is to scale up this EM train-
ing to large data sets. We have been able to run
EM for 50 iterations on our Chinese-English 54-
million word corpus. The derivation forests for
this corpus contain 2.2 billion nodes; the largest
forest contains 1.1 million nodes. The outcome
is to assign probabilities to over 254 million rules.
Our EM runs with either lhs normalization or lhs-
root normalization. In the former case, each lhs
has an average of three corresponding rhs?s that
compete with each other for probability mass.
5 Model coverage
We now present some examples illustrating the
benefit of composed rules. We trained three
p(rhs(ri)|lhs(ri)) models on a 54 million-word
English-Chinese parallel corpus (Table 2): the first
one (Cm) with only minimal rules, and the two
others (C3 and C4) additionally considering com-
posed rules with no more than three, respectively
four, internal nodes in lhs(ri). We evaluated these
models on a section of the NIST 2002 evaluation
corpus, for which we built derivation forests and
lhs: S(x0:NP-C VP(x1:VBD x2:NP-C) x3:.)
corpus rhsi p(rhsi|lhs)
Chinese x1 x0 x2 x3 .3681
(minimal) x0 x1 , x3 x2 .0357
x2 , x0 x1 x3 .0287
x0 x1 , x3 x2 . .0267
Chinese x0 x1 x2 x3 .9047
(composed) x0 x1 , x2 x3 .016
x0 , x1 x2 x3 .0083
x0 x1 ? x2 x3 .0072
Arabic x1 x0 x2 x3 .5874
(composed) x0 x1 x2 x3 .4027
x1 x2 x0 x3 .0077
x1 x0 x2 " x3 .0001
Table 3: Our model transforms English subject-verb-object
(SVO) structures into Chinese SVO and into Arabic VSO.
With only minimal rules, Chinese VSO is wrongly preferred.
extracted the most probable one (Viterbi) for each
sentence pair (based on an automatic alignment
produced by GIZA). We noticed in general that
Viterbi derivations according to C4 make exten-
sive usage of composed rules, as it is the case in
the example in Figure 4. It shows the best deriva-
tion according to Cm and C4 on the unseen (pi,f,a)
triple displayed on the right. The second deriva-
tion (log p = ?11.6) is much more probable than
the minimal one (log p = ?17.7). In the case
of Cm, we can see that many small rules must be
applied to explain the transformation, and at each
step, the decision regarding the re-ordering of con-
stituents is made with little syntactic context. For
example, from the perspective of a decoder, the
word by is immediately transformed into a prepo-
sition (IN), but it is in general useful to know
which particular function word is present in the
sentence to motivate good re-orderings in the up-
966
lhs1: NP-C(x0:NPB PP(IN(of) x1:NP-C)) (NP-of-NP)
lhs2: PP(IN(of) NP-C(x0:NPB PP(IN(of) NP-C(x1:NPB x2:VP)))) (of-NP-of-NP-VP)
lhs3: VP(VBD(said) SBAR-C(IN(that) x0:S-C)) (said-that-S)
lhs4: SBAR(WHADVP(WRB(when)) S-C(x0:NP-C VP(VBP(are) x1:VP-C))) (when-NP-are-VP)
rhs1i p(rhs1i|lhs1) rhs2i p(rhs2i|lhs2) rhs3i p(rhs3i|lhs3) rhs4i p(rhs4i|lhs4)
x1 x0 .54 x2 ? x1 ? x0 .6754 ? , x0 .6062 ( x1 x0 ? .6618
x0 x1 .2351 ( x2 ? x1 ? x0 .035 ? x0 .1073 S x1 x0 ? .0724
x1 ? x0 .0334 x2 ? x1 ? x0 , .0263 h: , x0 .0591 ( x1 x0 ? , .0579
x1 x0 ? .026 x2 ? x1 ? x0 	 .0116 ? ? , x0 .0234 , ( x1 x0 ? .0289
Table 4: Translation probabilities promote linguistically motivated constituent re-orderings (for lhs1 and lhs2), and enable
non-constituent (lhs3) and non-contiguous (lhs4) phrasal translations.
per levels of the tree. A rule like (e) is particu-
larly unfortunate, since it allows the word were to
be added without any other evidence that the VP
should be in passive voice. On the other hand, the
composed-rule derivation of C4 incorporates more
linguistic evidence in its rules, and re-orderings
are motivated by more syntactic context. Rule
(p) is particularly appropriate to create a passive
VP construct, since it expects a Chinese passive
marker (?), an NP-C, and a verb in its rhs, and
creates the were ... by construction at once in the
left hand side.
5.1 Syntactic translation tables
We evaluate the promise of our SBTM by analyz-
ing instances of translation tables (t-table). Table 3
shows how a particular form of SVO construc-
tion is transformed into Chinese, which is also an
SVO language. While the t-table for Chinese com-
posed rules clearly gives good estimates for the
?correct? x0 x1 ordering (p = .9), i.e. subject be-
fore verb, the t-table for minimal rules unreason-
ably gives preference to verb-subject ordering (x1
x0, p = .37), because the most probable transfor-
mation (x0 x1) does not correspond to a minimal
rule. We obtain different results with Arabic, an
VSO language, and our model effectively learns
to move the subject after the verb (p = .59).
lhs1 in Table 4 shows that our model is able
to learn large-scale constituent re-orderings, such
as re-ordering NPs in a NP-of-NP construction,
and put the modifier first as it is more commonly
the case in Chinese (p = .54). If more syntac-
tic context is available as in lhs2, our model
provides much sharper estimates, and appropri-
ately reverses the order of three constituents with
high probability (p = .68), inserting modifiers first
(possessive markers? are needed here for better
syntactic disambiguation).
A limitation of earlier syntax-based systems is
their poor handling of non-constituent phrases.
Table 4 shows that our model can learn rules for
such phrases, e.g., said that (lhs3). While the that
has no direct translation, our model effectively
learns to separate? (said) from the relative clause
with a comma, which is common in Chinese.
Another promising prospect of our model seems
to lie in its ability to handle non-contiguous
phrases, a feature that state of the art systems
such as (Och and Ney, 2004) do not incorpo-
rate. The when-NP-are-VP construction of lhs4
presents such a case. Our model identifies that are
needs to be deleted, that when translates into the
phrase( ...?, and that the NP needs to be moved
after the VP in Chinese (p = .66).
6 Empirical evaluation
The task of our decoder is to find the most likely
English tree pi that maximizes all models involved
in Equation 2. Since xRs rules can be converted to
context-free productions by increasing the number
of non-terminals, we implemented our decoder as
a standard CKY parser with beam search. Its rule
binarization is described in (Zhang et al, 2006).
We compare our syntax-based system against
an implementation of the alignment template
(AlTemp) approach to MT (Och and Ney, 2004),
which is widely considered to represent the state
of the art in the field. We registered both systems
in the NIST 2005 evaluation; results are presented
in Table 5. With a difference of 6.4 BLEU points
for both language pairs, we consider the results
of our syntax-based system particularly promis-
ing, since these are the highest scores to date that
we know of using linguistic syntactic transforma-
tions. Also, on the one hand, our AlTemp sys-
tem represents quite mature technology, and in-
corporates highly tuned model parameters. On
the other hand, our syntax decoder is still work in
progress: only one model was used during search,
i.e., the EM-trained root-normalized SBTM, and
as yet no language model is incorporated in the
search (whereas the search in the AlTemp sys-
tem uses two phrase-based translation models and
967
Syntactic AlTemp
Arabic-to-English 40.2 46.6
Chinese-to-English 24.3 30.7
Table 5: BLEU-4 scores for the 2005 NIST test set.
Cm C3 C4
Chinese-to-English 24.47 27.42 28.1
Table 6: BLEU-4 scores for the 2002 NIST test set, with rules
of increasing sizes.
12 other feature functions). Furthermore, our de-
coder doesn?t incorporate any syntax-based lan-
guage model, and admittedly our ability to penal-
ize ill-formed parse trees is still limited.
Finally, we evaluated our system on the NIST-
02 test set with the three different rule sets (see
Table 6). The performance with our largest rule
set represents a 3.63 BLEU point increase (14.8%
relative) compared to using only minimal rules,
which indicates positive prospects for using even
larger rules. While our rule inference algorithm
scales to higher thresholds, one important area of
future work will be the improvement of our de-
coder, conjointly with analyses of the impact in
terms of BLEU of contextually richer rules.
7 Related work
Similarly to (Poutsma, 2000; Wu, 1997; Yamada
and Knight, 2001; Chiang, 2005), the rules dis-
cussed in this paper are equivalent to productions
of synchronous tree substitution grammars. We
believe that our tree-to-string model has several
advantages over tree-to-tree transformations such
as the ones acquired by Poutsma (2000). While
tree-to-tree grammars are richer formalisms that
provide the potential benefit of rules that are lin-
guistically better motivated, modeling the syntax
of both languages comes as an extra cost, and it
is admittedly more helpful to focus our syntac-
tic modeling effort on the target language (e.g.,
English) in cases where it has syntactic resources
(parsers and treebanks) that are considerably more
available than for the source language. Further-
more, we think there is, overall, less benefit in
modeling the syntax of the source language, since
the input sentence is fixed during decoding and is
generally already grammatical.
With the notable exception of Poutsma, most
related works rely on models that are restricted
to synchronous context-free grammars (SCFG).
While the state-of-the-art hierarchical SMT sys-
tem (Chiang, 2005) performs well despite strin-
gent constraints imposed on its context-free gram-
mar, we believe its main advantage lies in its
ability to extract hierarchical rules across phrasal
boundaries. Context-free grammars (such as Penn
Treebank and Chiang?s grammars) make indepen-
dence assumptions that are arguably often unrea-
sonable, but as our work suggests, relaxations
of these assumptions by using contextually richer
rules results in translations of increasing quality.
We believe it will be beneficial to account for this
finding in future work in syntax-based SMT and in
efforts to improve upon (Chiang, 2005).
8 Conclusions
In this paper, we developed probability models for
the multi-level transfer rules presented in (Galley
et al, 2004), showed how to acquire larger rules
that crucially condition on more syntactic context,
and how to pack multiple derivations, including
interpretations of unaligned words, into derivation
forests. We presented some theoretical arguments
for not limiting extraction to minimal rules, val-
idated them on concrete examples, and presented
experiments showing that contextually richer rules
provide a 3.63 BLEU point increase over the min-
imal rules of (Galley et al, 2004).
Acknowledgments
We would like to thank anonymous review-
ers for their helpful comments and suggestions.
This work was partially supported under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
H. Fox. 2002. Phrasal cohesion and statistical machine trans-
lation. In Proc. of EMNLP, pages 304?311.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In Proc. of HLT/NAACL-04.
J. Graehl and K. Knight. 2004. Training tree transducers. In
Proc. of HLT/NAACL-04, pages 105?112.
F. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
A. Poutsma. 2000. Data-oriented translation. In Proc. of
COLING, pages 635?641.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proc. of ACL, pages 523?530.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Syn-
chronous binarization for machine translation. In Proc. of
HLT/NAACL.
968
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 455?460,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Two Easy Improvements to Lexical Weighting
David Chiang and Steve DeNeefe and Michael Pust
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{chiang,sdeneefe,pust}@isi.edu
Abstract
We introduce two simple improvements to the
lexical weighting features of Koehn, Och, and
Marcu  (2003)  for  machine  translation: one
which  smooths  the  probability  of  translating
word f to word e by simplifying English mor-
phology, and one which conditions it  on the
kind of training data that f and e co-occurred
in. These new variations lead to improvements
of up to +0.8 BLEU, with an average improve-
ment of +0.6 BLEU across two language pairs,
two genres, and two translation systems.
1 Introduction
Lexical weighting features (Koehn et al, 2003) es-
timate the probability of a phrase pair or translation
rule word-by-word. In this paper, we introduce two
simple improvements to these features: one which
smooths  the  probability  of  translating  word f to
word e using English morphology, and one which
conditions it on the kind of training data that f and
e co-occurred in. These new variations lead to im-
provements of up to+0.8BLEU, with an average im-
provement of +0.6BLEU across two language pairs,
two genres, and two translation systems.
2 Background
Since there  are  slight  variations  in  how the  lexi-
cal weighting features are computed, we begin by
defining the baseline lexical weighting features. If
f = f1 ? ? ? fn and e = e1 ? ? ? em are a training sentence
pair, let ai (1 ? i ? n) be the (possibly empty) set of
positions in f that ei is aligned to.
First, compute a word translation table from the
word-aligned parallel text: for each sentence pair and
each i, let
c( f j, ei)? c( f j, ei) +
1
|ai|
for j ? ai (1)
c(NULL, ei)? c(NULL, ei) + 1 if |ai| = 0 (2)
Then
t(e | f ) = c( f , e)?
e c( f , e)
(3)
where f can be NULL.
Second, during phrase-pair extraction, store with
each phrase pair the alignments between the words
in the phrase pair. If it is observed with more than
one word alignment pattern, store the most frequent
pattern.
Third, for each phrase pair ( f? , e?, a), compute
t(e? | f? ) =
|e?|
?
i=1
?
?
?
?
?
?
?
?
?
?
?
1
|ai|
?
j?ai
t(e?i | f? j) if |ai| > 0
t(e?i | NULL) otherwise
(4)
This generalizes to synchronous CFG rules in the ob-
vious way.
Similarly, compute the reverse probability t( f? | e?).
Then add two new model features
? log t(e? | f? ) and ? log t( f? | e?)
455
translation
feature (7) (8)
small LM 26.7 24.3
large LM 31.4 28.2
? log t(e? | f? ) 9.3 9.9
? log t( f? | e?) 5.8 6.3
Table 1: Although the language models prefer translation
(8), which translates ?? and ?? as singular nouns, the
lexical weighting features prefer translation (7), which in-
correctly generates plural nouns. All features are negative
log-probabilities, so lower numbers indicate preference.
3 Morphological smoothing
Consider the following example Chinese sentence:
(5) ???
W?n Ji?b?o
Wen Jiabao
??
bi?osh?
said
,
,
,
????
K?t?d?w?
C?te d?Ivoire
?
sh?
is
??
Zh?nggu?
China
?
z?i
in
??
F?izh?u
Africa
?
de
?s
?
h?o
good
??
p?ngy?u
friend
,
,
,
?
h?o
good
??
hu?b?n
partner
.
.
.
(6) Human: Wen Jiabao said that C?te d?Ivoire is
a good friend and a good partner of China?s in
Africa.
(7) MT (baseline): Wen  Jiabao  said  that  Cote
d?Ivoire  is  China?s  good friends, and  good
partners in Africa.
(8) MT (better):Wen Jiabao said that Cote d?Ivoire
is  China?s  good friend and  good partner in
Africa.
The baseline machine translation (7) incorrectly gen-
erates plural nouns. Even though the language mod-
els (LMs) prefer singular nouns, the lexical weight-
ing features prefer plural nouns (Table 1).
1
The reason for this is that the Chinese words do not
have any marking for number. Therefore the infor-
mation needed to mark friend and partner for num-
ber must come from the context. The LMs are able
to capture this context: the 5-gram is China?s good
1
The presence of an extra comma in translation (7) affects
the LM scores only slightly; removing the comma would make
them 26.4 and 32.0.
f e t(e | f ) t( f | e) tm(e | f ) tm( f | e)
?? friends 0.44 0.44 0.47 0.48
?? friend 0.21 0.58 0.19 0.48
?? partners 0.44 0.60 0.40 0.53
?? partner 0.13 0.40 0.17 0.53
Table 2: The morphologically-smoothed lexical weight-
ing features weaken the preference for singular or plural
translations, with the exception of t(friends | ??).
friend is observed in our large LM, and the 4-gram
China?s good friend in our small LM, but China?s
good friends is not observed in either LM. Likewise,
the 5-grams good friend and good partner and good
friends and good partners are both observed in our
LMs, but neither good friend and good partners nor
good friends and good partner is.
By contrast, the lexical weighting tables (Table 2,
columns 3?4), which ignore context, have a strong
preference for plural translations, except in the case
of t(?? | friend). Therefore  we hypothesize  that,
for Chinese-English translation, we should weaken
the lexical weighting features? morphological pref-
erences so that more contextual features can do their
work.
Running a morphological stemmer (Porter, 1980)
on  the  English  side  of  the  parallel  data  gives  a
three-way parallel text: for each sentence, we have
French f, English e, and stemmed English e?. We can
then build two word translation tables, t(e? | f ) and
t(e | e?), and form their product
tm(e | f ) =
?
e?
t(e? | f )t(e | e?) (9)
Similarly, we can compute tm( f | e) in the opposite
direction.
2
(See Table 2, columns 5?6.) These tables
can then be extended to phrase pairs or synchronous
CFG rules as before and added as two new features
of the model:
? log tm(e? | f? ) and ? log tm( f? | e?)
The feature tm(e? | f? ) does still prefer certain word-
forms, as can be seen in Table 2. But because e is
generated from e? and not from f , we are protected
from the situation where a rare f leads to poor esti-
mates for the e.
2
Since the Porter stemmer is deterministic, we always have
t(e? | e) = 1.0, so that tm( f | e) = t( f | e?), as seen in the last
column of Table 2.
456
When  we  applied  an  analogous  approach  to
Arabic-English translation, stemming  both  Arabic
and English, we generated very large lexicon tables,
but saw no statistically significant change in BLEU.
Perhaps this is  not surprising, because  in  Arabic-
English translation (unlike Chinese-English transla-
tion), the source language is morphologically richer
than the target language. So we may benefit from fea-
tures that preserve this information, while smoothing
over morphological differences blurs important dis-
tinctions.
4 Conditioning on provenance
Typical machine translation systems are trained on
a fixed set of training data ranging over a variety of
genres, and if the genre of an input sentence is known
in advance, it is usually advantageous to use model
parameters tuned for that genre.
Consider the following Arabic sentence, from a
weblog (words written left-to-right):
(10) ????
wlEl
perhaps
???
h*A
this
???
AHd
one
???
Ahm
main
??????
Alfrwq
differences
???
byn
between
???
Swr
images
?????
AnZmp
systems
?????
AlHkm
ruling
????????
AlmqtrHp
proposed
.
.
.
(11) Human: Perhaps this is one of the most impor-
tant differences between the images of the pro-
posed ruling systems.
(12) MT (baseline): This may be one of the most
important differences between pictures of the
proposed ruling regimes.
(13) MT (better): Perhaps this is one of the most im-
portant differences between the images of the
proposed regimes.
The Arabic word ???? can be translated asmay or per-
haps (among others), with the latter more common
according to t(e | f ), as shown in Table 3. But some
genres favor perhaps more or less strongly. Thus,
both translations (12) and (13) are good, but the lat-
ter uses a slightly more informal register appropriate
to the genre.
Following Matsoukas et al (2009), we assign each
training sentence pair a set of binary features which
we call s-features:
t(e | f ) ts(e | f )
f e ? nw web bn un
???? may 0.13 0.12 0.16 0.09 0.13
???? perhaps 0.20 0.23 0.32 0.42 0.19
Table 3: Different genres have different preferences for
word translations. Key: nw = newswire, web = Web, bn =
broadcast news, un = United Nations proceedings.
? Whether the sentence pair came from a particu-
lar genre, for example, newswire or web
? Whether the sentence pair came from a particu-
lar collection, for example, FBIS or UN
Matsoukas et  al. (2009)  use these s-features  to
compute  weights  for  each  training  sentence  pair,
which are in turn used for computing various model
features. They found that the sentence-level weights
were most helpful for computing the lexical weight-
ing  features  (p.c.). The  mapping  from  s-features
to  sentence  weights  was  chosen  to  optimize  ex-
pected TER on held-out data. A drawback of this
method is that we must now learn the mapping from
s-features to sentence-weights and then the model
feature weights. Therefore, we tried an alternative
that incorporates s-features into the model itself.
For each s-feature s, we compute new word trans-
lation tables ts(e | f ) and ts( f | e) estimated from
only those sentence pairsf on which s fires, and ex-
tend them to phrases/rules as before. The idea is to
use these probabilities as new features in the model.
However, two  challenges  arise: first, many  word
pairs are unseen for a given s, resulting in zero or
undefined probabilities; second, this adds many new
features for each rule, which requires a lot of space.
To address the problem of unseen word pairs, we
use Witten-Bell smoothing (Witten and Bell, 1991):
t?s(e | f ) = ? f sts(e | f ) + (1 ? ? f s)t(e | f ) (14)
? f s =
c( f , s)
c( f , s) + d( f , s)
(15)
where c( f , s) is the number of times f has been ob-
served in sentences with s-feature s, and d( f , s) is the
number of e types observed aligned to f in sentences
with s-feature s.
For each s-feature s, we add two model features
? log t?s(e? | f? )
t(e? | f? )
and ? log t?s( f? | e?)
t( f? | e?)
457
Arabic-English Chinese-English
newswire web newswire web
system features Dev Test Dev Test Dev Test Dev Test
string-to-string baseline 47.1 43.8 37.1 38.4 28.7 26.0 23.2 25.9
full
2
47.7 44.2
?
37.4 39.0 29.5 26.8 23.8 26.3
string-to-tree baseline 47.3 43.6 37.7 39.6 29.2 26.4 23.0 26.0
full 47.7 44.3 38.3 40.2 29.8 27.1 23.4 26.6
Table 4: Our variations on lexical weighting improve translation quality significantly across 16 different test conditions.
All improvements are significant at the p < 0.01 level, except where marked with an asterisk (?), indicating p < 0.05.
In order to address the space problem, we use the
following heuristic: for any given rule, if the absolute
value of one of these features is less than log 2, we
discard it for that rule.
5 Experiments
Setup We  tested  these  features  on  two  ma-
chine  translation  systems: a  hierarchical  phrase-
based (string-to-string) system (Chiang, 2005) and
a syntax-based (string-to-tree) system (Galley et al,
2004; Galley et al, 2006). For Arabic-English trans-
lation, both systems were trained on 190+220 mil-
lion words of parallel data; for Chinese-English, the
string-to-string system was trained on 240+260 mil-
lion words of parallel data, and the string-to-tree sys-
tem, 58+65 million words. Both used two language
models, one trained on the combined English sides
of the Arabic-English and Chinese-English data, and
one trained on 4 billion words of English data.
The baseline string-to-string system already incor-
porates some simple provenance features: for each
s-feature s, there is a feature P(s | rule). Both base-
line also include a variety of other features (Chiang
et al, 2008; Chiang et al, 2009; Chiang, 2010).
Both systems were trained using MIRA (Cram-
mer et al, 2006; Watanabe et al, 2007; Chiang et al,
2008) on a held-out set, then tested on two more sets
(Dev and Test) disjoint from the data used for rule
extraction and for  MIRA training. These datasets
have roughly 1000?3000 sentences (30,000?70,000
words) and are drawn from test sets from the NIST
MT evaluation and development sets from the GALE
program.
Individual  tests We  first  tested  morphological
smoothing  using  the  string-to-string  system  on
Chinese-English  translation. The  morphologically
smoothed system generated the improved translation
(8) above, and generally gave a small improvement:
task features Dev
Chi-Eng nw baseline 28.7
morph 29.1
We then tested the provenance-conditioned fea-
tures on both Arabic-English and Chinese-English,
again using the string-to-string system:
task features Dev
Ara-Eng nw baseline 47.1
(Matsoukas et al, 2009) 47.3
provenance
2
47.7
Chi-Eng nw baseline 28.7
provenance
2
29.4
The  translations  (12)  and  (13)  come  from  the
Arabic-English baseline and provenance systems.
For Arabic-English, we also compared against lex-
ical  weighting  features  that  use  sentence  weights
kindly provided to us by Matsoukas et al Our fea-
tures performed better, although it should be noted
that those sentence weights had been optimized for
a different translation model.
Combined  tests Finally, we  tested  the  features
across a wider range of tasks. For Chinese-English
translation, we  combined  the  morphologically-
smoothed  and  provenance-conditioned  lexical
weighting  features; for  Arabic-English, we  con-
tinued  to  use  only  the  provenance-conditioned
features. We  tested  using  both  systems, and  on
both  newswire  and  web  genres. The  results  are
shown in Table 4. The features produce statistically
significant improvements across all 16 conditions.
2
In these systems, an error crippled the t( f | e), tm( f | e), and
ts( f | e) features. Time did not permit rerunning all of these sys-
tems with the error fixed, but partial results suggest that it did
not have a significant impact.
458
-0.4-0.3-0.2-0.1 0
 0.1 0.2 0.3 0.4 0.5
-0.8 -0.6 -0.4 -0.2  0  0.2  0.4  0.6  0.8
Web
Newswire
bc bn LDC2005T06 NameEntityLDC2006E24LDC2006E92LDC2006G05LDC2007E08
LDC2007E101LDC2007E103 LDC2008G05lexiconng nwNewsExplorer UNweb
wl
Figure 1: Feature  weights  for  provenance-conditioned  features: string-to-string, Chinese-English, web  versus
newswire. A higher weight indicates a more useful source of information, while a negative weight indicates a less
useful or possibly problematic source. For clarity, only selected points are labeled. The diagonal line indicates where
the two weights would be equal relative to the original t(e | f ) feature weight.
Figure 1 shows the feature weights obtained for
the provenance-conditioned features ts( f | e) in the
string-to-string Chinese-English system, trained on
newswire and web data. On the diagonal are cor-
pora that were equally useful in either genre. Surpris-
ingly, the UN data received strong positive weights,
indicating usefulness in both genres. Two lists  of
named entities received large weights: the LDC list
(LDC2005T34)  in  the  positive  direction  and  the
NewsExplorer  list  in  the  negative  direction, sug-
gesting  that  there  are  noisy  entries  in  the  latter.
The corpus LDC2007E08, which contains parallel
data mined from comparable corpora (Munteanu and
Marcu, 2005), received strong negative weights.
Off the diagonal are corpora favored in only one
genre or the other: above, we see that the wl (we-
blog)  and ng (newsgroup)  genres  are  more help-
ful for web translation, as expected (although web
oddly seems less helpful), as well as LDC2006G05
(LDC/FBIS/NVTC Parallel Text V2.0). Below are
corpora  more  helpful  for  newswire  translation,
like LDC2005T06 (Chinese News Translation Text
Part 1).
6 Conclusion
Many  different  approaches  to  morphology  and
provenance in machine translation are possible. We
have chosen to implement our approach as exten-
sions  to  lexical  weighting  (Koehn  et  al., 2003),
which is nearly ubiquitous, because it is defined at
the level of word alignments. For this reason, the
features we have introduced should be easily ap-
plicable to a wide range of phrase-based, hierarchi-
cal phrase-based, and syntax-based systems. While
the improvements obtained using them are not enor-
mous, we have demonstrated that they help signif-
icantly across many different conditions, and over
very strong baselines. We therefore fully expect that
these  new  features  would  yield  similar  improve-
ments in other systems as well.
Acknowledgements
We would like to thank Spyros Matsoukas and col-
leagues at BBN for providing their sentence-level
weights  and  important  insights  into  their  corpus-
weighting work. This work was supported in part by
DARPA contract HR0011-06-C-0022 under subcon-
tract to BBN Technologies.
459
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of  syntactic  and struc-
tural translation features. In Proc. EMNLP 2008, pages
224?233.
David  Chiang, Kevin  Knight, and  Wei  Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL HLT, pages 218?226.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005,
pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, pages 1443?1452.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
HLT-NAACL 2004, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe,Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich
syntactic translation models. In Proc. COLING-ACL
2006, pages 961?968.
Philipp  Koehn, Franz Josef  Och, and  Daniel  Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127?133.
Spyros  Matsoukas, Antti-Veikko I.  Rosti, and  Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proc. EMNLP 2009,
pages 708?717.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007, pages 764?773.
Ian H.  Witten  and  Timothy C.  Bell. 1991. The
zero-frequency problem: Estimating the probabilities
of novel events in adaptive text compression. IEEE
Trans. Information Theory, 37(4):1085?1094.
460
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 10?18,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Decoder for
Probabilistic Synchronous Tree Insertion Grammars
Steve DeNeefe ? and Kevin Knight ?
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292, USA
{sdeneefe,knight}@isi.edu
Heiko Vogler ?
Department of Computer Science
Technische Universita?t Dresden
D-01062 Dresden
Heiko.Vogler@tu-dresden.de
Abstract
Synchronous tree insertion grammars
(STIG) are formal models for syntax-
based machine translation. We formal-
ize a decoder for probabilistic STIG; the
decoder transforms every source-language
string into a target-language tree and cal-
culates the probability of this transforma-
tion.
1 Introduction
Tree adjoining grammars (TAG) were invented in
(Joshi et al 1975) in order to better character-
ize the string sets of natural languages1. One of
TAG?s important features is the ability to introduce
two related syntactic units in a single rule, then
push those two units arbitrarily far apart in sub-
sequent derivation steps. For machine translation
(MT) between two natural languages, each being
generated by a TAG, the derivations of the two
TAG may be synchronized (Abeille et al, 1990;
Shieber and Shabes, 1990) in the spirit of syntax-
directed transductions (Lewis and Stearns, 1968);
this results in synchronous TAG (STAG). Recently,
in (Nesson et al, 2005, 2006) probabilistic syn-
chronous tree insertion grammars (pSTIG) were
discussed as model of MT; a tree insertion gram-
mar is a particular TAG in which the parsing prob-
lem is solvable in cubic-time (Schabes and Wa-
ters, 1994). In (DeNeefe, 2009; DeNeefe and
Knight 2009) a decoder for pSTIG has been pro-
posed which transforms source-language strings
into (modifications of) derivation trees of the
pSTIG. Nowadays, large-scale linguistic STAG
rule bases are available.
In an independent tradition, the automata-
theoretic investigation of the translation of trees
? financially supported by NSF STAGES project, grant
#IIS-0908532.
? financially supported by DFG VO 1011/5-1.
1see (Joshi and Shabes, 1997) for a survey
led to the rich theory of tree transducers (Ge?cseg
and Steinby, 1984, 1997). Roughly speaking, a
tree transducer is a finite term rewriting system. If
each rewrite rule carries a probablity or, in gen-
eral, a weight from some semiring, then they are
weighted tree transducers (Maletti, 2006, 2006a;
Fu?lo?p and Vogler, 2009). Such weighted tree
transducers have also been used for the specifi-
cation of MT of natural languages (Yamada and
Knight, 2001; Knight and Graehl, 2005; Graehl et
al., 2008; Knight and May 2009).
Martin and Vere (1970) and Schreiber (1975)
established the first connections between the two
traditions; also Shieber (2004, 2006) and Maletti
(2008, 2010) investigated their relationship.
The problem addressed in this paper is the
decoding of source-language strings into target-
language trees where the transformation is de-
scribed by a pSTIG. Currently, this decoding re-
quires two steps: first, every source string is
translated into a derivation tree of the underly-
ing pSTIG (DeNeefe, 2009; DeNeefe and Knight
2009), and second, the derivation tree is trans-
formed into the target tree using an embedded tree
transducer (Shieber, 2006). We propose a trans-
ducer model, called a bottom-up tree adjoining
transducer, which performs this decoding in a sin-
gle step and, simultaneously, computes the prob-
abilities of its derivations. As a basis of our ap-
proach, we present a formal definition of pSTIG.
2 Preliminaries
For two sets ? and A, we let U?(A) be the set of
all (unranked) trees over ? in which also elements
of A may label leaves. We abbreviate U?(?) by
U?. We denote the set of positions, leaves, and
non-leaves of ? ? U? by pos(?) ? N?, lv(?), and
nlv(?), resp., where ? denotes the root of ? and
w.i denotes the ith child of position w; nlv(?) =
pos(?) \ lv(?). For a position w ? pos(?), the la-
bel of ? at w (resp., subtree of ? at w) is denoted
10
by ?(w) (resp., ?|w). If additionally ? ? U?(A),
then ?[?]w denotes the tree which is obtained from
? by replacing its subtree at w by ?. For every
? ? ? ?A, the set pos?(?) is the set of all those
positions w ? pos(?) such that ?(w) ? ?. Simi-
larly, we can define lv?(?) and nlv?(?). The yield
of ? is the sequence yield(?) ? (? ? A)? of sym-
bols that label the leaves from left to right.
If we associate with ? ? ? a rank k ? N, then
we require that in every tree ? ? U?(A) every ?-
labeled position has exactly k children.
3 Probabilistic STAG and STIG
First we will define probabilistic STAG, and sec-
ond, as a special case, probabilistic STIG.
Let N and T be two disjoint sets of, resp., non-
terminals and terminals. A substitution rule r is a
tuple (?s, ?t, V,W, P radj) where
? ?s, ?t ? UN (T ) (source and target tree) and
|lvN (?s)| = |lvN (?t)|,
? V ? lvN (?s)?lvN (?t) (substitution sites), V
is a one-to-one relation, and |V | = |lvN (?s)|,
? W ? nlvN (?s)?nlvN (?t) (potential adjoin-
ing sites), and
? P radj : W ? [0, 1] (adjoining probability).
An auxiliary rule r is a tuple (?s, ?t, V,W, ?, P radj)
where ?s, ?t,W , and P radj are defined as above and
? V is defined as above except that |V | =
|lvN (?s)| ? 1 and
? ? = (?s, ?t) ? lvN (?s)? lvN (?t) and neither
?s nor ?t occurs in any element of V ; more-
over, ?s(?) = ?s(?s) and ?t(?) = ?t(?t), and
?s 6= ? 6= ?t; the node ?s (and ?t) is called
the foot-node of ?s (resp., ?t).
An (elementary) rule is either a substitution rule
or an auxiliary rule. The root-category of a rule r
is the tuple (?s(?), ?t(?)), denoted by rc(r).
A probabilistic synchronous tree ad-
joining grammar (pSTAG) is a tuple
G = (N,T, (Ss, St),S,A, P ) such that N
and T are two disjoint sets (resp., of nonterminals
and terminals), (Ss, St) ? N?N (start nontermi-
nal), S and A are finite sets of, resp., substitution
rules and auxiliary rules, and P : S ? A ? [0, 1]
such that for every (A,B) ? N ?N ,
?
r?S
rc(r)=(A,B)
P (r) = 1 and
?
r?A
rc(r)=(A,B)
P (r) = 1
assuming that in each case the number of sum-
mands is not zero. In the following, let G always
denote an arbitrary pSTAG.
Ss
? A? A
?
a
r1??
St
B
?
B? ?a
P (r1) = 1
P r1adj(a) = .9
A
A ?
b,c
?
r2??
B
B
?
B
b
c ?
P (r2) = .4
P r2adj(b) = .2
P r2adj(c) = .6
A
A A
?
d
? e
r3??
B
? B
d,e
?
P (r3) = .6
P r3adj(d) = .3
P r3adj(e) = .8
A
?
r4??
B
?
P (r4) = .1
A
?
r5??
B
?
P (r5) = .9
Figure 1: The running example pSTAG G.
In Fig. 1 we show the rules of our running ex-
ample pSTAG, where the capital Roman letters are
the nonterminals and the small Greek letters are
the terminals. The substitution site (in rule r1) is
indicated by ?, and the potential adjoining sites are
denoted2 by a, b, c, d, and e. For instance, in for-
mal notation the rules r1 and r2 are written as fol-
lows:
r1 = (Ss(?,A,A(?)), St(B(?), B, ?), {?}, {a}, P r1adj)
where ? = (2, 2) and a = (3, 1), and
r2 = (A(A, ?), B(B(?), B), ?, {b, c}, ?, P r2adj)
where b = (?, ?), c = (?, 1), and ? = (1, 2).
In the derivation relation of G we will distin-
guish four types of steps:
1. substitution of a rule at a substitution site
(substitution),
2. deciding to turn a potential adjoining site into
an activated adjoining site (activation),
3. deciding to drop a potential adjoining site,
i.e., not to adjoin, (non-adjoining) and
4. adjoining of a rule at an activated adjoining
site (adjoining).
In the sentential forms (defined below) we will
maintain for every adjoining site w a two-valued
flag g(w) indicating whether w is a potential
(g(w) = p) or an activated site (g(w) = a).
The set of sentential forms ofG is the set SF(G)
of all tuples ? = (?s, ?t, V,W, g) with
2Their placement (as left or right index) does not play a
role yet, but will later when we introduce pSTIG.
11
? ?s, ?t ? UN (T ),
? V ? lvN (?s) ? lvN (?t) is a one-to-one rela-
tion, |V | = |lvN (?s)| = |lvN (?t)|,
? W ? nlvN (?s)? nlvN (?t), and
? g : W ? {p, a}.
The derivation relation (of G) is the binary
relation ? ? SF(G) ? SF(G) such that
for every ?1 = (?1s , ?1t , V1,W1, g1) and ?2 =
(?2s , ?2t , V2,W2, g2) we have ?1 ? ?2 iff one of
the following is true:
1. (substitution) there are w = (ws, wt) ? V1
and r = (?s, ?t, V,W, P radj) ? S such that
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [?s]ws and ?2t = ?1t [?t]wt ,
? V2 = (V1 \ {w}) ? w.V ,3
? W2 = W1 ? w.W , and
? g2 is the union of g1 and the set of pairs
(w.u, p) for every u ?W ;
this step is denoted by ?1
w,r=? ?2;
2. (activation) there is a w ? W1 with g1(w) =
p and (?1s , ?1t , V1,W1) = (?2s , ?2t , V2,W2),
and g2 is the same as g1 except that g2(w) =
a; this step is denoted by ?1
w=? ?2;
3. (non-adjoining) there is w ? W1 with
g1(w) = p and (?1s , ?1t , V1) = (?2s , ?2t , V2),
W2 = W1 \ {w}, and g2 is g1 restricted to
W2; this step is denoted by ?1
?w=? ?2;
4. (adjoining) there are w ? W1 with g1(w) =
a, and r = (?s, ?t, V,W, ?, P radj) ? A such
that, for w = (ws, wt),
? (?1s (ws), ?1t (wt)) = rc(r),
? ?2s = ?1s [? ?s]ws where ? ?s = ?s[?1s |ws ]?s ,
?2t = ?1t [? ?t]wt where ? ?t = ?t[?1t |wt ]?t ,
? V2 is the smallest set such that (i) for
every (us, ut) ? V1 we have (u?s, u?t) ? V2
where
u?s =
{
us if ws is not a prefix of us,
ws. ?s .u if us = ws.u for some u;
and u?t is obtained in the same way from ut,
wt, and ?t, and (ii) V2 contains w.V ;
? W2 is the smallest set such that (i) for every
(us, ut) ? W1 \ {w} we have (u?s, u?t) ?
W2 where u?s and u?t are obtained in the
same way as for V2, and g2(u?s, u?t) =
g1(us, ut) and (ii) W2 contains w.W and
g2(w.u) = p for every u ?W ;
this step is denoted by ?1
w,r=? ?2.
3w.V = {(ws.vs, wt.vt) | (vs, vt) ? V }
In Fig. 2 we show a derivation of our running
example pSTAG where activated adjoining sites
are indicated by surrounding circles, the other ad-
joining sites are potential.
Ss? ?? St?
substitution of
r1 at (?, ?)
=? P (r1) = 1
Ss
? A? A
?
a ??
St
B
?
B? ?a
substitution of
r4 at (2, 2)
=? P (r4) = .1
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
activation
at a = (3, 1)
=? P r1adj(a) = .9
Ss
? A
?
A
?
a ??
St
B
?
B
?
?a
adjoining of
r2 at a = (3, 1)
=? P (r2) = .4
Ss
? A
?
A
A
?
?
b,c ??
St
B
B
?
B
?
B
?
?b
c
non-adjoining
at c = (3, 1.1)
=? 1? P r2adj(c) = .4
Ss
? A
?
A
A
?
?
b ??
St
B
B
?
B
?
B
?
?b
non-adjoining
at b = (3, 1)
=? 1? P r2adj(b) = .8
Ss
? A
?
A
A
?
?
??
St
B
B
?
B
?
B
?
?
Figure 2: An example derivation with total proba-
bility 1? .1? .9? .4? .4? .8 = .01152.
The only initial sentential form is ?in =
(Ss, St, {(?, ?)}, ?, ?). A sentential form ? is final
if it has the form (?s, ?t, ?, ?, ?). Let ? ? SF(G).
A derivation (of ?) is a sequence d of the form
?0u1?1 . . . un?n with ?0 = ?in and n ? 0,
?i?1
ui? ?i for every 1 ? i ? n (and ?n = ?). We
12
denote ?n also by last(d), and the set of all deriva-
tions of ? (resp., derivations) by D(?) (resp., D).
We call d ? D successful if last(d) is final.
The tree transformation computed by G is
the relation ?G ? UN (T ) ? UN (T ) with
(?s, ?t) ? ?G iff there is a successful derivation
of (?s, ?t, ?, ?, ?).
Our definition of the probability of a deriva-
tion is based on the following observation.4 Let
d ? D(?) for some ? = (?s, ?t, V,W, g). Then,
for every w ? W , the rule which created w and
the corresponding local position in that rule can
be retrieved from d. Let us denote this rule by
r(d, ?, w) and the local position by l(d, ?, w).
Now let d be the derivation ?0u1?1 . . . un?n.
Then the probability of d is defined by
P (d) =
?
1?i?n
Pd(?i?1
ui? ?i)
where
1. (substitution) Pd(?i?1 w,r=? ?i) = P (r)
2. (activation)
Pd(?i?1 w=? ?i) = P r
?
adj(w?) where r? =
r(d, ?i?1, w) and w? = l(d, ?i?1, w)
3. (non-adjoining)
Pd(?i?1 ?w=? ?i) = 1 ? P r
?
adj(w?) where r?
and w? are defined as in the activation case
4. (adjoining)
Pd(?i?1
w,r=? ?i) = P (r).
In order to describe the generative model of
G, we impose a deterministic strategy sel on the
derivation relation in order to obtain, for every
sentential form, a probability distribution among
the follow-up sentential forms. A deterministic
derivation strategy is a mapping sel : SF(G) ?
(N? ? N?) ? {?} such that for every ? =
(?s, ?t, V,W, g) ? SF(G), we have that sel(?) ?
V ?W if V ?W 6= ?, and sel(?) = ? otherwise.
In other words, sel chooses the next site to operate
on. Then we define ?sel in the same way as ? but
in each of the cases we require that w = sel(?1).
Moreover, for every derivation d ? D, we denote
by next(d) the set of all derivations of the form
du? where last(d) u?sel ?.
The generative model of G comprises all the
generative stories of G. A generative story is a
tree t ? UD; the root of t is labeled by ?in. Let
w ? pos(t) and t(w) = d. Then either w is a
leaf, because we have stopped the generative story
4We note that a different definition occurs in (Nesson et
al., 2005, 2006).
at w, or w has |next(d)| children, each one repre-
sents exactly one possible decision about how to
extend d by a single derivation step (where their
order does not matter). Then, for every generative
story t, we have that
?
w?lv(t)
P (t(w)) = 1 .
We note that (D,next, ?) can be considered as
a discrete Markov chain (cf., e.g. (Baier et al,
2009)) where the initial probability distribution
? : D ? [0, 1] maps d = ?in to 1, and all the
other derivations to 0.
A probabilistic synchronous tree insertion
grammar (pSTIG) G is a pSTAG except that
for every rule r = (?s, ?t, V,W, P radj) or r =
(?s, ?t, V,W, ?, P radj) we have that
? if r ? A, then |lv(?s)| ? 2 and |lv(?t)| ? 2,
? for ? = (?s, ?t) we have that ?s is either the
rightmost leaf of ?s or its leftmost one; then
we call r, resp., L-auxiliary in the source and
R-auxiliary in the source; similarly, we re-
strict ?t; the source-spine of r (target-spine
of r) is the set of prefixes of ?s (resp., of ?t)
? W ? nlvN (?s)?{L,R}?nlvN (?t)?{L,R}
where the new components are the direction-
type of the potential adjoining site, and
? for every (ws, ?s, wt, ?t) ? W , if ws lies on
the source-spine of r and r is L-auxiliary (R-
auxiliary) in the source, then ?s = L (resp.,
?s = R), and corresponding restrictions hold
for the target component.
According to the four possibilities for the foot-
node ? we call r LL-, LR-, RL-, or RR-auxiliary.
The restriction for the probability distribution P of
G is modified such that for every (A,B) ? N?N
and x, y ? {L,R}:
?
r?A, rc(r)=(A,B)
r is xy?auxiliary
P (r) = 1 .
In the derivation relation of the pSTIG G we
will have to make sure that the direction-type of
the chosen adjoining site w matches with the type
of auxiliarity of the auxiliary rule. Again we as-
sume that the data structure SF(G) is enriched
such that for every potential adjoining site w of
? ? SF(G) we know its direction-type dir(w).
We define the derivation relation of the pSTIG
G to be the binary relation ?I ? SF(G)?SF(G)
such that we have ?1 ?I ?2 iff (i) ?1 ? ?2 and
13
(ii) if adjoining takes place atw, then the used aux-
iliary rule must be dir(w)-auxiliary. Since ?I is
a subset of ?, the concepts of derivation, success-
ful derivation, and tree transformation are defined
also for a pSTIG.
In fact, our running example pSTAG in Fig. 1 is
a pSTIG, where r2 and r3 are RL-auxiliary and
every potential adjoining site has direction-type
RL; the derivation shown in Fig. 2 is a pSTIG-
derivation.
4 Bottom-up tree adjoining transducer
Here we introduce the concept of a bottom-up tree
adjoining transducer (BUTAT) which will be used
to formalize a decoder for a pSTIG.
A BUTAT is a finite-state machine which trans-
lates strings into trees. The left-hand side of each
rule is a string over terminal symbols and state-
variable combinations. A variable is either a sub-
stitution variable or an adjoining variable; a substi-
tution variable (resp., adjoining variable) can have
an output tree (resp., output tree with foot node) as
value. Intuitively, each variable value is a transla-
tion of the string that has been reduced to the cor-
responding state. The right-hand side of a rule has
the form q(?) where q is a state and ? is an output
tree (with or without foot-node); ? may contain the
variables from the left-hand side of the rule. Each
rule has a probability p ? [0, 1].
In fact, BUTAT can be viewed as the string-
to-tree version of bottom-up tree transducers (En-
gelfriet, 1975; Gecseg and Steinby, 1984,1997) in
which, in addition to substitution, adjoining is al-
lowed.
Formally, we let X = {x1, x2, . . .} and F =
{f1, f2, . . .} be the sets of substitution variables
and adjoining variables, resp. Each substitu-
tion variable (resp., adjoining variable) has rank
0 (resp., 1). Thus when used in a tree, substitu-
tion variables are leaves, while adjoining variables
have a single child.
A bottom-up tree adjoining transducer (BU-
TAT) is a tuple M = (Q,?,?, Qf , R) where
? Q is a finite set (of states),
? ? is an alphabet (of input symbols), assuming
that Q ? ? = ?,
? ? is an alphabet (of output symbols),
? Qf ? Q (set of final states), and
? R is a finite set of rules of the form
?0 q1(z1) ?1 . . . qk(zk) ?k
p? q(?) (?)
where p ? [0, 1] (probability of (?)), k ? 0,
?0, ?1, . . . , ?k ? ??, q, q1, . . . , qk ? Q,
z1, . . . , zk ? X ? F , and ? ? RHS(k)
where RHS(k) is the set of all trees over
? ? {z1, . . . , zk} ? {?} in which the nullary
? occurs at most once.
The set of intermediate results of M is the set
IR(M) = {? | ? ? U?({?}), |pos{?}(?)| ? 1}
and the set of sentential forms of M is the set
SF(M) = (? ? {q(?) | q ? Q, ? ? IR(M)})?.
The derivation relation induced by M is the bi-
nary relation ? ? SF(M) ? SF(M) such that
for every ?1, ?2 ? SF(M) we define ?1 ? ?2 iff
there are ?, ?? ? SF(M), there is a rule of the form
(?) in R, and there are ?1, . . . , ?k ? IR(M) such
that:
? for every 1 ? i ? k: if zi ? X , then ?i does
not contain ?; if zi ? F , then ?i contains ?
exactly once,
? ?1 = ? ?0 q1(?1) ?1 . . . qk(?k) ?k ??, and
? ?2 = ? q(?(?)) ??
where ? is a function that replaces variables
in a right-hand side with their values (subtrees)
from the left-hand side of the rule. Formally,
? : RHS(k) ? IR(M) is defined as follows:
(i) for every ? = ?(?1, . . . , ?n) ? RHS(k), ? ?
?, we have ?(?) = ?(?(?1), . . . , ?(?n)),
(ii) (substitution) for every zi ? X , we have
?(zi) = ?i,
(iii) (adjoining) for every zi ? F and ? ?
RHS(k), we have ?(zi(?)) = ?i[?(?)]v
where v is the uniquely determined position
of ? in ?i, and
(iv) ?(?) = ?.
Clearly, the probablity of a rule carries over to
derivation steps that employ this rule. Since, as
usual, a derivation d is a sequence of derivation
steps, we let the probability of d be the product of
the probabilities of its steps.
The string-to-tree transformation computed by
M is the set ?M of all tuples (?, ?) ? ???U? such
that there is a derivation of the form ? ?? q(?) for
some q ? Qf .
5 Decoder for pSTIG
Now we construct the decoder dec(G) for a pSTIG
G that transforms source strings directly into tar-
get trees and simultaneously computes the proba-
bility of the corresponding derivation of G. This
decoder is formalized as a BUTAT.
Since dec(G) is a string-to-tree transducer, we
14
have to transform the source tree ?s of a rule r
into a left-hand side ? of a dec(G)-rule. This is
done similarly to (DeNeefe and Knight, 2009) by
traversing ?s via recursive descent using a map-
ping ? (see an example after Theorem 1); this
creates appropriate state-variable combinations for
all substitution sites and potential adjoining sites
of r. In particular, the source component of the
direction-type of a potential adjoining site deter-
mines the position of the corresponding combina-
tion in ?. If there are several potential adjoining
sites with the same source component, then we
create a ? for every permutation of these sites. The
right-hand side of a dec(G)-rule is obtained by
traversing the target tree ?t via recursive descent
using a mapping ?? and, whenever a nonterminal
with a potential adjoining site w is met, a new po-
sition labeled by fw is inserted.5 If there is more
than one potential adjoining site, then the set of
all those sites is ordered as in the left-hand side ?
from top to bottom.
Apart from these main rules we will employ
rules which implement the decision of whether or
not to turn a potential adjoining site w into an ac-
tivated adjoining site. Rules for the first purpose
just pass the already computed output tree through
from left to right, whereas rules for the second pur-
pose create for an empty left-hand side the output
tree ?.
We will use the state behavior of dec(G) in or-
der to check that (i) the nonterminals of a substi-
tution or potential adjoining site match the root-
category of the used rule, (ii) the direction-type
of an adjoining site matches the auxiliarity of the
chosen auxiliary rule, and (iii) the decisions of
whether or not to adjoin for each rule r of G are
kept separate.
Whereas each pair (?s, ?t) in the translation of
G is computed in a top-down way, starting at the
initial sentential form and substituting and adjoin-
ing to the present sentential form, dec(G) builds
?t in a bottom-up way. This change of direction is
legitimate, because adjoining is associative (Vijay-
Shanker and Weir, 1994), i.e., it leads to the same
result whether we first adjoin r2 to r1, and then
align r3 to the resulting tree, or first adjoin r3 to
r2, and then adjoin the resulting tree to r1.
In Fig. 3 we show some rules of the decoder
of our running example pSTIG and in Fig. 4 the
5We will allow variables to have structured indices that
are not elements of N. However, by applying a bijective re-
naming, we can always obtain rules of the form (?).
derivation of this decoder which correponds to the
derivation in Fig. 2.
Theorem 1. Let G be a pSTIG over N and T .
Then there is a BUTAT dec(G) such that for ev-
ery (?s, ?t) ? UN (T ) ? UN (T ) and p ? [0, 1] the
following two statements are equivalent:
1. there is a successful derivation of
(?s, ?t, ?, ?, ?) by G with probability p,
2. there is a derivation from yield(?s) to
[Ss, St](?t) by dec(G) with probability p.
PROOF. Let G = (N,T, [Ss, St],S,A, P ) be a
pSTIG. We will construct the BUTAT dec(G) =
(Q,T,N ?T, {[Ss, St]}, R) as follows (where the
mappings ? and ?? will be defined below):
? Q = [N ?N ] ? [N ?{L,R}?N ?{L,R}]
?{[r, w] | r ? A, w is an adjoining site of r},
? R is the smallest set R? of rules such
that for every r ? S ? A of the form
(?s, ?t, V,W, P radj) or (?s, ?t, V,W, ?, P radj):
? for every ? ? ?(?), if r ? S, then the
main rule
? P (r)? [?s(?), ?t(?)]
(
??(?)
)
is in R?, and if r ? A and r is ?s?t-
auxiliary, then the main rule
? P (r)? [?s(?), ?s, ?t(?), ?t]
(
??(?)
)
is in R?, and
? for every w = (ws, ?s, wt, ?t) ? W the
rules
qw
(
fw
) P radj(w)?? [r, w]
(
fw(?)
)
with qw = [?(ws), ?s, ?t(wt), ?t] for ac-
tivation at w, and the rule
?
1?P radj(w)?? [r, w](?)
for non-adjoining at w are in R?.
We define the mapping
? : pos(?s) ? P((T ?Q(X ? F ))?)
with Q(X ? F ) = {q(z) | q ? Q, z ? X ? F}
inductively on its argument as follows. Let w ?
pos(?s) and let w have n children.
(a) Let ?s(w) ? T . Then ?(w) = {?s(w)}.
15
(b) (substitution site) Let ?s(w) ? N and let
w? ? pos(?t) such that (w,w?) ? V . Then
?(w) = {[?s(w), ?t(w?)]
(
x(w,w?)
)
}.
(c) (adjoining site) Let ?s(w) ? N and let there
be an adjoining site in W with w as first
component. Then, we define ?(w) to be the
smallest set such that for every permutation
(u1, . . . , ul) (resp., (v1, . . . , vm)) of all the L-
adjoining (resp., R-adjoining) sites inW with
w as first component, the set6
J ? ?(w.1) ? . . . ? ?(w.n) ?K
is a subset of ?(w), where J = {u?1 . . . u?l}
and K = {v?m . . . v?1} and
u?i = [r, ui]
(
fui
)
and v?j = [r, vj ]
(
fvj
)
for 1 ? i ? l and 1 ? j ? m.
(d) Let ?s(w) ? N , w 6= ?, and let w be neither
the first component of a substitution site in V
nor the first component of an adjoining site in
W . Then
?(w) = ?(w.1) ? . . . ? ?(w.n) .
(e) Let w = ?. Then we define ?(w) = {?}.
For every ? ? ?(?), we define the mapping
?? : pos(?t) ? UN?F?X(T ? {?})
inductively on its argument as follows. Let
w ? pos(?t) and let w have n children.
(a) Let ?t(w) ? T . Then ??(w) = ?t(w).
(b) (substitution site) Let ?t(w) ? N and let
w? ? pos(?s) such that (w?, w) ? V . Then
??(w) = x(w?,w).
(c) (adjoining site) Let ?t(w) ? N and let there
be an adjoining site in W with w as third
component. Then let {u1, . . . , ul} ? W be
the set of all potential adjoining sites with w
as third component, and we define
??(w) = fu1(. . . ful(?) . . .)
where ? = ?t(w)(??(w.1), . . . , ??(w.n))
and the ui?s occur in ??(w) (from the root
towards the leaves) in exactly the same order
as they occur in ? (from left to right).
(d) Let ?t(w) ? N , w 6= ?, and let w be neither
the second component of a substitution site
in V nor the third component of an adjoining
site in W . Then
??(w) = ?t(w)(??(w.1), . . . , ??(w.n)).
6using the usual concatenation ? of formal languages
(e) Let w = ?. Then ??(w) = ?.
With dec(G) constructed as shown, for each
derivation of G there is a corresponding deriva-
tion of dec(G), with the same probability, and vice
versa. The derivations proceed in opposite direc-
tions. Each sentential form in one has an equiv-
alent sentential form in the other, and each step
of the derivations correspond. There is no space
to present the full proof, but let us give a slightly
more precise idea about the formal relationship be-
tween the derivations of G and dec(G).
In the usual way we can associate a deriva-
tion tree dt with every successful derivation d of
G. Assume that last(d) = (?s, ?t, ?, ?, ?), and
let Es and Et be the embedded tree transducers
(Shieber, 2006) associated with, respectively, the
source component and the target component of
G. Then it was shown in (Shieber, 2006) that
?Es(dt) = ?s and ?Et(dt) = ?t where ?E de-
notes the tree-to-tree transduction computed by an
embedded tree transducer E. Roughly speaking,
Es and Et reproduce the derivations of, respec-
tively, the source component and the target com-
ponent of G that are prescribed by dt. Thus, for
? = (??s, ??t, V,W, g), if ?in ??G ? and ? is a prefix
of d, then there is exactly one subtree dt[(w,w?)]
of dt associated with every (w,w?) ? V ? W ,
which prescribes how to continue at (w,w?) with
the reproduction of d. Having this in mind, we ob-
tain the sentential form of the dec(G)-derivation
which corresponds to ? by applying a modifica-
tion of ? to ? where the modification amounts to
replacing x(w,w?) and f(w,w?) by ?Et(dt[(w,w?)]);
note that ?Et(dt[(w,w?)]) might contain ?. 
As illustration of the construction in Theorem 1
let us apply the mappings ? and ?? to rule r2 of
Fig. 1, i.e., to r2 = (?s, ?t, ?, {b, c}, ?, P r2adj)
with ?s = A(A, ?), ?t = B(B(?), B),
b = (?,R, ?,L), c = (?,R, 1,L), and ? = (1, 2).
Let us calculate ?(?) on ?s. Due to (c),
?(?) = J ? ?(1) ? ?(2) ?K.
Since there are no L-adjoinings at ?, we have that
J = {?}. Since there are the R-adjoinings b and c
at ?, we have the two permutations (b, c) and (c, b).
(v1, v2) = (b, c): K = {[r2, c](fc)[r2, b](fb)}
(v1, v2) = (c, b): K = {[r2, b](fb)[r2, c](fc)}
Due to (e) and (a), we have that ?(1) = {?} and
?(2) = {?}, resp. Thus, ?(?) is the set:
{? [r2, c](fc) [r2, b](fb), ? [r2, b](fb) [r2, c](fc)}.
16
r1
(r1, a)
r2
(r2,?b) (r2,?c)
r4
?
[A,B]
x(2,2)
?
[r1, a]
fa
1??
[Ss, St]
St
f
B
?
x ?a (2,2)
[A,R, B,L]
fa
.9??
[r1, a]
f
?
a
?
[r2, b]
f b
[r2, c]
fc
.4??
[A,R, B,L]
f
B
f
B
?
?
b
c
? .8??
[r2, b]
?
? .4??
[r2, c]
?
? .1??
[A,B]
B
?
Figure 3: Some rules of the running example de-
coder.
Now let ? = ? [r2, b](fb) [r2, c](fc). Let us cal-
culate ??(?) on ?t.
??(?)
(c)= fb(B(??(1), ??(2)))
(c)= fb(B(fc(B(??(1.1))), ??(2)))
(a)= fb(B(fc(B(?)), ??(2)))
(e)= fb(B(fc(B(?)), ?))
Hence we obtain the rule
? [r2, b](fb) [r2, c](fc) ?
[A,R, B,L](fb(B(fc(B(?)), ?)))
which is also shown in Fig. 3.
? ? ? ?
? ? ? ?
[r2, b]
?
? ? ? ?
[r2, b]
?
[r2, c]
?
? ? ?
? ? ?
?
[A,B]
B
?
?
[A,R, B,L]
B
B
?
?
=?
=?
=?
=?
=?
[Ss, St]
St
B
B
?
B
?
B
?
?
prob. .8
prob. .4
prob. .4
prob. .9
prob. .1
=? prob. .1
[r1, a]
B
B
?
?
(r2,?b)
(r2,?c)
(r2, bc)
(r1, a)
r4
r1
[r1, a]
B
B
?
?
Figure 4: Derivation of the decoder corresponding
to the derivation in Fig. 2.
17
References
A. Abeille, Y. Schabes, A.K. Joshi. Using lexicalized
TAGs for machine translation. In Proceedings of
the 13th International Conference on Computational
Linguistics, volume 3, pp. 1?6, Helsinki, Finland,
1990.
C. Baier, M. Gro??er, F. Ciesinski. Model checking
linear-time properties of probabilistic systems. In
Handbook of Weighted Automata, Chapter 13, pp.
519?570, Springer, 2009.
S. DeNeefe. Tree adjoining machine translation. Ph.D.
thesis proposal, Univ. of Southern California, 2009.
S. DeNeefe, K. Knight. Synchronous tree adjoining
machine translation. In Proc. of Conf. Empirical
Methods in NLP, pp. 727?736, 2009.
J. Engelfriet. Bottom-up and top-down tree transfor-
mations ? a comparison. Math. Systems Theory,
9(3):198?231, 1975.
J. Engelfriet. Tree transducers and syntax-directed se-
mantics. In CAAP 1982: Lille, France, 1982.
A. Fujiyoshi, T. Kasai. Spinal-formed context-free tree
grammars. Theory of Computing Systems, 33:59?
83, 2000.
Z. Fu?lo?p, H. Vogler. Weighted tree automata and tree
transducers. In Handbook of Weighted Automata,
Chapter 9, pp. 313?403, Spinger, 2009.
F. Ge?cseg, M. Steinby. Tree Automata. Akade?miai
Kiado?, Budapest, 1984.
F. Ge?cseg, M. Steinby. Tree languages. In Handbook
of Formal Languages, volume 3, chapter 1, pages
1?68. Springer-Verlag, 1997.
J. Graehl, K. Knight, J. May. Training tree transducers.
Computational Linguistics, 34(3):391?427, 2008
A.K. Joshi, L.S. Levy, M. Takahashi. Tree adjunct
grammars. Journal of Computer and System Sci-
ences, 10(1):136?163, 1975.
A.K. Joshi, Y. Schabes. Tree-adjoining grammars. In
Handbook of Formal Languages. Chapter 2, pp. 69?
123, Springer-Verlag, 1997.
K. Knight, J. Graehl. An overview of probabilis-
tic tree transducers for natural language processing.
In Computational Linguistics and Intelligent Text
Processing, CICLing 2005, LNCS 3406, pp. 1?24,
Springer, 2005.
K. Knight, J. May. Applications of Weighted Au-
tomata in Natural Language Processing. In Hand-
book of Weighted Automata, Chapter 14, pp. 571?
596, Springer, 2009.
P.M. Lewis, R.E. Stearns. Syntax-directed transduc-
tions. Journal of the ACM, 15:465?488, 1968.
A. Maletti. Compositions of tree series transforma-
tions. Theoret. Comput. Sci., 366:248?271, 2006.
A. Maletti. The Power of Tree Series Transducers.
Ph.D. thesis, TU Dresden, Germany, 2006.
A. Maletti. Compositions of extended top-down
tree transducers. Information and Computation,
206:1187?1196, 2008.
A. Maletti. Why synchronous tree substitution gram-
mars? in Proc. 11th Conf. North American Chap-
ter of the Association of Computational Linguistics.
2010.
D.F. Martin and S.A. Vere. On syntax-directed trans-
ductions and tree transducers. In Ann. ACM Sympo-
sium on Theory of Computing, pp. 129?135, 1970.
R. Nesson, S.M. Shieber, and A. Rush. Induction
of probabilistic synchronous tree-insertion gram-
mars. Technical Report TR-20-05, Computer Sci-
ence Group, Harvard Univeristy, Cambridge, Mas-
sachusetts, 2005.
R. Nesson, S.M. Shieber, and A. Rush. Induction of
probabilistic synchronous tree-inserting grammars
for machine translation. In Proceedings of the 7th
Conference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), 2006.
Y. Schabes, R.C. Waters. Tree insertion grammars:
a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees
produced. Computational Linguistics, 21:479?513,
1994.
P.P. Schreiber. Tree-transducers and syntax-connected
transductions. In Automata Theory and Formal
Languages, Lecture Notes in Computer Science 33,
pp. 202?208, Springer, 1975.
S.M. Shieber. Synchronous grammars and tree trans-
ducers. In Proc. 7th Workshop on Tree Adjoin-
ing Grammars and Related Formalisms, pp. 88?95,
2004.
S.M. Shieber. Unifying synchronous tree-adjoining
grammars and tree transducers via bimorphisms. In
Proc. 11th Conf. European Chapter of ACL, EACL
06, pp. 377?384, 2006.
S.M. Shieber, Y. Schabes. Synchronous tree-adjoining
grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguistics,
volume 3, pp. 253?258, Helsinki, Finland, 1990.
K. Vijay-Shanker, D.J. Weir. The equivalence of four
extensions of context-free grammars. Mathematical
Systems Theory, 27:511?546, 1994.
K. Yamada and K. Knight. A syntax-based statistical
translation model. In Proc. of 39th Annual Meeting
of the Assoc. Computational Linguistics, pp. 523?
530, 2001.
18
