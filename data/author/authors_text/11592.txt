Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 488?491,
Prague, June 2007. c?2007 Association for Computational Linguistics
XRCE-M: A Hybrid System for Named Entity Metonymy Resolution 
*Caroline Brun 
 
*Maud Ehrmann 
 
*Guillaume Jacquet 
 
 
* Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240 Meylan France 
*{Caroline.Brun, Maud.Ehrmann, Guillaume.Jacquet}@xrce.xerox.com 
 
Abstract 
This paper describes our participation to the 
Metonymy resolution at SemEval 2007 (task 
#8). In order to perform named entity me-
tonymy resolution, we developed a hybrid 
system based on a robust parser that extracts 
deep syntactic relations combined with a 
non-supervised distributional approach, also 
relying on the relations extracted by the 
parser.  
1 Description of our System 
SemEval 2007 introduces a task aiming at resolving 
metonymy for named entities, for location and or-
ganization names (Markert and Nissim 2007). Our 
system addresses this task by combining a symbolic 
approach based on robust deep parsing and lexical 
semantic information, with a distributional method 
using syntactic context similarities calculated on 
large corpora. Our system is completely unsuper-
vised, as opposed to state-of-the-art systems (see  
(Market and Nissim, 2005)).  
1.1 Robust and Deep Parsing Using XIP 
We use the Xerox Incremental Parser (XIP, (A?t et 
al., 2002)) to perform robust and deep syntactic 
analysis. Deep syntactic analysis consists here in the 
construction of a set of syntactic relations1 from an 
input text.  These relations, labeled with deep syn-
tactic functions, link lexical units of the input text 
and/or more complex syntactic domains that are 
constructed during the processing (mainly chunks, 
see (Abney, 1991)).  
                                                 
                                                
1 inspired from dependency grammars, see (Mel??uk, 
1998), and (Tesni?re, 1959). 
Moreover, together with surface syntactic relations, 
the parser calculates more sophisticated relations 
using derivational morphologic properties, deep 
syntactic properties2, and some limited lexical se-
mantic coding (Levin's verb class alternations, see 
(Levin, 1993)), and some elements of the Framenet3 
classification, (Ruppenhofer et al, 2006)). These 
deep syntactic relations correspond roughly to the 
agent-experiencer roles that is subsumed by the 
SUBJ-N relation and to the patient-theme role sub-
sumed by the OBJ-N relation, see (Brun and  Ha-
g?ge, 2003). Not only verbs bear these relations but 
also deverbal nouns with their corresponding argu-
ments.  
Here is an example of an output (chunks and 
deep syntactic relations): 
Lebanon still wanted to see the implementation of a UN 
resolution 
 
TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the 
implementation} PP{of NP{a UN resolution}} .} 
MOD_PRE(wanted,still) 
MOD_PRE(resolution,UN) 
MOD_POST(implementation,resolution) 
COUNTRY(Lebanon) 
ORGANISATION(UN) 
EXPERIENCER_PRE(wanted,Lebanon) 
EXPERIENCER(see,Lebanon) 
CONTENT(see,implementation) 
EMBED_INFINIT(see,wanted) 
OBJ-N(implement,resolution) 
1.2 Adaptation to the Task 
Our parser includes a module for ?standard? 
named entity recognition, but needs to be adapted to 
handle named entity metonymy. Following the 
guidelines of the SemEval task #8, we performed a 
 
2 Subject and object of infinitives in the context of con-
trol verbs. 
3 http://framenet.icsi.berkeley.edu/ 
488
corpus study on the trial data in order to detect lexi-
cal and syntactic regularities triggering a metonymy, 
for both location names and organization names. 
For example, we examined the subject relation be-
tween organizations or locations and verbs and we 
then classify the verbs accordingly: we draw hy-
pothesis like ?if a location name is the subject of a 
verb referring to an economic action, like import, 
provide, refund, repay, etc., then it is a place-for-
people?. We adapted our parser by adding dedicated 
lexicons that encode the information collected from 
the corpus and develop rules modifying the interpre-
tation of the entity, for example:  
 
 If (LOCATION(#1) & SUBJ-N(#2[v_econ],#1))4
 ? PLACE-FOR-PEOPLE(#1) 
 
We focus our study on relations like subject, object, 
experiencer, content, modifiers (nominal and prepo-
sitional) and attributes.  We also capitalize on the 
already-encoded lexical information attached to 
verbs by the parser, like communication verbs like 
say, deny, comment, or categories of the FrameNet 
Experiencer subject frame, i.e. verbs like feel, sense, 
see. This information was very useful since experi-
encers denote persons, therefore all organizations or 
locations having an experiencer role can be consid-
ered as organization-for-members or place-for-
people. Here is an example of output5, when apply-
ing the modified parser on the following sentence: 
?It was the largest Fiat everyone had ever seen?. 
ORG-FOR-PRODUCT(Fiat) 
MOD_PRE(seen,ever) 
SUBJ-N_PRE(was,It) 
EXPERIENCER_PRE(seen,everyone) 
SUBJATTR(It,Fiat) 
    QUALIF(Fiat,largest)  
 
Here, the relation QUALIF(Fiat, largest) triggers 
the metonymical interpretation of ?Fiat? as org-for-
product. 
This first development step is the starting point of 
our methodology, which is completed by a non-
supervised distributional approach described in the 
next section.  
                                                 
4 Which read as ?if the parser has detected a location 
name (#1), which is the subject of a verb (#2) bearing the 
feature ?v-econ?, then create a PLACE-FOR-PEOPLE 
unary predicate on #1.  
5 Only dependencies are shown. 
1.3 Hybridizing with a Distributional Approach 
The distributional approach proposes to establish a 
distance between words depending on there syntac-
tic distribution. 
The distributional hypothesis is that words that ap-
pear in similar contexts are semantically similar 
(Harris, 1951): the more two words have the same 
distribution, i.e. are found in the same syntactic con-
texts, the more they are semantically close. 
We propose to apply this principle for metonymy 
resolution. Traditionally, the distributional approach 
groups words like USA, Britain, France, Germany 
because there are in the same syntactical contexts:  
 
 (1) Someone live in Germany. 
(2) Someone works in Germany. 
(3) Germany declares something. 
(4) Germany signs something. 
 
The metonymy resolution task implies to distin-
guish the literal cases, (1) & (2), from the meto-
nymic ones, (3) & (4). Our method establishes these 
distinctions using the syntactic context distribution. 
We group contexts occurring with the same words: 
the syntactic contexts live in and work in are occur-
ring with Germany, France, country, city, place, 
when syntactic contexts subject-of-declare and sub-
ject-of-sign are occurring with Germany, France, 
someone, government, president. 
For each Named Entity annotation, the hybrid 
method consists in using symbolic annotation if 
there is (?1.2), else using distributional annotation 
(?1.3) as presented below. 
Method: We constructed a distributional space with 
the 100M-word BNC. We prepared the corpus by 
lemmatizing and then parsing with the same robust 
parser than for the symbolic approach (XIP, see sec-
tion 3.1). It allows us to identify triple instances. 
Each triple have the form w1.R.w2 where w1 and 
w2 are lexical units and R is a syntactic relation 
(Lin, 1998; Kilgarriff & al. 2004).  
Our approach can be distinguished from classical 
distributional approach by different points. 
First, we use triple occurrences to build a distribu-
tional space (one triple implies two contexts and 
two lexical units), but we use the transpose of the 
classical space: each point xi of this space is a syn-
tactical context (with the form R.w.), each dimen-
sion j is a lexical units, and each value xi(j) is the 
frequency of corresponding triple occurrences. Sec-
489
ond, our lexical units are words but also complex 
nominal groups or verbal groups. Third, contexts 
can be simple contexts or composed contexts6. 
We illustrate these three points on the phrase pro-
vide Albania with food aid. The XIP parser gives 
the following triples where for example, food aid is 
considered as a lexical unit: 
OBJ-N('VERB:provide','NOUN: Albania'). 
PREP_WITH('VERB: provide ','NOUN:aid'). 
PREP_WITH('VERB: provide ','NP:food aid'). 
From these triples, we create the following lexical 
units and contexts (in the context 1.VERB: provide. 
OBJ-N, ?1? mean that the verb provide is the gov-
ernor of the relation OBJ-N): 
Words: Contexts: 
VERB:provide 1.VERB: provide. OBJ-N 
NOUN:Albania 1.VERB: provide.PREP_WITH 
NOUN:aid 2.NOUN: Albania.OBJ-N 
NP:food aid 2.NOUN: aid. PREP_WITH 
 2.NP: food aid. PREP_WITH 
 1.VERB:provide.OBJ-N+2.NOUN:aid. PREP_WITH 
 1.VERB:provide.OBJ-N+2.NP:food aid. PREP_WITH 
 1.VERB:provide.PREP_WITH +2.NO:Albania.OBJ-N 
 
We use a heuristic to control the high productivity 
of these lexical units and contexts. Each lexical unit 
and each context should appear more than 100 times 
in the corpus. From the 100M-word BNC we ob-
tained 60,849 lexical units and 140,634 contexts. 
Then, our distributional space has 140,634 units and 
60,849 dimensions. 
Using the global space to compute distances be-
tween each context is too consuming and would 
induce artificial ambiguity (Jacquet, Venant, 2005). 
If any named entity can be used in a metonymic 
reading, in a given corpus each named entity has not 
the same distribution of metonymic readings. The 
country Vietnam is more frequently used as an event 
than France or Germany, so, knowing that a context 
is employed with Vietnam allow to reduce the meto-
nymic ambiguity. 
For this, we construct a singular sub-space de-
pending to the context and to the lexical unit (the 
ambiguous named entity): 
For a given couple context i + lexical unit j we 
construct a subspace as follows:  
Sub_contexts = list of contexts which are occur-
ring with the word i. If there are more than k con-
texts, we take only the k more frequents. 
Sub_dimension = list of lexical units which are 
occurring with at least one of the contexts from the 
                                                 
6 For our application, one context can be composed by 
two simple contexts. 
Sub_contexts list. If there are more than n words, 
we take only the n more frequents (relative fre-
quency) with the Sub_contexts list (for this applica-
tion, k = 100 and n = 1,000). 
We reduce dimensions of this sub-space to 10 
dimensions with a PCA (Principal Components 
Analysis). 
In this new reduced space (k*10), we compute 
the closest context of the context j with the Euclid-
ian distance. 
At this point, we use the results of the symbolic 
approach described before as starting point. We at-
tribute to each context of the Sub_contexts list, the 
annotation, if there is, attributed by symbolic rules. 
Each kind of annotation (literal, place-for-people, 
place-for-event, etc) is attributed a score corre-
sponding to the sum of the scores obtained by each 
context annotated with this category. The score of a 
context i  decreases in inverse proportion to its dis-
tance from the context j: score(context i) = 
1/d(context i, context j) where d(i,j) is the Euclidian 
distance between i and j. 
We illustrate this process with the sentence pro-
vide Albania with food aid. The unit Albania is 
found in 384 different contexts (|Sub_contexts| = 
384) and 54,183 lexical units are occurring with at 
least one of the contexts from the Sub_contexts list 
(|Sub_dimension| = 54,183). 
After reducing dimension with PCA, we obtain 
the context list below ordered by closeness with the 
given context (1.VERB:provide.OBJ-N):  
Contexts   d symb. annot. 
1.VERB:provide.OBJ-N  0.00  
1.VERB:allow.OBJ-N  0.76         place-for-people 
1.VERB:include.OBJ-N  0.96  
2.ADJ:new.MOD_PRE  1.02  
1.VERB:be.SUBJ-N  1.43  
1.VERB:supply.SUBJ-N_PRE 1.47 literal 
1.VERB:become.SUBJ-N_PRE 1.64  
1.VERB:come.SUBJ-N_PRE  1.69  
1.VERB:support.SUBJ-N_PRE 1.70          place-for-people 
etc. 
 
Score for each metonymic annotation of Albania: 
? place-for-people 3.11 
 literal  1.23 
place-for-event  0.00 
?  0.00 
The score obtained by each annotation type al-
lows annotating this occurrence of Albania as a 
place-for-people metonymic reading. If we can?t 
choose only one annotation (all score = 0 or equal-
ity between two annotations) we do not annotate.  
490
2 Evaluation and Results 
The following tables show the results on the test 
corpus: 
type Nb. 
samp 
accuracy coverage Baseline 
accuracy 
Baseline 
coverage 
Loc/coarse 908 0.851 1 0.794 1 
Loc/medium 908 0.848 1 0.794 1 
Loc /fine 908 0.841 1 0.794 1 
Org/coarse 842 0.732 1 0.618 1 
Org/medium 842 0.711 1 0.618 1 
Org/fine 842 0.700 1 0.618 1 
Table 1: Global Results 
 
 Nb 
occ. 
Prec. Recall F-score
Literal 721 0.867 0.960 0.911 
Place-for-people 141 0.651 0.490 0.559 
Place-for-event 10 0.5 0.1 0.166 
Place-for-product 1 _ 0 0 
Object-for-name 4 1 0.5 0.666 
Object-for-representation 0 _ _ _ 
Othermet 11 _ 0 0 
mixed 20 _ 0 0 
Table 2: Detailed Results for Locations 
 
 Nb 
occ. 
Prec. Recall F-score
Literal 520 0.730 0.906 0.808 
Organization-for-members 161 0.622 0.522 0.568 
Organization-for-event 1 _ 0 0 
Organization-for-product 67 0.550 0.418 0.475 
Organization-for-facility 16 0.5 0.125 0.2 
Organization-for-index 3 _ 0 0 
Object-for-name 6 1 0.666 0.8 
Othermet 8 _ 0 0 
Mixed  60 _ 0 0 
Table 3: Detailed Results for Organizations 
 
The results obtained on the test corpora are above 
the baseline for both location and organization 
names and therefore are very encouraging for the 
method we developed. However, our results on the 
test corpora are below the ones we get on the train 
corpora, which indicates that there is room for im-
provement for our methodology.  
Identified errors are of different nature: 
Parsing errors: For example in the sentence ?Many 
galleries in the States, England and France de-
clined the invitation.?, because the analysis of the 
coordination is not correct, France is calculated as 
subject of declined, a context triggering a place-for-
people interpretation, which is wrong here.  
Mixed cases: These phenomena, while relatively 
frequent in the corpora, are not properly treated. 
Uncovered contexts: some of the syntactico-
semantic contexts triggering a metonymy are not 
covered by the system at the moment.  
3 Conclusion 
This paper describes a system combining a sym-
bolic and a non-supervised distributional approach, 
developed for resolving location and organization 
names metonymy. We plan to pursue this work in 
order to improve the system on the already-covered 
phenomenon as well as on different names entities.  
References 
Abney S. 1991. Parsing by Chunks.  In Robert Berwick, Steven 
Abney and Carol Teny (eds.). Principle-based Parsing, Klu-
wer Academics Publishers.  
A?t-Mokhtar S., Chanod, J.P., Roux, C. 2002. Robustness be-
yond Shallowness: Incremental Dependency Parsing. Spe-
cial issue of NLE journal.  
Brun, C., Hag?ge C., 2003. Normalization and Paraphrasing 
Using Symbolic Methods, Proceeding of the Second Interna-
tional Workshop on Paraphrasing. ACL 2003, Vol. 16, Sap-
poro, Japan.  
Harris Z. 1951. Structural Linguistics, University of Chicago 
Press. 
Jacquet G.,Venant F. 2003. Construction automatique de clas-
ses de s?lection distributionnelle, In Proc. TALN 2003, 
Dourdan. 
Kilgarriff A., Rychly P., Smrz P., Tugwell D.  2004. The sketch 
engine. In Proc. EURALEX, pages 105-116. 
Levin, B. 1993. English Verb Classes and Alternations ? A 
preliminary Investigation. The University of Chicago Press.  
Nissim, M. and Markert, K. 2005. Learning to buy a Renault 
and to talk to a BMW: A supervised approach to conven-
tional metonymy. Proceedings of the 6th International Work-
shop on Computational Semantics, Tilburg. 
Nissim, M. and Markert, K. 2007. SemEval-2007 Task 08: Me-
tonymy Resolution at SemEval-2007. In Proceedings of Se-
mEval-2007.  
Lin D. 1998. Automatic retrieval and clustering of similar 
words. In COLING-ACL, pages 768-774. 
Mel??uk I. 1988. Dependency Syntax. State University of New 
York, Albany.  
Ruppenhofer, J. Michael Ellsworth, Miriam R. L. Petruck, 
Christopher R Johnson and Jan Scheffczyk. 2006. Framenet 
II: Extended Theory and Practice.  
Tesni?re L. 1959. El?ments de Syntaxe Structurale. Klincksiek 
Eds. (Corrected edition Paris 1969).  
491
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 142?145,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Towards a Methodology for Named Entities Annotation
Kar?n Fort
INIST / LIPN
2 all?e du Parc de Brabois,
54500 Vandoeuvre-l?s-Nancy, France
karen.fort@inist.fr
Maud Ehrmann
XRCE
6 Chemin de Maupertuis,
38240 Meylan, France
Ehrmann@xrce.xerox.com
Adeline Nazarenko
LIPN, Universit? Paris 13 & CNRS
99 av. J.B. Cl?ment,
93430 Villetaneuse, France
nazarenko@lipn.univ-paris13.fr
Abstract
Today, the named entity recognition task is
considered as fundamental, but it involves
some specific difficulties in terms of anno-
tation. Those issues led us to ask the fun-
damental question of what the annotators
should annotate and, even more important,
for which purpose. We thus identify the
applications using named entity recogni-
tion and, according to the real needs of
those applications, we propose to seman-
tically define the elements to annotate. Fi-
nally, we put forward a number of method-
ological recommendations to ensure a co-
herent and reliable annotation scheme.
1 Introduction
Named entity (NE) extraction appeared in the mid-
dle of the 1990s with the MUC conferences (Mes-
sage Understanding Conferences). It has now be-
come a successful Natural Language Processing
(NLP) task that cannot be ignored. However, the
underlying corpus annotation is still little studied.
The issues at stake in manual annotation are cru-
cial for system design, be it manual design, ma-
chine learning, training or evaluation. Manual an-
notations give a precise description of the expected
results of the target system. Focusing on manual
annotation issues led us to examine what named
entities are and what they are used for.
2 Named Entities Annotation: practice
and difficulties
Named entity recognition is a well-established
task (Nadeau and Sekine, 2007). One can recall its
evolution according to three main directions. The
first corresponds to work in the ?general? field,
0This work was partly realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for innova-
tion.
with the continuation of the task defined by MUC
for languages other than English, with a revised set
of categories, mainly with journalistic corpora1.
The second direction relates to work in ?special-
ized? domains, with the recognition of entities in
medicine, chemistry or microbiology, like gene
and protein names in specialized literature2. The
last direction, spanning the two previous ones, is
disambiguation.
For each of those evaluation campaigns, cor-
pora were built and annotated manually. They
are generally used to develop automatic annotation
tools. ?To Develop? is to be understood in a broad
sense: the goal is to describe what automatic sys-
tems should do, to help writing the symbolic rules
they are based on, to learn those rules or decision
criteria automatically, and, finally, to evaluate the
results obtained by comparing them with a gold
standard. The annotation process brings into play
two actors, an annotator and a text. The text anno-
tation must follow precise guidelines, satisfy qual-
ity criteria and support evaluation.
In the general field, the MUC, CoNLL and
ACE evaluation campaigns seem to have paid at-
tention to the process of manual NE annotation,
with the definition of annotation guidelines and
the calculation of inter-annotator (but not intra-
annotator) agreement, using a back-and-forth pro-
cess between annotating the corpus and defining
the annotation guidelines. Nevertheless, some as-
pects of the annotation criteria remained problem-
atic, caused mainly by ?different interpretations
of vague portions of the guidelines? (Sundheim,
1995). In the fields of biology and medicine, texts
from specialized databases (PubMed and Med-
Line3) were annotated. Annotation guidelines
1See the evaluation campaigns MET, IREX, CoNNL,
ACE, ESTER and HAREM (Ehrmann, 2008, pp. 19-21).
2See the evaluation campaigns BioCreAtIvE (Kim et al,
2004) and JNLPBA (Hirschman et al, 2005).
3www.ncbi.nlm.nih.gov/pubmed, http://medline.cos.com
142
were vague about the annotation of NEs 4, and few
studies measured annotation quality. For the GE-
NIA (Kim et al, 2003), PennBioIE (Kulick et al,
2004) or GENETAG (Tanabe et al, 2005) corpora,
no inter- or intra-annotator agreement is reported.
If NE annotation seems a well-established prac-
tice, it involves some difficulties.
As regards general language corpora, those dif-
ficulties are identified (Ehrmann, 2008). The first
one relates to the choice of annotation categories
and the determination of what they encompass.
Indeed, beyond the ?universal? triad defined by
the MUC conferences (ENAMEX, NUMEX and
TIMEX), the inventory of categories is difficult to
stabilize. For ENAMEX, although it may be ob-
vious that the name of an individual such as Kofi
Annan is to be annotated using this category, what
to do with the Kennedys, Zorro, the Democrats or
Santa Claus? For the other categories, it is just
as difficult to choose the granularity of the cat-
egories and to determine what they encompass.
Another type of difficulty relates to the selection
of the mentions to be annotated as well as the de-
limitation of NE boundaries. Let us consider the
NE ?Barack Obama? and the various lexemes that
can refer to it: Barack Obama, Mr Obama, the
President of the United States, the new president,
he. Should we annotate proper nouns only, or also
definite descriptions that identify this person, even
pronouns which, contextually, could refer to this
NE? And what to do with the various attributes
that go with this NE (Mr and president)? Coordi-
nation and overlapping phenomena can also raise
problems for the annotators. Finally, another dif-
ficulty results from phenomena of referential plu-
rality, with homonyms NEs (Java place and Java
language) and metonyms (England as a geograph-
ical place, a government or sport team).
Our experience in microbiology shows that
these difficulties are even more acute in special-
ized language. We carried out an annotation ex-
periment on an English corpus of PubMed notices.
The main difficulty encountered related to the
distinction required between proper and common
nouns, the morphological boundary between the
two being unclear in those fields where common
nouns are often reclassified as ?proper nouns?, as
is demonstrated by the presence of these names
4(Tanabe et al, 2005) notes that ?a more detailed defi-
nition of a gene/protein name, as well as additional annota-
tion rules, could improve inter-annotator agreement and help
solve some of the tagging inconsistencies?.
in nomenclatures (small, acid-soluble spore pro-
tein A is an extreme case) or acronymisation phe-
nomena (one finds for example across the outer
membrane (OM)). In those cases, annotators were
instructed to refer to official lists, such as Swiss-
Prot5, which requires a significant amount of time.
Delimiting the boundaries of the elements to be
annotated also raised many questions. One can
thus choose to annotate nifh messenger RNA if it is
considered that the mention of the state messenger
RNA is part of the determination of the reference,
or only nifh, if it is considered that the proper noun
is enough to build the determination. Selecting se-
mantic types was also a problem for the annota-
tors, in particular for mobile genetic elements, like
plasmids or transposons. Indeed, those were to be
annotated in taxons but not in genes whereas they
are chunks of DNA, therefore parts of genome. A
particularly confusing directive for the annotators
was to annotate the acronym KGFR as a proper
noun and the developed form keratinocyte growth
Factor receptor as a common noun. This kind of
instruction is difficult to comprehend and should
have been documented better.
These problems result in increased annotation
costs, too long annotation guidelines and, above
all, a lot of indecision for the annotators, which
induces inconsistencies and lower-quality annota-
tion. This led us to consider the issue of what the
annotators must annotate (semantic foundations of
NE) and, above all, why.
3 What to Annotate?
3.1 Various Defining Criteria
Ehrmann (2008) proposes a linguistic analysis
of the notion of NE, which is presented as an
NLP ?creation?. In the following paragraphs, we
take up the distinction introduced in LDC (2004):
NE are ?mentions? refering to domain ?entities?,
those mentions relate to different linguistic cate-
gories: proper nouns (?Rabelais?), but also pro-
nouns (?he?), and in a broader sense, definite de-
scriptions (?the father of Gargantua?). Several
defining criteria for NE can be identified.
Referential Unicity One of the main charac-
teristics of proper nouns is their referential be-
haviour: a proper noun refers to a unique refer-
ential entity, even if this unicity is contextual. We
consider that this property is essential in the usage
of NEs in NLP.
5http://www.expasy.org/sprot/
143
Referential Autonomy NEs are also au-
tonomous from the referential point of view. It
is obvious in the case of proper nouns, which are
self-sufficient to identify the referent, at least in a
given communication situation (Eurotunnel). The
case of definite descriptions (The Channel Tunnel
operator) is a bit different: they can be used to
identify the referent thanks to external knowledge.
Denominational Stability Proper nouns are
also stable denominations. Even if some varia-
tions may appear (A. Merkel/Mrs Merkel), they
are more regular and less numerous than for other
noun phrases6.
Referential Relativity Interpretation is always
carried out relatively to a domain model, that can
be implicit in simple cases (for example, a country
or a person) but has to be made explicit when the
diversity in entities to consider increases.
3.2 Different Annotation Perspectives
The defining criteria do not play the same role
in all applications. In some cases (indexing and
knowledge integration), we focus on referential
entities which are designated by stable and non-
ambiguous descriptors. In those cases, the NEs
to use are proper nouns or indexing NEs and they
should be normalized to identify variations that
can appear despite their referential stability. For
this type of application, the main point is not to
highlight all the mentions of an entity in a doc-
ument, but to identify which document mentions
which entity. Therefore, precision has to be fa-
vored over recall. On the other hand, in the tasks
of information extraction and domain modelling, it
is important to identify all the mentions, including
definite descriptions (therefore, coreference rela-
tions between mentions that are not autonomous
enough from a referential point of view are also
important to identify).
As it is impossible to identify the mentions of all
the referential entities, the domain model defines
which entities are ?of interest? and the boundary
between what has to be annotated or not. For
instance, when a human resources director is in-
terested in the payroll in the organization, s/he
thinks in terms of personnel categories and not
in terms of the employees as individuals. This
appears in the domain model: the different cate-
gories of persons (technicians, engineers, etc.) are
6A contrario, this explains the importance of synonyms
identification in domains where denominations are not stable
(like, for instance, in genomics).
modelled as instances attached to the concept CAT-
OF-EMPLOYEES and the individuals are not rep-
resented. On the opposite, when s/he deals with
employees? paychecks and promotion, s/he is in-
terested in individuals. In this case, the model
should consider the persons as instances and the
categories of personnel as concepts.
Domain modelling implies making explicit
choices where texts can be fuzzy and mix points
of view. It is therefore impossible to annotate the
NEs of a text without refering to a model. In the
case of the above experiment, as it is often the
case, the model was simply described by a list of
concepts: the annotators had to name genes and
proteins, but also their families, compositions and
components.
4 Annotation methodology
Annotation guidelines As the targeted annota-
tion depends on what one wants to annotate and
how it will be exploited, it is important to provide
annotators with guidelines that explain what must
be annotated rather than how it should be anno-
tated. Very often, feasibility constraints overcome
semantic criteria,7 which confuses annotators. Be-
sides, it is important to take into consideration the
complexity of the annotation task, without exclud-
ing the dubious annotations or those which would
be too difficult to reproduce automatically. On the
contrary, one of the roles of manual annotation
is to give a general idea of the task complexity.
The annotators must have a clear view of the tar-
get application. This view must be based on an
explicit reference model, as that of GENIA, with
precise definitions and explicit modelling choices.
Examples can be added for illustration but they
should not replace the definition of the goal. It
is important that annotators understand the under-
lying logic of annotation. It helps avoiding mis-
understandings and giving them a sense of being
involved and committed.
Annotation tools Although there exists many
annotation tools, few are actually available, free,
downloadable and usable. Among those tools are
Callisto, MMAX2, Knowtator or Cadixe8 which
was used in the reported experiment. The features
7"In [src homology 2 and 3], it seems excessive to require
an NER program to recognize the entire fragment, however,
3 alone is not a valid gene name." (Tanabe et al, 2005).
8http://callisto.mitre.org, http://mmax2.sourceforge.net,
http://knowtator.sourceforge.net, http://caderige.imag.fr
144
and the annotation language expressivity must be
adapted to the targeted annotation task: is it suf-
ficient to type the textual segments or should they
also be related? is it possible/necessary to have
concurrent or overlapping annotations? In our ex-
periment on biology, for instance, although the an-
notators had the possibility to mention their un-
certainty by adding an attribute to the annotations,
they seldom did so, because it was not easy to do
using the provided interface.
Annotation evaluation Gut and Bayerl (2004)
distinguishes the inter-annotator agreement, which
measures the annotation stability, and the intra-
annotation agreement that gives an idea on how
reproducible an annotation is. The inter- and intra-
annotator agreements do not have to be measured
on the whole corpus, but quite early in the annota-
tion process, so that the annotation guidelines can
be modified. Another way to evaluate annotation
relies on annotator introspection. Annotators are
asked to auto-evaluate the reliability of their an-
notations and their (un)certainty attributes can be
used afterwards to evaluate the overall quality of
the work. Since we did not have several anno-
tators working independently on our biology cor-
pus, we asked them to indicate the uncertainty of
their annotations on a carefully selected sample
corpus. 25 files were extracted out of the 499 texts
of our corpus (5%). This evaluation required only
few hours of work and it enabled to better qualify
and quantity annotation confidence. The annota-
tors declared that around 20% of the total number
of annotation tags were "uncertain". We observed
that more than 75% of these uncertain tags were
associated to common nouns of type bacteria and
that uncertainty was very often (77%) linked to the
fact that distinguishing common and proper nouns
was difficult.
More generally, a good annotation methodology
consists in having several annotators working in-
dependently on the same sample corpus very early
in the process. It allows to quickly identify the dis-
agreement causes. If they can be solved, new rec-
ommendations are added to the annotation guide-
lines. If not, the annotation task might be simpli-
fied and the dubious cases eliminated.
5 Conclusion and Prospects
In the end, two main points must be considered for
a rigorous and efficient NE annotation in corpus.
First, as for the content, it is important to focus,
not on how to annotate, but rather on what to anno-
tate, according to the final application. Once spec-
ified what is to be annotated, one has to be cau-
tious in terms of methodology and consider from
the very beginning of the campaign, the evaluation
of the produced annotation.
We intend to apply this methodology to other
annotation campaigns of the project we participate
in. As those campaigns cover terminology and se-
mantic relations extraction, we will have to adapt
our method to those applications.
References
Maud Ehrmann. 2008. Les entit?s nomm?es, de la
linguistique au TAL : statut th?orique et m?thodes
de d?sambigu?sation. Ph.D. thesis, Univ. Paris 7.
Ulrike Gut and Petra Saskia Bayerl. 2004. Measuring
the reliability of manual annotations of speech cor-
pora. In Proc. of Speech Prosody, pages 565?568,
Nara, Japan.
Lynette Hirschman, Alexander Yeh, Christian
Blaschke, and Alfonso Valencia. 2005. Overview
of biocreative: critical assessment of information
extraction for biology. BMC Bioinformatics, 6(1).
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19:180?182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proc. of JNLPBA COLING 2004 Workshop, pages
70?75.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
and Lyle Ungar. 2004. Integrated annotation for
biomedical information extraction. In HLT-NAACL
2004 Workshop: Biolink. ACL.
LDC. 2004. ACE (Automatic Content Extraction)
english annotation guidelines for entities. Livrable
version 5.6.1 2005.05.23, Linguistic Data Consor-
tium.
David Nadeau and Satoshi Sekine. 2007. A survey
of named entity recognition and classification. Lin-
guisticae Investigaciones, 30(1):3?26.
B. Sundheim. 1995. Overview of results of the MUC-6
evaluation. In Proc. of the 6th Message Understand-
ing Conference. Morgan Kaufmann Publishers.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne
Matten, and John Wilbur1. 2005. Genetag: a tagged
corpus for gene/protein named entity recognition.
Bioinformatics, 6.
145
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 28?36,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Creating Sentiment Dictionaries via Triangulation
Josef Steinberger,
Polina Lenkova, Mohamed Ebrahim,
Maud Ehrmann, Ali Hurriyetoglu,
Mijail Kabadjov, Ralf Steinberger,
Hristo Tanev and Vanni Zavarella
EC Joint Research Centre
21027, Ispra (VA), Italy
Name.Surname@jrc.ec.europa.eu
Silvia Va?zquez
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona
silvia.vazquez@upf.edu
Abstract
The paper presents a semi-automatic approach
to creating sentiment dictionaries in many lan-
guages. We first produced high-level gold-
standard sentiment dictionaries for two lan-
guages and then translated them automatically
into third languages. Those words that can
be found in both target language word lists
are likely to be useful because their word
senses are likely to be similar to that of the
two source languages. These dictionaries can
be further corrected, extended and improved.
In this paper, we present results that verify
our triangulation hypothesis, by evaluating tri-
angulated lists and comparing them to non-
triangulated machine-translated word lists.
1 Introduction
When developing software applications for senti-
ment analysis or opinion mining, there are basi-
cally two main options: (1) writing rules that assign
sentiment values to text or text parts (e.g. names,
products, product features), typically making use of
dictionaries consisting of sentiment words and their
positive or negative values, and (2) inferring rules
(and sentiment dictionaries), e.g. using machine
learning techniques, from previously annotated doc-
uments such as product reviews annotated with an
overall judgment of the product. While movie or
product reviews for many languages can frequently
be found online, sentiment-annotated data for other
fields are not usually available, or they are almost
exclusively available for English. Sentiment dictio-
naries are also mostly available for English only or,
if they exist for other languages, they are not com-
parable, in the sense that they have been developed
for different purposes, have different sizes, are based
on different definitions of what sentiment or opinion
means.
In this paper, we are addressing the resource bot-
tleneck for sentiment dictionaries, by developing
highly multilingual and comparable sentiment dic-
tionaries having similar sizes and based on a com-
mon specification. The aim is to develop such dic-
tionaries, consisting of typically one or two thou-
sand words, for tens of languages, although in this
paper we only present results for eight languages
(English, Spanish, Arabic, Czech, French, German,
Italian and Russian). The task raises the obvious
question how the human effort of producing this re-
source can be minimized. Simple translation, be it
using standard dictionaries or using machine trans-
lation, is not very efficient as most words have two,
five or ten different possible translations, depending
on context, part-of-speech, etc.
The approach we therefore chose is that of trian-
gulation. We first produced high-level gold-standard
sentiment dictionaries for two languages (English
and Spanish) and then translated them automatically
into third languages, e.g. French. Those words that
can be found in both target language word lists (En
Fr and Es Fr) are likely to be useful because their
word senses are likely to be similar to that of the
two source languages. These word lists can then be
used as they are or better they can be corrected, ex-
tended and improved. In this paper, we present eval-
uation results verifying our triangulation hypothesis,
by evaluating triangulated lists and comparing them
28
to non-triangulated machine-translated word lists.
Two further issues need to be addressed. The
first one concerns morphological inflection. Auto-
matic translation will yield one word form (often,
but not always the base form), which is not suffi-
cient when working with highly inflected languages:
A single English adjective typically has four Spanish
or Italian word forms (two each for gender and for
number) and many Russian word forms (due to gen-
der, number and case distinctions). The target lan-
guage word lists thus need to be expanded to cover
all these morphological variants with minimal effort
and considering the number of different languages
involved without using software, such as morpho-
logical analysers or generators. The second issue
has to do with the subjectivity involved in the human
annotation and evaluation effort. First of all, it is im-
portant that the task is well-defined (this is a chal-
lenge by itself) and, secondly, the inter-annotator
agreement for pairs of human evaluators working on
different languages has to be checked in order to get
an idea of the natural variation involved in such a
highly subjective task.
Our main field of interest is news opinion min-
ing. We would like to answer the question how cer-
tain entities (persons, organisations, event names,
programmes) are discussed in different media over
time, comparing different media sources, media in
different countries, and media written in different
languages. One possible end product would be a
graph showing how the popularity of a certain en-
tity has changed over time across different languages
and countries. News differs significantly from those
text types that are typically analysed in opinion min-
ing work, i.e. product or movie reviews: While a
product review is about a product (e.g. a printer)
and its features (e.g. speed, price or printing qual-
ity), the news is about any possible subject (news
content), which can by itself be perceived to be pos-
itive or negative. Entities mentioned in the news can
have many different roles in the events described.
If the method does not specifically separate positive
or negative news content from positive or negative
opinion about that entity, the sentiment analysis re-
sults will be strongly influenced by the news context.
For instance, the automatically identified sentiment
towards a politician would most likely to be low if
the politician is mentioned in the context of nega-
tive news content such as bombings or disasters. In
our approach, we therefore aim to distinguish news
content from sentiment values, and this distinction
has an impact on the sentiment dictionaries: unlike
in other approaches, words like death, killing, award
or winner are purposefully not included in the sen-
timent dictionaries as they typically represent news
content.
The rest of the paper is structured as follows: the
next section (2) describes related work, especially
in the context of creating sentiment resources. Sec-
tion 3 gives an overview of our approach to dic-
tionary creation, ranging from the automatic learn-
ing of the sentiment vocabulary, the triangulation
process, the expansion of the dictionaries in size
and regarding morphological inflections. Section 4
presents a number of results regarding dictionary
creation using simple translation versus triangula-
tion, morphological expansion and inter-annotator
agreement. Section 5 summarises, concludes and
points to future work.
2 Related Work
Most of the work in obtaining subjectivity lexicons
was done for English. However, there were some
authors who developed methods for the mapping of
subjectivity lexicons to other languages. Kim and
Hovy (2006) use a machine translation system and
subsequently use a subjectivity analysis system that
was developed for English. Mihalcea et al (2007)
propose a method to learn multilingual subjective
language via cross-language projections. They use
the Opinion Finder lexicon (Wilson et al, 2005)
and two bilingual English-Romanian dictionaries to
translate the words in the lexicon. Since word am-
biguity can appear (Opinion Finder does not mark
word senses), they filter as correct translations only
the most frequent words. The problem of translat-
ing multi-word expressions is solved by translating
word-by-word and filtering those translations that
occur at least three times on the Web. Another ap-
proach in obtaining subjectivity lexicons for other
languages than English was explored in Banea et al
(2008b). To this aim, the authors perform three dif-
ferent experiments, with good results. In the first
one, they automatically translate the annotations of
the MPQA corpus and thus obtain subjectivity an-
29
notated sentences in Romanian. In the second ap-
proach, they use the automatically translated entries
in the Opinion Finder lexicon to annotate a set of
sentences in Romanian. In the last experiment, they
reverse the direction of translation and verify the as-
sumption that subjective language can be translated
and thus new subjectivity lexicons can be obtained
for languages with no such resources. Finally, an-
other approach to building lexicons for languages
with scarce resources is presented in Banea et al
(2008a). In this research, the authors apply boot-
strapping to build a subjectivity lexicon for Roma-
nian, starting with a set of seed subjective entries,
using electronic bilingual dictionaries and a training
set of words. They start with a set of 60 words per-
taining to the categories of noun, verb, adjective and
adverb obtained by translating words in the Opin-
ion Finder lexicon. Translations are filtered using a
measure of similarity to the original words, based on
Latent Semantic Analysis (Landauer and Dumais,
1997) scores. Wan (2008) uses co-training to clas-
sify un-annotated Chinese reviews using a corpus
of annotated English reviews. He first translates
the English reviews into Chinese and subsequently
back to English. He then performs co-training using
all generated corpora. Banea et al (2010) translate
the MPQA corpus into five other languages (some
with a similar ethimology, others with a very differ-
ent structure). Subsequently, they expand the fea-
ture space used in a Naive Bayes classifier using the
same data translated to 2 or 3 other languages. Their
conclusion is that expanding the feature space with
data from other languages performs almost as well
as training a classifier for just one language on a
large set of training data.
3 Approach Overview
Our approach to dictionary creation starts with semi-
automatic way of colleting subjective terms in En-
glish and Spanish. These pivot language dictionaries
are then projected to other languages. The 3rd lan-
guage dictionaries are formed by the overlap of the
translations (triangulation). The lists are then man-
ually filtered and expanded, either by other relevant
terms or by their morphological variants, to gain a
wider coverage.
3.1 Gathering Subjective Terms
We started with analysing the available English
dictionaries of subjective terms: General Inquirer
(Stone et al, 1966), WordNet Affect (Strapparava
and Valitutti, 2004), SentiWordNet (Esuli and Se-
bastiani, 2006), MicroWNOp (Cerini et al, 2007).
Additionally, we used the resource of opinion words
with associated polarity from Balahur et al (2009),
which we denote as JRC Tonality Dictionary. The
positive effect of distinguishing two levels of inten-
sity was shown in (Balahur et al, 2010). We fol-
lowed the idea and each of the emloyed resources
was mapped to four categories: positive, negative,
highly positive and highly negative. We also got
inspired by the results reported in that paper and
we selected as the base dictionaries the combination
of MicroWNOp and JRC Tonality Dictionary which
gave the best results. Terms in those two dictionar-
ies were manually filtered and the other dictionar-
ies were used as lists of candidates (their highly fre-
quent terms were judged and the relevant ones were
included in the final English dictionary). Keeping in
mind the application of the dictionaries we removed
at this step terms that are more likely to describe bad
or good news content, rather than a sentiment to-
wards an entity. In addition, we manually collected
English diminishers (e.g. less or approximately), in-
tensifiers (e.g. very or indeed) and invertors (e.g.
not or barely). The English terms were translated to
Spanish and the same filtering was performed. We
extended all English and Spanish lists with the miss-
ing morphological variants of the terms.
3.2 Automatic Learning of Subjective Terms
We decided to expand our subjective term lists by
using automatic term extraction, inspired by (Riloff
and Wiebe, 2003). We look at the problem of ac-
quisition of subjective terms as learning of seman-
tic classes. Since we wanted to do this for two dif-
ferent languages, namely English and Spanish, the
multilingual term extraction algorithm Ontopopulis
(Tanev et al, 2010) was a natural choice.
Ontopopulis performs weakly supervised learning
of semantic dictionaries using distributional similar-
ity. The algorithm takes on its input a small set of
seed terms for each semantic class, which is to be
learnt, and an unannotated text corpus. For example,
30
if we want to learn the semantic class land vehicles,
we can use the seed set - bus, truck, and car. Then
it searches for the terms in the corpus and finds lin-
ear context patterns, which tend to co-occur imme-
diately before or after these terms. Some of the
highest-scored patterns, which Ontopopulis learned
about land vehicles were driver of the X, X was
parked, collided with another X, etc. Finally, the
algorithm searches for these context patterns in the
corpus and finds other terms which tend to fill the
slot of the patterns (designated by X). Considering
the land vehicles example, new terms which the sys-
tem learned were van, lorry, taxi, etc. Ontopop-
ulis is similar to the NOMEN algorithm (Lin et al,
2003). However, Ontopopulis has the advantage to
be language-independent, since it does not use any
form of language-specific processing, nor does it use
any language-specific resources, apart from a stop
word list.
In order to learn new subjective terms for each
of the languages, we passed the collected subjective
terms as an input to Ontopopulis. For English, we
divided the seed set in two classes: class A ? verbs
and class B ? nouns and adjectives. It was necessary
because each of these classes has a different syn-
tactic behaviour. It made sense to do the same for
Spanish, but we did not have enough Spanish speak-
ers available to undertake this task, therefore we put
together all the subjective Spanish words - verbs, ad-
jectives and nouns in one class. We ran Ontopopulis
for each of the three classes - the class of subjective
Spanish words and the English classes A and B. The
top scored 200 new learnt terms were taken for each
class and manually reviewed.
3.3 Triangulation and Expansion
After polishing the pivot language dictionaries we
projected them to other languages. The dictionaries
were translated by Google translator because of its
broad coverage of languages. The overlapping terms
between English and Spanish translations formed
the basis for further manual efforts. In some cases
there were overlapping terms in English and Span-
ish translations but they differed in intensity. There
was the same term translated from an English posi-
tive term and from a Spanish very positive term. In
these cases the term was assigned to the positive cat-
egory. However, more problematic cases arose when
the same 3rd language term was assigned to more
than one category. There were also cases with dif-
ferent polarity. We had to review them manually.
However, there were still lots of relevant terms in the
translated lists which were not translated from the
other language. These complement terms are a good
basis for extending the coverage of the dictionaries,
however, they need to be reviewed manually. Even if
we tried to include in the pivot lists all morpholog-
ical variants, in the triangulation output there were
only a few variants, mainly in the case of highly in-
flected languages. To deal with morphology we in-
troduced wild cards at the end of the term stem (*
stands for whatever ending and for whatever char-
acter). This step had to be performed carefully be-
cause some noise could be introduced. See the Re-
sults section for examples. Although this step was
performed by a human, we checked the most fre-
quent terms afterwards to avoid irrelavant frequent
terms.
4 Results
4.1 Pivot dictionaries
We gathered and filtered English sentiment terms
from the available corpora (see Section 3.1). The
dictionaries were then translated to Spanish (by
Google translator) and filtered afterwards. By ap-
plying automatic term extraction, we enriched the
sets of terms by 54 for English and 85 for Spanish,
after evaluating the top 200 candidates suggested by
the Ontopolulis tool for each language. The results
are encouraging, despite the relevance of the terms
(27% for English and 42.5% for Spanish where
some missing morphological variants were discov-
ered) does not seem to be very high, considering the
fact that we excluded the terms already contained
in the pivot lists. If we took them into account, the
precision would be much better. The initial step re-
sulted in obtaining high quality pivot sentiment dic-
tionaries for English and Spanish. Their statistics
are in table 1. We gathered more English terms than
Spanish (2.4k compared to 1.7k). The reason for
that is that some translations from English to Span-
ish have been filtered. Another observation is that
there is approximately the same number of negative
terms as positive ones, however, much more highly
negative than highly positive terms. Although the
31
Language English Spanish
HN 554 466
N 782 550
P 772 503
HP 171 119
INT 78 62
DIM 31 27
INV 15 10
TOTAL 2.403 1.737
Table 1: The size of the pilot dictionaries. HN=highly
negative terms, N=negative, P=positive, HP=highly posi-
tive, INV=invertors, DIM=diminishers, INV=invertors.
frequency analysis we carried out later showed that
even if there are fewer highly positive terms, they are
more frequent than the highly negative ones, which
results in almost uniform distribution.
4.2 Triangulation and Expansion
After running triangulation to other languages the
resulted terms were judged for relevance. Native
speakers could suggest to change term?s category
(e.g. negative to highly negative) or to remove it.
There were several reasons why the terms could
have been marked as ?non-sentiment?. For instance,
the term could tend to describe rather negative news
content than negative sentiment towards an entity
(e.g. dead, quake). In other cases the terms were
too ambiguous in a particular language. Examples
from English are: like or right.
Table 2 shows the quality of the triangulated dic-
tionaries. In all cases except for Italian we had only
one annotator assessing the quality. We can see that
the terms were correct in around 90% cases, how-
ever, it was a little bit worse in the case of Russian
in which the annotator suggested to change category
very often.
Terms translated from English but not from Span-
ish are less reliable but, if reviewed manually, the
dictionaries can be expanded significantly. Table 3
gives the statistics concerning these judgments. We
can see that their correctness is much lower than in
the case of the triangulated terms - the best in Italian
(54.4%) and the worst in Czech (30.7%). Of course,
the translation performance affects the results here.
However, this step extended the dictionaries by ap-
proximately 50%.
When considering terms out of context, the most
common translation error occurs when the original
word has several meanings. For instance, the En-
glish word nobility refers to the social class of no-
bles, as well as to the quality of being morally good.
In the news context we find this word mostly in the
second meaning. However, in the Russian triangu-
lated list we have found dvoryanstvo , which refers
to a social class in Russian. Likewise, we need to
keep in mind that a translation of a monosemantic
word might result polysemantic in the target lan-
guage, thereby leading to confusion. For example,
the Italian translation of the English word champion
campione is more frequently used in Italian news
context in a different meaning - sample, therefore
we must delete it from our sentiment words list for
Italian. Another difficulty we might encounter es-
pecially when dealing with inflectional languages is
the fact that a translation of a certain word might be
homographic with another word form in the target
language. Consider the English negative word ban-
dit and its Italian translation bandito, which is more
frequently used as a form of the verb bandire (to an-
nounce) in the news context. Also each annotator
had different point of view on classifying the bor-
derline cases (e.g. support, agreement or difficult).
Two main reasons are offered to explain the low
performance in Arabic. On the one hand, it seems
that some Google translation errors will be repeated
in different languages if the translated words have
the same etymological root. For example both words
? the English fresh and the Spanish fresca ? are
translated to the Arabic as YK
Yg. meaning new. The
Other reason is a more subtle one and is related to
the fact that Arabic words are not vocalized and to
the way an annotator perceive the meaning of a given
word in isolation. To illustrate this point, consider
the Arabic word ? J. ?A
	
J ?? @ , which could be used
as an adjective, meaning appropriate, or as a noun,
meaning The occasion. It appears that the annotator
would intuitively perceive the word in isolation as a
noun and not as an adjective, which leads to disre-
garding the evaluative aspects of a given word.
We tried to include in the pivot dictionaries all
morphological variants of the terms. However, in
highly inflected languages there are much more vari-
ants than those translated from English or Spanish.
32
We manually introduced wild cards to capture the
variants. We had to be attentive when compiling
wild cards for languages with a rich inflectional sys-
tem, as we might easily get undesirable words in the
output. To illustrate this, consider the third person
plural of the Italian negative word perdere (to lose)
perdono, which is also homographic with the word
meaning forgiveness in English. Naturally, it could
happen that the wildcard captures a non-sentiment
term or even a term with a different polarity. For in-
stance, the pattern care% would capture either care,
careful, carefully, but also career or careless. That
is way we perform the last manual checking after
matching the lists expanded by wildcards against a
large number of texts. The annotators were unable
to check all the variants, but only the most frequent
terms, which resulted in reviewing 70-80% of the
term mentions. This step has been performed for
only English, Czech and Russian so far. Table 5
gives the statistics. By introducing the wildcards,
the number of distinct terms grew up significantly
- 12x for Czech, 15x for Russian and 4x for En-
glish. One reason why it went up also for English
is that we captured compounds like: well-arranged,
well-balanced, well-behaved, well-chosen by a sin-
gle pattern. Another reason is that a single pat-
tern can capture different POSs: beaut% can cap-
ture beauty, beautiful, beautifully or beautify. Not
all of those words were present in the pivot dictio-
naries. For dangerous cases like care% above we
had to rather list all possible variants than using a
wildcard. This is also the reason why the number
of patterns is not much lower than the number of
initial terms. Even if this task was done manually,
some noise was added into the dictionaries (92-94%
of checked terms were correct). For example, highly
positive pattern hero% was introduced by an anno-
tator for capturing hero, heroes, heroic, heroical or
heroism. If not checked afterwards heroin would
score highly positively in the sentiment system. An-
other example is taken from Russian: word meaning
to steal ukra% - might generate Ukraine as one most
frequent negative word in Russian.
4.3 How subjective is the annotation?
Sentiment annotation is a very subjective task. In ad-
dition, annotators had to judge single terms without
any context: they had to think about all the senses of
Metric Percent Agreement Kappa
HN 0.909 0.465
N 0.796 0.368
P 0.714 0.281
HP 0.846 0
N+HN 0.829 0.396
P+HP 0.728 0.280
ALL 0.766 0.318
Table 6: Inter-annotator agreement on checking the trian-
gulated list. In the case of HP all terms were annotated as
correct by one of the annotators resulting in Kappa=0.
Metric Percent Agreement Kappa
HN 0.804 0.523
N 0.765 0.545
P 0.686 0.405
HP 0.855 0.669
N+HN 0.784 0.553
P+HP 0.783 0.559
ALL 0.826 0.614
Table 7: Inter-annotator agreement on checking the can-
didates. In ALL diminishers, intensifiers and invertors
are included as well.
the term. Only if the main sense was subjective they
agreed to leave it in the dictionary. Another sub-
jectivity level was given by concentrating on distin-
guishing news content and news sentiment. Defining
the line between negative and highly negative terms,
and similarly with positive, is also subjective. In the
case of Italian we compared judgments of two anno-
tators. The figures of inter-annotator agreement of
annotating the triangulated terms are in table 6 and
the complement terms in table 7. Based on the per-
cent agreement the annotators agree a little bit less
on the triangulated terms (76.6%) compared to the
complement terms (82.6%). However, if we look at
Kappa figures, the difference is clear. Many terms
translated only from English were clearly wrong
which led to a higher agreement between the annota-
tors (0.318 compared to 0.614). When looking at the
difference between positive and negative terms, we
can see that there was higher agreement on the neg-
ative triangulated terms then on the positive ones.
33
Language Triangulated Correct Removed Changed category
Arabic 926 606 (65.5%) 316 (34.1%) 4 (0.4%)
Czech 908 809 (89.1%) 68 (7.5%) 31 (3.4%)
French 1.085 956 (88.1%) 120 (11.1%) 9 (0.8%)
German 1.053 982 (93.3%) 50 (4.7%) 21 (2.0%)
Italian 1.032 918 (89.0%) 36 (3.5%) 78 (7.5%)
Russian 966 816 (84.5%) 49 (5.1%) 101 (10.4%)
Table 2: The size and quality of the triangulated dictionaries. Triangulated=No. of terms coming directly from triangu-
lation, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive from triangulation, but annotator changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 1.092 335 (30.7%) 675 (61.8%) 82 (7.5%)
French 1.226 617 (50.3%) 568 (46.3%) 41 (3.4%)
German 1.182 548 (46.4%) 610 (51.6%) 24 (2.0%)
Italian 1.069 582 (54.4%) 388 (36.3%) 99 (9.3%)
Russian 1.126 572 (50.8%) 457 (40.6%) 97 (8.6%)
Table 3: The size and quality of the candidate terms (translated from English but not from Spanish). Terms=No. of
terms translated from English but not from Spanish, Correct=terms annotated as correct, Removed=terms not relevant
to sentiment analysis, Change category=terms in wrong category (e.g., positive in the original list, but annotator
changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 2.000 1.144 (57.2%) 743 (37.2%) 113 (5.6%)
French 2.311 1.573 (68.1%) 688 (29.8%) 50 (2.1%)
German 2.235 1.530 (68.5%) 660 (29.5%) 45 (2.0%)
Italian 2.101 1.500 (71.4%) 424 (20.2%) 177 (8.4%)
Russian 2.092 1.388 (66.3%) 506 (24.2%) 198 (9.5%)
Table 4: The size and quality of the translated terms from English. Terms=No. of (distinct) terms translated from En-
glish, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive in the original list, but annotator changed the category to highly positive).
Language Initial terms Patterns Matched terms
Count Correct Checked
Czech 1.257 1.063 15.604 93.0% 74.4%
English 2.403 2.081 10.558 93.8% 81.1%
Russian 1.586 1.347 33.183 92.2% 71.0%
Table 5: Statistics of introducing wild cards and its evaluation. Initial terms=checked triangulated terms extended by
relevant translated terms from English, Patterns=number of patterns after introducing wildcards, Matched terms=terms
matched in the large corpus - their count and correctness + checked=how many mentions were checked (based on the
fact that the most frequent terms were annotated).
34
4.4 Triangulation vs. Translation
Table 4 present the results of simple translation from
English (summed up numbers from tables 2 and 3).
We can directly compare it to table 2 where only
results of triangulated terms are reported. The per-
formance of triangulation is significantly better than
the performance of translation in all languages. The
highest difference was in Czech (89.1% and 57.2%)
and the lowest was in Italian (89.0% and 71.4%).
As a task-based evaluation we used the triangu-
lated/translated dictionaries in the system analysing
news sentiment expressed towards entities. The sys-
tem analyses a fixed word window around entity
mentions. Subjective terms are summed up and the
resulting polarity is attached to the entity. Highly
negative terms score twice more than negative, di-
minishers lower and intensifiers lift up the score. In-
vertors invert the polarity but for instance inverted
highly positive terms score as only negative pre-
venting, for instance, not great to score as worst.
The system searches for the invertor only two words
around the subjective term.
We ran the system on 300 German sentences
taken from news gathered by the Europe Media
Monitor (EMM)1. In all these cases the system at-
tached a polarity to an entity mention. We ran it with
three different dictionaries - translated terms from
English, raw triangulated terms (without the man-
ual checking) and the checked triangulated terms.
This pilot experiment revealed the difference in per-
formance on this task. When translated terms were
used there were only 41.6% contexts with correct
polarity assigned by the system, with raw triangu-
lated terms 56.5%, and with checked triangulated
terms 63.4%. However, the number does not contain
neutral cases that would increase the overall perfor-
mance. There are lots of reasons why it goes wrong
here: the entity may not be the target of the sub-
jective term (we do not use parser because of deal-
ing with many languages and large amounts of news
texts), the system can miss or apply wrongly an in-
vertor, the subjective term is used in different sense,
and irony is hard to detect.
1http://emm.newsbrief.eu/overview.html
4.5 State of progress
We finished all the steps for English, Czech and Rus-
sian. French, German, Italian and Spanish dictio-
naries miss only the introduction of wild cards. In
Arabic we have checked only the triangulated terms.
For other 7 languages (Bulgarian, Dutch, Hungarian,
Polish, Portuguese, Slovak and Turkish) we have
only projected the terms by triangulation. However,
we have capabilities to finish all the steps also for
Bulgarian, Dutch, Slovak and Turkish. We haven?t
investigated using more than two pivot languages for
triangulation. It would probably results in more ac-
curate but shortened dictionaires.
5 Conclusions
We presented our semi-automatic approach and cur-
rent state of work of producing multilingual senti-
ment dictionaries suitable of assessing the sentiment
in news expressed towards an entity. The triangula-
tion approach works significantly better than simple
translation but additional manual effort can improve
it a lot in both recall and precision. We believe that
we can predict the sentiment expressed towards an
entity in a given time period based on large amounts
of data we gather in many languages even if the per-
case performance of the sentiment system as on a
moderate level. Now we are working on improving
the dictionaries in all the discussed languages. We
also run experiments to evaluate the system on vari-
ous languages.
Acknowledgments
We thank Alexandra Balahur for her collaboration
and useful comments. This research was partly sup-
ported by a IULA-Universitat Pompeu Fabra grant.
35
References
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
and Bruno Pouliquen. 2009. Opinion mining from
newspaper quotations. In Proceedings of the Work-
shop on Intelligent Analysis and Processing of Web
News Content at the IEEE / WIC / ACM International
Conferences on Web Intelligence and Intelligent Agent
Technology (WI-IAT).
A. Balahur, R. Steinberger, M. Kabadjov, V. Zavarella,
E. van der Goot, M. Halkia, B. Pouliquen, and
J. Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of LREC?10.
C. Banea, R. Mihalcea, and J. Wiebe. 2008a. A boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC.
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan.
2008b. Multilingual subjectivity analysis using ma-
chine translation. In Proceedings of EMNLP.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of COLING.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Andrea Sanso`,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, English linguis-
tics. Franco Angeli, Milano, IT.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceeding of the 6th International Conference on Lan-
guage Resources and Evaluation, Italy, May.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of the ACL Workshop on
Sentiment and Subjectivity in Text.
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of the ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104:211?240.
W. Lin, R. Yangarber, and R. Grishman. 2003. Boot-
strapped learning of semantic classes from positive
and negative examples. In Proceedings of the ICML-
2003 Workshop on The Continuum from Labeled to
Unlabeled Data, Washington DC.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of ACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceeding of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The general inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics, M.I.T. Press, Cambridge, MA.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceeding of the
4th International Conference on Language Resources
and Evaluation, pages 1083?1086, Lisbon, Portugal,
May.
H. Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-
rski, M. Atkinson, and R.Steinberger. 2010. Exploit-
ing machine learning techniques to build an event ex-
traction system for portuguese and spanish. Lingua-
matica: Revista para o Processamento Automatico das
Linguas Ibericas.
X. Wan. 2008. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics and 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP.
36
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 84?93,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
On Named Entity Recognition in Targeted Twitter Streams in Polish
Jakub Piskorski
Linguistic Engineering Group
Polish Academy of Sciences
Jakub.Piskorski@ipipan.waw.pl
Maud Ehrmann
Department of Computer Science
Sapienza University of Rome
ehrmann@di.uniroma1.it
Abstract
This paper reports on some experiments
aiming at tuning a rule-based NER sys-
tem designed for detecting names in Pol-
ish online news to the processing of tar-
geted Twitter streams. In particular, one
explores whether the performance of the
baseline NER system can be improved
through the incremental application of
knowledge-poor methods for name match-
ing and guessing. We study various set-
tings and combinations of the methods and
present evaluation results on five corpora
gathered from Twitter, centred around ma-
jor events and known individuals.
1 Introduction
Recently, Twitter emerged as an important so-
cial medium providing most up-to-date informa-
tion and comments on current events of any kind.
This results in an ever-growing interest of vari-
ous organizations in tools for real-time monitor-
ing of Twitter streams to collect their business-
specific information therefrom for analysis pur-
poses. Since monitoring the entire Twitter stream
appears to be unfeasible due to the high volume
of published tweets, one usually monitors targeted
Twitter streams, i.e., streams of tweets potentially
satisfying specific information needs.
Applications for monitoring Twitter streams
usually require named entity recognition (NER)
capacity. However, due to the nature of Twitter
messages, i.e., being short, noisy, written in an in-
formal style, lacking punctuation and capitaliza-
tion, containing misspellings, non-standard abbre-
viations, and non grammatically correct sentences,
the application of even basic NLP tools (trained on
formal texts) on tweets usually results in poor per-
formances. In the case of well-formed texts such
as online news, exploitation of contextual clues is
crucial to named entity identification and classifi-
cation (e.g., ?Mayor of ? in the left context of a cap-
italized token is a reliable pattern to classify it as
city name). Such external evidence is often miss-
ing in tweets, and entity names are frequently in-
complete, abbreviated or glued with other words.
Furthermore, deployment of supervised ML-based
techniques for NER from tweets is challenging
due to the dynamic nature of Twitter.
In this paper, we report on experiments aiming
at tuning a rule-based NER system, initially de-
signed for detecting names in Polish online news,
to the processing of targeted Twitter streams. In
particular, we explore whether the performance of
the baseline NER system can be improved through
the utilization of knowledge-poor methods (based
on string distance metrics) for name matching
and name guessing. In comparison to English,
Polish is a free-word order and highly inflective
language, with particularly complex declension
paradigm of proper names, which makes NER for
Polish a more difficult task.
The remaining part of the paper is structured
as follows. First, Section 2 provides information
on related work. Next, Section 3 describes the
baseline NER system and the knowledge-poor en-
hancements. Subsequently, Section 4 presents the
evaluation results. Finally, Section 5 gives a sum-
mary and an outlook as regards future research.
2 Related Work
The problem of NER has gained lot of attention in
the last two decades and a vast bulk of research
on development of NER from formal texts ex-
ists (Nadeau and Sekine, 2007). Although most of
the reported work focused on NER for major lan-
guages, efforts on NER for Polish have also been
reported. (Piskorski, 2005) describes a rule-based
NER system for Polish that covers the classical
named-entity types, i.e., persons, locations, orga-
nizations, as well as numeral and temporal expres-
84
sions. (Marcin?czuk and Piasecki, 2007) and (Mar-
cin?czuk and Piasecki, 2010) report on a memory-
based learning and Hidden Markov Model ap-
proach resp. to automatic extraction of informa-
tion on events in the reports of Polish Stockhold-
ers, which involves NER. Also in (Lubaszewski,
2007) and (Lubaszewski, 2009) some general-
purpose information extraction tools for Polish
are addressed. Efforts related to creation of a
dictionary of Warsaw urban proper names ori-
ented towards NER is reported in (Savary et al,
2009; Marciniak et al, 2009). (Gralin?ski et al,
2009) present NERT, another rule-based NER sys-
tem for Polish which covers similar types of NEs
as (Piskorski, 2005). Finally, some efforts on
CRF-based NER methods for Polish are reported
in (Waszczuk et al, 2010) and (Marcin?czuk and
Janicki, 2012).
While NER from formal texts has been well
studied, relatively little work on NER for Twit-
ter was reported. (Locke and Martin, 2009) pre-
sented a SVM-based classifier for classifying per-
sons, locations and organizations in Twitter. (Rit-
ter et al, 2011) described an approach to segmen-
tation and classification of a wider range of names
in tweets based on CRFs (using POS and shallow
parsing features) and Labeled LDA resp. (Liu et
al., 2011) proposed NER (segmentation and clas-
sification) approach for tweets, which combines
KNN and CRFs paradigms. The reported preci-
sion/recall figures are significantly lower than the
state-of-the-art results for NER from well-formed
texts and oscillate around 50-80%. Better results
were reported in case of extracting names from
targeted tweets (person names from tweets on
live sport events) (Choudhury and Breslin, 2011).
(Nebhi, 2012) presented a rule-based NER system
for detecting persons, organizations and locations
which exploits an external global knowledge base
on entities to disambiguate NE type. (Liu et al,
2012) proposed a factor graph-based approach to
jointly conducting NER and NEN (Named Entity
Normalization), which improves F-measure per-
formance of NER and accuracy of NEN when
run sequentially. An Expectation-Maximization
approach to NE disambiguation problem was re-
ported by (Davis et al, 2012). Finally, (Li et al,
2012) presented an unsupervised system for ex-
tracting (no classification) NEs in targeted Twitter
streams, which exploits knowledge gathered from
the web and exhibits comparable performance to
the supervised approaches mentioned earlier.
Most of the above mentioned work on NER in
tweets focused on English. To our best knowledge
no prior work on NER in tweets in Polish has been
reported, which makes our effort a pioneering con-
tribution in this specific field. Our work also con-
tributes to NER from targeted Twitter streams.
3 Named Entity Extraction from
Targeted Tweets in Polish
The objective of this work is to explore vari-
ous linguistically lightweight strategies to adapt
an existing news-oriented rule-based NER system
for Polish to the processing of tweets in targeted
Twitter streams. Starting from the adaptation of
a NER rule-based system to the processing of
tweets (Section 3.1), we incrementally refine the
approach with, first, the introduction of a string
similarity-based name matching step (Section 3.2)
and, second, the exploitation of corpus statistics
and knowledge-poor method for name guessing
(Section 3.3).
3.1 NER Grammar for Polish
The starting point of our explorations is an exist-
ing NER system for Polish, modeled as a cascade
of finite-state grammars using the EXPRESS for-
malism (Piskorski, 2007). Similarly to rule-based
approaches to NER for many other Indo-European
languages, the grammars consist of a set of extrac-
tion patterns for person, organization and location
names. The patterns exploit both internal (e.g.,
company designators) and external clues (e.g., ti-
tles and functions of a person, etc.) for name de-
tection and classification; a simple extraction pat-
tern for person names can be illustrated as follows:
PER :> ( ( gazetteer & [TYPE: "firstname",
SURFACE: #F] )
( gazetteer & [TYPE: "initial",
SURFACE: #I] ) ?
( surname-candidate & [SURFACE: #L] )
):name
-> name: person & [NAME: #FULL-NAME]
& #full_name := ConcWithBlanks(#F,#I,#L).
This rule first matches a sequence consisting of: a
first name (through a gazetteer look-up), an op-
tional initial (gazetteer look-up as well) and, fi-
nally, a sequence of characters considered as sur-
name candidate (e.g., capitalized tokens), which
was detected by a lower-level grammar1 and
is represented as a structure of type surname-
candidate. The right-hand side of the extraction
1Lower-level grammar extract small-scale structures
which might constitute parts of named entities.
85
pattern specifies the output structure of type per-
son with one attribute called NAME, whose value
is simply a concatenation of the values of the vari-
ables #F, #I and #L assigned to the surface forms
of the matched first name, initial and surname can-
didate respectively.
Overall the grammar contains 15 extraction pat-
terns for person names, 10 for location names,
and 10 for organization names. It relies on a
huge gazetteer of circa 294K entries, which is
an extended version of the gazetteer described
in (Savary and Piskorski, 2011) and includes, i.a.,
39K inflected forms of both Polish and foreign
first names, 86K inflected forms of surnames, 5K
of organisation names (only partially inflected),
10K of inflected location names (e.g., city names,
country names, rivers, etc.). No morphological an-
alyzer for Polish is used and only a tiny fraction of
the extraction patterns relies on morphological in-
formation (encoded in the gazetteer). In this orig-
inal grammar, the patterns are divided into sure-
fire patterns and less reliable patterns (whose pre-
cision is expected to be lower). The latter ones
are patterns that rely solely on gazetteer informa-
tion (simple look-up), which might have ambigu-
ous interpretation, e.g., patterns that only match
first names in text. When applied on conven-
tional online news, the performance of this orig-
inal NER grammar oscillates around 85% in terms
of F-measure.
In order to process tweets, we slightly modi-
fied this grammar, mostly by simplifying it. Since
mentions of entities in tweets frequently occur as
single tokens (e.g., external evidence as in clas-
sical news is often missing), we did not keep the
distinction between sure-fire and less-reliable pat-
terns. Furthermore, the original NER grammar
?included? a mechanism (encoded directly in pat-
tern specification) to lemmatize the recognized
names as well as to extract various attributes such
as titles (e.g., ?Pan? (Mr.)) and position (e.g.,
?Prezydent? (president)) for persons. As we are
mainly interested in the detection and classifica-
tion of NEs while processing tweets, these func-
tionalities were not needed and the grammar sim-
ply extracts names and their type. This ?reduced?
NER grammar constitutes the baseline approach,
and will be referred to as BASE in the remain-
ing part of the paper. It is worth mentioning that
we tested as well a version of the grammar with
lower-cased lexical resources, but due to poor re-
sults (mainly due to high ambiguity of lower-case
lexical entries) we did not conduct further explo-
rations in this direction.
3.2 String distance-based Name Matching
In tweets, names are often abbreviated (e.g., ?Parl.
Europ.? and ?PE? are abbreviations of ?Parla-
ment Europejski?), glued to other words (e.g.,
?prezydent Komorowski? is sometimes written as
?prezydentKomorowski?) and misspelled variants
are frequent (e.g., ?Donlad Tusk? is a frequent
misspelling of ?Donald Tusk?). The NER gram-
mar ?as is? would fail to recognize the particular
names in the aforementioned examples. There-
fore, in order to improve the recall of the ?tweet
grammar?, we perform a second run deploying
string distance metrics (in the entire targeted Twit-
ter stream) for matching new mentions of names
previously recognized by the NER grammar (see
Section 3.1). Furthermore, due to the highly in-
flective character of Polish, we also expect to cap-
ture with string distance metrics non-nominative
mentions of names (e.g., ?Rzeczpospolitej - geni-
tive/dative/locative form of ?Rzeczpospolita? - the
name of a Polish daily newspaper), which the NER
grammar might have failed to recognize.
Inspired by the work reported in (Piskorski et
al., 2009) we explored the performance of sev-
eral string distance metrics. First, we tested the
baseline Levenshtein edit distance metric given
by the minimum number of character-level oper-
ations (insertion, deletion, or substitution) needed
to transform one string into another (Levenshtein,
1965). Next, we used an extension thereof, namely
Smith-Waterman (SW) metric (Smith and Water-
man, 1981), which additionally allows for vari-
able cost adjustment to the cost of a gap and vari-
able cost of substitutions (mapping each pair of
symbols from alphabet to some cost). We used a
variant of this metric, where the Smith-Waterman
score is normalized using the Dice coefficient (the
average length of strings compared).
Subsequently, we explored variants of the Jaro
metric (Jaro, 1989; Winkler, 1999). It considers
the number and the order of the common char-
acters between the two strings being compared.
More precisely, given two strings s = a1 . . . aK
and t = b1 . . . bL, we say that ai in s is common
with t if there is a bj = ai in t such that i ? R ?
j ? i+R, where R = bmax(|s|, |t|)/2c? 1. Fur-
thermore, let s? = a?1 . . . a
?
K? be the characters in
86
s which are common with t (with preserved order
of appearance in s) and let t? = b?1 . . . b
?
L? be de-
fined analogously. A transposition for s? and t? is
defined as any position i such that a?i 6= b
?
i. Let us
denote the number of transpositions for s? and t?
as Ts?,t? . The Jaro similarity is then calculated as:
J(s, t) =
1
3
? (
|s?|
|s|
+
|t?|
|t|
+
|s?| ? bTs?,t?/2c
|s?|
)
A Winkler variant of Jaro metric boosts this
similarity for strings with agreeing initial charac-
ters and is calculated as:
JW (s, t) = J(s, t) + ? ? boostp(s, t) ? (1? J(s, t))
where ? denotes the common prefix adjustment
factor (default value is 0.1) and boostp(s, t) =
min(|lcp(s, t)|, p). Here lcp(s, t) denotes the
longest common prefix between s and t. Further, p
stands for the upper bound of |lcp(s, t)|2 , i.e., up
from a certain length of lcp(s, t) the ?boost value?
remains the same.
The q-gram metric (Ukkonen, 1992) is based
on the intuition that two strings are similar if
they share a large number of character-level q-
grams. We used a variant thereof, namely skip-
gram metric (Keskustalo et al, 2003), which ex-
hibited better performance than any other variant
of character-level q-grams based metrics. It is
based on the idea that in addition to forming bi-
grams of adjacent characters, bigrams that skip
characters are considered. Gram classes are de-
fined that specify what kind of skip-grams are cre-
ated, e.g. {0, 1} class means that normal bigrams
are formed, and bigrams that skip one character.
In particular, we tested {0, 1} and {0, 2} classes.
Due to the nature of Twitter we expected skip-
grams to be particularly useful in our experiments.
Considering the declension paradigm of Polish
we also considered the basic CommonPrefix met-
ric introduced in (Piskorski et al, 2009), which is
based on the longest common prefix. It is calcu-
lated as:
CP (s, t) = (|lcp(s, t)|)2/|s| ? |t|
Finally, we evaluated the performance of
longest common sub-strings distance metric,
which recursively finds and removes the longest
2Here p is set to 6.
common sub-string in the two strings compared.
Let lcs(s, t) denote the first longest common sub-
string for s and t and let s?p denote a string ob-
tained by removing from s the first occurrence of
p in s. The LCS metric is calculated as:
LCS(s, t) =
?
?
??
?
??
0 if |lcs(s, t)| ? 2
|lcs(s, t)|+ LCS(s?lcs(s,t), t?lcs(s,t))
otherwise
The string distance-based name matching de-
scribed in this section will be referred to as
MATCH-X, with X standing for the name of the
string distance metric being used.
3.3 Name Clustering
Since contextual clues for recognizing names in
formal texts are often missing in tweets, we ad-
ditionally developed a rudimentary name guesser
to boost the recall. Let us also observe that using
string distance metrics described in Section 3.2 to
match all not yet captured mentions of previously
recognized names might not be easy due the fact
that the process of creating abbreviations in Twit-
ter is very productive, e.g., ?Rzeczpospolita? ap-
pears abbreviated as ? Rzepa?, Rzp. or ?RP, which
are substantially different from the original name.
The main idea beyond the name guesser is based
on the following assumption: given a targeted
Twitter stream, if a capitalized word n-gram has
a couple of ?similar? word n-grams in the same
stream, most of which are not recognized as valid
word forms, then such a group of n-grams word
are most likely named mentions of the same entity
(e.g., person, organization or location, etc.). To be
more precise, the name guesser works as follows.
1. Compute S = {s1, s2, ....sk} - a set of word
uni- and bigrams (cluster seeds) in the Twit-
ter stream3, where frequency(si) ? ?4 and
character ? length(si) ? 3 for all si ? S.
2. Create an initial set of singleton ?name? clus-
ters: C = {C1, C2, . . . , Ck} with Ci = {si}.
3. Build clusters of simmilar n-grams
around the selected uni- and bigrams
3The vast majority of names annotated in our test corpus
are either word unigrams or bigrams (see Section 4.1.)
4? We explored various values of this parameter, which is
described in Section 4.2
87
using the string distance metric m: As-
sign each word n-gram w in the Twitter
stream to at most one cluster Cj with
j ? arg minx?{1,2,...,k} distm(sx, w)
5, and
distm(sj , w) ? maxDist, where maxDist
is a predefined constant.
4. Iteratively merge most-simmilar clusters in
C: If ?Cx, Cy ? C with DIST (Cx, Cy) ?
DIST (Ci, Cj) for i, j ? {1, . . . , |C|}6 and
DIST (Cx, Cy) ? maxDist then C = C \
{Cx, Cy} ? (Cx ? Cy).
5. Discard ?small? clusters:
C = {Cx ? C : |Cx| ? 3}
6. Discard clusters containing high number of
n-grams, whose parts are valid word forms,
but not proper names: C = {Cx ?
C : ?w?Cx
WordForm?(w)
|Cx|
? 0.3}, where
WordForm?(w) = 1 if all the words
constituting the word n-gram w are valid
word forms, but not proper names, and
WordForm?(w) = 0 otherwise, e.g.,
WordForm?(Jan Grzyb) = 0 since Grzyb
(eng. mushroom) can be interpreted as a
valid word form, which is not a proper name,
whereas Jan has only proper name interpre-
tation.
7. Use the n-grams in the remaining clusters
in C (each of them is considered to contain
named mentions of the same entity) to match
names in the Twitter stream through simple
lexicon look-up.
For computing similarity of n-grams and merg-
ing clusters we used the longest common sub-
strings (LCS) metric which performed on average
best (in terms of F-measure) in the context of name
matching (see Section 3.2 and 4). For checking
whether tokens constitute valid word forms we ex-
ploited PoliMorf (Wolin?ski et al, 2012), a freely
available morphological dictionary of Polish, con-
sisting of circa 6.7 million word forms, includ-
ing proper names. Proper names are distinguished
from other entries in the aforementioned resource.
The name guesser sketched above will be re-
ferred to as CLUSTERING. Instead of building the
5We denote the distance between two strings x and y mea-
sured with the string distance metric m as distm(x, y)
6DIST (Cx, Cy) = ?s?Cx?t?Cy
distm(s,t)
|Cx|?|Cy|
(average
distance between strings in the two clusters)
name clusters around n-grams, whose frequency
exceeds certain threshold, we also tested building
clusters around least frequent n-grams (i.e., whose
frequency is ? 3), which will be referred to as
CLUSTERING-INFRQ. The name guesser runs ei-
ther independently or on top of the NER grammar
described in Section 3.1 in order to detect ?new?
names in the unconsumed part of the tweet collec-
tion, i.e., names recognized by the grammar are
preserved. It is important to emphasize that the
clustering-based name guesser only detects names
without classifying them.
4 Experiments
4.1 Dataset
We have gathered tweet collections using Twit-
ter search API7 focusing on some major events in
2012/2013 and on famous individuals, namely: (a)
Boston marathon bombings, (b) general comments
on Donald Tusk, the prime minister of Poland,
(c) discussion on the public comments of Antoni
Macierewicz (a politician of the Law and Justice
opposition party in Poland) on the Polish presi-
dent crash in Smolen?sk (Russia) in 2010, (d) de-
bate on the controversial firing of the journalist
Cezary Gmyz from one of the major Polish news-
papers Rzeczpospolita and, (e) a collection of ran-
dom tweets in Polish. Each tweet collection was
extracted using simple queries, e.g., "zamach AND
(Boston OR Bostonie)" ("attack" AND "?Boston"?
either in nominative of locative form) for collect-
ing tweets on the Boston bombings. From each
collection a subset of randomly chosen tweets was
selected for evaluation purposes. We will refer
to the latter as the test corpus, whereas the entire
tweet collections will be referred to as the stream
corpus.
In the stream corpus, we computed for each
tweet: (a) the text-like fraction of its body, i.e., the
fraction of the body which contains text, and (b)
the lexical validity, i.e., the percentage of tokens in
the text-like part of the body of the tweet which are
valid word forms in Polish8. Figure 1 and 2 show
the histograms for text-like fraction and lexical va-
lidity of the tweets in each collection in the stream
corpus. We can observe that large portion of the
tweets contains significant text-like part, which is
7https://dev.twitter.com
8For computing lexical validity we used
PoliMorf (Wolin?ski et al, 2012), already mentioned in
the previous section.
88
also lexically valid. Interestingly, the random col-
lection exhibits lower lexical validity, which is due
to more colloquial language used in the tweets in
this collection.
  10 20 30 40 50 60 70 80 90 1000
5
10
15
20
25
30 Boston Tusk Macierewicz Gmyz Random
Text-like fraction
Percent
age of T
weets
Figure 1: Text-like fraction of the tweets in each
collection.
  10 20 30 40 50 60 70 80 90 1000
5
10
15
20
25
30
35 Boston Tusk Macierewicz Gmyz Random
Text-lik litftra
ceo-enr
lPekgwks
?eer?
Figure 2: Lexical validity of the tweets in each
collection.
We built the test corpus by randomly select-
ing tweets whose text-like fraction of the body
was ? 80%, additionally checking the language
and removing duplicates. These tweets were af-
terwards manually annotated with person, loca-
tion and organization names, according to the fol-
lowing guidelines: consideration of unigram en-
tities, non-inclusion of titles, functions and alike,
non-inclusion of spurious punctuation marks and
exclusion of names starting with ?@?, since their
recognition as names is trivial.
The test corpus statistics are provided in Ta-
ble 1. We provide in brackets the number of tweets
in the corresponding tweet collections in the en-
tire stream corpus. In this test corpus, 86,7% of
the annotated names are word unigrams, whereas
bigrams constitute 12,7% of the annotated names
and 3- and 4-grams account only for a tiny frac-
tion (0,6%); this is in line with the characteristics
of the Twitter language, which favours quick and
simple expressions. For each collection, we com-
puted the name diversity as the ratio between en-
tity occurrences and unique entities, as well as the
average number of entities per tweet9. Targeted
stream corpora show a medium name diversity
(except for Boston and Gmyz collections, centred
on a very specific location and person name resp.)
and a high rate of entity per tweet (around 2.2), in
contrast with random corpus which shows a high
name diversity (0.79) for a low average number of
entity per tweets. Reported to the limited number
of characters in tweets (140), the important signifi-
cant number of entity per tweet in targeted streams
accounts, on the one hand, for the usefulness of
working on targeted streams and, on the other, for
the importance of NER in tweets.
Corpus #tweets name #names #PER #LOC #ORG
diversity per
tweet
Boston 198 0.24 2.16 34 298 96
(2953)
Tusk 232 0.36 2.42 393 88 80
(1186)
Macierewicz 303 0.32 2.17 494 60 104
(931)
Gmyz 310 0.24 2.09 471 18 159
(672)
Random 286 0.79 0.36 59 19 27
(7806)
Table 1: Test corpus statistics.
4.2 Evaluation
In our experiments we evaluated the performance
of (i) the NER grammar (BASE), a combina-
tion thereof with (ii) different name matching
strategies (MATCH) and (iii) different variants of
the name guesser (CLUSTERING, CLUSTERING-
INFRQ) and, finally, (iv) the combinations of all
techniques. Within the MATCH configuration, we
experimented all string distance metrics presented
in 3.2 but since Jaro, Jaro-Winkler and Smith-
Waterman metrics performed on average worse
than the others, we did not consider them in
further experiments. We selected the best per-
forming metric, LCS 10, as the one used by the
name guesser (CLUSTERING) in subsequent exper-
iments. As a complement, we measured the per-
formance of the name guesser alone to compare
it with BASE. Furthermore, name matching and
9In the limit of our reference corpora, i.e. entities of type
person, location and organization.
10Skip-grams was the other metric which exhibited similar
performance
89
name guessing algorithms were using the tweet
collections in the stream corpus (as quasi ?Twitter
stream window?) in order to gather knowledge for
matching/guessing ?new? names in the test corpus.
We measured the performance of the different
configurations in terms of Precision (P), Recall
(R) and F-measure (F), according to two differ-
ent schemes: exact match, where entity types and
both boundaries should match perfectly, and fuzzy
match, which allows for one name boundary re-
turned by the system to be different from the ref-
erence, i.e., either too short or too long on the left
or on the right, but not on both. Furthermore, since
the clustering-based name guesser described in 3.3
does not classify names, for any settings with this
technique we only evaluated name detection per-
formance, i.e., no distinction between name types
was made. The overall summary of the results for
the entire pool of tweet collections, is presented in
Table 3.
In the context of the CLUSTERING algorithm we
explored various settings as regards the minimum
frequency of an n-gram to be considered as clus-
ter seed (? parameter - see Section 3.3). More
precisely, we tested values in the range of 1 to
30 for all corpora and system settings which in-
cluded CLUSTERING, and compared the resulting
P/R and F figures. An example of a curve with P/R
values (exact match) of BASE-CLUSTERING algo-
rithm applied on the ?Boston? corpus with vary-
ing values of ? is given in Figure 3. One can ob-
serve and hypothesize that the frequency threshold
does not impact much the performance. Suchlike
curves for other settings were of a similar nature.
Therefore we decided to set the ? to 1 in all set-
tings reported in Table 3.
4.3 Results analysis
The performance of the NER grammars is surpris-
ingly good, both in case of exact and fuzzy match
evaluation. Except for random corpus (which
shows rather low performance with 55% precision
and 39% recall), precision figures oscillate around
85-95%, whereas recall is somewhat worse (60-
75%), as was to be expected. The low recall for
?Gmyz? corpus is due to the non-matching of a fre-
quently occurring person name. Precision and re-
call figures for each entity type for BASE are given
in Table 2. In general, recognition of organization
names appears to be more difficult (lower recall),
especially in the random corpus.
  1 0 2 3 4 5 6 7 8 19 11 10 12 13 14 15 16 17 18 09 01 00 02 03 04 05 06 07 08 29
99B1
9B09B2
9B39B4
9B59B6
9B79B8
1 ostnTuTkM atncii
Texet-tlik frtlafco-cxnPllgwlsclnwx?e?cfc?lrln?-?gcfl?cc?
Figure 3: Precision and Recall figures for BASE-
CLUSTERING applied on ?Boston? corpus, with
different frequency thresholds of n-grams to be
considered cluster seeds.
Corpus PER ORG LOC
P R P R P R
Boston 31.6 35.3 87.9 30.2 94.3 71.8
Tusk 87.6 71.2 82.4 35.0 89.9 70.5
Gmyz 85.5 32.5 82.8 15.1 88.9 44.4
Macierewicz 93.6 80.2 71.2 35.6 83.7 60.0
Random 56.7 55.9 0 0 53.3 42.1
Table 2: Precision/recall figures for person, or-
ganization and location name recognition (exact
match) with BASE.
Extending BASE with MATCH yields some im-
provements in terms of recall (including random
corpus), whereas precision either oscillates around
the figures achieved by BASE, or deteriorates. In
case of ?Gmyz? corpus, we can observe significant
gain in both recall and precision through using the
name matching step. With regard to the other cor-
pora, the reason for not obtaining a significant gain
could be due to two reasons: (a) the n-grams iden-
tified as similar to the names recognized by BASE
are already covered by BASE with some patterns
(e.g., inflected forms of many entities are stored in
the gazetteer), or (b) using string distance metrics
in the MATCH step might not be the best method to
capture mentions of a recognized entity, as exem-
plified in Table 4, where the mentions of a news-
paper Rzeczpospolita (captured by BASE) may be
significantly different, e.g., in terms of the charac-
ter length.
Regarding the results for CLUSTERING-INFRQ,
running it alone, yielded poor results for all cor-
pora, only in case of the?Gmyz? corpus a gain
could be observed. CLUSTERING performed better
than CLUSTERING-INFRQ for all corpora.
Deploying BASE with CLUSTERING on top of
it results in up to 1.5-6% (exact match) and 4-
90
EXACT MATCH
Method Boston Tusk Gmyz Macierewicz AVERAGE
P R F P R F P R F P R F P R F
BASE 85.6 59.6 70.2 87.7 65.9 75.3 85.3 28.5 42.8 90.5 71.3 79.8 87.3 56.3 67.0
BASE-MATCH-LEV 80.8 62.9 70.7 87.4 66.5 75.5 90.9 63.6 74.8 90.2 72.3 80.3 87.3 66.3 75.3
BASE-MATCH-SW 70.9 62.1 66.3 76.6 67.5 71.8 78.0 59.1 68.0 89.4 73.1 80.4 78.7 65.5 71.6
BASE-MATCH-J 67.7 62.1 64.8 79.3 68.1 73.3 60.9 48.3 53.9 60.0 73.3 65.9 67.0 63.0 64.5
BASE-MATCH-JW 63.2 62.1 62.7 75.5 68.3 71.7 48.2 48.9 48.6 58.0 74.0 65.0 61.2 63.3 62.0
BASE-MATCH-SKIP(0,1) 80.9 62.1 70.3 87.6 66.5 75.6 91.3 63.0 74.5 90.3 72.2 80.2 87.5 66.0 75.2
BASE-MATCH-SKIP(0,2) 80.9 62.1 70.3 87.7 66.3 75.5 91.5 63.0 74.6 90.6 72.2 80.4 87.7 65.9 75.2
BASE-MATCH-CP 80.2 59.6 68.4 87.7 66.0 75.3 83.5 58.6 68.9 90.2 71.4 79.7 85.4 63.9 73.1
BASE-MATCH-LCS 80.7 63.6 71.1 86.8 67.0 75.7 82.3 59.0 68.7 90.2 72.9 80.7 85 65.6 74.1
CLUSTERING 66.2 10.0 17.4 60.6 33.2 42.9 61.3 36.0 45.3 52.9 33.4 41.0 60.3 28.2 36.7
CLUSTERING-INFRQ 37.5 1.4 2.7 27.3 1.1 2.1 60.7 31.5 41.5 54.8 28.6 37.6 45.1 15.7 21.0
BASE-CUSTERING 86.8 67.8 76.1 91.1 72.7 80.9 80.6 61.0 69.4 86.3 74.6 80.0 86.2 69.0 76.6
BASE-CLUSTERING-INFRQ 89.7 65.0 75.3 89.4 69.3 78.1 81.2 58.5 68.0 89.9 74.2 81.3 87.6 66.8 75.7
BASE-MATCH-CLUSTERING 87.6 75.9 81.4 90.2 73.8 81.2 74.1 62.8 68.0 86.1 76.3 80.9 84.5 72.2 77.9
BASE-MATCH-CLUSTERING-INFRQ 90.0 73.4 80.8 88.6 70.4 78.5 74.3 60.3 66.6 89.6 75.8 82.1 85.6 70.0 77.0
FUZZY MATCH
Method Boston Tusk Gmyz Macierewicz AVERAGE
P R F P R F P R F P R F P R F
BASE 86.6 60.3 71.1 92.2 69.3 79.1 88.0 29.5 44.2 95.0 74.8 83.7 90.5 58.5 69.5
BASE-MATCH-LEV 81.7 63.6 71.5 92.3 70.2 79.8 93.6 65.4 77.0 94.9 76.1 84.5 90.6 68.8 78.2
BASE-MATCH-SW 73.3 64.3 68.5 80.8 71.3 75.8 91.4 67.6 77.7 94.2 77.1 84.8 84.9 70.1 76.7
BASE-MATCH-J 70.5 64.7 67.5 85.5 73.4 79.0 86.2 68.4 76.2 63.4 77.5 69.8 76.4 71.0 73.1
BASE-MATCH-JW 65.8 64.7 65.3 81.9 74.0 77.7 68.2 69.1 68.7 61.4 78.4 68.9 69.3 71.6 70.2
BASE-MATCH-SKIP(0,1) 81.8 62.9 71.1 92.3 70.1 79.6 94.0 64.8 76.7 95.1 76.0 84.5 90.8 68.5 78.0
BASE-MATCH-SKIP(0,2) 81.8 62.9 71.1 92.2 69.7 79.4 94.2 64.8 76.8 95.0 75.7 84.3 90.8 68.3 77.9
BASE-MATCH-CP 81.1 60.3 69.2 92.2 69.3 79.1 93.8 65.9 77.4 95.0 75.2 84.0 90.5 67.7 77.4
BASE-MATCH-LCS 81.6 64.3 71.9 92.4 71.3 80.5 93.1 66.7 77.7 94.9 76.7 84.9 90.5 69.8 78.8
CLUSTERING 83.1 12.6 21.9 96.4 52.8 68.2 89.2 52.3 66.0 87.7 55.5 68.0 89.1 43.3 56.0
CLUSTERING-INFRQ 87.5 3.3 6.3 68.2 2.7 5.1 91.1 47.2 62.2 94.2 49.1 64.5 85.3 25.6 34.5
BASE-CLUSTERING 93.1 72.7 81.6 96.9 77.4 86.0 94.5 71.4 81.4 91.7 79.3 85.1 94.1 75.2 83.5
BASE-CLUSTERING-INFRQ 95.5 69.2 80.2 95.9 74.3 83.7 96.4 69.4 80.7 96.9 79.9 87.6 96.2 73.2 83.1
BASE-MATCH-CLUSTERING 93.3 80.8 86.6 96.5 79.0 86.9 92.9 78.7 85.2 91.8 81.3 86.2 93.6 80.0 86.2
BASE-MATCH-CLUSTERING-INFRQ 95.1 77.6 85.5 96.0 76.3 85.0 94.5 76.7 84.7 96.6 81.8 88.6 95.6 78.1 86.0
Table 3: Precision, Recall and F-measure figures for exact (top) and fuzzy match (bottom). The best
results are highlighted in bold.
CEZARY GMYZ zwolniony z "Rzeczpospolitej". To efekt spotkania z
Zarza?dem i Rada? Nadzorcza? wydawcy dziennika http://t.co/QspE3edh
@agawaa ...usi?ujesz czepic? sie szczeg??u, gdy istota sprawy jest taka:
Rzepa/Gmyz pitolili bez sensu.
Konflikt w Rzepie? Ta ca?a sytuacja na to wskazuje. Gmyz sie? nie wycofuje,
a Rzepa jak najbardziej.
@volanowski Nowa linia: Gmyz wyrzucony z Rzepy czyli PO we wszystkich
sprawach smolen?skich jest cacy i super. Ludzie na to nie p?jda.
@TomaszSkory Byc? moz?e "Rz" i Gmyz p?aca? teraz w?as?nie za "skr?ty
mys?lowe" swoich informator?w. Dlaczego RMF nie p?aci za "skr?ty" swoich?
Gmyz wylecia? z RP, a Ziemkiewicz straci? Subotnik? Nie lepiej by?o nieco
zejs?c? z 3.50 z?, czy chodzi o cos? zupe?nie innego?
Gmyz wyrzucony z "Rzeczpospolitej". "Dzisiaj zwolniono mnie dyscyp-
linarnie": Cezary Gmyz straci? prace? w "Rzeczp... http://t.co/ObZIxXML
Table 4: Examples of various ways of referring to
a newspaper Rzeczpospolita in tweets.
10% (fuzzy match) gain in F-measure compared
to BASE (mainly thanks to gain in recall), ex-
cept ?Gmyz? corpus, where the gain is higher.
The average gain over the four targeted corpora
against the best combination of BASE-MATCH in
F-measure is 1.3%. We observed comparable im-
provement for the random corpus. It turned out
CLUSTERING often contributes to the recognition
of names glued to other words and/or character se-
quences.
Combining BASE with MATCH-LCS and CLUS-
TERING/CLUSTERING-INFRQ yields further im-
provements against the other settings. In par-
ticular, the gain in F-measure of BASE-MATCH-
CLUSTERING against BASE, measured over the
four targeted corpora, is 10.9% and 16.7% for ex-
act and fuzzy match respectively (mainly due to
gain in recall).
Considering the nature of Twitter messages the
average F-measure score over the four targeted
corpora for BASE-MATCH-CLUSTERING, amount-
ing to 77.9% (exact match) and 86.2% (fuzzy
match) can be seen as a fairly good result. Al-
though the difference in some of the correspond-
ing scores for exact and fuzzy match appear sub-
stantial, it is worth mentioning that CLUSTERING
algorithm often guesses name candidates that are
either preceded or followed by some characters
not belonging to the name itself, which is pe-
nalized in exact-match evaluation. This problem
could be alleviated through deployment of heuris-
tics to trim such ?unwanted? characters. Another
source of false positives extracted by CLUSTER-
ING is the fact that this method might, beyond
person, organization and location types, recognize
any kind of NEs, which, even not very frequent, is
penalized since they are not present in our refer-
ence corpus.
In general, considering the shortness of names
in Twitter, the major type of errors in all settings
are either added or missed entities, but more rarely
overlapping problems. One of the main source of
errors is due to the fact that single-token names,
which are frequent in tweets, often exhibit type
91
ambiguity. Once badly recognized, these errors
are propagated over the next processing steps.
5 Conclusions and Outlook
In this paper we have reported on experiments on
tuning an existing finite-state based NER gram-
mar for processing formal texts to NER from
targeted Twitter streams in Polish through com-
bining it with knowledge-poor techniques for
string distance-based name matching and corpus
statistics-based name guessing. Surprisingly, the
NER grammar alone applied on the four test cor-
pora (including circa 2300 proper names) yielded
P, R, and F figures for exact (fuzzy) matching
proper names (including: person, organization and
locations) of 87.3% (90.5%), 56.3% (58.5) and
67% (69.5%) resp., which can be considered fairly
reasonable result, though some variations across
tweet collections could be observed (depending
on the topic and how people ?tweet? about).
The integration of the presented knowledge-poor
techniques for name matching/guessing resulted
in P, R and F figures for exact (fuzzy) match-
ing names of 84.5% (93.6%), 72.2% (80.0) and
77.9% (86.2%) resp. (setting with best F-measure
scores), which constitutes a substantial improve-
ment against the grammar-based approach. We
can observe that satisfactory-performing NER
from targeted Twitter streams in Polish can be
achieved in a relatively straightforward manner.
As future work to enhance our experiments, we
envisage to: (a) enlarge the pool of test corpora,
(b) carry out a more thorough error analysis, (c)
test a wider range of string distance metrics (Co-
hen et al, 2003), (d) study the applicability of the
particular NER grammar rules w.r.t. their useful-
ness in NER in targeted Twitter streams and (e),
compare our approach with an unsupervised ML-
approach, e.g. as in (Li et al, 2012).
Acknowledgments
The authors gratefully acknowledge the support of
the ERC Starting Grant MultiJEDI No. 259234
and the Polish National Science Centre grant N
N516 481940 ?Diversum?.
References
Smitashree Choudhury and John Breslin. 2011. Ex-
tracting Semantic Entities and Events from Sports
Tweets. In Proceedings of the 1st Workshop on
Making Sense of Microposts (#MSM2011), pages
22?32.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A Comparison of
String Distance Metrics for Name-matching Tasks.
In Proceedings of the IJCAI-2003 Workshop on
Information Integration on the Web (IIWeb-03),
pages 73?78.
Alexandre Davis, Adriano Veloso, Altigran S. da Silva,
Wagner Meira, Jr., and Alberto H. F. Laender.
2012. Named Entity Disambiguation in Streaming
Data. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers - Volume 1, ACL ?12, pages 815?824,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Filip Gralin?ski, Krzysztof Jassem, Micha? Marcin?czuk,
and Pawe? Wawrzyniak. 2009. Named entity recog-
nition in machine anonymization. In Recent Ad-
vances in Intelligent Information Systems, pages
247?260, Warsaw. Exit.
Mathew Jaro. 1989. Advances in record linking
methodology as applied to the 1985 census of Tampa
Florida. Journal of the American Statistical Society,
84(406):414?420.
Heikki Keskustalo, Ari Pirkola, Kari Visala, Erkka
Lepp?nen, and Kalervo J?rvelin. 2003. Non-
adjacent Digrams Improve Matching of Cross-
lingual Spelling Variants. In Proceedings of SPIRE,
LNCS 22857, Manaus, Brazil, pages 252?265.
Vladimir Levenshtein. 1965. Binary Codes for Cor-
recting Deletions, Insertions, and Reversals. Dok-
lady Akademii Nauk SSSR, 163(4):845?848.
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, An-
witaman Datta, Aixin Sun, and Bu-Sung Lee. 2012.
TwiNER: Named Entity Recognition in Targeted
Twitter Stream. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 721?730, New York, NY, USA. ACM.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing Named Entities in
Tweets. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
pages 359?367, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu,
and Xiangyang Zhou. 2012. Joint Inference of
Named Entity Recognition and Normalization for
Tweets. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 526?
535, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
92
Brian Locke and James Martin. 2009. Named Entity
Recognition: Adapting to Microblogging. Senior
Thesis, University of Colorado.
Wies?aw Lubaszewski. 2007. Information extraction
tools for polish text. In Proc. of LTC?07, Poznan?,
Poland, Poznan?. Wydawnictwo Poznanskie.
Wies?aw Lubaszewski. 2009. S?owniki komputerowe i
automatyczna ekstrakcja informacji z tekstu. AGH
Uczelniane Wydawnictwa Naukowo-Dydaktyczne,
Krak?w.
Micha? Marcin?czuk and Maciej Janicki. 2012. Opti-
mizing CRF-Based Model for Proper Name Recog-
nition in Polish Texts. In A. Gelbukh, editor,
CICLing 2012, Part I, volume 7181 of Lecture
Notes in Computer Science (LNCS), pages 258?
?269. Springer, Heidelberg.
Micha? Marcin?czuk and Maciej Piasecki. 2007. Pat-
tern extraction for event recognition in the reports
of polish stockholders. In Proceedings of IMCSIT?
AAIA?07, Wis?a, Poland, pages 275?284.
Micha? Marcin?czuk and Maciej Piasecki. 2010.
Named Entity Recognition in the Domain of Pol-
ish Stock Exchange Reports. In Proceedings of In-
telligent Information Systems 2010, Siedlce, Poland,
pages 127?140.
Ma?gorzata Marciniak, Joanna Rabiega-Wis?niewska,
Agata Savary, Marcin Wolin?ski, and Celina Heliasz.
2009. Constructing an Electronic Dictionary of Pol-
ish Urban Proper Names. In Recent Advances in In-
telligent Information Systems. Exit.
David Nadeau and Satoshi Sekine. 2007. A Sur-
vey of Named Entity Recognition and Classification.
Lingvisticae Investigationes, 30(1):3?26.
Kamel Nebhi. 2012. Ontology-Based Information Ex-
traction from Twitter. In Proceedings of the COL-
ING 2012 IEEASM Workshop, Mumbai, India.
Jakub Piskorski, Karol Wieloch, and Marcin Sydow.
2009. On Knowledge-poor Methods for Person
Name Matching and Lemmatization for Highly
Inflectional Languages. Information Retrieval,
12(3):275?299.
Jakub Piskorski. 2005. Named-Entity Recognition for
Polish with SProUT. In LNCS Vol 3490: Proceed-
ings of IMTCI 2004, Warsaw, Poland.
Jakub Piskorski. 2007. ExPRESS ? Extraction Pat-
tern Recognition Engine and Specification Suite. In
Proceedings of FSMNLP 2007.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named Entity Recognition in Tweets: An Ex-
perimental Study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 1524?1534, Ed-
inburgh, Scotland, UK. Association for Computa-
tional Linguistics.
Agata Savary and Jakub Piskorski. 2011. Language
Resources for Named Entity Annotation in the Na-
tional Corpus of Polish. Control and Cybernetics,
40(2):361?391.
Agata Savary, Joanna Rabiega-Wis?niewska, and
Marcin Wolin?ski. 2009. Inflection of Polish Multi-
Word Proper Names with Morfeusz and Multiflex.
LNAI, 5070.
T. Smith and M. Waterman. 1981. Identification
of Common Molecular Subsequences. Journal of
Molecular Biology, 147:195?197.
Esko Ukkonen. 1992. Approximate String Matching
with q-grams and Maximal Matches. Theoretical
Computer Science, 92(1):191?211.
Jakub Waszczuk, Katarzyna G?owin?ska, Agata Savary,
and Adam Przepi?rkowski. 2010. Tools and
Methodologies for Annotating Syntax and Named
Entities in the National Corpus of Polish. In Pro-
ceedings of the International Multiconference on
Computer Science and Information Technology (IM-
CSIT 2010): Computational Linguistics ? Applica-
tions (CLA?10), pages 531?539.
William Winkler. 1999. The State of Record Link-
age and Current Research Problems. Technical re-
port, Statistical Research Division, U.S. Bureau of
the Census, Washington, DC.
Marcin Wolin?ski, Marcin Mi?kowski, Maciej Ogrod-
niczuk, Adam Przepi?rkowski, and ?ukasz Sza-
lkiewicz. 2012. PoliMorf: A (not so) new open
morphological dictionary for Polish. In Proceedings
of the Eighth International Conference on Language
Resources and Evaluation, LREC 2012, pages 860?
864.
93
