Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 120?127,
New York, June 2006. c?2006 Association for Computational Linguistics
An Empirical Study of the Behavior of Active Learning for Word Sense 
Disambiguation 
1 Jinying Chen, 1 Andrew Schein, 1 Lyle Ungar, 2 Martha Palmer 
1 Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
{jinying,ais,ungar}@cis.upenn.edu 
2 Linguistic Department  
University of Colorado 
Boulder, CO, 80309 
Martha.Palmer@colorado.edu 
Abstract 
This paper shows that two uncertainty-
based active learning methods, combined 
with a maximum entropy model, work 
well on learning English verb senses. 
Data analysis on the learning process, 
based on both instance and feature levels, 
suggests that a careful treatment of feature 
extraction is important for the active 
learning to be useful for WSD. The 
overfitting phenomena that occurred 
during the active learning process are 
identified as classic overfitting in machine 
learning based on the data analysis. 
1 Introduction 
Corpus-based methods for word sense 
disambiguation (WSD) have gained popularity in 
recent years. As evidenced by the SENSEVAL 
exercises (http://www.senseval.org), machine 
learning models supervised by sense-tagged 
training corpora tend to perform better on the 
lexical sample tasks than unsupervised methods. 
However, WSD tasks typically have very limited 
amounts of training data due to the fact that 
creating large-scale high-quality sense-tagged 
corpora is difficult and time-consuming. Therefore, 
the lack of sufficient labeled training data has 
become a major hurdle to improving the 
performance of supervised WSD.  
A promising method for solving this problem 
could be the use of active learning. Researchers 
use active learning methods to minimize the 
labeling of examples by human annotators. A 
decrease in overall labeling occurs because active 
learners (the machine learning models used in 
active learning) pick more informative examples 
for the target word (a word whose senses need to 
be learned) than those that would be picked 
randomly. Active learning requires human labeling 
of the newly selected training data to ensure high 
quality. 
We focus here on pool-based active learning 
where there is an abundant supply of unlabeled 
data, but where the labeling process is expensive.  
In NLP problems such as text classification (Lewis 
and Gale, 1994; McCallum and Nigam, 1998), 
statistical parsing (Tang et al, 2002), information 
extraction (Thompson et al, 1999), and named 
entity recognition (Shen et al, 2004), pool-based 
active learning has produced promising results.  
This paper presents our experiments in applying 
two active learning methods, a min-margin based 
method and a Shannon-entropy based one, to the 
task of the disambiguation of English verb senses. 
The contribution of our work is not only in 
demonstrating that these methods work well for the 
active learning of coarse-grained verb senses, but 
also analyzing the behavior of the active learning 
process on two levels: the instance level and the 
feature level. The analysis suggests that a careful 
treatment of feature design and feature generation 
is important for a successful application of active 
learning to WSD. We also accounted for the 
overfitting phenomena that occurred in the learning 
process based on our data analysis.  
The rest of the paper is organized as follows. In 
Section 2, we introduce two uncertainty sampling 
methods used in our active learning experiments 
and review related work in using active learning 
for WSD. We then present our active learning 
experiments on coarse-grained English verb senses 
in Section 3 and analyze the active learning 
120
process in Section 4. Section 5 presents 
conclusions of our study.        
2 Active Learning Algorithms 
The methods evaluated in this work fit into a 
common framework described by Algorithm 1 (see 
Table 1). The key difference between alternative 
active learning methods is how they assess the 
value of labeling individual examples, i.e., the 
methods they use for ranking and selecting the 
candidate examples for labeling. The framework is 
wide open to the type of ranking rule employed. 
Usually, the ranking rule incorporates the model 
trained on the currently labeled data.  This is the 
reason for the requirement of a partial training set 
when the algorithm begins. 
                                Algorithm 1 
Require: initial training set, pool of unlabeled examples 
  Repeat 
Select T random examples from pool 
      Rank T examples according to active learning rule 
     Present the top-ranked example to oracle for labeling 
     Augment the training set with the new example 
  Until Training set reaches desirable size 
Table 1. A Generalized Active Learning Loop 
 
In our experiments we look at two variants of 
the uncertainty sampling heuristic: entropy 
sampling and margin sampling. Uncertainty 
sampling is a term invented by Lewis and Gale 
(Lewis and Gale, 1994) to describe a heuristic 
where a probabilistic classifier picks examples for 
which the model?s current predictions are least 
certain. The intuitive justification for this approach 
is that regions where the model is uncertain 
indicate a decision boundary, and clarifying the 
position of decision boundaries is the goal of 
learning classifiers. Schein (2005) demonstrates 
the two methods run quickly and compete 
favorably against alternatives when combined with 
the logistic regression classifier. 
2.1 Entropy Sampling 
A key question is how to measure uncertainty.  
Different methods of measuring uncertainty will 
lead to different variants of uncertainty sampling.  
We will look at two such measures.  As a 
convenient notation we use q (a vector) to 
represent the trained model?s predictions, with cq  
equal to the predicted probability of class c .  One 
method is to pick the example whose prediction 
vector q displays the greatest Shannon entropy: 
??
c
cc qq log    (1) 
Such a rule means ranking candidate examples 
in Algorithm 1 by Equation 1.  
2.2 Margin Sampling 
An alternative method picks the example with the 
smallest margin: the difference between the largest 
two values in the vector q (Abe and Mamitsuka, 
1998). In other words, if c and 'c are the two most 
likely categories for example nx , the margin is 
measured as follows: 
)|'Pr()|Pr( nnn xcxcM ?=   (2) 
In this case Algorithm 1 would rank examples 
by increasing values of margin, with the smallest 
value at the top of the ranking. 
Using either method of uncertainty sampling, 
the computational cost of picking an example from 
T candidates is: O(TD) where D is the number of 
model parameters.   
2.3 Related Work 
To our best knowledge, there have been very few 
attempts to apply active learning to WSD in the 
literature (Fujii and Inui, 1999; Chklovski and 
Mihalcea, 2002; Dang, 2004). Fujii and Inui (1999) 
developed an example sampling method for their 
example-based WSD system in the active learning 
of verb senses in a pool-based setting. Unlike the 
uncertainty sampling methods (such as the two 
methods we used), their method did not select 
examples for which the system had the minimal 
certainty. Rather, it selected the examples such that 
after training using those examples the system 
would be most certain about its predictions on the 
rest of the unlabeled examples in the next iteration. 
This sample selection criterion was enforced by 
calculating a training utility function. The method 
performed well on the active learning of Japanese 
verb senses. However, the efficient computation of 
the training utility function relied on the nature of 
the example-based learning method, which made 
their example sampling method difficult to export 
to other types of machine learning models. 
Open Mind Word Expert (Chklovski and 
Mihalcea, 2002) was a real application of active 
learning for WSD. It collected sense-annotated 
examples from the general public through the Web 
to create the training data for the SENSEVAL-3 
lexical sample tasks. The system used the 
121
disagreement of two classifiers (which employed 
different sets of features) on sense labels to 
evaluate the difficulty of the unlabeled examples 
and ask the web users to tag the difficult examples 
it selected. There was no formal evaluation for this 
active learning system.  
Dang (2004) used an uncertainty sampling 
method to get additional training data for her WSD 
system. At each iteration the system selected a 
small set of examples for which it had the lowest 
confidence and asked the human annotators to tag 
these examples. The experimental results on 5 
English verbs with fine-grained senses (from 
WordNet 1.7) were a little surprising in that active 
learning performed no better than random 
sampling. The proposed explanation was that the 
quality of the manually sense-tagged data was 
limited by an inconsistent or unclear sense 
inventory for the fine-grained senses. 
3 Active Learning Experiments 
3.1 Experimental Setting 
We experimented with the two uncertainty 
sampling methods on 5 English verbs that had 
coarse-grained senses (see Table 2), as described 
below. By using coarse-grained senses, we limit 
the impact of noisy data due to unclear sense 
boundaries and therefore can get a clearer 
observation of the effects of the active learning 
methods themselves.  
verb # of 
sen. 
baseline 
acc. (%) 
Size of data for 
active learning 
Size of 
test data  
Add 3 91.4 400 100 
Do 7 76.9 500 200 
Feel 3 83.6 400 90 
See 7 59.7 500 200 
Work 9 68.3 400 150 
Table 2. The number of senses, the baseline 
accuracy, the number of instances used for active 
learning and for held-out evaluation for each verb 
 
The coarse-grained senses are produced by 
grouping together the original WordNet senses 
using syntactic and semantic criteria (Palmer et al, 
2006). Double-blind tagging is applied to 50 
instances of the target word. If the ITA < 90%, the 
sense entry is revised by adding examples and 
explanations of distinguishing criteria. 
Table 2 summarizes the statistics of the data. 
The baseline accuracy was computed by using the 
?most frequent sense? heuristic to assign sense 
labels to verb instances (examples). The data used 
in active learning (Column 4 in Table 2) include 
two parts: an initial labeled training set and a pool 
of unlabeled training data. We experimented with 
sizes 20, 50 and 100 for the initial training set. The 
pool of unlabeled data had actually been annotated 
in advance, as in most pool-based active learning 
experiments. Each time an example was selected 
from the pool by the active learner, its label was 
returned to the learner. This simulates the process 
of asking human annotators to tag the selected 
unlabeled example at each time. The advantage of 
using such a simulation is that we can experiment 
with different settings (different sizes of the initial 
training set and different sampling methods).  
The data sets used for active learning and for 
held-out evaluation were randomly sampled from a 
large data pool for each round of the active 
learning experiment. We ran ten rounds of the 
experiments for each verb and averaged the 
learning curves for the ten rounds. 
In the experiments, we used random sampling 
(picking up an unlabeled example randomly at 
each time) as a lower bound. Another control 
(ultimate-maxent) was the learner?s performance 
on the test set when it was trained on a set of 
labeled data that were randomly sampled from a 
large data pool and equaled the amount of data 
used in the whole active learning process (e.g., 400 
training data for the verb add).  
The machine learning model we used for active 
learning was a regularized maximum entropy 
(MaxEnt) model (McCallum, 2002). The features 
used for disambiguating the verb senses included 
topical, collocation, syntactic (e.g., the subject, 
object, and preposition phrases taken by a target 
verb), and semantic (e.g., the WordNet synsets and 
hypernyms of the head nouns of a verb?s NP 
arguments) features (Chen and Palmer, 2005). 
3.2 Experimental Results 
Due to space limits, Figure 1 only shows the 
learning curves for 4 verbs do, feel, see, and work 
(size of the initial training set = 20). The curve for 
the verb add is similar to that for feel. These curves 
clearly show that the two uncertainty sampling 
methods, the entropy-based (called entropy-maxent 
in the figure) and the margin-based (called 
min_margin-maxent), work very well for active 
learning of the senses of these verbs. 
 
122
Figure 1 Active learning for four verbs  
Both methods outperformed the random 
sampling method in that they reached the upper-
bound accuracy earlier and had smoother learning 
curves. For the four verbs add, do, feel and see, 
their learning curves reached the upper bound at 
about 200~300 iterations, which means 1/2 or 1/3 
of the annotation effort can be saved for these 
verbs by using active learning, while still achieving 
the same level of performance as supervised WSD 
without using active learning. Given the large-
scale annotation effort currently underway in the 
OntoNotes project (Hovy et al, 2006), this could 
provide considerable savings in annotation effort 
and speed up the process of providing sufficient 
data for a large vocabulary. The OntoNotes project 
has now provided coarse-grained entries for over 
350 verbs, with corresponding double?blind 
annotation and adjudication in progress.  As this 
adjudicated data becomes available, we will be 
able to train our system accordingly. Preliminary 
results for 22 of these coarse-grained verbs (with 
an average grouping polysemy of 4.5) give us an 
average accuracy of 86.3%. This will also provide 
opportunities for more experiments with active 
learning, where there are enough instances.  Active 
learning could also be beneficial in porting these 
supervised taggers to new genres with different 
sense distributions. 
We also experimented with different sizes of 
the initial training set (20, 50 and 100) and found 
no significant differences in the performance at 
different settings. That means, for these 5 verbs, 
only 20 labeled training instances will be enough 
to initiate an efficient active learning process.        
From Figure 1, we can see that the two 
uncertainty sampling methods generally perform 
equally well except that for the verb do, the min-
margin method is slightly better than the entropy 
method at the beginning of active learning. This 
may not be so surprising, considering that the two 
methods are equal for two-class classification tasks 
(see Equations 1 and 2 for their definition) and the 
verbs used in our experiments have coarse-grained 
senses and often have only 2 or 3 major senses.   
An interesting phenomenon observed from 
these learning curves is that for the two verbs add 
and feel, the active learner reached the upper 
bound very soon (at about 100 iterations) and then 
even breached the upper bound. However, when 
the training set was extended, the learner?s 
performance dropped and eventually returned to 
123
the same level of the upper bound. We discuss the 
phenomenon below.  
4 Analysis of the Learning Process 
In addition to verifying the usefulness of active 
learning for WSD, we are also interested in a 
deeper analysis of the learning process. For 
example, why does the active learner?s 
performance drop sometimes during the learning 
process? What are the characteristics of beneficial 
features that help to boost the learner?s accuracy? 
How do we account for the overfitting phenomena 
that occurred during the active learning for the 
verbs add and feel? We analyzed the effect of both 
instances and features throughout the course of 
active learning using min-margin-based sampling.  
4.1 Instance-level Analysis  
Intuitively, if the learner?s performance drops after 
a new example is added to the training set, it is 
likely that something has gone wrong with the new 
example. To find out such bad examples, we 
define a measure credit_inst for instance i as: 
??
=
+
=
?
m
r
ll
n
l
AccAcclisel
m 1
1
1
)(),(1   (3) 
where Accl and Accl+1 are the classification 
accuracies of the active learner at the lth and 
(l+1)th iterations. n is the total number of 
iterations of active learning and m is the number of 
rounds of active learning (m=10 in our case). 
),( lisel is 1 iff instance i is selected by the active 
learner at the lth iteration and is 0 if otherwise. 
An example is a bad example if and only if it 
satisfies the following conditions: 
a)  its credit_inst value is negative 
b) it increases the learner?s performance, if it 
does, less often than it decreases the 
performance in the 10 rounds. 
We ranked the bad examples by their 
credit_inst values and their frequency of 
decreasing the learner?s performance in the 10 
rounds. Table 3 shows the top five bad examples 
for feel and work. There are several reasons why 
the bad examples may hurt the learner?s 
performance. Column 3 of Table 3 proposes 
reasons for many of our bad examples. We 
categorized these reasons into three major types. 
I. The major senses of a target verb depend 
heavily on the semantic categories of its NP 
arguments but WordNet sometimes fails to provide 
the appropriate semantic categories (features) for 
the head nouns of these NP arguments. For 
example, feel in the board apparently felt no 
pressure has Sense 1 (experience). In Sense 1, feel 
typically takes an animate subject. However, 
board, the head word of the verb?s subject in the 
above sentence has no animate meanings defined 
in WordNet. Even worse, the major meaning of 
board, i.e., artifact, is typical for the subject of feel 
in Sense 2 (touch, grope). Similar semantic type 
mismatches hold for the last four bad examples of 
the verb work in Table 3.  
II. The contexts of the target verb are difficult 
for our feature exaction module to analyze. For 
example, the antecedent for the pronoun subject 
they in the first example of work in Table 3 should 
be ringers, an agent subject that is typical for 
Sense 1 (exert oneself in an activity). However, the 
feature exaction module found the wrong 
antecedent changes that is an unlikely fit for the 
intended verb sense. In the fourth example for feel, 
the feature extraction module cannot handle the 
expletive ?it? (a dummy subject) in ?it was felt 
that?, therefore, it cannot identify the typical 
syntactic pattern for Sense 3 (find, conclude), i.e., 
subject+feel+relative clause. 
III. Sometimes, deep semantic and discourse 
analyses are needed to get the correct meaning of 
the target verb. For example, in the third example 
of feel, ??, he or she feels age creeping up?, it is 
difficult to tell whether the verb has Sense 1 
(experience) or Sense 3 (find) without an 
understanding of the meaning of the relative clause 
and without looking at a broader discourse context. 
The syntactic pattern identified by our feature 
extraction module, subject+feel+relative clause, 
favors Sense 3 (find), which leads to an inaccurate 
interpretation for this case. 
Recall that the motivation behind uncertainty 
samplers is to find examples near decision 
boundaries and use them to clarify the position of 
these boundaries. Active learning often does find 
informative examples, either ones from the less 
common senses or ones close to the boundary 
between the different senses. However, active 
learning also identifies example sentences that are 
difficult to analyze. The failure of our feature 
extraction module, the lack of appropriate semantic 
categories for certain NP arguments in WordNet, 
the lack of deep analysis (semantic and discourse 
analysis) of the context of the target verb can all 
124
         Table 3 Data analysis of the top-ranked bad examples found for two verbs 
produce misleading features. Therefore, in order to 
make active learning useful for its applications, 
both identifying difficult examples and getting 
good features for these examples are equally 
important. In other words, a careful treatment of 
feature design and feature generation is necessary 
for a successful application of active learning. 
There is a positive side to identifying such 
?bad? examples; one can have human annotators 
look at the features generated from the sentences 
(as we did above), and use this to improve the data 
or the classifier. Note that this is exactly what we 
did above: the identification of bad sentences was 
automatic, and they could then be reannotated or 
removed from the training set or the feature 
extraction module needs to be refined to generate 
informative features for these sentences. 
Not all sentences have obvious interpretations; 
hence the two question marks in Table 3. An 
example can be bad for many reasons: conflicting 
features (indicative of different senses), misleading 
features (indicative of non-intended senses), or just 
containing random features that are incorrectly 
incorporated into the model. We will return to this 
point in our discussion of the overfitting 
phenomena for active learning in Section 4.3. 
4.2 Feature-level Analysis 
The purpose of our feature-level analysis is to 
identify informative features for verb senses. The 
learning curve of the active learner may provide 
some clues. The basic idea is, if the learner?s 
performance increases after adding a new example, 
it is likely that the good example contains good 
features that contribute to the clarification of sense 
boundaries. However, the feature-level analysis is 
much less straightforward than the instance-level 
analysis since we cannot simply say the features 
that are active (present) in this good example are 
all good. Rather, an example often contains both 
good and bad features, and many other features 
that are somehow neutral or uninformative. The 
interaction or balance between these features 
determines the final outcome. On the other hand, a 
statistics based analysis may help us to find 
features that tend to be good or bad. For this 
analysis, we define a measure credit_feat for 
feature i as: 
feel Proposed reasons for bad examples Senses 
Some days the coaches make you feel as though you 
are part of a large herd of animals . 
? S1: experience 
And , with no other offers on the table , the board 
apparently felt no pressure to act on it.  
subject: board, no ?animate? meaning in 
WordNet  
S1: experience 
Sometimes a burst of aggressiveness will sweep over a 
man -- or his wife -- because he or she feels age 
creeping up.  
syntactic pattern: sbj+feel+relative clause 
headed by that, a typical pattern for Sense 
3 (find) rather than Sense 1 (experience)  
S1: experience 
At this stage it was felt I was perhaps more pertinent as 
chief. executive . 
syntactic pattern: sbj+feel+relative clause, 
typical for Sense 3 (find) but has not been 
detected by the feature exaction module 
S3: find, conclude
I felt better Tuesday evening when I woke up. ? S1: experience 
Work    
When their changes are completed, and after they have 
worked up a sweat, ringers often ?? 
subject: they, the feature exaction module 
found the wrong antecedent (changes 
rather than ringers) for they 
S1: exert oneself 
in an activity 
Others grab books, records , photo albums , sofas and 
chairs , working frantically in the fear that an 
aftershock will jolt the house again . 
subject: others (means people here), no 
definition in WordNet 
S1: exert oneself 
in an activity 
Security Pacific 's factoring business works with 
companies in the apparel, textile and food industries ?
subject: business, no ?animate? meaning 
in WordNet 
S1: exert oneself 
in an activity 
? ; blacks could work there , but they had to leave at 
night . 
subject: blacks, no ?animate? meaning in 
WordNet 
S1: exert oneself 
in an activity 
? has been replaced by alginates (gelatin-like material 
) that work quickly and accurately and with least 
discomfort to a child . 
subject: alginates, unknown by WordNet S2: perform, 
function, behave 
125
??
=
+
=
?
m
r l
ll
n
l act
AccAccliactive
m 1
1
1
1)(),(1         (4) 
where ),( liactive is 1 iff feature i is active in the 
example selected by the active learner at the lth 
iteration and is 0 if otherwise. actl is the total 
number of active features in the example selected 
at the lth iteration. n and m have the same 
definition as in Equation 3.  
A feature is regarded as good if its credit_feat 
value is positive. We ranked the good features by 
their credit_feat values.  By looking at the top-
ranked good features for the verb work (due to 
space limitations, we omit the table data), we 
identify two types of typically good features.  
The first type of good feature occurs frequently 
in the data and has a frequency distribution over 
the senses similar to the data distribution over the 
senses. Such features include those denoting that 
the target verb takes a subject (subj), is not used in 
a passive mode (morph_normal), does not take a 
direct object (intransitive), occurs in present tense 
(word_work, pos_vb, word_works, pos_vbz), and 
semantic features denoting an abstract subject 
(subjsyn_16993 1) or an entity subject (subjsyn_ 
1742), etc. We call such features background 
features. They help the machine learning model 
learn the appropriate sense distribution of the data. 
In other words, a learning model only using such 
features will be equal to the ?most frequent sense? 
heuristic used in WSD.  
Another type of good feature occurs less 
frequently and has a frequency distribution over 
senses that mismatches with the sense distribution 
of the data. Such features include those denoting 
that the target verb takes an inanimate subject 
(subj_it), takes a particle out (prt_out), is followed 
directly by the word out (word+1_out), or occurs at 
the end of the sentence. Such features are 
indicative of less frequent verb senses  that still 
occur fairly frequently in the data. For example, 
taking an inanimate subject (subj_it) is a strong 
clue for Sense 2 (perform, function, behave) of the 
verb work. Occurring at the end of the sentence is 
also indicative of Sense 2 since when work is used 
in Sense 1 (exert oneself in an activity), it tends to 
take adjuncts to modify the activity as in He is 
working hard to bring up his grade. 
                                                          
1 Those features are from the WordNet. The numbers are 
WordNet ids of synsets and hypernyms. 
There are some features that don?t fall into the 
above two categories, such as the topical feature 
tp_know and the collocation feature pos-2_nn. 
There are no obvious reasons why they are good 
for the learning process, although it is possible that 
the combination of two or more such features 
could make a clear sense distinction. However, this 
hypothesis cannot be verified by our current 
statistics-based analysis. It is also worth noting that 
our current feature analysis is post-experimental 
(i.e., based on the results). In the future, we will try 
automatic feature selection methods that can be 
used in the training phase to select useful features 
and/or their combinations.  
We have similar results for the feature analysis 
of the other four verbs. 
4.3 Account for the Overfitting Phenomena 
Recall that in the instance-level analysis in Section 
4.1, we found that some examples hurt the learning 
performance during active learning but for no 
obvious reasons (the two examples marked by ? in 
Table 3). We found that these two examples 
occurred in the overfitting region for feel. By 
looking at the bad examples (using the same 
definition for bad example as in Section 4.1) that 
occurred in the overfitting region for both feel and 
add, we identified two major properties of these 
examples. First, most of them occurred only once 
as bad examples (19 out 23 for add and 40 out of 
63 for feel). Second, many of the examples had no 
obvious reasons for their badness. 
Based on the above observations, we believe 
that the overfitting phenomena that occurred for 
the two verbs during active learning is typical of 
classic overfitting, which is consistent with a 
"death by a thousand mosquito bites" of rare bad 
features, and consistent with there often being (to 
mix a metaphor) no "smoking gun" of a bad 
feature/instance that is added in, especially in the 
region far away from the starting point of active 
learning. 
5 Conclusions 
We have shown that active learning can lead to 
substantial reductions (often by half) in the number 
of observations that need to be labeled to achieve a 
given accuracy in word sense disambiguation, 
compared to labeling randomly selected instances. 
In a follow-up experiment, we also compared a 
larger number of different active learning methods. 
126
The results suggest that for tasks like word sense 
disambiguation where maximum entropy methods 
are used as the base learning models, the minimum 
margin active criterion for active learning gives 
superior results to more comprehensive 
competitors including bagging and two variants of 
query by committee (Schein, 2005). By also taking 
into account the high running efficiency of the 
min-margin method, it is a very promising active 
learning method for WSD. 
We did an analysis on the learning process on 
two levels: instance-level and feature-level. The 
analysis suggests that a careful treatment of feature 
design and feature generation is very important for 
the active learner to take advantage of the difficult 
examples it finds during the learning process. The 
feature-level analysis identifies some 
characteristics of good features. It is worth noting 
that the good features identified are not particularly 
tied to active learning, and could also be obtained 
by a more standard feature selection method rather 
than by looking at how the features provide 
benefits as they are added in.   
For a couple of the verbs examined, we found 
that active learning gives higher prediction 
accuracy midway through the training than one 
gets after training on the entire corpus.  Analysis 
suggests that this is not due to bad examples being 
added to the training set. It appears that the widely 
used maximum entropy model with Gaussian 
priors is overfitting: the model by including too 
many features and thus fitting noise as well as 
signal.  Using different strengths of the Gaussian 
prior does not solve the problem. If a very strong 
prior is used, then poorer accuracy is obtained. We 
believe that using appropriate feature selection 
would cause the phenomenon to vanish. 
Acknowledgements 
This work was supported by National Science 
Foundation Grant NSF-0415923, Word Sense 
Disambiguation, the DTO-AQUAINT NBCHC-
040036 grant under the University of Illinois 
subcontract to University of Pennsylvania 2003-
07911-01 and the GALE program of the Defense 
Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not 
necessarily reflect the views of the National 
Science Foundation, the DTO, or DARPA.  
References 
Naoki Abe and Hiroshi Mamitsuka. 1998. Query 
learning strategies using boosting and bagging. In 
Proc. of ICML1998, pages 1?10. 
Jinying Chen and Martha Palmer. 2005. Towards 
Robust High Performance Word Sense 
Disambiguation of English Verbs Using Rich 
Linguistic Features, In Proc. of IJCNLP2005, Oct., 
Jeju, Republic of Korea. 
Tim Chklovski and Rada Mihalcea, Building a Sense 
Tagged Corpus with Open Mind Word Expert, in 
Proceedings of the ACL 2002 Workshop on "Word 
Sense Disambiguation: Recent Successes and Future 
Directions", Philadelphia, July 2002. 
Hoa T. Dang. 2004. Investigations into the role of 
lexical semantics in word sense disambiguation.  PhD 
Thesis. University of Pennsylvania. 
Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui, 
Hozumi Tanaka. 1998. Selective sampling for 
example-based word sense disambiguation, 
Computational Linguistics, v.24 n.4, p.573-597, Dec.  
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance 
Ramshaw and Ralph Weischedel. OntoNotes: The 
90% Solution. Accepted by HLT-NAACL06. Short 
paper. 
David D. Lewis and William A. Gale. 1994. A 
sequential algorithm for training text classifiers. In W. 
Bruce Croft and Cornelis J. van Rijsbergen, editors, 
Proceedings of SIGIR-94, Dublin, IE. 
Andrew K. McCallum. 2002. MALLET: A Machine 
Learning for Language Toolkit.  http://www.cs. 
umass.edu/~mccallum/mallet. 
Andew McCallum and Kamal Nigam. 1998. Employing 
EM in pool-based active learning for text 
classification. In Proc. of ICML ?98. 
Martha Palmer, Hoa Trang Dang and Christiane 
Fellbaum. (to appear, 2006). Making fine-grained and 
coarse-grained sense distinctions, both manually and 
automatically. Natural Language Engineering. 
Andrew I. Schein. 2005. Active Learning for Logistic 
Regression. Ph.D. Thesis. Univ. of Pennsylvania. 
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou and Chew 
Lim Tan. 2004 Multi-criteria-based active learning 
for named entity recognition, In Proc. of ACL04, 
Barcelona, Spain. 
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proc. of ACL 2002. 
Cynthia A. Thompson, Mary Elaine Califf, and 
Raymond J. Mooney. 1999. Active learning for 
natural language parsing and information extraction. 
In Proc. of ICML-99. 
127
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 257?260,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Transfer Learning, Feature Selection and Word Sense Disambguation
Paramveer S. Dhillon and Lyle H. Ungar
Computer and Information Science
University of Pennsylvania, Philadelphia, PA, U.S.A
{pasingh,ungar}@seas.upenn.edu
Abstract
We propose a novel approach for improv-
ing Feature Selection for Word Sense Dis-
ambiguation by incorporating a feature
relevance prior for each word indicating
which features are more likely to be se-
lected. We use transfer of knowledge from
similar words to learn this prior over the
features, which permits us to learn higher
accuracy models, particularly for the rarer
word senses. Results on the ONTONOTES
verb data show significant improvement
over the baseline feature selection algo-
rithm and results that are comparable to or
better than other state-of-the-art methods.
1 Introduction
The task of WSD has been mostly studied in
a supervised learning setting e.g. (Florian and
Yarowsky, 2002) and feature selection has always
been an important component of high accuracy
word sense disambiguation, as one often has thou-
sands of features but only hundreds of observa-
tions of the words (Florian and Yarowsky, 2002).
The main problem that arises with supervised
WSD techniques, including ones that do feature
selection, is the paucity of labeled data. For ex-
ample, the training set of SENSEVAL-2 English
lexical sample task has only 10 labeled examples
per sense (Florian and Yarowsky, 2002), which
makes it difficult to build high accuracy models
using only supervised learning techniques. It is
thus an attractive alternative to use transfer learn-
ing (Ando and Zhang, 2005), which improves per-
formance by generalizing from solutions to ?sim-
ilar? learning problems. (Ando, 2006) (abbrevi-
ated as Ando[CoNLL?06]) have successfully ap-
plied the ASO (Alternating Structure Optimiza-
tion) technique proposed by (Ando and Zhang,
2005), in its transfer learning configuration, to the
problem of WSD by doing joint empirical risk
minimization of a set of related problems (words
in this case). In this paper, we show how a novel
form of transfer learning that learns a feature rel-
evance prior from similar word senses, aids in the
process of feature selection and hence benefits the
task of WSD.
Feature selection algorithms usually put a uni-
form prior over the features. I.e., they consider
each feature to have the same probability of being
selected. In this paper we relax this overly sim-
plistic assumption by transferring a prior for fea-
ture relevance of a given word sense from ?simi-
lar? word senses. Learning this prior for feature
relevance of a test word sense makes those fea-
tures that have been selected in the models of other
?similar? word senses become more likely to be
selected.
We learn the feature relevance prior only from
distributionally similar word senses, rather than
?all? senses of each word, as it is difficult to find
words which are similar in ?all? the senses. We
can, however, often find words which have one or
a few similar senses. For example, one sense of
?fire? (as in ?fire someone?) should share features
with one sense of ?dismiss? (as in ?dismiss some-
one?), but other senses of ?fire? (as in ?fire the
gun?) do not. Similarly, other meanings of ?dis-
miss? (as in ?dismiss an idea?) should not share
features with ?fire?.
As just mentioned, knowledge can only be
fruitfully transfered between the shared senses of
different words, even though the models being
learned are for disambiguating different senses of
a single word. To address this problem, we cluster
similar word senses of different words, and then
use the models learned for all but one of the word
senses in the cluster (called the ?training word
senses?) to put a feature relevance prior on which
features will be more predictive for the held out
test word sense. We hold out each word sense in
the cluster once and learn a prior from the remain-
ing word senses in that cluster. For example, we
can use the models for discriminating the senses
of the words ?kill? and the senses of ?capture?, to
257
put a prior on what features should be included in
a model to disambiguate corresponding senses of
the distributionally similar word ?arrest?.
The remainder of the paper is organized as fol-
lows. In Section 2 we describe our ?baseline? in-
formation theoretic feature selection method, and
extend it to our ?TRANSFEAT? method. Section 3
contains experimental results comparing TRANS-
FEAT with the baseline and Ando[CoNLL?06] on
ONTONOTES data. We conclude with a brief sum-
mary in Section 4.
2 Feature Selection for WSD
We use an information theoretic approach to fea-
ture selection based on the Minimum Description
Length (MDL) (Rissanen, 1999) principle, which
makes it easy to incorporate information about
feature relevance priors. These information theo-
retic models have a ?dual? Bayesian interpretation,
which provides a clean setting for feature selec-
tion.
2.1 Information Theoretic Feature Selection
The state-of-the-art feature selection methods in
WSD use either an ?
0
or an ?
1
penalty on the coef-
ficients. ?
1
penalty methods such as Lasso, being
convex, can be solved by optimization and give
guaranteed optimal solutions. On the other hand,
?
0
penalty methods, like stepwise feature selec-
tion, give approximate solutions but produce mod-
els that are much sparser than the models given by
?
1
methods, which is quite crucial in WSD (Flo-
rian and Yarowsky, 2002). ?
0
models are also more
amenable to theoretical analysis for setting thresh-
olds, and hence for incorporating priors.
Penalized likelihood methods which are widely
used for feature selection minimize a score:
Score = ?2log(likelihood) + Fq (1)
where F is a function designed to penalize model
complexity, and q represents the number of fea-
tures currently included in the model at a given
point. The first term in the above equation repre-
sents a measure of the in-sample error given the
model, while the second term is a model complex-
ity penalty.
As is obvious from Eq. 1, the description length
of the MDL (Minimum Description Length) mes-
sage is composed of two parts: S
E
, the num-
ber of bits for encoding the residual errors given
the models and S
M
, the number of bits for en-
coding the model. Hence the description length
can be written as: S = S
E
+ S
M
. Now, when
we evaluate a feature for possible addition to our
model, we want to maximize the reduction of ?de-
scription length? incurred by adding this feature
to the model. This change in description length
is: ?S = ?S
E
? ?S
M
; where ?S
E
? 0 is the
number of bits saved in describing residual error
due to increase in the likelihood of the data given
the new feature and ?S
M
> 0 is the extra bits
used for coding this new feature.
In our baseline feature selection model, we use
the following coding schemes:
Coding Scheme for S
E
:
The term S
E
represents the cost of coding the
residual errors given the models and can be written
as:
S
E
= ? log(P (y|w, x))
?S
E
represents the increase in likelihood (in
bits) of the data by adding this new feature to the
model. We assume a Gaussian model, giving:
P (y|w, x) ? exp
(
?
(
?
n
i=1
(y
i
? w ? x
i
)
2
2?
2
))
where y is the response (word senses in our case),
x?s are the features, w?s are the regression weights
and ?2 is the variance of the Gaussian noise.
Coding Scheme for ?S
M
: For describing S
M
,
the number of bits for encoding the model, we
need the bits to code the index of the feature (i.e.,
which feature from amongst the total m candidate
features) and the bits to code the coefficient of this
feature.
The total cost can be represented as:
S
M
= l
f
+ l
?
where l
f
is the cost to code the index of the feature
and l
?
is the number of bits required to code the
coefficient of the selected feature.
In our baseline feature selection algorithm, we
code l
f
by using log(m) bits (where m is the
total number of candidate features), which is
equivalent to the standard RIC (or the Bonferroni
penalty) (Foster and George, 1994) commonly
used in information theory. The above coding
scheme1 corresponds to putting a uniform prior
over all the features; I.e., each feature is equally
likely to get selected.
For coding the coefficients of the selected fea-
ture we use 2 bits, which is quite similar to the AIC
1There is a duality between Information Theory and
Bayesian terminology: If there is 1
k
probability of a fact being
true, then we need ?log( 1
k
) = log(k) bits to code it.
258
(Akaike Information Criterion) (Rissanen, 1999).
Our final equation for S
M
is therefore:
S
M
= log(m) + 2 (2)
2.2 Extension to TRANSFEAT
We now extend the baseline feature selection al-
gorithm to include the feature relevance prior. We
define a binary random variable f
i
? {0,1} that
denotes the event of the ith feature being in or not
being in the model for the test word sense. We can
parameterize the distribution as p(f
i
= 1|?
i
) = ?
i
.
I.e., we have a Bernoulli Distribution over the fea-
tures.
Given the data for the ith feature for all the
training word senses, we can write: D
i
=
{f
i1
, ..., f
iv
, ..., f
it
}. We then construct the like-
lihood functions from the data (under the i.i.d as-
sumption) as:
p(D
f
i
|?
i
) =
t
?
v=1
p(f
iv
|?
i
) =
t
?
v=1
?
f
iv
(1? ?
i
)
1?f
iv
The posteriors can be calculated by putting a prior
over the parameters ?
i
and using Bayes rule as fol-
lows:
p(?
i
|D
f
i
) = p(D
f
i
|?
i
) ? p(?
i
|a, b)
where a and b are the hyperparameters of the Beta
Prior (conjugate of Bernoulli). The predictive dis-
tribution of ?
i
is:
p(f
i
= 1|D
f
i
) =
?
1
0
?
i
p(?
i
|D
f
i
)d?
i
= E[?
i
|D
f
i
]
=
k + a
k + l + a + b
(3)
where k is the number of times that the ith feature
is selected and l is the complement of k, i.e. the
number of times the ith feature is not selected in
the training data.
In light of above, the coding scheme, which in-
corporates the prior information about the predic-
tive quality of the various features obtained from
similar word senses, can be formulated as follows:
S
M
= ? log (p(f
i
= 1|D
f
i
)) + 2
In the above equation, the first term repre-
sents the cost of coding the features, and the sec-
ond term codes the coefficients. The negative
signs appear due to the duality between Bayesian
and Information-Theoretic representation, as ex-
plained earlier.
3 Experimental Results
In this section we present the experimental results
of TRANSFEAT on ONTONOTES data.
3.1 Similarity Determination
To determine which verbs to transfer from, we
cluster verb senses into groups based on the
TF/IDF similarity of the vector of features se-
lected for that verb sense in the baseline (non-
transfer learning) model. We use only those
features that are positively correlated with the
given sense; they are the features most closely
associated with the given sense. We cluster
senses using a ?foreground-background? cluster-
ing algorithm (Kandylas et al, 2007) rather than
the more common k-means clustering because
many word senses are not sufficiently similar to
any other word sense to warrant putting into a
cluster. Foreground-background clustering gives
highly cohesive clusters of word senses (the ?fore-
ground?) and puts all the remaining word senses
in the ?background?. The parameters that it takes
as input are the % of data points to put in ?back-
ground? (i.e., what would be the singleton clus-
ters) and a similarity threshold which impacts
the number of ?foreground? clusters. We exper-
imented with putting 20% and 33% data points in
background and adjusted the similarity threshold
to give us 50 ? 100 ?foreground? clusters. The
results reported below have 20% background and
50 ? 100 ?foreground? clusters.
3.2 Description of Data and Results
We performed our experiments on ONTONOTES
data of 172 verbs (Hovy et al, 2006). The data
consists of a rich set of linguistic features which
have proven to be beneficial for WSD.
A sample feature vector for the word ?add?,
given below, shows typical features.
word_added pos_vbd morph_normal
subj_use subjsyn_16993 dobj_money
dobjsyn_16993 pos+1+2+3_rp+to+cd
tp_account tp_accumulate tp_actual
The 172 verbs each had between 1,000 and 10,000
nonzero features. The number of senses varied
from 2 (For example, ?add?) to 15 (For example,
?turn?).
We tested our transfer learning algorithm in
three slightly varied settings to tease apart the con-
tributions of different features to the overall per-
formance. In our main setting, we cluster the word
259
senses based on the ?semantic + syntactic? fea-
tures. In Setting 2, we do clustering based only on
?semantic? features (topic features) and in Setting
3 we cluster based on only ?syntactic? (pos, dobj
etc.) features.
Table 1: 10-fold CV (microaveraged) accuracies
of various methods for various Transfer Learning
settings. Note: These are true cross-validation ac-
curacies; No parameters have been tuned on them.
Method Setting 1 Setting 2 Setting 3
TRANSFEAT 85.75 85.11 85.37
Baseline Feat. Sel. 83.50 83.09 83.34
SVM (Poly. Kernel) 83.77 83.44 83.57
Ando[CoNLL?06] 85.94 85.00 85.51
Most Freq. Sense 76.59 77.14 77.24
We compare TRANSFEAT against Baseline Fea-
ture Selection, Ando[CoNLL?06], SVM (libSVM
package) with a cross-validated polynomial kernel
and a simple most frequent sense baseline. We
tuned the ?d? parameter of the polynomial kernel
using a separate cross validation.
The results for the different settings are shown
in Table 1 and are significantly better at the 5%
significance level (Paired t-test) than the base-
line feature selection algorithm and the SVM. It
is comparable in accuracy to Ando[CoNLL?06].
Settings 2 and 3, in which we cluster based on
only ?semantic? or ?syntactic? features, respec-
tively, also gave significant (5% level in a Paired
t-Test) improvement in accuracy over the baseline
and SVM model. But these settings performed
slightly worse than Setting 1, which suggests that
it is a good idea to have clusters in which the word
senses have ?semantic? as well as ?syntactic? dis-
tributional similarity.
Some examples will help to emphasize the point
that we made earlier that transfer helps the most in
cases in which the target word sense has much less
data than the word senses from which knowledge
is being transferred. ?kill? had roughly 6 times
more data than all other word senses in its cluster
(i.e., ?arrest?, ?capture?, ?strengthen?, etc.) In this
case, TRANSFEAT gave 3.19 ? 8.67% higher ac-
curacies than competing methods2 on these three
words. Also, for the case of word ?do,? which
had roughly 10 times more data than the other
word senses in its cluster (E.g., ?die? and ?save?),
TRANSFEAT gave 4.09?6.21% higher accuracies
2TRANSFEAT does better than Ando[CoNLL?06] on these
words even though on average over all 172 verbs, the differ-
ence is slender.
than other methods. Transfer makes the biggest
difference when the target words have much less
data than the word senses they are generalizing
from, but even in cases where the words have sim-
ilar amounts of data we still get a 1.5 ? 2.5% in-
crease in accuracy.
4 Summary
This paper presented a Transfer Learning formula-
tion which learns a prior suggesting which features
are most useful for disambiguating ambiguous
words. Successful transfer requires finding similar
word senses. We used ?foreground/background?
clustering to find cohesive clusters for various
word senses in the ONTONOTES data, consider-
ing both ?semantic? and ?syntactic? similarity be-
tween the word senses. Learning priors on features
was found to give significant accuracy boosts,
with both syntactic and semantic features con-
tributing to successful transfer. Both feature sets
gave substantial benefits over the baseline meth-
ods that did not use any transfer and gave compa-
rable accuracy to recent Transfer Learning meth-
ods like Ando[CoNLL?06]. The performance im-
provement of our Transfer Learning becomes even
more pronounced when the word senses that we
are generalizing from have more observations than
the ones that are being learned.
References
R. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. JMLR, 6:1817?1853.
R. Ando. 2006. Applying alternating structure
optimization to word sense disambiguation. In
(CoNLL).
R. Florian and D. Yarowsky. 2002. Modeling consen-
sus: classifier combination for word sense disam-
biguation. In EMNLP ?02, pages 25?32.
D. P. Foster and E. I. George. 1994. The risk infla-
tion criterion for multiple regression. The Annals of
Statistics, 22(4):1947?1975.
E. H. Hovy, M. P. Marcus, M. Palmer, L. A. Ramshaw,
and R. M. Weischedel. 2006. Ontonotes: The 90%
solution. In HLT-NAACL.
V. Kandylas, S. P. Upham, and L. H. Ungar. 2007.
Finding cohesive clusters for analyzing knowledge
communities. In ICDM, pages 203?212.
J. Rissanen. 1999. Hypothesis selection and testing by
the mdl principle. The Computer Journal, 42:260?
269.
260
Integrated Annotation for Biomedical Information Extraction
Seth Kulick and Ann Bies and Mark Liberman and Mark Mandel
and Ryan McDonald and Martha Palmer and Andrew Schein and Lyle Ungar
University of Pennsylvania
Philadelphia, PA 19104
 
skulick,bies,myl  @linc.cis.upenn.edu,
mamandel@unagi.cis.upenn.edu,
 
ryantm,mpalmer,ais,ungar  @cis.upenn.edu
Scott Winters and Pete White
Division of Oncology,
Children?s Hospital of Philadelphia
Philadelphia, Pa 19104
 
winters,white  @genome.chop.edu
Abstract
We describe an approach to two areas of
biomedical information extraction, drug devel-
opment and cancer genomics. We have devel-
oped a framework which includes corpus anno-
tation integrated at multiple levels: a Treebank
containing syntactic structure, a Propbank con-
taining predicate-argument structure, and an-
notation of entities and relations among the en-
tities. Crucial to this approach is the proper
characterization of entities as relation compo-
nents, which allows the integration of the entity
annotation with the syntactic structure while
retaining the capacity to annotate and extract
more complex events. We are training statis-
tical taggers using this annotation for such ex-
traction as well as using them for improving the
annotation process.
1 Introduction
Work over the last few years in literature data mining
for biology has progressed from linguistically unsophisti-
cated models to the adaptation of Natural Language Pro-
cessing (NLP) techniques that use full parsers (Park et
al., 2001; Yakushiji et al, 2001) and coreference to ex-
tract relations that span multiple sentences (Pustejovsky
et al, 2002; Hahn et al, 2002) (For an overview, see
(Hirschman et al, 2002)). In this work we describe an ap-
proach to two areas of biomedical information extraction,
drug development and cancer genomics, that is based on
developing a corpus that integrates different levels of se-
mantic and syntactic annotation. This corpus will be a
resource for training machine learning algorithms useful
for information extraction and retrieval and other data-
mining applications. We are currently annotating only
abstracts, although in the future we plan to expand this to
full-text articles. We also plan to make publicly available
the corpus and associated statistical taggers.
We are collaborating with researchers in the Division
of Oncology at The Children?s Hospital of Philadelphia,
with the goal of automatically mining the corpus of can-
cer literature for those associations that link specified
variations in individual genes with known malignancies.
In particular we are interested in extracting three entities
(Gene, Variation Event, and Malignancy) in the follow-
ing relationship: Gene X with genomic Variation Event
Y is correlated with Malignancy Z. For example, WT1 is
deleted in Wilms Tumor #5. Such statements found in the
literature represent individual gene-variation-malignancy
observables. A collection of such observables serves
two important functions. First, it summarizes known
relationships between genes, variation events, and ma-
lignancies in the cancer literature. As such, it can be
used to augment information available from curated pub-
lic databases, as well as serve as an independent test for
accuracy and completeness of such repositories. Second,
it allows inferences to be made about gene, variation, and
malignancy associations that may not be explicitly stated
in the literature, both at the fact and entity instance lev-
els. Such inferences provide testable hypotheses and thus
future research targets.
The other major area of focus, in collaboration with
researchers in the Knowledge Integration and Discov-
ery Systems group at GlaxoSmithKline (GSK), is the ex-
traction of information about enzymes, focusing initially
on compounds that affect the activity of the cytochrome
P450 (CYP) family of proteins. For example, the goal is
to see a phrase like
Amiodarone weakly inhibited CYP2C9,
CYP2D6, and CYP3A4-mediated activities
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 61-68.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
with Ki values of 45.1?271.6 
and extract the facts
amiodarone inhibits CYP2C9 with
Ki=45.1-271.6
amiodarone inhibits CYP2D6 with
Ki=45.1-271.6
amiodarone inhibits CYP3A4 with
Ki=45.1-271.6
Previous work at GSK has used search algorithms that
are based on pattern matching rules filling template slots.
The rules rely on identifying the relevant passages by first
identifying compound names and then associating them
with a limited number of relational terms such as inhibit
or inactivate. This is similar to other work in biomedical
extraction projects (Hirschman et al, 2002).
Creating good pattern-action rules for an IE problem is
far from simple. There are many complexities in the dif-
ferent ways that a relation can be expressed in language,
such as syntactic alternations and the heavy use of co-
ordination. While sufficiently complex patterns can deal
with these issues, it requires a good amount of time and
effort to build such hand-crafted rules, particularly since
such rules are developed for each specific problem. A
corpus that is annotated with sufficient syntactic and se-
mantic structure offers the promise of training taggers for
quicker and easier information extraction.
The corpus that we are developing for the two differ-
ent application demands consists of three levels of anno-
tation: the entities and relations among the entities for the
oncology or CYP domain, syntactic structure (Treebank),
and predicate-argument structure (Propbank). This is a
novel approach from the point-of-view of NLP since pre-
vious efforts at Treebanking and Propbanking have been
independent of the special status of any entities, and pre-
vious efforts at entity annotation have been independent
of corresponding layers of syntactic and semantic struc-
ture. The decomposition of larger entities into compo-
nents of a relation, worthwhile by itself on conceptual
grounds for entity definition, also allows the component
entities to be mapped to the syntactic structure. These
entities can be viewed as semantic types associated with
syntactic constituents, and so our expectation is that au-
tomated analyses of these related levels will interact in a
mutually reinforcing and beneficial way for development
of statistical taggers. Development of such statistical tag-
gers is proceeding in parallel with the annotation effort,
and these taggers help in the annotation process, as well
as being steps towards automatic extraction.
In this paper we focus on the aspects of this project
that have been developed and are in production, while
also trying to give enough of the overall vision to place
the work that has been done in context. Section 2 dis-
cusses some of the main issues around the development
of the guidelines for entity annotation, for both the on-
cology and inhibition domains. Section 3 first discusses
the overall plan for the different levels of annotation, and
then focuses on the integration of the two levels currently
in production, entity annotation and syntactic structure.
Section 4 describes the flow of the annotation process,
including the development of the statistical taggers men-
tioned above. Section 5 is the conclusion.
2 Guidelines for Entity Annotation
Annotation has been proceeding for both the oncology
and the inhibition domains. Here we give a summary of
the main features of the annotation guidelines that have
been developed. We have been influenced in this by pre-
vious work in annotation for biomedical information ex-
traction (Ohta et al, 2002; Gaizauskas et al, 2003). How-
ever, we differ in the domains we are annotating and the
design philosophy for the entity guidelines. For exam-
ple, we have been concentrating on explicit concepts for
entities like genes rather than developing a wide-range
ontology for the various physical instantiations.
2.1 Oncology Domain
Gene Entity For the sake of this project the defini-
tion for ?Gene Entity? has two significant characteristics.
First, ?Gene? refers to a composite entity as opposed to
the strict biological definition. As has been noted by oth-
ers, there are often ambiguities in the usage of the en-
tity names. For example, it is sometimes unclear as to
whether it is the gene or protein being referenced, or the
same name might refer to the gene or the protein at dif-
ferent locations in the same document. Our approach to
this problem is influenced by the named entity annota-
tion in the Automatic Content Extraction (ACE) project
(Consortium, 2002), in which ?geopolitical? entities can
have different roles, such as ?location? or ?organization?.
Analogously, we consider a ?gene? to be a composite en-
tity that can have different roles throughout a document.
Standardization of ?Gene? references between different
texts and between gene synonyms is handled by exter-
nally referencing each instance to a standard ontology
(Ashburner et al, 2000).
In the context of this project, ?Gene? refers to a con-
ceptual entity as opposed to the specific manifestation of
a gene (i.e. an allele or nucleotide sequence). Therefore,
we consider genes to be abstract concepts identifying ge-
nomic regions often associated with a function, such as
MYC or TrkB; we do not consider actual instances of
such genes within the gene-entity domain. Since we are
interested in the association between Gene-entities and
malignancies, for this project genes are of interest to us
when they have an associated variation event. Therefore,
the combination of Gene entities and Variation events
provides us with an evoked entity representing the spe-
cific instance of a gene.
Variation Events as Relations Variations comprise a
relationship between the following entities: Type (e.g.
point mutation, translocation, or inversion), Location
(e.g. codon 14, 1p36.1, or base pair 278), Original-State
(e.g. Alanine), and Altered-State (e.g. Thymine). These
four components represent the key elements necessary
to describe any genomic variation event. Variations are
often underspecified in the literature, frequently having
only two or three of these specifications. Characterizing
individual variations as a relation among such compo-
nents provides us with a great deal of flexibility: 1) it al-
lows us to capture the complete variation event even when
specific components are broadly spaced in the text, often
spanning multiple sentences or even paragraphs; 2) it pro-
vides us with a convenient means of tracking anaphora
between detailed descriptions (e.g. a point mutation at
codon 14 and summary references (e.g. this variation);
and 3) it provides a single structure capable of capturing
the breadth of variation specifications (e.g. A-  T point
mutation at base pair 47, A48-  G or t(11;14)(q13;32)).
Malignancy The guidelines for malignancy annotation
are under development. We are planning to define it in a
manner analogous to variation, whereby a Malignancy is
composed of various attribute types (such as developmen-
tal stage, behavior, topographic site, and morphology).
2.2 CYP Domain
In the CYP Inhibition annotation task we are tagging
three types of entities:
1. CYP450 enzymes (cyp)
2. other substances (subst)
3. quantitative measurements (quant)
Each category has its own questions and uncertain-
ties. Names like CYP2C19 and cytochrome P450 en-
zymes proclaim their membership, but there are many
aliases and synonyms that do not proclaim themselves,
such as 17,20-lyase. We are compiling a list of such
names.
Other substances is a potentially huge and vaguely-
delimited set, which in the current corpus includes grape-
fruit juice and red wine as well as more obviously bio-
chemical entities like polyunsaturated fatty acids and ery-
thromycin. The quantitative measurements we are di-
rectly interested in are those directly related to inhibition,
such as IC50 and K(i). We tag the name of the measure-
ment, the numerical value, and the unit. For example, in
the phrase ...was inhibited by troleandomycin (ED50 = 1
microM), ED50 is the name, 1 the value, and microM the
unit. We are also tagging other measurements, since it
is easy to do and may provide valuable information for
future IE work.
3 Integrated Annotation
As has been noted in the literature on biomedical IE (e.g.,
(Pustejovsky et al, 2002; Yakushiji et al, 2001)), the
same relation can take a number of syntactic forms. For
example, the family of words based on inhibit occurs
commonly in MEDLINE abstracts about CYP enzymes
(as in the example in the introduction) in patterns like A
inhibited B, A inhibited the catalytic activity of B, inhibi-
tion of B by A, etc.
Such alternations have led to the use of pattern-
matching rules (often hand-written) to match all the rele-
vant configurations and fill in template slots based on the
resulting pattern matches. As discussed in the introduc-
tion, dealing with such complications in patterns can take
much time and effort.
Our approach instead is to build an annotated corpus
in which the predicate-argument information is annotated
on top of the parsing annotations in the Treebank, the re-
sulting corpus being called a ?proposition bank? or Prop-
bank. This newly annotated corpus is then used for train-
ing processors that will automatically extract such struc-
tures from new examples.
In a Propbank for biomedical text, the types of in-
hibit examples listed above would consistently have their
compounds labeled as Arg0 and their enzymes labeled as
Arg1, for nominalized forms such as A is an inhibitor of
B, A caused inhibition of B, inhibition of B by A, as well
the standard A inhibits B. We would also be able to la-
bel adjuncts consistently, such as the with prepositional
phrase in CYP3A4 activity was decreased by L, S and F
with IC(50) values of about 200 mM. In accordance with
other Calibratable verbs such as rise, fall, decline, etc.,
this phrase would be labeled as an Arg2-EXTENT, re-
gardless of its syntactic role.
A Propbank has been built on top of the Penn Tree-
bank, and has been used to train ?semantic taggers?, for
extracting argument roles for the predicates of interest,
regardless of the particular syntactic context.1
Such semantic taggers have been developed by using
machine learning techniques trained on the Penn Prop-
bank (Surdeanu et al, 2003; Gildea and Palmer, 2002;
Kingsbury and Palmer, 2002). However, the Penn Tree-
bank and Propbank involve the annotation of Wall Street
Journal text. This text, being a financial domain, differs
in significant ways from the biomedical text, and so it is
1The Penn Propbank is complemented by NYU?s Nom-
bank project (Meyers, October 2003), which includes tagging
of nominal predicate structure. This is particular relevant for
the biomedical domain, given the heavy use of nominals such
mutation and inhibition.
necessary for this approach to have a corpus of biomed-
ical texts such as MEDLINE articles annotated for both
syntactic structure (Treebanking) and shallow semantic
structure (Propbanking).
In this project, the syntactic and semantic annotation is
being done on a corpus which is also being annotated for
entities, as described in Section 2. Since semantic tag-
gers of the sort described above result in semantic roles
assigned to syntactic tree constituents, it is desirable to
have the entities correspond to syntactic constituents so
that the semantic roles are assigned to entities. The en-
tity information can function as type information and be
taken advantage of by learning algorithms to help charac-
terize the properties of the terms filling specified roles in
a given predicate.
This integration of these three different annotation lev-
els, including the entities, is being done for the first time2,
and we discuss here three main challenges to this corre-
spondence between entities and constituents: (1) entities
that are large enough to cut across multiple constituents,
(2) entities within prenominal modifiers, and (3) coordi-
nation.3
Relations and Large Entities One major area of con-
cern is the possibility of entities that contain more than
one syntactic constituent and do not match any node in
the syntax tree. For example, as discussed in Section 2, a
variation event includes material on a variation?s type, lo-
cation, and state, and can cut not only across constituents,
but even sentences and paragraphs. A simple example is
point mutations at codon 12, containing both the nominal
(the type of mutation) and following NP (the location).
Note that while in isolation this could also be considered
one syntactic constituent, the NP and PP together, the ac-
tual context is ...point mutations at codon 12 in duode-
nal lavage fluid.... Since all PPs are attached at the same
level, at codon 12 and in duodenal lavage fluid are sis-
ters, and so there is no constituent consisting of just point
mutations at codon 12.
Casting the variation event as a relation between dif-
ferent component entities allows the component entities
to correspond to tree constituents, while retaining the ca-
pacity to annotate and search for more complex events.
In this case, one component entity point mutations cor-
2An influential precursor to this integration is the system de-
scribed in (Miller et al, 1996). Our work is in much the same
spirit, although the representation of the predicate-argument
structure via Propbank and the linkage to the entities is quite
different, as well as of course the domain of annotation.
3There are cases where the entities are so minimal that they
are contained within a NP, not including the determiner, such as
CpG site in the NP a CpG site. entities. We are not as concerned
about these cases since we expect that such entity information
properly contained within a base NP can be associated with the
full base NP.
responds to a (base) NP node, and at codon 12 is corre-
sponds to the PP node that is the NP?s sister. At the same
time, the relation annotation contains the information re-
lating these two constituents.
Similarly, while the malignancy entity definition is cur-
rently under development, as mentioned in Section 2.1, a
guiding principle is that it will also be treated as a relation
and broken down into component entities. While this also
has conceptual benefits for the annotation guidelines, it
has the fortunate effect of making such otherwise syntax-
unfriendly malignancies as colorectal adenomas contain-
ing early cancer and acute myelomonocytic leukemia in
remission amenable for mapping the component parts to
syntactic nodes.
Entities within Prenominal Modifiers While we are
for the most part following the Penn Treebank guide-
lines (Bies et al, 1995), we are modifying them in two
important aspects. One concerns the prenominal mod-
ifiers, which in the Penn Treebank were left flat, with
no structure, but in this biomedical domain contain much
of the information - e.g., cancer-associated autoimmune
antigen. Not only would this have had no annotation
for structure, but even more bizarrely, cancer-associated
would have been a single token in the Penn Treebank,
thus making it impossible to capture the information as
to what is associated with what. We have developed new
guidelines to assign structure to prenominal entities such
as breast cancer, as well as changed the tokenization
guidelines to break up tokens such as cancer-associated.
Coordination We have also modified the treebank an-
notation to account for the well-known problem of enti-
ties that are discontinuous within a coordination structure
- e.g., K- and H-ras, where the entities are K-ras and H-
ras. Our annotation tool allows for discontinuous entities,
so that both K-ras and H-ras are annotated as genes.
Under standard Penn Treebank guidelines for tokeniza-
tion and syntactic structure, this would receive the flat
structure
NP
K- and H-ras
in which there is no way to directly associate the entity
K-ras with a constituent node.
We have modified the treebank guidelines so that K-ras
and H-ras are both constituents, with the ras part of K-ras
represented with an empty category co-indexed with ras
in H-ras:4.
4This is related to the approach to coordination in the GE-
NIA project.
NP
NP
K - NX-1
*P*
and NP
H - NX-1
ras
4 Annotation Process
We are currently annotating MEDLINE abstracts for both
the oncology and CYP domains. The flowchart for the
annotation process is shown in Figure 1. Tokenization,
POS-tagging, entity annotation (both domains), and tree-
banking are in full production. Propbank annotation and
the merging of the entities and treebanking remain to be
integrated into the current workflow. The table in Fig-
ure 2 shows the number of abstracts completed for each
annotation area.
The annotation sequence begins with tokenization and
part-of-speech annotating. While both aspects are simi-
lar to those used for the Penn Treebank, there are some
differences, partly alluded to in Section 3. Tokens are
somewhat more fine-grained than in the Penn Treebank,
so that H-ras, e.g., would consist of three tokens: H, -,
and ras.
Tokenized and part-of-speech annotated files are then
sent to the entity annotators, either for oncology or CYP,
depending on which domain the abstract has been chosen
for. The entities described in Section 2 are annotated at
this step. We are using WordFreak, a Java-based linguis-
tic annotation tool5, for annotation of tokenization, POS,
and entities. Figure 3 is a screen shot of the oncology do-
main annotation, here showing a variation relation being
created out of component entities for type and location.
In parallel with the entity annotation, a file is tree-
banked - i.e., annotated for its syntactic structure. Note
that this is done independently of the entity annotation.
This is because the treebanking guidelines are relatively
stable (once they were adjusted for the biomedical do-
main as described in Section 3), while the entity defini-
tions can require a significant period of study before sta-
bilizing, and with the parallel treatment the treebanking
can proceed without waiting for the entity annotation.
However, this does mean that to produce the desired
integrated annotation, the entity and treebanking annota-
tions need to be merged into one representation. The con-
sideration of the issues described in Section 3 has been
carried out for the purpose of allowing this integration
of the treebanking and entity annotation. This has been
completed for some pilot documents, but the full merging
remains to be integrated into the workflow system.
5http://www.sf.net/projects/wordfreak
As mentioned in the introduction, statistical taggers
are being developed in parallel with the annotation effort.
While such taggers are part of the final goal of the project,
providing the building blocks for extracting entities and
relations, they are also useful in the annotation process
itself, so that the annotators only need to perform correc-
tion of automatically tagged data, instead of starting from
scratch.
Until recently (Feb. 10), the part-of-speech annotation
was done by hand-correcting the results of tagging the
data with a part-of-speech tagger trained on a modified
form of the Penn Treebank.6 The tagger is a maximum-
entropy model utilizing the opennlp package available
at http://www.sf.net/projects/opennlp . It
has now been retrained using 315 files (122 from the
oncology domain, 193 from the cyp domain). Figure 4
shows the improvement of the new vs. the old POS tag-
ger on the same 294 files that have been hand-corrected.
These results are based on testing files that have already
been tokenized, and thus are an evaluation only of the
POS tagger and not the tokenizer. While not directly
comparable to results such as (Tateisi and Tsujii, 2004),
due to the different tag sets and tokenization, they are in
the same general range.7
The oncology and cyp entity annotation, as well as the
treebanking are still being done fully manually, although
that will change in the near future. Initial results for a tag-
ger to identify the various components of a variation re-
lation are promising, although not yet integrated into an-
notation process. The tagger is based on the implementa-
tion of Conditional Random Fields (Lafferty et al, 2001)
in the Mallet toolkit (McCallum, 2002). Briefly, Condi-
tional Random Fields are log-linear models that rely on
weighted features to make predictions on the input. Fea-
tures used by our system include standard pattern match-
ing and word features as well as some expert-created reg-
ular expression features8. Using 10-fold cross-validation
on 264 labelled abstracts containing 551 types, 1064 lo-
6Roughly, Penn Treebank tokens were split at hyphens, with
the individual components then sent through a Penn Treebank-
trained POS tagger, to create training data for another POS tag-
ger. For example (JJ York-based) is treated as (NNP
York) (HYPH -) (JJ based). While this works rea-
sonably well for tokenization, the POS tagger suffered severely
from being trained on a corpus with such different properties.
7The tokenizer has also been retrained and the new tokenizer
is being used for annotation, although although we do not have
the evaluation results here.
8e.g., chr|chromosome [1-9]|1[0-9]|2[0-
2]|X|Y p|q
Merged Entity/
Treebank Annotation
Tokenization
Entity Annotation
POS Annotation
Treebank/Propbank
Annotation
Figure 1: Annotation Flow
Annotation Task Start Date Annotated Documents
Part-of-Speech Tagging 8/22/03 422
Entity Tagging 9/12/03 414
Treebanking 1/8/04 127
Figure 2: Current Annotation Production Results
Figure 3: Relation Annotation in WordFreak
Tagger Training Material Token Instances
Old Sections 00-15 Penn Treebank 773832
New 315 abstracts 103159
Tagger Overall Accuracy Number Token Instances Accuracy on Accuracy on
Unseen in Training Data Unseen Seen
Old 88.53% 14542 58.80% 95.53%
New 97.33% 4096 85.05% 98.02%
(Testing Material: 294 abstracts from the oncology domain, with 76324 token instances.)
Figure 4: Evaluation of Part-of-Speech Taggers
cations and 557 states, we obtained the following results:
Entity Precision Recall F-measure
Type 0.80 0.72 0.76
Location 0.85 0.73 0.79
State 0.90 0.80 0.85
Overall 0.86 0.75 0.80
An entity is considered correctly identified if and only
if it matches the human labeling by both category (type,
location or state) and span (from position a to position b).
At this stage we have not distinguished between initial
and final states.
While it is difficult to compare taggers that tag
different types of entities (e.g., (Friedman et al, 2001;
Gaizauskas et al, 2003)), CRFs have been utilized for
state-of-the-art results in NP-chunking and gene and
protein tagging (Sha and Pereira, 2003; McDonald
and Pereira, 2004) Currently, we are beginning to
investigate methods to identify relations over the varia-
tion components that are extracted using the entity tagger.
5 Conclusion
We have described here an integrated annotation ap-
proach for two areas of biomedical information extrac-
tion. We discussed several issues that have arisen for this
integration of annotation layers. Much effort has been
spent on the entity definitions and how they relate to the
higher-level concepts which are desired for extraction.
There are promising initial results for training taggers to
extract these entities.
Next steps in the project include: (1) continued anno-
tation of the layers we are currently doing, (2) integra-
tion of the level of predicate-argument annotation, and
(3) further development of the statistical taggers, includ-
ing taggers for identifying relations over their component
entities.
Acknowledgements
The project described in this paper is based at the In-
stitute for Research in Cognitive Science at the Uni-
versity of Pennsylvania and is supported by grant EIA-
0205448 from the National Science Foundation?s Infor-
mation Technology Research (ITR) program.
We would like to thank Aravind Joshi, Jeremy
Lacivita, Paula Matuszek, Tom Morton, and Fernando
Pereira for their comments.
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, M.A. Harris, D.P. Hill, L. Issel-Tarver,
A. Kasarskis, S. Lewis, J.C. Matese, J.E. Richardson,
M. Ringwald, G.M. Rubin, and G. Sherlock. 2000.
Gene ontology: Tool for the unification of biology.
Nature Genetics, 25(1):25?29.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
Style, Penn Treebank Project. Tech report MS-CIS-
95-06, University of Pennsylvania, Philadelphia, PA.
Linguistic Data Consortium. 2002. Entity de-
tection and tracking - phase 1 - EDT and
metonymy annotation guidelines version 2.5
20021205. http://www.ldc.upenn.edu/Projects/ACE
/PHASE2/Annotation/.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. Genies: a
natural-language processing system for the extraction
of molecular pathways from journal articles. ISMB
(Supplement of Bioinformatics), pages 74?82.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Bioinformatics applications of information
extraction from journal articles. Journal of Bioinfor-
matics, 19(1):135?143.
Daniel Gildea and Martha Palmer. 2002. The Necessity
of Syntactic Parsing for Predicate Argument Recogni-
tion. In Proc. of ACL-2002.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: The
MEDSYNDIKATE text mining system. In Proceed-
ings of the Pacific Rim Symposium on Biocomputing,
pages 338?349.
Lynette Hirschman, Jong C. Park, Junichi Tsuji, Limsoon
Wong, and Cathy H. Wu. 2002. Accomplishments and
challenges in literature data mining for biology. Bioin-
formatics Review, 18(12):1553?1561.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC2002), Las Palmas, Spain.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2004. Identify-
ing gene and protein mentions in text using conditional
random fields. In A Critical Assessment of Text Min-
ing Methods in Molecular Biology workshop. To be
presented.
Adam Meyers. October, 2003. Nombank. Talk at Auto-
matic Content Extraction (ACE) PI Meeting, Alexan-
dria, VA.
Scott Miller, David Stallard, Robert Bobrow, and Richard
Schwartz. 1996. A fully statistical approach to
natural language interfaces. In Aravind Joshi and
Martha Palmer, editors, Proceedings of the Thirty-
Fourth Annual Meeting of the Association for Compu-
tational Linguistics, pages 55?61, San Francisco. Mor-
gan Kaufmann Publishers.
Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, and Jun?ici
Tsuji. 2002. The GENIA corpus: An annotated corpus
in molecular biology domain. In Proceedings of the
10th International Conference on Intelligent Systems
for Molecular Biology.
J. Park, H. Kim, and J. Kim. 2001. Bidirectional in-
cremental parsing for automatic pathway identification
with combinatory categorial grammar. In Proceedings
of the Pacific Rim Symposium on Biocomputing, pages
396?407.
J. Pustejovsky, J. Castano, and J. Zhang. 2002. Robust
relational parsing over biomedical literature: Extract-
ing inhibit relations. In Proceedings of the Pacific Rim
Symposium on Biocomputing, pages 362?373.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceeds of Human
Language Technology-NAACL 2003.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-speech
annotation of biology research abstracts. In Proceed-
ings of LREC04. To be presented.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001.
Event extraction from biomedical papers using a full
parser. In Proceedings of the Pacific Rim Symposium
on Biocomputing, pages 408?419.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 725?735,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A New Approach to Lexical Disambiguation of Arabic Text
Rushin Shah
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
rnshah@cs.cmu.edu
Paramveer S. Dhillon, Mark Liberman,
Dean Foster, Mohamed Maamouri
and Lyle Ungar
University of Pennsylvania
3451 Walnut Street
Philadelphia, PA 19104, USA
{dhillon|myl|ungar}@cis.upenn.edu,
foster@wharton.upenn.edu,
maamouri@ldc.upenn.edu
Abstract
We describe a model for the lexical analy-
sis of Arabic text, using the lists of alterna-
tives supplied by a broad-coverage morpho-
logical analyzer, SAMA, which include sta-
ble lemma IDs that correspond to combina-
tions of broad word sense categories and POS
tags. We break down each of the hundreds
of thousands of possible lexical labels into
its constituent elements, including lemma ID
and part-of-speech. Features are computed
for each lexical token based on its local and
document-level context and used in a novel,
simple, and highly efficient two-stage super-
vised machine learning algorithm that over-
comes the extreme sparsity of label distribu-
tion in the training data. The resulting system
achieves accuracy of 90.6% for its first choice,
and 96.2% for its top two choices, in selecting
among the alternatives provided by the SAMA
lexical analyzer. We have successfully used
this system in applications such as an online
reading helper for intermediate learners of the
Arabic language, and a tool for improving the
productivity of Arabic Treebank annotators.
1 Background and Motivation
This paper presents a methodology for generating
high quality lexical analysis of highly inflected lan-
guages, and demonstrates excellent performance ap-
plying our approach to Arabic. Lexical analysis of
the written form of a language involves resolving,
explicitly or implicitly, several different kinds of am-
biguities. Unfortunately, the usual ways of talking
about this process are also ambiguous, and our gen-
eral approach to the problem, though not unprece-
dented, has uncommon aspects. Therefore, in order
to avoid confusion, we begin by describing how we
define the problem.
In an inflected language with an alphabetic writ-
ing system, a central issue is how to interpret strings
of characters as forms of words. For example, the
English letter-string ?winds? will normally be in-
terpreted in one of four different ways, all four
of which involve the sequence of two formatives
wind+s. The stem ?wind? might be analyzed as (1) a
noun meaning something like ?air in motion?, pro-
nounced [wInd] , which we can associate with an ar-
bitrary but stable identifier like wind n1; (2) a verb
wind v1 derived from that noun, and pronounced the
same way; (3) a verb wind v2 meaning something
like ?(cause to) twist?, pronounced [waInd]; or (4)
a noun wind n2 derived from that verb, and pro-
nounced the same way. Each of these ?lemmas?, or
dictionary entries, will have several distinguishable
senses, which we may also wish to associate with
stable identifiers. The affix ?-s? might be analyzed
as the plural inflection, if the stem is a noun; or as
the third-person singular inflection, if the stem is a
verb.
We see this analysis as conceptually divided into
four parts: 1) Morphological analysis, which rec-
ognizes that the letter-string ?winds? might be (per-
haps among other things) wind/N + s/PLURAL or
wind/V + s/3SING; 2) Morphological disambigua-
tion, which involves deciding, for example, that in
the phrase ?the four winds?, ?winds? is probably a
plural noun, i.e. wind/N + s/PLURAL; 3) Lemma
analysis, which involves recognizing that the stem
wind in ?winds? might be any of the four lem-
mas listed above ? perhaps with a further listing of
senses or other sub-entries for each of them; and 4)
Lemma disambiguation, deciding, for example, that
725
the phrase ?the four winds? probably involves the
lemma wind n1.
Confusingly, the standard word-analysis tasks in
computational linguistics involve various combina-
tions of pieces of these logically-distinguished op-
erations. Thus, ?part of speech (POS) tagging? is
mainly what we?ve called ?morphological disam-
biguation?, except that it doesn?t necessarily require
identifying the specific stems and affixes involved.
In some cases, it also may require a small amount of
?lemma disambiguation?, for example to distinguish
a proper noun from a common noun. ?Sense disam-
biguation? is basically a form of what we?ve called
?lemma disambiguation?, except that the sense dis-
ambiguation task may assume that the part of speech
is known, and may break down lexical identity more
finely than our system happens to do. ?Lemmatiza-
tion? generally refers to a radically simplified form
of ?lemma analysis? and ?lemma disambiguation?,
where the goal is simply to collapse different in-
flected forms of any similarly-spelled stems, so that
the strings ?wind?, ?winds?, ?winded?, ?winding? will
all be treated as instances of the same thing, without
in fact making any attempt to determine the identity
of ?lemmas? in the traditional sense of dictionary
entries.
Linguists use the term morphology to include all
aspects of lexical analysis under discussion here.
But in most computational applications, ?morpho-
logical analysis? does not include the disambigua-
tion of lemmas, because most morphological ana-
lyzers do not reference a set of stable lemma IDs.
So for the purposes of this paper, we will continue to
discuss lemma analysis and disambiguation as con-
ceptually distinct from morphological analysis and
disambiguation, although, in fact, our system dis-
ambiguates both of these aspects of lexical analysis
at the same time.
The lexical analysis of textual character-strings
is a more complex and consequential problem in
Arabic than it is in English, for several reasons.
First, Arabic inflectional morphology is more com-
plex than English inflectional morphology is. Where
an English verb has five basic forms, for example,
an Arabic verb in principle may have dozens. Sec-
ond, the Arabic orthographic system writes elements
such as prepositions, articles, and possessive pro-
nouns without setting them off by spaces, roughly
as if the English phrase ?in a way? were written ?in-
away?. This leads to an enormous increase in the
number of distinct ?orthographic words?, and a sub-
stantial increase in ambiguity. Third, short vowels
are normally omitted in Arabic text, roughly as if
English ?in a way? were written ?nway?.
As a result, a whitespace/punctuation-delimited
letter-string in Arabic text typically has many more
alternative analyses than a comparable English
letter-string does, and these analyses have many
more parts, drawn from a much larger vocabulary of
form-classes. While an English ?tagger? can spec-
ify the morphosyntactic status of a word by choos-
ing from a few dozen tags, an equivalent level of
detail in Arabic would require thousands of alterna-
tives. Similarly, the number of lemmas that might
play a role in a given letter-sequence is generally
much larger in Arabic than in English.
We start our labeling of Arabic text with the alter-
native analyses provided by SAMA v. 3.1, the Stan-
dard Arabic Morphological Analyzer (Maamouri et
al., 2009). SAMA is an updated version of the ear-
lier Buckwalter analyzers (Buckwalter, 2004), with
a number of significant differences in analysis to
make it compatible with the LDC Arabic Treebank
3-v3.2 (Maamouri et al, 2004). The input to SAMA
is an Arabic orthographic word (a string of letters
delimited by whitespace or punctuation), and the
output of SAMA is a set of alternative analyses, as
shown in Table 1. For a typical word, SAMA pro-
duces approximately a dozen alternative analyses,
but for certain highly ambiguous words it can pro-
duce hundreds of alternatives.
The SAMA analyzer has good coverage; for typ-
ical texts, the correct analysis of an orthographic
word can be found somewhere in SAMA?s list of
alternatives about 95% of the time. However, this
broad coverage comes at a cost; the list of analytic
alternatives must include a long Zipfian tail of rare
or contextually-implausible analyses, which collec-
tively are correct often enough to make a large con-
tribution to the coverage statistics. Furthermore,
SAMA?s long lists of alternative analyses are not
evaluated or ordered in terms of overall or contex-
tual plausibility. This makes the results less useful
in most practical applications.
Our goal is to rank these alternative analyses so
that the correct answer is as near to the top of the list
726
Token Lemma Vocalization Segmentation Morphology Gloss
yHlm Halam-u 1 yaHolumu ya + Holum +
u
IV3MS + IV + IV-
SUFF MOOD:I
he / it + dream + [ind.]
yHlm Halam-u 1 yaHoluma ya + Holum +
a
IV3MS + IV + IV-
SUFF MOOD:S
he / it + dream + [sub.]
yHlm Halum-u 1 yaHolumo ya + Holum +
o
IV3MS + IV + IV-
SUFF MOOD:J
he / it + be gentle + [jus.]
qbl qabil-a 1 qabila qabil + a PV + PV-
SUFF SUBJ:3MS
accept/receive/approve +
he/it [verb]
qbl qabol 1 qabol qabol NOUN Before
Table 1: Partial output of SAMA for yHlm and qbl. On average, every token produces more than 10 such analyses
as possible. Despite some risk of confusion, we?ll
refer to SAMA?s list of alternative analyses for an
orthographic word as potential labels for that word.
And despite a greater risk of confusion, we?ll refer to
the assignment of probabilities to the set of SAMA
labels for a particular Arabic word in a particular
textual context as tagging, by analogy to the oper-
ation of a stochastic part-of-speech tagger, which
similarly assigns probabilities to the set of labels
available for a word in textual context.
Although our algorithms have been developed for
the particular case of Arabic and the particular set
of lexical-analysis labels produced by SAMA, they
should be applicable without modification to the sets
of labels produced by any broad-coverage lexical
analyzer for the orthographic words of any highly-
inflected language.
In choosing our approach, we have been moti-
vated by two specific applications. One applica-
tion aims to help learners of Arabic in reading text,
by offering a choice of English glosses with asso-
ciated Arabic morphological analyses and vocaliza-
tions. SAMA?s excellent coverage is an important
basis for this help; but SAMA?s long, unranked list
of alternative analyses for a particular letter-string,
where many analyses may involve rare words or al-
ternatives that are completely implausible in the con-
text, will be confusing at best for a learner. It is
much more helpful for the list to be ranked so that
the correct answer is almost always near the top, and
is usually one of the top two or three alternatives.
In our second application, this same sort of rank-
ing is also helpful for the linguistically expert native
speakers who do Arabic Treebank analysis. These
annotators understand the text without difficulty, but
find it time-consuming and fatiguing to scan a long
list of rare or contextually-implausible alternatives
for the correct SAMA output. Their work is faster
and more accurate if they start with a list that is
ranked accurately in order of contextual plausibility.
Other applications are also possible, such as vo-
calization of Arabic text for text-to-speech synthe-
sis, or lexical analysis for Arabic parsing. However,
our initial goals have been to rank the list of SAMA
outputs for human users.
We note in passing that the existence of set of sta-
ble ?lemma IDs? is an unusual feature of SAMA,
which in our opinion ought to be emulated by ap-
proaches to lexical analysis in other languages. The
lack of such stable lemma IDs has helped to disguise
the fact that without lemma analysis and disam-
biguation, morphological analyses and disambigua-
tion is only a partial solution to the problem of lexi-
cal analysis.
In principle, it is obvious that lemma disambigua-
tion and morphological disambiguation are mutually
beneficial. If we know the answer to one of the ques-
tions, the other one is easier to answer. However,
these two tasks require rather different sets of con-
textual features. Lemma disambiguation is similar
to the problem of word-sense disambiguation ? on
some definitions, they are identical ? and as a re-
sult, it benefits from paragraph-level and document-
level bag-of-words attributes that help to character-
ize what the text is ?about? and therefore which lem-
mas are more likely to play a role in it. In contrast,
morphological disambiguation mainly depends on
features of nearby words, which help to character-
727
ize how inflected forms of these lemmas might fit
into local phrasal structures.
2 Problem and Methodology
Consider a collection of tokens (observations), ti, re-
ferred to by index i ? {1, . . . , n}, where each token
is associated with a set of p features, xij , for the jth
feature, and a label, li, which is a combination of
a lemma and a morphological analysis. We use in-
dicator functions yik to indicate whether or not the
kth label for the ith token is present. We represent
the complete set of features and labels for the en-
tire training data using matrix notation as X and Y ,
respectively. Our goal is to predict the label l (or
equivalently, the vector y for a given feature vector
x.
A standard linear regression model of this prob-
lem would be
y = x? +  (1)
The standard linear regression estimate of ? (ig-
noring, for simplicity the fact that the ys are 0/1) is:
?? = (XTtrainXtrain)
?1XTtrainYtrain (2)
where Ytrain is an n?h matrix containing 0s and
1s indicating whether or not each of the h possible
labels is the correct label (li) for each of the n tokens
ti, Xtrain is an n ? p matrix of context features for
each of the n tokens, the coefficients ?? are p? h.
However, this is a large, sparse, multiple label
problem, and the above formulation is neither statis-
tically nor computationally efficient. Each observa-
tion (x,y) consists of thousands of features associ-
ated with thousands of potential labels, almost all of
which are zero. Worse, the matrix of coefficients ?,
to be estimated is large (p? h) and one should thus
use some sort of transfer learning to share strength
across the different labels.
We present a novel principled and highly compu-
tationally efficient method of estimating this multi-
label model. We use a two stage procedure, first
using a subset (Xtrain1, Ytrain1) of training data
to give a fast approximate estimate of ?; we then
use a second smaller subset of the training data
(Xtrain2, Ytrain2,) to ?correct? these estimates in a
way that we will show can be viewed as a spe-
cialized shrinkage. Our first stage estimation ap-
proximates ?, but avoids the expensive computa-
tion of (XTtrainXtrain)
?1. Our second stage corrects
(shrinks) these initial estimates in a manner special-
ized to this problem. The second stage takes ad-
vantage of the fact that we only need to consider
those candidate labels produced by SAMA. Thus,
only dozens of the thousands of possible labels are
considered for each token.
We now present our algorithm. We start with a
corpus D of documents d of labeled Arabic text. As
described above, each token, ti is associated with a
set of features characterizing its context, computed
from the other words in the same document, and a la-
bel, li = (lemmai,morphologyi), which is a combi-
nation of a lemma and a morphological analysis. As
described below, we introduce a novel factorization
of the morphology into 15 different components.
Our estimation algorithm, shown in Algorithm 1,
has two stages. We partition the training corpus into
two subsets, one of which (Xtrain1) is used to es-
timate the coefficients ?s and the other of which
(Xtrain2) is used to optimally ?shrink? these coeffi-
cient estimates to reduce variance and prevent over-
fitting due to data sparsity.
For the first stage of our estimation procedure, we
simplify the estimate of the (?) matrix (Equation 2)
to avoid the inversion of the very high dimensional
(p?p) matrix (XTX) by approximating (XTX) by
its diagonal, Var(X), the inverse of which is trivial
to compute; i.e. we estimate ? using
?? = Var(Xtrain1)
?1XTtrain1Ytrain1 (3)
For the second stage, we assume that the coeffi-
cients for each feature can be shrunk differently, but
that coefficients for each feature should be shrunk
the same regardless of what label they are predict-
ing. Thus, for a given observation we predict:
g?ik =
p?
j=1
wj ??jkxij (4)
where the weightswj indicate how much to shrink
each of the p features.
In practice, we fold the variance of each of the j
features into the weight, giving a slightly modified
equation:
g?ik =
p?
j=1
?j?
?
jkxij (5)
728
where ?? = XTtrain1Ytrain1 is just a matrix of the
counts of how often each context feature shows up
with each label in the first training set. The vec-
tor ?, which we will estimate by regression, is just
the shrinkage weightsw rescaled by the feature vari-
ance.
Note that the formation here is different from the
first stage. Instead of having each observation be
a token, we now let each observation be a (token,
label) pair, but only include those labels that were
output by SAMA. For a given token ti and poten-
tial label lk, our goal is to approximate the indica-
tor function g(i, k), which is 1 if the kth label of
token ti is present, and 0 otherwise. We find candi-
date labels using a morphological analyzer (namely
SAMA), which returns a set of possible candidate
labels, say C(t), for each Arabic token t. Our pre-
dicted label for ti is then argmaxk?C(ti)g(i, k).
The regression model for learning the weights ?j
in the second stage thus has a row for each label
g(i, k) associated with a SAMA candidate for each
token i = ntrain1+1 . . . ntrain2 in the second train-
ing set. The value of g(i, k) is predicted as a func-
tion of the feature vector zijk = ??jkxij .
The shrinkage coefficients, ?j , could be estimated
from theory, using a version of James-Stein shrink-
age (James and Stein, 1961), but in practice, superior
results are obtained by estimating them empirically.
Since there are only p of them (unlike the p ? h ?s),
a relatively small training set is sufficient. We found
that regression-SVMs work slightly better than lin-
ear regression and significantly better than standard
classification SVMs for this problem.
Prediction is then done in the obvious way by tak-
ing the tokens in a test corpusDtest, generating con-
text features and candidate SAMA labels for each
token ti, and selected the candidate label with the
highest score g?(i, k) that we set out to learn. More
formally, The model parameters ?? and ? produced
by the algorithm allow one to estimate the most
likely label for a new token ti out of a set of can-
didate labels C(ti) using
kpred = argmaxk?C(ti)
p?
j=1
?j?
?
jkxij (6)
The most expensive part of the procedure is es-
timating ??, which requires for each token in cor-
Algorithm 1 Training algorithm.
Input: A training corpusDtrain of n observations
(Xtrain, Ytrain)
PartitionDtrain into two sets,D1 andD2, of sizes
ntrain1 and ntrain2 = n? ntrain1 observations
// Using D1, estimate ??
??jk =
?ntrain1
i=1 xijyik for the j
th feature and kth
label
// Using D2, estimate ?j
// Generate new ?features? Z and the true labels
g(i, k) for each of the SAMA candidate labels for
each of the tokens in D2
zijk = ??jkxij for i in i = ntrain1 + 1 . . . ntrain2
Estimate ?j for the above (feature,label) pairs
(zijk, g(i, k)) using Regression SVMs
Output: ? and ??
pus D1, (a subset of D), finding the co-occurrence
frequencies of each label element (a lemma, or a
part of the morphological segmentation) with the
target token and jointly with the token and with
other tokens or characters in the context of the to-
ken of interest. For example, given an Arabic to-
ken, ?yHlm?, we count what fraction of the time
it is associated with each lemma (e.g. Halam-
u 1), count(lemma=Halam-u 1, token=yHlm) and
each segment (e.g. ?ya?), count(segment=ya, to-
ken=yHlm). (Of course, most tokens never show up
with most lemmas or segments; this is not a prob-
lem.) We also find the base rates of the components
of the labels (e.g., count(lemma=Halam-u 1), and
what fraction of the time the label shows up in vari-
ous contexts, e.g. count(lemma=Halam-u 1, previ-
ous token = yHlm). We describe these features in
more detail below.
3 Features and Labels used for Training
Our approach to tagging Arabic differs from conven-
tional approaches in the two-part shrinkage-based
method used, and in the choice of both features and
labels used in our model. For features, we study
both local context variables, as described above, and
document-level word frequencies. For the labels, the
key question is what labels are included and how
they are factored. Standard ?taggers? work by doing
an n-way classification of all the alternatives, which
is not feasible here due to the thousands of possi-
729
ble labels. Standard approaches such as Conditional
Random Fields (CRFs) are intractable with so many
labels. Moreover, few if any taggers do any lemma
disambiguation; that is partly because one must start
with some standard inventory of lemmas, which are
not available for most languages, perhaps because
the importance of lemma disambiguation has been
underestimated.
We make a couple of innovations to deal with
these issues. First, we perform lemma disambigua-
tion in addition to ?tagging?. As mentioned above,
lemmas and morphological information are not in-
dependent; the choice of lemma often influences
morphology and vice versa. For example, Table 1
contains two analyses for the word qbl. For the first
analysis, where the lemma is qabil-a 1 and the gloss
is accept/receive/approve + he/it [verb], the word is
a verb. However, for the second analysis, where the
lemma is qabol 1 and the gloss is before, the word
is a noun.
Simultaneous lemma disambiguation and tagging
introduces additional complexity: An analysis of
ATB and SAMA shows that there are approximately
2,200 possible morphological analyses (?tags?) and
40,000 possible lemmas; even accounting for the
fact that most combinations of lemmas and morpho-
logical analyses don?t occur, the size of the label
space is still in the order of tens of thousands. To
deal with data sparsity, our second innovation is to
factor the labels. We factor each label l into a set of
16 label elements (LEs). These include lemmas, as
well as morphological elements such as basic part-
of-speech, suffix, gender, number, mood, etc. These
are explained in detail below. Thus, since each la-
bel l is a set of 15 categorical variables, each y in
the first learning stage is actually a vector with 16
nonzero components and thousands of zeros. Since
we do simultaneous estimation of the entire set of
label elements, the value g(i, k) being predicted in
the second learning phase is 1 if the entire label set
is correct, and zero otherwise. We do not learn sep-
arate models for each label.
3.1 Label Elements (LEs)
The fact that there are tens of thousands of possible
labels presents the problem of extreme sparsity of
label distribution in the training data. We find that a
model that estimates coefficients ?? to predict a sin-
LE Description
lemma Lemma
pre1 Closer prefix
pre2 Farther prefix
det Determiner
pos Basic POS
dpos Additional data on basic pos
suf Suffix
perpos Person (basic pos)
numpos Number (basic pos)
genpos Gender (basic pos)
persuf Person (suffix)
numsuf Number (suffix)
gensuf Gender (suffix)
mood Mood of verb
pron Pronoun suffix
Table 2: Label Elements (LEs). Examples of additional
data on basic POS include whether a noun is proper or
common, whether a verb is transitive or not, etc. Both
the basic POS and its suffix may have person, gender and
number data.
gle label (a label being in the Cartesian product of
the set of label elements) yields poor performance.
Therefore, as just mentioned, we factor each label
l into a set of label elements (LEs), and learn the
correlations ?? between features and label elements,
rather than features and entire label sets. This re-
duces, but does not come close to eliminating, the
problem sparsity. A complete list of these LEs and
their possible values is detailed in Table 2.
3.2 Features
3.2.1 Local Context Features
We take (t, l) pairs from D2, and for each such
pair generate features Z based on co-occurrence
statistics ?? in D1, as mentioned in Algorithm 2.
These statistics include unigram co-occurrence fre-
quencies of each label with the target token and bi-
gram co-occurrence of the label with the token and
with other tokens or characters in the context of the
target token. We define them formally in Table 3.
Let Zbaseline denote the set of all such basic features
based on the local context statistics of the target to-
ken, namely the words and letters preceding and fol-
lowing it. We will use this set to create a baseline
model.
730
Statistic Description
Freq countD1(t, l)
PrevWord countD1(t, l, t?1)
NextWord countD1(t, l, t+1)
PreviLetter countD1(t, l, first letter(t?1))
NextiLetter countD1(t, l, first letter(t+1)
PrevfLetter countD1(t, l, last letter(t?1)
NextfLetter countD1(t, l, last letter(t+1)
Table 3: Co-occurrence statistics ??. We use these to
generate feature sets for our regression SVMs.
For each label element (LE) e, we define a set of
features Ze similar to Zbaseline; these features are
based on co-occurrence frequencies of the particular
LE e, not the entire label l.
Finally, we define an aggregate feature set Zaggr
as follows:
Zaggr = Zbaseline
?
{Ze} (7)
where e ? {lemma, pre1, pre2, det, pos, dpos,
suf, perpos, numpos, genpos, persuf, numsuf, gensuf,
mood, pron}.
3.2.2 Document Level Features
When trying to predict the lemma, it is useful to
include not just the words and characters immedi-
ately adjacent to the target token, but also the all the
words in the document. These words capture the
?topic? of the document, and help to disambiguate
different lemmas, which tend to be used or not used
based on the topic being discussed, similarly to the
way that word sense disambiguation systems in En-
glish sometimes use the ?bag of words? the docu-
ment to disambiguate, for example a ?bank? for de-
positing money from a ?bank? of a river. More pre-
cisely, we augment the features for each target token
with the counts of each word in the document (the
?term frequency? tf) in which the token occurs with
a given label.
Zfull = Zaggr
?
Ztf (8)
This setZfull is our final feature set. We useZfull
to train an SVM model Mfull; this is our final pre-
dictive model.
3.3 Corpora used for Training and Testing
We use three modules of the Penn Arabic Tree-
bank (ATB) (Maamouri et al, 2004), namely ATB1,
ATB2 and ATB3 as our corpus of labeled Ara-
bic text, D. Each ATB module is a collection
of newswire data from a particular agency. ATB1
uses the Associated Press as a source, ATB2 uses
Ummah, and ATB3 uses Annahar. D contains a total
of 1,835 documents, accounting for approximately
350,000 words. We construct the training and test-
ing setsDtrain andDtest fromD using 10-fold cross
validation, and we constructD1 andD2 fromDtrain
by randomly performing a 9:1 split.
As mentioned earlier, we use the SAMA mor-
phological analyzer to obtain candidate labels C(t)
for each token t while training and testing an SVM
model on D2 and Dtest respectively. A sample out-
put of SAMA is shown in Table 1. To improve cov-
erage, we also add to C(t) all the labels l seen for t
in D1. We find that doing so improves coverage to
98%. This is an upper bound on the accuracy of our
model.
C(t) = SAMA(t)
?
{l|(t, l) ? D1} (9)
4 Results
We use two metrics of accuracy: A1, which mea-
sures the percentage of tokens for which the model
assigns the highest score to the correct label or LE
value (or E1= 100?A1, the corresponding percent-
age error), and A2, which measures the percentage
of tokens for which the correct label or LE value
is one of the two highest ranked choices returned
by the model (or E2 = 100 ? A2). We test our
modelMfull onDtest and achieve A1 and A2 scores
of 90.6% and 96.2% respectively. The accuracy
achieved by our Mfull model is, to the best of our
knowledge, higher than prior approaches have been
able to achieve so far for the problem of combined
morphological and lemma disambiguation. This is
all the more impressive considering that the upper
bound on accuracy for our model is 98% because,
as described above, our set of candidate labels is in-
complete.
In order to analyze how well different LEs can be
predicted, we train an SVM model Me for each LE
e using the feature set Ze, and test all such models
731
on Dtest. The results for all the LEs are reported in
the form of error percentages E1 and E2 in Table 4.
Model E1 E2 Model E1 E2
Mlemma 11.1 4.9 Mpre1 1.9 1.4
Mpre2 0.2 0 Mdet 0.7 0.1
Mpos 23.4 4.0 Mdpos 10.3 1.9
Msuf 7.6 2.5 Mperpos 3.0 0.1
Mnumpos 3.2 0.2 Mgenpos 1.8 0.1
Mpersuf 3.2 0.1 Mnumsuf 8.2 0.5
Mgensuf 11.6 0.4 Mmood 1.6 1.4
Mpron 1.8 0.6 Mcase 14.7 5.9
Mfull 9.4 3.8 - - -
Table 4: Results of Me for each LE e. Note: The results
reported are 10 fold cross validation test accuracies and
no parameters have been tuned on them.
A comparison of the results for Mfull with the
results for Mlemma and Mpos is particularly infor-
mative. We see that Mfull is able to achieve a sub-
stantially lower E1 error score (9.4%) than Mlemma
(11.1%) and Mpos (23.4%); in other words, we find
that our full model is able to predict lemmas and ba-
sic parts-of-speech more accurately than the individ-
ual models for each of these elements.
We examine the effect of varying the size of D2,
i.e. the number of SVM training instances, on the
performance of Mfull on Dtest, and find that with
increasing sizes of D2, E1 reduces only slightly
from 9.5% to 9.4%, and shows no improvement
thereafter. We also find that the use of document-
level features in Mlemma reduces E1 and E2 per-
centages for Mlemma by 5.7% and 3.2% respec-
tively.
4.1 Comparison to Alternate Approaches
4.1.1 Structured Prediction Models
Preliminary experiments showed that knowing the
predicted labels (lemma + morphology) of the sur-
rounding words can slightly improve the predic-
tive accuracy of our model. To further investi-
gate this effect, we tried running experiments us-
ing different structured models, namely CRF (Con-
ditional Random Fields) (Lafferty et al, 2001),
(Structured) MIRA (Margin Infused Relaxation Al-
gorithm) (Crammer et al, 2006) and Structured
Perceptron (Collins, 2002). We used linear chain
CRFs as implemented in MALLET Toolbox (Mc-
Callum, 2001) and for Structured MIRA and Per-
ceptron we used their implementations from EDLIN
Toolbox (Ganchev and Georgiev, 2009). However,
given the vast label space of our problem, running
these methods proved infeasible. The time complex-
ity of these methods scales badly with the number of
labels; It took a week to train a linear chain CRF
for only ? 50 labels and though MIRA and Per-
ceptron are online algorithms, they also become in-
tractable beyond a few hundred labels. Since our
label space contains combinations of lemmas and
morphologies, so even after factoring, the dimension
of the label space is in the order of thousands.
We also tried a na??ve version (two-pass approxi-
mation) of these structured models. In addition to
the features in Zfull, we include the predicted la-
bels for the tokens preceding and following the tar-
get token as features. This new model is not only
slow to train, but also achieves only slightly lower
error rates (1.2% lower E1 and 1.0% lower E2) than
Mfull. This provides an upper bound on the bene-
fit of using the more complex structured models, and
suggests that given their computational demands our
(unstructured) model Mfull is a better choice.
4.1.2 MADA
(Habash and Rambow, 2005) perform morpho-
logical disambiguation using a morphological ana-
lyzer. (Roth et al, 2008) augment this with lemma
disambiguation; they call their system MADA. Our
work differs from theirs in a number of respects.
Firstly, they don?t use the two step regression proce-
dure that we use. Secondly, they use only ?unigram?
features. Also, they do not learn a single model from
a feature set based on labels and LEs; instead, they
combine models for individual elements by using
weighted agreement. We trained and tested MADA
v2.32 using its full feature set on the same Dtrain
andDtest. We should point out that this is not an ex-
act comparison, since MADA uses the older Buck-
walter morphological analyzer.1
4.1.3 Other Alternatives
Unfactored Labels: To illustrate the benefit ob-
tained by breaking down each label l into
1A new version of MADA was released very close to the
submission deadline for this conference.
732
LEs, we contrast the performance of our Mfull
model to an SVM model Mbaseline trained us-
ing only the feature set Zbaseline, which only
contains features based on entire labels, those
based on individual LEs.
Independent lemma and morphology prediction:
Another alternative approach is to pre-
dict lemmas and morphological analyses
separately. We construct a feature set
Zlemma? = Zfull ? Zlemma and train an SVM
model Mlemma? using this feature set. Labels
are then predicted by simply combining the
results predicted independently by Mlemma
and Mlemma? . Let Mind denote this approach.
Unigram Features: Finally, we also consider a
context-less approach, i.e. using only ?uni-
gram? features for labels as well as LEs. We
call this feature set Zuni, and the correspond-
ing SVM model Muni.
The results of these various models, along with
those of Mfull are summarized in Table 5. We see
thatMfull has roughly half the error rate of the state-
of-the-art MADA system.
Model E1 E2
Mbaseline 13.6 9.1
Mind 18.7 6.0
Muni 11.6 6.4
Mcheat 8.2 2.8
MADA 16.9 12.6
Mfull 9.4 3.8
Table 5: Percent error rates of alternative approaches.
Note: The results reported are 10 fold cross validation
test accuracies and no parameters have been tuned on
them. We used same train-test splits for all the datasets.
5 Related Work
(Hajic, 2000) show that for highly inflectional
languages, the use of a morphological analyzer
improves accuracy of disambiguation. (Diab et
al., 2004) perform tokenization, POS tagging
and base phrase chunking using an SVM based
learner. (Ahmed and Nu?rnberger, 2008) perform
word-sense disambiguation using a Naive Bayesian
model and rely on parallel corpora and match-
ing schemes instead of a morphological ana-
lyzer. (Kulick, 2010) perform simultaneous tok-
enization and part-of-speech tagging for Arabic by
separating closed and open-class items and focus-
ing on the likelihood of possible stems of open-
class words. (Mohamed and Ku?bler, 2010) present
a hybrid method between word-based and segment-
based POS tagging for Arabic and report good re-
sults. (Toutanova and Cherry, 2009) perform joint
lemmatization and part-of-speech tagging for En-
glish, Bulgarian, Czech and Slovene, but they do
not use the two step estimation-shrinkage model de-
scribed in this paper; nor do they factor labels. The
idea of joint lemmatization and part-of-speech tag-
ging has also been discussed in the context of Hun-
garian in (Kornai, 1994).
A substantial amount of relevant work has been
done previously for Hebrew. (Adler and Elhadad,
2006) perform Hebrew morphological disambigua-
tion using an unsupervised morpheme-based HMM,
but they report lower scores than those achieved by
our model. Moreover, their analysis doesn?t include
lemma IDs, which is a novelty of our model. (Gold-
berg et al, 2008) extend the work of (Adler and El-
hadad, 2006) by using an EM algorithm, and achieve
an accuracy of 88% for full morphological analy-
sis, but again, this does not include lemma IDs. To
the best of our knowledge, there is no existing re-
search for Hebrew that does what we did for Arabic,
namely to use simultaneous lemma and morpholog-
ical disambiguation to improve both. (Dinur et al,
2009) show that prepositions and function words can
be accurately segmented using unsupervised meth-
ods. However, by using this method as a preprocess-
ing step, we would lose the power of a simultaneous
solution for these problems. Our method is closer in
style to a CRF, giving much of the accuracy gains of
simultaneous solution, while being about 4 orders of
magnitude easier to train.
We believe that our use of factored labels is novel
for the problem of simultaneous lemma and mor-
phological disambiguation; however, (Smith et al,
2005) and (Hatori et al, 2008) have previously
made use of features based on parts of labels in
CRF models for morphological disambiguation and
word-sense disambiguation respectively. Also, we
note that there is a similarity between our two-stage
733
machine learning approach and log-linear models in
machine translation that break the data in two parts,
estimating log-probabilities of generative models
from one part, and discriminatively re-weighting the
models using the second part.
6 Conclusions
We introduced a new approach to accurately predict
labels consisting of both lemmas and morphologi-
cal analyses for Arabic text. We obtained an accu-
racy of over 90% ? substantially higher than current
state-of-the-art systems. Key to our success is the
factoring of labels into lemma and a large set of mor-
phosyntactic elements, and the use of an algorithm
that computes a simple initial estimate of the coef-
ficient relating each contextual feature to each la-
bel element (simply by counting co-occurrence) and
then regularizes these features by shrinking each of
the coefficients for each feature by an amount deter-
mined by supervised learning using only the candi-
date label sets produced by SAMA.
We also showed that using features of word n-
grams is preferable to using features of only individ-
ual tokens of data. Finally, we showed that a model
using a full feature set based on labels as well as
factored components of labels, which we call label
elements (LEs) works better than a model created
by combining individual models for each LE. We
believe that the approach we have used to create our
model can be successfully applied not just to Arabic
but also to other languages such as Turkish, Hungar-
ian and Finnish that have highly inflectional mor-
phology. The current accuracy of of our model, get-
ting the correct answer among the top two choices
96.2% of the time is high enough to be highly use-
ful for tasks such as aiding the manual annotation
of Arabic text; a more complete automation would
require that accuracy for the single top choice.
Acknowledgments
We woud like to thank everyone at the Linguis-
tic Data Consortium, especially Christopher Cieri,
David Graff, Seth Kulick, Ann Bies, Wajdi Za-
ghouani and Basma Bouziri for their help. We also
wish to thank the anonymous reviewers for their
comments and suggestions.
References
Meni Adler and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics.
Farag Ahmed and Andreas Nu?rnberger. 2008. Ara-
bic/English Word Translation Disambiguation using
Parallel Corpora and Matching Schemes. In Proceed-
ings of EAMT?08, Hamburg, Germany.
Tim Buckwalter. 2004. Buckwalter Arabic Morphologi-
cal Analyzer version 2.0.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP?02.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic text: From Raw Text to
Base Phrase Chunks. In Proceedings of the 5th Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics/Human Language
Technologies Conference (HLT-NAACL?04).
Elad Dinur, Dmitry Davidov, and Ari Rappoport. 2009.
Unsupervised Concept Discovery in Hebrew Using
Simple Unsupervised Word Prefix Segmentation for
Hebrew and Arabic. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages.
Kuzman Ganchev and Georgi Georgiev. 2009. Edlin:
An Easy to Read Linear Learning Framework. In Pro-
ceedings of RANLP?09.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM Can Find Pretty Good HMM POS-Taggers (When
Given a Good Start)*. In Proceedings of ACL?08.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
ACL?05, Ann Arbor, MI, USA.
Jan Hajic. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL?00).
Jun Hatori, Yusuke Miyao, and Jun?ichi Tsujii. 2008.
Word Sense Disambiguation for All Words using Tree-
Structured Conditional Random Fields. In Proceed-
ings of COLing?08.
W. James and Charles Stein. 1961. Estimation with
Quadratic Loss. In Proceedings of the Fourth Berkeley
734
Symposium on Mathematical Statistics and Probabil-
ity, Volume 1.
Andra?s Kornai. 1994. On Hungarian morphology (Lin-
guistica, Series A: Studia et Dissertationes 14). Lin-
guistics Institute of Hungarian Academy of Sciences,
Budapest.
Seth Kulick. 2010. Simultaneous Tokenization and Part-
of-Speech Tagging for Arabic without a Morphologi-
cal Analyzer. In Proceedings of ACL?10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML?01, pages 282?289.
Mohamed Maamouri, Ann Bies, and Tim Buckwalter.
2004. The Penn Arabic Treebank: Building a Large
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR Conference on Arabic Language Resources
and Tools.
Mohamed Maamouri, David Graff, Basma Bouziri, Son-
dos Krouna, and Seth Kulick. 2009. LDC Standard
Arabic Morphological Analyzer (SAMA) v. 3.0.
Andrew McCallum, 2001. MALLET: A Machine Learn-
ing for Language Toolkit. Software available at
http://mallet.cs.umass.edu.
Emad Mohamed and Sandra Ku?bler. 2010. Arabic Part
of Speech Tagging. In Proceedings of LREC?10.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL?08, Columbus, Ohio, USA.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-Based Morphological Disambiguation
with Random Fields*. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Kristina Toutanova and Colin Cherry. 2009. A Global
Model for Joint Lemmatization and Part-of-Speech
Prediction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 486?494.
735
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146?1151,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Developing Age and Gender Predictive Lexica over Social Media
Maarten Sap
1
Gregory Park
1
Johannes C. Eichstaedt
1
Margaret L. Kern
1
David Stillwell
3
Michal Kosinski
3
Lyle H. Ungar
2
and H. Andrew Schwartz
2
1
Department of Psychology, University of Pennsylvania
2
Computer & Information Science, University of Pennsylvania
3
Psychometrics Centre, University of Cambridge
maarten@sas.upenn.edu
Abstract
Demographic lexica have potential for
widespread use in social science, economic,
and business applications. We derive predic-
tive lexica (words and weights) for age and
gender using regression and classification
models from word usage in Facebook, blog,
and Twitter data with associated demographic
labels. The lexica, made publicly available,
1
achieved state-of-the-art accuracy in language
based age and gender prediction over Face-
book and Twitter, and were evaluated for
generalization across social media genres as
well as in limited message situations.
1 Introduction
Use of social media has enabled the study of psycho-
logical and social questions at an unprecedented scale
(Lazer et al., 2009). This allows more data-driven dis-
covery alongside the typical hypothesis-testing social
science process (Schwartz et al., 2013b). Social me-
dia may track disease rates (Paul and Dredze, 2011;
Google, 2014), psychological well-being (Dodds et al.,
2011; De Choudhury et al., 2013; Schwartz et al.,
2013a), and a host of other behavioral, psychological
and medical phenomena (Kosinski et al., 2013).
Unlike traditional hypothesis-driven social science,
such large-scale social media studies rarely take into
account?or have access to?age and gender informa-
tion, which can have a major impact on many ques-
tions. For example, females live almost five years
longer than males (cdc, 2014; Marengoni et al., 2011).
Men and women, on average, differ markedly in their
interests and work preferences (Su et al., 2009). With
age, personalities gradually change, typically becom-
ing less open to experiences but more agreeable and
conscientious (McCrae et al., 1999). Additionally, so-
cial media language varies by age (Kern et al., 2014;
Pennebaker and Stone, 2003) and gender (Huffaker and
Calvert, 2005). Twitter may have a male bias (Mislove
et al., 2011), while social media in general skew to-
wards being young and female (pew, 2014).
Accessible tools to predict demographic variables
can substantially enhance social media?s utility for so-
1
download at http://www.wwbp.org/data.html
cial science, economic, and business applications. For
example, one can post-stratify population-level results
to reflect a representative sample, understand variation
across age and gender groups, or produce personalized
marketing, services, and sentiment recommendations;
a movie may be generally disliked, except by people in
a certain age group, whereas a product might be used
primarily by one gender.
This paper describes the creation of age and gen-
der predictive lexica from a dataset of Facebook users
who agreed to share their status updates and reported
their age and gender. The lexica, in the form of words
with associated weights, are derived from a penalized
linear regression (for continuous valued age) and sup-
port vector classification (for binary-valued gender). In
this modality, the lexica are simply a transparent and
portable means for distributing predictive models based
on words. We test generalization and adapt the lex-
ica to blogs and Twitter, plus consider situations when
limited messages are available. In addition to use in
the computational linguistics community, we believe
the lexicon format will make it easier for social sci-
entists to leverage data-driven models where manually
created lexica currently dominate
2
(Dodds et al., 2011;
Tausczik and Pennebaker, 2010).
2 Related Work
Online behavior is representative of many aspects of
a user?s demographics (Pennacchiotti and Popescu,
2011; Rao et al., 2010). Many studies have used lin-
guistic cues (such as ngrams) to determine if someone
belongs to a certain age group, be it on Twitter or an-
other social media platform (Al Zamal et al., 2012;
Argamon et al., 2009; Nguyen et al., 2013; Rangel
and Rosso, 2013). Gender prediction has been studied
across blogs (Burger and Henderson, 2006; Goswami
et al., 2009), Yahoo! search queries (Jones et al., 2007),
and Twitter (Burger et al., 2011; Nguyen et al., 2013;
Liu and Ruths, 2013; Rao et al., 2010). Because Twit-
ter does not make gender or age available, such work
infers gender and age by leveraging profile informa-
tion, such as gender-discriminating names or crawling
for links to publicly available data (e.g. Burger et al.,
2
The LIWC lexicon, derived manually based on psycho-
logical theory, (Pennebaker et al., 2001) had 1136 citations in
2013 alone.
1146
2011).
While many studies have examined prediction of age
or gender, none (to our knowledge) have released a
model to the public, much less in the form of a lexi-
con. Additionally, most works in age prediction clas-
sify users into bins rather than predicting a continuous
real-valued age as we do (exceptions: Nguyen et al.,
2013; Jones et al., 2007). People have also used online
media to infer other demographic-like attributes such
as native language (Argamon et al., 2009), origin (Rao
et al., 2010), and location (Jones et al., 2007). An ap-
proach similar to the one presented here could be used
to create lexica for any of these outcomes.
While lexica are not often used for demographics,
data-driven lexicon creation over social media has been
well studied for sentiment, in which univariate tech-
niques (e.g. point-wise mutual information) domi-
nate
3
. For example, Taboada et al. (2011) expanded
an initial lexicon by adding on co-occurring words.
More recently, Mohammad?s sentiment lexicon (Mo-
hammad et al., 2013) was found to be the most in-
formative feature for the top system in the SemEval-
2013 social media sentiment analysis task (Wilson et
al., 2013). Approaches like point-wise mutual infor-
mation take a univariate view on words?i.e. the weight
given to one feature (word) is not affected by other
features. Since language is highly collinear, we take
a multivariate lexicon development approach, which
takes covariance into account (e.g. someone who men-
tions ?hair? often is more likely to mention ?brushing?,
?style?, and ?cut?; weighting these words in isolation
might ?double-count? some information).
3 Method
Primary data. Our primary dataset consists of Face-
book messages from users of the MyPersonality appli-
cation (Kosinski and Stillwell, 2012). Messages were
posted between January 2009 and October 2011. We
restrict our analysis to those Facebook users meeting
certain criteria: they must indicate English as a primary
language, have written at least 1,000 words in their sta-
tus updates, be younger than 65 years old (data beyond
this age becomes very sparse), and indicate their gen-
der and age. This resulted in a dataset of N = 75,394
users, who wrote over 300 million words collectively.
We split our sample into training and test sets. Our
primary test set consists of a 1,000 randomly selected
Facebook users, while the training set that we used for
creating the lexica was a subset (N = 72,874) of the
remaining users.
Additional data To evaluate our predictive lexica in
differing situations, we utilize three additional datasets:
3
Note that the point-wise information-derived sentiment
lexica are often used as features in a supervised model, essen-
tially dimensionally reducing a large set of words into posi-
tive and negative sentiment, while our lexica represent the
predictive model itself.
stratified Facebook data, blogs, and tweets. The strat-
ified Facebook data (exclusively used for testing) con-
sists of equal proportions of 1,520 males and females
across 12 4-year age bins starting at 13 and ending at
60.
4
This roughly matchs the size of the main test set.
Seeking out-of-domain data, we downloaded age
and gender annotated blogs from 2004 (Schler et al.,
2006) (also used in Goswami et al., 2009) and gender
labeled tweets (Volkova et al., 2013). Limiting the sam-
ple to users who wrote at least 1000 words, the total
number of bloggers is 15,006, of which 50.6% are fe-
male and only 15% are over 27 (reflecting the younger
population standard in social media). From this we use
a randomly selected 1,000 bloggers as a blogger test set
and the remaining 14,006 bloggers for training. Sim-
ilarly for the Twitter dataset, we use 11,000 random
gender-only annotated users, in which 51.9% are fe-
male. We again randomly select 1,000 users as a test
set for gender prediction and use the remaining 10,000
for training.
3.1 Lexicon Creation
We present a method of weighted lexicon creation by
using the coefficients from linear multivariate regres-
sion and classification models. Before delving into the
creation process, consider that a weighted lexicon is of-
ten applied as the sum of all weighted word relative
frequencies over a document:
usage
lex
=
?
word?lex
w
lex
(word) ?
freq(word, doc)
freq(?, doc)
where w
lex
(word) is the lexicon (lex) weight for the
word, freq(word, doc) is frequency of the word in the
document (or for a given user), and freq(?, doc) is the
total word count for that document (or user).
Further consider how one applies linear multivariate
models in which the goal is to optimize feature coeffi-
cients that best fit the continuous outcome (regression)
or separate two classes (classification):
y = (
?
f?features
w
f
? x
f
) + w
0
where x
f
is the value for a feature (f ), w
f
is the fea-
ture coefficient, and w
0
is the intercept (a constant fit
to shift the data such that it passes through the origin).
In the case of regression, y is the outcome value (e.g.
age) while in classification y is used to separate classes
(e.g. >= 0 is female, < 0 is male). If all features are
word relative frequencies (
freq(word,doc)
freq(?,doc)
) then many
multivariate modeling techniques can simply be seen
as learning a weighted lexicon plus an intercept
5
.
4
65 females and 65 males in each of the first 11 bins:
[13,16], [17,20], . . . , [53, 56]; the last bin ([57, 60]) contained
45 males and 45 females. The [61.64] bin was excluded as it
was much smaller.
5
included in the lexicon distribution
1147
age gender
model\corpus randFB stratFB randBG randFB stratFB randBG randT
r mae r mae r mae acc acc acc acc
baseline 0 6.14 0 11.62 0 6.11 .617 .500 .508 .518
FB
lex
.835 3.40 .801 6.94 .710 5.76 .917 .913 .774 .856
BG
lex
.664 4.26 .656 11.39 .768 3.63 .838 .803 .824 .834
FB+BG
lex
.831 3.42 .795 7.06 .762 3.76 .913 .909 .822 .858
T
lex
.816 .820 .763 .889
FB+BG+T
lex
.919 .910 .820 .900
Table 1: Prediction accuracies for age (Pearson correlation coefficient(r); mean absolute error (mae) in years)
and gender (accuracy %). Baseline for age is mean age of training sample; for gender, it is the most frequent
class (female). Lexica tested include those derived from Facebook (FB
lex
), blogs (BG
lex
), and Twitter (T
lex
).
We evaluate over a random Facebook sample (randFB), a stratified Facebook sample (stratFB), a random blogger
sample (randBG), and a random twitter sample (randT). All results were a significant (p < 0.001) improvement
over the baseline.
In practice, we learn our 1gram coefficients (i.e. lex-
icon weights) from ridge regression (Hoerl and Ken-
nard, 1970) for age (continuous variable) and from sup-
port vector classification (Fan et al., 2008) for gender
(binary variable). Ridge regression uses an L2 (?||?||
2
)
penalization to avoid overfitting (Hoerl and Kennard,
1970). Although some words no doubt have a non-
linear relationship with age (e.g., ?fiance? peaks in the
20s), we still find high accuracy from a linear model
(see Table 1) and it allows for a distribution of the
model in the accessible form of a lexicon. For gender
prediction, we use an SVM with a linear kernel with
L1 penalization (?||?||
1
) (Tibshirani, 1996). Because
the L1 penalization zeros-out many coefficients, it has
the added advantage of effectively reducing the size of
the lexica. Using the training data, we test a variety al-
gorithms including the lasso, elastic net regression, and
L2 penalized SVMs in order to decide which learning
algorithms to use.
To extract the words (1grams) to use as features
and which make up lexica, we use the Happier Fun
Tokenizer,
6
which handles social media content and
markup such as emoticons or hashtags. For our main
user-level models, word usage is aggregated as the rel-
ative frequency (
freq(word,user)
freq(?,user)
). Due to the sparse
and large vocabulary of social media data, we limit the
1grams to those used by at least 1% of users.
4 Evaluation
We evaluate our predictive lexica across held-out user
data. First, we see how well lexica derived from Face-
book users predict a random set of additional users.
Then, we explore generalization of the models in vari-
ous other settings: on a stratified Facebook test sample,
blogs, and Twitter. Finally, we compare lexica fit to a
restricted number of messages per user.
Results of our evaluation over Facebook users are
shown in Table 1 (randFB columns). Accuracies for
age are reported as Pearson correlation coefficients (r)
6
downloaded from http://www.wwbp.org/data.html
and mean absolute errors (mae), measured in years.
For gender, we use an accuracy % (number-correct over
test-size). As baselines, we use the mean for age (23.0
years old) and the most frequent class (female) for gen-
der. We see that for both age and gender, accuracies are
substantially higher than the baseline. These accuracies
were just below with no significant difference previous
state-of-the-art results (Schwartz et al., 2013; r = 0.84
for age and 91.9% accuracy for gender).
7
Because of the nature of our datasets (the Face-
book data is private) and task (user-level predictions),
comparable previous studies are nearly nonexistent.
Nonetheless, the Twitter data was a random subset of
users based on the (Burger et al., 2011) dataset exclud-
ing non-English tweets, making it somewhat compa-
rable. In this case, the lexica outperformed previous
results for gender prediction of Twitter users, which
ranged from 75.5% to 87% (Burger et al., 2011; Ciot
et al., 2013; Liu and Ruths, 2013; Al Zamal et al.,
2012). However, the lexica were unable to match the
92.0% accuracy Burger et al. (2011) achieved when
using profile information in addition to text. No other
similar studies ? to the best of our knowledge ? have
been conducted.
Application in other settings. While Facebook is the
ideal setting to apply our lexica, we hope that they gen-
eralize to other situations. To evaluate their utility in
other settings, we first tested them over a gender and
age stratified Facebook sample. Our random sample,
like all of Facebook, is biased toward the young; this
stratified test sample contains equal numbers of males
and females, ages 13 to 60. Next, we use the lexica to
predict data from other domains: blogs (Schler et al.,
2006) and Twitter (Volkova et al., 2013). In this case,
our goal was to account for the content and stylistic
variation that may be specific to Facebook.
7
Adding 2 and 3-grams increases the performance of our
model (r = 0.85, 92.7%), just above our previous results
(Schwartz et al., 2013b). However, with the accessibility of
single word lexica in mind, this current work focuses on fea-
tures based entirely on 1grams.
1148
# Msgs: all 100 20 5 1
age .831 .820 .688 .454 .156
gender .919 .901 .796 .635 .554
Table 2: Prediction accuracies for age (Pearson correla-
tion) and gender (accuracy %) when reducing the num-
ber of messages from each user.
Results over these additional datasets are shown in
Table 1 (stratFB, randBG, and randT columns). The
performance decreases as expected since these datasets
have differing distributions, but it is still substantially
above mean and most frequent class baselines on the
stratified dataset. Over blogs and Twitter, both age
and gender prediction accuracies drop to a greater de-
gree (when only using the Facebook-trained models),
suggesting stylistic or content differences between the
domains. However, when using lexica created with
data from across multiple domains, the results in Face-
book, blogs, and Twitter remain in line with results
from models created specifically over their respective
domains. In light of this result, we release the FB+BG
age & FB+BG+T gender models as lexica (available at
www.wwbp.org/data.html).
Limiting messages per user. As previously noted,
some applications of demographic estimation require
predictions over more limited messages. We explore
the accuracy of user-level age and gender predictions
as the number of messages per user decreases in Ta-
ble 2. For these tests we used the FB+BG age &
FB+BG+T gender lexica. Confirming findings by Van
Durme (2012), the fewer posts one has for each user,
the less accurate the gender and age predictions. Still,
given the average user posted 205 messages, it seems
that not all messages from a user are necessary to make
a decent inference on their age and gender. Future work
may explore models developed specifically for these
limited situations.
5 Conclusion
We created publicly available lexica (words and
weights) using regression and classification models
over language usage in social media. Evaluation of the
lexica over Facebook yielded accuracies in line with
state-of-the-art age (r = 0.831) and gender (91.9% ac-
curacy) prediction. By deriving the lexica from Face-
book, blogs, and Twitter, we found the predictive power
generalized across all three domains with little sacrifice
to any one domain, suggesting the lexica may be used
in additional social media domains. We also found the
lexica maintain reasonable accuracy when writing sam-
ples were somewhat small (e.g. 20 messages) but other
approaches may be best when dealing with more lim-
ited data.
Given that manual lexica are already extensively em-
ployed in social sciences such as psychology, eco-
nomics, and business, using lexical representations of
data-driven models allows the utility of our models to
extend beyond the borders of the field of NLP.
Acknowledgement
Support for this work was provided by the Templeton
Religion Trust and by Martin Seligman of the Univer-
sity of Pennsylvania?s Positive Psychology Center.
References
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of twitter users from neighbors. In
ICWSM.
Shlomo Argamon, Moshe Koppel, James W Pen-
nebaker, and Jonathan Schler. 2009. Automatically
profiling the author of an anonymous text. Commu-
nications of the ACM, 52(2):119?123.
John D Burger and John C Henderson. 2006. An ex-
ploration of observable features related to blogger
age. In AAAI Spring Symposium: Computational
Approaches to Analyzing Weblogs, pages 15?20.
John D Burger, John C Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301?1309. Association for Computational Linguis-
tics.
2014. Faststats: How healthy are we. http://www.
cdc.gov/nchs/fastats/healthy.htm.
Accessed on March 12, 2014.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Predicting postpartum changes in
emotion and behavior via social media. In Pro-
ceedings of the 2013 ACM annual conference on
Human factors in computing systems, pages 3267?
3276. ACM.
Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M Kloumann, Catherine A Bliss, and Christo-
pher M Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and twitter. PloS one, 6(12):e26752.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Inc. Google. 2014. Google flu trends. http://
www.google.org/flutrends. Accessed on
March 12, 2014.
1149
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers age and gen-
der. In Third International AAAI Conference on We-
blogs and Social Media.
Arthur E Hoerl and Robert W Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55?67.
David A Huffaker and Sandra L Calvert. 2005.
Gender, identity, and language use in teenage
blogs. Journal of Computer-Mediated Communica-
tion, 10(2):00?00.
Rosie Jones, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2007. I know what you did last summer:
query logs and user privacy. In Proceedings of the
sixteenth ACM conference on Conference on infor-
mation and knowledge management, pages 909?914.
ACM.
Margaret L Kern, Johannes C Eichstaedt, H Andrew
Schwartz, Gregory Park, Lyle H Ungar, David J
Stillwell, Michal Kosinski, Lukasz Dziurzynski, and
Martin EP Seligman. 2014. From sooo excited!!!
to so proud: Using language to study development.
Developmental psychology, 50(1):178?188.
Michal Kosinski and David J Still-
well. 2012. mypersonality project.
http://www.mypersonality.org/wiki/.
Michal Kosinski, David J Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. volume
110, pages 5802?5805. National Acad Sciences.
David Lazer, Alex Pentland, Lada Adamic, Sinan Aral,
Albert-Laszlo Barabasi, Devon Brewer, Nicholas
Christakis, Noshir Contractor, James Fowler, Myron
Gutmann, Tony Jebara, Gary King, Michael Macy,
Deb Roy, and Marshall Van Alstyne. 2009. Com-
putational social science. Science, 323(5915):721?
723.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter. In Analyzing Microtext: 2013 AAAI Spring
Symposium.
Alessandra Marengoni, Sara Angleman, Ren?e Melis,
Francesca Mangialasche, Anita Karp, Annika Gar-
men, Bettina Meinow, and Laura Fratiglioni. 2011.
Aging with multimorbidity: a systematic review of
the literature. Ageing research reviews, 10(4):430?
439.
Robert R McCrae, Paul T Costa, Margarida Pedroso
de Lima, Ant?onio Sim?oes, Fritz Ostendorf, Alois
Angleitner, Iris Maru?si?c, Denis Bratko, Gian Vitto-
rio Caprara, Claudio Barbaranelli, et al. 1999. Age
differences in personality across the adult life span:
parallels in five cultures. Developmental Psychol-
ogy, 35(2):466?477.
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, and J Niels Rosenquist. 2011.
Understanding the demographics of twitter users.
ICWSM, 11:5th.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. arXiv
preprint arXiv:1308.6242.
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. how old do you think i am?:
A study of language and age in twitter. In Proceed-
ings of the Seventh International AAAI Conference
on Weblogs and Social Media.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to twitter user classifi-
cation. In ICWSM.
James W Pennebaker and Lori D Stone. 2003. Words
of wisdom: language use over the life span. Journal
of Personality and Social Psychology, 85(2):291?
301.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. 71:2001.
2014. Social networking fact sheet. http:
//www.pewinternet.org/fact-sheets/
social-networking-fact-sheet/. Ac-
cessed on August 26, 2014.
Francisco Rangel and Paolo Rosso. 2013. Use of lan-
guage and author profiling: Identification of gender
and age. Natural Language Processing and Cogni-
tive Science, page 177.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37?44. ACM.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W Pennebaker. 2006. Effects of age
and gender on blogging. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, volume 6, pages 199?205.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Megha Agrawal,
Gregory J Park, Shrinidhi K Lakshmikanth, Sneha
Jha, Martin EP Seligman, Lyle Ungar, et al. 2013a.
Characterizing geographic variation in well-being
using tweets. In ICWSM.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David J Stillwell, Martin EP Seligman, et al.
2013b. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one, 8(9):e73791.
1150
Rong Su, James Rounds, and Patrick Ian Armstrong.
2009. Men and things, women and people: a meta-
analysis of sex differences in interests. Psychologi-
cal Bulletin, 135(6):859?884.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of language
and social psychology, 29(1):24?54.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267?288.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning, pages 48?58. Association
for Computational Linguistics.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic lan-
guage variations to improve multilingual sentiment
analysis in social media. In Proceedings of the
2013 Conference on Empirical Methods on Natural
Language Processing.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval, volume 13.
1151
Proceedings of NAACL-HLT 2013, pages 148?157,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Experiments with Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
Latent-variable PCFGs (L-PCFGs) are a
highly successful model for natural language
parsing. Recent work (Cohen et al, 2012)
has introduced a spectral algorithm for param-
eter estimation of L-PCFGs, which?unlike
the EM algorithm?is guaranteed to give con-
sistent parameter estimates (it has PAC-style
guarantees of sample complexity). This paper
describes experiments using the spectral algo-
rithm. We show that the algorithm provides
models with the same accuracy as EM, but is
an order of magnitude more efficient. We de-
scribe a number of key steps used to obtain
this level of performance; these should be rel-
evant to other work on the application of spec-
tral learning algorithms. We view our results
as strong empirical evidence for the viability
of spectral methods as an alternative to EM.
1 Introduction
Latent-variable PCFGS (L-PCFGs) are a highly suc-
cessful model for natural language parsing (Mat-
suzaki et al, 2005; Petrov et al, 2006). Recent
work (Cohen et al, 2012) has introduced a spectral
learning algorithm for L-PCFGs. A crucial prop-
erty of the algorithm is that it is guaranteed to pro-
vide consistent parameter estimates?in fact it has
PAC-style guarantees of sample complexity.1 This
is in contrast to the EM algorithm, the usual method
for parameter estimation in L-PCFGs, which has the
weaker guarantee of reaching a local maximum of
the likelihood function. The spectral algorithm is
relatively simple and efficient, relying on a singular
value decomposition of the training examples, fol-
lowed by a single pass over the data where parame-
ter values are calculated.
Cohen et al (2012) describe the algorithm, and
the theory behind it, but as yet no experimental re-
sults have been reported for the method. This paper
1under assumptions on certain singular values in the model;
see section 2.3.1.
describes experiments on natural language parsing
using the spectral algorithm for parameter estima-
tion. The algorithm provides models with slightly
higher accuracy than EM (88.05% F-measure on test
data for the spectral algorithm, vs 87.76% for EM),
but is an order of magnitude more efficient (9h52m
for training, compared to 187h12m, a speed-up of
19 times).
We describe a number of key steps in obtain-
ing this level of performance. A simple backed-off
smoothing method is used to estimate the large num-
ber of parameters in the model. The spectral algo-
rithm requires functions mapping inside and outside
trees to feature vectors?we make use of features
corresponding to single level rules, and larger tree
fragments composed of two or three levels of rules.
We show that it is important to scale features by their
inverse variance, in a manner that is closely related
to methods used in canonical correlation analysis.
Negative values can cause issues in spectral algo-
rithms, but we describe a solution to these problems.
In recent work there has been a series of results in
spectral learning algorithms for latent-variable mod-
els (Vempala and Wang, 2004; Hsu et al, 2009;
Bailly et al, 2010; Siddiqi et al, 2010; Parikh et
al., 2011; Balle et al, 2011; Arora et al, 2012;
Dhillon et al, 2012; Anandkumar et al, 2012). Most
of these results are theoretical (although see Luque
et al (2012) for empirical results of spectral learn-
ing for dependency parsing). While the focus of
our experiments is on parsing, our findings should
be relevant to the application of spectral methods to
other latent-variable models. We view our results as
strong empirical evidence for the viability of spec-
tral methods as an alternative to EM.
2 Background
In this section we first give basic definitions for L-
PCFGs, and then describe the spectral learning algo-
rithm of Cohen et al (2012).
148
2.1 L-PCFGs: Basic Definitions
We follow the definition in Cohen et al (2012)
of L-PCFGs. An L-PCFG is an 8-tuple
(N , I,P,m, n, pi, t, q) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We as-
sume thatN = I?P , and I?P = ?. Hence we
have partitioned the set of non-terminals into
two subsets.
? [m] is the set of possible hidden states.2
? [n] is the set of possible words.
? For all a ? I, b, c ? N , h1, h2, h3 ?
[m], we have a context-free rule a(h1) ?
b(h2) c(h3). The rule has an associated pa-
rameter t(a? b c, h2, h3|a, h1).
? For all a ? P , h ? [m], x ? [n], we have a
context-free rule a(h) ? x. The rule has an
associated parameter q(a? x|a, h).
? For all a ? I, h ? [m], pi(a, h) is a parameter
specifying the probability of a(h) being at the
root of a tree.
A skeletal tree (s-tree) is a sequence of rules
r1 . . . rN where each ri is either of the form a? b c
or a? x. The rule sequence forms a top-down, left-
most derivation under a CFG with skeletal rules.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
For a given skeletal tree r1 . . . rN , define ai to be
the non-terminal on the left-hand-side of rule ri. For
any i ? [N ] such that ri is of the form a? b c, de-
fine h(2)i and h
(3)
i as the hidden state value of the left
and right child respectively. The model then defines
a probability mass function (PMF) as
p(r1 . . . rN , h1 . . . hN ) =
pi(a1, h1)
?
i:ai?I
t(ri, h
(2)
i , h
(3)
i |ai, hi)
?
i:ai?P
q(ri|ai, hi)
The PMF over skeletal trees is p(r1 . . . rN ) =?
h1...hN
p(r1 . . . rN , h1 . . . hN ).
2For any integer n, we use [n] to denote the set {1, 2, . . . n}.
The parsing problem is to take a sentence as in-
put, and produce a skeletal tree as output. A stan-
dard method for parsing with L-PCFGs is as follows.
First, for a given input sentence x1 . . . xn, for any
triple (a, i, j) such that a ? N and 1 ? i ? j ? n,
the marginal ?(a, i, j) is defined as
?(a, i, j) =
?
t:(a,i,j)?t
p(t) (1)
where the sum is over all skeletal trees t for
x1 . . . xn that include non-terminal a spanning
words xi . . . xj . A variant of the inside-outside
algorithm can be used to calculate marginals.
Once marginals have been computed, Good-
man?s algorithm (Goodman, 1996) is used to find
arg maxt
?
(a,i,j)?t ?(a, i, j).
3
2.2 The Spectral Learning Algorithm
We now give a sketch of the spectral learning algo-
rithm. The training data for the algorithm is a set
of skeletal trees. The output from the algorithm is a
set of parameter estimates for t, q and pi (more pre-
cisely, the estimates are estimates of linearly trans-
formed parameters; see Cohen et al (2012) and sec-
tion 2.3.1 for more details).
The algorithm takes two inputs in addition to the
set of skeletal trees. The first is an integer m, speci-
fying the number of latent state values in the model.
Typically m is a relatively small number; in our ex-
periments we test values such as m = 8, 16 or 32.
The second is a pair of functions ? and ?, that re-
spectively map inside and outside trees to feature
vectors in Rd and Rd
?
, where d and d? are integers.
Each non-terminal in a skeletal tree has an associ-
ated inside and outside tree. The inside tree for a
node contains the entire subtree below that node; the
outside tree contains everything in the tree excluding
the inside tree. We will refer to the node above the
inside tree that has been removed as the ?foot? of the
outside tree. See figure 1 for an example.
Section 3.1 gives definitions of ?(t) and ?(o)
used in our experiments. The definitions of ?(t) and
3In fact, in our implementation we calculate marginals
?(a? b c, i, k, j) for a, b, c ? N and 1 ? i ? k < j, and
?(a, i, i) for a ? N , 1 ? i ? n, then apply the CKY algorithm
to find the parse tree that maximizes the sum of the marginals.
For simplicity of presentation we will refer to marginals of the
form ?(a, i, j) in the remainder of this paper.
149
VP
V
saw
NP
D
the
N
dog
S
NP
D
the
N
cat
VP
Figure 1: The inside tree (shown left) and out-
side tree (shown right) for the non-terminal VP
in the parse tree [S [NP [D the ] [N cat]]
[VP [V saw] [NP [D the] [N dog]]]]
?(o) are typically high-dimensional, sparse feature
vectors, similar to those in log-linear models. For
example ? might track the rule immediately below
the root of the inside tree, or larger tree fragments;
? might include similar features tracking rules or
larger rule fragments above the relevant node.
The spectral learning algorithm proceeds in two
steps. In step 1, we learn an m-dimensional rep-
resentation of inside and outside trees, using the
functions ? and ? in combination with a projection
step defined through singular value decomposition
(SVD). In step 2, we derive parameter estimates di-
rectly from training examples.
2.2.1 Step 1: An SVD-Based Projection
For a given non-terminal a ? N , each instance of
a in the training data has an associated outside tree,
and an associated inside tree. We define Oa to be
the set of pairs of inside/outside trees seen with a in
the training data: each member of Oa is a pair (o, t)
where o is an outside tree, and t is an inside tree.
Step 1 of the algorithm is then as follows:
1. For each a ? N calculate ??a ? Rd?d
?
as
[??a]i,j =
1
|Oa|
?
(o,t)?Oa
?i(t)?j(o)
2. Perform an SVD on ??a. Define Ua ? Rd?m
(V a ? Rd
??m) to be a matrix containing the
m left (right) singular vectors corresponding
to the m largest singular values; define ?a ?
Rm?m to be the diagonal matrix with the m
largest singular values on its diagonal.
3. For each inside tree in the corpus with root la-
bel a, define
Y (t) = (Ua)>?(t)
For each outside tree with a foot node labeled
a, define
Z(o) = (?a)?1(V a)>?(o)
Note that Y (t) and Z(o) are both m-dimensional
vectors; thus we have used SVD to project inside
and outside trees to m-dimensional vectors.
2.3 Step 2: Parameter Estimation
We now describe how the functions Y (t) and Z(o)
are used in estimating parameters of the model.
First, consider the t(a? b c, h2, h3|a, h1) parame-
ters. Each instance of a given rule a? b c in the
training corpus has an outside tree o associated with
the parent labeled a, and inside trees t2 and t3 as-
sociated with the children labeled b and c. For any
rule a? b cwe defineQa?b c to be the set of triples
(o, t(2), t(3)) occurring with that rule in the corpus.
The parameter estimate is then
c?(a? b c, j, k|a, i) =
count(a? b c)
count(a)
? Ea?b ci,j,k
(2)
where
Ea?b ci,j,k =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))? Yk(t(3))
|Qa?b c|
Here we use count(a? b c) and count(a) to refer
to the count of the rule a? b c and the non-terminal
a in the corpus. Note that once the SVD step has
been used to compute representations Y (t) andZ(o)
for each inside and outside tree in the corpus, calcu-
lating the parameter value c?(a? b c, j, k|a, i) is a
very simple operation.
Similarly, for any rule a ? x, define Qa?x to
be the set of outside trees seen with that rule in the
training corpus. The parameter estimate is then
c?(a? x|a, i) =
count(a? x)
count(a)
? Ea?xi (3)
where Ea?xi =
?
o?Qa?x Zi(o)/|Q
a?x|.
A similar method is used for estimating parame-
ters c?(a, i) that play the role of the pi parameters (de-
tails omitted for brevity; see Cohen et al (2012)).
2.3.1 Guarantees for the Algorithm
Once the c?(a? b c, j, k|a, i), c?(a? x|a, i) and
c?(a, i) parameters have been estimated from the
150
training corpus, they can be used in place of the t,
q and pi parameters in the inside-outside algorithm
for computing marginals (see Eq. 1). Call the re-
sulting marginals ??(a, i, j). The guarantees for the
parameter estimation method are as follows:
? Define ?a = E[?(T )(?(O))>|A = a] where
A,O, T are random variables corresponding to
the non-terminal label at a node, the outside
tree, and the inside tree (see Cohen et al (2012)
for a precise definition). Note that ??a, as de-
fined above, is an estimate of ?a. Then if ?a
has rank m, the marginals ?? will converge to
the true values ? as the number of training ex-
amples goes to infinity, assuming that the train-
ing samples are i.i.d. samples from an L-PCFG.
? Define ? to be the m?th largest singular value
of ?a. Then the number of samples required
for ?? to be -close to ? with probability at least
1? ? is polynomial in 1/, 1/?, and 1/?.
Under the first assumption, (Cohen et al,
2012) show that the c? parameters converge to
values that are linear transforms of the orig-
inal parameters in the L-PCFG. For example,
define c(a? b c, j, k|a, i) to be the value that
c?(a? b c, j, k|a, i) converges to in the limit of infi-
nite data. Then there exist invertible matrices Ga ?
Rm?m for all a ? N such that for any a? b c, for
any h1, h2, h3 ? Rm,
t(a? b c, h2, h3|a, h1) =
?
i,j,k
[Ga]i,h1 [(G
b)?1]j,h2 [(G
c)?1]k,h3c(a? b c, j, k|a, i)
The transforms defined by the Ga matrices are be-
nign, in that they cancel in the inside-outside algo-
rithm when marginals ?(a, i, j) are calculated. Sim-
ilar relationships hold for the pi and q parameters.
3 Implementation of the Algorithm
Cohen et al (2012) introduced the spectral learning
algorithm, but did not perform experiments, leaving
several choices open in how the algorithm is imple-
mented in practice. This section describes a number
of key choices made in our implementation of the
algorithm. In brief, they are as follows:
The choice of functions ? and ?. We will de-
scribe basic features used in ? and ? (single-level
rules, larger tree fragments, etc.). We will also de-
scribe a method for scaling different features in ?
and ? by their variance, which turns out to be im-
portant for empirical results.
Estimation ofEa?b ci,j,k andE
a?x
i . There are a very
large number of parameters in the model, lead-
ing to challenges in estimation. The estimates in
Eqs. 2 and 3 are unsmoothed. We describe a simple
backed-off smoothing method that leads to signifi-
cant improvements in performance of the method.
Handling positive and negative values. As de-
fined, the c? parameters may be positive or negative;
as a result, the ?? values may also be positive or neg-
ative. We find that negative values can be a signif-
icant problem if not handled correctly; but with a
very simple fix to the algorithm, it performs well.
We now turn to these three issues in more detail.
Section 4 will describe experiments measuring the
impact of the different choices.
3.1 The Choice of Functions ? and ?
Cohen et al (2012) show that the choice of feature
definitions ? and ? is crucial in two respects. First,
for all non-terminals a ? N , the matrix ?a must
be of rank m: otherwise the parameter-estimation
algorithm will not be consistent. Second, the num-
ber of samples required for learning is polynomial
in 1/?, where ? = mina?N ?m(?a), and ?m(?a)
is the m?th smallest singular value of ?a. (Note that
the second condition is stronger than the first; ? > 0
implies that ?a is of rank m for all a.) The choice
of ? and ? has a direct impact on the value for ?:
roughly speaking, the value for ? can be thought of
as a measure of how informative the functions ? and
? are about the hidden state values.
With this in mind, our goal is to define a rel-
atively simple set of features, which nevertheless
provide significant information about hidden-state
values, and hence provide high accuracy under the
model. The inside-tree feature function ?(t) makes
use of the following indicator features (throughout
these definitions assume that a? b c is at the root
of the inside tree t):
? The pair of nonterminals (a, b). E.g., for the in-
side tree in figure 1 this would be the pair (VP, V).
151
? The pair (a, c). E.g., (VP, NP).
? The rule a? b c. E.g., VP ? V NP.
? The rule a? b c paired with the rule at the
root of t(i,2). E.g., for the inside tree in fig-
ure 1 this would correspond to the tree fragment
(VP (V saw) NP).
? The rule a? b c paired with the rule at
the root of t(i,3). E.g., the tree fragment
(VP V (NP D N)).
? The head part-of-speech of t(i,1) paired with a.4
E.g., the pair (VP, V).
? The number of words dominated by t(i,1) paired
with a (this is an integer valued feature).
In the case of an inside tree consisting of a single
rule a? x the feature vector simply indicates the
identity of that rule.
To illustrate the function ?, it will be useful to
make use of the following example outside tree:
S
NP
D
the
N
cat
VP
V
saw
NP
D N
dog
Note that in this example the foot node of the out-
side tree is labeled D. The features are as follows:
? The rule above the foot node. We take care
to mark which non-terminal is the foot, using a
* symbol. In the above example this feature is
NP ? D? N.
? The two-level and three-level rule fragments
above the foot node. In the above example these fea-
tures would be
VP
V NP
D? N
S
NP VP
V NP
D? N
? The label of the foot node, together with the
label of its parent. In the above example this is
(D, NP).
? The label of the foot node, together with the la-
bel of its parent and grandparent. In the above ex-
ample this is (D, NP, VP).
? The part of speech of the first head word along
the path from the foot of the outside tree to the root
of the tree which is different from the head node of
4We use the English head rules from the Stanford parser
(Klein and Manning, 2003).
the foot node. In the above example this is N.
? The width of the span to the left of the foot node,
paired with the label of the foot node.
? The width of the span to the right of the foot
node, paired with the label of the foot node.
Scaling of features. The features defined above
are almost all binary valued features. We scale the
features in the following way. For each feature ?i(t),
define count(i) to be the number of times the feature
is equal to 1, and M to be the number of training
examples. The feature is then redefined to be
?i(t)?
?
M
count(i) + ?
where ? is a smoothing term (the method is rela-
tively insensitive to the choice of ?; we set ? = 5 in
our experiments). A similar process is applied to the
? features. The method has the effect of decreasing
the importance of more frequent features in the SVD
step of the algorithm.
The SVD-based step of the algorithm is very
closely related to previous work on CCA (Hotelling,
1936; Hardoon et al, 2004; Kakade and Foster,
2009); and the scaling step is derived from previ-
ous work on CCA (Dhillon et al, 2011). In CCA
the ? and ? vectors are ?whitened? in a preprocess-
ing step, before an SVD is applied. This whiten-
ing process involves calculating covariance matrices
Cx = E[??>] and Cy = E[??>], and replacing ?
by (Cx)?1/2? and ? by (Cy)?1/2?. The exact cal-
culation of (Cx)?1/2 and (Cy)?1/2 is challenging in
high dimensions, however, as these matrices will not
be sparse; the transformation described above can
be considered an approximation where off-diagonal
members of Cx and Cy are set to zero. We will see
that empirically this scaling gives much improved
accuracy.
3.2 Estimation of Ea?b ci,j,k and E
a?x
i
The number of Ea?b ci,j,k parameters is very large,
and the estimation method described in Eqs. 2?3 is
unsmoothed. We have found significant improve-
ments in performance using a relatively simple back-
off smoothing method. The intuition behind this
method is as follows: given two random variablesX
and Y , under the assumption that the random vari-
ables are independent, E[XY ] = E[X] ? E[Y ]. It
152
makes sense to define ?backed off? estimates which
make increasingly strong independence assumptions
of this form.
Smoothing of binary rules For any rule a? b c
and indices i, j ? [m] we can define a second-order
moment as follows:
Ea?b ci,j,? =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))
|Qa?b c|
The definitions ofEa?b ci,?,k andE
a?b c
?,j,k are analogous.
We can define a first-order estimate as follows:
Ea?b c?,?,k =
?
(o,t(2),t(3))
?Qa?b c
Yk(t(3))
|Qa?b c|
Again, we have analogous definitions of Ea?b ci,?,? and
Ea?b c?,j,? . Different levels of smoothed estimate can
be derived from these different terms. The first is
E2,a?b ci,j,k =
Ea?b ci,j,? ? E
a?b c
?,?,k + E
a?b c
i,?,k ? E
a?b c
?,j,? + E
a?b c
?,j,k ? E
a?b c
i,?,?
3
Note that we give an equal weight of 1/3 to each of
the three backed-off estimates seen in the numerator.
A second smoothed estimate is
E3,a?b ci,j,k = E
a?b c
i,?,? ? E
a?b c
?,j,? ? E
a?b c
?,?,k
Using the definition of Oa given in section 2.2.1, we
also define
F ai =
?
(o,t)?Oa Yi(t)
|Oa|
Hai =
?
(o,t)?Oa Zi(o)
|Oa|
and our next smoothed estimate asE4,a?b ci,j,k = H
a
i ?
F bj ? F
c
k .
Our final estimate is
?Ea?b ci,j,k + (1? ?)
(
?E2,a?b ci,j,k + (1? ?)K
a?b c
i,j,k
)
where Ka?b ci,j,k = ?E
3,a?b c
i,j,k + (1? ?)E
4,a?b c
i,j,k .
Here ? ? [0, 1] is a smoothing parameter, set to?
|Qa?b c|/(C +
?
|Qa?b c|) in our experiments,
where C is a parameter that is chosen by optimiza-
tion of accuracy on a held-out set of data.
Smoothing lexical rules We define a similar
method for the Ea?xi parameters. Define
Eai =
?
x?
?
o?Qa?x? Zi(o)
?
x? |Q
a?x? |
hence Eai ignores the identity of x in making its es-
timate. The smoothed estimate is then defined as
?Ea?xi +(1??)E
a
i . Here, ? is a value in [0, 1] which
is tuned on a development set. We only smooth lex-
ical rules which appear in the data less than a fixed
number of times. Unlike binary rules, for which the
estimation depends on a high order moment (third
moment), the lexical rules use first-order moments,
and therefore it is not required to smooth rules with
a relatively high count. The maximal count for this
kind of smoothing is set using a development set.
3.3 Handling Positive and Negative Values
As described before, the parameter estimates may
be positive or negative, and as a result the
marginals computed by the algorithm may in some
cases themselves be negative. In early exper-
iments we found this to be a signficant prob-
lem, with some parses having a very large num-
ber of negatives, and being extremely poor in qual-
ity. Our fix is to define the output of the parser
to be arg maxt
?
(a,i,j)?t |?(a, i, j)| rather than
arg maxt
?
(a,i,j)?t ?(a, i, j) as defined in Good-
man?s algorithm. Thus if a marginal value ?(a, i, j)
is negative, we simply replace it with its absolute
value. This step was derived after inspection of the
parsing charts for bad parses, where we saw evi-
dence that in these cases the entire set of marginal
values had been negated (and hence decoding under
Eq. 1 actually leads to the lowest probability parse
being output under the model). We suspect that this
is because in some cases a dominant parameter has
had its sign flipped due to sampling error; more the-
oretical and empirical work is required in fully un-
derstanding this issue.
4 Experiments
In this section we describe parsing experiments us-
ing the L-PCFG estimation method. We give com-
parisons to the EM algorithm, considering both
speed of training, and accuracy of the resulting
model; we also give experiments investigating the
various choices described in the previous section.
153
We use the Penn WSJ treebank (Marcus et al,
1993) for our experiments. Sections 2?21 were
used as training data, and sections 0 and 22 were
used as development data. Section 23 is used as
the final test set. We binarize the trees in train-
ing data using the same method as that described in
Petrov et al (2006). For example, the non-binary
rule VP ? V NP PP SBAR would be converted
to the structure [VP [@VP [@VP V NP] PP]
SBAR] where @VP is a new symbol in the grammar.
Unary rules are removed by collapsing non-terminal
chains: for example the unary rule S ? VP would
be replaced by a single non-terminal S|VP.
For the EM algorithm we use the initialization
method described in Matsuzaki et al (2005). For ef-
ficiency, we use a coarse-to-fine algorithm for pars-
ing with either the EM or spectral derived gram-
mar: a PCFG without latent states is used to calcu-
late marginals, and dynamic programming items are
removed if their marginal probability is lower than
some threshold (0.00005 in our experiments).
For simplicity the parser takes part-of-speech
tagged sentences as input. We use automatically
tagged data from Turbo Tagger (Martins et al,
2010). The tagger is used to tag both the devel-
opment data and the test data. The tagger was re-
trained on sections 2?21. We use the F1 measure
according to the Parseval metric (Black et al, 1991).
For the spectral algorithm, we tuned the smoothing
parameters using section 0 of the treebank.
4.1 Comparison to EM: Accuracy
We compare models trained using EM and the spec-
tral algorithm using values form in {8, 16, 24, 32}.5
For EM, we found that it was important to use de-
velopment data to choose the number of iterations
of training. We train the models for 100 iterations,
then test accuracy of the model on section 22 (devel-
opment data) at different iteration numbers. Table 1
shows that a peak level of accuracy is reached for all
values of m, other than m = 8, at iteration 20?30,
with sometimes substantial overtraining beyond that
point.
The performance of a regular PCFG model, esti-
mated using maximum likelihood and with no latent
5Lower values of m, such as 2 or 4, lead to substantially
lower performance for both models.
section 22 section 23
EM spectral EM spectral
m = 8 86.87 85.60 ? ?
m = 16 88.32 87.77 ? ?
m = 24 88.35 88.53 ? ?
m = 32 88.56 88.82 87.76 88.05
Table 2: Results on the development data (section 22,
with machine-generated POS tags) and test data (section
23, with machine-generated POS tags).
states, is 68.62%.
Table 2 gives results for the EM-trained models
and spectral-trained models. The spectral models
give very similar accuracy to the EM-trained model
on the test set. Results on the development set with
varying m show that the EM-based models perform
better for m = 8, but that the spectral algorithm
quickly catches up as m increases.
4.2 Comparison to EM: Training Speed
Table 3 gives training times for the EM algorithm
and the spectral algorithm for m ? {8, 16, 24, 32}.
All timing experiments were done on a single Intel
Xeon 2.67GHz CPU. The implementations for the
EM algorithm and the spectral algorithm were writ-
ten in Java. The spectral algorithm also made use
of Matlab for several matrix calculations such as the
SVD calculation.
For EM we show the time to train a single iter-
ation, and also the time to train the optimal model
(time for 30 iterations of training for m = 8, 16, 24,
and time for 20 iterations for m = 32). Note that
this latter time is optimistic, as it assumes an oracle
specifying exactly when it is possible to terminate
EM training with no loss in performance. The spec-
tral method is considerably faster than EM: for ex-
ample, for m = 32 the time for training the spectral
model is just under 10 hours, compared to 187 hours
for EM, a factor of almost 19 times faster.6
The reason for these speed ups is as follows.
Step 1 of the spectral algorithm (feature calculation,
transfer + scaling, and SVD) is not required by EM,
but takes a relatively small amount of time (about
1.2 hours for all values of m). Once step 1 has been
completed, step 2 of the spectral algorithm takes a
6In practice, in order to overcome the speed issue with EM
training, we parallelized the E-step on multiple cores. The spec-
tral algorithm can be similarly parallelized, computing statistics
and parameters for each nonterminal separately.
154
10 20 30 40 50 60 70 80 90 100
m = 8 83.51 86.45 86.68 86.69 86.63 86.67 86.70 86.82 86.87 86.83
m = 16 85.18 87.94 88.32 88.21 88.10 87.86 87.70 87.46 87.34 87.24
m = 24 83.62 88.19 88.35 88.25 87.73 87.41 87.35 87.26 87.02 86.80
m = 32 83.23 88.56 88.52 87.82 87.06 86.47 86.38 85.85 85.75 85.57
Table 1: Results on section 22 for the EM algorithm, varying the number of iterations used. Best results in each row
are in boldface.
single EM spectral algorithm
EM iter. best model total feature transfer + scaling SVD a? b c a? x
m = 8 6m 3h 3h32m
22m 49m
36m 1h34m 10m
m = 16 52m 26h6m 5h19m 34m 3h13m 19m
m = 24 3h7m 93h36m 7h15m 36m 4h54m 28m
m = 32 9h21m 187h12m 9h52m 35m 7h16m 41m
Table 3: Running time for the EM algorithm and the various stages in the spectral algorithm. For EM we show the
time for a single iteration, and the time to train the optimal model (time for 30 iterations of training for m = 8, 16, 24,
time for 20 iterations of training for m = 32). For the spectral method we show the following: ?total? is the total
training time; ?feature? is the time to compute the ? and ? vectors for all data points; ?transfer + scaling? is time
to transfer the data from Java to Matlab, combined with the time for scaling of the features; ?SVD? is the time for
the SVD computation; a? b c is the time to compute the c?(a? b c, h2, h3|a, h1) parameters; a? x is the time to
compute the c?(a? x, h|a, h) parameters. Note that ?feature? and ?transfer + scaling? are the same step for all values
of m, so we quote a single runtime for these steps.
single pass over the data: in contrast, EM requires
a few tens of passes (certainly more than 10 passes,
from the results in table 1). The computations per-
formed by the spectral algorithm in its single pass
are relatively cheap. In contrast to EM, the inside-
outside algorithm is not required; however various
operations such as calculating smoothing terms in
the spectral method add some overhead. The net re-
sult is that form = 32 the time for training the spec-
tral method takes a very similar amount of time to a
single pass of the EM algorithm.
4.3 Smoothing, Features, and Negatives
We now describe experiments demonstrating the im-
pact of various components described in section 3.
The effect of smoothing (section 3.2) Without
smoothing, results on section 22 are 85.05% (m =
8, ?1.82), 86.84% (m = 16, ?1.48), 86.47%
(m = 24, ?1.88), 86.47% (m = 32, ?2.09) (in
each case we show the decrease in performance from
the results in table 2). Smoothing is clearly impor-
tant.
Scaling of features (section 3.1) Without scaling
of features, the accuracy on section 22 with m = 32
is 84.40%, a very significant drop from the 88.82%
accuracy achieved with scaling.
Handling negative values (section 3.3) Replac-
ing marginal values ?(a, i, j) with their absolute
values is also important: without this step, accu-
racy on section 22 decreases to 80.61% (m = 32).
319 sentences out of 1700 examples have different
parses when this step is implemented, implying that
the problem with negative values described in sec-
tion 3.3 occurs on around 18% of all sentences.
The effect of feature functions To test the effect
of features on accuracy, we experimented with a
simpler set of features than those described in sec-
tion 3.1. This simple set just includes an indicator
for the rule below a nonterminal (for inside trees)
and the rule above a nonterminal (for outside trees).
Even this simpler set of features achieves relatively
high accuracy (m = 8: 86.44 , m = 16: 86.86,
m = 24: 87.24 , m = 32: 88.07 ).
This set of features is reminiscent of a PCFG
model where the nonterminals are augmented their
parents (vertical Markovization of order 2) and bina-
rization is done while retaining sibling information
(horizontal Markovization of order 1). See Klein
and Manning (2003) for more information. The per-
155
formance of this Markovized PCFG model lags be-
hind the spectral model: it is 82.59%. This is prob-
ably due to the complexity of the grammar which
causes ovefitting. Condensing the sibling and parent
information using latent states as done in the spectral
model leads to better generalization.
It is important to note that the results for both
EM and the spectral algorithm are comparable to
state of the art, but there are other results previ-
ously reported in the literature which are higher.
For example, Hiroyuki et al (2012) report an ac-
curacy of 92.4 F1 on section 23 of the Penn WSJ
treebank using a Bayesian tree substitution gram-
mar; Charniak and Johnson (2005) report accuracy
of 91.4 using a discriminative reranking model; Car-
reras et al (2008) report 91.1 F1 accuracy for a dis-
criminative, perceptron-trained model; Petrov and
Klein (2007) report an accuracy of 90.1 F1, using
L-PCFGs, but with a split-merge training procedure.
Collins (2003) reports an accuracy of 88.2 F1, which
is comparable to the results in this paper.
5 Conclusion
The spectral learning algorithm gives the same level
of accuracy as EM in our experiments, but has sig-
nificantly faster training times. There are several ar-
eas for future work. There are a large number of pa-
rameters in the model, and we suspect that more so-
phisticated regularization methods than the smooth-
ing method we have described may improve perfor-
mance. Future work should also investigate other
choices for the functions ? and ?. There are natu-
ral ways to extend the approach to semi-supervised
learning; for example the SVD step, where repre-
sentations of outside and inside trees are learned,
could be applied to unlabeled data parsed by a first-
pass parser. Finally, the methods we have described
should be applicable to spectral learning for other
latent variable models.
Acknowledgements
Columbia University gratefully acknowledges the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of DARPA,
AFRL, or the US government. Shay Cohen was
supported by the National Science Foundation un-
der Grant #1136996 to the Computing Research As-
sociation for the CIFellows Project. Dean Foster
was supported by National Science Foundation grant
1106743.
References
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for learn-
ing latent-variable models. arXiv:1210.7559.
S. Arora, R. Se, and A. Moitra. 2012. Learning topic
models - going beyond SVD. In Proceedings of
FOCS.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings of ALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In Proceedings of CoNLL, pages
9?16.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589?637.
P. Dhillon, D. P. Foster, and L. H. Ungar. 2011. Multi-
view learning of word embeddings via CCA. In Pro-
ceedings of NIPS.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with latent
variables. In Proceedings of EMNLP.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
156
D. Hardoon, S. Szedmak, and J. Shawe-Taylor. 2004.
Canonical correlation analysis: An overview with ap-
plication to learning methods. Neural Computation,
16(12):2639?2664.
S. Hiroyuki, M. Yusuke, F. Akinori, and N. Masaaki.
2012. Bayesian symbol-refined tree substitution gram-
mars for syntactic parsing. In Proceedings of ACL,
pages 440?448.
H. Hotelling. 1936. Relations between two sets of vari-
ants. Biometrika, 28:321?377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
S. M. Kakade and D. P. Foster. 2009. Multi-view regres-
sion via canonical correlation analysis. In COLT.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL, pages 423?430.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. T.
Figueiredo, and M. Q. Aguiar. 2010. TurboParsers:
Dependency parsing by approximate variational infer-
ence. In Proceedings of EMNLP.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of COLING-ACL.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. JMLR, 9:741?748.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
157
Tutorials, NAACL-HLT 2013, pages 13?15,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Spectral Learning Algorithms for
Natural Language Processing
Shay Cohen?, Michael Collins?, Dean P. Foster?, Karl Stratos?, Lyle Ungar?
?Columbia University
?University of Pennsylvania
scohen,mcollins,stratos@cs.columbia.edu
dean@foster.net
ungar@cis.upenn.edu
1 Introduction
Recent work in machine learning and NLP has developed spectral algorithms for
many learning tasks involving latent variables. Spectral algorithms rely on sin-
gular value decomposition as a basic operation, usually followed by some simple
estimation method based on the method of moments. From a theoretical point
of view, these methods are appealing in that they offer consistent estimators (and
PAC-style guarantees of sample complexity) for several important latent-variable
models. This is in contrast to the EM algorithm, which is an extremely success-
ful approach, but which only has guarantees of reaching a local maximum of the
likelihood function.
From a practical point of view, the methods (unlike EM) have no need for
careful initialization, and have recently been shown to be highly efficient (as one
example, in work under submission by the authors on learning of latent-variable
PCFGs, a spectral algorithm performs at identical accuracy to EM, but is around
20 times faster).
2 Outline
In this tutorial we will aim to give a broad overview of spectral methods, describing
theoretical guarantees, as well as practical issues. We will start by covering the
basics of singular value decomposition and describe efficient methods for doing
singular value decomposition. The SVD operation is at the core of most spectral
algorithms that have been developed.
13
We will then continue to cover canonical correlation analysis (CCA). CCA is an
early method from statistics for dimensionality reduction. With CCA, two or more
views of the data are created, and they are all projected into a lower dimensional
space which maximizes the correlation between the views. We will review the
basic algorithms underlying CCA, give some formal results giving guarantees for
latent-variable models and also describe how they have been applied recently to
learning lexical representations from large quantities of unlabeled data. This idea
of learning lexical representations can be extended further, where unlabeled data is
used to learn underlying representations which are subsequently used as additional
information for supervised training.
We will also cover how spectral algorithms can be used for structured predic-
tion problems with sequences and parse trees. A striking recent result by Hsu,
Kakade and Zhang (2009) shows that HMMs can be learned efficiently using a
spectral algorithm. HMMs are widely used in NLP and speech, and previous al-
gorithms (typically based on EM) were guaranteed to only reach a local maximum
of the likelihood function, so this is a crucial result. We will review the basic me-
chanics of the HMM learning algorithm, describe its formal guarantees, and also
cover practical issues.
Last, we will cover work about spectral algorithms in the context of natural
language parsing. We will show how spectral algorithms can be used to estimate
the parameter models of latent-variable PCFGs, a model which serves as the base
for state-of-the-art parsing models such as the one of Petrov et al (2007). We will
show what are the practical steps that are needed to be taken in order to make spec-
tral algorithms for L-PCFGs (or other models in general) practical and comparable
to state of the art.
3 Speaker Bios
Shay Cohen1 is a postdoctoral research scientist in the Department of Computer
Science at Columbia University. He is a computing innovation fellow. His re-
search interests span a range of topics in natural language processing and machine
learning. He is especially interested in developing efficient and scalable parsing
algorithms as well as learning algorithms for probabilistic grammars.
Michael Collins2 is the Vikram S. Pandit Professor of computer science at
Columbia University. His research is focused on topics including statistical pars-
ing, structured prediction problems in machine learning, and applications including
machine translation, dialog systems, and speech recognition. His awards include a
1http://www.cs.columbia.edu/?scohen/
2http://www.cs.columbia.edu/?mcollins/
14
Sloan fellowship, an NSF career award, and best paper awards at EMNLP (2002,
2004, and 2010), UAI (2004 and 2005), and CoNLL 2008.
Dean P. Foster3 is currently the Marie and Joseph Melone Professor of Statis-
tics at the Wharton School of the University of Pennsylvania. His current research
interests are machine learning, stepwise regression and computational linguistics.
He has been searching for new methods of finding useful features in big data sets.
His current set of hammers revolve around fast matrix methods (which decompose
2nd moments) and tensor methods for decomposing 3rd moments.
Karl Stratos4 is a Ph.D. student in the Department of Computer Science at
Columbia. His research is focused on machine learning and natural language pro-
cessing. His current research efforts are focused on spectral learning of latent-
variable models, or more generally, uncovering latent structure from data.
Lyle Ungar5 is a professor at the Computer and Information Science Depart-
ment at the University of Pennsylvania. His research group develops scalable ma-
chine learning and text mining methods, including clustering, feature selection,
and semi-supervised and multi-task learning for natural language, psychology, and
medical research. Example projects include spectral learning of language models,
multi-view learning for gene expression and MRI data, and mining social media to
better understand personality and well-being.
3http://gosset.wharton.upenn.edu/?foster/index.pl
4http://www.cs.columbia.edu/?stratos/
5http://www.cis.upenn.edu/?ungar/
15
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 296?305, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Choosing the Right Words:
Characterizing and Reducing Error of the Word Count Approach
H. Andrew Schwartz,1 Johannes Eichstaedt,1 Lukasz Dziurzynski,1 Eduardo Blanco,2
Margaret L. Kern,1 Stephanie Ramones,1 Martin Seligman,1 and Lyle Ungar1
1University of Pennsylvania
2Lymba Corporation
hansens@seas.upenn.edu
Abstract
Social scientists are increasingly using the
vast amount of text available on social me-
dia to measure variation in happiness and
other psychological states. Such studies count
words deemed to be indicators of happiness
and track how the word frequencies change
across locations or time. This word count ap-
proach is simple and scalable, yet often picks
up false signals, as words can appear in differ-
ent contexts and take on different meanings.
We characterize the types of errors that occur
using the word count approach, and find lex-
ical ambiguity to be the most prevalent. We
then show that one can reduce error with a
simple refinement to such lexica by automat-
ically eliminating highly ambiguous words.
The resulting refined lexica improve precision
as measured by human judgments of word oc-
currences in Facebook posts.
1 Introduction
Massive social media corpora, such as blogs, tweets,
and Facebook statuses have recently peaked the in-
terest of social scientists. Compared to traditional
samples in tens or hundreds, social media sample
sizes are orders of magnitude larger, often contain-
ing millions or billions of posts or queries. Such text
provides potential for unobtrusive, inexpensive, and
real-time measurement of psychological states (such
as positive or negative affect) and aspects of sub-
jective well-being (such as happiness and engage-
ment). Social scientists have recently begun to use
social media text in a variety of studies (Cohn et
al., 2004; Kramer, 2010; Tausczik and Pennebaker,
2010; Kamvar and Harris, 2011; Dodds et al, 2011;
Golder and Macy, 2011).
One of the most popular approaches to estimate
psychological states is by using the word count
method (Pennebaker et al, 2007), where one tracks
the frequency of words that have been judged to be
associated with a given state. Greater use of such
words is taken to index the prevalence of the cor-
responding state. For example, the use of the word
?happy? is taken to index positive emotion, and ?an-
gry? to index negative emotion. The most widely
used tool to carry out such analysis, and the one we
investigate in this paper, is Pennebaker?s Linguistic
Inquiry and Word Count, (LIWC) (Pennebaker et al,
2001; Pennebaker et al, 2007). LIWC, originally de-
veloped to analyze writing samples for emotion and
control, has grown to include a variety of lexica for
linguistic and psychosocial topics including positive
and negative emotions, pronouns, money, work, and
religion. The word count approach has high appeal
to social scientists in need of a tool to approach so-
cial media, and although others have been used (see,
for example (Gottschalk and Bechtel, 1998; Bollen
et al, 2010), LIWC?s lexica are generally perceived
as a ?tried-and-tested? list of words (Miller, 2011).
Unfortunately, the word count approach has some
drawbacks when used as indicators for psycholog-
ical states. Words are the unit of measurement, but
words can carry many different meanings depending
on context. Consider the Facebook posts below con-
taining instances of ?play?, a word associated with
positive emotion in LIWC.
296
1. so everyone should come to the play tomor-
row...
2. Does anyone what type of file i need to convert
youtube videos to play on PS3???
3. Time to go play with Chalk from the Easter
Bunny!
Out of the three instances, only (3) seems to com-
municate positive emotion. In (1), ?play? is used as
a noun rather than the expected verb, while in (2),
?play? is a verb but it is used in a sense that is not
directly associated with positive emotion. (1) and
(2) demonstrate how lexical ambiguities (i.e. multi-
ple parts-of-speech or word senses) can affect accu-
racy of words in a lexicon. Additionally, even when
appearing as the expected part of speech and word
sense, signal from a word may change due to its con-
text, such as being within the scope of a negation as
in (4), or describing something desired as in (5).
4. ...all work no play :-(
5. i sure wish i had about 50 hours a day to play
cod
Our goal is to characterize the errors of the widely
used word count approach, and show that such lex-
ica can be significantly improved by employing an
ambiguity metric to refine such lexica. Rather than
work on a new method of measuring psychological
states, we work within the bounds of word count and
ask how accurate it is and whether we can improve
it without sacrificing its simplicity and scalability.
We attempt to reduce the erroneous signal of
the word count approach while maintaining legiti-
mate signal simply by refining the lexicon. In other
words, we would like to move closer to the goal in
Figure 1, by eliminating words that often carry er-
roneous signal such as ?play?, and keeping words
which often carry the sought-after signal, such as
?cheerful?. The difficulty in doing this is that we do
not have the data to tell us which words are most
likely to carry signal (even if we had such data we
would like to develop a method that could be applied
to any newly created lexica). Instead we leverage
part-of-speech and word sense data to help us deter-
mine which words are lexically ambiguous.
Figure 1: The relationship between text expressing posi-
tive emotion (POSEMO) and text containing LIWC terms
for POSEMO.
Our approach of eliminating ambiguous words
increases the precision at the expense of recall, a
reasonable trade-off in social media where we are
working with millions or even billions of word in-
stances. Additionally, it is minimally-supervised, in
that we do not require training data on human-state;
instead we use existing hand-labeled corpora, such
as SemCor (Miller et al, 1993), for word sense in-
formation. Not requiring training data also means
our refinement is flexible; it can be applied to mul-
tiple domains and lexica, it makes few assumptions
that might introduce problems of over-fitting, and it
is parsimonious in that it merely improves an estab-
lished approach.
This paper makes two primary contributions: (1)
an analysis of the types of errors common for the
word count approach (Section 3), and (2) a general
method for refining psychosocial lexica based on the
ambiguity of words (Section 4). Before describing
these contributions, we discuss related work, mak-
ing the case for using social media in social science
and surveying some work in computational linguis-
tics. We then evaluate both the original LIWC lex-
icon and our refinement of it against human judg-
ments of expression of positive and negative emo-
tions on hand-annotated Facebook posts, and show
the benefit of lexicon refinement for estimating well-
being over time for large aggregates of posts. Fi-
nally, we discuss the implications of our work and
possible future directions.
297
2 Background
Compared to traditional approaches in the social sci-
ences, large scale analysis of social media is cheap,
near real-time, unobtrusive, and gives high cover-
age. We outline these advantages below.
Inexpensive Extracting information from sources
such Facebook and Twitter is vastly cheaper than the
more conventional polling done by companies such
as Gallup ? and by many social science researchers.
Social media data does not require phone calls to be
made or doors to be knocked on. For example, a rep-
resentative survey asking 1,000 people by a leading
polling company costs to the order of $10,0001. In
contrast, once the software exists, social media data
from tens of millions of users can be obtained and
analyzed at a fraction of the cost.
Temporal Resolution Much of the attraction of
social media stems from the fact that it captures
a written live stream of collective thought. When
Google relied on search queries to monitor health-
seeking behavior to predict influenza epidemics, the
reporting lag was a mere day, whereas traditional
CDC surveillance systems take 1-2 weeks to pub-
lish their data (Ginsberg et al, 2009). Infrastructure
based on social media and Internet use data allows
reporting and analysis systems with little to no re-
porting lag. Additionally, traditional survey designs
are typically only designed to assess psychological
states at a given point in time.
Unobtrusive Estimation Traditional self-report
survey approaches, even those implemented on the
web, suffer from social desirability, priming, and
other biases. For example, Kahneman et al (Kah-
neman et al, 2006) found that the order in which
questions are asked on questionnaires can determine
how they are answered. By looking directly into the
social worlds, many of these self-report biases can
be avoided. The traces of human interactions in so-
cial media represent the goings-on in their original
ecologies of meaning and signification. This ap-
proach diminishes the inferential distance between
the context of the phenomena and the context of
measurement ? and thus decreases the room for sys-
tematic distortion of signal.
1Gallup, Personal correspondence.
2.1 The Word Count Approach
As previously noted, the word count approach is
most often used by social scientists through the tool
known as Linguistic Inquiry and Word Count or
LIWC (Pennebaker et al, 2007). The LIWC2007
dictionary is composed of almost 4,500 words and
word stems organized across one or more word cat-
egories, including 406 positive emotion words and
499 negative emotion words. When long form texts
are analyzed with LIWC, the program simply re-
turns the percentages of words belonging to the dif-
ferent analytical categories ? the simplicity of this
approach makes it popular with non-technical social
scientists.
LIWC?s positive and negative emotion lexica have
recently begun to be used on ?short form? writing in
social media. For example, Golder and Macy (2011)
used LIWC to study diurnal and seasonal variation
in mood in a collection of 400 million Twitter mes-
sages. Kramer (2010) proposed the ?Gross National
Happiness? index and Kivran-Swaine and Naaman
(2011) examined associations between user expres-
sions of positive and negative emotions and the size
and density of social networks. A comprehensive
review can be found in Tausczik and Pennebaker
(2010).
To our knowledge there is only one work which
has evaluated LIWC?s accuracy over social media.
Bantum and Owen (2009) evaluated LIWC on a set
of posts to an Internet-based breast cancer support
group. By annotating expression of emotion within
this text, they were able to produce accuracy figures
of sensitivity (much like recall) and predictive va-
lidity (precision). Sensitivity measured how often
a word (in context) expressing positive or negative
emotion was captured by LIWC. Predictive validity
measured how often a word (in context) captured
by LIWC as measuring positive or negative emotion
was indeed expressing positive or negative emotion.
While they found a recall of 0.88, the precision was
only 0.31 ? that is, only 31% of instances contain-
ing words indexed by LIWC actually conveyed the
associated emotion. We contend that this is a major
drawback for applying LIWC to social media, be-
cause while it is not important to catch every expres-
sion of emotion out of a million Tweets, it is impor-
tant that when something is captured it is an accurate
298
estimate of the true state.
2.2 Related Work in Computational
Linguistics
Researchers have been exploring the use of lexica
that define the subjective orientation of words for
tasks such as sentiment or subjectivity analysis. A
common weakly-supervised approach starts with a
small set of sentiment knowledge (seed words as-
sociated with a given sentiment) and expands the
words into a large lexicon (Hatzivassiloglou and
McKeown, 1997; Kamps and Marx, 2002; Kim and
Hovy, 2004; Kanayama and Nasukawa, 2006; Bac-
cianella et al, 2010). We take a different approach.
Rather than expanding lexica, we start with a large
set of words and refine the set. The refinement in-
creases precision at the cost of recall, which is a
reasonable exchange when we are looking at mil-
lions or even billions of word instances. Standard
applications of sentiment analysis, such as annotat-
ing movie reviews, may not be as inclined to skip
instances, since they want to make predictions for
items which have very few reviews.
Another line of work in sentiment analysis has
created lexicons using supervised learning. One of
the first works to do so was by Pang and colleagues
(2002), who used data including author ratings of
reviews, such as IMDB movie reviews. The author
ratings become training data for sentiment classifi-
cation. Pang et al showed that human-created lexi-
cons did not perform as well as lexicons based on
simple word statistics over the training data. In-
terestingly, they found that words like ?still? were
most predictive of positive movie reviews, and that
punctuation marks of ?!? and ??? were strong signs
of negative movie reviews. Unfortunately, training
data for subjective well-being or happiness is not
yet available, preventing the use of such supervised
learning methods. Additionally, this work seeks to
experiment within the bounds of what social sci-
entists are in fact using (with publications in high-
impact venues such as Science). We thus take a dif-
ferent approach, and automatically improve human
created lexicons.
Wiebe and Cardie (2005) generalized the task of
sentiment analysis to that of discovering subjectiv-
ity such as ?opinions, emotions, sentiments, specu-
lations, evaluations, etc.?. More recently, Wilson et
POSEMO NEGEMO
term frequency term frequency
like 774,663 hate 167,109
love 797,833 miss 158,274
good 571,687 bad 151,496
friend* 406,568 bore* 140,684
happy 384,797 shit* 114,923
LOL 370,613 hurt* 98,291
well* 284,002 craz* 94,518
great 263,113 lost 94,059
haha* 240,587 damn* 93,666
best 227,381 fuck 90,212
better 212,547 stupid* 85,587
fun 216,432 kill* 83,593
please* 174,597 hell 80,046
hope 170,998 fuckin* 79,959
thank 161,827 wrong* 70,714
Table 1: Most frequent POSEMO and NEGEMO terms in
LIWC in the 12.7 million Facebook posts. ?*? indicates a
wildcard, so that ?well*? matches ?wellness?.
al. (2009) contended that the context may neutralize
or change the polarity of the subjective orientation
of a word. It is difficult to determine where concepts
of happiness such as quality of relationships or de-
gree of achievement in life fit in with subjectivity.
Thus, we do not claim to be measuring subjectivity
and instead we use the general term of ?psychologi-
cal state?, referring to ?the way something [a person]
is with respect to its main attributes? (Miller, 1993).
To the best of our knowledge, while part-of-
speech tagging and word sense disambiguation are
staple tasks in the computational linguistics commu-
nity, the utility of a lexical ambiguity metric has yet
to be explored.
3 Annotation and Analysis of Errors from
the Word Count Method
One objective of our work is to document and de-
scribe how often different types of errors occur when
using the word count approach on social media. To
do this, we first judged a sample of 1,000 instances
of LIWC terms occurring in Facebook posts to indi-
cate whether they contribute signal towards the as-
sociated LIWC category (i.e. positive emotion). We
then took instances that were deemed to carry erro-
neous signal and annotated them with a label for the
299
category agreement instances base rate
POSEMO 0.742 500 .654
NEGEMO 0.746 500 .697
TOTAL 0.744 1,000 .676
random 0.343 - -
Table 2: Inter-annotator agreement over 1,000 instances
of LIWC terms in Facebook posts. Base rate is the aver-
age of how often an annotator answered true.
type of signal error. This section describes the pro-
cess we used in generating these annotations and the
results we found.
3.1 Annotation Process
Annotating social media instances of lexica terms
provides insight into how well the word count ap-
proach works, and also yields a ?ground truth? for
evaluating our lexicon refinement methods. We ran-
domly selected for labeling a sample of 1,000 sta-
tus updates containing words from a given lexicon
drawn from a collection of 12.7 million Facebook
status updates provided by the Cambridge myPer-
sonality project (Kosinski and Stillwell, 2012).
We used terms from the LIWC positive emotion
(POSEMO) and negative emotion (NEGEMO) lex-
ica, which are the same lexica used by the works of
Kramer (2010), Kivran-Swaine and Naaman (2011),
and Golder and Macy (2011). Table 1 lists the
most frequent POSEMO and NEGEMO terms in our
Facebook sample.
As mentioned above, we did two types of annota-
tions. First, we judged whether each given instance
of a word conveyed the correct associated type of
emotion. The second task took a sample of instances
judged to have incorrect signal and labeled them
with a reason for the error; We refer to this as signal
error type.
For the first task, we had three human judges inde-
pendently evaluate the 1,000 status update instances
as to whether they were indeed correct signal. The
question the judges were told to answer was ?Does
the word contribute to the associated psychological-
state (POSEMO or NEGEMO) within the sentence
it appears??. In other words, ?would the sentence
convey less [positive emotion or negative emotion]
without this word??. Subjective feedback from the
judges indicated that it was often difficult to make
a decision, so we used three judges per instance. In
the case of conflict between judges, the ?correct? la-
bel for validation of the refined lexicon was defined
to be the majority vote. A sampling of Facebook sta-
tuses demonstrates a mixed picture of relevance for
the unrefined LIWC dictionaries:
1. has had a very good day (?good? - POSEMO)
2. is so very bored. (?bore*? - NEGEMO)
3. damn, that octopus is good, lol (?damn? -
NEGEMO)
4. thank you for his number (?numb*? -
NEGEMO)
5. I got pranked sooooo bad (?bad? - NEGEMO)
6. don?t be afraid to fail (?afraid? - NEGEMO)
7. I wish I could . . . and we could all just be happy
(?happy? - POSEMO)
Some posts clearly use positive or negative lexicon
words such as (1) and (2). Curse words can signify
negative emotion or emphasize the opposite state as
in (3), which is clearly emphasizing positive emo-
tion here. Example (5) demonstrates the word sense
issue we discussed previously. Words with wild-
cards that expand into other words with different
meanings can be particularly problematic, as the ex-
panded word can be far more frequent ? and very
different in meaning ? from the original word. For
example, ?numb*? matches ?number? in 4.
A different problem occurs when the context
of the word changes its implication for the emo-
tional state of the writer. This can either occur
through negation such as in (6) where ?afraid? sig-
nals NEGEMO, but is negated with ?don?t? or the
signal can be changed indirectly through a variety of
words indicating that the writer desires (and hence
lacks) the state, as in (7) where someone is wishing
to be ?happy?.
Table 2 shows the agreement between an-
notators calculated as
?
i agree(A
(i)
1 ,A
(i)
2 ,A
(i)
3 )
1,000 , where
agree(A1, A2, A3) was 1 when all three annota-
tions matched and 0 otherwise. Given the aver-
age positive base rate across annotators was 0.676
the chance that all three reviewers agree accord-
ing to chance (random agreement) is calculated as
300
category precision instances
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
Table 4: Accuracy of LIWC POSEMO and NEGEMO
lexica over Facebook posts.
0.6763+(1?0.676)3 = 0.343, the probability of all
three answering yes plus the probability of all three
answering no.
For the second task, we selected 100 instances
judged to be incorrect signal from the first task, and
labeled them according to the best reason for the
mistake. This task required more linguistic exper-
tise and was performed by a single annotator. La-
bels and descriptions are given in Table 3, which
breaks down the cases into lexical ambiguity, direct
or indirect negation, and other reasons such as the
stemming issue (stem plus wildcard expanding into
words indicating a different (or no) emotional state).
3.2 Analysis of Errors
Before discussing the types of errors we found when
using the word count approach, we examine LIWC?s
overall accuracy on our dataset. Table 4 shows the
precision broken down for both the positive emotion
(POSEMO) and the negative emotion (NEGEMO)
lexica. We see that the precision for NEGEMO is
slightly higher than POSEMO, indicating the terms
in that category may be more likely to indicate their
associated state.
Although the overall accuracy seems decent, one
should keep in mind our subjective judgement crite-
ria were quite tolerant, allowing any amount of con-
tribution of the corresponding signal to be consid-
ered accurate. For example, a salutation like ?Happy
New Year? was judged to be a correct use of ?happy?
to signal POSEMO, even though it clearly does not
have as strong a signal as someone saying ?I feel
deliriously happy?.
Frequencies of signal errors are given in Table
5. The most common signal error was wrong word
sense, where the word did not signal emotional
state and some other sense or definition of the word
was intended (e.g. ?u feel like ur living in a mu-
sic video?; corresponding to the sense ?to inhabit?
rather than the intended sense, ?to have life; be
category label frequency
Lexical Ambiguity
Wrong POS 15
Wrong WS 38
Signal Negation
Strict Negation 16
Desiring 6
Other
Stem Issue 5
Other 24
Table 5: Frequency of the signal error types.
alive? (Miller, 1993)). Other common signal errors
include strict negation where the word is canceled
out by a clear negative quantifier (e.g. ?Don?t be
afraid to fail?) and wrong part of speech where the
word is signaling a different part of speech than the
emotion (e.g. ?well, we cant afford to go to NYC?).
There were also various other signal error types that
include stem issues where the stem matched clearly
unintended words, desiring statuses where the status
is commenting on wanting the emotion instead of
experiencing it and other less prevalent issues such
as non-English language post, memes, or clear sar-
casm.
4 Method for Refining Lexica
The idea behind our refinement method is to remove
words that are likely to carry erroneous signal about
the underlying state or emotion of the person writ-
ing the tweet or Facebook post.2 We do so in an
indirect fashion, without actually using training data
of which posts are, in fact indicative of positive or
negative emotion. Instead, we focus on reducing er-
rors that are due to lexical ambiguity. By remov-
ing words that are often used with multiple parts of
speech or multiple senses, we can tilt the balance to-
ward precision at some cost in recall (losing some
signal from the ambiguous words). This makes the
word count approach more suitable for use in the
massive corpora afforded by social media.
4.1 Lexical Ambiguity
We address lexical ambiguity at the levels of both
part of speech (POS) and word sense. As a metric
of inverse-ambiguity, we determine the probability
that a random instance is the most frequent sense
(mfs) of the most frequent part of speech (mfp) of
2Refinement tool is available at wwbp.org.
301
category label description examples
Lexical Ambiguity
Wrong POS Not a valid signal because it is
the wrong POS
so everyone should come to the
play tomorrow...
Wrong WS Not a valid signal because it is
the wrong word sense (includes
metaphorical senses)
Does anyone what type of file i
need to convert youtube videos
to play on PS3???
Signal Negation
Strict Negation Within the scope of a negation,
where there is a clear negative
quantifier
...all work no play :-(
Desiring Within the scope of a desire /
wishing for something
i sure wish i had about 50 hours
a day to play cod
Other
Stem Issue Clearly not intended to be
matched with the given stem
numb* for NEGEMO match-
ing number
Other Any other issue or difficult to
classify
Table 3: Signal error types.
the word, denoted TSP (for top sense probability).
Given a wordw, we consider all parts of speech ofw
(POS(w)) and all senses for the most frequent part
of speech (senses(mfp(w))):
pmfp(w) =
max
[wpos?POS(w)]
fp(wpos)
?
wpos?POS(w)
fp(wpos)
pmfs(w) =
max
[wsense?senses(mfp(w))]
fs(wsense)
?
wsense?senses(mfp(w))
fs(wsense)
TSP (w) = (pmfp(w) ? pmfs(w))
2 (1)
Here, fp and fs represent the frequencies of a cer-
tain part-of-speech and a certain sense of a word,
respectively. This is the squared-probability that an
instance of w is the top sense ? the most-frequent
part-of-speech and the most-frequency sense of that
part-of-speech. The probability is squared because
both the word in the lexicon and the word occurring
in context should be the top sense (two independent
probabilities: given an instance of a word in a cor-
pus, and another instance of the word in the lexicon,
what is the probability that both are the top POS
and sense?). Frequency data is provided for parts-
of-speech from the Google N-Grams 2.0 (Lin et al,
2010) and for word senses from SemCor (Miller et
al., 1993). This aspect of the refinement is inspired
by the most frequent sense heuristic for word sense
disambiguation (McCarthy et al, 2004; Yarowsky,
1993), in which the sense of a word is chosen with-
out regard to the context, but rather is simply based
on the frequencies of senses in corpora. In our case,
we restrict ourselves this way in order for the appli-
cation of the lexicon to remain unchanged.
For some words, we were unable to find sense fre-
quency data. We decided to keep such terms, on
the assumption that a lack in available frequency in-
formation implies that the word is not very ambigu-
ous. Many of these terms include Web speak such as
?haha? or ?lol?, which we believe can carry a strong
signal for positive and negative emotion.
Lastly, since TSP is only a metric for the in-
verse ambiguity of a word, we must apply a thresh-
old to determine which words to keep. We denote
this threshold as ?, and the description of the refined
lexicon for a category, cat, is below.
lex?(cat) = {w|w ? cat ? TSP (w) > ?}
4.2 Handling Stems
Some lexica, such as the LIWC dictionary, include
word stems that are intended to match multiple
forms of a word. Stems are marked by the suffix
?*?. LIWC describes the application of stems as fol-
lows ?the asterisk, then, denotes the acceptance of
all letters, hyphens, or numbers following its ap-
302
lex cat prec size
full
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
lex0.10
POSEMO 70.9% 392
NEGEMO 71.6% 423
both 71.3% 815
lex0.50
POSEMO 75.7% 239
NEGEMO 78.9% 232
both 77.3% 471
lex0.90
POSEMO 72.5% 109
NEGEMO 78.1% 128
both 75.5% 237
Table 6: Precision (prec) and instance subset size (size)
of refinements to the LIWC POSEMO and NEGEMO lex-
ica with various ? thresholds (0.10, 0.50, 0.90)
pearance.?3 This presents a problem because, while
the creators of such lexica obviously intended stems
to match multiple forms of a word, stems also often
match completely different words, such as ?numb*?
matching ?number? or ?won*? matching ?won?t?.
We identified how often unintended matches hap-
pen in Section 3. Finding that the stemming issues
were not the biggest problem, here, we just describe
how they fit into our lexical ambiguity metric, rather
than describe a technique to rid the lexicon of stem-
ming problems. One approach might be to deter-
mine how ambiguous a stem is ? i.e. determine
how many words, parts-of-speech, and senses a stem
could be expanded into, but this ignores the fact that
the dictionary creators obviously intended the stem
to match multiple words. Instead, we expand stems
into all words that they match and replace them into
the lexica.
We base our expansion on the actual terms used
in social media. We find all words matching stems
among 1 million randomly selected Twitter mes-
sages posted over a 6-month period (August 2009
- February 2010), and restrict to those occurring at
least 20 times. Then, each word stem in the lexicon
is replaced with the expanded set of matching words.
Figure 2: The relationship between precision and size
when increasing the TSP threshold (?).
5 Evaluation
We evaluate our refinement by comparing against
human judgements of the emotion conveyed by
words in individual posts. In the case of hu-
man judgements, we find that the subset of human-
annotated instances matching the refined lexica are
more accurate than the complete set.
In section 3 we discussed the method we used to
judge instances of LIWC POSEMO and NEGEMO
words as to whether they contributed the associated
affect. Each of the 1,000 instances in our evaluation
corpus were judged three times such that the major-
ity was taken as truth. In order to validate our refined
lexica, we find the accuracy (precision) of the subset
of instances which contain the refined lexica terms.
Table 6 shows the change in precision when us-
ing the refined lexica. size represents the number of
instances from the full evaluation corpus matching
words in the refined lexica. One can see that ini-
tially precision increase as the size becomes smaller.
This is more clearly seen in Figure 2. As discussed
in the method section, our goal with the refine-
ment is improving precision, making lexica more
suitable to applications over massive social media
where one can more readily afford to skip instances
(i.e. smaller size) in order to achieve more accu-
racy. Still, removing more ambiguous words does
3?How it works?: http://www.liwc.net/howliwcworks.php
303
not guarantee improved precision at capturing the
intended psychological state; it is possible that that
all senses of an ambiguous word do in fact carry in-
tended signal or that the intended sense a low ambi-
guity word is not the most frequent.
Our maximum precision occurs with a threshold
of 0.50, where things somewhat level-out. This rep-
resents approximately a 23% reduction in error, and
verifies that we can increase precision through the
automatic lexicon refinement based on lexical ambi-
guity.
6 Conclusions
Social scientists and other researchers are starting
to measure psychological states such as happiness
through text in Facebook and Twitter. We have
shown that the widely used word count method,
where one simply counts occurrences of positive or
negative words, can often produce noisy and inaccu-
rate estimates of expressions of psychological states.
We characterized and measured the frequency of
different types of errors that occur using this ap-
proach, and found that when counting words without
considering context, it is lexical ambiguities (unin-
tended parts-of-speech or word senses) which cause
the most errors. We proposed a method for refin-
ing lexica by removing those words most likely to
be ambiguous, and showed that we can significantly
reduce error as measured by human judgements.
Acknowledgments
Support for this research was provided by the
Robert Wood Johnson Foundation?s Pioneer Portfo-
lio, through a grant to Martin Seligman, ?Exploring
Concepts of Positive Health?. We thank the review-
ers for their constructive and insightful comments.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, may. European Language Resources
Association (ELRA).
Erin O.C. Bantum and J.E. Owen. 2009. Evaluating the
validity of computerized content analysis programs for
identification of emotional expression in cancer narra-
tives. Psychological assessment, 21(1):79.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2010.
Twitter mood predicts the stock market. Computer and
Information Science, 1010:1?8.
Michael A. Cohn, M.R. Mehl, and J.W. Pennebaker.
2004. Linguistic markers of psychological change sur-
rounding september 11, 2001. Psychological Science,
15(10):687.
Peter Sheridan Dodds, Kameron Decker Harris, Isabel M
Kloumann, Catherine A Bliss, and Christopher M
Danforth. 2011. Temporal patterns of happiness and
information in a global social network: Hedonomet-
rics and twitter. Diversity, page 26.
Jeremy Ginsberg, M.H. Mohebbi, R.S. Patel, L. Bram-
mer, M.S. Smolinski, L. Brilliant, et al 2009. De-
tecting influenza epidemics using search engine query
data. Nature, 457(7232):1012?4.
Scott A. Golder and M.W. Macy. 2011. Diurnal and
seasonal mood vary with work, sleep, and daylength
across diverse cultures. Science, 333(6051):1878?
1881.
Louis A. Gottschalk and RJ Bechtel. 1998. Psychiatric
content analysis and diagnosis (pcad2000).
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Annual Meeting of the Association for Com-
putational Linguistics, pages 174?181.
Daniel Kahneman, A.B. Krueger, D. Schkade,
N. Schwarz, and A.A. Stone. 2006. Would you
be happier if you were richer? a focusing illusion.
Science, 312(5782):1908.
Jaap Kamps and Maarten Marx. 2002. Words with atti-
tude. In 1st International WordNet Conference, pages
332?341, Mysore, India.
Sepandar D. Kamvar and J. Harris. 2011. We feel fine
and searching the emotional web. In Proceedings
of the fourth ACM international conference on Web
search and data mining, pages 117?126. ACM.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 355?363, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Funda Kivran-Swaine and M. Naaman. 2011. Network
properties and social sharing of emotions in social
awareness streams. In Proceedings of the ACM 2011
304
conference on Computer supported cooperative work,
pages 379?382. ACM.
Michal. Kosinski and David J. Stillwell. 2012.
mypersonality research wiki. mypersonality project.
http://www.mypersonality.org/wiki/.
Adam D.I. Kramer. 2010. An unobtrusive behavioral
model of gross national happiness. In Proceedings of
the 28th international conference on Human factors in
computing systems, pages 287?290. ACM.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kaufman.
George A. Miller. 1993. Five papers on wordnet. Tech-
nical Report, Princeton University.
Greg Miller. 2011. Social scientists wade into the tweet
stream. Science, 333(6051):1814?1815.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Word Journal Of The International Lin-
guistic Association.
James W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development and
psychometric properties of liwc2007. Austin, TX,
LIWC. Net.
Yla R. Tausczik and J.W. Pennebaker. 2010. The psy-
chological meaning of words: Liwc and computerized
text analysis methods. Journal of Language and So-
cial Psychology, 29(1):24.
Janyce Wiebe and Claire Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. lan-
guage resources and evaluation. In Language Re-
sources and Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433, September.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the workshop on Human Language
Technology, HLT ?93, pages 266?271, Stroudsburg,
PA, USA. Association for Computational Linguistics.
305
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 118?125,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Towards Assessing Changes in Degree of Depression through Facebook
H. Andrew Schwartz
?
Johannes Eichstaedt
?
Margaret L. Kern
?
Gregory Park
?
Maarten Sap
?
David Stillwell
?
Michal Kosinski
?
and Lyle Ungar
?
?
Psychology and Computer & Information Science, University of Pennsylvania
?
Psychometrics Centre, University of Cambridge
hansens@seas.upenn.edu
Abstract
Depression is typically diagnosed as be-
ing present or absent. However, depres-
sion severity is believed to be continu-
ously distributed rather than dichotomous.
Severity may vary for a given patient daily
and seasonally as a function of many vari-
ables ranging from life events to environ-
mental factors. Repeated population-scale
assessment of depression through ques-
tionnaires is expensive. In this paper we
use survey responses and status updates
from 28,749 Facebook users to develop a
regression model that predicts users? de-
gree of depression based on their Face-
book status updates. Our user-level pre-
dictive accuracy is modest, significantly
outperforming a baseline of average user
sentiment. We use our model to estimate
user changes in depression across seasons,
and find, consistent with literature, users?
degree of depression most often increases
from summer to winter. We then show the
potential to study factors driving individ-
uals? level of depression by looking at its
most highly correlated language features.
1 Introduction
Depression, a common mental disorder, greatly
contributes to the economic, social, and phys-
ical burden of people worldwide. Along with
other mental disorders it has been related to
early termination of education, unstable mar-
riages, teenage pregnancy, financial problems, role
impairment, heart disease, and other negative out-
comes (Kessler and Bromet, 2013; Lichtman et al.,
2014)
Currently, depression is primarily assessed
through surveys. Diagnoses require a medical or
psychological evaluation, and are typically classi-
fied into discrete categories (absent, mild, moder-
ate, severe). Clinicians rely on retrospective re-
ports by patients to monitor symptoms and treat-
ment. Unobtrusive assessments based on language
use in Facebook and social media usage could
amend both the self-help resources available to pa-
tients as well as repertoire of clinicians with richer
information. Such a tool could allow for more fre-
quent and fine grained (i.e., continuously scored)
assessment and could provide contextualized in-
formation (e.g. specific words and online activi-
ties that are contributing to the user?s depression
score).
Here, we predict and characterize one?s degree
of depression (DDep) based on their language use
in Facebook. Datasets connecting surveyed de-
pression with language in Facebook are rare at
best. To operationalize DDep, we use the depres-
sion facet scores of the ?Big 5? item pool (Gold-
berg, 1999) from the MyPersonality dataset. This
provides a continuous value outcome, for which
we fit a regression model based on ngrams, LDA
topics, and lexica usage. By predicting continuous
values, rather than classes, one can track changes
in DDep of varying size across time; we find sig-
nificantly more users? DDep increases from sum-
mer to winter than vice-versa.
Our primary contribution is the exploration
of predicting continuous-valued depression scores
from individuals? social media messages. To the
best of our knowledge this has not previously been
studied, with other social media and depression
work focused on discrete classes: present or ab-
sent. We compare our predictive model of DDep
to one derived from a state-of-the-art sentiment
lexicon and look at changes across seasons. Fi-
nally, we characterize DDep by looking at its top
ngram and topic correlates.
118
2 Background
2.1 Depression
Depression is generally characterized by persistent
low mood, poor concentration, fatigue, and little
interest in normally enjoyable activities. Depres-
sion can range from mild to severe, and can occur
as an acute episode (major depressive episode),
extend chronically over time (major depressive
disorder, persistent depressive disorder), reoccur
after a period of remission (recurrent depression),
or occur at specific periods (seasonal affective dis-
order, postpartum depression, premenstrual dys-
phoric disorder). Prevalence rates vary; the World
Health Organization estimates that over 350 mil-
lion people worldwide have a depressive disorder,
with many more reporting at least some symptoms
(Organization, 2012). In the U.S., in the World
Health Mental Survey, over half of the respondents
(62%) endorsed at least one diagnostic stem ques-
tions for depression, with 19.2% meeting criteria
for at least one major depressive episode (Kessler
et al., 2010).
Although depression has long been defined as
a single disease with a set of diagnostic criteria,
it often occurs comorbidly with other psycholog-
ical and physical disorders. Anxiety, anger, and
other psychological disorders often co-occur with
depression, and some have suggested that anx-
iety and depression are different manifestations
of the same underlying pathology (Mineka et al.,
1998). An expert panel convened by the Ameri-
can Heart Association recently recommended that
depression be considered a formal risk factor for
heart disease (Lichtman et al., 2014). Depres-
sion has been related to a range of physical con-
ditions, including asthma, cancer, cardiovascular
disease, diabetes, and chronic pain (Kessler and
Bromet, 2013), although the causal direction is
confounded; it may be that other factors cause
both depression and physical illness (Friedman
and Kern, 2014).
As noted previously, assessing degree of de-
pression as a continuous value allows us to look
at changes in depression across time. There has
been longstanding interest and discussion of sea-
sonal patterns of depression, with observations of
seasonal depressive patterns apparent in ancient
times, and the first systematic description occur-
ring in 1984 (Westrin and Lam, 2007). Com-
monly called Seasonal Affective Disorder (SAD),
the DSM-V now refers to this pattern as recur-
rent major depressive disorder with a seasonal pat-
tern. A clinical diagnosis of seasonal depression
requires that two major depressive episodes have
occurred in the past two years, with the onset and
remission showing a regular temporal pattern (pre-
dominantly with onset occurring in the fall/winter
and full remission in spring/summer).
Patients with depression often have common
symptoms of low energy, reduced or intensified
psychomotor movements, low concentration, in-
decisiveness, and thoughts of death, as well as
related symptoms such as fatigue, insomnia, and
weight gain. A challenge in diagnosis is that it re-
lies on a patient?s historical report, and other pos-
sible causes such as physical illness must be ruled
out. Further, with stigmas against mental illness
and feats about seeking treatment, many cases go
unrecognized, causing considerable burden on the
individual and society as a whole. Prevalence rates
vary, but rigorous reviews suggest a prevalence of
.4% in the U.S., although estimates have been re-
ported as high as 10% (Blazer et al., 1998; Mag-
nusson and Partonen, 2005).
There are a number of different hypotheses
about the pathophysiology of S A D, including cir-
cadian, neurotransmitter, and genetic causes (Lam
and Levitan, 2000). Reviews suggest that light
therapy is an effective and well-tolerated treat-
ment, with effects equal to or larger than antide-
pressants (Golden et al., 2005; Lam and Levitan,
2000; Thompson, 2001; Westrin and Lam, 2007).
Attempts to explain why light therapy is so ef-
fective have included shifting photoperiods (light-
dark cycles, with less light in the winter), changes
in melotonin secretion, and circadian phase shifts
(Lam and Levitan, 2000).
One related explanation for the photoperiod ef-
fect is latitude, with the prevalence of seasonal
depression increasing with growing distance from
the equator. Although there has been some support
for this hypothesis in the U.S. (Rosen et al., 1990),
findings in other countries have been mixed (Mer-
sch et al., 1999). Although latitude may play some
role, other factors such as climate, genetic vulner-
ability, and the sociocultural context may have a
stronger impact.
Altogether, inconsistent results suggest that
there is considerable variation in the magnitude,
causes and manifestations of seasonal depression,
much of which is not fully understood, in part due
to diagnostic issues (Lam and Levitan, 2000). A
119
Dislike myself.
Am often down in the dumps.
Have frequent mood swings.
Feel desperate.
Feel comfortable with myself. (-)
Seldom feel blue. (-)
Am very pleased with myself. (-)
Table 1: The seven items of the depression facet
from the 100-item International Personality Item
Pool (IPIP) proxy to the NEO-PI-R (Goldberg,
1999). (-) indicates a reverse coded item.
weekly or even daily depression assessment tool
would allow us to more fully understand the sea-
sonal and other temporal changes in depression.
We use the ?depression facet? scores de-
rived from a subset of the ?big-5? personality
items. Specifically, depression is one of sev-
eral facets (e.g. anger, depression, anxiety, self-
consciousness, impulsiveness, vulnerability) of
the neuroticism personality factor. Neuroticism
refers to individual differences in the tendency to
experience negative, distressing emotions, and be-
havioral and cognitive styles that result from this
(McCrae and John, 1992). It includes traits such
as tension, depression, frustration, guilt, and self-
consciousness, and is associated with low self-
esteem, irrational thoughts and behaviors, ineffec-
tive coping styles, and somatic complaints.
Various scales have been developed to mea-
sure neuroticism, such as the Eysenck Personality
Questionnaire (Eysenck and Eysenck, 1975) and
the NEO-PI-R (Costa and McCrae, 1992). Some
items on these scales overlap with self-reported
items that screen for depression (e.g., personality
item: ?I am often down in the dumps?; depression
screening item: ?how often have you been feel-
ing down, depressed, or hopeless??; see Table 1.),
such that the personality items effectively provide
a proxy measure of depressive tendencies.
2.2 Related Work
Depression has been linked with many online be-
haviors. In fact, even Internet usage itself seems to
vary as a function of being depressed(Katikalapudi
et al., 2012). Other behaviors include social net-
working (Moreno et al., 2011) and differences in
location sharing on Facebook (Park et al., 2013).
Most related to our work, are those using lin-
guistic features to assess various measures of de-
pression. For example, De Choudhury et al.
(2013) used online posting behavior, network
characteristics, and linguistic features when try-
ing to predict depression rather than find its corre-
lates. They used crowdsourcing to screen Twitter
users with the CES-D test (Beekman et al., 1997),
while others analyzed one year of Facebook sta-
tus updates for DSM diagnostic critera of a Major
Depressive Episode (Moreno et al., 2011). In ad-
dition, Park et al. (2013) predicted results of the
Beck Depression Inventory (Beck et al., 1961).
While previous works have made major head-
way toward automatic depression assessment tools
from social media, to the best of our knowledge,
none have tried to predict depression as a con-
tinuum rather than a discrete, present or absent,
attribute. For instance, Neuman et al. (2012)
classified blog posts based on whether they con-
tained signs of depression, and De Choudhury et
al. (2013) classified which newfound mothers
would suffer from postpartum depression.
3 Predicting Degree of Depression
3.1 Method
Dataset. We used a dataset of 28,749 nonclini-
cal users who opted into a Facebook application
(?MyPersonality?; Kosinski and Stillwell, 2012)
between June 2009 and March 2011, completed
a 100-item personality questionnaire (an Interna-
tional Personality Item Pool (IPIP) proxy to the
NEO-PI-R (Goldberg, 1999), and shared access
to their status updates containing at least 500
words. Users wrote on average of 4,236 words
(69,917,624 total word instances), and a subset of
16,507 users provided gender and age, in which
57.0% were female and the mean age was 24.8.
The dataset was divided into training and test-
ing samples. In particular, the testing sample con-
sisted of a random set of 1000 users who wrote
at least 1000 words and completed the personal-
ity measure, while the training set contained the
27,749 remaining users.
Degree of depression. We estimated user-level
degree of depression (DDep) as the average re-
sponse to seven depression facet items, which are
nested within the larger Neuroticism item pool.
For each item, users indicated how accurately
short phrases described themselves (e.g., ?often
feel blue?, ?dislike myself?; responses ranged
from 1 = very inaccurate to 5 = very accu-
120
(a)
0
50
100
150
?2 ?1 0 1 2Degree of Depression (DDep) as assessed by survey
Numb
er of u
sers
(b)
0
50
100
150
?2 ?1 0 1 2Degree of Depression (DDep) as predicted by language
Numb
er of u
sers
Figure 1: Histograms of (a) survey-assessed
and (b) predicted user-level degree of depression
DDep.
rate). Figure 1a shows the distribution of survey-
assessed DDep (standardized). The items can be
seen in Table 1.
Figure 2 shows the daily averages of survey-
assessed DDep, collapsed across years. A LOESS
smoother over the daily averages illustrates a sea-
sonal trend, with depression rising over the winter
months and dropping during the summer.
Regression modeling. In order to get a contin-
uous value output from our model, we explored
regression techniques over our training data.
Since this first work exploring regression was
concerned primarily with language content, our
features for predicting depression were based
entirely on language use (other social media
activity and friend networks may be considered in
future work). These features can be broken into
four categories:
ngrams: Ngrams of order to 1 to 3, found via Hap-
pierFunTokenizer, and restricted to those used by
at least 5% of users (resulting in 10,450 ngrams).
The features were encoded as relative frequency of
mentioning each ngram (ng):
rel freq(user, ng) =
freq(user, ng)
?
ng
?
?ngs
freq(user, ng
?
)
topics: 2000 LDA derived Facebook topics.
1
Us-
age was calculated as the probability of the topic
given the user:
usage(top|user) =
?
ng?topic
p(top|ng) ? rel freq(user, ng)
1
downloaded from wwbp.org/data.html
?0.25
0.00
0.25
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov DecDate of Survey Administration
Surve
y?ass
essed
Degre
eofD
epress
ion
Figure 2: Seasonal trends in degree of depres-
sion as assessed by surveys. Red line is a LOESS
smoothed trend (+/- 1 SE) over the average of
scores from users who completed the survey on
that day.
lexica: 64 LIWC categories (Pennebaker et al.,
2007) as well as the sentiment lexicon from NRC
Canada (Mohammad et al., 2013).
2
Usage of a
lexicon (lex) was calculated similar to the LDA
topics, where w is the weight of the word in the
lexicon in the case of sentiment and always 1 in
the case of LIWC which has no weights:
usage(lex, user) =
?
ng?lex
w(ng, lex) ? rel freq(user, ng)
number of words: Encoded simply as the integer
value for that user.
We used penalized linear regression to fit our
features to DDep. We experimented with a few pe-
nalization types over the training set and settled on
L2 (?ridge regression?), using Principal Compo-
nents Analysis to first reduce the ngram and topic
features to 10 % of their original size. In order to
ensure users tested provided an adequate amount
of features, we only tested over those with at least
1,000 words. However, we found that including
more users in our training set at the expense of
words per user increased model accuracy. Thus,
we only required our training data users to men-
tion 500 words, essentially allowing more noise in
order to increase the number of training examples.
We also experimented with training models on
two sets of messages: all messages and the sub-
set of messages written in the same three-month
season as the survey administration (season only
2
downloaded from www.saifmohammad.com
121
Model Season test (r) All test (r)
Baseline
sentiment
.124 .149
Season .321 .340
All .351 .386
Table 2: Accuracy of various models against test
sets containing only messages from the season
and year in which the user took the survey as
well as a test using all of user?s messages. Mod-
els: Baseline
sentiment
a model based on a state-
of-the-art sentiment lexicon (Mohammad et al.,
2013); Season: model trained on messages sent
only during the same season and year in which
each user took the survey; All model trained on
all messages of each user.
messages). Because the degree of depression may
vary over time, we reasoned that messages written
closer to survey administration might better reflect
the degree of depression assessed by the survey.
When generating predictions on users in the test
set, we applied both the all messages model and
the season only messages model to features from
all messages and then to just the features from the
same season as the survey administration.
3.2 Evaluation and Results
We evaluated accuracy using the Pearson corre-
lation coefficient r between our predictions and
survey-assessed DDep. As a baseline, we built a
regression model simply using the NRC sentiment
(Mohammad et al., 2013) feature.
Accuracies are shown in Table 2. Accuracy was
highest (r = .386) when we trained a model over
all messages from users in the training set and then
applied this model to all messages by users in the
test set. Though our model allows for seasonal
change in depression, we suspect the test across all
messages was more accurate than that of only us-
ing the season in which the users depression was
assessed due to the larger amount messages and
language features provided to the model.
Both models (season-only messages, and all
messages) gave significant (p < 0.05) improve-
ment over the baseline (r = .149) and though
these accuracies may look small, it?s worth not-
ing that a correlation above r = 0.3 is often re-
garded as a strong link between a behavior and a
psychological outcome (Meyer et al., 2001). Still,
we fit many behavior variables (i.e., language use
features) to an outcome and so we might hope
0
40
80
120
160
?0.6 ?0.3 0.0 0.3 0.6Seasonal Difference (Winter?Summer) in Predicted Degree of Depression
Number
ofusers
Figure 3: Histogram of differences between winter
and summer predictions of user-level DDep. Av-
erage user-level predicted DDep values were sig-
nificantly higher in the winter months (t = 4.63,
p < .001).
for higher variance explained. We suspect hav-
ing more users to train on and taking more fea-
tures into account could improve results. For ex-
ample, people who nearly stopped writing for a
season would be thrown out of our analyses since
it is completely based on language content, even
though they are more likely to be depressed (so-
cial isolation is a common symptom in depres-
sion). Similarly, we do not use demographics in
our models, even though women are more likely
to become depressed than men.
To assess individual seasonal changes in de-
gree of depression, we predicted summer and win-
ter DDep values for each user with at least 1000
words across both summer-only and winter-only
messages, respectively. We then compared the
differences across the seasonal predictions; Fig-
ure 3 shows the distribution of user-level seasonal
differences across 676 users with sufficient lan-
guage for both seasonal predictions. In line with
the trends seen in survey data, average user-level
DDep values, as predicted by language, were sig-
nificantly higher in the winter months (t = 4.63,
p < .001).
4 Differential Language Analysis
Figure 4 shows the 100 ngrams most highly cor-
related with depression score across the 21,913
Facebook users in our dataset writing at least
1,000 words. Unlike typical word clouds, the
clouds represent language that differentiates users
scoring high on depression. The size of a word
represents its correlation with depression (larger
122
= stronger), the color its relative frequency (grey
= rarely used, blue = moderately used, red = fre-
quently used).
The f-word emerges as both the most correlated
feature (as indicated by the size of the word) and
is highly frequent (indicated by the red color). To-
gether with words such as ?pissed? and ?bloody?,
these curse words suggest hostility or aggression.
Similarly, words such has ?hate? and ?lonely? sug-
gest negative social relationships.
Perhaps surprisingly, the words ?depression?
and ?depressed? emerge as highly correlated fea-
tures. These face valid features occur infrequently
(as indicated by their grey color), yet are strongly
associated with depressive tendencies, demon-
strating the high statistical power of our approach
applied to this large dataset in identifying signif-
icant but rarely used language features. The both
frequent and highly correlated word ?why? hints at
signs of hopelessness and meaninglessness, a core
feature of depressive disorders.
As illustrated in Figure 5, extending the words
and phrase results, automatically derived topics
demonstrate substantial overlap with the major
clinical symptoms of major depressive disorder
(American Psychiatric Association et al., 2013).
Hopelessness and meaninglessness are seemingly
expressed by ?hopeless? and ?helpless?. Perhaps
the most noticable symptom of depression, de-
pressed mood, is expressed in topics mentioning
?feel?, ?crap?, ?sad?, and ?miserable?.
Depression often affects psychomotor function,
either in terms of fatigue and low energy or in-
versely as insomnia and hyperactivity. Such symp-
toms are reflected in words such as ?tired?, and
?sleep?. Depression is often expressed somati-
cally through bodily symptoms, captured through
?hurt?, ?my head? and ?pain?.
One of the most predictive questions on de-
pressive screening questionnaires asks about sui-
cidal thought, which appears with topics related to
thoughts of death, with words such as ?kill?, ?die?,
and ?dying?.
Topics also reflected hostility, aggression, and
negative relationships with other people. Loneli-
ness has emerged as one of the strongest predic-
tors of physical morbidity and mortality (Hawk-
ley and Cacioppo, 2010), and both ?lonely? and
?alone? appear as some of the most correlated sin-
gle words. Given such striking descriptive results,
future work might try to detect depression associ-
Figure 5: Top ten topics most positively correlated
with depression (from r = .14 at top to r = .11
at bottom). All are significant at a Bonferroni-
corrected threshold of p < 0.001. Word size cor-
responds to prevalence within the topics.
ated conditions as well such as insomnia, loneli-
ness, and aggression.
5 Conclusion
Depression can be viewed as a continuous con-
struct that changes over time, rather than simply as
being a disease that one has or does not have. We
showed that regression models based on Facebook
language can be used to predict an individual?s de-
gree of depression, as measured by a depression
facet survey. In line with survey seasonal trends
and the broader literature, we found that language-
based predictions of depression were higher in
the winter than the summer, suggesting that our
123
Figure 4: The 100 ngrams most correlated with DDep (ranging from r = .05 to r = .10). All are
significant at a Bonferroni-corrected threshold of p < 0.001. Ngram size corresponds to correlation
strength (larger words are more distinguishing). Color corresponds to relative frequency (red if frequent,
blue moderate, grey infrequent).
continuous predictions are capturing small, yet
meaningful within-person changes. With further
development of regression models, many users
write enough on Facebook that we could estimate
changes in their level of depression on a monthly
or even weekly basis. Such estimates, correlated
with word use over time offers potential both for
research at the group-level (?What are the social
and environmental determinants of depression??,
?How well are talk or medication-based interven-
tions working??) as well as, eventually, for med-
ical and therapeutic application at the individual
level (?How well am I doing and what depression-
relevant thoughts or behaviors have I disclosed in
the past week??).
References
APA American Psychiatric Association, Ameri-
can Psychiatric Association, et al. 2013. Diagnostic
and statistical manual of mental disorders.
Aaron T Beck, Calvin H Ward, Mock Mendelson,
Jeremiah Mock, and JK Erbaugh. 1961. An inven-
tory for measuring depression. Archives of general
psychiatry, 4(6):561.
Aartjan TF Beekman, DJH Deeg, J Van Limbeek,
AW Braam, MZ De Vries, W Van Tilburg, et al.
1997. Criterion validity of the Center for Epi-
demiologic Studies Depression scale (CES-D): re-
sults from a community-based sample of older sub-
jects in The Netherlands. Psychological medicine,
27(1):231?236.
Dan G Blazer, Ronald C Kessler, and Marvin S Swartz.
1998. Epidemiology of recurrent major and minor
depression with a seasonal pattern. The National Co-
morbidity Survey. The British Journal of Psychia-
try, 172(2):164?167.
Paul T Costa and Robert R McCrae. 1992. Profes-
sional manual: revised NEO personality inventory
(NEO-PI-R) and NEO five-factor inventory (NEO-
FFI). Odessa, FL: Psychological Assessment Re-
sources.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013a. Predicting postpartum changes in
emotion and behavior via social media. In Pro-
ceedings of the 2013 ACM annual conference on
Human factors in computing systems, pages 3267?
3276. ACM.
Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013b. Predicting de-
pression via social media. In AAAI Conference on
Weblogs and Social Media.
Hans Jurgen Eysenck and Sybil Bianca Giuletta
Eysenck. 1975. Manual of the Eysenck Personal-
ity Questionnaire (junior and adult). Hodder and
Stoughton.
Howard S Friedman and Margaret L Kern. 2014.
Personality, Well-Being, and Health*. Psychology,
65(1):719.
124
Lewis R Goldberg. 1999. A broad-bandwidth, public
domain, personality inventory measuring the lower-
level facets of several five-factor models. Personal-
ity psychology in Europe, 7:7?28.
Robert N Golden, Bradley N Gaynes, R David Ek-
strom, Robert M Hamer, Frederick M Jacobsen, Tr-
isha Suppes, Katherine L Wisner, and Charles B Ne-
meroff. 2005. The efficacy of light therapy in the
treatment of mood disorders: a review and meta-
analysis of the evidence. American Journal of Psy-
chiatry, 162(4):656?662.
Louise C Hawkley and John T Cacioppo. 2010. Lone-
liness matters: a theoretical and empirical review of
consequences and mechanisms. Annals of Behav-
ioral Medicine, 40(2):218?227.
R Katikalapudi, Sriram Chellappan, Frances Mont-
gomery, Donald Wunsch, and Karl Lutzen. 2012.
Associating Internet usage with depressive behav-
ior among college students. Technology and Society
Magazine, IEEE, 31(4):73?80.
Ronald C. Kessler and Evelyn J. Bromet. 2013. The
Epidemiology of Depression Across Cultures. An-
nual Review of Public Health, 34(1):119?138, Mar.
Ronald C Kessler, Howard G Birnbaum, Victoria
Shahly, Evelyn Bromet, Irving Hwang, Katie A
McLaughlin, Nancy Sampson, Laura Helena An-
drade, Giovanni de Girolamo, Koen Demyttenaere,
et al. 2010. Age differences in the prevalence and
co-morbidity of DSM-IV major depressive episodes:
results from the WHO World Mental Health Survey
Initiative. Depression and anxiety, 27(4):351?364.
M. Kosinski and D.J. Stillwell. 2012. myPersonality
Project. http://www.mypersonality.org/wiki/.
Raymond W Lam and Robert D Levitan. 2000. Patho-
physiology of seasonal affective disorder: a review.
Journal of Psychiatry and Neuroscience, 25(5):469.
Judith H Lichtman, Erika S Froelicher, James A Blu-
menthal, Robert M Carney, Lynn V Doering, Nancy
Frasure-Smith, Kenneth E Freedland, Allan S Jaffe,
Erica C Leifheit-Limson, David S Sheps, et al.
2014. Depression as a Risk Factor for Poor Prog-
nosis Among Patients With Acute Coronary Syn-
drome: Systematic Review and Recommendations
A Scientific Statement From the American Heart
Association. Circulation.
Andres Magnusson and Timo Partonen. 2005. Focus
Points. CNS Spectr, 10(8):625?634.
Robert R McCrae and Oliver P John. 1992. An intro-
duction to the five-factor model and its applications.
Journal of personality, 60(2):175?215.
Peter Paul A Mersch, Hermine M Middendorp, An-
toinette L Bouhuys, Domien GM Beersma, and Rut-
ger H van den Hoofdakker. 1999. Seasonal affec-
tive disorder and latitude: a review of the literature.
Journal of affective disorders, 53(1):35?48.
Gregory J Meyer, Stephen E Finn, Lorraine D Eyde,
Gary G Kay, Kevin L Moreland, Robert R Dies,
Elena J Eisman, Tom W Kubiszyn, and Geoffrey M
Reed. 2001. Psychological testing and psycholog-
ical assessment: A review of evidence and issues.
American psychologist, 56(2):128?165.
S Mineka, D Watson, and LA Clark. 1998. Comorbid-
ity of anxiety and unipolar mood disorders. Annual
review of psychology, 49:377.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. In Pro-
ceedings of the seventh international workshop on
Semantic Evaluation Exercises (SemEval-2013), At-
lanta, Georgia, USA, June.
Megan A Moreno, Lauren A Jelenchick, Katie G Egan,
Elizabeth Cox, Henry Young, Kerry E Gannon,
and Tara Becker. 2011. Feeling bad on Face-
book: Depression disclosures by college students on
a social networking site. Depression and anxiety,
28(6):447?455.
Yair Neuman, Yohai Cohen, Dan Assaf, and Gabbi
Kedma. 2012. Proactive screening for depression
through metaphorical and automatic text analysis.
Artificial intelligence in medicine, 56(1):19?25.
World Health Organization.
2012. Depression fact sheet.
http://www.who.int/mediacentre/factsheets/fs369/en/.
Sungkyu Park, Sang Won Lee, Jinah Kwak, Meeyoung
Cha, and Bumseok Jeong. 2013. Activities on Face-
book Reveal the Depressive State of Users. Journal
of medical Internet research, 15(10).
James W. Pennebaker, C.K. Chung, M. Ireland,
A. Gonzales, and R.J. Booth. 2007. The devel-
opment and psychometric properties of LIWC2007.
Austin, TX, LIWC. Net.
Leora N Rosen, Steven D Targum, Michael Terman,
Michael J Bryant, Howard Hoffman, Siegfried F
Kasper, Joelle R Hamovit, John P Docherty, Betty
Welch, and Norman E Rosenthal. 1990. Prevalence
of seasonal affective disorder at four latitudes. Psy-
chiatry research, 31(2):131?144.
C Thompson. 2001. Evidence-based treatment. Sea-
sonal affective disorder: practice and research,
pages 151?158.
Asa Westrin and Raymond W Lam. 2007. Seasonal
affective disorder: a clinical update. Annals of Clin-
ical Psychiatry, 19(4):239?246.
125
