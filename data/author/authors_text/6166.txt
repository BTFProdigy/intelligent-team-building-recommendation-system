Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43?50,
Prague, June 2007. c?2007 Association for Computational Linguistics
Towards the Automatic Extraction of Definitions in Slavic
1Adam Przepio?rkowski
2?ukasz Dego?rski
8Beata Wo?jtowicz
Institute of Computer Science PAS
Ordona 21, Warsaw, Poland
adamp@ipipan.waw.pl
ldegorski@bach.ipipan.waw.pl
beataw@bach.ipipan.waw.pl
4Kiril Simov
5Petya Osenova
Institute for Parallel Processing BAS
Bonchev St. 25A, Sofia, Bulgaria
kivs@bultreebank.org
petya@bultreebank.org
3Miroslav Spousta
7Vladislav Kubon?
Charles University
Malostranske? na?me?st?? 25
Prague, Czech Republic
spousta@ufal.ms.mff.cuni.cz
vk@ufal.ms.mff.cuni.cz
6Lothar Lemnitzer
University of Tu?bingen
Wilhelmstr. 19, Tu?bingen, Germany
lothar@sfs.uni-tuebingen.de
Abstract
This paper presents the results of the prelim-
inary experiments in the automatic extrac-
tion of definitions (for semi-automatic glos-
sary construction) from usually unstructured
or only weakly structured e-learning texts
in Bulgarian, Czech and Polish. The ex-
traction is performed by regular grammars
over XML-encoded morphosyntactically-
annotated documents. The results are less
than satisfying and we claim that the rea-
son for that is the intrinsic difficulty of the
task, as measured by the low interannota-
tor agreement, which calls for more sophis-
ticated deeper linguistic processing, as well
as for the use of machine learning classifica-
tion techniques.
1 Introduction
The aim of this paper is to report on the preliminary
results of a subtask of the European Project Lan-
guage Technology for eLearning (http://www.
lt4el.eu/) consisting in the identification of
term definitions in eLearning materials (Learning
Objects; henceforth: LOs), where definitions are
understood pragmatically, as those text fragments
which may, after perhaps some minor editing, be
put into a glossary. Such automatically extracted
term definitions are to be presented to the author or
the maintainer of the LO and, thus, significantly fa-
cilitate and accelerate the creation of a glossary for
a given LO. From this specification of the task it fol-
lows that good recall is much more important than
good precision, as it is easier to reject wrong glos-
sary candidates than to browse the LO for term def-
initions which were not automatically spotted.
The project involves 9 European languages in-
cluding 3 Slavic (and, regrettably, no Baltic) lan-
guages: one South Slavic, i.e., Bulgarian, and two
West Slavic, i.e., Czech and Polish. For all lan-
guages, shallow grammars identifying definitions
have been constructed; after mentioning some previ-
ous work on Information Extraction (IE) for Slavic
languages and on extraction of definitions in sec-
tion 2, we briefly describe the three Slavic grammars
developed within this project in section 3. Section 4
presents the results of the application of these gram-
mars to LOs in respective languages. These results
are evaluated in section 5, where main problems, as
well as some possible solutions, are discussed. Fi-
nally, section 6 concludes the paper.
43
2 Related Work
Definition extraction is an important NLP task,
most frequently a subtask of terminology extraction
(Pearson, 1996), the automatic creation of glossaries
(Klavans and Muresan, 2000; Klavans and Muresan,
2001), question answering (Miliaraki and Androut-
sopoulos, 2004; Fahmi and Bouma, 2006), learning
lexical semantic relations (Malaise? et al, 2004; Stor-
rer and Wellinghoff, 2006) and automatic construc-
tion of ontologies (Walter and Pinkal, 2006). Tools
for definition extraction are invariably language-
specific and involve shallow or deep processing,
with most work done for English (Pearson, 1996;
Klavans and Muresan, 2000; Klavans and Muresan,
2001) and other Germanic languages (Fahmi and
Bouma, 2006; Storrer and Wellinghoff, 2006; Wal-
ter and Pinkal, 2006), as well as French (Malaise? et
al., 2004). To the best of our knowledge, no previ-
ous attempts at definition extraction have been made
for Slavic, with the exception of some work on Bul-
garian (Tanev, 2004; Simov and Osenova, 2005).
Other work on Slavic information extraction has
been carried out mainly for the last 5 years. Prob-
ably the first forum where such work was compre-
hensively presented was the International Workshop
on Information Extraction for Slavonic and Other
Central and Eastern European Languages (IESL),
RANLP, Borovets, 2003, Bulgaria. One of the pa-
pers presented there, (Droz?dz?yn?ski et al, 2003), dis-
cusses shallow SProUT (Becker et al, 2002) gram-
mars for Czech, Polish and Lithuanian. SProUT has
subsequently been extensively used for the informa-
tion extraction from Polish medical texts (Piskorski
et al, 2004; Marciniak et al, 2005).1
3 Shallow Grammars for Definition
Extraction
The input to the task of definition extraction is
XML-encoded morphosyntactically-annotated text,
possibly with some keywords already marked by an
1SProUT has not been seriously considered for the task at
hand for two reasons: first, it was decided that only open source
tools will be used in the current project, if only available, sec-
ond, the input format to the current task is morphosyntactically-
annotated XML-encoded text, rather than raw text, as normally
expected by SProUT. The second obstacle could be removed by
converting input texts to the SProUT-internal XML representa-
tion.
independent process. For example, the representa-
tion of a Polish sentence starting as Konstruktywizm
k?adzie nacisk na (Eng. ?Constructivism puts em-
phasis on?) may be as follows:2
<s id="s9">
<markedTerm id="mt7" kw="y">
<tok base="konstruktywizm" ctag="subst"
id="t253"
msd="sg:nom:m3">Konstruktywizm</tok>
</markedTerm>
<tok base="klasc" ctag="fin" id="t254"
msd="sg:ter:imperf">kladzie</tok>
<tok base="nacisk" ctag="subst" id="t255"
msd="sg:acc:m3">nacisk</tok>
<tok base="na" ctag="prep" id="t256"
msd="acc">na</tok>
[...]
<tok base="." ctag="interp" id="t273">.
</tok>
</s>
For each language, definitions were manually
marked in two batches of texts: the first batch, con-
sulted during the process of grammar development,
contained at least 300 definitions, and the second
batch, held out for evaluation, contained about 150
definitions. All grammars are regular grammars im-
plemented with the use of the lxtransduce tool
(Tobin, 2005), a component of the LTXML2 toolset
developed at the University of Edinburgh.3 An ex-
ample of a simple rule for prepositional phrases is
given below:
<rule name="PP">
<seq>
<query match="tok[@ctag = ?prep?]"/>
<ref name="NP1">
<with-param name="case" value="??"/>
</ref>
</seq>
</rule>
This rule identifies a sequence whose first element
is a token tagged as a preposition and whose subse-
quent elements are identified by a rule called NP1.
This latter rule (not shown here for brevity) is a pa-
rameterised rule which finds a nominal phrase of a
given case, but the way it is called above ensures that
it will find an NP of any case.
2Part of the representation has been replaced by ?[...]?.
3Among the tools considered here were also CLaRK (Simov
et al, 2001), ultimately rejected because it currently does not
work in batch mode, and GATE / JAPE (Cunningham et al,
2002), not used here because we found GATE?s handling of
previously XML-annotated texts rather cumbersome and ill-
documented. Cf. also fn. 1.
44
Currently the grammars show varying degrees of
sophistication, with a small Bulgarian grammar (8
rules in a 2.5-kilobyte file), a larger Polish grammar
(34 rules in a 11KiB file) and a sophisticated Czech
grammar most developed (147 rules in a 28KiB
file). The patterns defined by these three grammars
are similar, but sufficiently different to defy an at-
tempt to write a single parameterised grammar.4 The
remainder of this section briefly describes the gram-
mars.
3.1 Bulgarian
The Bulgarian grammar is manually constructed af-
ter examination of the manually annotated defini-
tions. Here is a list of the rule schemata, together
with the number and percentage of matching defini-
tions:
Pattern # %
NP is NP 140 34.2
NP verb NP 18 29.8
NP - NP 21 5.0
This is NP 15 3.7
It represents NP 4 1.0
other patterns 107 26.2
Table 1: Bulgarian definition types
In the second schema above, ?verb? is a verb or
a verb phrase (not necessarily a constituent) which
is one of the following: ?????????????? (to repre-
sent), ????????? (to show), ?????????? (to mean),
???????? (to describe), ??? ????????? (to be used),
??????????? (to allow), ????? ?????????? ???
(to give opportunity), ??? ??????? (is called),
??????????? (to improve), ??????????? (to ensure),
?????? ??? (to serve as), ??? ???????? (to be under-
stood as), ???????????? (to denote), ????????? (to
contain), ?????????? (to determine), ?????????
(to include), ??? ???????? ????? (is defined as),
??? ???????? ??? (is based on).
We classify the rules in five types: copula defi-
nitions, copula definitions with anaphoric relation,
copula definitions with ellipsis of the copula, defi-
nitions with a verb phrase, definitions with a verb
4Because of this relative language-dependence of definition
patters, which includes, e.g., idiosyncratic case information,
we have not seriously considered re-using rules for other, non-
Slavic, languages.
phrase and anaphoric relation. Each of these types of
definitions defines an NP (sometimes via anaphoric
relation) by another one. There are some variations
of the models where some parenthetical expressions
are presented in the definition.
The grammar contains several most important
rules for each type. The different verb patterns are
encoded as a lexicon. For some of the rules, variants
with parenthetical phrases are also encoded. The rest
of the grammar is devoted to the recognition of noun
phrases and parenthetical phrases. For parentheti-
cal phrases, we have encoded a list of such possible
phrases, extracted on the basis of a bigger corpus.
The NP grammar in our view is the crucial grammar
for recognition of the definitions. Most work now
has to be invested into developing the more complex
and recursive NPs.
3.2 Czech
The Czech grammar for definition context extraction
is constructed to follow both linguistic intuition and
observation of common patterns in manually anno-
tated data.
We adapted a grammar5 based mainly on the ob-
servation of Czech Wikipedia entries. Encyclopedia
definitions are usually clear and very well structured,
but it is quite difficult to find such well-formed defi-
nitions in common texts, including learning objects.
The rules were extended using part of our manually
annotated texts, evaluated and adjusted in several it-
erations, based on the observation of the annotated
data.
Pattern # %
NP is/are NP 52 21.2
NP verb NP 45 18.4
structural 39 15.9
NP (NP) 30 12.2
NP -/:/= NP 20 8.2
other patterns 59 24.1
Table 2: Czech definition types
There are 21 top level rules, divided into five cate-
gories. Most of the correctly marked definitions fall
into the copula verb (?is/are?) category. The sec-
5The grammar was originally developed by Nguyen Thu
Trang.
45
ond most successful rule is the one using selected
verbs like ?definuje? (defines), ?znamen?? (means),
?vymezuje? (delimits), ?pr?edstavuje? (presents) and
several others. The remaining categories make use
of the typical patterns of characters (dash, colon,
equal sign and brackets) or additional structural in-
formation (e.g., HTML tags).
3.3 Polish
The Polish grammar rules are divided into three lay-
ers. Similarly to the Czech grammar, each layer only
refers to itself or lower layers. This allows for ex-
pressing top level rules in a clear and easily man-
ageable way.
The top level layer consists of rules representing
typical patterns found in Polish documents:
Pattern # %
NP (...) are/is NP-INS 40 15.6
NP -/: NP 39 15.2
NP (are/is) to NP-NOM 27 10.6
NP VP-3PERS 25 9.8
NP - i.e./or WH-question 11 4.3
N ADJ - PPAS 8 3.1
NP, i.e./or NP 7 2.7
NP-ACC one may
describe/define as NP-ACC 5 2.0
other patterns
(not in the grammar) 94 36.7
Table 3: Polish definition types
The middle layer consists of rules catching pat-
terns such as ?simple NP in given case, followed by
a sequence of non-punctuation elements? or ?cop-
ula?.
The bottom layer rules basically only refer to
POS markup in the input files (or other bottom layer
rules).
4 Results
As mentioned above, the testing corpus for each lan-
guage consists of about 150 definitions, unseen dur-
ing the construction of the grammar.6
6Obviously, three different corpora had to be used to eval-
uate the grammars for the three languages, but the corpora are
similar in size and character, so any differences in results stem
mostly from the differences in the three grammars.
The Bulgarian test corpus, containing around
76,800 tokens, consists of the third part of the
Calimera guidelines (http://www.calimera.
org/). We view this document as appropriate for
testing because it reflects the chosen domain and it
combines definitions from otherwise different sub-
domains, such as XML language, Internet usage,
etc. There are 203 manually annotated definitions
in this corpus: 129 definitions contained in one sen-
tence, 69 definitions split across 2 sentences, 4 def-
initions in 3 sentences and one definition in 4 sen-
tences. Note that the real test part is the set of the
129 definitions in one sentence, since the Bulgar-
ian grammar does not consider cross-sentence def-
initions in any way.
Czech data used for evaluation consist of several
chapters of the Calimera guidelines and Microsoft
Excel tutorial. The tutorial is a typical text used
in e-learning, consisting of five chapters describing
sheets, tables, formating, graphs and lists. The cor-
pus consists of over 90,000 tokens and contains 162
definitions, out of which 153 are contained in a sin-
gle sentence, 6 span 2 sentences, and 3 definitions
span 3 sentences.
Polish test corpus consists of over 83,200 tokens
containing 157 definitions: 148 definitions are con-
tained within one sentence, while 9 span 2 sen-
tences. The corpus is made up of 10 chapters of a
popular introduction to and history of computer sci-
ence and computer hardware.
Each grammar was quantitatively evaluated by
comparing manually annotated files with the same
files annotated automatically by the grammar. After
considering various ways of quantitative evaluation,
we decided to do the comparison at token level: pre-
cision was calculated as the ratio of the number of
those tokens which were parts of both a manually
marked definition and an automatically discovered
definition to the number of all tokens in automati-
cally discovered definitions, while recall was taken
to be the ratio of the number of tokens simultane-
ously in both kinds of definitions to the number of
tokens in all manually annotated definitions. Since,
for this task, recall is more important than precision,
we used the F2-measure for the combined result.7
7In general, F? = (1 + ?) ? (precision ? recall)/(? ?
precision+recall). Perhaps? larger than 2 could be used, but it
is currently not clear to us what criteria should be assumed when
46
The results for the three grammars are given in
Table 4. Note that the processing model for Czech
precision recall F2
Bulgarian 20.5% 2.2% 3.1
Czech 18.3% 40.7% 28.9
Polish 14.8% 22.2% 19.0
Table 4: Token-based evaluation of shallow gram-
mars
differs from the other two languages, as the input
text is converted to a flat format, as described in sec-
tion 5.3, and grammar rules are sensitive to sentence
boundaries (and may operate over them).
5 Evaluation and Possible Improvements
5.1 Interannotator Agreement
We calculated Cohen?s kappa statistic (1) for the cur-
rent task, where both the relative observed agree-
ment among raters Pr(a) and the probability that
agreement is due to chance Pr(e) where calculated
at token level.
? =
Pr(a) ? Pr(e)
1 ? Pr(e)
(1)
More specifically, we assumed that two annotators
agree on a token if the token belongs to a definition
either according to both annotations or according to
neither. In order to estimate the probability of agree-
ment due to chance Pr(e), we measured, separately
for each annotator, the proportion of tokens found in
definitions to all tokens in text, which resulted in two
probability estimates p1 and p2, and treated Pr(e) as
the probability that the two annotators agree if they
randomly, with their own probability, classify a to-
ken as belonging to a definition, i.e.:
Pr(e) = p1 ? p2 + (1 ? p1) ? (1 ? p2) (2)
The interannotator agreement (IAA) was mea-
sured this way for Czech and Polish, where ? for
each language ? the respective test corpus was an-
notated by two annotators. The results are 0.44 for
Czech and 0.31 for Polish. Such results are very low
for any classification task, and especially low for a
deciding on the exact value of ?. Note that it would not make
sense to use recall alone, as it is trivial to write all-accepting
grammars with 100% recall.
binary classification task. They show that the task of
identifying definitions in running texts and agreeing
on which parts of text count as a definition is intrin-
sically very difficult. They also call for the recon-
sideration of the evaluation and IAA measurement
methodology based on token classification.8
5.2 Evaluation Methodology
To the best of our knowledge, there is no estab-
lished evaluation methodology for the task of def-
inition extraction, where definitions may span sev-
eral sentences.9 For this reason we evaluated the re-
sults again, in a different way: we treated an auto-
matically discovered definition as correct, if it over-
lapped with a manually annotated definition. We
calculated precision as the number of automatic defi-
nitions overlapping with manual definitions, divided
by the number of automatic definitions, while re-
call ? as the number of manual definitions overlap-
ping automatic definitions, divided by the number of
manual definitions.10
The results for the three grammars, given in Ta-
ble 5, are much higher than those in Table 4 above,
although still less than satisfactory.
precision recall F2
Bulgarian 22.5% 8.9% 11.1
Czech 22.3% 46% 33.9
Polish 23.3% 32% 28.4
Table 5: Definition-based evaluation of shallow
grammars
5.3 Definitions and Sentence Boundaries
Regardless of the inherent difficulties of the task and
difficulties with the evaluation of the results, there
is clear room for improvement; one possible path
8A better approximation would be to measure IAA on the
basis of sentence or (as suggested by an anonymous reviewer)
NP classification; we intend to pursue this idea in future work.
9With the assumption that definitions are no longer than
a sentence, usually the task is treated as a classification task,
where sentences are classified as definitional or not, and ap-
propriate precision and recall measures are applied at sentence
level.
10At this stage definition fragments distributed across a num-
ber of different sentences were treated as different definitions,
which negatively affects the evaluation of the Bulgarian gram-
mar, as the Bulgarian test corpus contains a large number of
multi-sentence definitions.
47
to explore concerns multi-sentence definitions. As
noted above, for all languages considered here, there
were definitions which were spanning 2 or more sen-
tences; this turned out to be a problem especially for
Bulgarian, were 36% of definitions crossed a sen-
tence boundary.11
Such multi-sentence definitions are a problem be-
cause in the DTD adopted in this project definitions
are subelements of sentences rather than the other
way round. In case of a multi-sentence definition,
for each sentence there is a separate element en-
capsulating the part of the definition contained in
this sentence. Although these are linked via spe-
cial attributes and the information that they are part
of the same definition can subsequently be recov-
ered, it is difficult to construct an lxtransduce
grammar which would be able to automatically mark
such multi-sentence definitions: an lxtransduce
grammar expects to find a sequence of elements and
wrap them in a single larger element.
A solution to this technical problem has been im-
plemented in the Czech grammar, where first the in-
put text is flattened (via an XSLT script), so that,
e.g.:
<par id="d1p2">
<s id="d1p2s1">
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
</s>
</par>
becomes:
<par id="Sd1p2"/>
<s id="Sd1p2s1"/>
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
<s id="Ed1p2s1"/>
<par id="Ed1p2"/>
11An example of a Polish manually annotated multi-sentence
definition is: . . . opracowano techniki antyspamowe. Tech-
niki te drastycznie zaniz?aja? wartos?c? strony albo ja? banuja?. . .
(Eng. ?. . . anti-spam techniques were developed. Such tech-
niques drastically lower the value of the page or they ban it. . . ?).
The definition is split into two fragments fully contained in re-
spective sentences: techniki antyspamowe and Techniki te. . . .
No attempt at anaphora resolution is made.
This flattened representation is an input to a gram-
mar which is sensitive to the empty s and par ele-
ments and may discover definitions containing such
elements; in such a case, the postprocessing script,
which restores the hierarchical paragraph and sen-
tence structure, splits such definitions into smaller
elements, fully contained in respective sentences.
5.4 Problems Specific to Slavic
At least in case of the two West Slavic languages
considered here, the task of writing a definition
grammar is intrinsically more difficult than for Ger-
manic or Romance languages, mainly for the follow-
ing two reasons.
First, Czech and Polish have very rich nominal
inflection with a large number of paradigm-internal
syncretisms. These syncretisms are a common cause
of tagger errors, which percolate to further stages of
processing. Moreover, the number of cases makes it
more difficult to encode patterns like ?NP verb NP?,
as different verbs may combine with NPs of different
case. In fact, even two different copulas in Polish
take different cases!
Second, the relatively free word order increases
the number of rules that must be encoded, and makes
the grammar writing task more labour-intensive and
error-prone. The current version of the Polish gram-
mar, with 34 rules, is rather basic, and even the 147
rules of the Czech grammar do not take into consid-
eration all possible patterns of grammar definitions.
As Tables 4 and 5 show, there is a positive corre-
lation between the grammar size and the value of
F2, and the Bulgarian and Polish grammars certainly
have room to grow. Moreover, a path that is well
worth exploring is to drastically increase the num-
ber of rules and, hence, the recall, and then deal with
precision via Machine Learning methods (cf. sec-
tion 5.6).
5.5 Levels of Linguistic Processing
The work reported here has been an excercise in
definition extraction using shallow parsing methods.
However, the poor results suggest that this is one
of the tasks that require a much more sophisticated
and deeper approach to language analysis. In fact,
in turns out that virtually all successful attempts at
definition extraction that we are aware of build on
worked-out deep linguistic approaches (Klavans and
48
Muresan, 2000; Fahmi and Bouma, 2006; Walter
and Pinkal, 2006), some of them combining syn-
tactic and semantic information (Miliaraki and An-
droutsopoulos, 2004; Walter and Pinkal, 2006).
Unfortunately, for most Baltic and Slavic lan-
guages, such deep parsers are unavailable or have
not yet been extensively tested on real texts. One
exception is Czech, where a number of parsers were
already described and evaluated (on the Prague De-
pendency Treebank) in (Zeman, 2004, ? 14.2); the
best of these parsers reach 80?85% accuracy.
For Polish, apart from a number of linguistically
motivated toy parsers, there is a possibly wide cov-
erage deep parser (Wolin?ski, 2004), but it has not yet
been evaluated on naturally occurring texts. The sit-
uation is probably most dire for Bulgarian, although
there have been attempts at the induction of a depen-
dency parser from the BulTreeBank (Marinov and
Nivre, 2005; Chanev et al, 2006).
Nevertheless, if other possible paths of improve-
ment suggested in this section do not bring satisfac-
tory results, we plan to make an attempt at adapting
these parsers to the task at hand.
5.6 Postprocessing: Machine Learning and
Keyword Identification
Various approaches to the machine learning treat-
ment of the task of classifying sentences or snippets
as definitions or non-definitions can be found, e.g.,
in (Miliaraki and Androutsopoulos, 2004; Fahmi
and Bouma, 2006) and references therein. In the
context of the present work, such methods may be
used to postprocess apparent definitions found at
earlier processing stages and decide which of them
are genuine definitions. For example, (Fahmi and
Bouma, 2006) report that a system trained on 2299
sentences, including 1366 definition sentences, may
increase the accuracy of a definition extraction tool
from 59% to around 90%.12
Another possible improvement may consist in,
again, aiming at very high recall and then using
an independent keyword detector to mark keywords
(and key phrases) in text and classifying as genuine
definitions those definitions, whose defined term has
been marked as a keyword.
12The numbers are so high ?probably due to the fact that the
current corpus consists of encyclopedic material only? (Fahmi
and Bouma, 2006, fn. 4).
Whatever postprocessing technique or combina-
tion of techniques proves most efficient, it seems that
the linguistic processing should aim at high recall
rather than high precision, which further justifies the
use of the F2 measure for evaluation.13
6 Conclusion
To the best of our knowledge, this paper is the first
report on the task of definition extraction for a num-
ber of Slavic languages. It shows that the task is
intrinsically very difficult, which partially explains
the relatively low results obtained. It also calls atten-
tion to the fact that there is no established evaluation
methodology where possibly multi-sentence defini-
tions are involved and suggests what such method-
ology could amount to. Finally, the paper suggests
ways of improving the results, which we hope to fol-
low and report in the future.
References
Markus Becker et al 2002. SProUT ? shallow process-
ing with typed feature structures and unification. In
Proceedings of the International Conference on NLP
(ICON 2002), Mumbai, India.
Sharon A. Caraballo. 2001. Automatic Construction of a
Hypernym-Labeled Noun Hierarchy from Text. Ph. D.
dissertation, Brown University.
Atanas Chanev, Kiril Simov, Petya Osenova, and Sve-
toslav Marinov. 2006. Dependency conversion and
parsing of the BulTreeBank. In proceedings of the
LREC workshop Merging and Layering Linguistic In-
formation, Genoa, Italy.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Witold Droz?dz?yn?ski, Petr Homola, Jakub Piskorski, and
Vytautas Zinkevic?ius. 2003. Adapting SProUT to
processing Baltic and Slavonic languages. In Infor-
mation Extraction for Slavonic and Other Central and
Eastern European Languages, pp. 18?25.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications.
13Note that the situation here is different than in the task of
acquiring hyponymic relations from texts, where high-precision
manual rules (Hearst, 1992) must be augmented with statistical
clustering methods to increase recall (Caraballo, 2001).
49
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, Nantes, France.
Judith L. Klavans and Smaranda Muresan. 2000.
DEFINDER: Rule-based methods for the extraction of
medical terminology and their associated definitions
from on-line text. In Proceedings of the Annual Fall
Symposium of the American Medical Informatics As-
sociation.
Judith L. Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of AMIA Sym-
posium 2001.
V?ronique Malais?, Pierre Zweigenbaum, and Bruno
Bachimont. 2004. Detecting semantic relations be-
tween terms in definitions. In S. Ananadiou and
P. Zweigenbaum, editors, COLING 2004 CompuTerm
2004: 3rd International Workshop on Computational
Terminology, pp. 55?62, Geneva, Switzerland. COL-
ING.
Ma?gorzata Marciniak, Agnieszka Mykowiecka, Anna
Kups?c?, and Jakub Piskorski. 2005. Intelligent con-
tent extraction from Polish medical texts. In L. Bolc
et al, editors, Intelligent Media Technology for Com-
municative Intelligence, Second International Work-
shop, IMTCI 2004, Warsaw, Poland, September 13-14,
2004, Revised Selected Papers, volume 3490 of Lec-
ture Notes in Computer Science, pp. 68?78. Springer-
Verlag.
Svetoslav Marinov and Joakim Nivre. 2005. A data-
driven parser for Bulgarian. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, pp. 89?100, Barcelona.
Spyridoula Miliaraki and Ion Androutsopoulos. 2004.
Learning to identify single-snippet answers to defi-
nition questions. In Proceedings of COLING 2004,
pp. 1360?1366, Geneva, Switzerland. COLING.
Jennifer Pearson. 1996. The expression of defini-
tions in specialised texts: a corpus-based analysis.
In M. Gellerstam et al, editors, Proceedings of the
Seventh Euralex International Congress, pp. 817?824,
G?teborg.
Jakub Piskorski et al 2004. Information extraction for
Polish using the SProUT platform. In M. A. K?opotek
et al, editors, Intelligent Information Processing and
Web Mining, pp. 227?236. Springer-Verlag, Berlin.
Kiril Simov and Petya Osenova. 2005. BulQA:
Bulgarian-Bulgarian Question Answering at CLEF
2005. In CLEF, pp. 517?526.
Kiril Simov et al 2001. CLaRK ? an XML-based sys-
tem for corpora development. In P. Rayson et al, edi-
tors, Proceedings of the Corpus Linguistics 2001 Con-
ference, pp. 558?560, Lancaster.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
German text corpora. In Proceedings of LREC 2006.
Hristo Tanev. 2004. Socrates: A question answering
prototype for Bulgarian. In Recent Advances in Nat-
ural Language Processing III, Selected Papers from
RANLP 2003, pages 377?386. John Benjamins.
Richard Tobin, 2005. Lxtransduce, a replace-
ment for fsgmatch. University of Edinburgh.
http://www.cogsci.ed.ac.uk/~richard/
ltxml2/lxtransduce-manual.html.
Stephan Walter and Manfred Pinkal. 2006. Automatic
extraction of definitions from German court decisions.
In Proceedings of the Workshop on Information Ex-
traction Beyond The Document, pp. 20?28, Sydney,
Australia. Association for Computational Linguistics.
Marcin Wolin?ski. 2004. Komputerowa weryfikacja gra-
matyki S?widzin?skiego. Ph. D. dissertation, ICS PAS,
Warsaw.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph. D. dissertation, Charles University,
Prague.
50
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 35?42,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
A web-enabled and speech-enhanced parallel corpus 
of Greek - Bulgarian cultural texts 
 
 
Voula Giouli 
Institute for Language & Speech 
Processing Athens, Greece 
voula@ilsp.gr 
Nikos Glaros 
Institute for Language & Speech 
Processing Athens, Greece 
nglaros@ilsp.gr 
 
Kiril Simov 
Institute for Parallel Processing, 
BAS, Sofia, Bulgaria 
kivs@bultreebank.or 
Petya Osenova 
Institute for Parallel Processing, 
BAS, Sofia, Bulgaria 
petya@bultreebank.org 
 
Abstract 
This paper reports on completed work carried 
out in the framework of an EU-funded project 
aimed at (a) developing a bilingual collection 
of cultural texts in Greek and Bulgarian, (b) 
creating a number of accompanying resources 
that will facilitate study of the primary texts 
across languages, and (c) integrating a system 
which aims to provide web-enabled and 
speech-enhanced access to digitized bilingual 
Cultural Heritage resources. This simple user 
interface, which incorporates advanced search 
mechanisms, also offers innovative accessibil-
ity for visually impaired Greek and Bulgarian 
users. The rationale behind the work (and the 
relative resource) was to promote the com-
parative study of the cultural heritage of the 
two countries. 
1 Introduction 
The document describes a bilingual Greek (EL) 
and Bulgarian (BG) collection of literary and 
folklore texts along with the metadata that were 
deemed necessary for the efficient management 
and retrieval of the textual data. Section 2 out-
lines the project aims that guided selection and 
annotation of the texts, whereas Section 3 pre-
sents the primary data that comprise the bilingual 
textual collection and the methodology adopted 
for collecting them. Section 4 elaborates on the 
metadata scheme that has been implemented to 
describe the primary data and the linguistic anno-
tation tailored to facilitate search and retrieval at 
the document, phrase or word level. This scheme 
is compliant to widely accepted standards so as 
to ensure reusability of the resource at hand. Sec-
tion 5 presents the Language Technologies (LT) 
deployed in the project elaborating on the Greek 
and the Bulgarian text processing tools, and dis-
cusses the LT methods that have been (a) ex-
ploited in the course of the project to facilitate 
the web-interface construction and (b) integrated 
in the search and retrieval mechanisms to im-
prove the system performance. Finally, Section 6 
describes the main components of the web inter-
face and the way various features are exploited to 
facilitate users? access to the data. In the last sec-
tion, we present conclusions and future work. 
2 Project description 
The project aims at highlighting cultural re-
sources that, as of yet, remain non-exploited to 
their greatest extent, and at creating the neces-
sary infrastructure with the support of LT with a 
view to promoting the study of cultural heritage 
of the eligible neighboring areas and  raising 
awareness about their common cultural identity. 
To serve these objectives, the project had a con-
crete target, that is, the creation of a textual col-
lection and of accompanying material that would 
be appropriate for the promotion and study of the 
cultural heritage of the neighboring areas in 
Greece and Bulgaria (Thrace and the neighboring 
Smolyan, Blagoevgrad, Kardjali, Khaskovo ar-
eas), the focus being on literature, folklore and 
language. To this end, the main activities within 
the project life-cycle were to: 
? record and roadmap the literary production 
of the afore mentioned areas spanning from 
the 19th century till the present days along 
with written records on folk culture and folk-
tales from the eligible areas. These should 
form a pool of candidate texts from which 
35
the most appropriate for the project objec-
tives could be selected; 
? record and roadmap existing translations of 
literary works in both languages to serve for 
the creation of the parallel corpus; 
? select textual material representative of the 
two cultures, and thus, suitable for their 
comparative study; 
? digitize the selected (printed) material to a 
format suitable for long-term preservation; 
? collect meta-texts relevant to the selected 
literary and folklore texts, that is, texts about 
the literary works, biographies of the se-
lected authors, criticism, etc.; these comprise 
part of the accompanying material 
? document the data with any information 
deemed necessary for its preservation and 
exploitation, catering for their interrelation 
so as to highlight their common features and 
allow unified access to the whole set alng 
text types / genres and languages; 
? extract bilingual glossaries from the primary 
collection of literary and folklore texts also 
accounted for as accompanying material; the 
project caters for the extraction of EL and 
BG terms and names of Persons and Loca-
tions and their translation equivalents in the 
other language; 
? make the primary resource along with the 
accompanying material (meta-texts and glos-
saries) publicly available over the internet to 
all interested parties, ranging from the re-
search community to laypersons, school stu-
dents and people interested in finding out 
more about the particular areas; 
? facilitate access to the material that wouldn?t 
be hampered by users? computer literacy 
and/or language barriers. To cater for the lat-
ter, the web interface would be as simple as 
possible ? yet functional ? and the data 
should be available in both languages (Greek 
and Bulgarian) plus in English. 
3 The bilingual Greek ? Bulgarian Cul-
tural Corpus 
Along with the aforementioned lines, the col-
lection comprises parallel EL ? BG literary and 
folklore texts. The main specifications for the 
Greek - Bulgarian Cultural Corpus (GBCC) crea-
tion were: 
? to build a bilingual resource that could be 
used as a means to study cultural similarities 
and/or differences between the neighboring 
areas of Greece and Bulgaria the focus being 
on literature, folklore and folktales;  
? to provide a representative sample of (a) lit-
erature written by authors from Thrace -that 
is from the entire area of Thrace- or about 
Thrace, spanning between the 19th century - 
today, (b) folklore texts about Thrace, that 
would normally reflect cultural as well as 
linguistic elements either shared by the two 
people or unique to each culture, and (c) 
folktales and legends from Thrace, the latter 
being the intermediate between literature and 
folklore. 
In order to gather the candidate texts and au-
thors for such a collection we exploited both 
printed and digitized sources, i.e., (on-line and 
printed) anthologies of Bulgarian, Greek or Bal-
kan literature, digital archives, web resources and 
library material. The outcome of this extensive 
research was a wealth of literary works including 
titles by the most prominent authors in Bulgaria 
and Greece. The selection of the authors, who 
would finally participate in GBCC, was based on 
the following criteria: (a) author's impact to 
Greek or Bulgarian literature respectively; and 
(b) author's contribution to his county's folk 
study or other major sectors such as journalism 
and education.  
Additionally, to ensure corpus ?representa-
tiveness? to some extend, we tried to include the 
full range of the literary texts (poetry, fiction, 
short stories) and in proportion to the literary 
production with respect to the parameters of 
place, time and author. To this end, we think we 
have avoided biases and the corpus models all 
language varieties spoken in the areas and at dif-
ferent periods. 
Moreover, the "inner" content characteristics 
of texts were used as the basic criteria for text 
selection. To this end, we chose texts which 
demonstrate the two people's cultural similarities 
and affinity along with each author's most impor-
tant and representative works. Beyond the above, 
the availability of a translation in the other lan-
guage and IPR issues also influenced text selec-
tion. 
The collection of the primary data currently 
comprises of (135) literary works, (70) BG (Bul-
garian) and 65 EL (Greek). Moreover, (30) BG 
folk texts and 30 EL folk texts along with (25) 
BG folktales and 31 EL folktales were added in 
order to build a corpus as balanced as possible 
and representative of each country's culture. In 
terms of tokens, the corpus amounts to 700,000 
36
in total (circa 350,000 tokens per language): the 
literature part is about 550,000 tokens, whereas, 
the folklore and legend sub-corpus is about 
150,000 tokens. 
Moreover, to cater for the project requirement 
that the corpus should be bilingual, available 
translations of the primary EL ? BG literary 
works were also selected to form the parallel lit-
erary corpus. Additionally, an extensive transla-
tion work was also carried out by specialized 
translators where applicable (folklore texts and 
folktales). 
The collection covers EL and BG literary pro-
duction dating from the 19th century till the pre-
sent day, and also texts (both literary or folklore) 
that are written in the dialect(s) used in the eligi-
ble areas. This, in effect, is reflected in the lan-
guage varieties represented in the textual collec-
tion that range from contemporary to non-
contemporary, and from normal to dialectical or 
even mixed language. 
Finally, the collection of primary data was 
also coupled with accompanying material (con-
tent metadata) for each literary work (literary 
criticism) and for each author (biographical in-
formation, list of works, etc.). Along with all the 
above, texts about the common cultural elements 
were also included. 
4 Corpus Annotation 
After text selection, digitization and extended 
manual validation (where appropriate) were per-
formed. Normalization of the primary data was 
kept to a minimum so as to cater, for example, 
for the conversion from the Greek polytonic to 
the monotonic encoding system. Furthermore, to 
ensure efficient content handling and retrieval 
and also to facilitate access to the resource at 
hand via the platform that has been developed, 
metadata descriptions and linguistic annotations 
were added across two pillars: (a) indexing and 
retrieval, and (b) further facilitating the compara-
tive study of textual data. To this end, metadata 
descriptions and linguistic annotations compliant 
with internationally accepted standards were 
added to the raw material. The metadata scheme 
deployed in this project is compliant with inter-
nationally accredited standards with certain 
modifications that cater for the peculiarities of 
the data. 
More specifically, the metadata scheme im-
plemented in this project builds on XCES, the 
XML version of the Corpus Encoding Standard 
(XCES, http://www.cs.vassar.edu/XCES/ and 
CES, http://www.cs.vassar.edu/CES/CES1-
0.html), which has been proposed by EAGLES 
(http://www.ilc.cnr.it/EAGLES96/home.html) 
and is compliant with the specifications of the 
Text Encoding Initiative (http://www.tei-c.org, 
Text Encoding Initiative (TEI Guidelines for 
Electronic Text Encoding and Interchange). 
From the total number of elements proposed by 
these guidelines, the annotation of the parallel 
corpus at hand has been restricted to the recogni-
tion of structural units at the sentence level, 
which is the minimum level required for the 
alignment and term extraction processes. That 
means that the requirements of CES Level 1 con-
formance are met; as regards CES Level 2 the 
requirements (but not the recommendations) are 
also met, and from CES Level 3 requirements, 
annotation for sentence boundaries is met. 
Additionally, metadata elements have been 
deployed which encode information necessary 
for text indexing with respect to text title, author, 
publisher, publication date, etc. (bibliographical 
information) and for the classification of each 
text according to text type/genre and topic, the 
latter being applicable to folklore texts and folk 
tales. Classification of folklore texts is based on 
the widely accepted Aarne-Thompson classifica-
tion system (Aarne, 1961). 
To this end, to assure documentation com-
pleteness, and facilitate the inter-relation among 
primary data and the accompanying material (bi-
ographies, criticism, etc) the documentation 
scheme has been extended accordingly. The 
aforementioned metadata descriptions are kept 
separately from the data in an xml header that is 
to be deployed by the web interface for search 
and retrieval purposes. 
The external structural annotation (including 
text classification) of the corpus also adheres to 
the IMDI metadata scheme (IMDI, Metadata 
Elements for Session Descriptions, Version 
3.0.4, Sept. 2003). Adaptations proposed specifi-
cally concerning Written Language Resources 
have been taken into account. IMDI metadata 
elements for catalogue descriptions (IMDI, 
Metadata Elements for Catalogue Descriptions, 
Version 2.1, June 2001) were also taken into ac-
count to render the corpus compatible with exist-
ing formalisms (ELRA, and LDC). This type of 
metadata descriptions was added manually to the 
texts. 
To further enhance the capabili-
ties/functionalities of the final application, ren-
dering, thus the collection a useful resource to 
prospective users and researchers, further annota-
37
tions at various levels of linguistic analysis were 
integrated across two pillars: (a) efficient index-
ing and retrieval; and (b) further facilitating the 
comparative study of textual data by means of 
bilingual glossaries which were constructed 
semi-automatically, and via the visualization of 
aligned parallel texts.  
Text processing at the monolingual level com-
prises the following procedures: (a) handling and 
tokenization, (b) Part-of-Speech (POS) tagging 
and lemmatization, (c) surface syntactic analysis, 
(d) indexing with terms/keywords and 
phrases/Named Entities (NEs) pertaining to the 
types Location (LOC) and Person (PER). 
Annotations at these levels were added semi-
automatically, by deploying existing generic 
Natural Language Processing (NLP) tools that 
were developed for the languages at hand, 
whereas extensive and intensive validations were 
performed via several ways. Indeed, although the 
tools deployed have reported to achieve high ac-
curacy rates in the domains/genres they were 
intended for, the specific nature of the data led to 
a significant reduction. To this end, half of the 
annotations were checked manually. After the 
identification of the errors in this part of the cor-
pus, we have performed a manual check in the 
second part of the corpus only for these cases 
which were recognized as errors during the vali-
dation of the first part. For some of the cases 
relevant constraints in the systems were written, 
which automatically find places where some 
rules were not met. Tools customization was also 
performed by adding new rules applicable for the 
language varieties to be handled, and also by ex-
tending/modifying the resources used (word and 
name lists, etc.).  
Finally, alignment of parallel texts (primary 
source documents and their translations) has also 
been performed at both sentence and phrase 
level. As expected, poems posited the major dif-
ficulties due the fuzziness in identifying sentence 
boundaries, and alignments at the phrase level 
were favored instead. 
5 Language Technologies 
In what follows the Greek and Bulgarian Text 
Processing Components will be described. 
5.1 The Greek pipe-line 
In the case of the Greek data, text processing 
was applied via an existing pipeline of shallow 
processing tools for the Greek language. These 
include: 
? Handling and tokenization; following com-
mon practice, the Greek tokenizer makes use 
of a set of regular expressions, coupled with 
precompiled lists of abbreviations, and a set 
of simple heuristics (Papageorgiou et al, 
2002) for the recognition of word and sen-
tence boundaries, abbreviations, digits, and 
simple dates.  
? POS-tagging and lemmatization; a tagger 
that is based on Brill's TBL architecture 
(Brill, 1997), modified to address peculiari-
ties of the Greek language (Papageorgiou et 
al., 2000) was used in order to assign mor-
phosyntactic information to tokenized words. 
Furthermore, the tagger uses a PAROLE-
compliant tagset of 584 different part-of-
speech tags. Following POS tagging, lemmas 
are retrieved from a Greek morphological 
lexicon. 
? Surface syntactic analysis; the Greek chun-
ker is based on a grammar of 186 rules 
(Boutsis et al, 2000) developed for the 
automatic recognition of non-recursive 
phrasal categories: adjectives, adverbs, 
prepositional phrases, nouns, verbs (chunks) 
(Papageorgiou et al, 2002). 
? Term extraction; a Greek Term Extractor 
was used for spotting terms and idiomatic 
words (Georgantopoulos, Piperidis, 2000). 
Term Extractor's method proceeds in three 
pipelined stages: (a) morphosyntactic anno-
tation of the domain corpus, (b) corpus pars-
ing based on a pattern grammar endowed 
with regular expressions and feature-
structure unification, and (c) lemmatization. 
Candidate terms are then statistically evalu-
ated with an aim to skim valid domain terms 
and lessen the overgeneration effect caused 
by pattern grammars (hybrid methodology). 
Named Entity Recognition was then per-
formed using MENER (Maximum Entropy 
Named Entity Recognizer), a system compatible 
with the ACE (Automatic Content Extraction) 
scheme, catering for the recognition and classifi-
cation of the following types of NEs: person 
(PER), organization (ORG), location (LOC) and 
geopolitical entity (GPE) (Giouli et al, 2006). 
5.2 Bulgarian Tools 
In the processing of the Bulgarian part of the 
corpus we have been using generic language 
technology tools developed for Bulgarian. Here 
is the list of tools that we have used. They are 
38
implemented within the CLaRK System (Simov 
et al 2001) via:  
Tokenization, Morphosyntactic tagging, 
Lemmatization; Tokenization is implemented as 
a hierarchy of tokenizers within the CLaRK sys-
tem. Morphosyntactic tagging is done on the ba-
sis a morphological lexicon which covers the 
grammatical information of about 100 000 lex-
emes (1 600 000 word forms); a gazetteers of 
about 25000 names and 1500 abbreviations. We 
are using the BulTreeBank tagset, which is a 
more specialized version of Multext-east tagset. 
The disambiguation is done in two steps. Ini-
tially, a rule-based module solves the sure cases 
for which manual rules can be written. Then, for 
the next step, a neural-network-based disam-
biguator is being exploited (Simov and Osenova 
2001). Lemmatization is implemented as rules 
which convert each word form in the lemma. The 
rules are assigned to the word forms in the lexi-
con. This ensures very high level of accuracy. 
Partial Grammars have also been constructed 
for Sentence splitting, Named-entity recognition, 
and Chunking. 
5.3 Alignments 
To facilitate the comparative study of parallel 
documents, source texts were automatically 
aligned with their translations. Alignments at the 
sentence level were performed semi-
automatically by means of the ILSP Aligner, 
which is a language independent tool that uses 
surface linguistic information coupled with in-
formation about possible unit delimiters depend-
ing on the level at which the alignment is sought. 
The resulting translation equivalents were stored 
in files conformant to the internationally accred-
ited TMX standard (Translation Memory eX-
change, http://www.lisa.org/tmx/), which is 
XML-compliant, vendor-neutral open standard 
for storing and exchanging translation memories 
created by Computer Aided Translation (CAT) 
and localization tools. 
Moreover, terms pertaining to the folklore do-
main as well as names of Persons and Locations 
identified in the EL - BG parallel texts were 
semi-automatically aligned. The outcome of the 
process of text alignment at below the sentence 
level was then validated manually. 
5.4 Tools Customization and metadata 
harmonization 
As it has already been stated, the tools that 
were deployed for the linguistic processing are 
generic ones that were initially developed for 
different text types/genres. Moreover, the data at 
hand posed another difficulty that is, coping with 
older/obsolete language usage. In fact, some of 
the literary works were written in the 19th cen-
tury or the beginning of 20th century, and their 
language reflects the writing standards of the 
corresponding period. 
Therefore, as it was expected, the overall per-
formance of the afore-mentioned tools was lower 
than the one reported for the texts these tools 
were initially trained for. 
To this end, performance at POS-tagging level 
dropped from 97% to 77% for the Greek data 
since no normalization of the primary data was 
performed. On the other hand, the BG morpho-
logical analyzer coverage, whose benchmark per-
formance is 96% 
dropped to 92 % on poems and folktales and to 
94% on literary texts and legends. 
The reason was that the language of processed 
literary texts and legends came normalized from 
the sources, while the poems and folktales kept 
some percentage of archaic or dialect words. 
Thus, additionally to the guesser, a post POS 
processing was performed on the unknown 
words. Moreover, the accuracy of the neural 
network disambiguator and the rule-based one 
was 97 %. i.e. the same as for other applications. 
Processing at the levels of chunks and NEs were 
even lower. Within the project we had to tune the 
tools to the specific language types, such as dia-
chronically remote texts and domain specific 
texts (folklore). Also, some words with higher 
distribution in the target regions appear in some 
of the works. In order to deal with them we had 
to extend the used lexicons, to create a guesser 
for the unknown words and add new rules to the 
chunk grammar to handle some specific word 
order within the texts. 
Additionally, the deployment of tools that are 
specific to each language and compatible with 
completely distinct annotation standards brought 
about the issue of metadata harmonization. To 
this end, although the Greek tools were devel-
oped to confront to the afore-mentioned annota-
tion standards, this was not the case for Bulgar-
ian. The first encoding scheme followed the 
BulTreeBank morphological and chunk 
annotation scheme. Afterwards, the information 
was transferred into the project 
scheme in order to be consistent with the Greek 
data and applicable for web representation. As a 
result, the morphosyntactic features of the BG 
tagset, which is a more specialized version of the 
39
Multext-East tagset were mapped onto the rela-
tive PAROLE tags. 
6 The web interface 
All the data collected (being the primary liter-
ary or folklore texts or meta-documents, etc.) 
along with their translations, the multi-layered 
annotations, and the resulting glossaries were 
integrated in a database platform that was devel-
oped to serve as a content management system. 
Being the backbone of that platform, the meta-
data material facilitates the interlinking of similar 
documents, and the access to the primary data 
via the web. To this end, a specially designed 
web site was developed to satisfy the needs of 
end-users (the general public and the special 
groups of researchers and other scientists). The 
website features a trilingual interface (Greek, 
Bulgarian, English) as well as advanced search 
and retrieval mechanisms on the entire bilingual 
content or a user-specified part of it. The users 
can perform combined searches by author name, 
title, genre, etc. Furthermore, they can search for 
single keywords/wordforms or for two word-
forms that can be a user-specified number of 
words apart from each other. Searches by lemma 
and/or by phrase have been also implemented. 
The latter rely on a matcher, which tries to link 
the query word(s) with the stored lem-
mas/wordforms. Additionally, a stemmer for 
Greek and Bulgarian has been used for the on-
line stemming of queries, which will then be 
matched with the already stemmed corpus. When 
all the above fails, fuzzy matching techniques are 
being employed, facilitating, thus, effective 
query expansion functionality. Finally, apart 
from wordforms and lemmas, the collection can 
also be queried for morphosyntactic tags or any 
combination thereof; results, then, come in the 
form of concordances and statistics (frequency 
information), hence the relative document(s) can 
also be retrieved. Moreover, users can search the 
whole corpus or define a sub-corpus based on the 
classification and annotation parameters accom-
panying each text, thus, creating sub-corpora of a 
specific author, or belonging to a specific genre, 
text type, domain, time period, etc. 
In addition, the web interface lets the users to 
simultaneously view on screen both Greek and 
Bulgarian texts, aligned and in parallel,, so that 
to become acquainted with the comparative as-
pects of the two languages or perform specific 
linguistic, lexicographic or translation tasks. Al-
ternatively, the user can consult the bilingual 
glossary of terms and the aligned list of NEs. The 
latter is often very interesting, especially with 
respect to Location entities, since transliteration 
is usually non-adequate.  
The design of the web interface effectively 
blends simplicity and advanced functionality so 
that to fully support the intended usage scenarios 
(comparative study of literary and folklore texts 
equally by specialists, laymen or students, lan-
guage and/or literary teaching and learning, lexi-
cographic projects, etc.). Finally, the web inter-
face has been enhanced by integrating last gen-
eration of synthetic speech technology for both 
Greek and Bulgarian. This speech-enhanced user 
interface (S. Raptis et al 2005), offers innovative 
web accessibility for blind and vision impaired 
Greek and Bulgarian users as well as for other 
users who use speech as their preferable modal-
ity to information access. The key-feature of this 
web-speech technology is that it lets users to in-
teract with the underlying system; so that they 
can hear only the portions of a specific web page 
they are interested in, being able at the same time 
to navigate through the entire web site and visit 
only the web pages of their choice. 
7 Conclusions and future work  
We have described work targeted at the promo-
tion and study of the cultural heritage of the 
cross-border regions of Greece ? Bulgaria, the 
focus been on literature, folklore and language of 
the two people, by means of modern and techno-
logically advanced platforms. To this end, a digi-
tal collection of literary and folklore texts has 
been compiled along with accompanying mate-
rial selected from various (online and printed 
sources), which is integrated into a platform with 
advanced search and retrieval mechanisms. 
However, the cultural value of the bilingual cul-
tural Greek-Bulgarian corpus goes beyond the 
border areas that it was intended for, because it 
shows the similarities and the differences be-
tween the two neighboring countries. More spe-
cifically, it can be used for supporting the acqui-
sition of the other language in both countries. 
Also, it can be explored for comparing the cul-
tural and social attitudes in diachronic depth and 
genre variety. Apart from the usages from a hu-
manities point of view, the corpus can become a 
good base for testing taggers, parsers and align-
ers. It would especially challenge the processing 
of the regional dialects, the language of poems, 
and the language of non-contemporary works. 
40
Future work is being envisaged in the following 
directions: extending the corpus with more texts, 
and respectively the glossaries ? with more 
terms, adding more layers of linguistic analysis 
(predicate-argument structure, etc.), and further 
enhance search and retrieval with the construc-
tion and deployment of an applicable thesaurus. 
 
Acknowledgments 
We would like to thank the anonymous review-
ers for useful suggestions and comments. Most 
of the work presented in this paper was done in 
the framework of a project that was funded under 
the Community Initiative Programme INTER-
REG III A / PHARE CBC Greece ? Bulgaria. 
The project was implemented by the Institute for 
Language and Speech Processing (ILSP, 
www.ilsp.gr) and a group of academics and re-
searchers from the Sofia University St. Kliment 
Ohridski (www.uni-sofia.bg). 
References  
Antti Aarne. 1961. The Types of the Folktale: A Clas-
sification and Bibliography. Translated and 
Enlarged by Stith Thompson. 2nd rev. ed. Helsinki: 
Suomalainen Tiedeakatemia / FF Communications. 
Sotiris Boutsis, Prokopis Prokopidis, Voula Giouli 
and Stelios Piperidis. 2000. A Robust Parser for 
Unrestricted Greek Tex. In Proceedings of the 2nd 
Language and Resources Evaluation Conference, 
467-473, Athens, Greece. 
Michel G?n?reux. 2007. Cultural Heritage Digital 
Resources: From Extraction to Querying, Lan-
guage Technology for Cultural Heritage Data 
(LaTeCH 2007), Workshop at ACL 2007, June 
23rd?30th 2007, Prague, Czech Republic. 
Byron Georgantopoulos and Stelios Piperidis, 2000. 
Term-based Identification of Sentences for Text 
Summarization. In Proceedings of LREC2000 
Voula Giouli, Alexis Konstandinidis, Elina Desypri, 
Harris Papageorgiou. 2006. Multi-domain Multi-
lingual Named Entity Recognition: Revisiting & 
Grounding the resources issue. In Proceedings of 
LREC 2006. 
IMDI, Metadata Elements for Catalogue Descriptions, 
Version 2.1, June 2001 
IMDI, Metadata Elements for Session Descriptions, 
Version 3.0.4, Sept. 2003. 
Harris Papageorgiou, L. Cranias, Stelios 
Piperidis1994. Automatic alignment in parallel 
corpora. In Proceedings of ACL 1994. 
Harris Papageorgiou, Prokopis Prokopidis, Voula 
Giouli, Iasonas Demiros, Alexis Konstantinidis, 
and Stelios Piperidis. 2002. Multi-level XML-based 
Corpus Annotation. Proceedings of the 3nd Lan-
guage and Resources Evaluation Conference. 
Harris Papageorgiou, Prokopis Prokopidis, Voula 
Giouli, and Stelios Piperidis. 2000. A Unified POS 
Tagging Architecture and its Application to Greek. 
In Proceedings of the 2nd Language and Resources 
Evaluation Conference, Athens, Greece, pp 1455-
1462. 
Stelios Piperidis. 1995. Interactive corpus based 
translation drafting tool. In ASLIB Proceedings 
47(3), March 1995. 
Spyros Raptis, I. Spais and P. Tsiakoulis. 2005.  A 
Tool for Enhancing Web Accessibility: Synthetic 
Speech and Content Restructuring?. In Proc. HCII 
2005: 11th International Conference on Human-
Computer Interaction, 22-27 July, Las Vegas, Ne-
vada, USA. 
Kiril Simov, Z. Peev, M. Kouylekov, A. Simov, M. 
Dimitrov, and A. Kiryakov. 2001. CLaRK - an 
XML-based System for Corpora Development. 
Corpus Linguistics 2001 Conference. pp 558-560. 
Kiril Simov, and Petya Osenova. A Hybrid System for 
MorphoSyntactic Disambiguation in Bulgarian. In: 
Proc. of the RANLP 2001 Conference, Tzigov 
Chark, Bulgaria, 5-7 September 2001. pages 288-
290. 
Ren? Witte, Thomas Gitzinger, Thomas Kappler, and 
Ralf Krestel. 2008. A Semantic Wiki Approach to 
Cultural Heritage Data Management. Language 
Technology for Cultural Heritage Data (LaTeCH 
2008), Workshop at LREC 2008, June 1st, 2008, 
Marrakech, Morocco. 
41
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 492?502,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Feature-Rich Part-of-speech Tagging
for Morphologically Complex Languages: Application to Bulgarian
Georgi Georgiev and Valentin Zhikov
Ontotext AD
135 Tsarigradsko Sh., Sofia, Bulgaria
{georgi.georgiev,valentin.zhikov}@ontotext.com
Petya Osenova and Kiril Simov
IICT, Bulgarian Academy of Sciences
25A Acad. G. Bonchev, Sofia, Bulgaria
{petya,kivs}@bultreebank.org
Preslav Nakov
Qatar Computing Research Institute, Qatar Foundation
Tornado Tower, floor 10, P.O. Box 5825, Doha, Qatar
pnakov@qf.org.qa
Abstract
We present experiments with part-of-
speech tagging for Bulgarian, a Slavic lan-
guage with rich inflectional and deriva-
tional morphology. Unlike most previous
work, which has used a small number of
grammatical categories, we work with 680
morpho-syntactic tags. We combine a large
morphological lexicon with prior linguis-
tic knowledge and guided learning from a
POS-annotated corpus, achieving accuracy
of 97.98%, which is a significant improve-
ment over the state-of-the-art for Bulgarian.
1 Introduction
Part-of-speech (POS) tagging is the task of as-
signing each of the words in a given piece of text a
contextually suitable grammatical category. This
is not trivial since words can play different syn-
tactic roles in different contexts, e.g., can is a
noun in ?I opened a can of coke.? but a verb in
?I can write.? Traditionally, linguists have classi-
fied English words into the following eight basic
POS categories: noun, pronoun, adjective, verb,
adverb, preposition, conjunction, and interjection;
this list is often extended a bit, e.g., with deter-
miners, particles, participles, etc., but the number
of categories considered is rarely more than 15.
Computational linguistics works with a larger
inventory of POS tags, e.g., the Penn Treebank
(Marcus et al 1993) uses 48 tags: 36 for part-
of-speech, and 12 for punctuation and currency
symbols. This increase in the number of tags
is partially due to finer granularity, e.g., there
are special tags for determiners, particles, modal
verbs, cardinal numbers, foreign words, existen-
tial there, etc., but also to the desire to encode
morphological information as part of the tags.
For example, there are six tags for verbs in the
Penn Treebank: VB (verb, base form; e.g., sing),
VBD (verb, past tense; e.g., sang), VBG (verb,
gerund or present participle; e.g., singing), VBN
(verb, past participle; e.g., sung) VBP (verb, non-
3rd person singular present; e.g., sing), and VBZ
(verb, 3rd person singular present; e.g., sings);
these tags are morpho-syntactic in nature. Other
corpora have used even larger tagsets, e.g., the
Brown corpus (Kuc?era and Francis, 1967) and the
Lancaster-Oslo/Bergen (LOB) corpus (Johansson
et al 1986) use 87 and 135 tags, respectively.
POS tagging poses major challenges for mor-
phologically complex languages, whose tagsets
encode a lot of additional morpho-syntactic fea-
tures (for most of the basic POS categories), e.g.,
gender, number, person, etc. For example, the
BulTreeBank (Simov et al 2004) for Bulgarian
uses 680 tags, while the Prague Dependency Tree-
bank (Hajic?, 1998) for Czech has over 1,400 tags.
Below we present experiments with POS tag-
ging for Bulgarian, which is an inflectional lan-
guage with rich morphology. Unlike most previ-
ous work, which has used a reduced set of POS
tags, we use all 680 tags in the BulTreeBank. We
combine prior linguistic knowledge and statistical
learning, achieving accuracy comparable to that
reported for state-of-the-art systems for English.
The remainder of the paper is organized as fol-
lows: Section 2 provides an overview of related
work, Section 3 describes Bulgarian morphology,
Section 4 introduces our approach, Section 5 de-
scribes the datasets, Section 6 presents our exper-
iments in detail, Section 7 discusses the results,
Section 8 offers application-specific error analy-
sis, and Section 9 concludes and points to some
promising directions for future work.
492
2 Related Work
Most research on part-of-speech tagging has fo-
cused on English, and has relied on the Penn Tree-
bank (Marcus et al 1993) and its tagset for train-
ing and evaluation. The task is typically addressed
as a sequential tagging problem; one notable ex-
ception is the work of Brill (1995), who proposed
non-sequential transformation-based learning.
A number of different sequential learning
frameworks have been tried, yielding 96-97%
accuracy: Lafferty et al(2001) experimented
with conditional random fields (CRFs) (95.7%
accuracy), Ratnaparkhi (1996) used a maximum
entropy sequence classifier (96.6% accuracy),
Brants (2000) employed a hidden Markov model
(96.6% accuracy), Collins (2002) adopted an av-
eraged perception discriminative sequence model
(97.1% accuracy). All these models fix the order
of inference from left to right.
Toutanova et al(2003) introduced a cyclic de-
pendency network (97.2% accuracy), where the
search is bi-directional. Shen et al(2007) have
further shown that better results (97.3% accu-
racy) can be obtained using guided learning, a
framework for bidirectional sequence classifica-
tion, which integrates token classification and in-
ference order selection into a single learning task
and uses a perceptron-like (Collins and Roark,
2004) passive-aggressive classifier to make the
easiest decisions first. Recently, Tsuruoka et al
(2011), proposed a simple perceptron-based clas-
sifier applied from left to right but augmented
with a lookahead mechanism that searches the
space of future actions, yielding 97.3% accuracy.
For morphologically complex languages, the
problem of POS tagging typically includes mor-
phological disambiguation, which yields a much
larger number of tags. For example, for Arabic,
Habash and Rambow (2005) used support vector
machines (SVM), achieving 97.6% accuracy with
139 tags from the Arabic Treebank (Maamouri et
al., 2003). For Czech, Hajic? et al(2001) com-
bined a hidden Markov model (HMM) with lin-
guistic rules, which yielded 95.2% accuracy using
an inventory of over 1,400 tags from the Prague
Dependency Treebank (Hajic?, 1998). For Ice-
landic, Dredze and Wallenberg (2008) reported
92.1% accuracy with 639 tags developed for the
Icelandic frequency lexicon (Pind et al 1991),
they used guided learning and tag decomposition:
First, a coarse POS class is assigned (e.g., noun,
verb, adjective), then, additional fine-grained
morphological features like case, number and
gender are added, and finally, the proposed tags
are further reconsidered using non-local features.
Similarly, Smith et al(2005) decomposed the
complex tags into factors, where models for pre-
dicting part-of-speech, gender, number, case, and
lemma are estimated separately, and then com-
posed into a single CRF model; this yielded com-
petitive results for Arabic, Korean, and Czech.
Most previous work on Bulgarian POS tagging
has started with large tagsets, which were then
reduced. For example, Dojchinova and Mihov
(2004) mapped their initial tagset of 946 tags to
just 40, which allowed them to achieve 95.5%
accuracy using the transformation-based learning
of Brill (1995), and 98.4% accuracy using manu-
ally crafted linguistic rules. Similarly, Georgiev
et al(2009), who used maximum entropy and
the BulTreeBank (Simov et al 2004), grouped
its 680 fine-grained POS tags into 95 coarse-
grained ones, and thus improved their accuracy
from 90.34% to 94.4%. Simov and Osenova
(2001) used a recurrent neural network to predict
(a) 160 morpho-syntactic tags (92.9% accuracy)
and (b) 15 POS tags (95.2% accuracy).
Some researchers did not reduce the tagset:
Savkov et al(2011) used 680 tags (94.7% ac-
curacy), and Tanev and Mitkov (2002) used 303
tags and the BULMORPH morphological ana-
lyzer (Krushkov, 1997), achieving P=R=95%.
3 Bulgarian Morphology
Bulgarian is an Indo-European language from the
Slavic language group, written with the Cyrillic
alphabet and spoken by about 9-12 million peo-
ple. It is also a member of the Balkan Sprachbund
and thus differs frommost other Slavic languages:
it has no case declensions, uses a suffixed definite
article (which has a short and a long form for sin-
gular masculine), and lacks verb infinitive forms.
It further uses special evidential verb forms to ex-
press unwitnessed, retold, and doubtful activities.
Bulgarian is an inflective language with very
rich morphology. For example, Bulgarian verbs
have 52 synthetic wordforms on average, while
pronouns have altogether more than ten grammat-
ical features (not necessarily shared by all pro-
nouns), including case, gender, person, number,
definiteness, etc.
493
This rich morphology inevitably leads to ambi-
guity proliferation; our analysis of BulTreeBank
shows four major types of ambiguity:
1. Between the wordforms of the same lexeme,
i.e., in the paradigm. For example, divana,
an inflected form of divan (?sofa?, mascu-
line), can mean (a) ?the sofa? (definite, singu-
lar, short definite article) or (b) a count form,
e.g., as in dva divana (?two sofas?).
2. Between two or more lexemes, i.e., conver-
sion. For example, kato can be (a) a subor-
dinator meaning ?as, when?, or (b) a preposi-
tion meaning ?like, such as?.
3. Between a lexeme and an inflected wordform
of another lexeme, i.e., across-paradigms.
For example, politika can mean (a) ?the
politician? (masculine, singular, definite,
short definite article) or (b) ?politics? (fem-
inine, singular, indefinite).
4. Between the wordforms of two or more
lexemes, i.e., across-paradigms and quasi-
conversion. For example, vrvi can mean
(a) ?walks? (verb, 2nd or 3rd person, present
tense) or (b) ?strings, laces? (feminine, plu-
ral, indefinite).
Some morpho-syntactic ambiguities in Bulgar-
ian are occasional, but many are systematic, e.g.,
neuter singular adjectives have the same forms
as adverbs. Overall, most ambiguities are local,
and thus arguably resolvable using n-grams, e.g.,
compare hubavo dete (?beautiful child?), where
hubavo is a neuter adjective, and ?Pe hubavo.?
(?I sing beautifully.?), where it is an adverb of
manner. Other ambiguities, however, are non-
local and may require discourse-level analysis,
e.g., ?Vidh go.? can mean ?I saw him.?, where
go is a masculine pronoun, or ?I saw it.?, where
it is a neuter pronoun. Finally, there are ambi-
guities that are very hard or even impossible1 to
resolve, e.g., ?Deteto vleze veselo.? can mean
both ?The child came in happy.? (veselo is an ad-
jective) and ?The child came in happily.? (it is an
adverb); however, the latter is much more likely.
1The problem also exists for English, e.g., the annotators
of the Penn Treebank were allowed to use tag combinations
for inherently ambiguous cases: JJ|NN (adjective or noun as
prenominal modifier), JJ|VBG (adjective or gerund/present
participle), JJ|VBN (adjective or past participle), NN|VBG
(noun or gerund), and RB|RP (adverb or particle).
In many cases, strong domain preferences exist
about how various systematic ambiguities should
be resolved. We made a study for the newswire
domain, analyzing a corpus of 546,029 words,
and we found that ambiguity type 2 (lexeme-
lexeme) prevailed for functional parts-of-speech,
while the other types were more frequent for in-
flecting parts-of-speech. Below we show the most
frequent types of morpho-syntactic ambiguities
and their frequency in our corpus:
? na: preposition (?of?) vs. emphatic particle,
with a ratio of 28,554 to 38;
? da: auxiliary particle (?to?) vs. affirmative
particle, with a ratio of 12,035 to 543;
? e: 3rd person present auxiliary verb (?to be?)
vs. particle (?well?) vs. interjection (?wow?),
with a ratio of 9,136 to 21 to 5;
? singular masculine noun with a short definite
article vs. count form of a masculine noun,
with a ratio of 6,437 to 1,592;
? adverb vs. neuter singular adjective, with a
ratio of 3,858 to 1,753.
Overall, the following factors should be taken
into account when modeling Bulgarian morpho-
syntax: (1) locality vs. non-locality of grammat-
ical features, (2) interdependence of grammatical
features, and (3) domain-specific preferences.
4 Method
We used the guided learning framework described
in (Shen et al 2007), which has yielded state-of-
the-art results for English and has been success-
fully applied to other morphologically complex
languages such as Icelandic (Dredze and Wallen-
berg, 2008); we found it quite suitable for Bul-
garian as well. We used the feature set defined in
(Shen et al 2007), which includes the following:
1. The feature set of Ratnaparkhi (1996), in-
cluding prefix, suffix and lexical, as well as
some bigram and trigram context features;
2. Feature templates as in (Ratnaparkhi, 1996),
which have been shown helpful in bidirec-
tional search;
3. More bigram and trigram features and bi-
lexical features as in (Shen et al 2007).
Note that we allowed prefixes and suffixes of
length up to 9, as in (Toutanova et al 2003) and
(Tsuruoka and Tsujii, 2005).
494
We further extended the set of features with
the tags proposed for the current word token by a
morphological lexicon, which maps words to pos-
sible tags; it is exhaustive, i.e., the correct tag is
always among the suggested ones for each token.
We also used 70 linguistically-motivated, high-
precision rules in order to further reduce the num-
ber of possible tags suggested by the lexicon.
The rules are similar to those proposed by Hin-
richs and Trushkina (2004) for German; we im-
plemented them as constraints in the CLaRK sys-
tem (Simov et al 2003).
Here is an example of a rule: If a wordform
is ambiguous between a masculine count noun
(Ncmt) and a singular short definite masculine
noun (Ncmsh), the Ncmt tag should be chosen if
the previous token is a numeral or a number.
The 70 rules were developed by linguists based
on observations over the training dataset only.
They target primarily the most frequent cases of
ambiguity, and to a lesser extent some infrequent
but very problematic cases. Some rules operate
over classes of words, while other refer to partic-
ular wordforms. The rules were designed to be
100% accurate on our training dataset; our exper-
iments show that they are also 100% accurate on
the test and on the development dataset.
Note that some of the rules are dependent on
others, and thus the order of their cascaded appli-
cation is important. For example, the wordform 
is ambiguous between an accusative feminine sin-
gular short form of a personal pronoun (?her?) and
an interjection (?wow?). To handle this properly,
the rule for interjection, which targets sentence
initial positions, followed by a comma, needs to
be executed first. The rule for personal pronouns
is only applied afterwards.
Word Tags
To$i Ppe-os3m
obaqe Cc; Dd
nma Afsi; Vnitf-o3s; Vnitf-r3s;
Vpitf-o2s; Vpitf-o3s; Vpitf-r3s
vzmonost Ncfsi
da Ta;Tx
sledi Ncfpi; Vpitf-o2s; Vpitf-o3s; Vpitf-r3s;
Vpitz?2s
. . . . . .
Table 1: Sample fragment showing the possible tags
suggested by the lexicon. The tags that are further
filtered by the rules are in italic; the correct tag is bold.
The rules are quite efficient at reducing the POS
ambiguity. On the test dataset, before the rule ap-
plication, 34.2% of the tokens (excluding punctu-
ation) had more than one tag in our morphological
lexicon. This number is reduced to 18.5% after
the cascaded application of the 70 linguistic rules.
Table 1 illustrates the effect of the rules on a small
sentence fragment. In this example, the rules have
left only one tag (the correct one) for three of the
ambiguous words. Since the rules in essence de-
crease the average number of tags per token, we
calculated that the lexicon suggests 1.6 tags per
token on average, and after the application of the
rules this number decreases to 1.44 per token.
5 Datasets
5.1 BulTreeBank
We used the latest version of the BulTree-
Bank (Simov and Osenova, 2004), which contains
20,556 sentences and 321,542 word tokens (four
times less than the English Penn Treebank), anno-
tated using a total of 680 unique morpho-syntactic
tags. See (Simov et al 2004) for a detailed de-
scription of the BulTreeBank tagset.
We split the data into training/development/test
as shown in Table 2. Note that only 552 of all 680
tag types were used in the training dataset, and
the development and the test datasets combined
contain a total of 128 new tag types that were not
seen in the training dataset. Moreover, 32% of the
word types in the development dataset and 31%
of those in the testing dataset do not occur in the
training dataset. Thus, data sparseness is an issue
at two levels: word-level and tag-level.
Dataset Sentences Tokens Types Tags
Train 16,532 253,526 38,659 552
Dev 2,007 32,995 9,635 425
Test 2,017 35,021 9,627 435
Table 2: Statistics about our datasets.
5.2 Morphological Lexicon
In order to alleviate the data sparseness issues,
we further used a large morphological lexicon for
Bulgarian, which is an extended version of the
dictionary described in (Popov et al 1998) and
(Popov et al 2003). It contains over 1.5M in-
flected wordforms (for 110K lemmata and 40K
proper names), each mapped to a set of possible
morpho-syntactic tags.
495
6 Experiments and Evaluation
State-of-the-art POS taggers for English typically
build a lexicon containing all tags a word type has
taken in the training dataset; this lexicon is then
used to limit the set of possible tags that an input
token can be assigned, i.e., it imposes a hard con-
straint on the possibilities explored by the POS
tagger. For example, if can has only been tagged
as a verb and as a noun in the training dataset,
it will be only assigned those two tags at test
time; other tags such as adjective, adverb and pro-
noun will not be considered. Out-of-vocabulary
words, i.e., those that were not seen in the train-
ing dataset, are constrained as well, e.g., to a small
set of frequent open-class tags.
In our experiments, we used a morphological
lexicon that is much larger than what could be
built from the training corpus only: building a
lexicon from the training corpus only is of lim-
ited utility since one can hardly expect to see in
the training corpus all 52 synthetic forms a verb
can possibly have. Moreover, we did not use the
tags listed in the lexicon as hard constraints (ex-
cept in one of our baselines); instead, we experi-
mented with a different, non-restrictive approach:
we used the lexicon?s predictions as features or
soft constraints, i.e., as suggestions only, thus al-
lowing each token to take any possible tag. Note
that for both known and out-of-vocabulary words
we used all 680 tags rather than the 552 tags ob-
served in the training dataset; we could afford to
explore this huge search space thanks to the effi-
ciency of the guided learning framework. Allow-
ing all 680 tags on training helped the model by
exposing it to a larger set of negative examples.
We combined these lexicon features with stan-
dard features extracted from the training corpus.
We further experimented with the 70 contextual
linguistic rules, using them (a) as soft and (b) as
hard constraints. Finally, we set four baselines:
three that do not use the lexicon and one that does.
Accuracy (%)
# Baselines (token-level)
1 MFT + unknowns are wrong 78.10
2 MFT + unknowns are Ncmsi 78.52
3 MFT + guesser for unknowns 79.49
4 MFT + lexicon tag-classes 94.40
Table 3: Most-frequent-tag (MFT) baselines.
6.1 Baselines
First, we experimented with the most-frequent-
tag baseline, which is standard for POS tagging.
This baseline ignores context altogether and as-
signs each word type the POS tag it was most
frequently seen with in the training dataset; ties
are broken randomly. We coped with word types
not seen in the training dataset using three sim-
ple strategies: (a) we considered them all wrong,
(b) we assigned them Ncmsi, which is the most
frequent open-class tag in the training dataset, or
(c) we used a very simple guesser, which assigned
Ncfsi, Ncnsi, Ncfsi, and Ncmsf, if the target word
ended by -a, -o, -i, and -t, respectively, other-
wise, it assigned Ncmsi. The results are shown
in lines 1-3 of Table 3: we can see that the token-
level accuracy ranges in 78-80% for (a)-(c), which
is relatively high, given that we use a large inven-
tory of 680 morpho-syntactic tags.
We further tried a baseline that uses the above-
described morphological lexicon, in addition to
the training dataset. We first built two frequency
lists, containing respectively (1) the most frequent
tag in the training dataset for each word type, as
before, and (2) the most frequent tag in the train-
ing dataset for each class of tags that can be as-
signed to some word type, according to the lexi-
con. For example, the most frequent tag for poli-
tika is Ncfsi, and the most frequent tag for the
tag-class {Ncmt;Ncmsi} is Ncmt.
Given a target word type, this new baseline first
tries to assign it the most frequent tag from the
first list. If this is not possible, which happens
(i) in case of ties or (ii) when the word type was
not seen on training, it extracts the tag-class from
the lexicon and consults the second list. If there
is a single most frequent tag in the corpus for this
tag-class, it is assigned; otherwise a random tag
from this tag-class is selected.
Line 4 of Table 3 shows that this latter baseline
achieves a very high accuracy of 94.40%. Note,
however, that this is over-optimistic: the lexicon
contains a tag-class for each word type in our test-
ing dataset, i.e., while there can be word types
not seen in the training dataset, there are no word
types that are not listed in the lexicon. Thus, this
high accuracy is probably due to a large extent
to the scale and quality of our morphological lexi-
con, and it might not be as strong with smaller lex-
icons; we plan to investigate this in future work.
496
6.2 Lexicon Tags as Soft Constraints
We experimented with three types of features:
1. Word-related features only;
2. Word-related features + the tags suggested
by the lexicon;
3. Word-related features + the tags suggested
by the lexicon but then further filtered using
the 70 contextual linguistic rules.
Table 4 shows the sentence-level and the token-
level accuracy on the test dataset for the three
kinds of features: shown on lines 1, 3 and 4, re-
spectively. We can see that using the tags pro-
posed by the lexicon as features (lines 3 and 4)
has a major positive impact, yielding up to 49%
error reduction at the token-level and up to 37%
at the sentence-level, as compared to using word-
related features alone (line 1).
Interestingly, filtering the tags proposed by the
lexicon using the 70 contextual linguistic rules
yields a minor decrease in accuracy both at the
word token-level and at the sentence-level (com-
pare line 4 to line 2). This is surprising since
the linguistic rules are extremely reliable: they
were designed to be 100% accurate on the train-
ing dataset, and we found them experimentally to
be 100% correct on the development and on the
testing dataset as well.
One possible explanation is that by limiting the
set of available tags for a given token at training
time, we prevent the model from observing some
potentially useful negative examples. We tested
this hypothesis by using the unfiltered lexicon
predictions at training time but then making use
of the filtered ones at testing time; the results are
shown on line 5. We can observe a small increase
in accuracy compared to line 4: from 97.80% to
97.84% at the token-level, and from 70.30% to
70.40% at the sentence-level. Although these dif-
ferences are tiny, they suggest that having more
negative examples at training is helpful.
We can conclude that using the lexicon as a
source of soft constraints has a major positive im-
pact, e.g., because it provides access to impor-
tant external knowledge that is complementary
to what can be learned from the training corpus
alone; the improvements when using linguistic
rules as soft constraints are more limited.
6.3 Linguistic Rules as Hard Constraints
Next, we experimented with using the suggestions
of the linguistic rules as hard constraints. Table 4
shows that this is a very good idea. Comparing
line 1 to line 2, which do not use the morpholog-
ical lexicon, we can see very significant improve-
ments: from 95.72% to 97.20% at the token-level
and from 52.95% to 64.50% at the sentence-level.
The improvements are smaller but still consistent
when the morphological lexicon is used: compar-
ing lines 3 and 4 to lines 6 and 7, respectively, we
see an improvement from 97.83% to 97.91% and
from 97.80% to 97.93% at the token-level, and
about 1% absolute at the sentence-level.
6.4 Increasing the Beam Size
Finally, we increased the beam size of guided
learning from 1 to 3 as in (Shen et al 2007).
Comparing line 7 to line 8 in Table 4, we can see
that this yields further token-level improvement:
from 97.93% to 97.98%.
7 Discussion
Table 5 compares our results to previously re-
ported evaluation results for Bulgarian. The
first four lines show the token-level accuracy for
standard POS tagging tools trained and evalu-
ated on the BulTreeBank:2 TreeTagger (Schmid,
1994), which uses decision trees, TnT (Brants,
2000), which uses a hidden Markov model,
SVMtool (Gime?nez and Ma`rquez, 2004), which
is based on support vector machines, and
ACOPOST (Schro?der, 2002), implementing the
memory-based model of Daelemans et al(1996).
The following lines report the token-level accu-
racy reported in previous work, as compared to
our own experiments using guided learning.
We can see that we outperform by a very large
margin (92.53% vs. 97.98%, which represents
73% error reduction) the systems from the first
four lines, which are directly comparable to our
experiments: they are trained and evaluated on the
BulTreeBank using the full inventory of 680 tags.
We further achieved statistically significant im-
provement (p < 0.0001; Pearson?s chi-squared
test (Plackett, 1983)) over the best pervious result
on 680 tags: from 94.65% to 97.98%, which rep-
resents 62.24% error reduction at the token-level.
2We used the pre-trained TreeTagger; for the rest, we re-
port the accuracy given on the Webpage of the BulTreeBank:
www.bultreebank.org/taggers/taggers.html
497
Lexicon Linguistic Rules (applied to filter): Beam Accuracy (%)
# (source of) (a) the lexicon features (b) the output tags size Sentence-level Token-level
1 ? ? ? 1 52.95 95.72
2 ? ? yes 1 64.50 97.20
3 features ? ? 1 70.40 97.83
4 features yes ? 1 70.30 97.80
5 features yes, for test only ? 1 70.40 97.84
6 features ? yes 1 71.34 97.91
7 features yes yes 1 71.69 97.93
8 features yes yes 3 71.94 97.98
Table 4: Evaluation results on the test dataset. Line 1 shows the evaluation results when using features derived
from the text corpus only; these features are used by all systems in the table. Line 2 further uses the contextual
linguistic rules to limit the set of possible POS tags that can be predicted. Note that these rules (1) consult the
lexicon, and (2) always predict a single POS tag. Line 3 uses the POS tags listed in the lexicon as features, i.e.,
as soft suggestions only. Line 4 is like line 3, but the list of feature-tags proposed by the lexicon is filtered by
the contextual linguistic rules. Line 5 is like line 4, but the linguistic rules filtering is only applied at test time;
it is not done on training. Lines 6 and 7 are similar to lines 3 and 4, respectively, but here the linguistic rules
are further applied to limit the set of possible POS tags that can be predicted, i.e., the rules are used as hard
constraints. Finally, line 8 is like line 7, but here the beam size is increased to 3.
Overall, we improved over almost all previ-
ously published results. Our accuracy is sec-
ond only to the manual rules approach of Do-
jchinova and Mihov (2004). Note, however, that
they used 40 tags only, i.e., their inventory is 17
times smaller than ours. Moreover, they have op-
timized their tagset specifically to achieve very
high POS tagging accuracy by choosing not to at-
tempt to resolve some inherently hard systematic
ambiguities, e.g., they do not try to choose be-
tween second and third person past singular verbs,
whose inflected forms are identical in Bulgarian
and hard to distinguish when the subject is not
present (Bulgarian is a pro-drop language).
In order to compare our results more closely
to the smaller tagsets in Table 5, we evaluated
our best model with respect to (a) the first letter
of the tag only (which is part-of-speech only, no
morphological information; 13 tags), e.g., Ncmsf
becomes N, and (b) the first two letters of the
tag (POS + limited morphological information;
49 tags), e.g., Ncmsf becomes Nc. This yielded
99.30% accuracy for (a) and 98.85% for (b).
The latter improves over (Dojchinova and Mihov,
2004), while using a bit larger number of tags.
Our best token-level accuracy of 97.98% is
comparable and even slightly better than the state-
of-the-art results for English: 97.33% when using
Penn Treebank data only (Shen et al 2007), and
97.50% for Penn Treebank plus some additional
unlabeled data (S?gaard, 2011). Of course, our
results are only indirectly comparable to English.
Still, our performance is impressive because
(1) our model is trained on 253,526 tokens only
while the standard training sections 0-18 of the
Penn Treebank contain a total of 912,344 tokens,
i.e., almost four times more, and (2) we predict
680 rather than just 48 tags as for the Penn Tree-
bank, which is 14 times more.
Note, however, that (1) we used a large exter-
nal morphological lexicon for Bulgarian, which
yielded about 50% error reduction (without it,
our accuracy was 95.72% only), and (2) our
train/dev/test sentences are generally shorter, and
thus arguably simpler for a POS tagger to analyze:
we have 17.4 words per test sentence in the Bul-
TreeBank vs. 23.7 in the Penn Treebank.
Our results also compare favorably to the state-
of-the-art results for other morphologically com-
plex languages that use large tagsets, e.g., 95.2%
for Czech with 1,400+ tags (Hajic? et al 2001),
92.1% for Icelandic with 639 tags (Dredze and
Wallenberg, 2008), 97.6% for Arabic with 139
tags (Habash and Rambow, 2005).
8 Error Analysis
In this section, we present error analysis with re-
spect to the impact of the POS tagger?s perfor-
mance on other processing steps in a natural lan-
guage processing pipeline, such as lemmatization
and syntactic dependency parsing.
First, we explore the most frequently confused
pairs of tags for our best-performing POS tagging
system; these are shown in Table 6.
498
Accuracy
Tool/Authors Method # Tags (token-level, %)
*TreeTagger Decision Trees 680 89.21
*ACOPOST Memory-based Learning 680 89.91
*SVMtool Support Vector Machines 680 92.22
*TnT Hidden Markov Model 680 92.53
(Georgiev et al 2009) Maximum Entropy 680 90.34
(Simov and Osenova, 2001) Recurrent Neural Network 160 92.87
(Georgiev et al 2009) Maximum Entropy 95 94.43
(Savkov et al 2011) SVM + Lexicon + Rules 680 94.65
(Tanev and Mitkov, 2002) Manual Rules 303 95.00(=P=R)
(Simov and Osenova, 2001) Recurrent Neural Network 15 95.17
(Dojchinova and Mihov, 2004) Transformation-based Learning 40 95.50
(Dojchinova and Mihov, 2004) Manual Rules + Lexicon 40 98.40
Guided Learning 680 95.72
Guided Learning + Lexicon 680 97.83
This work Guided Learning + Lexicon + Rules 680 97.98
Guided Learning + Lexicon + Rules 49 98.85
Guided Learning + Lexicon + Rules 13 99.30
Table 5: Comparison to previous work for Bulgarian. The first four lines report evaluation results for various
standard POS tagging tools, which were retrained and evaluated on the BulTreeBank. The following lines report
token-level accuracy for previously published work, as compared to our own experiments using guided learning.
We can see that most of the wrong tags share
the same part-of-speech (indicated by the initial
uppercase letter), such as V for verb, N for noun,
etc. This means that most errors refer to the mor-
phosyntactic features. For example, personal or
impersonal verb; definite or indefinite feminine
noun; singular or plural masculine adjective, etc.
At the same time, there are also cases, where the
error has to do with the part-of-speech label itself.
For example, between an adjective and an adverb,
or between a numeral and an indefinite pronoun.
We want to use the above tagger to develop
(1) a rule-based lemmatizer, using the morpholog-
ical lexicon, e.g., as in (Plisson et al 2004), and
(2) a dependency parser like MaltParser (Nivre et
al., 2007), trained on the dependency part of the
BulTreeBank. We thus study the potential impact
of wrong tags on the performance of these tools.
The lemmatizer relies on the lexicon and uses
string transformation functions defined via two
operations ? remove and concatenate:
if tag = Tag then
{remove OldEnd; concatenate NewEnd}
where Tag is the tag of the wordform, OldEnd is
the string that has to be removed from the end of
the wordform, and NewEnd is the string that has
to be concatenated to the beginning of the word-
form in order to produce the lemma.
Here is an example of such a rule:
if tag = Vpitf-o1s then
{remove oh; concatenate a}
The application of the above rule to the past
simple verb form qetoh (?I read?) would remove
oh, and then concatenate a. The result would be
the correct lemma qeta (?to read?).
Such rules are generated for each wordform in
the morphological lexicon; the above functional
representation allows for compact representation
in a finite state automaton. Similar rules are ap-
plied to the unknown words, where the lemma-
tizer tries to guess the correct lemma.
Obviously, the applicability of each rule cru-
cially depends on the output of the POS tagger.
If the tagger suggests the correct tag, then the
wordform would be lemmatized correctly. Note
that, in some cases of wrongly assigned POS tags
in a given context, we might still get the correct
lemma. This is possible in the majority of the
erroneous cases in which the part-of-speech has
been assigned correctly, but the wrong grammat-
ical alternative has been selected. In such cases,
the error does not influence lemmatization.
In order to calculate the proportion of such
cases, we divided each tag into two parts:
(a) grammatical features that are common for all
wordforms of a given lemma, and (b) features that
are specific to the wordform.
499
Freq. Gold Tag Proposed Tag
43 Ansi Dm
23 Vpitf-r3s Vnitf-r3s
16 Npmsh Npmsi
14 Vpiif-r3s Vniif-r3s
13 Npfsd Npfsi
12 Dm Ansi
12 Vpitcam-smi Vpitcao-smi
12 Vpptf-r3p Vpitf-r3p
11 Vpptf-r3s Vpptf-o3s
10 Mcmsi Pfe-os-mi
10 Ppetas3n Ppetas3m
10 Ppetds3f Psot?3?f
9 Npnsi Npnsd
9 Vpptf-o3s Vpptf-r3s
8 Dm A-pi
8 Ppxts Ppxtd
7 Mcfsi Pfe-os-fi
7 Npfsi Npfsd
7 Ppetas3m Ppetas3n
7 Vnitf-r3s Vpitf-r3s
7 Vpitcam-p-i Vpitcao-p-i
Table 6: Most frequently confused pairs of tags.
The part-of-speech features are always deter-
mined by the lemma. For example, Bulgarian
verbs have the lemma features aspect and tran-
sitivity. If they are correct, then the lemma is pre-
dicted also correctly, regardless of whether cor-
rect or wrong on the grammatical features. For
example, if the verb participle form (aorist or
imperfect) has its correct aspect and transitivity,
then it is lemmatized also correctly, regardless
of whether the imperfect or aorist features were
guessed correctly; similarly, for other error types.
We evaluated these cases for the 711 errors in our
experiment, and we found that 206 of them (about
29%) were non-problematic for lemmatization.
For the MaltParser, we encode most of the
grammatical features of the wordforms as spe-
cific features for the parser. Hence, it is much
harder to evaluate the problematic cases due to
the tagger. Still, we were able to make an es-
timation of some cases. Our strategy was to ig-
nore the grammatical features that do not always
contribute to the syntactic behavior of the word-
forms. Such grammatical features for the verbs
are aspect and tense. Thus, proposing perfective
instead of imperfective for a verb or present in-
stead of past tense would not cause problems for
the MaltParser. Among our 711 errors, 190 cases
(or about 27%) were not problematic for parsing.
Finally, we should note that there are two spe-
cial classes of tokens for which it is generally
hard to predict some of the grammatical features:
(1) abbreviations and (2) numerals written with
digits. In sentences, they participate in agreement
relations only if they are pronounced as whole
phrases; unfortunately, it is very hard for the tag-
ger to guess such relations since it does not have
at its disposal enough features, such as the inflec-
tion of the numeral form, that might help detect
and use the agreement pattern.
9 Conclusion and Future Work
We have presented experiments with part-of-
speech tagging for Bulgarian, a Slavic language
with rich inflectional and derivational morphol-
ogy. Unlike most previous work for this language,
which has limited the number of possible tags, we
used a very rich tagset of 680 morpho-syntactic
tags as defined in the BulTreeBank. By com-
bining a large morphological lexicon with prior
linguistic knowledge and guided learning from a
POS-annotated corpus, we achieved accuracy of
97.98%, which is a significant improvement over
the state-of-the-art for Bulgarian. Our token-level
accuracy is also comparable to the best results re-
ported for English.
In future work, we want to experiment with a
richer set of features, e.g., derived from unlabeled
data (S?gaard, 2011) or from the Web (Umansky-
Pesin et al 2010; Bansal and Klein, 2011). We
further plan to explore ways to decompose the
complex Bulgarian morpho-syntactic tags, e.g., as
proposed in (Simov and Osenova, 2001) and
(Smith et al 2005). Modeling long-distance
syntactic dependencies (Dredze and Wallenberg,
2008) is another promising direction; we believe
this can be implemented efficiently using poste-
rior regularization (Graca et al 2009) or expecta-
tion constraints (Bellare et al 2009).
Acknowledgments
We would like to thank the anonymous reviewers
for their useful comments, which have helped us
improve the paper.
The research presented above has been par-
tially supported by the EU FP7 project 231720
EuroMatrixPlus, and by the SmartBook project,
funded by the Bulgarian National Science Fund
under grant D002-111/15.12.2008.
500
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, ACL-HLT ?10, pages 693?702, Portland, Ore-
gon, USA.
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning
with expectation constraints. In Proceedings of the
25th Conference on Uncertainty in Artificial Intel-
ligence, UAI ?09, pages 43?50, Montreal, Quebec,
Canada.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing, ANLP ?00, pages
224?231, Seattle, Washington, USA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. Comput. Linguist.,
21:543?565.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics, Main Volume, ACL ?04,
pages 111?118, Barcelona, Spain.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?02, pages 1?8,
Philadelphia, PA, USA.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. MBT: A memory-based part
of speech tagger generator. In Eva Ejerhed and
Ido Dagan, editors, Fourth Workshop on Very Large
Corpora, pages 14?27, Copenhagen, Denmark.
Veselka Dojchinova and Stoyan Mihov. 2004. High
performance part-of-speech tagging of Bulgarian.
In Christoph Bussler and Dieter Fensel, editors,
AIMSA, volume 3192 of Lecture Notes in Computer
Science, pages 246?255. Springer.
Mark Dredze and Joel Wallenberg. 2008. Icelandic
data driven part of speech tagging. In Proceedings
of the 44th Annual Meeting of the Association of
Computational Linguistics: Short Papers, ACL ?08,
pages 33?36, Columbus, Ohio, USA.
Georgi Georgiev, Preslav Nakov, Petya Osenova, and
Kiril Simov. 2009. Cross-lingual adaptation as
a baseline: adapting maximum entropy models to
Bulgarian. In Proceedings of the RANLP?09 Work-
shop on Adaptation of Language Resources and
Technology to New Domains, AdaptLRTtoND ?09,
pages 35?38, Borovets, Bulgaria.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of the 4th Inter-
national Conference on Language Resources and
Evaluation, LREC ?04, Lisbon, Portugal.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fer-
nando Pereira. 2009. Posterior vs parameter spar-
sity in latent variable models. In Yoshua Bengio,
Dale Schuurmans, John D. Lafferty, Christopher
K. I. Williams, and Aron Culotta, editors, Advances
in Neural Information Processing Systems 22, NIPS
?09, pages 664?672. Curran Associates, Inc., Van-
couver, British Columbia, Canada.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?05, pages
573?580, Ann Arbor, Michigan.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva,
and Vladim??r Petkevic?. 2001. Serial combination
of rules and statistics: A case study in Czech tag-
ging. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
ACL ?01, pages 268?275, Toulouse, France.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?
19. Prague Karolinum, Charles University Press.
Erhard W. Hinrichs and Julia S. Trushkina. 2004.
Forging agreement: Morphological disambiguation
of noun phrases. Research on Language & Compu-
tation, 2:621?648.
Stig Johansson, Eric Atwell, Roger Garside, and Geof-
frey Leech, 1986. The Tagged LOB Corpus: Users?
manual. ICAME, The Norwegian Computing Cen-
tre for the Humanities, Bergen University, Norway.
Hristo Krushkov. 1997. Modelling and building ma-
chine dictionaries and morphological processors
(in Bulgarian). Ph.D. thesis, University of Plov-
div, Faculty of Mathematics and Informatics, Plov-
div, Bulgaria.
Henry Kuc?era and Winthrop Nelson Francis. 1967.
Computational analysis of present-day American
English. Brown University Press, Providence, RI.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA.
Mohamed Maamouri, Ann Bies, Hubert Jin, and Tim
Buckwalter. 2003. Arabic Treebank: Part 1 v 2.0.
LDC2003T06.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
put. Linguist., 19:313?330.
501
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Jo?rgen Pind, Fridrik Magnu?sson, and Stefa?n Briem.
1991. The Icelandic frequency dictionary. Techni-
cal report, The Institute of Lexicography, University
of Iceland, Reykjavik, Iceland.
Robin L. Plackett. 1983. Karl Pearson and the Chi-
Squared Test. International Statistical Review / Re-
vue Internationale de Statistique, 51(1):59?72.
Joe?l Plisson, Nada Lavrac?, and Dunja Mladenic?. 2004.
A rule based approach to word lemmatization. In
Proceedings of the 7th International Multiconfer-
ence: Information Society, IS ?2004, pages 83?86,
Ljubljana, Slovenia.
Dimitar Popov, Kiril Simov, and Svetlomira Vidinska.
1998. Dictionary of Writing, Pronunciation and
Punctuation of Bulgarian Language (in Bulgarian).
Atlantis KL, Sofia, Bulgaria.
Dimityr Popov, Kiril Simov, Svetlomira Vidinska, and
Petya Osenova. 2003. Spelling Dictionary of Bul-
garian. Nauka i izkustvo, Sofia, Bulgaria.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Eva Ejerhed
and Ido Dagan, editors, Fourth Workshop on Very
Large Corpora, pages 133?142, Copenhagen, Den-
mark.
Aleksandar Savkov, Laska Laskova, Petya Osenova,
Kiril Simov, and Stanislava Kancheva. 2011.
A web-based morphological tagger for Bulgarian.
In Daniela Majchra?kova? and Radovan Garab??k,
editors, Slovko 2011. Sixth International Confer-
ence. Natural Language Processing, Multilingual-
ity, pages 126?137, Modra/Bratislava, Slovakia.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Ingo Schro?der. 2002. A case study in part-of-speech-
tagging using the ICOPOST toolkit. Technical Re-
port FBI-HH-M-314/02, Department of Computer
Science, University of Hamburg.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ?07, pages 760?767, Prague, Czech Republic.
Kiril Simov and Petya Osenova. 2001. A hybrid
system for morphosyntactic disambiguation in Bul-
garian. In Proceedings of the EuroConference on
Recent Advances in Natural Language Processing,
RANLP ?01, pages 5?7, Tzigov chark, Bulgaria.
Kiril Simov and Petya Osenova. 2004. BTB-TR04:
BulTreeBank morphosyntactic annotation of Bul-
garian texts. Technical Report BTB-TR04, Bulgar-
ian Academy of Sciences.
Kiril Ivanov Simov, Alexander Simov, Milen
Kouylekov, Krasimira Ivanova, Ilko Grigorov, and
Hristo Ganev. 2003. Development of corpora
within the CLaRK system: The BulTreeBank
project experience. In Proceedings of the 10th con-
ference of the European chapter of the Association
for Computational Linguistics, EACL ?03, pages
243?246, Budapest, Hungary.
Kiril Simov, Petya Osenova, and Milena Slavcheva.
2004. BTB-TR03: BulTreeBank morphosyntac-
tic tagset. Technical Report BTB-TR03, Bulgarian
Academy of Sciences.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambigua-
tion with random fields. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 475?482, Vancouver, British
Columbia, Canada.
Anders S?gaard. 2011. Semi-supervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL-HLT ?10,
pages 48?52, Portland, Oregon, USA.
Hristo Tanev and Ruslan Mitkov. 2002. Shallow
language processing architecture for Bulgarian. In
Proceedings of the 19th International Conference
on Computational Linguistics, COLING ?02, pages
1?7, Taipei, Taiwan.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the Conference of
the North American Chapter of the Association
for Computational Linguistics, NAACL ?03, pages
173?180, Edmonton, Canada.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT-EMNLP ?05, pages 467?474, Vancouver,
British Columbia, Canada.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: Can
history-based models rival globally optimized mod-
els? In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, ACL-HLT ?10, pages
238?246, Portland, Oregon, USA.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algo-
rithm for POS tagging of unknown words. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ?10,
pages 1274?1282, Beijing, China.
502
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 119?128,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Linguistically-Augmented Bulgarian-to-English Statistical Machine
Translation Model
Rui Wang
Language Technology Lab
DFKI GmbH
Saarbru?cken, Germany
ruiwang@dfki.de
Petya Osenova and Kiril Simov
Linguistic Modelling Department, IICT
Bulgarian Academy of Sciences
Sofia, Bulgaria
{petya,kivs}@bultreebank.org
Abstract
In this paper, we present our linguistically-
augmented statistical machine translation
model from Bulgarian to English, which
combines a statistical machine translation
(SMT) system (as backbone) with deep lin-
guistic features (as factors). The motiva-
tion is to take advantages of the robust-
ness of the SMT system and the linguis-
tic knowledge of morphological analysis
and the hand-crafted grammar through sys-
tem combination approach. The prelimi-
nary evaluation has shown very promising
results in terms of BLEU scores (38.85) and
the manual analysis also confirms the high
quality of the translation the system deliv-
ers.
1 Introduction
In the recent years, machine translation (MT)
has achieved significant improvement in terms
of translation quality (Koehn, 2010). Both
data-driven approaches (e.g., statistical MT
(SMT)) and knowledge-based (e.g., rule-based
MT (RBMT)) have achieved comparable results
shown in the evaluation campaigns (Callison-
Burch et al, 2011). However, according to the
human evaluation, the final outputs of the MT sys-
tems are still far from satisfactory.
Fortunately, recent error analysis shows that the
two trends of the MT approaches tend to be com-
plementary to each other, in terms of the types
of the errors they made (Thurmair, 2005; Chen et
al., 2009). Roughly speaking, RBMT systems of-
ten have missing lexicon and thus lack of robust-
ness, while handling linguistic phenomena requir-
ing syntactic information better. SMT systems, on
the contrary, are in general more robust, but some-
times output ungrammatical sentences.
In fact, instead of competing with each other,
there is also a line of research trying to com-
bine the advantages of the two sides using a
hybrid framework. Although many systems
can be put under the umbrella of ?hybrid? sys-
tems, there are various ways to do the combi-
nation/integration. Thurmair (2009) summarized
several different architectures of hybrid systems
using SMT and RBMT systems. Some widely
used ones are: 1) using an SMT to post-edit the
outputs of an RBMT; 2) selecting the best trans-
lations from several hypotheses coming from dif-
ferent SMT/RBMT systems; and 3) selecting the
best segments (phrases or words) from different
hypotheses.
For the language pair Bulgarian-English, there
has not been much study on it, mainly due to the
lack of resources, including corpora, preproces-
sors, etc. There was a system published by Koehn
et al (2009), which was trained and tested on the
European Union law data, but not on other do-
mains like news. They reported a very high BLEU
score (Papineni et al, 2002) on the Bulgarian-
English translation direction (61.3), which in-
spired us to further investigate this direction.
In this paper, we focus on the Bulgarian-to-
English translation and mainly explore the ap-
proach of annotating the SMT baseline with lin-
guistic features derived from the preprocessing
and hand-crafted grammars. There are three mo-
tivations behind our approach: 1) the SMT base-
line trained on a decent amount of parallel cor-
pora outputs surprisingly good results, in terms of
both statistical evaluation metrics and preliminary
manual evaluation; 2) the augmented model gives
119
us more space for experimenting with different
linguistic features without losing the ?basic? ro-
bustness; 3) the MT system can profit from con-
tinued advances in the development of the deep
grammars thereby opening up further integration
possibilities.
The rest of the paper will be organized as fol-
lows: Section 2 presents our work on cleaning
the corpora and Section 3 briefly describes the
preprocessing of the data. Section 4 introduces
our factor-based SMT model which allows us
to incorporate various linguistic features into an
SMT baseline, among which those features com-
ing from the MRS are described in Section 5 in
detail. We show our experiments in Section 6 as
well as both automatic and manual evaluation of
the results. Section 7 briefly mentions some re-
lated work and then we summarize this paper in
Section 8.
2 Data Preparation
In our experiments we are using the SETIMES
parallel corpus, which is part of the OPUS parallel
corpus1. The data in the corpus was aligned auto-
matically. Thus, we first checked the consistency
of the automatic alignments. It turned out that
more than 25% of the sentence alignments were
not correct. Since SETIMES appeared to be a
noisy dataset, our effort was directed into cleaning
it as much as possible before the start of the ex-
periments. We first corrected manually more than
25.000 sentence alignments. The the rest of the
data set includes around 135,000 sentences. Al-
together the data set is about 160,000 sentences,
when the manually checked part is added. Thus,
two actions were taken:
1. Improving the tokenization of the Bulgar-
ian part. The observations from the man-
ual check of the set of 25,000 sentences
showed systematic errors in the tokenized
text. Hence, these cases have been detected
and fixed semi-automatically.
2. Correcting and removing the suspicious
alignments. Initially, the ratio of the lengths
of the English and Bulgarian sentences was
calculated in the set of the 25,000 manually
annotated sentences. As a rule, the Bulgarian
1OPUS?an open source parallel corpus,
http://opus.lingfil.uu.se/
sentences are longer than the English ones.
The ratio is 1.34. Then we calculated the ra-
tio for each pair of sentences. After this, the
optimal interval was manually determined,
such that if the ratio for a given pair of sen-
tences is within the interval, then we assume
that the pair is a good one. The interval for
these experiments is set to [0.7; 1.8]. All the
pairs with ratio outside of the interval have
been deleted. Similarly, we have cleaned
EMEA dataset.
The size of the resulting datasets are: 151,718
sentence pairs for the SETIMES dataset. Simi-
lar approach was undertaken for another dataset
from OPUS corpus - EMEA. After the cleaning
704,631 sentence pairs were selected from the
EMEA dataset. Thus, the size of the original
datasets was decreased by 10%.
3 Linguistic Preprocessing
The data in SETIMES dataset was analysed on the
following levels:
? POS tagging. POS tagging is performed by
a pipe of several modules. First we apply
SVM POS tagger which takes as an input
a tokenised text and its output is a tagged
text. The performance is near 91% accuracy.
The SVM POS tagger is implemented us-
ing SVMTool (Gimnez and Mrquez, 2004).
Then we apply a morphological lexicon and
a set of rules. The lexicon add all the pos-
sible tags for the known words. The rules
reduce the ambiguity for some of the sure
cases. The result of this step is a tagged text
with some ambiguities unresolved. The third
step is application of the GTagger (Georgiev
et al, 2012). It is trained on an ambigu-
ous data and select the most appropriate tags
from the suggested ones. The accuracy of the
whole pipeline is 97.83%. In this pipeline
SVM POS Tagger plays the role of guesser
for the GTagger.
? Lemmatization. The lemmatization mod-
ule is based on the same morphological lexi-
con. From the lexicon we extracted functions
which convert each wordform into its basic
form (as a representative of the lemma). The
functions are defined via two operations on
120
wordforms: remove and concatenate. The
rules have the following form:
if tag = Tag then {remove OldEnd; concatenate
NewEnd}
where Tag is the tag of the wordform, Old-
End is the string which has to be removed
from the end of the wordform and NewEnd
is the string which has to be concatenated to
the beginning of the word form in order to
produce the lemma. The rules are for word
forms in the lexicon. Less than 2% of the
wordforms are ambiguous in the lexicon (but
they are very rare in real texts). Similar rules
are defined for unknown words. The accu-
racy of the lemmatizer is 95.23%.
? Dependency parsing. We have trained the
MALT Parser on the dependency version of
BulTreeBank2. We did this work together
with Svetoslav Marinov who has experience
in using the MALT Parser and Johan Hall
who is involved in thedevelopment of Malt
Parser. The trained model achieves 85.6%
labeled parsing accuracy. It is integrated in
a language pipe with the POS tagger and the
lemmatizer.
After the application of the language pipeline,
the result is represented in a table form following
the CoNLL shared task format3.
4 Factor-based SMT Model
Our approach is built on top of the factor-based
SMT model proposed by Koehn and Hoang
(2007), as an extension of the traditional phrase-
based SMT framework. Instead of using only the
word form of the text, it allows the system to take
a vector of factors to represent each token, both
for the source and target languages. The vec-
tor of factors can be used for different levels of
linguistic annotations, like lemma, part-of-speech
(POS), or other linguistic features. Furthermore,
this extension actually allows us to incorporate
various kinds of features if they can be (somehow)
represented as annotations to the tokens.
The process is quite similar to supertagging
(Bangalore and Joshi, 1999), which assigns ?rich
descriptions (supertags) that impose complex
2http://www.bultreebank.org/dpbtb/
3http://ufal.mff.cuni.cz/conll2009-st/
task-description.html
constraints in a local context?. In our case, all
the linguistic features (factors) associated with
each token form a supertag to that token. Singh
and Bandyopadhyay (2010) had a similar idea
of incorporating linguistic features, while they
worked on Manipuri-English bidirectional trans-
lation. Our approach is slightly different from
(Birch et al, 2007) and (Hassan et al, 2007), who
mainly used the supertags on the target language
side, English. We primarily experiment with the
source language side, Bulgarian. This potentially
huge feature space provides us with various possi-
bilities of using our linguistic resources developed
in and out of our project.
In particular, we consider the following factors
on the source language side (Bulgarian):
? WF - word form is just the original text to-
ken.
? LEMMA is the lexical invariant of the orig-
inal word form. We use the lemmatizer
described in Section 3, which operates on
the output from the POS tagging. Thus,
the 3rd person, plural, imperfect tense verb
form ?varvyaha? (?walking-were?, They were
walking) is lemmatized as the 1st person,
present tense verb ?varvya?.
? POS - part-of-speech of the word. We use
the positional POS tag set of the BulTree-
Bank, where the first letter of the tag indi-
cates the POS itself, while the next letters re-
fer to semantic and/or morphosyntactic fea-
tures, such as: Dm - where ?D? stands for
?adverb?, and ?m? stand for ?modal?; Ncmsi
- where ?N? stand for ?noun?, ?c? means
?common?, ?m? is ?masculine?, ?s? is ?singu-
lar?,and ?i? is ?indefinite?.
? LING - other linguistic features derived from
the POS tag in the BulTreeBank tagset (see
above).
In addition to these, we can also incorporate
syntactic structure of the sentence by breaking
down the tree into dependency relations. For in-
stance, a dependency tree can be represented as
a set of triples in the form of <parent, relation,
child>. <loves, subject, John> and <loves, ob-
ject, Mary> will represent the sentence ?John
loves Mary?. Consequently, three additional fac-
tors are included for both languages:
121
? DEPREL - is the dependency relation be-
tween the current word and the parent node.
? HLEMMA is the lemma of the current word?s
parent node.
? HPOS is the POS tag of the current word?s
parent node.
Here is an example of a processed sentence.
The sentence is ?spored odita v elektricheskite
kompanii politicite zloupotrebyavat s dyrzhavnite
predpriyatiya.? The glosses for the words in
the Bulgarian sentence are: spored (according)
odita (audit-the) v (in) elektricheskite (electrical-
the) kompanii (companies) politicite (politicians-
the) zloupotrebyavat (abuse) s (with) dyrzhavnite
(state-the) predpriyatiya (enterprises). The trans-
lation in the original source is : ?electricity au-
dits prove politicians abusing public companies.?
The result from the linguistic processing and the
addition of information about head elements are
presented in the first seven columns of Table 1.
We extend the grammatical features to have the
same size. All the information is concatenated to
the word forms in the text. In the next section we
present how we extend this format to incorporate
the MRS analysis. In the next section we will ex-
tend this example to incorporate the MRS analysis
of the sentence.
5 MRS Supertagging
Our work on Minimal Recursion Semantic anal-
ysis of Bulgarian text is inspired by the work
on MRS and RMRS (Robust Minimal Recursion
Semantic) (see (Copestake, 2003) and (Copes-
take, 2007)) and the previous work on transfer
of dependency analyses into RMRS structures de-
scribed in (Spreyer and Frank, 2005) and (Jakob
et al, 2010). In this section we present first a short
overview of MRS and RMRS. Then we discuss
the new features added on the basis of the RMRS
structures.
MRS is introduced as an underspecified se-
mantic formalism (Copestake et al, 2005). It is
used to support semantic analyses in the English
HPSG grammar ERG (Copestake and Flickinger,
2000), but also in other grammar formalisms like
LFG. The main idea is that the formalism avoids
spelling out the complete set of readings resulting
from the interaction of scope bearing operators
and quantifiers, instead providing a single under-
specified representation from which the complete
set of readings can be constructed. Here we will
present only basic definitions from (Copestake et
al., 2005). For more details the cited publication
should be consulted. An MRS structure is a tu-
ple ? GT , R, C ?, where GT is the top handle,
R is a bag of EPs (elementary predicates) and C
is a bag of handle constraints, such that there is
no handle h that outscopes GT . Each elementary
predication contains exactly four components: (1)
a handle which is the label of the EP; (2) a rela-
tion; (3) a list of zero or more ordinary variable
arguments of the relation; and (4) a list of zero or
more handles corresponding to scopal arguments
of the relation (i.e., holes). RMRS is introduced
as a modification of MRS which to capture the se-
mantics resulting from the shallow analysis. Here
the following assumption is taken into account the
shallow processor does not have access to a lexi-
con. Thus it does not have access to arity of the
relations in EPs. Therefore, the representation has
to be underspecified with respect to the number
of arguments of the relations. The names of rela-
tions are constructed on the basis of the lemma for
each wordform in the text and the main argument
for the relation is specified. This main argument
could be of two types: referential index for nouns
and event for the other part of speeches.
Because in this work we are using only the
RMRS relation and the type of the main argument
as features to the translation model, we will skip
here the explanation of the full structure of RMRS
structures and how they are constructed. Thus, we
firstly do a match between the surface tokens and
the MRS elementary predicates (EPs) and then
extract the following features as extra factors:
? EP - the name of the elementary predicate,
which usually indicates an event or an entity
semantically.
? EOV indicates the current EP is either an
event or a reference variable.
Notice that we do not take all the information
provided by the MRS, e.g., we throw away the
scopal information and the other arguments of the
relations. This kind of information is not straight-
forward to be represented in such ?tagging?-style
models, which will be tackled in the future.
This information for the example sentence is
122
WF Lemma POSex Ling DepRel HLemma HPOS EP EoV
spored spored R adjunct zloupotrebyavam VP spored r e
odita odit Nc npd prepcomp spored R odit n v
v v R mod odit Nc v r e
elektricheskite elektricheski A pd mod kompaniya Nc elekticheski a e
kompanii kompaniya Nc fpi prepcomp v R kompaniya n v
politicite politik Nc mpd subj zloupotrebyavam Vp politik n v
zloupotrebyavat zloupotrebyavam Vp tir3p root - - zloupotrebyavam v e
s s R indobj zloupotrebyavam Vp s r e
dyrzhavnite dyrzhaven A pd mod predpriyatie Nc dyrzhaven a e
predpriyatiya predpriyatie Nc npi prepcomp s R predpriyatie n v
Table 1: The sentence analysis with added head information ? HLemma and HPOS.
represented for each word form in the last two
columns of Table 1.
All these factors encoded within the corpus
provide us with a rich selection of factors for dif-
ferent experiments. Some of them are presented
within the next section. The model of encoding
MRS information in the corpus as additional fea-
tures does not depend on the actual semantic anal-
ysis ? MRS or RMRS, because both of them pro-
vide enough semantic information.
6 Experiments
6.1 Experiments with the Bulgarian raw
corpus
To run the experiments, we use the phrase-based
translation model provided by the open-source
statistical machine translation system, Moses4
(Koehn et al, 2007). For training the translation
model, the parallel corpora (mentioned in Sec-
tion 2) were preprocessed with the tokenizer and
lowercase converter provided by Moses. Then the
procedure is quite standard:
? We run GIZA++ (Och and Ney, 2003) for bi-
directional word alignment, and then obtain
the lexical translation table and phrase table.
? A tri-gram language model is estimated us-
ing the SRILM toolkit (Stolcke, 2002).
? Minimum error rate training (MERT) (Och,
2003) is applied to tune the weights for the
set of feature weights that maximizes the of-
ficial f-score evaluation metric on the devel-
opment set.
The rest of the parameters we use the default
setting provided by Moses.
4http://www.statmt.org/moses/
We split the corpora into the training set, the
development set and the test set. For SETIMES,
the split is 100,000/500/1,000 and for EMEA, it
is 700,000/500/1,000. For reference, we also run
tests on the JRC-Acquis corpus5. The final results
under the standard evaluation metrics are shown
in the following table in terms of BLEU (Papineni
et al, 2002):
Corpora Test Dev Final Drop
SETIMES? SETIMES 34.69 37.82 36.49 /
EMEA? EMEA 51.75 54.77 51.62 /
SETIMES? EMEA 13.37 / / 61.5%
SETIMES? JRC-Acquis 7.19 / / 79.3%
EMEA? SETIMES 7.37 / / 85.8%
EMEA? JRC-Acquis 9.21 / / 82.2%
Table 2: Results of the baseline SMT system
(Bulgarian-English)
As we mentioned before, the EMEA corpus
is mainly about the description of medicine us-
age, and the format is quite fixed. Therefore, it
is not surprising to see high performance on the
in-domain test (2nd row in Table 2). SETIMES,
consisting of news articles, is in a less controlled
setting. The BLEU score is lower6. The results on
the out-of-domain tests are in general much lower
with a drop of more than 60% in BLEU score (the
last column). For the JRC-Acquis corpus, in con-
trast to the in-domain scores given by Koehn et
al. (2009) (61.3), the low out-of-domain results
shows a very similar situation as EMEA. A brief
manual check of the results indicate that the out-
of-domain tests suffer severely from the missing
5http://optima.jrc.it/Acquis/
6Actually, the BLEU score itself is higher than for most
of the other language pairs http://matrix.statmt.
org/. As the datasets are different, the results are not di-
rectly comparable. Here, we just want to get a rough pic-
ture. Achieving better performance for Bulgarian-to-English
translation than for other language pairs is not the focus of
the paper.
123
lexicon, while the in-domain test for the news arti-
cles contains more interesting issues to look into.
The better translation quality also makes the sys-
tem outputs human readable.
6.2 Experiments with the
Linguistically-Augmented Bulgarian
Corpus
As we described the factor-based model in Sec-
tion 4, we also perform experiments to test the
effectiveness of different linguistic annotations.
The different configurations we considered are
shown in the first column of Table 3.
These models can be roughly grouped into
five categories: word form with linguistic fea-
tures; lemma with linguistic features; models
with dependency features; MRS elementary pred-
icates (EP) and the type of the main argument of
the predicate (EOV); and MRS features without
word forms. The setting of the system is mostly
the same as the previous experiment, except for
1) increasing the training data from 100,000 to
150,000 sentence pairs; 2) specifying the factors
during training and decoding; and 3) without do-
ing MERT7. We perform the finer-grained model
only on the SETIMES data, as the language is
more diverse (compared to the other two corpora).
The results are shown in Table 3.
The first model is served as the baseline here.
We show all the n-gram scores besides the final
BLEU, since the some of the differences are very
small. In terms of the numbers, POS seems to
be an effective factor, as Model 2 has the highest
score. Model 3 indicates that linguistic features
also improve the performance. Model 4-6 show
the necessity of including the word form as one
of the factors, in terms of BLEU scores. Model
10 shows significant decrease after incorporating
HLEMMA feature. This may be due to the data
sparsity, as we are actually aligning and translat-
ing bi-grams instead of tokens. This may also in-
dicate that increasing the number of factors does
not guarantee performance enhancement. After
replacing the HLEMMA with HPOS, the result is
close to the others (Model 8). The experiments
with features from the MRS analyses (Model 11-
16) show improvements over the baseline consis-
tently and using only the MRS features (Model
7This is mainly due to the large amount of computation
required. We will perform MERT on the better-performing
configurations in the future.
17-18) also delivers descent results. In future ex-
periments we will consider to include more fea-
ture from the MRS analyses.
So far, incorporating additional linguistic
knowledge has not shown huge improvement in
terms of statistical evaluation metrics. However,
this does not mean that the translations delivered
are the same. In order to fully evaluate the system,
manual analysis is absolutely necessary. We are
still far from drawing a conclusion at this point,
but the preliminary scores calculated already indi-
cate that the system can deliver decent translation
quality consistently.
6.3 Manual Evaluation
We manually validated the output for all the mod-
els mentioned in Table 3. The guideline in-
cludes two aspects of the quality of the transla-
tion: Grammaticality and Content. Grammati-
cality can be evaluated solely on the system out-
put and Content by comparison with the reference
translation. We use a 1-5 score for each aspect as
follows:
Grammaticality
1. The translation is not understandable.
2. The evaluator can somehow guess the mean-
ing, but cannot fully understand the whole
text.
3. The translation is understandable, but with
some efforts.
4. The translation is quite fluent with some mi-
nor mistakes or re-ordering of the words.
5. The translation is perfectly readable and
grammatical.
Content
1. The translation is totally different from the
reference.
2. About 20% of the content is translated, miss-
ing the major content/topic.
3. About 50% of the content is translated, with
some missing parts.
4. About 80% of the content is translated, miss-
ing only minor things.
5. All the content is translated.
For the missing lexicons or not-translated
Cyrillic tokens, we ask the evaluators to score 2
124
ID Model BLEU 1-gram 2-gram 3-gram 4-gram
1 WF 38.61 69.9 44.6 31.5 22.7
2 WF, POS 38.85 69.9 44.8 31.7 23.0
3 WF, LEMMA, POS, LING 38.84 69.9 44.7 31.7 23.0
4 LEMMA 37.22 68.8 43.0 30.1 21.5
5 LEMMA, POS 37.49 68.9 43.2 30.4 21.8
6 LEMMA, POS, LING 38.70 69.7 44.6 31.6 22.8
7 WF, DEPREL 36.87 68.4 42.8 29.9 21.1
8 WF, DEPREL, HPOS 36.21 67.6 42.1 29.3 20.7
9 WF, LEMMA, POS, LING, DEPREL 36.97 68.2 42.9 30.0 21.3
10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 29.57 60.8 34.9 23.0 15.7
11 WF, POS, EP 38.74 69.8 44.6 31.6 22.9
12 WF, POS, LING, EP 38.76 69.8 44.6 31.7 22.9
13 WF, EP, EOV 38.74 69.8 44.6 31.6 22.9
14 WF, POS, EP, EOV 38.74 69.8 44.6 31.6 22.9
15 WF, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9
16 WF, POS, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9
17 EP, EOV 37.22 68.5 42.9 30.2 21.6
18 EP, EOV, LING 38.38 69.3 44.2 31.3 22.7
Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000)
for one Cyrillic token and score 1 for more than
one tokens in the output translation.
The results are shown in the following two ta-
bles, Table 4 and Table 5, respectively. The cur-
rent results from the manual validation are on the
basis of 150 sentence pairs. The numbers shown
in the tables are the number of sentences given the
corresponding scores. The ?Total? column sums
up the scores of all the output sentences by each
model.
The results show that linguistic and seman-
tic analyses definitely improve the quality of the
translation. Exploiting the linguistic processing
on word level ? LEMMA, POS and LING ? pro-
duces the best result. However, the model with
only EP and EOV features also delivers very good
results, which indicates the effectiveness of the
MRS features from the deep hand-crafted gram-
mars. Including more factors (especially the in-
formation from the dependency parsing) drops the
results because of the sparseness effect over the
dataset, which is consistent with the automatic
evaluation BLEU score. The last two rows are
shown for reference. ?Google? shows the results
of using the online translation service provided by
http://translate.google.com/. The
high score (very close to the reference translation)
may be because our test data are not excluded
from their training data. In future we plan to do
the same evaluation with a larger dataset.
The problem with the untranslated Cyrillic to-
kens in our view could be solved in most of the
cases by providing additional lexical information
from a Bulgarian-English lexicon. Thus, we also
evaluated the possible impact of such a lexicon if
it had been available. In order to do this, we sub-
stituted each copied Cyrillic token with its trans-
lation when there was only one possible transla-
tion. We did such substitutions for 189 sentence
pairs. Then we evaluated the result by classify-
ing the translations as acceptable or unacceptable.
The number of the acceptable translations are 140
in this case.
The manual evaluation of the translation mod-
els on a bigger scale is in progress. The current re-
sults are promising. Statistical evaluation metrics
can give us a brief overview of the system perfor-
mance, but the actual translation quality is much
more interesting to us, as in many cases, the dif-
ferent surface translations can convey exactly the
same meaning in the context.
7 Related Work
Our work is also enlightened by another line of
research, transfer-based MT models, which are
seemingly different but actually very close. In this
section, before we mention some previous work
in this research direction, we firstly introduce the
background of the development of the deep HPSG
grammars.
The MRSes are usually delivered together with
the HPSG analyses of the text. There already
125
ID Model 1 2 3 4 5 Total
1 WF 20 47 5 32 46 487
2 WF, POS 20 48 5 37 40 479
3 WF, LEMMA, POS, LING 20 47 6 34 43 483
4 LEMMA 15 34 11 46 44 520
5 LEMMA, POS 15 38 12 51 34 501
6 LEMMA, POS, LING 20 48 5 34 43 482
7 WF, DEPREL 32 48 3 29 38 443
8 WF, DEPREL, HPOS 45 41 7 23 34 410
9 WF, LEMMA, POS, LING, DEPREL 34 47 5 30 34 433
10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 101 32 0 8 9 242
11 WF, POS, EP 19 49 4 34 44 485
12 WF, POS, LING, EP 19 49 3 39 40 482
13 WF, EP, EOV 20 49 2 41 38 478
14 WF, POS, EP, EOV 19 50 3 31 47 487
15 WF, LING, EP, EOV 19 48 5 37 41 483
16 WF, POS, LING, EP, EOV 19 49 5 37 40 480
17 EP, EOV 15 41 10 44 40 503
18 EP, EOV, LING 20 49 7 38 36 471
19 GOOGLE 0 2 20 52 76 652
20 REFERENCE 0 0 5 51 94 689
Table 4: Manual evaluation of the grammaticality
exist quite extensive implemented formal HPSG
grammars for English (Copestake and Flickinger,
2000), German (Mu?ller and Kasper, 2000), and
Japanese (Siegel, 2000; Siegel and Bender, 2002).
HPSG is the underlying theory of the interna-
tional initiative LinGO Grammar Matrix (Bender
et al, 2002). At the moment, precise and lin-
guistically motivated grammars, customized on
the base of the Grammar Matrix, have been or
are being developed for Norwegian, French, Ko-
rean, Italian, Modern Greek, Spanish, Portuguese,
Chinese, etc. There also exists a first version of
the Bulgarian Resource Grammar - BURGER. In
the research reported here, we use the linguistic
modeled knowledge from the existing English and
Bulgarian grammars. Since the Bulgarian gram-
mar has limited coverage on news data, depen-
dency parsing has been performed instead. Then,
mapping rules have been defined for the construc-
tion of RMRSes.
However, the MRS representation is still quite
close to the syntactic level, which is not fully lan-
guage independent. This requires a transfer at the
MRS level, if we want to do translation from the
source language to the target language. The trans-
fer is usually implemented in the form of rewrit-
ing rules. For instance, in the Norwegian LO-
GON project (Oepen et al, 2004), the transfer
rules were hand-written (Bond et al, 2005; Oepen
et al, 2007), which included a large amount of
manual work. Graham and van Genabith (2008)
and Graham et al (2009) explored the automatic
rule induction approach in a transfer-based MT
setting involving two lexical functional grammars
(LFGs), which was still restricted by the perfor-
mance of both the parser and the generator. Lack
of robustness for target side generation is one of
the main issues, when various ill-formed or frag-
mented structures come out after transfer. Oepen
et al (2007) use their generator to generate text
fragments instead of full sentences, in order to in-
crease the robustness. We want to make use of
the grammar resources while keeping the robust-
ness, therefore, we experiment with another way
of transfer involving information derived from the
grammars.
In our approach, we take an SMT system as our
?backbone? which robustly delivers some trans-
lation for any given input. Then, we augment
SMT with deep linguistic knowledge. In general,
what we are doing is still along the lines of previ-
ous work utilizing deep grammars, but we build a
more ?light-weighted? transfer model.
8 Conclusion and Future Work
In this paper, we report our work on build-
ing a linguistically-augmented statistical machine
translation model from Bulgarian to English.
126
ID Model 1 2 3 4 5 Total
1 WF 20 46 5 23 56 499
2 WF, POS 20 48 5 24 53 492
3 WF, LEMMA, POS, LING 20 47 1 24 58 503
4 LEMMA 15 32 5 33 65 551
5 LEMMA, POS 15 35 9 32 59 535
6 LEMMA, POS, LING 20 48 5 22 55 494
7 WF, DEPREL 32 49 4 14 51 453
8 WF, DEPREL, HPOS 45 41 2 21 41 422
9 WF, LEMMA, POS, LING, DEPREL 34 48 3 20 45 444
10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 101 32 0 6 11 244
11 WF, POS, EP 19 49 3 20 59 501
12 WF, POS, LING, EP 19 50 2 20 59 500
13 WF, EP, EOV 19 50 4 16 61 500
14 WF, POS, EP, EOV 19 50 2 23 56 497
15 WF, LING, EP, EOV 19 48 4 18 61 504
16 WF, POS, LING, EP, EOV 19 50 3 24 54 494
17 EP, EOV 14 38 7 31 60 535
18 EP, EOV, LING 19 49 7 20 55 493
19 GOOGLE 1 0 9 42 98 686
20 REFERENCE 1 0 5 37 107 699
Table 5: Manual evaluation of the content
Based on our observations of the previous ap-
proaches on transfer-based MT models, we de-
cide to build a hybrid system by combining an
SMT system with deep linguistic resources. We
perform a preliminary evaluation on several con-
figurations of the system (with different linguis-
tic knowledge). The high BLEU score shows the
high quality of the translation delivered by the
SMT baseline; and manual analysis confirms the
consistency of the system.
There are various aspects we can improve the
ongoing project: 1) The MRSes are not fully ex-
plored yet, since we have only considered the EP
and EOV features. 2) We would like to add factors
on the target language side (English) as well. 3)
The guideline of the manual evaluation needs fur-
ther refinement for considering the missing lexi-
cons as well as how much of the content is truly
conveyed (Farreu?s et al, 2011). 4) We also need
more experiments to evaluate the robustness of
our approach in terms of out-domain tests.
Acknowledgements
This work was supported by the EuroMatrix-
Plus project (IST-231720) funded by the Euro-
pean Community under the Seventh Framework
Programme for Research and Technological De-
velopment. The authors would like to thank Tania
Avgustinova for fruitful discussions and her help-
ful linguistic analysis; and also to Laska Laskova,
Stanislava Kancheva and Ivaylo Radev for doing
the human evaluation of the data.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertag-
ging: an approach to almost parsing supertagging: an ap-
proach to almost parsing supertagging: an approach to
almost parsing. Computational Linguistics, 25(2), June.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar Matrix. An open-source starter-kit
for the rapid development of cross-linguistically consis-
tent broad-coverage precision grammar. In Proceedings
of the Workshop on Grammar Engineering and Evalua-
tion at the 19th International Conference on Computa-
tional Linguistics, Taipei, Taiwan.
Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007.
Ccg supertags in factored statistical machine translation.
In Proceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 9?16, Prague, Czech Republic,
June.
Francis Bond, Stephan Oepen, Melanie Siegel, Ann Copes-
take, and Dan Flickinger. 2005. Open source machine
translation with DELPH-IN. In Proceedings of the Open-
Source Machine Translation Workshop at the 10th Ma-
chine Translation Summit, pages 15 ? 22, Phuket, Thai-
land, September.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and
Omar F. Zaidan. 2011. Findings of the 2011 workshop
on statistical machine translation. In Proceedings of the
6th Workshop on SMT.
Yu Chen, M. Jellinghaus, A. Eisele, Yi Zhang, S. Hunsicker,
S. Theison, Ch. Federmann, and H. Uszkoreit. 2009.
127
Combining multi-engine translations with moses. In Pro-
ceedings of the 4th Workshop on SMT.
Ann Copestake and Dan Flickinger. 2000. An open source
grammar development environment and broad-coverage
english grammar using hpsg. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation, Athens, Greece.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag.
2005. Minimal recursion semantics: An introduction.
Research on Language & Computation, 3(4):281?332.
Ann Copestake. 2003. Robust minimal recursion semantics
(working paper).
Ann Copestake. 2007. Applying robust semantics. In Pro-
ceedings of the 10th Conference of the Pacific Assocation
for Computational Linguistics (PACLING), pages 1?12.
Mireia Farreu?s, Marta R. Costa-jussa`, and Maja Popovic?
Morse. 2011. Study and correlation analysis of linguis-
tic, perceptual and automatic machine translation evalu-
ations. Journal of the American Society for Information
Sciences and Technology, 63(1):174?184, October.
Georgi Georgiev, Valentin Zhikov, Petya Osenova, Kiril
Simov, and Preslav Nakov. 2012. Feature-rich part-of-
speech tagging for morphologically complex languages:
Application to bulgarian. In Proceedings of EACL 2012.
MIT Press, Cambridge, MA, USA.
Jess Gimnez and Llus Mrquez. 2004. Svmtool: A general
pos tagger generator based on support vector machines.
In Proceedings of the 4th LREC.
Yvette Graham and Josef van Genabith. 2008. Packed rules
for automatic transfer-rule induction. In Proceedings of
the European Association of Machine Translation Con-
ference (EAMT 2008), pages 57?65, Hamburg, Germany,
September.
Yvette Graham, Anton Bryl, and Josef van Genabith. 2009.
F-structure transfer-based statistical machine translation.
In Proceedings of the Lexical Functional Grammar Con-
ference, pages 317?328, Cambridge, UK. CSLI Publica-
tions, Stanford University, USA.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation. In
Proceedings of ACL, Prague, Czech Republic, June.
Max Jakob, Marke?ta Lopatkova?, and Valia Kordoni. 2010.
Mapping between dependency structures and composi-
tional semantic representations. In Proceedings of the
7th International Conference on Language Resources and
Evaluation (LREC 2010), pages 2491?2497.
Philipp Koehn and Hieu Hoang. 2007. Factored translation
models. In Proceedings of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of ACL
(demo session).
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe. In
Proceedings of MT Summit XII.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, January.
Stefan Mu?ller and Walter Kasper. 2000. HPSG analy-
sis of German. In Wolfgang Wahlster, editor, Verbmo-
bil. Foundations of Speech-to-Speech Translation, pages
238 ? 253. Springer, Berlin, Germany, artificial intelli-
gence edition.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer,
Torbj?rn Nordga?rd, , and Victoria Rose?n. 2004. Som a?
kapp-ete med trollet? towards MRS-based norwegian to
english machine translation. In Proceedings of the 10th
International Conference on Theoretical and Method-
ological Issues in Machine Translation, Baltimore, MD.
Stephan Oepen, Erik Velldal, Jan Tore L?nning, Paul
Meurer, Victoria Rose?n, and Dan Flickinger. 2007. To-
wards hybrid quality-oriented machine translation ? on
linguistics and probabilities in MT. In Proceedings of the
11th Conference on Theoretical and Methodological Is-
sues in Machine Translation (TMI-07), Skovde, Sweden.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of ACL.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of japanese. In Proceedings of the 19th
International Conference on Computational Linguistics,
Taipei, Taiwan.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil. Foundations of Speech-
to-Speech Translation, pages 265 ? 280. Springer, Berlin,
Germany, artificial intelligence edition.
Thoudam Doren Singh and Sivaji Bandyopadhyay. 2010.
Manipuri-english bidirectional statistical machine trans-
lation systems using morphology and dependency rela-
tions. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 83?91,
Beijing, China, August.
Kathrin Spreyer and Anette Frank. 2005. Projecting RMRS
from TIGER Dependencies. In Proceedings of the HPSG
2005 Conference, pages 354?363, Lisbon, Portugal.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, volume 2.
Gregor Thurmair. 2005. Hybrid architectures for machine
translation systems. Language Resources and Evalua-
tion, 39(1).
Gregor Thurmair. 2009. Comparing different architectures
of hybrid machine translation systems. In Proceedings of
MT Summit XII.
128
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10?19,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Linguistically-Enriched Models for Bulgarian-to-English Machine
Translation
Rui Wang
Language Technology Lab
DFKI GmbH
Saarbru?cken, Germany
ruiwang@dfki.de
Petya Osenova and Kiril Simov
Linguistic Modelling Department, IICT
Bulgarian Academy of Sciences
Sofia, Bulgaria
{petya,kivs}@bultreebank.org
Abstract
In this paper, we present our linguistically-
enriched Bulgarian-to-English statistical ma-
chine translation model, which takes a sta-
tistical machine translation (SMT) system as
backbone various linguistic features as fac-
tors. The motivation is to take advantages of
both the robustness of the SMT system and
the rich linguistic knowledge from morpho-
logical analysis as well as the hand-crafted
grammar resources. The automatic evaluation
has shown promising results and our extensive
manual analysis confirms the high quality of
the translation the system delivers. The whole
framework is also extensible for incorporating
information provided by different sources.
1 Introduction
Incorporating linguistic knowledge into statistical
models is an everlasting topic in natural language
processing. The same story happens in the ma-
chine translation community. Along with the suc-
cess of statistical machine translation (SMT) models
(summarized by Koehn (2010)), various approaches
have been proposed to include linguistic informa-
tion, ranging from early work by Wu (1997) to re-
cent work by Chiang (2010), from deep transfer-
based models (Graham and van Genabith, 2008) to
mapping rules at the syntactic level (Galley et al,
2004; Liu et al, 2006; Zhang et al, 2008). Although
the purely data-driven approaches achieve signifi-
cant results as shown in the evaluation campaigns
(Callison-Burch et al, 2011), according to the hu-
man evaluation, the final outputs of the SMT sys-
tems are still far from satisfactory.
Koehn and Hoang (2007) proposed a factored
SMT model as an extension of the traditional
phrase-based SMT model, which opens up an easy
way to incorporate linguistic knowledge at the to-
ken level. Birch et al (2007) and Hassan et al
(2007) have shown the effectiveness of adding su-
pertags on the target side, and Avramidis and Koehn
(2008) have focused on the source side, translat-
ing a morphologically-poor language (English) to a
morphologically-rich language (Greek). However,
all of them attempt to enrich the English part of
the language pairs being translated. For the lan-
guage pairs like Bulgarian-English, there has not
been much study on it, mainly due to the lack of
resources, including corpora, preprocessors, etc, on
the Bulgarian part. There was a system published
by Koehn et al (2009), which was trained and tested
on the European Union law data, but not on other
popular domains like news. They reported a very
high BLEU score (Papineni et al, 2002) on the
Bulgarian-English translation direction (61.3).
Apart from being morphologically-rich, Bulgar-
ian has a number of challenging linguistic phenom-
ena to consider, including free word order, long dis-
tance dependency, coreference relations, clitic dou-
bling, etc. For instance, the following two sentences:
(1) Momcheto
Boy-the
j
her-dat
go
it-acc
dava
gives
buketa
bouquet-the
na
to
momicheto.
girl-the.
The boy gives the bouquet to the girl.
(2) Momcheto
Boy-the
j
her-dat
go
it-acc
dava.
gives.
The boy gives it to her.
10
are difficult for the traditional phrase-based SMT
system, because the clitic in the first sentence must
not be translated, while in the second case it is oblig-
atory. Via the semantic analysis (e.g., Minimal Re-
cursion Semantics), the clitic information will be in-
corporated in the representation of the correspond-
ing arguments.
In this work, we rely on the linguistic processing
to cope with some of these phenomena and improve
the correspondences between the two languages: 1)
The lemmatization factors out the difference be-
tween word forms and ensures better coverage of the
Bulgarian-English lexicon. 2) The dependency pars-
ing helps to identify the grammatical functions such
as subject, object in sentences with a non-standard
word order. 3) The semantic analysis provides a fur-
ther abstraction which hides some of the language
specific features. Example of the last is the case of
clitic doubling.
As for the Bulgarian-to-English translation
model, we basically ?annotate? the SMT baseline
with various linguistic features derived from the
preprocessing and hand-crafted grammars. There
are three contributions of this work:
? The models trained on a decent amount of par-
allel corpora output surprisingly good results,
in terms of automatic evaluation metrics.
? The enriched models give us more space for ex-
perimenting with different linguistic features
without losing the ?basic? robustness.
? According to our extensive manual analyses,
the approach has shown promising results for
future integration of more knowledge from the
continued advances of the deep grammars.
The rest of the paper will be organized as fol-
lows: Section 2 briefly introduces some background
of the hand-crafted grammar resources we use and
also some previous related work on transfer-based
MT. Section 3 describes the linguistic analyses we
perform on the Bulgarian text, whose output is used
in the factored SMT model. We show our exper-
iments in Section 4 as well as both automatic and
detailed manual evaluation of the results. We sum-
marize this paper in Section 5 and point out several
directions for future work.
2 Machine Translation with Deep
Grammars
Our work is also enlightened by another line of re-
search, transfer-based MT models using deep lin-
guistic knowledge, which are seemingly different
but actually very related. In this section, before
we describe our model of incorporating linguis-
tic knowledge from the hand-crafted grammars, we
firstly introduce the background of such resources as
well as some previous work on MT using them.
Our usage of Minimal Recursion Semantic
(MRS) analysis of Bulgarian text is inspired by the
work on MRS and RMRS (Robust Minimal Recur-
sion Semantic) (see (Copestake, 2003) and (Copes-
take, 2007)) and the previous work on transfer of de-
pendency analyses into RMRS structures described
in (Spreyer and Frank, 2005) and (Jakob et al,
2010). Although being a semantic representation,
MRS is still quite close to the syntactic level, which
is not fully language independent. This requires a
transfer at the MRS level, if we want to do trans-
lation from the source language to the target lan-
guage. The transfer is usually implemented in the
form of rewriting rules. For instance, in the Nor-
wegian LOGON project (Oepen et al, 2004), the
transfer rules were hand-written (Bond et al, 2005;
Oepen et al, 2007), which included a large amount
of manual work. Graham and van Genabith (2008)
and Graham et al (2009) explored the automatic rule
induction approach in a transfer-based MT setting
involving two lexical functional grammars (LFGs)1,
which was still restricted by the performance of both
the parser and the generator. Lack of robustness for
target side generation is one of the main issues, when
various ill-formed or fragmented structures come
out after transfer. Oepen et al (2007) used their
generator to generate text fragments instead of full
sentences, in order to increase the robustness.
In our approach, we want to make use of the
grammar resources while keeping the robustness,
therefore, we experiment with another way of trans-
fer involving information derived from the gram-
mars. In particular, we take a robust SMT system
as our ?backbone? and then we augment it with deep
linguistic knowledge. In general, what we are doing
1Although their grammars are automatically induced from
treebanks, the formalism supports rich linguistic information.
11
is still along the lines of previous work utilizing deep
grammars, but we build a more ?light-weighted? but
yet extensible statistical transfer model.
3 Factor-based SMT Model
Our translation model is built on top of the factored
SMT model proposed by Koehn and Hoang (2007),
as an extension of the traditional phrase-based SMT
framework. Instead of using only the word form
of the text, it allows the system to take a vector of
factors to represent each token, both for the source
and target languages. The vector of factors can be
used for different levels of linguistic annotations,
like lemma, part-of-speech, or other linguistic fea-
tures, if they can be (somehow) represented as an-
notations to each token.
The process is quite similar to supertagging (Ban-
galore and Joshi, 1999), which assigns ?rich descrip-
tions (supertags) that impose complex constraints in
a local context?. In our case, all the linguistic fea-
tures (factors) associated with each token form a
supertag to that token. Singh and Bandyopadhyay
(2010) had a similar idea of incorporating linguis-
tic features, while they worked on Manipuri-English
bidirectional translation. Our approach is slightly
different from (Birch et al, 2007) and (Hassan et al,
2007), who mainly used the supertags on the target
language side, English. Instead, we primarily ex-
periment with the source language side, Bulgarian.
This potentially huge feature space provides us with
various possibilities of using our linguistic resources
developed within and out of our project.
Firstly, the data was processed by the NLP pipe
for Bulgarian (Savkov et al, 2012) including a mor-
phological tagger, GTagger (Georgiev et al, 2012), a
lemmatizer and a dependency parser2. Then we con-
sider the following factors on the source language
side (Bulgarian):
? WF ? word form is just the original text token.
? LEMMA is the lexical invariant of the original word
form. We use the lemmatizer, which operates on
the output from the POS tagging. Thus, the 3rd per-
son, plural, imperfect tense verb form ?varvyaha?
(?walking-were?, They were walking) is lemmatized
as the 1st person, present tense verb ?varvya?.
2We have trained the MaltParser3 (Nivre et al, 2007)
on the dependency version of BulTreeBank: http://www.
bultreebank.org/dpbtb/. The trained model achieves
85.6% labeled parsing accuracy.
? POS ? part-of-speech of the word. We use the po-
sitional POS tag set of the BulTreeBank, where the
first letter of the tag indicates the POS itself, while
the next letters refer to semantic and/or morphosyn-
tactic features, such as: Dm - where ?D? stands for
?adverb?, and ?m? stand for ?modal?; Ncmsi - where
?N? stand for ?noun?, ?c? means ?common?, ?m? is
?masculine?, ?s? is ?singular?,and ?i? is ?indefinite?.
? LING ? other linguistic features derived from the
POS tag in the BulTreeBank tagset.
? DEPREL is the dependency relation between the
current word and the parent node.
? HLEMMA is the lemma of the parent node.
? HPOS is the POS tag of the parent node.
Here is an example of a processed sentence. The
sentence is ?spored odita v elektricheskite kompanii
politicite zloupotrebyavat s dyrzhavnite predpriy-
atiya.? The glosses for the words in the Bulgarian
sentence are: spored (according) odita (audit-the) v
(in) elektricheskite (electrical-the) kompanii (com-
panies) politicite (politicians-the) zloupotrebyavat
(abuse) s (with) dyrzhavnite (state-the) predpriy-
atiya (enterprises). The translation in the original
source is : ?electricity audits prove politicians abus-
ing public companies.? The result from the linguistic
processing are presented in Table 1.
As for the deep linguistic knowledge, we also ex-
tract features from the semantic analysis ? Minimal
Recursion Semantics (MRS). MRS is introduced as
an underspecified semantic formalism (Copestake et
al., 2005). It is used to support semantic analyses
in the English HPSG grammar ERG (Copestake and
Flickinger, 2000), but also in other grammar for-
malisms like LFG. The main idea is that the for-
malism avoids spelling out the complete set of read-
ings resulting from the interaction of scope bearing
operators and quantifiers, instead providing a single
underspecified representation from which the com-
plete set of readings can be constructed. Here we
will present only basic definitions from (Copestake
et al, 2005). For more details the cited publication
should be consulted.
An MRS structure is a tuple ? GT , R, C ?, where
GT is the top handle, R is a bag of EPs (ele-
mentary predicates) and C is a bag of handle con-
straints, such that there is no handle h that outscopes
GT . Each elementary predicate contains exactly
four components: 1) a handle which is the label of
12
No WF Lemma POS Ling DepRel HLemma HPOS
1 spored spored R adjunct zloupotrebyavam VP
2 odita odit Nc npd prepcomp spored R
3 v v R mod odit Nc
4 elektricheskite elektricheski A pd mod kompaniya Nc
5 kompanii kompaniya Nc fpi prepcomp v R
6 politicite politik Nc mpd subj zloupotrebyavam Vp
7 zloupotrebyavat zloupotrebyavam Vp tir3p root - -
8 s s R indobj zloupotrebyavam Vp
9 dyrzhavnite dyrzhaven A pd mod predpriyatie Nc
10 predpriyatiya predpriyatie Nc npi prepcomp s R
Table 1: The sentence analysis with added head information ? HLemma and HPOS.
No EP EoV EP1 /POS1 EP2 /POS2 EP3 /POS3
1 spored r e zloupotrebyavam v/Vp odit n/Nc -
2 odit n v - - -
3 v r e odit n/Nc kompaniya n/Nc -
4 elekticheski a e kompaniya n/Nc - -
5 kompaniya n v - - -
6 politik n v - - -
7 zloupotrebyavam v e politik n/Nc - s r/R
8 s r e zloupotrebyavam v/Vp predpriyatie n/Nc -
9 dyrzhaven a e predpriyatie n/Nc - -
10 predpriyatie n v - - -
Table 2: Representation of MRS factors for each wordform in the sentence.
the EP; 2) a relation; 3) a list of zero or more or-
dinary variable arguments of the relation; and 4) a
list of zero or more handles corresponding to scopal
arguments of the relation (i.e., holes).
Robust MRS (RMRS) is introduced as a modifica-
tion of MRS which captures the semantics resulting
from the shallow analysis. Here the following as-
sumption is taken into account: the shallow proces-
sor does not have access to a lexicon. Thus it does
not have access to the arity of the relations in EPs.
Therefore, the representation has to be underspeci-
fied with respect to the number of arguments of the
relations. The names of relations are constructed on
the basis of the lemma for each wordform in the text
and the main argument for the relation is specified.
This main argument could be of two types: referen-
tial index for nouns and event for the other parts of
speech. Because in this work we are using only the
RMRS relation and the type of the main argument as
features to the translation model, we will skip here
the explanation of the full RMRS structures and how
they are constructed.
As for the factors, we firstly do a match between
the surface tokens and the MRS elementary predi-
cates (EPs) and then extract the following features
as extra factors:
? EP ? the name of the elementary predicate, which
usually indicates an event or an entity semantically.
? EOV indicates the current EP is either an event or a
reference variable.
? ARGnEP indicates the elementary predicate of the
argument which belongs to the predicate. n is usu-
ally from 1 to 3.
? ARGnPOS indicates the POS tag of the argument
which belongs to the predicate.
Notice that we do not take all the information pro-
vided by the MRS, e.g., we throw away the scopal
information and the other arguments of the relations.
Those kinds of information is not straightforward to
be represented in such ?tagging?-style models, which
will be tackled in the future.
The extra information for the example sentence
is represented in Table 2. All these factors encoded
13
within the corpus provide us with a rich selection of
features for different experiments.
4 Experiments
To run the experiments, we use the phrase-based
translation model provided by the open-source sta-
tistical machine translation system, Moses4 (Koehn
et al, 2007). For training the translation model,
the SETIMES parallel corpus has been used, which
is part of the OPUS parallel corpus5. As for the
choice of the datasets, the language is more diverse
in the news articles, compared with other corpora in
more controlled settings, e.g., the JRC-Acquis cor-
pus6 used by Koehn et al (2009).
We split the corpus into the training set and the
test set by 150,000 and 1,000 sentence pairs re-
spectively7. Both datasets are preprocessed with
the tokenizer and lowercase converter provided by
Moses. Then the procedure is quite standard: We
run GIZA++ (Och and Ney, 2003) for bi-directional
word alignment, and then obtain the lexical trans-
lation table and phrase table. A tri-gram language
model is estimated using the SRILM toolkit (Stol-
cke, 2002). For the rest of the parameters we use the
default setting provided by Moses.
Notice that, since on the target language side (i.e.,
English) we do not have any other factors than the
word form, the factor-based models we use here
only differentiate from each other in the translation
phase, i.e., there is no ?generation? models involved.
4.1 Automatic Evaluation Metrics
The baseline results (non-factored model) under the
standard evaluation metrics are shown in the first
row of Table 3 in terms of BLEU (Papineni et al,
2002) and METEOR (Denkowski and Lavie, 2011).
We then design various configurations to test the
effectiveness of different linguistic annotations de-
scribed in Section 3. The detailed configurations we
considered are shown in the first column of Table 3.
The first impression is that the BLEU scores in
general are high. These models can be roughly
4http://www.statmt.org/moses/
5OPUS ? an open source parallel corpus, http://
opus.lingfil.uu.se/
6http://optima.jrc.it/Acquis/
7We did not preform MERT (Och, 2003), as it is quite com-
putationally heavy for such various configurations.
grouped into six categories (separated by double
lines): word form with linguistic features; lemma
with linguistic features; models with dependency
features; MRS elementary predicates (EP) and the
type of the main argument of the predicate (EOV);
EP features without word forms; and EP features
with MRS ARGn features.
In terms of the resulting scores, POS and Lemma
seem to be effective features, as Model 2 has the
highest BLEU score and Model 4 the best METEOR
score. Model 3 indicates that linguistic features also
improve the performance. Model 4-6 show the ne-
cessity of including the word form as one of the
factors. Incorporating HLEMMA feature largely de-
creases the results due to the vastly increasing vo-
cabulary, i.e., aligning and translating bi-grams in-
stead of tokens. Therefore, we did not include the
results in the table. After replacing the HLEMMA
with HPOS, the result is close to the others (Model
8). Model 9 may also indicate that increasing the
number of factors does not guarantee performance
enhancement. The experiments with predicate fea-
tures (EP and EOV) from the MRS analyses (Model
10-12) show improvements over the baseline con-
sistently and using only the MRS features (Model
13-14) also delivers descent results. Concerning
the MRS ARGn features, the models with ARGnEP
again suffer from the sparseness problem as the de-
pendency HLEMMA features, but the models with
ARGnPOS (Model 15-16) achieve better perfor-
mance than those with dependency HPOS features.
This is mainly because the dependency information
is encoded together with the (syntactically) depen-
dent word, while the MRS arguments are grouped
around the semantic heads.
So far, incorporating additional linguistic knowl-
edge has not shown huge improvement in terms of
statistical evaluation metrics. However, this does not
mean that the translations delivered are the same. In
order to fully evaluate the system, manual analysis is
absolutely necessary. We are still far from drawing a
conclusion at this point, but the automatic evaluation
scores already indicate that the system can deliver
decent translation quality consistently.
4.2 Manual Evaluation
We manually validated the output for all the models
mentioned in Table 3. The guideline includes two
14
ID Model BLEU 1-gram 2-gram 3-gram 4-gram METEOR
1 WF (Baseline) 38.61 69.9 44.6 31.5 22.7 0.3816
2 WF, POS 38.85 69.9 44.8 31.7 23.0 0.3812
3 WF, LEMMA, POS, LING 38.84 69.9 44.7 31.7 23.0 0.3803
4 LEMMA 37.22 68.8 43.0 30.1 21.5 0.3817
5 LEMMA, POS 37.49 68.9 43.2 30.4 21.8 0.3812
6 LEMMA, POS, LING 38.70 69.7 44.6 31.6 22.8 0.3800
7 WF, DEPREL 36.87 68.4 42.8 29.9 21.1 0.3627
8 WF, DEPREL, HPOS 36.21 67.6 42.1 29.3 20.7 0.3524
9 WF, LEMMA, POS, LING, DEPREL 36.97 68.2 42.9 30.0 21.3 0.3610
10 WF, POS, EP 38.74 69.8 44.6 31.6 22.9 0.3807
11 WF, EP, EOV 38.74 69.8 44.6 31.6 22.9 0.3807
12 WF, POS, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9 0.3802
13 EP, EOV 37.22 68.5 42.9 30.2 21.6 0.3711
14 EP, EOV, LING 38.38 69.3 44.2 31.3 22.7 0.3691
15 EP, EOV, ARGnPOS 36.21 67.4 41.9 29.2 20.9 0.3577
16 WF, EP, EOV, ARGnPOS 37.37 68.4 43.2 30.3 21.8 0.3641
Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000/1,000)
aspects of the quality of the translation: Grammati-
cality and Content. Grammaticality can be evaluated
solely on the system output and Content by compar-
ison with the reference translation. We use a 1-5
score for each aspect as follows:
Grammaticality
1. The translation is not understandable.
2. The evaluator can somehow guess the meaning, but
cannot fully understand the whole text.
3. The translation is understandable, but with some ef-
forts.
4. The translation is quite fluent with some minor mis-
takes or re-ordering of the words.
5. The translation is perfectly readable and grammati-
cal.
Content
1. The translation is totally different from the refer-
ence.
2. About 20% of the content is translated, missing the
major content/topic.
3. About 50% of the content is translated, with some
missing parts.
4. About 80% of the content is translated, missing only
minor things.
5. All the content is translated.
For the missing lexicons or not-translated Cyril-
lic tokens, we ask the evaluators to score 2 for one
Cyrillic token and score 1 for more than one tokens
in the output translation. We have two annotators
achieving the inter-annotator agreement according
to Cohen?s Kappa (Cohen, 1960) ? = 0.73 for gram-
maticality and ? = 0.75 for content, both of which
are substantial agreement. For the conflict cases,
we take the average value of both annotators and
rounded the final score up or down in order to have
an integer.
The current results from the manual validation
are on the basis of randomly sampled 150 sentence
pairs. The numbers shown in Table 4 are the number
of sentences given the corresponding scores. The
?Sum? column shows the average score of all the out-
put sentences by each model and the ?Final? column
shows the average of the two ?Sum? scores.
The results show that linguistic and semantic
analyses definitely improve the quality of the trans-
lation. Exploiting the linguistic processing on
word level ? LEMMA, POS and LING ? pro-
duces the best result. However, the model with
only EP and EOV features also delivers very good
results, which indicates the effectiveness of the
MRS features from the deep hand-crafted gram-
mars, although incorporating the MRS ARGn fea-
tures shows similar performance drops as depen-
dency features. Including more factors in general
reduces the results because of the sparseness effect
over the dataset, which is consistent with the au-
tomatic evaluation. The last two rows are shown
15
ID Model
Grammaticality Content
Final
1 2 3 4 5 Sum 1 2 3 4 5 Sum
1 WF (Baseline) 20 47 5 32 46 3.25 20 46 5 23 56 3.33 3.29
2 WF, POS 20 48 5 37 40 3.19 20 48 5 24 53 3.28 3.24
3 WF, LEMMA, POS, LING 20 47 6 34 43 3.22 20 47 1 24 58 3.35 3.29
4 LEMMA 15 34 11 46 44 3.47 15 32 5 33 65 3.67 3.57
5 LEMMA, POS 15 38 12 51 34 3.34 15 35 9 32 59 3.57 3.45
6 LEMMA, POS, LING 20 48 5 34 43 3.21 20 48 5 22 55 3.29 3.25
7 WF, DEPREL 32 48 3 29 38 2.95 32 49 4 14 51 3.02 2.99
8 WF, DEPREL, HPOS 45 41 7 23 34 2.73 45 41 2 21 41 2.81 2.77
9 WF, LEMMA, POS, LING, DEPREL 34 47 5 30 34 2.89 34 48 3 20 45 2.96 2.92
10 WF, POS, EP 19 49 4 34 44 3.23 19 49 3 20 59 3.34 3.29
11 WF, EP, EOV 20 49 2 41 38 3.19 19 50 4 16 61 3.33 3.26
12 WF, POS, LING, EP, EOV 19 49 5 37 40 3.20 19 50 3 24 54 3.29 3.25
13 EP, EOV 15 41 10 44 40 3.35 14 38 7 31 60 3.57 3.46
14 EP, EOV, LING 20 49 7 38 36 3.14 19 49 7 20 55 3.29 3.21
15 EP, EOV, ARGnPOS 23 49 9 34 35 3.06 23 47 8 33 39 3.12 3.09
16 WF, EP, EOV, ARGnPOS 34 47 10 30 29 2.82 34 47 10 20 39 2.89 2.85
* GOOGLE 0 2 20 52 76 4.35 1 0 9 42 98 4.57 4.46
* REFERENCE 0 0 5 51 94 4.59 1 0 5 37 107 4.66 4.63
Table 4: Manual evaluation of the grammaticality and the content
for reference. ?Google? shows the results of using
the online translation service provided by http://
translate.google.com/ on 06.02.2012. The
high score (very close to the reference translation)
may be because our test data are not excluded from
their training data. In future we plan to do the same
evaluation with a larger dataset.
Concerning the impact from the linguistic pro-
cessing pipeline to the final translation results,
Lemma and MRS elementary predicates help at the
level of rich morphology. For example, the baseline
model correctly translates the adjective ?Egyptian?
in ?Egyptian Scientists? (plural), but not in ?Egyp-
tian Government, as in the second phrase the adjec-
tive has a neutral gender. Model 4 and Model 13 are
correct for both.
Generally speaking, if we roughly divide the lin-
guistic processing pipeline in two categories: statis-
tical processing (POS tagger and dependency parser)
and rule-based processing (lemmatizer and MRS
construction), the latter category (almost perfect)
highly relies on the former one. For example, the
lemma depends on the word form and the tag, and
the result is unambiguous in more than 98% of the
morphological lexicon and in text this is almost
100% (because the ambiguous cases are very rare).
The errors come mainly from new words and errors
in the tagger. Similarly, the RMRS rules are good
when the parser is correct. Here, the main problems
are duplications of the ROOT elements and the sub-
ject elements, which we plan to fix using heuristics
in the future.
4.3 Question-Based Evaluation
Although the reported manual evaluation in the pre-
vious section demonstrates that linguistic knowl-
edge improves the translation, we notice that the
evaluators tend to give marks at the two ends of
scale, and less in the middle. Generally, this is
because the measurement is done on the basis of
the content that the evaluators extract from the Bul-
garian sentence using there own cognitive capacity.
Then they start to overestimate or underestimate the
translation, knowing in advance what has to be trans-
lated. In order to avoid this subjectivity, we design
a different manual evaluation in which the evalua-
tor does not know the original Bulgarian sentences.
Then the evaluation is based only on the content rep-
resented within the English translation.
In order to do this, we represent the content of the
Bulgarian sentences as a set of questions that have
a list of possible answers, assigned to them. During
the judgement of the content transfer, the evaluators
16
need to answer these questions. As the list of an-
swers also contains false answers, the evaluators are
forced to select the right answer which can be in-
ferred from the English translation.
The actual questions are created semi-
automatically from the dependency analysis of
the sentences. We defined a set of rules for genera-
tion of the questions on the basis of the dependency
relations. For example, if a sentence has only a
subject relation presented within the analysis, the
question will be about who is doing the event. If
the analysis presents subject and direct object, the
question will be about who is doing something with
what/whom. These automatically generated ques-
tions are manually investigated and, if necessary,
edited. Also, additional answers are formulated on
the basis of general language knowledge. The main
idea is that the possible answers are conceptually
close to each other, but not in a hypernymy relation.
Always there is an answer ?none?.
Then the questions are divided into small groups
and distributed to be answered by three evaluators
in such a way that each question is answered by two
evaluators, but no evaluator answers the whole set of
questions for a given sentence. In this way, we try
to minimize the influence of one question to the an-
swers of the next questions. The answers are com-
pared to the true answers of the questions for each
given sentence. We evaluated 192 questions for each
model and sum up the scores (correctly answered
questions) in Table 5.
This evaluation is more expensive, but we expect
them to be more objective. As for a related work,
(Yuret et al, 2010) used textual entailment to eval-
uate different parser outputs. The way they con-
structed the hypotheses is similar to our creation of
questions (based on dependency relations). How-
ever, they focused on the automatic evaluation and
we adopt it for the manual evaluation.
5 Conclusion and Future Work
In this paper, we report our work on building a
linguistically-enriched statistical machine transla-
tion model from Bulgarian to English. Based on our
observations of the previous approaches on transfer-
based MT models, we decide to build a factored
model by feeding an SMT system with deep lin-
ID Model Score
1 WF (Baseline) 127
2 WF, POS 126
3 WF, LEMMA, POS, LING 131
4 LEMMA 133
5 LEMMA, POS 133
6 LEMMA, POS, LING 128
7 WF, DEPREL 131
8 WF, DEPREL, HPOS 120
9 WF, LEMMA, POS, LING, DEPREL 124
10 WF, POS, EP 125
11 WF, EP, EOV 126
12 WF, POS, LING, EP, EOV 128
13 EP, EOV 138
14 EP, EOV, LING 122
15 EP, EOV, ARGnPOS 130
16 WF, EP, EOV, ARGnPOS 121
Table 5: Question-based evaluation
guistic features. We perform various experiments on
several configurations of the system (with different
linguistic knowledge). The high BLEU score shows
the high quality of the translation delivered by the
SMT baseline; and various manual analyses confirm
the consistency of the system.
There are various aspects of the current approach
we can improve: 1) The MRSes are not fully ex-
plored yet, although we have considered the most
important predicate and argument features. 2) We
would like to add factors on the target language side
(English) as well to fulfill a ?complete? transfer. 3)
Incorporating reordering rules on the Bulgarian side
may help the alignment and larger language mod-
els on the English side should also help improving
the translation results. 4) Due to the morphologi-
cal complexity of the Bulgarian language, the other
translation direction, from Bulgarian to English, is
also worth investigation in this framework.
Acknowledgements
This work was partially supported by the EuroMa-
trixPlus project (IST-231720) funded by the Euro-
pean Community?s Seventh Framework Programme.
The authors would like to thank Laska Laskova,
Stanislava Kancheva and Ivaylo Radev for doing the
human evaluation of the data.
17
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2), June.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. Ccg supertags in factored statistical machine
translation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 9?16, Prague,
Czech Republic, June.
Francis Bond, Stephan Oepen, Melanie Siegel, Ann
Copestake, and Dan Flickinger. 2005. Open source
machine translation with DELPH-IN. In Proceedings
of the Open-Source Machine Translation Workshop at
the 10th Machine Translation Summit, pages 15 ? 22,
Phuket, Thailand, September.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the 6th Workshop on SMT.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL, pages 1443?
1452.
J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37.
Ann Copestake and Dan Flickinger. 2000. An open
source grammar development environment and broad-
coverage english grammar using hpsg. In Proceedings
of the 2nd International Conference on Language Re-
sources and Evaluation, Athens, Greece.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics: An in-
troduction. Research on Language & Computation,
3(4):281?332.
Ann Copestake. 2003. Robust minimal recursion seman-
tics (working paper).
Ann Copestake. 2007. Applying robust semantics. In
Proceedings of the 10th Conference of the Pacific As-
socation for Computational Linguistics (PACLING),
pages 1?12.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Proceedings of HLT-NAACL, Boston, Massachusetts,
USA, May.
G. Georgiev, V. Zhikov, P. Osenova, K. Simov, and
P. Nakov. 2012. Feature-rich part-of-speech tagging
for morphologically complex languages: Application
to bulgarian. In EACL 2012.
Yvette Graham and Josef van Genabith. 2008. Packed
rules for automatic transfer-rule induction. In Pro-
ceedings of the European Association of Machine
Translation Conference (EAMT 2008), pages 57?65,
Hamburg, Germany, September.
Y. Graham, A. Bryl, and J. van Genabith. 2009. F-
structure transfer-based statistical machine translation.
In Proceedings of the Lexical Functional Grammar
Conference, pages 317?328, Cambridge, UK. CSLI
Publications, Stanford University, USA.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL, Prague, Czech Republic, June.
Max Jakob, Marke?ta Lopatkova?, and Valia Kordoni.
2010. Mapping between dependency structures and
compositional semantic representations. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC 2010), pages 2491?
2497.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL (demo session).
P. Koehn, A. Birch, and R. Steinberger. 2009. 462 ma-
chine translation systems for europe. In Proceedings
of MT Summit XII.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, January.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: a language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
18
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, , and Victoria Rose?n.
2004. Som a? kapp-ete med trollet? towards MRS-
based norwegian to english machine translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, MD.
Stephan Oepen, Erik Velldal, Jan Tore L?nning, Paul
Meurer, Victoria Rose?n, and Dan Flickinger. 2007.
Towards hybrid quality-oriented machine translation
? on linguistics and probabilities in MT. In Pro-
ceedings of the 11th Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-
07), Skovde, Sweden.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
Aleksandar Savkov, Laska Laskova, Stanislava
Kancheva, Petya Osenova, and Kiril Simov. 2012.
Linguistic processing pipeline for bulgarian. In
Proceedings of LREC, Istanbul, Turkey.
Thoudam Doren Singh and Sivaji Bandyopadhyay.
2010. Manipuri-english bidirectional statistical ma-
chine translation systems using morphology and de-
pendency relations. In Proceedings of the Fourth
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 83?91, Beijing, China, August.
Kathrin Spreyer and Anette Frank. 2005. Projecting
RMRS from TIGER Dependencies. In Proceedings of
the HPSG 2005 Conference, pages 354?363, Lisbon,
Portugal.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, volume 2.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Septem-
ber.
Deniz Yuret, Ayd?n Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the SemEval-2010
Evaluation Exercises on Semantic Evaluation.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proceedings of ACL-HLT, pages 559?567.
19
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 15?25 Dublin, Ireland, August 23-29 2014.
Joint Ensemble Model for POS Tagging and Dependency Parsing
Iliana Simova Dimitar Vasilev Alexander Popov
Linguistic Modelling Laboratory, IICT-BAS
Sofia, Bulgaria
{iliana|dvasilev|alex.popov|kivs|petya}@bultreebank.org
Kiril Simov Petya Osenova
Abstract
In this paper we present several approaches towards constructing joint ensemble models for mor-
phosyntactic tagging and dependency parsing for a morphologically rich language ? Bulgarian.
In our experiments we use state-of-the-art taggers and dependency parsers to obtain an extended
version of the treebank for Bulgarian, BulTreeBank, which, in addition to the standard CoNLL
fields, contains predicted morphosyntactic tags and dependency arcs for each word. In order to
select the most suitable tag and arc from the proposed ones, we use several ensemble techniques,
the result of which is a valid dependency tree. Most of these approaches show improvement over
the results achieved individually by the tools for tagging and parsing.
1 Introduction
Language processing pipelines are the standard means for preprocessing natural language text for various
natural language processing (NLP) tasks. A typical pipeline applies the following modules sequentially:
a tokenizer, a part-of-speech (POS) tagger, a lemmatizer, and a parser. The main drawback of such
an architecture is that the erroneous output of one module in the pipeline propagates through to its final
step. This usually has a more significant impact on the processing of languages with segmentation issues,
like Chinese, or languages with rich morphological systems, like the Slavic and Romance ones, which
exhibit greater morphological and syntactic ambiguity due to the high number of word forms and freer
word order.
In this paper we present several experiments in which we simultaneously solve two of the aforemen-
tioned tasks ? tagging and parsing. The motivation behind this idea is that the two tasks are highly
dependent on each other when working with a morphologically rich language, and thus a better solution
could be found for each if they are solved jointly. We assemble the outputs of three morphosyntactic tag-
gers (POS taggers) and five dependency parsers in a single step. The ensemble approach uses weights in
order to select the best solution from a number of alternatives. We follow (Surdeanu and Manning, 2010)
and use two classes of approaches for selecting weights for the alternatives: voting, where the weights
are assigned by simple calculations over the number of used models and their performance measures;
machine learning weighting1, where machine learning is exploited in order to rank the alternatives on
the basis of a joint feature model. We refer to both types of approaches as ranking. The language of
choice in our experiments is Bulgarian, but the techniques presented here are easily applicable to other
languages, given the availability of training data.
The interaction between the two levels ? morphology and syntax ? is carried out via a joint model of
features for machine learning. Its aim is to determine the best possible combination out of the predictions
of the different taggers and dependency parsers. Working only with the outputs of the taggers and parsers,
instead of considering all possibilities for tag, head and syntactic relation for each word in the sentence,
reduces the search space and allows us to experiment with more complex features. One limitation of
this approach is that the correct combination of an POS tag and a dependency arc might not have been
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1Surdeanu and Manning (Surdeanu and Manning, 2010) call them meta-classification.
15
predicted by any of the tools in the first place. Therefore the ensemble approach can be beneficial only
to a certain extent.
The data used throughout our experiments consists of the dependency conversion2 of the HPSG-based
Treebank of Bulgarian ? the BulTreeBank. This data set contains non-projective dependency trees, which
are more suitable for describing the relatively free word order of Bulgarian sentences.
The structure of the paper is as follows: in Section 2 we introduce related work on joint models and
ensemble models; in Section 3 we introduce related work on Bulgarian parsing and POS tagging; in
Section 4 we present our ensemble model; in Section 5 we report on our current experimental setup,
including the construction of a parsebank of parses and tagging results; Section 6 presents the results
from our ensemble experiments; the last section concludes the paper.
2 Related Work
Our work on ensemble systems for dependency parsing is inspired by the in-depth performance analysis
of two of the most influential dependency parsing models: transition-based and graph-based (McDonald
and Nivre, 2007). This analysis shows that the two frameworks make different errors when trained and
tested on the same datasets. The authors conclude the paper by proposing three approaches for using the
advantages of both frameworks: (1) ensemble systems ? weighted combinations of the output of both
systems; (2) hybrid systems ? a single system designed to integrate the strengths of the individual ones;
and (3) novel approaches ? based on a combination of new training and inference methods. In their
further work (Nivre and McDonald, 2008) on the subject they present a hybrid system that combines
the two models. The work presented in this paper is along the lines of their first suggestion ? a system
to facilitate the combination of the outputs of several parsing and tagging models, in order to find an
optimal solution.
An experiment with ensemble systems is presented in (Surdeanu and Manning, 2010). This work
describes several approaches to the combination of dependency parsers via different types of voting
and meta-classification. Voting determines the correct dependency arcs by choosing the ones that are
selected by the majority of parsers. Weighted voting uses the accuracy of each parser in order to choose
between their predictions for each arc. We also employ these two ranking techniques in our current
experiment. Surdeanu and Manning (Surdeanu and Manning, 2010) conclude that meta-classification
does not improve the results in comparison to voting. They divide the dependencies in two categories:
majority dependencies and minority dependencies. Their conclusion is that meta-classification cannot
provide a better selection of minority dependencies, and in this way is comparable to voting. In our
work we show that depending on the feature selection for meta-classification, it can actually outperform
the voting approach. The experiments presented in (Surdeanu and Manning, 2010) do not use a specific
algorithm for the selection of dependencies, and do not ensure that the result of voting is a well-formed
dependency tree. In our work we use two algorithms to ensure the construction of trees. We show that
the results also depend on the algorithm for tree construction.
Joint models have been successfully used for processing other morphologically rich languages. For
instance, (Lee et al., 2011) propose a joint model for inference of morphological properties and syntactic
structures, which outperforms a standard pipelined solution when tested on highly-inflected languages
such as Latin, Czech, Ancient Greek and Hungarian. It uses a graphical model that employs ?local? and
?link? factors to impose local word context constraints and to handle long-distance dependencies.
(Cohen and Smith, 2007) and (Goldberg and Tsarfaty, 2008) focus on a joint model for morphological
segmentation and syntactic parsing with application to Hebrew. The authors argue that syntactic context
is crucial for the correct segmentation of tokens into lexemes and that a model wherein the segmenta-
tion and parsing modules share information during processing is better suited to carry out the task. To
solve the two tasks jointly, the different morphological analyses of a given utterance are represented
simultaneously in a lattice structure; a path through the lattice corresponds to a specific morphologi-
cal segmentation of the utterance. In (Cohen and Smith, 2007), paths in the lattice and parse trees are
combined through a joint probability model and the best combination is found through chart parsing.
2www.bultreebank.org/dpbtb/
16
(Hatori et al., 2012) employ an incremental joint approach to solve three tasks in Chinese: word
segmentation, POS tagging, and dependency parsing. The motivation for solving them simultaneously
is that some segmentation ambiguities in the language cannot be resolved without considering the sur-
rounding grammatical constructions, while syntactic information can improve the segmentation of out-
of-vocabulary words. Parsing is done through a dynamic programming framework ? a version of the
shift-reduce algorithm.
Joint morphological and syntactic analysis of several morphologically rich languages is presented in
(Bohnet et al., 2013). They use an extended transition system for dependency parsing to incorporate POS
tagging, tagging with morphological descriptions and lemmas. In addition they define new evaluation
metrics. They include the standard POS accuracy, Labeled and Unlabled Arc Accuracy, but also accuracy
of combination of features like POS tags, morphological description, lemmas and dependency arcs.
Several experiments with different parameters controlling the selection of best tags and morphosyntactic
descriptions are presented.
The approach presented in our work is joint in the sense that we solve two tasks simultaneously ? the
choice for POS tag is dependent on the choice for dependency arc, and vice versa. However, our approach
is also ensemble, since it combines the outputs of several systems for solving the two tasks, instead of
exploring the whole search space of all combinations of tags and arcs. In this way, the approach is better
described as a joint ensemble model.
3 Related Work on Bulgarian
Bulgarian is still under-studied with respect to parsing. Although several systems were trained on the
BulTreeBank treebank during the CoNLL-X 2006 Shared Task (Buchholz and Marsi, 2006) and after it,
a pipeline including a dependency parser with state-of-the-art performance does not exist. A state-of-
the-art POS tagger with nearly 98% accuracy is available for Bulgarian (Georgiev et al., 2012). The best
result for dependency parsing of Bulgarian reported in literature is 93.5% UAS (Martins et al., 2011).
The best result for a pipeline including POS tagging and dependency parsing for Bulgarian is not known
because most available tools were trained on the whole BulTreeBank and there is no way to measure
their actual performance.
Our work is motivated by previous efforts to solve several NLP tasks simultaneously with application
to Bulgarian (Zhikov et al., 2013). The presented joint model for POS tagging, dependency parsing,
and co-reference resolution achieved results comparable to a state-of-the-art pipeline with respect to
dependency parsing. This pipeline, however, used gold standard POS tags as input to the parser. Note
that in the current work we do not rely on gold standard POS tags in the dependency parsing step in order
to achieve more realistic results.
The usefulness of the ensemble parsing approach for Bulgarian is investigated in our previous works
? (Simov et al., 2013) and (Simov et al., 2014). We trained 21 dependency parsing models with different
configurations (parsing algorithm settings and features), including 12 MaltParser (Nivre et al., 2006)
models, 6 MSTParser (McDonald, 2006) models, two TurboParser (Martins et al., 2010) models, and
one Mate-tools parser (Bohnet, 2010) model. The best achieved ensemble result was 93,63% UAS, but
gold POS tags were used as input for parsing. In our current work the best result is slightly lower, but
more realistic, since no gold POS tags were used.
In this work as well as in the previous mentioned works we use Chu-Liu-Edmonds algorithm for maxi-
mum spanning tree as implemented in the MSTParser to ensure the construction of complete dependency
trees. In (Zhikov et al., 2013), POS tags and the co-referential chains are encoded as an extension of the
dependency tree in order to apply the same algorithm. We make use of this representation in the current
work, as described in the following lines.
4 Ensemble Model
Our ensemble model works over extended dependency trees and graphs. First we define a dependency
tree. Then we extend the dependency tree to include alternative POS tags for each wordform node and
alternative dependency arcs.
17
Let us have a set D of dependency tags (ROOT ?D) and a sentence x = w1, ...,wn. A dependency tree
is a tree T = (V,A,?) where:
1. V = {0,1, ...,n} is an ordered set of nodes, that corresponds to an enumeration of the words in the
sentence (the root of the tree has index 0);
2. A?V ?V is a set of arcs;
3. ? : A? D is a labeling function for arcs;
4. 0 is the root of the tree.
In order to extend the tree, we assume a range of possible POS tags for each wordform in the sentence.
Such a range of tags has to contain the correct tag for the wordform in the given context. In this work
we assume they are coming from results of several POS taggers. These tags are included in the tree
as service nodes. In the linear representation of the sentence, they are inserted after the node for the
corresponding wordform, and before the node for the next wordform to the right. They are connected to
the corresponding wordform with a special link $TAG. In order to indicate the correct tag, we introduce
another type of service node. In the linear representation of the sentence, it is inserted after the last POS
tag candidate node, and before the one corresponding to the next wordform to the right. This node is
connected to the correct tag via a special arc $CTAG (correct tag). In this way, all information about
the potential tags and the correct tag is represented in the form of a subtree, attached to the wordform.
Figure 1 depicts the encoding of a word with POS tag ambiguity as a tree. The correct tag is indicated:
verb, personal, perfective, transitive, finite, aorist, third person, singular, or ?Vpptf-03s?. The TAG arcs
are represented as red links. The CTAG arc is represented as an oval.
Figure 1: Subtree of a word with candidate POS tags and the correct tag.
Our ensemble model starts working on a set of extended dependency trees from which it to select the
an extended tree. We represent this set as an extended dependency graph.
Let us have a set G of POS tags, and a set D of dependency tags (ROOT ? D). Let us have a sentence
x = w1, ...,wn. An extended dependency graph is a directed graph Gr = (Ve,A,pi,?,?) where:
1. Ve = {0,1,$TAG11,TAG12, ...,TAG1 j1 ,T T 1, ...,n,TAGn1,TAGn2, ...,TAGn jn ,T T n} is an ordered
set of nodes, that corresponds to an enumeration of the words in the sentence (the root of the tree
has index 0), additional nodes for alternative POS tags - TAGk j, such that for each wordform k there
is at least one such node and for each wordform k there is one T T k selecting its correct POS tag;
2. V = {0,1, ...,n} is the ordered subset of Ve corresponding to words in the sentence including the
root element;
3. A?V ?V is a set of arcs;
4. pi : TAGk j ? G is a labeling function from tag nodes to POS tags (node 0 does not have POS tag).
These nodes are called POS nodes;
5. TAGk j is connected by an arc with label $TAG to the wordform k;
6. T T k is connected each TAGk j by an arc with label $CTAG, where k is the number of the wordform;
18
7. ? : A? D is a labeling relation for arcs. Each arc has at least one label;
8. ? : ?a, l?? R, is a ranking function, where a is either an arc in A and l ? ?(a) or a = ?TAGk j,k? and
l = $CTAG. ? assigns to each labeled dependency arc or tagging arc a rank. The arcs from POS
tags nodes to wordform node always have rank 1;
9. 0 is the root of the graph.
We use an extended dependency graph Gr to represent the initial data from which we select an analysis.
Each extended dependency graph could incorporate the results from several POS taggers and several
dependency parsers. At the beginning each node for true tag is connected to all corresponding tagger
predictions, and after ensemble is assigned to a single parent node, the correct tag. In this way, all
information about the potential tags and the correct tag is represented in the form of a subtree, attached
to the word form.
Our ensemble model starts from an extended dependency graph Gr and constructed an extended tagged
dependency tree T which is a subgraph of Gr. In the rest of the paper we will use just dependency tree
and dependency graph terms to denote the extended ones. We use two algorithms for the construction of
a single dependency tree from the predictions of all tagger and parser models (dependency graph).
The first algorithm, denoted LocTr, is presented in (Attardi and Dell?Orletta, 2009). It constructs the
dependency tree incrementally, starting from an empty tree and then selecting the arc with the highest
rank that could extend the current partial tree. The arcs are selected from the extended dependency graph.
The algorithm chooses the best arc locally.
The second algorithm, denoted GloTr, is the Chu-Liu-Edmonds algorithm for maximal spanning tree
implemented in the MSTParser (McDonald, 2006). This algorithm starts with a complete dependency
graph including all possible dependency arcs. Then it selects the maximal spanning tree on the basis of
the ranks assigned to the potential arcs. The arcs that are not proposed by any of the parsers are deleted
(or we could think about them as having infinite small rank). The arcs for the service nodes include only
the once from the definition of extended dependency graph. The algorithm is global with respect to the
selection of arcs. In our scenario, however, we do not construct a full graph, but one containing only the
suggestions of the parsers and taggers.
These two ensemble algorithms are included in our system for experiments with dependency parsers.
The user can specify which one should be used in their experiments, or alternatively compare the perfor-
mance of both. Making choice for each of the T T nodes both algorithms select the best POS tag for the
corresponding wordform.
In the next section we define the experimental setup: the creation of parsebank where for each tree in
the original treebank a dependency graph is created; the definition of voting approaches and the machine
learning weighting.
5 Experimental Setup
In this section we present in detail the way in which our ensemble experiment was set up, including the
data format and choice of ranking and features for machine learning.
5.1 Tagger and Parser Models and Parsebank
In the current experiments we use the five parsing models which achieved the highest LAS and UAS
scores for the BulTreeBank data in previous experiments3 (Simov et al., 2014). They include two Malt-
Parser models, MLT07 and MLT09, one MSTParser model, MST05, one TurboParser model, Turbo02, and
one Mate-tools Parser model, MATE01. The following configurations were used for each model:
1. MLT07 - Convington non-projective algorithm with extended feature set for lemmas.
2. MLT09 - Stack eager algorithm with extended feature set for morphosyntactic descriptions.
3. MST05 - default parser settings, with the exception of the order of features. The parser was set to
use features over pairs of adjacent edges (second-order: true).
3The names of the models were left unchanged for easier reference to previous work.
19
4. MATE01 - default parser settings.
5. Turbo02 - the parser was set to use a complex set of features (model_type=full), which include
arbitrary sibling parts, non-projectivity parts, grand-sibling third-order parts, and tri-sibling third-
order parts.
The models were initially trained on the gold standard values for lemma, part-of-speech tags and
features, from the dependency version of the BulTreeBank. The best performing model in a 10-fold
cross validation is MATE01, with 92.9% UAS, followed by TURBO02 with 92.7% UAS.
In addition to the parsing models, three part-of-speech taggers were trained to predict the morphosyn-
tactic tags from the BTB-tagset (Simov et al., 2004) for the data. They include a baseline tagger which
makes use of a lexicon (BLL tagger), the morphology tagger of Mate-tools, and the TreeTagger (Schmid,
1994).
The standard approach for setting up a POS tagging baseline ? selection of the most frequent tag
? cannot be applied for our experiment with Bulgarian, because of the rich morphosyntactic tagset of
680 tags. We construct a baseline tagger on the basis of the corpus and a morphological lexicon. This
baseline ignores context altogether and assigns each word type the POS tag it was most frequently seen
with in the training dataset; ties are broken randomly. For words not seen in the training dataset we use a
simple guesser which assigns POS tags on the basis of the word suffix. We first built two frequency lists,
containing respectively (1) the most frequent tag in the training dataset for each word type, as before, and
(2) the most frequent tag in the training dataset for each class of tags that can be assigned to some word
type, according to the lexicon. Given a target word type, this new baseline first tries to assign to it the
most frequent tag from the first list. If this is not possible, which happens (i) in case of ties or (ii) when
the word type was not seen during training, it extracts the tag class from the lexicon and consults the
second list. If there is a single most frequent tag in the corpus for this tag class, it is assigned; otherwise
a random tag from this tag class is selected. This strategy gives us a very high accuracy for this tagger.
Although we refer to it as a baseline, it achieves the best score among the taggers we used in these
experiments. Our explanation for this is the fact that we are using a morphological lexicon to predict the
possible tags for the unseen words in the test sets.
The Mate morphology tagger constitutes one step of the processing pipeline in Mate-tools, and makes
use of previously predicted values for lemma and tag. Therefore, we trained a Mate-tools lemmatizer and
tagger in addition, so that no gold standard data is used directly to obtain the morphological information
for each word in our experiment.
The third tagger trained on the BulTreeBank data and used in the experiments is TreeTagger, a tool that
estimates transition probabilities via decision trees. In training mode it was run with its default options.
The tagger takes as parameters a lexicon of all the words that are found in the training corpus, one per
line, followed by the respective POS tags encountered in the corpus and, optionally, by the lemmas for
those forms. We extracted this information from our training data, but skipped the optional training for
lemma, since lemmas can be automatically generated for each word form using the predicted BTB tag.
Using the taggers in a 10-fold experiment, we obtained three new versions of the dependency treebank,
with predicted values for the fields lemma, tag, and features, which brings us closer to a real-world
parsing scenario with unseen data. The output of the Mate morphology tagger is a prediction of the
values in the features field of the CoNLL dependency format. The BLL and TheeTagger predictions for
morphosyntactic tags were used to generate the fields lemma, tag, and features, for each word in the
original treebank. The taggers achieved accuracy of 95.91% (BLL Tagger), 94.92% (Mate morphology
tagger), and 93.12% (TreeTagger). Each of the five parsing models was evaluated on the new data sets
(Table 1). This evaluation exemplifies the extent to which a decrease in tagger accuracy can influence
parsing accuracy. There is a decrease in performance in terms of UAS score ranging from 1.6% to 4.6%,
compared to the performance of the models when using the gold data fields.
We define an upper bound for the potential improvement through ensemble for each task as the per-
centage of words in the parsebank for which there is at least one correct prediction by a tagger or parser.
The upper bound for the combination of taggers is 98.38%. For the parses the upper bound is 96.95 %
20
MLT07 MLT09 MATE01 MST05 Turbo02 training data
0.900 0.908 0.929 0.911 0.927 gold
0.881 0.890 0.910 0.890 0.911 BLL tagger
0.881 0.889 0.908 0.890 0.910 Mate tagger
0.857 0.865 0.883 0.865 0.883 TreeTagger
Table 1: Average UAS scores from the 10-fold cross validation of the parsing models trained on gold
data and on data containing automatically generated fields obtained using the outputs of three taggers.
for UAS and 95.38 % for LAS. These upper bounds can be reached if the algorithm is able to select the
correct solution in all cases.
A rich search space of possible combinations of POS tags and parses is available for the voting and
machine learning weighting modules to choose from. An increase in tagging accuracy through ensemble
can lead to obtaining better parsing results. In order to allow for ensemble to be performed on the output
of several POS taggers and parsers, the tree that stores the POS tags and the head and relation predicted
by each parsing model was represented in a dependency graph.
Thus, the parsebank consists of dependency graphs constructed by the three POS tagging models and
the five dependency parsing models. All the models are trained on gold data from the original treebank.
Because the parsing depends on the POS tags assigned to the wordform we applied the parsing models
for the results from each POS taggers. In this way we potentially up to fifteen arcs per wordform.
5.2 Combining Parses by Voting
We investigate three voting modes for the calculation of the weight assigned to each candidate depen-
dency arc: (1) the arcs are ranked by the number of parsers/taggers that predicted them (Rank01); (2)
the arcs are ranked by the sum of the accuracy of all parsers/taggers that predicted them (these metrics
include the LAS and UAS measures from the 10-fold cross validation and the tagger accuracies individ-
ually achieved by each tool) (Rank02); and (3) the arcs are ranked by the average of the accuracy of the
parsers/taggers that predicted them (Rank03).
5.3 Combining Taggers and Parsers by Machine Learning Weighting
In order to evaluate the interaction between morphosyntactic information and the dependency parsing,
we conducted an experiment in which a machine learning technique was used for ranking the tags and
arcs suggested by the different models. This was done with the help of the package RandomForest4
of the system R5. The parsebank was once again divided into training and test parts, using the same
proportion, but orthogonally: 90% and 10%.
For each word node there are up to three different tags and for each tag there are up to five arcs. We
constructed pairs of tags and arcs on the basis of these suggestions. Each pair was compared with the
gold data and classified as correct or incorrect for a given context. To each pair (Tag , Arc) a vector
of features was assigned. Arc was modelled by three features: relation (Rel), distance in words to the
parent node (Dist) and direction of the parent node (Dir) ? Left, meaning that the parent node is on
the left, and Right, meaning that the parent node is on the right. Tag was represented as a vector of
its grammatical features including POS, gender, number, etc. In this way the agreement features were
represented explicitly. We also included the word form string as a feature, as well as the corresponding
information for the word form context ? words before and after it, and the same for the parent node in
the dependency tree.
A representation of this data as a value vector for RandomForest is given in Table 2.
4http://cran.r-project.org/web/packages/randomForest/randomForest.pdf
5http://www.r-project.org/
21
Feature Value
Word the current node
WordBefore the word before the current node
WordAfter the word after the current node
ParentWord the parent word
PWordBefore the word before the parent word
PWordAfter the word after the parent word
SelectedArc one of the arcs suggested by one of
the models for the node
SelectedTag one of the arcs suggested by one of
the models for the node
CorrectIncorrect true or false depending on whether
the selected pair is
the correct one for the node
Table 2: Feature vector used with RandomForest for the experiment.
The tuples generated from the training part of the treebank were used to train the RandomForest in
regression mode, then the model was applied to the test set to rank each pair. After this the ranks were
distributed to tags and arcs. These weights were used by the algorithms LocTr and GloTr.
Each tag and arc for a given word could participate in several different feature vectors. Thus each of
them could receive different weights from the evaluation of the vectors. In our view, the best selection
among these weights could be determined only through experimentation. We have tested three rankings:
WMax ? the maximum tag weight for all word vectors, WMin ? the minimum tag weight for all word
vectors, and MSum ? the sum of all tag weights for all word vectors.
6 Experiments
We ran both algorithms (LocTr and GloTr) for construction of dependency trees using various combina-
tions of the outputs of our dependency parsing and tagging models. Table 3 shows the parsing accuracy
results when combining all models (1), only the models of the two best performing parsers, Turbo02 and
Malt01 (2), and the best combination we have found by trying all possible combinations (around 32K)
(3). We included the results for (1) and (2) to demonstrate that the best combination cannot be predicted
in advance by simply selecting the candidate with the largest number of models, or the one with the best
performing individual parsers.
The best combination in this experiment in terms of UAS score is: MLT09+BLL, Mate01+BLL,
MST05+BLL, Turbo02+BLL, MLT07+MateTagger, Mate01+MateTagger, Turbo02+MateTagger. This
combination achieves better UAS score (92.47%) than any of the individual parsers (see Table 1).
There was an improvement of 1.37% over the best performing individual parser Turbo01+BLL, which
achieves 91.10% UAS. The unlabelled accuracy after voting is, however, still 0.43% lower than the best
result on the gold data achieved by an individual model Mate01. We suspect that this is due to having
only three tagger models in the current experiment, and that adding a few more tagger models for voting
can help improve the result.
Table 4 presents the accuracy achieved for all possible combinations of the three taggers by voting per
rank. We have to stress the fact that the selection of the morphosyntactic tag in the extended dependency
tree is independent from the selection of dependency arcs, because each new tag node is connected to
the word node by equal weight. The interaction between dependency arcs and morphosyntactic arcs is
ensured by the features used in machine learning weighting. The results for the combination improve the
individual accuracy for all taggers.
The results in Table 4 show that it is hard to predict the best combinations in advance without enumer-
ating all possibilities. Note that for voting (Rank01, Rank02, and Rank03) it is meaningless to investigate
22
Models Algorithm Rank01 Rank02 Rank03
Number Sum Average
LAS UAS LAS UAS LAS UAS
(1) all models LocTr 88.55 92.05 88.61 92.10 85.57 88.82
GloTr 88.55 91.96 88.65 92.04 84.54 88.75
(2) all Mate01 and Turbo02 models LocTr 87.68 91.38 87.80 91.48 86.94 90.55
GloTr 87.58 91.21 87.82 91.45 86.84 90.62
(3) best combination LocTr 88.90 92.34 89.05 92.47 86.19 89.40
GloTr 88.94 92.31 89.14 92.45 85.23 89.27
Table 3: UAS and LAS obtained after voting using the algorithms LocTr and GloTr for tree construction.
(1) All 18 models; (2) A combination of the best individual models: Mate01 and Turbo02 + each tag-
ger; (3) best combination: MLT09+BLL, Mate01+BLL, MST05+BLL, Turbo02+BLL, MLT07+MateTagger,
Mate01+MateTagger, Turbo02+MateTagger;
Voting Rank01 Rank02 Rank03
Number Sum Average
BLL, Mate, TreeTagger 96.24 96.24 95.22
MLearning WMax WMin WSum
BLL, Mate, TreeTagger 96.10 96.20 96.25
BLL, Mate 96.62 96.59 96.63
BLL, TreeTagger 95.89 96.08 96.09
Mate, TreeTagger 95.29 95.40 96.25
Table 4: Tagger accuracy after voting and machine learning weighting.
the combinations involving only two taggers, because in this case the output of voting will always be the
same as the output of the better tagger.
Table 5 presents the UAS and LAS measures achieved using machine learning weighting. In this
case the best combination is Mate01+BLL, Turbo02+BLL, Mate01+MateTagger, Turbo02+MateTagger.
Again, the results are better than the ones obtained by the individual parsing models. They also demon-
strate some small improvement over the voting ranking.
Model Algorithm LAS UAS
all LocTr 89.17 92.46
GloTr 89.23 92.27
all Mate01 and Turbo02 models LocTr 88.26 91.81
GloTr 88.32 91.87
best combination LocTr 89.76 93.18
GloTr 89.81 93.22
Table 5: Results from the experiments with RandomForest. The best combination is Mate01+BLL,
Turbo02+BLL, Mate01+MateTagger, Turbo02+MateTagger.
These experiments show the following: (1) the combination of taggers and parsers is a feasible task;
(2) the combination improves the accuracy of both the taggers and the parsers; (3) the combination of
both tasks is better than the pipeline approach; (4) there is room for improvement in order to reach the
upper bounds presented in Section 5.1.
7 Conclusion and Future Work
In this paper we have presented several approaches for combining parses produced by five parsing models
and tagging results from three taggers. The motivation behind a joint ensemble model is the interaction
23
between the morphosyntactic features of the word forms and the dependency relations between them.
The interaction could be considered as local and global interaction. The local interaction is usually
captured by n-gram models for tagging. The global interaction is represented by such phenomena like
subject ? verb agreement, verb clitic ? object ? indirect object agreement, agreement between head noun
and relative pronouns, agreement between secondary predication, agreement within co-reference chains,
agreement within NPs. With relation to these cases, our current model deals with local interaction on
the basis of an n-gram model. Global agreement phenomena are currently modeled via dependency arcs
between word forms that agree in their grammatical features.
We deal with some of the interaction between local and some global patterns via a machine learning
approach in which the appropriateness of the MorphoSyntactic tag and the dependency arc for a given
word form are evaluated in conjunction. The appropriateness is expressed as a number between 0 and 1,
where 0 means inappropriate and 1 means appropriate. This number is used as a rank for the ensemble
algorithms.
Some of the global agreement phenomena such as verb clitic ? object ? indirect object agreement,
secondary predication and relative pronoun agreement are not covered by the current model. In the
future we plan to extend the model with global features defined not by arcs in the dependency tree, but
by patterns of dependency paths. These feature patterns will depend on the grammatical characteristics
of the given word form. In some cases they might not be directly related to the word form in the tree.
Our experiments show that a joint architecture is a good alternative to a pipeline architecture. There is
an improvement in accuracy for both tasks in our joint model. However, this approach has its limitations
with respect to possible improvement.
Future extensions of the experiments in several directions are envisaged. First, more linguistic knowl-
edge will be included from the morphological lexicon, valency lexicon and semantic categories of the
words as features for machine learning. Second, we plan to extend the experiments by including more
tagger and parser models, which could lead to an increase in the upper bound for potential improvement
in accuracy. In future work we envisage to compare our work with the work of (Bohnet et al., 2013)
applied on Bulgarian data. Also we will would like to include as features word clusters as they suggested
in the paper and as we did in parsing context (Ghayoomi et al., 2014).
Acknowledgements
This research has received partial funding from the EC?s FP7 (FP7/2007-2013) under grant agreement
number 610516: ?QTLeap: Quality Translation by Deep Language Engineering Approaches? and grant
agreement number 611760: ?EUCases: EUropean and National CASE Law and Legislation Linked in
Open Data Stack?.
References
Giuseppe Attardi and Felice Dell?Orletta. 2009. Reverse revision and linear tree combination for dependency
parsing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 261?264,
Boulder, Colorado.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich??rd Farkas, Filip Ginter, and Jan Hajic. 2013. Joint mor-
phological and syntactic analysis for richly inflected languages. TACL, 1:415?428.
Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics, COLING ?10, pages 89?97, Stroudsburg, PA,
USA.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149?164, New York
City.
Shay B Cohen and Noah A Smith. 2007. Joint morphological and syntactic disambiguation. Proceedings of
the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL). Prague, Czech Republic.
24
Georgi Georgiev, Valentin Zhikov, Kiril Ivanov Simov, Petya Osenova, and Preslav Nakov. 2012. Feature-rich
part-of-speech tagging for morphologically complex languages: Application to Bulgarian. In EACL?12, pages
492?502.
Masood Ghayoomi, Kiril Simov, and Petya Osenova. 2014. Constituency parsing of bulgarian: Word- vs class-
based parsing. Proceedings of LREC 2014.
Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and
syntactic parsing. In ACL 2008, pages 371?379.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2012. Incremental joint approach to word
segmentation, pos tagging, and dependency parsing in chinese. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long Papers-Volume 1, pages 1045?1053.
John Lee, Jason Naradowsky, and David A Smith. 2011. A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-Volume 1, pages 885?894.
Andr? F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and M?rio A. T. Figueiredo. 2010. Turbo
parsers: Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing, EMNLP ?10, pages 34?44, Stroudsburg, PA, USA.
Andre Martins, Noah Smith, Mario Figueiredo, and Pedro Aguiar. 2011. Dual decomposition with many overlap-
ping components. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process-
ing, pages 238?249, Edinburgh, Scotland, UK.
Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing mod-
els. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-CoNLL), pages 122?131.
Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D.
thesis.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, pages 950?958, Columbus, Ohio.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: a data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international
conference on new methods in language processing, volume 12, pages 44?49. Manchester, UK.
Kiril Simov, Petya Osenova, and Milena Slavcheva. 2004. BTB:TR03: BulTreeBank morphosyntactic tagset
BTB-TS version 2.0.
Kiril Simov, Ginka Ivanova, Maria Mateva, and Petya Osenova. 2013. Integration of dependency parsers for
Bulgarian. In The Twelfth Workshop on Treebanks and Linguistic Theories, pages 145?156, Sofia, Bulgaria.
Kiril Simov, Iliana Simova, Ginka Ivanova, Maria Mateva, and Petya Osenova. 2014. A system for experiments
with dependency parsers. In Proceedings of LREC 2014), Reykjavik, Iceland.
Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good?
In Proceedings of the North American Chapter of the Association for Computational Linguistics Conference
(NAACL-2010), Los Angeles, CA.
Valentin Zhikov, Georgi Georgiev, Kiril Simov, and Petya Osenova. 2013. Combining pos tagging, dependency
parsing and coreferential resolution for Bulgarian. In Proceedings of the International Conference Recent Ad-
vances in Natural Language Processing RANLP 2013, pages 755?762, Hissar, Bulgaria.
25
