Semantic Extraction with Wide-Coverage Lexical Resources
Behrang Mohit 
School of Information Management & Systems 
University of California, Berkeley 
Berkeley, CA 94720, USA 
behrangm@sims.berkeley.edu 
Srini Narayanan 
International Computer Science Institute 
Berkeley, CA 94704, USA 
snarayan@icsi.berkeley.edu 
 
 
 
 
Abstract 
 
We report on results of combining graphical 
modeling techniques with Information 
Extraction resources (Pattern Dictionary and 
Lexicon) for both frame and semantic role 
assignment. Our approach demonstrates the 
use of two human built knowledge bases 
(WordNet and FrameNet) for the task of 
semantic extraction. 
1. Introduction 
Portability and domain independence are critical 
challenges for Natural Language Processing (NLP) 
systems. The ongoing development of public 
knowledge bases such as WordNet, FrameNet, CYC, 
etc. has the potential to support domain independent 
solutions to NLP. The task of harnessing the 
appropriate information from these resources for an 
application remains significant. This paper reports on 
the use of semantic resources for a necessary 
component of scalable NLP systems, Semantic 
Extraction (SE) .  
Semantic Extraction pertains to the assignment 
of semantic bindings to short units of text (usually 
sentences). The SE problem is quite similar to the 
Information Extraction (IE) task, in that in both cases 
we are interested only in certain predicates and their 
argument bindings and not in full understanding. 
However there are major differences as well. IE is a 
pre-specified and autonomous task with a narrow 
domain of focus, where all the information of interest 
is represented in the extraction template. SE involves 
finding predicate-argument structures in open 
domains and is a crucial semantic parsing step in a 
text understanding task.  
In this paper we report results obtained from 
combining IE and graphical modeling techniques, 
with semantic resources (WordNet and FrameNet) 
for automatic Semantic Extraction. 
2. Background 
Semantic Extraction has become a strong 
research focus in the last few years. A good example 
is the work of Gildea and Jurafsky (2002) (GJ). GJ 
present a comprehensive empirical approach to the 
problem of semantic role assignment. Their work 
looked at the problem of assigning semantic roles to 
text based on a statistical model of the FrameNet1 
data. In their work, GJ assume that the frame of 
interest is determined a-priori for every sentence.  
In the IE community, there has been an ongoing 
effort to build systems that can automatically 
generate required pattern sets as well as the 
extraction relevant lexicon. Jones and Riloff (JR) 
(1999) describe a bootstrapping approach to the 
problem of IE pattern extension. They use a small 
seed lexicon and pattern set, to iteratively generate 
new patterns and expand their lexicon until they 
achieve an optimized set of patterns and lexicon. 
In the area of lexicon acquisition, many 
researchers have employed public knowledge bases 
such as WordNet in IE systems. Bagga et. al. (1997) 
and later Harabagiu and Maiorano (HM) (2000) 
investigated the acquisition of the lexical concept 
space using WordNet and have applied their methods 
to the Information Extraction task. 
In this paper, we describe work that blends the 
semantic labeling approach exemplified by the GJ 
effort and the bootstrapping approach of JR and HM. 
Our work differs from the previous efforts in the 
following respects. 1) We used FrameNet annotations 
as seeds both for patterns and for the extraction 
lexicon. We expand the seed lexicon using WordNet. 
2) We built a graphical model for the semantic 
extraction task, which allows us to integrate 
automatic frame assignment as part of the extraction. 
3) We employed IE methods (including pattern sets 
and Named Entity Recognition) as initial extraction 
steps. 
                                                                 
1 http://www.icsi.berkeley.edu/~framenet 
3. FrameNet 
FrameNet (Baker et. al. 1998) is building a 
lexicon based on the theory of Frame Semantics. 
Frame Semantics suggests that the meanings of 
lexical items (lexical units (LU)) are best defined 
with respect to larger conceptual chunks, called 
Frames. Individual lexical units evoke specific frames 
and establish a binding pattern to specific slots or 
roles (frame elements (FE)) within the frame. The 
Berkeley FrameNet project describes the underlying 
frames for different lexical units, examines sentences 
related to the frames using a very large corpus, and 
records (annotates) the ways in which information 
from the associated frames are expressed in these 
sentences. The result is a database that contains a set 
of frames (related through hierarchy and 
composition), a set of frame elements for each frame, 
and a set of frame annotated sentences that covers the 
different patterns of usage for lexical units in the 
frame. 
3.1 FrameNet data as seed patterns for IE: 
Using the FrameNet annotated dataset, we 
compiled a set of IE patterns and also the lexicon for 
each of the lexical units in FrameNet.  
We filtered out all of the non-relevant terms in 
all frame element lexicons. We hypothesized that 
using a highly precise set of patterns along with 
precise lexicon should enable a promising IE 
performance.  For our Information Extraction 
experiments, we used GATE (Cunningham et. al. 
2002), an open source natural language engineering 
system. The component-based architecture of GATE 
enabled us to plug-in our FrameNet based lexicon 
and pattern set and run IE experiments on this 
system.  
3.2 Initial Experiment: 
As a preliminary test, we compiled a set of 100 
news stories from Yahoo News Service with topics 
related to Criminal Investigation. We also compiled a 
set of IE patterns and also the lexicon from the crime 
related frames (?Arrest?, ?Detain?, ?Arraign? and 
?Verdict?.) We ran the GATE system on this corpus 
with our FrameNet data. We evaluated the IE 
performance by human judgment and hand counting 
the semantic role assignments. The systems achieved 
an average of 55% Recall while the precision was 
68.8%. The fairly high precision (given just the 
FrameNet annotations) is the result of  a highly 
precise lexicon and pattern set, while we see the low 
recall as the result of the small coverage. That is the 
reason that employed WordNet to enlarge our 
lexicon. 
4.  Expanding the Lexicon 
In order to expand our lexicon for each of the 
frame elements, we used the human-built knowledge 
base (WordNet (Fellbaum 1998)) and its rich 
hierarchical structure.  
We built a graphical model of WordNet making 
some assumptions about the structure of the induced 
WordNet graph. For our initial experiments, we built 
a graph whose leaf was the enclosing category of the 
FrameNet annotated frame element. We then looked 
at an ancestor tree following the WordNet hypernym 
relation. This gave us a graphical model of the form 
shown in Figure 1 for the FrameNet frame element 
Suspect and WordNet category Thief. 
 
Figure 1 
We then used the sum-product algorithm (Frey 
1998) for statistical inference on the frame element 
induced graph (such as in Figure 1). We now 
illustrate our use of the algorithm to expand the 
FrameNet derived lexicon.  
4.1 Statistical Inference 
We employed a statistical inference algorithm to 
find the relevant nodes of WordNet. For each of the 
frame elements, we took the terms in FrameNet FE 
annotations as ground truth which means that the 
relevance probability of the WordNet nodes for those 
terms is equal to 1.  The Sum Product algorithm helps 
us find the relevance probability of higher level 
nodes as a lexical category for the frame element 
through a bottom up computation of the inter-node 
messages.  For example the message between nodes 1 
and 0 in the Figure 1 can be computed as: 
 
? ?
?
?
1 0\)1(
1101000,1 )()|()()(
N Nk
k NmNNPNPNm  
We should note that based on the WordNet?s 
hypernym relation, the conditional relevance 
probability of each parent node (given any child 
node) is equal to 1. Therefore the Sum Product inter-
node messages are computed as: 
? ?
?
=
jN ijNk
jkjjiji NmNPNm
\)(
)()()(  
and the probability of each WordNet node can be 
computed by a normalized interpolation of all of the 
incoming messages from the children nodes: 
 
|)(\)(|
)(
)( )(\)(
iparentiN
Nm
Np iparentiNj
iji
i
?
?=  
4.2 Relevance of a WordNet Nodes 
Throughout our experiments with the training 
data, we discovered that some infrequent tail terms in 
the frame element lexicon that might not be filtered 
out by the statistical inference algorithm but still are 
frequently used in relevant text.  
Therefore, we defined the relevance metric for 
the WordNet nodes to achieve a larger coverage. We 
compiled a large corpus of text (News stories) and 
made a second smaller corpus from the original one 
which contains only sentences which are relevant to 
the IE task. For each of the WordNet nodes we 
defined the relevance of the node based on the 
proportion of the occurrence of the node in IE related 
Text (Orel) to the occurrence of the node in the 
general text (Ogen). 
gen
rel
O
O
Nl =)(Re  
Using this relevance metric, we evaluated all of 
the WordNet nodes for the training data (found in the 
previous step) and re-ranked and picked the top ?m? 
relevant nodes (m=5 for our reported experiment) and 
added them to the previous set of WordNet nodes. 
With a set of relevant WordNet nodes, we 
extended the lexicon for the IE system and re-ran our 
IE task on the same 100 Yahoo news stories that 
were used in the initial experiments. The average 
recall rose up to 76.4% this time with an average 
precision equal to 66%.  
5.  Frame Assignment 
Using FrameNet data with IE techniques shows 
promising results for semantic extraction. Our current 
efforts are geared toward extending the extraction 
program to include automatic frame assignment. For 
this task, we assume that that the frame is a latent 
class variable (whose domain is the set of lexical 
units) and the frame elements are variables whose 
domain is the expanded lexicon (FrameNet + 
WordNet). We assume that the frame elements are 
conditionally independent from each other, given the 
frame. For our initial experiments, we assume that 
each frame is an independent model and frame 
assignment is the task of selecting the Maximum A 
Posteriori (MAP) frame given the input and the priors 
of the frame. Figure 2 shows the graphical model 
exemplifying this assertion. With this model, we are 
able to estimate the overall joint distribution for each 
FrameNet frame, given the lexical items in the 
candidate sentence from the corpus. During training 
frame priors and model parameters )|( framefep are 
estimated from a large corpus using our SE 
machinery outlined in sections 3 and 4. While our 
initial results seem promising, the work is ongoing 
and we should have more results to report on this 
aspect of the work by the time of the conference. 
 
 
Figure 2 
6. References 
Bagga A., Chai J.Y. & Biermann A. 1997.  The Role 
of WordNet in The Creation of a Trainable 
Message Understanding System. In Proceedings 
of the Sixth Message Understanding Conference 
on Artificial Intelligence (AAAI/IAAI-97) 
Baker C., Fillmore C. & Lowe J. 1998, The Berkeley 
FrameNet project, In Proceedings of COLING/ACL 
pages 86?90, Montreal, Canada. 
Cunningham H., Maynard D., Bontcheva K., Tablan 
V. 2002, GATE: A Framework and Graphical 
Development Environment for Robust NLP Tools 
and Applications. In Proceedings of the 40th 
Anniversary Meeting of the Association for 
Computational Linguistics (ACL'02).  
Fellbaum C., WordNet: an Electionic Lexical 
Database, Cambridge, MA, The MIT Press. 
Frey B.J. 1998, Graphical Models for Machine 
Learning and Digital Communication, 
Cambridge, MA, MIT Press 
Gildea D., Jurafsky D. 2002, Automatic labeling of 
semantic roles, Computational Linguistics, 
28(3):245-288.  
Harabagiu S., Maiorano, S. 2000, Acquisition of 
Linguistic Patterns for Knowledge-Based 
Information Extraction, in Proceedings of LREC-
2000,Athens Greece. 
Riloff, E. and Jones, R. 1999, Learning Dictionaries 
for Information Extraction by Multi-Level 
Bootstrapping, In Proceedings AAAI-99  pp. 474-
479. 
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 57?60, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Syntax-based Semi-Supervised Named Entity Tagging 
 
 
Behrang Mohit Rebecca Hwa 
Intelligent Systems Program Computer Science Department 
University of Pittsburgh University of Pittsburgh 
Pittsburgh, PA 15260 USA Pittsburgh, PA 15260, USA 
behrang@cs.pitt.edu hwa@cs.pitt.edu 
 
 
 
 
Abstract 
We report an empirical study on the role 
of syntactic features in building a semi-
supervised named entity (NE) tagger.  
Our study addresses two questions: What 
types of syntactic features are suitable for 
extracting potential NEs to train a classi-
fier in a semi-supervised setting? How 
good is the resulting NE classifier on test-
ing instances dissimilar from its training 
data? Our study shows that constituency 
and dependency parsing constraints are 
both suitable features to extract NEs and 
train the classifier.  Moreover, the classi-
fier showed significant accuracy im-
provement when constituency features are 
combined with new dependency feature.  
Furthermore, the degradation in accuracy 
on unfamiliar test cases is low, suggesting 
that the trained classifier generalizes well. 
1 Introduction 
Named entity (NE) tagging is the task of recogniz-
ing and classifying phrases into one of many se-
mantic classes such as persons, organizations and 
locations. Many successful NE tagging systems 
rely on a supervised learning framework where 
systems use large annotated training resources 
(Bikel et. al. 1999). These resources may not al-
ways be available for non-English domains.  This 
paper examines the practicality of developing a 
syntax-based semi-supervised NE tagger.  In our 
study we compared the effects of two types of syn-
tactic rules (constituency and dependency) in ex-
tracting and classifying potential named entities.  
We train a Naive Bayes classification model on a 
combination of labeled and unlabeled examples 
with the Expectation Maximization (EM) algo-
rithm.  We find that a significant improvement in 
classification accuracy can be achieved when we 
combine both dependency and constituency extrac-
tion methods.  In our experiments, we evaluate the 
generalization (coverage) of this bootstrapping ap-
proach under three testing schemas.  Each of these 
schemas represented a certain level of test data 
coverage (recall).  Although the system performs 
best on (unseen) test data that is extracted by the 
syntactic rules (i.e., similar syntactic structures as 
the training examples), the performance degrada-
tion is not high when the system is tested on more 
general test cases. Our experimental results suggest 
that a semi-supervised NE tagger can be success-
fully developed using syntax-rich features.  
2 Previous Works and Our Approach 
Supervised NE Tagging has been studied exten-
sively over the past decade (Bikel et al 1999, 
Baluja et. al. 1999, Tjong Kim Sang and De 
Meulder 2003).  Recently, there were increasing 
interests in semi-supervised learning approaches. 
Most relevant to our study, Collins and Singer 
(1999) showed that a NE Classifier can be devel-
oped by bootstrapping from a small amount of la-
beled examples.  To extract potentially useful 
training examples, they first parsed the sentences 
and looked for expressions that satisfy two con-
stituency patterns (appositives and prepositional 
phrases).  A small subset of these expressions was 
then manually labeled with their correct NE tags.  
The training examples were a combination of the 
labeled and unlabeled data.  In their studies, 
57
Collins and Singer compared several learning 
models using this style of semi-supervised training.  
Their results were encouraging, and their studies 
raised additional questions.  First, are there other 
appropriate syntactic extraction patterns in addition 
to appositives and prepositional phrases?  Second, 
because the test data were extracted in the same 
manner as the training data in their experiments, 
the characteristics of the test cases were biased.  In 
this paper we examine the question of how well a 
semi-supervised system can classify arbitrary 
named entities.  In our empirical study, in addition 
to the constituency features proposed by Collins 
and Singer, we introduce a new set of dependency 
parse features to recognize and classify NEs.  We 
evaluated the effects of these two sets of syntactic 
features on the accuracy of the classification both 
separately and in a combined form (union of the 
two sets). 
Figure 1 represents a general overview of our sys-
tem?s architecture which includes the following 
two levels: NE Recognizer and NE Classifier. 
Section 3 and 4 describes these two levels in de-
tails and section 5 covers the results of the evalua-
tion of our system. 
 
Figure 1: System's architecture 
3 Named Entity Recognition  
In this level, the system used a group of syntax-
based rules to recognize and extract potential 
named entities from constituency and dependency 
parse trees.  The rules are used to produce our 
training data; therefore they needed to have a nar-
row and precise coverage of each type of named 
entities to minimize the level of training noise. 
The processing starts from construction of con-
stituency and dependency parse trees from the in-
put text. Potential NEs are detected and extracted 
based on these syntactic rules. 
3.1 Constituency Parse Features 
Replicating the study performed by Collins-Singer 
(1999), we used two constituency parse rules to 
extract a set of proper nouns (along with their as-
sociated contextual information). These two con-
stituency rules extracted proper nouns within a 
noun phrase that contained an appositive phrase 
and a proper noun within a prepositional phrase. 
3.2 Dependency Parse Features 
We observed that a proper noun acting as the sub-
ject or the object of a sentence has a high probabil-
ity of being a particular type of named entity. 
Thus, we expanded our syntactic analysis of the 
data into dependency parse of the text and ex-
tracted a set of proper nouns that act as the subjects 
or objects of the main verb.  For each of the sub-
jects and objects, we considered the maximum 
span noun phrase that included the modifiers of the 
subjects and objects in the dependency parse tree. 
4 Named Entity Classification 
In this level, the system assigns one of the 4 class 
labels (<PER>, <ORG>, <LOC>, <NONE>) to a 
given test NE.  The NONE class is used for the 
expressions mistakenly extracted by syntactic fea-
tures that were not a NE.  We will discuss the form 
of the test NE in more details in section 5.  The 
underlying model we consider is a Na?ve Bayes 
classifier; we train it with the Expectation-
Maximization algorithm, an iterative parameter 
estimation procedure. 
4.1 Features 
We used the following syntactic and spelling fea-
tures for the classification: 
Full NE Phrase.  
Individual word: This binary feature indicates the 
presence of a certain word in the NE. 
58
Punctuation pattern: The feature helps to distin-
guish those NEs that hold certain patterns of punc-
tuations like (?) for U.S.A. or (&.) for A&M.  
All Capitalization:  This binary feature is mainly 
useful for some of the NEs that have all capital 
letters.  such as AP, AFP, CNN, etc. 
Constituency Parse Rule:  The feature indicates 
which of the two constituency rule is used for ex-
tract the NE. 
Dependency Parse Rule:  The feature indicates if 
the NE is the subject or object of the sentence. 
Except for the last two features, all features are 
spelling features which are extracted from the ac-
tual NE phrase.  The constituency and dependency 
features are extracted from the NE recognition 
phase (section 3).  Depending on the type of testing 
and training schema, the NEs might have 0 value 
for the dependency or constituency features which 
indicate the absence of the feature in the recogni-
tion step.  
4.2 Na?ve Bayes Classifier 
We used a Na?ve Bayes classifier where each NE 
is represented by a set of syntactic and word-level 
features (with various distributions) as described 
above.  The individual words within the noun 
phrase are binary features.  These, along with other 
features with multinomial distributions, fit well 
into Na?ve Bayes assumption where each feature is 
dealt independently (given the class value).  In or-
der to balance the effects of the large binary fea-
tures on the final class probabilities, we used some 
numerical methods techniques to transform some 
of the probabilities to the log-space. 
4.3 Semi-supervised learning 
Similar to the work of Nigam et al (1999) on 
document classification, we used Expectation 
Maximization (EM) algorithm along with our Na-
?ve Bayes classifier to form a semi supervised 
learning framework.  In this framework, the small 
labeled dataset is used to do the initial assignments 
of the parameters for the Na?ve Bayes classifier.  
After this initialization step, in each iteration the 
Na?ve Bayes classifier classifies all of the unla-
beled examples and updates its parameters based 
on the class probability of the unlabeled and la-
beled NE instances.  This iterative procedure con-
tinues until the parameters reach a stable point.  
Subsequently the updated Na?ve Bayes classifies 
the test instances for evaluation.   
5 Empirical Study 
Our study consists of a 9-way comparison that in-
cludes the usage of three types of training features 
and three types of testing schema. 
5.1 Data  
We used the data from the Automatic Content Ex-
traction (ACE)?s entity detection track as our la-
beled (gold standard) data.1 
For every NE that the syntactic rules extract from 
the input sentence, we had to find a matching NE 
from the gold standard data and label the extracted 
NE with the correct NE class label.  If the ex-
tracted NE did not match any of the gold standard 
NEs (for the sentence), we labeled it with the 
<NONE> class label. 
We also used the WSJ portion of the Penn Tree 
Bank as our unlabeled dataset and ran constituency 
and dependency analyses2 to extract a set of unla-
beled named entities for the semi-supervised clas-
sification. 
5.2 Evaluation 
In order to evaluate the effects of each group of 
syntactic features, we experimented with three dif-
ferent training strategies (using constituency rules, 
dependency rules or combinations of both). We 
conducted the comparison study with three types 
of test data that represent three levels of coverage 
(recall) for the system: 
1. Gold Standard NEs:  This test set contains in-
stances taken directly from the ACE data, and are 
therefore independent of the syntactic rules. 
2. Any single or series of proper nouns in the text:  
This is a heuristic for locating potential NEs so as 
to have the broadest coverage. 
3. NEs extracted from text by the syntactic rules.  
This evaluation approach is similar to that of Col-
lins and Singer.  The main difference is that we 
have to match the extracted expressions to a pre-
                                                           
1 We only used the NE portion of the data and removed the 
information for other tracking and extraction tasks. 
2 We used the Collins parser (1997) to generate the constitu-
ency parse and a dependency converter (Hwa and Lopez, 
2004) to obtain the dependency parse of English sentences. 
59
labeled gold standard from ACE rather than per-
forming manual annotations ourselves.   
All tests have been performed under a 5-fold cross 
validation training-testing setup.  Table 1 presents 
the accuracy of the NE classification and the size 
of labeled data in the different training-testing con-
figurations.  The second line of each cell shows the 
size of labeled training data and the third line 
shows the size of testing data.  Each column pre-
sents the result for one type of the syntactic fea-
tures that were used to extract NEs.  Each row of 
the table presents one of the three testing schema.  
We tested the statistical significance of each of the 
cross-row accuracy improvements against an alpha 
value of 0.1 and observed significant improvement 
in all of the testing schemas.   
 
Training Features Testing Data Const. Dep. Union 
Gold Standard NEs 
(ACE Data) 
76.7% 
668 
579 
78.5% 
884 
579 
82.4% 
1427 
579 
All Proper Nouns 
70.2% 
668 
872 
71.4% 
884 
872 
76.1% 
1427 
872 
NEs Extracted by 
Training Rules 
78.2% 
668 
169 
80.3% 
884 
217 
85.1% 
1427 
354 
Table 1: Classification Accuracy, labeled training & 
testing data size  
 
Our results suggest that dependency parsing fea-
tures are reasonable extraction patterns, as their 
accuracy rates are competitive against the model 
based solely on constituency rules.  Moreover, they 
make a good complement to the constituency rules 
proposed by Collins and Singer, since the accuracy 
rates of the union is higher than either model alone. 
As expected, all methods perform the best when 
the test data are extracted in the same manner as 
the training examples.  However, if the systems 
were given a well-formed named entity, the per-
formance degradation is reasonably small, about 
2% absolute difference for all training methods.  
The performance is somewhat lower when classi-
fying very general test cases of all proper nouns. 
6 Conclusion and Future Work 
In this paper, we experimented with different syn-
tactic extraction patterns and different NE recogni-
tion constraints.  We find that semi-supervised 
methods are compatible with both constituency and 
dependency extraction rules.  We also find that the 
resulting classifier is reasonably robust on test 
cases that are different from its training examples. 
An area that might benefit from a semi-supervised 
NE tagger is machine translation. The semi-
supervised approach is suitable for non-English 
languages that do not have very much annotated 
NE data.  We are currently applying our system to 
Arabic.  The robustness of the syntactic-based ap-
proach has allowed us to port the system to the 
new language with minor changes in our syntactic 
rules and classification features. 
Acknowledgement  
We would like to thank the NLP group at Pitt and 
the anonymous reviewers for their valuable com-
ments and suggestions. 
References 
Shumeet Baluja, Vibhu Mittal and Rahul Sukthankar, 
1999. Applying machine learning for high perform-
ance named-entity extraction. In Proceedings of Pa-
cific Association for Computational Linguistics. 
Daniel Bikel, Robert Schwartz & Ralph Weischedel, 
1999. An algorithm that learns what?s in a name. 
Machine Learning 34. 
Michael Collins, 1997.  Three generative lexicalized 
models for statistical parsing. In Proceedings of the 
35th Annual Meeting of the ACL. 
Michael Collins, and Yoram Singer, 1999. Unsuper-
vised Classification of Named Entities. In Proceed-
ings of SIGDAT. 
A. P. Dempster, N. M. Laird and D. B. Rubin, 1977. 
Maximum Likelihood from incomplete data via the 
EM algorithm. Journal of Royal Statistical Society, 
Series B, 39(1), 1-38. 
Rebecca Hwa and Adam Lopez, 2004.  On the Conver-
sion of Constituent Parsers to Dependency Parsers.  
Technical Report TR-04-118, Department of Com-
puter Science, University of Pittsburgh. 
Kamal Nigam, Andrew McCallum, Sebastian Thrun and 
Tom Mitchell, 2000. Text Classification from La-
beled and Unlabeled Documents using EM.  Machine 
Learning 39(2/3). 
Erik F. Tjong Kim Sang and Fien De Meulder, 2003. 
Introduction to the CoNLL-2003 Shared Task: Lan-
guage-Independent Named Entity Recognition. In 
Proceedings of CoNLL-2003. 
60
Proceedings of the Second Workshop on Statistical Machine Translation, pages 248?255,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Localization of Difficult-to-Translate Phrases 
 
 
Behrang Mohit1 and Rebecca Hwa1,2 
Intelligent Systems Program1 
Department of Computer Science2 
University of Pittsburgh 
Pittsburgh, PA 15260 U.S.A. 
{behrang, hwa}@cs.pitt.edu 
 
 
Abstract 
This paper studies the impact that difficult-to-
translate source-language phrases might have 
on the machine translation process. We formu-
late the notion of difficulty as a measurable 
quantity; we show that a classifier can be 
trained to predict whether a phrase might be 
difficult to translate; and we develop a frame-
work that makes use of the classifier and ex-
ternal resources (such as human translators) to 
improve the overall translation quality. 
Through experimental work, we verify that by 
isolating difficult-to-translate phrases and 
processing them as special cases, their nega-
tive impact on the translation of the rest of the 
sentences can be reduced. 
1 Introduction 
For translators, not all source sentences are created 
equal. Some are straight-forward enough to be 
automatically translated by a machine, while others 
may stump even professional human translators. 
Similarly, within a single sentence there may be 
some phrases that are more difficult to translate 
than others. The focus of this paper is on identify-
ing Difficult-to-Translate Phrases (DTPs) within a 
source sentence and determining their impact on 
the translation process. We investigate three ques-
tions: (1) how should we formalize the notion of 
difficulty as a measurable quantity over an appro-
priately defined phrasal unit? (2) To what level of 
accuracy can we automatically identify DTPs? (3) 
To what extent do DTPs affect an MT system's 
performance on other (not-as-difficult) parts of the 
sentence? Conversely, would knowing the correct 
translation for the DTPs improve the system?s 
translation for the rest of the sentence?  
In this work, we model difficulty as a meas-
urement with respect to a particular MT system.  
We further assume that the degree of difficulty of a 
phrase is directly correlated with the quality of the 
translation produced by the MT system, which can 
be approximated using an automatic evaluation 
metric, such as BLEU (Papineni et al, 2002).  Us-
ing this formulation of difficulty, we build a 
framework that augments an off-the-shelf phrase-
based MT system with a DTP classifier that we 
developed.  We explore the three questions in a set 
of experiments, using the framework as a testbed.  
In the first experiment, we verify that our pro-
posed difficulty measurement is sensible.  The sec-
ond experiment evaluates the classifier's accuracy 
in predicting whether a source phrase is a DTP.  
For that, we train a binary SVM classifier via a 
series of lexical and system dependent features. 
The third is an oracle study in which the DTPs are 
perfectly identified and human translations are ob-
tained. These human-translated phrases are then 
used to constrain the MT system as it translates the 
rest of the sentence. We evaluate the translation 
quality of the entire sentence and also the parts that 
are not translated by humans.  Finally, the frame-
work is evaluated as a whole. Results from our 
experiments suggest that improved handling of 
DTPs will have a positive impact the overall MT 
output quality.  Moreover, we find the SVM-
trained DTP classifier to have a promising rate of 
accuracy, and that the incorporation of DTP infor-
mation can improve the outputs of the underlying 
MT system. Specifically, we achieve an improve-
ment of translation quality for non-difficult seg-
248
ments of a sentence when the DTPs are translated 
by humans. 
2 Motivation 
There are several reasons for investigating ways to 
identify DTPs.  For instance, it can help to find 
better training examples in an active learning 
framework; it can be used to coordinate outputs of 
multiple translation systems; or it can be used as 
means of error analysis for MT system 
development.  It can also be used as a pre-
processing step, an alternative to post-editing.  For 
many languages, MT output requires post-
translation editing that can be cumbersome task for 
low quality outputs, long sentences, complicated 
structures and idioms.  Pre-translation might be 
viewed as a kind of preventive medicine; that is, a 
system might produce an overall better output if it 
were not thwarted by some small portion of the 
input. By identifying DTPs and passing those cases 
off to an expensive translation resource (e.g. 
humans) first, we might avoid problems further 
down the MT pipeline. Moreover, pre-translation 
might not always have to be performed by humans.  
What is considered difficult for one system might 
not be difficult for another system; thus, pre-
translation might also be conducted using multiple 
MT systems. 
3 Our Approach 
Figure 1 presents the overall dataflow of our 
system.  The input is a source sentence (a1 ... an), 
from which DTP candidates are proposed. Because 
the DTPs will have to be translated by humans as 
independent units, we limit the set of possible 
phrases to be syntactically meaningful units. 
Therefore, the framework requires a source-
language syntactic parser or chunker. In this paper, 
we parse the source sentence with an off-the-shelf 
syntactic parser (Bikel, 2002). From the parse tree 
produced for the source sentence, every constituent 
whose string span is between 25% and 75% of the 
full sentence length is considered a DTP candidate.  
Additionally we have a tree node depth constraint 
that requires the constituent to be at least two 
levels above the tree?s yield and two levels below 
the root.  These two constraints ensure that the 
extracted phrases have balanced lengths. 
We apply the classifier on each candidate and 
select the one labeled as difficult with the highest 
classification score.  Depending on the underlying 
classifier, the score can be in various formats such 
as class probablity, confidence measure, etc.  In 
our SVM based classifier, the score is the distance 
from the margin. 
 
 
Figure 1: An overview of our translation frame-
work.  
 
The chosen phrase (aj ... ak) is translated by a 
human (ei ... em). We constrain the underlying 
phrase-based MT system (Koehn, 2003) so that its 
decoding of the source sentence must contain the 
human translation for the DTP. In the following 
subsections, we describe how we develop the DTP 
classifier with machine learning techniques and 
how we constrain the underlying MT system with 
human translated DTPs. 
3.1 Training the DTP Classifier 
Given a phrase in the source language, the DTP 
classifier extracts a set of features from it and pre-
dicts whether it is difficult or not based on its fea-
ture values. We use an SVM classifier in this work.  
We train the SVM-Light implementation of the 
249
algorithm (Joachims 1999).  To train the classifier, 
we need to tackle two challenges.  First, we need to 
develop some appropriate training data because 
there is no corpus with annotated DTPs. Second, 
we need to determine a set of predictive features 
for the classifier. 
Development of the Gold Standard 
Unlike the typical SVM training scenario, labeled 
training examples of DTPs do not exist. Manual 
creation of such data requires deep understanding 
of the linguistics differences of source and target 
languages and also deep knowledge about the MT 
system and its training data.  Such resources are 
not accessible to us.  Instead, we construct the gold 
standard automatically.  We make the strong as-
sumption that difficulty is directly correlated to 
translation quality and that translation quality can 
be approximately measured by automatic metrics 
such as BLEU.  We have two resource require-
ments ? a sentence-aligned parallel corpus (differ-
ent from the data used to train the underlying MT 
system), and a syntactic parser for the source lan-
guage. The procedure for creating the gold stan-
dard data is as follows:  
1. Each source sentence is parsed. 
2. Phrase translations are extracted from the par-
allel corpus. Specifically, we generate word-
alignments using GIZA++ (Och 2001) in both 
directions and combine them using the refined 
methodology (Och and Ney 2003), and then 
we applied Koehn?s toolkit (2004) to extract 
parallel phrases. We have relaxed the length 
constraints of the toolkit to ensure the extrac-
tion of long phrases (as long as 16 words).  
3. Parallel phrases whose source parts are not 
well-formed constituents are filtered out.   
4. The source phrases are translated by the under-
lying MT system, and a baseline BLEU score 
is computed over this set of MT outputs. 
5. To label each source phrase, we remove that 
phrase and its translation from the MT output 
and calculate the set?s new BLEU score. If 
new-score is greater than the baseline score by 
some threshold value (a tunable parameter), we 
label the phrase as difficult, otherwise we label 
it as not difficult.   
Rather than directly calculating the BLEU score 
for each phrase, we performed the round-robin 
procedure described in steps 4 and 5 because 
BLEU is not reliable for short phrases. BLEU is 
calculated as a geometric mean over n-gram 
matches with references, assigning a score of zero 
to an entire phrase if no higher-ordered n-gram 
matches were found against the references. How-
ever, some phrases with a score of 0 might have 
more matches in the lower-ordered n-grams than 
other phrases (and thus ought to be considered 
?easier?). A comparison of the relative changes in 
BLEU scores while holding out a phrase from the 
corpus gives us a more sensitive measurement than 
directly computing BLEU for each phrase. 
Features 
By analyzing the training corpus, we have found 
18 features that are indicative of DTPs. Some 
phrase-level feature values are computed as an av-
erage of the feature values of the individual words.  
The following first four features use some prob-
abilities that are collected from a parallel data and 
word alignments.  Such a resource does not exist at 
the time of testing.  Instead we use the history of 
the source words (estimated from the large parallel 
corpus) to predict the feature value. 
  (I) Average probability of word alignment 
crossings: word alignment crossings are indicative 
of word order differences and generally structural 
difference across two languages.  We collect word 
alignment crossing statistics from the training cor-
pus to estimate the crossing probability for each 
word in a new source phrase.  For example the 
Arabic word rhl has 67% probability of alignment 
crossing (word movement across English).  These 
probabilities are then averaged into one value for 
the entire phrase.  
(II) Average probability of translation ambi-
guity: words that have multiple equally-likely 
translations contribute to translation ambiguity.  
For example a word that has 4 different transla-
tions with similar frequencies tends to be more 
ambiguous than a word that has one dominant 
translation. We collect statistics about the lexical 
translational ambiguities from the training corpus 
and lexical translation tables and use them to pre-
dict the ambiguity of each word in a new source 
phrase. The score for the phrase is the average of 
the scores for the individual words. 
(III) Average probability of POS tag changes:  
Change of a word?s POS tagging is an indication 
of deep structural differences between the source 
phrase and the target phrase.  Using the POS tag-
ging information for both sides of the training cor-
pus, we learn the probability that each source 
word?s POS gets changed after the translation.  To 
250
overcome data sparseness, we only look at the col-
lapsed version of POS tags on both sides of the 
corpus.  The phrase?s score is the average the indi-
vidual word probabilities.  
(IV) Average probability of null alignments: 
In many cases null alignments of the source words 
are indicative of the weakness of information about 
the word.  This feature is similar to average ambi-
guity probability.  The difference is that we use the 
probability of null alignments instead of lexical 
probabilities. 
(V-IX) Normalized number of unknown 
words, content words, numbers, punctuations: 
For each of these features we normalize the count 
(e.g.: unknown words) with the length of the 
phrase.  The normalization of the features helps the 
classifier to not have length preference for the 
phrases.  
(X) Number of proper nouns: Named entities 
tend to create translation difficulty, due to their 
diversity of spellings and also domain differences.  
We use the number of proper nouns to estimate the 
occurrence of the named entities in the phrase. 
(XI Depth of the subtree: The feature is used as 
a measure of syntactic complexity of the phrase.  
For example continuous right branching of the 
parse tree which adds to the depth of the subtree 
can be indicative of a complex or ambiguous struc-
ture that might be difficult to translate. 
(XII) Constituency type of the phrase:  We 
observe that the different types of constituents 
have varied effects on the translations of the 
phrase.  For example prepositional phrases tend to 
belong to difficult phrases.  
(XIII) Constituency type of the parent phrase 
(XIV) Constituency types of the children 
nodes of the phrase: We form a set from the chil-
dren nodes of the phrase (on the parse tree).   
(XV) Length of the phrase: The feature is 
based on the number of the words in the phrase. 
(XVI) Proportional length of the phrase: The 
proportion of the length of the phrase to the length 
of the sentence.  As this proportion gets larger, the 
contextual effect on the translation of the phrase 
becomes less. 
    (XVII) Distance from the start of the sentence 
and: Phrases that are further away from the start of 
the sentence tend to not be translated as well due to 
compounding translational errors.   
(XVIII) Distance from a learned translation 
phrase: The feature measure the number of words 
before reaching a learned phrase.  In other words it 
s an indication of the level of error that is intro-
duced in the early parts of the phrase translation. 
3.2 Constraining the MT System 
Once human translations have been obtained for 
the DTPs, we want the MT system to only consider 
output candidates that contain the human transla-
tions. The additional knowledge can be used by the 
phrase-based system without any code modifica-
tion. Figure 2 shows the data-flow for this process. 
First, we append the pre-trained phrase-translation 
table with the DTPs and their human translations 
with a probability of 1.0. We also include the hu-
man translations for the DTPs as training data for 
the language model to ensure that the phrase vo-
cabulary is familiar to the decoder and relax the 
phrase distortion parameter that the decoder can 
include all phrase translations with any length in 
the decoding.  Thus, candidates that contain the 
human translations for the DTPs will score higher 
and be chosen by the decoder. 
 
 
Figure 2: Human translations for the DTPs can be 
incorporated into the MT system?s phrase table and 
language model. 
4 Experiments 
The goal of these four experiments is to gain a bet-
ter understanding of the DTPs and their impact on 
the translation process. All our studies are con-
ducted for Arabic-to-English MT.  We formed a 
one-million word parallel text out of two corpora 
released by the Linguistic Data Consortium: Ara-
251
bic News Translation Text Part 1 and Arabic Eng-
lish Parallel News Part 1.  The majority of the data 
was used to train the underlying phrase-based MT 
system. We reserve 2000 sentences for develop-
ment and experimentation.  Half of these are used 
for the training and evaluation of the DTP classi-
fier (Sections 4.1 and 4.2); the other half is used 
for translation experiments on the rest of the 
framework (Sections 4.3 and 4.4).  
In both cases, translation phrases are extracted 
from the sentences and assigned ?gold standard? 
labels according to the procedure described in Sec-
tion 3.1. It is necessary to keep two separate data-
sets because the later experiments make use of the 
trained DTP classifier.   
For the two translation experiments, we also face 
a practical obstacle: we do not have an army of 
human translators at our disposal to translate the 
identified phrases. To make the studies possible, 
we rely on a pre-translated parallel corpus to simu-
late the process of asking a human to translate a 
phrase. That is, we use the phrase extraction toolkit 
to find translation phrases corresponding to each 
DTP candidate (note that the data used for this ex-
periment is separate from the main parallel corpus 
used to train the MT system, so the system has no 
knowledge about these translations).  
4.1 Automatic Labeling of DTP  
In this first experiment, we verify whether our 
method for creating positive and negative labeled 
examples of DTPs (as described in Section 3.1) is 
sound. Out of 2013 extracted phrases, we found  
949 positive instances (DTPs) and 1064 negative 
instances. The difficult phrases have an average 
length of 8.8 words while the other phrases have an 
average length of 7.8 words1.  We measured the 
BLEU scores for the MT outputs for both groups 
of phrases (Table 1).  
 
Experiment BLEU Score 
DTPs 14.34 
Non-DTPs 61.22 
Table 1: Isolated Translation of the selected training 
phrases 
 
The large gap between the translation qualities 
of the two phrase groups suggests that the DTPs 
are indeed much more ?difficult? than the other 
phrases. 
                                                          
1
 Arabic words are tokenized and lemmatized by Diab?s Ara-
bic Toolset (Diab 2004). 
4.2 Evaluation of the DTP Classifier 
We now perform a local evaluation of the trained 
DTP classifier for its classification accuracy.  The 
classifier is trained as an SVM using a linear ker-
nel.  The ?gold standard? phrases from the section 
4.1 are split into three groups: 2013 instances are 
used as training data for the classifier; 100 in-
stances are used for development (e.g., parameter 
tuning and feature engineering); and 200 instances 
are used as test instances.  The test set has an equal 
number of difficult and non-difficult phrases (50% 
baseline accuracy).  
In order to optimize the accuracy of classifica-
tion, we used a development set for feature engi-
neering and trying various SVM kernels and asso-
ciated parameters.  For the feature engineering 
part, we used the all-but-one heuristic to test the 
contribution of each individual feature.  Table 2 
presents the most and least contributing four fea-
tures that we used in our classification.  Among 
various features, we observed that the syntactic 
features are the most contributing sources of in-
formation for our classification. 
 
Least Useful Features Most Useful Features 
Ft1: Align Crossing Ft 2: Lexical Ambiguity 
Ft 8: Count of Nums Ft 11: Depth of subtree 
Ft:9: Count of Puncs Ft 12: Const type of Phr 
Ft 10: Count of NNPs Ft 13: Const type of Par 
Table 2: The most and least useful features 
 
The DTP classifier achieves an average accu-
racy of 71.5%, using 10 fold cross validation on 
the test set. 
4.3 Study on the effect of DTPs 
This experiment concentrates on the second half of 
the framework: that of constraining the MT system 
to use human-translations for the DTPs. Our objec-
tive is to assess to what degree do the DTPs nega-
tively impact the MT process. We compare the MT 
outputs of two groups of sentences.  Group I is 
made up of 242 sentences that contain the most 
difficult to translate phrases in the 1000 sentences 
we reserved for this study. Group II is a control 
group made up of 242 sentences with the least dif-
ficult to translate phrases.  The DTPs make up 
about 9% of word counts in the above 484 sen-
tences.  We follow the procedure described in Sec-
tion 3.1 to identify and score all the phrases; thus, 
252
this experiment can be considered an oracle study. 
We compare four scenarios: 
1. Adding phrase translations for Group I: MT 
system is constrained using the method de-
scribed in Section 3.2 to incorporate human 
translations of the pre-identified DTPs in 
Group I.2 
2. Adding phrase translations for Group II: 
MT system is constrained to use human trans-
lations for the identified (non-difficult) phrases 
in Group II.  
3. Adding translations for random phrases: 
randomly replace 242 phrases from either 
Group I or Group II. 
4. Adding translations for classifier labeled  
DTPs: human translations for phrases that our 
trained classifier has identified as DTPs from 
both Group I and Group II. 
 
  All of the above scenarios are evaluated on a 
combined set of 484 sentences (group 1 + group 2).  
This set up normalizes the relative difficulty of 
each grouping. 
If the DTPs negatively impact the MT process, 
we would expect to see a greater improvement 
when Group I phrases are translated by humans 
than when Group II phrases are translated by 
humans.  
The baseline for the comparisons is to evaluate 
the outputs of the MT system without using any 
human translations. This results in a BLEU score 
of 24.0. When human translations are used, the 
BLEU score of the dataset increases, as shown in 
Table 3. 
 
Experiment BLEU  
Baseline (no human trans) 24.0 
w/ translated DTPs (Group I) 39.6 
w/ translated non-DTPs (Group II) 33.7 
w/ translated phrases (random) 35.1 
w/ translated phrases (classifier) 37.0 
Table 3: A comparison of BLEU scores for the entire set 
of sentences under the constraints of using human trans-
lations for different types of phrases. 
 
While it is unsurprising that the inclusion of 
human translations increases the overall BLEU 
score, this comparison shows that the boost is 
sharper when more DTPs are translated. This is 
                                                          
2
 In this study, because the sentences are from the training 
parallel corpus, we can extract human translations directly 
from the corpus. 
consistent with our conjecture that pre-translating 
difficult phrases may be helpful. 
A more interesting question is whether the hu-
man translations still provide any benefit once we 
factor out their direct contributions to the increase 
in BLEU scores. To answer this question, we com-
pute the BLEU scores for the outputs again, this 
time filtering out all 484 identified phrases from 
the evaluation.  In other words in this experiment 
we focus on the part of the sentence that is not la-
beled and does include any human translations.  
Table 4 presents the results.   
 
Experiment BLEU 
Baseline (no human trans) 23.0 
w/ translated DTPs (Group I) 25.4 
w/ translated non-DTPs (Group II) 23.9 
w/ translated phrases (random) 24.5 
w/ translated phrases (classifier) 25.1 
Table 4: BLEU scores for the translation outputs ex-
cluding the 484 (DTP and non-DTP) phrases. 
 
The largest gain (2.4 BLEU increment from 
baseline) occurs when all and only the DTPs were 
translated. In contrast, replacing phrases from 
Group II did not improve the BLEU score very 
much. These results suggest that better handling of 
DTPs will have a positive effect on the overall MT 
process. We also note that using our SVM-trained 
classifier to identify the DTPs, the constrained MT 
system?s outputs obtained a BLEU score that is 
nearly as high as if a perfect classifier was used.  
4.4 Full evaluation of the framework       
This final experiment evaluates the complete 
framework as described in Section 3. The setup of 
this study is similar to that of the previous section.   
The main difference is that now, we rely on the 
classifier to predict which phrase would be the 
most difficult to translate and use human transla-
tions for those phrases. 
Out of 1000 sentences, 356 have been identified 
to contain DTPs (that are in the phrase extraction 
list). In other words, only 356 sentences hold DTPs 
that we can find their human translations through 
phrase projection.  For the remaining sentences, we 
do not use any human translation. 
 
 
 
 
 
253
Table 5 presents the increase in BLEU scores 
when human translations for the 356 DTPs are 
used. As expected the BLEU score increases, but 
the improvement is less dramatic than in the previ-
ous experiment because most sentences are un-
changed. 
 
Experiment BLEU  
Baseline (no human trans) 24.9 
w/ human translations  29.0 
Table 5: Entire Corpus level evaluation (1000 sen-
tences) when replacing DTPs in the hit list 
 
Table 6 summarizes the experimental results on 
the subset of the 356 sentences.  The first two rows 
compare the translation quality at the sentence 
level (similar to Table 3); the next two rows com-
pare the translation quality of the non-DTP parts 
(similar to Table 4).  Rows 1 and 3 are conditions 
when we do not use human translation; and rows 2 
and 4 are conditions when we replace DTPs with 
their associated human translations.  The im-
provements of the BLEU score for the hit list are 
similar to the results we have previously seen. 
 
Experiment on 356 sentences BLEU 
Baseline: full sent. 25.1 
w/ human translation: full sent.  37.6 
Baseline: discount DTPs 26.0 
w/ human translation: discount 
DTPs 
27.8 
Table 6: Evaluation of the subset of 356 sentences: both 
for the full sentence and for non-DTP parts, with and 
without human translation replacement of DTPs.  
5 Related Work 
Our work is related to the problem of confidence 
estimation for MT (Blatz et. al. 2004; Zen and Ney 
2006).  The confidence measure is a score for n-
grams generated by a decoder3. The measure is 
based on the features like lexical probabilities 
(word posterior), phrase translation probabilities, 
N-best translation hypothesis, etc.  Our DTP classi-
fication differs from the confidence measuring in 
several aspects: one of the main purposes of our 
classification of DTPs is to optimize the usage of 
outside resources.  To do so, we focus on classifi-
cation of phrases which are syntactically meaning-
ful, because those syntactic constituent units have 
                                                          
3
 Most of the confidence estimation measures are for unigrams 
(word level measures). 
less dependency to the whole sentence structure 
and can be translated independently.  Our classifi-
cation relies on syntactic features that are impor-
tant source of information about the MT difficulty 
and also are useful for further error tracking (rea-
sons behind the difficulty).  Our classification is 
performed as a pre-translation step, so it does not 
rely on the output of the MT system for a test sen-
tence; instead, it uses a parallel training corpus and 
the characteristics of the underlying MT system 
(e.g.: phrase translations, lexical probabilities).   
Confidence measures have been used for error 
correction and interactive MT systems. Ueffing 
and Ney (2005) employed confidence measures 
within a trans-type-style interactive MT system.  In 
their system, the MT system iteratively generates 
the translation and the human translator accepts a 
part of the proposed translation by typing one or 
more prefix characters.  The system regenerates a 
new translation based on the human prefix input 
and word level confidence measures.  In contrast, 
our proposed usage of human knowledge is for 
translation at the phrase level.  We use syntactic 
restrictions to make the extracted phrases meaning-
ful and easy to translate in isolation.  In other 
words, by the usage of our framework trans-type 
systems can use human knowledge at the phrase 
level for the most difficult segments of a sentence.  
Additionally by the usage of our framework, the 
MT system performs the decoding task only once.   
The idea of isolated phrase translation has been 
explored successfully in MT community.  Koehn 
and Knight (2003) used isolated translation of NP 
and PP phrases and merge them with the phrase 
based MT system to translate the complete sen-
tence.  In our work, instead of focusing on specific 
type of phrases (NP or PP), we focus on isolated 
translation of difficult phrases with an aim to im-
prove the translation quality of non-difficult seg-
ments too. 
6 Conclusion and Future Work  
We have presented an MT framework that makes 
use of additional information about difficult-to-
translate source phrases.  Our framework includes 
an SVM-based phrase classifier that finds the seg-
ment of a sentence that is most difficult to trans-
late.  Our classifier achieves a promising 71.5% 
accuracy. By asking external sources (such as hu-
man translators) to pre-translate these DTPs and 
using them to constrain the MT process, we im-
254
prove the system outputs for the other parts of the 
sentences.  
We plan to extend this work in several direc-
tions.  First, our framework can be augmented to 
include multiple MT systems. We expect different 
systems will have difficulties with different con-
structs, and thus they may support each other, and 
thus reducing the need to ask human translators for 
help with the difficult phrases. Second, our current 
metric for phrasal difficulty depends on BLEU.  
Considering the recent debates about the shortcom-
ings of the BLEU score (Callison-Burch et. al. 
2006), we are interested in applying alternative 
metrics such a Meteor (Banerjee and Lavie 2005).  
Third, we believe that there is more room for im-
provement and extension of our classification fea-
tures.  Specifically, we believe that our syntactic 
analysis of source sentences can be improved by 
including richer parsing features.  Finally, the 
framework can also be used to diagnose recurring 
problems in the MT system.  We are currently de-
veloping methods for improving the translation of 
the difficult phrases for the phrase-based MT sys-
tem used in our experiments. 
Acknowledgements  
This work is supported by NSF Grant IIS-0612791. 
We would like to thank Alon Lavie, Mihai Rotaru 
and the NLP group at Pitt as well as the anony-
mous reviewers for their valuable comments. 
References  
Satanjeev Banerjee, Alon Lavie. 2005. METEOR: An 
automatic metric for MT evaluation with improved 
correlation with human judgments. In Proceedings of 
the ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization, pages 
65?72. 
Daniel M. Bikel. 2002. Design of a multi-lingual, paral-
lel-processing statistical parsing engine. In Proceed-
ings of ARPA Workshop on Human Language Tech-
nology 
John Blatz, Erin Fitzgerald, George Foster, Simona Gan 
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, 
and Nicola Ueffing. 2003. Confidence estimation for 
machine translation. Technical report, Center for 
Language and Speech Processing, Johns Hopkins 
University, Baltimore. Summer Workshop Final Re-
port. 
Chris Callison-Burch, Miles Osborne, and Philip 
Koehn. 2006. Re-evaluating the Role of Bleu in Ma-
chine Translation Research. In Proc. of the European 
Chapter of the Association for Computational Lin-
guistics (EACL), Trento, Italy. 
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004. 
Automatic tagging of Arabic text: From raw text to 
base phrase chunks. In Proceeding of NAACL-HLT 
2004. Boston, MA. 
Thorsten Joachims, Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and A. 
Smola (ed.), MIT-Press, 1999. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the As-
sociation for Machine Translation in the Americas, 
pages 115?124 
Philipp Koehn and Kevin Knight. 2003. Feature-rich 
statistical translation of noun phrases. In Proceedings 
of 41st  the Annual Meeting on Association for Com-
putational Linguistics (ACL-2003), pages 311?318. 
Franz Och, 2001, ?Giza++: Training of statistical trans-
lation model?:  http://www.fjoch.com/GIZA++.html 
Franz. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Kishore Papineni and Salim Roukos and Todd Ward 
and Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation.  In Proceed-
ings of the 40th Annual Meeting on Association for 
Computational Linguistics (ACL-2002), Pages 311-
318, Philadelphia, PA 
Nicola Ueffing and Hermann Ney. 2005. Application of 
word-level confidence measures in translation. In 
Proceedings of the conference of the European Asso-
ciation of Machine Translation (EAMT 2005) , pages 
262?270, Budapest, Hungary 
Richard Zens and Hermann Ney, 2006. N -Gram Poste-
rior Probabilities for Statistical Machine Translation. 
In Proceedings of ACL Workshop on Statistical Ma-
chine Translation. 2006 
 
255
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207?213,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Human Judgment Corpus and a Metric for Arabic MT Evaluation
Houda Bouamor, Hanan Alshikhabobakr, Behrang Mohit and Kemal Oflazer
Carnegie Mellon University in Qatar
{hbouamor,halshikh,behrang,ko}@cmu.edu
Abstract
We present a human judgments dataset
and an adapted metric for evaluation of
Arabic machine translation. Our medium-
scale dataset is the first of its kind for Ara-
bic with high annotation quality. We use
the dataset to adapt the BLEU score for
Arabic. Our score (AL-BLEU) provides
partial credits for stem and morphologi-
cal matchings of hypothesis and reference
words. We evaluate BLEU, METEOR and
AL-BLEU on our human judgments cor-
pus and show that AL-BLEU has the high-
est correlation with human judgments. We
are releasing the dataset and software to
the research community.
1 Introduction
Evaluation of Machine Translation (MT) contin-
ues to be a challenging research problem. There
is an ongoing effort in finding simple and scal-
able metrics with rich linguistic analysis. A wide
range of metrics have been proposed and evaluated
mostly for European target languages (Callison-
Burch et al., 2011; Mach?a?cek and Bojar, 2013).
These metrics are usually evaluated based on their
correlation with human judgments on a set of MT
output. While there has been growing interest in
building systems for translating into Arabic, the
evaluation of Arabic MT is still an under-studied
problem. Standard MT metrics such as BLEU (Pa-
pineni et al., 2002) or TER (Snover et al., 2006)
have been widely used for evaluating Arabic MT
(El Kholy and Habash, 2012). These metrics use
strict word and phrase matching between the MT
output and reference translations. For morpholog-
ically rich target languages such as Arabic, such
criteria are too simplistic and inadequate. In this
paper, we present: (a) the first human judgment
dataset for Arabic MT (b) the Arabic Language
BLEU (AL-BLEU), an extension of the BLEU
score for Arabic MT evaluation.
Our annotated dataset is composed of the output
of six MT systems with texts from a diverse set of
topics. A group of ten native Arabic speakers an-
notated this corpus with high-levels of inter- and
intra-annotator agreements. Our AL-BLEU met-
ric uses a rich set of morphological, syntactic and
lexical features to extend the evaluation beyond
the exact matching. We conduct different exper-
iments on the newly built dataset and demonstrate
that AL-BLEU shows a stronger average correla-
tion with human judgments than the BLEU and
METEOR scores. Our dataset and our AL-BLEU
metric provide useful testbeds for further research
on Arabic MT and its evaluation.
1
2 Related Work
Several studies on MT evaluation have pointed out
the inadequacy of the standard n-gram based eval-
uation metrics for various languages (Callison-
Burch et al., 2006). For morphologically complex
languages and those without word delimiters, sev-
eral studies have attempted to improve upon them
and suggest more reliable metrics that correlate
better with human judgments (Denoual and Lep-
age, 2005; Homola et al., 2009).
A common approach to the problem of mor-
phologically complex words is to integrate some
linguistic knowledge in the metric. ME-
TEOR (Denkowski and Lavie, 2011), TER-
Plus (Snover et al., 2010) incorporate limited lin-
guistic resources. Popovi?c and Ney (2009) showed
that n-gram based evaluation metrics calculated on
POS sequences correlate well with human judg-
ments, and recently designed and evaluated MPF,
a BLEU-style metric based on morphemes and
POS tags (Popovi?c, 2011). In the same direc-
1
The dataset and the software are available at:
http://nlp.qatar.cmu.edu/resources/
AL-BLEU
207
tion, Chen and Kuhn (2011) proposed AMBER,
a modified version of BLEU incorporating re-
call, extra penalties, and light linguistic knowl-
edge about English morphology. Liu et al. (2010)
propose TESLA-M, a variant of a metric based
on n-gram matching that utilizes light-weight lin-
guistic analysis including lemmatization, POS tag-
ging, and WordNet synonym relations. This met-
ric was then extended to TESLA-B to model
phrase synonyms by exploiting bilingual phrase
tables (Dahlmeier et al., 2011). Tantug et al.
(2008) presented BLEU+, a tool that implements
various extension to BLEU computation to allow
for a better evaluation of the translation perfor-
mance for Turkish.
To the best of our knowledge the only human
judgment dataset for Arabic MT is the small cor-
pus which was used to tune parameters of the ME-
TEOR metric for Arabic (Denkowski and Lavie,
2011). Due to the shortage of Arabic human judg-
ment dataset, studies on the performance of eval-
uation metrics have been constrained and limited.
A relevant effort in this area is the upper-bound es-
timation of BLEU and METEOR scores for Ara-
bic MT output (El Kholy and Habash, 2011). As
part of its extensive functionality, the AMEANA
system provides the upper-bound estimate by an
exhaustive matching of morphological and lexical
features between the hypothesis and the reference
translations. Our use of morphological and lex-
ical features overlaps with the AMEANA frame-
work. However, we extend our partial matching
to a supervised tuning framework for estimating
the value of partial credits. Moreover, our human
judgment dataset allows us to validate our frame-
work with a large-scale gold-standard data.
3 Human judgment dataset
We describe here our procedure for compiling a
diverse Arabic MT dataset and annotating it with
human judgments.
3.1 Data and systems
We annotate a corpus composed of three datasets:
(1) the standard English-Arabic NIST 2005 cor-
pus, commonly used for MT evaluations and com-
posed of news stories. We use the first English
translation as the source and the single corre-
sponding Arabic sentence as the reference. (2) the
MEDAR corpus (Maegaard et al., 2010) that con-
sists of texts related to the climate change with
four Arabic reference translations. We only use
the first reference in this study. (3) a small dataset
of Wikipedia articles (WIKI) to extend our cor-
pus and metric evaluation to topics beyond the
commonly-used news topics. This sub-corpus
consists of our in-house Arabic translations of
seven English Wikipedia articles. The articles are:
Earl Francis Lloyd, Western Europe, Citizenship,
Marcus Garvey, Middle Age translation, Acadian,
NBA. The English articles which do not exist in
the Arabic Wikipedia were manually translated by
a bilingual linguist.
Table 1 gives an overview of these sub-corpora
characteristics.
NIST MEDAR WIKI
# of Documents 100 4 7
# of Sentences 1056 509 327
Table 1: Statistics on the datasets.
We use six state-of-the-art English-to-Arabic
MT systems. These include four research-oriented
phrase-based systems with various morphological
and syntactic features and different Arabic tok-
enization schemes and also two commercial off-
the-shelf systems.
3.2 Annotation of human judgments
In order conduct a manual evaluation of the six
MT systems, we formulated it as a ranking prob-
lem. We adapt the framework used in the WMT
2011 shared task for evaluating MT metrics on
European language pairs (Callison-Burch et al.,
2011) for Arabic MT. We gather human ranking
judgments by asking ten annotators (each native
speaker of Arabic with English as a second lan-
guage) to assess the quality of the English-Arabic
systems, by ranking sentences relative to each
other, from the best to the worst (ties are allowed).
We use the Appraise toolkit (Federmann, 2012)
designed for manual MT evaluation. The tool dis-
plays to the annotator, the source sentence and
translations produced by various MT systems. The
annotators received initial training on the tool and
the task with ten sentences. They were presented
with a brief guideline indicating the purpose of the
task and the main criteria of MT output evaluation.
Each annotator was assigned to 22 ranking
tasks. Each task included ten screens. Each screen
involveed ranking translations of ten sentences. In
total, we collected 22, 000 rankings for 1892 sen-
208
tences (22 tasks?10 screens?10 judges). In each
annotation screen, the annotator was shown the
source-language (English) sentences, as well as
five translations to be ranked. We did not provide
annotators with the reference to avoid any bias in
the annotation process. Each source sentence was
presented with its direct context. Rather than at-
tempting to get a complete ordering over the sys-
tems, we instead relied on random selection and a
reasonably large sample size to make the compar-
isons fair (Callison-Burch et al., 2011).
An example of a source sentence and its five
translations to be ranked is given in Table 2.
3.3 Annotation quality and analysis
In order to ensure the validity of any evaluation
setup, a reasonable of inter- and intra-annotator
agreement rates in ranking should exist. To mea-
sure these agreements, we deliberately reassigned
10% of the tasks to second annotators. More-
over, we ensured that 10% of the screens are re-
displayed to the same annotator within the same
task. This procedure allowed us to collect reliable
quality control measure for our dataset.
?
inter
?
intra
EN-AR 0.57 0.62
Average EN-EU 0.41 0.57
EN-CZ 0.40 0.54
Table 3: Inter- and intra-annotator agreement
scores for our annotation compared to the aver-
age scores for five English to five European lan-
guages and also English-Czech (Callison-Burch et
al., 2011).
We measured head-to-head pairwise agreement
among annotators using Cohen?s kappa (?) (Co-
hen, 1968), defined as follows:
? =
P (A)? P (E)
1? P (E)
where P(A) is the proportion of times annotators
agree and P(E) is the proportion of agreement by
chance.
Table 3 gives average values obtained for inter-
annotator and intra-annotator agreement and com-
pare our results to similar annotation efforts in
WMT-13 on different European languages. Here
we compare against the average agreement for En-
glish to five languages and also from English to
one morphologically rich language (Czech).
4
Based on Landis and Koch (1977) ? interpre-
tation, the ?
inter
value (57%) and also compar-
ing our agreement scores with WMT-13 annota-
tions, we believe that we have reached a reliable
and consistent annotation quality.
4 AL-BLEU
Despite its well-known shortcomings (Callison-
Burch et al., 2006), BLEU continues to be the
de-facto MT evaluation metric. BLEU uses an
exact n-gram matching criterion that is too strict
for a morphologically rich language like Arabic.
The system outputs in Table 2 are examples of
how BLEU heavily penalizes Arabic. Based on
BLEU, the best hypothesis is from Sys
5
which has
three unigram and one bigram exact matches with
the reference. However, the sentence is the 4
th
ranked by annotators. In contrast, the output of
Sys
3
(ranked 1
st
by annotators) has only one ex-
act match, but several partial matches when mor-
phological and lexical information are taken into
consideration.
We propose the Arabic Language BLEU (AL-
BLEU) metric which extends BLEU to deal with
Arabic rich morphology. We extend the matching
to morphological, syntactic and lexical levels with
an optimized partial credit. AL-BLEU starts with
the exact matching of hypothesis tokens against
the reference tokens. Furthermore, it considers the
following: (a) morphological and syntactic feature
matching, (b) stem matching. Based on Arabic lin-
guistic intuition, we check the matching of a sub-
set of 5 morphological features: (i) POS tag, (ii)
gender (iii) number (iv) person (v) definiteness.
We use the MADA package (Habash et al., 2009)
to collect the stem and the morphological features
of the hypothesis and reference translation.
Figure 1 summarizes the function in which we
consider partial matching (m(t
h
, t
r
)) of a hypoth-
esis token (t
h
) and its associated reference token
(t
r
). Starting with the BLEU criterion, we first
check if the hypothesis token is same as the ref-
erence one and provide the full credit for it. If
the exact matching fails, we provide partial credit
for matching at the stem and morphological level.
The value of the partial credits are the sum of
the stem weight (w
s
) and the morphological fea-
4
We compare against the agreement score for annotations
performed by WMT researchers which are higher than the
WMT annotations on Mechanical Turk.
209
Source France plans to attend ASEAN emergency summit.
Reference .

?

KPA??@
	
?AJ


?B@

??

?
P?
	
?k ?
	
Q

?

K A?
	
Q
	
?
frnsaA tEtzm HDwr qmp AaAlaAsyaAn AaAlTaAr}ip
Hypothesis
Systems Rank
Annot
BLEU Rank
BLEU
AL-BLEU Rank
AL?BLEU
Sys
1
2 0.0047 2 0.4816 1

?

KPA??@
	
?AJ


?

B@

??

?
P?
	
?m
?
A?
	
Q
	
? ??
	
m
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162?173,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Recall-Oriented Learning of Named Entities in Arabic Wikipedia
Behrang Mohit? Nathan Schneider? Rishav Bhowmick? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?P.O. Box 24866, Doha, Qatar ?Pittsburgh, PA 15213, USA
{behrang@,nschneid@cs.,rishavb@qatar.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the problem of NER in Arabic
Wikipedia, a semisupervised domain adap-
tation setting for which we have no labeled
training data in the target domain. To fa-
cilitate evaluation, we obtain annotations
for articles in four topical groups, allow-
ing annotators to identify domain-specific
entity types in addition to standard cate-
gories. Standard supervised learning on
newswire text leads to poor target-domain
recall. We train a sequence model and show
that a simple modification to the online
learner?a loss function encouraging it to
?arrogantly? favor recall over precision?
substantially improves recall and F1. We
then adapt our model with self-training
on unlabeled target-domain data; enforc-
ing the same recall-oriented bias in the self-
training stage yields marginal gains.1
1 Introduction
This paper considers named entity recognition
(NER) in text that is different from most past re-
search on NER. Specifically, we consider Arabic
Wikipedia articles with diverse topics beyond the
commonly-used news domain. These data chal-
lenge past approaches in two ways:
First, Arabic is a morphologically rich lan-
guage (Habash, 2010). Named entities are ref-
erenced using complex syntactic constructions
(cf. English NEs, which are primarily sequences
of proper nouns). The Arabic script suppresses
most vowels, increasing lexical ambiguity, and
lacks capitalization, a key clue for English NER.
Second, much research has focused on the use
of news text for system building and evaluation.
Wikipedia articles are not news, belonging instead
to a wide range of domains that are not clearly
1The annotated dataset and a supplementary document
with additional details of this work can be found at:
http://www.ark.cs.cmu.edu/AQMAR
delineated. One hallmark of this divergence be-
tween Wikipedia and the news domain is a dif-
ference in the distributions of named entities. In-
deed, the classic named entity types (person, or-
ganization, location) may not be the most apt for
articles in other domains (e.g., scientific or social
topics). On the other hand, Wikipedia is a large
dataset, inviting semisupervised approaches.
In this paper, we describe advances on the prob-
lem of NER in Arabic Wikipedia. The techniques
are general and make use of well-understood
building blocks. Our contributions are:
? A small corpus of articles annotated in a new
scheme that provides more freedom for annota-
tors to adapt NE analysis to new domains;
? An ?arrogant? learning approach designed to
boost recall in supervised training as well as
self-training; and
? An empirical evaluation of this technique as ap-
plied to a well-established discriminative NER
model and feature set.
Experiments show consistent gains on the chal-
lenging problem of identifying named entities in
Arabic Wikipedia text.
2 Arabic Wikipedia NE Annotation
Most of the effort in NER has been fo-
cused around a small set of domains and
general-purpose entity classes relevant to those
domains?especially the categories PER(SON),
ORG(ANIZATION), and LOC(ATION) (POL),
which are highly prominent in news text. Ara-
bic is no exception: the publicly available NER
corpora?ACE (Walker et al 2006), ANER (Be-
najiba et al 2008), and OntoNotes (Hovy et al
2006)?all are in the news domain.2 However,
2OntoNotes contains news-related text. ACE includes
some text from blogs. In addition to the POL classes, both
corpora include additional NE classes such as facility, event,
product, vehicle, etc. These entities are infrequent and may
not be comprehensive enough to cover the larger set of pos-
162
History Science Sports Technology
dev: Damascus Atom Rau?l Gonza?les Linux
Imam Hussein Shrine Nuclear power Real Madrid Solaris
test: Crusades Enrico Fermi 2004 Summer Olympics Computer
Islamic Golden Age Light Christiano Ronaldo Computer Software
Islamic History Periodic Table Football Internet
Ibn Tolun Mosque Physics Portugal football team Richard Stallman
Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System
Claudio Filippone (PER) 	??J. ?J

	
? ?K
X???; Linux (SOFTWARE) ??
	
JJ
?; Spanish
League (CHAMPIONSHIPS) ?


	
GAJ.?B@ ?

P?Y?@; proton (PARTICLE) 	??K?QK. ; nuclear
radiation (GENERIC-MISC) ?


??
	
J? @ ?A? ?B@; Real Zaragoza (ORG)

???

Q??? ?AK
P
Table 1: Translated titles
of Arabic Wikipedia arti-
cles in our development
and test sets, and some
NEs with standard and
article-specific classes.
Additionally, Prussia and
Amman were reserved
for training annotators,
and Gulf War for esti-
mating inter-annotator
agreement.
appropriate entity classes will vary widely by do-
main; occurrence rates for entity classes are quite
different in news text vs. Wikipedia, for instance
(Balasuriya et al 2009). This is abundantly
clear in technical and scientific discourse, where
much of the terminology is domain-specific, but it
holds elsewhere. Non-POL entities in the history
domain, for instance, include important events
(wars, famines) and cultural movements (roman-
ticism). Ignoring such domain-critical entities
likely limits the usefulness of the NE analysis.
Recognizing this limitation, some work on
NER has sought to codify more robust invento-
ries of general-purpose entity types (Sekine et al
2002; Weischedel and Brunstein, 2005; Grouin
et al 2011) or to enumerate domain-specific
types (Settles, 2004; Yao et al 2003). Coarse,
general-purpose categories have also been used
for semantic tagging of nouns and verbs (Cia-
ramita and Johnson, 2003). Yet as the number
of classes or domains grows, rigorously docu-
menting and organizing the classes?even for a
single language?requires intensive effort. Ide-
ally, an NER system would refine the traditional
classes (Hovy et al 2011) or identify new entity
classes when they arise in new domains, adapting
to new data. For this reason, we believe it is valu-
able to consider NER systems that identify (but
do not necessarily label) entity mentions, and also
to consider annotation schemes that allow annota-
tors more freedom in defining entity classes.
Our aim in creating an annotated dataset is to
provide a testbed for evaluation of new NER mod-
els. We will use these data as development and
sible NEs (Sekine et al 2002). Nezda et al(2006) anno-
tated and evaluated an Arabic NE corpus with an extended
set of 18 classes (including temporal and numeric entities);
this corpus has not been released publicly.
testing examples, but not as training data. In ?4
we will discuss our semisupervised approach to
learning, which leverages ACE and ANER data
as an annotated training corpus.
2.1 Annotation Strategy
We conducted a small annotation project on Ara-
bic Wikipedia articles. Two college-educated na-
tive Arabic speakers annotated about 3,000 sen-
tences from 31 articles. We identified four top-
ical areas of interest?history, technology, sci-
ence, and sports?and browsed these topics un-
til we had found 31 articles that we deemed sat-
isfactory on the basis of length (at least 1,000
words), cross-lingual linkages (associated articles
in English, German, and Chinese3), and subjec-
tive judgments of quality. The list of these arti-
cles along with sample NEs are presented in ta-
ble 1. These articles were then preprocessed to
extract main article text (eliminating tables, lists,
info-boxes, captions, etc.) for annotation.
Our approach follows ACE guidelines (LDC,
2005) in identifying NE boundaries and choos-
ing POL tags. In addition to this traditional form
of annotation, annotators were encouraged to ar-
ticulate one to three salient, article-specific en-
tity categories per article. For example, names
of particles (e.g., proton) are highly salient in the
Atom article. Annotators were asked to read the
entire article first, and then to decide which non-
traditional classes of entities would be important
in the context of article. In some cases, annotators
reported using heuristics (such as being proper
3These three languages have the most articles on
Wikipedia. Associated articles here are those that have been
manually hyperlinked from the Arabic page as cross-lingual
correspondences. They are not translations, but if the associ-
ations are accurate, these articles should be topically similar
to the Arabic page that links to them.
163
Token position agreement rate 92.6% Cohen?s ?: 0.86
Token agreement rate 88.3% Cohen?s ?: 0.86
Token F1 between annotators 91.0%
Entity boundary match F1 94.0%
Entity category match F1 87.4%
Table 2: Inter-annotator agreement measurements.
nouns or having an English translation which is
conventionally capitalized) to help guide their de-
termination of non-canonical entities and entity
classes. Annotators produced written descriptions
of their classes, including example instances.
This scheme was chosen for its flexibility: in
contrast to a scenario with a fixed ontology, anno-
tators required minimal training beyond the POL
conventions, and did not have to worry about
delineating custom categories precisely enough
that they would extend straightforwardly to other
topics or domains. Of course, we expect inter-
annotator variability to be greater for these open-
ended classification criteria.
2.2 Annotation Quality Evaluation
During annotation, two articles (Prussia and Am-
man) were reserved for training annotators on
the task. Once they were accustomed to anno-
tation, both independently annotated a third ar-
ticle. We used this 4,750-word article (Gulf War,

?J

	
K A

J? @ i. J
?
	
m?'@ H. Qk) to measure inter-annotator
agreement. Table 2 provides scores for token-
level agreement measures and entity-level F1 be-
tween the two annotated versions of the article.4
These measures indicate strong agreement for
locating and categorizing NEs both at the token
and chunk levels. Closer examination of agree-
ment scores shows that PER and MIS classes have
the lowest rates of agreement. That the mis-
cellaneous class, used for infrequent or article-
specific NEs, receives poor agreement is unsur-
prising. The low agreement on the PER class
seems to be due to the use of titles and descriptive
terms in personal names. Despite explicit guide-
lines to exclude the titles, annotators disagreed on
the inclusion of descriptors that disambiguate the
NE (e.g., the father in H.

B@ ??K. h. Qk. : George
Bush, the father).
4The position and boundary measures ignore the distinc-
tions between the POLM classes. To avoid artificial inflation
of the token and token position agreement rates, we exclude
the 81% of tokens tagged by both annotators as not belong-
ing to an entity.
History: Gulf War, Prussia, Damascus, Crusades
WAR CONFLICT ? ? ?
Science: Atom, Periodic table
THEORY ? CHEMICAL ? ?
NAME ROMAN ? PARTICLE ? ?
Sports: Football, Rau?l Gonza?les
SPORT ? CHAMPIONSHIP ?
AWARD ? NAME ROMAN ?
Technology: Computer, Richard Stallman
COMPUTER VARIETY ? SOFTWARE ?
COMPONENT ?
Table 3: Custom NE categories suggested by one or
both annotators for 10 articles. Article titles are trans-
lated from Arabic. ? indicates that both annotators vol-
unteered a category for an article; ? indicates that only
one annotator suggested the category. Annotators were
not given a predetermined set of possible categories;
rather, category matches between annotators were de-
termined by post hoc analysis. NAME ROMAN indi-
cates an NE rendered in Roman characters.
2.3 Validating Category Intuitions
To investigate the variability between annotators
with respect to custom category intuitions, we
asked our two annotators to independently read
10 of the articles in the data (scattered across our
four focus domains) and suggest up to 3 custom
categories for each. We assigned short names to
these suggestions, seen in table 3. In 13 cases,
both annotators suggested a category for an article
that was essentially the same (?); three such cat-
egories spanned multiple articles. In three cases
a category was suggested by only one annotator
(?).5 Thus, we see that our annotators were gen-
erally, but not entirely, consistent with each other
in their creation of custom categories. Further, al-
most all of our article-specific categories corre-
spond to classes in the extended NE taxonomy of
(Sekine et al 2002), which speaks to the reason-
ableness of both sets of categories?and by exten-
sion, our open-ended annotation process.
Our annotation of named entities outside of the
traditional POL classes creates a useful resource
for entity detection and recognition in new do-
mains. Even the ability to detect non-canonical
types of NEs should help applications such as QA
and MT (Toral et al 2005; Babych and Hart-
ley, 2003). Possible avenues for future work
include annotating and projecting non-canonical
5When it came to tagging NEs, one of the two annota-
tors was assigned to each article. Custom categories only
suggested by the other annotator were ignored.
164
NEs from English articles to their Arabic coun-
terparts (Hassan et al 2007), automatically clus-
tering non-canonical types of entities into article-
specific or cross-article classes (cf. Frietag, 2004),
or using non-canonical classes to improve the
(author-specified) article categories in Wikipedia.
Hereafter, we merge all article-specific cate-
gories with the generic MIS category. The pro-
portion of entity mentions that are tagged as MIS,
while varying to a large extent by document, is
a major indication of the gulf between the news
data (<10%) and the Wikipedia data (53% for the
development set, 37% for the test set).
Below, we aim to develop entity detection mod-
els that generalize beyond the traditional POL en-
tities. We do not address here the challenges of
automatically classifying entities or inferring non-
canonical groupings.
3 Data
Table 4 summarizes the various corpora used in
this work.6 Our NE-annotated Wikipedia sub-
corpus, described above, consists of several Ara-
bic Wikipedia articles from four focus domains.7
We do not use these for supervised training data;
they serve only as development and test data. A
larger set of Arabic Wikipedia articles, selected
on the basis of quality heuristics, serves as unla-
beled data for semisupervised learning.
Our out-of-domain labeled NE data is drawn
from the ANER (Benajiba et al 2007) and
ACE-2005 (Walker et al 2006) newswire cor-
pora. Entity types in this data are POL cate-
gories (PER, ORG, LOC) and MIS. Portions of the
ACE corpus were held out as development and
test data; the remainder is used in training.
4 Models
Our starting point for statistical NER is a feature-
based linear model over sequences, trained using
the structured perceptron (Collins, 2002).8
In addition to lexical and morphological9 fea-
6Additional details appear in the supplement.
7We downloaded a snapshot of Arabic Wikipedia
(http://ar.wikipedia.org) on 8/29/2009 and pre-
processed the articles to extract main body text and metadata
using the mwlib package for Python (PediaPress, 2010).
8A more leisurely discussion of the structured percep-
tron and its connection to empirical risk minimization can
be found in the supplementary document.
9We obtain morphological analyses from the MADA tool
(Habash and Rambow, 2005; Roth et al 2008).
Training words NEs
ACE+ANER 212,839 15,796
Wikipedia (unlabeled, 397 docs) 1,110,546 ?
Development
ACE 7,776 638
Wikipedia (4 domains, 8 docs) 21,203 2,073
Test
ACE 7,789 621
Wikipedia (4 domains, 20 docs) 52,650 3,781
Table 4: Number of words (entity mentions) in data sets.
tures known to work well for Arabic NER (Be-
najiba et al 2008; Abdul-Hamid and Darwish,
2010), we incorporate some additional features
enabled by Wikipedia. We do not employ a
gazetteer, as the construction of a broad-domain
gazetteer is a significant undertaking orthogo-
nal to the challenges of a new text domain like
Wikipedia.10 A descriptive list of our features is
available in the supplementary document.
We use a first-order structured perceptron; none
of our features consider more than a pair of con-
secutive BIO labels at a time. The model enforces
the constraint that NE sequences must begin with
B (so the bigram ?O, I? is disallowed).
Training this model on ACE and ANER data
achieves performance comparable to the state of
the art (F1-measure11 above 69%), but fares much
worse on our Wikipedia test set (F1-measure
around 47%); details are given in ?5.
4.1 Recall-Oriented Perceptron
By augmenting the perceptron?s online update
with a cost function term, we can incorporate a
task-dependent notion of error into the objective,
as with structured SVMs (Taskar et al 2004;
Tsochantaridis et al 2005). Let c(y,y?) denote
a measure of error when y is the correct label se-
quence but y? is predicted. For observed sequence
x and feature weights (model parameters) w, the
structured hinge loss is `hinge(x,y,w) =
max
y?
(
w>g(x,y?) + c(y,y?)
)
?w>g(x,y)
(1)
The maximization problem inside the parentheses
is known as cost-augmented decoding. If c fac-
10A gazetteer ought to yield further improvements in line
with previous findings in NER (Ratinov and Roth, 2009).
11Though optimizing NER systems for F1 has been called
into question (Manning, 2006), no alternative metric has
achieved widespread acceptance in the community.
165
tors similarly to the feature function g(x,y), then
we can increase penalties for y that have more
local mistakes. This raises the learner?s aware-
ness about how it will be evaluated. Incorporat-
ing cost-augmented decoding into the perceptron
leads to this decoding step:
y? ? arg max
y?
(
w>g(x,y?) + c(y,y?)
)
, (2)
which amounts to performing stochastic subgradi-
ent ascent on an objective function with the Eq. 1
loss (Ratliff et al 2006).
In this framework, cost functions can be for-
mulated to distinguish between different types of
errors made during training. For a tag sequence
y = ?y1, y2, . . . , yM ?, Gimpel and Smith (2010b)
define word-local cost functions that differently
penalize precision errors (i.e., yi = O ? y?i 6= O
for the ith word), recall errors (yi 6= O? y?i = O),
and entity class/position errors (other cases where
yi 6= y?i). As will be shown below, a key problem
in cross-domain NER is poor recall, so we will
penalize recall errors more severely:
c(y,y?) =
M?
i=1
?
?
?
0 if yi = y?i
? if yi 6= O ? y?i = O
1 otherwise
(3)
for a penalty parameter ? > 1. We call our learner
the ?recall-oriented? perceptron (ROP).
We note that Minkov et al(2006) similarly ex-
plored the recall vs. precision tradeoff in NER.
Their technique was to directly tune the weight
of a single feature?the feature marking O (non-
entity tokens); a lower weight for this feature will
incur a greater penalty for predicting O. Below
we demonstrate that our method, which is less
coarse, is more successful in our setting.12
In our experiments we will show that injecting
?arrogance? into the learner via the recall-oriented
loss function substantially improves recall, espe-
cially for non-POL entities (?5.3).
4.2 Self-Training and Semisupervised
Learning
As we will show experimentally, the differences
between news text and Wikipedia text call for do-
main adaptation. In the case of Arabic Wikipedia,
12The distinction between the techniques is that our cost
function adjusts the whole model in order to perform better
at recall on the training data.
Input: labeled data ??x(n),y(n)??Nn=1; unlabeled
data ?x?(j)?Jj=1; supervised learner L;
number of iterations T ?
Output: w
w? L(??x(n),y(n)??Nn=1)
for t = 1 to T ? do
for j = 1 to J do
y?(j) ? arg maxy w
>g(x?(j),y)
w? L(??x(n),y(n)??Nn=1 ? ??x?
(j), y?(j)??Jj=1)
Algorithm 1: Self-training.
there is no available labeled training data. Yet
the available unlabeled data is vast, so we turn to
semisupervised learning.
Here we adapt self-training, a simple tech-
nique that leverages a supervised learner (like the
perceptron) to perform semisupervised learning
(Clark et al 2003; Mihalcea, 2004; McClosky
et al 2006). In our version, a model is trained
on the labeled data, then used to label the un-
labeled target data. We iterate between training
on the hypothetically-labeled target data plus the
original labeled set, and relabeling the target data;
see Algorithm 1. Before self-training, we remove
sentences hypothesized not to contain any named
entity mentions, which we found avoids further
encouragement of the model toward low recall.
5 Experiments
We investigate two questions in the context of
NER for Arabic Wikipedia:
? Loss function: Does integrating a cost func-
tion into our learning algorithm, as we have
done in the recall-oriented perceptron (?4.1),
improve recall and overall performance on
Wikipedia data?
? Semisupervised learning for domain adap-
tation: Can our models benefit from large
amounts of unlabeled Wikipedia data, in addi-
tion to the (out-of-domain) labeled data? We
experiment with a self-training phase following
the fully supervised learning phase.
We report experiments for the possible combi-
nations of the above ideas. These are summarized
in table 5. Note that the recall-oriented percep-
tron can be used for the supervised learning phase,
for the self-training phase, or both. This leaves us
with the following combinations:
? reg/none (baseline): regular supervised learner.
? ROP/none: recall-oriented supervised learner.
166
Figure 1: Tuning the recall-oriented cost parame-
ter for different learning settings. We optimized
for development set F1, choosing penalty ? = 200
for recall-oriented supervised learning (in the plot,
ROP/*?this is regardless of whether a stage of
self-training will follow); ? = 100 for recall-
oriented self-training following recall-oriented su-
pervised learning (ROP/ROP); and ? = 3200 for
recall-oriented self-training following regular super-
vised learning (reg/ROP).
? reg/reg: standard self-training setup.
? ROP/reg: recall-oriented supervised learner, fol-
lowed by standard self-training.
? reg/ROP: regular supervised model as the initial la-
beler for recall-oriented self-training.
? ROP/ROP (the ?double ROP? condition): recall-
oriented supervised model as the initial labeler for
recall-oriented self-training. Note that the two
ROPs can use different cost parameters.
For evaluating our models we consider the
named entity detection task, i.e., recognizing
which spans of words constitute entities. This
is measured by per-entity precision, recall, and
F1.13 To measure statistical significance of differ-
ences between models we use Gimpel and Smith?s
(2010) implementation of the paired bootstrap re-
sampler of (Koehn, 2004), taking 10,000 samples
for each comparison.
5.1 Baseline
Our baseline is the perceptron, trained on the
POL entity boundaries in the ACE+ANER cor-
pus (reg/none).14 Development data was used to
select the number of iterations (10). We per-
formed 3-fold cross-validation on the ACE data
and found wide variance in the in-domain entity
detection performance of this model:
P R F1
fold 1 70.43 63.08 66.55
fold 2 87.48 81.13 84.18
fold 3 65.09 51.13 57.27
average 74.33 65.11 69.33
(Fold 1 corresponds to the ACE test set described
in table 4.) We also trained the model to perform
POL detection and classification, achieving nearly
identical results in the 3-way cross-validation of
ACE data. From these data we conclude that our
13Only entity spans that exactly match the gold spans are
counted as correct. We calculated these scores with the
conlleval.pl script from the CoNLL 2003 shared task.
14In keeping with prior work, we ignore non-POL cate-
gories for the ACE evaluation.
baseline is on par with the state of the art for Ara-
bic NER on ACE news text (Abdul-Hamid and
Darwish, 2010).15
Here is the performance of the baseline entity
detection model on our 20-article test set:16
P R F1
technology 60.42 20.26 30.35
science 64.96 25.73 36.86
history 63.09 35.58 45.50
sports 71.66 59.94 65.28
overall 66.30 35.91 46.59
Unsurprisingly, performance on Wikipedia data
varies widely across article domains and is much
lower than in-domain performance. Precision
scores fall between 60% and 72% for all domains,
but recall in most cases is far worse. Miscella-
neous class recall, in particular, suffers badly (un-
der 10%)?which partially accounts for the poor
recall in science and technology articles (they
have by far the highest proportion of MIS entities).
5.2 Self-Training
Following Clark et al(2003), we applied self-
training as described in Algorithm 1, with the
perceptron as the supervised learner. Our unla-
beled data consists of 397 Arabic Wikipedia ar-
ticles (1 million words) selected at random from
all articles exceeding a simple length threshold
(1,000 words); see table 4. We used only one iter-
ation (T ? = 1), as experiments on development
data showed no benefit from additional rounds.
Several rounds of self-training hurt performance,
15Abdul-Hamid and Darwish report as their best result a
macroaveraged F1-score of 76. As they do not specify which
data they used for their held-out test set, we cannot perform
a direct comparison. However, our feature set is nearly a
superset of their best feature set, and their result lies well
within the range of results seen in our cross-validation folds.
16Our Wikipedia evaluations use models trained on
POLM entity boundaries in ACE. Per-domain and overall
scores are microaverages across articles.
167
SELF-TRAINING
SUPERVISED none reg ROP
reg 66.3 35.9 46.59 66.7 35.6 46.41 59.2 40.3 47.97
ROP 60.9 44.7 51.59 59.8 46.2 52.11 58.0 47.4 52.16
Table 5: Entity detection precision, recall, and F1 for each learning setting, microaveraged across the 24 articles
in our Wikipedia test set. Rows differ in the supervised learning condition on the ACE+ANER data (regular
vs. recall-oriented perceptron). Columns indicate whether this supervised learning phase was followed by self-
training on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training.
baseline
entities words recall
PER 1081 1743 49.95
ORG 286 637 23.92
LOC 1019 1413 61.43
MIS 1395 2176 9.30
overall 3781 5969 35.91
Figure 2: Recall improve-
ment over baseline in the test
set by gold NER category,
counts for those categories in
the data, and recall scores for
our baseline model. Markers
in the plot indicate different
experimental settings corre-
sponding to cells in table 5.
an effect attested in earlier research (Curran et al
2007) and sometimes known as ?semantic drift.?
Results are shown in table 5. We find that stan-
dard self-training (the middle column) has very
little impact on performance.17 Why is this the
case? We venture that poor baseline recall and the
domain variability within Wikipedia are to blame.
5.3 Recall-Oriented Learning
The recall-oriented bias can be introduced in ei-
ther or both of the stages of our semisupervised
learning framework: in the supervised learn-
ing phase, modifying the objective of our base-
line (?5.1); and within the self-training algorithm
(?5.2).18 As noted in ?4.1, the aim of this ap-
proach is to discourage recall errors (false nega-
tives), which are the chief difficulty for the news
text?trained model in the new domain. We se-
lected the value of the false positive penalty for
cost-augmented decoding, ?, using the develop-
ment data (figure 1).
The results in table 5 demonstrate improve-
ments due to the recall-oriented bias in both
stages of learning.19 When used in the super-
17In neither case does regular self-training produce a sig-
nificantly different F1 score than no self-training.
18Standard Viterbi decoding was used to label the data
within the self-training algorithm; note that cost-augmented
decoding only makes sense in learning, not as a prediction
technique, since it deliberately introduces errors relative to a
correct output that must be provided.
19In terms of F1, the worst of the 3 models with the ROP
supervised learner significantly outperforms the best model
with the regular supervised learner (p < 0.005). The im-
vised phase (bottom left cell), the recall gains
are substantial?nearly 9% over the baseline. In-
tegrating this bias within self-training (last col-
umn of the table) produces a more modest im-
provement (less than 3%) relative to the base-
line. In both cases, the improvements to recall
more than compensate for the amount of degra-
dation to precision. This trend is robust: wher-
ever the recall-oriented perceptron is added, we
observe improvements in both recall and F1. Per-
haps surprisingly, these gains are somewhat addi-
tive: using the ROP in both learning phases gives
a small (though not always significant) gain over
alternatives (standard supervised perceptron, no
self-training, or self-training with a standard per-
ceptron). In fact, when the standard supervised
learner is used, recall-oriented self-training suc-
ceeds despite the ineffectiveness of standard self-
training.
Performance breakdowns by (gold) class, fig-
ure 2, and domain, figure 3, further attest to the
robustness of the overall results. The most dra-
matic gains are in miscellaneous class recall?
each form of the recall bias produces an improve-
ment, and using this bias in both the supervised
and self-training phases is clearly most success-
ful for miscellaneous entities. Correspondingly,
the technology and science domains (in which this
class dominates?83% and 61% of mentions, ver-
provements due to self-training are marginal, however: ROP
self-training produces a significant gain only following reg-
ular supervised learning (p < 0.05).
168
Figure 3: Supervised
learner precision vs.
recall as evaluated
on Wikipedia test
data in different
topical domains. The
regular perceptron
(baseline model) is
contrasted with ROP.
No self-training is
applied.
sus 6% and 12% for history and sports, respec-
tively) receive the biggest boost. Still, the gaps
between domains are not entirely removed.
Most improvements relate to the reduction of
false negatives, which fall into three groups:
(a) entities occurring infrequently or partially
in the labeled training data (e.g. uranium);
(b) domain-specific entities sharing lexical or con-
textual features with the POL entities (e.g. Linux,
titanium); and (c) words with Latin characters,
common in the science and technology domains.
(a) and (b) are mostly transliterations into Arabic.
An alternative?and simpler?approach to
controlling the precision-recall tradeoff is the
Minkov et al(2006) strategy of tuning a single
feature weight subsequent to learning (see ?4.1
above). We performed an oracle experiment to
determine how this compares to recall-oriented
learning in our setting. An oracle trained with
the method of Minkov et aloutperforms the three
models in table 5 that use the regular perceptron
for the supervised phase of learning, but under-
performs the supervised ROP conditions.20
Overall, we find that incorporating the recall-
oriented bias in learning is fruitful for adapting to
Wikipedia because the gains in recall outpace the
damage to precision.
6 Discussion
To our knowledge, this work is the first sugges-
tion that substantively modifying the supervised
learning criterion in a resource-rich domain can
reap benefits in subsequent semisupervised appli-
cation in a new domain. Past work has looked
20Tuning the O feature weight to optimize for F1 on our
test set, we found that oracle precision would be 66.2, recall
would be 39.0, and F1 would be 49.1. The F1 score of our
best model is nearly 3 points higher than the Minkov et al
style oracle, and over 4 points higher than the non-oracle
version where the development set is used for tuning.
at regularization (Chelba and Acero, 2006) and
feature design (Daume? III, 2007); we alter the
loss function. Not surprisingly, the double-ROP
approach harms performance on the original do-
main (on ACE data, we achieve 55.41% F1, far
below the standard perceptron). Yet we observe
that models can be prepared for adaptation even
before a learner is exposed a new domain, sacri-
ficing performance in the original domain.
The recall-oriented bias is not merely encour-
aging the learner to identify entities already seen
in training. As recall increases, so does the num-
ber of new entity types recovered by the model:
of the 2,070 NE types in the test data that were
never seen in training, only 450 were ever found
by the baseline, versus 588 in the reg/ROP condi-
tion, 632 in the ROP/none condition, and 717 in
the double-ROP condition.
We note finally that our method is a simple
extension to the standard structured perceptron;
cost-augmented inference is often no more ex-
pensive than traditional inference, and the algo-
rithmic change is equivalent to adding one addi-
tional feature. Our recall-oriented cost function
is parameterized by a single value, ?; recall is
highly sensitive to the choice of this value (fig-
ure 1 shows how we tuned it on development
data), and thus we anticipate that, in general, such
tuning will be essential to leveraging the benefits
of arrogance.
7 Related Work
Our approach draws on insights from work in
the areas of NER, domain adaptation, NLP with
Wikipedia, and semisupervised learning. As all
are broad areas of research, we highlight only the
most relevant contributions here.
Research in Arabic NER has been focused on
compiling and optimizing the gazetteers and fea-
169
ture sets for standard sequential modeling algo-
rithms (Benajiba et al 2008; Farber et al 2008;
Shaalan and Raza, 2008; Abdul-Hamid and Dar-
wish, 2010). We make use of features identi-
fied in this prior work to construct a strong base-
line system. We are unaware of any Arabic NER
work that has addressed diverse text domains like
Wikipedia. Both the English and Arabic ver-
sions of Wikipedia have been used, however, as
resources in service of traditional NER (Kazama
and Torisawa, 2007; Benajiba et al 2008). Attia
et al(2010) heuristically induce a mapping be-
tween Arabic Wikipedia and Arabic WordNet to
construct Arabic NE gazetteers.
Balasuriya et al(2009) highlight the substan-
tial divergence between entities appearing in En-
glish Wikipedia versus traditional corpora, and
the effects of this divergence on NER perfor-
mance. There is evidence that models trained
on Wikipedia data generalize and perform well
on corpora with narrower domains. Nothman
et al(2009) and Balasuriya et al(2009) show
that NER models trained on both automatically
and manually annotated Wikipedia corpora per-
form reasonably well on news corpora. The re-
verse scenario does not hold for models trained
on news text, a result we also observe in Arabic
NER. Other work has gone beyond the entity de-
tection problem: Florian et al(2004) addition-
ally predict within-document entity coreference
for Arabic, Chinese, and English ACE text, while
Cucerzan (2007) aims to resolve every mention
detected in English Wikipedia pages to a canoni-
cal article devoted to the entity in question.
The domain and topic diversity of NEs has been
studied in the framework of domain adaptation
research. A group of these methods use self-
training and select the most informative features
and training instances to adapt a source domain
learner to the new target domain. Wu et al(2009)
bootstrap the NER leaner with a subset of unla-
beled instances that bridge the source and target
domains. Jiang and Zhai (2006) and Daume? III
(2007) make use of some labeled target-domain
data to tune or augment the features of the source
model towards the target domain. Here, in con-
trast, we use labeled target-domain data only for
tuning and evaluation. Another important dis-
tinction is that domain variation in this prior
work is restricted to topically-related corpora (e.g.
newswire vs. broadcast news), whereas in our
work, major topical differences distinguish the
training and test corpora?and consequently, their
salient NE classes. In these respects our NER
setting is closer to that of Florian et al(2010),
who recognize English entities in noisy text, (Sur-
deanu et al 2011), which concerns information
extraction in a topically distinct target domain,
and (Dalton et al 2011), which addresses English
NER in noisy and topically divergent text.
Self-training (Clark et al 2003; Mihalcea,
2004; McClosky et al 2006) is widely used
in NLP and has inspired related techniques that
learn from automatically labeled data (Liang et
al., 2008; Petrov et al 2010). Our self-training
procedure differs from some others in that we use
all of the automatically labeled examples, rather
than filtering them based on a confidence score.
Cost functions have been used in non-
structured classification settings to penalize cer-
tain types of errors more than others (Chan and
Stolfo, 1998; Domingos, 1999; Kiddon and Brun,
2011). The goal of optimizing our structured NER
model for recall is quite similar to the scenario ex-
plored by Minkov et al(2006), as noted above.
8 Conclusion
We explored the problem of learning an NER
model suited to domains for which no labeled
training data are available. A loss function to en-
courage recall over precision during supervised
discriminative learning substantially improves re-
call and overall entity detection performance, es-
pecially when combined with a semisupervised
learning regimen incorporating the same bias.
We have also developed a small corpus of Ara-
bic Wikipedia articles via a flexible entity an-
notation scheme spanning four topical domains
(publicly available at http://www.ark.cs.
cmu.edu/AQMAR).
Acknowledgments
We thank Mariem Fekih Zguir and Reham Al Tamime
for assistance with annotation, Michael Heilman for
his tagger implementation, and Nizar Habash and col-
leagues for the MADA toolkit. We thank members of
the ARK group at CMU, Hal Daume?, and anonymous
reviewers for their valuable suggestions. This publica-
tion was made possible by grant NPRP-08-485-1-083
from the Qatar National Research Fund (a member of
the Qatar Foundation). The statements made herein
are solely the responsibility of the authors.
170
References
Ahmed Abdul-Hamid and Kareem Darwish. 2010.
Simplified feature set for Arabic named entity
recognition. In Proceedings of the 2010 Named En-
tities Workshop, pages 110?115, Uppsala, Sweden,
July. Association for Computational Linguistics.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-
ica Monachini, and Josef van Genabith. 2010.
An automatically built named entity lexicon for
Arabic. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, Mike Rosner, and Daniel Tapias, ed-
itors, Proceedings of the Seventh Conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Bogdan Babych and Anthony Hartley. 2003. Im-
proving machine translation quality with automatic
named entity recognition. In Proceedings of the 7th
International EAMT Workshop on MT and Other
Language Technology Tools, EAMT ?03.
Dominic Balasuriya, Nicky Ringland, Joel Nothman,
Tara Murphy, and James R. Curran. 2009. Named
entity recognition in Wikipedia. In Proceedings
of the 2009 Workshop on The People?s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 10?18, Suntec, Singapore, August.
Association for Computational Linguistics.
Yassine Benajiba, Paolo Rosso, and Jose? Miguel
Bened??Ruiz. 2007. ANERsys: an Arabic named
entity recognition system based on maximum en-
tropy. In Alexander Gelbukh, editor, Proceedings
of CICLing, pages 143?153, Mexico City, Mexio.
Springer.
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic named entity recognition using optimized
feature sets. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 284?293, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Philip K. Chan and Salvatore J. Stolfo. 1998. To-
ward scalable learning with non-uniform class and
cost distributions: a case study in credit card fraud
detection. In Proceedings of the Fourth Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 164?168, New York City, New
York, USA, August. AAAI Press.
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help
a lot. Computer Speech and Language, 20(4):382?
399.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
168?175.
Stephen Clark, James Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 49?55.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1?
8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual
Exclusion Bootstrapping. In Proceedings of PA-
CLING, 2007.
Jeffrey Dalton, James Allan, and David A. Smith.
2011. Passage retrieval for incorporating global
evidence in sequence labeling. In Proceedings of
the 20th ACM International Conference on Infor-
mation and Knowledge Management (CIKM ?11),
pages 355?364, Glasgow, Scotland, UK, October.
ACM.
Hal Daume? III. 2007. Frustratingly easy domain
adaptation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 256?263, Prague, Czech Republic,
June. Association for Computational Linguistics.
Pedro Domingos. 1999. MetaCost: a general method
for making classifiers cost-sensitive. Proceedings
of the Fifth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining,
pages 155?164.
Benjamin Farber, Dayne Freitag, Nizar Habash, and
Owen Rambow. 2008. Improving NER in Arabic
using a morphological tagger. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, and Daniel Tapias,
editors, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), pages
2509?2514, Marrakech, Morocco, May. European
Language Resources Association (ELRA).
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A
statistical model for multilingual entity detection
and tracking. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, Proceedings of the Hu-
man Language Technology Conference of the North
171
American Chapter of the Association for Compu-
tational Linguistics: HLT-NAACL 2004, page 18,
Boston, Massachusetts, USA, May. Association for
Computational Linguistics.
Radu Florian, John Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection ro-
bustness to noisy input. In Proceedings of EMNLP
2010, pages 335?345, Cambridge, MA, October.
Association for Computational Linguistics.
Dayne Freitag. 2004. Trained named entity recog-
nition using distributional clusters. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 262?269, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2010a. Softmax-
margin CRFs: Training log-linear models with loss
functions. In Proceedings of the Human Language
Technologies Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 733?736, Los Angeles, California,
USA, June.
Kevin Gimpel and Noah A. Smith. 2010b.
Softmax-margin training for structured log-
linear models. Technical Report CMU-LTI-
10-008, Carnegie Mellon University. http:
//www.lti.cs.cmu.edu/research/
reports/2010/cmulti10008.pdf.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Karn Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: from guidelines to evaluation,
an overview. In Proceedings of the 5th Linguis-
tic Annotation Workshop, pages 92?100, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
573?580, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan and Claypool Pub-
lishers.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by ex-
ploiting comparable and parallel corpora. In Pro-
ceedings of the Conference on Recent Advances
in Natural Language Processing (RANLP ?07),
Borovets, Bulgaria.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the Human Language Technology Conference of
the NAACL (HLT-NAACL), pages 57?60, New York
City, USA, June. Association for Computational
Linguistics.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Peas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1466?1475, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2006. Exploit-
ing domain structure for named entity recognition.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL (HLT-NAACL), pages
74?81, New York City, USA, June. Association for
Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2007.
Exploiting Wikipedia as external knowledge for
named entity recognition. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 698?707, Prague, Czech Republic,
June. Association for Computational Linguistics.
Chloe Kiddon and Yuriy Brun. 2011. That?s what
she said: double entendre identification. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 89?94, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction)
Arabic annotation guidelines for entities, version
5.3.3. Linguistic Data Consortium, Philadelphia.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for fea-
tures. In Proceedings of the 25th International Con-
ference on Machine Learning (ICML), pages 592?
599, Helsinki, Finland.
Chris Manning. 2006. Doing named entity recogni-
tion? Don?t optimize for F1. http://nlpers.
blogspot.com/2006/08/doing-named-
entity-recognition-dont.html.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In HLT-NAACL
2004 Workshop: Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004),
Boston, Massachusetts, USA.
172
Einat Minkov, Richard Wang, Anthony Tomasic, and
William Cohen. 2006. NER systems that suit user?s
preferences: adjusting the recall-precision trade-off
for entity extraction. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 93?96,
New York City, USA, June. Association for Com-
putational Linguistics.
Luke Nezda, Andrew Hickl, John Lehmann, and Sar-
mad Fayyaz. 2006. What in the world is a Shahab?
Wide coverage named entity recognition for Arabic.
In Proccedings of LREC, pages 41?46.
Joel Nothman, Tara Murphy, and James R. Curran.
2009. Analysing Wikipedia and gold-standard cor-
pora for NER training. In Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2009),
pages 612?620, Athens, Greece, March. Associa-
tion for Computational Linguistics.
PediaPress. 2010. mwlib. http://code.
pediapress.com/wiki/wiki/mwlib.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705?713, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. Subgradient methods for maxi-
mum margin structured learning. In ICML Work-
shop on Learning in Structured Output Spaces,
Pittsburgh, Pennsylvania, USA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona
Diab, and Cynthia Rudin. 2008. Arabic morpho-
logical tagging, diacritization, and lemmatization
using lexeme models and feature ranking. In Pro-
ceedings of ACL-08: HLT, pages 117?120, Colum-
bus, Ohio, June. Association for Computational
Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Pro-
ceedings of LREC.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Nigel Collier, Patrick Ruch, and Adeline
Nazarenko, editors, COLING 2004 International
Joint workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA/BioNLP)
2004, pages 107?110, Geneva, Switzerland, Au-
gust. COLING.
Khaled Shaalan and Hafsa Raza. 2008. Arabic
named entity recognition from diverse text types. In
Advances in Natural Language Processing, pages
440?451. Springer.
Mihai Surdeanu, David McClosky, Mason R. Smith,
Andrey Gusev, and Christopher D. Manning. 2011.
Customizing an information extraction system to
a new domain. In Proceedings of the ACL 2011
Workshop on Relational Models of Semantics, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004. Max-margin Markov networks. In Sebastian
Thrun, Lawrence Saul, and Bernhard Scho?lkopf,
editors, Advances in Neural Information Processing
Systems 16. MIT Press.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question an-
swering using named entity recognition. Natu-
ral Language Processing and Information Systems,
3513/2005:181?191.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484, September.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multi-
lingual training corpus. LDC2006T06, Linguistic
Data Consortium, Philadelphia.
Ralph Weischedel and Ada Brunstein. 2005.
BBN pronoun coreference and entity type cor-
pus. LDC2005T33, Linguistic Data Consortium,
Philadelphia.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1523?1532, Singapore, August.
Association for Computational Linguistics.
Tianfang Yao, Wei Ding, and Gregor Erbach. 2003.
CHINERS: a Chinese named entity recognition sys-
tem for the sports domain. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing, pages 55?62, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
173
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 301?304,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Phrase-Based Translation with Prototypes of Short Phrases
Frank Liberato?, Behrang Mohit?, Rebecca Hwa??
?Department of Computer Science ?Intelligent Systems Program
University of Pittsburgh
{frank,behrang,hwa@cs.pitt.edu}
Abstract
We investigate methods of generating addi-
tional bilingual phrase pairs for a phrase-
based decoder by translating short sequences
of source text. Because our translation task
is more constrained, we can use a model that
employs more linguistically rich features than
a traditional decoder. We have implemented
an example of this approach. Experimental re-
sults suggest that the phrase pairs produced by
our method are useful to the decoder, and lead
to improved sentence translations.
1 Introduction
Recently, there have been a number of successful
attempts at improving phrase-based statistical ma-
chine translation by exploiting linguistic knowledge
such as morphology, part-of-speech tags, and syn-
tax. Many translation models use such knowledge
before decoding (Xia and McCord, 2004) and dur-
ing decoding (Birch et al, 2007; Gimpel and Smith,
2009; Koehn and Hoang, 2007; Chiang et al, 2009),
but they are limited to simpler features for practi-
cal reasons, often restricted to conditioning left-to-
right on the target sentence. Traditionally, n-best
rerankers (Shen et al, 2004) have applied expen-
sive analysis after the translation process, on both
the source and target side, though they suffer from
being limited to whatever is on the n-best list (Hasan
et al, 2007).
We argue that it can be desirable to pre-translate
parts of the source text before sentence-level decod-
ing begins, using a richer model that would typically
be out of reach during sentence-level decoding. In
this paper, we describe a particular method of gen-
erating additional bilingual phrase pairs for a new
source text, using what we call phrase prototypes,
which are are learned from bilingual training data.
Our goal is to generate improved translations of rel-
atively short phrase pairs to provide the SMT de-
coder with better phrasal choices. We validate the
idea through experiments on Arabic-English trans-
lation. Our method produces a 1.3 BLEU score in-
crease (3.3% relative) on a test set.
2 Approach
Re-ranking tends to use expensive features of the en-
tire source and target sentences, s and t, and align-
ments, a, to produce a score for the translation. We
will call this scoring function ?(s, t, a). While ?(?)
might capture quite a bit of linguistic information, it
can be problematic to use this function for decoding
directly. This is due to both the expense of com-
puting it, and the difficulty in using it to guide the
decoder?s search. For example, a choice of ?(?) that
relies on a top-down parser is difficult to integrate
into a left-to-right decoder (Charniak et al, 2003).
Our idea is to use an expensive scoring function
to guide the search for potential translations for part
of a source sentence, S, even if translating all of it
isn?t feasible. We can then provide these transla-
tions to the decoder, along with their scores, to in-
corporate them as it builds the complete translation
of S. This differs from approaches such as (Och and
Ney, 2004) because we generate new phrase pairs in
isolation, rather than incorporating everything into
the sentence-level decoder. The baseline system is
the Moses phrase-based translation system (Koehn
301
et al, 2007).
2.1 Description of Our Scoring Function
For this work, we consider a scoring function based
on part-of-speech (POS) tags, ?POS(?). It oper-
ates in two steps: it converts the source and target
phrases, plus alignments, into what we call a phrase
prototype, then assigns a score to it based on how
common that prototype was during training.
Each phrase pair prototype is a tuple containing
the source prototype, target prototype, and align-
ment prototype, respectively. The source and tar-
get prototypes are a mix of surface word forms and
POS tags, such as the Arabic string ?NN Al JJ?,
or the English string ?NN NN?. For example, the
source and target prototypes above might be used in
the phrase prototype ?NN0 Al JJ1 , NN1 NN0?,
with the alignment prototype specified implicitly via
subscripts for brevity. For simplicity, the alignment
prototype is restricted to allow a source or target
word/tag to be unaligned, plus 1:1 alignments be-
tween them. We do not consider 1:many, many:1, or
many:many alignments in this work.
For any input ?s, t, a?, it is possible to con-
struct potentially many phrase prototypes from it by
choosing different subsets of the source and target
words to represent as POS tags. In the above ex-
ample, the Arabic determiner Al could be converted
into an unaligned POS tag, making the source pro-
totype ?NN DT JJ?. For this work, we convert all
aligned words into POS tags. As a practical con-
cern, we insist that unaligned words are always kept
as their surface form.
?POS(s, t, a) assign a score based on the proba-
bility of the resulting prototypes; more likely proto-
types should yield higher scores. We choose:
?POS(s, t, a) = p(SP,AP |TP ) ? p(TP,AP |SP )
where SP is the source prototype constructed from
s, t, a. Similarly, TP and AP are the target and
alignment prototypes, respectively.
To compute ?POS(?), we must build a model for
each of p(SP,AP |TP ) and p(TP,AP |SP ). To do
this, we start with a corpus of aligned, POS-tagged
bilingual text. We then find phrases that are consis-
tent with (Koehn et al, 2003). As we extract these
phrase pairs, we convert each into a phrase proto-
type by replacing surface forms with POS tags for
all aligned words in the prototype.
After we have processed the bilingual training
text, we have collected a set of phrase prototypes
and a count of how often each was observed.
2.2 Generating New Phrases
To generate phrases, we scan through the source text
to be translated, finding any span of source words
that matches the source prototype of at least one
phrase prototype. For each such phrase, and for each
phrase prototype which it matches, we generate all
target phrases which also match the target and align-
ment prototypes.
To do this, we use a word-to-word dictionary to
generate all target phrases which honor the align-
ments required by the alignment prototype. For each
source word which is aligned to a POS tag in the tar-
get prototype, we substitute all single-word transla-
tions in our dictionary1.
For each target phrase that we generate, we must
ensure that it matches the target prototype. We give
each phrase to a POS tagger, and check the resulting
tags against any tags in the target prototype. If there
are no mismatches, then the phrase pair is retained
for the phrase table, else it is discarded. In the latter
case, ?POS(?) would assign this pair a score of zero.
2.3 Computing Phrase Weights
In the Moses phrase table, each entry has four pa-
rameters: two lexical weights, and the two condi-
tional phrase probabilities p(s|t) and p(t|s). While
the lexical weights can be computed using the stan-
dard method (Koehn et al, 2003), estimating the
conditional phrase probabilities is not straightfor-
ward for our approach because they are not ob-
served in bilingual training data. Instead, we esti-
mate the maximum conditional phrase probabilities
that would be assigned by the sentence-level decoder
for this phrase pair, as if it had generated the tar-
get string from the source string using the baseline
phrase table2. To do this efficiently, we use some
1Since we required that all unaligned target words are kept
as surface forms in the target prototype, this is sufficient. If we
did not insist this, then we might be faced with the unenviable
task of choosing a target languange noun, without further guid-
ance from the source text.
2If we use these probabilities for our generated phrase pair?s
probability estimates, then the sentence-level decoder would see
302
simplifying assumptions: we do not restrict how of-
ten a source word is used during the translation, and
we ignore distortion / reordering costs. These admit
a simple dynamic programming solution.
We must also include the score from ?POS(?), to
give the decoder some idea of our confidence in the
generated phrase pair. We include the phrase pair?s
score as an additional weight in the phrase table.
3 Experimental Setup
The Linguistic Data Consortium Arabic-English
corpus23 is used to train the baseline MT system
(34K sentences, about one million words), and to
learn phrase prototypes. The LDC multi-translation
Arabic-English corpus (NIST2003)4 is used for tun-
ing and testing; the tuning set consists of the first
500 sentences, and the test set consists of the next
500 sentences. The language model is a 4-gram
model built from the English side of the parallel cor-
pus, plus the English side of the wmt07 German-
English and French-English news commentary data.
The baseline translation system is Moses (Koehn
et al, 2007), with the msd-bidirectional-fe
reordering model. Evaluation is done using the
BLEU (Papineni et al, 2001) metric with four ref-
erences. All text is lowercased before evaluation;
recasing is not used. We use the Stanford Arabic
POS Tagging system, based on (Toutanova et al,
2003)5. The word-to-word dictionary that is used in
the phrase generation step of our method is extracted
from the highest-scoring translations for each source
word in the baseline phrase table. For some closed-
class words, we use a small, manually constructed
dictionary to reduce the noise in the phrase table that
exists for very common words. We use this in place
of a stand-alone dictionary to reduce the need for
additional resources.
4 Experiments
To see the effect on the BLEU score of the result-
ing sentence-level translation, we vary the amount
of bilingual data used to build the phrase prototypes.
(approximately) no difference between building the generated
phrase using the baseline phrase table, or using our generated
phrase pair directly.
3Catalogue numbers LDC2004T17 and LDC2004T18
4Catalogue number: LDC2003T18
5It is available at http://nlp.stanford.edu/software/tagger.shtml
 0.36 0.37 0.38 0.39 0.4 0.41 0.42  
0 5
00 1
000 1
500 2
000 2
500 3
000 3
500 4
000 4
500 5
000 0.6 0.6
5 0.7 0.75 0.8 0.85 0.9 0.95 1
BLEU
Percentage of Generated Phrases in Phrase Table
# Bilin
gual T
raining
 Sente
nces
Effect
 of Bili
gual D
ata on
 Arabi
c Dev
elopm
ent Se
t
Baseli
ne BL
EU
Our A
pproa
ch BL
EU
% Gen
erated
 Phras
es
Figure 1: Bilingual training size vs. BLEU score (mid-
dle line, left axis) and phrase table composition (top line,
right axis) on Arabic Development Set. The baseline
BLEU score (bottom line) is included for comparison.
As we increase the amount of training data, we ex-
pect that the phrase prototype extraction algorithm
will observe more phrase prototypes. This will cause
it to generate more phrase pairs, introducing both
more noise and more good phrases into the phrase
table. Because quite a few phrase prototypes are
built in any case, we require that each is seen at
least three times before we use it to generate phrases.
Phrase prototypes seen fewer times than this are dis-
carded before phrase generation begins. Varying this
minimum support parameter does not affect the re-
sults noticeably.
The results on the tuning set are seen in Figure 1.
The BLEU score on the tuning set generally im-
proves as the amount of bilingual training data is in-
creased, even as the percentage of generated phrases
approaches 100%. Manual inspection of the phrase
pairs reveals that many are badly formed; this sug-
gests that the language model is doing its job in fil-
tering out disfluent phrases.
Using the first 5,000 bilingual training sentences
to train our model, we compare our method to the
baseline moses system. Each system was tuned via
MERT (Och, 2003) before running it on the test set.
The tuned baseline system scores 38.45. Including
our generated phrases improves this by 1.3 points to
39.75. This is a slightly smaller gain than exists in
the tuning set experiment, due in part that we did not
303
run MERT for experiment shown in Figure 1.
5 Discussion
As one might expect, generated phrases both
help and hurt individual translations. A sentence
that can be translated starting with the phrase
?korea added that the syrian prime
minister? is translated by the baseline system as
?korean | foreign minister | added |
that | the syrian?. While ?the syrian
foreign minister? is an unambiguous source
phrase, the baseline phrase table does not include it;
the language and reordering models must stitch the
translation together. Ours method generates ?the
syrian foreign minister? directly.
Generated phrases are not always correct. For
example, a generated phrase causes our system to
choose ?europe role?, while the baseline sys-
tem picks ?the role of | europe?. While
the same prototype is used (correctly) for reordering
Arabic ?NN0 JJ1? constructs into English as ?NN1
NN0? in many instances, it fails in this case. The lan-
guage model shares the blame, since it does not pre-
fer the correct phrase over the shorter one. In con-
trast, a 5-gram language model based on the LDC
Web IT 5-gram counts6 prefers the correct phrase.
6 Conclusion
We have shown that translating short spans of source
text, and providing the results to a phrase-based
SMT decoder can improve sentence-level machine
translation. Further, it permits us to use linguisti-
cally informed features to guide the generation of
new phrase pairs.
Acknowledgements
This work is supported by U.S. National Science Foun-
dation Grant IIS-0745914. We thank the anonymous re-
viewers for their suggestions.
References
A. Birch, M. Osborne, and P. Koehn. 2007. CCG su-
pertags in factored statistical machine translation. In
Proc. of the Second Workshop on SMT.
6Catalogue number LDC2006T13.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for statistical machine transla-
tion. In Proceedings of MT Summit IX.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In NAACL
?09: Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Assoc. for Computational Linguistics.
K. Gimpel and N.A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP.
S. Hasan, R. Zens, and H. Ney. 2007. Are very large n-
best lists useful for SMT? Proc. NAACL, Short paper,
pages 57?60.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology-Volume 1, page 54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
Annual meeting-Association for Computational Lin-
guistics, volume 45, page 2.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
F.J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual
Meeting on Assoc. for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
Association for Computational Linguistics.
L. Shen, A. Sarkar, and F.J. Och. 2004. Discrimina-
tive reranking for machine translation. In Proceedings
of the Joint HLT and NAACL Conference (HLT 04),
pages 177?184.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics.
304
Proceedings of NAACL-HLT 2013, pages 439?444,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Dudley North visits North London:
Learning When to Transliterate to Arabic
Mahmoud Azab Houda Bouamor
Carnegie Mellon University
P.O. Box 24866, Doha, Qatar
{mazab, hbouamor, behrang, ko}@qatar.cmu.edu
Behrang Mohit Kemal Oflazer
Abstract
We report the results of our work on automat-
ing the transliteration decision of named en-
tities for English to Arabic machine trans-
lation. We construct a classification-based
framework to automate this decision, evalu-
ate our classifier both in the limited news and
the diverse Wikipedia domains, and achieve
promising accuracy. Moreover, we demon-
strate a reduction of translation error and
an improvement in the performance of an
English-to-Arabic machine translation sys-
tem.
1 Introduction
Translation of named entities (NEs) is important
for NLP applications such as Machine Translation
(MT) and Cross-lingual Information Retrieval. For
MT, NEs are major subset of the out-of-vocabulary
terms (OOVs). Due to their diversity, they cannot
always be found in parallel corpora, dictionaries or
gazetteers. Thus, state-of-the-art of MT needs to
handle NEs in specific ways. For instance, in the
English-Arabic automatic translation example given
in Figure 1, the noun ?North? has been erroneously
translated to ? ?J
?A?
??@ /Al$mAlyp ? (indicating the
north direction in English) instead of being translit-
erated to ? HP?	K / nwrv?.
As shown in Figure 1, direct translation of in-
vocabulary terms could degrade translation quality.
Also blind transliteration of OOVs does not neces-
sarily contribute to translation adequacy and may ac-
tually create noisy contexts for the language model
and the decoder.
English Input: Dudley North was an English merchant.
SMT output: . ?K

	Q
?m.Proceedings of NAACL-HLT 2013, pages 661?667,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supersense Tagging for Arabic: the MT-in-the-Middle Attack
Nathan Schneider? Behrang Mohit? Chris Dyer? Kemal Oflazer? Noah A. Smith?
School of Computer Science
Carnegie Mellon University
?Pittsburgh, PA 15213, USA
?Doha, Qatar
{nschneid@cs.,behrang@,cdyer@cs.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the task of tagging Arabic nouns
with WordNet supersenses. Three approaches
are evaluated. The first uses an expert-
crafted but limited-coverage lexicon, Arabic
WordNet, and heuristics. The second uses un-
supervised sequence modeling. The third and
most successful approach uses machine trans-
lation to translate the Arabic into English,
which is automatically tagged with English
supersenses, the results of which are then pro-
jected back into Arabic. Analysis shows gains
and remaining obstacles in four Wikipedia
topical domains.
1 Introduction
A taxonomic view of lexical semantics groups word
senses/usages into categories of varying granulari-
ties. WordNet supersense tags denote coarse seman-
tic classes, including person and artifact (for nouns)
and motion and weather (for verbs); these categories
can be taken as the top level of a taxonomy. Nominal
supersense tagging (Ciaramita and Johnson, 2003)
is the task of identifying lexical chunks in the sen-
tence for common as well as proper nouns, and la-
beling each with one of the 25 nominal supersense
categories. Figure 1 illustrates two such labelings of
an Arabic sentence. Like the narrower problem of
named entity recognition, supersense tagging of text
holds attraction as a way of inferring representations
that move toward language independence. Here we
consider the problem of nominal supersense tagging
for Arabic, a language with ca. 300 million speak-
ers and moderate linguistic resources, including a
WordNet (Elkateb et al, 2006), annotated datasets
(Maamouri et al, 2004; Hovy et al, 2006), monolin-
gual corpora, and large amounts of Arabic-English
parallel data.
The supervised learning approach that is used
in state-of-the-art English supersense taggers (Cia-
. HA

?J
J.?

J? @
	
Y
	
? @?
	
K ?? ?? ?
	
?? ?


	
?
	
Y
	
? @?
	
J? @ QK
Y? ??j

JK

Ann-A Gloss Ann-B
controls
communication
manager
communicationthe-windows
in
attribute configuration relation
shape and-layout shape
communication
windows
communicationthe-applications
?The window manager controls the configuration and
layout of application windows.?
Figure 1: A sentence from the ?X Window System? ar-
ticle with supersense taggings from two annotators and
post hoc English glosses and translation.
ramita and Altun, 2006) is problematic for Ara-
bic, since there are supersense annotations for only
a small amount of Arabic text (65,000 words by
Schneider et al, 2012, versus the 360,000 words that
are annotated for English). Here, we reserve that
corpus for development and evaluation, not training.
We explore several approaches in this paper, the
most effective of which is to (1) translate the Arabic
sentence into English, returning the alignment struc-
ture between the source and target, (2) apply En-
glish supersense tagging to the target sentence, and
(3) heuristically project the tags back to the Arabic
sentence across these alignments. This ?MT-in-the-
middle? approach has also been successfully used
for mention detection (Zitouni and Florian, 2008)
and coreference resolution (Rahman and Ng, 2012).
We first discuss the task and relevant resources
(?2), then the approaches we explored (?3), and fi-
nally present experimental results and analysis in ?4.
2 Task and Resources
A gold standard corpus of sentences annotated
with nominal supersenses (as in figure 1) fa-
cilitates automatic evaluation of supersense tag-
gers. For development and evaluation we use
661
the AQMAR Arabic Wikipedia Supersense Corpus1
(Schneider et al, 2012), which augmented a named
entity corpus (Mohit et al, 2012) with nominal
supersense tags. The corpus consists of 28 ar-
ticles selected from four topical areas: history
(e.g., ?Islamic Golden Age?), science (?Atom?),
sports (?Real Madrid?), and technology (?Linux?).
Schneider et al (2012) found the distributions of
supersense categories in these four topical domains
to be markedly different; e.g., most instances of
communication (which includes kinds of software)
occurred in the technology domain, whereas most
substances were found in the science domain.
The 18 test articles have 1,393 sentences (39,916
tokens) annotated at least once.2 As the corpus
was released with two annotators? (partially overlap-
ping) taggings, rather than a single gold standard,
we treat the output of each annotator as a separate
test set. Both annotated some of every article; the
first (Ann-A) annotated 759 sentences, the second
(Ann-B) 811 sentences.
Lexicon. What became known as ?supersense
tags? arose from a high-level partitioning of synsets
in the original English WordNet (Fellbaum, 1998)
into lexicographer files. Arabic WordNet (AWN)
(Elkateb et al, 2006) allows us to recover super-
sense categories for some 10,500 Arabic nominal
types, since many of the synsets in AWN are cross-
referenced to English WordNet, and can therefore
be associated with supersense categories. Further,
OntoNotes contains named entity annotations for
Arabic (Hovy et al, 2006).
From these, we construct an Arabic supersense
lexicon, mapping Arabic noun lemmas to supersense
tags. This lexicon contains 23,000 types, of which
11,000 are multiword units. Token coverage of the
test set is 18% (see table 1). Lexical units encoun-
tered in the test data were up to 9-ways supersense-
ambiguous; the average ambiguity of in-vocabulary
tokens was 2.0 supersenses.
Unlabeled Arabic text. For unsupervised learn-
ing we collected 100,000 words of Arabic Wikipedia
text, not constrained by topic. The articles in this
sample were subject to a minimum length threshold
1http://www.ark.cs.cmu.edu/ArabicSST
2Our development/test split of the data follows Mohit et al
(2012), but we exclude two test set documents??Light? and
?Ibn Tolun Mosque??due to preprocessing issues.
and are all cross-linked to corresponding articles in
English, Chinese, and German.
Arabic?English machine translation. We used
two independently developed Arabic-English MT
systems. One (QCRI) is a phrase-based system
(Koehn et al, 2003), similar to Moses (Koehn et
al., 2007); the other (cdec) is a hierarchical phrase-
based system (Chiang, 2007), as implemented in
cdec (Dyer et al, 2010). Both were trained on
about 370M tokens of parallel data provided by the
LDC (by volume, mostly newswire and UN data).
Each system includes preprocessing for Arabic mor-
phological segmentation and orthographic normal-
ization.3 The QCRI system used a 5-gram modi-
fied Kneser-Ney language model that generated full-
cased forms (Chen and Goodman, 1999). cdec
used a 4-gram KN language model over lowercase
forms and was recased in a post-processing step.
Both language models were trained using the Giga-
word v. 4 corpus. Both systems were tuned to opti-
mize BLEU on a held-out development set (Papineni
et al, 2002).
English supersense tagger. For English super-
sense tagging, an open-source reimplementation of
the approach of Ciaramita and Altun (2006) was
released by Michael Heilman.4 This tagger was
trained on the SemCor corpus (Miller et al, 1993)
and achieves 77% F1 in-domain.
3 Methods
We explored 3 approaches to the supersense tagging
of Arabic: heuristic tagging with a lexicon, unsuper-
vised sequence tagging, and MT-in-the-middle.
3.1 Heuristic Tagging with a Lexicon
Using the lexicon built from AWN and OntoNotes
(see ?2), our heuristic approach works as follows:
1. Stem and vocalize; we used MADA (Habash
and Rambow, 2005; Roth et al, 2008).
2. Greedily detect word sequences matching lexi-
con entries from left to right.
3. If a lexicon entry has more than one associated
supersense, Arabic WordNet synsets are
3QCRI accomplishes this using MADA (Habash and Ram-
bow, 2005; Roth et al, 2008). cdec includes a custom CRF-
based segmenter and standard normalization rules.
4http://www.ark.cs.cmu.edu/mheilman/questions
662
E? person location artifact substance Automatic English supersense tagging
e? 1 2 3 4 5 6 7 8 9 English sentence
a 1 2 3 4 5 6 Arabic sentence (e.g., token 6 aligns to English tokens 7?9)
N P N A N N Arabic POS tagging
A? person location artifact Projected supersense tagging
Figure 2: A hypothetical aligned sentence pair of 9 English words (with their supersense tags) and 6 Ara-
bic words (with their POS tags). Step 4 of the projection procedure constructs the Arabic-to-English mapping
{1?person11, 4?location
4
3, {5, 6}?artifact
7
6}, resulting in the tagging shown in the bottom row.
weighted to favor earlier senses (presumed
by lexicographers to be more frequent) and
then the supersense with the greatest aggregate
weight is selected. Formally: Let senses(w) be
the ordered list of AWN senses of lemma w.
Let senses(w, s) ? senses(w) be those senses
that map to a given supersense s. We choose
arg maxs(|senses(w, s)|/ mini:senses(w)i?senses(w,s) i).
3.2 Unsupervised Sequence Models
Unsupervised sequence labeling is our second ap-
proach (Merialdo, 1994). Although it was largely
developed for part-of-speech tagging, the hope is
to use in-domain Arabic data (the unannotated
Wikipedia corpus discussed in ?2) to infer clus-
ters that correlate well with supersense groupings.
We applied the generative, feature-based model of
Berg-Kirkpatrick et al (2010), replicating a feature-
set used previously for NER (Mohit et al, 2012)?
including context tokens, character n-grams, and
POS?and adding the vocalized stem and several
stem shape features: 1) ContainsDigit?; 2) dig-
its replaced by #; 3) digit sequences replaced by
# (for stems mixing digits with other characters);
4) YearLike??true for 4-digit numerals starting with
19 or 20; 5) LatinWord?, per the morphological an-
alysis; 6) the shape feature of Ciaramita and Al-
tun (2006) (Latin words only). We used 50 itera-
tions of learning (tuned on dev data). For evaluation,
a many-to-one mapping from unsupervised clusters
to supersense tags is greedily induced to maximize
their correspondence on evaluation data.
3.3 MT-in-the-Middle
A standard approach to using supervised linguistic
resources in a second language is cross-lingual pro-
jection (Yarowsky and Ngai, 2001; Yarowsky et al,
2001; Smith and Smith, 2004; Hwa et al, 2005; Mi-
halcea et al, 2007; Burkett and Klein, 2008; Burkett
et al, 2010; Das and Petrov, 2011; Kim et al, 2012,
who use parallel sentences extracted from Wikipedia
for NER). The simplest such approach starts with an
aligned parallel corpus, applies supersense tagging
to the English side, and projects the labels through
the word alignments. A supervised monolingual tag-
ger is then trained on the projected labels. Prelimi-
nary experiments, however, showed that this under-
performed even the simple heuristic baseline above
(likely due to domain mismatch), so it was aban-
doned in favor of a technique that we call MT-in-
the-middle projection.
This approach does not depend on having par-
allel data in the training domain, but rather on an
Arabic?English machine translation system that
can be applied to the sentences we wish to tag. The
approach is inspired by token-level pseudo-parallel
data methods of previous work (Zitouni and Flo-
rian, 2008; Rahman and Ng, 2012). MT output for
this language pair is far from perfect?especially for
Wikipedia text, which is distant from the domain
of the translation system?s training data?but, in the
spirit of Church and Hovy (1993), we conjecture that
it may still be useful. The method is as follows:
1. Preprocess the input Arabic sentence a to
match the decoder?s model of Arabic.
2. Translate the sentence, recovering not just
the English output e? but also the deriva-
tion/alignment structure z relating words and/or
phrases of the English output to words and/or
phrases of the Arabic input.
3. Apply the English supersense tagger to the En-
glish translation, discarding any verbal super-
sense tags. Call the tagger output E?.
4. Project the supersense tags back to the Ara-
bic sentence, yielding A?: Each Arabic token
a ? a that is (a) a noun, or (b) an adjec-
tive following 0 or more adjectives following a
noun is mapped to the first English supersense
mention in E? containing some word aligned
to a. Then, for each English supersense men-
663
Coverage Ann-A Ann-B
Nouns All Tokens Mentions P R F1 P R F1
Lexicon heuristics (?3.1) 8,058 33% 8,465 18% 8,407 32 55 16 29 21.6 37.9 29 53 15 27 19.4 35.6
Unsupervised (?3.2) 20 59 16 48 17.5 52.6 14 56 10 39 11.6 45.9
MT-in-the-middle
(?3.3)
QCRI 14,401 59% 16,461 35% 12,861 34 65 27 50 29.9 56.4 36 64 28 51 31.6 56.6
cdec 14,270 58% 15,542 33% 13,704 37 69 31 57 33.8 62.4 38 67 32 56 34.6 61.0
MTitM + Lex. cdec 16,798 68% 18,461 40% 16,623 35 64 36 65 35.5 64.6 36 63 36 63 36.0 63.2
Table 1: Supersense tagging results on the test set: coverage measures5 and gold-standard evaluation?exact la-
beled/unlabeled6 mention precision, recall, and F-score against each annotator. The last row is a hybrid: MT-in-the-
middle followed by lexicon heuristics to improve recall. Best single-technique and best hybrid results are bolded.
tion, all its mapped Arabic words are grouped
into a single mention and the supersense cat-
egory for that mention is projected. Figure 2
illustrates this procedure. The cdec system
provides word alignments for its translations
derived from the training data; whereas QCRI
only produces phrase-level alignments, so for
every aligned phrase pair ?a?, e?? ? z, we con-
sider every word in a? as aligned to every word
in e? (introducing noise when English super-
sense mention boundaries do not line up with
phrase boundaries).
4 Experiments and Analysis
Table 1 compares the techniques (?3) for full Arabic
supersense tagging.7 The number of nouns, tokens,
and mentions covered by the automatic tagging is
reported, as is the mention-level evaluation against
human annotations. The evaluation is reported sep-
arately for the two annotators in the dataset.
With heuristic lexicon lookup, 18% of the tokens
are marked as part of a nominal supersense mention.
Both labeled and unlabeled mention recall with this
method are below 30%; labeled precision is about
30%, and unlabeled mention precision is above
50%. From this we conclude that the biggest prob-
lems are (a) out-of-vocabulary items and (b) poor
semantic disambiguation of in-vocabulary items.
The unsupervised sequence tagger does even
worse on the labeled evaluation. It has some success
at detecting supersense mentions?unlabeled recall
is substantially improved, and unlabeled precision is
5The unsupervised evaluation greedily maps clusters to tags,
separately for each version of the test set; coverage numbers
thus differ and are not shown here.
6Unlabeled tagging refers to noun chunk detection only.
7It was produced in part using the chunkeval.py script: see
https://github.com/nschneid/pyutil
slightly improved. But it seems to be much worse
at assigning semantic categories; the number of la-
beled true positive mentions is actually lower than
with the lexicon-based approach.
MT-in-the-middle is by far the most success-
ful single approach: both systems outperform the
lexicon-only baseline by about 10 F1 points, de-
spite many errors in the automatic translation, En-
glish tagging, and projection, as well as underlying
linguistic differences between English and Arabic.
The baseline?s unlabeled recall is doubled, indicat-
ing substantially more nominal expressions are de-
tected, in addition to the improved labeled scores.
We further tested simple hybrids combining the
lexicon-based and MT-based approaches. Applying
MT-in-the-middle first, then expanding token cover-
age with the lexicon improves recall at a small cost
to precision (table 1, last row). Combining the tech-
niques in the reverse order is slightly worse than MT-
based projection without consulting the lexicon.
MT-in-the middle improves upon the lexicon-only
baseline, yet performance is still dwarfed by the su-
pervised English tagger (at least in the SemCor eval-
uation; see ?2), and also well below the 70% inter-
annotator F1 reported by Schneider et al (2012). We
therefore examine the weaknesses of our approach
for Arabic.
4.1 MT for Projection
In analyzing our projection framework, we per-
formed a small-scale MT evaluation with the
Wikipedia data. Reference English translations for
140 Arabic Wikipedia sentences?5 per article in
the corpus?were elicited from a bilingual linguist.
Table 2 compares the two systems under three stan-
dard metrics of overall sentence translation quality.8
8BLEU (Papineni et al, 2002); METEOR (Banerjee and
Lavie, 2005; Lavie and Denkowski, 2009), with default options;
664
. ????@ ?


	
? @Yg.

?Q

	??

?
	
Jj ??@

?J.k. ??

?@?
	
K ??k ??mProceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 176?180,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transforming Standard Arabic to Colloquial Arabic 
 
 Emad Mohamed, Behrang Mohit and Kemal Oflazer 
 
Carnegie Mellon University - Qatar 
Doha, Qatar 
emohamed@qatar.cmu.edu, behrang@cmu.edu, ko@cs.cmu.edu  
 
 
Abstract 
We present a method for generating Colloquial 
Egyptian Arabic (CEA) from morphologically dis-
ambiguated Modern Standard Arabic (MSA). 
When used in POS tagging, this process improves 
the accuracy from 73.24% to 86.84% on unseen 
CEA text, and reduces the percentage of out-of-
vocabulary words from 28.98% to 16.66%. The 
process holds promise for any NLP task targeting 
the dialectal varieties of Arabic; e.g., this approach 
may provide a cheap way to leverage MSA data 
and morphological resources to create resources 
for colloquial Arabic to English machine transla-
tion. It can also considerably speed up the annota-
tion of Arabic dialects. 
 
1. Introduction 
Most of the research on Arabic is focused on Mod-
ern Standard Arabic. Dialectal varieties have not 
received much attention due to the lack of dialectal 
tools and annotated texts (Duh and Kirchoff, 
2005). In this paper, we present a rule-based me-
thod to generate Colloquial Egyptian Arabic (CEA) 
from Modern Standard Arabic (MSA), relying on 
segment-based part-of-speech tags. The transfor-
mation process relies on the observation that di-
alectal varieties of Arabic differ mainly in the use 
of affixes and function words while the word stem 
mostly remains unchanged. For example, given the 
Buckwalter-encoded MSA sentence ?AlAxwAn 
Almslmwn lm yfwzwA fy AlAntxbAt? the rules pro-
duce ?AlAxwAn Almslmyn mfAzw$ f AlAntxAbAt? 
(????????? ? ?????? ???????? ??????, The Muslim Bro-
therhood did not win the elections). The availabili-
ty of segment-based part-of-speech tags is essential 
since many of the affixes in MSA are ambiguous. 
For example, lm could be either a negative particle 
or a question work, and the word AlAxwAn could 
be either made of two segments (Al+<xwAn, the 
brothers), or three segments (Al+>xw+An, the two 
brothers). 
    We first introduce the transformation rules, and 
show that in many cases it is feasible to transform 
MSA to CEA, although there are cases that require 
much more than POS tags.  We then provide a typ-
ical case in which we utilize the transformed text 
of the Arabic Treebank (Bies and Maamouri, 2003) 
to build a part-of-speech tagger for CEA. The tag-
ger improves the accuracy of POS tagging on au-
thentic Egyptian Arabic by 13% absolute (from 
73.24% to 86.84%) and reduces the percentage of 
out-of-vocabulary words from 28.98% to 16.66%. 
  
  2. MSA to CEA Conversion Rules 
Table 1 shows a sentence in MSA and its CEA 
counterpart. Both can be translated into: ?We did 
not write it for them.? MSA has three words while 
CEA is more synthetic as the preposition and the 
negative particle turn into clitics.  Table 1 illu-
strates the end product of one of the Imperfect 
transformation rules, namely the case where the 
Imperfect Verb is preceded by the negative particle 
lm. 
 
 Arabic Buckwalter 
MSA ??? ?????? ?? lm nktbhA lhn 
CEA ?????????? mktbnhlhm$ 
English We did not write it for them 
Table 1: a sentence in MSA and CEA 
 
Our 103 rules cover nominals (number and case 
affixes), verbs (tense, number, gender, and modali-
ty), pronouns (number and gender), and demon-
strative pronouns (number and gender).  
    The rules also cover certain lexical items as 400 
words in MSA have been converted to their com-
176
mon CEA counterparts.  Examples of lexical con-
versions include ZlAm and Dlmp (darkness), rjl 
and rAjl (man), rjAl and rjAlp (men), and kvyr and 
ktyr (many), where the first word is the MSA ver-
sion and the second is the CEA version.  
   Many of the lexical mappings are ambiguous. 
For example, the word rjl can either mean man or 
leg. When it means man, the CEA form is rAjl, but 
the word for leg is the same in both MSA and 
CEA. While they have different vowel patterns 
(rajul and rijol respectively), the vowel informa-
tion is harder to get correctly than POS tags. The 
problem may arise especially when dealing with 
raw data for which we need to provide POS tags 
(and vowels) so we may be able to convert it to the 
colloquial form. Below, we provide two sample 
rules:  
The imperfect verb is used, inter alia, to express 
the negated past, for which CEA uses the perfect 
verb. What makes things more complicated is that 
CEA treats negative particles and prepositional 
phrases as clitics. An example of this is the word 
mktbthlhm$ (I did not write it for them) in Table 1 
above. It is made of the negative particle m, the 
stem ktb (to write), the object pronoun h, the pre-
position l, the pronoun hm (them) and the negative 
particle $. Figure 1, and the following steps show 
the conversions of lm nktbhA lhm to 
mktbnhAlhm$: 
1. Replace the negative word lm with one of 
the prefixes m, mA or the word mA. 
2. Replace the Imperfect Verb prefix with its 
Perfect Verb suffix counterpart.  For exam-
ple, the IV first person singular subject pre-
fix > turns into t in the PV. 
3. If the verb is followed by a prepositional 
phrase headed by the preposition l that con-
tains a pronominal object, convert the pre-
position to a prepositional clitic. 
4. Transform the dual to plural and the plural 
feminine to plural masculine. 
5. Add the negative suffix $ (or the variant $y, 
which is less probable) 
As alluded to in 1) above, given that colloquial 
orthography is not standardized, many affixes and 
clitics can be written in different ways. For exam-
ple, the word mktbnhlhm$, can be written in 24 
ways. All these forms are legal and possible, as 
attested by their existence in a CEA corpus (the 
Arabic Online Commentary Dataset v1.1), which 
we also use for building a language model later. 
Figure 1: One negated IV form in MSA can generate 24 
(3x2x2x2) possible forms in CEA 
 
MSA possessive pronouns inflect for gender, num-
ber (singular, dual, and plural), and person. In 
CEA, there is no distinction between the dual and 
the plural, and a single pronoun is used for the 
plural feminine and masculine. The three MSA 
forms ktAbhm, ktAbhmA and ktAbhn (their book 
for the masculine plural, the dual, and the feminine 
plural respectively) all collapse to ktAbhm.  
 
Table 2 has examples of some other rules we have 
applied.  We note that the stem, in bold, hardly 
changes, and that the changes mainly affect func-
tion segments. The last example is a lexical rule in 
which the stem has to change. 
 
Rule MSA CEA 
Future swf  yktb Hyktb/hyktb 
Future_NEG ln >ktb m$ hktb/ m$ Hktb 
IV yktbwn byktbw/ bktbw/ bktbwA 
Passive ktb Anktb/ Atktb 
NEG_PREP lys mnhn mmnhm$ 
Lexical trkhmA sAbhm 
Table 2: Examples of Conversion Rules. 
 
3. POS Tagging Egyptian Arabic 
We use the conversion above to build a POS tagger 
for Egyptian Arabic. We follow Mohamed and 
Kuebler (2010) in using whole word tagging, i.e., 
without any word segmentation. We use the Co-
lumbia Arabic Treebank 6-tag tag set: PRT (Par-
ticle), NOM (Nouns, Adjectives, and Adverbs), 
PROP (Proper Nouns), VRB (Verb), VRB-pass 
(Passive Verb), and PNX (Punctuation) (Habash 
and Roth, 2009). For example, the word 
wHnktblhm (and we will write to them, ?????????) 
receives the tag PRT+PRT+VRB+PRT+NOM. 
This results in 58 composite tags, 9 of which occur 
5 times or less in the converted ECA training set. 
177
    We converted two sections of the Arabic Tree-
bank (ATB): p2v3 and p3v2. For all the POS tag-
ging experiments, we use the memory-based POS 
tagger (MBT) (Daelemans et al, 1996) The best 
results, tuned on a dev set,  were obtained, in non-
exhaustive search,  with the Modified Value Dif-
ference Metric as a distance metric and with k  (the 
number of nearest neighbors) = 25. For known 
words, we use the IGTree algorithm and 2 words to 
the left, their POS tags, the focus word and its list 
of possible tags, 1 right context word and its list of 
possible tags as features. For unknown words, we 
use the IB1 algorithm and the word itself, its first 5 
and last 3 characters, 1 left context word and its 
POS tag, and 1 right context word and its list of 
possible tags as features. 
     
3.1. Development and Test Data 
As a development set, we use 100 user-contributed 
comments (2757 words) from the website ma-
srawy.com, which were judged to be highly collo-
quial. The test set contains 192 comments (7092 
words) from the same website with the same crite-
rion. The development and test sets were hand-
annotated with composite tags as illustrated above 
by two native Arabic-speaking students. 
The test and development sets contained spel-
ling errors (mostly run-on words). The most com-
mon of these is the vocative particle yA, which is 
usually attached to following word (e.g. yArAjl, 
(you man, ??????)). It is not clear whether it should 
be treated as a proclitic, since it also occurs as a 
separate word, which is the standard way of writ-
ing. The same holds true for the variation between 
the letters * and z, (? and ? in Arabic) which are 
pronounced exactly the same way in CEA to the 
extent that the substitution may not be considered a 
spelling error. 
 
3.2. Experiments and Results 
We ran five experiments to test the effect of MSA 
to CEA conversion on POS tagging: (a) Standard, 
where we train the tagger on the ATB MSA data, 
(b) 3-gram LM, where for each MSA sentence we 
generate all transformed sentences (see Section 2.1 
and Figure 1) and pick the most probable sentence 
according to a trigram language model built from 
an 11.5 million words of user contributed 
comments.1 This corpus is highly dialectal 
                                                 
1Available from  http://www.cs.jhu.edu/~ozaidan/AOC 
Egyptian Arabic, but like all similar collections, it 
is diglossic and demonstrates a high degree of 
code-switching between MSA and CEA. We use 
the SRILM toolkit (Stolcke, 2002) for language 
modeling and sentence scoring, (c) Random, 
where we choose a random sentence from all the 
correct sentences generated for each MSA 
sentence, (d) Hybrid, where we combine the data 
in a) with the best settings (as measured on the dev 
set) using the converted colloquial data (namely 
experiment c). Hybridization is necessary since 
most Arabic data in blogs and comments are a mix 
of MSA and CEA, and (e) Hybrid + dev, where 
we enrich the Hybrid training set with the dev data.  
  We use the following metrics for evaluation: 
KWA: Known Word Accuracy (%), UWA: 
Unknown Word Accuracy (%), TA: Total Accuracy 
(%), and UW: unknown words (%) in the 
respective set in the respective experiment. Table 
3(a) presents the results on the development set 
while Table 3(b) the results on the test set.  
 
Experiment KWA UWA TA UW 
(a) Standard 92.75 39.68 75.77 31.99 
(b) 3-gram LM 89.12 43.46 76.21 28.29 
(c) Random 92.36 43.51 79.25 26.84 
(d) Hybrid 94.13 52.22 84.87 22.09 
Table 3(a): POS results on the development set.   
 
We notice that randomly selecting a sentence from 
the correct generated sentences yields better results 
than choosing the most probable sentence accord-
ing to a language model. The reason for this may 
be that randomization guarantees more coverage of 
the various forms. We have found that the vocabu-
lary size (the number of unique word types) for the 
training set generated for the Random experiment 
is considerably larger than the vocabulary size for 
the 3-gram LM experiment (55367 unique word 
types in Random versus 51306 in 3-gram LM), 
which results in a drop of 4.6% absolute in the per-
centage of unknown words: 27.31% versus 
22.30%). This drop in the percentage of unknown 
words may indicate that generating all possible 
variations of CEA may be more useful than using a 
language model in general. Even in a CEA corpus 
of 35 million words, one third of the words gener-
ated by the rules are not in the corpus, while many 
178
of these are in both the test set and the develop-
ment set. 
 
Experiment KWA UWA TA UW 
(a) Standard 89.03 40.67 73.24 28.98 
(b) 3-gram LM 84.33 47.70 74.32 27.31 
(c) Random 90.24 48.90 79.67 22.70 
(d) Hybrid 92.22 53.92 83.81 19.45 
(e) Hybrid+dev 94.87 56.46 86.84 16.66 
Table 3(b): POS results on the test set 
 
    We also notice that the conversion alone im-
proves tagging accuracy from 75.77% to 79.25% 
on the development set, and from 73.24% to 
79.67% on the test set. Combining the original 
MSA and the best scoring converted data (Ran-
dom) raises the accuracies to 84.87% and 83.81% 
respectively.  The percentage of unknown words 
drops from 29.98% to 19.45% in the test set when 
we used the hybrid data. The fact that the percen-
tage of unknown words drops further to 16.66% in 
the Hybrid+dev experiment points out the authen-
tic colloquial data contains elements that have not 
been captured using conversion alone.    
 
4. Related Work 
To the best of our knowledge, ours is the first work 
that generates CEA automatically from morpholog-
ically disambiguated MSA, but Habash et al 
(2005) discussed root and pattern morphological 
analysis and generation of Arabic dialects within 
the MAGED morphological analyzer. MAGED 
incorporates the morphology, phonology, and or-
thography of several Arabic dialects. Diab et al 
(2010) worked on the annotation of dialectal Arab-
ic through the COLABA project, and they used the 
(manually) annotated resources to facilitate the 
incorporation of the dialects in Arabic information 
retrieval. 
  Duh and Kirchhoff (2005) successfully designed 
a POS tagger for CEA that used an MSA morpho-
logical analyzer and information gleaned from the 
intersection of several Arabic dialects.  This is dif-
ferent from our approach for which POS tagging is 
only an application.  Our focus is to use any exist-
ing MSA data to generate colloquial Arabic re-
sources that can be used in virtually any NLP task. 
   At a higher level, our work resembles that of 
Kundu and Roth (2011), in which they chose to 
adapt the text rather than the model. While they 
adapted the test set, we do so at the training set 
level. 
 
5. Conclusions and Future Work 
We have a presented a method to convert Modern 
Standard Arabic to Egyptian Colloquial Arabic 
with an example application to the POS tagging 
task. This approach may provide a cheap way to 
leverage MSA data and morphological resources to 
create resources for colloquial Arabic to English 
machine translation, for example. 
     While the rules of conversion were mainly 
morphological in nature, they have proved useful 
in handling colloquial data. However, morphology 
alone is not enough for handling key points of dif-
ference between CEA and MSA. While CEA is 
mainly an SVO language, MSA is mainly VSO, 
and while demonstratives are pre-nominal in MSA, 
they are post-nominal in CEA. These phenomena 
can be handled only through syntactic conversion.  
We expect that converting a dependency-based 
treebank to CEA can account for many of the phe-
nomena part-of-speech tags alone cannot handle 
  We are planning to extend the rules to other lin-
guistic phenomena and dialects, with possible ap-
plications to various NLP tasks for which MSA 
annotated data exist. When no gold standard seg-
ment-based POS tags are available, tools that pro-
duce segment-based annotation can be used, e.g.   
segment-based POS tagging (Mohamed and Kueb-
ler, 2010) or MADA (Habash et al 2009), although 
these are not expected to yield the same results as 
gold standard part-of-speech tags. 
 
Acknowledgements  
This publication was made possible by a NPRP 
grant (NPRP 09-1140-1-177) from the Qatar Na-
tional Research Fund (a member of The Qatar 
Foundation). The statements made herein are sole-
ly the responsibility of the authors.  
    We thank the two native speaker annotators and 
the anonymous reviewers for their instructive and 
enriching feedback. 
 
 
  
179
 References 
    Bies, Ann and Maamouri, Mohamed  (2003). Penn 
Arabic Treebank guidelines. Technical report, LDC, 
University of Pennsylvania. 
    Buckwalter, T. (2002). Arabic Morphological Analyz-
er (AraMorph). Version 1.0. Linguistic Data Consor-
tium, catalog number LDC2002L49 and ISBN 1-58563-
257- 0 
    Daelemans, Walter and van den Bosch, Antal ( 2005). 
Memory Based Language Processing. Cambridge Uni-
versity Press.   
    Daelemans, Walter; Zavrel, Jakub; Berck, Peter, and 
Steven Gillis (1996). MBT: A memory-based part of 
speech tagger-generator. In Eva Ejerhed and Ido Dagan, 
editors, Proceedings of the 4th Workshop on Very Large 
Corpora, pages 14?27, Copenhagen, Denmark. 
   Diab, Mona; Habash, Nizar; Rambow, Owen; Altan-
tawy, Mohamed, and Benajiba, Yassine. COLABA: 
Arabic Dialect Annotation and Processing. LREC 2010. 
    Duh, K. and Kirchhoff, K. (2005). POS Tagging of 
Dialectal Arabic: A Minimally Supervised Approach. 
Proceedings of the ACL Workshop on Computational 
Approaches to Semitic Languages, Ann Arbor, June 
2005. 
    Habash, Nizar; Rambow, Own and Kiraz, George 
(2005). Morphological analysis and generation for 
Arabic dialects. Proceedings of the ACL Workshop on 
Computational Approaches to Semitic Languages, pages 
17?24, Ann Arbor, June 2005 
    Habash, Nizar and Roth, Ryan. CATiB: The Colum-
bia Arabic Treebank. Proceedings of the ACL-IJCNLP 
2009 Conference Short Papers, pages 221?224, Singa-
pore, 4 August 2009. c 2009 ACL and AFNLP 
    Habash, Nizar, Owen Rambow and Ryan Roth. MA-
DA+TOKAN: A Toolkit for Arabic Tokenization, Dia-
critization, Morphological Disambiguation, POS Tag-
ging, Stemming and Lemmatization. In Proceedings of 
the 2nd International Conference on Arabic Language 
Resources and Tools (MEDAR), Cairo, Egypt, 2009 
   Kundu, Gourab abd Roth, Don (2011). Adapting Text 
instead of the Model: An Open Domain Approach. Pro-
ceedings of the Fifteenth Conference on Computational 
Natural Language Learning, pages 229?237,Portland, 
Oregon, USA, 23?24 June 2011 
    Mohamed, Emad. and Kuebler, Sandra (2010). Is 
Arabic Part of Speech Tagging Feasible Without Word 
Segmentation? Proceedings of HLT-NAACL 2010, Los 
Angeles, CA. 
    Stolcke, A. (2002). SRILM - an extensible language 
modeling toolkit. In Proc. of ICSLP, Denver, Colorado 
180
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?258,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Coarse Lexical Semantic Annotation with Supersenses:
An Arabic Case Study
Nathan Schneider? Behrang Mohit? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?Doha, Qatar ?Pittsburgh, PA 15213, USA
{nschneid@cs.,behrang@,ko@cs.,nasmith@cs.}cmu.edu
Abstract
?Lightweight? semantic annotation of text
calls for a simple representation, ideally with-
out requiring a semantic lexicon to achieve
good coverage in the language and domain.
In this paper, we repurpose WordNet?s super-
sense tags for annotation, developing specific
guidelines for nominal expressions and ap-
plying them to Arabic Wikipedia articles in
four topical domains. The resulting corpus
has high coverage and was completed quickly
with reasonable inter-annotator agreement.
1 Introduction
The goal of ?lightweight? semantic annotation of
text, particularly in scenarios with limited resources
and expertise, presents several requirements for a
representation: simplicity; adaptability to new lan-
guages, topics, and genres; and coverage. This
paper describes coarse lexical semantic annotation
of Arabic Wikipedia articles subject to these con-
straints. Traditional lexical semantic representations
are either narrow in scope, like named entities,1 or
make reference to a full-fledged lexicon/ontology,
which may insufficiently cover the language/domain
of interest or require prohibitive expertise and ef-
fort to apply.2 We therefore turn to supersense tags
(SSTs), 40 coarse lexical semantic classes (25 for
nouns, 15 for verbs) originating in WordNet. Previ-
ously these served as groupings of English lexicon
1Some ontologies like those in Sekine et al (2002) and BBN
Identifinder (Bikel et al, 1999) include a large selection of
classes, which tend to be especially relevant to proper names.
2E.g., a WordNet (Fellbaum, 1998) sense annotation effort
reported by Passonneau et al (2010) found considerable inter-
annotator variability for some lexemes; FrameNet (Baker et
al., 1998) is limited in coverage, even for English; and Prop-
Bank (Kingsbury and Palmer, 2002) does not capture semantic
relationships across lexemes. We note that the Omega ontol-
ogy (Philpot et al, 2003) has been used for fine-grained cross-
lingual annotation (Hovy et al, 2006; Dorr et al, 2010).
Q.

J?K

considers
H. A

J?
book
?

	
JJ
k.
Guinness
?A

P?

C?
for-records

?J
?AJ


?? @
the-standard
COMMUNICATION
	
?

@
that

???Ag.
university
	
?@?Q


?? @
Al-Karaouine
ARTIFACT
?


	
?
in
?A
	
?
Fez
H. Q
	??? @
Morocco
LOCATION
?Y

?

@
oldest

???Ag.
university
GROUP
?


	
?
in
??A?? @
the-world
LOCATION
IJ
k
where
??

'
was
A?D?J
?

A

K
established
ACT
?


	
?
in

?
	
J?
year
859 ?


XCJ
?
AD
TIME
.
?The Guinness Book of World Records considers the
University of Al-Karaouine in Fez, Morocco, established
in the year 859 AD, the oldest university in the world.?
Figure 1: A sentence from the article ?Islamic Golden
Age,? with the supersense tagging from one of two anno-
tators. The Arabic is shown left-to-right.
entries, but here we have repurposed them as target
labels for direct human annotation.
Part of the earliest versions of WordNet, the
supersense categories (originally, ?lexicographer
classes?) were intended to partition all English noun
and verb senses into broad groupings, or semantic
fields (Miller, 1990; Fellbaum, 1990). More re-
cently, the task of automatic supersense tagging has
emerged for English (Ciaramita and Johnson, 2003;
Curran, 2005; Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009), as well as for Italian (Picca et al,
2008; Picca et al, 2009; Attardi et al, 2010) and
Chinese (Qiu et al, 2011), languages with WordNets
mapped to English WordNet.3 In principle, we be-
lieve supersenses ought to apply to nouns and verbs
in any language, and need not depend on the avail-
ability of a semantic lexicon.4 In this work we focus
on the noun SSTs, summarized in figure 2 and ap-
plied to an Arabic sentence in figure 1.
SSTs both refine and relate lexical items: they
capture lexical polysemy on the one hand?e.g.,
3Note that work in supersense tagging used text with fine-
grained sense annotations that were then coarsened to SSTs.
4The noun/verb distinction might prove problematic in some
languages.
253
Crusades ?Damascus ? Ibn Tolun Mosque ? Imam Hussein Shrine ? Islamic Golden Age ? Islamic History ?Ummayad Mosque 434s 16,185t 5,859m
Atom ? Enrico Fermi ? Light ? Nuclear power ? Periodic Table ? Physics ? Muhammad al-Razi 777s 18,559t 6,477m
2004 Summer Olympics ?Christiano Ronaldo ?Football ?FIFA World Cup ?Portugal football team ?Rau?l Gonza?les ?Real Madrid 390s 13,716t 5,149m
Computer ? Computer Software ? Internet ? Linux ? Richard Stallman ? Solaris ? X Window System 618s 16,992t 5,754m
Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts
of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239
mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?.
disambiguating PERSON vs. POSSESSION for the
noun principal?and generalize across lexemes on
the other?e.g., principal, teacher, and student can
all be PERSONs. This lumping property might be
expected to give too much latitude to annotators; yet
we find that in practice, it is possible to elicit reason-
able inter-annotator agreement, even for a language
other than English. We encapsulate our interpreta-
tion of the tags in a set of brief guidelines that aims
to be usable by anyone who can read and understand
a text in the target language; our annotators had no
prior expertise in linguistics or linguistic annotation.
Finally, we note that ad hoc categorization
schemes not unlike SSTs have been developed for
purposes ranging from question answering (Li and
Roth, 2002) to animacy hierarchy representation for
corpus linguistics (Zaenen et al, 2004). We believe
the interpretation of the SSTs adopted here can serve
as a single starting point for diverse resource en-
gineering efforts and applications, especially when
fine-grained sense annotation is not feasible.
2 Tagging Conventions
WordNet?s definitions of the supersenses are terse,
and we could find little explicit discussion of the
specific rationales behind each category. Thus,
we have crafted more specific explanations, sum-
marized for nouns in figure 2. English examples
are given, but the guidelines are intended to be
language-neutral. A more systematic breakdown,
formulated as a 43-rule decision list, is included
with the corpus.5 In developing these guidelines
we consulted English WordNet (Fellbaum, 1998)
and SemCor (Miller et al, 1993) for examples and
synset definitions, occasionally making simplifying
decisions where we found distinctions that seemed
esoteric or internally inconsistent. Special cases
(e.g., multiword expressions, anaphora, figurative
5For example, one rule states that all man-made structures
(buildings, rooms, bridges, etc.) are to be tagged as ARTIFACTs.
language) are addressed with additional rules.
3 Arabic Wikipedia Annotation
The annotation in this work was on top of a small
corpus of Arabic Wikipedia articles that had al-
ready been annotated for named entities (Mohit et
al., 2012). Here we use two different annotators,
both native speakers of Arabic attending a university
with English as the language of instruction.
Data & procedure. The dataset (table 1) consists of
the main text of 28 articles selected from the topical
domains of history, sports, science, and technology.
The annotation task was to identify and categorize
mentions, i.e., occurrences of terms belonging to
noun supersenses. Working in a custom, browser-
based interface, annotators were to tag each relevant
token with a supersense category by selecting the to-
ken and typing a tag symbol. Any token could be
marked as continuing a multiword unit by typing <.
If the annotator was ambivalent about a token they
were to mark it with the ? symbol. Sentences were
pre-tagged with suggestions where possible.6 Anno-
tators noted obvious errors in sentence splitting and
grammar so ill-formed sentences could be excluded.
Training. Over several months, annotators alter-
nately annotated sentences from 2 designated arti-
cles of each domain, and reviewed the annotations
for consistency. All tagging conventions were deve-
loped collaboratively by the author(s) and annotators
during this period, informed by points of confusion
and disagreement. WordNet and SemCor were con-
sulted as part of developing the guidelines, but not
during annotation itself so as to avoid complicating
the annotation process or overfitting to WordNet?s
idiosyncracies. The training phase ended once inter-
annotator mention F1 had reached 75%.
6Suggestions came from the previous named entity annota-
tion of PERSONs, organizations (GROUP), and LOCATIONs, as
well as heuristic lookup in lexical resources?Arabic WordNet
entries (Elkateb et al, 2006) mapped to English WordNet, and
named entities in OntoNotes (Hovy et al, 2006).
254
O NATURAL OBJECT natural feature or nonliving object in
nature barrier reef nest neutron star
planet sky fishpond metamorphic rock Mediterranean cave
stepping stone boulder Orion ember universe
A ARTIFACT man-made structures and objects bridge
restaurant bedroom stage cabinet toaster antidote aspirin
L LOCATION any name of a geopolitical entity, as well as
other nouns functioning as locations or regions
Cote d?Ivoire New York City downtown stage left India
Newark interior airspace
P PERSON humans or personified beings; names of social
groups (ethnic, political, etc.) that can refer to an individ-
ual in the singular Persian deity glasscutter mother
kibbutznik firstborn worshiper Roosevelt Arab consumer
appellant guardsman Muslim American communist
G GROUP groupings of people or objects, including: orga-
nizations/institutions; followers of social movements
collection flock army meeting clergy Mennonite Church
trumpet section health profession peasantry People?s Party
U.S. State Department University of California population
consulting firm communism Islam (= set of Muslims)
$ SUBSTANCE a material or substance krypton mocha
atom hydrochloric acid aluminum sand cardboard DNA
H POSSESSION term for an entity involved in ownership or
payment birthday present tax shelter money loan
T TIME a temporal point, period, amount, or measurement
10 seconds day Eastern Time leap year 2nd millenium BC
2011 (= year) velocity frequency runtime latency/delay
middle age half life basketball season words per minute
curfew industrial revolution instant/moment August
= RELATION relations between entities or quantities
ratio scale reverse personal relation exponential function
angular position unconnectedness transitivity
Q QUANTITY quantities and units of measure, including
cardinal numbers and fractional amounts 7 cm 1.8 million
12 percent/12% volume (= spatial extent) volt real number
square root digit 90 degrees handful ounce half
F FEELING subjective emotions indifference wonder
murderousness grudge desperation astonishment suffering
M MOTIVE an abstract external force that causes someone
to intend to do something reason incentive
C COMMUNICATION information encoding and transmis-
sion, except in the sense of a physical object
grave accent Book of Common Prayer alphabet
Cree language onomatopoeia reference concert hotel bill
broadcast television program discussion contract proposal
equation denial sarcasm concerto software
? COGNITION aspects of mind/thought/knowledge/belief/
perception; techniques and abilities; fields of academic
study; social or philosophical movements referring to the
system of beliefs Platonism hypothesis
logic biomedical science necromancy hierarchical structure
democracy innovativeness vocational program woodcraft
reference visual image Islam (= Islamic belief system) dream
scientific method consciousness puzzlement skepticism
reasoning design intuition inspiration muscle memory skill
aptitude/talent method sense of touch awareness
S STATE stable states of affairs; diseases and their symp-
toms symptom reprieve potency
poverty altitude sickness tumor fever measles bankruptcy
infamy opulence hunger opportunity darkness (= lack of light)
@ ATTRIBUTE characteristics of people/objects that can be
judged resilience buxomness virtue immateriality
admissibility coincidence valence sophistication simplicity
temperature (= degree of hotness) darkness (= dark coloring)
! ACT things people do or cause to happen; learned pro-
fessions meddling malpractice faith healing dismount
carnival football game acquisition engineering (= profession)
E EVENT things that happens at a given place and time
bomb blast ordeal miracle upheaval accident tide
R PROCESS a sustained phenomenon or one marked by
gradual changes through a series of states
oscillation distillation overheating aging accretion/growth
extinction evaporation
X PHENOMENON a physical force or something that hap-
pens/occurs electricity suction tailwind tornado effect
+ SHAPE two and three dimensional shapes
D FOOD things used as food or drink
B BODY human body parts, excluding diseases and their
symptoms
Y PLANT a plant or fungus
N ANIMAL non-human, non-plant life
Science chemicals, molecules, atoms, and subatomic
particles are tagged as SUBSTANCE
Sports championships/tournaments are EVENTs
(Information) Technology Software names, kinds, and
components are tagged as COMMUNICATION (e.g. kernel,
version, distribution, environment). A connection is a RE-
LATION; project, support, and a configuration are tagged
as COGNITION; development and collaboration are ACTs.
Arabic conventions Masdar constructions (verbal
nouns) are treated as nouns. Anaphora are not tagged.
Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME,
short description, and examples. Some examples and longer descriptions have been omitted due to space constraints.
Below: A few domain- and language-specific elaborations of the general guidelines.
255
Figure 3: Distribution of supersense mentions by
domain (left), and counts for tags occurring over
800 times (below). (Counts are of the union of the
annotators? choices, even when they disagree.)
tag num tag num
ACT (!) 3473 LOCATION (G) 1583
COMMUNICATION (C) 3007 GROUP (L) 1501
PERSON (P) 2650 TIME (T) 1407
ARTIFACT (A) 2164 SUBSTANCE ($) 1291
COGNITION (?) 1672 QUANTITY (Q) 1022
Main annotation. After training, the two annota-
tors proceeded on a per-document basis: first they
worked together to annotate several sentences from
the beginning of the article, then each was inde-
pendently assigned about half of the remaining sen-
tences (typically with 5?10 shared to measure agree-
ment). Throughout the process, annotators were en-
couraged to discuss points of confusion with each
other, but each sentence was annotated in its entirety
and never revisited. Annotation of 28 articles re-
quired approximately 100 annotator-hours. Articles
used in pilot rounds were re-annotated from scratch.
Analysis. Figure 3 shows the distribution of SSTs in
the corpus. Some of the most concrete tags?BODY,
ANIMAL, PLANT, NATURAL OBJECT, and FOOD?
were barely present, but would likely be frequent
in life sciences domains. Others, such as MOTIVE,
POSSESSION, and SHAPE, are limited in scope.
To measure inter-annotator agreement, 87 sen-
tences (2,774 tokens) distributed across 19 of the ar-
ticles (not including those used in pilot rounds) were
annotated independently by each annotator. Inter-
annotator mention F1 (counting agreement over en-
tire mentions and their labels) was 70%. Excluding
the 1,397 tokens left blank by both annotators, the
token-level agreement rate was 71%, with Cohen?s
? = 0.69, and token-level F1 was 83%.7
We also measured agreement on a tag-by-tag ba-
sis. For 8 of the 10 most frequent SSTs (fig-
ure 3), inter-annotator mention F1 ranged from 73%
to 80%. The two exceptions were QUANTITY at
63%, and COGNITION (probably the most heteroge-
neous category) at 49%. An examination of the con-
fusion matrix reveals four pairs of supersense cate-
gories that tended to provoke the most disagreement:
COMMUNICATION/COGNITION, ACT/COGNITION,
ACT/PROCESS, and ARTIFACT/COMMUNICATION.
7Token-level measures consider both the supersense label
and whether it begins or continues the mention.
The last is exhibited for the first mention in figure 1,
where one annotator chose ARTIFACT (referring to
the physical book) while the other chose COMMU-
NICATION (the content). Also in that sentence, an-
notators disagreed on the second use of university
(ARTIFACT vs. GROUP). As with any sense anno-
tation effort, some disagreements due to legitimate
ambiguity and different interpretations of the tags?
especially the broadest ones?are unavoidable.
A ?soft? agreement measure (counting as matches
any two mentions with the same label and at least
one token in common) gives an F1 of 79%, show-
ing that boundary decisions account for a major por-
tion of the disagreement. E.g., the city Fez, Mo-
rocco (figure 1) was tagged as a single LOCATION
by one annotator and as two by the other. Further
examples include the technical term ?thin client?,
for which one annotator omitted the adjective; and
?World Cup Football Championship?, where one an-
notator tagged the entire phrase as an EVENT while
the other tagged ?football? as a separate ACT.
4 Conclusion
We have codified supersense tags as a simple an-
notation scheme for coarse lexical semantics, and
have shown that supersense annotation of Ara-
bic Wikipedia can be rapid, reliable, and robust
(about half the tokens in our data are covered
by a nominal supersense). Our tagging guide-
lines and corpus are available for download at
http://www.ark.cs.cmu.edu/ArabicSST/.
Acknowledgments
We thank Nourhen Feki and Sarah Mustafa for assistance
with annotation, as well as Emad Mohamed, CMU ARK
members, and anonymous reviewers for their comments.
This publication was made possible by grant NPRP-08-
485-1-083 from the Qatar National Research Fund (a
member of the Qatar Foundation). The statements made
herein are solely the responsibility of the authors.
256
References
Giuseppe Attardi, Stefano Dei Rossi, Giulia Di Pietro,
Alessandro Lenci, Simonetta Montemagni, and Maria
Simi. 2010. A resource and tool for super-sense
tagging of Italian texts. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics (COLING-
ACL ?98), pages 86?90, Montreal, Quebec, Canada,
August. Association for Computational Linguistics.
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning, 34(1).
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages 168?
175, Sapporo, Japan, July.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan, June.
Bonnie J. Dorr, Rebecca J. Passonneau, David Farwell,
Rebecca Green, Nizar Habash, Stephen Helmreich,
Eduard Hovy, Lori Levin, Keith J. Miller, Teruko
Mitamura, Owen Rambow, and Advaith Siddharthan.
2010. Interlingual annotation of parallel text corpora:
a new framework for annotation and evaluation. Nat-
ural Language Engineering, 16(03):197?243.
Sabri Elkateb, William Black, Horacio Rodr??guez, Musa
Alkhalifa, Piek Vossen, Adam Pease, and Christiane
Fellbaum. 2006. Building a WordNet for Arabic.
In Proceedings of The Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
pages 29?34, Genoa, Italy.
Christiane Fellbaum. 1990. English verbs as a semantic
net. International Journal of Lexicography, 3(4):278?
301, December.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL (HLT-
NAACL), pages 57?60, New York City, USA, June.
Association for Computational Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Las Palmas, Canary Islands,
May.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING?02),
pages 1?7, Taipei, Taiwan, August. Association for
Computational Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the Workshop on Human Language
Technology (HLT ?93), HLT ?93, pages 303?308,
Plainsboro, NJ, USA, March. Association for Compu-
tational Linguistics.
George A. Miller. 1990. Nouns in WordNet: a lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264, December.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, and Noah A. Smith. 2012.
Recall-oriented learning of named entities in Arabic
Wikipedia. In Proceedings of the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 162?173, Avi-
gnon, France, April. Association for Computational
Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploiting
semantic constraints for estimating supersenses with
CRFs. In Proceedings of the Ninth SIAM International
Conference on Data Mining, pages 485?496, Sparks,
Nevada, USA, May. Society for Industrial and Applied
Mathematics.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
257
Andrew G. Philpot, Michael Fleischman, and Eduard H.
Hovy. 2003. Semi-automatic construction of a general
purpose ontology. In Proceedings of the International
Lisp Conference, New York, NY, USA, October.
Davide Picca, Alfio Massimiliano Gliozzo, and Mas-
similiano Ciaramita. 2008. Supersense Tagger
for Italian. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
Piperidis, and Daniel Tapias, editors, Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC?08), pages 2386?2390, Marrakech, Mo-
rocco, May. European Language Resources Associa-
tion (ELRA).
Davide Picca, Alfio Massimiliano Gliozzo, and Simone
Campora. 2009. Bridging languages by SuperSense
entity tagging. In Proceedings of the 2009 Named
Entities Workshop: Shared Task on Transliteration
(NEWS 2009), pages 136?142, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Likun Qiu, Yunfang Wu, Yanqiu Shao, and Alexander
Gelbukh. 2011. Combining contextual and struc-
tural information for supersense tagging of Chinese
unknown words. In Computational Linguistics and In-
telligent Text Processing: Proceedings of the 12th In-
ternational Conference on Computational Linguistics
and Intelligent Text Processing (CICLing?11), volume
6608 of Lecture Notes in Computer Science, pages 15?
28. Springer, Berlin.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC-02), Las Pal-
mas, Canary Islands, May.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O?Connor, and Tom Wasow. 2004. An-
imacy encoding in English: why and how. In Bon-
nie Webber and Donna K. Byron, editors, ACL 2004
Workshop on Discourse Annotation, pages 118?125,
Barcelona, Spain, July. Association for Computational
Linguistics.
258
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 181?185,
Dublin, Ireland, August 23-24, 2014.
CMUQ-Hybrid: Sentiment Classification
By Feature Engineering and Parameter Tuning
Kamla Al-Mannai
1
, Hanan Alshikhabobakr
2
,
Sabih Bin Wasi
2
, Rukhsar Neyaz
2
, Houda Bouamor
2
, Behrang Mohit
2
Texas A&M University in Qatar
1
, Carnegie Mellon University in Qatar
2
almannaika@hotmail.com
1
{halshikh, sabih, rukhsar, hbouamor, behrang}@cmu.edu
Abstract
This paper describes the system we sub-
mitted to the SemEval-2014 shared task
on sentiment analysis in Twitter. Our sys-
tem is a hybrid combination of two system
developed for a course project at CMU-
Qatar. We use an SVM classifier and cou-
ple a set of features from one system with
feature and parameter optimization frame-
work from the second system. Most of the
tuning and feature selection efforts were
originally aimed at task-A of the shared
task. We achieve an F-score of 84.4% for
task-A and 62.71% for task-B and the sys-
tems are ranked 3rd and 29th respectively.
1 Introduction
With the proliferation of Web2.0, people increas-
ingly express and share their opinion through so-
cial media. For instance, microblogging websites
such as Twitter
1
are becoming a very popular com-
munication tool. An analysis of this platform re-
veals a large amount of community messages ex-
pressing their opinions and sentiments on differ-
ent topics and aspects of life. This makes Twit-
ter a valuable source of subjective and opinionated
text that could be used in several NLP research
works on sentiment analysis. Many approaches
for detecting subjectivity and determining polarity
of opinions in Twitter have been proposed (Pang
and Lee, 2008; Davidov et al., 2010; Pak and
Paroubek, 2010; Tang et al., 2014). For instance,
the Twitter sentiment analysis shared task (Nakov
et al., 2013) is an interesting testbed to develop
and evaluate sentiment analysis systems on social
media text. Participants are asked to implement
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
a system capable of determining whether a given
tweet expresses positive, negative or neutral sen-
timent. In this paper, we describe the CMUQ-
Hybrid system we developed to participate in the
two subtasks of SemEval 2014 Task 9 (Rosenthal
et al., 2014). Our system uses an SVM classifier
with a rich set of features and a parameter opti-
mization framework.
2 Data Preprocessing
Working with tweets presents several challenges
for NLP, different from those encountered when
dealing with more traditional texts, such as
newswire data. Tweet messages usually contain
different kinds of orthographic and typographical
errors such as the use of special and decorative
characters, letter duplication used generally for
emphasis, word duplication, creative spelling and
punctuation, URLs, #hashtags as well as the use
of slangs and special abbreviations. Hence, before
building our classifier, we start with a preprocess-
ing step on the data, in order to normalize it. All
letters are converted to lower case and all words
are reduced to their root form using the WordNet
Lemmatizer in NLTK
2
(Bird et al., 2009). We kept
only some punctuation marks: periods, commas,
semi-colons, and question and exclamation marks.
The excluded characters were identified to be per-
formance boosters using the best-first branch and
bound technique described in Section 3.
3 Feature Extraction
Out of a wide variety of features, we selected the
most effective features using the best-first branch
and bound method (Neapolitan, 2014), a search
tree technique for solving optimization problems.
We used this technique to determine which punc-
tuation marks to keep in the preprocessing step and
2
http://www.nltk.org/api/nltk.stem.
html
181
in selecting features as well. In the feature selec-
tion step, the root node is represented by a bag of
words feature, referred as textual tokens.
At each level of the tree, we consider a set of
different features, and iteratively we carry out the
following steps: we process the current feature by
generating its successors, which are all the other
features. Then, we rank features according to the
f-score and we only process the best feature and
prune the rest. We pass all the current pruned fea-
tures as successors to the next level of the tree. The
process iterates until all partial solutions in the tree
are processed or terminated. The selected features
are the following:
Sentiment lexicons : we used the Bing Liu Lex-
icon (Hu and Liu, 2004), the MPQA Subjectivity
Lexicon (Wilson et al., 2005), and NRC Hashtag
Sentiment Lexicon (Mohammad et al., 2013). We
count the number of words in each class, result-
ing in three features: (a) positive words count, (b)
negative words count and (c) neutral words count.
Negative presence: presence of negative words
in a term/tweet using a list of negative words. The
list used is built from the Bing Liu Lexicon (Hu
and Liu, 2004).
Textual tokens: the target term/tweet is seg-
mented into tokens based on space. Token identity
features are created and assigned the value of 1.
Overall polarity score: we determine the polar-
ity scores of words in a target term/tweet using the
Sentiment140 Lexicon (Mohammad et al., 2013)
and the SentiWordNet lexicon (Baccianella et al.,
2010). The overall score is computed by adding
up all word scores.
Level of association: indicates whether the
overall polarity score of a term is greater than 0.2
or not. The threshold value was optimized on the
development set.
Sentiment frequency: indicates the most fre-
quent word sentiment in the tweet. We determine
the sentiment of words using an automatically
generated lexicon. The lexicon comprises 3,247
words and their sentiments. Words were obtained
from the provided training set for task-A and sen-
timents were generated using our expression-level
classifier.
We used slightly different features for Task-A
and Task-B. The features extracted for each task
are summarized in Table 1.
Feature Task A Task B
Positive words count X
Negative words count X
Neutral words count X
Negative presence X X
Textual tokens X X
Overall polarity score X X
Level of association X
Sentiment frequency X
Table 1: Feature summary for each task.
4 Modeling Kernel Functions
Initially we experimented with both logistic
regression and the Support Vector Machine
(SVM) (Fan et al., 2008), using the Stochastic
Gradient Descent (SGD) algorithm for parame-
ter optimization. In our development experiments,
SVM outperformed and became our single classi-
fier. We used the LIBSVM package (Chang and
Lin, 2011) to train and test our classifier.
An SVM kernel function and associated param-
eters were optimized for best F-score on the de-
velopment set. In order to avoid the model over-
fitting the data, we select the optimal parameter
value only if there are smooth gaps between the
near neighbors of the corresponded F-score. Oth-
erwise, the search will continue to the second op-
timal value.
In machine learning, the difference between the
number of training samples, m, and the number
of features, n, is crucial in the selection process
of SVM kernel functions. The Gaussian kernel is
suggested when m is slightly larger than n. Other-
wise, the linear kernel is recommended. In Task-
B, the n : m ratio was 1 : 3 indicating a large
difference between the two numbers. Whereas in
Task-A, a ratio of 5 : 2 indicated a small differ-
ence between the two numbers. We selected the
theoretical types, after conducting an experimen-
tal verification to identify the best kernel function
according to the f-score.
We used a radical basis function kernel for the
expression-level task and the value of its gamma
parameter was adjusted to 0.319. Whereas, we
used a linear function kernel for the message-level
task and the value of its cost parameter was ad-
justed to 0.053.
182
5 Experiments and Results
In this section, we describe the data and the sev-
eral experiments we conducted for both tasks. We
train and evaluate our classifier with the training,
development and testing datasets provided for the
SemEval 2014 shared task. A short summary of
the data distribution is shown in Table 2.
Dataset Postive Negative Neutral
Task-A:
Train (9,451) 62% 33% 5%
Dev (1,135) 57% 38% 5%
Test (10,681) 60% 35% 5%
Task-B:
Train (9,684) 38% 15% 47%
Dev (1,654) 35% 21% 44%
Test (5,754) 45% 15% 40%
Table 2: Datasets distribution percentage per class.
Our test dataset is composed of five different
sets: The test dataset is composed of five dif-
ferent sets: Twitter2013 a set of tweets collected
for the SemEval2013 test set, Twitter2014, tweets
collected for this years version, LiveJournal2014
consisting of formal tweets, SMS2013, a collection
of sms messages, TwitterSarcasm, a collection of
sarcastic tweets.
5.1 Task-A
For this task, we train our classifier on 10,586
terms (9,451 terms in the training set and 1,135
in the development set), tune it on 4,435 terms,
and evaluate it using 10,681 terms. The average
F-score of the positive and negative classes for
each dataset is given in the first part of Table 3.
The best F-score value of 88.94 is achieved on the
Twitter2013.
We conducted an ablation study illustrated in
the second part of Table 3 shows that all the se-
lected features contribute well in our system per-
formance. Other than the textual tokens feature,
which refers to a bag of preprocessed tokens, the
study highlights the role of the term polarity score
feature: ?4.20 in the F-score, when this feature is
not considered on the TwitterSarcasm dataset.
Another study conducted is a feature correlation
analysis, in which we grouped features with sim-
ilar intuitions. Namely the two features negative
presence and negative words count are grouped
as ?negative features?, and the features positive
words count and negative words count are grouped
as ?words count?. We show in Table 4 the effect
on f-score after removing each group from the fea-
tures set. Also we show the f-score after remov-
ing each individual feature within the group. This
helps us see whether features within a group are
redundant or not. For the Twitter2014 dataset, we
notice that excluding one of the features in any of
the two groups leads to a significant drop, in com-
parison to the total drop by its group. The uncor-
related contributions of features within the same
group indicate that features are not redundant to
each other and that they are indeed capturing dif-
ferent information. However, in the case of the
TwitterSarcasm dataset, we observe that the neg-
ative presence feature is not only not contributing
to the system performance but also adding noise
to the feature space, specifically, to the negative
words count feature.
5.2 Task-B
For this task, we trained our classifier on 11,338
tweets (9,684 terms in the training set and 1,654
in the development set), tuned it on 3,813 tweets,
and evaluated it using 8,987 tweets. Results for
different feature configurations are reported in Ta-
ble 5.
It is important to note that if we exclude the tex-
tual tokens feature, all datasets benefit the most
from the polarity score feature. It is interesting to
note that the bag of words, referred to as textual
tokens, is not helping in one of the datasets, the
TwitterSarcasm set. For all datasets, performance
could be improved by removing different features.
In Table 5, we observe that the Negative pres-
ence feature decreases the F-score on the Twitter-
Sarcasm dataset. This could be explained by the
fact that negative words do not usually appear in
a negative implication in sarcastic messages. For
example, this tweet: Such a fun Saturday catch-
ing up on hw. which has a negative sentiment, is
classified positive because of the absence of neg-
ative words. Table 5 shows that the textual tokens
feature increases the classifier?s performance up to
+21.07 for some datasets. However, using a large
number of features in comparison to the number
of training samples could increase data sparseness
and lower the classifier?s performance.
We conducted a post-competition experiment to
examine the relationship between the number of
features and the number of training samples. We
183
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative presence -0.45 0.00 -0.45 -0.23 +0.30
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Polarity score -1.83 -4.20 -0.23 -2.14 -3.00
Level of association -0.18 0.00 -0.18 -0.07 +0.57
Textual tokens -8.74 -2.40 -3.02 -4.37 -6.06
Table 3: Task-A feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative features -1.53 -0.84 -3.05 -1.88 -0.67
Negative presence -0.45 0.00 -0.45 -0.23 +0.3
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Words count -1.07 -2.2 -0.79 -0.62 -2.01
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Table 4: Task-A features correlation analysis. We grouped features with similar intuitions and we calcu-
lated F-scores on each set along with the effect when removing one feature at a time.
fixed the size of our training dataset. Then, we
compared the performance of our classifier using
only the bag of tokens feature, in two different
sizes. In the first experiment, we included all to-
kens collected from all tweets. In the second, we
only considered the top 20 ranked tokens from
each tweet. Tokens were ranked according to the
difference between their highest level of associa-
tion into one of the sentiments and the sum of the
rest. The level of associations for tokens were de-
termined using the Sentiment140 and SentiWord-
Net lexicons. The threshold number of tokens was
identified empirically for best performance. We
found that the classifier?s performance has been
improved by 2 f-score points when the size of to-
kens bag is smaller. The experiment indicates that
the contribution of the bag of words feature can be
increased by reducing the size of vocabulary list.
6 Error Analysis
Our efforts are mostly tuned towards task-A,
hence our inspection and analysis is focused on
task-A. The error rate calculated per sentiment
class: positive, negative and neutral are 6.8%,
14.9% and 93.8%, respectively. The highest error
rate in the neutral class, 93.8%, is mainly due to
the few neutral examples in the training data (only
5% of the data). Hence the system could not learn
from such a small set of neutral class examples.
In the case of negative class error rate, 14.9%,
most of which were classified as positive. An ex-
ample of such classification: I knew it was too
good to be true OTL. Since our system highly re-
lies on lexicon, hence looking at lexicon assigned
polarity to the phrase too good to be true which is
positive, happens because the positive words good
and true has dominating positive polarity.
Lastly for the positive error rate, which is rel-
atively lower, 6%, most of which were classified
negative instead of positive. An example of such
classification: Looks like we?re getting the heavi-
est snowfall in five years tomorrow. Awesome. I?ll
never get tired of winter. Although the phrase car-
ries a positive sentiment, the individual negative
words of the phrase never and tired again domi-
nates over the phrase.
7 Conclusion
We described our systems for Twitter Sentiment
Analysis shared task. We participated in both
tasks, but were mostly focused on task-A. Our hy-
brid system was assembled by integrating a rich
set of lexical features into a framework of fea-
ture selection and parameter tuning, The polarity
184
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 62.71 40.95 65.14 63.22 61.75
Negative presence -1.65 +1.26 -3.37 -3.66 -0.95
Neutral words count +0.05 0.00 -0.72 -0.57 -0.54
Polarity score -4.03 -6.92 -3.82 -3.83 -4.84
Sentiment frequency +0.10 0.00 +0.18 -0.12 -0.05
textual tokens -17.91 +6.5 -21.07 -19.97 -15.8
Table 5: Task B feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
score feature was the most important feature for
our model in both tasks. The F-score results were
consistent across all datasets, except the Twitter-
Sarcasm dataset. It indicates that feature selection
and parameter tuning steps were effective in gen-
eralizing the model to unseen data.
Acknowledgment
We would like to thank Kemal Oflazer and also the
shared task organizers for their support throughout
this work.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC?10), pages 2200?2204, Valletta, Malta.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology, 2:27:1?27:27.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 241?249, Uppsala, Sweden.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 168?
177, Seattle, WA, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA.
Richard E. Neapolitan, 2014. Foundations of Algo-
rithms, pages 257?262. Jones & Bartlett Learning.
Alexander Pak and Patrick Paroubek. 2010. Twitter
Based System: Using Twitter for Disambiguating
Sentiment Ambiguous Adjectives. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 436?439, Uppsala, Sweden.
Bo Pang and Lillian Lee, 2008. Opinion Mining and
Sentiment Analysis, volume 2, pages 1?135. Now
Publishers Inc.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14), Dublin, Ireland.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1555?1565, Baltimore, Maryland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Vancouver, B.C.,
Canada.
185
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 186?191,
Dublin, Ireland, August 23-24, 2014.
CMUQ@Qatar:Using Rich Lexical Features for
Sentiment Analysis on Twitter
Sabih Bin Wasi, Rukhsar Neyaz, Houda Bouamor, Behrang Mohit
Carnegie Mellon University in Qatar
{sabih, rukhsar, hbouamor, behrang}@cmu.edu
Abstract
In this paper, we describe our system for
the Sentiment Analysis of Twitter shared
task in SemEval 2014. Our system uses
an SVM classifier along with rich set of
lexical features to detect the sentiment of
a phrase within a tweet (Task-A) and also
the sentiment of the whole tweet (Task-
B). We start from the lexical features that
were used in the 2013 shared tasks, we en-
hance the underlying lexicon and also in-
troduce new features. We focus our fea-
ture engineering effort mainly on Task-
A. Moreover, we adapt our initial frame-
work and introduce new features for Task-
B. Our system reaches weighted score of
87.11% in Task-A and 64.52% in Task-B.
This places us in the 4th rank in the Task-
A and 15th in the Task-B.
1 Introduction
With more than 500 million tweets sent per day,
containing opinions and messages, Twitter
1
has
become a gold-mine for organizations to monitor
their brand reputation. As more and more users
post about products and services they use, Twit-
ter becomes a valuable source of people?s opin-
ions and sentiments: what people can think about
a product or a service, how positive they can be
about it or what would people prefer the product to
be like. Such data can be efficiently used for mar-
keting. However, with the increasing amount of
tweets posted on a daily basis, it is challenging and
expensive to manually analyze them and locate the
meaningful ones. There has been a body of re-
cent work to automatically learn the public sen-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
timents from tweets using natural language pro-
cessing techniques (Pang and Lee, 2008; Jansen
et al., 2009; Pak and Paroubek, 2010; Tang et al.,
2014). However, the task of sentiment analysis of
tweets in their free format is harder than that of any
well-structured document. Tweet messages usu-
ally contain different kinds of orthographic errors
such as the use of special and decorative charac-
ters, letter or word duplication, extra punctuation,
as well as the use of special abbreviations.
In this paper, we present our machine learn-
ing based system for sentiment analysis of Twitter
shared task in SemEval 2014. Our system takes
as input an arbitrary tweet and assigns it to one
of the following classes that best reflects its sen-
timent: positive, negative or neutral. While pos-
itive and negative tweets are subjective, neutral
class encompasses not only objective tweets but
also subjective tweets that does not contain any
?polar? emotion. Our classifier was developed as
an undergrad course project but later pursued as
a research topic. Our training, development and
testing experiments were performed on data sets
published in SemEval 2013 (Nakov et al., 2013).
Motivated with its performance, we participated
in SemEval 2014 Task 9 (Rosenthal et al., 2014).
Our approach includes an extensive usage of off-
the-shelf resources that have been developed for
conducting NLP on social media text. Our orig-
inal aim was enhancement of the task-A. More-
over, we adapted our framework and introduced
new features for task-B and participated in both
shared tasks. We reached an F-score of 83.3% in
Task-A and an F-score of 65.57% in Task-B. That
placed us in the 4th rank in the task-A and 15th
rank in the task-B.
Our approach includes an extensive usage of
off-the-shelf resources that have been developed
for conducting NLP on social media text. That
includes the Twitter Tokenizer and also the Twit-
ter POS tagger, several sentiment analysis lexica
186
and finally our own enhanced resources for spe-
cial handling of Twitter-specific text. Our origi-
nal aim in introducing and evaluating many of the
features was enhancement of the task-A. More-
over, we adapted our framework and introduced
new features for task-B and participated in both
shared tasks. We reached an F-score of 83.3% in
Task-A and an F-score of 65.57% in Task-B. That
placed us in the 4th rank in the task-A and 15th
rank in the task-B.
2 System Overview
We participate in tasks A and B. We use three-
way classification framework in which we design
and use a rich feature representation of the Twitter
text. In order to process the tweets, we start with
a pre-processing step, followed by feature extrac-
tion and classifier training.
2.1 Data Pre-processing
Before the tweet is fed to the system, it goes
through pre-processing phase that breaks tweet
string into words (tokenization), attaches more in-
formation to each word (POS tagging), and other
treatments.
Tokenization: We use CMU ARK Tok-
enizer (Owoputi et al., 2013) to tokenize each
tweet. This tokenizer is developed to tokenize
not only space-separated text but also tokens that
need to be analyzed separately.
POS tagging: We use CMU ARK POS Tag-
ger (Owoputi et al., 2013) to assign POS tags to
the different tokens. In addition to the grammat-
ical tags, this tagger assigns also twitter-specific
tags like @ mentions, hash tags, etc. This infor-
mation is used later for feature extraction.
Other processing: In order to normalize the dif-
ferent tokens and convert them into a correct En-
glish, we find acronyms in the text and add their
expanded forms at the end of the list. We decide
to keep both the acronym and the new word to en-
sure that if the token without its expansion was
the word the user meant, then we are not losing
any information by getting its acronym. We ex-
tend the NetLingo
2
top 50 Internet acronym list to
add some missing acronyms. In order to reduce in-
flectional forms of a word to a common base form
we use WordNetlemmatizer in NLTK (Bird et al.,
2
http://www.netlingo.com/top50/
popular-text-terms.php
Tweet ?This is so awesome
@henry:D! #excited?
Bag of Words ?This?:1, ?is?:1, ?so?:1,
?awesome?:1, ?@henry?:1,
?:D?:1, ?!,?:1, #excited?:1
POS features numHashTags:1, numAd-
verb:1, numAdjective:1
Polarity features positiveWords:1, negWords:0,
avgScore: -0.113
Task-B specific
features
numCapsWords:0, numEmo-
ticons:1, numUrls:0
Table 1: Set of Features demonstrated on a sample
tweet for Task-B.
2009)
3
. This could be useful for the feature extrac-
tion, to get as much matches as possible between
the train and test data (e.g., for bag-of-words fea-
ture).
2.2 Feature Extraction
Assigning a sentiment to a single word, phrase or
a full tweet message requires a rich set of fea-
tures. For this, we adopt a forward selection ap-
proach (Ladha and Deepa, 2011) to select the fea-
tures that characterize to the best the different sen-
timents and help distinguishing them. In this ap-
proach, we incrementally add the features one by
one and test whether this boosts the development
results. We heavily rely on a binary feature rep-
resentation (Heinly et al., 2012) to ensure the ef-
ficiency and robustness of our classifier. The dif-
ferent features used are illustrated in the example
given in Table 1.
Bag-of-words feature: indicates whether a
given token is present in the phrase.
Morpho-syntactic feature: we use the POS and
twitter-specific tags extracted for each token. We
count the number of adjectives, adverbs and hash-
tags present in the focused part of the tweet mes-
sage (entire tweet or phrase). We tried adding
other POS based features (e.g., number of posses-
sive pronouns, etc.), but only the aforementioned
tags increased the result figures for both tasks.
Polarity-based features: we use freely avail-
able sentiment resources to explicitly define the
polarity at a token-level. We define three feature
categories, based on the lexicon used:
3
http://www.nltk.org/api/nltk.stem.
html
187
Task-A Task-B
Dev Train Test Dev Train Test
Positive 57.09 % 62.06% 59.49% 34.76% 37.59% 39.01%
Negative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15%
Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84%
All 1,135 9,451 10,681 1,654 9,684 8,987
Table 2: Class size distribution for all the three sets for both Task-A and Task-B.
? Subjectivity: number of words mapped to
?positive? from the MPQA Subjectivity lexi-
con (Wilson et al., 2005).
? Hybrid Lexicon: We combine the Senti-
ment140 lexicon (Mohammad et al., 2013)
with the Bing Liu?s bag of positive and neg-
ative words (Hu and Liu, 2004) to create a
dictionary in which each token is assigned a
sentiment.
? Token weight: we use the SentiWordNet
lexicon (Baccianella et al., 2010) to define
this feature. SentiWordNet contains positive,
negative and objective scores between 0 and
1 for all senses in WordNet. Based on this
sense level annotation, we first map each to-
ken to its weight in this lexicon and then the
sum of all these weights was used as the tweet
weight.
Furthermore, in order to take into account the
presence of negative words, which modify the po-
larity of the context within which they are invoked,
we reverse the polarity score of adjectives or ad-
verbs that come within 1-2 token distance after a
negative word.
Task specific features: In addition to the fea-
tures described above, we also define some task-
specific ones. For example, we indicate the num-
ber of capital letters in the phrase as a feature in
Task-A. This could help in this task, since we are
focusing on short text. For Task-B we indicate
instead the number of capital words. This relies
on the intuition that polarized tweets would carry
more (sometimes all) capital words than the neu-
tral or objective ones. We also added the number
of emoticons and number of URL links as fea-
tures for Task-B. Here, the goal is to segregate
fact-containing objective tweets from emotion-
containing subjective tweets.
2.3 Classifier
We use a Support Vector Machine (SVM) classi-
fier (Chang and Lin, 2011) to which we provide
the rich set of features described in the previous
section. We use a linear kernel and tune its param-
eter C separately for the two tasks. Task-A sys-
tem was bound tight to the development set with
C=0.18 whereas in Task-B the system was given
freedom by setting C=0.55. These values were
optimized during the development using a brute-
force mechanism.
Task-A Task-B
LiveJournal 2014 83.89 65.63
SMS 2013 88.08 62.95
Twitter 2013 89.85 65.11
Twitter 2014 83.45 65.53
Sarcasm 78.07 40.52
Weighted average 87.11 64.52
Table 3: F1 measures and final results of the sys-
tem for Task-A and Task-B for all the data sets
including the weighted average of the sets.
3 Experiments and Results
In this section, we explain details of the data and
the general settings for the different experiments
we conducted. We train and evaluate our classifier
for both tasks with the training, development and
testing datasets provided for the SemEval 2014
shared task. The size of the three datasets we
use as well as their class distributions are illus-
trated in Table 2 . It is important to note that
the total dataset size for training and development
set (10,586) is about the same as test set mak-
ing the learning considerably challenging for cor-
rect predictions. Positive instances covered more
than half of each dataset for Task-A while Neutral
were the most popular class for Task-B. The class
distribution of training set is the same as the test
set.
188
Task-A Task-B
all features 87.11 64.52
all-preprocessing 80.79(-6.32) 59.20(-5.32)
all-ARK tokenization 83.69(-3.42) 60.61(-3.91)
all-other treatments 85.06(-2.05) 62.19(-2.33)
only BOW 81.69(-5.42) 57.85(-6.67)
all-bow 82.05(-5.06) 52.04(-12.48)
all-pos 86.92(-0.19) 64.31(-0.21)
all-polarity based features 81.80(-5.31) 57.95(-6.57)
all-SVM tuning 80.82(-6.29) 21.41(-43.11)
all-SVM c=0.01 84.20(-2.91) 59.87(-4.65)
all-SVM c=selected 87.11(0.00) 64.52(0.00)
all-SVM c=1 86.39(-0.72) 62.51(-2.01)
Table 4: F-scores obtained on the test sets with the specific feature removed.
The test dataset is composed of five differ-
ent sets: Twitter2013 a set of tweets collected
for the SemEval2013 test set, Twitter2014, tweets
collected for this years version, LiveJournal2014
consisting of formal tweets, SMS2013, a collec-
tion of sms messages,TwitterSarcasm, a collection
of sarcastic tweets. The results of our system are
shown in Table 3. The top five rows shows the
results by the SemEval scorer for all the data sets
used by them. This scorer took the average of F1-
score of only positive and negative classes. The
last row shows the weighted average score of all
the scores for Task A and B from the different data
sets.
Our scores for Task-A and Task-B were 83.45
and 65.53 respectively for Twitter 2014.
Our system performed better on Twitter and
SMS test sets from 2013. This was reasonable
since we tuned our system on these datasets. On
the other hand, the system performed worst on sar-
casm test set. This drop is extremely evident in
Task-B where the results were dropped by 25%.
To analyze the effects of each step of our sys-
tem, we experimented with our system using dif-
ferent configurations. The results are shown in Ta-
ble 4 and our analysis is described in the following
subsections. The results were scored by SemEval
2014 scorer and we took the weighted average of
all data sets to accurately reflect the performance
of our system.
We show the polarities values assigned to each
token of a tweet by our classifier, in Table 5.
Tokens POS Tags Sentiments Polarity
This O Neutral -0.194
Is V Neutral -0.115
So R Neutral -0.253
Awesome A Positive 2.351
@Henry @ - -
#excited # Positive 1.84
Table 5: Polarity assigned using our classifier to
each word of a Tweet message.
3.1 Preprocessing Effects
We compared the effects of basic tokenization
(based on white space) against the richer ARK
Twitter tokenizer. The scores dropped by 3.42%
and 3.91% for Task-A and Task-B, respectively.
Other preprocessing enhancements like lemmati-
zation and acronym additions also gave our sys-
tem performance a boost. Again, the effects were
more visible for Task-B than for Task-A. Over-
all, the system performance was boosted by 6.32%
for Task-A and 5.32% for Task-B. Considering
the overall score for Task-B, this is a significant
change.
3.2 Feature Engineering Effects
To analyze the effect of feature extraction pro-
cess, we ran our system with different kind of
features disabled - one at a time. For Task-A,
unigram model and polarity based features were
equally important. For Task-B, bag of words fea-
ture easily outperformed the effects of any other
feature. However, polarity based features were
second important class of features for our system.
These suggest that if more accurate, exhaustive
189
and social media representative lexicons are made,
it would help both tasks significantly. POS based
features were not directly influential in our system.
However, these tags helped us find better matches
in lexicons where words are further identified with
their POS tag.
3.3 Classifier Tuning
We also analyzed the significance of SVM tuning
to our system. Without setting any parameter to
SVMutil library (Chang and Lin, 2011), we no-
ticed a drop of 6.29% to scores of Task-A and a
significant drop of 43.11% to scores of Task-B.
Since the library use poly kernel by default, the
results were drastically worse for Task-B due to
large feature set. We also compared the perfor-
mance with SVM kernel set to C=1. In this re-
stricted setting, the results were slightly lower than
the result obtained for our final system.
4 Discussion
During this work, we found that two improve-
ments to our system would have yielded better
scores. The first would be lexicons: Since the
lexicons like Sentiment140 Lexicon are automati-
cally generated, we found that they contain some
noise. As we noticed a drop of that our results
were critically dependent on these lexicons, this
noise would have resulted in incorrect predictions.
Hence, more accurate and larger lexicons are re-
quired for better classification, especially for the
tweet-level task. Unlike SentiWordNet these lexi-
cons should contain more informal words that are
common in social media. Additionally, as we can
see our system was not able to confidently predict
sarcasm tweets on both expression and message
level, special attention is required to analyze the
nature of sarcasm on Twitter and build a feature
set that can capture the true sentiment of the tweet.
5 Conclusion
We demonstrated our classification system that
could predict sentiment of an input tweet. Our
system performed more accurately in expression-
level prediction than on entire tweet-level predic-
tion. Our system relied heavily on bag-of-words
feature and polarity based features which in turn
relied on correct part-of-speech tagging and third-
party lexicons. With this system, we ranked 4th
in SemEval 2014 expression-level prediction task
and 15th in tweet-level prediction task.
Acknowledgment
We would like to thank Kemal Oflazer and the
shared task organizers for their support through-
out this work.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC?10), pages 2200?2204, Valletta, Malta.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology, 2:27:1?27:27.
Jared Heinly, Enrique Dunn, and Jan-Michael Frahm.
2012. Comparative Evaluation of Binary Features.
In Proceedings of the 12th European Conference on
Computer Vision, pages 759?773, Firenze, Italy.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 168?
177, Seattle, WA, USA.
Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter Power: Tweets as
Electronic Word of Mouth. Journal of the Ameri-
can society for information science and technology,
60(11):2169?2188.
L. Ladha and T. Deepa. 2011. Feature Selection Meth-
ods and Algorithms. International Journal on Com-
puter Science and Engineering (IJCSE), 3:1787?
1797.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA.
190
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved Part-of-Speech Tagging for
Online Conversational Text with Word Clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390, Atlanta, Georgia.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a Corpus for Sentiment Analysis and Opinion Min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), pages 1320?1326, Valletta, Malta.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis, volume 2. Now Publishers Inc.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14), Dublin, Ireland.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1555?1565, Baltimore, Maryland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347?354.
191
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39?47,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The First QALB Shared Task on Automatic Text Correction for Arabic
Behrang Mohit
1?
, Alla Rozovskaya
2?
, Nizar Habash
3
, Wajdi Zaghouani
1
, Ossama Obeid
1
1
Carnegie Mellon University in Qatar
2
Center for Computational Learning Systems, Columbia University
3
New York University Abu Dhabi
behrang@cmu.edu, alla@ccls.columbia.edu, nizar.habash@nyu.edu
wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu
Abstract
We present a summary of the first shared
task on automatic text correction for Ara-
bic text. The shared task received 18 sys-
tems submissions from nine teams in six
countries and represented a diversity of ap-
proaches. Our report includes an overview
of the QALB corpus which was the source
of the datasets used for training and eval-
uation, an overview of participating sys-
tems, results of the competition and an
analysis of the results and systems.
1 Introduction
The task of text correction has recently gained a
lot of attention in the Natural Language Process-
ing (NLP) community. Most of the effort in this
area concentrated on English, especially on errors
made by learners of English as a Second Lan-
guage. Four competitions devoted to error cor-
rection for non-native English writers took place
recently: HOO (Dale and Kilgarriff, 2011; Dale
et al., 2012) and CoNLL (Ng et al., 2013; Ng et
al., 2014). Shared tasks of this kind are extremely
important, as they bring together researchers who
focus on this problem and promote development
and dissemination of key resources, such as bench-
mark datasets.
Recently, there have been several efforts aimed
at creating data resources related to the correc-
tion of Arabic text. Those include human anno-
tated corpora (Zaghouani et al., 2014; Alfaifi and
Atwell, 2012), spell-checking lexicon (Attia et al.,
2012) and unannotated language learner corpora
(Farwaneh and Tamimi, 2012). A natural exten-
sion to these resource production efforts is the cre-
ation of robust automatic systems for error correc-
tion.
* These authors contributed equally to this work.
In this paper, we present a summary of the
QALB shared task on automatic text correction
for Arabic. The Qatar Arabic Language Bank
(QALB) project
1
is one of the first large scale data
and system development efforts for automatic cor-
rection of Arabic which has resulted in annota-
tion of the QALB corpus. In conjunction with the
EMNLP Arabic NLP workshop, the QALB shared
task is the first community effort for construction
and evaluation of automatic correction systems for
Arabic.
The results of the competition indicate that the
shared task attracted a lot of interest and generated
a diverse set of approaches from the participating
teams.
In the next section, we present the shared task
framework. This is followed by an overview of
the QALB corpus (Section 3). Section 4 describes
the shared task data, and Section 5 presents the ap-
proaches adopted by the participating teams. Sec-
tion 6 discusses the results of the competition. Fi-
nally, in Section 7, we offer a brief analysis and
present preliminary experiments on system com-
bination.
2 Task Description
The QALB shared task was created as a forum for
competition and collaboration on automatic error
correction in Modern Standard Arabic. The shared
task makes use of the QALB corpus (Zaghouani et
al., 2014), which is a manually-corrected collec-
tion of Arabic texts. The shared task participants
were provided with training and development data
to build their systems, but were also free to make
use of additional resources, including corpora, lin-
guistic resources, and software, as long as these
were publicly available.
For evaluation, a standard framework devel-
1
http://nlp.qatar.cmu.edu/qalb/
39
Original Corrected

??

K @Q? @

HCJ


?j

J? @

?
	
Y?

?

K @Q

? Y
	
J? ?



GXA?? ?


Y? @?P??

J

K B
?


X

?

@
	
?@ ?

<? @
	
?? ?


	
??

JK
.

I
	
J? ? H
.
A

? ?


	
G @

B

??
Q

j?
?
@ ?
YJ


?K
.
@
	
Y?
	
?@ @?YJ
.
K


	
?A? ? ?


??

?B@ Yj
.
??
?
AK
.
@P?Q?

?Q???@
	
???
?
?
	
K @ ??

?J


K
.
	
?A?

?J


	
J?B@ ???


Yg ?


	
? A? ??
	
? ?A
	
J?
?
@
.

??J


j

?? ?

J


	
J?@
	
?

BA??

?

?m
