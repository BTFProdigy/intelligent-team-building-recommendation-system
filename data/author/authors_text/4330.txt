A Machine Learning Approach to German Pronoun Resolution
Beata Kouchnir
Department of Computational Linguistics
Tu?bingen University
72074 Tu?bingen, Germany
kouchnir@sfs.uni-tuebingen.de
Abstract
This paper presents a novel ensemble
learning approach to resolving German
pronouns. Boosting, the method in
question, combines the moderately ac-
curate hypotheses of several classifiers
to form a highly accurate one. Exper-
iments show that this approach is su-
perior to a single decision-tree classi-
fier. Furthermore, we present a stan-
dalone system that resolves pronouns in
unannotated text by using a fully auto-
matic sequence of preprocessing mod-
ules that mimics the manual annotation
process. Although the system performs
well within a limited textual domain,
further research is needed to make it
effective for open-domain question an-
swering and text summarisation.
1 Introduction
Automatic coreference resolution, pronominal and
otherwise, has been a popular research area in
Natural Language Processing for more than two
decades, with extensive documentation of both
the rule-based and the machine learning approach.
For the latter, good results have been achieved
with large feature sets (including syntactic, se-
mantic, grammatical and morphological informa-
tion) derived from handannotated corpora. How-
ever, for applications that work with plain text (e.g.
question answering, text summarisation), this ap-
proach is not practical.
The system presented in this paper resolves
German pronouns in free text by imitating the
manual annotation process with off-the-shelf lan-
guage sofware. As the avalability and reliability of
such software is limited, the system can use only
a small number of features. The fact that most
German pronouns are morphologically ambiguous
proves an additional challenge.
The choice of boosting as the underlying ma-
chine learning algorithm is motivated both by its
theoretical concept as well as its performance for
other NLP tasks. The fact that boosting uses the
method of ensemble learning, i.e. combining the
decisions of several classifiers, suggests that the
combined hypothesis will be more accurate than
one learned by a single classifier. On the practical
side, boosting has distinguished itself by achieving
good results with small feature sets.
2 Related Work
Although extensive research has been conducted
on statistical anaphora resolution, the bulk of
the work has concentrated on the English lan-
guage. Nevertheless, comparing different strate-
gies helped shape the system described in this pa-
per.
(McCarthy and Lehnert, 1995) were among
the first to use machine learning for coreference
resolution. RESOLVE was trained on data from
MUC-5 English Joint Venture (EJV) corpus and
used the C4.5 decision tree algorithm (Quinlan,
1993) with eight features, most of which were tai-
lored to the joint venturte domain. The system
achieved an F-measure of 86.5 for full coreference
resolution (no values were given for pronouns).
Although a number this high must be attributed to
the specific textual domain, RESOLVE also out-
performed the authors? rule-based algorithm by
7.6 percentage points, which encouraged further
reseach in this direction.
Unlike the other systems presented in this sec-
tion, (Morton, 2000) does not use a decision tree
algorithm but opts instead for a maximum entropy
model. The model is trained on a subset of the
Wall Street Journal, comprising 21 million tokens.
The reported F-measure for pronoun resolution is
81.5. However, (Morton, 2000) only attempts to
resolve singular pronouns, and there is no mention
of what percentage of total pronouns are covered
by this restriction.
(Soon et al, 2001) use the C4.5 algorithm with
a set of 12 domain-independent features, ten syn-
tactic and two semantic. Their system was trained
on both the MUC-6 and the MUC-7 datasets, for
which it achieved F-scores of 62.6 and 60.4, re-
spectively. Although these results are far worse
than the ones reported in (McCarthy and Lehnert,
1995), they are comparable to the best-performing
rule-based systems in the respective competitions.
As (McCarthy and Lehnert, 1995), (Soon et al,
2001) do not report separate results for pronouns.
(Ng and Cardie, 2002) expanded on the work
of (Soon et al, 2001) by adding 41 lexical, se-
mantic and grammatical features. However, since
using this many features proved to be detrimen-
tal to performance, all features that induced low
precision rules were discarded, leaving only 19.
The final system outperformed that of (Soon et al,
2001), with F-scores of 69.1 and 63.4 for MUC-6
and MUC-7, respectively. For pronouns, the re-
ported results are 74.6 and 57.8, respectively.
The experiment presented in (Strube et al,
2002) is one of the few dealing with the applica-
tion of machine learning to German coreference
resolution covering definite noun phrases, proper
names and personal, possessive and demonstrative
pronouns. The research is based on the Heidelberg
Text Corpus (see Section 4), which makes it ideal
for comparison with our system. (Strube et al,
2002) used 15 features modeled after those used
by state-of-the-art resolution systems for English.
The results for personal and possessive pronouns
are 82.79 and 84.94, respectively.
3 Boosting
All of the systems described in the previous sec-
tion use a single classifier to resolve coreference.
Our intuition, however, is that a combination of
classifiers is better suited for this task. The con-
cept of ensemble learning (Dietterich, 2000) is
based on the assumption that combining the hy-
potheses of several classifiers yields a hypothesis
that is much more accurate than that of an individ-
ual classifier.
One of the most popular ensemble learning
methods is boosting (Schapire, 2002). It is based
on the observation that finding many weak hy-
potheses is easier than finding one strong hypothe-
sis. This is achieved by running a base learning al-
gorithm over several iterations. Initially, an impor-
tance weight is distributed uniformly among the
training examples. After each iteration, the weight
is redistributed, so that misclassified examples get
higher weights. The base learner is thus forced to
concentrate on difficult examples.
Although boosting has not yet been applied
to coreference resolution, it has outperformed
stateof-the-art systems for NLP tasks such as part-
ofspeech tagging and prepositional phrase attach-
ment (Abney et al, 1999), word sense disam-
biguation (Escudero et al, 2000), and named en-
tity recognition (Carreras et al, 2002).
The implementation used for this project is
BoosTexter (Schapire and Singer, 2000), a toolkit
freely available for research purposes. In addition
to labels, BoosTexter assigns confidence weights
that reflect the reliability of the decisions.
4 System Description
Our system resolves pronouns in three stages:
preprocessing, classification, and postprocessing.
Figure 1 gives an overview of the system archi-
tecture, while this section provides details of each
component.
4.1 Training and Test Data
The system was trained with data from the Heidel-
berg Text Corpus (HTC), provided by the Euro-
pean Media Laboratory in Heidelberg, Germany.
Figure 1: System Architecture
The HTC is a collection of 250 short texts (30-700
tokens) describing architecture, historical events
and people associated with the city of Heidelberg.
To examine its domain (in)dependence, the system
was tested on 40 unseen HTC texts as well as on
25 articles from the Spiegel magazine, the topics
of which include current events, science, arts and
entertainment, and travel.
4.2 The MMAX Annotation Tool
The manual annotation of the training data was
done with the MMAX (Multi-Modal Annotation
in XML) annotation tool (Mu?ller and Strube,
2001). The fist step of coreference annotation is to
identify the markables, i.e. noun phrases that refer
to real-word entities. Each markable is annotated
with the following attributes:
  np form: proper noun, definite NP, indefinite
NP, personal pronoun, possessive pronoun, or
demonstrative pronoun.
  grammatical role: subject, object (direct or
indirect), or other.
  agreement: this attribute is a combination of
person, number and gender. The possible val-
ues are 1s, 1p, 2s, 2p, 3m, 3f, 3n, 3p.
  semantic class: human, physical object (in-
cludes animals), or abstract. When the se-
mantic class is ambiguous, the ?abstract? op-
tion is chosen.
  type: if the entity that the markable refers to
is new to the discourse, the value is ?none?. If
the markable refers to an already mentioned
entity, the value is ?anaphoric?. An anaphoric
markable has another attribute for its rela-
tion to the antecedent. The values for this at-
tribute are ?direct?, ?pronominal?, and ?ISA?
(hyponym-hyperonym).
To mark coreference, MMAX uses coreference
sets, such that every new reference to an already
mentioned entity is added to the set of that entity.
Implicitly, there is a set for every entity in the dis-
course - if an entity occurs only once, its set con-
tains one markable.
4.3 Feature Vector
The features used by our system are summarised
in Table 4.3. The individual features for anaphor
Feature Description
pron the pronoun
ana npform NP form of the anaphor
ana gramrole grammatical role of the
anaphor
ana agr agreement of the anaphor
ana semclass* semantic class of the anaphor
ante npform NP form of the antecedent
ante gramrole grammatical role of the an-
tecedent
ante agr agreement of the antecedent
ante semclass* semantic class of the an-
tecedent
dist distance in markables
between anaphor and an-
tecedent (1 .. 20)
same agr same agreement of anaphor
and antecedent?
same gramrole same grammatical role of
anaphor and antecedent?
same semclass* same semantic class of
anaphor and antecedent?
Table 1: Features used by our system. *-ed fea-
tures were only used for 10-fold cross-validation
on the manually annotated data
and antecedent - NP form, grammatical role, se-
mantic class - are extracted directly from the an-
notation. The relational features are generated by
comparing the individual ones. The binary tar-
get function - coreferent, non-coreferent - is de-
termined by comparing the values of the member
attribute. If both markables are members of the
same set, they are coreferent, otherwise they are
not.
Due to lack of resources, the semantic class at-
tribute cannot be annotated automatically, and is
therefore used only for comparison with (Strube
et al, 2002).
4.4 Noun Phrase Chunking, NER and
POS-Tagging
To identify markables automatically, the sys-
tem uses the noun phrase chunker described in
(Schmid and Schulte im Walde, 2000), which
displays case information along with the chunks.
The chunker is based on a head-lexicalised prob-
abilistic context free grammar (H-L PCFG) and
achieves an F-measure of 92 for range only and
83 for range and label, whereby a range of a noun
chunk is defined as ?all words from the beginning
of the noun phrase to the head noun?. This is dif-
ferent from manually annotated markables, which
can be complex noun phrases.
Despite good overall performance, the chunker
fails on multi-word proper names in which case it
marks each word as an individual chunk.1 Since
many pronouns refer to named entities, the chun-
ker needs to be supplemented by a named entity
recogniser. Although, to our knowledge, there cur-
rently does not exist an off-the-shelf named entity
recogniser for German, we were able to obtain the
system submitted by (Curran and Clark, 2003) to
the 2003 CoNLL competition. In order to run the
recogniser, the data needs to be tokenised, tagged
and lemmatised, all of which is done by the Tree-
Tagger (Schmid, 1995).
4.5 Markable Creation
After the markables are identified, they are auto-
matically annotated with the attributes described
in Section 4.4. The NP form can be reliably deter-
mined by examining the output of the noun chun-
ker and the named entity recogniser. Pronouns and
named entities are already labeled during chunk-
ing. The remaining markables are labelled as def-
inite NPs if their first words are definite articles
or possessive determiners, and as indefinite NPs
otherwise. Grammatical role is determined by the
case assigned to the markable - subject if nomi-
native, object if accusative. Although datives and
genitives can also be objects, they are more likely
to be adjuncts and are therefore assigned the value
?other?.
For non-pronominal markables, agreement is
determined by lexicon lookup of the head nouns.
Number ambiguities are resolved with the help of
the case information. Most proper names, except
for a few common ones, do not appear in the lexi-
con and have to remain ambiguous. Although it is
impossible to fully resolve the agreement ambigu-
ities of pronominal markables, they can be classi-
1An example is [Verteidigunsminister Donald]
[Rumsfeld] ([Minister of Defense Donald] [Rumsfeld]).
fied as either feminine/plural or masculine/neuter.
Therefore we added two underspecified values to
the agreement attribute: 3f 3p and 3m 3n. Each
of these values was made to agree with both of its
subvalues.
4.6 Antecedent Selection
After classification, one non-pronominal an-
tecedent has to be found for each pronoun. As
BoosTexter assigns confidence weights to its pre-
dictions, we have a choice between selecting the
antecedent closest to the anaphor (closest-first)
and the one with the highest weight (best-first).
Furthermore, we have a choice between ignoring
pronominal antecedents (and risking to discard all
the correct antecedents within the window) and re-
solving them (and risking multiplication of errors).
In case all of the instances within the window have
been classified as non-coreferent, we choose the
negative instance with the lowest weight as the an-
tecedent. The following section presents the re-
sults for each of the selection strategies.
5 Evaluation
Before evaluating the actual system, we compared
the performance of boosting to that of C4.5, as re-
ported in (Strube et al, 2002). Trained on the same
corpus and evaluated with the 10-fold crossvali-
dation method, boosting significantly outperforms
C4.5 on both personal and possessive pronouns
(see Table 2). These results support the intuition
that ensemble methods are superior to single clas-
sifiers.
To put the performance of our system into per-
spective, we established a baseline and an upper
bound for the task. The baseline chooses as the an-
tecedent the closest non-pronominal markable that
agrees in number and gender with the pronoun.
The upper bound is the system?s performance on
the manually annotated (gold standard) data with-
out the semantic features.
For the baseline, accuracy is significantly higher
for the gold standard data than for the two test
sets (see Table 3). This shows that agreement is
the most important feature, which, if annotated
correctly, resolves almost half of the pronouns.
The classification results of the gold standard data,
which are much lower than the ones in Table 2 also
PPER PPOS
(Strube et al, 2002) 82.8 84.9
our system 87.4 86.9
Table 2: Comparison of classification perfor-
mance (F   ) with (Strube et al, 2002)
demonstrate the importance of the semantic fea-
tures. As for the test sets, while the classifier sig-
nificantly outperformed the baseline for the HTC
set, it did nothing for the Spiegel set. This shows
the limitations of an algorithm trained on overly
restricted data.
Among the selection heuristics, the approach of
resolving pronominal antecedents proved consis-
tently more effective than ignoring them, while
the results for the closest-first and best-first strate-
gies were mixed. They imply, however, that the
bestfirst approach should be chosen if the classifier
performed above a certain threshold; otherwise the
closest-first approach is safer.
Overall, the fact that 67.2 of the pronouns were
correctly resolved in the automatically annotated
HTC test set, while the upper bound is 82.0, vali-
dates the approach taken for this system.
6 Conclusion and Future Work
The pronoun resolution system presented in this
paper performs well for unannotated text of a lim-
ited domain. While the results are encouraging
considering the knowledge-poor approach, exper-
iments with a more complex textual domain show
that the system is unsuitable for wide-coverage
tasks such as question answering and summarisa-
tion.
To examine whether the system would yield
comparable results in unrestricted text, it needs to
be trained on a more diverse and possibly larger
corpus. For this purpose, Tu?ba-D/Z, a treebank
consisting of German newswire text, is presently
being annotated with coreference information. As
the syntactic annotation of the treebank is richer
than that of the HTC corpus, additional features
may be derived from it. Experiments with Tu?ba-
D/Z will show whether the performance achieved
for the HTC test set is scalable.
For future versions of the system, it might also
HTC-Gold HTC-Test Spiegel
Baseline accuracy 46.7% 30.9% 31.1%
Classification F   score 77.9 62.8 30.4
Best-first, ignoring pronominal ant. 82.0% 67.2% 28.3%
Best-first, resolving pronominal ant. 72.2% 49.1% 21.7%
Closest-first, ignoring pronominal ant. 82.0% 57.3% 34.4%
Closest-first, resolving pronominal ant. 72.2% 49.1% 22.8%
Table 3: Accuracy of the different selection heuristics compared with baseline accuracy and classification
F-score. HTC-Gold and HTC-Test stand for manually and automatically annotated test sets, respectively.
be beneficial to use full parses instead of chunks.
As most German verbs are morphologically un-
ambiguous, an analysis of them could help disam-
biguate pronouns. However, due to the relatively
free word order of the German language, this ap-
proach requires extensive reseach.
References
Steven Abney, Robert E. Schapire, and Yoram Singer.
1999. Boosting applied to tagging and PP attach-
ment. In Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?.
2002. Named entity extraction using AdaBoost.
In Proceedings of CoNLL-2002, pages 167?170,
Taipei, Taiwan.
James R. Curran and Stephen Clark. 2003. Language-
independent NER using a maximum entropy tagger.
In Proceedings of CoNLL-2003, pages 164?167, Ed-
monton, Canada.
Thomas G. Dietterich. 2000. Ensemble methods in
machine learning. In First International Workshop
on Multiple Classifier Systems, Lecture Notes in
Computer Science, pages 1?15. Springer, New York.
Gerard Escudero, Llu??s Ma`rquez, and German Rigau.
2000. Boosting applied to word sense disambigua-
tion. In Proceedings of the 12th European Confer-
ence on Machine Learning, pages 129?141.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI?95), pages 1050?
1055, Montreal, Canada.
Thomas S. Morton. 2000. Coreference for nlp appli-
cations. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?00), Hong Kong.
Christoph Mu?ller and Michael Strube. 2001. Annotat-
ing anaphoric and bridging relations with MMAX.
In Proceedings of the 2nd SIGdial Workshop on Dis-
course and Dialogue, pages 90?95, Aalborg, Den-
mark.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?02),
pages 104?111, Philadelphia, PA, USA.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufman, San Mateo, CA.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Robert E. Schapire. 2002. The boosting approach to
machine learning: an overview. In Proceedings of
the MSRI Workshop on Nonlinear Estimation and
Classification.
Helmut Schmid and Sabine Schulte im Walde. 2000.
Robust German noun chunking with a probabilis-
tic context-free grammar. In Proceedings of
the 18th International Conference on Computa-
tional Linguistics (COLING-00), pages 726?732,
Saarbru?cken, Germany.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT-Workshop.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Michael Strube, Stefan Rapp, and Christoph Mu?ller.
2002. The influence of minimum edit distance
on reference resolution. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP?02), pages 312?319,
Philadelphia, PA, USA.
A Memory-Based Approach for Semantic Role Labeling
Beata Kouchnir
Department of Computational Linguistics
University of Tu?bingen
Wilhelmstrasse 19, 72074 Tu?bingen, Germany
kouchnir@sfs.uni-tuebingen.de
1 Introduction
This paper presents a system for Semantic Role Label-
ing (SRL) for the CoNLL 2004 shared task (Carreras and
Ma`rquez, 2004). The task is divided into two sub-tasks,
recognition and labeling. These are performed indepen-
dently with different feature representations. Both mod-
ules are based on the principle of memory-based learning.
For the first module, we use the IOB2 format to deter-
mine whether a chunk belongs to an argument or not. Fur-
thermore, we test two different strategies for extracting
arguments from the classifier output. The second module
labels the extracted arguments with one of the 30 seman-
tic roles.
2 Memory-Based Learning
The concept of Memory-Based Learning (MBL) (Lin and
Vitter, 1994) is to classify unseen (test) instances based
on their similarity to known (training) instances. In prac-
tice, this is done by storing all training in memory with-
out abstraction and computing the similarity between new
and old examples based on a distance metric. New in-
stances are then assigned the most frequent class within a
set of k most similar examples (k-nearest neighbors).
Memory-based learning algorithms have proven to be
effective for several NLP tasks, including named entity
recognition (Hendrickx and van den Bosch, 2003), clause
identification (Tjong Kim Sang, 2001) and most rele-
vantly, grammatical relation finding (Buchholz, 2002).
As testing all possible distance metrics in combination
with different values for k is not feasible, we have limited
the experiment to the Overlap and Modified Value Dif-
ference (MVDM) metrics. The values for k tested each
metric were 1, 3, 5, 7, and 9. Even values were omitted
in order to avoid ties.
The Overlap metric computes the distance between
two instances by adding up the differences between the
features. For symbolic features, each mismatch has a
value of 1. MVDM, however, allows different degrees
of similarity by examining co-occurrence of feature val-
ues with target classes. While this concept seems more
suitable for the underlying task, it is only reliable when
used with large amounts of data. For a more detailed de-
scription of the distance metrics, see (Daelemans et al,
2003).
TiMBL1 (Daelemans et al, 2003), the MBL imple-
mentation used in this experiment, is freely available
from the ILK research group at Tilburg University.
3 The Recognition Module
This module identifies the arguments of a proposition,
without assigning a label. For this task we use the IOB2
format, where B marks an element at the beginning of
an argument, I an element inside an argument and O an
element that does not belong to an argument.
As all argument boundaries, except for those within the
target verb chunks, coincide with base chunk boundaries,
the data is processed by words only within the target verb
chunk, and by chunks otherwise.
The recognition module uses the following features:
? Head word and POS of the focus element, where
the head of a multi-word chunk is its last words.
? Chunk type: one of the 12 chunks types, without
the B- or I- prefix.
? Clause information: whether the element is at the
beginning, at the end or inside a clause.
? Directionality: whether the focus element comes
before the target verb, after the target verb, or co-
incides with the target verb.
? Distance: numerical distance (1 .. n) between the
focus element and the target verb.
1http://ilk.kub.nl/software.html
Metric / k B I O
Overlap k=1 87.27 69.34 80.49
MVDM k=1 85.96 74.22 83.83
Overlap k=3 87.91 71.96 82.61
MVDM k=3 87.68 75.35 85.12
Overlap k=5 88.37 73.43 83.47
MVDM k=5 89.21 76.70 86.52
Overlap k=7 88.56 73.41 83.54
MVDM k=7 89.31 77.43 86.83
Overlap k=9 88.69 73.61 84.04
MVDM k=9 89.39 77.38 86.77
Table 1: Results for different distance metrics and values
of k
? Adjacency: whether the focus element is adjacent to
the verb chunk or not, or it is within the verb chunk.
? The target verb and voice: the voice is passive if
the target verb is a past participle preceded by a form
of to be, and active otherwise.
? Context: in addition, the features head word, part
of speech, chunk type and adjacency of the three
chunks each to the left and right of the focus chunk
are used as context information.
Testing each feature separately showed the direction-
ality and adjacency features to be most useful. Omitting
one feature at a time showed to decrease performance for
every omitted feature. Therefore, all of the above features
were used in the final system.
The best TiMBL parameter setting for this task was
determined to be the Modified Value Difference metric
paired with a set of seven nearest neighbors. As we an-
ticipated, the nature of the task requires a more subtle
differentiation than the Overlap metric can provide. Fur-
thermore, the size of the training set is apparently suffi-
cient to take full advantage of MVDM. The results for
both metrics and all values of k are summarized in Table
1. It is interesting to observe the effect of the k value for
each class. Although the results for the I- and O-classes
decrease after k=7, those for the B-class do not. However,
since the overall results are best for k=7, this values was
chosen for the final system.
For all metric/k combination, the results for the I class
are much lower than for the other two. The most com-
mon error is the assignment of the O class to I-elements,
or vice versa. This performance distribution implies that
while the beginning of most arguments is recognized cor-
rectly, their span is not, which results in many ?broken-
up? arguments.
To filter out the actual arguments, we try a strict and
a lenient approach. For the latter, any sequence of ele-
ments that is not labeled as O is considered an argument
(i.e. also those not starting with a B-element). Although
this approach slightly reduces the number of missed ar-
guments, it also vastly overgenerates, which ultimately
decreases performance. The former approach recognizes
as arguments only those sequences beginning with a B-
element. Since B is the class most reliably predicted by
the classifier, this approach yields better overall perfor-
mance.
4 The Labeling Module
This module assigns one of the 30 semantic role labels to
the arguments extracted by the recognition module. Here,
we used only ten features, of which four are ?recycled?
from the previous module:
? Word, POS and chunk sequence: the head words
of all the chunks in the argument, their respective
parts of speech and chunk types. As TiMBL only
allows feature vectors of a fixed length, each of the
sequences represents one value.
? Clause information: as an element sequences can
be a whole clause we added this value to the begin-
ning, end and inside values described in Section 3.
? Length: the length in chunks of the argument.
? Directionality and adjacency: same as in Section
3.
? The target verb and voice: same as in Section 3.
? Prop Bank roleset of the target verb: as an analysis
of the training data showed that about 86% of the
verbs were used in their first sense, and many times,
the rolesets for the first two senses are identical, we
only considered the roleset of first sense.
Just as for the recognition module, the directionality
and adjacency features had the highest information gain.
The POS sequence and length features showed no effect,
and their omission even slightly improved performance.
Therefore, the final system uses only eight features.
To test the performance of this module independently
from the first, it was evaluated on the gold-standard ar-
guments (i.e. recognition score of 100). While MVDM
once again outperforms the Overlap metric, the optimal
value for k in this setting is one. The former supports the
assumption that for feature values such as words, or word
sequences, some values are more similar than others. The
latter suggest that the size of the nearest neighbor set (1
vs. 7) should be somewhat proportional to the length of
the feature vector (8 vs. 45).
The results for each semantic role are summarized in
Table 2. It can be seen that arguments with very restricted
surface patterns (e.g. AM-DIS, AM-MOD, AM-NEG)
Precision Recall F?=1
Overall 75.71% 74.60% 75.15
A0 82.35% 83.41% 82.88
A1 80.69% 82.14% 81.40
A2 61.89% 64.68% 63.25
A3 36.18% 36.91% 36.54
A4 58.39% 63.95% 61.04
A5 33.33% 50.00% 40.00
AM 0.00% 0.00% 0.00
AM-ADV 41.89% 35.23% 38.27
AM-CAU 16.67% 9.43% 12.05
AM-DIR 40.91% 30.00% 34.62
AM-DIS 84.04% 87.75% 85.85
AM-EXT 48.72% 38.78% 43.18
AM-LOC 55.06% 42.61% 48.04
AM-MNR 55.81% 35.93% 43.72
AM-MOD 89.90% 96.14% 92.92
AM-NEG 95.52% 97.71% 96.60
AM-PNC 55.32% 26.00% 35.37
AM-PRD 25.00% 33.33% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 70.06% 64.43% 67.12
R-A0 82.63% 85.19% 83.89
R-A1 67.90% 74.32% 70.97
R-A2 72.22% 76.47% 74.29
R-A3 0.00% 0.00% 0.00
R-AM-LOC 100.00% 50.00% 66.67
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 44.44% 66.67% 53.33
V 99.84% 99.84% 99.84
Table 2: Results for the labeling module with perfect ar-
gument spans
are fairly easy to predict. However, it must be noted that
given the correct span, the complex (and most frequently
occurring) arguments A0 and A1 can be also predicted
with very high accuracy. On the down side, the accuracy
for most adjuncts is rather low, even though their surface
patterns are thought to be somewhat restricted (e.g. AM-
LOC, AM-TMP, AM-MNR, AM-EXT).
5 Evaluation
Tables 3 and 4 show the final results for the develop-
ment and test set, respectively. Although each module
performs fairly well separately, their combined results are
suboptimal. This is probably due to the fact that the label-
ing module is trained with gold standard arguments, and
is not able to deal with noise induced by the recognition
module. The argument type whose results suffer the most
is A1, because it usually spans over several chunks, and is
difficult to retrieve correctly by the recognition module.
Improvements to the system could be made on the syn-
Precision Recall F?=1
Overall 44.93% 63.12% 52.50
A0 59.19% 80.31% 68.15
A1 48.03% 63.53% 54.70
A2 24.40% 44.55% 31.53
A3 15.92% 30.87% 21.00
A4 33.06% 55.10% 41.33
A5 25.00% 25.00% 25.00
AM 0.00% 0.00% 0.00
AM-ADV 18.77% 29.55% 22.96
AM-CAU 3.57% 7.55% 4.85
AM-DIR 11.01% 20.00% 14.20
AM-DIS 51.75% 86.76% 64.84
AM-EXT 31.15% 38.78% 34.55
AM-LOC 17.26% 25.22% 20.49
AM-MNR 27.69% 30.84% 29.18
AM-MOD 82.93% 96.14% 89.05
AM-NEG 92.65% 96.18% 94.38
AM-PNC 16.35% 17.00% 16.67
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 31.09% 47.43% 37.56
R-A0 79.21% 87.04% 82.94
R-A1 54.21% 78.38% 64.09
R-A2 68.42% 76.47% 72.22
R-A3 0.00% 0.00% 0.00
R-AM-LOC 44.44% 100.00% 61.54
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 60.00% 100.00% 75.00
V 98.14% 98.26% 98.20
Table 3: Results for the development set
tactic, lexical, as well as semantic levels. Firstly, it is cru-
cial to improve the performance of the recognition mod-
ule on I-elements. This could either be done by using
a head-lexicalized parser, or, on a lower level, by a pre-
processing module that resolves prepositional phrase at-
tachment. Performance for adjuncts such as AM-LOC
or AM-TMP could be improved, by using gazetteers of
trigger words (e.g. Tuesday) or morphemes (e.g. -day).
Furthermore, one could use a semantic database such as
WordNet to cluster words. Last but not least, more advan-
tage could be taken from the information in Prop Bank,
so different representations of the rolesets should be ex-
plored.
References
Sabine Buchholz. 2002. Memory-based grammatical re-
lation finding. Ph.D. thesis, Tilburg University.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
Precision Recall F?=1
Overall 56.86% 49.95% 53.18
A0 68.12% 63.05% 65.49
A1 55.79% 53.22% 54.48
A2 30.95% 30.95% 30.95
A3 21.77% 18.00% 19.71
A4 30.56% 44.00% 36.07
A5 0.00% 0.00% 0.00
AA 0.00% 0.00% 0.00
AM-ADV 23.91% 10.75% 14.83
AM-CAU 0.00% 0.00% 0.00
AM-DIR 28.89% 26.00% 27.37
AM-DIS 53.30% 53.05% 53.18
AM-EXT 15.00% 21.43% 17.65
AM-LOC 21.78% 9.65% 13.37
AM-MNR 45.19% 23.92% 31.28
AM-MOD 91.18% 91.99% 91.58
AM-NEG 90.77% 92.91% 91.83
AM-PNC 26.09% 7.06% 11.11
AM-PRD 0.00% 0.00% 0.00
AM-TMP 47.49% 31.73% 38.04
R-A0 82.61% 71.70% 76.77
R-A1 64.91% 52.86% 58.27
R-A2 50.00% 44.44% 47.06
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 66.67% 14.29% 23.53
V 97.77% 97.82% 97.79
Table 4: Results for the test set
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proceedings of ConNLL-2004.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2003. TiMBL: Tilburg memory
based learner, version 5.0, reference guide. Technical
report, ILK.
Iris Hendrickx and Antal van den Bosch. 2003. Memory-
based one-step named-entity recognition: Effects of
seed list features, classifier stacking, and unannotated
data. In Proceedings of CoNLL-2003, pages 176?179.
Jyh-Han Lin and Jeffrey Scott Vitter. 1994. A theory for
memory-based learning. Machine Learning, 17:1?26.
Erik Tjong Kim Sang. 2001. Memory-based clause iden-
tification. In Proceedings of CoNLL-2001, pages 67?
69.
