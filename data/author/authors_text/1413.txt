Scoring Algorithms for Wordspotting Systems
Robert W. Morris and Jon A. Arrowood and Peter S. Cardillo
Nexidia Inc.
3060 Peachtree Rd Suite 730
Atlanta, Georgia 30305-2240
{rmorris,jarrowood,pcardillo}@nexidia.com
Mark A. Clements
Center for Signal & Image Processing
Georgia Institute of Technology
Atlanta, Georgia 30332-0250
clements@ece.gatech.edu
Abstract
When evaluating wordspotting systems, one
normally compares receiver operating charac-
teristic curves and different measures of accu-
racy. However, there are many other factors
that are relevant to the system?s usability for
searching speech. In this paper, we discuss
both measures of quality for confidence scores
and propose algorithms for producing scores
that are optimal with respect to these criteria.
1 Introduction
In order to evaluate any system, it is useful to have objec-
tive quality measures that can be automatically applied to
systems for comparison. For wordspotting systems, these
measures are oriented towards recall accuracy. Most of
these measures are based on receiver operating character-
istic (ROC) curves and functions of these curves. How-
ever, there are many other factors that are relevant to the
systems usability.
When a user enters a query to the Nexidia wordspot-
ter (Clements et al, 2001), the system returns a sorted re-
sult list that marks the times where the query matches the
audio. In addition, scores are associated with each result.
These scores are related to the likelihood that the tagged
audio matches the query. Although this score gives an
indication of the strength of the match, users have had
difficulty interpreting the scores.
We found that most users want to use the score in one
of two ways. The first application is to provide a score
threshold for monitoring applications. Alternatively, peo-
ple also assume that the score reflects the probability that
the tagged audio segment is actually a match.
However, without any objective quality measure of
these scores, it was difficult to evaluate different score
generation algorithms. In this paper, we discuss both
measures of quality for confidence scores and propose
algorithms for producing scores that are optimal with re-
spect to these criteria.
2 Assumptions
In order to derive a scoring algorithm, a key assumption
must be made by the wordspotting algorithm: each match
must have a numeric score associated with it. In addition,
there must be some theoretical basis for an additive de-
composition of this score. This decomposition is given
by
R(q) =
L
?
l=1
R(q)l , (1)
where R(q) is the score returned by query q, and R(q)l is
the score associated with the lth phoneme in the query.
With this assumption, we also assume that these compo-
nents can be modeled with a Gaussian distribution with
dependence on whether the match is truly a hit or a miss.
The distributions are then given by
R(q)l |Hit ? N
(
?H
(
S(q)l
)
, ?2H
)
(2)
R(q)l |Miss ? N
(
?M
(
S(q)l
)
, ?2M
)
, (3)
where S(q)l is the lth phoneme in query q. In this model,
the means, ? are dependent on the phoneme, but the vari-
ance, ?2, is not. Using the additive model, the raw scores
are distributed by
R(q)|Hit ? N
( L
?
l=1
?H
(
S(q)l
)
, L?2H
)
(4)
R(q)|Miss ? N
( L
?
l=1
?M
(
S(q)l
)
, L?2M
)
. (5)
3 Performance Measures
We propose two scoring evaluation measures. In each of
these methods, the raw score is modified by some scoring
function F (). The first measure evaluates a scoring algo-
rithms usefulness for setting detection thresholds. This
method assumes that the scoring function calculates the
cdf of the missed score distributions. The measurement
is based on the Kolmogorov-Smirnov test statistic, which
is given by
KS = max
i
?
?
?
?
F
(
R(i)M
)
? iN
?
?
?
?
(6)
where R(i)M are the raw scores for the false alarms in de-
scending order.
A metric for measuring scoring algorithms based on
result confidence is given by
B = 1NH
NH
?
n=1
[
1?F
(
R(n)H
)]2
+ 1NM
NM
?
n=1
[
F
(
R(n)M
)]2
,
(7)
where NM and NH are the number of hits and misses.
This value is equal to zero when all hits are scored to one
and all misses are scored as zero. On the other hand, B is
equal to 0.5 if F (R) is set to 0.5 regardless of the input.
4 Algorithms
If one is interested in setting a detection threshold based
on false alarms per hour, then one can set the score using
the cumulative density function of the misses. This yields
the score
FC
(
R(q)
)
= Pr
(
x < R(q)
)
= Q
[
1?
L?M
(
R(q) ?
L
?
l=1
?M
(
S(q)l
)
)]
, (8)
whereQ is the cdf of the unit normal distribution. To set a
threshold for K false alarms per hour, then the threshold
should be set to
? = 1.0? KKT
, (9)
where KT is the range of false alarms per hour that the
miss model is trained.
If one is looking at a list of scores, one might be inter-
ested in the probability that the score was generated by a
true match. By Bayes law, the conditional probability can
be calculated by
FB
(
R(q)
)
= Pr
(
Hit|R(q)
)
= PHp(R
(q)|Hit)
PHp(R(q)|Hit) + (1?PH)p(R(q)|Miss)
,(10)
where PH is the prior probability of a hit.
5 Model Training
Each of the scoring methods described above require
models of how the phonemes relate to the scores through
the parameters: ?M , ?H , ?2M , and ?2H . For this purpose,
a series of hits and misses over the desired range of false
alarms rates must be collected from the wordspotter. With
these scores, it is possible to train the miss and hit mod-
els independently. For this reason, only the miss model
training is described here.
Given the model in Equation 5, the following distribu-
tion holds with N observations:
p(R|S, ?M , ?2M ) =
N
?
n=1
N
(
R(n) ?
L
?
l=1
?M
(
S(n)l
)
, L?2M
)
. (11)
The maximum likelihood solution for ?M and ?2M is a
difficult optimization problem. However, if the phoneme
components R(n)l from Equation 1, the distribution sim-
plifies to observations of the Gaussian components. By
using the Expectation Maximization (EM) algorithm, the
overall likelihood in Equation 11 can be iteratively max-
imized (Dempster et al, 1977).
Similarly, the training problem can also be viewed in a
Bayesian framework, where a Minimum Mean Squared
Error (MMSE) estimate can be calculated. Like the
maximum likelihood estimate, this requires an iterative
method where the components of the score are generated.
This can be computed by a Gibbs sampler (Gamerman,
1997).
In addition to providing a mechanism for creating
meaningful scores, these models can be useful for other
purposes. For example, one can analyze the mean vectors
to determine which phonemes provide better discrimina-
tion for wordspotting. These can also be used to diagnose
problems in performance that are phoneme specific.
6 Results
The experiments for this algorithm were conducted us-
ing the Nexidia wordspotting system trained on broadcast
quality North American English speech. The effect of us-
ing different scoring algorithms was accomplished using
a nine hour subset of the HUB-4 1996 North American
English broadcast corpus. This data was chosen since this
corpus is widely available and is disjoint from the train-
ing data used for the wordspotter. From this corpus, 8500
search terms were randomly selected from the transcripts.
These queries were equally distributed in length from 4
to 20 phonemes, and then split into a testing and training
set. For each search term, results ranging from the top
score down to the 90th false alarm were collected. The
results from the training terms were then used to train the
score models using both the EM algorithm and a Gibbs
sampler.
These trained models were then then used to gener-
ate both FB and FC for all of the test queries. In addi-
tion, the ?Standard? scores were generated. These scores
are what the Nexidia wordspotting product reveals to the
users, and are calculated by scaling the raw scores by
the number of phonemes and mapping these from zero
to one.
The resulting scores from these tests are listed in Ta-
ble 1. As expected, the CFAR based score performed
well on the KS metric, while the Bayesian score was
more accurate on the B measure. Both of these methods
performed much better than the previous ad-hoc ?Stan-
dard? method. However, performance improvements on
one measure resulted in very poor scores on the other.
This is due to the fact that the objective of each mea-
sure is very different. In addition, the estimation scheme
had little effect on the overall scores. Since the EM al-
gorithm requires a small fraction of the computation that
the Gibbs sampler requires, this method is preferable.
Table 1: Comparison of different scoring algorithms
based on two scoring measurements
Algorithm Performance MeasureKS B
Gibbs CFAR 0.312 0.350Bayes 0.790 0.197
EM CFAR 0.322 0.351Bayes 0.789 0.196
Standard 0.633 0.496
To illustrate the differences between the three scoring
algorithms, the hits and misses were also collected and
plotted in Figure 6. In each subplot, there are histograms
of the hits and misses. In all three cases, most of the hits
tend to have scores close to one. However, the misses
in the standard scoring scheme are concentrated from 0.5
to 0.8. When the Bayes scoring method is used, half of
the hits are very close to 1.0, while half of the misses are
very close to 0.0. The other half of the scores are dis-
tributed along the score range. Finally, the misses from
the CFAR scoring algorithm are distributed evenly along
entire range of scores. Because the normal score assump-
tion does not strictly hold, this distribution is not perfectly
flat at the start and the end, but it is fairly close.
7 Conclusions
Several methods for for both generating and evaluating
scores from wordspotting systems have been proposed.
These methods can operate on any system that generates
scores where an additive model based on phonemes is
valid. The scores that are produced by the algorithms
described can be used to both give intuitive confidence
levels, as well as provide a simple mechanisms for setting
thresholds in monitoring environments. These methods
have been shown to provide superior performance when
compared to their relevant metrics.
References
M. A. Clements, P. S. Cardillo, and M. S. Miller. Pho-
netic searching vs. LVCSR: How to find what you re-
ally want in audio archives, in AVIOS 2001.
Dani Gamerman. 1997. Markov Chain Monte Carlo:
Stochastic Simulation for Bayesian Inference, vol-
ume 1. Chapman & Hall, Boca Raton, FL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society Se-
ries B, 39(1):1?38.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
Score
Histogram of hits and misses using standard scoring
Misses
Hits
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
Score
Histogram of hits and misses using Bayes scoring
Misses
Hits
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
Score
Histogram of hits and misses using CFAR scoring
Misses
Hits
Figure 1: Comparison of different scoring methods on Broadcast English queries. Scores are derived from results
ranging from zero to ten false alarms per hour.
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 108?114,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentence-Level Subjectivity Detection Using Neuro-Fuzzy Models 
 
Samir Rustamov Mark A. Clements 
Georgia Institute of Technology Georgia Institute of Technology 
225 North Avenue NW 225 North Avenue NW 
Atlanta, GA 30332, USA Atlanta, GA 30332, USA 
samir.rustamov@gmail.com clements@ece.gatech.edu 
 
 
Abstract 
In this work, we attempt to detect sentence-
level subjectivity by means of two supervised 
machine learning approaches: a Fuzzy Control 
System and Adaptive Neuro-Fuzzy Inference 
System. Even though these methods are popu-
lar in pattern recognition, they have not been 
thoroughly investigated for subjectivity analy-
sis. We present a novel ?Pruned ICF 
Weighting Coefficient,? which improves the 
accuracy for subjectivity detection. Our fea-
ture extraction algorithm calculates a feature 
vector based on the statistical occurrences of 
words in a corpus without any lexical 
knowledge. For this reason, these machine 
learning models can be applied to any lan-
guage; i.e., there is no lexical, grammatical, 
syntactical analysis used in the classification 
process. 
1 Introduction 
There has been a growing interest, in recent years, 
in identifying and extracting subjective infor-
mation from Web documents that contain opinions. 
Opinions are usually subjective expressions that 
describe people's sentiments, appraisals, or feel-
ings. Subjectivity detection seeks to identify 
whether the given text expresses opinions (subjec-
tive) or reports facts (objective) (Lin et al, 2011). 
Automatic subjectivity analysis methods have been 
used in a wide variety of text processing and natu-
ral language  applications. In many natural lan-
guage processing tasks, subjectivity detection has 
been used as a first phase of filtering to generate 
more informative data.  
The goal of our research is to develop learning 
methods to create classifiers that can distinguish 
subjective from objective sentences. In this paper, 
we achieve sentence-level subjectivity classifica-
tion using language independent feature weighting. 
As a test problem, we employed a subjectivity da-
tabase from the "Rotten Tomatoes" movie reviews 
(see http://www.cs.cornell.edu/people/pabo/movie-
review-data). 
We present two supervised machine learning 
approaches in our development of sentence-level 
subjectivity detection: Fuzzy Control System 
(FCS), and Adaptive Neuro-Fuzzy Inference Sys-
tem (ANFIS). Even though these methods are pop-
ular in pattern recognition, they have not been 
thoroughly investigated for subjectivity analysis. 
We present a novel ?Pruned ICF Weighting Coef-
ficient,? which improves the accuracy for subjec-
tivity detection. Our feature extraction algorithm 
calculates a feature vector based on statistical oc-
currences of words in the corpus without any lexi-
cal knowledge. For this reason, the machine 
learning models can be applied to any language; 
i.e., there is no lexical, grammatical, syntactical 
analysis used in the classification process.  
2 Related work 
In recent years, several different supervised and 
unsupervised learning algorithms were investigated 
for defining subjective information in text or 
speech.  
Riloff and Wiebe (2003) presented a bootstrap-
ping method to learn subjectivity classifiers from a 
collection of non-annotated texts.  Wiebe and 
Riloff (2005) used a similar method, but they also 
learned objective expressions apart from subjective 
expressions.  
Pang and Lee (2004) proposed a MinCut based 
algorithm to classify each sentence as being sub-
jective or objective. The goal of this research was 
to remove objective sentences from each review to 
improve document-level sentiment classification 
(82.8% improved to 86.4%).   
108
Grefenstette et al (2004) presented a Web min-
ing method for identifying subjective adjectives.  
Wilson et al (2004) and Kim et al (2005) pre-
sented methods of classifying the strength of opin-
ion being expressed in individual clauses (or 
sentences). 
Riloff et al (2006) defined subsumption rela-
tionships among unigrams, n -grams, and lexico-
syntactic patterns. They found that if a feature is 
subsumed by another, the subsumed feature is not 
needed.  The subsumption hierarchy reduces a fea-
ture set and reduced feature sets can improve clas-
sification performance. 
Raaijmakers et al(2008) investigated the use of 
prosodic features, word n -grams, character n -
grams, and phoneme n -grams for subjectivity 
recognition and polarity classification of dialog 
acts in multiparty conversation. They found that 
for subjectivity recognition, a combination of pro-
sodic, word-level, character-level, and phoneme-
level information yields the best performance and 
for polarity classification, the best performance is 
achieved with a combination of words, characters 
and phonemes.   
Murray and Carenini (2009) proposed to learn 
subjective patterns from both labeled and unla-
beled data using n -gram word sequences with 
varying level of lexical instantiation. They showed 
that learning subjective trigrams with varying in-
stantiation levels from both annotated and raw data 
can improve subjectivity detection and polarity 
labeling for meeting speech and email threads. 
Martineau and Finin (2009) presented Delta 
TFIDF, an intuitive general purpose technique, to 
efficiently weight word scores before 
classification. They compared SVM Difference of 
TFIDFs and SVM Term Count Baseline results for 
subjectivity classification. As a result, they showed 
that SVM based on Delta TFIDF gives high 
accuracy and low variance.  
Barbosa and Feng (2010) classified the subjec-
tivity of tweets (postings on Twitter) based on two 
kind of features: meta-information about the words 
on tweets and characteristics of how tweets are 
written.  
Yulan He (2010) proposed subjLDA for sen-
tence-level subjectivity detection by modifying the 
latent Dirichlet alocation (LDA) model through 
adding an additional layer to model sentence-level 
subjectivity labels. 
Benamara et al (2011) proposed subjectivity 
classification at the segment level for discourse-
based sentiment analysis. They classified each 
segment into four classes, S, OO, O and SN, where 
S segments are segments that contain explicitly 
lexicalized subjective and evaluative expressions, 
OO segments are positive or negative opinion im-
plied in an objective segment, O segments contain 
neither a lexicalized subjective term nor an implied 
opinion, SN segments are subjective, though non-
evaluative, segments that are used to introduce 
opinions.  
Remus (2011) showed that by using readability 
formulae and their combinations as features in 
addition to already well-known subjectivity clues 
leads to significant accuracy improvements in 
sentence-level subjectivity classification. 
Lin et al (2011) presented a hierarchical Bayes-
ian model based on latent Dirichlet alocation, 
called subjLDA, for sentence-level subjectivity 
detection, which automatically identifies whether a 
given sentence expresses opinion or states facts. 
All the aforementioned work focused on English 
data and most of them used an English subjectivity 
dictionary. Recently, there has been some work on 
subjectivity classification of sentences in Japanese 
(Kanayama et al, 2006), Chinese (Zagibalov et al, 
2008; Zhang et al, 2009), Romanian (Banea et al, 
2008; Mihalcea et al, 2007), Urdu (Mukund and 
Srihari, 2010), Arabic (Abdul-Mageed et al, 2011) 
and others based on different machine learning 
algorithms using general and language specific 
features.  
Mihalcea et al, (2007) and Banea et al, (2008) 
investigated methods to automatically generate 
resources for subjectivity analysis for a new target 
language by leveraging the resources and tools 
available for English. Another approach (Banea et 
al., 2010) used a multilingual space with meta clas-
sifiers to build high precision classifiers for subjec-
tivity classification. 
Recently, there has been some work focused on 
finding features that can be applied to any lan-
guage. For example, Mogadala and Varma (2012) 
presented sentence-level subjectivity classification 
using language independent feature weighting and 
performed experiments on 5 different languages 
including English and a South Asian language 
(Hindi).  
109
Rustamov et. al., (2013) applied hybrid Neuro-
Fuzzy and HMMs to document level sentiment 
analysis of movie reviews.  
In the current work, our main goal is to apply 
supervised methods based on language independ-
ent features for classification of subjective and ob-
jective sentences.  
3 Feature Extraction 
Most language independent feature extraction al-
gorithms are based on the presence or occurrence 
statistics within the corpus. We describe such an 
algorithm which is intuitive, computationally effi-
cient, and does not require either additional human 
annotation or lexical knowledge.  
We use a subjectivity dataset 1v.0: 5000 subjec-
tive and 5000 objective processed sentences in 
movie reviews [Pang/Lee ACL 2004].  
As our target does not use lexical knowledge, 
we consider every word as one code word. In our 
algorithm we do not combine verbs in different 
tenses, such as present and past  ("decide" vs "de-
cided") nor nouns as singular or plural ("fact" vs 
"facts"). Instead, we consider them as the different 
code words. 
Below, we describe some of the parameters: 
? N  is the number of classes ( in our problem 
N=2: i.e. subjective and objective classes); 
? M is the number of different words (terms) 
in the corpus; 
? R is the number of observed sequences in 
the training process; 
? ? ?rTrrr roooO ,...,, 21?   are the sentences in the 
training dataset, where 
rT  is the length of r-
th sentence, Rr ,...,2,1? ; 
? 
ji ,?  describes the association between i-th 
term (word) and the j-th class 
? ?NjMi ,...,2,1;,...,1 ?? ;  
? 
jic ,
 is the number of times i-th term oc-
curred in the j-th class; 
? 
?? j jii ct ,
 denotes the occurrence times of 
the i-th term in the corpus; 
? frequency of the i -th term in the j -th class 
i
ji
ji t
cc ,, ?
; 
We present a new weighting coefficient, which 
affects the accuracy of the system, so that instead 
of the number of documents we take the number of 
classes in the well-known IDF (Inverse-Document 
Frequency) formula. Similar to IDF, we call it 
Pruned ICF (Inverse-Class Frequency) 
???
?
???
??
i
i dN
NICF 2log
, 
where i  is a term, idN  is the number of classes 
containing the term i , which qc ji ?, , where  
Nq ?? ?
1 . 
The value of  ?  is found empirically with 
4.1??  being best for the corpus investigated.  
The membership degree of the terms (
ji,? ) for 
appropriate classes can be estimated by experts or 
can be calculated by analytical formulas. Since a 
main goal is to avoid using human annotation or 
lexical knowledge, we calculated the membership 
degree of each term by an analytical formula as 
follows ? ?NjMi ,...,2,1;,...,1 ?? : 
TF:   
?
?
? N
v
vi
ji
ji
c
c
1
,
,
,?
;   (1) 
ICFTF ? :   
?
?
?
?
? N
v
vvi
jji
ji
ICFc
ICFc
1
,
,
,?
;  (2) 
4 Subjectivity detection using Fuzzy Con-
trol System  
We use a statistical approach for estimation of the 
membership function, instead of expert knowledge, 
at the first stage. Then we apply fuzzy operations 
and modify parameters by the back-propagation 
algorithm.  
We now introduce our algorithm ( Rr ,...,2,1? ).  
1. The membership degree of terms (
r ji ,? ) of the 
r -th sentence are calculated from formulas (1)-(2).  
2. Maximum membership degree is found with 
respect to the  classes for every term of the r-th 
sentence  
 
.,...,1
,maxarg
,
,1
,,
Mi
j rviN
r
ji
r
ji
?
?
?
??
?
??
?
   (3) 
3. Means of maxima are calculated for all clas-
ses: 
110
 ? ?
.,...,1
,max:
,
,
1
,
,
Nj
iZ
T
r
vi
N
r
ji
r
j
r
Zk
r
jk
r
j
r
j
?
??
?
??
?
?
??
?
?
?
     (4) 
We use the Center of Gravity Defuzzification 
(CoGD) method for the defuzzification operation.  
Objective and subjective sentences selected ac-
cording to classes are trained by a fuzzy control 
model. The objective function is defined as follows 
(Aida-zade et. al, 2012): 
? ?
NRy
R
r
rN
j
r
j
N
j
j
r
j
d
y
yE
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?? min
2
1
1
2
1
1
?
?
,    (5) 
? ?Nyyyy ,...,, 21? , ? ?Ndr ,...,2,1? desired output. 
The partial derivatives of this function are 
calculated in following form:   
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
? R
r
rN
j
r
j
N
j
j
r
j
N
j
r
j
r
t
t
d
y
y
yE
1
1
1
1
?
?
?
? , N1,2,...,t ? . 
Function (5) is minimized by the conjugate 
gradient method with the defined optimal values of 
*y .  
Rounding of y  shows the index of the classes 
obtained in the result: 
?
?
?
?
?
N
j
j
N
j
jj y
y
1
1
*
?
?
.        (6) 
Acceptance strategy (s): 
? ?
??
? ??????? otherwisereject
iiyifIis sss ,
,, 11, 
where 
si  is the index of the appropriate class, 
? ?NI ,...,2,1? . Here ? ?5.0;01??  is the main quan-
tity, which influences the reliability of the system. 
It is straightforward to check which feature vec-
tor gives the best results for FCS. Table 1. shows 
average accuracy over 10 fold cross validation of 
FCS based on (1)-(2) features in the  non-restricted 
case. Note that these results depend on the classifi-
cation method these results might be different for 
different classifiers. 
 
Features Accuracy (%) 
 TF 89.87 
ICFTF ?  91.3 
Table 1. Results of  FCS based on TF and 
ICFTF ? features. 
 
We also checked FCS based on Delta TFIDF 
features (Martineau and Finin, 2009). As DeltaIDF 
weighting coefficients of both classes are the same, 
application of DeltaIDF weighting does not change 
the accuracy of the FCS. As we see from Table 1., 
the accuracy of the method increases after applica-
tion of Pruned ICF weighting.  
We show results of subjectivity detection by 
FCS with different values of 
1?  based on ICFTF ?  
in Table 2. It can be seen that the rejection per-
centage is 0.01 for 5.01 ?? . In the testing process 
0.01% of the sentences have such words, which 
after pruned ICF weighting, becomes 0 and the 
system rejects such sentences. 
 
 Correct 
(%) 
Rejection 
(%) 
Error 
(%) 
3.01 ??  76.41 20.86 2.73 
4.01 ??  85.11 10.14 4.75 
5.01 ??  91.3 0.01 8.69 
 
Table 2. Average results of 10 folds cross vali-
dation accuracy of FCS based on ICFTF ? feature 
with different value of 
1? . 
5 Subjectivity detection using Adaptive 
Neuro Fuzzy Inference System  
Fig. 1 illustrates the general structure of Adap-
tive Neuro Fuzzy Inference System. In response to 
linguistic statements, the fuzzy interface block 
provides an input vector to a Multilayer Artificial 
Neural Network (MANN) (Fuller, 1995).  
We used statistical estimation of membership 
degree of terms by (2) instead of linguistic state-
ments at the first stage. Then we applied fuzzy op-
erations (3) and (4).  
 
111
 Fig. 1. The structure of ANFIS. 
MANN was applied to the output of the fuzzyfi-
cation operation. The input vector of neural net-
work is taken from the output vector of the 
fuzzyfication operation (fig. 2). Outputs of MANN 
are taken as indexes of classes appropriate to the 
sentences. MANN is trained by the back-
propagation algorithm. 
 
 
 
Fig. 2. The structure of MANN in ANFIS. 
 
We set two boundary conditions for the  ac-
ceptance decision: 
1)
2??ky , 
2) 
3~ ??? pk yy
, 
where y  is the output vector of MANN,  ky and 
py~
 are two successive maximum elements of the 
vector y , i.e. 
iNik yy ??? 1max
, 
iNi yk ??? 1maxarg
, 
iNikkip yy ??????? 1;11 max~
. 
There is shown results of subjectivity detection 
in  movie reviews by ANFIS with different values 
of 
2?  and 3? in Table 3. 
 
 Correct 
(%) 
Rejection 
(%) 
Error 
(%) 
5.0;8.0 32 ????  78.66 18.84 2.5 
5.0;5.0 32 ????  85.77 8.62 5.61 
No restriction 91.66 0.01 8.33 
Table 3. Average results of 10 folds cross valida-
tion accuracy ANFIS based on ICFTF ?  for sub-
jectivity detection in movie reviews. 
 
The accuracy of the ANFIS (91.66%) is higher 
than that of FCS (91.3%) at the cost of additional 
variables being required in the middle layer of the 
neural network.  
 
6 Conclusion  
We have described two different classification sys-
tem structures, FCS, ANFIS, and applied them to 
sentence-level subjectivity detection in a movie 
review data base. We have specifically shown how 
to train and test these methods  for classification of 
sentences as being either objective or subjective. A 
goal of the research was to formulate methods that 
did not depend on linguistic knowledge and there-
fore would be applicable to any language. An im-
portant component of these  methods is the feature 
extraction process. We focused on analysis of  in-
formative features that improve the accuracy of the 
systems with no language-specific constraints. As 
a result,  a novel  "Pruned ICF Weighting Func-
tion" was devised with a parameter specifically 
estimated for the subjectivity data set. 
When comparing the current system with others, 
it is necessary to emphasize that the use of linguis-
tic knowledge does improve accuracy. Since we do 
not use such  knowledge, our results should only 
be compared with other methods having similar 
constraints, such as those which use features based 
on bags of words that are tested on the same data 
set. Examples include studies by  Pang and Lee 
(2004) and Martineau and Finin (2009). Pang and 
Lee report 92% accuracy on sentence-level subjec-
tivity classification using Na?ve Bayes classifiers 
and 90% accuracy using SVMs on the same data 
set. Martineau and Finin (2009) reported 91.26% 
accuracy using SVM Difference of TFIDFs. The 
currently reported results: FCS (91.3%), ANFIS 
(91.7%) are similar. However, our presented meth-
ods have some advantages. Because the function 
(5) is minimized only with respect to 
? ?Nyyyy ,...,, 21?  (in the defined problem N=2), 
FCS is the fastest algorithm among supervised ma-
chine learning methods. At the cost of additional 
variables added within the middle layer of the neu-
ral network, ANFIS is able to improve accuracy a 
112
small amount. It is anticipated that when IF-THEN 
rules and expert knowledge are inserted into 
ANFIS and FCS, accuracy will improve to a level 
commensurate with human judgment.  
References  
Aditya Mogadala. Vasudeva Varma. 2012. Lan-
guage Independent Sentence-Level Subjectivity 
Analysis with Feature Selection. Proceedings of 
the 26th Pacific Asia Conference on Lan-
guage,Information and Computation, pages 
171?180. 
Alina Andreevskaia and Sabine Bergler. 2006. 
Mining wordnet for fuzzy sentiment: Sentiment 
tag extraction from WordNet glosses. In Pro-
ceedings of EACL 2006. 
Bing Liu. Sentiment Analysis and Opinion Mining. 
2012. Synthesis Lectures on Human Language 
Technologies. 
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity 
summarization based on minimum cuts. In Pro-
ceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics (ACL), pp. 
271-278.  
Bo Pang and Lillian Lee. 2008. Opinion Mining 
and Sentiment Analysis. Now Publishers Inc. 
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 
2010. Multilingual subjectivity: are more lan-
guages better. Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics 
(Coling 2010), pp. 28?36. 
Carmen Banea, Rada Mihalcea, Janyce Wiebe and 
Samer Hassan. 2008. Multilingual subjectivity 
analysis using machine translation. Proceedings 
of the Conference on Empirical Methods in Nat-
ural Language Processing, pp. 127?135.  
Chenghua Lin, Yulan He and Richard Everson. 
2011. Sentence Subjectivity Detection with 
Weakly-Supervised Learning. Proceedings of 
the 5th International Joint Conference on Natu-
ral Language Processing, pp. 1153?1161. 
Ellen Riloff and Janyce Wiebe. 2003. Learning 
Extraction Patterns for Subjective Expressions. 
In: Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, pp. 
105?112.  
Ellen Riloff, Siddharth Patwardhan, and Janyce 
Wiebe. Feature subsumption for opinion analy-
sis. 2006. In Proceedings of the Conference on 
Empirical Methods in Natural Language Pro-
cessing (EMNLP-2006). 
Farah Benamara, Baptiste Chardon, Yannick 
Mathieu, and Vladimir Popescu. 2011. Towards 
Context-Based Subjectivity Analysis. In Pro-
ceedings of the 5th International Joint Confer-
ence on Natural Language Processing (IJCNLP-
2011). 
 Gabriel Murray and Giuseppe Carenini. 2009. 
Predicting subjectivity in multimodal conversa-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language 
Processing (EMNLP), pages 1348?1357. 
Gregory Grefenstette, Yan Qu, David A. Evans,  
and James G. Shanahan. 2006. Validating the 
Coverage of Lexical Resources for Affect Anal-
ysis and Automatically Classifying New Words 
along Semantic Axes. In: Proceedings of AAAI 
Spring Symposium on Exploring Attitude and 
Affect in Text: Theories and Applications, pp. 
93?107. 
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. 
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. Proceedings of the 
2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 355?363. 
Janyce Wiebe and Ellen Riloff. 2005. Creating 
subjective and objective sentence classifiers 
from unannotated texts. Computational Linguis-
tics and Intelligent Text Processing, Springer, 
pp. 486?497.  
Justin Martineau, and Tim Finin. 2009. Delta 
TFIDF: An Improved Feature Space for 
Sentiment Analysis. In Proceedings of the 3rd 
AAAI International Conference on Weblogs and 
Social Media. 
Kamil Aida-zade, Samir Rustamov, Elshan Mustafayev, 
and Nigar Aliyeva, 2012. Human-Computer Dia-
logue Understanding Hybrid System. IEEE  Xplore, 
International Symposium on Innovations in Intelli-
gent Systems and Applications. Trabzon, Turkey, pp. 
1-5. 
Luciano Barbosa and Junlan Feng. 2010. Robust 
sentiment detection on twitter from biased and 
noisy data. In Proceedings of the International 
Conference on Computational Linguistics (CO 
LING-2010). 
Muhammad Abdul-Mageed, Mona T. Diab, and 
Mohammed Korayem. 2011. Subjectivity and 
sentiment analysis of modern standard Arabic, 
In Proceedings of the 49th Annual Meeting of 
113
the Association for Computational Linguistics: 
short papers, pages 587?591. 
Rada Mihalcea, Carmen Banea and Janyce Wiebe. 
2007. Learning multilingual subjective language 
via cross-lingual projections. Proceedings of the 
45th Annual Meeting of the Association of Com-
putational Linguistics, pages 976?983. 
Robert Fuller. Neural Fuzzy Systems, 1995. 
Robert Remus. 2011. Improving Sentence-level 
Subjectivity Classification through Readability 
Measurement. NODALIDA-2011 Conference 
Proceedings, pp. 168?174. 
Samir Rustamov, Elshan Mustafayev, Mark 
Clements. 2013. Sentiment Analysis using 
Neuro-Fuzzy and Hidden Markov Models of 
Text. IEEE Southeastcon 2013, Jacksonvilla, 
Florida,USA. 
Smruthi Mukund and Rohini K. Srihari. 2010. A 
vector space model for subjectivity classifica-
tion in Urdu aided by co-training. In Proceed-
ings of Coling 2010: Poster Volume, pages 860?
868. 
Soo-Min Kim and Eduard Hovy. 2005. Automatic 
Detection of Opinion Bearing Words and Sen-
tences. In: Companion Volume to the Proceed-
ings of the International Joint Conference on 
Natural Language Processing, pp. 61?66. 
Stephan Raaijmakers, Khiet Truong, and Theresa 
Wilson. 2008. Multimodal subjectivity analysis 
of multiparty conversation. In Proceedings of 
the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 466?
474. 
Taras Zagibalov and John Carroll. 2008. Unsuper-
vised classification of sentiment and objectivity 
in Chinese text. In Proceedings of International 
Joint Conference on Natural Language Pro-
cessing (IJCNLP-2008), pp. 304?311. 
Theresa Wilson, Janyce Wiebe, Rebecca Hwa. 
2004. Just How Mad Are You? Finding Strong 
and Weak Opinion Clauses. In: Proceedings of 
the National Conference on Artificial Intelli-
gence, pp. 761?769. 
Yulan He. 2010. Bayesian Models for Sentence-
Level Subjectivity Detection. Technical Report 
KMI-10-02,  June 2010. 
Ziqiong Zhang, Qiang Ye, Rob Law, and Yijun Li. 
2009. Automatic Detection of Subjective Sentences 
Based on Chinese Subjective Patterns. Proceedings  
of 20th International Conference, MCDM-2009, 
pp. 29-36.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
114
