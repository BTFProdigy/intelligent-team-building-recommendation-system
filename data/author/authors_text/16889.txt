Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 116?125,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Lattice-Based Minimum Error Rate Training using Weighted Finite-State
Transducers with Tropical Polynomial Weights
Aurelien Waite? Graeme Blackwood?
? Department of Engineering, University of Cambridge, Trumpington Street, CB2 1PZ, U.K.
{aaw35|wjb31}@cam.ac.uk
? IBM T.J. Watson Research, Yorktown Heights, NY-10598
blackwood@us.ibm.com
William Byrne?
Abstract
Minimum Error Rate Training (MERT) is a
method for training the parameters of a log-
linear model. One advantage of this method
of training is that it can use the large num-
ber of hypotheses encoded in a translation lat-
tice as training data. We demonstrate that the
MERT line optimisation can be modelled as
computing the shortest distance in a weighted
finite-state transducer using a tropical polyno-
mial semiring.
1 Introduction
Minimum Error Rate Training (MERT) (Och, 2003)
is an iterative procedure for training a log-linear sta-
tistical machine translation (SMT) model (Och and
Ney, 2002). MERT optimises model parameters
directly against a criterion based on an automated
translation quality metric, such as BLEU (Papineni
et al, 2002). Koehn (2010) provides a full descrip-
tion of the SMT task and MERT.
MERT uses a line optimisation procedure (Press
et al, 2002) to identify a range of points along a line
in parameter space that maximise an objective func-
tion based on the BLEU score. A key property of the
line optimisation is that it can consider a large set of
hypotheses encoded as a weighted directed acyclic
graph (Macherey et al, 2008), which is called a lat-
tice. The line optimisation procedure can also be ap-
plied to a hypergraph representation of the hypothe-
ses (Kumar et al, 2009).
?The work reported in this paper was carried out while the
author was at the University of Cambridge.
It has been noted that line optimisation over a lat-
tice can be implemented as a semiring of sets of lin-
ear functions (Dyer et al, 2010). Sokolov and Yvon
(2011) provide a formal description of such a semir-
ing, which they denote the MERT semiring. The dif-
ference between the various algorithms derives from
the differences in their formulation and implemen-
tation, but not in the objective they attempt to opti-
mise.
Instead of an algebra defined in terms of trans-
formations of sets of linear functions, we propose
an alternative formulation using the tropical polyno-
mial semiring (Speyer and Sturmfels, 2009). This
semiring provides a concise formalism for describ-
ing line optimisation, an intuitive explanation of the
MERT shortest distance, and draws on techniques
in the currently active field of Tropical Geometry
(Richter-Gebert et al, 2005) 1.
We begin with a review of the line optimisation
procedure, lattice-based MERT, and the weighted
finite-state transducer formulation in Section 2. In
Section 3, we introduce our novel formulation
of lattice-based MERT using tropical polynomial
weights. Section 4 compares the performance of our
approach with k-best and lattice-based MERT.
2 Minimum Error Rate Training
Following Och and Ney (2002), we assume that
we are given a tuning set of parallel sentences
{(r1, f1), ..., (rS , fS)}, where rs is the reference
translation of the source sentence fs. We also as-
sume that sets of hypotheses Cs = {es,1, ..., es,K}
1An associated technical report contains an extended discus-
sion of our approach (Waite et al, 2011)
116
are available for each source sentence fs.
Under the log-linear model formulation with fea-
ture functions hM1 and model parameters ?M1 , the
most probable translation in a set Cs is selected as
e?(fs;?M1 ) = argmax
e?Cs
{ M?
m=1
?mhm(e, fs)
}
. (1)
With an error function of the form E(rS1 , eS1 ) =?S
s=1E(rs, es), MERT attempts to find model pa-
rameters to minimise the following objective:
??M1 = argmin
?M1
{ S?
s=1
E(rs, e?(fs;?M1 ))
}
. (2)
Note that for MERT the hypotheses set Cs is
a k-best list of explicitly enumerated hypotheses,
whereas lattice-based MERT uses a larger space.
2.1 Line Optimisation
Although the objective function in Eq. (2) cannot be
solved analytically, the line optimisation procedure
of Och (2003) can be used to find an approxima-
tion of the optimal model parameters. Rather than
evaluating the decision rule in Eq. (1) over all pos-
sible points in parameter space, the line optimisa-
tion considers a subset of points defined by the line
?M1 +?dM1 , where ?M1 corresponds to an initial point
in parameter space and dM1 is the direction along
which to optimise. Eq. (1) can be rewritten as:
e?(fs; ?) = argmax
e?Cs
{
(?M1 + ?dM1 )ThM1 (e, f s)
}
= argmax
e?Cs
{?
m
?mhm(e, f s)
? ?? ?
a(e,fs)
+?
?
m
dmhm(e, f s)
? ?? ?
b(e,fs)
}
= argmax
e?Cs
{a(e, f s) + ?b(e, f s)? ?? ?
`e(?)
} (3)
This decision rule shows that each hypothesis
e ? Cs is associated with a linear function of ?:
`e(?) = a(e, f s) + ?b(e, f s), where a(e, f s) is the
y-intercept and b(e, f s) is the gradient. The opti-
misation problem is further simplified by defining a
subspace over which optimisation is performed. The
subspace is found by considering a form of the func-
tion in Eq. (3) defined with a range of real numbers
(Macherey et al, 2008; Och, 2003):
Env(f) = max
e?C
{a(e, f) + ?b(e, f)? ?? ?
`e(?)
: ? ? R} (4)
?
Env(fs; ?)
`e1
`e2 `e3
`e4
?
E(rs, e?(fs; ?))
e4
e3
e1
?1 ?2
Figure 1: An upper envelope and projected error. Note
that the upper envelope is completely defined by hypothe-
ses e4, e3, and e1, together with the intersection points ?1
and ?2 (after Macherey et al (2008), Fig. 1).
For any value of ? the linear functions `e(?) associ-
ated with Cs take (up to) K values. The function in
Eq. (4) defines the ?upper envelope? of these values
over all ?. The upper envelope has the form of a con-
tinuous piecewise linear function in ?. The piece-
wise linear function can be compactly described by
the linear functions which form line segments and
the values of ? at which they intersect. The example
in the upper part of Figure 1 shows how the upper
envelope associated with a set of four hypotheses
can be represented by three associated linear func-
tions and two values of ?. The first step of line op-
timisation is to compute this compact representation
of the upper envelope.
Macherey et al (2008) use methods from com-
putational geometry to compute the upper envelope.
The SweepLine algorithm (Bentley and Ottmann,
1979) computes the upper envelope from a set of lin-
ear functions with a complexity of O(K log(K)).
Computing the upper envelope reduces the run-
time cost of line optimisation as the error function
need only be evaluated for the subset of hypotheses
in Cs that contribute to the upper envelope. These
errors are projected onto intervals of ?, as shown in
the lower part of Figure 1, so that Eq. (2) can be
readily solved.
2.2 Incorporation of Line Optimisation into
MERT
The previous algorithm finds the upper envelope
along a particular direction in parameter space over
117
a hypothesis set Cs. The line optimisation algorithm
is then embedded within a general optimisation pro-
cedure. A common approach to MERT is to select
the directions using Powell?s method (Press et al,
2002). A line optimisation is performed on each co-
ordinate axis. The axis giving the largest decrease
in error is replaced with a vector between the initial
parameters and the optimised parameters. Powell?s
method halts when there is no decrease in error.
Instead of using Powell?s method, the Downhill
Simplex algorithm (Press et al, 2002) can be used
to explore the criterion in Eq. (2). This is done by
defining a simplex in parameter space. Directions
where the error count decreases can be identified by
considering the change in error count at the points
of the simplex. This has been applied to parameter
searching over k-best lists (Zens et al, 2007).
Both Powell?s method and the Downhill Simplex
algorithms are approaches based on heuristics to se-
lect lines ?M1 + ?dM1 . It is difficult to find theoret-
ically sound reasons why one approach is superior.
Therefore Cer et al (2008) instead choose the di-
rection vectors dM1 at random. They report that this
method can find parameters that are as good as the
parameters produced by more complex algorithms.
2.3 Lattice Line Optimisation
Macherey et al (2008) describe a procedure for con-
ducting line optimisation directly over a word lattice
encoding the hypotheses in Cs. Each lattice edge is
labelled with a word e and has a weight defined by
the vector of word specific feature function values
hM1 (e, f) so that the weight of a path in the lattice
is found by summing over the word specific feature
function values on that path. Given a line through
parameter space, the goal is to extract from a lattice
its upper envelope and the associated hypotheses.
Their algorithm proceeds node by node through
the lattice. Suppose that for a state q the upper enve-
lope is known for all the partial hypotheses on all
paths leading to q. The upper envelope defines a
set of functions {`e?1(?), ..., `e?N (?)} over the partial
hypotheses e?n. Two operations propagate the upper
envelope to other lattice nodes.
We refer to the first operation as the ?extend? op-
eration. Consider a single edge from state q to state
q?. This edge defines a linear function associated
with a single word `e(?). A path following this edge
transforms all the partial hypotheses leading to q by
concatenating the word e. The upper envelope as-
sociated with the edge from q to q? is changed by
adding `e(?) to the set of linear functions. The in-
tersection points are not changed by this operation.
The second operation is a union. Suppose q?
has another incoming edge from a state q?? where
q 6= q??. There are now two upper envelopes rep-
resenting two sets of linear functions. The first up-
per envelope is associated with the paths from the
initial state to state q? via the state q. Similarly the
second upper envelope is associated with paths from
the initial state to state q? via the state q??. The upper
envelope that is associated with all paths from the
initial state to state q? via both q and q?? is the union
of the two sets of linear functions. This union is no
longer a compact representation of the upper enve-
lope as there may be functions which never achieve
a maximum for any value of ?. The SweepLine al-
gorithm (Bentley and Ottmann, 1979) is applied to
the union to discard redundant linear functions and
their associated hypotheses (Macherey et al, 2008).
The union and extend operations are applied to
states in topological order until the final state is
reached. The upper envelope computed at the final
state compactly encodes all the hypotheses that max-
imise Eq. (1) along the line ?M1 + ?dM1 . Macherey?s
theorem (Macherey et al, 2008) states that an upper
bound for the number of linear functions in the up-
per envelope at the final state is equal to the number
of edges in the lattice.
2.4 Line Optimisation using WFSTs
Formally, a weighted finite-state transducer (WFST)
T = (?,?, Q, I, F,E, ?, ?) over a semiring
(K,?,?, 0?, 1?) is defined by an input alphabet ?, an
output alphabet ?, a set of states Q, a set of initial
states I ? Q, a set of final states F ? Q, a set
of weighted transitions E, an initial state weight as-
signment ? : I ? K, and a final state weight assign-
ment ? : F ? K (Mohri et al, 2008). The weighted
transitions of T form the setE ? Q?????K?Q,
where each transition includes a source state fromQ,
input symbol from ?, output symbol from ?, cost
from the weight set K, and target state from Q.
For each state q ? Q, let E[q] denote the set of
edges leaving state q. For each transition e ? E[q],
let p[e] denote its source state, n[e] its target state,
118
and w[e] its weight. Let pi = e1 ? ? ? eK denote a
path in T from state p[e1] to state n[eK ], so that
n[ek?1] = p[ek] for k = 2, . . . ,K. The weight as-
sociated by T to path pi is the generalised product ?
of the weights of the individual transitions:
w[pi] =
K?
k=1
w[ek] = w[e1]? ? ? ? ? w[eK ] (5)
If P(q) denotes the set of all paths in T start-
ing from an initial state in I and ending in state q,
then the shortest distance d[q] is defined as the gen-
eralised sum ? of the weights of all paths leading to
q (Mohri, 2002):
d[q] = ?pi?P(q)w[pi] (6)
For some semirings, such as the tropical semir-
ing, the shortest distance is the weight of the short-
est path. For other semirings, the shortest distance
is associated with multiple paths (Mohri, 2002); for
these semirings there are shortest distances but need
not any be shortest paths. That will be the case in
what follows. However, the shortest distance algo-
rithms rely only on general properties of semirings,
and once the semiring is specified, the general short-
est distance algorithms can be directly employed.
Sokolov and Yvon (2011) define the MERT
semiring based on operations described in the pre-
vious section. The extend operation is used for the
generalised product ?. The union operation fol-
lowed by an application of the SweepLine algorithm
becomes the generalised sum ?. The word lattice
is then transformed for an initial parameter ?M1 and
direction dM1 . The weight of edge is mapped from
a word specific feature function hM1 (e, f) to a word
specific linear function `e(?). The weight of each
path is the generalised product ? of the word spe-
cific feature linear functions. The upper envelope is
the shortest distance of all the paths in the WFST.
3 The Tropical Polynomial Semiring
In this section we introduce the tropical polynomial
semiring (Speyer and Sturmfels, 2009) as a replace-
ment for the MERT semiring (Sokolov and Yvon,
2011). We then provide a full description and a
worked example of our MERT algorithm.
3.1 Tropical Polynomials
A polynomial is a linear combination of a finite
number of non-zero monomials. A monomial con-
sists of a real valued coefficient multiplied by one or
more variables, and these variables may have expo-
nents that are non-negative integers. In this section
we limit ourselves to a description of a polynomial
in a single variable. A polynomial function is de-
fined by evaluating a polynomial:
f(?) = an?n + an?1?n?1 + ? ? ?+ a2?2 + a1?+ a0
A useful property of these polynomials is that they
form a ring2 (Cox et al, 2007) and therefore are can-
didates for use as weights in WFSTs.
Speyer and Sturmfels (2009) apply the defini-
tion of a classical polynomial to the formulation of
a tropical polynomial. The tropical semiring uses
summation for the generalised product ? and a min
operation for the generalised sum ?. In this form,
let ? be a variable that represents an element in the
tropical semiring weight set R ? {??,+?}. We
can write a monomial of ? raised to an integer expo-
nent as
?i = ? ? ? ? ? ? ?? ?? ?
i
where i is a non-negative integer. The monomial
can also have a constant coefficient: a? ?i, a ? R.
We can define a function that evaluates a tropical
monomial for a particular value of ?. For example,
the tropical monomial a? ?i is evaluated as:
f(?) = a? ?i = a+ i?
This shows that a tropical monomial is a linear
function with the coefficient a as its y-intercept and
the integer exponent i as its gradient. A tropical
polynomial is the generalised sum of tropical mono-
mials where the generalised sum is evaluated using
the min operation. For example:
f(?) = (a? ?i)? (b? ?j) = min(a+ i?, b+ j?)
Evaluating tropical polynomials in classical arith-
metic gives the minimum of a finite collection of
linear functions.
Tropical polynomials can also be multiplied by a
monomial to form another tropical polynomial. For
example:
f(?) = [(a? ?i)? (b? ?j)]? (c? ?k)
= [(a+ c)? ?i+k]? [(b+ c)? ?j+k]
= min((a+ c) + (i+ k)?, (b+ c) + (j + k)?)
2A ring is a semiring that includes negation.
119
Our re-formulation of Eq. (4) negates the feature
function weights and replaces the argmax by an
argmin. This allows us to keep the usual formu-
lation of tropical polynomials in terms of the min
operation when converting Eq. (4) to a tropical rep-
resentation. What remains to be addressed is the role
of integer exponents in the tropical polynomial.
3.2 Integer Realisations for Tropical
Monomials
In the previous section we noted that the function
defined by the upper envelope in Eq. (4) is simi-
lar to the function represented by a tropical poly-
nomial. A significant difference is that the formal
definition of a polynomial only allows integer expo-
nents, whereas the gradients in Eq. (4) are real num-
bers. The upper envelope therefore encodes a larger
set of model parameters than a tropical polynomial.
To create an equivalence between the upper enve-
lope and tropical polynomials we can approximate
the linear functions {`e(?) = a(e, f s)+? ? b(e, f s)}
that compose segments of the upper envelope. We
define a?(e, f s) = [a(e, f s) ? 10n]int and b?(e, f s) =
[b(e, f s)?10n]int where [x]int denotes the integer part
of x. The approximation to `e(?) is:
`e(?) ? ?`e(?) =
a?(e, f s)
10n + ? ?
b?(e, f s)
10n (7)
The result of this operation is to approximate
the y-intercept and gradient of `e(?) to n decimal
places. We can now represent the linear function
?`e(?) as the tropical monomial?a?(e, fs)???b?(e,fs).
Note that a?(e, fs) and b?(e, fs) are negated since trop-
ical polynomials define the lower envelope as op-
posed to the upper envelope defined by Eq. (4).
The linear function represented by the tropical
monomial is a scaled version of `e(?), but the up-
per envelope is unchanged (to the accuracy allowed
by n). If for a particular value of ?, `ei(?) > `ej (?),
then ?`ei(?) > ?`ej (?). Similarly, the boundary
points are unchanged: if `ei(?) = `ej (?), then
?`ei(?) = ?`ej (?). Setting n to a very large value re-
moves numerical differences between the upper en-
velope and the tropical polynomial representation,
as shown by the identical results in Table 1.
Using a scaled version of `e(?) as the basis for a
tropical monomial may cause negative exponents to
be created. Following Speyer and Sturmfels (2009),
?
f(?)
0
a? ?i
(a? ?i)? (b? ?j)? (c? ?k)
b? ?j
c? ?k
Figure 2: Redundant terms in a tropical polynomial. In
this case (a??i)?(b??j)?(c??k) = (a??i)?(c??k).
we widen the definition of a tropical polynomial to
allow for these negative exponents.
3.3 Canonical Form of a Tropical Polynomial
We noted in Section 2.1 that linear functions induced
by some hypotheses do not contribute to the upper
envelope and can be discarded. Terms in a tropi-
cal polynomial can have similar behaviour. Figure
2 plots the lines associated with the three terms of
the example polynomial function f(?) = (a??i)?
(b??j)?(c??k). We note that the piecewise linear
function can also be described with the polynomial
f(?) = (a??i)?(c??k). The latter representation
is simpler but equivalent.
Having multiple representations of the same poly-
nomial causes problems when implementing the
shortest distance algorithm defined by Mohri (2002).
This algorithm performs an equality test between
values in the semiring used to weight the WFST. The
behaviour of the equality test is ambiguous when
there are multiple polynomial representations of the
same piecewise linear function. We therefore re-
quire a canonical form of a tropical polynomial so
that a single polynomial represents a single function.
We define the canonical form of a tropical polyno-
mial to be the tropical polynomial that contains only
the monomial terms necessary to describe the piece-
wise linear function it represents.
We remove redundant terms from a tropical poly-
nomial after computing the generalised sum. For a
tropical polynomial of one variable we can take ad-
vantage of the equivalence with Lattice MERT and
compute the canonical form using the SweepLine al-
gorithm (Bentley and Ottmann, 1979). Each term
120
corresponds to a linear function; linear functions
that do not contribute to the upper envelope are dis-
carded. Only monomials which correspond to the
remaining linear functions are kept in the canonical
form. The canonical form of a tropical polynomial
thus corresponds to a unique and minimal represen-
tation of the upper envelope.
3.4 Relationship to the Tropical Semiring
Tropical monomial weights can be transformed into
regular tropical weights by evaluating the tropical
monomial for a specific value of ?. For example, a
tropical polynomial evaluated at ? = 1 corresponds
to the tropical weight:
f(1) = ?a?(e, fs)? 1?b?(e,fs) = ?a?(e, fs)? b?(e, fs)
Each monomial term in the tropical polynomial
shortest distance represents a linear function. The
intersection points of these linear functions define
intervals of ? (as in Fig. 1). This suggests an alter-
nate explanation for what the shortest distance com-
puted using the tropical polynomial semiring rep-
resents. Conceptually, there is a continuum of lat-
tices which have identical edges and vertices but
with varying, real-valued edge weights determined
by values of ? ? R, so that each lattice in the contin-
uum is indexed by ?. The tropical polynomial short-
est distance agrees with the shortest distance through
each lattice in the continuum.
Our alternate explanation is consistent with the
Theorem of Macherey (Section 2.3), as there could
never be more paths than edges in the lattice. There-
fore the upper bound for the number of monomial
terms in the tropical polynomial shortest distance is
the number of edges in the input lattice.
We can use the mapping to the tropical semiring
to compute the error surface. Let us assume we have
n + 1 intervals separated by n interval boundaries.
We use the midpoint of each interval to transform the
lattice of tropical monomial weights into a lattice of
tropical weights. The sequence of words that label
the shortest path through the transformed lattice is
the MAP hypothesis for the interval. The shortest
path can be extracted using the WFST shortest path
algorithm (Mohri and Riley, 2002). As a technical
matter, the midpoints of the first interval [??, ?1)
and last interval [?n,?) are not defined. We there-
fore evaluate the tropical polynomial at ? = ?1 ? 1
and ? = ?n + 1 to find the MAP hypothesis in the
first and last intervals, respectively.
3.5 The TGMERT Algorithm
We now describe an alternative algorithm to Lat-
tice MERT that is formulated using the tropical
polynomial shortest distance in one variable. We
call the algorithm TGMERT, for Tropical Geome-
try MERT. As input to this procedure we use a word
lattice weighted with word specific feature functions
hM1 (e, f), a starting point ?M1 , and a direction dM1 in
parameter space.
1. Convert the word specific feature functions
hM1 (e, f) to a linear function `e(?) using ?M1
and dM1 , as in Eq. (3).
2. Convert `e(?) to ?`e(?) by approximating y-
intercepts and gradients to n decimal places, as
in Eq. (7).
3. Convert ?`e(?) in Eq. (7) to the tropical mono-
mial ?a?(e, fs)? ??b?(e,fs).
4. Compute the WFST shortest distance to the exit
states (Mohri, 2002) with generalised sum ?
and generalised product ? defined by the trop-
ical polynomial semiring. The resulting trop-
ical polynomial represents the upper envelope
of the lattice.
5. Compute the intersection points of the linear
functions corresponding to the monomial terms
of the tropical polynomial shortest distance.
These intersection points define intervals of ?
in which the MAP hypothesis does not change.
6. Using the midpoint of each interval convert the
tropical monomial?a?(e, fs)???b?(e,fs) to a reg-
ular tropical weight. Find the MAP hypothesis
for this interval by extracting the shortest path
using the WFST shortest path algorithm (Mohri
and Riley, 2002).
3.6 TGMERT Worked Example
This section presents a worked example showing
how we can use the TGMERT algorithm to compute
the upper envelope of a lattice. We start with a three
state lattice with a two dimensional feature vector
shown in the upper part of Figure 3.
We want to optimise the parameters along a line
in two-dimensional feature space. Suppose the ini-
tial parameters are ?21 = [0.7, 0.4] and the direction
121
0 1 2
z/[?0.2, 0.7]?
x/[?1.4, 0.3]?
y/[?0.9,?0.8]?
z/[?0.2,?0.6]?
0 1 2
z/?14? ??29
x/86? ?27
y/95? ?67
z/38? ?36
Figure 3: The upper part is a translation lattice with 2-
dimensional log feature vector weights hM1 (e, f) where
M = 2. The lower part is the lattice from the upper part
with weights transformed into tropical monomials.
is d21 = [0.3, 0.5]. Step 1 of the TGMERT algorithm
(Section 3.5) maps each edge weight to a word spe-
cific linear function. For example, the weight of the
edge labelled ?x? between states 0 and 1 is trans-
formed as follows:
`e(?) =
2?
m=1
?mhM1 (e, f)
? ?? ?
a(e,f)
+?
2?
m=1
dmhM1 (e,fs)
? ?? ?
b(e,f)
= 0.7 ? ?1.4 + 0.4 ? 0.3? ?? ?
a(e,f)
+? ? 0.3 ? ?1.4 + 0.5 ? 0.3? ?? ?
b(e,f)
= ?0.86? 0.27?
Step 2 of the TGMERT algorithm converts the
word specific linear functions into tropical mono-
mial weights. Since all y-intercepts and gradients
have a precision of two decimal places, we scale the
linear functions `e(?) by 102 and negate them to cre-
ate tropical monomials (Step 3). The edge labelled
?x? now has the monomial weight of 86? ?27. The
transformed lattice with weights mapped to the trop-
ical polynomial semiring is shown in the lower part
of Figure 3.
We can now compute the shortest distance
(Mohri, 2002) from the transformed example lattice
with tropical monomial weights. There are three
unique paths through the lattice corresponding to
three distinct hypotheses. The weights associated
with these hypotheses are:
?14? ??29 ? 38? ?36 = 24? ?7 z z
86? ?27 ? 38? ?36 = 122? ?63 x z
95? ?67 ? 38? ?36 = 133? ?103 y z
0 1 2
z/-2.4
x/75.2
y/68.2
z/23.6
0 1 2
z/55.6
x/21.2
y/-65.8
z/-48.4
Figure 4: The lattice in the lower part of Figure 3 trans-
formed to regular tropical weights: ? = ?0.4 (top) and
? = ?1.4 (bottom).
The shortest distance from initial to final state is
the generalised sum of the path weights: (24??7)?
(133? ?103). The monomial term 122? ?63 corre-
sponding to ?x z? can be dropped because it is not
part of the canonical form of the polynomial (Sec-
tion 3.3). The shortest distance to the exit state can
be represented as the minimum of two linear func-
tions: min(24 + 7?, 133 + 103?).
We now wish to find the hypotheses that define
the error surface by performing Steps 5 and 6 of the
TGMERT algorithm. These two linear functions de-
fine two intervals of ?. The linear functions intersect
at ? ? ?1.4; at this value of ? the MAP hypothesis
changes. Two lattices with regular tropical weights
are created using ? = ?0.4 and ? = ?2.4. These
are shown in Figure 4. For the lattice shown in the
upper part the value for the edge labelled ?x? is com-
puted as 86??0.427 = 86 + 0.4 ? 27 = 75.2.
When ? = ?0.4 the lattice in the upper part in
Figure 4 shows that the shortest path is associated
with the hypothesis ?z z?, which is the MAP hy-
pothesis for the range ? < 1.4. The lattice in the
lower part of Figure 4 shows that when ? = ?2.4
the shortest path is associated with the hypothesis
?y z?, which is the MAP hypothesis when ? > 1.4.
3.7 TGMERT Implementation
TGMERT is implemented using the OpenFst Toolkit
(Allauzen et al, 2007). A weight class is added
for tropical polynomials which maintains them in
canonical form. The ? and ? operations are im-
plemented for piece-wise linear functions, with the
SweepLine algorithm included as discussed.
122
Iteration Arabic-to-English
MERT LMERT TGMERT
Tune Test Tune Test Tune Test
1 36.2 36.2 36.242.1 40.9 39.7 38.9 39.7 38.9
2 42.0 44.5 44.545.1 43.2 45.8 44.3 45.8 44.3
3 44.545.5 44.1
4 45.645.7 44.0
Iteration Chinese-to-English
MERT LMERT TGMERT
Tune Test Tune Test Tune Test
1 19.5 19.5 19.525.3 16.7 29.3 22.6 29.3 22.6
2 16.4 22.5 22.518.9 23.9 31.4 32.1 31.4 32.1
3 23.6 31.6 31.628.2 29.1 32.2 32.5 32.2 32.5
4 29.2 32.2 32.231.3 31.5 32.2 32.5 32.2 32.5
5 31.331.8 32.1
6 32.132.4 32.3
7 32.432.4 32.3
Table 1: GALE AR?EN and ZH?EN BLEU scores
by MERT iteration. BLEU scores at the initial and final
points of each iteration are shown for the Tune sets.
4 Experiments
We compare feature weight optimisation using k-
best MERT (Och, 2003), lattice MERT (Macherey
et al, 2008), and tropical geometry MERT. We refer
to these as MERT, LMERT, and TGMERT, resp.
We investigate MERT performance in the context
of the Arabic-to-English GALE P4 and Chinese-
to-English GALE P3 evaluations3. For Arabic-to-
English translation, word alignments are generated
over around 9M sentences of GALE P4 parallel text.
Following de Gispert et al (2010b), word align-
ments for Chinese-to-English translation are trained
from a subset of 2M sentences of GALE P3 paral-
lel text. Hierarchical rules are extracted from align-
ments using the constraints described in (Chiang,
2007) with additional count and pattern filters (Igle-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html
sias et al, 2009b). We use a hierarchical phrase-
based decoder (Iglesias et al, 2009a; de Gispert et
al., 2010a) which directly generates word lattices
from recursive translation networks without any in-
termediate hypergraph representation (Iglesias et al,
2011). The LMERT and TGMERT optimisation al-
gorithms are particularly suitable for this realisation
of hiero in that the lattice representation avoids the
need to use the hypergraph formulation of MERT
given by Kumar et al (2009).
MERT optimises the weights of the following fea-
tures: target language model, source-to-target and
target-to-source translation models, word and rule
penalties, number of usages of the glue rule, word
deletion scale factor, source-to-target and target-to-
source lexical models, and three count-based fea-
tures that track the frequency of rules in the parallel
data (Bender et al, 2007). In both Arabic-to-English
and Chinese-to-English experiments all MERT im-
plementations start from a flat feature weight initial-
ization. At each iteration new lattices and k-best lists
are generated from the best parameters at the previ-
ous iteration, and each subsequent iteration includes
100 hypotheses from the previous iteration. For
Arabic-to-English we consider an additional twenty
random starting parameters at every iteration. All
translation scores are reported for the IBM imple-
mentation of BLEU using case-insensitive match-
ing. We report BLEU scores for the Tune set at the
start and end of each iteration.
The results for Arabic-to-English and Chinese-
to-English are shown in Table 1. Both TGMERT
and LMERT converge to a small gain over MERT
in fewer iterations, consistent with previous re-
ports (Macherey et al, 2008).
5 Discussion
We have described a lattice-based line optimisation
algorithm which can be incorporated into MERT
for parameter tuning of SMT systems and systems
based on log-linear models. Our approach recasts
the optimisation procedure used in MERT in terms
of Tropical Geometry; given this formulation imple-
mentation is relatively straightforward using stan-
dard WFST operations and algorithms.
123
References
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A general and efficient
weighted finite-state transducer library. In Proceed-
ings of the Ninth International Conference on Imple-
mentation and Application of Automata, pages 11?23.
O. Bender, E. Matusov, S. Hahn, S. Hasan, S. Khadivi,
and H. Ney. 2007. The RWTH Arabic-to-English spo-
ken language translation system. In Automatic Speech
Recognition Understanding, pages 396 ?401.
J.L. Bentley and T.A. Ottmann. 1979. Algorithms for
reporting and counting geometric intersections. Com-
puters, IEEE Transactions on, C-28(9):643 ?647.
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 26?34.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
David A. Cox, John Little, and Donal O?Shea. 2007.
Ideals, Varieties, and Algorithms: An Introduction to
Computational Algebraic Geometry and Commutative
Algebra, 3/e (Undergraduate Texts in Mathematics).
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010a. Hier-
archical phrase-based translation with weighted finite-
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3):505?533.
Adria` de Gispert, Juan Pino, and William Byrne. 2010b.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 545?554.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12, July.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of HLT: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
380?388.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchical
phrase-based translation representations. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 1373?1383. As-
sociation for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725?734.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing 2002.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. 2008. Speech recognition with weighted finite-
state transducers. Handbook on Speech Processing
and Speech Communication.
Mehryar Mohri. 2002. Semiring frameworks and algo-
rithms for shortest-distance problems. J. Autom. Lang.
Comb., 7(3):321?350.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
K. A. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
W. H. Press, W. T. Vetterling, S. A. Teukolsky, and B. P.
Flannery. 2002. Numerical Recipes in C++: the art
of scientific computing. Cambridge University Press.
J. Richter-Gebert, B. Sturmfels, and T. Theobald. 2005.
First steps in tropical geometry. In Idempotent mathe-
matics and mathematical physics.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the Euro-
pean Association for Machine Translation.
124
David Speyer and Bernd Sturmfels. 2009. Tropical
mathematics. Mathematics Magazine.
Aurelien Waite, Graeme Blackwood, and William Byrne.
2011. Lattice-based minimum error rate training using
weighted finite-state transducers with tropical polyno-
mial weights. Technical report, Department of Engi-
neering, University of Cambridge.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 524?532.
125
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 200?205,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The University of Cambridge Russian-English System at WMT13
Juan Pino Aurelien Waite Tong Xiao
Adria` de Gispert Federico Flego William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, UK
{jmp84,aaw35,tx212,ad465,ff257,wjb31}@eng.cam.ac.uk
Abstract
This paper describes the University of
Cambridge submission to the Eighth
Workshop on Statistical Machine Transla-
tion. We report results for the Russian-
English translation task. We use mul-
tiple segmentations for the Russian in-
put language. We employ the Hadoop
framework to extract rules. The decoder
is HiFST, a hierarchical phrase-based de-
coder implemented using weighted finite-
state transducers. Lattices are rescored
with a higher order language model and
minimum Bayes-risk objective.
1 Introduction
This paper describes the University of Cam-
bridge system submission to the ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion (WMT13). Our translation system is HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder that generates translation lattices directly.
Decoding is guided by a CYK parser based on a
synchronous context-free grammar induced from
automatic word alignments (Chiang, 2007). The
decoder is implemented with Weighted Finite
State Transducers (WFSTs) using standard op-
erations available in the OpenFst libraries (Al-
lauzen et al, 2007). The use of WFSTs allows
fast and efficient exploration of a vast translation
search space, avoiding search errors in decoding.
It also allows better integration with other steps
in our translation pipeline such as 5-gram lan-
guage model (LM) rescoring and lattice minimum
Bayes-risk (LMBR) decoding (Blackwood, 2010).
We participate in the Russian-English transla-
tion shared task in the Russian-English direction.
This is the first time we train and evaluate a sys-
tem on this language pair. This paper describes the
development of the system.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing and Section 3 presents and discusses
results.
2 System Development
2.1 Pre-processing
We use all the Russian-English parallel data avail-
able in the constraint track. We filter out non
Russian-English sentence pairs with the language-
detection library.2 A sentence pair is filtered out if
the language detector detects a different language
with probability more than 0.999995 in either the
source or the target. This discards 78543 sen-
tence pairs. In addition, sentence pairs where the
source sentence has no Russian character, defined
by the Perl regular expression [\x0400-\x04ff],
are discarded. This further discards 19000 sen-
tence pairs.
The Russian side of the parallel corpus is to-
kenised with the Stanford CoreNLP toolkit.3 The
Stanford CoreNLP tokenised text is additionally
segmented with Morfessor (Creutz and Lagus,
2007) and with the TreeTagger (Schmid, 1995).
In the latter case, we replace each token by its
stem followed by its part-of-speech. This of-
fers various segmentations that can be taken ad-
vantage of in hypothesis combination: CoreNLP,
CoreNLP+Morfessor and CoreNLP+TreeTagger.
The English side of the parallel corpus is tokenised
with a standard in-house tokeniser. Both sides of
the parallel corpus are then lowercased, so mixed
case is restored in post-processing.
Corpus statistics after filtering and for various
segmentations are summarised in Table 1.
2http://code.google.com/p/language-detection/
3http://nlp.stanford.edu/software/corenlp.shtml
200
Lang Segmentation # Tokens # Types
RU CoreNLP 47.4M 1.2M
RU Morfessor 50.0M 0.4M
RU TreeTagger 47.4M 1.5M
EN Cambridge 50.4M 0.7M
Table 1: Russian-English parallel corpus statistics
for various segmentations.
2.2 Alignments
Parallel data is aligned using the MTTK toolkit
(Deng and Byrne, 2008). We train a word-
to-phrase HMM model with a maximum phrase
length of 4 in both source-to-target and target-to-
source directions. The final alignments are ob-
tained by taking the union of alignments obtained
in both directions.
2.3 Rule Extraction and Retrieval
A synchronous context-free grammar (Chiang,
2007) is extracted from the alignments. The con-
straints are set as in the original publication with
the following exceptions:
? phrase-based rule maximum number of
source words: 9
? maximum number of source element (termi-
nal or nonterminal): 5
? maximum span for nonterminals: 10
Maximum likelihood estimates for the transla-
tion probabilities are computed using MapReduce.
We use a custom Hadoop-based toolkit which im-
plements method 3 of Dyer et al (2008). Once
computed, the model parameters are stored on disk
in the HFile format (Pino et al, 2012) for fast
querying. Rule extraction and feature computa-
tion takes about 2h30. The HFile format requires
data to be stored in a key-value structure. For the
key, we use shared source side of many rules. The
value is a list of tuples containing the possible tar-
gets for the source key and the associated param-
eters of the full rule. The query set of keys for
the test set is all possible source phrases (includ-
ing nonterminals) found in the test set.
During HFile querying we add other features.
These include IBM Model 1 (Brown et al, 1993)
lexical probabilities. Loading these models in
memory doesn?t fit well with the MapReduce
model so lexical features are computed for each
test set rather than for the entire parallel corpus.
The model parameters are stored in a client-server
based architecture. The client process computes
the probability of the rule by querying the server
process for the Model 1 parameters. The server
process stores the model parameters completely
in memory so that parameters are served quickly.
This architecture allows for many low-memory
client processes across many machines.
2.4 Language Model
We used the KenLM toolkit (Heafield et al, 2013)
to estimate separate 4-gram LMs with Kneser-Ney
smoothing (Kneser and Ney, 1995), for each of the
corpora listed in Tables 2 (self-explanatory abbre-
viations). The component models were then in-
terpolated with the SRILM toolkit (Stolcke, 2002)
to form a single LM for use in first-pass trans-
lation decoding. The interpolation weights were
optimised for perplexity on the news-test2008,
newstest2009 and newssyscomb2009 development
sets. The weights reflect both the size of the com-
ponent models and the genre of the corpus the
component models are trained on, e.g. weights are
larger for larger corpora in the news genre.
Corpus # Tokens
EU + NC + UN + CzEng + Yx 652.5M
Giga + CC + Wiki 654.1M
News Crawl 1594.3M
afp 874.1M
apw 1429.3M
cna + wpb 66.4M
ltw 326.5M
nyt 1744.3M
xin 425.3M
Total 7766.9M
Table 2: Statistics for English monolingual cor-
pora.
2.5 Decoding
For translation, we use the HiFST decoder (Igle-
sias et al, 2009). HiFST is a hierarchical decoder
that builds target word lattices guided by a prob-
abilistic synchronous context-free grammar. As-
suming N to be the set of non-terminals and T the
set of terminals or words, then we can define the
grammar as a set R = {R} of rules R : N ?
??,?? / p, where N ? N, ?, ? ? {N ?T}+ and p
the rule score.
201
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N, x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N, x, y) of the CYK grid, we build a
target language word lattice L(N, x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
We are using shallow-1 hierarchical gram-
mars (de Gispert et al, 2010) in our experiments.
This model is constrained enough that the decoder
can build exact search spaces, i.e. there is no prun-
ing in search that may lead to spurious undergen-
eration errors.
2.6 Features and Parameter Optimisation
We use the following standard features:
? language model
? source-to-target and target-to-source transla-
tion scores
? source-to-target and target-to-source lexical
scores
? target word count
? rule count
? glue rule count
? deletion rule count (each source unigram, ex-
cept for OOVs, is allowed to be deleted)
? binary feature indicating whether a rule is ex-
tracted once, twice or more than twice (Ben-
der et al, 2007)
No alignment information is used when com-
puting lexical scores as done in Equation (4) in
(Koehn et al, 2005). Instead, the source-to-target
lexical score is computed in Equation 1:
s(ru, en) = 1(E + 1)R
R?
r=1
E?
e=0
pM1(ene|rur)
(1)
where ru are the terminals in the Russian side of
a rule, en are the terminals in the English side of
a rule, including the null word, R is the number
of Russian terminals, E is the number of English
terminals and pM1 is the IBM Model 1 probability.
In addition to these standard features, we also
use provenance features (Chiang et al, 2011). The
parallel data is divided into four subcorpora: the
Common Crawl (CC) corpus, the News Commen-
tary (NC) corpus, the Yandex (Yx) corpus and the
Wiki Headlines (Wiki) corpus. For each of these
subcorpora, source-to-target and target-to-source
translation and lexical scores are computed. This
requires computing IBM Model 1 for each sub-
corpus. In total, there are 28 features, 12 standard
features and 16 provenance features.
When retrieving relevant rules for a particular
test set, various thresholds are applied, such as
number of targets per source or translation prob-
ability cutoffs. Thresholds involving source-to-
target translation scores are applied separately for
each provenance and the union of all surviving
rules for each provenance is kept. This strategy
gives slight gains over using thresholds only for
the general translation table.
We use an implementation of lattice minimum
error rate training (Macherey et al, 2008) to op-
timise under the BLEU score (Papineni et al,
2001) the feature weights with respect to the odd
sentences of the newstest2012 development set
(newstest2012.tune). The weights obtained match
our expectation, for example, the source-to-target
translation feature weight is higher for the NC cor-
pus than for other corpora since we are translating
news.
2.7 Lattice Rescoring
The HiFST decoder is set to directly generate
large translation lattices encoding many alterna-
tive translation hypotheses. These first-pass lat-
tices are rescored with second-pass higher-order
LMs prior to LMBR.
202
2.7.1 5-gram LM Lattice Rescoring
We build a sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over the data described in section 2.4. Lat-
tices obtained by first-pass decoding are rescored
with this 5-gram LM (Blackwood, 2010).
2.7.2 LMBR Decoding
Minimum Bayes-risk decoding (Kumar and
Byrne, 2004) over the full evidence space of the 5-
gram rescored lattices is applied to select the trans-
lation hypothesis that maximises the conditional
expected gain under the linearised sentence-level
BLEU score (Tromble et al, 2008; Blackwood,
2010). The unigram precision p and average re-
call ratio r are set as described in Tromble et al
(2008) using the newstest2012.tune development
set.
2.8 Hypothesis Combination
LMBR decoding (Tromble et al, 2008) can also be
used as an effective framework for multiple lattice
combination (Blackwood, 2010). We used LMBR
to combine translation lattices produced by sys-
tems trained on alternative segmentations.
2.9 Post-processing
Training data is lowercased, so we apply true-
casing as post-processing. We used the disam-
big tool provided by the SRILM toolkit (Stolcke,
2002). The word mapping model which contains
the probability of mapping a lower-cased word
to its mixed-cased form is trained on all avail-
able data. A Kneser-Ney smoothed 4-gram lan-
guage model is also trained on the following cor-
pora: NC, News Crawl, Wiki, afp, apw, cna, ltw,
nyt, wpb, xin, giga. In addition, several rules are
manually designed to improve upon the output of
the disambig tool. First, casing information from
pass-through translation rules (for OOV source
words) is used to modify the casing of the output.
For example, this allows us to get the correct cas-
ing for the word Bundesrechnungshof. Other rules
are post-editing rules which force some words
to their upper-case forms, such as euro ? Euro.
Post-editing rules are developed based on high-
frequency errors on the newstest2012.tune devel-
opment set. These rules give an improvement of
0.2 mixed-cased NIST BLEU on the development
set.
Finally, the output is detokenised before sub-
mission and Cyrillic characters are transliterated.
We assume for human judgment purposes that it
is better to have a non English word in Latin al-
phabet than in Cyrillic (e.g. uprazdnyayushchie);
sometimes, transliteration can also give a correct
output (e.g. Movember), especially in the case of
proper nouns.
3 Results and Discussion
Results are reported in Table 3. We use the inter-
nationalisation switch for the NIST BLEU scor-
ing script in order to properly lowercase the hy-
pothesis and the reference. This introduces a
slight discrepancy with official results going into
the English language. The newstest2012.test de-
velopment set consists of even sentences from
newstest2012. We observe that the CoreNLP
system (A) outperforms the other two systems.
The CoreNLP+Morfessor system (B) has a much
smaller vocabulary but the model size is compa-
rable to the system A?s model size. Translation
did not benefit from source side morphological de-
composition. We also observe that the gain from
LMBR hypothesis combination (A+B+C) is mini-
mal. Unlike other language pairs, such as Arabic-
English (de Gispert et al, 2009), we have not yet
found any great advantage in multiple morpho-
logical decomposition or preprocessing analyses
of the source text. 5-gram and LMBR rescoring
give consistent improvements. 5-gram rescoring
improvements are very modest, probably because
the first pass 4-gram model is trained on the same
data. As noted, hypothesis combination using the
various segmentations gives consistent but modest
gains over each individual system.
Two systems were submitted to the evalua-
tion. System A+B+C achieved a mixed-cased
NIST BLEU score of 24.6, which was the top
score achieved under this measure. System A sys-
tem achieved a mixed-cased NIST BLEU score of
24.5, which was the second highest score.
4 Summary
We have successfully trained a Russian-English
system for the first time. Lessons learned include
that simple tokenisation is enough to process the
Russian side, very modest gains come from com-
bining alternative segmentations (it could also be
that the Morfessor segmentation should not be per-
formed after CoreNLP but directly on untokenised
data), and reordering between Russian and En-
glish is such that a shallow-1 grammar performs
203
Configuration newstest2012.tune newstest2012.test newstest2013
CoreNLP(A) 33.65 32.36 25.55
+5g 33.67 32.58 25.63
+5g+LMBR 33.98 32.89 25.89
CoreNLP+Morfessor(B) 33.21 31.91 25.33
+5g 33.28 32.12 25.44
+5g+LMBR 33.58 32.43 25.78
CoreNLP+TreeTagger(C) 32.92 31.54 24.78
+5g 32.94 31.85 24.97
+5g+LMBR 33.12 32.12 25.05
A+B+C 34.32 33.13 26.00
Table 3: Translation results, shown in lowercase NIST BLEU. Bold results correspond to submitted
systems.
competitively.
Future work could include exploring alterna-
tive grammars, applying a 5-gram Kneser-Ney
smoothed language model directly in first-pass de-
coding, and combining alternative segmentations
that are more diverse from each other.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762. Tong Xiao was sup-
ported in part by the National Natural Science
Foundation of China (Grant 61073140 and Grant
61272376) and the China Postdoctoral Science
Foundation (Grant 2013M530131).
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood. 2010. Lattice rescoring meth-
ods for statistical machine translation. Ph.D. thesis,
Cambridge University Engineering Department and
Clare College.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang, Steve DeNeefe, and Michael Pust.
2011. Two easy improvements to lexical weighting.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 455?460, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars. In
Computational Linguistics.
Yonggang Deng and William Byrne. 2008. Hmm word
and phrase alignment for statistical machine trans-
lation. IEEE Transactions on Audio, Speech, and
Language Processing, 16(3):494?507.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy
Lin. 2008. Fast, easy, and cheap: Construc-
tion of statistical machine translation models with
204
MapReduce. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 199?207,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 iwslt speech translation evaluation. In
International Workshop on Spoken Language Trans-
lation, volume 8.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
725?734, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Juan Pino, Aurelien Waite, and William Byrne. 2012.
Simple and efficient model filtering in statistical ma-
chine translation. The Prague Bulletin of Mathemat-
ical Linguistics, 98(1):5?24.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
205
