Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 244?252,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Non-negative Matrix Tri-factorization Approach to
Sentiment Classification with Lexical Prior Knowledge
Tao Li Yi Zhang
School of Computer Science
Florida International University
{taoli,yzhan004}@cs.fiu.edu
Vikas Sindhwani
Mathematical Sciences
IBM T.J. Watson Research Center
vsindhw@us.ibm.com
Abstract
Sentiment classification refers to the task
of automatically identifying whether a
given piece of text expresses positive or
negative opinion towards a subject at hand.
The proliferation of user-generated web
content such as blogs, discussion forums
and online review sites has made it possi-
ble to perform large-scale mining of pub-
lic opinion. Sentiment modeling is thus
becoming a critical component of market
intelligence and social media technologies
that aim to tap into the collective wis-
dom of crowds. In this paper, we consider
the problem of learning high-quality senti-
ment models with minimal manual super-
vision. We propose a novel approach to
learn from lexical prior knowledge in the
form of domain-independent sentiment-
laden terms, in conjunction with domain-
dependent unlabeled data and a few la-
beled documents. Our model is based on a
constrained non-negative tri-factorization
of the term-document matrix which can
be implemented using simple update rules.
Extensive experimental studies demon-
strate the effectiveness of our approach on
a variety of real-world sentiment predic-
tion tasks.
1 Introduction
Web 2.0 platforms such as blogs, discussion fo-
rums and other such social media have now given
a public voice to every consumer. Recent sur-
veys have estimated that a massive number of in-
ternet users turn to such forums to collect rec-
ommendations for products and services, guid-
ing their own choices and decisions by the opin-
ions that other consumers have publically ex-
pressed. Gleaning insights by monitoring and an-
alyzing large amounts of such user-generated data
is thus becoming a key competitive differentia-
tor for many companies. While tracking brand
perceptions in traditional media is hardly a new
challenge, handling the unprecedented scale of
unstructured user-generated web content requires
new methodologies. These methodologies are
likely to be rooted in natural language processing
and machine learning techniques.
Automatically classifying the sentiment ex-
pressed in a blog around selected topics of interest
is a canonical machine learning task in this dis-
cussion. A standard approach would be to manu-
ally label documents with their sentiment orienta-
tion and then apply off-the-shelf text classification
techniques. However, sentiment is often conveyed
with subtle linguistic mechanisms such as the use
of sarcasm and highly domain-specific contextual
cues. This makes manual annotation of sentiment
time consuming and error-prone, presenting a bot-
tleneck in learning high quality models. Moreover,
products and services of current focus, and asso-
ciated community of bloggers with their idiosyn-
cratic expressions, may rapidly evolve over time
causing models to potentially lose performance
and become stale. This motivates the problem of
learning robust sentiment models from minimal
supervision.
In their seminal work, (Pang et al, 2002)
demonstrated that supervised learning signifi-
cantly outperformed a competing body of work
where hand-crafted dictionaries are used to assign
sentiment labels based on relative frequencies of
positive and negative terms. As observed by (Ng et
al., 2006), most semi-automated dictionary-based
approaches yield unsatisfactory lexicons, with ei-
ther high coverage and low precision or vice versa.
However, the treatment of such dictionaries as
forms of prior knowledge that can be incorporated
in machine learning models is a relatively less ex-
plored topic; even lesser so in conjunction with
semi-supervised models that attempt to utilize un-
244
labeled data. This is the focus of the current paper.
Our models are based on a constrained non-
negative tri-factorization of the term-document
matrix, which can be implemented using simple
update rules. Treated as a set of labeled features,
the sentiment lexicon is incorporated as one set of
constraints that enforce domain-independent prior
knowledge. A second set of constraints introduce
domain-specific supervision via a few document
labels. Together these constraints enable learning
from partial supervision along both dimensions of
the term-document matrix, in what may be viewed
more broadly as a framework for incorporating
dual-supervision in matrix factorization models.
We provide empirical comparisons with several
competing methodologies on four, very different
domains ? blogs discussing enterprise software
products, political blogs discussing US presiden-
tial candidates, amazon.com product reviews and
IMDB movie reviews. Results demonstrate the ef-
fectiveness and generality of our approach.
The rest of the paper is organized as follows.
We begin by discussing related work in Section 2.
Section 3 gives a quick background on Non-
negative Matrix Tri-factorization models. In Sec-
tion 4, we present a constrained model and compu-
tational algorithm for incorporating lexical knowl-
edge in sentiment analysis. In Section 5, we en-
hance this model by introducing document labels
as additional constraints. Section 6 presents an
empirical study on four datasets. Finally, Section 7
concludes this paper.
2 Related Work
We point the reader to a recent book (Pang and
Lee, 2008) for an in-depth survey of literature on
sentiment analysis. In this section, we briskly
cover related work to position our contributions
appropriately in the sentiment analysis and ma-
chine learning literature.
Methods focussing on the use and generation of
dictionaries capturing the sentiment of words have
ranged from manual approaches of developing
domain-dependent lexicons (Das and Chen, 2001)
to semi-automated approaches (Hu and Liu, 2004;
Zhuang et al, 2006; Kim and Hovy, 2004), and
even an almost fully automated approach (Turney,
2002). Most semi-automated approaches have met
with limited success (Ng et al, 2006) and super-
vised learning models have tended to outperform
dictionary-based classification schemes (Pang et
al., 2002). A two-tier scheme (Pang and Lee,
2004) where sentences are first classified as sub-
jective versus objective, and then applying the sen-
timent classifier on only the subjective sentences
further improves performance. Results in these
papers also suggest that using more sophisticated
linguistic models, incorporating parts-of-speech
and n-gram language models, do not improve over
the simple unigram bag-of-words representation.
In keeping with these findings, we also adopt a
unigram text model. A subjectivity classification
phase before our models are applied may further
improve the results reported in this paper, but our
focus is on driving the polarity prediction stage
with minimal manual effort.
In this regard, our model brings two inter-
related but distinct themes from machine learning
to bear on this problem: semi-supervised learn-
ing and learning from labeled features. The goal
of the former theme is to learn from few labeled
examples by making use of unlabeled data, while
the goal of the latter theme is to utilize weak
prior knowledge about term-class affinities (e.g.,
the term ?awful? indicates negative sentiment and
therefore may be considered as a negatively la-
beled feature). Empirical results in this paper
demonstrate that simultaneously attempting both
these goals in a single model leads to improve-
ments over models that focus on a single goal.
(Goldberg and Zhu, 2006) adapt semi-supervised
graph-based methods for sentiment analysis but
do not incorporate lexical prior knowledge in the
form of labeled features. Most work in machine
learning literature on utilizing labeled features has
focused on using them to generate weakly labeled
examples that are then used for standard super-
vised learning: (Schapire et al, 2002) propose one
such framework for boosting logistic regression;
(Wu and Srihari, 2004) build a modified SVM
and (Liu et al, 2004) use a combination of clus-
tering and EM based methods to instantiate simi-
lar frameworks. By contrast, we incorporate lex-
ical knowledge directly as constraints on our ma-
trix factorization model. In recent work, Druck et
al. (Druck et al, 2008) constrain the predictions of
a multinomial logistic regression model on unla-
beled instances in a Generalized Expectation for-
mulation for learning from labeled features. Un-
like their approach which uses only unlabeled in-
stances, our method uses both labeled and unla-
beled documents in conjunction with labeled and
245
unlabeled words.
The matrix tri-factorization models explored in
this paper are closely related to the models pro-
posed recently in (Li et al, 2008; Sindhwani et al,
2008). Though, their techniques for proving algo-
rithm convergence and correctness can be readily
adapted for our models, (Li et al, 2008) do not
incorporate dual supervision as we do. On the
other hand, while (Sindhwani et al, 2008) do in-
corporate dual supervision in a non-linear kernel-
based setting, they do not enforce non-negativity
or orthogonality ? aspects of matrix factorization
models that have shown benefits in prior empirical
studies, see e.g., (Ding et al, 2006).
We also note the very recent work of (Sind-
hwani and Melville, 2008) which proposes a dual-
supervision model for semi-supervised sentiment
analysis. In this model, bipartite graph regulariza-
tion is used to diffuse label information along both
sides of the term-document matrix. Conceptually,
their model implements a co-clustering assump-
tion closely related to Singular Value Decomposi-
tion (see also (Dhillon, 2001; Zha et al, 2001) for
more on this perspective) while our model is based
on Non-negative Matrix Factorization. In another
recent paper (Sandler et al, 2008), standard regu-
larization models are constrained using graphs of
word co-occurences. These are very recently pro-
posed competing methodologies, and we have not
been able to address empirical comparisons with
them in this paper.
Finally, recent efforts have also looked at trans-
fer learning mechanisms for sentiment analysis,
e.g., see (Blitzer et al, 2007). While our focus
is on single-domain learning in this paper, we note
that cross-domain variants of our model can also
be orthogonally developed.
3 Background
3.1 Basic Matrix Factorization Model
Our proposed models are based on non-negative
matrix Tri-factorization (Ding et al, 2006). In
these models, an m? n term-document matrix X
is approximated by three factors that specify soft
membership of terms and documents in one of k-
classes:
X ? FSGT . (1)
where F is an m? k non-negative matrix repre-
senting knowledge in the word space, i.e., i-th row
of F represents the posterior probability of word
i belonging to the k classes, G is an n? k non-
negative matrix representing knowledge in docu-
ment space, i.e., the i-th row of G represents the
posterior probability of document i belonging to
the k classes, and S is an k? k nonnegative matrix
providing a condensed view of X .
The matrix factorization model is similar to
the probabilistic latent semantic indexing (PLSI)
model (Hofmann, 1999). In PLSI, X is treated
as the joint distribution between words and doc-
uments by the scaling X ? X? = X/?i j Xi j thus
?i j X?i j = 1). X? is factorized as
X? ?WSDT ,?
k
Wik = 1,?
k
D jk = 1,?
k
Skk = 1.
(2)
where X is the m ? n word-document seman-
tic matrix, X = WSD, W is the word class-
conditional probability, and D is the document
class-conditional probability and S is the class
probability distribution.
PLSI provides a simultaneous solution for the
word and document class conditional distribu-
tion. Our model provides simultaneous solution
for clustering the rows and the columns of X . To
avoid ambiguity, the orthogonality conditions
FT F = I, GT G = I. (3)
can be imposed to enforce each row of F and G
to possess only one nonzero entry. Approximating
the term-document matrix with a tri-factorization
while imposing non-negativity and orthogonal-
ity constraints gives a principled framework for
simultaneously clustering the rows (words) and
columns (documents) of X . In the context of co-
clustering, these models return excellent empiri-
cal performance, see e.g., (Ding et al, 2006). Our
goal now is to bias these models with constraints
incorporating (a) labels of features (coming from
a domain-independent sentiment lexicon), and (b)
labels of documents for the purposes of domain-
specific adaptation. These enhancements are ad-
dressed in Sections 4 and 5 respectively.
4 Incorporating Lexical Knowledge
We used a sentiment lexicon generated by the
IBM India Research Labs that was developed for
other text mining applications (Ramakrishnan et
al., 2003). It contains 2,968 words that have been
human-labeled as expressing positive or negative
sentiment. In total, there are 1,267 positive (e.g.
?great?) and 1,701 negative (e.g., ?bad?) unique
246
terms after stemming. We eliminated terms that
were ambiguous and dependent on context, such
as ?dear? and ?fine?. It should be noted, that this
list was constructed without a specific domain in
mind; which is further motivation for using train-
ing examples and unlabeled data to learn domain
specific connotations.
Lexical knowledge in the form of the polarity
of terms in this lexicon can be introduced in the
matrix factorization model. By partially specify-
ing term polarities via F , the lexicon influences
the sentiment predictions G over documents.
4.1 Representing Knowledge in Word Space
Let F0 represent prior knowledge about sentiment-laden words in the lexicon, i.e., if word i is a
positive word (F0)i1 = 1 while if it is negative
(F0)i2 = 1. Note that one may also use soft sen-timent polarities though our experiments are con-
ducted with hard assignments. This information
is incorporated in the tri-factorization model via a
squared loss term,
min
F,G,S
?X ?FSGT?2 +?Tr[(F?F0)TC1(F?F0)]
(4)
where the notation Tr(A) means trace of the matrix
A. Here, ? > 0 is a parameter which determines
the extent to which we enforce F ? F0, C1 is a m?
m diagonal matrix whose entry (C1)ii = 1 if thecategory of the i-th word is known (i.e., specified
by the i-th row of F0) and (C1)ii = 0 otherwise.The squared loss terms ensure that the solution for
F in the otherwise unsupervised learning problem
be close to the prior knowledge F0. Note that if
C1 = I, then we know the class orientation of allthe words and thus have a full specification of F0,Eq.(4) is then reduced to
min
F,G,S
?X?FSGT?2 +??F?F0?2 (5)
The above model is generic and it allows certain
flexibility. For example, in some cases, our prior
knowledge on F0 is not very accurate and we usesmaller ? so that the final results are not depen-
dent on F0 very much, i.e., the results are mostlyunsupervised learning results. In addition, the in-
troduction of C1 allows us to incorporate partialknowledge on word polarity information.
4.2 Computational Algorithm
The optimization problem in Eq.( 4) can be solved
using the following update rules
G jk? G jk
(XT FS) jk
(GGT XT FS) jk
, (6)
Sik ? Sik
(FT XG)ik
(FT FSGT G)ik
. (7)
Fik? Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
. (8)
The algorithm consists of an iterative procedure
using the above three rules until convergence. We
call this approach Matrix Factorization with Lex-
ical Knowledge (MFLK) and outline the precise
steps in the table below.
Algorithm 1 Matrix Factorization with Lexical
Knowledge (MFLK)
begin
1. Initialization:
Initialize F = F0
G to K-means clustering results,
S = (FT F)?1FT XG(GT G)?1.
2. Iteration:
Update G: fixing F,S, updating G
Update F: fixing S,G, updating F
Update S: fixing F,G, updating S
end
4.3 Algorithm Correctness and Convergence
Updating F,G,S using the rules above leads to an
asymptotic convergence to a local minima. This
can be proved using arguments similar to (Ding
et al, 2006). We outline the proof of correctness
for updating F since the squared loss term that in-
volves F is a new component in our models.
Theorem 1 The above iterative algorithm con-
verges.
Theorem 2 At convergence, the solution satisfies
the Karuch, Kuhn, Tucker optimality condition,
i.e., the algorithm converges correctly to a local
optima.
Theorem 1 can be proved using the standard
auxiliary function approach used in (Lee and Se-
ung, 2001).
Proof of Theorem 2. Following the theory of con-
strained optimization (Nocedal and Wright, 1999),
247
we minimize the following function
L(F)= ||X?FSGT ||2 +?Tr[(F?F0)TC1(F?F0)]
Note that the gradient of L is,
?L
?F =?2XGS
T +2FSGT GST +2?C1(F?F0).
(9)
The KKT complementarity condition for the non-
negativity of Fik gives
[?2XGST +FSGT GST +2?C1(F?F0)]ikFik = 0.(10)
This is the fixed point relation that local minima
for F must satisfy. Given an initial guess of F , the
successive update of F using Eq.(8) will converge
to a local minima. At convergence, we have
Fik = Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
.
which is equivalent to the KKT condition of
Eq.(10). The correctness of updating rules for G in
Eq.(6) and S in Eq.(7) have been proved in (Ding
et al, 2006). u?Note that we do not enforce exact orthogonality
in our updating rules since this often implies softer
class assignments.
5 Semi-Supervised Learning With
Lexical Knowledge
So far our models have made no demands on hu-
man effort, other than unsupervised collection of
the term-document matrix and a one-time effort in
compiling a domain-independent sentiment lexi-
con. We now assume that a few documents are
manually labeled for the purposes of capturing
some domain-specific connotations leading to a
more domain-adapted model. The partial labels
on documents can be described using G0 where
(G0)i1 = 1 if the document expresses positive sen-timent, and (G0)i2 = 1 for negative sentiment. Aswith F0, one can also use soft sentiment labelingfor documents, though our experiments are con-
ducted with hard assignments.
Therefore, the semi-supervised learning with
lexical knowledge can be described as
min
F,G,S
?X?FSGT?2 +?Tr[(F?F0)TC1(F?F0)]+
?Tr[(G?G0)TC2(G?G0)]
Where ? > 0,? > 0 are parameters which deter-
mine the extent to which we enforce F ? F0 and
G ? G0 respectively, C1 and C2 are diagonal ma-trices indicating the entries of F0 and G0 that cor-respond to labeled entities. The squared loss terms
ensure that the solution for F,G, in the otherwise
unsupervised learning problem, be close to the
prior knowledge F0 and G0.
5.1 Computational Algorithm
The optimization problem in Eq.( 4) can be solved
using the following update rules
G jk? G jk
(XT FS+?C2G0) jk
(GGT XT FS+?GGTC2G0) jk (11)
Sik ? Sik
(FT XG)ik
(FT FSGT G)ik
. (12)
Fik? Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
. (13)
Thus the algorithm for semi-supervised learning
with lexical knowledge based on our matrix fac-
torization framework, referred as SSMFLK, con-
sists of an iterative procedure using the above three
rules until convergence. The correctness and con-
vergence of the algorithm can also be proved using
similar arguments as what we outlined earlier for
MFLK in Section 4.3.
A quick word about computational complexity.
The term-document matrix is typically very sparse
with z nm non-zero entries while k is typically
also much smaller than n,m. By using sparse ma-
trix multiplications and avoiding dense intermedi-
ate matrices, the updates can be very efficiently
and easily implemented. In particular, updating
F,S,G each takes O(k2(m + n) + kz) time per it-
eration which scales linearly with the dimensions
and density of the data matrix. Empirically, the
number of iterations before practical convergence
is usually very small (less than 100). Thus, com-
putationally our approach scales to large datasets
even though our experiments are run on relatively
small-sized datasets.
6 Experiments
6.1 Datasets Description
Four different datasets are used in our experi-
ments.
Movies Reviews: This is a popular dataset in
sentiment analysis literature (Pang et al, 2002).
It consists of 1000 positive and 1000 negative
movie reviews drawn from the IMDB archive of
the rec.arts.movies.reviews newsgroups.
248
Lotus blogs: The data set is targeted at detect-
ing sentiment around enterprise software, specif-
ically pertaining to the IBM Lotus brand (Sind-
hwani and Melville, 2008). An unlabeled set
of blog posts was created by randomly sampling
2000 posts from a universe of 14,258 blogs that
discuss issues relevant to Lotus software. In ad-
dition to this unlabeled set, 145 posts were cho-
sen for manual labeling. These posts came from
14 individual blogs, 4 of which are actively post-
ing negative content on the brand, with the rest
tending to write more positive or neutral posts.
The data was collected by downloading the lat-
est posts from each blogger?s RSS feeds, or ac-
cessing the blog?s archives. Manual labeling re-
sulted in 34 positive and 111 negative examples.
Political candidate blogs: For our second blog
domain, we used data gathered from 16,742 polit-
ical blogs, which contain over 500,000 posts. As
with the Lotus dataset, an unlabeled set was cre-
ated by randomly sampling 2000 posts. 107 posts
were chosen for labeling. A post was labeled as
having positive or negative sentiment about a spe-
cific candidate (Barack Obama or Hillary Clinton)
if it explicitly mentioned the candidate in posi-
tive or negative terms. This resulted in 49 posi-
tively and 58 negatively labeled posts. Amazon
Reviews: The dataset contains product reviews
taken from Amazon.com from 4 product types:
Kitchen, Books, DVDs, and Electronics (Blitzer
et al, 2007). The dataset contains about 4000 pos-
itive reviews and 4000 negative reviews and can
be obtained from http://www.cis.upenn.
edu/?mdredze/datasets/sentiment/.
For all datasets, we picked 5000 words with
highest document-frequency to generate the vo-
cabulary. Stopwords were removed and a nor-
malized term-frequency representation was used.
Genuinely unlabeled posts for Political and Lo-
tus were used for semi-supervised learning experi-
ments in section 6.3; they were not used in section
6.2 on the effect of lexical prior knowledge. In the
experiments, we set ?, the parameter determining
the extent to which to enforce the feature labels,
to be 1/2, and ?, the corresponding parameter for
enforcing document labels, to be 1.
6.2 Sentiment Analysis with Lexical
Knowledge
Of course, one can remove all burden on hu-
man effort by simply using unsupervised tech-
niques. Our interest in the first set of experi-
ments is to explore the benefits of incorporating a
sentiment lexicon over unsupervised approaches.
Does a one-time effort in compiling a domain-
independent dictionary and using it for different
sentiment tasks pay off in comparison to simply
using unsupervised methods? In our case, matrix
tri-factorization and other co-clustering methods
form the obvious unsupervised baseline for com-
parison and so we start by comparing our method
(MFLK) with the following methods:
? Four document clustering methods: K-
means, Tri-Factor Nonnegative Ma-
trix Factorization (TNMF) (Ding et al,
2006), Information-Theoretic Co-clustering
(ITCC) (Dhillon et al, 2003), and Euclidean
Co-clustering algorithm (ECC) (Cho et al,
2004). These methods do not make use of
the sentiment lexicon.
? Feature Centroid (FC): This is a simple
dictionary-based baseline method. Recall
that each word can be expressed as a ?bag-
of-documents? vector. In this approach, we
compute the centroids of these vectors, one
corresponding to positive words and another
corresponding to negative words. This yields
a two-dimensional representation for docu-
ments, on which we then perform K-means
clustering.
Performance Comparison Figure 1 shows the
experimental results on four datasets using accu-
racy as the performance measure. The results are
obtained by averaging 20 runs. It can be observed
that our MFLK method can effectively utilize the
lexical knowledge to improve the quality of senti-
ment prediction.
Movies Lotus Political Amazon
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
ECC
ITCC
K?Means
Figure 1: Accuracy results on four datasets
249
Size of Sentiment Lexicon We also investigate
the effects of the size of the sentiment lexicon on
the performance of our model. Figure 2 shows
results with random subsets of the lexicon of in-
creasing size. We observe that generally the per-
formance increases as more and more lexical su-
pervision is provided.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Fraction of sentiment words labeled
Ac
cu
ra
cy
 
 
Movies
Lotus
Political
Amazon
Figure 2: MFLK accuracy as size of sentiment
lexicon (i.e., number of words in the lexicon) in-
creases on the four datasets
Robustness to Vocabulary Size High dimen-
sionality and noise can have profound impact on
the comparative performance of clustering and
semi-supervised learning algorithms. We simu-
late scenarios with different vocabulary sizes by
selecting words based on information gain. It
should, however, be kept in mind that in a tru-
ely unsupervised setting document labels are un-
available and therefore information gain cannot
be practically computed. Figure 3 and Figure 4
show results for Lotus and Amazon datasets re-
spectively and are representative of performance
on other datasets. MLFK tends to retain its po-
sition as the best performing method even at dif-
ferent vocabulary sizes. ITCC performance is also
noteworthy given that it is a completely unsuper-
vised method.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Fraction of Original Vocabulary 
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
K?Means
ITCC
ECC
Figure 3: Accuracy results on Lotus dataset with
increasing vocabulary size
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
Fraction of Original Vocabulary
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
K?Means
ITCC
ECC
Figure 4: Accuracy results on Amazon dataset
with increasing vocabulary size
6.3 Sentiment Analysis with Dual
Supervision
We now assume that together with labeled features
from the sentiment lexicon, we also have access to
a few labeled documents. The natural question is
whether the presence of lexical constraints leads
to better semi-supervised models. In this section,
we compare our method (SSMFLK) with the fol-
lowing three semi-supervised approaches: (1) The
algorithm proposed in (Zhou et al, 2003) which
conducts semi-supervised learning with local and
global consistency (Consistency Method); (2) Zhu
et al?s harmonic Gaussian field method coupled
with the Class Mass Normalization (Harmonic-
CMN) (Zhu et al, 2003); and (3) Green?s function
learning algorithm (Green?s Function) proposed
in (Ding et al, 2007).
We also compare the results of SSMFLK with
those of two supervised classification methods:
Support Vector Machine (SVM) and Naive Bayes.
Both of these methods have been widely used in
sentiment analysis. In particular, the use of SVMs
in (Pang et al, 2002) initially sparked interest
in using machine learning methods for sentiment
classification. Note that none of these competing
methods utilizes lexical knowledge.
The results are presented in Figure 5, Figure 6,
Figure 7, and Figure 8. We note that our SSMFLK
method either outperforms all other methods over
the entire range of number of labeled documents
(Movies, Political), or ultimately outpaces other
methods (Lotus, Amazon) as a few document la-
bels come in.
Learning Domain-Specific Connotations In
our first set of experiments, we incorporated the
sentiment lexicon in our models and learnt the
sentiment orientation of words and documents via
F,G factors respectively. In the second set of
250
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 5: Accuracy results with increasing number
of labeled documents on Movies dataset
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function 
SVM
Naive Bayes
Figure 6: Accuracy results with increasing number
of labeled documents on Lotus dataset
experiments, we additionally introduced labeled
documents for domain-specific adjustments. Be-
tween these experiments, we can now look for
words that switch sentiment polarity. These words
are interesting because their domain-specific con-
notation differs from their lexical orientation. For
amazon reviews, the following words switched
polarity from positive to negative: fan, impor-
tant, learning, cons, fast, feature, happy, memory,
portable, simple, small, work while the following
words switched polarity from negative to positive:
address, finish, lack, mean, budget, rent, throw.
Note that words like fan, memory probably refer
to product or product components (i.e., computer
fan and memory) in the amazon review context
but have a very different connotation say in the
context of movie reviews where they probably re-
fer to movie fanfare and memorable performances.
We were surprised to see happy switch polarity!
Two examples of its negative-sentiment usage are:
I ended up buying a Samsung and I couldn?t be
more happy and BORING, not one single exciting
thing about this book. I was happy when my lunch
break ended so I could go back to work and stop
reading.
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 7: Accuracy results with increasing number
of labeled documents on Political dataset
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 8: Accuracy results with increasing number
of labeled documents on Amazon dataset
7 Conclusion
The primary contribution of this paper is to pro-
pose and benchmark new methodologies for sen-
timent analysis. Non-negative Matrix Factoriza-
tions constitute a rich body of algorithms that have
found applicability in a variety of machine learn-
ing applications: from recommender systems to
document clustering. We have shown how to build
effective sentiment models by appropriately con-
straining the factors using lexical prior knowledge
and document annotations. To more effectively
utilize unlabeled data and induce domain-specific
adaptation of our models, several extensions are
possible: facilitating learning from related do-
mains, incorporating hyperlinks between docu-
ments, incorporating synonyms or co-occurences
between words etc. As a topic of vigorous current
activity, there are several very recently proposed
competing methodologies for sentiment analysis
that we would like to benchmark against. These
are topics for future work.
Acknowledgement: The work of T. Li is par-
tially supported by NSF grants DMS-0844513 and
CCF-0830659. We would also like to thank Prem
Melville and Richard Lawrence for their support.
251
References
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-phies, bollywood, boom-boxes and blenders: Do-main adaptation for sentiment classification. In Pro-
ceedings of ACL, pages 440?447.
H. Cho, I. Dhillon, Y. Guan, and S. Sra. 2004. Mini-
mum sum squared residue co-clustering of gene ex-pression data. In Proceedings of The 4th SIAM Data
Mining Conference, pages 22?24, April.
S. Das and M. Chen. 2001. Yahoo! for amazon:Extracting market sentiment from stock messageboards. In Proceedings of the 8th Asia Pacific Fi-
nance Association (APFA).
I. S. Dhillon, S. Mallela, and D. S. Modha. 2003.Information-theoretical co-clustering. In Proceed-
ings of ACM SIGKDD, pages 89?98.
I. S. Dhillon. 2001. Co-clustering documents andwords using bipartite spectral graph partitioning. In
Proceedings of ACM SIGKDD.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogo-
nal nonnegative matrix tri-factorizations for cluster-ing. In Proceedings of ACM SIGKDD, pages 126?135.
C. Ding, R. Jin, T. Li, and H.D. Simon. 2007. Alearning framework using green?s function and ker-nel regularization with application to recommender
system. In Proceedings of ACM SIGKDD, pages260?269.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-tion criteria. In SIGIR.
A. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization. In
HLT-NAACL 2006: Workshop on Textgraphs.
T. Hofmann. 1999. Probabilistic latent semantic in-dexing. Proceeding of SIGIR, pages 50?57.
M. Hu and B. Liu. 2004. Mining and summarizingcustomer reviews. In KDD, pages 168?177.
S.-M. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceedings of International
Conference on Computational Linguistics.
D.D. Lee and H.S. Seung. 2001. Algorithms for non-negative matrix factorization. In Advances in Neural
Information Processing Systems 13.
T. Li, C. Ding, Y. Zhang, and B. Shao. 2008. Knowl-edge transformation from word space to documentspace. In Proceedings of SIGIR, pages 187?194.
B. Liu, X. Li, W.S. Lee, and P. Yu. 2004. Text classifi-cation by labeling words. In AAAI.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-amining the role of linguistic knowledge sources inthe automatic identification and classification of re-
views. In COLING & ACL.
J. Nocedal and S.J. Wright. 1999. Numerical Opti-
mization. Springer-Verlag.
B. Pang and L. Lee. 2004. A sentimental education:sentiment analysis using subjectivity summarizationbased on minimum cuts. In ACL.
B. Pang and L. Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trendsin Information Retrieval: Vol. 2: No 12, pp
1-135 http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learningtechniques. In EMNLP.
G. Ramakrishnan, A. Jadhav, A. Joshi, S. Chakrabarti,and P. Bhattacharyya. 2003. Question answeringvia bayesian inference on lexical relations. In ACL,pages 1?10.
T. Sandler, J. Blitzer, P. Talukdar, and L. Ungar. 2008.Regularized learning with networks of features. In
NIPS.
R.E. Schapire, M. Rochery, M.G. Rahim, andN. Gupta. 2002. Incorporating prior knowledge into
boosting. In ICML.
V. Sindhwani and P. Melville. 2008. Document-word co-regularization for semi-supervised senti-
ment analysis. In Proceedings of IEEE ICDM.
V. Sindhwani, J. Hu, and A. Mojsilovic. 2008. Regu-larized co-clustering with dual supervision. In Pro-
ceedings of NIPS.
P. Turney. 2002. Thumbs up or thumbs down? Se-mantic orientation applied to unsupervised classifi-
cation of reviews. Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424.
X. Wu and R. Srihari. 2004. Incorporating priorknowledge with weighted margin support vector ma-chines. In KDD.
H. Zha, X. He, C. Ding, M. Gu, and H.D. Simon.2001. Bipartite graph partitioning and data cluster-ing. Proceedings of ACM CIKM.
D. Zhou, O. Bousquet, T.N. Lal, J. Weston, andB. Scholkopf. 2003. Learning with local and globalconsistency. In Proceedings of NIPS.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-supervised learning using gaussian fields and har-monic functions. In Proceedings of ICML.
L. Zhuang, F. Jing, and X. Zhu. 2006. Movie reviewmining and summarization. In CIKM, pages 43?50.
252
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 297?300,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Multi-Document Summarization using Sentence-based Topic Models
Dingding Wang
1
Shenghuo Zhu
2
Tao Li
1
Yihong Gong
2
1. School of Computer Science, Florida International University, Miami, FL, 33199
2. NEC Laboratories America, Cupertino, CA 95014, USA.
{dwang003,taoli}@cs.fiu.edu {zsh,ygong}@sv.nec-labs.com
Abstract
Most of the existing multi-document
summarization methods decompose the
documents into sentences and work
directly in the sentence space using a
term-sentence matrix. However, the
knowledge on the document side, i.e. the
topics embedded in the documents, can
help the context understanding and guide
the sentence selection in the summariza-
tion procedure. In this paper, we propose a
new Bayesian sentence-based topic model
for summarization by making use of both
the term-document and term-sentence
associations. An efficient variational
Bayesian algorithm is derived for model
parameter estimation. Experimental
results on benchmark data sets show the
effectiveness of the proposed model for
the multi-document summarization task.
1 Introduction
With the continuing growth of online text
resources, document summarization has found
wide-ranging applications in information retrieval
and web search. Many multi-document summa-
rization methods have been developed to extract
the most important sentences from the documents.
These methods usually represent the documents
as term-sentence matrices (where each row rep-
resents a sentence and each column represents a
term) or graphs (where each node is a sentence
and each edge represents the pairwise relationship
among corresponding sentences), and ranks the
sentences according to their scores calculated by a
set of predefined features, such as term frequency-
inverse sentence frequency (TF-ISF) (Radev et al,
2004; Lin and Hovy, 2002), sentence or term
position (Yih et al, 2007), and number of key-
words (Yih et al, 2007). Typical existing summa-
rization methods include centroid-based methods
(e.g., MEAD (Radev et al, 2004)), graph-ranking
based methods (e.g., LexPageRank (Erkan and
Radev, 2004)), non-negative matrix factorization
(NMF) based methods (e.g., (Lee and Seung,
2001)), Conditional random field (CRF) based
summarization (Shen et al, 2007), and LSA based
methods (Gong and Liu, 2001).
There are two limitations with most of the exist-
ing multi-document summarization methods: (1)
They work directly in the sentence space and many
methods treat the sentences as independent of each
other. Although few work tries to analyze the
context or sequence information of the sentences,
the document side knowledge, i.e. the topics em-
bedded in the documents are ignored. (2) An-
other limitation is that the sentence scores calcu-
lated from existing methods usually do not have
very clear and rigorous probabilistic interpreta-
tions. Many if not all of the sentence scores
are computed using various heuristics as few re-
search efforts have been reported on using genera-
tive models for document summarization.
In this paper, to address the above issues,
we propose a new Bayesian sentence-based topic
model for multi-document summarization by mak-
ing use of both the term-document and term-
sentence associations. Our proposal explicitly
models the probability distributions of selecting
sentences given topics and provides a principled
way for the summarization task. An efficient vari-
ational Bayesian algorithm is derived for estimat-
ing model parameters.
2 Bayesian Sentence-based Topic Models
(BSTM)
2.1 Model Formulation
The entire document set is denoted by D. For each
document d ? D, we consider its unigram lan-
guage model,
297
p(W
n
1
|?
d
) =
n
?
i=1
p(W
i
|?
d
),
where ?
d
denotes the model parameter for docu-
ment d, W
n
1
denotes the sequence of words {W
i
?
W}
n
i=1
, i.e. the content of the document. W is the
vocabulary. As topic models, we further assume
the unigram model as a mixture of several topic
unigram models,
p(W
i
|?
d
) =
?
T
i
?T
p(W
i
|T
i
)p(T
i
|?
d
),
where T is the set of topics. Here, we assume
that given a topic, generating words is independent
from the document, i.e.
p(W
i
|T
i
, ?
d
) = p(W
i
|T
i
).
Instead of freely choosing topic unigram mod-
els, we further assume that topic unigram models
are mixtures of some existing base unigram mod-
els, i.e.
p(W
i
|T
i
) =
?
s?S
p(W
i
|S
i
= s)p(S
i
= s|T
i
),
where S is the set of base unigram models. Here,
we use sentence language models as the base mod-
els. One benefit of this assumption is that each
topic is represented by meaningful sentences, in-
stead of directly by keywords. Thus we have
p(W
i
|?
d
) =
?
t?T
?
s?S
p(W
i
|S
i
= s)p(S
i
= s|T
i
= t)p(T
i
= t|?
d
).
Here we use parameter U
st
for the probability
of choosing base model s given topic t, p(S
i
=
s|T
i
= t) = U
st
, where
?
s
U
st
= 1. We use
parameters {?
d
} for the probability of choosing
topic t given document d, where
?
t
?
dt
= 1.
We assume that the parameters of base models,
{B
ws
}, are given, i.e. p(W
i
= w|S
i
= s) = B
ws
,
where
?
w
B
ws
= 1. Usually, we obtain B
ws
by
empirical distribution words of sentence s.
2.2 Parameter Estimation
For summarization task, we concern how to de-
scribe each topic with the given sentences. This
can be answered by the parameter of choosing
base model s given topic t, U
st
. Comparing to
parameter U
st
, we concern less about the topic
distribution of each document, i.e. ?
dt
. Thus
we choose Bayesian framework to estimate U
st
by
marginalizing ?
dt
. To do so, we assume a Dirich-
let prior for ?
d?
? Dir(?), where vector ? is a
hyperparameter. Thus the likelihood is
f(U;Y) =
?
d
?
?
i
p(Y
id
|?
d
)pi(?
d
|?)d?
d
= B(?)
?D
?
?
id
[BU?
>
]
Y
id
id
?
?
dk
?
?
k
?1
dk
d?.
(1)
As Eq. (1) is intractable, LDA (Blei et al, 2001)
applies variational Bayesian, which is to maximize
a variational bound of the integrated likelihood.
Here we write the variational bound.
Definition 1 The variational bound is
?
f(U,V;Y) =
?
d
B(?+ ?
d,?
)
B(?)
?
vkwd
(
B
wv
U
vk
?
vk;wd
)
Y
wd
?
vk;wd
(2)
where the domain of V isV = {V ? R
D?K
+
:
?
k
V
dk
=
1}, ?
vk;wd
= B
wv
U
vk
V
dk
/[BUV
>
]
wd
, ?
dk
=
?
wv
Y
wd
?
vk;wd
.
We have the following proposition.
Proposition 1 f(U;Y) ? sup
V?V
?
f(U,V;Y).
Actually the optimum of this variational bound is
the same as that obtained variational Bayesian ap-
proach. Due to the space limit, the proof of the
proposition is omitted.
3 The Iterative Algorithm
The LDA algorithm (Blei et al, 2001) em-
ployed the variational Bayesian paradigm, which
estimates the optimal variation bound for each U.
The algorithm requires an internal Expectation-
Maximization (EM) procedure to find the optimal
variational bound. The nested EM slows down
the optimization procedure. To avoid the internal
EM loop, we can directly optimize the variational
bound to obtain the update rules.
3.1 Algorithm Derivation
First, we define the concept of Dirichlet adjust-
ment, which is used in the algorithm for vari-
ational update rules involving Dirichlet distribu-
tion. Then, we define some notations for the up-
date rules.
Definition 2 We call vector y of size K is the
Dirichlet adjustment of vector x of size K with re-
spect to Dirichlet distribution D
K
(?) if
y
k
= exp(?(?
k
+ x
k
)??(
?
l
(?
l
+ x
l
))),
where ?(?) is digamma function. We denote it by
y = P
D
(x;?).
We denote element-wise product of matrix X and
matrix Y by X ? Y, element-wise division by
X
Y
, obtaining Y via normalizing of each column
of X as Y
1
? X, and obtaining Y via Dirich-
let adjustment P
D
(?;?) and normalization of each
row of X as
P
D
(?;?),2
?? , i.e., z = P
D
((X
d,?
)
>
;?) and
Y
d,k
= z
k
/
?
k
z
k
. The following is the update rules
for LDA:
U
1
? B
>
[
Y
B
?
U
?
V
>
]
?
V ?
?
U (3)
V
P
D
(?;?),2
?
[
Y
BU
?
V
>
]
>
(BU) ?
?
V (4)
298
Algorithm 1 Iterative Algorithm
Input: Y : term-document matrix
B : term-sentence matrix
K : the number of latent topics
Output: U : sentence-topic matrix
V : auxiliary document-topic matrix
1: Randomly initialize U and V, and normalize them
2: repeat
3: Update U using Eq. (3);
4: Update V using Eq. (4);
5: Compute
?
f using Eq. (2);
6: until
?
f converges.
3.2 Algorithm Procedure
The detail procedure is listed as Algorithm 1.
?From the sentence-topic matrix U, we include
the sentence with the highest probability in each
topic into the summary.
4 Relations with Other Models
In this section, we discuss the connections and
differences of our BSTM model with two related
models.
Recently, a new language model, factorization
with sentence bases (FGB) (Wang et al, 2008) is
proposed for document clustering and summariza-
tion by making use of both term-document matrix
Y and term-sentence matrix B. The FGB model
computes two matrices U and V by optimizing
U,V = argmin
U,V
`(U,V),
where
`(U,V) = KL
(
Y?BUV
>
)
? ln Pr(U,V).
Here, Kullback-Leibler divergence is used to mea-
sure the difference between the distributions of Y
and the estimated BUV
>
. Our BSTM is similar
to the FGB summarization since they are all based
on sentence-based topic model. The difference is
that the document-topic allocation V is marginal-
ized out in BSTM. The marginalization increases
the stability of the estimation of the sentence-topic
parameters. Actually, from the algorithm we can
see that the difference lies in the Dirichlet adjust-
ment. Experimental results show that our BSTM
achieves better summarization results than FGB
model.
Our BSTM model is also related to 3-
factor non-negative matrix factorization (NMF)
model (Ding et al, 2006) where the problem is to
solve U and V by minimizing
`
F
(U,V) = ?Y ?BUV
>
?
2
F
. (5)
Both BSTM and NMF models are used for solv-
ing U and V and have similar multiplicative up-
date rules. Note that if the matrix B is the identity
matrix, Eq. (5) leads to the derivation of the NMF
algorithm with Frobenius norm in (Lee and Seung,
2001). However, our BSTM model is a generative
probabilistic model and makes use of Dirichlet ad-
justment. The results obtained in our model have
clear and rigorous probabilistic interpretations that
the NMF model lacks. In addition, by marginaliz-
ing out V, our BSTM model leads to better sum-
marization results.
5 Experimental Results
5.1 Data Set
To evaluate the summarization results empirically,
we use the DUC2002 and DUC2004 data sets,
both of which are open benchmark data sets from
Document Understanding Conference (DUC) for
generic automatic summarization evaluation. Ta-
ble 1 gives a brief description of the data sets.
DUC2002 DUC2004
number of
document collections 59 50
number of documents ?10 10
in each collection
data source TREC TDT
summary length 200 words 665bytes
Table 1: Description of the data sets for multi-document
summarization
Systems ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SU
DUC Best 0.49869 0.25229 0.46803 0.28406
Random 0.38475 0.11692 0.37218 0.18057
Centroid 0.45379 0.19181 0.43237 0.23629
LexPageRank 0.47963 0.22949 0.44332 0.26198
LSA 0.43078 0.15022 0.40507 0.20226
NMF 0.44587 0.16280 0.41513 0.21687
KM 0.43156 0.15135 0.40376 0.20144
FGB 0.48507 0.24103 0.45080 0.26860
BSTM 0.48812 0.24571 0.45516 0.27018
Table 2: Overall performance comparison on DUC2002
data using ROUGE evaluation methods.
Systems ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SU
DUC Best 0.38224 0.09216 0.38687 0.13233
Random 0.31865 0.06377 0.34521 0.11779
Centroid 0.36728 0.07379 0.36182 0.12511
LexPageRank 0.37842 0.08572 0.37531 0.13097
LSA 0.34145 0.06538 0.34973 0.11946
NMF 0.36747 0.07261 0.36749 0.12918
KM 0.34872 0.06937 0.35882 0.12115
FGB 0.38724 0.08115 0.38423 0.12957
BSTM 0.39065 0.09010 0.38799 0.13218
Table 3: Overall performance comparison on DUC2004 data using
ROUGE evaluation methods.
5.2 Implemented Systems
We implement the following most widely used
document summarization methods as the base-
line systems to compare with our proposed BSTM
method. (1) Random: The method selects sen-
tences randomly for each document collection.
299
(2) Centroid: The method applies MEAD algo-
rithm (Radev et al, 2004) to extract sentences ac-
cording to the following three parameters: cen-
troid value, positional value, and first-sentence
overlap. (3) LexPageRank: The method first con-
structs a sentence connectivity graph based on
cosine similarity and then selects important sen-
tences based on the concept of eigenvector cen-
trality (Erkan and Radev, 2004). (4) LSA: The
method performs latent semantic analysis on terms
by sentences matrix to select sentences having
the greatest combined weights across all impor-
tant topics (Gong and Liu, 2001). (5) NMF: The
method performs non-negative matrix factoriza-
tion (NMF) on terms by sentences matrix and then
ranks the sentences by their weighted scores (Lee
and Seung, 2001). (6) KM: The method performs
K-means algorithm on terms by sentences matrix
to cluster the sentences and then chooses the cen-
troids for each sentence cluster. (7) FGB: The
FGB method is proposed in (Wang et al, 2008).
5.3 Evaluation Measures
We use ROUGE toolkit (version 1.5.5) to measure
the summarization performance, which is widely
applied by DUC for performance evaluation. It
measures the quality of a summary by counting the
unit overlaps between the candidate summary and
a set of reference summaries. The full explanation
of the evaluation toolkit can be found in (Lin and
E.Hovy, 2003). In general, the higher the ROUGE
scores, the better summarization performance.
5.4 Result Analysis
Table 2 and Table 3 show the comparison results
between BSTM and other implemented systems.
From the results, we have the follow observa-
tions: (1) Random has the worst performance.
The results of LSA, KM, and NMF are similar
and they are slightly better than those of Random.
Note that LSA and NMF provide continuous so-
lutions to the same K-means clustering problem
while LSA relaxes the non-negativity of the clus-
ter indicator of K-means and NMF relaxes the
orthogonality of the cluster indicator (Ding and
He, 2004; Ding et al, 2005). Hence all these
three summarization methods perform clustering-
based summarization: they first generate sentence
clusters and then select representative sentences
from each sentence cluster. (2) The Centroid sys-
tem outperforms clustering-based summarization
methods in most cases. This is mainly because
the Centroid based algorithm takes into account
positional value and first-sentence overlap which
are not used in clustering-based summarization.
(3) LexPageRank outperforms Centroid. This is
due to the fact that LexPageRank ranks the sen-
tence using eigenvector centrality which implic-
itly accounts for information subsumption among
all sentences (Erkan and Radev, 2004). (4) FGB
performs better than LexPageRank. Note that
FGB model makes use of both term-document and
term-sentence matrices. Our BSTM model outper-
forms FGB since the document-topic allocation is
marginalized out in BSTM and the marginaliza-
tion increases the stability of the estimation of the
sentence-topic parameters. (5) Our BSTM method
outperforms all other implemented systems and its
performance is close to the results of the best team
in the DUC competition. Note that the good per-
formance of the best team in DUC benefits from
their preprocessing on the data using deep natural
language analysis which is not applied in our im-
plemented systems.
The experimental results provide strong evi-
dence that our BSTM is a viable method for docu-
ment summarization.
Acknowledgement: The work is partially
supported by NSF grants IIS-0546280, DMS-
0844513 and CCF-0830659.
References
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet alocation. In Advances
in Neural Information Processing Systems 14.
C. Ding and X. He. K-means clustering and principal component analysis. In
Prodeedings of ICML 2004.
Chris Ding, Xiaofeng He, and Horst Simon. 2005. On the equivalence of
nonnegative matrix factorization and spectral clustering. In Proceedings of
Siam Data Mining.
Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonneg-
ative matrix tri-factorizations for clustering. In Proceedings of SIGKDD
2006.
G. Erkan and D. Radev. 2004. Lexpagerank: Prestige in multi-document text
summarization. In Proceedings of EMNLP 2004.
Y. Gong and X. Liu. 2001. Generic text summarization using relevance mea-
sure and latent semantic analysis. In Proceedings of SIGIR.
Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix
factorization. In Advances in Neural Information Processing Systems 13.
C-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of NLT-NAACL 2003.
C-Y. Lin and E. Hovy. 2002. From single to multi-document summarization:
A prototype system and its evaluation. In Proceedings of ACL 2002.
I. Mani. 2001. Automatic summarization. John Benjamins Publishing Com-
pany.
D. Radev, H. Jing, M. Stys, and D. Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and Management, pages
919?938.
B. Ricardo and R. Berthier. 1999. Modern information retrieval. ACM Press.
D. Shen, J-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document summariza-
tion using conditional random fields. In Proceedings of IJCAI 2007.
Dingding Wang, Shenghuo Zhu, Tao Li, Yun Chi, and Yihong Gong. 2008.
Integrating clustering and multi-document summarization to improve doc-
ument understanding. In Proceedings of CIKM 2008.
W-T. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-
document summarization by maximizing informative content-words. In
Proceedings of IJCAI 2007.
300
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 984?992,
Beijing, August 2010
Multi-Document Summarization via
the Minimum Dominating Set
Chao Shen and Tao Li
School of Computing and Information Sciences
Florida Internation University
{cshen001|taoli}@cs.fiu.edu
Abstract
Multi-document summarization has
been an important problem in infor-
mation retrieval. It aims to dis-
till the most important information
from a set of documents to gener-
ate a compressed summary. Given
a sentence graph generated from a
set of documents where vertices repre-
sent sentences and edges indicate that
the corresponding vertices are simi-
lar, the extracted summary can be de-
scribed using the idea of graph dom-
ination. In this paper, we propose
a new principled and versatile frame-
work for multi-document summariza-
tion using the minimum dominating
set. We show that four well-known
summarization tasks including generic,
query-focused, update, and compara-
tive summarization can be modeled as
different variations derived from the
proposed framework. Approximation
algorithms for performing summariza-
tion are also proposed and empirical
experiments are conducted to demon-
strate the effectiveness of our proposed
framework.
1 Introduction
As a fundamental and effective tool for docu-
ment understanding and organization, multi-
document summarization enables better infor-
mation services by creating concise and infor-
mative reports for a large collection of doc-
uments. Specifically, in multi-document sum-
marization, given a set of documents as input,
the goal is to produce a condensation (i.e.,
a generated summary) of the content of the
entire input set (Jurafsky and Martin, 2008).
The generated summary can be generic where
it simply gives the important information con-
tained in the input documents without any
particular information needs or query/topic-
focused where it is produced in response to a
user query or related to a topic or concern the
development of an event (Jurafsky and Mar-
tin, 2008; Mani, 2001).
Recently, new summarization tasks such as
update summarization (Dang and Owczarzak,
2008) and comparative summarization (Wang
et al, 2009a) have also been proposed. Up-
date summarization aims to generate short
summaries of recent documents to capture
new information different from earlier docu-
ments and comparative summarization aims
to summarize the differences between compa-
rable document groups.
In this paper, we propose a new principled
and versatile framework for multi-document
summarization using the minimum dominat-
ing set. Many known summarization tasks in-
cluding generic, query-focused, update, and
comparative summarization can be modeled
as different variations derived from the pro-
posed framework. The framework provides an
elegant basis to establish the connections be-
tween various summarization tasks while high-
lighting their differences.
In our framework, a sentence graph is first
generated from the input documents where
vertices represent sentences and edges indicate
that the corresponding vertices are similar. A
natural method for describing the extracted
summary is based on the idea of graph dom-
ination (Wu and Li, 2001). A dominating set
of a graph is a subset of vertices such that
every vertex in the graph is either in the sub-
set or adjacent to a vertex in the subset; and
984
a minimum dominating set is a dominating
set with the minimum size. The minimum
dominating set of the sentence graph can be
naturally used to describe the summary: it
is representative since each sentence is either
in the minimum dominating set or connected
to one sentence in the set; and it is with
minimal redundancy since the set is of mini-
mum size. Approximation algorithms are pro-
posed for performing summarization and em-
pirical experiments are conducted to demon-
strate the effectiveness of our proposed frame-
work. Though the dominating set problem has
been widely used in wireless networks, this pa-
per is the first work on using it for modeling
sentence extraction in document summariza-
tion.
The rest of the paper is organized as fol-
lows. In Section 2, we review the related work
about multi-document summarization and the
dominating set. After introducing the min-
imum dominating set problem in graph the-
ory in Section 3, we propose the minimum
dominating set based framework for multi-
document summarization and model the four
summarization tasks including generic, query-
focused, update, and comparative summariza-
tion in Section 4. Section 5 presents the exper-
imental results and analysis, and finally Sec-
tion 6 concludes the paper.
2 Related Work
Generic Summarization For generic sum-
marization, a saliency score is usually as-
signed to each sentence and then the sen-
tences are ranked according to the saliency
score. The scores are usually computed based
on a combination of statistical and linguistic
features. MEAD (Radev et al, 2004) is an
implementation of the centroid-based method
where the sentence scores are computed based
on sentence-level and inter-sentence features.
SumBasic (Nenkova and Vanderwende, 2005)
shows that the frequency of content words
alone can also lead good summarization re-
sults. Graph-based methods (Erkan and
Radev, 2004; Wan et al, 2007b) have also
been proposed to rank sentences or passages
based on the PageRank algorithm or its vari-
ants.
Query-Focused Summarization In
query-focused summarization, the informa-
tion of the given topic or query should be
incorporated into summarizers, and sentences
suiting the user?s declared information need
should be extracted. Many methods for
generic summarization can be extended to
incorporate the query information (Saggion
et al, 2003; Wei et al, 2008). Wan et al
(Wan et al, 2007a) make full use of both
the relationships among all the sentences in
the documents and relationship between the
given query and the sentences by manifold
ranking. Probability models have also been
proposed with different assumptions on the
generation process of the documents and
the queries (Daume? III and Marcu, 2006;
Haghighi and Vanderwende, 2009; Tang et
al., 2009).
Update Summarization and Compara-
tive Summarization Update summariza-
tion was introduced in Document Understand-
ing Conference (DUC) 2007 (Dang, 2007) and
was a main task of the summarization track in
Text Analysis Conference (TAC) 2008 (Dang
and Owczarzak, 2008). It is required to sum-
marize a set of documents under the assump-
tion that the reader has already read and
summarized the first set of documents as the
main summary. To produce the update sum-
mary, some strategies are required to avoid re-
dundant information which has already been
covered by the main summary. One of the
most frequently used methods for remov-
ing redundancy is Maximal Marginal Rele-
vance(MMR) (Goldstein et al, 2000). Com-
parative document summarization is proposed
by Wang et. al. (Wang et al, 2009a) to
summarize the differences between compara-
ble document groups. A sentence selection
approach is proposed in (Wang et al, 2009a)
to accurately discriminate the documents in
different groups modeled by the conditional
entropy.
985
The Dominating Set Many approxima-
tion algorithms have been developed for find-
ing minimum dominating set for a given
graph (Guha and Khuller, 1998; Thai et al,
2007). Kann (Kann, 1992) shows that the
minimum dominating set problem is equiv-
alent to set cover problem, which is a well-
known NP-hard problem. Dominating set has
been widely used for clustering in wireless net-
works (Chen and Liestman, 2002; Han and
Jia, 2007). It has been used to find topic
words for hierarchical summarization (Lawrie
et al, 2001), where a set of topic words is ex-
tracted as a dominating set of word graph. In
our work, we use the minimum dominating set
to formalize the sentence extraction for docu-
ment summarization.
3 The Minimum Dominating Set
Problem
Given a graph G =< V,E >, a dominating
set of G is a subset S of vertices with the
following property: each vertex of G is either
in the dominating set S, or is adjacent to some
vertices in S.
Problem 3.1. Given a graph G, the mini-
mum dominating set problem (MDS) is to find
a minimum size subset S of vertices, such that
S forms a dominating set.
MDS is closely related to the set cover prob-
lem (SC), a well-known NP-hard problem.
Problem 3.2. Given F , a finite collection
{S1, S2, . . . , Sn} of finite sets, the set cover
problem (SC) is to find the optimal solution
F ? = arg min
F ??F
|F ?| s.t.
?
S??F ?
S? =
?
S?F
S.
Theorem 3.3. There exists a pair of polyno-
mial time reduction between MDS and SC.
So, MDS is also NP-hard and it has been
shown that there are no approximate solutions
within c log |V |, for some c > 0 (Feige, 1998;
Raz and Safra, 1997).
3.1 An Approximation Algorithm
A greedy approximation algorithm for the SC
problem is described in (Johnson, 1973). Ba-
sically, at each stage, the greedy algorithm
chooses the set which contains the largest
number of uncovered elements.
Based on Theorem 3.3, we can obtain a
greedy approximation algorithm for MDS.
Starting from an empty set, if the current sub-
set of vertices is not the dominating set, a new
vertex which has the most number of the ad-
jacent vertices that are not adjacent to any
vertex in the current set will be added.
Proposition 3.4. The greedy algorithm ap-
proximates SC within 1 + ln s where s is the
size of the largest set.
It was shown in (Johnson, 1973) that the
approximation factor for the greedy algorithm
is no more thanH(s) , the s-th harmonic num-
ber:
H(s) =
s?
k=1
1
k ? ln s+ 1
Corollary 3.5. MDS has a approximation al-
gorithm within 1 + ln? where ? is the maxi-
mum degree of the graph.
Corollary 3.5 follows directly from Theo-
rem 3.3 and Proposition 3.4.
4 The Summarization Framework
4.1 Sentence Graph Generation
To perform multi-document summarization
via minimum dominating set, we need to first
construct a sentence graph in which each node
is a sentence in the document collection. In
our work, we represent the sentences as vec-
tors based on tf-isf, and then obtain the cosine
similarity for each pair of sentences. If the
similarity between a pair of sentences si and
sj is above a given threshold ?, then there is
an edge between si and sj .
For generic summarization, we use all sen-
tences for building the sentence graph. For
query-focused summarization, we only use the
sentences containing at least one term in the
query. In addition, when a query q is involved,
we assign each node si a weight, w(si) =
d(si, q) = 1 ? cos(si, q), to indicate the dis-
tance between the sentence and the query q.
After building the sentence graph, we can
formulate the summarization problem using
986
Generic Summary
(a)
Query-focused Summary
query
(b)
Updated Summary
C
1
C
2
(c)
Comparative Summary
Comparative Summary
Comparative Summary
C
2
C
1
C
3
(d)
Figure 1: Graphical illustrations of multi-document summarization via the minimum domi-
nating set. (a): The minimum dominating set is extracted as the generic summary. (b):The
minimum weighted dominating set is extracted as the query-based summary. (c):Vertices in
the right rectangle represent the first document set C1, and ones in the left represent the sec-
ond document set where update summary is generated. (d):Each rectangle represents a group
of documents. The vertices with rings are the dominating set for each group, while the solid
vertices are the complementary dominating set, which is extracted as comparative summaries.
the minimum dominating set. A graphical il-
lustration of the proposed framework is shown
in Figure 1.
4.2 Generic Summarization
Generic summarization is to extract the most
representative sentences to capture the impor-
tant content of the input documents. Without
taking into account the length limitation of
the summary, we can assume that the sum-
mary should represent all the sentences in the
document set (i.e., every sentence in the docu-
ment set should either be extracted or be sim-
ilar with one extracted sentence). Meanwhile,
a summary should also be as short as possi-
ble. Such summary of the input documents
under the assumption is exactly the minimum
dominating set of the sentence graph we con-
structed from the input documents in Section
4.1. Therefore the summarization problem
can be formulated as the minimum dominat-
ing set problem.
However, usually there is a length restric-
tion for generating the summary. Moreover,
the MDS is NP-hard as shown in Section 3.
Therefore, it is straightforward to use a greedy
approximation algorithm to construct a subset
of the dominating set as the final summary. In
the greedy approach, at each stage, a sentence
which is optimal according to the local crite-
ria will be extracted. Algorithm 1 describes
Algorithm 1 Algorithm for Generic Summariza-
tion
INPUT: G, W
OUTPUT: S
1: S = ?
2: T = ?
3: while L(S) < W and V (G)! = S do
4: for v ? V (G)? S do
5: s(v) = |{ADJ(v) ? T}|
6: v? = argmaxv s(v)
7: S = S ? {v?}
8: T = T ?ADJ(v?)
an approximation algorithm for generic sum-
marization. In Algorithm 1, G is the sen-
tence graph, L(S) is the length of the sum-
mary, W is the maximal length of the sum-
mary, and ADJ(v) = {v?|(v?, v) ? E(G)} is
the set of vertices which are adjacent to the
vertex v. A graphical illustration of generic
summarization using the minimum dominat-
ing set is shown in Figure 1(a).
4.3 Query-Focused Summarization
Letting G be the sentence graph constructed
in Section 4.1 and q be the query, the query-
focused summarization can be modeled as
D? = argminD?G
?
s?D d(s, q) (1)
s.t. D is a dominating set of G.
Note that d(s, q) can be viewed as the weight
of vertex in G. Here the summary length is
minimized implicitly, since if D? ? D, then
987
?
s?D? d(s, q) ?
?
s?D d(s, q). The problem
in Eq.(1) is exactly a variant of the minimum
dominating set problem, i.e., the minimum
weighted dominating set problem (MWDS).
Similar to MDS, MWDS can be reduced
from the weighted version of the SC problem.
In the weighted version of SC, each set has a
weight and the sum of weights of selected sets
needs to be minimized. To generate an ap-
proximate solution for the weighted SC prob-
lem, instead of choosing a set i maximizing
|SET (i)|, a set i minimizing w(i)|SET (i)| is cho-
sen, where SET (i) is composed of uncovered
elements in set i, and w(i) is the weight of set
i. The approximate solution has the same ap-
proximation ratio as that for MDS, as stated
by the following theorem (Chvatal, 1979).
Theorem 4.1. An approximate weighted
dominating set can be generated with a size at
most 1+log??|OPT |, where ? is the maximal
degree of the graph and OPT is the optimal
weighted dominating set.
Accordingly, from generic summarization to
query-focused summarization, we just need to
modify line 6 in Algorithm 1 to
v? = argmin
v
w(v)
s(v) , (2)
where w(v) is the weight of vertex v. A graph-
ical illustration of query-focused summariza-
tion using the minimum dominating set is
shown in Figure 1(b).
4.4 Update Summarization
Give a query q and two sets of documents C1
and C2, update summarization is to generate
a summary of C2 based on q, given C1. Firstly,
summary of C1, referred as D1 can be gener-
ated. Then, to generate the update summary
of C2, referred as D2, we assume D1 and D2
should represent all query related sentences in
C2, and length of D2 should be minimized.
Let G1 be the sentence graph for C1. First
we use the method described in Section 4.3 to
extract sentences from G1 to form D1. Then
we expand G1 to the whole graph G using the
second set of documents C2. G is then the
graph presentation of the document set in-
cluding C1 and C2. We can model the update
summary of C2 as
D? = argminD2
?
s?D2 w(s) (3)
s.t. D2 ?D1 is a dominating set of G.
Intuitively, we extract the smallest set of sen-
tences that are closely related to the query
from C2 to complete the partial dominating
set of G generated from D1. A graphical il-
lustration of update summarization using the
minimum dominating set is shown in Fig-
ure 1(c).
4.5 Comparative Summarization
Comparative document summarization aims
to summarize the differences among compara-
ble document groups. The summary produced
for each group should emphasize its difference
from other groups (Wang et al, 2009a).
We extend our method for update sum-
marization to generate the discriminant sum-
mary for each group of documents. Given N
groups of documents C1, C2, . . . , CN , we first
generate the sentence graphs G1, G2, . . . , GN ,
respectively. To generate the summary for
Ci, 1 ? i ? N , we view Ci as the update
of all other groups. To extract a new sen-
tence, only the one connected with the largest
number of sentences which have no represen-
tatives in any groups will be extracted. We
denote the extracted set as the complemen-
tary dominating set, since for each group we
obtain a subset of vertices dominating those
are not dominated by the dominating sets of
other groups. To perform comparative sum-
marization, we first extract the standard dom-
inating sets for G1, . . . , GN , respectively, de-
noted as D1, . . . , DN . Then we extract the
so-called complementary dominating set CDi
for Gi by continuing adding vertices in Gi to
find the dominating set of ?1?j?NGj given
D1, . . . ,Di?1,Di+1, . . . ,DN . A graphical il-
lustration of comparative summarization is
shown in Figure 1(d).
988
DUC04 DUC05 DUC06 TAC08 A TAC08 B
Type of Summarization Generic Topic-focused Topic-focused Topic-focused Update
#topics NA 50 50 48 48
#documents per topic 10 25-50 25 10 10
Summary length 665 bytes 250 words 250 words 100 words 100 words
Table 1: Brief description of the data set
5 Experiments
We have conducted experiments on all four
summarization tasks and our proposed meth-
ods based on the minimum dominating set
have outperformed many existing methods.
For the generic, topic-focused and update
summarization tasks, the experiments are per-
form the DUC data sets using ROUGE-2 and
ROUGE-SU (Lin and Hovy, 2003) as evalua-
tion measures. For comparative summariza-
tion, a case study as in (Wang et al, 2009a) is
performed. Table 1 shows the characteristics
of the data sets. We use DUC04 data set to
evaluate our method for generic summariza-
tion task and DUC05 and DUC06 data sets
for query-focused summarization task. The
data set for update summarization, (i.e. the
main task of TAC 2008 summarization track)
consists of 48 topics and 20 newswire articles
for each topic. The 20 articles are grouped
into two clusters. The task requires to pro-
duce 2 summaries, including the initial sum-
mary (TAC08 A) which is standard query-
focused summarization and the update sum-
mary (TAC08 B) under the assumption that
the reader has already read the first 10 docu-
ments.
We apply a 5-fold cross-validation proce-
dure to choose the threshold ? used for gener-
ating the sentence graph in our method.
5.1 Generic Summarization
We implement the following widely used or
recent published methods for generic summa-
rization as the baseline systems to compare
with our proposed method (denoted as MDS).
(1) Centroid: The method applies MEAD al-
gorithm (Radev et al, 2004) to extract sen-
tences according to the following three pa-
rameters: centroid value, positional value,
and first-sentence overlap. (2) LexPageR-
ank: The method first constructs a sentence
connectivity graph based on cosine similarity
and then selects important sentences based on
the concept of eigenvector centrality (Erkan
and Radev, 2004). (3) BSTM: A Bayesian
sentence-based topic model making use of
both the term-document and term-sentence
associations (Wang et al, 2009b).
Our method outperforms the simple Cen-
troid method and another graph-based Lex-
PageRank, and its performance is close to the
results of the Bayesian sentence-based topic
model and those of the best team in the DUC
competition. Note however that, like clus-
tering or topic based methods, BSTM needs
the topic number as the input, which usually
varies by different summarization tasks and is
hard to estimate.
5.2 Query-Focused Summarization
We compare our method (denoted as MWDS)
described in Section 4.3 with some recently
published systems. (1) TMR (Tang et al,
2009): incorporates the query information
into the topic model, and uses topic based
score and term frequency to estimate the im-
portance of the sentences. (2) SNMF (Wang
et al, 2008): calculates sentence-sentence
similarities by sentence-level semantic analy-
sis, clusters the sentences via symmetric non-
negative matrix factorization, and extracts
the sentences based on the clustering result.
(3) Wiki (Nastase, 2008): uses Wikipedia
as external knowledge to expand query and
builds the connection between the query and
the sentences in documents.
Table 3 presents the experimental compar-
ison of query-focused summarization on the
two datasets. From Table 3, we observe that
our method is comparable with these systems.
This is due to the good interpretation of the
summary extracted by our method, an ap-
989
DUC04
ROUGE-2 ROUGE-SU
DUC Best 0.09216 0.13233
Centroid 0.07379 0.12511
LexPageRank 0.08572 0.13097
BSTM 0.09010 0.13218
MDS 0.08934 0.13137
Table 2: Results on generic summariza-
tion.
DUC05 DUC06
ROUGE-2 ROUGE-SU ROUGE-2 ROUGE-SU
DUC Best 0.0725 0.1316 0.09510 0.15470
SNMF 0.06043 0.12298 0.08549 0.13981
TMR 0.07147 0.13038 0.09132 0.15037
Wiki 0.07074 0.13002 0.08091 0.14022
MWDS 0.07311 0.13061 0.09296 0.14797
Table 3: Results on query-focused summariza-
tion.
proximate minimal dominating set of the sen-
tence graph. On DUC05, our method achieves
the best result; and on DUC06, our method
outperforms all other systems except the best
team in DUC. Note that our method based
on the minimum dominating set is much sim-
pler than other systems. Our method only
depends on the distance to the query and has
only one parameter (i.e., the threshold ? in
generating the sentence graph).
 0.065
 0.07
 0.075
 0.08
 0.085
 0.09
 0.095
 0.05  0.1  0.15  0.2  0.25
R
O
UG
E-
2
Similarity threshold ?
DUC 06
DUC 05
Figure 2: ROUGE-2 vs. threshold ?
We also conduct experiments to empirically
evaluate the sensitivity of the threshold ?.
Figure 2 shows the ROUGE-2 curve of our
MWDS method on the two datasets when ?
varies from 0.04 to 0.26. When ? is small,
edges fail to represent the similarity of the sen-
tences, while if ? is too large, the graph will
be sparse. As ? is approximately in the range
of 0.1? 0.17, ROUGE-2 value becomes stable
and relatively high.
5.3 Update Summarization
Table 5 presents the experimental results on
update summarization. In Table 5, ?TAC
Best? and ?TAC Median? represent the best
and median results from the participants of
TAC 2008 summarization track in the two
tasks respectively according to the TAC 2008
report (Dang and Owczarzak, 2008). As seen
from the results, the ROUGE scores of our
methods are higher than the median results.
The good results of the best team typically
come from the fact that they utilize advanced
natural language processing (NLP) techniques
to resolve pronouns and other anaphoric ex-
pressions. Although we can spend more efforts
on the preprocessing or language processing
step, our goal here is to demonstrate the ef-
fectiveness of formalizing the update summa-
rization problem using the minimum dominat-
ing set and hence we do not utilize advanced
NLP techniques for preprocessing. The exper-
imental results demonstrate that our simple
update summarization method based on the
minimum dominating set can lead to compet-
itive performance for update summarization.
TAC08 A TAC08 B
ROUGE-2 ROUGE-
SU
ROUGE-2 ROUGE-
SU
TAC Best 0.1114 0.14298 0.10108 0.13669
TAC Median 0.08123 0.11975 0.06927 0.11046
MWDS 0.09012 0.12094 0.08117 0.11728
Table 5: Results on update summarization.
5.4 Comparative Summarization
We use the top six largest clusters of doc-
uments from TDT2 corpora to compare the
summary generated by different comparative
summarization methods. The topics of the six
document clusters are as follows: topic 1: Iraq
Issues; topic 2: Asia?s economic crisis; topic 3:
Lewinsky scandal; topic 4: Nagano Olympic
Games; topic 5: Nuclear Issues in Indian and
Pakistan; and topic 6: Jakarta Riot. From
each of the topics, 30 documents are extracted
990
Topic Complementary Dominating Set Discriminative Sentence Selection Dominating Set
1 ? ? ? U.S. Secretary of State
Madeleine Albright arrives to
consult on the stand-off between
the United Nations and Iraq.
the U.S. envoy to the United
Nations, Bill Richardson, ? ? ?
play down China?s refusal to sup-
port threats of military force
against Iraq
The United States and Britain
do not trust President Sad-
dam and wants cdotswarning
of serious consequences if Iraq
violates the accord.
2 Thailand?s currency, the
baht, dropped through a
key psychological level of ? ? ?
amid a regional sell-off sparked
by escalating social unrest in
Indonesia.
Earlier, driven largely by the de-
clining yen, South Korea?s
stock market fell by ? ? ? , while
the Nikkei 225 benchmark in-
dex dipped below 15,000 in the
morning ? ? ?
In the fourth quarter, IBM
Corp. earned $2.1 billion, up
3.4 percent from $2 billion a
year earlier.
3 ? ? ? attorneys representing Pres-
ident Clinton and Monica
Lewinsky.
The following night Isikoff ? ? ? ,
where he directly followed the
recitation of the top-10 list: ?Top
10 White House Jobs That
Sound Dirty.?
In Washington, Ken Starr?s
grand jury continued its inves-
tigation of theMonica Lewin-
sky matter.
4 Eight women and six men were
named Saturday night as the
first U.S. Olympic Snow-
board Team as their sport
gets set to make its debut in
Nagano, Japan.
this tunnel is finland?s cross coun-
try version of tokyo?s alpine ski
dome, and olympic skiers flock
from russia, ? ? ? , france and aus-
tria this past summer to work out
the kinks ? ? ?
If the skiers the men?s super-
G and the women?s downhill
on Saturday, they will be back
on schedule.
5 U.S. officials have announced
sanctions Washington will im-
pose on India and Pakistan
for conducting nuclear tests.
The sanctions would stop all for-
eign aid except for humanitarian
purposes, ban military sales to
India ? ? ?
And Pakistan?s prime min-
ister says his country will sign
the U.N.?s comprehensive
ban on nuclear tests if In-
dia does, too.
6 ? ? ? remain in force around
Jakarta, and at the Parliament
building where thousands of
students staged a sit-in Tues-
day ? ? ? .
?President Suharto has given
much to his country over the
past 30 years, raising Indone-
sia?s standing in the world ? ? ?
What were the students doing
at the time you were there, and
what was the reaction of the
students to the troops?
Table 4: A case study on comparative document summarization. Some unimportant words are skipped due to
the space limit. The bold font is used to annotate the phrases that are highly related with the topics, and italic
font is used to highlight the sentences that are not proper to be used in the summary.
randomly to produce a one-sentence summary.
For comparison purpose, we extract the sen-
tence with the maximal degree as the base-
line. Note that the baseline can be thought
as an approximation of the dominating set
using only one sentence. Table 4 shows the
summaries generated by our method (comple-
mentary dominating set (CDS)), discrimina-
tive sentence selection (DSS) (Wang et al,
2009a) and the baseline method. Our CDS
method can extract discriminative sentences
for all the topics. DSS can extract discrimina-
tive sentences for all the topics except topic 4.
Note that the sentence extracted by DSS for
topic 4 may be discriminative from other top-
ics, but it is deviated from the topic Nagano
Olympic Games. In addition, DSS tends to
select long sentences which should not be pre-
ferred for summarization purpose. The base-
line method may extract some general sen-
tences, such as the sentence for topic 2 and
topic 6 in Table 4.
6 Conclusion
In this paper, we propose a framework to
model the multi-document summarization us-
ing the minimum dominating set and show
that many well-known summarization tasks
can be formulated using the proposed frame-
work. The proposed framework leads to sim-
ple yet effective summarization methods. Ex-
perimental results show that our proposed
methods achieve good performance on several
multi-document document tasks.
7 Acknowledgements
This work is supported by NSF grants IIS-
0549280 and HRD-0833093.
991
References
Chen, Y.P. and A.L. Liestman. 2002. Approximating
minimum size weakly-connected dominating sets for
clustering mobile ad hoc networks. In Proceedings
of International Symposium on Mobile Ad hoc Net-
working & Computing. ACM.
Chvatal, V. 1979. A greedy heuristic for the set-
covering problem. Mathematics of operations re-
search, 4(3):233?235.
Dang, H.T. and K Owczarzak. 2008. Overview of the
TAC 2008 Update Summarization Task. In Pro-
ceedings of the Text Analysis Conference (TAC).
Dang, H.T. 2007. Overview of DUC 2007. In Docu-
ment Understanding Conference.
Daume? III, H. and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the ACL-
COLING.
Erkan, G. and D.R. Radev. 2004. Lexpagerank: Pres-
tige in multi-document text summarization. In Pro-
ceedings of EMNLP.
Feige, U. 1998. A threshold of lnn for approximating
set cover. Journal of the ACM (JACM), 45(4):634?
652.
Goldstein, J., V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document summariza-
tion by sentence extraction. In NAACL-ANLP 2000
Workshop on Automatic summarization.
Guha, S. and S. Khuller. 1998. Approximation algo-
rithms for connected dominating sets. Algorithmica,
20(4):374?387.
Haghighi, A. and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of HLT-NAACL.
Han, B. and W. Jia. 2007. Clustering wireless ad
hoc networks with weakly connected dominating
set. Journal of Parallel and Distributed Computing,
67(6):727?737.
Johnson, D.S. 1973. Approximation algorithms for
combinatorial problems. In Proceedings of STOC.
Jurafsky, D. and J.H. Martin. 2008. Speech and lan-
guage processing. Prentice Hall New York.
Kann, V. 1992. On the approximability of NP-
complete optimization problems. PhD thesis, De-
partment of Numerical Analysis and Computing
Science, Royal Institute of Technology, Stockholm.
Lawrie, D., W.B. Croft, and A. Rosenberg. 2001.
Finding topic words for hierarchical summarization.
In Proceedings of SIGIR.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL.
Mani, I. 2001. Automatic summarization. Computa-
tional Linguistics, 28(2).
Nastase, V. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
spreading activation. In Proceedings of EMNLP.
Nenkova, A. and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.
Radev, D.R., H. Jing, M. Stys?, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40(6):919?938.
Raz, R. and S. Safra. 1997. A sub-constant error-
probability low-degree test, and a sub-constant
error-probability PCP characterization of NP. In
Proceedings of STOC.
Saggion, H., K. Bontcheva, and H. Cunningham. 2003.
Robust generic and query-based summarisation. In
Proceedings of EACL.
Tang, J., L. Yao, and D. Chen. 2009. Multi-topic
based Query-oriented Summarization. In Proceed-
ings of SDM.
Thai, M.T., N. Zhang, R. Tiwari, and X. Xu. 2007.
On approximation algorithms of k-connected m-
dominating sets in disk graphs. Theoretical Com-
puter Science, 385(1-3):49?59.
Wan, X., J. Yang, and J. Xiao. 2007a. Manifold-
ranking based topic-focused multi-document sum-
marization. In Proceedings of IJCAI.
Wan, X., J. Yang, and J. Xiao. 2007b. Towards an
iterative reinforcement approach for simultaneous
document summarization and keyword extraction.
In Proceedings of ACL.
Wang, D., T. Li, S. Zhu, and C. Ding. 2008. Multi-
document summarization via sentence-level seman-
tic analysis and symmetric matrix factorization. In
Proceedings of SIGIR.
Wang, D., S. Zhu, T. Li, and Y. Gong. 2009a. Com-
parative document summarization via discrimina-
tive sentence selection. In Proceeding of CIKM.
Wang, D., S. Zhu, T. Li, and Y. Gong. 2009b.
Multi-document summarization using sentence-
based topic models. In Proceedings of the ACL-
IJCNLP.
Wei, F., W. Li, Q. Lu, and Y. He. 2008. Query-
sensitive mutual reinforcement chain and its ap-
plication in query-oriented multi-document summa-
rization. In Proceedings of SIGIR.
Wu, J. and H. Li. 2001. A dominating-set-based rout-
ing scheme in ad hoc wireless networks. Telecom-
munication Systems, 18(1):13?36.
992
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 949?958,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Non-negative Matrix Factorization Based Approach for Active Dual
Supervision from Document and Word Labels
Chao Shen and Tao Li
School of Computing and Information Sciences
Florida International University
Miami, FL 33199 USA
{cshen001,taoli}@cs.fiu.edu
Abstract
In active dual supervision, not only informa-
tive examples but also features are selected for
labeling to build a high quality classifier with
low cost. However, how to measure the infor-
mativeness for both examples and feature on
the same scale has not been well solved. In
this paper, we propose a non-negative matrix
factorization based approach to address this is-
sue. We first extend the matrix factorization
framework to explicitly model the correspond-
ing relationships between feature classes and
examples classes. Then by making use of
the reconstruction error, we propose a unified
scheme to determine which feature or exam-
ple a classifier is most likely to benefit from
having labeled. Empirical results demonstrate
the effectiveness of our proposed methods.
1 Introduction
Active learning, as an effective paradigm to optimize
the learning benefit from domain experts? feedback
and to reduce the cost of acquiring labeled examples
for supervised learning, has been intensively stud-
ied in recent years (McCallum and Nigam, 1998;
Tong and Koller, 2002; Settles, 2009). Traditional
approaches for active learning query the human ex-
perts to obtain the labels for intelligently chosen
data samples. However, in text classification where
the input data is generally represented as document-
word matrices, human supervision can be obtained
on both documents and words. For example, in sen-
timent analysis of product reviews, human labelers
can label reviews as positive or negative, they can
also label the words that elicit positive sentiment
(such as ?sensational? and ?electrifying?) as posi-
tive and words that evoke negative sentiment (such
as ?depressed? and ?unfulfilling?) as negative. Re-
cent work has demonstrated that labeled words (or
feature supervision) can greatly reduce the number
of labeled samples for building high-quality classi-
fiers (Druck et al, 2008; Zaidan and Eisner, 2008).
In fact, different kinds of supervision generally have
different acquisition costs, different degrees of util-
ity and are not mutually redundant (Sindhwani et
al., 2009). Ideally, effective active learning schemes
should be able to utilize different forms of supervi-
sion.
To incorporate the supervision on words and doc-
uments at same time into the active learning scheme,
recently an active dual supervision (or dual active
learning) has been proposed (Melville and Sind-
hwani, 2009; Sindhwani et al, 2009). Comparing
with traditional active learning which aims to select
the most ?informative? examples (e.g., documents)
for domain experts to label, active dual supervi-
sion selects both the ?informative? examples (e.g.,
documents) and features (e.g., words) for labeling.
For active dual supervision to be effective, there
are three important components: a) an underlying
learning mechanism that is able to learn from both
the labeled examples and features (i.e., incorporat-
ing supervision on both examples and features); b)
methods for estimating the value of information for
example and feature labels; and c) a scheme that
should be able to trade-off the costs and benefits of
the different forms of supervision since they have
different labeling costs and different benefits.
949
In Sindhwani et al?s initial work on active dual
supervision (Sindhwani et al, 2009), a transductive
bipartite graph regularization approach is used for
learning from both labeled examples and features.
In addition, uncertainty sampling and experimental
design are used for selecting informative examples
and features for labeling. To trade-off between dif-
ferent types of supervision, a simple probabilistic
interleaving scheme where the active learner prob-
abilistically queries the example oracle and the fea-
ture oracle is used. One problem in their method is
that the values of acquiring the feature labels and
the example labels are not on the same scale.
Recently, Li et al (Li et al, 2009) proposed a
dual supervision method based on constrained non-
negative tri-factorization of the document-term ma-
trix where the labeled features and examples are
naturally incorporated as sets of constraints. Hav-
ing a framework for incorporating dual-supervision
based on matrix factorization, gives rise to the nat-
ural question of how to perform active dual super-
vision in this setting. Since rows and columns are
treated equally in estimating the errors of matrix fac-
torization, another question is can we address the
scaling issue in comparing the value of feature la-
bels and example labels.
In this paper, we study the problem of ac-
tive dual supervision using non-negative matrix tri-
factorization. Our work is based on the dual supervi-
sion framework using constrained non-negative tri-
factorization proposed in (Li et al, 2009). We first
extend the framework to explicitly model the corre-
sponding relationships between feature classes and
example classes. Then by making use of the recon-
struction error criterion in matrix factorization, we
propose a unified scheme to evaluate the value of
feature and example labels. Instead of comparing
the estimated performance increase of new feature
labels or example labels, our proposed scheme as-
sumes that a better supervision (a feature label or a
example label) should lead to a more accurate re-
construction of the original data matrix. In our pro-
posed scheme, the value of feature labels and ex-
ample labels is computed on the same scale. The
experiments show that our proposed unified scheme
to query selection (i.e., feature/example selection for
labeling) outperforms the interleaving schemes and
the scheme based on expected log gain.
The rest of this paper is organized as follows: the
related work is discussed in Section 2, and the dual
supervision framework based on non-negative ma-
trix tri-factorization is introduced in Section 3. We
extend non-negative matrix tri-factorization to active
learning settings in Section 4, and propose a unified
scheme for query selection in Section 5. Experi-
ments are presented in Section 6, and finally Section
7 concludes the paper.
2 Related Work
We point the reader to a recent report (Settles, 2009)
for an in-depth survey on active learning. In this
section, we briefly cover related work to position our
contributions appropriately.
Active Learning/Active Dual Supervision Most
prior work in active learning has focused on pooled-
based techniques, where examples from an unla-
beled pool are selected for labeling (Cohn et al,
1994). With the study of learning from labeled fea-
tures, many research efforts on active learning with
feature supervision are also reported (Melville et al,
2005; Raghavan et al, 2006). (Godbole et al, 2004)
proposed the notion of feature uncertainty and in-
corporated the acquired feature labels into learning
by creating one-term mini-documents. (Druck et al,
2009) performed active learning via feature labeling
using several uncertainty reduction heuristics using
the learning model developed in (Druck et al, 2008).
(Sindhwani et al, 2009) studied the problem of ac-
tive dual supervision from examples and features
using a graph-based dual supervision method with
a simple probabilistic method for interleaving fea-
ture labels and example labels. In our work, we de-
velop our active dual supervision framework using
constrained non-negative tri-factorization and also
propose a unified scheme to evaluate the value of
feature and example labels. We note the very re-
cent work of (Attenberg et al, 2010), which pro-
poses a unified approach for the dual active learn-
ing problem using expected utility where the utility
is defined as the log gain of the classification model
with a new labeled document or word. Conceptu-
ally, our proposed unified scheme is a special case
of the expected utility framework where the utility
is computed using the matrix reconstruction error.
The utility based on the log gain of the classification
950
model may not be reliable as small model changes
resulted from a single additional example label or
feature label may not be reflected in the classifica-
tion performance (Attenberg et al, 2010). The em-
pirical comparisons show that our proposed unified
scheme based on reconstruction error outperforms
the expected log gain.
Dual Supervision Note that a learning method
that is capable of performing dual supervision (i.e.,
learning from both labeled examples and features)
is the basis for active dual supervision. Dual su-
pervision is a relatively new area of research and
few methods have been developed for dual super-
vision. In (Sindhwani and Melville, 2008; Sind-
hwani et al, 2008), a bipartite graph regularization
model (GRADS) is used to diffuse label informa-
tion along both sides of the document-term matrix
and to perform dual supervision for semi-supervised
sentiment analysis. Conceptually, their model im-
plements a co-clustering assumption closely related
to Singular Value Decomposition (see also (Dhillon,
2001; Zha et al, 2001) for more on this perspec-
tive). In (Sandler et al, 2008), standard regulariza-
tion models are constrained using graphs of word co-
occurrences. In (Melville et al, 2009), Naive Bayes
classifier is extended, where the parameters, the con-
ditional word distributions given the classes, are es-
timated by combining multiple sources, e.g. docu-
ment labels and word labels. Our work is based on
the dual supervision framework using constrained
non-negative tri-factorization.
3 Learning with Dual Supervision via
Tri-NMF
Our dual supervision model is based on non-
negative matrix tri-factorization (Tri-NMF), where
the non-negative input document-word matrix is ap-
proximated by 3 factor matrices as X ? GSF T , in
which,X is an n?m document-term matrix,G is an
n ? k non-negative orthogonal matrix representing
the probability of generating a document from a doc-
ument cluster, F is an m? k non-negative orthogo-
nal matrix representing the probability of generating
a word from a word cluster, and S is a k ? k non-
negative matrix providing the relationship between
document cluster space and word cluster space.
While Tri-NMF is first applied in co-clustering, Li
et al (Li et al, 2009) extended it to incorporate la-
beled words and documents as dual supervision via
two loss terms in the objective function of Tri-NMF
as following:
minF,G,S ?X ?GSF T ?2
+? trace[(F ? F0)TC1(F ? F0)]
+? trace[(G?G0)TC2(G?G0)].
(1)
Here, ? > 0 is a parameter which determines the
extent to which we enforce F ? F0 to its labeled
rows. C1 is a m ? m diagonal matrix whose en-
try (C1)ii = 1 if the row of F0 is labeled, that is,
the class of the i-th word is known and (C1)ii = 0
otherwise. ? > 0 is a parameter which determines
the extent to which we enforce G ? G0 to its la-
beled rows. C2 is a n ? n diagonal matrix whose
entry (C2)ii = 1 if the row of G0 is labeled, that
is, the category of the i-th document is known and
(C2)ii = 0 otherwise. The squared loss terms ensure
that the solution for G,F in the otherwise unsuper-
vised learning problem be close to the prior knowl-
edge G0, F0. So the partial labels on documents and
words can be described using G0 and F0, respec-
tively.
4 Dual Supervision with Explicit Class
Alignment
4.1 Modeling the Relationships between Word
Classes and Document Classes
In the solution to Equation 1, we have S = GTXF ,
or
Slk = gTl Xfk =
1
|Rl|1/2|Ck|1/2
?
i?Rl
?
j?Ck
Xij ,
(2)
where |Rl| is the size of the l-th document class, and
|Ck| is the size of the k-th word class (Ding et al,
2006). Note that Slk represents properly normalized
within-class sum of weights (l = k) and between-
class sum of weights (l 6= k). So, S represents the
relationship between the classes over documents and
the classes over words. Under the assumption that
the i-th document class should correspond to the i-
th word class, S should be an approximate diago-
nal matrix, since the documents of i-th class is more
likely to contain the words of the i-th class. Note
951
that S is not an exact diagonal matrix, since a doc-
ument of one class apparently can use words from
other classes (especially G and F are required to be
approximately orthogonal, which means the classi-
fication is rigorous). However, in Equation 1, there
are no explicit constraints on the relationship be-
tween word classes and document classes. Instead,
the relationship is established and enforced implic-
itly using existing labeled documents and words.
In active learning, the set of starting labeled doc-
uments or words is small, and this may generate an
ill-formed S, leading to an incorrect alignment of
word classes and document classes. To explicitly
model the relationships between word classes and
document classes, we constrain the shape of S via
an extra loss term in the objective function as fol-
lows:
minF,G,S ?X ?GSF T ?2
+? trace[(F ? F0)TC1(F ? F0)]
+? trace[(G?G0)TC2(G?G0)]
+? trace[(S ? S0)T (S ? S0)]
(3)
where S0 is a diagonal matrix.
How to Choose S0 If there is no orthogonal con-
straint on F,G and I-divergence is used as the ob-
jective function, it can been shown that the factors
of Tri-NMF have probabilistic interpretation (Ding
et al, 2008; Shen et al, 2011):
Fil = P (w = wi|zw = l),
Gjk = P (d = dj |zd = k),
Skl = P (zd = k, zw = l),
(4)
where w is word variable, d is document variable,
and zw, zd are random variables indicating word
class and document class respectively. F and G
represent posterior distributions for words and docu-
ments, and S represents the joint distribution of doc-
ument class and word class. With such an interpre-
tation, S0 can be easily decided in balanced classifi-
cation problems with each diagonal entry equals to
one over the number of classes.
However, in our setting of Tri-NMF, orthogonal
constraints are enforced on F,G and Euclidean dis-
tance is used as the objective function. To pre-
compute S0, one way is to first solve the optimiza-
tion problem Equation 1 with another constraint that
S should be diagonal. Alternatively, to keep it sim-
ple, we ignore the known label information and just
assume there exists a diagonal matrix S0 and two
orthogonal matrices G,F , that
GS0F T ? X.
Then
trace[XXT ] ? trace[GS0F TFST0 GT ],
= trace[S0ST0 F TFGTG],
= trace[S0ST0 ],
= ?k(S0)2kk.
(5)
So if a classification problem is balanced with K
classes, S0 can be estimated as following:
(S0)kl =
{ ?
trace[XXT ]
K l = k,
0 otherwise. (6)
4.2 Computing Algorithm
This optimization problem can be solved using the
following update rules
Gjk ? Gjk XFS+?C2G0(GGTXFS+?GGTC2G)jk ,
Sjk ? Sjk F
TXTG+?S0
(FTFSGTG+?S)jk ,
Fjk ? Fjk X
TGST+?C1F0
(FFTXTGST+?C1F )jk .
(7)
The algorithm consists of an iterative procedure us-
ing the above three rules until convergence.
Theorem 4.1 The above iterative algorithm con-
verges.
Theorem 4.2 At convergence, the solution satisfies
the Karuch-Kuhn-Tucker (KKT) optimality condi-
tion, i.e., the algorithm converges correctly to a lo-
cal optima.
Theorem 4.1 can be proved using the standard aux-
iliary function approach (Lee and Seung, 2001).
Proof of Theorem 4.2: Proof for the update rules
of G,F is the same as in (Li et al, 2009). Here we
focus on the update rule of S. We want to minimize
L(S) = ?X ?GSF T ?2
+? trace[(F ? F0)TC1(F ? F0)]
+? trace[(G?G0)TC2(G?G0)]
+? trace[(S ? S0)T (S ? S0)].
(8)
952
The gradient of L is
?L
?S = 2F
TFSGTG? 2F TXTG+ 2?(S ? S0)
The KKT complementarity condition for the non-
negativity of Sjk gives
[2F TFSGTG?2F TXTG+2?(S?S0)]jkSjk = 0.
This is the fixed point relation that local minima for
S must satisfy, which is equivalent with the update
rule of S in Equation 7.
5 A Unified Scheme for Query Selection
Using the Reconstruction Error
5.1 Introduction
An ideal active dual supervision scheme should be
able to evaluate the value of acquiring labels for doc-
uments and words on the same scale. In the initial
study of dual active supervision, different scores are
used for documents and words (e.g. uncertainty for
documents and certainty for words), and thus they
are not on the same scale (Sindhwani et al, 2009).
Recently, the framework of Expected Utility (Esti-
mated Risk Minimization) is proposed in (Attenberg
et al, 2010). At each step of the framework, the next
word or document selected for labeling is the one
that will result in the highest estimated improvement
in classifier performance as defined as:
EU(qj) =
K?
k=1
P (qj = ck)U(qj = ck), (9)
where K is the class number, P (qj = ck) indicates
the probability that qj , j-th query (a word or docu-
ment), belongs to the k-th class, and the U(qj = ck)
indicates the utility that qj belongs to the k-th class.
However, the choice of the utility measure is still a
challenge.
5.2 Reconstruction Error
In our matrix factorization framework, rows and
columns are treated equally in estimating the errors
of matrix factorization, and the reconstruction error
is thus a natural measure of utility. Let the current
supervision knowledge be G0, F0. To select a new
unlabeled document/word for labeling, we assume
that a good supervision should lead to a good con-
strained factorization for the document-term matrix,
X ? GSF T . If the new query qj is a word and its
label is k, then the new factorization is
G?j=k, S?j=k, F ?j=k
= argminG,S,F ?X ?GSF T ?2
+ ? trace[(G?G0)TC2(G?G0)]
+ ? trace[(F ? F0,j=k)TC1(F ? F0,j=k)]
+ ? trace[(S ? S0)T (S ? S0)],
(10)
where F0,j=k is same as F0 except that
F0,j=k(j, k) = 1. In other words, we obtained
a new factorization using the labeled words. Sim-
ilarly, if the new query qj is a document, then the
new factorization is
G?j=k, S?j=k, F ?j=k
= argminG,S,F ?X ?GSF T ?2
+ ? trace[(G?G0,j=k)TC2(G?G0,j=k)]
+ ? trace[(F ? F0)TC1(F ? F0)]
+ ? trace[(S ? S0)T (S ? S0)],
(11)
where G0,j=k is same as G0 except that
G0,j=k(j, k) = 1. In other words, we obtained
a new factorization using the labeled documents.
Then the new reconstruction error is
RE(qj = k) = ?X ?G?j=kS?j=kF ?j=k?2. (12)
So the expected utility of a document or word label
query, qj , can be computed as
EU(qj) =
K?
k=1
P (qj = k)? (?RE(qj = k)). (13)
To calculate the P (qj = k), which is the posterior
distribution for words or documents, probabilistic
interpretation of Tri-NMF is abused. When a query
qj is a word, P (qj = k) is
P (zw = k|w = wi)
? P (w = wi|zw = k)
?K
j=1 P (zw = k, zd = j)
= Fik ?
?K
j=1 Skj , (14)
otherwise,
P (zd = k|d = di)
? P (d = di|zd = k)
?K
j=1 P (zw = j, zd = k)
= Gik ?
?K
j=1 Sjk. (15)
953
5.3 Algorithm Description
Computational Improvement: It can be computa-
tionally intensive if the reconstruction error is com-
puted for all unknown documents and words. In-
spired by (Attenberg et al, 2010), we first select the
top 100 unknown words that the current model is
most certain about, and the top 100 unknown docu-
ments that the current model is most uncertain about.
Then we identify the words or documents in this
pool with the highest expected utility (reconstruc-
tion error). Equations 14 and 15 are used to perform
the initial selection of top 100 unknown words and
top 100 unknown documents.
Algorithm 1 Active Dual Supervision Algorithm
Based on Matrix Factorization
INPUT: X , document-word matrix; F0, current la-
beled words; G0, current labeled documents; O, the
oracle
OUTPUT: G, classification result for all documents
in X
1. Get base factorization of X: G,S, F .
2. Active dual supervision
repeat
D is the set of top 100 unlabeled documents
with most uncertainty;
W is the set of top 100 unlabeled words with
most certainty;
Q = D ?W ;
for all q ? Q do
for k = 1 to K do
Get G?q=k, F ?q=k, S?q=k by Equation 10 or
Equation 11 according to whether the
query q is a document or a word;
Calculate EU(q) by Equation 13;
q? = argmaxq EU(q);
Acquire new label of q?, l from O;
G,F, S = G?q?=l, F ?q?=l, S?q?=l;until stop criterion is met.
The overall algorithm procedure is described in
Algorithm 1. First we iteratively use the updat-
ing rules of Equation 7 to obtain the factoriza-
tion G,F, S based on initial labeled documents and
words. Then to select a new query, for each unla-
beled document or word in the pool and for each
possible class, we compute the reconstruction error
with new supervision (using the current factoriza-
tion results as initialization values). It is efficient to
compute a new factorization due to the sparsity of
the matrices. The document-term matrix is typically
very sparse with z  nm non-zero entries while k is
typically also much smaller than document number
n, and word numberm. By using sparse matrix mul-
tiplications and avoiding dense intermediate matri-
ces, updating F, S,G each takesO(k2(m+n)+kz)
time per iteration which scales linearly with the di-
mensions and density of the data matrix (Li et al,
2009). Empirically, the number of iterations that is
needed to compute the new factorization is usually
very small (less than 10).
6 Experiments
6.1 Experiments Settings
Three popular binary text classification datasets are
used in the experiments: ibm-mac (1937 examples),
baseball-hockey (1988 examples) and med-space
(1972 examples) datasets. All of them are drawn
from the 20-newsgroups text collection1 where the
task is to assign messages into the newsgroup in
which they appeared. Top 1500 frequent words in
each dataset are used as features in the binary vec-
tor representation. These datasets have labels for all
the documents. For a document query, the oracle re-
turns its label. We construct the word oracle in the
same manner as in (Sindhwani et al, 2009): first
compute the information gain of words with respect
to the known true class labels in the training splits of
a dataset, and then the top 100 words as ranked by
information gain are assigned the label which is the
class in which the word appears more frequently. To
those words with labels, the word oracle returns its
label; otherwise, the oracle returns a ?don?t know?
response (no word label is obtained for learning, but
the word is excluded from the following query se-
lection).
Results are averaged over 10 random training-
test splits. For each split, 30% examples are used
for testing. All methods are initialized by a ran-
dom choice of 10 document labels and 10 word la-
bels. For simplicity, we follow the widely used cost
model (Raghavan and Allan, 2007; Druck et al,
1http://www.ai.mit.edu/people/jrennie/
20_newsgroups/
954
2008; Sindhwani et al, 2009) where features are
roughly 5 times cheaper to label than examples, so
we assume the cost is 1 for a word query and is 5 for
a document query. We set ? = ? = 5, ? = 1 for all
the following experiments2.
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
10-10
20-15
30-20
40-25
50-30
400-50
500-60
600-70
700-80
800-90
Ac
cu
ra
cy
#labeled documents-#labeled words
w/o. constraint on S
w/. constraint on S
(a) baseball-hockey
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
10-10
20-15
30-20
40-25
50-30
400-50
500-60
600-70
700-80
800-90
Ac
cu
ra
cy
#labeled documents-#labeled words
w/o. constraint on S
w/. constraint on S
(b) ibm-mac
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
10-10
20-15
30-20
40-25
50-30
400-50
500-60
600-70
700-80
800-90
Ac
cu
ra
cy
#labeled documents-#labeled words
w/o. constraint on S
w/. constraint on S
(c) med-space
Figure 1: Comparing the performance of dual supervision
via Tri-NMF w/ and w/o the constraint on S.
2We do not perform fine tuning on the parameters since the
main objective of the paper is to demonstrate the effectiveness
of matrix factorization based methods for dual active supervi-
sion. A vigorous investigation on the parameter choices is our
further work.
6.2 Experimental Results
Effect of Constraints on S in Constrained Tri-
NMF Figure 1 demonstrates the effectiveness of
dual supervision with explicit class alignment via
Tri-NMF as described in Section 4. When there
are enough labeled documents and words, the con-
straints on S have a relative small impact on the per-
formance of dual supervision. However, in the be-
ginning phase of active learning, the labeled dataset
can be small (such as 10 labeled documents and 10
labeled words). In this case, without the constraint
of S, the matrix factorization may generate incorrect
class alignment, thus lead to almost random classi-
fication results (around 50% accuracy), as shown in
Figure 1, and further make unreasonable the follow-
ing evaluation of queries.
Comparing Query Selection Approaches Figure
2 compares our proposed unified scheme (denoted as
Expected-reconstruction-error) with the following
baselines using Tri-NMF as the classifier for dual
supervision: (1). Interleaved-uncertainty which
first selects feature query by certainty and sample
query by uncertainty and then combines the two
types of queries using an interleaving scheme. The
interleaving probability (probability to select the
query as a document) is set as 0.2, 0.4, 0.6 and
0.8. (2). Expected-log-gain which selects feature
and sample query by maximizing the expected log
gain. Expected-reconstruction-error outperforms
interleaving schemes with all the different interleav-
ing probability values with which we experimented.
It also has a better performance than Expected-log-
gain. Although log gain is a finer-grained utility
measure of classifier performance than accuracy and
has a good performance in the setting with a large set
of starting labeled documents (e.g., 100 documents),
it is not reliable especially in the setting with a small
set of labeled data. Different from the Expected-log-
gain, Expected-reconstruction-error estimates the
utility using the matrix reconstruction error, making
use of information of all documents and words, in-
cluding those unlabeled.
Comparing Interleaving Scheme vs. the Uni-
fied Scheme To further demonstrate the benefit
of the proposed unified scheme , we compare it
with its interleaved version: Interleaved-expected-
955
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Expected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error
(a) baseball-hockey
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Expected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error
(b) ibm-mac
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Expected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error
(c) med-space
Figure 2: Comparing the different query selection approaches in active learning via Tri-NMF with dual supervision.
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Interleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error
(a) baseball-hockey
 0.68
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Interleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error
(b) ibm-mac
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Interleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error
(c) med-space
Figure 3: Comparing the unified and interleaving scheme based on reconstruction error.
construction-error which computes the utility of a
query using the reconstruction error, but uses inter-
leaving scheme to decide which type of query to
select. We experiment with different interleaving
probability values ranging from 0.2 to 0.8, which
lead to quite different performance results. From
Figure 3, the optimal interleaving probability value
varies on different datasets. For example, the proba-
bility value of 0.8 is among the optimal interleaving
probability values on baseball-hockey dataset but
performs poorly on ibm-mac dataset. This obser-
vation also illustrates the need for a unified scheme,
because of the difficulty in choosing the optimal in-
terleaving probability value. Although the proposed
unified scheme is not significantly better than its in-
terleaving counterparts for all interleaving probabil-
ity values on all datasets, it avoids the bad choices.
Figure 5 presents the sequence of different query
types selected by our unified scheme and it clearly
demonstrates the distribution patterns of different
query types. At the beginning phase of active learn-
ing, word queries have much higher probabilities to
be selected, which is consistent with the result of
previous work: feature labels can be more effec-
tive than examples in text classification (Druck et
 50  100  150  200  250  300
Qu
ery
 Ty
pe
Query Sequence
Word
Document
(a) baseball-hockey
 50  100  150  200  250  300
Qu
ery
 Ty
pe
Query Sequence
Word
Document
(b) ibm-mac
Figure 5: Example of query sequence.
al., 2008). And in the later learning phase, docu-
ments are more likely to be selected, since the num-
ber of words that can benefit the classification is
much smaller than the effective documents.
Reconstruction Error vs. Interleaving uncer-
tainty using GRADS It should be pointed out that
our unified scheme for query selection based on re-
construction error does not rely on the estimation
of model performance on training data and can be
easily integrated with other dual supervision mod-
956
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
GRADS-Interleaving-0.5GRADS-Reconstruction-Error
(a) baseball-hockey
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
GRADS-Interleaving-0.5GRADS-Reconstruction-Error
(b) ibm-mac
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
GRADS-Interleaving-0.5GRADS-Reconstruction-Error
(c) med-space
Figure 4: GRADS with reconstruction error and interleaving uncertainty.
els such as GRADS (Sindhwani et al, 2008). Fig-
ure 4 shows the comparison of GRADS using the
interleaved scheme with an interleaving probability
of 0.5, and using our unified scheme based on recon-
struction error. Among the 3 datasets we used, the
reconstruction error based approach outperforms the
interleaving scheme on baseball-hockey and ibm-
mac, and has similar performance with the interleav-
ing scheme on med-space.
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  100  200  300  400  500  600  700  800
Ac
cu
ra
cy
Labeling Cost
GRADS-Interleaving-0.2
GRADS-Interleaving-0.4
GRADS-Interleaving-0.6
GRADS-Interleaving-0.8
Tri-NMF-Reconstruction-Error
Figure 6: Comparing active dual supervision using ma-
trix factorization with GRADS on sentiment analysis.
Comparing Active Dual Supervision Using Ma-
trix Factorization with GRADS on Sentiment
Analysis The sentiment analysis experiment is
conducted on the movies review dataset (Pang et al,
2002), containing 1000 positive and 1000 negative
movie reviews. The results are shown in Figure 6.
The experimental results clearly demonstrate the ef-
fectiveness of our approach, denoted as Tri-NMF-
Reconstruction-Error.
7 Conclusions
In this paper, we study the problem of active dual
supervision, and propose a matrix tri-factorization
based approach to address the issue, how to evaluate
labeling benifit of different types of queries (exam-
ples or features) in the same scale. Following ex-
tending the nonnegative matrix tri-factorization to
the active dual supervision setting, we use the recon-
struction error to evaluate the value of feature and
example labels. Experimental results show that our
proposed approach outperforms existing methods.
Acknowledgement
The work is partially supported by NSF grants
DMS-0915110, CCF-0830659, and HRD-0833093.
We would like to thank Dr. Vikas Sindhwani for
his insightful discussions and for sharing us with his
GRADS code.
References
J. Attenberg, P. Melville, and F. Provost. 2010. A Uni-
fied Approach to Active Dual Supervision for Label-
ing Features and Examples. Machine Learning and
Knowledge Discovery in Databases, pages 40?55.
D. Cohn, L. Atlas, and R. Ladner. 1994. Improving gen-
eralization with active learning. Machine Learning,
15(2):201?221.
I.S. Dhillon. 2001. Co-clustering documents and words
using bipartite spectral graph partitioning. In Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 269?274. ACM.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogonal
nonnegative matrix t-factorizations for clustering. In
Proceedings of the 12th ACM SIGKDD international
957
conference on Knowledge discovery and data mining,
pages 126?135. ACM.
C. Ding, T. Li, and W. Peng. 2008. On the equiva-
lence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the 31st annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 595?602.
ACM.
G. Druck, B. Settles, and A. McCallum. 2009. Active
learning by labeling features. In Proceedings of the
2009 conference on Empirical methods in natural lan-
guage processing, pages 81?90. Association for Com-
putational Linguistics.
S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.
2004. Document classification through interactive su-
pervision of document and term labels. Knowledge
Discovery in Databases: PKDD 2004, pages 185?196.
D.D. Lee and H.S. Seung. 2001. Algorithms for non-
negative matrix factorization. Advances in neural in-
formation processing systems, 13.
T. Li, Y. Zhang, and V. Sindhwani. 2009. A non-negative
matrix tri-factorization approach to sentiment classifi-
cation with lexical prior knowledge. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the
ACL, pages 244?252. Association for Computational
Linguistics.
A.K. McCallum and K. Nigam. 1998. Employing EM
and pool-based active learning for text classification.
In Proceedings of the Fifteenth International Confer-
ence on Machine Learning. Citeseer.
P. Melville and V. Sindhwani. 2009. Active dual su-
pervision: Reducing the cost of annotating examples
and features. In Proceedings of the NAACL HLT 2009
Workshop on Active Learning for Natural Language
Processing, pages 49?57. Association for Computa-
tional Linguistics.
P. Melville, M. Saar-Tsechansky, F. Provost, and
R. Mooney. 2005. An expected utility approach to
active feature-value acquisition. In Proceedings of
Fifth IEEE International Conference on Data Mining.
IEEE.
P. Melville, W. Gryc, and R.D. Lawrence. 2009. Senti-
ment analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1275?
1284. ACM.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the 2002 conference
on Empirical methods in natural language processing,
pages 79?86. Association for Computational Linguis-
tics.
H. Raghavan and J. Allan. 2007. An interactive algo-
rithm for asking and incorporating feature feedback
into support vector machines. In Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 79?86. ACM.
H. Raghavan, O. Madani, and R. Jones. 2006. Active
learning with feedback on features and instances. The
Journal of Machine Learning Research, 7:1655?1686.
T. Sandler, P.P. Talukdar, L.H. Ungar, and J. Blitzer.
2008. Regularized learning with networks of features.
Advances in Neural Information Processing Systems,
pages 1401?1408.
B. Settles. 2009. Active Learning Literature Survey.
Technical Report 1648.
C. Shen, T. Li, and C. Ding. 2011. Integrating Clustering
and Multi-Document Summarization by Bi-mixture
Probabilistic Latent Semantic Analysis (PLSA) with
Sentence Bases. In Proceedings of the national con-
ference on Artificial intelligence. AAAI Press.
V. Sindhwani and P. Melville. 2008. Document-word
co-regularization for semi-supervised sentiment anal-
ysis. In Data Mining, Eighth IEEE International Con-
ference on, pages 1025?1030. IEEE.
V. Sindhwani, J. Hu, and A. Mojsilovic. 2008. Regular-
ized co-clustering with dual supervision. Advances in
Neural Information Processing Systems, 21.
V. Sindhwani, P. Melville, and R.D. Lawrence. 2009.
Uncertainty sampling and transductive experimental
design for active dual supervision. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 953?960. ACM.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
The Journal of Machine Learning Research, 2:45?66.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: A generative approach to learning from annota-
tor rationales. In Proceedings of the 2008 conference
on Empirical methods in natural language processing,
pages 31?40. Association for Computational Linguis-
tics, October.
H. Zha, X. He, C. Ding, H. Simon, and M. Gu. 2001. Bi-
partite graph partitioning and data clustering. In Pro-
ceedings of the tenth international conference on In-
formation and knowledge management, pages 25?32.
ACM.
958
Proceedings of NAACL-HLT 2013, pages 1152?1162,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Participant-based Approach for Event Summarization Using
Twitter Streams
Chao Shen1, Fei Liu2, Fuliang Weng2, Tao Li1
1School of Computing and Information Sciences, Florida International University
Miami, Florida 33199, USA
2Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{cshen001, taoli}@cs.fiu.edu
{fei.liu, fuliang.weng}@us.bosch.com
Abstract
Twitter offers an unprecedented advantage on
live reporting of the events happening around
the world. However, summarizing the Twit-
ter event has been a challenging task that was
not fully explored in the past. In this paper,
we propose a participant-based event summa-
rization approach that ?zooms-in? the Twit-
ter event streams to the participant level, de-
tects the important sub-events associated with
each participant using a novel mixture model
that combines the ?burstiness? and ?cohesive-
ness? properties of the event tweets, and gen-
erates the event summaries progressively. We
evaluate the proposed approach on different
event types. Results show that the participant-
based approach can effectively capture the
sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events,
yielding summaries with considerably better
coverage than the state-of-the-art.
1 Introduction
Twitter has increasingly become a critical source of
information. People report the events they are ex-
periencing or publish comments on a wide variety
of events happening around the world, ranging from
the unexpected natural disasters, regional riots, to
many scheduled events, such as sports games, po-
litical debates, local festivals, and even academic
conferences. The Twitter data streams thus cover
a broad range of events and broadcast these in-
formation in a live manner. Event summarization
in this paper aims to generate a representative and
concise textual description of the scheduled events
that are being lively reported on Twitter, providing
people with an alternative means of observing the
world beyond the traditional journalism. Specifi-
cally, we investigate scheduled events of different
types, including six of the NBA (National Basket-
ball Association) sports games and a representative
conference event, namely the Apple CEO?s keynote
speech in the Apple Worldwide Developers Confer-
ence (WWDC 2012)1. All these events have excited
great discussion among the Twitter community.
Summarizing the Twitter event is a challenging
task that has yet been fully explored in the past.
Most previous summarization studies focus on the
well-formatted news documents, as driven by the
annual DUC2 and TAC3 evaluations. In contrast,
the Twitter messages (a.k.a., tweets) are very short
and noisy, containing nonstandard terms such as ab-
breviations, acronyms, emoticons, etc. (Liu et al,
2011b; Liu et al, 2012; Eisenstein, 2013). The
noisy contents also cause great difficulties to the tra-
ditional NLP tools such as NER and dependency
parser (Ritter et al, 2011; Foster et al, 2011), lim-
iting the possibility of applying finer-grained event
analysis tools. In nature, the event tweets are closely
associated with the timeline and are drastically dif-
ferent from a static collection of news documents.
The tweets converge into text streams that pulse
along the timeline and cluster around the important
moments or sub-events. These ?sub-events? are of
crucial importance since they represent a surge of in-
terest from the Twitter audience and the correspond-
1https://developer.apple.com/wwdc/
2http://duc.nist.gov/
3http://www.nist.gov/tac/
1152
Figure 1: Example Twitter event stream (upper) and par-
ticipant stream (lower). Event stream contains tweets
related to an NBA basketball game (Spurs vs Thunder)
scheduled on May 31, 2012; participant stream contains
tweets corresponding to the player Russell Westbrook in
team Thunder. X-axis denotes the timeline and y-axis
represents the number of tweets per 10-second interval.
ing key information must be reflected in the event
summary. As such, event summarization research
has been focusing on developing accurate sub-event
detection systems and generating text descriptions
that can best summarize the sub-events in a progres-
sive manner (Chakrabarti and Punera, 2011; Nichols
et al, 2012; Zubiaga et al, 2012).
In Figure 1, we show an example Twitter event
stream and one of its ?participant? streams. The
event stream contains all the tweets related to an
NBA basketball game Spurs vs Thunder; while
the participant stream contains only tweets corre-
sponding to the player Russell Westbrook in this
game. Previous research on event summarization
focuses on identifying the important moments from
the coarse-level event stream. This may yield sev-
eral side effects: first, the spike patterns are not
clearly identifiable from the overall event stream,
though they are more clearly seen if we ?zoom-in? to
the participant level; second, it is arguable whether
the important sub-events can be accurately detected
based solely on the tweet volume change; third, a
popular participant or sub-event can elicit huge vol-
ume of tweets which dominant the event discussion
and shield less prominent sub-events. For example,
in the NBA games, discussions about the key players
(e.g., ?LeBron James?, ?Kobe Bryant?) can heavily
shadow other important participants or sub-events,
resulting in an event summary with repetitive de-
scriptions about the dominant players.
In this work, we propose a novel participant-
based event summarization approach, which dynam-
ically identifies the participants from data streams,
then ?zooms-in? the event stream to participant
level, detects the important sub-events related to
each participant using a novel time-content mixture
model, and generates the event summary progres-
sively by concatenating the descriptions of the im-
portant sub-events. Results show that the mixture
model-based sub-event detection approach can effi-
ciently incorporate the ?burstiness? and ?cohesive-
ness? of the participant streams, and the participant-
based event summarization can effectively capture
the sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events, yield-
ing summaries with considerably better coverage
than the state-of-the-art approach.
2 Related Work
Mining Twitter for event information has received
increasing attention in recent years. Many research
studies focus on identifying the trending events from
Twitter and providing a concise and dynamic visual-
ization of the information. The identified events are
often represented using a set of keywords. (Petro-
vic et al, 2010) proposed an algorithm based on
locality-sensitive hashing for detecting new events
from a stream of Twitter posts. (O?Connor et al,
2010; Becker et al, 2011b; Becker et al, 2011a;
Weng et al, 2011) proposed demo systems to dis-
play the event-related themes and popular tweets,
allowing the users to navigate through their topic
of interest. (Zhao et al, 2011) described an effort
to perform data collection and event recognition de-
spite various limits to the free access of Twitter data.
(Diao et al, 2012) integrated both temporal infor-
mation and users? personal interests for bursty topic
detection from the microblogs. (Ritter et al, 2012)
described an open-domain event-extraction and cat-
egorization system, which extracts an open-domain
calendar of significant events from Twitter.
With the identified events of interest, there is an
ever-increasing demand for event summarization,
which distills the huge volume of Twitter discus-
sions into a concise and representative textual de-
scription of the events. Many studies start with
the text summarization approaches that have been
shown to perform well on the news documents and
1153
develop adaptations to fit these methods to a col-
lection of event tweets. (Sharifi et al, 2010b) pro-
posed a graph-based phrase reinforcement algorithm
to build a one-sentence summary from a collection
of topic tweets. (Sharifi et al, 2010a; Inouye and
Kalita, 2011) presented a hybrid TF-IDF approach
to extract one- or multiple-sentence summary for
each topic. (Liu et al, 2011a) proposed to use
the concept-based ILP framework for summarizing
the Twitter trending topics, using both tweets and
the webpages linked from the tweets as input text
sources. (Harabagiu and Hickl, 2011) introduced a
generative framework that incorporates event struc-
ture and user behavior information in summarizing
multiple microblog posts related to the same topic.
Regarding summarizing the data streams, (Mar-
cus et al, 2011) introduced a ?TwitInfo? system to
visually summarize and track the events on Twit-
ter. They proposed an automatic peak detection and
labeling algorithm for the social streams. (Taka-
mura et al, 2011) proposed a summarization model
based on the facility location problem, which gener-
ates summary for a stream of short documents along
the timeline. (Chakrabarti and Punera, 2011) pro-
posed an event summarization algorithm based on
learning an underlying hidden state representation
of the event via hidden Markov models. (Louis and
Newman, 2012) presented a method for summariz-
ing a collection of tweets related to a business. The
proposed procedure aggregates tweets into subtopic
clusters which are then ranked and summarized
by a few representative tweets from each cluster.
(Nichols et al, 2012; Zubiaga et al, 2012) focused
on real-time event summarization, which detects the
sub-events by identifying those moments where the
tweet volume has increases sharply, then uses var-
ious weighting schemes to perform tweet selection
and finally generates the event summary.
Our work is different from the above research
studies in three folds: first, we propose to ?zoom-
in? the Twitter event streams to the participant
level, which allows us to clearly identify the im-
portant sub-events associated with each participant
and generate a balanced event summary with com-
prehensive coverage of all the important sub-events;
second, we propose a novel time-content mixture
model approach for sub-event detection, which ef-
fectively leverages the ?burstiness? and ?cohesive-
ness? of the event tweets and accurately detects
the participant-level sub-events. Third, we evalu-
ate the participant-based event summarization sys-
tem on different event types and demonstrate that the
proposed approach outperforms the state-of-the-art
method by a considerable margin.
3 Participant-based Event Summarization
We propose a novel participant-centered event sum-
marization approach that consists of three key com-
ponents: (1) ?Participant Detection? dynamically
identifies the event participants and divides the
entire event stream into a number of participant
streams (Section 3.1); (2) ?Sub-event Detection? in-
troduces a novel time-content mixture model ap-
proach to identify the important sub-events associ-
ated with each participant; these ?participant-level
sub-events? are then merged along the timeline to
form a set of ?global sub-events?4, which capture
all the important moments in the event stream (Sec-
tion 3.2); (3) ?Summary Tweet Extraction? extracts
the representative tweets from the global sub-events
and forms a comprehensive coverage of the event
progress (Section 3.3).
3.1 Participant Detection
We define event participants as the entities that play
a significant role in shaping the event progress. ?Par-
ticipant? is a general concept to denote the event
participating persons, organizations, product lines,
etc., each of which can be captured by a set of
correlated proper nouns. For example, the NBA
player ?LeBron Raymone James? can be represented
by {LeBron James, LeBron, LBJ, King James, L.
James}, where each proper noun represents a unique
mention of the participant. In this work, we automat-
ically identify the proper nouns from tweet streams,
filter out the infrequent ones using a threshold ?,
and cluster them into individual event participants.
This process allows us to dynamically identify the
key participating entities and provide a full-coverage
for these participants in the event summary.
4We use ?participant sub-events? and ?global sub-events?
respectively to represent the important moments happened on
the participant-level and on the entire event-level. A ?global
sub-event? may consist of one or more ?participant sub-events?.
For example., the ?steal? action in the basketball game typically
involves both the defensive and offensive players, and can be
generated by merging the two participant-level sub-events.
1154
We formulate the participant detection in a hier-
archical agglomerative clustering framework. The
CMU TweetNLP tool (Gimpel et al, 2011) was used
for proper noun tagging. The proper nouns (a.k.a.,
mentions) are grouped into clusters in a bottom-up
fashion. Two mentions are considered similar if they
share (1) lexical resemblance, and (2) contextual
similarity. For example, in the following two tweets
?Gotta respect Anthony Davis, still rocking the uni-
brow?, ?Anthony gotta do something about that uni-
brow?, the two mentions Anthony Davis and An-
thony are referring to the same participant and they
share both character overlap (?anthony?) and con-
text words (?unibrow?, ?gotta?). We use sim(ci, cj)
to represent the similarity between two mentions ci
and cj , defined as:
sim(ci, cj) = lex sim(ci, cj)?cont sim(ci, cj)
where the lexical similarity (lex sim(?)) is defined
as a binary function representing whether a mention
ci is an abbreviation, acronym, or part of another
mention cj , or if the character edit distance between
the two mentions is less than a threshold ?5:
lex sim(ci, cj)=
?
?
?
1 ci(cj) is part of cj(ci)
1 EditDist(ci, cj) < ?
0 Otherwise
We define the context similarity (cont sim(?)) of
two mentions as the cosine similarity between their
context vectors ~vi and ~vj . Note that on the tweet
stream, two temporally distant tweets can be very
different even though they are lexically similar, e.g.,
two slam dunk shots performed by the same player
at different time points are different. We there-
fore restrain the context to a segment of the tweet
stream |Sk| and then take the weighted average of
the segment-based similarity as the final context
similarity. To build the context vector, we use term
frequency (TF) as the term weight and remove all the
stopwords. We use |D| to represent the total tweets
in the event stream.
cont sim|Sk|(ci, cj) = cos(~vi, ~vj)
cont sim(ci, cj) =
?
k
|Sk|
|D|
? cont sim|Sk|(ci, cj)
5? was empirically set as 0.2?min{|ci|, |cj |}
t w
Wz? |D|
? ? ? ?'K B
Figure 2: Plate notation of the mixture model.
Similarity between two clusters of mentions are de-
fined as the maximum possible similarity between a
pair of mentions, each from one cluster:
sim(Ci, Cj) = max
ci?Ci,cj?Cj
sim(ci, cj)
We perform bottom-up agglomerative clustering on
the mentions until a stopping threshold ? has been
reached for sim(Ci, Cj). The clustering approach
naturally groups the frequent proper nouns into par-
ticipants. The participant streams are then formed
by gathering the tweets that contain one or more
mentions in the participant cluster.
3.2 Mixture Model-based Sub-event Detection
A sub-event corresponds to a topic that emerges
from the data stream, being intensively discussed
during a short period, and then gradually fades away.
The tweets corresponding to a sub-event thus de-
mand not only ?temporal burstiness? but also a cer-
tain degree of ?lexical cohesiveness?. To incorporate
both the time and content aspects of the sub-events,
we propose a mixture model approach for sub-event
detection. Figure 2 shows the plate notation.
In the proposed model, each tweet d in the data
stream D is generated from a topic z, weighted by
piz . Each topic is characterized by both its content
and time aspects. The content aspect is captured by
a multinomial distribution over the words, param-
eterized by ?; while the time aspect is character-
ized by a Gaussian distribution, parameterized by ?
and ?, with ? represents the average time point that
the sub-event emerges and ? determines the duration
of the sub-event. These distributions bear similari-
ties with the previous work (Hofmann, 1999; Allan,
2002; Haghighi and Vanderwende, 2009). In addi-
tion, there are often background or ?noise? topics
that are being constantly discussed over the entire
1155
event evolvement process and do not present the de-
sired ?burstiness? property. We use a uniform dis-
tribution U(tb, te) to model the time aspect of these
?background? topics, with tb and te being the event
beginning and end time points. The content aspect
of a background topic is modeled by similar multi-
nomial distribution, parameterized by ??. We use the
maximum likelihood parameter estimation. The data
likelihood can be represented as:
L(D) =
?
d?D
?
z
{pizpz(td)
?
w?d
pz(w)}
where pz(td) models the timestamp of tweet d under
the topic z; pz(w) corresponds to the word distribu-
tion in topic z. They are defined as:
pz(td) =
{
N(td;?z, ?z) if z is a sub-event topic
U(tb, te) if z is background topic
pz(w) =
{
p(w; ?z) if z is a sub-event topic
p(w; ??z) if z is background topic
where both p(w; ?z) and p(w; ??z) are multinomial
distributions over the words. Initially, we assume
there are K sub-event topics and B background top-
ics and use the EM algorithm for model fitting. The
EM equations are listed below:
E-step:
p(zd = j) ?
?
?
?
pijN(d;?j , ?j)
?
w?d
p(w; ?j) if j <= K
pijU(tb, te)
?
w?d
p(w; ??j) else
M-step:
pij ?
?
d
p(zd = j)
p(w; ?j) ?
?
d
p(zd = j)? c(w, d)
p(w; ??j) ?
?
d
p(zd = j)? c(w, d)
?j =
?
d p(zd = j)? td
?K
j=1
?
d p(zd = j)
?2j =
?
d p(zd = j)? (td ? ?j)
2
?K
j=1
?
d p(zd = j)
To process the data stream D, we divide the data
into 10-second bins and process each bin at a time.
The peak time of a sub-event was determined as
the bin that has the most tweets related to this sub-
event. During EM initialization, the number of sub-
event topics K was empirically decided by scanning
through the data stream and examine tweets in ev-
ery 3-minute stream segment. If there was a spike6,
we add a new sub-event to the model and use the
tweets in this segment to initialize the value of ?,
?, and ?. Initially, we use a fixed number of back-
ground topics with B = 4. A topic re-adjustment
was performed after the EM process. We merge two
sub-events in a data stream if they (1) locate closely
in the timeline, with peaks times within a 2-minute
window; and (2) share similar word distributions:
among the top-10 words with highest probability in
the word distributions, there are over 5 words over-
lap. We also convert the sub-event topics to back-
ground topics if their ? values are greater than a
threshold ?7. We then re-run the EM to obtain the
updated parameters. The topic re-adjustment pro-
cess continues until the number of sub-events and
background topics do not change further.
We obtain the ?participant sub-events? by ap-
plying this sub-event detection approach to each of
the participant streams. The ?global sub-events?
are obtained by merging the participant sub-events
along the timeline. We merge two participant sub-
events into a global sub-event if (1) their peaks are
within a 2-minute window, and (2) the Jaccard simi-
larity (Lee, 1999) between their associated tweets is
greater than a threshold (set to 0.1 empirically). The
tweets associated with each global sub-event are the
ones with p(z|d) greater than a threshold ?, where z
is one of the participant sub-events and ? was set to
0.7 empirically. After the sub-event detection pro-
cess, we obtain a set of global sub-events and their
associated event tweets.8
3.3 Summary Tweet Extraction
We extract a representative tweet from each of the
global sub-events and concatenate them to form an
informative event summary. Note that our goal in
this work is to identify all the important moments
6We use the algorithm described in (Marcus et al, 2011) as
a baseline and ad hoc spike detection algorithm.
7? was set to 5 minutes in our experiments.
8We empirically set some threshold values in the topic re-
adjustment and sub-event merging process. In future, we would
like to explore more principled way of parameter selection.
1156
Event Date Duration #Tweets
Lakers vs Okc 05/19/2012 3h10m 218,313
N Celtics vs 76ers 05/23/2012 3h30m 245,734
B Celtics vs Heat 05/30/2012 3h30m 345,335
A Spurs vs Okc 05/31/2012 3h 254,670
Heat vs Okc (1) 06/12/2012 3h30m 331,498
Heat vs Okc (2) 06/21/2012 3h30m 332,223
Apple?s WWDC?12 Conf. 06/11/2012 3h30m 163,775
Table 1: Statistics of the data set, including six NBA bas-
ketball games and the WWDC 2012 conference event.
for event summarization, but not on proposing new
methods for tweet selection. We thus use the Hybrid
TF-IDF approach (Sharifi et al, 2010a; Liu et al,
2011a) to extract the representative sentences from
a collection of tweets. In this approach, each tweet
was considered as a sentence. The sentences were
ranked according to the average TF-IDF score of the
consisting words; top weighted sentences were it-
eratively extracted, while excluding those that have
high cosine similarity with the existing summary
sentences. (Inouye and Kalita, 2011) showed the
Hybrid TF-IDF approach performs constantly better
than the phrase reinforcement algorithm and other
traditional summarization systems.
4 Data Corpus
We evaluate the proposed event summarization ap-
proach on six NBA basketball games and a repre-
sentative conference event, namely the Apple CEO?s
keynote speech in the Apple Worldwide Develop-
ers Conference (WWDC 2012)9. We use the het-
erogeneous event types to verify that the proposed
approach can robustly and efficiently produce sum-
maries on different event streams. The tweet streams
corresponding to these events are collected using
the Twitter Streaming API10 with pre-defined key-
word set. For NBA games, we use the team names,
first name and last name of the players and head
coaches as keywords for retrieving the event tweets;
for the WWDC conference, the keyword set contains
about 20 terms related to the Apple event, such as
?wwdc?, ?apple?, ?mac?, etc. We crawl the tweets
in real-time when these scheduled events are taking
place; nevertheless, certain non-event tweets could
be mis-included due to the broad coverage of the
used keywords. During preprocessing, we filter out
9https://developer.apple.com/wwdc/
10https://dev.twitter.com/docs/streaming-apis
Time Action (Sub-event) Score
9:22 Chris Bosh misses 10-foot two point shot 7-2
9:22 Serge Ibaka defensive rebound 7-2
9:11 Kevin Durant makes 15-foot two point shot 9-2
8:55 Serge Ibaka shooting foul (Shane Battier draws 9-2
the foul)
8:55 Shane Battier misses free throw 1 of 2 9-2
8:55 Miami offensive team rebound 9-2
8:55 Shane Battier makes free throw 2 of 2 9-3
Table 2: An example clip of the play-by-play live cov-
erage of an NBA game (Heat vs Okc). ?Time? corre-
sponds to the minutes left in the current quarter of the
game; ?Score? shows the score between the two teams.
the tweets containing URLs, non-English tweets,
and retweets since they are less likely containing
new information regarding the event progress. Ta-
ble 1 shows statistics of the event tweets after the
filtering process. In total, there are over 1.8 million
tweets used in the event summarization experiments.
We use the play-by-play live coverage collected
from the ESPN11 and MacRumors12 websites as ref-
erence, which provide detailed descriptions of the
NBA and WWDC events as they unfold. Table 2
shows an example clip of the play-by-play descrip-
tions of an NBA game. Ideally, each item in the live
coverage descriptions may correspond to a sub-event
in the tweet streams, but in reality, not all actions
would attract enough attention from the Twitter au-
dience. We use a human annotator to manually filter
out the actions that did not lead to any spike in the
corresponding participant stream. The rest items are
projected to the participant and event streams as the
goldstandard sub-events. The projection was man-
ually performed since the ?game clock? associated
with the goldstandard (first column in Table 2) does
not align well with the ?wall clock? due to the game
rules such as timeout and halftime rest. To evalu-
ate the participant detection performance, we ask the
annotator to manually group the proper noun men-
tions into clusters, each cluster corresponds to a par-
ticipant. The mentions that do not correspond to any
participant are discarded. The goldstandard event
summaries are generated by manually selecting one
representative tweet from each of the groundtruth
global sub-events. We choose not to use the play-
by-play descriptions as reference summaries since
their vocabulary is rather limited and do not overlap
with the tweet language.
11http://espn.go.com/nba/scoreboard
12http://www.macrumorslive.com/archive/wwdc12/
1157
Example Participants - NBA game
westbrook, russell westbrook
stephen jackson, steven jackson, jackson
james, james harden, harden
ibaka, serge ibaka
oklahoma city thunder, oklahoma
gregg popovich, greg popovich, popovich
kevin durant, kd, durant
thunder, okc, #okc, okc thunder, #thunder
Example Participants - WWDC Conference
macbooks, mbp, macbook pro, macbook air,...
google maps, google, apple maps
wwdc, apple wwdc, #wwdc
os, mountain, os x mountain, os x
iphone 4s, iphone 3gs, iphone
Table 3: Example participants automatically detected
from the NBA game Spurs vs Okc (2012-5-31) and the
WWDC?12 conference.
5 Experimental Results
We evaluate the participant-based event summariza-
tion in a cascaded fashion and present results for
each of the three components, including the par-
ticipant detection (Section 5.1), sub-event detection
(Section 5.2), and quantitative and qualitative evalu-
ation of example event summaries (Section 5.3).
5.1 Participant Detection Results
In Table 3, we show example participants that were
automatically detected by the proposed hierarchical
agglomerative clustering approach. We note that the
clusters include various mentions of the same event
participant, e.g., ?gregg popovich?, ?greg popovich?,
and ?popovich? are both referring to the head coach
of the team Spurs; ?macbooks?, ?macbook pro?,
?mbp? are referring to a line of products from Apple.
Quantitatively, we evaluate the participant detection
results on both participant- and mention-level. As-
sume the system-detected and the goldstandard par-
ticipant clusters are Ts and Tg respectively. We de-
fine a correct participant as a system detected par-
ticipant with more than half of its associated men-
tions are included in a goldstandard participant (re-
ferred to as the hit participant). As a result, we
can define the participant-level precision and recall
as below:
participant-prec = #correct-participants/|Ts|
participant-recall = #hit-participants/|Tg|
Note that a correct participant may include incor-
rect mentions, and that more than one correct par-
Figure 3: Participant detection performance. The upper
figures represent the participant-level precision and re-
call scores; while the lower figures represent the mention-
level precision and recall. X-axis corresponds to the six
NBA games and the WWDC conference.
ticipants may correspond to the same hit participant,
both of which are undesired. In the latter case, we
use representative participant to refer to the cor-
rect participant which contains the most mentions
in the hit participant. In this way, we build a 1-
to-1 mapping from the detected participants to the
groundtruth participants. Next, we define correct
mentions as the union of the overlapping mentions
between all pairs of representative and hit partici-
pants. Then we calculate the mention-level precision
and recall as the number of correct mentions divided
by the total mentions in the system or goldstandard
participant clusters.
Figure 3 shows the participant- and mention-level
precision and recall scores. We experimented with
different similarity measures for the agglomerative
clustering approach13. The ?global context? means
that the context vectors are created from the entire
data stream; this may not perform well since dif-
ferent participants can share similar global context.
E.g., the terms ?shot?, ?dunk?, ?rebound? can ap-
pear in the context of any NBA players and are not
13The stopping threshold ? was set to 0.15, local context
length is 3 minutes, and frequency threshold ? was set to 200.
1158
Participant-level Sub-event Detection Global Sub-event Detection
Event
#P #S
Spike MM
#S
Spike Participant + Spike Participant + MM
R P F R P F R P F R P F R P F
Lakers vs Okc 9 65 0.75 0.31 0.44 0.71 0.39 0.50 48 0.67 0.38 0.48 0.94 0.19 0.32 0.88 0.40 0.55
Celtics vs 76ers 10 88 0.52 0.39 0.45 0.53 0.43 0.47 60 0.65 0.51 0.57 0.72 0.18 0.29 0.78 0.39 0.52
Celtics vs Heat 14 152 0.53 0.29 0.37 0.50 0.38 0.43 67 0.57 0.41 0.48 0.97 0.21 0.35 0.91 0.28 0.43
Spurs vs Okc 12 98 0.78 0.46 0.58 0.84 0.57 0.68 81 0.41 0.42 0.41 0.88 0.35 0.50 0.91 0.54 0.68
Heat vs Okc (1) 15 123 0.75 0.27 0.40 0.72 0.35 0.47 85 0.41 0.47 0.44 0.94 0.20 0.33 0.96 0.34 0.50
Heat vs okc (2) 13 153 0.74 0.36 0.48 0.76 0.43 0.55 92 0.41 0.33 0.37 0.88 0.21 0.34 0.87 0.38 0.53
WWDC?12 10 56 0.64 0.14 0.23 0.59 0.33 0.42 43 0.53 0.26 0.35 0.77 0.14 0.24 0.70 0.31 0.43
Average 12 105 0.67 0.32 0.42 0.66 0.41 0.50 68 0.52 0.40 0.44 0.87 0.21 0.34 0.86 0.38 0.52
Table 4: Sub-event detection results on both participant and the event streams. ?Spike? corresponds to the spike
detection algorithm proposed in (Marcus et al, 2011); ?MM? represents our proposed time-content mixture model
approach. ?#P? and ?#S? list the number of participants and sub-events in each event stream.
discriminative enough. We found that adding the
lexical similarity measure greatly boosted the clus-
tering performance, especially on the mention-level,
and that combining the lexical similarity with the lo-
cal context is even more helpful for some events.
We notice that two events (celtics vs 76ers and
celtics vs heat) yield relatively low precision on both
participant- and mention-level. Taking a close look
at the data, we found that these two events acciden-
tally co-occurred with other popular events, namely
the TV program ?American Idol? finale and the NBA
Draft. The keyword based data crawler thus includes
many noisy tweets in the event streams, leading to
some false participants being detected.
5.2 Sub-event Detection Results
We compare our proposed time-content mixture
model (noted as ?MM?) against the spike detection
algorithm proposed in (Marcus et al, 2011) (noted
as ?Spike?) . The spike algorithm is based on the
tweet volume change. It uses 10 seconds as a time
unit, calculates the tweet arrival rate in each unit,
and identifies the rates that are significantly higher
than the mean tweet rate. For these rate spikes, the
algorithm finds the local maximum of tweet rate and
identify a window surrounding the local maximum.
We tune the parameter of the ?Spike? approach (set
? = 4) so that it yields similar recall values as the
mixture model approach. We then apply the ?MM?
and ?Spike? approaches to both the participant and
event streams and evaluate the sub-event detection
performance. Results are shown in Table 4. A sys-
tem detected sub-event is considered to match the
goldstandard sub-event if its peak time is within a
2-minute window of the goldstandard.
We first apply the ?Spike? and ?MM? approach to
the participant streams. The participant streams on
which we cannot detect any meaningful sub-events
have been excluded, the resulting number of partic-
ipants are listed in Table 4 and denoted as ?#P?.
In general, we found the ?MM? approach can per-
form better since it inherently incorporates both the
?burstiness? and ?lexical cohesiveness? of the event
tweets, while the ?Spike? approach relies solely on
the ?burstiness? property. Note that although we di-
vide the entire event stream into participant streams,
some key participants still own huge amount of dis-
cussion and the spike patterns are not always clearly
identifiable. The time-content mixture model gains
advantages in these cases.
We apply three settings to detect global sub-
events on the data streams. ?Spike? directly ap-
plies the spike algorithm on the entire event stream;
the ?Participant + Spike? and ?Participant + MM?
approaches first perform sub-event detection on the
participant streams and then merge the detected sub-
events along the timeline to generate global sub-
events. Note that there are fewer goldstandard
sub-events (?#S?) on the global streams since each
global sub-event may correspond to one or multiple
participant-level sub-events. Because of the averag-
ing effect, spike patterns on the entire event stream
is less obvious than those on the participant streams.
As a result, few spikes have been detected on the
event stream using the ?Spike? algorithm, which
leads to low recall as compared to other participant-
based approaches. It also indicates that, by dividing
the entire event stream into participant streams, we
have a better chance of identifying the sub-events
that have otherwise been shadowed by the domi-
nant sub-events or participants. The two participant-
based methods yield similar recall but ?Participant
1159
+ Spike? yields slightly worse precision, since it is
very sensitive to the spikes on the participant-level,
leading to the rise of false alarms. The ?Participant +
MM? approach is much better in precision, which is
consistent to our findings on the participant streams.
5.3 Summarization Results
Summarization evaluation has been a longstanding
issue in the literature (Nenkova and Mckeown, 2011;
Liu and Liu, 2010). There are even less studies fo-
cusing on evaluating the event summaries generated
from data streams. Since the summary annotation
takes quite some effort, we sample a 10-minute seg-
ment from each of the seven event streams and ask
a human annotator to select representative tweets
for each segment. We then compare the system
summaries against the manual summaries using the
ROUGE-1 (Lin, 2004) metric. The quantitative re-
sults and qualitative analysis are presented in Table 5
and Table 6 respectively. Note that the ROUGE
scores are based solely on the n-gram overlap be-
tween the system and reference summaries, which
may not be the most appropriate measure for eval-
uating the Twitter event summaries. However, we
do notice that the accurate sub-event detection per-
formance can successfully translate into a gain of
the ROUGE scores. Qualitatively, the participant-
based event summarization approach focus more on
extracting tweets associated with the targeted partic-
ipants, which could lead to better text coherence.
6 Conclusion and Future Work
In this work, we made an initial attempt to gen-
erate event summaries using Twitter data streams.
We proposed a participant-based event summariza-
tion approach which ?zooms-in? the Twitter event
streams to the participant level, detects the impor-
tant sub-events associated with each participant us-
ing a novel mixture model that incorporates both the
?burstiness? and ?cohesiveness? of tweets, and gen-
erates the event summaries progressively. Results
show that the proposed approach can effectively cap-
ture the sub-events that have otherwise been shad-
owed by the long-tail of other dominant sub-events,
yielding summaries with considerably better cover-
age. Without loss of generality, we report results
on the entire event streams, though the proposed ap-
proach can well be applied in an online fashion.
Event Method R(%) P(%) F(%)
NBA Average
Spike 14.73 23.24 16.87
Participant + Spike 54.60 14.65 22.40
Participant + MM 54.36 23.06 31.53
WWDC Conf.
Spike 26.58 39.62 31.82
Participant + Spike 49.37 25.16 33.33
Participant + MM 42.77 31.73 36.07
Table 5: ROUGE-1 scores of summarization
Method Summary
Manual
Good drive for durant
Pretty shot by Duncan
Good 3 point tony parker
Nice move westbrook
Good shot Westbrook
Spike
Game 3. Spurs vs. OKC
Okc and spurs game.
Participant
+ Spike
OKLAHOMA CITY THUNDER vs san antonio
spurs!! YA
I hope okc win the series. Ill hate too see the heat
play San Antonio
we aint in San Antonio anymore.
NBA: SA 0 OKC 8, 9:11 1st.#TeamOkc
San antonio spurs for 21 consecutive win? #nba
Somebody Should Stop Tim Duncan.
Pass the damn ball Westbrook
Good 3 pointer tony parker!
Participant
+ MM
Tim Duncan shot is so precise
Tim Duncan is gettin started
Good 3 pointer tony parker!
Sefalosa guarding tony parker. Good fucking move
coach brooks
Westbrook = 2 Fast 2 Furious
Niggas steady letting Tim Duncan shoot
Westbrook mid range shot is automatic
Table 6: Example summaries for an event segment. Par-
ticipants are marked using italicized text.
There are many challenges left in this line of re-
search. Having a standardized evaluation metric for
event summaries is one of them. In the current work,
we employed ROUGE-1 for summary evaluation,
since it has been shown to correlate well with the hu-
man judgements on noisy text genres (Liu and Liu,
2010). We would like to explore other evaluation
metrics (e.g., ROUGE-2, -SU4, Pyramid (Nenkova
et al, 2007)) and the human evaluation in future.
We will also explore better ways of integrating the
sub-event detection and summarization approaches.
Acknowledgments
Part of this work was done during the first author?s
internship in Bosch Research and Technology Cen-
ter. The work is also partially supported by NSF
grants DMS-0915110 and HRD-0833093.
1160
References
James Allan. 2002. Topic detection and tracking: Event-
based information organization. Kluwer Academic
Publishers Norwell, MA, USA.
Hila Becker, Feiyang Chen, Dan Iter, Mor Naaman, and
Luis Gravano. 2011a. Automatic identification and
presentation of twitter content for planned events. In
Proceedings of the Fifth International AAAI Confer-
ence on Weblogs and Social Media (ICWSM), pages
655?656.
Hila Becker, Mor Naaman, and Luis Gravano. 2011b.
Beyond trending topics: Real-world event identifica-
tion on twitter. In Proceedings of the Fifth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM), pages 438?441.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 66?73.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 536?544.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL/HLT).
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS tagging and parsing the twitterverse. In
Proceedings of the AAAI Workshop on Analyzing Mi-
crotext, pages 20?25.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 42?47.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-Document summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL), pages 362?370.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Proceed-
ings of the Fifth International AAAI Conference on We-
blogs and Social Media (ICWSM), pages 514?517.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence (UAI).
David Inouye and Jugal K. Kalita. 2011. Compar-
ing twitter summarization algorithms for multiple post
summaries. In Proceedings of 2011 IEEE Third Inter-
national Conference on Social Computing, pages 290?
306.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
25?32.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. IEEE Transactions on Audio, Speech, and
Language Processing, 18(1):187?196.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?SXSW? trending? Exploring multiple text sources
for twitter topic summarization. In Proceedings of the
ACL Workshop on Language in Social Media (LSM),
pages 66?75.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011b. Insertion, deletion, or substitution? Normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1035?1044.
Annie Louis and Todd Newman. 2012. Summarization
of business-related tweets: A concept-based approach.
In Proceedings of the 24th International Conference
on Computational Linguistics (COLING).
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: Aggregating and visualizing
microblogs for event exploration. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, pages 227?236.
Ani Nenkova and Kathleen Mckeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval, 5(2?3):103?233.
Ani Nenkova, Rebecca Passonneau, and Kathleen Mcke-
own. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2).
1161
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In
Proceedings of the 2012 ACM Interntional Conference
on Intelligent User Interfaces (IUI), pages 189?198.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for twitter. In Proceedings of the Fourth
International AAAI Conference on Weblogs and Social
Media (ICWSM), pages 384?385.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of the 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL), pages
181?189.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1524?1534.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 1104?1112.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010a. Experiments in microblog summariza-
tion. In Proceedings of the 2010 IEEE Second Interna-
tional Conference on Social Computing, pages 49?56.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010b. Summarizing microblogs automati-
cally. In Proceedings of the 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 685?688.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. In
Proceedings of the 33rd European Conference on Ad-
vances in Information Retrieval (ECIR), pages 177?
188.
Jui-Yu Weng, Cheng-Lun Yang, Bo-Nian Chen, Yen-Kai
Wang, and Shou-De Lin. 2011. Imass: An intelli-
gent microblog analysis and summarization system. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 133?138.
Siqi Zhao, Lin Zhong, Jehan Wickramasuriya, and Venu
Vasudevan. 2011. Human as real-time sensors of so-
cial and physical events: A case study of twitter and
sports games. Technical Report TR0620-2011, Rice
University and Motorola Labs.
Arkaitz Zubiaga, Damiano Spina, Enrique Amigo?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM Conference on Hypertext
and Social Media, pages 319?320.
1162
