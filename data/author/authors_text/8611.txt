When is an Embedded MT System "Good Enough" for Filtering? 
Clare R. Voss 
Army Research Laboratory 
Adelphi, MD 20783 
voss@arl.mil 
Carol Van Ess-Dykema 
Department of Defense 
Ft. Meade, MD 
cjvanes@ afterlife.ncsc.mil 
Abstract 
This paper proposes an end-to-end process 
analysis template with replicable measures 
to evaluate the filtering performance of a 
Scan-OCR-MT system. Preliminary results 1
across three language-specific FALCon 2 
systems how that, with one exception, the 
derived measures consistently yield the 
same performance ranking: Haitian Creole 
at the low end, Arabic in the middle, and 
Spanish at the high end. 
1 The Filtering Problem 
How do people quickly determine whether a 
particular foreign language text document is 
relevant to their interest when they do not 
understand that foreign language? FALCon, our 
embedded MT system, has been designed to 
assist an English-speaking person in filtering, 
i.e., deciding which foreign language documents 
are worth having an expert translator process 
further. In this paper, we seek to determine when 
such systems are "good enough" for filtering. 
We define "filtering" to be a forced-choice 
decision-making process on individual 
documents, where each document is assigned a
single value, either a "yes, relevant" or a "no, 
irrelevant" by the system user) The singl e - 
document relevance assessment is performed 
For a more extensive report of our work, see Voss 
and Van Ess-Dykema (2000). 
2 FALCon (Forward Area Language CONverter) is a 
laptop-based embedded MT system integrated at the 
Army Research Laboratory for field use. (Fisher and 
Voss, 1997) 
3 See the report entitled "Mulfilingual Information 
Management: Current Levels and Future Abilities" 
for other definitions of filtering, available at 
http://www.cs.cmu.edu/People/ref/mlim/. 
independent of the content of other documents in
the processing collection. 
When Church and Hovy (1993) introduced the 
notion that "crummy" MT engines could be put 
to good use on tasks less-demanding than 
publication-quality translation, MT research 
efforts did not typically evaluate system 
performance in the context of specific tasks. 
(Sparck Jones and Galliers, 1996). In the last 
few years, however, the Church and Hovy 
insight has led to innovative xperiments, like 
those reported by Resnik (1997), Pomarede t al. 
(1998), and Taylor and White (1998), using 
task-based evaluation methods. Most recently, 
research on task-based evaluation has been. 
proposed within TIDES, a recent DARPA 
initiative whose goals include enabling English- 
speaking individuals to access, correlate, and 
interpret multilingual sources of information 
(DARPA, 1999; Harmon, 1999). 
This paper introduces a method of assessing 
when an embedded MT system is "good 
enough" for the filtering of hard-copy foreign 
language (FL) documents by individuals with no 
knowledge of that language. We describe 
preliminary work developing measures on 
system-internal components hat assess: (i) the 
flow of words relevant o the filtering task and 
domain through the steps of document 
processing in our embedded MT system, and (ii) 
the level of "noise," i.e., processing errors, 
passing through the system. We present an 
analysis template that displays the processing 
steps, the sequence of document versions, and 
the basic measures of our evaluation method. 
After tracing the processing of Spanish, Arabic, 
and Haitian Creole parallel texts that is recorded 
in the analysis templates, we discuss our 
preliminary results on the filtering performance 
of the three language-specific embedded MT 
systems from this process flow. 
Processes Document Versions 
1 
g 
ords 
'~--~ image doe in S 
~__~ post-OCR doc ! 
) 
post-MT doe i~ 
tagged TL d 
open/dosed w 
tagged TL do 
sere. related w~ 
tagged TL do 
domain wore 
Figure 1 Analysis Template 
Measures 
2 An Embedded MT System Design 4 
Our three systems process documents using a 
sequence of three software modules. First, the 
Scan software module creates an online bitmap 
image in real-time as the user feeds the 
document into the page-feed scanner-. 5 Second, 
the optical character recognition (OCR) software 
converts that image to character text and, third, 
the machine translation (MT) software converts 
the foreign language character text to English, 
where it may be stored to disk or displayed on 
screen directly to the user. The user interface 
only requires that the user push one or two 
buttons to carry out all of the system's 
processing on an individual document. 
We tested three separate language-specific 
embedded MT systems for Spanish, Arabic and 
Haitian Creole. These systems differ in their 
4 We use "embedded MT systems" as defined in Voss 
and Reeder (1998). 
5 We chose a small scanner for portability of the 
system. Substituting in a flatbed scanner would not 
affect performance. 
OCR and MT components, but otherwise they 
share the same software, Omnipage's Paperport 
for scaning and Windows95 as the operating 
system. 6 
3 Approach  
As we sought to measure the performance of 
each component in the systems, it quickly 
became apparent that not all available measures 
may be equally applicable for our filtering task. 
For example, counting the number of source 
language (SL) characters correctly OCR-ed may 
be overly specific: as discussed below, we only 
need to make use of the number of SL words 
that are correctly OCR-ed. In the sections to 
follow, we describe those measures that have 
been most informative for the task of filtering. 
Analysis Template 
We use three types of information in our 
evaluation of the end-to-end embedded MT 
systems that we have available to us: 
transformation processes, document versions, 
and basic count measures. The transformation 
processes are listed vertically in the diamonds on 
the left side of figure 1. Starting with the 
hardcopy original document, each process 
transforms its input text and creates a new 
version. These document versions are listed 
vertically in the boxes in the second column of 
the figure. For each version, we compute one or 
more basic count measures on the words in that 
version's text. That is, for each process, there is 
an associated ocument version and for each 
document version, there are associated basic 
count measures. These count measures hown 
as A. through M. are defined in figure 2 below. 
Two-Pass Evaluation 
For each end-to-end system and language pair, 
we follow two separate passes in creating 
analysis files from scanned-in bitmap images. 
The first pass is for end-to-end Scan-OCR-MT 
evaluation: "OCR" the original document, then 
MT the resulting OCR-output file. The second 
pass is for Ground Truth-MT evaluation: 
"ground-truth" (GT) the original document, hen 
MT the resulting GT-ed output file. 
6 See Voss and Van Ess-Dykema (2000) for a 
description of the products used. 
2 
A. 
B. 
Measures 
image doe 
"~ C. 
:# "words" lost/added~ \[ 
t i,,OCR oo , J I 
SPANISH 
S CAN/ GT/ 
OCR/ MT 
MT 
H. _I. ..................... 
~ #  dosLed class i ? 
# incorrect i 
L . . . . . . . . . . . . .  M~ ..................... 
t , -wor~-A-7~. - - ) l  rol~,~it  i 
I *ords in TL  I .  
Figure2 Comparison of 
The two passes represent he "worst" and 
"best" cases respectively for filtering within 
each of the three embedded MT systems. By 
"ground truth" versions of the document, we 
mean online duplicated versions that match, 
character-for-character, the input text. 
We intentionally chose low-performance 
OCR software (for each language) to simulate a
"worst case" performance by our systems, 
enabling us to compare them with the ideal high- 
performance ground-truth input to simulate a 
"best case" performance. 
Texts from the Center for Disease Control 
In order to compare the three language-specific 
systems, we had to fred a corpus in a domain 
well-defined for filtering 7 that included parallel 
texts in Spanish, Arabic, and Haitian Creole. We 
found parallel corpora for these and many other 
ARABIC 
SCAN/ 
OCR/ 
MT 
GT/ 
MT 
HAITIAN C REOLE 
S CAN/ OT/ 
OCR/ MT 
MT 
Language-Spedfic System Results 
languages at a website of the Center for Disease 
Control (CDC). 8 
We chose a paragraph from the chicken 
pox/varicella bulletin, page 2, for each of our 
three languages. This passage contains narrative 
full-length sentences and minimizes the OCR 
complications that arise with variable layouts. 
Our objective for selecting this input paragraph 
was to illustrate our methodology in a tractable 
way for multiple languages. Our next step will 
be to increase the amount of data analyzed for 
each language. 
4 Analyses 
We fill out one analysis template for each 
document tested in a language-specific system. 
Example templates with the basic count 
II II 7 Filtering judgments are well-defined when 
multiple readers of a text in a domain agree on the 
"yes, relevant" status of the text. 
8 See http://www.irnmunize.org/vis/index.htm. The 
texts are "Vaccine Information Statements" 
describing basic medical symptoms that individuals 
should know about in advance of being vaccinated. 
3 
measures 9 are presented in figure 2 for each Of 
the three embedded MT systems that we tested. 
Notice that in figure 2 we distinguish valid 
words of a language from OCR-generated 
strings of characters that we identify as "words." 
The latter "words" may include any of the 
following: wordstrings with OCR-induced 
spelling changes (valid or invalid for the specific 
language), wordstrings duplicating misspellings 
in the source document, and words accurately 
OCR-ed. "Words" may also be lost in the MT 
process (see F.). 1? 
The wide, block arrow in figure 2 connect,,; E .  
and G. because they are both based on the MT 
output document. (We do not compute a sum for 
these counts because the E "words" are in the SL 
and the G words are in the TL.) The open class 
words (see H.) are nouns, verbs, adjectives, and 
adverbs. Closed class words (see I.) include: all 
parts of speech not listed as open class 
categories. 
In this methodology, we track the conltent 
words that ultimately contribute to the final 
filtering decision. Clearly for other tasks, such 
as summarization or information extraction, 
other measures may be more appropriate. The 
basic count measures A. through M. are 
preliminary and will require refinement as more 
data sets are tested. From these basic count 
measures, we define four derived percentage 
measures in section 5 and summarize these cases 
across our three systems in figure 3 of that 
section. 
4.1 Embedded Spanish MT System Test 
"Worst"  case (Scan-OCR-MT pass) 
As can be seen in figure 2, not all of the original 
80 Spanish words in the source document retain 
their correct spelling after being OCR-ed. Only 
26 OCR-ed "words" are found in the NIT 
lexicon, i.e., recognized as valid Spanish words. 
Forty-nine of the OCR-ed "words" are treated as 
"not found words" (NFWs) by the MT engine, 
even though they may in fact be actual Spanish 
words. Five other OCR-ed "words" are lost in 
9 The following formulas ummarize the relations 
among the count measures: A ffi B+C; B ffi D+E+F; 
G ffi H+I; H = J+K;  J ffi L+M. 
10 For example, we found that the word la in the 
Spanish text was not present in the TL output, i.e., 
the English equivalent the did not appear in the 
English translation. 
the MT process. Thus, the OCR process reduced 
the number of Spanish words that the MT engine 
could accept as input by more than 60%. 
Of the remaining 40% that generated 29 
English words, we found that 5 were "filter- 
relevant" as follows. The MT engine ignored 49 
post-OCR Spanish "words" and working from 
the remaining 26 Spanish words, generated 29 
English words? 1 Seventeen were open class 
words and 12 were closed class words. Nearly 
all of the open class words were translated 
correctly or were semantically appropriate for 
the domain (16 out of 17). From this correct set 
of 16 open class words, 5 were domain-relevant 
and 9 were not. That is, 5 of the 29 generated 
English words, or 17%, were semantically 
related and domain relevant words, i.e., triggers 
for filtering judgments. 
"Best" case (GT-MT pass) 
The MT engine generated 77 English words 
from the 80 original Spanish words. Thirty- 
eight, or half of the 77, were open class words; 
39 were closed class words. All of the 38 open 
class words were correctly translated or 
semantically related to the preferred translation. 
And half of those, 17, were domain-relevant. 
Thus, the 77 English words generated by the MT 
engine contained 17 "filter-relevant" words, or 
22%. 
Comparing the Two Passes 
Surprisingly the GT-MT pass only yields a 5% 
improvement in filtering judgments over the 
Scan-OCR-MT pass, even though the OCR itself 
reduced the number of Spanish words that the 
MT engine could accept as input by more than 
60%. We must be cautious in interpreting the 
significance of this comparison, given the single, 
short paragraph used only for illustrating our 
methodology. 
4.2 Embedded Arabic MT System Test 
"Worst"  case  (Scan-OCR-MT pass) 
The OCR process converted the original 84 
Arabic words into 88 "words". Of the original 
84 Arabic words in the source document, only 
11 This occurred because the MT engine was not 
using a word-for-word scheme. The Spanish verb for 
debo is translated into 2 English words, I must. As we 
will note further on, different languages have 
different expansion rates into English. 
4. 
55 retain their correct spelling after being OCR- 
ed and are found in the MT lexicon, i.e., 
recognized as valid Arabic words. Ten of the 
other OCR-ed "words" are treated as NFWs by 
the MT engine. The remaining 23 OCR-ed 
mixture of original words and OCR-induced 
"words" are not found in the Arabic MT lexicon. 
Thus, the OCR process reduced the number of 
original Arabic words that the MT engine could 
accept as input by slightly more than 65%. 
Of the remaining 35% that generated 70 
English words, we found that 7 were "filter- 
relevant" as follows. The MT lexicon did not 
contain 10 post-OCR Arabic "words" and 
working from the remaining 55 Arabic words, 
the MT engine generated 70 English words. 12 
Thirty of the 70 were open class words and 40 
were closed class words. Only one-third of the 
open class words were translated correctly or 
were semantically appropriate for the domain 
(10 out of 30). From this correct set of 10 open 
class words, 7 were domain-relevant and 3 were 
not. Thus, this pass yields 7 words for filtering 
judgments from the 70 generated English words, 
or 10%, were semantically related and domain 
relevant words. 
"Best" case (GT-MT pass) 
Of the 84 original Arabic words, even with the 
GT as input, 28 were not found in the MT 
lexicon, reflecting the engine's emerging status 
and the need for further development. Two 
others were not found in the Arabic MT lexicon, 
leaving 54 remaining words as input to the MT 
engine. The MT engine generated 68 English 
words from these 54 words. Thirty-one of the 
68 were open class words; 37 were closed class 
words. Of the open class words, 25 were 
translated correctly or semantically related. And 
8 of those 25 were domain-relevant. Thus, the 
68 English words generated by the MT engine 
contained 8 "filter-relevant" words, or 12%. 
Comparing the Two Passes 
The GT-MT pass yields a 2% improvement in
filtering judgments over the Scan-OCR-MT 
pass, even though the OCR itself reduced the 
12 This expansion rate is consistent with the rule-of- 
thumb that Arabic linguists have for every one 
Arabic word yielding on average 1.3 words in 
English. 
number of Arabic words that the MT engine 
could accept as input by about 65%. 
One of the interesting findings about OCR-ed 
Arabic "words" was the presence of "false 
positives," inaccurately OCR-ed source 
document words that were nonetheless valid in 
Arabic. That is, we found instances of valid 
Arabic words in the OCR output that appeared 
as different words in the original document. 13
4.3 Embedded Haitian MT System Test 
"Worst" case (Scan-OCR-MT pass) 
In the template for the 76-word Haitian Creole 
source document, we see that 27 words were lost 
in the OCR process, leaving only 49 in the post- 
OCR document. Of those 49, only 20 exhibit 
their correct spelling after being OCR-ed and are 
found in the MT lexicon. Twenty-nine of the 49 
OCR-ed "words" are not found (NFWs) by the 
MT engine. The OCR process reduced the 
number of original Haitian Creole words 
acceptable by the MT engine from 76 to 20, or 
74%. 
Of the remaining 26% that generated 22 
English words, we found that none were "filter- 
relevant," i.e., 0%, as follows. The MT engine 
ignored 29 post-OCR "words" and working from 
the remaining 20 Haitian words, generated 22 
English words. Ten were open class words and 
12 were closed class words. Only 2 out of the 10 
open class words were translated correctly or 
were semantically appropriate for the domain. 
From this correct set of 2 open class words, 
none were domain-relevant. The human would 
be unable to use this final document version to 
make his or her f'dtering relevance judgments. 
"Best" case (GT-MT pass) 
The MT engine generated 63 English words 
from the 76 original Haitian Creole words. 
Thirty of the 63 were open class words; 33 were 
closed class words. Only 11 of the 30 open class 
words were correctly translated or semantically 
related. Of those 11 words, 3 were domain- 
relevant. So, from the 63 generated English 
words, only 3 were "filter-relevant", or 5%. 
13 As a result, the number of words in the two passes 
can differ. As we see in figure 2 in the Scan-OCR- 
MT pass, there were 55 SL words translated but, in 
the GT-MT pass, only 54 SL words in the original 
text. 
5 
Derived Spanish 
Meas. OCR GT 
W. 40 95 
X. 55 49 
Y. 17 22 
Z. 94 100 
Arabic 
OCR GT 
35 64 
14 37 
10 12 
33 67 
Haitian Creole 
OCR GT 
26 79 
9 17 
0 5 
20 33 
Figure 3 Summary of Language-Specific Results 
(percentages) 
Comparing the Two Passes 
With an OCR package not trained for this 
specific language and an MT engine from a 
research effort, the embedded MT system with 
these components does not assist the human on 
the filtering task. And even with the ground- 
truth input, the MT engine is not sufficiently 
robust to produce useful translations of walid 
Haitian Creole words. 
5 Cross-System Results 
In figure 3 we compare the three language- 
specific systems, we make use of four measures 
derived from the basic counts, A. through M., as 
defined in figure 2. 
W. Original Doeument-MT Word Recall 
% of original SL document words translatable 
by the MT engine after being OCR-ed. (D/A) 
This measure on the GT pass in all 3 systems 
gives us the proportion of words in the original 
SL document hat are in the individual lVIT 
lexicons. The Spanish lexicon is strong for the 
domain of our document (W -- 95%). The 
measures for Arabic and Haitian Creole reflect 
the fact that their MT lexicons are still under 
development (W -- 64% and 79%, respectively). 
This measure on the OCR pass, given the 
corresponding measure on the GT pass as a 
baseline, captures the degradation i troduced by 
the Scan-OCR processing of the document. 
From figure 3 we see that the Spanish system 
loses approximately 55% of its original 
document words going into the MT engine (95% 
minus 40%), the Haitian Creole 53% (79% 
minus 26%), and the Arabic 29% (64% minus 
35%). Recall that the Spanish and Haitian 
Creole systems included the same OCR 
software, which may account for the similar 
level of performance h re. This software was not 
available to us for Arabic. 
X. MT Semantic Adequacy 
% of TL words generated by MT engine that are 
open class & semantically adequate in their 
translation (J/G ) 
This measure is intended to assess whether a 
system can be used for filtering broad-level 
topics (in contrast o domains with specialized 
vocabulary that we discuss below). Here we see 
evidence for two patterns that recur in the two 
measures below. First, the GT pass---with one 
exception---exhibits better performance than the 
OCR pass. Second, there is a ranking of the 
systems with Haitian Creole at the low end, 
Arabic in the middle, and Spanish at the high 
end. We will need more data to determine the 
significance of the one exception (55% versus 
49%). 
Y. MT Domain-Relevant Adequacy 
% of TL words generated by MT engine that are 
open class, semantically adequate in their 
translation, and domain-relevant (L/G) 
In all of the systems there was a slight gain in 
domain-relevant faltering performance from the 
OCR pass to the GT pass. We can rank the 
systems with the Haitian Creole at the low end, 
the Arabic in the middle, and the Spanish at the 
high end: the measures in both the OCR and GT 
passes in Haitian Creole are lower than in the 
Arabic, which are lower than in the Spanish. 
Only the Spanish documents, but not the Arabic 
or Haitian Creole ones, when machine translated 
in either pass were judged domain-relevant by
five people dunng an informal test. 14 Thus, our 
data suggests that the Spanish system's lower 
bound (OCR pass) of 17% on this measure is 
needed for faltering. 
Z. MT Open Class Semantic Adequacy 
% of open class TL words generated by MT 
engine that are semantically adequate in their 
translation (J/H) 
14 We are in the process of running an experiment to
validate the protocol for establishing domain-relevant 
judgments as part of our research in measures of 
effectiveness (MOEs) for task-based valuation. 
6 
The same pattern emerges with this measure. In 
each system there is an improvement in 
performance stepping from the OCR pass to the 
GT pass. Across systems we see the same 
ranking, with the OCR and GT passes of the 
Haitian Creole falling below the Arabic which 
falls below the Spanish. 
Conclusion and Future Work 
Our main contribution has been the proposal of 
an end-to-end process analysis template and a 
replicable valuation methodology. We present 
measures to evaluate filtering performance and 
preliminary results on Spanish, Arabic and 
Haitian Creole FALCon systems. 
The cross-system comparisons using the 
measures presented, with one exception, yielded 
the following expected rankings: (i) the GT-MT 
pass exhibits better performance than the Scan- 
OCR-MT pass and (ii) the Haitian Creole 
system is at the low end, Arabic is in the middle, 
and Spanish is at the high end. 
Our long-term objective is to compare the 
results of the system-internal "measures of 
performance" (MOPs) presented here with 
results we still need from system-external 
"measures of effectiveness" (MOEs)25 MOE- 
based methods evaluate (i) baseline unaided 
human performance, (ii) human performance 
using a new system and (iii) human expert 
performance. From this comparison we will be 
able to determine whether these two 
independently derived sets of measures are 
replicable and validate each other. So far, we 
have only addressed our original question, 
"when is an embedded MT system good enough 
for filtering?" in terms of MOPs. We found that, 
for our particular passage in the medical domain, 
documents need to reach at least 17% on our 
derived measure Y., MT domain-relevant 
adequacy (recall discussion of derived measure 
Y, in section 5). 
Given that all but one process tep ("ID wrong 
TL words" as shown in figure 1 where a human 
stick figure appears) in filling the template can 
be automated, the next phase of this work will 
be to create a software tool to speed up and 
systematize this process, improving our system 
evaluation by increasing the number of 
15 See Roche and Watts (1991) for definitions of 
these terms. 
documents hat can be regularly Used to test each 
new system and reducing the burden on the 
operational linguists who assist us for the one 
critical step. Currently available tools for 
parallel text processing, including text alignment 
software, may provide new user interface 
options as well, improving the interactive 
assessment process and possibly extending the 
input set to include transcribed speech. 
Acknowledgements 
We would like to acknowledge Lisa Decrozant 
(Army Research Laboratory) and Brian 
Branagan (Department of Defense) for language 
expertise and Francis Fisher (Army Research 
Laboratory) for systems engineering expertise. 
References 
Church, K. and Hovy, E. 1993. Good Applications 
for Crummy Machine Translation. Machine 
Translation, Volume 8, pages 239 - 258. 
DARPA 1999. Translingual Information Detection, 
Extraction, and Summarization (TIDES) Initiative. 
http://www.darpa.mil/ito/research/tides/index.html 
Fisher, F. and Voss, C. R. 1997. "FALCon, an MT 
System Support Tool for Non-linguists." In 
Proceedings of the Advanced Information 
Processing and Analysis Conference. McLean,VA. 
Harmon, D. 1999. "A Framework for Evaluation in 
TIDES." Presentation at TIDES Planning 
Workshop, with link at http://www.dyncorp- 
is.com/darpa/meetings/fides99jul/agenda.html, July 
28-30, Leesburg, VA. 
Pomarede, J.-M., Taylor, K., and Van Ess-Dykema, 
C. 1998. Sparse Training Data and EBMT. In 
Proceedings of the Workshop on Embedded MT 
Systems: Design, Construction, and Evaluation of 
Systems with an MT Component held in 
conjunction with the Association for Machine 
Translation in  the Americas (AMTA'98), 
Langhorne, PA, October. 
Resnik, P. 1997. Evaluating Multilingual Gisting of 
Web Pages. In Working Notes of the AAA1 Spring 
Symposium on Natural Language Processing for 
the Worm Wide Web, Palo Alto, CA. 
Roche, J. G. and Watts, B. D. 1991. Choosing 
Analytic Measures. The Journal of Strategic 
Studies, Volume 14, pages 165-209, June. 
Sparck Jones, K. and Galliers, J. 1996. Evaluating 
Natural Language Processing Systems. Springer- 
Verlag Publishers, Berlin, Germany. 
Taylor, K. and White, J. 1998. Predicting What MT 
is Good for: User Judgments and Task 
Performance. In Proceedings of the Third 
7 
Conference of the Association for Machine 
Translation in the Americas (AMTA'98), pages 
364 -373, Langhome, PA, October. 
Voss, C. R. and Reeder, F. (eds.). 1!998. 
Proceedings of the Workshop on Embedded MT 
Systems: Design, Construction, and Evaluation of 
Systems with an MT Component held in 
conjunction with the Association for Machine 
Translation in the Americas (AMTA'98), 
Langhorne, PA, October. 
Voss, C. R. and Van Ess-Dykema, C. 2000. 
Evaluating Scan-OCR-MT Processing for the 
Filtering Task. Army Research Laboratory 
Technical Report, Adelphi, MD. 
8 
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 66?67,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Buckwalter-based Lookup Tool as Language Resource
for Arabic Language Learners
Jeffrey Micher Clare R. Voss
Multilingual Computing Branch Multilingual Computing Branch
Army Research Laboratory Army Research Laboratory
Adelphi, MD 20783 USA Adelphi, MD 20783 USA
jmicher@arl.army.mil voss@arl.army.mil
     The morphology of the Arabic language is rich
and complex; words are inflected to express varia-
tions in tense-aspect, person, number, and gender,
while they may also appear with clitics attached to
express possession on nouns, objects on verbs and
prepositions, and conjunctions. Furthermore, Ara-
bic script allows the omission of short vowel dia-
critics. For the Arabic language learner trying to
understand non-diacritized text, the challenge
when reading new vocabulary is first to isolate in-
dividual words within text tokens and then to de-
termine the underlying lemma and root forms to
look up the word in an Arabic dictionary.
     Buckwalter (2005)?s morphological analyzer
(BMA) provides an exhaustive enumeration of the
possible internal structures for individual Arabic
strings in XML, spelling out all possible vocaliza-
tions (diacritics added back in), parts of speech on
each token identified within the string, lemma ids,
and English glosses for each tokenized substring.
     The version of our Buckwalter-based Lookup
Tool (BBLT) that we describe in this poster pro-
vides an interactive interface for language learners
to copy and paste, or type in, single or multiple
Arabic strings for analysis by BMA (see Fig. 1)
Figure 1.  BBLT Input Screen
     We originally developed BBLT for ourselves as
machine translation (MT) developers and evaluat-
ors, to rapidly see the meanings of Arabic strings
that were not being translated by our Arabic-
English (MT) engines (Voss et al 2006), while we
were also testing synonym lookup capabilities in
Arabic WordNet tool (Elkateb et al 2006). While
BBLT allows users to see the ?raw? BMA XML
(see Fig. 2), the look-up capability that sorts the
entries by distinct lemma and presents by English
gloss has proved the most useful to English-
speaking users who cannot simply lookup Arabic
words in the Hans Wehr dictionary (considered the
most complete source of Arabic words with about
13,000 entries, but requires the user to be able to
?know? the underlying form to search for).
Figure 2.  BBLT Output for single token with option
 ?meanings with clitics and person inflections? on
The BBLT user can opt to see the glosses with or
without the clitics or inflections, with their
diacritized forms either transliterated or rewritten
66
Figure 3.  BBLT Output for single token with additional
option ?Buckwalter encoded vocalizations? on
in Arabic script (see Fig. 3) or in full table form for
full sentence glossing (see Fig. 4).
The web application is written as a Java webapp
to be run in a tomcat web server. It makes use of
wevlets written as both standalone sevlets, ex-
tending HttpServlet, and .jsp pages. One servlet
handles running BMA as a socket-server process
and another servlet handles request from the input
.jsp page, retrieves the raw output from the former,
process the output according to input page pa-
rameters, and redirects the output to the appropri-
ate .jsp page for display.
References
Buckwalter Arabic Morphological Analyzer (BAMA),
Version 2.0, LDC Catalog number LDC2004L02,
www.ldc.upenn.edu/Catalog.
Buckwalter,T. (2005) www.qamus.org/morphology.htm
Elkateb, S., Black, W., Rodriguez, H, Alkhalifa, M.,
Vossen, P., Pease, A. and Fellbaum, C., (2006).
Building a WordNet for Arabic, in Proceedings of
The fifth international conference on Language Re-
sources and Evaluation (LREC 2006).
Voss, C., J. Micher, J. Laoudi, C. Tate (2006) ?Ongoing
Machine Translation Evaluation at ARL,? Presenta-
tion, In Proceedings of the NIST Machine Transla-
tion Workshop, Washington, DC.
Wehr, Hans (1979) Arabic-English Dictionary:: The
Hans Wehr Dictionary of Modern Written Arabic.
Edited by J. M. Cowan. 4th ed..Wiesbaden, Harras-
sowitz.
Figure 4.  BBLT Output for Full Sentence with option ?meanings with clitics & person inflections?
67
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1567?1578, Dublin, Ireland, August 23-29 2014.
The Wisdom of Minority: Unsupervised Slot Filling Validation based on
Multi-dimensional Truth-Finding
Dian Yu
1
, Hongzhao Huang
1
, Taylor Cassidy
2,3
, Heng Ji
1
Chi Wang
4
, Shi Zhi
4
, Jiawei Han
4
, Clare Voss
2
, Malik Magdon-Ismail
1
1
Computer Science Department, Rensselaer Polytechnic Institute
2
U.S. Army Research Lab
3
IBM T. J. Watson Research Center
4
Computer Science Department, Univerisity of Illinois at Urbana-Champaign
1
{yud2,huangh9,jih,magdon}@rpi.edu,
2,3
{taylor.cassidy.ctr,clare.r.voss.civ}@mail.mil
4
{chiwang1,shizhi2,hanj}@illinois.edu
Abstract
Information Extraction using multiple information sources and systems is beneficial due to multi-
source/system consolidation and challenging due to the resulting inconsistency and redundancy.
We integrate IE and truth-finding research and present a novel unsupervised multi-dimensional
truth finding framework which incorporates signals from multiple sources, multiple systems and
multiple pieces of evidence by knowledge graph construction through multi-layer deep linguistic
analysis. Experiments on the case study of Slot Filling Validation demonstrate that our approach
can find truths accurately (9.4% higher F-score than supervised methods) and efficiently (finding
90% truths with only one half the cost of a baseline without credibility estimation).
1 Introduction
Traditional Information Extraction (IE) techniques assess the ability to extract information from
individual documents in isolation. However, similar, complementary or conflicting information may
exist in multiple heterogeneous sources. We take the Slot Filling Validation (SFV) task of the NIST Text
Analysis Conference Knowledge Base Population (TAC-KBP) track (Ji et al., 2011) as a case study. The
Slot Filling (SF) task aims at collecting from a large-scale multi-source corpus the values (?slot fillers?)
for certain attributes (?slot types?) of a query entity, which is a person or some type of organization. KBP
2013 has defined 25 slot types for persons (per) (e.g., age, spouse, employing organization) and 16 slot
types for organizations (org) (e.g., founder, headquarters-location, and subsidiaries). Some slot types
take only a single slot filler (e.g., per:birthplace), whereas others take multiple slot fillers (e.g., org:top
employees).
We call a combination of query entity, slot type, and slot filler a claim. Along with each claim, each
system must provide the ID of a source document and one or more detailed context sentences as evidence
which supports the claim. A response (i.e., a claim, evidence pair) is correct if and only if the claim is
true and the evidence supports it.
Given the responses produced by multiple systems from multiple sources in the SF task, the goal of
the SFV task is to determine whether each response is true or false. Though it?s a promising line of
research, it raises two complications: (1) different information sources may generate claims that vary
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1567
in trustability; and (2) a large-scale number of SF systems using different resources and algorithms may
generate erroneous, conflicting, redundant, complementary, ambiguously worded, or inter-dependent
claims from the same set of documents. Table 1 presents responses from four SF systems for the query
entity Ronnie James Dio and the slot type per:city of death. Systems A, B and D return Los Angeles
with different pieces of evidence
1
extracted from different information sources, though the evidence of
System D does not decisively support the claim. System C returns Atlantic City, which is neither true
nor supported by the corresponding evidence.
Such complications call for ?truth finding?: determining the veracity of multiple conflicting claims
from various sources and systems. We propose a novel unsupervised multi-dimensional truth finding
framework to study credibility perceptions in rich and wide contexts. It incorporates signals from
multiple sources and systems, using linguistic indicators derived from knowledge graphs constructed
from multiple evidences using multi-layer deep linguistic analysis. Experiments demonstrate that our
approach can find truths accurately (9.4% higher F-score than supervised methods) and efficiently (find
90% truths with only one half cost of a baseline without credibility estimation).
System Source Slot Filler Evidence
A Agence France-
Presse, News
Los Angeles The statement was confirmed by publicist Maureen O?Connor, who said Dio
died in Los Angeles.
B New York
Times, News
Los Angeles Ronnie James Dio, a singer with the heavy-metal bands Rainbow, Black
Sabbath and Dio, whose semioperatic vocal style and attachment to demonic
imagery made him a mainstay of the genre, died on Sunday in Los Angeles.
C Discussion Fo-
rum
Atlantic City Dio revealed last summer that he was suffering from stomach cancer shortly
after wrapping up a tour in Atlantic City.
D Associated
Press
Worldstream,
News
Los Angeles LOS ANGELES 2010-05-16 20:31:18 UTC Ronnie James Dio, the metal god
who replaced Ozzy Osbourne in Black Sabbath and later piloted the bands
Heaven, Hell and Dio, has died, according to his wife and manager.
Table 1: Conflicting responses across different SF systems and different sources (query entity = Ronnie
James Dio, slot type = per:city of death).
2 Related Work & Our Novel Contributions
Most previous SFV work (e.g., (Tamang and Ji, 2011; Li and Grishman, 2013)) focused on filtering
incorrect claims from multiple systems by simple heuristic rules, weighted voting, or costly supervised
learning to rank algorithms. We are the first to introduce the truth finding concept to this task.
The ?truth finding? problem has been studied in the data mining and database communities (e.g., (Yin
et al., 2008; Dong et al., 2009a; Dong et al., 2009b; Galland et al., 2010; Blanco et al., 2010; Pasternack
and Roth, 2010; Yin and Tan, 2011; Pasternack and Roth, 2011; Vydiswaran et al., 2011; Ge et al.,
2012; Zhao et al., 2012; Wang et al., 2012; Pasternack and Roth, 2013)). Compared with the previous
work, our truth finding problem is defined under a unique setting: each response consists of a claim and
supporting evidence, automatically generated from unstructured natural language texts by a SF system.
The judgement of a response concerns both the truth of the claim and whether the evidence supports
the claim. This has never been modeled before. We mine and exploit rich linguistic knowledge from
multiple lexical, syntactic and semantic levels from evidence sentences for truth finding. In addition,
previous truth finding work assumed most claims are likely to be true. However, most SF systems have
hit a performance ceiling of 35% F-measure, and false responses constitute the majority class (72.02%)
due to the imperfect algorithms as well as the inconsistencies of information sources. Furthermore,
certain truths might only be discovered by a minority of good systems or from a few good sources. For
example, 62% of the true responses are produced only by 1 or 2 of the 18 SF systems.
1
Hereafter, we refer to ?pieces of evidence? with the shorthand ?evidences?. Note that SF systems may include multiple
sentences as ?evidence? within their responses.
1568
r1 
      Response 
<Claim, Evidence> 
t1 
t2 
System 
s1 
r2 
r3 
s2 
Source 
t3 
r4 t4 
s3 
r5 
Figure 1: Heterogeneous networks for MTM.
3 MTM: A Multi-dimensional Truth-Finding Model
MTM Construction
A response is trustworthy if its claim is true and its evidence supports the claim. A trusted
source always supports true claims by giving convincing evidence, and a good system tends to extract
trustworthy responses from trusted sources. We propose a multi-dimensional truth-finding model (MTM)
to incorporate and compute multi-dimensional credibility scores.
Consider a set of responses R = {r
1
, . . . , r
m
} extracted from a set of sources S = {s
1
, . . . , s
n
} and
provided by a set of systems T = {t
1
, . . . , t
l
}. A heterogeneous network is constructed as shown in
Fig. 1. Let weight matrices be W
rs
m?n
= {w
rs
ij
} and W
rt
m?l
= {w
rt
ik
}. A link w
rs
ij
= 1 is generated
between r
i
and s
j
when response r
i
is extracted from source s
j
, and a link w
rt
ik
= 1 is generated between
r
i
and t
k
when response r
i
is provided by system t
k
.
Credibility Initialization
Each source is represented as a combination of publication venue and genre. The credibility scores
of sources S are initialized uniformly as
1
n
, where n is the number of sources. Given the set of systems
T = {t
1
, . . . , t
l
}, we initialize their credibility scores c
0
(t) based on their interactions on the predicted
responses. Suppose each system t
i
generates a set of responses R
t
i
. The similarity between two systems
t
i
and t
j
is defined as similarity(t
i
, t
j
) =
|R
t
i
?R
t
j
|
log (|R
t
i
|)+log (|R
t
j
|)
(Mihalcea, 2004). Then we construct a
weighted undirected graph G = ?T,E?, where T (G) = {t
1
, . . . , t
l
} and E(G) = {?t
i
, t
j
?}, ?t
i
, t
j
? =
similarity(t
i
, t
j
), and apply the TextRank algorithm (Mihalcea, 2004) on G to obtain c
0
(t).
We got negative results by initializing system credibility scores uniformly. We also got negative results
by initializing system credibility scores using system metadata, such as the algorithms and resources the
system used at each step, its previous performance in benchmark tests, and the confidence values it
produced for its responses. We found the quality of an SF system depends on many different resources
instead of any dominant one. For example, an SF system using a better dependency parser does not
necessarily produce more truths. In addition, many systems are actively being improved, rendering
previous benchmark results unreliable. Furthermore, most SF systems still lack reliable confidence
estimation.
The initialization of the credibility scores for responses relies on deep linguistic analysis of the
evidence sentences and the exploitation of semantic clues, which will be described in Section 4.
Credibility Propagation
1569
We explore the following heuristics in MTM.
HEURISTIC 1: A response is more likely to be true if derived from many trustworthy sources. A source
is more likely to be trustworthy if many responses derived from it are true.
HEURISTIC 2: A response is more likely to be true if it is extracted by many trustworthy systems. A
system is more likely to be trustworthy if many responses generated by it are true.
Based on these two heuristics we design the following credibility propagation approach to mutually
reinforce the trustworthiness of linked objects in MTM.
By extension of Co-HITS (Deng et al., 2009), designed for bipartite graphs, we develop a propagation
method to handle heterogeneous networks with three types of objects: source, response and system. Let
the weight matrices beW
rs
(between responses and sources) andW
rt
(between responses and systems),
and their transposes beW
sr
andW
tr
. We can obtain the transition probability that vertex s
i
in S reaches
vertex r
j
in R at the next iteration, which can be formally defined as a normalized weight p
sr
ij
=
w
sr
ij?
k
w
sr
ik
such that
?
r
j
?R
p
sr
ij
= 1. We compute the transition probabilities p
rs
ji
, p
rt
jk
and p
tr
kj
in an analogous
fashion.
Given the initial credibility scores c
0
(r), c
0
(s) and c
0
(t), we aim to obtain the refined credibility scores
c(r), c(s) and c(t) for responses, sources, and systems, respectively. Starting with sources, the update
process considers both the initial score c
0
(s) and the propagation from connected responses, which we
formulated as:
c(s
i
) = (1? ?
rs
)c
0
(s
i
) + ?
rs
?
r
j
?R
p
rs
ji
c(r
j
) (1)
Similarly, the propagation from responses to systems is formulated as:
c(t
k
) = (1? ?
rt
)c
0
(t
k
) + ?
rt
?
r
j
?R
p
rt
jk
c(r
j
) (2)
Each response?s score c(r
j
) is influenced by both linked sources and systems:
c(r
j
) = (1? ?
sr
? ?
tr
)c
0
(r
j
) + ?
sr
?
s
i
?S
p
sr
ij
c(s
i
) + ?
tr
?
t
k
?T
p
tr
kj
c(t
k
) (3)
where ?
rs
, ?
rt
, ?
sr
and ?
tr
? [0, 1]. These parameters control the preference for the propagated over
initial score for every type of random walk link. The larger they are, the more we rely on link structure
2
.
The propagation algorithm converges (10 iterations in our experimental settings) and a similar theoretical
proof to HITS (Peserico and Pretto, 2009) can be constructed. Algorithm 1 summarizes MTM.
4 Response Credibility Initialization
Each evidence along with a claim is expressed as a few natural language sentences that include the query
entity and the slot filler, along with semantic content to support the claim. We analyze the evidence of
each response in order to initialize that response?s credibility score. This is done using heuristic rules
defined in terms of the binary outputs of various linguistic indicators (Section 4.1).
4.1 Linguistic Indicators
We encode linguistic indicators based on deep linguistic knowledge acquisition and use them to
determine whether responses provide supporting clues or carry negative indications (Section 4.3). These
indicators make use of linguistic features on varying levels - surface form, sentential syntax, semantics,
and pragmatics - and are defined in terms of knowledge graphs (Section 4.2). We define a heuristic rule
for each slot type in terms of the binary-valued linguistic indicator outputs to yield a single binary value
(1 or 0) for each response. If a response?s linguistic indicator value is 1, the credibility score of a response
is initialized at 1.0, and 0.5 otherwise.
2
We set ?
rs
= 0.9, ?
sr
= 0.1, ?
rt
= 0.3 and ?
tr
= 0.2, optimized from a development set. See Section 5.1.
1570
Input: A set of responses (R), sources (S) and systems (T ).
Output: Credibility scores (c(r)) for R.
1: Initialize the credibility scores c
0
(s) for S as c
0
(s
i
) =
1
|S|
;
2: Use TextRank to compute initial credibility scores c
0
(t) for T ;
3: Initialize the credibility scores c
0
(r) using linguistic indicators (Section 4);
4: Construct heterogeneous networks across R, S and T ;
5: k ? 0, diff? 10e6;
6: while k < MaxIteration and diff > MinThreshold do
7: Use Eq. (1) to compute c
k+1
(s);
8: Use Eq. (2) to compute c
k+1
(t);
9: Use Eq. (3) to compute c
k+1
(r);
10: Normalize c
k+1
(s), c
k+1
(t), and c
k+1
(r);
11: diff?
?
(|c
k+1
(r)? c
k
(r)|);
12: k ? k + 1
13: end while
Algorithm 1:Multi-dimensional Truth-Finding.
4.2 Knowledge Graph Construction
A semantically rich knowledge graph is constructed that links a query entity, all of its relevant slot
filler nodes, and nodes for other intermediate elements excerpted from evidence sentences. There is one
knowledge graph per sentence.
Fig. 2 shows a subregion of the knowledge graph built from the sentence: ?Mays, 50, died in his sleep
at his Tampa home the morning of June 28.?. It supports 3 claims: [Mays, per: city of death, Tampa],
[Mays, per: date of death, 06/28/2009] and [Mays, per: age, 50].
Formally, a knowledge graph is an annotated graph of entity mentions, phrases and their links. It must
contain one query entity node and one or more slot filler nodes. The annotation of a node includes its
entity type, subtype, mention type, referent entities, and semantic category (though not every node has
each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation
between the two linked nodes.
The knowledge graph is constructed using the following procedure. First, we annotate the evidence
text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and
event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the
annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is
labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads.
Thus, we extend the boundaries of entity, time, and value mentions (e.g., people?s titles) to include an
entire phrase where possible. We then enrich each node with annotation for entity type, subtype and
mention type. Entity type and subtype refer to the role played by the entity in the world, the latter being
more fine-grained, whereas mention type is syntactic in nature (it may be pronoun, nominal, or proper
name). For example, ?Tampa? in Fig. 2 is annotated as a Geopolitical (entity type) Population-Center
(subtype) Name (mention type) mention. Every time expression node is annotated with its normalized
reference date (e.g., ?June, 28? in Fig. 2 is normalized as ?06/28/2009?).
Second, we perform co-reference resolution, which introduces implicit links between nodes that refer
to the same entity. Thus, an entity mention that is a nominal or pronoun will often be co-referentially
linked to a mention of a proper name. This is important because many queries and slot fillers are
expressed only as nominal mentions or pronouns in evidence sentences, their canonical form appearing
elsewhere in the document.
Finally, we address the fact that a given relation type may be expressed in a variety of ways. For
example, ?the face of ? indicates the membership relation in the following sentence: ?Jennifer Dunn was
the face of the Washington state Republican Party for more than two decades.? We mined a large
1571
Mays 
had 
died 
sleep 
his 
home 
Tampa 
50 
June,28 
amod 
nsubj 
aux 
prep_in 
poss 
prep_at 
prep_of 
nn 
poss 
  located_in 
{PER.Individual, NAM, Billy Mays} 
?Query? 
{NUM } 
?Per:age? 
{Death-Trigger} 
{PER.Individual.PRO, Mays} 
{GPE.Population-Center.NAM, FL-USA} 
? Per:place_of_death? 
{FAC.Building-Grounds.NOM} 
{06/28/2009, TIME-WITHIN}  
? per:date_of_death? 
Figure 2: Knowledge Graph Example.
number of trigger phrases for each slot type by mapping various knowledge bases, including Wikipedia
Infoboxes, Freebase (Bollacker et al., 2008), DBPedia (Auer et al., 2007) and YAGO (Suchanek et
al., 2007), into the Gigaword corpus
3
and Wikipedia articles via distant supervision (Mintz et al.,
2009)
4
. Each intermediate node in the knowledge graph that matches a trigger phrase is then assigned a
corresponding semantic category. For example, ?died? in Fig. 2 is labeled a Death-Trigger.
4.3 Knowledge Graph-Based Verification
We design linguistic indicators in terms of the properties of nodes and paths that are likely to be bear on
the response?s veracity. Formally, a path consists of the list of nodes and links that must be traversed
along a route from a query node to a slot filler node.
Node indicators contribute information about a query entity or slot filler node in isolation, that
may bear on the trustworthiness of the containing evidence sentence. For instance, a slot filler for the
per:date of birth slot type must be a time expression.
Node Indicators
1. Surface: Whether the slot filler includes stop words; whether it is lower cased but appears in news.
These serve as negative indicators.
2. Entity type, subtype and mention type: For example, the slot fillers for ?org:top employees? must be
person names; and fillers for ?org:website? must match the url format. Besides the entity extraction
system, we also exploited the entity attributes mined by the NELL system (Carlson et al., 2010)
from the KBP source corpus.
Each path contains syntactic and/or semantic relational information that may shed light on the manner
in which the query entity and slot filler are related, based on dependency parser output, IE output,
and trigger phrase labeling. Path indicators are used to define properties of the context in which
which query-entity and slot-filler are related in an evidence sentence. For example, whether the path
3
http://catalog.ldc.upenn.edu/LDC2011T07
4
Under the distant supervision assumption, sentences that appear to mention both entities in a binary relation contained in
the knowledge base were assumed to express that relation.
1572
associated with a claim about an organization?s top employee includes a title commonly associated with
decision-making power can be roughly represented using the trigger phrases indicator.
Path Indicators
1. Trigger phrases: Whether the path includes any trigger phrases as described in Section 4.2.
2. Relations and events: Whether the path includes semantic relations or events indicative of the slot
type. For example, a ?Start-Position? event indicates a person becomes a ?member? or ?employee?
of an organization.
3. Path length: Usually the length of the dependency path connecting a query node and a slot filler
node is within a certain range for a given slot type. For example, the path for ?per:title? is usually
no longer than 1. A long dependency path between the query entity and slot filler indicates a lack
of a relationship. In the following evidence sentence, which does not entail the ?per:religion?
relation between ?His? and the religion ?Muslim?, there is a long path (?his-poss-moment-nsubj-
came-advcl-seized-militant-acmod-Muslim?): ?His most noticeable moment in the public eye came
in 1979, when Muslim militants in Iran seized the U.S. Embassy and took the Americans stationed
there hostage.?.
Detecting and making use of interdependencies among various claims is another unique challenge in
SFV. After initial response credibility scores are calculated by combining linguistic indicator values, we
identify responses that have potentially conflicting or potentially supporting slot-filler candidates. For
such responses, their credibility scores are changed in accordance with the binary values returned by the
following indicators.
Interdependent Claims Indicators
1. Conflicting slot fillers: When fillers for two claims with the same query entity and slot type appear
in the same evidence sentence, we apply an additional heuristic rule designed for the slot type in
question. For example, the following evidence sentence indicates that compared to ?Cathleen P.
Black?, ?Susan K. Reed? is more likely to be in a ?org:top employees/members? relation with ?The
Oprah Magazine? due to the latter pair?s shorter dependency path: ?Hearst Magazine?s President
Cathleen P. Black has appointed Susan K. Reed as editor-in-chief of the U.S. edition of The
Oprah Magazine.?. The credibility scores are accordingly changed (or kept at) 0.5 for responses
associated with the former claim, and 1.0 for those associated with the latter.
2. Inter-dependent slot types: Many slot types are inter-dependent, such as ?per:title? and
?per:employee of ?, and various family slots. After determining initial credibility scores for each
response, we check whether evidence exists for any implied claims. For example, given initial
credibility scores of 1.0 for two responses supporting the claims that (1)?David? is ?per:children?
of ?Carolyn Goodman? and (2)?Andrew? is ?per:sibling? of ?David?, we check for any responses
supporting the claim that (3)?Andrew? is ?per:children? of ?Carolyn Goodman?, and set their
credibility scores to 1.0. For example, a response supporting this claim included the evidence
sentence, ?Dr. Carolyn Goodman, her husband, Robert, and their son, David, said goodbye to
David?s brother, Andrew.?.
5 Experimental Results
This section presents the experiment results and analysis of our approach.
5.1 Data
The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the
merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot
1573
Methods Precision Recall F-measure Accuracy Mean Average Precision
1.Random 28.64% 50.48% 36.54% 50.54% 34%
2.Voting 42.16% 70.18% 52.68% 62.54% 62%
3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60%
4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56%
5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70%
Table 2: Overall Performance Comparison.
Filling (SF) task. The source collection has 1,000,257 newswire documents, 999,999 web documents
and 99,063 discussion forum posts, which results in 10 different sources (combinations of publication
venues and genres) in our experiment. There are 100 queries: 50 person and 50 organization entities.
After removing redundant responses within each single system run, we use 45,950 unique responses as
the input to truth-finding. Linguistic Data Consortium (LDC) human annotators manually assessed all
of these responses and produced 12,844 unique responses as ground truth. In order to compare with
state-of-the-art supervised learning methods for SFV (Tamang and Ji, 2011; Li and Grishman, 2013), we
trained a SVMs classifier
5
as a counterpart, incorporating the same set of linguistic indicators, sources
and systems as features. We picked 10% (every 10th line) to compose the development set for MTM and
the training set for the SVMs. The rest is used for blind test.
5.2 Overall Performance
Table 2 shows the overall performance of various truth finding methods on judging each response as true
or false. MTM achieves promising results and even outperforms supervised learning approach. Table 3
presents some examples ranked at the top and the bottom based on the credibility scores produced by
MTM.
We can see that majority voting across systems performs much better than random assessment, but its
accuracy is still low. For example, the true claim T5 was extracted by only one system because most
systems mistakenly identified ?Briton Stuart Rose? as a person name. In comparison, MTM obtained
much better accuracy by also incorporating multiple dimensions of source and evidence information.
Method 3 using linguistic indicators alone, already achieved promising results. For example, many
claims are judged as truths through trigger phrases (T1 and T5), event extraction (T2), coreference (T4),
and node type indicators (T3). On the other hand, many claims are correctly judged as false because
their evidence sentences did not include the slot filler (F1, F4, F5) or valid knowledge paths to connect
the query entity and the slot filler (F2, F3). The performance gain (2.99% F-score) from Method 3 to
Method 5 shows the need for incorporating system and source dimensions. For example, most truths are
from news while many false claims are from newsgroups and discussion forum posts (F1, F2, F5).
The SVMs model got very low recall because of the following two reasons: (1) It ignored the inter-
dependency between multiple dimensions; (2) the negative instances are dominant in the training data,
so the model is biased towards labeling responses as false.
5.3 Truth Finding Efficiency
Table 3 shows that some truths (T1) are produced from low-ranked systems whereas some false responses
from high-ranked systems (F1, F2). Note that systems are ranked by their performance in KBP SF task.
In order to find all the truths, human assessors need to go through all the responses returned by multiple
systems. This process was proven very tedious and costly (Ji et al., 2010; Tamang and Ji, 2011).
Our MTM approach can expedite this process by ranking responses based on their credibility scores
and asking human to assess the responses with high credibility first. Traditionally, when human assess
responses, they follow an alphabetical order or system IDs in a ?passive learning? style. This is set as
our baseline. For comparison, we also present the results using only linguistic indicators, using voting
in which the responses which get more votes across systems are assessed first, and the oracle method
assessing all correct responses first. Table 2 shows our model can successfully rank trustworthy responses
at high positions compared with other approaches.
5
We used the LIBSVM toolkit (Chang and Lin, 2011) with Gaussian radial basis function kernel.
1574
Response Ranked by MTM
Source
System
Rank
Claim
Evidence
Query Entity Slot Type Slot Filler
Top
Truths
T1 China
Banking
Regulatory
Commission
org:top
member-
s/employees
Liu
Mingkang
Liu Mingkang, the chairman of
the China Banking Regulatory
Commission
Central
News
Agency
of Taiwan
News
News 15
T2 Galleon
Group
org:founded
by
Raj
Rajaratnam
Galleon Group, founded by bil-
lionaire Raj Rajaratnam
New York
Times
News 9
T3 Mike Penner per:age 52 L.A. Times Sportswriter Mike
Penner, 52, Dies
New York
Times
News 1
T4 China
Banking
Regulatory
Commission
org:alternate
names
CBRC ...China Banking Regulatory Com-
mission said in the notice. The five
banks ... according to CBRC.
Xinhua,
News
News 5
T5 Stuart Rose per:origin Briton Bolland, 50, will replace Briton
Stuart Rose at the start of 2010.
Agence
France-
Presse
News 3
Bottom
False
Claims
F1 American
Association
for the Ad-
vancement of
Science
org:top
members
employees
Freedman erica.html &gt; American Library
Association, President: Maurice
Freedman &lt; http://www.aft.org
&gt; American Federation of
Teachers ...
Google Newsgroup4
F2 Jade Goody per:origin Britain because Jade Goody?s the only
person to ever I love Britain
Discussion Forum 3
F3 Don Hewitt per:spouse Swap ...whether ?Wife Swap? on ABC
or ?Jon &amp; Kate? on TLC
New York
Times
News 7
F4 Council of
Mortgage
Lenders
org:website www.cml.org.ukme purchases in the U.K. jumped
by 16 percent in April, suggesting
the property market slump may
have bottomed out
Associated
Press
World-
stream
News 18
F5 Don Hewitt per:alternate
names
Hewitt M-
chen
US DoMIna THOMPson LACtaTe
haVeD [3866 words]
Google Newsgroup13
Table 3: Top and Bottom Response Examples Ranked by MTM.
Fig. 3 summarizes the results from the above 6 approaches. The common end point of all curves
represents the cost and benefit of assessing all system responses. We can see that the baseline is very
inefficient at finding the truths. If we employ linguistic indicators, the process can be dramatically
expedited. MTM provides further significant gains, with performance close to the Oracle. With only half
the cost of the baseline, MTM can already find 90% truths.
5.4 Enhance Individual SF Systems
Finally, as a by-product, our MTM approach can also be exploited to validate the responses from each
individual SF system based on their credibility scores. For fair comparison with the official KBP
evaluation, we use the same ground-truth in KBP2013 and standard precision, recall and F-measure
metrics as defined in (Ji et al., 2011). To increase the chance of including truths which may be particularly
difficult for a system to find, LDC prepared a manual key which was assessed and included in the final
ground truth. According to the SF evaluation setting, F-measure is computed based on the number of
unique true claims. After removing redundancy across multiple systems, there are 1,468 unique true
claims. The cutoff criteria for determining whether a response is true or not was optimized from the
development set.
Fig. 4 presents the F-measure scores of the best run from each individual SF system. We can see that
our MTM approach consistently improves the performance of almost all SF systems, in an absolute gain
range of [-1.22%, 5.70%]. It promotes state-of-the-art SF performance from 33.51% to 35.70%. Our
MTM approach provides more gains to SF systems which mainly rely on lexical or syntactic patterns
than other systems using distant supervision or logic rules.
1575
1 
0 10000 20000 30000 40000
0
2000
4000
6000
8000
10000
12000
14000
13 2
4
5
#tr
uth
s
 6 Oracle 
 5 MTM
 4 SVM
 3 Linguistic Indicator
 2 Voting
 1 Baseline
#total responses
6
Figure 3: Truth Finding Efficiency.
0 2 4 6 8 10 12 14 16 18 20
0
5
10
15
20
25
30
35
F-m
es
au
re 
(%
)
System
 Before
 After
Figure 4: Impact on Individual SF Systems.
1576
6 Conclusions and Future Work
Truth finding has received attention from both Natural Language Processing (NLP) and Data Mining
communities. NLP work has mostly explored linguistic analysis of the content, while Data Mining
work proposed advanced models in resolving conflict information from multiple sources. They have
relative strengths and weaknesses. In this paper we leverage the strengths of these two distinct,
but complementary research paradigms and propose a novel unsupervised multi-dimensional truth-
finding framework incorporating signals both from multiple sources, multiple systems and multiple
evidences based on knowledge graph construction with multi-layer linguistic analysis. Experiments on
a challenging SFV task demonstrated that this framework can find high-quality truths efficiently. In the
future we will focus on exploring more inter-dependencies among responses such as temporal and causal
relations.
Acknowledgments
This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), the U.S. Army Research Office under Cooperative Agreement
No. W911NF-13-1-0193, U.S. National Science Foundation grants IIS-0953149, CNS-0931975,
IIS-1017362, IIS-1320617, IIS-1354329, U.S. DARPA Award No. FA8750-13-2-0041 in the Deep
Exploration and Filtering of Text (DEFT) Program, IBM Faculty Award, Google Research Award,
DTRA, DHS and RPI faculty start-up grant. The views and conclusions contained in this document are
those of the authors and should not be interpreted as representing the official policies, either expressed
or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding any copyright notation here on.
References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, and Z. Ives. 2007. Dbpedia: A nucleus for a web of open data. In
Proc. the 6th International Semantic Web Conference.
L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti. 2010. Probabilistic models to reconcile complex data
from inaccurate data sources. In Proc. Int. Conf. on Advanced Information Systems Engineering (CAiSE?10),
Hammamet, Tunisia, June.
K. Bollacker, R. Cook, and P. Tufts. 2008. Freebase: A shared database of structured general human knowledge.
In Proc. National Conference on Artificial Intelligence.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, Estevam R Hruschka Jr, and Tom M Mitchell. 2010. Toward an
architecture for never-ending language learning. In AAAI.
C. Chang and C. Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent
Systems and Technology (TIST), 2(3):27.
H. Deng, M. R. Lyu, and I. King. 2009. A generalized co-hits algorithm and its application to bipartite graphs. In
Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ?09, pages 239?248, New York, NY, USA. ACM.
X. L. Dong, L. Berti-Equille, and D. Srivastavas. 2009a. Integrating conflicting data: The role of source
dependence. In Proc. 2009 Int. Conf. Very Large Data Bases (VLDB?09), Lyon, France, Aug.
X. L. Dong, L. Berti-Equille, and D. Srivastavas. 2009b. Truth discovery and copying detection in a dynamic
world. In Proc. 2009 Int. Conf. Very Large Data Bases (VLDB?09), Lyon, France, Aug.
A. Galland, S. Abiteboul, A. Marian, and P. Senellart. 2010. Corroborating information from disagreeing views.
In Proc. ACM Int. Conf. on Web Search and Data Mining (WSDM?10), New York, NY, Feb.
L. Ge, J. Gao, X. Yu, W. Fan, and A. Zhang. 2012. Estimating local information trustworthiness via multi-
source joint matrix factorization. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages
876?881. IEEE.
1577
H. Ji, R. Grishman, H. T. Dang, K. Griffitt, and J. Ellis. 2010. An overview of the tac2010 knowledge base
population track. In Proc. Text Analytics Conf. (TAC?10), Gaithersburg, Maryland, Nov.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview of the tac 2011 knowledge base population track. In Text
Analysis Conf. (TAC) 2011.
X. Li and R. Grishman. 2013. Confidence estimation for knowledge base population. In Proc. Recent Advances
in Natural Language Processing (RANLP).
Q. Li and H. Ji. 2014. Incremental joint extraction of entity mentions and relations.
Q. Li, H. Ji, and L. Huang. 2013. Joint event extraction via structured prediction with global features.
M. D. Marneffe, B. Maccartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase
structure parses. In LREC, pages 449,454.
R. Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In
Proc. ACL2004.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled
data. In Proc. ACL2009.
J. Pasternack and D. Roth. 2010. Knowing what to believe (when you already know something). In
Proceedings of the 23rd International Conference on Computational Linguistics, pages 877?885. Association
for Computational Linguistics.
J. Pasternack and D. Roth. 2011. Making better informed trust decisions with generalized fact-finding. In Proc.
2011 Int. Joint Conf. on Artificial Intelligence (IJCAI?11), Barcelona, Spain, July.
J. Pasternack and D. Roth. 2013. Latent credibility analysis. In Proc. WWW 2013.
E. Peserico and L. Pretto. 2009. Score and rank convergence of hits. In Proceedings of the 32nd international
ACM SIGIR conference on Research and development in information retrieval, pages 770?771. ACM.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In
16th international World Wide Web conference (WWW 2007), New York, NY, USA. ACM Press.
S. Tamang and H. Ji. 2011. Adding smarter systems instead of human annotators: Re-ranking for slot filling
system combination. In Proc. CIKM2011 Workshop on Search & Mining Entity-Relationship data, Glasgow,
Scotland, UK, Oct.
VG Vydiswaran, C.X. Zhai, and D. Roth. 2011. Content-driven trust propagation framework. In Proceedings
of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 974?982.
ACM.
D. Wang, L. Kaplan, H. Le, and T. Abdelzaher. 2012. On truth discovery in social sensing: A maximum likelihood
estimation approach. In Proc. ACM/IEEE Int. Conf. on Information Processing in Sensor Networks (IPSN?12),
pages 233?244, Beijing, China, April.
X. Yin and W. Tan. 2011. Semi-supervised truth discovery. In Proc. 2011 Int. World Wide Web Conf. (WWW?11),
Hyderabad, India, March.
X. Yin, J. Han, and P. S. Yu. 2008. Truth discovery with multiple conflicting information providers on the Web.
IEEE Trans. Knowledge and Data Engineering, 20:796?808.
B. Zhao, B. I. P. Rubinstein, J. Gemmell, and J. Han. 2012. A Bayesian approach to discovering truth from
conflicting sources for data integration. In Proc. 2012 Int. Conf. Very Large Data Bases (VLDB?12), Istanbul,
Turkey, Aug.
1578
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 135?139,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Tweet Conversation Annotation Tool
with a Focus on an Arabic Dialect, Moroccan Darija
Stephen Tratz?, Douglas Briesch?, Jamal Laoudi?, and Clare Voss?
?Army Research Laboratory, Adelphi, MD 20783
?ArtisTech, Inc., Fairfax, VA 22030
{stephen.c.tratz.civ,douglas.m.briesch.civ,jamal.laoudi.ctr,clare.r.voss.civ}@mail.mil
Abstract
This paper presents the DATOOL, a graph-
ical tool for annotating conversations con-
sisting of short messages (i.e., tweets), and
the results we obtain in using it to annotate
tweets for Darija, an historically unwritten
Arabic dialect spoken by millions but not
taught in schools and lacking standardiza-
tion and linguistic resources.
With the DATOOL, a native-Darija
speaker annotated hundreds of mixed-
language and mixed-script conversations
at approximately 250 tweets per hour. The
resulting corpus was used in developing
and evaluating Arabic dialect classifiers
described briefly herein.
The DATOOL supports downstream dis-
course analysis of tweeted ?conversations?
by mapping extracted relations such as,
who tweets to whom in which language,
into graph markup formats for analysis in
network visualization tools.
1 Overview
For historically unwritten languages, few textual
resources exist for developing NLP applications
such as machine translation engines. Even when
audio resources are available, difficulties arise
when converting sound to text (Robinson and
Gadelii, 2003). Increasingly, however, with the
widespread use of mobile phones, these languages
are being written in social media such as Twitter.
Not only can these languages be written in multi-
ple scripts, but conversations, and even individual
messages, often involve multiple languages. To
build useful textual resources for documenting and
translating these languages (e.g., bilingual dictio-
naries), tools are needed to assist in language an-
notation for this noisy, multiscript, multilingual
form of communication.
This paper presents the Dialect Annotation Tool
(DATOOL), a graphical tool for annotating conver-
sations consisting of short messages (i.e., tweets),
and the results we obtain in using it to annotate
tweets for Darija, an historically unwritten North
African Arabic dialect spoken by millions but not
taught in schools and lacking in standardardiza-
tion and linguistic resources. The DATOOL can
retrieve the conversation for each tweet on a user?s
timeline or via Apollo (Le et al, 2011) and display
the discourse, enabling annotators to make more
informed decisions. It has integrated classifiers for
automatically annotating data so a user can either
verify or alter the automatically-generated annota-
tions rather than start from scratch. The tool can
also export annotated data to GEPHI (Bastian et
al., 2009), an open source network visualization
tool with many layout algorithms, which will fa-
cilitate future ?code-switching? research.
2 Tool Description
2.1 Version 1.0
The first version of the tool is depicted in Figure
1. It is capable of loading a collection of tweets
and extracting the full conversations they belong
to. Each conversation is displayed within its own
block in the conversation display table. An anno-
tator can mark multiple tweets as Darija (or other
language) by selecting multiple checkboxes in the
lefthand side of the table. Also, if a tweet is writ-
ten in multiple languages, the annotator can anno-
tate the different sections using the Message text
box below the conversation display table.
The tool also calculates user and collection level
summary statistics, which it displays below the
main annotation section.
We worked with a Darija-speaking annotator
during the tool?s development, who provided
valuable feedback, helping to shape the overall
design of the tool and improve its functionality.
135
Figure 1: The Dialect Annotation Tool (DATOOL) displaying a possible Twitter conversation.
Data Annotation Using version 1.0, the annotator
marked up 3013 tweets from 3 users for the pres-
ence of the Darija (approximately 1,000 per user),
averaging about 250 tweets per hour. Of the 1,400
tweets with Arabic script, 1,013 contained Darija.
This annotated data is used to evaluate the Arabic
dialect classifier discussed in Section 3.
2.2 Version 2.0
The second version of the tool contains the ad-
ditional ability to invoke pre-trained classification
models to automatically annotate tweets. The tool
displays the classifier?s judgment confidence next
to each tweet, and the user can set a minimal con-
fidence threshold, below which automatic annota-
tions are hidden. Figure 2 illustrates the new clas-
sification functionality.
2.3 XML Output
The DATOOL stores data in an XML-based for-
mat that can be reloaded for continuing or re-
vising annotation. It can also export four differ-
ent views of the data in Graph Exchange XML
Format (GEXF), a format that can be read by
GEPHI. In the social network view, users are
represented by nodes, and tweets are represented
as directed edges between the nodes. The in-
formation network view displays tweets as nodes
with directed edges between time-ordered tweets
within a conversation. In the social-information
network view, both users and tweets are repre-
sented by nodes, and there are directed edges both
from tweet senders to their tweets and from tweets
to recipients. The social-information network plus
view provides all the information of both the so-
cial network and the information network.
3 Classifier
For the second version of the DATOOL, we inte-
grated an Arabic dialect classifier capable of dis-
tinguishing among Darija, Egyptian, Gulf, Lev-
antine and MSA with the goal of improving the
speed and consistency of the annotation process.
Though language classification is sometimes
viewed as a solved problem (McNamee, 2005),
with some experiments achieving over 99% ac-
curacy (Cavnar and Trenkle, 1994), it is signifi-
cantly more difficult when distinguishing closely-
related languages or short texts (Vatanen et al,
2010; da Silva and Lopes, 2006). The only lan-
guage classification work for distinguishing be-
tween these closely-related Arabic dialects that
we are aware of was performed by Zaidan and
Callison-Burch (2013). They collected web com-
mentary data written in MSA, Egyptian, Levan-
tine, and Gulf and performed dialect identifica-
tion experiments, their strongest classifier achiev-
136
Figure 2: Screenshot showcasing the automatic classification output, including confidence values.
ing 81.0% accuracy.
3.1 Training Data
Since Zaidan and Callison-Burch?s dataset in-
cludes no Darija, we collected Darija exam-
ples from the following sources to augment their
dataset: Moroccan jokes from noktazwina.
com, web pages collected using Darija-specific
query terms with a popular search engine, and
37,538 Arabic script commentary entries from
hespress.com (a Moroccan news website).
Nearly all the joke (N=399) and query term
(N=874) data contained Darija. By contrast, the
commentary data was mostly MSA. To extract
a subset of the commentary entries most likely
to contain Darija, we applied an iterative, semi-
supervised approach similar to that described by
Tratz and Sanfilippo (2007), in which the joke and
query term data were treated as initial seeds and,
in each iteration, a small portion of commentary
data with the highest Darija scores were added to
the training set. After having run this process to
its completion, we examined 131 examples at in-
tervals of 45 from the resulting ranked list of com-
mentary. The 62nd example was the first of these
to have been incorrectly classified as containing
Darija. We thus elected to assume all examples up
to the 61st of the 131 contain Darija, for a total of
2,745 examples (61*45=2,745). As an additional
check, we examined two more commentary entries
from each of the 61 blocks, finding that 118 of 122
contain Darija.
3.2 Initial Classifier
The integrated dialect classifier is a Maximum En-
tropy model (Berger et al, 1996) that we train us-
ing the LIBLINEAR (Fan et al, 2008) toolkit.
In preprocessing, Arabic diacritics are removed,
all non-alphabetic and non-Arabic script charac-
ters are converted to whitespace, and sequences of
any repeating character are collapsed to a single
character. The following set of feature templates
are applied to each of the resulting whitespace-
separated tokens:
? The full token
? ?Shape? of the token?all consonants are replaced by
the letter C, alefs by A, and waws and yehs by W
? First character plus the last character (if length ? 2)
? Character unigrams, bigrams, and trigrams
? The last character of the token plus the first character
of the next token
? Prefixes of length 1, 2, and 3
? Indicators that token starts with mA and
? ends with $
? the next token ends with $
? is length 5 or greater
3.3 LDA Model
As an exploratory effort, we investigated using La-
tent Direchlet Allocation (LDA) (Blei et al, 2003)
as a method of language identification. Unfor-
tunately, using the aforementioned feature tem-
plates, LDA produced topics that corresponded
poorly with the training data labels. But, after
several iterations of feature engineering, the topics
began to reflect the dialect distinctions. Our final
LDA model feature templates are listed below.
? The full token
? Indicators that the token contains
? theh; thal; zah; theh, thal, or zah
? Indicators the token is of length 5+ and starts with
? hah plus yeh, teh, noon, or alef
? seen plus yeh, teh, noon, or alef
? beh plus yeh, teh, noon, or alef
? ghain plus yeh, teh, or noon
? or kaf plus yeh, teh, or noon
? Indicators that token starts with mA and
? ends with $
? the next token ends with $
? is length 5 or greater
The following features produced using the LDA
model for each document are given to the Maxi-
mum Entropy classifier: 1) indicator of the most-
likely cluster, 2) product of scores for each pair of
clusters.
3.4 Classifier Evaluation
We evaluated the versions of the classifier by ap-
plying them to the annotated data discussed in
137
Section 2.1. The initial classifier without the
LDA-derived features achieved 96.9% precision
and 24.1% recall. The version with LDA-derived
features achieved 97.2% precision and 44.1% re-
call, a substantial improvement. Upon review, we
concluded that most cases where the classifier ?in-
correctly? selected the Darija label were due to er-
rors in the gold standard.
4 Analysis of Annotated Conversations
Visualization of Darija in Conversations
The DATOOL may recover the conversation in
which a tweet occurs, providing the annotator with
the tweet?s full, potentially-multilingual context.
To visualize the distribution of Darija1 by script
in ?1K tweets from each user?s conversations, the
DATOOL transforms and exports annotated data
into a GEXF information network (cf. Figure 3),
which can be displayed in GEPHI.2 Currently,
GEPHI displays at most one edge between any two
nodes?GEPHI automatically augments the edge?s
weight for each additional copy of the edge.
The Darija in this user?s conversations, unlike
our two other users, is predominantly Romanized.
With more data, we plan to assess the impact of
one user?s script and language choice on others.
Figure 3: Information network visualization.
Red?contains Romanized Darija; green?
contains Arabic-script Darija; blue?no Darija.
Code-Switching
The alternation of Darija with non-Darija in the
1In our initial annotation work, words and tweets in lan-
guages other than Darija received no markup.
2GEPHI?s Force Atlas layout automatically positions sub-
graphs by size, with larger ones further away from the center.
information network (red and green nodes vs.
blue nodes) within conversations is consistent with
well-known code-switching among Arabic speak-
ers, extending spoken discourse into informal
writing (Bentahila and Davies, 1983; Redouane,
2005). Code-switching also appears within our
tweet corpus where Romanized Darija frequently
alternates with French. Given the prevalence of
code-switching within tweets, future work will en-
tail training a Roman-script classifier at the to-
ken level.3 Since our DATOOL already supports
token-level as well as multi-token, tweet-internal
annotation in the mid-screen Message box, our
current corpus provides a seed set for this effort.
5 Conclusion and Future Work
The DATOOL now supports semi-automated an-
notation of tweet conversations for Darija. As
we scale the process of building low-resource lan-
guage corpora, we will document its impact on an-
notation time when few native speakers are avail-
able, a condition also relevant and critical to pre-
serving endangered languages. We have begun ex-
tending the classifier to support additional Arabic
script languages (e.g., Farsi, Urdu), leveraging re-
sources from others (Bergsma et al, 2012).
Many other open questions remain regarding
the annotation process, the visualizations, and the
human expert. Which classified examples should
the language expert review? When should an an-
notator adjust the confidence threshold in the DA-
TOOL? For deeper linguistic analysis and code-
switching prediction, would seeing participants
and tweets, turn by turn, in network diagrams such
as Figure 4 help experts understand new patterns
emerging in tweet conversations?
Figure 4: Social-Information Network Plus.
3As described in Section 3, our current classifier works at
the tweet level and only on Arabic-script tweets.
138
Acknowledgments
We would like to thank Tarek Abdelzaher for all
his feedback regarding our work and guidance in
using Apollo. We would also like to thank our re-
viewers for their valuable comments and sugges-
tions.
References
Mathieu Bastian, Sebastien Heymann, and Mathieu Ja-
comy. 2009. Gephi: An Open Source Software for
Exploring and Manipulating Networks. In Interna-
tional AAAI Conference on Weblogs and Social Me-
dia.
Abdelali Bentahila and Eirlys E Davies. 1983. The
Syntax of Arabic-French Code-Switching. Lingua,
59(4):301?330.
Adam L. Berger, Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A Maximum En-
tropy Approach to Natural Language Processing.
Computational Linguistics, 22(1):39?71.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
Identification for Creating Language-Specific Twit-
ter Collections. In Proceedings of the 2012 Work-
shop on Language in Social Media (LSM 2012),
pages 65?74.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. The Journal of Machine
Learning Research, 3:993?1022.
William B Cavnar and John M Trenkle. 1994. N-
gram-based text categorization. Ann Arbor MI,
48113(2):161?175.
Joaquim Ferreira da Silva and Gabriel Pereira Lopes.
2006. Identification of document language is not yet
a completely solved problem. In Computational In-
telligence for Modelling, Control and Automation,
2006 and International Conference on Intelligent
Agents, Web Technologies and Internet Commerce,
International Conference on, pages 212?212. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Hieu Khac Le, Jeff Pasternack, Hossein Ahmadi,
M. Gupta, Y. Sun, Tarek F. Abdelzaher, Jiawei Han,
Dan Roth, Boleslaw K. Szymanski, and Sibel Adali.
2011. Apollo: Towards factfinding in participatory
sensing. In IPSN, pages 129?130.
Paul McNamee. 2005. Language identification: A
solved problem suitable for undergraduate instruc-
tion. Journal of Computing Sciences in Colleges,
20(3):94?101.
Rabia Redouane. 2005. Linguistic constraints on
codeswitching and codemixing of bilingual Moroc-
can Arabic-French speakers in Canada. In ISB4:
Proceedings of the 4th International Symposium on
Bilingualism, pages 1921?1933.
Clinton Robinson and Karl Gadelii. 2003. Writing
Unwritten Languages, A Guide to the Process.
http://portal.unesco.org/education/en/ev.php-URL
ID=28300&URL DO=DO TOPIC&URL SECTIO
N=201.html, UNESCO, Paris, France. December.
Stephen Tratz and Antonio Sanfilippo. 2007. A
High Accuracy Method for Semi-supervised Infor-
mation Extraction. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
169?172.
Tommi Vatanen, Jaakko J Va?yrynen, and Sami Virpi-
oja. 2010. Language identification of short text seg-
ments with n-gram models. In Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation LREC?10.
Omar Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguistics
(To Appear).
139
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 43?47,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Collaborative Exploration in Human-Robot Teams:
What?s in Their Corpora of Dialog, Video, & LIDAR Messages?
Clare R. Voss
?
Taylor Cassidy
??
Douglas Summers-Stay
?
?
Army Research Laboratory, Adelphi, MD 20783
?
IBM T. J. Watson Research Center, Hawthorne, NY 10532
{clare.r.voss.civ,taylor.cassidy.ctr,douglas.a.summers-stay.civ}@mail.mil
Abstract
This paper briefly sketches new work-in-
progress (i) developing task-based scenar-
ios where human-robot teams collabora-
tively explore real-world environments in
which the robot is immersed but the hu-
mans are not, (ii) extracting and construct-
ing ?multi-modal interval corpora? from
dialog, video, and LIDAR messages that
were recorded in ROS bagfiles during task
sessions, and (iii) testing automated meth-
ods to identify, track, and align co-referent
content both within and across modalities
in these interval corpora. The pre-pilot
study and its corpora provide a unique,
empirical starting point for our longer-
term research objective: characterizing the
balance of explicitly shared and tacitly as-
sumed information exchanged during ef-
fective teamwork.
1 Overview
Robots that are able to move into areas where peo-
ple cannot during emergencies and collaboratively
explore these environments by teaming with hu-
mans, have tremendous potential to impact search
and rescue operations. For human-robot teams
to conduct such shared missions, humans need to
trust that they will be kept apprised, at a miniu-
Figure 1: Outside View: Video Image & LIDAR.
mum, of where the robot is and what it is sensing,
as it moves about without them present.
To begin documenting the communication chal-
lenges humans face in taking a robot?s perspective,
we conducted a pre-pilot study
1
to record, iden-
tify and track the dialog, video, and LIDAR in-
formation that is explicitly shared by, or indirectly
available to, members of human-robot teams when
conducting collaborative tasks.
1.1 Approach
We enlisted colleagues to be the commander (C) or
the human (R) controlling a mobile physical robot
in such tasks. Neither could see the robot. Only
R could ?see for? the robot, via its onboard video
camera and LIDAR. C and R communicated by
text chat on their computers, as in this example,
R 41: I can see in the entrance.
C 42: Enter and scan the first room.
R 44: I see a door to the right and a door to the left.
C 45: Scan next open room on left.
Utterances R 41 & C 42 occur when the robot is
outdoors (Fig. 1) and R 44 & C 45 occur after it
moves indoors (Fig. 2). Although our approach re-
sembles a Wizard and Oz paradigm (Riek, 2012),
1
Statisticians say pre-pilots are for ?kicking the tires,?
early-stage tests of scenarios, equipment, and data collection.
Figure 2: Inside View: Video Image & LIDAR.
Brightness and contrast of video image increased
for print publication.
43
with C as User and R as Wizard controlling the
robot, there is no intent for R to deceive C.
In these dialog snippets, notice that the doors
mentioned in R 44 are not visible in the image
of that utterance?s time interval and, even if they
had been visible, their referents were context-
dependent and ambiguous. How are the robot and
human to refer to the same door? This challenge
entails resolving several types of co-reference (lin-
guistic, are they talking about the same door? vi-
sual, are they looking at the door? navigational, is
one backing into a door no longer in view but pre-
viosuly stored in its map?) Successful communi-
cation on human-robot teams, where humans send
messages to direct robot movements and receive
robot-processed messages as the robot navigates,
entails effective identification of named referents
(such as doors), both within and across available
modalities during exploratory tasks. The research
question is, how might the identification and align-
ment of entities using combinations of (i) NLP
on dialog, (ii) image processing on the video and
LIDAR stream, with (iii) robot position, motion,
and orientation coordinates, support more effec-
tive human-robot missions?
We conducted the pre-pilot study with ten trial
sessions to collect multi-modal data from C-R and
R-only scenarios (Table 1). Each session involved
a single participant playing the role of R with con-
trol over the physical robot, or two participants,
one person playing R and one playing C.
Team R?s Task
R only Rotate in place and describe surroundings.
R only Move along road, describe surroundings.
C, R Follow C?s guidance in navigating build-
ing?s perimeter, describe surroundings.
C, R Follow C?s guidance in searching buildings
for specified objects.
Table 1: Pre-pilot Scenarios.
Participants sat indoors and could not see the robot
outside, roughly 30 meters away. In each session,
R was instructed to act as though he or she were
situated in the robot?s position and to obey C. R
was to consider the robot?s actions as R?s own,
and to consider available video and LIDAR point
cloud feeds as R?s own perceptions.
1.2 Equipment
All participants worked from their own comput-
ers. Each was instructed, for a given scenario, to
be either C or R and to communicate by text only.
On their screen they saw a dedicated dialog (chat)
window in a Linux terminal. For sessions with
both C and R, the same dialog content (the ongo-
ing sequence of typed-in utterances) appeared in
the dialog window on each of their screens.
The physical robot ran under the Robot Operat-
ing System (ROS) (Quigley et al., 2009), equipped
with a video camera, laser sensors, magnetome-
ter, GPS unit, and rotary encoders. R could ?see
for the robot? via two ROS rviz windows with live
feeds for video from the robot?s camera and con-
structed 3D point cloud frames.
2
R had access to
rotate and zoom functions to alter the screen dis-
play of the point cloud. C saw only a static bird?s-
eye-view map of the area. R remotely controlled
over a network connection the robot?s four wheels
and its motion, using the left joystick of an X-Box
controller.
1.3 Collection
During each session, all data from the robot?s sen-
sors and dialog window was recorded via the ros-
bag tool and stored in a single bagfile.
3
A bagfile
contains typed messages. Each message contains
a timestamp (specified at nanosecond granularity)
and values for that message type?s attributes. Mes-
sage types geometry msgs/PoseStamped, for ex-
ample, contain a time stamp, a three-dimensional
location vector and a four-dimensional orientation
vector that indicates an estimate of the robot?s lo-
cation and the direction in which it is facing. The
robot?s rotary encoders generate these messages
as the robot moves. The primary bagfile message
types most relevant to our initial analyses
4
were:
1) instant messenger/StringStamped
that included speaker id, text utterances
2) sensor msgs/PointCloud2
that included LIDAR data
3) sensor msgs/CompressedImage
with compressed, rectified video images
4) sensor msgs/GPS, with robot coordinates
Message types are packaged and published at dif-
ferent rates: some are published automatically at
regular intervals (e.g., image frames), while oth-
ers depend on R, C, or robot activity (e.g., dialog
utterances). And the specific rate of publication
for some message types can be limited at times by
network bandwidth constraints (e.g. LIDAR data).
Summary statistics for our initial pre-pilot collec-
2
LIDAR measures distance from robot by illuminating
targets with robot lasers and generates point cloud messages.
3
http://wiki.ros.org/rosbag
4
We omit here details of ROS topics, transformation mes-
sages, and other sensor data collected in the pre-pilot.
44
tion consisting of ten task sessions conducted over
two days, and that together spanned roughly five
hours in real-time, are presented in Table 2.
#bagfile msgs 15, 131K #dialog utts 434
min per sn 140, 848 min per sn 15
max per sn 3, 030K max per sn 116
#tokens 3, 750 #image msgs 10, 650
min per sn 200 min per sn 417
max per sn 793 max per sn 1, 894
#unique words 568 #LIDAR msgs 8, 422
min per sn 84 min per sn 215
max per sn 176 max per sn 2, 250
Table 2: Collection Statistics (sn = session).
2 From Collection to Interval Corpora
After collecting millions of messages in the pre-
pilot with content in different modalities, the im-
mediate research challenge has been identifying
the time interval that covers the messages directly
related to the content in each utterance.
We extracted each utterance message u and its
corresponding time stamp t. For a given u, we ex-
tracted the five image, five point cloud, and five
GPS messages immediately preceding and the five
of each immediately following u, based on mes-
sage time-stamps, for a total of thirty sensor mes-
sages per utterance. These message types were
published independent of the robot?s movement,
approximately once per second. In the second
phase, we assigned the earliest and latest time
stamp from the first-phase messages to delimit an
interval [t
s
, t
e
] and conducted another extraction
round from the bagfile, this time pulling out all
messages with time stamps in that interval as pub-
lished by the rotary encoders, compass, and iner-
tial measurement unit, only when the robot moved.
The messages from both phases constitute a ten-
second interval corpus for u.
These interval corpora serve as a first approx-
imation at segmenting the massive stream pub-
lished at nanosecond-level into units pertaining to
commander-robot dialog during the task at hand.
With manual inspection, we found that many
automatically-constructed intervals do track rele-
vant changes in the robot?s location. For exam-
ple, the latest interval in a task?s time sequence
that was constructed with the robot being outside a
building is distinct from the first interval that cov-
ers when the robot moves inside the building.
5
5
This appears likely due to the paced descriptions in R?s
utterances. Another pre-pilot is needed to test this hypothesis.
3 Corpora Language Processing
Each utterance collected from the sessions was
tokenized, parsed, and semantically interpreted
using SLURP (Brooks et al., 2012), a well-
tested NLP front-end component of a human-robot
system.
6
The progression in SLURP?s analysis
pipeline for utterance C 45 is shown in Figure 3.
SLURP extracts a parse tree (top-left), identifies
a sub-tree that constitutes a verb-argument struc-
ture, and enumerates possibly matching sense-
specific verb frames from VerbNet (Schuler, 2005)
(bottom-left). VerbNet provides a syntactic to se-
mantic role mapping for each frame (top-right).
SLURP selects the best mapping and generates a
compact semantic representation (bottom-right).
7
In this example, the correct sense of ?scan? is se-
lected (investigate-35.4) along with a frame that
matches the syntactic parse. Overall, half the com-
mands run through SLURP generated a semantic
interpretation. Of the other half, roughly one quar-
ter failed or had errors at parsing and the other
quarter at the argument matching stage.
Figure 3: Analyses of Scan next open room on left.
Our next step is to augment SLURP?s lexicon
and retrain a parser for new vocabulary so that we
can directly map semantic structures of the pre-
pilot corpora into ResearchCyc
8
, an extensive on-
tology, for cross-reference to other events and ob-
jects, already stored and possibly originated as vi-
sual input. Following McFate (2010), we will test
6
https://github.com/PennNLP/SLURP.
7
Verbnet associates each frame with a conjunction of
boolean semantic predicates that specify how and when event
participants interact, for an event variable (not shown).
8
ResearchCyc and CycL are trademarks of Cycorp, Inc.
45
Figure 4: Outside View: Image, Zones, Overlay
the mapping of matched VerbNet frames to Re-
searchCyc?s semantic predicates to assess its lexi-
cal coverage for our corpora.
4 Image Processing
Interval corpus images were labelled by a neu-
ral network trained for visual scene classifica-
tion (Munoz, 2013) of nine material classes: dirt,
foliage, grass, road, sidewalk, sky, wall, wood, and
ground cover (organic debris). Figures 4 and 5
show the images from Figures 1 and 2 with two
additional versions: one with colored zones for
system-recognized class boundaries and another
with colored zones as trasparent overlays on the
original. The classes differentiate terrain types
that work well with route-finding techniques that
leverage them in selecting traversible paths. As the
robot systems are enhanced with more sophisti-
cated path planning software, that knowledge may
be combined with recognized zones to send team
members messages about navigation problems as
the robot explores where they cannot go.
Accuracy is limited at the single image level:
the actual grass in Figure 4 is mostly mis-classified
as dirt (blue) along with some correctly identified
grass (green), while the floor in Figure 5 is mis-
classified as road, although much of what shows
through the window is correctly classified as fo-
liage. We are experimenting with automatically
assigning natural language (NL) labels to a range
of objects and textures recognized in images from
other larger datasets. We can retrieve labeled im-
ages stored in ResearchCyc via NL query con-
verted into CycL, allowing a commander to, for
example, ask questions about objects and regions
using terms related to but not necessarily equal to
the original recognition system-provided labels.
5 Related Work
We are aware of no other multi-modal corpora
obtained from human-robot teams conducting ex-
ploratory missions with collected dialog, video
and other sensor data. Corpora with a robot
Figure 5: Inside View: Image, Zones, Overlay.
Brightness and contrast of video image and over-
lay increased for print publication.
recording similar data modalities do exist (Green
et al., 2006; Wienke et al., 2012; Maas et al., 2006)
but for fundamentally different tasks. Tellex et al.
(2011) and Matuszek et al. (2012) pair commands
with formal plans without dialog and Zender et al.
(2008) and Randelli et al. (2013) build multi-level
maps but with a situated commander.
Eberhard et al. (2010)?s CReST corpus contains
a set-up similar to ours minus the robot; a hu-
man task-solver wears a forward-facing camera
instead. The SCARE corpus (Stoia et al., 2008)
records similar modalities but in a virtual environ-
ment, where C has full access to R?s video feed.
Other projects yielded corpora from virtual envi-
ronments that include route descriptions without
dialog (Marge and Rudnicky, 2011; MacMahon et
al., 2006; Vogel and Jurafsky, 2010) or referring
expressions without routes (Sch?utte et al., 2010;
Fang et al., 2013), assuming pre-existing abstrac-
tions from sensor data.
6 Conclusion and Ongoing Work
We have presented our pre-pilot study with data
collection and corpus construction phases. This
work-in-progress requires further analysis. We are
now processing dialog utterances for more system-
atic semantic interpretation using disambiguated
VerbNet frames that map into ResearchCyc pred-
icates. We will run object recognition software
retrained on a broader range of objects so that
it can be applied to images that will be labelled
and stored in ResearchCyc micro-worlds for sub-
sequent co-reference with terms in the dialog ut-
terances. Ultimately we want to establish in real
time links across parts of messages in different
modalities that refer to the same abstract enti-
ties, so that humans and robots can share their
separately-obtained knowledge about the entities
and their spatial relations ? whether seen, sensed,
described, or inferred ? when communicating on
shared tasks in environments.
46
Acknowledgments
Over a dozen engineers and researchers assisted
us in many ways before, during, and after the pre-
pilot, providing technical help with equipment and
data collection, as well as participating in the pre-
pilot. We cannot list everyone here, but special
thanks to Stuart Young for providing clear guid-
ance to everyone working with us.
References
Daniel J. Brooks, Constantine Lignos, Cameron Finu-
cane, Mikhail S. Medvedev, Ian Perera, Vasumathi
Raman, Hadas Kress-Gazit, Mitch Marcus, and
Holly A. Yanco. 2012. Make it so: Continu-
ous, flexible natural language interaction with an au-
tonomous robot. In Proc. AAAI, pages 2?8.
Kathleen M. Eberhard, Hannele Nicholson, Sandra
K?ubler, Susan Gundersen, and Matthias Scheutz.
2010. The indiana ?cooperative remote search task?
(crest) corpus. In Proc. LREC.
Rui Fang, Changsong Liu, Lanbo She, and Joyce Y.
Chai. 2013. Towards situated dialogue: Revisiting
referring expression generation. In Proc. EMNLP,
pages 392?402.
Anders Green, Helge Httenrauch, and Kerstin Severin-
son Eklundh. 2006. Developing a contextualized
multimodal corpus for human-robot interaction. In
Proc. LREC.
Jan F. Maas, Britta Wrede, and Gerhard Sagerer. 2006.
Towards a multimodal topic tracking system for a
mobile robot. In Proc. INTERSPEECH.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, and action in route instructions. In Proc.
AAAI, pages 1475?1482.
Matthew Marge and Alexander I Rudnicky. 2011.
The teamtalk corpus: Route instructions in open
spaces. In Proc. RSS, Workshop on Grounding
Human-Robot Dialog for Spatial Tasks.
Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer,
and Dieter Fox. 2012. Learning to parse natural
language commands to a robot control system. In
Proc. ISER, pages 403?415.
Clifton McFate. 2010. Expanding verb coverage in
cyc with verbnet. In Proc. ACL, Student Research
Workshop, pages 61?66.
Daniel Munoz. 2013. Inference Machines: Pars-
ing Scenes via Iterated Predictions. Ph.D. thesis,
Carnegie Mellon University.
Morgan Quigley, Ken Conley, Brian Gerkey, Josh
Faust, Tully B. Foote, Jeremy Leibs, Rob Wheeler,
and Andrew Y. Ng. 2009. ROS: an open-source
robot operating system. In Proc. ICRA, Workshop
on Open Source Software.
Gabriele Randelli, Taigo Maria Bonanni, Luca Iocchi,
and Daniele Nardi. 2013. Knowledge acquisition
through human?robot multimodal interaction. Intel-
ligent Service Robotics, 6(1):19?31.
Laurel D Riek. 2012. Wizard of oz studies in hri:
A systematic review and new reporting guidelines.
Journal of Human-Robot Interaction, 1(1).
Karin Kipper Schuler. 2005. Verbnet: A Broad-
coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Niels Sch?utte, John D. Kelleher, and Brian Mac
Namee. 2010. Visual salience and reference reso-
lution in situated dialogues: A corpus-based evalu-
ation. In Proc. AAAI, Fall Symposium: Dialog with
Robots.
Laura Stoia, Darla Magdalena Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2008. Scare: a situ-
ated corpus with annotated referring expressions. In
Proc. LREC.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth J.
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. AAAI.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proc. ACL, pages
806?814.
Johannes Wienke, David Klotz, and Sebastian Wrede.
2012. A framework for the acquisition of mul-
timodal human-robot interaction data sets with a
whole-system perspective. In Proc. LREC, Work-
shop on Multimodal Corpora for Machine Learning.
Hendrik Zender, O Mart??nez Mozos, Patric Jensfelt, G-
JM Kruijff, and Wolfram Burgard. 2008. Concep-
tual spatial representations for indoor mobile robots.
Robotics and Autonomous Systems, 56(6):493?502.
47
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 42?47,
Gothenburg, Sweden, April 27, 2014.
c
?2014 Association for Computational Linguistics
Resumptive Pronoun Detection
for Modern Standard Arabic to English MT
Stephen Tratz
?
Clare Voss
?
Jamal Laoudi
?
?
Army Research Laboratory, Adelphi, MD 20783
?
Advanced Resource Technologies, Inc. Alexandria, VA 22314
{stephen.c.tratz.civ,clare.r.voss.civ,jamal.laoudi.ctr}@mail.mil
Abstract
Many languages, including Modern Stan-
dard Arabic (MSA), insert resumptive pro-
nouns in relative clauses, whereas many
others, such as English, do not, using
empty categories instead. This discrep-
ancy is a source of difficulty when trans-
lating between these languages because
there are words in one language that cor-
respond to empty categories in the other,
and these words must either be inserted
or deleted?depending on translation di-
rection. In this paper, we first examine
challenges presented by resumptive pro-
nouns in MSA-English translations and re-
view resumptive pronoun translations gen-
erated by a popular online MSA-English
MT engine. We then present what is, to
the best of our knowledge, the first system
for automatic identification of resumptive
pronouns. The system achieves 91.9 F1
and 77.8 F1 on Arabic Treebank data when
using gold standard parses and automatic
parses, respectively.
1 Introduction
One of the challenges for modern machine trans-
lation (MT) is the need to systematically insert
or delete information that is overtly expressed
in only one of the languages in order to main-
tain intelligibility and/or fluency. For example,
word alignment between pro-drop and non-pro-
drop languages can be negatively impacted by the
systematic dropping of pronouns in only one of the
languages (Xiang et al., 2013). A similar type of
linguistic phenomenon of great interest to linguists
that has not yet received significant attention in
MT research is the mismatch between languages
in their usage of resumptive pronouns. Some lan-
guages, such as Modern Standard Arabic (MSA),
require the insertion of resumptive pronouns in
many relative clauses, whereas other languages,
including English, rarely permit them. An exam-
ple of an MSA sentence is given below, with its
English gloss showing the resumptive pronoun in
bold, its reference translation (RT), and an MT
system output where the roles of patient and doc-
tor are incorrectly reversed:

?J
.
fi


J
.
??@ ?

K
	
Y

?
	
K

@ ?


	
Y?@
	
?fl


Q?
?
@

IK



@P
Gloss: I.saw the.patient who rescued.him the.doctor.
RT: I saw the patient whom the doctor rescued.
MT: I saw a patient who rescued the doctor.
In this paper, we examine translations pro-
duced by a popular online translation system for
MSA resumptive pronouns occurring in several
different syntactic positions to gain insight into
the types of errors generated by current MT en-
gines. In a test suite of 300 MSA sentences with
resumptive pronouns, over 30% of the relative
clauses with resumptive pronouns were translated
inaccurately. We then present an automatic classi-
fier that we built for identifying MSA resumptive
pronouns and the results obtained from using it in
experiments with the Arabic Treebank (Maamouri
et al., 2004; Maamouri and Bies, 2004). The
system achieves 91.9 F1 and 77.8 F1 on Arabic
Treebank data when using gold standard parses
and automatic parses, respectively. To the best
of our knowledge, this is the first attempt to
automatically identify resumptive pronouns in any
language.
2 Relevant MSA Linguistics
MSA and English relative clauses differ in struc-
ture, with one of the most prominent differences
being in regard to resumptive pronouns. Resump-
tive pronouns are required in many MSA rela-
tive clauses but are almost never grammatical in
English. In MSA, like English, if the external
42
Arabic (. . .
	
?Q?

@) Gloss (I know...) English RT (I know...) MT Output (I know...)
1a @
Q
ffi



J? ???

Jfi
.

K ?



?? @

?YJ


??@ the+lady who
i

i
smiles a lot the lady who
i

i
smiles a lot the lady who smiles a lot
1b @
Q
ffi



J? ???

Jfi
.

K

?YJ


? lady ?
i
smiles 
i
a lot a lady who
i

i
smiles a lot a lot lady smiling
1c @
Q
ffi



J? ???

Jfi
.
K


	
?? who
i
smiles 
i
a lot who
i

i
smiles a lot a lot of smiles
2a ?g
.
Q?@ A???? ?



?? @

??Q?

?? @ the+company that
i
financed+it
i
the+man the company that
i
the man financed 
i
the company that financed the man
2b ?g
.
Q?@ A????

??Q?

? company ?
i
financed+it
i
the+man a company ?
i
the man financed 
i
a company funded by the man
2c ?g
.
Q?@ ???? A? what
i
financed+it
i
the+man what
i
the man financed 
i
what the man-funded
3a ???

?A

J
	
?? @

I???

K ?


	
Y?@ Y???@ the+boy whom
i
talked the+girl with+him
i
the boy whom
i
the girl talked with 
i
the boy who spoke with the girl
3b ???

?A

J
	
?? @

I???

K @Y?? boy ?
i
talked the+girl with+him
i
a boy ?
i
the girl talked with 
i
the girl was born I spoke with him
3c

?A

J
	
?? @

I???

K
	
?? ?? [with whom]
i
talked the+girl 
i
[with whom]
i
the girl talked 
i
from speaking with the girl
4a ??
	
Q
	
ffi? PA?
	
E @ ?


	
Y?@ ?g
.
Q?@ the+man who
i
collapsed house+his
i
the man [whose house]
i

i
collapsed a man who collapsed home
4b ??
	
Q
	
ffi? PA?
	
E @ Cg
.
P man ?
i
collapsed house+his
i
a man [whose house]
i

i
collapsed a man of his house collapsed
4c ??
	
Q
	
ffi? PA?
	
E @
	
?? who
i
collapsed house+his
i
[whose house]
i

i
collapsed of his house collapsed
5 ?



??
	
J? ?? A? what
i
it
i
logical what
i

i
is logical what is logical
Table 1: A list of MSA sentences starting with relative clauses
	
?Q?

@ (translation: I know) along with their
English glosses, English reference translation (RT), and the output of MT system X. Empty categories
are indicated with  and empty WH nodes are indicated with ?. Subscripts indicate coreference. To
avoid clutter, the glosses do not explicitly indicate person, number, or gender.
antecedent plays the role of the subject, no re-
sumptive pronoun is inserted
1
; instead, MSA in-
flects the verb to agree with the subject in number
and gender by attaching an affix
2
. A second sig-
nificant difference between the two languages is
that, in MSA, relative pronouns are required for
relative clauses modifying definite noun phrases
but are prohibited when modifying indefinite noun
phrases; in English, definitiveness neither prevents
nor necessitates the inclusion of a relative pro-
noun. A third significant difference is that, for free
relative clauses?that is, relative clauses that are
not attached to an external antecedent?MSA has
a different set of relative pronouns for introducing
the clause
3
. A fourth challenge is that MSA has no
equivalent word for the English word ?whose? and,
to convey a similar meaning, employs resumptive
pronouns as possessive modifiers. Examples illus-
trating these differences are provided in Table 1.
For further background on MSA relative clauses
and MSA grammar, we refer readers to books by
Ryding (2005) and Badawi et al. (2004).
1
A notable exception to this rule is for equational sen-
tences. MSA lacks an overt copula corresponding to the En-
glish word ?is? and, to convey a similar meaning, resumptive
subject pronouns must be inserted in these contexts.
2
In standard VSO and VOS constructions, the verbs in-
flect as singular regardless of the number of the subject.
3
These pronouns are also employed to introduce ques-
tions.
3 Data
In our research, we rely on the conversion of con-
stituent into dependency structures and the train-
ing/dev/test splits of the Arabic Treebank (ATB)
parts 1, 2, & 3 (Maamouri et al., 2004; Maamouri
and Bies, 2004) as presented by Tratz (2013).
We extract features from labeled dependency trees
(rather than constituent trees) generated by Tratz?s
(2013) Arabic NLP system, which separates cli-
tics, labels parts-of-speech, produces dependency
parses, and identifies and labels affixes.
The original ATB dependency conversion does
not mark pronouns for resumptiveness, so we
modify the conversion process to obtain this infor-
mation. The original ATB constituent trees mark
this by labeling WHNP nodes and NP nodes with
identical indices. If the NP node corresponds to a
null subject and the head of the S under the SBAR
is a verb, we mark the inflectional affix on the
verb, which agrees with the subject in gender and
number, as resumptive. These inflectional affixes
are included as their own category within our anal-
yses since their presence precludes the appearance
of another resumptive pronoun within the relative
clause (e.g., as a direct object).
The total number of resumptive pronouns and
?resumptive? inflectional affixes in the training,
dev, and test sections are presented in Table 2. In
43
Training Dev Test
Pronouns 5775 794 796
Inflectional affixes 6161 807 845
Table 2: Number of resumptive pronouns and ?re-
sumptive? inflectional affixes by data section.
the training data, the four most likely positions
4
for the resumptive pronouns are:
i) direct object of relative clause?s main verb (33.9%)
ii) object of a preposition attached to the verb (20.8%)
iii) possessive modifier of the subject of the verb (5.4%)
iv) subject pronoun in an equational sentence (4.2%).
4 Translation Error Analysis
As an exploratory exercise to gain insight into the
types of errors generated by current MT engines
when translating from a language that inserts re-
sumptive pronouns (i.e., MSA) to one that doesn?t
(i.e., English), we worked with a native Arabic
speaker to produce a list of Arabic sentences that
vary in terms of definitiveness (and existence, as
with free relatives) of the external antecedent, and
the syntactic position of the resumptive pronoun,
along with English glosses and reference transla-
tions for these sentences. This set was then pro-
cessed using a popular online translation system,
which we refer to as system X. The sentences,
their glosses, reference translations, and automatic
translations are presented in Table 1.
Although system X did not typically produce
English pronouns corresponding to the resumptive
pronouns in the source, most of the translations
proved problematic, with many of the issues be-
ing related to reordering. Thus, while system X
appears to be good at not translating resumptive
pronouns, its performance on the relative clauses
that contain them has ample room for improve-
ment. Our working hypothesis is that system X?s
English language model is effective in discount-
ing candidate translations that keep the resumptive
pronoun.
As a second exploratory exercise, we automat-
ically extracted all the resumptive pronoun exam-
ples in the training section of the data described
in Section 3 and grouped them based upon the se-
quence of dependency arc labels from the resump-
tive pronoun up to the head of the relative clause
4
Examples of these frequent configurations are in Table 1.
and the first letter of the POS tag of the interven-
ing words (e.g., ?N? for noun, ?A? for adjective).
For each of the thirty most common configura-
tions, we took ten examples (for a total of 300), ran
them through system X?s Arabic-English model
and gave both the translation and the source text
to our native Arabic expert. Our expert examined
whether 1) the translation engine generated a pro-
noun corresponding to the source side resumptive
pronoun and 2) whether the translation was correct
locally within the relative clause (whether the pro-
noun was retained or not)
5
. The results for these
two judgments are presented in Table 3.
Corresponding Pronoun?
Yes No
Correct?
Yes 17 189
No 20 74
Table 3: Expert judgments
Our expert concluded that a corresponding En-
glish pronoun was produced in only 37 of the
300 examples (12.3%). Seventeen of these were
judged correct, although in many of these cases a
significant portion of the relative clause was trans-
lated incorrectly even though a small portion in-
cluding the pronoun was translated properly, mak-
ing judgment difficult. Our expert noted that many
of the correct translations involved switching the
voice of the verb in the relative clause from ac-
tive to passive voice using a past participle. Of
the 189 that had no corresponding pronoun and
were judged correct, 46 (24.3%) involved switch-
ing to passive voice. In general, it appears that
system X does a good job at not generating En-
glish pronouns corresponding to MSA resumptive
pronouns, although it makes numerous mistakes
with the data we presented to it.
5 System Description
Our MSA resumptive pronoun identification sys-
tem processes one sentence at a time and relies
upon the (averaged) structured perceptron algo-
rithm (Collins, 2002) to rank the feasible actions.
When processing a sentence containing n pro-
nouns and affixes, a total of n iterations are per-
formed. During each processing iteration, the
system considers two actions for every unlabeled
5
This latter task was challenging, but permitted, as in-
tended, lenient judgment of the MT output.
44
Function Definitions:
path(x) ? returns a list of dependency arcs from x up through the first ?ripcmp?, ?rcmod?, or ?ROOT? arc (link from affix to the
core word is also treated as an arc)
rDescendants(x) ? returns a list of paths (dependency arc lists) from x to each descendant already marked as resumptive
pDescendants(x) ? returns a list of paths (dependency arc lists) from x to each pronoun / verbal inflectional affix, not following
?cc?, ?ripcmp?, or ?rcmod? arcs
hasDepArc(x,y) ? returns a Boolean value indicating if an arc with label y descends from x
pathToString(x) ? concatenates the labels of the arcs in a list to create a string
last(x) ? returns the last element in the list x
split(x, y) ? splits a string x apart wherever it contains substring y, returning these pieces
deps(x), parent(x) ? return dependency arc(s) of which x is the {head, child}
head(x), child(x) ? returns the {head, child} of arc x
pro(x) ? if x is an affix, the word attached to it is returned, otherwise x is returned
l(x) ? return the label/part-of-speech for a dependency arc, affix, or word
T(x), t(x), suffixes(x) ? return the {type (?affix? or ?pro?), written text, suffixes} for x
n(x,y) ? returns the word node that is y words after pro(x)
Given: p ? pronoun or inflectional affix
Pseudocode:
?0:?+T(p), ?1:?+t(p), ?2:?+l(p), ?3:?+l(parent(p)), for(s in split(l(p),? ?)) { ?4:?+s }
if(T(p)=?affix?) { for(a in deps(pro(p))) { ?5:?+l(a) }, if(T(p)=?pro? or not(hasDepArc(pro(p), ?subj?))) { ?6? }
for(i in {-3,-2,-1,0,+1,+2,+3,+4}) { ?7:?+i+t(n(pro(p),i)), ?8:?+i+l(n(pro(p),i)), ?9:?+i+l(parent(n(pro(p),i))) }
?10:?+pathToString(path(p)), end := last(path(p)), resumptives := rDescendants(child(end))
if(l(end) != ?ROOT?) {
if(size(resumptives) > 0) {?11a? } else {?11b?+(size(pDescendants(child(end))) > 0)}
for(s in split(l(head(end)), ? ?)) ?12:?+s, for(arc in path(p)) { ?13?+l(arc) }
?14:?+t(head(end)), ?15:?+l(head(end)), ?16:?+l(parent(head(end)))
?17:?+t(child(end)), ?18:?+l(child(end)), ?19:?+l(parent(child(end)))
if(l(child(end)) = ?VB PV? and size(suffixes(child(end)))=0) { ?20? }
for(suff in suffixes(head(end))) { for(s in split(l(suff), ? ?)) { ?21:?+suff }} }
Figure 1: Pseudocode for feature production. Statements in bold font produce strings that are used to
identify features. The feature set consists of all pairwise combinations of these strings.
personal pronoun and inflectional verbal affix
6
within a given sentence, these actions being label-
as-?resumptive? and label-as-?not-resumptive?.
The highest scored action is performed and the
newly-labeled pronoun or affix is removed from
further processing.
The system scores each action by computing the
dot product between the feature vector derived for
the pronoun/inflectional affix and the weight vec-
tor. The feature vectors consist entirely of Boolean
values, each of which indicates the presence or ab-
sence of a particular feature. Each feature is iden-
tified by a unique string and these strings are gen-
erated using the pseudocode presented in Figure
1. (All pairwise combinations of the strings gen-
erated by the pseudocode are included as features.)
For space reasons, we omit a review of the train-
ing procedure for the structured perceptron and re-
fer the interested reader to work by Goldberg and
Elhadad (2010).
6
Occasionally an imperfect verb will have both a written
inflectional prefix and a written inflectional suffix. For these
cases, the system only considers the prefix as there is no need
to make two separate judgments.
6 Experiments
We trained our system on the training data us-
ing the gold standard clitic segmentation, parse,
and part-of-speech information and optimized it
for overall F1 (pronouns and inflectional affixes
combined) on the development data. Performance
peaked on training iteration 8, and we applied the
resulting model to two treatments of the test data,
once using the gold standard annotation and once
using the Tratz (2013) Arabic NLP system to au-
tomatically pre-process the data.
6.1 Results and Discussion
The scores for the development and test sections,
both for gold and automatic annotation, are pre-
sented in Table 4.
The system performs well when given input
with gold standard clitic segmentation, POS tags,
and dependency parses, achieving 91.9 F1 for re-
sumptive pronouns on the test set and 95.4 F1 for
the affixes. Performance however degrades sub-
stantially when automatic pre-processing of the
source is input instead. Some of this drop can
be explained by the use of gold standard markup
in training?more weight was likely assigned to
45
Pronoun Inflectional Affix
P R F1 P R F1
Dev
Gold 92.5 92.8 92.6 96.7 96.4 96.5
Auto 88.0 81.0 84.4 86.1 77.3 81.5
Test
Gold 92.1 91.7 91.9 95.0 95.9 95.4
Auto 83.6 72.8 77.8 86.6 76.0 81.0
Table 4: Precision, recall, and F1 results for the
?is-resumptive? label on the development and test
sets for gold standard clitic separation/POS tag-
ging/parsing and automatic preprocessing.
parse and POS tag-related features than would
have if automatic pre-processing of the source had
been used in training.
Having examined the classification system er-
rors on the development data, we conclude that
the main source of this drop is due to poor iden-
tification and attachment of bare relatives
7
by the
Tratz (2013) NLP system. While the NLP system
achieves 88.5 UAS and 86.1 LAS on the develop-
ment section,
8
its performance on identifying bare
relatives is comparatively low, with 70.0 precision
and 60.5 recall. For the test section, the NLP sys-
tem performance on bare relatives is even lower at
69.6 precision and 52.7 recall. This helps to ex-
plain why our resumptive pronoun classifier per-
forms worse on the test data than on the devel-
opment data when using automatic pre-processing
but not when using gold standard markup.
7 Related Work
The computational linguistics research most rele-
vant to ours is the work on identifying empty cat-
egories for several languages, including English,
Chinese, Korean, and Hindi. Empty categories
are nodes in a parse tree that do not correspond
to any written morpheme; these are used to han-
dle several linguistic phenomena, including pro-
drop. Recent research demonstrates that recovery
of empty categories can lead to improved transla-
tion quality for some language pairs (Chung and
Gildea, 2010; Xiang et al., 2013). For more in-
formation on the recovery of empty categories, we
refer the interested reader to work by Kukkadapu
and Mannem (2013), Cai et al. (2011), Yang and
Xue (2010), Gabbard et al. (2006), Schmid (2006),
Dienes and Dubey (2003), and Johnson (2002).
7
Relative clauses lacking a relative pronoun. As explained
in Section 2, MSA lacks relative pronouns for relative clauses
modifying indefinite noun phrases.
8
UAS and LAS stand for unlabeled and labeled attach-
ment scores.
8 Conclusion
In this paper, we present the challenge of translat-
ing MSA relative clauses, which often contain re-
sumptive pronouns, into English, which relies on
(inferred) empty categories instead. We examine
errors made by a popular online translation service
on MSA relative clauses and present an automatic
system for identifying MSA resumptive pronouns.
The online translation service occasionally gen-
erates English pronouns corresponding to MSA
resumptive pronouns, producing resumptive pro-
nouns for only 37 of 300 examples that cover a
variety of frequent MSA relative clause structures.
Our MSA resumptive pronoun identification
system achieves high levels of precision (92.1)
and recall (91.7) on resumptive pronoun identifi-
cation when using gold standard markup. Perfor-
mance drops significantly when using automatic
pre-processing, with precision and recall falling to
83.6 and 72.8, respectively. One of the sources
of the drop appears to be the weak performance
of the Tratz (2013) Arabic NLP system in identi-
fying and attaching bare relative clauses?that is,
relative clauses that lack a relative pronoun.
This work is the first attempt we are aware of to
automatically identify resumptive pronouns in any
language, and it presents a baseline for compari-
son for future research efforts.
9 Future Work
Going forward, we plan to experiment with apply-
ing our resumptive pronoun identifier to enhance
MT performance, likely by deleting all resumptive
pronouns during alignment and, again, at transla-
tion time. Another natural next step is to train the
system using automatically generated parse, part-
of-speech tag, and clitic segmentation information
instead of gold standard annotation to see if this
produces a similar drop in performance. We also
plan to investigate the use of frame information of
Arabic VerbNet (Mousser, 2010) as features, and
we would like to focus in greater detail on the dif-
ficulties in generating resumptive pronouns when
translating from English into MSA.
References
Elsaid Badawi, Michael G. Carter, and Adrian Gully.
2004. Modern Wrtitten Arabic: A Comprehensive
Grammar. Psychology Press.
46
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty ele-
ments. In ACL (Short Papers), pages 212?216.
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636?
645.
Michael J. Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory and
experiments with Perceptron Algorithms. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing.
P?eter Dienes and Amit Dubey. 2003. Antecedent re-
covery: Experiments with a trace tagger. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 33?40.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the penn treebank. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 184?191.
Y. Goldberg and M. Elhadad. 2010. An efficient
algorithm for easy-first non-directional dependency
parsing. In HLT-NAACL 2010.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 136?143.
Puneeth Kukkadapu and Prashanth Mannem. 2013. A
statistical approach to prediction of empty categories
in hindi dependency treebank. In Fourth Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages, page 91.
Mohamed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, Guidelines, Proce-
dures, and Tools. In Proceedings of the Workshop on
Computational Approaches to Arabic Script-based
languages, pages 2?9.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109.
Jaouad Mousser. 2010. A Large Coverage Verb Taxon-
omy for Arabic. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Eval-
uation.
Karin C. Ryding. 2005. A Reference Grammar of
Modern Standard Arabic. Cambridge University
Press.
Helmut Schmid. 2006. Trace prediction and recov-
ery with unlexicalized pcfgs and slash features. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 177?184.
Stephen Tratz. 2013. A cross-task flexible transition
model for arabic tokenization, affix detection, affix
labeling, pos tagging, and dependency parsing. In
Fourth Workshop on Statistical Parsing of Morpho-
logically Rich Languages.
Bing Xiang, Xiaoqiang Luo, and Bowen Zhou. 2013.
Enlisting the Ghost: Modeling Empty Categories for
Machine Translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistic.
Yaqin Yang and Nianwen Xue. 2010. Chasing the
ghost: recovering empty categories in the chinese
treebank. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 1382?1390.
47
Proceedings of the 25th International Conference on Computational Linguistics, pages 9?16,
Dublin, Ireland, August 23-29 2014.
Joint Navigation in Commander/Robot Teams:
Dialog & Task Performance When Vision is Bandwidth-Limited
Douglas Summers-Stay
Army Research Laboratory
douglas.a.summers-stay.civ
Taylor Cassidy
IBM Research
Army Research Laboratory
taylor.cassidy.ctr@mail.mil
Clare R. Voss
Army Research Laboratory
clare.r.voss.civ@mail.mil
Abstract
The prospect of human commanders teaming with mobile robots ?smart enough? to under-
take joint exploratory tasks?especially tasks that neither commander nor robot could perform
alone?requires novel methods of preparing and testing human-robot teams for these ventures
prior to real-time operations. In this paper, we report work-in-progress that maintains face valid-
ity of selected configurations of resources and people, as would be available in emergency cir-
cumstances. More specifically, from an off-site post, we ask human commanders (C) to perform
an exploratory task in collaboration with a remotely located human robot-navigator (Rn) who
controls the navigation of, but cannot see the physical robot (R). We impose network bandwidth
restrictions in two mission scenarios comparable to real circumstances by varying the availabil-
ity of sensor, image, and video signals to Rn, in effect limiting the human Rn to function as an
automation stand-in. To better understand the capabilities and language required in such con-
figurations, we constructed multi-modal corpora of time-synced dialog, video, and LIDAR files
recorded during task sessions. We can now examine commander/robot dialogs while replaying
what C and Rn saw, to assess their task performance under these varied conditions.
1 Introduction
Our research addresses a paradoxical situation in developing a robot capable of teaming with humans.
To know what capabilities such a robot needs, we seek to determine how a human commander would in-
teract ? choice of vocabulary and sentence types, expected capabilities and world knowledge, resources
used to accomplish tasks efficiently, etc. But without such a robot to interact with, we cannot know
how a commander would behave. The prospect of human commanders teaming with mobile robots that
are ?smart enough? to undertake joint exploratory tasks requires novel methods of preparing and testing
actual human-robot teams for these ventures, in advance of actual real-time operations. Furthermore,
given the need for human/robot teams during emergencies (such as Japan?s tsunami/Fukishima disaster),
we are interested in particular in the feasibility of commander/robot shared tasks that include NL com-
munication specifically for network contexts when bandwidth is limited by emergencies. Here we ask,
how can multimodal data, as collected and processed by robots, and the robots themselves contribute
real-time alerts and responses to human commanders over geographically-distributed networks?
The first phase of our approach is to introduce a human stand-in who navigates the robot, posing as
an intelligent control system. At this stage, following our prior work (Voss et al., 2014), we seek to
determine how the commander communicates to accomplish different tasks with the robot, while we
limit the information made available in passing from the robot?s sensors and camera to the commander
by way of the stand-in. In future phases, we will progressively automate away this actor?s role, replacing
the audio that the stand-in hears with what is ?understood? by automatic natural language semantic
interpretation within a dialog manager, and replacing the joystick that it uses to navigate as the robot
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
9
with ?actions? as automatically generated from micro-controller commands produced by transformation
of semantic commands.
In this paper, we report work-in-progress that maintains face validity of selected configurations of re-
sources and people, as would be available in emergency circumstances. From an off-site post, we ask
human commanders (C) to perform an exploratory task in collaboration with a remotely located human
robot-navigator (Rn) who actually controls the navigation of, but cannot see, the physical robot (R). We
restrict the information Rn receives from R by imposing network bandwidth restrictions comparable to
real circumstances which limit what Rn is able to communicate to C. We then examine the comman-
der/robot dialogs and task performance under these varied conditions.
To better understand the capabilities and language required in such configurations, we constructed
multi-modal corpora of time-synced dialog, video, and LIDAR files recorded during task sessions. We
can now examine commander/robot dialogs while replaying what C and Rn saw, to identify the impact
of varying the shared visual information on discourse, and to assess task performance under these var-
ied conditions. We hypothesized that more explicit, mututally available information (visual or verbal)
between participants would yield better understanding with more common ground, leading to more task
success. We also hypothesized that exploration in a more complex physical environment would lead both
to more dialog, as needed in resolving references to more locations, and also then on occasion, to less
overall task success. We have found in preliminary analyses that, with more explicit visual information,
some Cs reduce their level of communication, with fewer requests for images from Rn. In one such case,
this led to the Rn getting lost. We also noticed that some Cs increased their level of verbal communica-
tion, requesting far more still images from the robot when Rn could not itself see the robot?s images (as
opposed to when Rn had access to sent images). Taken together, these observations suggest?contrary to
our hypothesis that more information is better, especially in a complex environment?that there may be
a ?teeter totter? effect in the communication between C and Rn as visual information varies. When Rn
has access to more of the robot?s visual information, C communicates less with Rn, possibly assuming
more shared information than is correct. Whereas when Rn is able to see less, C communicates more
with Rn, possibly compensating for the lack of certainty Rn expresses.
2 Related Work
For human-robot communication in joint exploration tasks, we wish to understand two issues. The
first is ?scene to text?: when exploring new locations, how do people talk about what they see, and
how does that inform how they want robot team members to communicate about what they ?see? while
exploring? The second is ?text to scene?: given natural language instructions, how do people move about
in new locations, and how does that impact their expectations of robot navigation? These issues span
both generation and understanding of spatial language. There exists a large literature on spatial language,
starting several decades ago (Talmy, 1983; Anderson et al., 1991; Gurney et al., 1996; Bloom et al., 1996;
Olivier and Gapp, 1998) inter alia. This work yielded linguistic insights into the underlying structure of
spatial expressions, that has led more recently to annotation efforts like SpaceML (Morarescu, 2006) and
spatial role labeling (Kordjamshidi et al., 2010). These results, theoretical and computational, have been
incorporated into NLP research, such as spoken dialog systems (Meena et al., 2014).
For ?scene to text? processing, starting from a robot?s perception of the scene or environment, ex-
ploiting even known dependencies among objects (spatial relations, relative motion, etc.) is a central
problem in computer vision research. In the current state of robotics, the perceived world (a.k.a. se-
mantic perception) derived from data collected by the robot is limited by what is available within its
immediate sensor and video reach (Hebert et al., 2012). Within computational linguistic research, (Feng
and Lapata, 2013) have tackled going from news images to text, leveraging the news story content as
contextual knowledge, and automatically generating captions describing the image content as relevant
for the story. For ?text to scene? processing, a robot ?understanding? a commander?s language entails
going beyond linguistic semantic interpretation down to the the robot controller level, as in, for example,
Kress-Gazit et al. (2008). Within computational linguistics, Srihari and Burhans (1994) tackled going
from text to images, exploiting the conventions and spatial language in news caption to identify people
10
by their relative positions in accompanying images. More recently Coyne et al. (2011) presented work
for text-to-graphics generation, grounding conceptual knowledge in relational semantic encoding of lex-
ical meanings from FrameNet. These one-way, directional approaches provide strong evidence that text
and image modalities can each inform the processing of the other, and that, with concurrent audio and
video streaming data, the alignment of time-stamped files across the two data modalities should also
yield additional benefits in shared structural analyses and disambiguating references.
1
3 Approach
In previous work, we had teams search a series of buildings, where all information from the Rn to C was
strictly limited to text (Voss et al., 2014). While verbal descriptions of scenery were successfully elicited
during exploratory missions, the communication was painfully slow and this scenario yielded unrealistic
results from our stand-in: we would not expect a robot to generate the complex verbal descriptions we
collected. Furthermore we also learned that our equipment could be adjusted for transmission of LIDAR
map data and video stream from the robot to Rn and then to C. In this second study, we allowed individual
map and image updates to be sent to C, but only on request. This work provides more explicitly shared
knowledge between C and Rn, with its form and quantity more realistically varied and dynamic.
Equipment: We used an iRobot PackBot equipped with a forward-facing Kinect camera and a Hokuyo
LIDAR sensor.
2
We use GPS and inertial sensors for Simultaneous Localization and Mapping (SLAM).
Each participant had their own laptop with speakers and separate push-to-talk microphones. For navigat-
ing the robot, the Rn pushed a joystick on an X-box controller that was held. Additionally for transmitting
visual information available from the robot during the missions, the Rn pushed separate buttons on the
same controller to transfer image and map data to C, but only at C?s request.
Pre-pilot Design: We conducted training sessions at one location and test sessions at a second loca-
tion. A top down view of these sites is provided in Figure 1. We asked participants to perform distinct
missions (task conditions) in the training and test sessions, with different levels of visual information
available to Rn (vision conditions). Due to wireless networking timeouts and hardware integration diffi-
culties, a number of sessions ended prematurely. Descriptive statistics for the sessions are in Table 1.
Vision Condition
Video +
Task Condition LIDAR LIDAR + LIDAR +
- quality of dataset only Image last-sent Image last-sent
Mission 1 - complete ? ? 6 sessions (77 min)
Mission 1 - partial ? ? 1 session (1 min)
Mission 2 - complete 4 sessions (57 min) 2 sessions (28 min) 2 sessions (18 min)
Mission 2 - partial 11 sessions (15 min) 3 sessions (3 min) ?
Table 1: Total #sessions attempted by configuration (different task & vision conditions)
Vision Conditions: The Rn always saw (i) a continuously updated LIDAR map built up progressively
from the robot?s sensors as the Rn navigated the robot using the joystick on an X-box controller. On the
map during training, the Rn could also see (ii) an avatar shape for the robot?s location based on GPS and
(iii) an arrow for the robot?s facing direction generated by its internal components (updated intermittently
by GPS). However the GPS signal was also sporadic during these sessions, causing confusion for Rn
navigating the robot. As a result, during test sessions, we turned off the GPS to avoid this source of
confusion, mirroring what actual operators do in this scenario. During test sessions, the Rn only saw
(iii) the arrow, again within (i) the streamed LIDAR map. Beyond these Rn screen specifics, we ran
three conditions controlling for the visual information that the C and Rn could see. During mission 1
(training), Rn was given ?full? view of the streaming video, any specific images sent to C at C?s request,
and the map with arrow and avatar. During mission 2 (test) in one ?partially blinded? condition, the Rn
1
We are also eager to learn more from recent research examining streaming multimodal data for how and where the compo-
sition of natural language and the composition of visual scenes can inform one another (Barbu et al., 2012) and (Barbu et al.,
2013).
2
iRobot, PackBot, Kinect, and Hokuyo are all trademarks or registered trademarks.
11
Figure 1: On left side: view of Mission 1 courtyard and building, with doorways marked. On right side:
view of Mission 2 courtyards and buildings.
saw no video, but could see the specific images he sent to C as well as the map with arrow, and in the
other even ?more blinded? condition, Rn saw only the map with arrow. By contrast, the C only ever saw
what the Rn sent (by pushing buttons) as snapshots at C?s request. During all conditions ? independent
of what was presented to Rn (?full? view in mission 1, partially blinded or more-blinded in mission 2) ?
C could always request an updated snapshot image from the video feed or an updated snapshot map from
the LIDAR feed or both. As a result, Rn?s view was ?pushed? and current from the robot?s streaming
data, whereas the C?s view had to be ?pulled,? requiring C to ask for more snapshots. Note that in Rn?s
more-blinded condition, images were passed to C with Rn?s button push, but Rn could not see the images.
Mission 1: Enter courtyard and building via safe doorways. We hypothesized a robot with the ability
to carry on limited conversation regarding simple navigation and exploration, but without sufficient vi-
sion capabilities to analyze more subtle clues about whether a doorway was safe to enter. We designed
the task to simulate a low-bandwidth condition where constant transmission of the map and video infor-
mation is impossible. The robot was placed in one of two undisclosed positions outside the courtyard
surrounding a building. All sessions adopted the L+I+V vision condition. The site for this mission was a
Figure 2: Robot-navigator?s screen during Mission 1: upper left is static Image (clip from video, most
recently sent to Commander), upper right is video window, gray-scale background is LIDAR map
12
single rectangular building enclosed by a single rectangular courtyard. The site for mission 2 was more
complex, consisting of 5 buildings in a complex series of interconnected courtyards (see Figure 1). There
are five doorways into the courtyard and two doorways into the building. These doorways are marked as
safe or unsafe in a way that C can recognize but Rn cannot (C is given a key to the meaning of objects
placed just beyond open doorways as symbols). The participants are not informed about doorway loca-
tion or safety status. Figure 2 shows Rn?s screen during a mission 1 session. The grey-scale background
is an overhead, 2D view of a 3D map being built on the fly by combining various sensor data, which
contains a white robot avatar and blue arrow indicating its current pose. C?s view is similar, but without
video. Success on this task was gauged by whether the robot stayed safe in gaining entry to the house.
Mission 2: Find and classify all building doorways within a compound.
As noted above and shown in Figure 1, the location in this mission had a more complex layout. The
robot?s location within the compound was not disclosed to C nor Rn (no clues were provided), so that
the C and Rn team would need to work hard to place the robot on the map. The team was tasked with
thoroughly exploring the compound to capture images of each building doorway. In the LIDAR-only (L)
condition, Rn sees only the grey-scale map, whereas in the LIDAR and image condition (L+I) Rn sees
the most recently sent image as well as the grey-scale map (same screen layout as in Figure 2 but without
video window in upper right). Success on this mission was gauged both by the number of doors (open or
closed) that were identified and photographed and by whether the participants were lost at some stage in
the exploration.
4 Observations and Preliminary Results
We recorded rich, multi-modal datasets including: dialogue between C and Rn, video, LIDAR 3D point
clouds, scene classification output on video frames, and robot pose. The data is used to build up a 3D
model of the scene, and automatically align RGB images to the model by mapping pixels to 3D regions.
Examples of scene classification performance can be seen in Figure 3. The data for each run consists of
a ROS bag file (Quigley et al., 2009) and two audio files.
3
Figure 3: left: view from robot camera. right: automated scene classification. Mix of colors indicates
probability of belonging to a particular class. Classes found in this scene include sky, foliage, building,
grass, concrete, and asphalt. Performance degrades in lighting conditions unattested in training data.
4.1 Results from Session Path Analysis
Figure 4 shows an overhead 2D view of the final 3D map built using the SLAM module. An orange
line depicts the robot?s path from mission start to finish, with ordinal numbers indicating the robot?s high
level trajectory (the robot traveled from ?start? to ?1?, then to the location marked by ?2?, etc., finally
ending on the location marked by ?15?). Doorways that were successfully captured in images sent to C
are highlighted with a green solid-lined circle, whereas doorways that were passed by are indicated with
3
A bag file stores nano-second accuracy timestamped, discrete data messages, such as an individual video frame, the fact
that a joystick button was pressed, or the robot?s current velocity.
13
Mission 1 Vision Total # Images # Images Task Success:
Sessions Condition # Images sent with sent with Stayed Safe?
(duration) sent (any) door safe door Gained Entry?
1 (21 min.) L + I + V 0 0 0 S, E
2 (5 min.) L + I + V 0 0 0 N, E
3 (17 min.) L + I + V 3 3 2 S, E
4 (15 min.) L + I + V 8 7 2 S, E
5 (13 min.) L + I + V 12 7 4 S, E
Table 2: Mission 1 sessions: These training sessions provided the robot-navigators (Rn) with ?full?
real-time vision, i.e., their screens displayed all sensed data, as collected by the physical robot (R)
Mission 2 Vision Total Total Total # # deictic # refs Task Success:
Sessions Conditions #Images #Maps Im & Map refs to past # Doors id?
(duration) (LIDAR) (sent (sent (sent one, by by Got Lost?
(Image) w/o map) w/o img) then other) C, Rn C, Rn Recovered?
A (21 min.) L map 27 7 5 13, 2 6, 3 9, n/a , n/a
B (20 min.) L map + I 7 9 7 7, 2 7, 2 7, L, R
Table 3: Mission 2 per-session events: request and reference types, task success.
a dotted line. There is a point in the run depicted where Rn states that he is ?lost?, which is marked in
the figure by a green dot at step 10.
Figure 4: Robot path during Mission 2 session, doorways marked
4.2 Language Phenomena in Dialogs
Referring Expressions: There were few named environment features, necessitating the use of referring
expressions. Participants often used pronouns (?behind it?), deictic expressions (?that wall?), and both
definite and indefinite noun phrase descriptors (?a wall directly in front of you?). The frequency of
referring expressions other than proper names highlights the need for a dialog manager to robustly handle
human-robot dialog in our setting. In six mission 2 dialogs consisting of 6,593 words total, we annotated
1,593 referring expressions - 1,213 definite and 380 indefinite. The most common were first and second
person singular pronouns (287 and 245), definite expressions of the form the x (265) and indefinite
expressions of the form a(n) x (256). Most references are to things, either in the physical (?face the
doorway?) or software (?update your map?) environment, though there are references to events as well
(?do that again?).
Lexical Ambiguity: The same objects were sometimes referred to as ?doors? or ?doorways,? although by
a dictionary definition, those refer to somewhat different things. Based on context, the robot would need
to be able to understand which sense was intended.
14
Spatial Relations: Since these were navigation and observation tasks, much of the discussion involved
spatial language pertaining to object configurations and robot paths. There were references to distances
and angles, both specific (?turn 15 degrees to your right?) and vague (?turn around.?) The robot was asked
to ?follow the wall?, ?go north?, and to travel ?around,? ?behind,? and ?near? various objects.
Clarifications and Suggestions in Dialogs: When uncertain about the meaning of commands, Rn some-
times asked for clarification. At other times, Rn reminded C of its capabilities when appropriate: ?Would
you like me to send you an updated map??
4.3 The Role of Shared Visual Information
Participants were generally able to use both image and map data in conjunction with dialog to gain
enough common ground to communicate about the environment and accomplish the tasks at hand. For
example, after discussing environment features against the backdrop of an updated 2D map, we were
often surprised at the extent to which C apparently kept track of R?s location using dialog alone without
further map updates, as evidenced by C?s ability to correctly use Rn?s egocentric frame of reference in
verbal descriptions (recall that the robot avatar remained static on C?s map between updates). In such
cases C and R took advantage of mutually accessible visual information - their 2D maps were identical
during discussion. The role of mutually accessible information for achieiving common ground is further
supported by the fact that C requested significantly more images in the LIDAR-only condition, when
Rn could not see those sent images (see Table 3). Although shared visual knowledge proved useful for
resolving referring expressions, C and Rn rarely mentioned the media explicitly (?the building? vs ?the
building in the image you sent me?). In this way, the transfer of visual information served to introduce
entities into their discourse, but was taken for granted and not called out per se.
5 Ongoing Work
We have found in preliminary analyses that, with more explicit visual information, some Cs reduce their
level of communication, with fewer requests for images from Rn. In one such case, this led to the Rn
getting lost. We also noticed that some Cs increased their level of verbal communication, requesting far
more still images from the robot when Rn could not itself see the robot?s images (as opposed to when
Rn had access to sent images). Taken together, these observations suggest?contrary to our hypotheses
that more information is better, especially in a complex environment?that there may be a ?teeter totter?
effect in the communication between C and Rn as visual information varies. When Rn ?sees as the robot?
with access to more transmitted visual information, C communicates less with Rn, possibly assuming
more shared information than is correct. Whereas when Rn ?sees? less, C communicates more with
Rn, possibly compensating for the lack of certainty Rn expresses. We plan to extend our analysis of
how C and Rn communicate uncertainty, and look at how this topic is addressed in first aid and military
manuals (US Dept. of the Army, 1993).
We are currently developing a framework to automate many of the tasks currently performed by Rn.
Our studies and data collections so far are best understood in the context of the capabilities and limitations
of the overall system we are in the process of building. A crucial gap to address is associating referring
expressions with corresponding concrete spatial structures in the 3D map. Consider one sentence spoken
by the commander in one of the dialogues: ?When you get to the wall, turn left and drive along the wall
until you reach either a corner or what you believe to be a door.? To interpret this correctly, the robot
must understand an entire set of points as a single object or part of an object, so it can recognize doors,
walls, and corners in the combined vision and point-cloud. Moreover, it needs to plan a path that obeys
the constraint ?along the wall? and stops at some point which may be a door or a corner, that has not
yet been observed. Thus, objects need to be represented independent of the observed world map.
4
At
present, scene parsing techniques can analyze images and assign each pixel a probability of belonging to
a particular object class (wall, stucco, road, etc.) allowing us to propagate these labels to corresponding
points in the 3D model of the scene. In the future, we will use the 3D model to resolve visual ambiguities
and attach labels to particular objects that persist from one video frame to the next.
4
Resolving references to unvisited locations is a largely unexplored problem (Williams et al., 2013; Duvallet et al., 2013).
15
Acknowledgements
We thank members of the Asset Control and Behavior Branch at ARL for participation in our study and
for continuing to provide the technical support that makes our work possible. The work of Taylor Cassidy
was funded by IBM under the International Technology Alliance in Network & Information Sciences.
References
A. Anderson, M. Bader, E. Bard, E. Boyd, G.M. Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllister, C. Sotillo,
H.S. Thompson, and R. Weinert. 1991. The HCRC Map Task Corpus. Language and Speech, 34:351?366.
A. Barbu, A. Bridge, D. Coroian, S. J. Dickinson, S. Mussman, S. Narayanaswamy, D.l Salvi, L. Schmidt, J. Shang-
guan, J. M. Siskind, J. W. Waggoner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. 2012. Large-scale automatic
labeling of video events with verbs based on event-participant interaction. CoRR, abs/1204.3616.
A. Barbu, S. Narayanaswamy, and J. Siskind. 2013. Saying what you?re looking for: Linguistics meets video
search. CoRR, abs/1309.5174.
P. Bloom, M. Peterson, L. Madel, and M. F. Garrett, editors. 1996. Language and Space. The MIT Press.
B. Coyne, D. Bauer, and O. Rambow. 2011. Vignet: Grounding language in graphics using frame semantics. In
ACL Workshop on Relational Models of Semantics (RELMS 2011).
F. Duvallet, T. Kollar, and A. Stentz. 2013. Imitation learning for natural language direction following through
unknown environments. In IEEE Intl. Conference on Robotics and Automation (ICRA), pages 1047?1053.
Y. Feng and M. Lapata. 2013. Automatic caption generation for news images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 35:4:797?812.
J. Gurney, E. Klipple, and C. Voss. 1996. Talking about what we think we see: natural language processing for a
real-time virtual environment. IEEE International Joint Symposia on Intelligence and Systems.
M. Hebert, J. A. Bagnell, M. Bajracharya, K. Daniilidis, L. H. Matthies, L. Mianzo, L. Navarro-Serment, J. Shi, and
M. Wellfare. 2012. Semantic perception for ground robotics. In R. E. Karlsen; D. W. Gage; C. M. Shoemaker;
G. R. Gerhart, editor, SPIE Proceedings Vol. 8387: Unmanned Systems Technology XIV.
P. Kordjamshidi, M. Van Otterlo, and Marie-Francine Moens. 2010. Spatial Role Labeling: Task Definition and
Annotation Scheme. In Proceedings of Language Resources and Evaluation Conference.
H. Kress-Gazit, G. E. Fainekos, and G. J. Pappas. 2008. Translating Structured English to Robot Controllers.
Advanced Robotics Special Issue on Selected Papers from IROS, Vol. 22, No. 12:1343?1359.
R. Meena, J. Boye, G. Skantze, and J. Gustafson. 2014. Crowdsourcing street-level geographic information using
a spoken dialogue system. In Proceedings of SIGDIAL. Association for Computational Linguistics.
P. C. Morarescu. 2006. Principles for annotating and reasoning with spatial information. In LREC.
P. Olivier and K-P. Gapp, editors. 1998. Representation and Processing of Spatial Expressions. Lawrence Erlbaum
Associates, Hillsdale, NJ, USA.
M. Quigley, K. Conley, B. Gerkey, J. Faust, T. B. Foote, J. Leibs, R. Wheeler, and A. Y. Ng. 2009. ROS: an
open-source robot operating system. In ICRA Workshop on Open Source Software.
R. K. Srihari and D. T. Burhans. 1994. Visual semantics: Extracting visual information from text accompanying
pictures. In Proc. Of Twelfth National Conference on Artificial Intelligence (AAAI-94), pages 793?798.
L. Talmy. 1983. How Language Structures Space. In Jr. H. L. Pick and L. P. Acredolo, editors, Spatial Orientation:
Theory, Research, and Application, pages 225?282. Plenum Press, London.
US Dept. of the Army. 1993. Physical fitness training: Field manual 3-25.26. Washington, D.C.
C.R. Voss, T. Cassidy, and D. Summers-Stay. 2014. Collaborative Exploration in Human-Robot Teams: What?s
in Their Corpora of Dialog, Video, & LIDAR Messages? In Proceedings of EACL Dialog in Motion Workshop.
T. E. Williams, R. Cantrell, G. Briggs, P. W. Schermerhorn, and M. Scheutz. 2013. Grounding natural language
references to unvisited and hypothetical locations. In AAAI.
16
