Robust Segmentation of Japanese Text into a Lattice for Parsing 
Gary Kaemarcik, Chris Brockett, Hisami Suzuki 
M icrosofl Research 
One Microsoft Way 
Redmond WA, 98052 USA 
{ garykac,chrisbkt, hisamis }@in i croso ft.com 
Abstract 
We describe a segmentation component that 
utilizes minimal syntactic knowledge to produce a
lattice of word candidates for a broad coverage 
Japanese NL parser. The segmenter is a finite 
state morphological nalyzer and text normalizer 
designed to handle the orthographic variations 
characteristic of written Japanese, including 
alternate spellings, script variation, vowel 
extensions and word-internal parenthetical 
material. This architecture differs from con- 
ventional Japanese wordbreakers in that it does 
not attempt to simultaneously attack the problems 
of identifying segmentation candidates and 
choosing the most probable analysis. To minimize 
duplication of effort between components and to 
give the segmenter greater fi'eedom to address 
orthography issues, the task of choosing the best 
analysis is handled by the parser, which has access 
to a much richer set of linguistic information. By 
maximizing recall in the segmenter and allowing a 
precision of 34.7%, our parser currently achieves a
breaking accuracy of ~97% over a wide variety of 
corpora. 
Introduction 
The task of segmenting Japanese text into word 
units (or other units such as bunsetsu (phrases)) 
has been discussed at great length in Japanese NL 
literature (\[Kurohashi98\], \[Fuchi98\], \[Nagata94\], 
et al). Japanese does not typically have spaces 
between words, which means that a parser must 
first have the input string broken into usable units 
before it can analyze a sentence. Moreover, a 
variety of issues complicate this operation, most 
notably that potential word candidate records may 
overlap (causing ambiguities for the parser) or 
there may be gaps where no suitable record is 
found (causing a broken span). 
These difficulties are commonly addressed using 
either heuristics or statistical methods to create a 
model for identifying the best (or n-best) sequence 
of records for a given input string. This is 
typically done using a connective-cost model 
(\[Hisamitsu90\]), which is either maintained 
laboriously by hand, or trained on large corpora. 
Both of these approaches uffer fiom problems. 
Handcrafted heuristics may become a maintenance 
quagmire, and as \[Kurohashi98\] suggests in his 
discussion of the JUMAN scgmenter, statistical 
models may become increasingly fi'agile as the 
system grows and eventually reach a point where 
side effects rule out fiwther improvements. The 
sparse data problem commonly encountered in 
statistical methods is exacerbated in Japanese by 
widespread orthographic variation (see ?3). 
Our system addresses these pitfalls by assigning 
completely separate roles to the segmeuter and the 
parser to allow each to delve deeper into the 
complexities inherent in its tasks. 
Other NL systems (\[Kitani93\], \[Ktu'ohashi98\]) 
have separated the segmentation and parsing 
components. However, these dual-level systems 
are prone to duplication of effort since mauy 
segmentation ambiguities cannot be resolved 
without invoking higher-level syntactic or 
semantic knowledge. Our system avoids this 
duplication by relaxing the requirement that the 
segmenter identify the best path (or even n-best 
paths) through the lattice of possible records. The 
segmenter is responsible only for ensuring that a 
correct set of records is present in its output. It is 
the filnction of the parsing component to select he 
best analysis from this lattice. With tiffs model, 
our system achieves roughly 97% recall/precision 
(see \[Suzuki00\] for more details). 
1 System Overview 
Figure shows a simple block diagram of our 
Natural Language Understanding system for 
Japanese, the goal of which is to robustly produce 
syntactic and logical forms that allow automatic 
390 
Word Segmentation 
l{ 
\[ Dcrivational .,\sscmhl~ I 
Syntactic Analysis \] 
\[ \[,ogical Form \] 
,0 ( )rthograph.v 
l.exicon 
Syntax 
I.cxicon 
% 
Figure 1: Block diagram of Japanese NL system 
extraction of semantic relationships (see 
\[Richardson98\]) and support other lirlguistic 
projects like information retrieval, NL interfaces 
and dialog systems, auto-.summarization and 
machine translation. 
The segmenter is the frst level of' processing. This 
is a finite-state morphological nalyzer esponsible 
for generating all possible word candidates into a 
word lattice. It has a custom lexicon (auto: 
matically derived from the main lexicon to ensure 
consistency) that is designed to facilitate the 
identification of orfllographic variants. 
Records representing words and morphemes are 
handed off by the segmenter to the derivational 
assembly component, which uses syntax-like rules 
to generate additional derived forms that are then 
used by the parser to create syntax trees and logical 
forms. Many of the techniques here are similar to 
what we use in our Chinese NI., system (see 
\[Wu98\] for more details). 
The parser (described exterisively in \[Jensen93\]) 
generates syntactic representatioris arm logical 
forms. This is a bottomoup chart parser with 
binary rnles within the Augnmnted Phrase 
Structure Grammar formalism. The grammar rules 
are language--specific while the core engine is 
shared among 7 languages (Chinese, Japanese, 
Korean, English, French, German, Spanish). The 
Japanese parser is described in \[Suzuki00\]. 
2 Recall vs? Precision 
In this architecture, data is fed forward from one 
COlnponent to the next; hence, it is crucial that the 
base components (like the segmenter) generate a
minimal number of omission errors. 
Since segmentation errors may affect subsequent 
components, it is convenient to divide these errors 
into two types: recoverable and non-recoverable. 
A ram-recoverable error is one that prevents the 
syntax (or any downstream) component from 
arriving at a correct analysis (e.g., a missing 
record). A recoverable rror is one that does not 
interfere with the operation of following 
components. An example of the latter is the 
inchision of an extra record. This extra record 
does not (theoretically) prevent the parser from 
doing its lob (although in practice it may since it 
eonsun les  resot l rces) .  
Using standard definitions of recall (R) and 
precision (P): 
*~ Jr R - Seg~,,,.,.,.,., p = Seg~,,,.,.~.~., 
7bg,,,,,/ &g,,,,,,i 
where Segcor~ec t and .<,egmxal are the number q/" "'cotwect" 
and total number o/'segments returned by the segmentet; 
and "\['agto~a I is the total Jlttmber of "correct" segments 
fi'om a tagged corpus, 
we can see that recall measures non-recoverable 
errors and precision measures recoverable rrors. 
Since our goal is to create a robust NL system, it 
behooves us to maximize recall (i.e., make very 
few non-recoverable errors) in open text while 
keeping precision high enough that the extra 
records (recoverable errors) do not interfere with 
the parsing component. 
Achieving near-100% recall might initially seem to 
be a relatively straightforward task given a 
sufficiently large lexicon - simply return every 
possible record that is found in the input string, in 
practice, tile mixture of scripts and flexible 
orthography rules of Japanese (in addition to the 
inevitable non-lexicalized words) make the task of 
identifying potential lexical boundaries an 
interesting problem in its own right. 
3 Japanese Orthographic Variation 
Over tile centuries, Japanese has evolved a 
complex writing system that gives tile writer a 
great deal of flexibility when composing text. 
Four scripts are in common use (kanji, hiragana, 
katakana and roman), and can co-occur within 
lexical entries (as shown ill Table 1). 
Some mixed-script entries could be handled as 
syntactic ompounds, for example, ID ~a---1-" /at 
dii kaado="ID card'7 could be derived fl'om 
1DNotJN + 79-- I ~ NOUN. tlowever, many such items 
are preferably treated as lexical entries because 
391 
i!i \[~ ~ ' \[atarashii ,: "'new "\] 
Kanji-I l iragana I~LII~J \[ho~(vtnn'ui = "'mammal"\] 
~ 7/" "7 :./\[haburashi -~ "'toothbrush "\] 
. . . . . . . . . . . . . . . . . . .  . . . . . . . . .  
K'}n.!i-<~!P!\]!! ...................... E ( !S  ' ! t ( !Z . /~( :GS tm! ' i  t? ::c:(';S's:vstet!* :'1 ........ 
12 ) J \[lmtmL, atsu - "December"/ 
Kallji-Synlbol v ,{'~ \ [gcmma sen = "'~amma rays "\] 
.i-3 1" 4" t/ \[otoim - "'toilet "'\] 
Mixed kana 
............................. \[ -/")3 hl~?!,,!! ? .;;(9 c?f~,,,~/5' ;7/ ........... 
II3 )-~ - -  b" \[aidtt kaado = "lP card"\] 
Kana-Alpha ..t .y-t~ .>"-5;-V - -RNA \[messe~gaa RA'A = 
................................................................. T ie~t~:s~'~lxe~ ~U.:I. '7 .................................. 
7, J- U 2/-)'- ~) 2, 90 \[suloronchiumu 90 - 
"'Strontiunt 90 "\] 
Kana-Symbol I~ 2_ 7~ 40 ? \[hoeru yonjuu do = "'roaring 
............................................................................ fo t : ! !~, , ; .7  ......................................................... 
~i~ b ~" ~, \[keshigomu - "'eraser "1 
a >,I- 21" ~ e/ ~) ~: \[artiSt kentauri set : 
Other mixed 
"'Alpha Centauri "\] 
\[. ~: ~ \[togaki = "'stage directions"\] 
Table 1: Mixed-script lexical entries 
they have non-compositional syntactic or semantic 
attributes. 
In addition, many Japanese verbs and adjectives 
(and words derived from them) have a variety of 
accepted spellings associated with okurigana, 
optional characters representing inflectional 
endings. For example, the present ense of e)j b ~;?; 
~- (kiriotosu = "to prune ") can be written as any 
of: ~)Ji:~~J --, ~;)J b .."?,:t, ?:JJTf; & ,~, ~JJb.~s&~-, ~ ~:; ~s ~ I 
or even ~ ~) ~'g ~ 4-, -~ 9 7~;-~-. 
Matters become even more complex when one 
script is substituted for another at the word or sub- 
word level. This can occur for a var iety o f  
reasons: to replace a rare or d i f f i cu l t  ka@ (.~?~ 
\[rachi= "kMnap"\] instead of f,):~); to highlight a 
word in a sentence ( ~ >" t2 -h, o ~_ *) \[henna 
kakkou = "strange appearance '7); or to indicate a 
particular, often technical, sense (7 Y o -c \[watatte 
="crossing over"\] instead of iA~o-c, to emphasize 
the domain-specific sense of "connecting 2 
groups" in Go literature). 
More colloquial writing allows for a variety of 
contracted tbrms like ~ t~j~.\-~  ~ t~ + !=t \[ore~ 
tacha = ore-tachi + wa = "we'" + TOPIC\] and 
phonological inutations as in ~d.~:--~- = -d'4 \ [dee- 
su ~ desu = "is "\]. 
This is only a sampling of the orthographic issues 
present in Japanese. Many of these variations pose 
serious sparse-data problems, and lexicalization of 
all variants is clearly out of the questioi1. 
II.\]'~., ~lJ < . .~  lt,l~L~l~;'~q ?gJ \[H/ikc,kkoku "'every 
repeat moment "'\] 
characters Ill ~ ~., e L I~, .~ HI :!:~ til-!~ U ~ ' \[kaigaishu 
"'diligent "'\] 
distribution of  t: " '>" v~- ~ t:":#v\]- \[huleo "vMeo"\] voicina nl,qrks 
halt\vidth & 
lhllwidth 
composite 
symbols 
F M b2J~" ~ FM )/ZJ~ \[I"M housml ~ "FM 
broadcast "'\] 
;~ (i'?" ~U, "-~ 5" 4, "V <e" :>, 2, \[daivaguramu : 
"'diagram 'J 
;~; "-~ . . . . .  L" 2/ 1- \[paasento =: "percent :;\] 
r tz  , .  . . /~ ,  . 
"'incorporated 'i\] 
N\] ~ 2 8 FI \[n!jmthactH niciu = "28 'j' day of the 
month "\] 
Table 2: Character type normalizations 
4 Segmenter Design 
Given the broad long-term goals for' the overall 
system, we address the issues of recall/precision 
and orthographic variation by narrowly defining 
the responsibilities of the segmenter as: 
(i) Maximize recall 
(2) Normalize word variants 
4.1 Maxinf fze Recall  
Maximal recall is imperative, Any recall mistake 
lnade in the segmenter prevents the parser from 
reaching a successful analysis. Since the parser in 
our NL system is designed to handle ambiguous 
input in the fbrm of a word lattice of potentially 
overlapping records, we can accept lower precision 
if that is what is necessary to achieve high recall? 
Conversely, high precision is specifically not a 
goal for the segmenter. While desirable, high 
precision may be at odds with the primary goal of 
maximizing recall. Note that the lower bound for 
precision is constrained by the lexicon. 
4?2 Normal ize word variants 
Given tile extensive amount of orthographic 
variability present in Japanese, some form of 
normalization into a canonical form is a pre- 
requisite for any higher-.order linguistic processing. 
The segmenter performs two basic kinds of 
nomlalization: \[,emmatization f inflected forms 
and Orthographic Norlnalization. 
392 
,~kur ieana . . . .  n),: g.). z~ -+ i,J,: ~- ~),~ :~ \[lhkmuk~ :: "drafty"/  ;5'./~ <% J)-tJ- ; '3_ \[11i1:i:;5,.,7~-\]\[ ~", ,. ,,,,.. ~ ' >\]-~J-'"o kammwaseru = 7o  
. . . . . . . . . . . . . . . . . . .  ~ !o{ ~ c!i ~s /./!,a!!{,6! : ::e ,!:e!l?!el<, 71 . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . .  engage (gear.w i' 
a ml l .~ ' l lmor i  - "'~111 non-s tandard  tA'ct) ::~ "~ ~-0~ \]'- \[onnanoko :: "gir l"/  .;.'/. <) ~, l) \[ ~u:<;'~-\]\[ ~ [:~-). {,. ~') \] estimate "' 
script +~:t , - t  - "+ 0 :4  7, :~ \[d,s'uko : "d i sco" /  I I )  ) :2  I" \ [ l i ' , ;4 -  l l l \ ]) :<fi4 - - i :DZ i D: a/ih, kaado2:" lDc iml  '" 
? D, I\] "+ " "~ Jl \[tkka,4etsu :: "one month" /  . . . . . . . . . . . . . . . .  
9:0  var iants  kagi tahako "'smf/'f --I, I!:~ " \['~\] \[ktl.~'uml~aseki = "Kasunli~aseki "\] 
numera ls  fi~r 5 i\[i~ .+ ti\]!i~ \[gorm : "'Olympics "\] 
kalt j i  1 ),, ~ ")v \[hitori : "one person"/  
:t.; i Z - -  ~ ,t~, k.  ~ -~J { 2 b ' ~5 /~ \[oniisan = "'older 
vovcel 
extens ions  brother "'\] 
-7 -,- 4' \]" 4 "+ 7 -," 4" I" \[/'aito 'ftght "'\] 
'Y - /  4- ~- 9 "../-i~ i<.( ~- 9 "-./\[batorm - "'vloti~F7 
ka lakana  
-)" 9 :E- - -  Iv ..+ 9 ~ . O ? :E- --" F' \ [aramoode "'h la v ari all Is too& "'\] 
IPL~(!rt)2_ ~ + ll~),t 2. ".'~ \[haeru = "'to shine "7 
in l ine yomi  ~ ~_lt(f:l: ~o ~I> ") )\]~(i "+l/'~ ~t JL?\[ \[hachuurui 
D,3 '/, < :~' II~';L D ', ~ II:t:~'i 0:: 'J "< -~ I toDac~'o" 
/g;'~; V, 1-{{:/~ D;\]\[!i,~:l '  '1 na~ai "'a long visit" 
Table 4: Orthography lattices 
lexical entry. Examples are given in "fable 3. Two 
cases of special interest are okurigana and inline 
yomi/kanji normalizations. The okurigana 
normalization expands shortened forms into fully 
specified forms (i.e., fornls with all optional 
................................. 7"~'!~t!?C'\] ..................................................................................... characters present). Tile yomi/kanji handling takes 
in l ine kanj i  l Yo (N)~: i i l2d  -'~ tgs ~ft~ \[~essh~rul =: "'rodent"/ 
Table 3: Script normalizations 
4.Z 1 Lemmatization 
LEMMATIZATION iri Japanese is the same as that 
for any language with inflected forms - a lemma, 
or dictionary form, is returned along with the 
inflection attributes. So, a form like ~:~-7~ \[tabeta 
= "ate "J would retum a lemma of f,~ ~<~; \[taberu = 
"'eat"\] along with a PAST attribute. 
Contracted forms are expanded and lemmatized 
individually, so that f,~ ~<-<o ~:~ .> o ?= \[tabe~ 
tecchatta = "has ectten and gone'7 is returned as: 
f~  ~-Z. 7 0 G\[-RUND -F (, x < GERUND -F L. +E ") PASr  
\ [taberu: "eat" + iku--++go" F s \ ]T imaz l=. . iSpE(7\ [ . ' \ ] .  
4..2.2 Orthographic Normalizatio,, 
ORTIIOGRAPttlC NORMALIZATION smoothes out 
orthographic variations o that words are returned 
in a standardized form. This facilitates lexical 
lookup and allows tile system to map the variant 
representations to a single lexicon entry. 
We distinguish two classes of orqthographic 
normalization: character type normalization and 
script normalization. 
CI IARAC' IER  TYPE  NORMAI . IZAT ION takes tile 
various representations allowed by the Unicode 
specification and converts them into a single 
consistent form. Table 2 summarizes this class of 
normalization. 
SCR. I I ' T  NORMAI,IZAI'ION rewrites the word so that 
it conforms to tile script and :~pelling used in the 
infixed parenthetical material and normalizes it out 
(after using the parenthetical infommtion to verify 
segmentation accuracy). 
5 Lexicon Structures 
Several special lexicon structures were developed 
to support hese features. Tile most significant is 
an orthography lattice* that concisely encapsulates 
all orthographic variants for each lexicon entry and 
implicitly specifies the normalized form. This has 
the advantage of compactness and facilitates 
lexicon maintenance since lexicographic inform- 
ation is stored in one location. 
The orthography lattice stores kana inforrnation 
about each kanji or group of kanji in a word. For 
example, the lattice far the verb Y~:-<~D \[taberu = 
"eat'7 is \[~:#_\]-<~, because the first character 
(ta) can be written as either kanji 5~ or kana 1=. A 
richer lattice is needed for entries with okurigana 
variants~ like LJJ 0 i'~:~ 4 \[kiriotosu = "'to prune "\] 
cited earlier: commas separate each okurigana 
grouping. The lattice for kiriotosu is \[OJ:~, 0 \]\[i"#: 
~,  E \]~j-. Table 4 contains more lattice examples. 
Enabling all possible variants can proliferate 
records and confiise the analyzer (see \[Kurohashi 
94\]). We therefore suppress pathological variants 
that cause confusion with more common words 
and constructions. For example, f:L-t,q- \[n~gai = "a 
long visit'7 never occurs as I.~ ~' since this is 
ambiguous with the highly fi'equent adjective ~-~v, 
/nasal - "l<mg'7. Likewise, a word like !t 
' Not to be confiised with the word lattice, which is the 
set of records passed fi'om the segmenter tothe parser. 
393 
\[nihon = ",Aq)an "7 is constrained to inhibit invalid 
variants like 124< which cause confusion with: {c 
I'OSl' + # NOUN \ [ t I i : : I ' . - tRT IC I . I : "  + /1on  = "book  " \ ] .  
We default to enabling all possible orthographies 
for each ennT and disable only those that are 
required. This saves US from having to update the 
lexicon whenever we encounter a novel 
orthographic variant since the lattice anticipates all 
possible variants. 
6 Unknown Words 
Unknown words pose a significant recall problem 
in languages that don't place spaces between 
words. The inability to identify a word in the input 
stream of characters can cause neighboring words 
to be misidentified. 
We have divided this problem space into six 
categories: variants of lexical entries (e.g., 
okurigana variations, vowel extensions, et al); 
non-lexiealized proper nouns; derived forms; 
foreign Ioanwords; mimetics; and typographical 
errors. This allows us to devise focused heuristics 
to attack each class of unfound words. 
The first category, variants of lexical entries, has 
been addressed through the script normalizations 
discussed earlier. 
Non-lexicalized proper nouns and derived words, 
which account for the vast majority of unfound 
words, are handled in the derivational assembly 
component. This is where compounds like -: ~ >i 
x ~':, ffuransugo = "French (language)"\] are 
assembled from their base components ;1 5~ J x 
\[furansu : "France "\] and at~ \[go = "language "J. 
Unknown foreign Ioanwords are identified by a 
simple maximal-katakana heuristic that returns the 
longest run of katakana characters. Despite its 
simplicity, this algorithm appears to work quite 
reliably when used in conjunction with the other 
mechanisms in our system. 
Mimetic words in Japanese tend to follow simple 
ABAB or ABCABC patterns in hiragana or 
katakana, so we look for these patterns and 
propose them as adverb records. 
The last category, typographical errors, remains 
mostly the subject for future work. Currently, we 
only address basic : (kanji) ~-~ -: (katakana) and 
i-, (hiragana) +~ : ' -  (katakana) substitutions. 
50% 
40% 
30% 
20% 
"10% 
0% 
15 25 35 45 55 65 75 85 95 105 115 
- - -~ Japanese  =-~t-=Chinese \]
? . . . . .  72 .27_~_z z zs?  27 ~ 7 ~Lz77 ~z ~25z ~ 2 7~ . . . . . . . .  
Figure 2: Worst-case segmenter precision (y-axis) versus 
sentence length (x-axis - in characters) 
7 Eva|uation 
Our goal is to improve the parser coverage by 
improving the recall in the segmenter. Evaluation 
of this component is appropriately conducted in the 
context of its impact on the entire system, 
Z 1 Parser Evaluation 
Running on top of our segmenter, our current 
parsing system reports ~71% coverage + (i.e, input 
strings for which a complete and acceptable 
sentential parse is obtained), and -,97% accuracy 
for POS labeled breaking accuracy? A full 
description of these results is given in \[Suzuki00\]. 
Z 2 Segmenter Evaluatkm 
Three criteria are relevant to segmenter per- 
formance: recall precision and speed. 
Z Z 1 Recall 
Analysis of a randonlly chosen set of tagged 
sentences gives a recall of 99.91%. This result is 
not surprising since maxindzing recall was a 
prinlary focus of our efforts. 
The breakdown of the recall errors is as follows: 
missing proper nouns = 47%, missing nouns = 
15%.. missing verbs/adjs = 15%, orthographic 
idiosyncrasies = 15%, archaic inflections = 8%. 
It is worth noting that for derived forms (those that 
Tested on a 15,000 sentence blind, balanced corpus. 
See \[SuzuldO0\] fordetails. 
394 
3000 \ [ i  
2000 I 
1 
<':> ,# ,~, ?> e <# ~,~, e e @,, ,+>,e,e 
Figure 3: Characters/second (y~axis) vs. sentence 
length (x-axis) for se~<ginenter alone (upper curve) 
and our NL system as a whole (lower curve) 
are tiandled in the derivational assembly corn-. 
ponent), tim segmenter is considered correct as 
long as it produces the necessary base records 
needed to build the derived fom-t. 
ZZ2 Precision 
Since we focused our effbrts on maximizing recall,, 
a valid concern is the impact of the extra records 
on the parser, that is, the effect of lower segmenter 
precision oll the system as a whole. 
Figure 2 shows the baselirie segrnenter precision 
plotted against sentence length using the 3888 
tagged sentences ~: For compaiison~ data for 
Chinese ~is included. These are baseline vahles in 
the sense they represent the riumber of records 
looked up in the lexicon without application of ariy 
heuristics to suppress invalid records. Thus, these 
mnnbers represent worst--case segmenter p ecision. 
The baseline precisior, for the Japariese segmenter 
averages 24.8%, whicl-i means that a parser would 
need to discard 3 records for each record it used in 
the final parse. TMs value stays fairly constant as 
the sentence length increases. The baseline 
precision for Chir, ese averages 37.1%. The 
disparity between the Japanese and Chinese worst- 
case scenario is believed to reflect the greater 
ambiguity inherent in the Japanese v<'riting system, 
owing to orthographic w~riation and the use of a 
syllabic script. 
++ The " <,<," o .~ t<%~,% was obtained by usin,,the results of the 
parser on untagged sentences. 
39112 sentences tagged in a sirnilar fashion using our 
Chinese NI,P system. 
100% 
70% "-:-: 5 ~ ::.::~::,~::-5.. ,'i ,': ~ -"r.'-~,'~7,:'s~'-,.: ~ :  .~ ~ ::,~ x;K< ~ 
50% 
40% ~ ~ - . ---..-~ 
30% ::::::::::::::::::::::::::::::::::::::::::::::::::::::: 
20% :::i:!)?i:~:~)}ii!:i\]::{i)~:,x::i!illii.:i!:'-.~!!~\]:!21{7-i\[.g{:!:'7:7:~::?. . . . . . . . . . . . . . . . . . . . . .  ,~< ............ 
10% !::::::ii'::::ii!i'ii{f!}{ii'.".'::iii::::ii 
0% ~'~S 2 
15 25 35 45 55 65 75 85 95 105 115 125 135 
\ [BSegmenter  \ [ \ ]Lex ica l  E IDer iv  BOther  El Parser  
Figure 4: Percentage oftime spent in each component (y- 
axis) vs. sentence l ngth x-axis) 
Using conservative pruning heuristics, we are able 
to bring the precision tip to 34.7% without 
affecting parser recall. Primarily, these heuristics 
work by suppressing the hiragana form of shork 
ambiguous words (like ~ \[ki="tree, air, .slJirit, 
season, record, yellow,... '7, which is normally 
written using kanji to identify the intended sense). 
Z2..3 Speed 
Another concern with lower precision values has to 
do with performance measured in terms of speed. 
Figure 3 summarizes characters-per.-second per- 
formance of the segmentation component and our 
NL system as a whole (irmluding the segmentation 
component). As expected, the system takes more 
time for longer senterlces. Crucially, however, the 
system slowdowri s shown to be roughly linear, 
Figure 4 shows how nluch time is spent in each 
component during sentence analysis. As the sen- 
tence length increases, lexical lookup+ derivational 
morphology and '+other" stay approximately con- 
starit while the percentage of time spent in the 
parsing component increases. 
Table 5 compares parse time performance for 
tagged and untagged sentences. This table 
qnantifies the potential speed improvement that the 
parser could realize if the segmenter precision was 
improved. Cohunn A provides baseline lexical 
lookup and parsing times based on untagged input. 
Note that segmenter time is not given this table 
because it would not be comparable to tile hypothetical 
segmenters devised for columns P, and C. 
395 
A 
Lexical processing 7.66 s 
Parsing 3.480 s
Other 4. 95 s 
Total 25.336 s 
Overall 
Percent Lexical 
Improvement I'arsing 
Other 
B c 
2.5 0 s 2.324 s
8.865 s 7. 79 s 
3.620 s 3.5 9 s 
4.995 s 3.022 s
40.82% 48.60% 
67.24% 69.66% 
34.24% 46.74% 
3.7 % 6. % 
Table 5: Summary of performance (speed) 
experiment where untagged input (A) is compared 
with space-broken i put (B) and space-broken i put 
with POS tags (C). 
Columns B and C give timings based on a 
(hypothetical) segmenter that correctly identifies 
all word botmdaries (B) and one that identifies all 
word boundaries and POS (C) 1'I". C represents the 
best-case parser performance since it assumes 
perfect precision and recall in the segmenter. The 
bottom portion of Table ,5 restates these 
improvements a percentages. 
This table suggests that adding conservative 
pruning to enhance segmenter precision may 
improve overall system performance. It also 
provides a metric for evaluating the impact of 
heuristic rule candidates. The parse-time 
improvemeuts from a rule candidate can be 
weighed against the cost of implementing this 
additional code to determine the overall benefit o 
the entire system. 
8 Future 
Planued near-term enhancements include adding 
context-sensitive h uristic rules to the segmenter as 
appropriate. In addition to the speed gains 
quantified in Table 5, these heuristics can also be 
expected to improve parser coverage by reducing 
resource requiremeuts. 
Other areas for improvement are unfotmd word 
models, particularly typographical error detection, 
and addressing the issue of probabilities as they 
apply to orthographic variants. Additionally, we 
are experimenting with various lexicon formats to 
more efficiently support Japanese. 
tt For the hypothetical segmenters, our segmenter was 
modified to return only the records consistent with a 
tagged input set. 
9 Conclusion 
The complexities involved in segmenting Japanese 
text make it beneficial to treat this task 
independently from parsing. These separate tasks 
are each simplified, thcilitating the processing of a 
wider range of phenomenon specific to their 
respective domains. The gains in robustness 
greatly outweigh the impact on parser performance 
caused by the additional records. Our parsing 
results demonstrate that this compartmentalized 
approach works well, with overall parse times 
increasing linearly with sentence length. 
10 References 
\[Fuchi98\] Fuchi,T., Takagi,S., "Japanese 
Morphological Analyzer using Word Co-occurrence", 
ACL/COLING 98, pp409-4 3, 998. 
\[Hisamitsu90\] Hisamitsu,T., Nitta, Y., 
Morphological Analyis by Minimum Connective-Cost 
Method", SIGNLC 90-8, IEICE pp 7-24, 990 (in 
Japanese). 
\[Jensen93\] Jensen,K., Heidorn,G., Richardson,S, 
(eds.) "Natural Language Processing: The PLNLP 
Approach", Kluwer, Boston, 993. 
\[Kitani93\] Kitani,T., Mitamura,T., "A Japanese 
Preprocessor for Syntactic and Semantic Parsing", 9 th 
Conference on AI in Applications, pp86-92, 993. 
\[Kurohashi94\] Kurohashi,S., Nakamura,Y., 
Matsumoto,Y., Nagao,M., "hnprovements of Japanese 
Morphological Analyzer JUMAN", SNLR, pp22-28, 
994. 
\[Kurohashi98\] Kurohashi,S., Nagao,M., "Building a 
Japanese Parsed Corpus while hnproving the Parsing 
System", First LREC Proceedings, pp7 9-724, 998. 
\[Nagata94\] Nagata,M., "A Stochastic Japanese 
Morphological Analyzer Using a Forward-DP 
Backward-A* N-Best Search Algorithm", COL1NG, 
pp20-207, 994. 
\[Richardson98\] Richardson,S.D., Dolan,W.B., 
Vanderwende,L., "MindNet: Acquiring and Structuring 
Semantic Information from Text", COLING/ACL 98, 
pp 098- 02, 998. 
\[Suzuki00\] Suzuki,H., Brockett,C., Kacmarcik,G., 
"Using a broad-coverage parser for word-breaking in 
Japanese", COLING 2000. 
\[Wu98\] Wu,A., Zixin,J., "Word Segmentation in 
Sentence Analysis", Microsoft Technical Report MSR- 
TR-99- 0, 999. 
396 
Using a Broad-Coverage Parser for Word-Breaking in Japanese 
Hisami Suzuki, Chris Brockett and Gary Kacmarcik 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
{ hisamis, chrisbkt, garykac }@ microsoft.corn 
Abstract 
We describe a method of word segmentation i
Japanese in which a broad-coverage parser selects 
the best word sequence while producing a syntactic 
analysis. This technique is substantially different 
from traditional statistics- or heuristics-based 
models which attempt o select the best word 
sequence before handing it to the syntactic 
component. By breaking up the task of finding the 
best word sequence into the identification of words 
(in the word-breaking component) and the selection 
of the best sequence (a by-product of parsing), we 
have been able to simplify the task of each 
component and achieve high accuracy over a wide 
varicty of data. Word-breaking accuracy of our 
system is currently around 97-98%. 
1. Introduction 
Word-breaking is an unavoidable and crucial first 
step toward sentence analysis in Japanese. In a 
sequential model of word-breaking and syntactic 
analysis without a feedback loop, the syntactic 
analyzer assumes that the results of word-breaking 
are correct, so for the parse to be successful, the 
input from the word-breaking component must 
include all words needed for a desired syntactic 
analysis. Previous approaches to Japanese word 
segmentation have relied on heuristics- or 
statistics-based models to find the single most 
likely sequence of words for a given string, which 
can then be passed to the syntactic omponent for 
further processing. The most common 
heuristics-based approach utilizes a connectivity 
matrix between parts-of-speech and word 
probabilities. The most likely analysis can be 
obtained by searching for the path with the 
minimum connective cost (Hisamitsu and Nitta 
1990), often supplemented by additional heuristic 
devices such as the longest-string-match or the 
least-number-of-bunsetsu (phrase). Despite its 
popularity, the connective cost method has a major 
disadvantage in that hand-tuning is not only 
labor-intensive but also unsafe, since adjusting the 
cost for one string may cause another to break. 
Various heuristic (e.g. Kurohashi and Nagao 1998) 
and statistical (e.g. Takeuchi and Matsumoto 1997) 
augmentations of the minimum connective cost 
method have been proposed, bringing 
segmentation accuracy up to around 98-99% (e.g. 
Kurohashi and Nagao 1998, Fuchi and Takagi 
1998). 
Fully stochastic language models (e.g. Nagata 
1994), on the other hand, do not allow such manual 
cost manipulation and precisely for that reason, 
improvements in segmentation accuracy are harder 
to achieve. Attaining a high accuracy using fully 
stochastic methods is particularly difficult for 
Japanese due to the prevalence of orthographic 
variants (a word can be spelled in many different 
ways by combining different character sets), which 
exacerbates the sparse data problem. As a result, 
the performance of stochastic models is usually not 
as good as the heuristics-based language models. 
The best accuracy reported for statistical methods 
to date is around 95% (e.g. Nagata 1994). 
Our approach contrasts with the previous 
approaches in that the word-breaking component 
itself does not perform the selection of the best 
segmentation analysis at all. Instead, the 
word-breaker returns all possible words that span 
the given string in a word lattice, and the best word 
sequence is determined by applying the syntactic 
rules for building parse trees. In other words, there 
is no task of selecting the best segmentation per se; 
the best word-breaking analysis is merely a 
concomitant of the best syntactic parse. We 
demonstrate hat a robust, broad-coverage parser 
can be implemented irectly on a word lattice 
input and can be used to resolve word-breaking 
ambiguities effectively without adverse 
performance effects. A similar model of 
word-breaking is reported for the problem of 
Chinese word segmentation (Wu and Jiang 1998), 
but the amount of ambiguity that exists in the word 
822 
lattice is nmch larger in Japanese, which requires a
different treatment. In the l'ollowing, we first 
describe the word-breaker and the parser in more 
detail (Section 2); we then report the results of 
segmentation accuracy (Section 3) and the results 
of related experinaents a sessing the effects of the 
segmentation ambiguities in the word lattice to 
parsing (Section 4). In Conclusion, we discuss 
implications for future research. 
2. Using a broad-coverage parser for 
word-breaking 
The word-breaking and syntactic components 
discussed in the current study are implelnented 
within a broad-coverage, multi-purpose natural 
hmguage understanding system being developed at 
Microsoft Research, whose ultimate goal is to 
achieve deep Selnantic understanding of natural 
language I. A detailed escription of the system is 
found in Heidom (in press). Though we focus on 
the word-breaking and syntactic components in 
this paper, the syntactic analysis is by no means 
the final goal of the system; rather, a parse tree is 
considered to be an approxilnate first step toward a 
more useful meaning representation. We also aim 
at being truly broad-coverage, i.e., returning useful 
analyses irrespective of the genre or the subject 
matter of the input text, be it a newspaper articlc or 
a piece of e-mail. For the proposed model of 
word-breaking to work well, the following 
properties of the parser are particularly important. 
? The bottom-up chart parser creates syntactic 
analyses by building incrementally arger phrases 
fl'om individual words and phrases (Jensen et al 
1993). The analyses that span the entire input 
string are the complete analyses, and the words 
used in that analysis constitutes the word-breaking 
analysis for the string. Incorrect words returned by 
the word-breaker are filtered out by the syntactic 
rules, and will not make it into the final complete 
parse. 
? All the grammar rules, written in the 
formalism of Augmented Phrase Structure 
Grammar (lteidorn 1975), are binary, a feature 
crucial for dealing with free word-order and 
i Japanese is one of the seven languages under 
development in our lab, along with Chinese, English, 
French, German, Korean and Spanish. 
missing constituents (Jensen 1987). Not only has 
the rule formalism proven to be indispensable for 
parsing a wide range of English texts, it is all tile 
more critical 1'o1 parsing Japanese, as the free 
word-order and missing constituents are the norm 
for Japanese sentences. 
? There is very little semantic dependency in the 
grammar rules, which is essential if the grammar is
to be domain-independent. However, the grammar 
rules are elaborately conditioned on morphological 
and syntactic l'eatums, enabling much finer-grained 
parsing analyses than just relying on a small 
number of basic parts-of speech (POS). This gives 
the grammar the power to disambiguate multiple 
word analyses in the input lattice. 
13ecause we do not utilize semantic information, 
we perforln no selnantically motivated attachlnent 
of phrases during parsing. Instead, we parse them 
into a default analysis, which can then be expanded 
and disambiguatcd at later stages of processing 
using a large semantic knowledge base 
(Richardson 1997, Richardson et al 1998). One of 
the goals o1' Ihis paper is to show that the syntactic 
information alone can resolve the ambiguities in 
the word lattice sufficiently well to select the best 
breaking analysis in the absence of elaborate 
semantic information. Figure 1 (see Appendix) 
shows the default attachment of the relative clause 
to the closest NP. Though this structure may be 
semantically implausible, the word-breaking 
analysis is correct. 
The word-breaking colnponent of out" system is 
described in detail in Kacmarcik et al (2000). For 
the lmrpose of robust parsing, the component is 
expected to solve the following two problems: 
? Lemmatization: Find possible words in the 
input text using a dictionary and its inflectional 
morphology, and return the dictionary entry forms 
(lemmas). Note that multiple lemmas are often 
possible for a given inflected form (e.g. surface 
form /o,~z -(- (kalte) could be an inflected form of 
the verbs /9~3 (kau "buy"), /0~o (katu "win")or 
/o,~ (karu "trim"), in which case all these forms 
must be returned. The dictionary the word-breaker 
uses has about 70,000 unique entries. 
? Orthography norlnalization: Identify and 
norlnalize orthographic variants. This is a 
non-trivial task in Japanese, as words can be 
spelled using any colnbination of the tout" chanmter 
823 
4.3 Parser precision 
An initial concern in implementing the present 
model was that parsing ambiguous input might 
proliferate syntactic analyses. In theory, the 
number of analyses might grow exponentially as 
the input sentence length increased, making the 
reliable ranking of parse results unmanageable. In 
practice, however, pathological proliferation of 
syntactic analyses is not a problem s. Figure 4 
tallies the average number of parses obtained in 
relation to sentence l ngth for all successful parses 
in the 5,000-sentence t st corpus (corpus A in 
Table 1). There were 4,121 successful parses in the 
corpus, corresponding to 82.42% coverage. From 
Figure 4, we can see that the number of parses 
does increase as the sentence grows longer, but the 
increment is linear and the slope is very moderate. 
Even in the highest-scoring range, the mean 
number of parses is only 2.17. Averaged over all 
sentence lengths, about 68% of the successfully 
parsed sentences receive only one parse, and 22% 
receive two parses. Only about 10% of sentences 
receive more than 2 analyses. From these results 
we conclude that the overgeneration f parse trees 
is not a practical concern within our approach. 
3 
"6 2 
E 
==t 
1-10 11- 21- 31- 41- 51- 61- 71- 81- 91- >101 
20 30 40 50 60 70 80 90 100 
sentence length (in char) 
Figure 4. Average number ol'parses for corpus A 
(5,000 sentences) 
4.4 Performance 
A second potential concern was performance: 
would the increased number of records in the chart 
cause unacceptable degradation of system speed? 
5 A similar observation is made by Charniak et al 
(forthcoming), who find that the number ot' final parses 
caused by additional POS tags is far less than the 
theoretical worst case in reality. 
This concern also proved unfounded in practice. In 
another experiment, we evaluated the processing 
speed of the system by measuring the time it takes 
per character in the input sentence (in 
milliseconds) relative to the sentence length. The 
results are given in Figure 5. This figure shows 
that the processing time per-character grows 
moderately as the sentence grows longer, due to 
the increased number of intermediate analyses 
created during the parsing. But the increase is 
linear, and we interpret these results as indicating 
that our approach is fully viable and realistic in 
terms of processing speed, and robust against input 
sentence length. The current average parsing time 
for our 15,000-sentence corpus (with average 
sentence length of 49.02 characters) is 23.09 
sentences per second on a Dell 550MHz Pentium 
III machine with 512MB of RAM. 
1.5 
1.4 
1.3 
1.2 
1.1 
1 
0.9 
0.8 
0.7 
15 25 35 45 55 65 75 85 95 105 115 125 135 
sentence length (in char) 
Figure 5. Processing speed on a 15,000-sentence corpus 
5. Conclusion 
We have shown that a practical, broad-coverage 
parser can be implemented without requiring the 
word-breaking component to return a single 
segmentation a alysis, and that it can at the same 
time achieve high accuracy in POS-labeled 
word-breaking. Separating the tasks of word 
ident~/'l'cation a d best sequence selection offers 
flexibility in enhancing both recall and precision 
without sacrificing either at the cost of the other. 
Our results show that morphological nd syntactic 
information alone can resolve most word-breaking 
ambiguities. Nonetheless, some ambiguities 
require semantic and contextual information. For 
example, the following sentence allows two parses 
corresponding to two word-breaking analyses, of 
which the first is semantically preferred: 
826 
(1) ocha-ni haitte-irtt arukaroido 
tea-in contain-ASP alkaloid 
"the alkaloid contained in lea" 
(2) ocha-ni-ha itte-iru arukatwido 
tea-in-TOP go-ASP alkaloid 
? ? the alkaloid that has gone to the tea" 
Likewise, the sentence below allows two different 
interpretations of the morpheme de, either as a 
locative marker (1) or as a copula (2). Both 
interpretations are syntactically and semantically 
wflid; only contextual information can resolve the 
ambiguity. 
(1) minen-ha isuraeru-de aru 
next year-TOP Israel-LOC be-held 
"It will be held in Israel next year". 
(2) rainen-ha isuraeru de-artt 
next year-TOP Israel be-PP, ES 
"It will be Israel next year". 
In both these sentences, we create syntactic trees 
for all syntactically valid interpretations, leaving 
the ambiguity intact. Such ambiguities can only be 
resolved with semantic and contextual information 
eventually made available by higher processing 
components. This will be Ihe focus of our ongoing 
rese.arclt. 
Acknowledgements 
We: would like to thank Mari Bmnson and Kazuko 
Robertshaw for annotating corpora for target 
word-breaking and POS tagging. We also thank 
the anonymous reviewers and the members of the 
MSR NLP group for their comments on the earlier 
version of the paper. 
References  
Charniak, Eugene, Glenn Carroll, John Adcock, Antony 
Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael 
Littmaa, and John McCann. Forthcoming. Taggers l'or 
parsers. To appear in Artificial Intelligence. 
Fuchi, Takeshi and Shinichiro Takagi. 1998. Japanese 
Morphological Analyzer Using Word Co-Occurrence 
-JTAG-. Proceeding of ACL-COLING: 409-413. 
Gamon, Michael, Carmen Lozano, Jessie Pinkham and 
Tom Reutter. 1997. Practical Experience with 
Grammar Sharing in Multilingual NLP. Jill Burstcin 
and Chmdia Leacock (eds.), From Research to 
Commercial Applications: Making NLP Work in 
Practice (Proceedings of a Workshop Sponsored by 
the Association for Computational Linguistics). 
pp.49-56. 
Heidorn, George. 1975. Attgmented Phrase Structure 
Grammars. In B.L. Webber and R.C. Schank (eds.), 
Theoretical Issues in Natural Language Processing. 
ACL 1975: 1-5. 
Heidorn, George. In press. Intelligent Writing 
Assistance. To appear in Robert Dale, Hermann Moisl 
and Harold Seiners (eds.), lfandbook of Natural 
Language Processing. Chapter 8. 
Hisamitsu, T. and Y. Nitta. 1990. Morphological 
Analysis by Minimum Connective-Cost Method. 
Technical Report, SIGNLC 90-8. IEICE pp.17-24 (in 
Japanese). 
Jensen, Karen. 1987. Binary Rules and Non-Binary 
Trees: Breaking Down the Concept o1' Phrase 
Structure. In Alexis Manasler-Ramer (ed.), 
Mathematics of Language. Amsterdam: John 
Benjamins Publishing. pp.65-86. 
Jensen, Karen, George E. Hektorn and Stephen D. 
Richardson (eds.). 1993. Natural Language 
Processing: The PLNLP approach. Kluwer: Boston. 
Kacmareik, Gary, Chris Brockett and Hisami St, zuki. 
2000. Robust Segmentation of Japanese Text into a 
l,attice for Parsing. Proceedings of COLING 2000. 
Kurohashi, Sadao and Makoto Nagao. 1998. Building a 
Japanese Parsed Corpus While hnproving the Parsing 
System. First LREC Proceedings: 719-724. 
Murakami, J. and S. Sagayama. 1992. Hidden Markov 
Model Applied to Morphological Analysis. lPSJ 3: 
161 - 162 (in Japanese). 
Nagata, Masaaki. 1994. A Stochastic Japanese 
Morphological Analyzer Using a Forward-DP 
Backward-A* N-Best Search Algorithm. Proceedings 
o1' COLING '94:201-207. 
Richardson, Stephen D. 1997. Determining Similarity 
and Inferring Relatkms in a Lexical Knowledge Base. 
Ph.D. dissertation. The City University of New York. 
Richardson, Stephen D., William B. Dolan and Lucy 
Vanderwende. 1998. MindNet: acquiring and 
structuring semantic information f l 'om text. 
Proceedings of COLING-ACL OS: 1098-1102. 
Taket,chi, Koichi and Yuji Matsumoto. 1997. HMM 
Parameter Learning for Japanese Morphological 
Analyzer. IPSJ: 38-3 (in Japanese). 
Wu, Andi and Zixin Jiang. 1998. Word Segmentation in 
Sentence Analysis. Technical Report, MSR-TR-99-10. 
Microsoft Reseamh. 
827 
Appendix 
"(It) has been annoyed by the successive interventions by the neighboring country". 
~b~O<" VERB1 
~t~ NOUN1 
~t~ VERB2 
:::::::: O< ~ VERB3 
::: : : : : : : : : : : : : :  ~ NOUN2 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ NOUN3 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ PRONI 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ POSPI 
: : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ NOUN4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[C~ NOUN5 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[: VERB4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[C POSP2 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB5 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~NOUN6 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB~ 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ IJl 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ POSP3 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB7 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ Ia2 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ POSP4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~NOUN7 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB8 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ CONJI 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  b%~NOUN8 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  L%~ VERB9 
DECL I  NP I  NP2  RELCL I  VERB1*  "~%O?"  
NOUN2*  "~"  
PP I  POSP I*  "~"  
NOUN4*  "=~\]~" 
PP2 POSP2*  "\[C" 
VERBS* "~&~"  
AUXPI  VERB9*  " ~ "  
CHAR1 "o" 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
(successive) 
(neighboring country) 
(GEN) 
(intervention) 
(by) 
(annoyed) 
(be) 
Figure 1. Example of ambiguous attachment. RELCL I  can syntactically modify either NOUN2 or NOUNa. 
NOUN4 (non-local attachment) is the semantically correct choice. Shown above the parse tree is the 
input word lattice returned from the word-breaker. 
94~-~l~:{~,,~,W~\[7_-tk~bvEl,~Teo "Classical Thai literature is based on tradition and history". 
F ITTED1 NP I  
NP2  
NOUN1*  "9-1" i~  ~ "  (classical Thai literature) 
PPI POSP i* " \ [~"  (TOP IC)  
UP3 NOUn2 * "{~"  (tradition) 
posp2 * ,, k" ,, (and) 
NP4 NOUN3 * "~"  (history) 
PP2 POSP3 * "17-" (on) 
VPI  VERB1*  ":6~'-5t VC"  (based) 
AUXPI  VERB2 * ,, I, xT~ ,, (be) 
CHAR1 " o " 
Figure 2. Example of an incomplete parse with correct word-breaking results. 
828 
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 8?9,
Vancouver, October 2005.
MindNet: an automatically-created lexical resource 
 
Lucy Vanderwende, Gary Kacmarcik, Hisami Suzuki, Arul Menezes 
Microsoft Research 
Redmond, WA 98052, USA 
{lucyv, garykac, hisamis, arulm}@microsoft.com 
 
Abstract 
We will demonstrate MindNet, a lexical resource 
built automatically by processing text.  We will 
present two forms of MindNet: as a static lexical 
resource, and, as a toolkit which allows MindNets 
to be built from arbitrary text.  We will also intro-
duce a web-based interface to MindNet lexicons 
(MNEX) that is intended to make the data con-
tained within MindNets more accessible for explo-
ration.  Both English and Japanese MindNets will 
be shown and will be made available, through 
MNEX, for research purposes. 
1 MindNet 
A MindNet is a collection of semantic relations 
that is automatically extracted from text data using 
a broad coverage parser. Previous publications on 
MindNet (Suzuki et al, 2005, Richardson et al, 
1998, Vanderwende 1995) have focused on the 
effort required to build a MindNet from the data 
contained in Japanese and English lexicons. 
Semantic Relations 
The semantic relations that are stored in MindNet 
are directed, labeled relationships between two 
words; see Table 1:  
Attributive Manner Source 
Cause Means Synonym 
Goal Part Time 
Hypernym Possessor TypicalObject 
Location Result TypicalSubject 
Table 1: A sampling of the semantic relations stored in 
MindNet 
 
These semantic relations are obtained from the 
Logical Form analysis of our broad coverage 
parser NLPwin (Heidorn, 2000).  The Logical 
Form is a labeled dependency analysis with func-
tion words removed.  We have not completed an 
evaluation of the quality of the extracted semantic 
relations.  Anecdotally, however, the quality varies 
according to the relation type, with Hypernym and 
grammatical relations TypicalSubject and Typi-
calObj being reliable, while relations such as Part 
and Purpose are less reliable. By making MindNet 
available, we solicit feedback on the utility of these 
labeled relationships, especially in contrast to sim-
ple co-occurrence statistics and to the heavily used 
hypernymy and synonymy links. Furthermore, we 
solicit feedback on the level of accuracy which is 
tolerable for specific applications. 
Semantic Relation Structures 
We refer to the hierarchical collection of semantic 
relations (semrels) that are automatically extracted 
from a source sentence as a semrel structure. Each 
semrel structure contains all of the semrels ex-
tracted from a single source sentence.  A semrel 
structure can be viewed from the perspective of 
each unique word that occurs in the structure; we 
call these inverted structures.  They contain the 
same information as the original, but with a differ-
ent word placed at the root of the structure. An ex-
ample semrel structure for the definition of 
swallow is given in Figure 1a, and its inversion, 
from the perspective of wing is given in Figure 1b: 
 
swallow           wing 
 Hyp bird           PartOf bird 
       Part wing             Attrib small 
       Attrib small          HypOf swallow 
 
Figure 1a and b: Figure 1a is the semrel structure for the 
definition of swallow1, Figure 1b the inversion on wing. 
2 MNEX 
MNEX (MindNet Explorer) is the web-based inter-
face to MindNet that is designed to facilitate 
browsing MindNet structure and relations. MNEX 
displays paths based on the word or words that the 
                                                          
1
 Swallow: a small bird with wings (LDOCE).  Definition 
abbreviated for purposes of exposition.   
8
user enters. A path is a set of links that connect one 
word to another within either a single semrel struc-
ture or by combining fragments from multiple 
semrel structures.  Paths are weighted for compari-
son (Richardson, 1997). Currently, either one or 
two words can be specified and we allow some 
restrictions to refine the path search.  A user can 
restrict the intended part of speech of the words 
entered, and/or the user can restrict the paths to 
include only the specified relation. When two 
words are provided, the UI returns a list of the 
highest ranked paths between those two words. 
When only one word is given, then all paths from 
that word are ranked and displayed.  Figure 2 
shows the MNEX interface, and a query requesting 
all paths from the word bird, restricted to Noun 
part of speech, through the Part relation:  
 
 
Figure 2: MNEX output for ?bird (Noun) Part? query 
3 Relation to other work 
For English, WordNet is the most widely used 
knowledgebase. Aside from being English-only, 
this database was hand-coded and significant effort 
is required to create similar databases for different 
domains and languages. Projects like EuroWord-
Net address the monolingual aspect of WordNet, 
but these databases are still labor intensive to cre-
ate.  On the other hand, the quality of the informa-
tion contained in a WordNet (Fellbaum et al, 
1998) is very reliable, exactly because it was 
manually created.  FrameNet (Baker et al, 1998) 
and OpenCyc are other valuable resources for Eng-
lish, also hand-created, that contain a rich set of 
relations between words and concepts. Their use is 
still being explored as they have been made avail-
able only recently. For Japanese, there are also 
concept dictionaries providing semantic relations, 
similarly hand-created, e.g., EDR and Nihongo 
Goi-taikei (NTT). 
The demonstration of MindNet will highlight 
that this resource is automatically created, allowing 
domain lexical resources to be built quickly, albeit 
with lesser accuracy.  We are confident that this is 
a trade-off worth making in many cases, and en-
courage experimentation in this area.  MNEX al-
lows the exploration of the rich set of relations 
through which paths connecting words are linked. 
4 References 
Baker, Collin F., Fillmore, Charles J., and Lowe, John 
B. (1998): The Berkeley FrameNet project. in Pro-
ceedings of the COLING-ACL, Montreal, Canada. 
Fellbaum, C. (ed). 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press. 
Heidorn, G. 2000. Intelligent writing assistance. in 
R.Dale, H.Moisl and H.Somers (eds.), A Handbook 
of Natural Langauge Processing: Techniques and 
Applications for the Processing of Language as Text. 
New York: Marcel Dekker. 
National Institute of Information and Communications 
Technology. 2001. EDR Electronic Dictionary Ver-
sion 2.0 Technical Guide. 
NTT Communications Science Laboratories. 1999. Goi-
Taikei - A Japanese Lexicon. Iwanami Shoten. 
OpenCyc. Available at: http://www.cyc.com/opencyc. 
Richardson, S.D. 1997, Determining Similarity and In-
ferring Relations in a Lexical Knowledge Base. PhD. 
dissertation, City University of New York. 
Richardson, S.D., W. B. Dolan, and L. Vanderwende. 
1998. MindNet: Acquiring and Structuring Semantic 
Information from Text, In Proceedings of ACL-
COLING. Montreal, pp. 1098-1102. 
Suzuki, H., G. Kacmarcik, L. Vanderwende and A. 
Menezes. 2005. Mindnet and mnex. In Proceedings 
of the 11th Annual meeting of the Society of Natural 
Language Processing (in Japanese).  
Vanderwende, L. 1995. Ambiguity in the acquisition of 
lexical information. In Proceedings of the AAAI 
1995 Spring Symposium Series, symposium on rep-
resentation and acquisition of lexical knowledge, 
174-179. 
9
Multi-Modal Question-Answering: Questions without Keyboards 
 
Gary Kacmarcik 
Natural Language Processing Group 
Microsoft Research 
garykac@microsoft.com 
 
Abstract 
This paper describes our work to allow 
players in a virtual world to pose ques-
tions without relying on textual input. 
Our approach is to create enhanced vir-
tual photographs by annotating them 
with semantic information from the 3D 
environment?s scene graph. The player 
can then use these annotated photos to 
interact with inhabitants of the world 
through automatically generated que-
ries that are guaranteed to be relevant, 
grammatical and unambiguous. While 
the range of queries is more limited 
than a text input system would permit, 
in the gaming environment that we are 
exploring these limitations are offset by 
the practical concerns that make text 
input inappropriate. 
1 Introduction 
The question-posing part of Question-
Answering (QA) has long relied on the coopera-
tive nature of the person posing the question. 
This assumption is not unreasonable because it 
generally behooves the querent to assist the QA 
system wherever possible. 
However, even given this cooperative nature, 
QA systems that rely on text input still have to 
deal with input that is malformed, underspeci-
fied or problematic in some other way. This 
problem is further compounded when the system 
is open to users who may find it more entertain-
ing to explore the boundaries, limitations and 
humorous errors of the text input and parsing 
system instead of using the system as intended. 
Thus, any system that is intended to be released 
to a wide audience needs to be designed to han-
dle these problems in a robust manner. 
Additionally, there are applications where 
QA technologies would be beneficial, but the 
reliance on text input renders them impractical. 
The focus of the present work, interactive virtual 
game worlds, is one such area where text input 
is not desirable ? both because it interrupts the 
game flow and because many game systems do 
not have keyboards available. 
In this paper, we explore one method of cre-
ating a non-text input mode for QA that relies on 
specially annotated virtual photographs. 
Our approach is to create a virtual game 
world where all of the objects (and some non-
objects) are annotated with semantic information 
that is constructed automatically by parsing 
natural language text descriptions. By interact-
ing with world objects, a player is actually se-
lecting portions of the semantic network that can 
in turn be used to enable a limited QA dialog 
with denizens of the game world. While this 
method is clearly not as flexible as full natural 
language input, it successfully avoids most of 
the serious natural language input problems in 
much the same way that Tennent et al (1983) 
avoided the ambiguity, paraphrase and incorrect 
input problems in their NLMENU system. In ad-
dition, our system does so without the awk-
wardness of forcing players to build utterances 
word-by-word from a series of menus. 
In our system, players interact with non-
player characters (NPCs: characters in the 
world whose actions are controlled by the com-
puter) by taking virtual photographs of objects 
in the world that they want to discuss and then 
showing the photos to the NPCs. When the 
photo is taken, all of the relevant semantic in-
formation from the world is attached to the 
photo so that it acts as a standalone object ? 
even if the game world changes, the contents of 
the photo are still valid and consistent. By com-
bining these photo annotations with information 
in the NPC?s knowledgebase (KB), we can cre-
ate the illusion that the NPC has a rudimentary 
understanding of the photo contents and create a 
novel interaction modality that gives the player a 
167
wide range of expression. 
It is worth noting that while we discuss using 
these annotations in the context of a virtual 
photo, the annotations can also be applied in 
realtime interactive systems. In this work, we 
restrict ourselves to the use of virtual photos 
primarily because it allows us to interact with a 
static scene, thus eliminating the temporal diffi-
culties (graphical and linguistic) that would be 
caused by interacting with a dynamic dataset. 
2 Previous Work 
A wide variety of work has been done on inte-
grating graphics and/or virtual environments 
with natural language dating back to Winograd?s 
(1972) classic ?blockworld? simulation. More 
recently, researchers have been investigating 
how graphics and natural language can work 
together to create more compelling interfaces. 
2.1 Multimodal Interfaces 
A large body of work has been created on mul-
timodal interfaces ? combining multiple modes 
of interaction so that the advantages of one 
mode offset the limitations of another. In the 
specific case of combining natural language and 
graphics, there have been two main areas of 
study: interacting with graphical elements to 
resolve ambiguous references on the natural lan-
guage side (Bolt, 1980; Kobsa et al, 1986); and 
generating coordinated text and graphic presen-
tations using information from a knowledgebase 
(Andr? and Rist (1994); Towns et al (1998)). 
In addition to these two main areas, early 
work by Tennant (1983) experimented with us-
ing a predictive left-corner parser to populate 
dynamic menus that the user would navigate to 
construct queries that were guaranteed to be cor-
rect and task-relevant. 
Our work contains elements from all of these 
categories in that we use input gestures to re-
solve reference ambiguity and we make use of a 
KB to coordinate the linguistic and graphical 
information. We were also inspired by 
Tennant?s work on restricting the player?s input 
to avoid parsing problems. However, our work 
differs from previous efforts in that we: 
? Do not use text input at runtime 
? Use virtual cameras and input gestures for in-
teraction 
? Do not require that interactions be built one 
unit (word or graphical references) at a time 
? Focus primarily on text generation 
2.2 Virtual Photographs 
The concept of a virtual photograph has existed 
as long as people have taken screenshots of their 
view into a 3D environment. Recently, however, 
there have been a few applications that have ex-
perimented with adding a limited amount of in-
teractivity to these static images. Video games, 
notably POK?MON SNAP (Nintendo, 1999), in-
corporate a limited form of interactive virtual 
photos. While there is no published information 
about the techniques used in these games, we 
can infer much by examining the level of inter-
action permitted. 
In POK?MON SNAP, the player zooms around 
each level on a rail car taking as many photo-
graphs of ?wild? pok?mon as possible. Scoring 
in the game is based not only on the number of 
unique subjects found (and successfully photo-
graphed), but also on the quality of the individ-
ual photographs. The judging criteria include: 
? Is the subject centered in the photo? 
? Is the face visible? (for identifiability) 
? Does the subject occupy a large percentage 
of the image? 
? Are there multiple pok?mon (same type)? 
? What is the subject doing? (pose) 
In order to properly evaluate the photos, the 
game must perform some photo annotation 
when the photo is taken. However, since interac-
tion with the photo is limited to scoring and dis-
play, these annotations are easily reduced to the 
set of values necessary to calculate the score. 
From the players? perspective, since there is no 
mechanism for interacting with the contents of 
the photo, all interaction is completed by the 
time the photo is taken - the photo merely serves 
as an additional game object.  
2.3 Interactive Images 
Recently, a lot of work has gone on in the field 
of making images (including electronic versions 
of real photographs) more interactive by manu-
ally or automatically annotating image contents 
or by making use of existing image metadata. 
The most commonly used example of this are 
the HTML image maps (Berners-Lee and Con-
nolly, 1995) supported by most web browsers. 
An example that is more relevant to our work 
168
is the ALFRESCO system (Stock, 1991), which 
uses graphical representations of Italian frescos 
and allows the user to query using a combina-
tion of natural language and pointing gestures. 
Beyond the obvious difference that our system 
doesn?t permit direct natural language input, our 
work also differs in that we annotate the images 
with scene information beyond a simple object 
ID and we calculate the image regions automati-
cally from the objects in the virtual world. 
3 Interacting with Virtual Photos 
As mentioned in the Introduction, virtual photos 
can become a useful metaphor for interaction 
with NPCs in games. Ideally, the player should 
be able to take a picture of anything in the vir-
tual world and then show that photo to an NPC 
to engage in a dialog about the photo contents. 
In our implementation, the player interacts 
with the NPC by clicking on an object in the 
photo to pull up a menu of context-dependent 
natural language queries. When the player se-
lects an item from this menu, the query is sent to 
the NPC that the player is currently ?talking to?. 
This menu of context sensitive queries is crucial 
to the interaction because a pointing gesture 
without an accompanying description is am-
biguous (Schmauks, 1987) and it is through this 
menu selection that the player expresses intent 
and restricts the scope of the dialog. 
There are two obvious benefits to approach-
ing the QA interaction in this way. First, even 
though the topic is limited by the objects in the 
photo, the player is given control over the direc-
tion of the dialog. This is an improvement over 
the traditional scripted NPC interaction where 
the player has little control over the dialog. The 
other benefit is that while the player is given 
control over the content, the player is not 
granted too much control since the photo meta-
phor limits the topic to things that are relevant to 
the game. This effectively avoids the out-of-
domain, paraphrase and ambiguity problems that 
commonly plague natural language interfaces. 
3.1 Annotations 
The quality of player-NPC interaction is di-
rectly dependent on the kind of annotations that 
are used. For example, associating a literal text 
string with each object would result in a system 
where the NPCs would not exhibit individuality 
since they would all produce the exact same an-
swer to a query. Alternately, using a global ob-
ject identifier would also cause problems 
because in a dynamically changing world we 
would need to create a system to keep track of 
differences from object at the time of the photo 
and the object?s current state. 
It is for these reasons that we record for each 
object an abstract representation that we can 
manipulate and merge with data from other 
sources like the NPC?s KB. Beyond providing a 
place to record information about the objects 
that are specific to a particular photo, this also 
allows us to individualize the NPC responses 
and create a more interesting QA interaction. 
3.2  Example Interaction 
As a simple example, imagine a photo taken by 
a player that shows a few houses in a town. Tak-
ing this photo to an NPC and clicking on one of 
the houses will bring up a menu of possible 
questions that is determined by the object and 
the contents of the NPC?s KB. Selecting the de-
fault ?What is this?? query for an NPC that has 
no special knowledge of the objects in this photo 
will result in the generic description (stored in 
the photo) being used for the NPC?s response 
(e.g., ?That is a blue house?). 
If, however, the NPC has some knowledge 
about the object, then the NPC will be able to 
provide information beyond that provided within 
the photo. Given the following information: 
This is John?s house. 
My name for John is my father. 
the NPC can piece it all together and generate 
?That is my father?s house? as an answer. 
4 Representing Knowledge 
A key component of our system is the semantic 
representation that is used to encode not only the 
information that the NPC has about the sur-
roundings, but also to encode the contents of the 
virtual photo. These KBs, which are created 
from text documents containing natural lan-
guage descriptions, form the core document set 
on which the QA process operates. 
4.1 Semantic Representation 
While there are a variety of representations that 
can be used to encode semantic information, we 
opted to use a representation that is automati-
169
cally extracted from natural language text. We 
chose this representation because we desired a 
notation that: 
? Is easy to create 
? Provides broad coverage over structures 
found in natural language 
? Is easy to manipulate, and 
? Is easy to convert into text for display 
Because of these requirements, we use a predi-
cate-argument style representation (Campbell 
and Suzuki, 2002) that is produced by our 
parser. These structures, called logical forms 
(LFs), are the forms that are stored in the KB. 
This tree structure has many advantages. 
First, since it is based on our broad coverage 
grammar it provides a reasonable representation 
for all of the things that a player or NPC is likely 
to want to talk about in a game. We also are 
readily able to generate output text from this 
representation by making use of our generation 
component. In addition, the fact that this repre-
sentation is created directly from natural lan-
guage input means that game designers can 
create these KBs without any special training in 
knowledge representation. 
Another advantage of this tree structure is 
that it is easy to manipulate by copying subtrees 
from one tree into another. Passing this manipu-
lated tree to our generation component results in 
the text output that is presented to the user. The 
ease with which we can manipulate these struc-
tures allows us to dynamically create new trees 
and provide the NPC with the ability to talk 
about a wide array of subjects without having to 
author all of the interactions. 
4.2 Anaphora 
As mentioned, once these sentences for the KB 
have been authored, our parser automatically 
handles the work required to create the LFs from 
the text. However, we do not have a fully auto-
matic solution for the issue of reference resolu-
tion or anaphora. For this, we currently rely on 
the person creating the KB to resolve references 
to objects within the text or KB (endophora) and 
in the virtual world (exophora). 
5 Posing Questions 
In our system questions are posed by first nar-
rowing down the scope of the query by selecting 
an object in a virtual photo, and then choosing a 
query from a list that is automatically produced 
by the QA system. This architecture places a 
heavy burden on the query generation compo-
nent since that is the component that determines 
the ultimate limitations of the system. 
5.1 Query Generation 
In a system where the only automatically gener-
ated queries are allowed, it is important to be 
able to create a set of interesting queries to avoid 
frustrating the user. Beyond the straightforward 
?Who/What/Where is this??-style of questions, 
we also use a question generator (originally de-
scribed by Schwartz et al (2004) in the context 
of language learning) to produce a set of an-
swerable questions about the selected object. 
Once the player selects a query, the final step 
in query generation is to create the LF represen-
tation of the question. This is required so that we 
can more easily find matches in the KB. Fortu-
nately, because the queries are either formulaic 
(e.g., the ?Who/What/Where? queries), or ex-
tracted from the KB, the LF is trivially created 
with requiring a runtime parsing system. 
5.2 Knowledgebase Matching 
When the player poses a query to an NPC, we 
need to find an appropriate match in the KB. To 
do this, we perform subtree matches between the 
query?s LF and the contents of the KB, after first 
modifying the original query so that question 
words (e.g., Who, What, ...) are replaced with 
special identifiers that permit wildcard matches. 
When a match is found, a complete, grammati-
cal response is created by replacing the wildcard 
node with the matching subtree and then passing 
this structure to the text generation component. 
5.3 Deixis 
In order to make the NPC?s responses believ-
able, the final step is to incorporate deictic refer-
ences into the utterance. These are references 
that depend on the extralinguistic context, such 
as the identity, time or location of the speaker or 
listener. Because the semantic structures are 
easy to manipulate, we can easily replace these 
references with the appropriate reference. An 
example of this was given earlier when the sub-
tree corresponding to ?my father? was used to 
refer to the owner of the house. 
This capability gives us a convenient way to 
170
support having separate KBs for shared knowl-
edge and individual knowledge. General infor-
mation can be placed in the shared KB, while 
knowledge that is specific to an individual (like 
the fact that John is ?my father?) is stored in a 
separate KB that is specific to that individual. 
This allows us to avoid having to re-author the 
knowledge for each NPC while still allowing 
individualized responses. 
6 Creating Annotated Photographs 
Our virtual photos consist of three major parts: 
the image, the object locator map and the object 
descriptors. In addition, we define some simple 
metadata. We use the term ?annotations? to refer 
to the combination of the object locator map, the 
descriptors and the metadata. 
While the photo image is trivially created by 
recording the camera view when the photo is 
taken, the other parts require special techniques 
and are described in the following sections. 
6.1 The Object Locator Map (OLM) 
The object locator map (OLM) is an image-
space map that corresponds 1-to-1 with the pix-
els in the virtual photograph image. For each 
image pixel, the corresponding OLM ?pixel? 
contains information about the object that corre-
sponds to that image-space location. We create 
the OLM using the back buffer technique attrib-
uted originally to Weghorst et al (1984). 
6.2 The Object Descriptors 
The object descriptors contain the semantic de-
scription of the objects plus some metadata that 
helps determine how the player and NPC can 
interact with the objects in the photo. 
In our system, we use the semantic annota-
tions associated with each object as a generic 
description that contains information that would 
be readily apparent to someone looking at the 
object. Thus, these descriptions focus on the 
physical characteristics (derived from the object 
description) or current actions (derived from the 
current animation state) of the object. 
7 3D Modeling 
The modeling of 3D scenes and objects has 
typically been done in isolation, where only 
graphical (display and performance) concerns 
were considered. In this section, we discuss 
some of the changes that are required on the 
modeling side to better support our interaction. 
7.1 Enhancements 
Beyond the enhancement of attaching abstract 
semantic descriptions (rather than simple text 
labels as in Feiner et al (1992)) to each object in 
the virtual world?s scene graph, we introduce a 
few other features to enhance the interactivity of 
the virtual photos. 
Semantic Anchors 
A limitation of attaching the semantic descrip-
tions to objects in the 3D world is that this only 
covers concrete objects that have a physical rep-
resentation in the world. Semi-abstract objects 
(called ?negative parts? by Landau and 
Jackendoff (1993)) like a cave or a hole do not 
have a direct representation in the world and 
thus do not have objects onto which semantic 
descriptions can be attached. However, it is cer-
tainly possible that the player might wish to re-
fer to these objects in the course of a game. 
We provide support for these referable, non-
physical objects through the use of semantic 
anchors, which are invisible objects in the world 
that provide anchor points onto which we can 
attach information. For example, abstract objects 
like a hole or a cave can be filled with a seman-
tic anchor so that when a photo is taken of a re-
gion that includes the cave, the player can click 
on that region and get a meaningful result. 
Since these objects are not displayed, there is 
no requirement that they be closed 3D forms. 
This gives us the flexibility to create view-
dependent semantic anchors by tagging regions 
of space based on the current view. For exam-
ple, a cave entrance could be labeled simply as a 
?cave? for viewpoints outside the cave while 
this same portal can be termed an ?exit? (or left 
unlabeled) from vantage points inside the cave. 
By orienting these open forms correctly, we can 
rely on the graphic engine?s backface culling1 to 
automatically remove the anchors that are inap-
                                                          
1
 Backface culling is an optimization technique that re-
moves back-facing surfaces (i.e., surfaces on the side of the 
object away from the viewer) from the graphic engine pipe-
line so that resources are not wasted processing them. This 
technique relies on the assumption that all the objects in the 
virtual world are closed 3D forms so that drawing only the 
front-facing surfaces doesn?t change the resulting image. 
171
propriate for the current view. 
Action Descriptions 
In addition to attaching semantic descriptions to 
objects, we also allow semantic descriptions be 
added to animation sequences in the game. This 
provides a convenient mechanism for identify-
ing what a person is doing in a photo so that 
questions relating to action can be proposed. 
Key Features 
We also permit key features to be defined (as 
was apparently done for POK?MON SNAP) so 
that we can approximate object identifiability. In 
our implementation, we require that (at least a 
portion of) all key features are visible to satisfy 
this requirement. 
The advantage of this approach is that it is 
easy to implement (since there?s no need to de-
termine if the entire key feature is visible), but it 
requires that the key features be chosen carefully 
in order to produce reasonable results. 
7.2 Limitations 
Even with the proposed enhancements, there are 
clear limitations to the annotated 3D model ap-
proach that will require further investigation. 
First, there is an unfortunate disconnect be-
tween the modeled structures and semantic 
structures. When a designer creates a 3D model, 
the only consideration is the graphical presenta-
tion of the model and so joints like a wrist or 
elbow are likely to be modeled as a single point. 
This contrasts with a more semantic representa-
tion, which would have the wrist extend slightly 
into the hand and forearm. 
Another problem is the creation of relation-
ships between the objects in the photo. This is 
difficult because many relationships (like ?next 
to? or ?behind?) can mean different things in 
world-space (as they are in the virtual world) 
and image-space (as they appear in the photo). 
And finally, there is the standard ?picking 
from an object hierarchy? problem where, when 
a node in the hierarchy is selected, the user?s 
intent is ambiguous since the intended item 
could be the node or any of its parent nodes. 
8 Conclusions 
In this paper we have described our approach to 
allowing users to specify queries by interacting 
with virtual photographs that have been anno-
tated with semantic information. While this ap-
proach is clearly more limited than allowing full 
text input, it is useful for applications like games 
that do not always have a keyboard available. 
References 
E. Andr? and T. Rist. 1994. ?Referring to World Ob-
jects with Text and Pictures?. In Proceedings of 
COLING 1994. 530-534. 
T. Berners-Lee and D. Connolly. 1995. ?HyperText 
Markup Language Spec. - 2.0?. W3C RFC1866. 
R.A. Bolt. 1980. ??Put-that-there?: Voice and Gesture 
at the Graphics Interface?. SIGGRAPH 80. ACM 
Press. 262-270. 
R. Campbell and H. Suzuki. 2002. ?Language Neu-
tral Representation of Syntactic Structure?. In 
Proceedings of SCANALU 2002.  
S. Feiner. B. MacIntyre and D. Seligmann. 1992. 
?Annotating the real world with knowledge-based 
graphics on a see-through head-mounted display?. 
Graphics Interface 92. 78-85. 
A. Kobsa, J. Allgayer, C. Reddig, N. Reithinger, D. 
Schmauks, K. Harbusch and W. Wahlster. 1986. 
?Combining Deictic Gestures and Natural Lan-
guage for Referent Identification?. In Proceedings 
of COLING 1986. 356-361. 
B. Landau and R. Jackendoff. 1993. ??What? and 
?Where? in spatial language and spatial cognition?. 
Behavioral and Brain Sciences, 16, 217-265. 
D. Schmauks. 1987. ?Natural and Simulated Point-
ing?. In Proceedings of the 3rd European ACL 
Conference, Copenhagen. 179-185. 
L. Schwartz, T. Aikawa, and M. Pahud, ?Dynamic 
Language Learning Tools?, in InSTIL/ICALL 
Symp. 2004: NLP and Speech Technologies in 
Adv. Lang. Learning, Venice, Italy, June 2004. 
O. Stock. 1991. ?AlFresco: Enjoying the Combina-
tion of NLP and Hypermedia for Information Ex-
ploration?, In AAAI Workshop on Intelligent 
Multimedia Interfaces 1991. 197-224. 
H. Tennant, K. Ross, R. Saenz, C. Thompson and J. 
Miller. 1983. ?Menu-based natural language un-
derstanding?. In Proc. of ACL ?83. 151-158. 
S. Towns, C. Callaway and J. Lester. 1998. ?Generat-
ing Coordinated Natural Language and 3D Anima-
tions for Complex Spatial Explanations?. In Proc. 
of the 15th National Conf. on AI. 112-119. 
T. Winograd. 1972. Understanding Natural Lan-
guage. Academic Press. 1972. 
H. Weghorst, G. Greenberg and D. Greenberg. 1984. 
?Improved Computational Methods for Ray Trac-
ing?. ACM Transactions on Graphics 3, 1, 52-69. 
172
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 444?451,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Obfuscating Document Stylometry to Preserve Author Anonymity 
 
 
Gary Kacmarcik         Michael Gamon 
Natural Language Processing Group 
Microsoft Research 
Redmond, WA  USA 
{garykac,mgamon}@microsoft.com 
 
  
 
Abstract 
This paper explores techniques for reduc-
ing the effectiveness of standard author-
ship attribution techniques so that an au-
thor A can preserve anonymity for a par-
ticular document D. We discuss feature 
selection and adjustment and show how 
this information can be fed back to the 
author to create a new document D? for 
which the calculated attribution moves 
away from A. Since it can be labor inten-
sive to adjust the document in this fash-
ion, we attempt to quantify the amount of 
effort required to produce the ano-
nymized document and introduce two 
levels of anonymization: shallow and 
deep. In our test set, we show that shal-
low anonymization can be achieved by 
making 14 changes per 1000 words to 
reduce the likelihood of identifying A as 
the author by an average of more than 
83%. For deep anonymization, we adapt 
the unmasking work of Koppel and 
Schler to provide feedback that allows 
the author to choose the level of ano-
nymization. 
1 Introduction 
Authorship identification has been a long stand-
ing topic in the field of stylometry, the analysis 
of literary style (Holmes 1998). Issues of style, 
genre, and authorship are an interesting sub-area 
of text categorization. In authorship detection it 
is not the topic of a text but rather the stylistic 
properties that are of interest. The writing style 
of a particular author can be identified by analyz-
ing the form of the writing, rather than the con-
tent. The analysis of style therefore needs to ab-
stract away from the content and focus on the 
content-independent form of the linguistic ex-
pressions in a text. 
Advances in authorship attribution have raised 
concerns about whether or not authors can truly 
maintain their anonymity (Rao and Rohatgi 
2000). While there are clearly many reasons for 
wanting to unmask an anonymous author, nota-
bly law enforcement and historical scholarship, 
there are also many legitimate reasons for an au-
thor to wish to remain anonymous, chief among 
them the desire to avoid retribution from an em-
ployer or government agency. Beyond the issue 
of personal privacy, the public good is often 
served by whistle-blowers who expose wrong-
doing in corporations and governments. The loss 
of an expectation of privacy can result in a chill-
ing effect where individuals are too afraid to 
draw attention to a problem, because they fear 
being discovered and punished for their actions. 
It is for this reason that we set out to investi-
gate the feasibility of creating a tool to support 
anonymizing a particular document, given the 
assumption that the author is willing to expend a 
reasonable amount of effort in the process. More 
generally, we sought to investigate the sensitivity 
of current attribution techniques to manipulation. 
For our experiments, we chose a standard data 
set, the Federalist Papers, since the variety of 
published results allows us to simulate author-
ship attribution ?attacks? on the obfuscated docu-
ment. This is important since there is no clear 
consensus as to which features should be used 
for authorship attribution. 
2 Document Obfuscation 
Our approach to document obfuscation is to 
identify the features that a typical authorship at-
tribution technique will use as markers and then 
adjust the frequencies of these terms to render 
them less effective on the target document. 
444
While it is obvious that one can affect the attri-
bution result by adjusting feature values, we 
were concerned with: 
? How easy is it to identify and present the 
required changes to the author? 
? How resilient are the current authorship 
detection techniques to obfuscation? 
? How much work is involved for the au-
thor in the obfuscation process? 
The only related work that we are aware of is 
(Rao and Rohatgi 2000) who identify the prob-
lem and suggest (somewhat facetiously, they 
admit) using a round-trip machine translation 
(MT) process (e.g., English ? French ? Eng-
lish) to obscure any traces of the original au-
thor?s style. They note that the current quality of 
MT would be problematic, but this approach 
might serve as a useful starting point for some-
one who wants to scramble the words a bit be-
fore hand-correcting egregious errors (taking 
care not to re-introduce their style). 
2.1 The Federalist Papers 
One of the standard document sets used in au-
thorship attribution is the Federalist Papers, a 
collection of 85 documents initially published 
anonymously, but now known to have been writ-
ten by 3 authors: Alexander Hamilton, John 
Madison and John Jay. Due to illness, Jay only 
wrote 5 of the papers, and most of the remaining 
papers are of established authorship (Hamilton = 
51; Madison = 14; and 3 of joint authorship be-
tween Hamilton and Madison). The 12 remaining 
papers are disputed between Hamilton and Madi-
son. In this work we limit ourselves to the 65 
known single-author papers and the 12 disputed 
papers. 
While we refer to these 12 test documents as 
?disputed?, it is generally agreed (since the work 
of Mosteller and Wallace (1964)) that all of the 
disputed papers were authored by Madison. In 
our model, we accept that Madison is the author 
of these papers and adopt the fiction that he is 
interested in obscuring his role in their creation. 
2.2 Problem Statement 
A more formal problem statement is as follows: 
We assume that an author A (in our case, Madi-
son) has created a document D that needs to be 
anonymized. The author self-selects a set K of N 
authors (where A ? K) that some future agent 
(the ?attacker? following the convention used in 
cryptography) will attempt to select between. 
The goal is to use authorship attribution tech-
niques to create a new document D? based on D 
but with features that identify A as the author 
suppressed. 
3 Document Preparation 
Before we can begin with the process of obfus-
cating the author style in D, we need to gather a 
training corpus and normalize all of the docu-
ments. 
3.1 Training Corpus 
While the training corpus for our example is 
trivially obtained, authors wishing to anonymize 
their documents would need to gather their own 
corpus specific for their use. 
The first step is to identify the set of authors K 
(including A) that could have possibly written the 
document. This can be a set of co-workers or a 
set of authors who have published on the topic. 
Once the authors have been selected, a suitable 
corpus for each author needs to be gathered. This 
can be emails or newsgroup postings or other 
documents. In our experiments, we did not in-
clude D in the corpus for A, although it does not 
seem unreasonable to do so. 
For our example of the Federalist Papers, K is 
known to be {Hamilton, Madison} and it is al-
ready neatly divided into separate documents of 
comparable length. 
3.2 Document Cleanup 
Traditional authorship attribution techniques rely 
primarily on associating idiosyncratic formatting, 
language usage and spelling (misspellings, typos, 
or region-specific spelling) with each author in 
the study. Rao and Rohatgi (2000) and Koppel 
and Schler (2003) both report that these words 
serve as powerful discriminators for author attri-
bution. Thus, an important part of any obfusca-
tion effort is to identify these idiosyncratic usage 
patterns and normalize them in the text. 
Koppel and Schler (2003) also note that many 
of these patterns can be identified using the basic 
spelling and grammar checking tools available in 
most word processing applications. Correcting 
the issues identified by these tools is an easy first 
step in ensuring the document conforms to con-
ventional norms. This is especially important for 
work that will not be reviewed or edited since 
these idiosyncrasies are more likely to go unno-
ticed. 
445
However, there are distinctive usage patterns 
that are not simple grammar or spelling errors 
that also need to be identified. A well-known 
example of this is the usage of while/whilst by 
the authors of the Federalist Papers. 
 
 Hamilton Madison Disputed 
while 36 0 0 
whilst 1 12 9 
 
Table 1  : Occurrence counts of ?while? and ?whilst? 
in the Federalist Papers (excluding documents au-
thored by Jay and those which were jointly authored). 
 
In the disputed papers, ?whilst? occurs in 6 of 
the documents (9 times total) and ?while? occurs 
in none. To properly anonymize the disputed 
documents, ?whilst? would need to be eliminated 
or normalized. 
This is similar to the problem with idiosyn-
cratic spelling in that there are two ways to apply 
this information. The first is to simply correct the 
term to conform to the norms as defined by the 
authors in K. The second approach is to incorpo-
rate characteristic forms associated with a par-
ticular author. While both approaches can serve 
to reduce the author?s stylometric fingerprint, the 
latter approach carries the risk of attempted style 
forgery and if applied indiscriminately may also 
provide clues that the document has been ano-
nymized (if strong characteristics of multiple 
authors can be detected). 
For our experiments, we opted to leave these 
markers in place to see how they were handled 
by the system. We did, however, need to normal-
ize the paragraph formatting, remove all capitali-
zation and convert all footnote references to use 
square brackets (which are otherwise unused in 
the corpus). 
3.3 Tokenization 
To tokenize the documents, we separated se-
quences of letters using spaces, newlines and the 
following punctuation marks: .,()-:;`'?![]. No 
stemming or morphological analysis was per-
formed. This process resulted in 8674 unique 
tokens for the 65 documents in the training set. 
4 Feature Selection 
The process of feature selection is one of the 
most crucial aspects of authorship attribution. By 
far the most common approach is to make use of 
the frequencies of common function words that 
are content neutral, but practitioners have also 
made use of other features such as letter metrics 
(e.g., bi-grams), word and sentence length met-
rics, word tags and parser rewrite rules. For this 
work, we opted to limit our study to word fre-
quencies since these features are generally ac-
knowledged to be effective for authorship attri-
bution and are transparent, which allows the au-
thor to easily incorporate the information for 
document modification purposes. 
We wanted to avoid depending on an initial 
list of candidate features since there is no guaran-
tee that the attackers will limit themselves to any 
of the commonly used lists. Avoiding these lists 
makes this work more readily useful for non-
English texts (although morphology or stemming 
may be required). 
We desire two things from our feature selec-
tion process beyond the actual features. First, we 
need a ranking of the features so that the author 
can focus efforts on the most important features. 
The second requirement is that we need a thresh-
old value so that the author knows how much the 
feature frequency needs to be adjusted. 
To rank and threshold the features, we used 
decision trees (DTs) and made use of the readily 
available WinMine toolkit (Chickering 2002). 
DTs produced by WinMine for continuously val-
ued features such as frequencies are useful since 
each node in the tree provides the required 
threshold value. For term-ranking, we created a 
Decision Tree Root (DTR) ranking metric to or-
der the terms based on how discriminating they 
are. DTR Rank is computed by creating a series 
of DTs where we remove the root feature, i.e. the 
most discriminating feature, before creating the 
next DT. In this fashion we create a ranking 
based on the order in which the DT algorithm 
determined that the term was most discrimina-
tory. The DTR ranking algorithm is as follows: 
1) Start with a set of features 
2) Build DT and record root feature 
3) Remove root feature from list of features 
4) Repeat from step 2 
It is worth noting that the entire DT need not 
be calculated since only the root is of interest. 
The off-the-shelf DT toolkit could be replaced 
with a custom implementation1 that returned only 
the root (also known as a decision stump). Since 
                                                 
1
 Many DT learners are information-gain based, but 
the WinMine toolkit uses a Bayesian scoring criterion 
described in Chickering et al (1997) with normal-
Wishart parameter priors used for continuously val-
ued features. 
446
our work is exploratory, we did not pursue op-
timizations along these lines. 
For our first set of experiments, we applied 
DTR ranking starting with all of the features 
(8674 tokens from the training set) and repeated 
until the DT was unable to create a tree that per-
formed better than the baseline of p(Hamilton) = 
78.46%. In this fashion, we obtained an ordered 
list of 2477 terms, the top 10 of which are shown 
in Table 2, along with the threshold and bias. 
The threshold value is read directly from the DT 
root node and the bias (which indicates whether 
we desire the feature value to be above or below 
the threshold) is determined by selecting the 
branch of the DT which has the highest ratio of 
non-A to A documents.  
Initially, this list looks promising, especially 
since known discriminating words like ?upon? 
and ?whilst? are the top two ranked terms. How-
ever, when we applied the changes to our base-
line attribution model (described in detail in the 
Evaluation section), we discovered that while it 
performed well on some test documents, others 
were left relatively unscathed. This is shown in 
Figure 1 which graphs the confidence in assign-
ing the authorship to Madison for each disputed 
document as each feature is adjusted. We expect 
the confidence to start high on the left side and 
move downward as more features are adjusted. 
After adjusting all of the identified features, half 
of the documents were still assigned to Madison 
(i.e., confidence > 0.50). 
Choosing just the high-frequency terms was 
also problematic since most of them were not 
considered to be discriminating by DTR ranking 
(see Table 3). The lack of DTR rank not only 
means that these are poor discriminators, but it 
also means that we do not have a threshold value 
to drive the feature adjustment process. 
 
Token DTR Frequency Token DTR Frequency 
the 
, 
of 
to 
. 
and 
in 
a 
be 
that 
- 
595 
- 
39 
- 
185 
119 
515 
- 
- 
0.094227 
0.068937 
0.063379 
0.038404 
0.027977 
0.025408 
0.023838 
0.021446 
0.020139 
0.014823 
it 
is 
which 
as 
by 
; 
this 
would 
have 
or 
- 
- 
- 
- 
58 
57 
575 
477 
- 
- 
0.013404 
0.011873 
0.010933 
0.008811 
0.008614 
0.007773 
0.007701 
0.007149 
0.006873 
0.006459 
 
Table 3  : Top 20 terms sorted by frequency.  
 
We next combined the DTR and the term fre-
quency approaches by computing DTR one the 
set of features whose frequency exceeds a speci-
fied threshold for any one of the authors. Select-
ing a frequency of 0.001 produces a list of 35 
terms, the first 14 of which are shown in Table 4. 
 
Token Frequency Threshold ? 49 
upon 
on 
powers 
there 
to 
men 
; 
by 
less 
in 
at 
those 
and 
any 
0.002503 
0.004429 
0.001485 
0.002707 
0.038404 
0.001176 
0.007773 
0.008614 
0.001176 
0.023838 
0.002990 
0.002615 
0.025408 
0.002930 
> 0.003111 
< 0.004312 
< 0.002012 
< 0.002911 
> 0.039071 
> 0.001531 
< 0.007644 
< 0.008110 
< 0.001384 
> 0.023574 
> 0.003083 
> 0.002742 
< 0.025207 
> 0.003005 
+6 
-9 
0 
+3 
+7 
+1 
0 
-2 
-1 
+6 
0 
+4 
-1 
+2 
 
Table 4  : Top 14 DTR(0.001) ranked items. The last 
column is the number of changes required to achieve 
the threshold frequency for document #49. 
 
Results for this list were much more promising 
and are shown in Figure 2. The confidence of 
attributing authorship to Madison is reduced by 
an average of 84.42% (? = 12.51%) and all of the 
documents are now correctly misclassified as 
being written by Hamilton. 
 
Token DTR Threshold Occurrence #49 
upon 
whilst 
on 
powers 
there 
few 
kind 
consequently 
wished 
although 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
> 0.003111 
< 0.000516 
< 0.004312 
< 0.002012 
> 0.002911 
< 0.000699 
> 0.001001 
< 0.000513 
> 0.000434 
< 0.000470 
0 ? 6 
1 ? 0 
16 ? 7 
2 ? 2 
2 ? 5 
1 ? 2 
0 ? 2 
1 ? 0 
1 ? 0 
0 ? 0 
 
Table 2  : Top 10 DTR Rank ordered terms with threshold 
and corresponding occurrence count (original document ? 
obfuscated version) for one of the disputed documents 
(#49). 
 
 
0.00
0.25
0.50
0.75
1.00
up
on
wh
ilst on
po
we
rs
the
re few kin
d
co
ns
eq
ue
ntl
y
wis
he
d
alt
ho
ug
h
 
 
Figure 1 : Confidence in assigning disputed papers to 
Madison graphed as each feature is adjusted. Each line cor-
responds to one of the 12 disputed documents. Features are 
ordered by DTR Rank and the attribution model is SVM30. 
Values above 0.5 are assigned to Madison and those below 
0.5 are assigned to Hamilton. 
447
0.00
0.25
0.50
0.75
1.00
up
on on
po
we
rs
the
re to
m
en by ; les
s in at
tho
se an
d
an
y
 
 
Figure 2 : Confidence in assigning disputed papers to 
Madison graphed as each feature is adjusted. Feature order 
is DTR(0.001) and the attribution model is SVM30. 
 
5 Evaluation 
Evaluating the effectiveness of any authorship 
obfuscation approach is made difficult by the 
fact that it is crucially dependent on the author-
ship detection method that is being utilized. An 
advantage of using the Federalist Papers as the 
test data set is that there are numerous papers 
documenting various methods that researchers 
have used to identify the authors of the disputed 
papers. 
However, because of differences in the exact 
data set2 and machine learning algorithm used, it 
is not reasonable to create an exact and complete 
implementation of each system. For our experi-
ments, we used only the standard Federalist Pa-
pers documents and tested each feature set using 
linear-kernel SVMs, which have been shown to 
be effective in text categorization (Joachims 
1998). To train our SVMs we used a sequential 
minimal optimization (SMO) implementation 
described in (Platt 1999). 
The SVM feature sets that we used for the 
evaluation are summarized in Table 5. 
For the early experiments described in the 
previous section we used SVM30, which incor-
porates the final set of 30 terms that Mosteller & 
Wallace used for their study. As noted earlier, 
they made use of a different data set than we did, 
so we did expect to see some differences in the 
results. The baseline model (plotted as the left-
most column of points in Figure 1 and Figure 2) 
assigned all of the disputed papers to Madison 
except one3. 
                                                 
2
 Mosteller & Wallace and some others augmented the 
Federalist Papers with additional document samples 
(5 Hamilton and 36 Madison), but this has not been 
done universally by all researchers. 
3
 Document #55. However, this is not inconsistent 
with Mosteller &Wallace?s results: ?Madison is ex-
tremely likely [?] to have written all the disputed 
SVM70 (Mosteller & Wallace 
1964) 
70 common function 
words.4 
SVM30 (Mosteller & Wallace 
1964) 
Final 30 terms.5 
SVM11 (Tweedie, Singh & 
Holmes 1996) 
on, upon, there, any, 
an, every, his, from, 
may, can, do 
SVM08 (Holmes & Forsyth 
1995) 
upon, both, on, there, 
whilst, kind, by, 
consequently 
SVM03 (Bosch & Smith 1998) upon, our, are 
 
Table 5  : Summary of feature words used in other Federal-
ist Papers studies. 
 
5.1 Feature Modification 
Rather than applying the suggested modifications 
to the original documents and regenerating the 
document feature vectors from scratch each time, 
we simplified the evaluation process by adjusting 
the feature vector directly and ignoring the im-
pact of the edits on the overall document prob-
abilities. The combination of insertions and dele-
tions results in the total number of words in the 
document being increased by an average of 19.58 
words (? = 7.79), which is less than 0.5% of the 
document size. We considered this value to be 
small enough that we could safely ignore its im-
pact. 
Modifying the feature vector directly also al-
lows us to consider each feature in isolation, 
without concern for how they might interact with 
each other (e.g. converting whilst?while or re-
writing an entire sentence). It also allows us to 
avoid the problem of introducing rewrites into 
the document with our distinctive stylometric 
signature instead of a hypothetical Madison re-
write. 
5.2 Experiments 
We built SVMs for each feature set listed in 
Table 5 and applied the obfuscation technique 
described above by adjusting the values in the 
feature vector by increments of the single-word 
probability for each document. The results that 
we obtained were the same as observed with our 
test model ? all of the models were coerced to 
prefer Hamilton for each of the disputed docu-
ments. 
 
                                                                          
Federalists [?] with the possible exception of No. 55. 
For No. 55 our evidence is relatively weak [?].? 
(Mosteller & Wallace 1964) p.263. 
4
 ibid p.38. 
5
 ibid p.66. 
448
0.00
0.25
0.50
0.75
1.00
up
on on
po
we
rs
the
re to
m
en by ; les
s in at
tho
se an
d
an
y
 
 
Figure 3 : Confidence in assigning disputed papers to 
Madison graphed as each feature is adjusted. Feature order 
is DTR(0.001) and the attribution model is SVM70. 
 
Figure 3 shows the graph for SVM70, the 
model that was most resilient to our obfuscation 
techniques. The results for all models are sum-
marized in Table 6. The overall reduction 
achieved across all models is 86.86%. 
 
 % Reduction ? 
SVM70 74.66% 12.97% 
SVM30 84.42% 12.51% 
SVM11 82.65% 10.99% 
SVM08 93.54% 4.44% 
SVM03 99.01% 0.74% 
 
Table 6  : Percent reduction in the confidence 
of assigning the disputed papers to Madison 
for each of the tested feature sets. 
 
Of particular note in the results are those for 
SVM03, which proved to be the most fragile 
model because of its low dimension. If we con-
sider this case an outlier and remove it from 
study, our overall reduction becomes 83.82%. 
5.3 Feature Changes 
As stated earlier, an important aspect of any ob-
fuscation approach is the number of changes re-
quired to effect the mis-attribution. Table 7 
summarizes the absolute number of changes 
(both insertions and deletions) and also expresses 
this value related to the original document size. 
The average number of changes required per 
1000 words in the document is 14.2. While it is 
difficult to evaluate how much effort would be 
required to make each of these individual 
changes, this value seems to be within the range 
that a motivated person could reasonably under-
take. 
More detailed summaries of the number of 
feature changes required for single document 
(#49) are given in Table 2 and Table 4. 
By calculating the overall number of changes 
required, we implicitly consider insertions and 
deletions to be equally weighted. However, while 
deletion sites in the document are easy to identify, 
 
Document Changes Doc Size Changes/1000 
49 42 3849 10.9 
50 46 2364 19.5 
51 67 4039 16.6 
52 52 3913 13.3 
53 62 4592 13.5 
54 53 4246 12.5 
55 52 4310 12.1 
56 59 3316 17.8 
57 60 4610 13.0 
58 54 4398 12.3 
62 78 5048 15.5 
63 91 6429 14.2 
 
Table 7  : Changes required per document 
 
proposing insertion sites can be more problem-
atic. We do not address this difference in this 
paper, although it is clear that more investigation 
is required in this area. 
6 Deep Obfuscation 
The techniques described above result in what 
we term shallow obfuscation since they focus on 
a small number of features and are only useful as 
a defense against standard attribution attacks. 
More advanced attribution techniques, such as 
that described in (Koppel and Schler 2004) look 
deeper into the author?s stylometric profile and 
can identify documents that have been obfus-
cated in this manner. 
Koppel and Schler introduce an approach they 
term ?unmasking? which involves training a se-
ries of SVM classifiers where the most strongly 
weighted features are removed after each itera-
tion. Their hypothesis is that two texts from dif-
ferent authors will result in a steady and rela-
tively slow decline of classification accuracy as 
features are being removed. In contrast, two texts 
from the same author will produce a relatively 
fast decline in accuracy. According to the authors, 
a slow decline indicates deep and fundamental 
stylistic differences in style - beyond the ?obvi-
ous? differences in the usage of a few frequent 
words. A fast decline indicates that there is an 
underlying similarity once the impact of a few 
superficial distinguishing markers has been re-
moved. 
We repeated their experiments using 3-fold 
cross-validation to compare Hamilton and Madi-
son with each other and the original (D) and ob-
fuscated (D?) documents. The small number of 
documents required that we train the SVM using 
the 50 most frequent words. Using a larger pool 
of feature words resulted in unstable models, es-
pecially when comparing Madison (14 docu-
ments) with D and D? (12 documents). The re-
sults of this comparison are shown in Figure 4. 
449
0.3000
0.4000
0.5000
0.6000
0.7000
0.8000
0.9000
1.0000
HvD
HvD'
HvM
MvD
MvD'
 
 
Figure 4 : Unmasking the obfuscated document. The y-axis 
plots the accuracy of a classifier trained to distinguish be-
tween two authors; the x-axis plots each iteration of the 
unmasking process. The top three lines compare Hamilton 
(H) versus Madison (M), the original document (D) and the 
obfuscated document (D?). The bottom line is M vs. D and 
the middle line is M vs. D?. 
 
In this graph, the comparison of Hamilton and 
the modified document (MvD?) exhibits the 
characteristic curve described by Koppel and 
Schler, which indicates that the original author 
can still be detected. However, the curve has 
been raised above the curve for the original 
document which suggests that our approach does 
help insulate against attacks that identify deep 
stylometric features. 
Modifying additional features continues this 
trend and raises the curve further. Figure 5 sum-
marizes this difference by plotting the difference 
between the accuracy of the HvD? and MvD? 
curves for documents at different levels of fea-
ture modification. An ideal curve in this graph 
would be one that hugged the x-axis since this 
would indicate that it was as difficult to train a 
classifier to distinguish between M and D? as it is 
to distinguish between H and D?. In this graph, 
the ?0? curve corresponds to the original docu-
ment, and the ?14? curve to the modified docu-
ment shown in Figure 4. The ?35? curve uses all 
of the DTR(0.001) features. 
This graph demonstrates that using DTR rank-
ing to drive feature adjustment can produce 
documents that are increasingly harder to detect 
as being written by the author. While it is unsur-
prising that a deep level of obfuscation is not 
achieved when only a minimal number of fea-
tures are modified, this graph can be used to 
measure progress so that the author can deter-
mine enough features have been modified to 
achieve the desired level of anonymization. 
Equally unsurprising is that this increased ano-
nymization comes at an additional cost, summa-
rized in Table 8. 
 
Num Features Changes/1000 
7 9.9 
14 14.2 
21 18.3 
28 22.5 
35 25.1 
 
Table 8  : Relationship between number 
of features modified and corresponding 
changes required per 1000 words. 
 
While in this work we limited ourselves to the 
35 DTR(0.001) features, further document modi-
fication can be driven by lowering the DTR 
probability threshold to identify additional terms 
in an orderly fashion. 
7 Conclusion 
In this paper, we have shown that the standard 
approaches to authorship attribution can be con-
founded by directing the author to selectively 
edit the test document. We have proposed a tech-
nique to automatically identify distinctive fea-
tures and their frequency thresholds. By using a 
list of features that are both frequent and highly 
ranked according to this automatic technique, the 
amount of effort required to achieve reasonable 
authorship obfuscation seems to be well within 
the realm of a motivated author. While we make 
no claim that this is an easy task, and we make 
the assumption that the author has undertaken 
basic preventative measures (like spellchecking 
and grammar checking), it does not seem to be 
an onerous task for a motivated individual. 
It not surprising that we can change the out-
come by adjusting the values of features used in 
authorship detection. Our contribution, however, 
is that many of the important features can be de-
termined by simultaneously considering term-
frequency and DTR rank, and that this process 
results in a set of features and threshold values 
that are transparent and easy to control. 
 
-0.1000
0.0000
0.1000
0.2000
0.3000
0.4000
0.5000
0
14
35
 
 
Figure 5 : Overall impact of feature modification for dif-
ferent levels of obfuscation. The y-axis plots the accuracy 
delta between the HvD' and MvD' curves; the x-axis plots 
each iteration of the unmasking process. The legend indi-
cates the number of features modified for each curve. 
450
Given this result, it is not unreasonable to ex-
pect that a tool could be created to provide feed-
back to an author who desires to publish a docu-
ment anonymously. A sophisticated paraphrase 
tool could theoretically use the function word 
change information to suggest rewrites that 
worked toward the desired term frequency in the 
document. 
For our experiments, we used a simplified 
model of the document rewrite process by evalu-
ating the impact of each term modification in 
isolation. However, modifying the document to 
increase or decrease the frequency of a term will 
necessarily impact the frequencies of other terms 
and thus affect the document's stylometric signa-
ture. Further experimentation is clearly needed in 
this area needs to address the impact of this in-
terdependency. 
One limitation to this approach is that it ap-
plies primarily to authors that have a reasonably-
sized corpus readily available (or easily created). 
However, for situations where a large corpus is 
not available, automated authorship attribution 
techniques are likely to be less effective (and 
thus obfuscation is less necessary) since the 
number of possible features can easily exceed the 
number of available documents. An interesting 
experiment would be to explore how this ap-
proach applies to different types of corpora like 
email messages. 
We also recognize that these techniques could 
be used to attempt to imitate another author?s 
style. We do not address this issue other than to 
say that our thresholding approach is intended to 
push feature values just barely across the thresh-
old away from A rather than to mimic any one 
particular author. 
Finally, in these results, there is a message for 
those involved in authorship attribution: simple 
SVMs and low-dimensional models (like 
SVM03) may appear to work well, but are far 
less resilient to obfuscation attempts than Koppel 
and Schler?s unmasking approach. Creating clas-
sifiers with the minimum number of features 
produces a model that is brittle and more suscep-
tible to even simplistic obfuscation attempts. 
8 Acknowledgements 
Thanks are in order to the reviewers of earlier 
drafts of this document, notably Chris Brockett 
and our anonymous reviewers. In addition, Max 
Chickering provided useful information regard-
ing his implementation of DTs in the WinMine 
toolkit. 
References 
R. A. Bosch and J. A. Smith. 1998. Separating Hy-
perplanes and the Authorship of the Federalist Pa-
pers. American Mathematical Monthly, Vol. 105 
#7 pp. 601-608. 
D. M. Chickering, D. Heckerman and C. Meek. 1997. 
A Bayesian Approach to Learning Bayesian Net-
works with Local Structure. In Proceedings of the 
Thirteenth Conference on Uncertainty in Artificial 
Intelligence (UAI97 Providence, RI), pp. 80-89. 
D. M. Chickering. 2002. The WinMine Toolkit. 
Technical Report MSR-TR-2002-103. 
D. I. Holmes and R. S. Forsyth. 1995. The Federalist 
Revisited: New Directions in Authorship Attribu-
tion. Literary and Linguistic Computing 10(2), 
pp.111-127. 
D. I. Holmes. 1998. The Evolution of Stylometry in 
Humanities Scholarship. Literary and Linguistic 
Computing 13(3), pp.111-117. 
T. Joachims. 1998. Text Categorization with Support 
Vector Machines: Learning with many Relevant 
Features. In Proceedings of the 10th European 
Conference on Machine Learning, pp.137-142. 
M. Koppel and J. Schler. 2003. Exploiting Stylistic 
Idiosyncrasies for Authorship Attribution. In Pro-
ceedings of IJCAI'03 Workshop on Computational 
Approaches to Style Analysis and Synthesis (Aca-
pulco, Mexico). pp.69-72. 
M. Koppel and J. Schler, 2004. Authorship Verifica-
tion as a One-Class Classification Problem. In Pro-
ceedings of the Twenty-First International Confer-
ence on Machine Learning (ICML 04 Banff, Al-
berta, Canada), pp.489-495. 
F. Mosteller and D. L. Wallace. 1964. Inference and 
Disputed Authorship: The Federalist. Addison-
Wesley (Reading, Massachusetts, USA). 
J. Platt. 1999. Fast Training of SVMs Using Sequen-
tial Minimal Optimization. In B. Sch?lkopf, C. 
Burges and A. Smola (eds.) Advances in Kernel 
Methods: Support Vector Learning. MIT Press 
(Cambridge, MA, USA), pp.185-208. 
J. R. Rao and P. Rohatgi. 2000. Can Pseudonymity 
Really Guarantee Privacy?, In Proceedings of the 
9th USENIX Security Symposium (Denver, Colo-
rado, USA), pp.85-96. 
F. J. Tweedie, S. Singh and D. I. Holmes. 1996. Neu-
ral Network Applications in Stylometry: The Fed-
eralist Papers. In Computers and the Humanities 
30(1), pp.1-10. 
 
451
