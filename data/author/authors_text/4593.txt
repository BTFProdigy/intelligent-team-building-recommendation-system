An Unsupervised Method for Detecting Grammatical Errors 
Martin Chodorow 
Hunter College of CUNY 
695 Park Avenue 
New York, NY 
martin.chodorow @.hunter.cuny.edu 
Claudia Leacock 
Educational Testing Service 
Rosedale Road 
Princeton, NJ 
cleacock@ets.org 
Abstract 
We present an unsupervised method for 
detecting rammatical errors by inferring 
negative evidence from edited textual 
corpora. The system was developed and 
tested using essay-length responses to 
prompts on the Test of English as a 
Foreign Language (TOEFL). The error- 
recognition system, ALEK, performs with 
about 80% precision and 20% recall. 
Introduction 
A good indicator of whether aperson knows the 
meaning of a word is the ability to use it 
appropriately in a sentence (Miller and Gildea, 
1987). Much information about usage can be 
obtained from quite a limited context: Choueka 
and Lusignan (1985) found that people can 
typically recognize the intended sense of a 
polysemous word by looking at a narrow 
window of one or two words around it. 
Statistically-based computer programs have 
been able to do the same with a high level of 
accuracy (Kilgarriff and Palmer, 2000). The goal 
of our work is to automatically identify 
inappropriate usage of specific vocabulary 
words in essays by looking at the local 
contextual cues around a target word. We have 
developed a statistical system, ALEK (Assessing 
Le____xical Knowledge), that uses statistical 
analysis for this purpose. 
A major objective of this research is to avoid the 
laborious and costly process of collecting errors 
(or negative vidence) for each word that we 
wish to evaluate. Instead, we train ALEK on a 
general corpus of English and on edited text 
containing example uses of the target word. The 
system identifies inappropriate usage based on 
differences between the word's local context 
cues in an essay and the models of context it has 
derived from the corpora of well-formed 
sentences. 
A requirement for ALEK has been that all steps 
in the process be automated, beyond choosing 
the words to be tested and assessing the results. 
Once a target word is chosen, preprocessing, 
building a model of the word's appropriate 
usage, and identifying usage errors in essays is 
performed without manual intervention. 
ALEK has been developed using the Test of 
English as a Foreign Language (TOEFL) 
administered bythe Educational Testing 
Service. TOEFL is taken by foreign students 
who are applying to US undergraduate and 
graduate-level programs. 
1 Background 
Approaches to detecting errors by non-native 
writers typically produce grammars that look for 
specific expected error types (Schneider and 
McCoy, 1998; Park, Palmer and Washburn, 
1997). Under this approach, essays written by 
ESL students are collected and examined for 
errors. Parsers are then adapted to identify those 
error types that were found in the essay 
collection. 
We take a different approach, initially viewing 
error detection as an extension of the word sense 
disambiguation (WSD) problem. Corpus-based 
WSD systems identify the intended sense of a 
polysemous word by (1) collecting a set of 
example sentences for each of its various enses 
and (2) extracting salient contextual cues from 
these sets to (3) build a statistical model for each 
sense. They identify the intended sense of a 
word in a novel sentence by extracting its 
contextual cues and selecting the most similar 
word sense model (e.g., Leacock, Chodorow and 
Miller (1998), Yarowsky (1993)). 
Golding (1995) showed how methods used for 
WSD (decision lists and Bayesian classifiers) 
could be adapted to detect errors resulting from 
140 
common spelling confusions among sets such as 
there, their, and they 're. He extracted contexts 
from correct usage of each confusable word in a 
training corpus and then identified a new 
occurrence as an error when it matched the 
wrong context. 
However, most grammatical errors are not the 
result of simple word confusions. This 
complicates the task of building a model of 
incorrect usage. One approach we considered 
was to proceed without such a model: represent 
appropriate word usage (across enses) in a 
single model and compare a novel example to 
that model. The most appealing part of this 
formulation was that we could bypass the 
knowledge acquisition bottleneck. All 
occurrences of the word in a collection of edited 
text could be automatically assigned to a single 
training set representing appropriate usage. 
Inappropriate usage would be signaled by 
contextual cues that do not occur in training. 
Unfortunately, this approach was not effective 
for error detection. An example of a word usage 
error is often very similar to the model of 
appropriate usage. An incorrect usage can 
contain two or three salient contextual elements 
as well as a single anomalous element. The 
problem of error detection does not entail 
finding similarities to appropriate usage, rather it 
requires identifying one element among the 
contextual cues that simply does not fit. 
2 ALEK Architecture 
What kinds of anomalous elements does ALEK 
identify? Writers sometimes produce rrors that 
violate basic principles of English syntax (e.g., a 
desks), while other mistakes how a lack of 
information about a specific vocabulary item 
(e.g., a knowledge). In order to detect hese two 
types of problems, ALEK uses a 30-million 
word general corpus of English from the San 
Jose Mercury News (hereafter referred to as the 
general corpus) and, for each target word, a set 
of 10,000 example sentences from North 
American newspaper text I (hereafter referred to 
as the word-specific corpus). 
i The corpora re extracted from the ACL-DCI 
corpora. In selecting the sentences for the word 
ALEK infers negative vidence from the 
contextual cues that do not co-occur with the 
target word - either in the word specific corpus 
or in the general English one. It uses two kinds 
of contextual cues in a +2 word window around 
the target word: function words (closed-class 
items) and part-of-speech tags (Brill, 1994). The 
Brill tagger output is post-processed to "enrich" 
some closed class categories of its tag set, such 
as subject versus object pronoun and definite 
versus indefinite determiner. The enriched tags 
were adapted from Francis and Ku~era (I 982). 
After the sentences have been preprocessed, 
ALEK counts sequences of adjacent part-of- 
speech tags and function words (such as 
determiners, prepositions, and conjunctions). For 
example, the sequence a/ATfull-time/JJjob/NN 
contributes one occurrence ach to the bigrams 
AT+J J, JJ+NN, a+JJ, and to the part-of-speech tag 
trigram AT+JJ+NN. Each individual tag and 
function word also contributes to its own 
unigram count. These frequencies form the basis 
for the error detection measures. 
From the general corpus, ALEK computes a 
mutual information measure to determine which 
sequences of part-of-speech tags and function 
words are unusually rare and are, therefore, 
likely to be ungrammatical in English (e.g., 
singular determiner preceding plural noun, as in 
*a desks). Mutual information has often been 
used to detect combinations of words that occur 
more frequently than we would expect based on 
the assumption that the words are independent. 
Here we use this measure for the opposite 
purpose - to find combinations that occur less 
often than expected. ALEK also looks for 
sequences that are common in general but 
unusual in the word specific corpus (e.g., the 
singular determiner a preceding a singular noun 
is common in English but rare when the noun is 
specific orpora, we tried to minimize the mismatch 
between the domains of newspapers and TOEFL 
essays. For example, in the newspaper domain, 
concentrate is usually used as a noun, as in orange 
juice concentrate but in TOEFL essays it is a verb 
91% of the time. Sentence selection for the word 
specific orpora was constrained to reflect he 
distribution of part-of-speech tags for the target word 
in a random sample of TOEFL essays. 
141 
knowledge). These divergences between the two 
corpora reflect syntactic properties that are 
peculiar to the target word. 
2.1 Measures based on the general 
corpus: 
The system computes mutual information 
comparing the proportion of observed 
occurrences ofbigrams in the general corpus to 
the proportion expected based on the assumption 
of independence, as shown below: 
P(A) ? P(B)) 
Here, P(AB) is the probability of the occurrence 
of the AB bigram, estimated from its frequency 
in the general corpus, and P(A) and P(B) are the 
probabilities of the first and second elements of 
the bigram, also estimated from the general 
corpus. Ungrammatical sequences should 
produce bigram probabilities that are much 
smaller than the product of the unigram 
probabilities (the value of MI will be negative). 
Trigram sequences are also used, but in this case 
the mutual information computation compares 
the co-occurrence of ABC to a model in which 
A and C are assumed to be conditionally 
independent given B (see Lin, 1998). 
M/= log 2 P( B) x P( A I B ) x P(C I B) 
Once again, a negative value is often indicative 
of a sequence that violates a rule of English. 
2.2 Comparing the word-specific orpus 
to the general corpus: 
ALEK also uses mutual information to compare 
the distributions of tags and function words in 
the word-specific corpus to the distributions that 
are expected based on the general corpus. The 
measures for bigrams and trigrams are similar to 
those given above except that the probability in 
the numerator isestimated from the word- 
specific orpus and the probabilities in the 
denominator come from the general corpus. To 
return to a previous example, the phrase a
knowledge contains the tag bigram for singular 
determiner followed by singular noun (AT Nil). 
This sequence ismuch less common in the 
word-specific corpus for knowledge than would 
be expected from the general corpus unigram 
probabilities of AT and NN. 
In addition to bigram and trigram measures, 
ALEK compares the target word's part-of- 
speech tag in the word-specific corpus and in the 
general corpus. Specifically, it looks at the 
conditional probability of the part-of-speech tag 
given the major syntactic ategory (e.g., plural 
noun given noun) in both distributions, by 
computing the following value. 
( P=p~c~c _ corm(taglcategory) I 
o g 2 / ~ ~  
t. p=o,e,o, _ co, =(tag I ) 
For example, in the general corpus, about half of 
all noun tokens are plural, but in the training set 
for the noun knowledge, the plural knowledges 
occurs rarely, if at all. 
The mutual information measures provide 
candidate errors, but this approach overgenerates 
- it finds rare, but still quite grammatical, 
sequences. To reduce the number of false 
positives, no candidate found by the MI 
measures i considered an error if it appears in 
the word-specific corpus at least wo times. This 
increases ALEK's precision at the price of 
reduced recall. For example, aknowledge will 
not be treated as an error because it appears in 
the training corpus as part of the longer a 
knowledge of sequence (as in a knowledge of 
mathematics). 
ALEK also uses another statistical technique for 
finding rare and possibly ungrammatical t g and 
function word bigrams by computing the %2 (chi 
square) statistic for the difference between the 
bigram proportions found in the word-specific 
and in the general corpus: 
~ Pgeneral_corpu~ i -egerneral_corpus ) / Nword specific 
The %2 measure faces the same problem of 
overgenerating errors. Due to the large sample 
sizes, extreme values can be obtained even 
though effect size may be minuscule. To reduce 
false positives, ALEK requires that effect sizes 
be at least in the moderate-to-small r nge 
(Cohen and Cohen, 1983). 
142 
Direct evidence from the word specific corpus 
can also be used to control the overgeneration f 
errors. For each candidate rror, ALEK 
compares the larger context in which the bigram 
appears to the contexts that have been analyzed 
in the word-specific corpus. From the word- 
specific corpus, ALEK forms templates, 
sequences ofwords and tags that represent the 
local context of the target. If a test sentence 
contains a low probability bigram (as measured 
by the X2 test), the local context of the target is 
compared to all the templates of which it is a 
part. Exceptions to the error, that is longer 
grammatical sequences that contain rare sub- 
sequences, are found by examining conditional 
probabilities. To illustrate this, consider the 
example of a knowledge and a knowledge of. 
The conditional probability of of  given a 
knowledge is high, as it accounts for almost all 
of the occurrences ofa knowledge in the word- 
specific corpus. Based on this high conditional 
probability, the system will use the template for 
a knowledge of to keep it from being marked as 
an error. Other function words and tags in the +1 
position have much lower conditional 
probability, so for example, a knowledge iswill 
not be treated as an exception to the error. 
2.3 Validity of  the n-gram measures 
TOEFL essays are graded on a 6 point scale, 
where 6 demonstrates "clear competence" in 
writing on rhetorical and syntactic levels and 1 
demonstrates "incompetence in writing". If low 
probability n-grams ignal grammatical errors, 
then we would expect TOEFL essays that 
received lower scores to have more of these n- 
grams. To test this prediction, we randomly 
selected from the TOEFL pool 50 essays for 
each of the 6 score values from 1.0 to 6.0. For 
Score 
1.0 
% of bigrams 
3.6 
% O f trigrams 
1.4 
2.0 3.4 0.8 
3.0 2.6 0.6 
4.0 1.9 0.3 
5.0 1.3 0.4 
6.0 1.5 0.3 
Table 1: Percent of n-grams with mutual 
information <-3.60, by score point 
each score value, all 50 essays were 
concatenated to form a super-essay. In every 
super-essay, for each adjacent pair and triple of 
tags containing a noun, verb, or adjective, the 
bigram and trigram mutual information values 
were computed based on the general corpus. 
Table 1 shows the proportions ofbigrams and 
trigrams with mutual information less than 
-3.60. As predicted, there is a significant 
negative correlation between the score and the 
proportion of low probability bigrams (rs = -.94, 
n=6, p<.01, two-tailed) and trigrams (r~= -.84, 
n=6, p<.05, two-tailed). 
2.4 System development 
ALEK was developed using three target words 
that were extracted from TOEFL essays: 
concentrate, interest, and knowledge. These 
words were chosen because they represent 
different parts of speech and varying degrees of 
polysemy. Each also occurred in at least 150 
sentences inwhat was then a small pool of 
TOEFL essays. Before development began, each 
occurrence of these words was manually labeled 
as an appropriate or inappropriate usage - 
without aking into account grammatical errors 
that might have been present elsewhere in the 
sentence but which were not within the target 
word's scope. 
Critical values for the statistical measures were 
set during this development phase. The settings 
were based empirically on ALEK's performance 
so as to optimize precision and recall on the 
three development words. Candidate rrors were 
those local context sequences that produced a 
mutual information value of less than -3.60 
based on the general corpus; mutual information 
of less than -5.00 for the specific/general 
comparisons; ora X2 value greater than 12.82 
with an effect size greater than 0.30. Precision 
and recall for the three words are shown below. 
Target word n Precision Recall 
Concentrate 169 .875 .280 
Interest 416 .840 .330 
Knowledge 761 .918 .570 
Table 2: Development Words 
143 
Test Word Precision Recall Total Recall Test Word Precision Recall Total Recall 
(estimated) (estimated) 
Affect .848 .762 .343 .768 .666 .104 
Area 
Aspect 
Benefit 
.752 
.792 
.744 
.846 
.717 
.709 
.205 
.217 
.276 
Energy 
Function 
Individual 
Job 
.800 
.576 
.714 
.742 
.168 
.302 
.728 .679 .103 
Career .736 .671 .110 Period .832 .670 .102 
Communicate .784 .867 .274 Pollution .912 .780 .310 
Concentrate .848 .791 .415 Positive .784 .700 .091 
Conclusion .944 .756 .119 Role ' .728 .674 .098 
Culture .704 .656 .083 Stress .768 .578 .162 
.816 .728 
.779 
Economy .666 .674 
.716 
Technology ~
Mean 
.235 .093 
.190 
Table 3: Precision and recall for 20 test words 
3 Experimental Design and Results 
ALEK was tested on 20 words. These words 
were randomly selected from those which met 
two criteria: (1) They appear in a university 
word list ('Nation, 1990) as words that a student 
in a US university will be expected to encounter 
and (2) there were at least 1,000 sentences 
containing the word in the TOEFL essay pool. 
To build the usage model for each target word, 
10,000 sentences containing it were extracted 
from the North American News Corpus. 
Preprocessing included etecting sentence 
boundaries and part-of-speech tagging. As in the 
development system, the model of general 
English was based on bigram and trigram 
frequencies of function words and part-of- 
speech tags from 30-million words of the San 
Jose Mercury News. 
For each test word, all of the test sentences were 
marked by ALEK as either containing an error 
or not containing an error. The size of the test set 
for each word ranged from 1,400 to 20,000 with 
a mean of 8,000 sentences. 
3.1 Results 
To evaluate the system, for each test word we 
randomly extracted 125 sentences that ALEK 
classified as containing no error (C-set) and 125 
sentences which it labeled as containing an error 
(E-set). These 250 sentences were presented to
a linguist in a random order for blind evaluation. 
The linguist, who had no part in ALEK's 
development, marked each usage of  the target 
word as incorrect or correct and in the case of 
incorrect usage indicated how far from the target 
one would have to look in order to recognise that 
there was an error. For example, in the case of 
"an period" the error occurs at a distance of one 
word from period. When the error is an 
omission, as in "lived in Victorian period", the 
distance is where the missing word should have 
appeared. In this case, the missing determiner is 
2 positions away from the target. When more 
than one error occurred, the distance of the one 
closest o the target was marked. 
Table 3 lists the precision and recall for the 20 
test words. The column labelled "Recall" is the 
proportion of human-judged rrors in the 250- 
sentence sample that were detected by ALEK. 
"Total Recall" is an estimate that extrapolates 
from the human judgements of the sample to the 
entire test set. We illustrate this with the results 
for pollution. The human judge marked as 
incorrect usage 91.2% of the sample from 
ALEK's E-set and 18.4% of the sample from its 
C-set. To estimate overall incorrect usage, we 
computed a weighted mean of these two rates, 
where the weights reflected the proportion of 
sentences that were in the E-set and C-set. The 
E-set contained 8.3% of the pollution sentences 
and the C-set had the remaining 91.7%. With the 
human judgements as the gold standard, the 
estimated overall rate of incorrect usage is (.083 
x .912 + .917 x .184) = .245. ALEK's estimated 
recall is the proportion of sentences in the E-set 
times its precision, divided by the overall 
estimated error rate (.083 ? .912) / .245 = .310. 
144 
The precision results vary from word to word. 
Conclusion and pollution have precision in the 
low to middle 90's while individual's precision 
is 57%. Overall, ALEK's predictions are about 
78% accurate. The recall is limited in part by the 
fact that the system only looks at syntactic 
information, while many of the errors are 
semantic. 
3.2 Analysis of  Hits and Misses 
Nicholls (1999) identifies four error types: an 
unnecessary word (*affect o their emotions), a 
missing word (*opportunity of job.), a word or 
phrase that needs replacing (*every jobs), a word 
used in the wrong form (*pollutions). ALEK 
recognizes all of these types of errors. For closed 
class words, ALEK identified whether a word 
was missing, the wrong word was used (choice), 
and when an extra word was used. Open class 
words have a fourth error category, form, 
including inappropriate compounding and verb 
agreement. During the development stage, we 
found it useful to add additional error categories. 
Since TEOFL graders are not supposed to take 
punctuation i to account, punctuation errors 
were only marked when they caused the judge to 
"garden path" or initially misinterpret the 
sentence. Spelling was marked either when a 
function word was misspelled, causing part-of- 
speech tagging errors, or when the writer's 
intent was unclear. 
The distributions of categories for hits and 
misses, shown in Table 4, are not strikingly 
different. However, the hits are primarily 
syntactic in nature while the misses are both 
semantic (as in open-class:choice) and syntactic 
(as in closed-class:missing). 
ALEK is sensitive to open-class word 
confusions (affect vs effect) where the part of 
speech differs or where the target word is 
confused with another word (*ln this aspect,... 
instead ofln this respect, ...). In both cases, the 
system recognizes that the target is in the wrong 
syntactic environment. Misses can also be 
syntactic - when the target word is confused 
with another word but the syntactic environment 
fails to trigger an error. In addition, ALEK does 
not recognize semantic errors when the error 
involves the misuse of an open-class word in 
Category % Hits % Misses 
Closed-class - choice 22.5 15.5 
--extra 15.5 13.0 
-missing 
.Open-class - choice 
8.0 8.5 
12.0 19.0 
- extra .5 1.0 
- missing 15 
- form 
1.5 
28.0 28.5 
Punctuation 5.5 1.5 
1.5 
5.5 
Sentence fragment 
Spelling/typing error 
Word order .5 
2.0 
8.5 
1.0 
Table 4: Hits and misses based on a random sample 
of 200 hits and 200 misses 
combination with the target (for example, make 
in "*they make benefits"). 
Closed class words typically are either selected 
by or agree with a head word. So why are there 
so many misses, especially with prepositions? 
The problem is caused in part by polysemy - 
when one sense of the word selects apreposition 
that another sense does not. When concentrate is
used spatially, it selects the preposition i , as 
"the stores were concentrated in the downtown 
area". When it denotes mental activity, it selects 
the preposition on, as in "Susan concentrated on
her studies". Since ALEK trains on all senses of 
concentrate, it does not detect he error in 
"*Susan concentrated in her studies". Another 
cause is that adjuncts, especially temporal and 
locative adverbials, distribute freely in the word- 
specific corpora, as in "Susan concentrated in 
her room." This second problem is more 
tractable than the polysemy problem - and 
would involve training the system to recognize 
certain types of adjuncts. 
3.3 Analysis of  False Positives 
False positives, when ALEK "identifies" an 
error where none exists, fall into six major 
categories. The percentage of each false positive 
type in a random sample of 200 false positives is 
shown in Table 5. 
Domain mismatch: Mismatch of the 
newspaper-domain word-specific corpora nd 
essay-domain test corpus. One notable 
difference is that some TOEFL essay prompts 
call for the writer's opinion. Consequently, 
145 
Error Type % Occurrence 
Domain mismatch 12.5 
17.0 Tagger 
Syntactic 
Free distribution 
14.5 
16.5 
Punctuation 12.0 
Infrequent tags 
Other 
9.0 
18.5 
Table 5. Distribution of false positive types 
TOEFL essays often contain first person 
references, whereas newspaper a ticles are 
written in the third person. We need to 
supplement the word-specific corpora with 
material that more closely resembles the test 
corpus. 
Tagger: Incorrect analysis by the part-of-speech 
tagger. When the part-of-speech tag is wrong, 
ALEK often recognizes the resulting n-gram as 
anomalous. Many of these errors are caused by 
training on the Brown corpus instead of a corpus 
of essays. 
Syntactic analysis: Errors resulting from using 
part-of-speech tags instead of supertags or a full 
parse, which would give syntactic relations 
between constituents. For example, ALEK false 
alarms on arguments of ditransitive verbs such 
as offer and flags as an error "you benefits" in 
"offers you benefits". 
Free distribution: Elements that distribute 
freely, such as adverbs and conjunctions, as well 
as temporal and locative adverbial phrases, tend 
to be identified as errors when they occur in 
some positions. 
Punctuation: Most notably omission of periods 
and commas. Since these errors are not 
indicative of one's ability to use the target word, 
they were not considered as errors unless they 
caused the judge to misanalyze the sentence. 
Infrequent tags. An undesirable r sult of our 
"enriched" tag set is that some tags, e.g., the 
post-determiner last, occur too infrequently in
the corpora to provide reliable statistics. 
Solutions to some of these problems will clearly 
be more tractable than to others. 
4 Comparison of Results 
Comparison of these results to those of other 
systems i  difficult because there is no generally 
accepted test set or performance baseline. Given 
this limitation, we compared ALEK's 
performance toa widely used grammar checker, 
the one incorporated in Microsoft's Word97. We 
created files of sentences used for the three 
development words concentrate, interest, and 
knowledge, and manually corrected any errors 
outside the local context around the target before 
checking them with Word97. The performance 
for concentrate showed overall precision of 0.89 
and recall of 0.07. For interest, precision was 
0.85 with recall of 0.11. In sentences containing 
knowledge, precision was 0.99 and recall was 
0.30. Word97 correctly detected the 
ungrammaticality ofknowledges as well as a 
knowledge, while it avoided flagging a
knowledge of. 
In summary, Word97's precision in error 
detection is impressive, but the lower recall 
values indicate that it is responding tofewer 
error types than does ALEK. In particular, 
Word97 is not sensitive to inappropriate 
selection of prepositions for these three words 
(e.g., *have knowledge on history, *to 
concentrate at science). Of course, Word97 
detects many kinds of errors that ALEK does 
not. 
Research as been reported on grammar 
checkers pecifically designed for an ESL 
population. These have been developed by hand, 
based on small training and test sets. Schneider 
and McCoy (1998) developed a system tailored 
to the error productions of American Sign 
Language signers. This system was tested on 79 
sentences containing determiner and agreement 
errors, and 101 grammatical sentences. We 
calculate that their precision was 78% with 54% 
recall. Park, Palmer and Washburn (1997) 
adapted a categorial grammar to recognize 
"classes of errors \[that\] dominate" in the nine 
essays they inspected. This system was tested on 
eight essays, but precision and recall figures are 
not reported. 
5 Conclusion 
The unsupervised techniques that we have 
presented for inferring negative vidence are 
effective in recognizing rammatical errors in 
written text. 
146 
Preliminary results indicate that ALEK's error 
detection is predictive of TOEFL scores. If 
ALEK accurately detects usage errors, then it 
should report more errors in essays with lower 
scores than in those with higher scores. We have 
already seen in Table 1 that there is a negative 
correlation between essay score and two of 
ALEK's component measures, the general 
corpus n-grams. However, the data in Table 1 
were not based on specific vocabulary items and 
do not reflect overall system performance, which 
includes the other measures as well. 
Table 6 shows the proportion of test word 
occurrences that were classified by ALEK as 
containing errors within two positions of the 
target at each of 6 TOEFL score points. As 
predicted, the correlation is negative (rs = -1.00, 
n = 6, p < .001, two-tailed). These data support 
the validity of the system as a detector of 
inappropriate usage, even when only a limited 
number of words are targeted and only the 
immediate context of each target is examined. 
Score 
1 
2 
3 
4 
5 
6 
ALEK Human 
.091 . . . . .  
.085 .375 
.067 .268 
.057 .293 
.048 .232 
.041 .164 
Table 6: Proportion of  test word occurrences, by 
score point, classified as containing an error by 
ALEK and by a human judge 
For comparison, Table 6 also gives the estimated 
proportions of inappropriate usage by score 
point based on the human judge's classification. 
Here, too, there is a negative correlation: rs = 
-.90, n = 5, p < .05, two-tailed. 
Although the system recognizes a wide range of 
error types, as Table 6 shows, it detects only 
about one-fifth as many errors as a human judge 
does. To improve recall, research needs to focus 
on the areas identified in section 3.2 and, to 
improve precision, efforts should be directed at 
reducing the false positives described in 3.3. 
ALEK is being developed as a diagnostic tool 
for students who are learning English as a 
foreign language. However, its techniques could 
be incorporated into a grammar checker for 
native speakers. 
Acknowledgments 
We thank Susanne Wolff for evaluating the test 
sentences, and Robert Kantor, Ken Sheppard and 3 
anonymous reviewers for their helpful suggestions. 
References 
Brill, E. 1994. Some advances in rule-based part-of- 
speech tagging. Proceedings of the Twelfth 
National Conference on Artificial Intelligence, 
Seattle, AAAI. 
Choueka, Y. and S. Lusignan. 1985. Disambiguation 
by short contexts. Computers and the Humanities, 
19:147-158. 
Cohen, J. and P. Cohen. 1983. Applied Multiple 
Regression~Correlation Analysis for the 
Behavioral Sciences. Hillsdale, N J: Erlbaum. 
Francis, W. and H. Ku~era. 1982. Frequency 
Analysis of English Usage: Lexicon and Grammar. 
Boston, Houghton Mifflin. 
Golding, A. 1995. A Bayesian hybrid for context- 
sensitive spelling correction. Proceedings of the 3 ~a 
Workshop on Very Large Corpora. Cambridge, 
MA. 39--53. 
Kilgarriff, A. and M. Palmer. 2000. Introduction to 
the special issue on SENSEVAL. Computers and 
the Humanities, 34:1----2. 
Leacock, C., M. Chodorow and G.A. Miller. 1998. 
1998. Using corpus tatistics and WordNet's 
lexical relations for sense identification. 
Computational Linguistics, 24:1. 
Lin, D. 1998. Extracting collocations from text 
corpora. First Workshop on Computational 
Terminology. Montreal, Canada. 
Miller, G.A. and P. Gildea. 1987. How children learn 
words. Scientific American, 257. 
Nation, I.S.P. 1990. Teaching and learning 
vocabulary. New York: Newbury House. 
Nicholls, D. 1999. The Cambridge Learner Corpus -
Error coding and analysis. Summer Workshop on 
Learner Corpora. Tokyo 
Park, J.C., M. Palmer and G. Washburn. 1997. 
Checking rammatical mistakes for English-as-a- 
second-language (ESL) students. Proceedings of 
KSEA-NERC. New Brunswick, NJ. 
Schneider, D.A. and K.F. McCoy. 1998. Recognizing 
syntactic errors in the writing of second language 
learners. Proceedings of Coling-ACL-98, Montr6al. 
Yarowsky, D. 1993. One sense per collocation. 
Proceedings of the ARPA Workshop on Human 
Language Technology. San Francisco. Morgan 
Kaufman. 
147 
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73?81,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
User Input and Interactions on Microsoft Research ESL Assistant 
 
 
Claudia Leacock Michael Gamon Chris Brockett 
Butler Hill Group Microsoft Research Microsoft Research 
P.O. Box 935 One Microsoft Way One Microsoft Way 
Ridgefield, CT, 06877, USA Redmond, WA, 98052, USA Redmond, WA, 98052, USA 
claudia.leacock@gmail.com mgamon@microsoft.com chrisbkt@microsoft.com 
 
 
 
 
 
 
Abstract 
ESL Assistant is a prototype web-based writ-
ing-assistance tool that is being developed for 
English Language Learners. The system fo-
cuses on types of errors that are typically 
made by non-native writers of American Eng-
lish. A freely-available prototype was dep-
loyed in June 2008. User data from this 
system are manually evaluated to identify 
writing domain and measure system accuracy. 
Combining the user log data with the eva-
luated rewrite suggestions enables us to de-
termine how effectively English language 
learners are using the system, across rule 
types and across writing domains. We find 
that repeat users typically make informed 
choices and can distinguish correct sugges-
tions from incorrect.  
1 Introduction 
Much current research in grammatical error detec-
tion and correction is focused on writing by Eng-
lish Language Learners (ELL). The Microsoft 
Research ESL Assistant is a web-based proofread-
ing tool designed primarily for ELLs who are na-
tive speakers of East-Asian languages. Initial 
system development was informed by pre-existing 
ELL error corpora, which were used both to identi-
fy common ELL mistakes and to evaluate system 
performance. These corpora, however, were 
created from data collected under arguably artifi-
cial classroom or examination conditions, leaving 
unresolved the more practical question as to 
whether the ESL Assistant can actually help a per-
son who produced the text to improve their English 
language writing skills in course of more realistic 
everyday writing tasks. 
In June of 2008, a prototype version of this sys-
tem was made freely available as a web service1
2 Related Work 
. 
Both the writing suggestions that visitors see and 
the actions that they then take are recorded. As 
these more realistic data begin to accumulate, we 
can now begin to answer the above question. 
Language learner error correction techniques  typi-
cally fall into either of two categories: rule-based 
or data-driven. Eeg-Olofsson and Knutsson (2003) 
report on a rule-based system that detects and cor-
rects preposition errors in non-native Swedish text. 
Rule-based approaches have also been used to pre-
dict definiteness and indefiniteness of Japanese 
noun phrases as a preprocessing step for Japanese 
to English machine translation (Murata and Nagao 
1993; Bond et al 1994; Heine, 1998), a task that is 
similar to the prediction of English articles. More 
recently, data-driven approaches have gained 
popularity and been applied to article prediction in 
English (Knight and Chander 1994; Minnen et al 
2000; Turner and Charniak 2007), to an array of 
Japanese learners? errors in English (Izumi et al 
2003), to verb errors (Lee and Seneff, 2008), and 
to article and preposition correction in texts written 
by non-native ELLs (Han et al 2004, 2006; Nagata 
et al 2005; Nagata et al 2006; De Felice and Pul-
man, 2007; Chodorow et al 2007; Gamon et al 
2008, 2009; Tetreault and Chodorow, 2008a). 
                                                          
1  http://www.eslassistant.com 
73
3 ESL Assistant 
ESL Assistant takes a hybrid approach that com-
bines statistical and rule-based techniques. Ma-
chine learning is used for those error types that are 
difficult to identify and resolve without taking into 
account complex contextual interactions, like ar-
ticle and preposition errors. Rule-based approaches 
handle those error types that are amenable to simp-
ler solutions. For example, a regular expression is 
sufficient for identifying when a modal is (incor-
rectly) followed by a tensed verb. 
The output of all modules, both machine-learned 
and rule-based, is filtered through a very large lan-
guage model. Only when the language model finds 
that the likelihood of the suggested rewrite is suffi-
ciently larger than the original text is a suggestion 
shown to the user. For a detailed description of 
ESL Assistant?s architecture, see Gamon et al
(2008, 2009). 
Although this and the systems cited in section 2 
are designed to be used by non-native writers, sys-
tem performance is typically reported in relation to 
native text ? the prediction of a preposition, for 
example, will ideally be consistent with usage in 
native, edited text. An error is counted each time 
the system predicts a token that differs from the 
observed usage and a correct prediction is counted 
each time the system predicts the usage that occurs 
in the text. Although somewhat artificial, this ap-
proach to evaluation offers the advantages of being 
fully automatable and having abundant quantities 
N
ou
n 
R
el
at
ed
 
(6
1%
) 
Articles (ML) 
We have just checked *the our stock. 
life is *journey/a journey, travel it well! 
I think it 's *a/the best way to resolve issues like this. 
Noun Number 
London is one of the most attractive *city/cities in the world. 
You have to write down all the details of each *things/thing to do. 
Conversion always takes a lot of *efforts/effort. 
Noun Of Noun 
Please send the *feedback of customer/customer feedback to me by 
mail. 
P
re
po
si
ti
on
 
R
el
at
ed
 
(2
7%
) Preposition (ML) 
I'm *on home today, call me if you have a problem.  
It seems ok and I did not pay much attention *on/to it. 
Below is my contact, looking forward *your/to your response, thanks! 
Verb and Preposition 
Ben is involved *this/in this transaction. 
I should *to ask/ask a rhetorical question ? 
But I?ll think *it/about it a second time. 
V
er
b 
R
el
at
ed
  
(1
0%
) 
Gerund / Infinitive 
(ML) 
He got me *roll/to roll up my sleeve and make a fist. 
On Saturday, I with my classmate went *eating/to eat. 
After *get/getting a visa, I want to study in New York. 
Auxiliary Verb (ML) 
To learn English we should *be speak/speak  it as much as possible . 
Hope you will *happy/be happy in Taiwan . 
what *is/do you want to say? 
Verb formation 
If yes, I will *attached/attach and resend to Geoff . 
The time and setting are *display/displayed at the same time. 
You had *order/ordered 3 items ? this time. 
I am really *hope/hoping to visit UCLA. 
Cognate/Verb Con-
fusion 
We cannot *image/imagine what the environment really is at the site 
of end user . 
Irregular Verbs I *teached/taught him all the things that I know ? 
A
dj
 
R
el
at
ed
 
(2
%
) Adjective Confu-
sions 
She is very *interesting/interested in the problem. 
So *Korea/Korean Government is intensively fostering trade . 
? and it is *much/much more reliable than your Courier Service. 
Adjective order Employing the *Chinese ancient/ancient Chinese proverb, that is  ? 
Table 1: ESL Assistant grammatical error modules.  ML modules are machine learned. 
 
74
of edited data readily available. With respect to 
prepositions and articles, the ESL Assistant's clas-
sifiers achieve state-of-the-art performance when 
compared to results reported in the literature (Ga-
mon et al 2008), inasmuch as comparison is possi-
ble when the systems are evaluated on different 
samples of native text. For articles, the system had 
86.76% accuracy as compared to 86.74% reported 
by Turner and Charniak (2007), who have the most 
recently reported results. For the harder problem of 
prepositions, ESL Assistant?s accuracy is compara-
ble to those reported by Tetreault and Chodorow 
(2008a) and De Felice and Pulman (2007).  
3.1 Error Types 
The ELL grammatical errors that  ESL Assistant 
tries to correct were distilled from analysis of the 
most frequent errors made in Chinese and Japanese 
English language learner corpora (Gui and Yang, 
2001; Izumi et al 2004). The error types are shown 
in Table 1: modules identified with ML are ma-
chine-learned, while the remaining modules are 
rule-based. ESL Assistant does not attempt to iden-
tify those errors currently found by Microsoft 
Word?, such as subject/verb agreement.  
ESL Assistant further contains a component to 
help address lexical selection issues. Since this 
module is currently undergoing major revision, we 
will not report on the results here. 
3.2 System Development  
Whereas evaluation on native writing is essential 
for system development and enables us to compare 
ESL Assistant performance with that of other re-
ported results, it tells us little about how the system 
would perform when being used by its true target 
audience ? non-native speakers of English engaged 
in real-life writing tasks. In this context, perfor-
mance measurement inevitably entails manual 
evaluation, a process that is notoriously time con-
suming, costly and potentially error-prone. Human 
inter-rater agreement is known to be problematic 
 
Figure 1: Screen shot of ESL Assistant 
 
75
on this task: it is likely to be high in the case of 
certain user error types, such as over-regularized 
verb inflection (where the system suggests replac-
ing ?writed? with ?wrote?), but other error types 
are difficult to evaluate, and much may hinge upon 
who is performing the evaluation: Tetreault and 
Chodorow (2008b) report that for the annotation of 
preposition errors ?using a single rater as a gold 
standard, there is the potential to over- or under-
estimate precision by as much as 10%.?  
With these caveats in mind, we employed a sin-
gle annotator to evaluate system performance on 
native data from the 1-million-word Chinese 
Learner?s of English corpus (Gui and Yang, 2001; 
2003). Half of the corpus was utilized to inform 
system development, while the remaining half was 
held back for "unseen" evaluation. While the abso-
lute numbers for some modules are more reliable 
than for others, the relative change in numbers 
across evaluations has proven a beneficial 
yardstick of improved or degraded performance in 
the course of development.  
3.3 The User Interface and Data Collection 
Figure 1 shows the ESL Assistant user interface. 
When a visitor to the site types or pastes text into 
the box provided and clicks the "Check" button, 
the text is sent to a server for analysis. Any loca-
tions in the text that trigger an error flag are then 
displayed as underscored with a wavy line (known 
as a "squiggle"). If the user hovers the mouse over 
a squiggle, one or more suggested rewrites are dis-
played in a dropdown list. Then, if the user hovers 
over one of these suggestions, the system launches 
parallel web searches for both original and rewrite 
phrases in order to allow the user to compare real-
word examples found on the World Wide Web. To 
accept a suggestion, the user clicks on the sug-
gested rewrite, and the text is emended. Each of 
these actions, by both system and user, are logged 
on the server. 
Since being launched in June, 2008, ESL Assis-
tant has been visited over 100,000 times. Current-
ly, the web page is being viewed between one to 
two thousand times every day. From these numbers 
alone it seems safe to conclude that there is much 
public interest in an ESL proofreading tool. 
Fifty-three percent of visitors to the ESL Assis-
tant web site are from countries in East Asia ? its 
primary target audience ? and an additional 15% 
are from the United States. Brazil, Canada, Ger-
many, and the United Kingdom each account for 
about 2% of the site?s visitors. Other countries 
represented in the database each account for 1% or 
less of all those who visit the site.  
3.4 Database of User Input 
User data are collected so that system performance 
can be evaluated on actual user input ? as opposed 
to running pre-existing learner corpora through the 
system. User data provide invaluable insight into 
which rewrite suggestions users spend time view-
ing, and what action they subsequently take on the 
basis of those suggestions.  
These data must be screened, since not all of the 
textual material entered by users in the web site is 
valid learner English language data. As with any 
publicly deployed web service, we find that nu-
merous users will play with the system, entering 
nonsense strings or copying text from elsewhere on 
the website and pasting it into the text box.  
To filter out the more obvious non-English data, 
we eliminate input that contains, for example, no 
alphabetic characters, no vowels/consonants in a 
sentence, or no white space. ?Sentences? consist-
ing of email subject lines are also removed, as are 
all the data entered by the ESL Assistant develop-
ers themselves. Since people often enter the same 
sentence many times within a session, we also re-
move repetitions of identical sentences within a 
single session.  
Approximately 90% of the people who have vi-
sited the web site visit it once and never return. 
This behavior is far from unusual on the web, 
where site visits may have no clear purpose beyond 
idle curiosity. In addition, some proportion of visi-
tors may in reality be automated "bots" that can be 
nearly indistinguishable from human visitors. 
Nevertheless, we observe a significant number 
of repeat visitors who return several times to use 
the system to proofread email or other writing, and 
these are the users that we are intrinsically interest-
ed in. To measure performance, we therefore de-
cided to evaluate on data collected from users who 
logged on and entered plausibly English-like text 
on at least four occasions. As of 2/10/2009, the 
frequent user database contained 39,944 session-
unique sentences from 578 frequent users in 5,305 
sessions.  
76
Data from these users were manually annotated 
to identify writing domains as shown in Table 2. 
Fifty-three percent of the data consists of people 
proofreading email.2
 
 The dominance of email data 
is presumably due to an Outlook plug-in that is 
available on the web site, and automates copying 
email content into the tool. The non-technical do-
main consists of student essays, material posted on 
a personal web site, or employees writing about 
their company ? for example, its history or 
processes. The technical writing is largely confe-
rence papers or dissertations in the fields of, for 
example, medicine and computer science. The 
?other? category includes lists and resumes (a writ-
ing style that deliberately omits articles and gram-
matical subjects), as well as text copied from 
online newspapers or other media and pasted in. 
Writing Domain Percent 
Email 53% 
Non-technical / essays 24% 
Technical / scientific 14% 
Other (lists, resumes, etc) 4% 
Unrelated sentences 5% 
Table 2: Writing domains of frequent users 
 
Sessions categorized as ?unrelated sentences? typi-
cally consist of a series of short, unrelated sen-
tences that each contain one or more errors. These 
users are testing the system to see what it does. 
While this is a legitimate use of any grammar 
checker, the user is unlikely to be proofreading his 
or her writing, so these data are excluded from 
evaluation.  
4 System Evaluation & User Interactions 
We are manually evaluating the rewrite sugges-
tions that ESL Assistant generated in order to de-
termine both system accuracy and whether user 
acceptances led to an improvement in their writing.  
These categories are shown in Table 3. Note that 
results reported for non-native text look very dif-
ferent from those reported for native text (dis-
cussed in Section 3) because of the neutral 
categories which do not appear in the evaluation of 
native text. Systems reporting 87% accuracy on 
native text cannot achieve anything near that on 
                                                          
2 These are anonymized to protect user privacy. 
non-native ELL text because almost one third of 
the flags fall into a neutral category. 
In 51% of the 39,944 frequent user sentences, 
the system generated at least one grammatical error 
flag, for a total of 17,832 flags. Thirteen percent of 
the time, the user ignored the flags. The remaining 
87% of the flags were inspected by the user, and of 
those, the user looked at the suggested rewrites 
without taking further action 31% of the time. For 
28% of the flags, the user hovered over a sugges-
tion to trigger a parallel web search but did not 
accept the proposed rewrite. Nevertheless, 41% of 
inspected rewrites were accepted, causing the orig-
inal string in the text to be revised. Overall, the 
users inspected about 15.5K suggested rewrites to 
accept about 6.4K. A significant number of users 
appear to be inspecting the suggested revisions and 
making deliberate choices to accept or not accept. 
The next question is: Are users making the right 
choices? To help answer this question, 34% of the 
user sessions have been manually evaluated for 
system accuracy ? a total of approximately 5.1K 
grammatical error flags. For each error category 
and for the three major writing domains, we: 
Evaluation Subcategory: Description 
Good 
Correct flag: The correction fixes a 
problem in the user input. 
Neutral 
Both Good: The suggestion is a legiti-
mate alternative to well-formed original 
input: I like working/to work. 
Misdiagnosis: the original input con-
tained an error but the suggested rewrite 
neither improves nor further degrades 
the input: If you have fail machine on 
hand. 
Both Wrong: An error type is correctly 
diagnosed but the suggested rewrite 
does not correct the problem: can you 
give me suggestion
Non-ascii: A non-ascii or text markup 
character is in the immediate context. 
. (suggests the in-
stead of a) 
Bad 
False Flag: The suggestion resulted in 
an error or would otherwise lead to a 
degradation over the original user input. 
Table 3: Evaluation categories 
 
77
1. Calculated system accuracy for all flags, 
regardless of user actions. 
2. Calculated system accuracy for only those 
rewrites that the user accepted 
3. Compared the ratio of good to bad flags. 
 
Results for the individual error categories are 
shown in Figure 2. Users consistently accept a 
greater proportion of good suggestions than they 
do bad ones across all error categories. This is 
most pronounced for the adjective-related modules, 
where the overall rate of good suggestions im-
proved 17.6% after the user made the decision to 
accept a  suggestion, while the system?s false posi-
tive rate dropped 14.1% after the decision. For the 
noun-related modules, the system?s most produc-
 
 Rewrite Suggestion Evaluation Accepted Suggestion 
Noun  
Related  
Modules 
 
3,017 suggestions 
   972 acceptances 
  
Preposition  
Related  
Modules 
 
1,465 suggestions 
   479 acceptances 
  
Verb  
Related  
Modules 
 
469 suggestions 
157 acceptances 
  
Adjective 
Related  
Modules 
 
125 suggestions 
  40 acceptances 
  
Figure 2: User interactions by module category 
good
56%
neut
28%
bad
16%
good
63%
neut
26%
bad
11%
good
37%
neut
39%
bad
24% good
45%
neut
42%
bad
13%
good
62%
neut
32%
bad
6%
good
72%
neut
25%
bad
3%
good
45%
neut
32%
bad
23%
good
63%
neut
28%
bad
9%
78
tive modules, the overall good flag rate increased 
by 7% while the false positive rate dropped 5%. 
All differences in false positive rates are statistical-
ly significant in Wilcoxon?s signed-ranks test.  
When all of the modules are evaluated across 
the three major writing domains, shown in figure 3, 
the same pattern of user discrimination between 
good and bad flags holds. This is most evident in 
the technical writing domain, where the overall 
rate of good suggestions improved 13.2% after 
accepting the suggestion and the false positive rate 
dropped 15.1% after the decision. It is least marked 
for the essay/nontechnical writing domain. Here 
the overall good flag rate increased by only .3% 
while the false positive rate dropped 1.6%. Again, 
all of the differences in false positive rates are sta-
tistically significant in Wilcoxon?s signed-ranks 
test. These findings are consistent with those for 
the machine learned articles and prepositions mod-
ules in the email domain (Chodorow et al under 
review).  
A probable explanation for the differences seen 
across the domains is that those users who are 
proofreading non-technical writing are, as a whole, 
less proficient in English than the users who are 
writing in the other domains. Users who are proof-
reading technical writing are typically writing a 
dissertation or paper in English and therefore tend 
to relatively fluent in the language. The email do-
main comprises people who are confident enough 
in their English language skills to communicate 
with colleagues and friends by email in English. 
With the essay/non-technical writers, it often is not 
clear who the intended audience is. If there is any 
indication of audience, it is often an instructor. Us-
ers in this domain appear to be the least English-
 Rewrite Suggestion Evaluation Accepted Suggestion 
Email  
Domain 
 
2,614 suggestions 
   772 acceptances 
  
Non-Technical 
Writing 
 Domain 
 
1,437 suggestions 
   684 acceptances 
  
Technical  
Writing 
Domain 
 
1,069 suggestions 
    205 acceptances 
  
Figure 3: User interactions by writing domain 
good
53%
neutral
32%
bad
15%
good
63%
neutral
28%
bad
9%
good
56%
neutral
32%
bad
12%
good
56%
neutral
34%
bad
10%
good
38%
neutral
28%
bad
34%
good
52%
neutral
29%
bad
19%
79
language proficient of the ESL Assistant users, so it 
is unsurprising that they are less effective in dis-
criminating between good and bad flags than their 
more proficient counterparts. Thus it appears that 
those users who are most in need of the system are 
being helped by it least ? an important direction for 
future work. 
Finally, we look at whether the neutral flags, 
which account for 29% of the total flags, have any 
effect. The two neutral categories highlighted in 
Table 3, flags that either misdiagnose the error or 
that diagnose it but do not correct it, account for 
74% of ESL Assistant?s neutral flags. Although 
these suggested rewrites do not improve the sen-
tence, they do highlight an environment that con-
tains an error. The question is: What is the effect of 
identifying an error when the rewrite doesn?t im-
prove the sentence?  
To estimate this, we searched for cases where 
ESL Assistant produced a neutral flag and, though 
the user did not accept the suggestion, a revised 
sentence that generated no flag was subsequently 
submitted for analysis. For example, one user en-
tered: ?This early morning  i got a from head office 
??. ESL Assistant suggested deleting from, which 
does not improve the sentence. Subsequently, in 
the same session, the user submitted, ?This early 
morning I heard from the head office ?? In this 
instance, the system correctly identified the loca-
tion of an error. Moreover, even though the sug-
gested rewrite was not a good solution, the 
information was sufficient to enable the user to fix 
the error on his or her own. 
Out of 1,349 sentences with neutral suggestions 
that were not accepted, we identified (using a 
fuzzy match) 215 cases where the user voluntarily 
modified the sentence so that it contained no flag, 
without accepting the suggestion. In 44% of these 
cases, the user had simply typed in the suggested 
correction instead of accepting it ? indicating that 
true acceptance rates might be higher than we orig-
inally estimated. Sixteen percent of the time, the 
sentence was revised but there remained an error 
that the system failed to detect. In the other 40% of 
cases, the voluntary revision improved the sen-
tence. It appears that merely pointing out the poss-
ible location of an error to the user is often 
sufficient to be helpful. 
5 Conclusion 
In conclusion, judging from the number of people 
who have visited the ESL Assistant web site, there 
is considerable interest in ESL proofreading tools 
and services. 
When using the tool to proofread text, users do 
not accept the proposed corrections blindly ? they 
are selective in their behavior. More importantly, 
they are making informed choices ? they can dis-
tinguish correct suggestions from incorrect ones. 
Sometimes identifying the location of an error, 
even when the solution offered is wrong, itself ap-
pears sufficient to cause the user to repair a prob-
lem on his or her own. Finally, the user 
interactions that we have recorded indicate that 
current state-of-the-art grammatical error correc-
tion technology has reached a point where it can be 
helpful to English language learners in real-world 
contexts. 
Acknowledgments 
We thank Bill Dolan, Lucy Vanderwende, Jianfeng 
Gao, Alexandre Klementiev and Dmitriy Belenko 
for their contributions to the ESL Assistant system. 
We are also grateful to the two reviewers of this 
paper who provided valuable feedback. 
References  
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. 
Countability and number in Japanese to English ma-
chine translation. In Proceedings of the 15th Confe-
rence on Computational Linguistics (pp. 32-38). 
Kyoto, Japan. 
Martin Chodorow, Michael Gamon, and Joel Tetreault. 
Under review. The utility of grammatical error detec-
tion systems for English language learners: Feedback 
and Assessment. 
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving pre-
positions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions (pp. 25-30). Pra-
gue, Czech Republic. 
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In 
Proceedings of the Fourth ACL-SIGSEM Workshop 
on Prepositions (pp. 45-50). Prague, Czech Republic. 
Jens Eeg-Olofsson and Ola Knutsson.  2003. Automatic 
grammar checking for second language learners ? the 
use of prepositions.  Proceedings of NoDaLiDa 2003. 
Reykjavik, Iceland. 
80
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitriy Belenko, 
and Lucy Vanderwende. 2008. Using contextual 
speller techniques and language modeling for ESL 
error correction. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language 
Processing (pp. 449-455). Hyderabad, India. 
Michael Gamon, Claudia Leacock, Chris Brockett, Wil-
liam B. Dolan,  Jianfeng Gao, Dmitriy Belenko, and 
Alexandre Klementiev. 2009. Using statistical tech-
niques and web search to correct ESL errors. To ap-
pear in CALICO Journal, Special Issue on Automatic 
Analysis of Learner Language. 
Shicun Gui and Huizhong Yan. 2001. Computer analy-
sis of Chinese learner English. Presentation at Hong 
Kong University of Science and Technolo-
gy.http://lc.ust.hk/~centre/conf2001/keynote/subsect4 
/yang.pdf. 
Shicun Gui and Huizhong Yang. (Eds.). 2003. Zhong-
guo Xuexizhe Yingyu Yuliaohu. (Chinese Learner 
English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. 
(In Chinese). 
Na-Rae Han, Martin Chodorow, and Claudia Leacock 
2004). Detecting errors in English article usage with 
a maximum entropy classifier trained on a large, di-
verse corpus. In Proceedings of the 4th Interna-
tional Conference on Language Resources and 
Evaluation. Lisbon, Portugal. 
Na-Rae Han, Martin Chodorow, and Claudia Leacock 
2006. Detecting errors in English article usage by 
non-native speakers. Natural Language Engineer-
ing, 12(2), 115-129. 
Julia E. Heine. 1998. Definiteness predictions for Japa-
nese noun phrases. In Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (pp. 519-525). Montreal, 
Canada. 
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi, and Hitoshi Isahara. 2003. Automatic 
error detection in the Japanese learners' English spo-
ken data. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics (pp. 
145-148). Sapporo, Japan. 
Kevin Knight and Ishwar Chander,. 1994. Automatic 
postediting of documents. In Proceedings of the 12th 
National Conference on Artificial Intelligence (pp. 
779-784). Seattle: WA. 
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the Human Language Technology Confe-
rence of the North American Chapter of the 
Association for Computational Linguistics (pp. 31-
36). Boston, MA. 
John Lee and Stephanie Seneff. 2008. Correcting mi-
suse of verb forms. In Proceedings of ACl-08/HLT  
(pp. 174-182). Columbus, OH. 
Guido Minnen, Francis Bond, and Anne Copestake. 
2000. Memory-based learning for article generation. 
In Proceedings of the Fourth Conference on Compu-
tational Natural Language Learning and of the 
Second Learning Language in Logic Workshop (pp. 
43-48). Lisbon, Portugal. 
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in 
Japanese sentences for machine translation into Eng-
lish. In Proceedings of the Fifth International Confe-
rence on Theoretical and Methodological Issues in 
Machine Translation (pp. 218-225). Kyoto, Japan. 
Ryo Nagata, Takahiro Wakana, Fumito Masui, Atsui 
Kawai, and Naoki Isu. 2005. Detecting article errors 
based on the mass count distinction. In R. Dale, W. 
Kam-Fie, J. Su and O.Y. Kwong (Eds.) Natural Lan-
guage Processing - IJCNLP 2005, Second Interna-
tional Joint Conference Proceedings (pp. 815-826). 
New York: Springer. 
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and 
Naoki Isu. 2006. A feedback-augmented method for 
detecting errors in the writing of learners of English. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(pp. 241-248). Sydney, Australia. 
Joel Tetreault and Martin Chodorow. 2008a. The ups 
and downs of preposition error detection in ESL. 
COLING. Manchester, UK. 
Joel Tetreault and Martin Chodorow. 2008b. Native 
judgments of non-native usage: Experiments in pre-
position error detection. In Proceedings of the Work-
shop on Human Judgments in Computational 
Linguistics, 22nd International Conference on Com-
putational Linguistics (pp 43-48). Manchester, UK. 
Jenine Turner and Eugene Charniak. 2007. Language 
modeling for determiner selection. In Human Lan-
guage Technologies 2007: The Conference of the 
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short 
Papers (pp. 177-180). Rochester, NY. 
 
 
81
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 8?10, Dublin, Ireland, August 23-29 2014.
Automated Grammatical Error Correction for Language Learners
Joel Tetreault
Yahoo Labs
111 W. 40th Street
New York, NY, 10018, USA
tetreaul@yahoo-inc.com
Claudia Leacock
McGraw-Hill Education CTB
22 Ryan Ranch Road
Monterey, CA, 93940
claudia.leacock@ctb.com
Tutorial Description
A fast growing area in Natural Language Processing is the use of automated tools for identifying and
correcting grammatical errors made by language learners. This growth, in part, has been fueled by
the needs of a large number of people in the world who are learning and using a second or foreign
language. For example, it is estimated that there are currently over one billion people who are non-native
writers of English. These numbers drive the demand for accurate tools that can help learners to write
and speak proficiently in another language. Such demand also makes this an exciting time for those in
the NLP community who are developing automated methods for grammatical error correction (GEC).
Our motivation for the COLING tutorial is to make others more aware of this field and its particular set
of challenges. For these reasons, we believe that the tutorial will potentially benefit a broad range of
conference attendees.
In general, there has been a surge in interest in using NLP to address educational needs, which in turn,
has spawned the recurring ACL/NAACL workshop ?Innovative Use of Natural Language Processing
for Building Educational Applications? that had its 9th edition at ACL 2014. The last three years, in
particular, have been pivotal for GEC. Papers on the topic have become more commonplace at main
conferences such as ACL, NAACL and EMNLP, as well as two editions of a Morgan Claypool Synthesis
Series book on the topic (Leacock et al., 2010; Leacock et al., 2014). In 2011 and 2012, the first shared
tasks in GEC (Dale and Kilgarriff, 2011; Dale et al., 2012) were created, and dozens of teams from all
over the world participated. This was followed by two successful CoNLL Shared Tasks on the topic in
2013 and 2014 (Ng et al., 2013; Ng et al., 2014).
While there have been many exciting developments in GEC over the last few years, there is still
considerable room for improvement as state-of-the-art performance in detecting and correcting several
important error types is still inadequate for real world applications. We hope to engage researchers from
other NLP fields to develop novel and effective approaches to these problems. Our tutorial is specifically
designed to:
? Introduce an NLP audience to the challenges that language learners face and thus the challenges of
designing NLP tools to assist in language acquisition
? Provide a history of GEC and the state-of-the-art approaches for different error types
? Show the need for multi-lingual error correction approaches and discuss novel methods for achiev-
ing this
? Discuss ways in which error correction techniques can have an impact on other NLP tasks
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
8
Outline
1. Introduction
2. Special Problems of Language Learners
? Errors made by English Language Learners (ELLs)
? Influence of L1
3. Heuristic and Data Driven Approaches to Error Correction
(a) Early heuristic rule-based methods
(b) Methods for detection and correction
(c) Types of training data
(d) Features
(e) Web-based methods
4. Annotation and Evaluation
(a) Annotation schemes
(b) Proposals for efficient annotation
(c) Evaluation Measures
(d) Crowdsourcing for annotation and evaluation
5. Current Trends in Error Correction
(a) Detection of ungrammatical sentences and Other error types
(b) Shared tasks
(c) Going beyond the classification methodology
(d) Error correction in other languages
6. Conclusions
Organizers
Joel Tetreault is a Senior Research Scientist at Yahoo Labs in New York City. His research focus is
Natural Language Processing with specific interests in anaphora, dialogue and discourse processing,
machine learning, and applying these techniques to the analysis of English language learning and au-
tomated essay scoring. Previously he was Principal Manager of the Core Natural Language group at
Nuance Communications, Inc. where he worked on the research and development of NLP tools and
components for the next generation of intelligent dialogue systems. Prior to Nuance, he worked at Ed-
ucational Testing Service for six years as a Managing Senior Research Scientist where he researched
automated methods for detecting grammatical errors by non-native speakers, plagiarism detection, and
content scoring. Tetreault received his B.A. in Computer Science from Harvard University (1998) and
his M.S. and Ph.D. in Computer Science from the University of Rochester (2004). He was also a post-
doctoral research scientist at the University of Pittsburgh?s Learning Research and Development Center
(2004-2007), where he worked on developing spoken dialogue tutoring systems. In addition he has co-
organized the Building Educational Application workshop series for 7 years, the CoNLL 2013 Shared
Task on Grammatical Error Correction, and is currently NAACL Treasurer.
Claudia Leacock is a Research Scientist at McGraw-Hill Education CTB who has been working on
using NLP in educational applications for 20 years focusing on automated scoring and grammatical er-
ror detection. She was previously a consultant for Microsoft Research where she collaborated on the
development of ESL Assistant: a web-based prototype tool for detecting and correcting grammatical er-
rors of English language learners. As a Distinguished Member of Technical Staff at Pearson Knowledge
9
Technologies, and previously as a Principal Development Scientist at Educational Testing Service, she
developed tools for automated assessment of short-response content-based questions and for grammati-
cal error detection. As a member of the WordNet group at Princeton University?s Cognitive Science Lab,
her research focused on word sense disambiguation. Dr. Leacock received a B.A. in English from NYU,
a Ph.D. in Linguistics from the City University of New York, Graduate Center and was a post-doctoral
fellow at IBM, T.J. Watson Research Center.
References
Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings
of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages
242?249, Nancy, France, September. Association for Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and determiner
error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applications
Using NLP, pages 54?62, Montr?eal, Canada, June. Association for Computational Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreaukt. 2010. Automated grammatical error
detection for language learners. Morgan & Claypool Publishers.
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreaukt. 2014. Automated grammatical error
detection for language learners, second edition. Morgan & Claypool Publishers.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, and Joel Tetreault. 2013. The conll-2013 shared task on grammatical
error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,
Sofia, Bulgaria, August. Association for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher
Bryant. 2014. The conll-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth
Conference on Computational Natural Language Learning: Shared Task, pages 1?14, Baltimore, Maryland,
June. Association for Computational Linguistics.
10
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 37?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Search right and thou shalt find ...
Using Web Queries for Learner Error Detection 
Michael Gamon Claudia Leacock 
Microsoft Research Butler Hill Group 
One Microsoft Way P.O. Box 935 
Redmond, WA 981052, USA Ridgefield, CT 06877, USA 
mgamon@microsoft.com Claudia.leacock@gmail.com 
 
 
Abstract
We investigate the use of web search queries 
for detecting errors in non-native writing. Dis-
tinguishing a correct sequence of words from 
a sequence with a learner error is a baseline 
task that any error detection and correction 
system needs to address. Using a large corpus 
of error-annotated learner data, we investigate 
whether web search result counts can be used 
to distinguish correct from incorrect usage. In 
this investigation, we compare a variety of 
query formulation strategies and a number of 
web resources, including two major search 
engine APIs and a large web-based n-gram 
corpus. 
1 Introduction 
Data-driven approaches to the detection and cor-
rection of non-native errors in English have been 
researched actively in the past several years. Such 
errors are particularly amenable to data-driven me-
thods because many prominent learner writing er-
rors involve a relatively small class of phenomena 
that can be targeted with specific models, in par-
ticular article and preposition errors. Preposition 
and determiner errors (most of which are article 
errors) are the second and third most frequent er-
rors in the Cambridge Learner Corpus (after the 
more intractable problem of content word choice). 
By targeting the ten most frequent prepositions 
involved in learner errors, more than 80% of pre-
position errors in the corpus are covered.  
Typically, data-driven approaches to learner er-
rors use a classifier trained on contextual informa-
tion such as tokens and part-of-speech tags within 
a window of the preposition/article (Gamon et al 
2008, 2010, DeFelice and Pulman 2007, 2008, Han 
et al 2006, Chodorow et al 2007, Tetreault and 
Chodorow 2008).  
Language models are another source of evidence 
that can be used in error detection. Using language 
models for this purpose is not a new approach, it 
goes back to at least Atwell (1987). Gamon et al 
(2008) and Gamon (2010) use a combination of 
classification and language modeling. Once lan-
guage modeling comes into play, the quantity of 
the training data comes to the forefront. It has been 
well-established that statistical models improve as 
the size of the training data increases (Banko and 
Brill 2001a, 2001b). This is particularly true for 
language models: other statistical models such as a 
classifier, for example, can be targeted towards a 
specific decision/classification, reducing the appe-
tite for data somewhat, while language models 
provide probabilities for any sequence of words - a 
task that requires immense training data resources 
if the language model is to consider increasingly 
sparse longer n-grams.  
Language models trained on data sources like 
the Gigaword corpus have become commonplace, 
but of course there is one corpus that dwarfs any 
other resource in size: the World Wide Web. This 
has drawn the interest of many researchers in natu-
ral language processing over the past decade. To 
mention just a few examples, Zhu and Rosenfeld 
(2001) combine trigram counts from the web with 
an existing language model where the estimates of 
the existing model are unreliable because of data 
sparseness. Keller and Lapata (2003) advocate the 
use of the web as a corpus to retrieve backoff 
probabilities for unseen bigrams. Lapata and Keller 
(2005) extend this method to a range of additional 
natural language processing tasks, but also caution 
that web counts have limitations and add noise. 
Kilgariff (2007) points out the shortcomings of 
37
accessing the web as a corpus through search que-
ries: (a) there is no lemmatization or part-of-speech 
tagging in search indices, so a linguistically mea-
ningful query can only be approximated, (b) search 
syntax, as implemented by search engine provid-
ers, is limited, (c) there is often a limit on the num-
ber of automatic queries that are allowed by search 
engines, (c) hit count estimates are estimates of 
retrieved pages, not of retrieved words. We would 
like to add to that list that hit count estimates on 
the web are just that -- estimates. They are com-
puted on the fly by proprietary algorithms, and ap-
parently the algorithms also access different slices 
of the web index, which causes a fluctuation over 
time, as Tetrault and Chodorow (2009) point out. 
In 2006, Google made its web-based 5gram lan-
guage model available through the Linguistic Data 
Consortium, which opens the possibility of using 
real n-gram statistics derived from the web direct-
ly, instead of using web search as a proxy. 
In this paper we explore the use of the web as a 
corpus for a very specific task: distinguishing be-
tween a learner error and its correction. This is ob-
viously not the same as the more ambitious 
question of whether a system can be built to detect 
and correct errors on the basis of web counts alone, 
and this is a distinction worth clarifying. Any sys-
tem that successfully detects and corrects an error 
will need to accomplish three tasks1: (1) find a part 
of the user input that contains an error (error de-
tection). (2) find one or multiple alternative 
string(s) for the alleged error (candidate genera-
tion) and (3) score the alternatives and the original 
to determine which alternative (if any) is a likely 
correction (error correction). Here, we are only 
concerned with the third task, specifically the 
comparison between the incorrect and the correct 
choice. This is an easily measured task, and is also 
a minimum requirement for any language model or 
language model approximation: if the model can-
not distinguish an error from a well-formed string, 
it will not be useful. 
                                                          
1 Note that these tasks need not be addressed by separate com-
ponents. A contextual classifier for preposition choice, for 
example, can generate a probability distribution over a set of 
prepositions (candidate generation). If the original preposition 
choice has lower probability than one or more other preposi-
tions, it is a potential error (error detection), and the preposi-
tions with higher probability will be potential corrections 
(error correction). 
We focus on two prominent learner errors in this 
study: preposition inclusion and choice and article 
inclusion and choice. These errors are among the 
most frequent learner errors (they comprise nearly 
one third of all errors in the learner corpus used in 
this study). 
In this study, we compare three web data 
sources: The public Bing API, Google API, and the 
Google 5-gram language model. We also pay close 
attention to strategies of query formulation. The 
questions we address are summarized as follows: 
Can web data be used to distinguish learner er-
rors from correct phrases? 
What is the better resource for web-data: the 
Bing API, the Google API, or the Google 5-
gram data? 
What is the best query formulation strategy 
when using web search results for this task? 
How much context should be included in the 
query? 
2 Related Work
Hermet et al (2008) use web search hit counts for 
preposition error detection and correction in 
French. They use a set of confusable prepositions 
to create a candidate set of alternative prepositional 
choices and generate queries for each of the candi-
dates and the original. The queries are produced 
using linguistic analysis to identify both a govern-
ing and a governed element as a minimum mea-
ningful context. On a small test set of 133 
sentences, they report accuracy of 69.9% using the 
Yahoo! search engine. 
Yi et al (2008) target article use and collocation 
errors with a similar approach. Their system first 
analyzes the input sentence using part-of-speech 
tagging and a chunk parser. Based on this analysis, 
potential error locations for determiners and verb-
noun collocation errors are identified. Query gen-
eration is performed at three levels of granularity: 
the sentence (or clause) level, chunk level and 
word level. Queries, in this approach, are not exact 
string searches but rather a set of strings combined 
with the chunk containing the potential error 
through a boolean operator. An example for a 
chunk level query for the sentence "I am learning 
economics at university" would be "[economics] 
AND [at university] AND [learning]". For article 
38
errors the hit count estimates (normalized for query 
length) are used directly. If the ratio of the norma-
lized hit count estimate for the alternative article 
choice to the normalized hit count estimate of the 
original choice exceeds a manually determined 
threshold, the alternative is suggested as a correc-
tion. For verb-noun collocations, the situation is 
more complex since the system does not automati-
cally generate possible alternative choices for 
noun/verb collocations. Instead, the snippets (doc-
ument summaries) that are returned by the initial 
web search are analyzed and potential alternative 
collocation candidates are identified. They then 
submit a second round of queries to determine 
whether the suggestions are more frequent than the 
original collocation. Results on a 400+ sentence 
corpus of learner writing show 62% precision and 
41% recall for determiners, and 30.7% recall and 
37.3% precision for verb-noun collocation errors. 
Tetreault and Chodorow (2009) make use of the 
web in a different way. Instead of using global web 
count estimates, they issue queries with a region-
specific restriction and compare statistics across 
regions. The idea behind this approach is that re-
gions that have a higher density of non-native 
speakers will show significantly higher frequency 
of erroneous productions than regions with a high-
er proportion of native speakers. For example, the 
verb-preposition combinations married to versus 
married with show very different counts in the UK 
versus France regions. The ratio of counts for mar-
ried to/married with in the UK is 3.28, whereas it 
is 1.18 in France. This indicates that there is signif-
icant over-use of married with among native 
French speakers, which serves as evidence that this 
verb-preposition combination is likely to be an er-
ror predominant for French learners of English. 
They test their approach on a list of known verb-
preposition errors. They also argue that, in a state-
of-the-art preposition error detection system, recall 
on the verb-preposition errors under investigation 
is still so low that systems can only benefit from 
increased sensitivity to the error patterns that are 
discoverable through the region web estimates. 
Bergsma et al(2009) are the closest to our work. 
They use the Google N-gram corpus to disambi-
guate usage of 34 prepositions in the New York 
Times portion of the Gigaword corpus. They use a 
sliding window of n-grams (n ranging from 2 to 5) 
across the preposition and collect counts for all 
resulting n-grams. They use two different methods 
to combine these counts. Their SuperLM model 
combines the counts as features in a linear SVM 
classifier, trained on a subset of the data. Their 
SumLM model is simpler, it sums all log counts 
across the n-grams. The preposition with the high-
est score is then predicted for the given context. 
Accuracy on the New York Times data in these ex-
periments reaches 75.4% for SuperLM and 73.7% 
for SumLM. 
Our approach differs from Bergsma et al in 
three crucial respects. First, we evaluate insertion, 
deletion, and substitution operations, not just subs-
titution, and we extend our evaluation to article 
errors. Second, we focus on finding the best query 
mechanism for each of these operations, which 
requires only a single query to the Web source. 
Finally, the focus of our work is on learner error 
detection, so we evaluate on real learner data as 
opposed to well-formed news text. This distinction 
is important: in our context, evaluation on edited 
text artificially inflates both precision and recall 
because the context surrounding the potential error 
site is error-free whereas learner writing can be, 
and often is, surrounded by errors. In addition, 
New York Times writing is highly idiomatic while 
learner productions often include unidiomatic word 
choices, even though the choice may not be consi-
dered an error. 
3 Experimental Setup 
3.1 Test Data 
Our test data is extracted from the Cambridge Uni-
versity Press Learners? Corpus (CLC). Our ver-
sion of CLC currently contains 20 million words 
from non-native English essays written as part of 
one of Cambridge?s English language proficiency 
tests (ESOL) ? at all proficiency levels. The essays 
are annotated for error type, erroneous span and 
suggested correction. We perform a number of 
preprocessing steps on the data. First, we correct 
all errors that were flagged as being spelling errors. 
Spelling errors that were flagged as morphology 
errors were left alone. We also changed confusable 
words that are covered by MS Word. In addition, 
we changed British English spelling to American 
English. We then eliminate all annotations for non-
pertinent errors (i.e. non-preposition/article errors, 
or errors that do not involve any of the targeted 
prepositions), but we retain the original (errone-
39
ous) text for these. This makes our task harder 
since we will have to make predictions in text con-
taining multiple errors, but it is more realistic giv-
en real learner writing. Finally, we eliminate 
sentences containing nested errors (where the an-
notation of one error contains an annotation for 
another error) and multiple article/preposition er-
rors. Sentences that were flagged for a replacement 
error but contained no replacement were also elim-
inated from the data. The final set we use consists 
of a random selection of 9,006 sentences from the 
CLC with article errors and 9,235 sentences with 
preposition errors. 
3.2 Search APIs and Corpora 
We examine three different sources of data to dis-
tinguish learner errors from corrected errors. First, 
we use two web search engine APIs, Bing and 
Google. Both APIs allow the retrieval of a page-
count estimate for an exact match query. Since 
these estimates are provided based on proprietary 
algorithms, we have to treat them as a "black box". 
The third source of data is the Google 5-gram cor-
pus (Linguistic Data Consortium 2006) which con-
tains n-grams with n ranging from 1 to 5. The 
count cutoff for unigrams is 200, for higher order 
n-grams it is 40. 
3.3 Query Formulation 
There are many possible ways to formulate an ex-
act match (i.e. quoted) query for an error and its 
correction, depending on the amount of context 
that is included on the right and left side of the er-
ror. Including too little context runs the risk of 
missing the linguistically relevant information for 
determining the proper choice of preposition or 
determiner. Consider, for example, the sentence we
rely most of/on friends. If we only include one 
word to the left and one word to the right of the 
preposition, we end up with the queries "most on 
friends" and "most of friends" - and the web hit 
count estimate may tell us that the latter is more 
frequent than the former. However, in this exam-
ple, the verb rely determines the choice of preposi-
tion and when it is included in the query as in "rely 
most on friends" versus "rely most of friends", the 
estimated hit counts might correctly reflect the in-
correct versus correct choice of preposition. Ex-
tending the query to cover too much of the context, 
on the other hand, can lead to low or zero web hit 
estimates because of data sparseness - if we in-
clude the pronoun we in the query as in "we rely 
most on friends" versus "we rely most of friends", 
we get zero web count estimates for both queries.  
Another issue in query formulation is what 
strategy to use for corrections that involve dele-
tions and insertions, where the number of tokens 
changes. If, for example, we use queries of length 
3, the question for deletion queries is whether we 
use two words to the left and one to the right of the 
deleted word, or one word to the left and two to the 
right. In other words, in the sentence we traveled 
to/0 abroad last year, should the query for the cor-
rection (deletion) be "we traveled abroad" or "tra-
veled abroad last"? 
Finally, we can employ some linguistic informa-
tion to design our query. By using part-of-speech 
tag information, we can develop heuristics to in-
clude a governing content word to the left and the 
head of the noun phrase to the right. 
The complete list of query strategies that we 
tested is given below. 
SmartQuery: using part-of-speech information 
to include the first content word to the left and the 
head noun to the right. If the content word on the 
left cannot be established within a window of 2 
tokens and the noun phrase edge within 5 tokens, 
select a fixed window of 2 tokens to the left and 2 
tokens to the right. 
FixedWindow Queries: include n tokens to the 
left and m tokens to the right. We experimented 
with the following settings for n and m: 1_1, 2_1, 
1_2, 2_2, 3_2, 2_3. The latter two 6-grams were 
only used for the API?s, because the Google corpus 
does not contain 6-grams. 
FixedLength Queries: queries where the length 
in tokens is identical for the error and the correc-
tion. For substitution errors, these are the same as 
the corresponding FixedWindow queries, but for 
substitutions and deletions we either favor the left 
or right context to include one additional token to 
make up for the deleted/inserted token. We expe-
rimented with trigrams, 4-grams, 5-grams and 6-
grams, with left and right preference for each, they 
are referred to as Left4g (4-gram with left prefe-
rence), etc. 
40
3.4 Evaluation Metrics 
For each query pair <qerror, qcorrection>, we produce 
one of three different outcomes: 
correct (the query results favor the correction of 
the learner error over the error itself):  
count(qcorrection) > count(qerror) 
incorrect (the query results favor the learner error 
over its correction):   
count(qerror) >= count(qcorrection) 
 where(count(qerror) &  0 OR 
 count(qcorrection) &  0) 
noresult:  
count(qcorrection) = count(qerror) = 0 
For each query type, each error (preposition or ar-
ticle), each correction operation (deletion, inser-
tion, substitution) and each web resource (Bing 
API, Google API, Google N-grams) we collect 
these counts and use them to calculate three differ-
ent metrics. Raw accuracy is the ratio of correct 
predictions to all query pairs: 
!"#$"%%&'"%( ) $
%*''
%*'' + ,-%*'' + -*'./&01 
We also calculate accuracy for the subset of query 
pairs where at least one of the queries resulted in a 
successful hit, i.e. a non-zero result. We call this 
metric Non-Zero-Result-Accurracy (NZRA), it is 
the ratio of correct predictions to incorrect predic-
tions, ignoring noresults: 
2*-3.'*!./&014%%&'"%( ) $
%*''
%*'' + ,-%*'' 
Finally, retrieval ratio is the ratio of queries that 
returned non-zero results: 
4 Results
We show results from our experiments in Table 1 -   
Table 6. Since space does not permit a full tabula-
tion of all the individual results, we restrict our-
selves to listing only those query types that achieve 
best results (highlighted) in at least one metric. 
Google 5-grams show significantly better results 
than both the Google and Bing APIs. This is good 
news in terms of implementation, because it frees 
the system from the vagaries involved in relying on 
search engine page estimates: (1) the latency, (2) 
query quotas, and (3) fluctuations of page esti-
mates over time. The bad news is that the 5-gram 
corpus has much lower retrieval ratio because, pre-
sumably, of its frequency cutoff. Its use also limits 
the maximum length of a query to a 5-gram (al-
though neither of the APIs outperformed Google 5-
grams when retrieving 6-gram queries). 
The results for substitutions are best, for fixed 
window queries. For prepositions, the SmartQue-
ries perform with about 86% NZRA while a fixed 
length 2_2 query (targeted word with a ?2-token 
window) achieves the best results for articles, at 
about 85% (when there was at least one non-zero 
match). Retrieval ratio for the prepositions was 
about 6% lower than retrieval ratio for articles ?
41% compared to 35%.  
The best query type for insertions was fixed-
length LeftFourgrams with about 95% NZRA and 
71% retrieval ratio for articles and 89% and 78% 
retrieval ratio for prepositions. However, Left-
Fourgrams favor the suggested rewrites because, 
by keeping the query length at four tokens, the 
original has more syntactic/semantic context. If the 
original sentence contains is referred as the and the 
annotator inserted to before as, the original query 
will be is referred as the and the correction query 
is referred to as.  
Conversely, with deletion, having a fixed win-
dow favors the shorter rewrite string. The best 
query types for deletions were: 2_2 queries for ar-
ticles (94% NZRA and 46% retrieval ratio) and 
SmartQueries for prepositions (97% NZRA and 
52% retrieval ratio). For prepositions the fixed 
length 1_1 query performs about the same as the 
SmartQueries, but that query is a trigram (or 
smaller at the edges of a sentence) whereas the av-
erage length of SmartQueries is 4.7 words for pre-
positions and 4.3 words for articles. So while the 
coverage for SmartQueries is much lower, the 
longer query string cuts the risk of matching on 
false positives.  
The Google 5-gram Corpus differs from search 
engines in that it is sensitive to upper and lower 
case distinctions and to punctuation. While intui-
tively it seemed that punctuation would hurt n-
gram performance, it actually helps because the 
punctuation is an indicator of a clause boundary. A 
recent Google search for have a lunch and have 
lunch produced estimates of about 14 million web 
pages for the former and only 2 million for the lat-
ter. Upon inspecting the snippets for have a lunch, 
the next word was almost always a noun such as 
menu, break, date, hour, meeting, partner, etc. The 
relative frequencies for have a lunch would be 
much different if a clause boundary marker were 
41
required. The 5-gram corpus also has sentence 
boundary markers which is especially helpful to 
identify changes at the beginning of a sentence. 
 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
SmartQuery 0.8637 0.9548 0.9742 0.8787 0.8562 0.5206 0.7589 0.8176 0.5071
1_1 0.4099 0.9655 0.9721 0.9986 0.9978 0.9756 0.4093 0.9634 0.9484
Table 1: Preposition deletions (1395 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
Left4g 0.7459 0.8454 0.8853 0.9624 0.9520 0.7817 0.7178 0.8048 0.6920
1_1 0.5679 0.2983 0.3550 0.9973 0.9964 0.9733 0.5661 0.2971 0.3456
Right3g 0.6431 0.8197 0.8586 0.9950 0.9946 0.9452 0.6399 0.8152 0.8116
Table 2: Preposition insertions (2208 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
SmartQuery 0.7396 0.8183 0.8633 0.7987 0.7878 0.4108 0.5906 0.6446 0.5071
1_1=L3g=R3g 0.4889 0.6557 0.6638 0.9870 0.9856 0.9041 0.4826 0.6463 0.6001
1_2=R4g 0.6558 0.7651 0.8042 0.9178 0.9047 0.6383 0.6019 0.6921 0.5133
Table 3: Preposition substitutions (5632 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
2_2 0.7678 0.9056 0.9386 0.8353 0.8108 0.4644 0.6414 0.7342 0.4359
1_1 0.3850 0.8348 0.8620 0.9942 0.9924 0.9606 0.3828 0.8285 0.8281
1_2 0.5737 0.8965 0.9097 0.9556 0.9494 0.7920 0.5482 0.8512 0.7205
Table 4: Article deletions (2769 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
Left4g 0.8292 0.9083 0.9460 0.9505 0.9428 0.7072 0.7880 0.8562 0.6690
1_1 0.5791 0.3938 0.3908 0.9978 0.9975 0.9609 0.5777 0.3928 0.3755
Left3g 0.6642 0.8983 0.8924 0.9953 0.9955 0.9413 0.6611 0.8942 0.8400
Table 5: Article insertions (5520 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
2_2=Left5g= 
Right5g 0.6970 0.7842 0.8486 0.8285 0.8145 0.4421 0.5774 0.6388 0.3752
1_1=L3g=R3g 0.4385 0.7063 0.7297 0.9986 0.9972 0.9596 0.4379 0.7043 0.7001
1_2=R4g 0.5268 0.7493 0.7917 0.9637 0.9568 0.8033 0.5077 0.7169 0.6360
Table 6: Article substitutions (717 query pairs). 
 
42
5 Error Analysis 
We manually inspected examples where the 
matches on the original string were greater than 
matches on the corrected string. The results of this 
error analysis are shown in table 7. Most of the 
time, (1) the context that determined article or pre-
position use and choice was not contained within 
the query. This includes, for articles, cases where 
article usage depends either on a previous mention 
or on the intended sense of a polysemous head 
noun. Some other patterns also emerged. Some-
times (2) both and the original and the correction 
seemed equally good in the context of the entire 
sentence, for example it?s very important to us and 
it?s very important for us.  In other cases, (3) there 
was another error in the query string (recall that we 
retained all of the errors in the original sentences 
that were not the targeted error). Then there is a 
very subjective category (4) where the relative n-
gram frequencies are unexpected, for example 
where the corpus has 171 trigrams guilty for you 
but only 137 for guilty about you. These often oc-
cur when both of the frequencies are either low 
and/or close. This category includes cases where it 
is very likely that one of the queries is retrieving an 
n-gram whose right edge is the beginning of a 
compound noun (as in with the trigram have a 
lunch). Finally, (5) some of the ?corrections? either 
introduced an error into the sentence or the original 
and ?correction? were equally bad. In this catego-
ry, we also include British English article usage 
like go to hospital. For prepositions, (6) some of 
the corrections changed the meaning of the sen-
tence ? where the disambiguation context is often 
not in the sentence itself and either choice is syn-
tactically correct, as in I will buy it from you 
changed to I will buy it for you. 
 
 Articles Preps 
 freq ratio freq ratio
1.N-gram does not con-
tain necessary context 187 .58 183 .52
2.Original and correc-
tion both good 39 .12 51 .11
3.Other error in n-gram 30 .9 35 .10
4.Unexpected ratio 36 .11 27 .09
5.Correction is wrong 30 .9 30 .08
6.Meaning changing na na 24 .07
Table 7: Error analysis 
 
If we count categories 2 and 5 in Table 7 as not 
being errors, then the error rate for articles drops 
20% and the error rate for prepositions drops 19%. 
A disproportionately high subcategory of query 
strings that did not contain the disambiguating con-
text (category 1) was at the edges of the sentence ? 
especially for the LeftFourgrams at the beginning 
of a sentence where the query will always be a bi-
gram. 
6 Conclusion and Future Work 
We have demonstrated that web source counts can 
be an accurate predictor for distinguishing between 
a learner error and its correction - as long as the 
query strategy is tuned towards the error type. 
Longer queries, i.e. 4-grams and 5-grams achieve 
the best non-zero-result accuracy for articles, while 
SmartQueries perform best for preposition errors. 
Google N-grams across the board achieve the best 
non-zero-result accuracy, but not surprisingly they 
have the lowest retrieval ratio due to count cutoffs. 
Between the two search APIs, Bing tends to have 
better retrieval ratio, while Google achieves higher 
accuracy. 
In terms of practical use in an error detection 
system, a general "recipe" for a high precision 
component can be summarized as follows. First, 
use the Google Web 5-gram Corpus as a web 
source. It achieves the highest NZRA, and it avoids 
multiple problems with search APIs: results do not 
fluctuate over time, results are real n-gram counts 
as opposed to document count estimates, and a lo-
cal implementation can avoid the high latency as-
sociated with search APIs. Secondly, carefully 
select the query strategy depending on the correc-
tion operation and error type. 
We hope that this empirical investigation can 
contribute to a more solid foundation for future 
work in error detection and correction involving 
the web as a source for data. While it is certainly 
not sufficient to use only web data for this purpose, 
we believe that the accuracy numbers reported here 
indicate that web data can provide a strong addi-
tional signal in a system that combines different 
detection and correction mechanisms. One can im-
agine, for example, multiple ways to combine the 
n-gram data with an existing language model. Al-
ternatively, one could follow Bergsma et al (2009) 
and issue not just a single pair of queries but a 
43
whole series of queries and sum over the results. 
This would increase recall since at least some of 
the shorter queries are likely to return non-zero 
results. In a real-time system, however, issuing 
several dozen queries per potential error location 
and potential correction could cause performance 
issues. Finally, the n-gram counts can be incorpo-
rated as one of the features into a system such as 
the one described in Gamon (2010) that combines 
evidence from various sources in a principled way 
to optimize accuracy on learner errors. 
Acknowledgments 
We would like to thank Yizheng Cai for making 
the Google web ngram counts available through a 
web service and to the anonymous reviewers for 
their feedback. 
References  
Eric Steven Atwell. 1987. How to detect grammatical 
errors in a text without parsing it. Proceedings of the 
3rd EACL, Copenhagen, Denmark, pp 38 - 45. 
Michele Banko and Eric Brill. 2001a. Mitigating the 
paucity-of-data problem: Exploring the effect of 
training corpus size on classifier performance for 
natural language processing. In James Allan, editor, 
Proceedings of the First International Conference on 
Human Language Technology Research. Morgan 
Kaufmann, San Francisco. 
Michele Banko and Eric Brill. 2001b. Scaling to very 
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting of 
the Association for Computational Linguistics and 
the 10th Conference of the European Chapter of the 
Association for Computational Linguistics, pp. 26?
33, Toulouse, France. 
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009. 
Web-scale n-gram models for lexical disambiguation. 
In Proceedings for the 21st International Joint Confe-
rence on Artificial Intelligence, pp. 1507 ? 1512. 
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving pre-
positions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions, pp. 25-30.  
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In 
Proceedings of the Fourth ACL-SIGSEM Workshop 
on Prepositions, pp. 45-50. Prague. 
Rachele De Felice and Stephen Pulman.  2008.  A clas-
sifier-based approach to preposition and determiner 
error correction in L2 English.  COLING. Manches-
ter, UK. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, 
and Lucy Vanderwende. 2008. Using contextual 
speller techniques and language modeling for ESL 
error correction. In Proceedings of IJCNLP, Hydera-
bad, India.  
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners? writing. In Proceedings of 
NAACL. 
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 
2006. Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Matthieu Hermet, Alain D?silets, Stan Szpakowicz. 
2008. Using the web as a linguistic resource to auto-
matically correct lexico-syntactic errors. In Proceed-
ings of the 6th Conference on Language Resources 
and Evaluation (LREC), pp. 874-878. 
Frank Keller and Mirella Lapata. 2003. Using the web 
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3): 459-484. 
Adam Kilgariff. 2007. Googleology is bad science. 
Computational Linguistics 33(1): 147-151. 
Mirella Lapata and Frank Keller. 2005. Web-Based 
Models for Natural Language Processing. ACM 
Transactions on Speech and Language Processing 
(TSLP), 2(1):1-31. 
Linguistic Data Consortium. 2006. Web 1T 5-gram 
version 1. 
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13 . 
Joel Tetreault and Martin Chodorow. 2008. The ups and 
downs of preposition error detection in ESL. 
COLING. Manchester, UK. 
Joel Tetreault and Martin Chodorow. 2009. Examining 
the use of region web counts for ESL error detection. 
Web as Corpus Workshop (WAC-5), San Sebastian, 
Spain. 
Xing Yi, Jianfeng Gao and Bill Dolan.  2008.  A web-
based English proofing system for English as a 
second language users.  In Proceedings of the Third 
International Joint Conference on Natural Language 
Processing (IJCNLP). Hyderabad, India. 
Zhu, X. and Rosenfeld, R. 2001. Improving trigram 
language modeling with the world wide web. In Pro-
ceedings of International Conference on Acoustics 
Speech and Signal Processing. Salt Lake City. 
44
