Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 97?102,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Parsing Syntactic and Semantic Dependencies for Multiple Languages 
with A Pipeline Approach 
 
Han Ren, Donghong Ji Jing Wan, Mingyao Zhang 
School of Computer Science Center for Study of Language & Information 
Wuhan University Wuhan University 
Wuhan 430079, China Wuhan 430079, China 
cslotus@mail.whu.edu.cn 
donghong_ji@yahoo.com 
{jennifer.wanj, my.zhang}@gmail.com
 
 
 
Abstract 
This paper describes a pipelined approach for 
CoNLL-09 shared task on joint learning of 
syntactic and semantic dependencies. In the 
system, we handle syntactic dependency pars-
ing with a transition-based approach and util-
ize MaltParser as the base model. For SRL, 
we utilize a Maximum Entropy model to iden-
tify predicate senses and classify arguments. 
Experimental results show that the average 
performance of our system for all languages 
achieves 67.81% of macro F1 Score, 78.01% 
of syntactic accuracy, 56.69% of semantic la-
beled F1, 71.66% of macro precision and 
64.66% of micro recall. 
1 Introduction 
Given a sentence with corresponding part-of-
speech for each word, the task of syntactic and se-
mantic dependency parsing contains two folds: (1) 
identifying the syntactic head of each word and 
assigning the dependency relationship between the 
word and its head; (2) identifying predicates with 
proper senses and labeling semantic dependencies 
for them. 
For data-driven syntactic dependency parsing, 
many approaches are based on supervised learning 
using treebank or annotated datasets. Currently, 
graph-based and transition-based algorithms are 
two dominating approaches that are employed by 
many researchers, especially in previous CoNLL 
shared tasks. Graph-based algorithms (Eisner, 
1996; McDonald et al, 2005) assume a series of 
dependency tree candidates for a sentence and the 
goal is to find the dependency tree with highest 
score. Transition-based algorithms (Yamada and 
Matsumoto, 2003; Nivre et al, 2004) utilize transi-
tion histories learned from dependencies within 
sentences to predict next state transition and build 
the optimal transition sequence. Although different 
strategies were considered, two approaches yielded 
comparable results at previous tasks. 
Semantic role labeling contains two problems: 
identification and labeling. Identification is a bi-
nary classification problem, and the goal is to iden-
tify annotated units in a sentence; while labeling is 
a multi-class classification problem, which is to 
assign arguments with appropriate semantic roles. 
Hacioglu (2004) utilized predicate-argument struc-
ture and map dependency relations to semantic 
roles. Liu et al (2005) combined two problems 
into a classification one, avoiding some annotated 
units being excluded due to some incorrect identi-
fication results. In addition, various features are 
also selected to improve accuracy of SRL. 
In this paper, we propose a pipelined approach 
for CoNLL-09 shared task on joint learning of syn-
tactic and semantic dependencies, and describe our 
system that can handle multiple languages. In the 
system, we handle syntactic dependency parsing 
with a transition-based approach. For SRL, we util-
ize Maximum Entropy model to identify predicate 
senses and classify arguments. 
The remain of the paper is organized as follows. 
In Section 2, we discuss the processing mechanism 
containing syntactic and semantic dependency 
parsing of our system in detail. In Section 3, we 
give the evaluation results and analysis. Finally, 
the conclusion and future work are given in Sec-
tion 4. 
97
2 System Description  
The system, which is a two-stage pipeline, proc-
esses syntactic and semantic dependencies respec-
tively. To reduce the difficulties in SRL, predicates 
of each sentence in all training and evaluation data 
are labeled, thus predicate identification can be 
ignored. 
 
Figure 1. System Architectures 
 
For syntactic dependencies, we employ a state-
of-the-art dependency parser and basic plus ex-
tended features for parsing. For semantic depend-
encies, a Maximum Entropy Model is used both in 
predicate sense identification and semantic role 
labeling. Following subsections will show compo-
nents of our system in detail. 
2.1 Syntactic Dependency Parsing 
In the system, MaltParser1 is employed for syntac-
tic dependency parsing. MaltParser is a data-driven 
deterministic dependency parser, based on a Sup-
port Vector Machine classifier. An extensive re-
search (Nivre, 2007) parsing with 9 different 
languages shows that the parser is language-
independent and yields good results. 
MaltParser supports two kinds of parsing algo-
rithms: Nivre?s algorithms and Covington?s incre-
mental algorithms. Nivre?s algorithms, which are 
deterministic algorithms consisting of a series of 
shift-reduce procedures, defines four operations: 
?Right. For a given triple <t|S, n|I, A>, S 
represents STACK and I represents INPUT. If 
dependency relation t ? n exists, it will be 
                                                          
1 http://w3.msi.vxu.se/~jha/maltparser/ 
pendency relation t?n exists, it will be appended 
into A and t will be removed from S. 
?Left. For a given triple <t|S, n|I, A>, if de-
pendency relation n?t exists, it will be appended 
into A and n will be pushed into S. 
?Reduce. If dependency relation n?t does not 
exist, and the parent node of t exists left to it, t will 
be removed from S. 
?Shift. If none of the above satisfies, n will be 
pushed into S. 
The deterministic algorithm simplifies determi-
nation for Reduce operation. As a matter of fact, 
some languages, such as Chinese, have more flexi-
ble word order, and some words have a long dis-
tance with their children. In this case, t should not 
be removed from S, but be handled with Shift op-
eration. Otherwise, dependency relations between t 
and its children will never be identified, thus se-
quential errors of dependency relations may occur 
after the Reduce operation. 
For syntactic dependencies with long distance, 
an improved Reduce strategy is: if the dependency 
relation between n and t does not exist, and the 
parent node of t exists left to it and the dependency 
relation between the parent node and n, t will be 
removed from S. The Reduce operation is projec-
tive, since it doesn?t influence the following pars-
ing procedures. The Improved algorithm is 
described as follows: 
(1) one of the four operations is performed ac-
cording to the dependency relation between t and n 
until EOS; if only one token remains in S, go to (3). 
(2) continue to select operations for remaining 
tokens in S; when Shift procedure is performed, 
push t to S; if only one token remains in S and I 
contains more tokens than only EOS, goto (1). 
(3) label all odd tokens in S as ROOT, pointing 
to EOS. 
We also utilize history-based feature models 
implemented in the parser to predict the next action 
in the deterministic derivation of a dependency 
structure. The parser provides some default fea-
tures that is general for most languages: (1) part-
of-speech features of TOP and NEXT and follow-
ing 3 tokens; (2) dependency features of TOP con-
taining leftmost and rightmost dependents, and of 
NEXT containing leftmost dependents; (3) Lexical 
98
features of TOP, head of TOP, NEXT and follow-
ing one token. We also extend features for multiple 
languages: (1) count of part-of-speech features of 
following tokens extend to 5; (2) part-of-speech 
and dependent features of head of TOP. 
2.2 Semantic Dependency Parsing 
Each defacto predicate in training and evaluation 
data of CoNLL09 is labeled with a sign ?Y?, which 
simplifies the work of semantic dependency pars-
ing. In our system, semantic dependency parsing is 
a pipeline that contains two parts: predicate sense 
identification and semantic role labeling. For 
predicate sense identification, each predicate is 
assigned a certain sense number. For semantic role 
labeling, local and global features are selected. 
Features of each part are trained by a classification 
algorithm.  Both parts employ a Maximum Entropy 
Tool MaxEnt in a free package OpenNLP 2 as a 
classifier. 
2.2.1  Predicate Sense Identification 
The goal of predicate sense identification is to de-
cide the correct frame for a predicate. According to 
PropBank (Palmer, et al, 2005), predicates contain 
one or more rolesets corresponding to different 
senses. In our system, a classifier is employed to 
identify each predicate?s sense.  
Suppose { }01, 02, , LC = ? N
s t
                                                          
 is the sense set 
(NL is the count of categories corresponding to the 
language L, eg., in Chinese training set NL = 10 
since predicates have at most 10 senses in the set), 
and ti is the ith sense of word w in sentence s. The 
model is implemented to assign each predicate to 
the most probatilistic sense. 
( | , )i C it P w?=argmax                (1) 
Features for predicate sense identification are 
listed as follows: 
?WORD, LEMMA, DEPREL: The lexical 
form and lemma of the predicate; the dependency 
relation between the predicate and its head; for 
Chinese and Japanese, WORD is ignored. 
? HEAD_WORD, HEAD_POS: The lexical 
form and part-of-speech of the head of the predi-
cate. 
2 http://maxent.sourceforge.net/ 
? CHILD_WORD_SET, CHILD_POS_SET, 
CHILD_DEP_SET: The lexical form, part-of-
speech and dependency relation of dependents of 
the predicate. 
?LSIB_WORD, LSIB_POS, LSIB_DEPREL, 
RSIB_WORD, RSIB_POS, RSIB_DEPREL: The 
lexical form, part-of-speech and dependency rela-
tion of the left and right sibling token of the predi-
cate. Features of sibling tokens are adopted, 
because senses of some predicates can be inferred 
from its left or right sibling. 
For English data set, we handle verbal and 
nominal predicates respectively; for other lan-
guages, we handle all predicates with one classifier. 
If a predicate in the evaluation data does not exist 
in the training data, it is assigned the most frequent 
sense label in the training data. 
2.2.2  Semantic Role Labeling 
Semantic role labeling task contains two parts: ar-
gument identification and argument classification. 
In our system the two parts are combined as one 
classification task. Our reason is that those argu-
ment candidates that potentially become semantic 
roles of corresponding predicates should not be 
pruned by incorrect argument identification. In our 
system, a predicate-argument pair consists of any 
token (except predicates) and any predicate in a 
sentence. However, we find that argument classifi-
cation is a time-consuming procedure in the ex-
periment because the classifier spends much time 
on a great many of invalid predicate-argument 
pairs. To reduce useless computing, we add a sim-
ple pruning method based on heuristic rules to re-
move invalid pairs, such as punctuations and some 
functional words.  
Features used in our system are based on (Ha-
cioglu, 2004) and (Pradhan et al 2005), and de-
scribed as follows:  
?WORD, LEMMA, DEPREL: The same with 
those mentioned in section 2.2.1. 
?VOICE: For verbs, the feature is Active or 
Passive; for nouns, it is null. 
?POSITION: The word?s position correspond-
ing to its predicate: Left, Right or Self. 
?PRED: The lemma plus sense of the word. 
?PRED_POS: The part-of-speech of the predi-
cate. 
99
?LEFTM_WORD, LEFTM_POS, RIGHTM_ 
WORD, RIGHTM_POS: Leftmost and rightmost 
word and their part-of-speech of the word. 
? POS_PATH: All part-of-speech from the 
word to its predicate, including Up, Down, Left 
and Right, eg. ?NN?VV?CC?VV?. 
?DEPREL_PATH: Dependency relations from 
the word to its predicate, eg. ?COMP?RELC?
COMP??. 
?ANC_POS_PATH, ANC_DEPREL_PATH: 
Similar to POS_PATH and DEPREL_PATH, part-
of-speech and dependency relations from the word 
to the common ancestor with its predicate. 
?PATH_LEN: Count of passing words from 
the word to its predicate. 
? FAMILY: Relationship between the word 
and its predicate, including Child, Parent, Descen-
dant, Ancestor, Sibling, Self and Null. 
? PRED_CHD_POS, PRED_CHD_DEPREL: 
Part-of-speech and dependency relations of all 
children of the word?s predicate. 
For different languages, some features men-
tioned above are invalid and should be removed, 
and some extended features could improve the per-
formance of the classifier. In our system we mainly 
focus on Chinese, therefore, WORD and VOICE 
should be removed when processing Chinese data 
set. We also adopt some features proposed by (Xue, 
2008): 
? POS_PATH_BA, POS_PATH_SB, POS_ 
PATH_LB: BA and BEI are functional words that 
impact the order of arguments. In PropBank, BA 
words have the POS tag BA, and BEI words have 
two POS tags: SB (short BEI) and LB (long BEI). 
3 Experimental Results  
Our experiments are based on a PC with a Intel 
Core 2 Duo 2.1G CPU and 2G memory. Training 
and evaluation data (Taul? et al, 2008; Xue et al, 
2008; Haji? et al, 2006; Palmer et al, 2002; Bur-
chardt et al, 2006; Kawahara et al, 2002) have 
been converted to a uniform CoNLL Shared Task 
format. In all experiments, SVM and ME model 
are trained using training data, and tested with 
development data of all languages.  
The system for closed challenge is designed as 
two parts. For syntactic dependency training and 
parsing, we utilize the projective model in Malt-
Parser for data sets. We also follow default settings 
in MaltParser, such as assigned parameters for 
LIBSVM and combined prediction strategy, and 
utilize improved approaches mentioned in section 
2. For semantic dependency training and parsing, 
we choose the count of iteration as 100 and cutoff 
value as 10 for the ME model. Table 1 shows the 
training time for syntactic and semantic depend-
ency of all languages. Parsing time for syntactic is 
not more than 30 minutes, and for semantic is not 
more than 5 minutes of each language. 
 
 syn prd sem 
English 7h 12min 47min 
Chinese 8h 18min 61min 
Japanese 7h 14min 46min 
Czech 13h 46min 77min 
German 6h 16min 54min 
Spanish 6h 15min 55min 
Catalan 6h 15min 50min 
Table 1. Training cost for all languages. syn, prd and 
sem mean training time for syntactic dependency, predi-
cate identification and semantic dependency. 
3.1 Syntactic Dependency Parsing 
We utilize MaltParser with improved algorithms 
mentioned in section 2.1 for syntactic dependency 
parsing, and the results are shown in Table 2. 
 
 LAS UAS label-acc. 
English 87.57 89.98 92.19 
Chinese 79.17 81.22 85.94 
Japanese 91.47 92.57 97.28 
Czech 57.30 75.66 65.39 
German 76.63 80.31 85.97 
Spanish 76.11 84.40 84.69 
Catalan 77.84 86.41 85.78 
Table 2. Performance of syntactic dependency parsing 
 
Table 2 indicates that parsing for Japanese and 
English data sets has a better performance than 
other languages, partly because determinative algo-
rithm and history-based grammar are more suited 
for these two languages. To compare the perform-
ance of our approach of improved deterministic 
algorithm and extended features, we make another 
experiment that utilize original arc-standard algo-
rithm and base features for syntactic experiments. 
Due to time limitation, the experiments are only 
based on Chinese training and evaluation data. The 
results show that LAS and UAS drops about 2.7% 
and 2.2% for arc-standard algorithm, 1.6% and 
1.2% for base features. They indicate that our de-
100
terministic algorithm and the extend features can 
help to improve syntactic dependency parsing. We 
also notice that the results of Czech achieve a 
lower performance than other languages. It mainly 
because the language has more rich morphology, 
usually accompanied by more flexible word order. 
Although using a large training set, linguistic prop-
erties greatly influence the parsing result. In addi-
tion, extended features are not suited for this 
language and the feature model should be opti-
mized individually. 
For all of the experiments we mainly focus on 
the language of Chinese. When parsing Chinese 
data sets we find that the focus words where most 
of the errors occur are almost punctuations, such as 
commas and full stops. Apart from errors of punc-
tuations, most errors occur on prepositions such as 
the Chinese word ?at?. Most of these problems 
come from assigning the incorrect dependencies, 
and the reason is that the parsing algorithm con-
cerns the form rather than the function of these 
words. In addition, the prediction of dependency 
relation ROOT achieves lower precision and recall 
than others, indicating that MaltParser overpredicts 
dependencies to the root. 
3.2 Semantic Dependency Parsing 
MaxEnt is employed as our classifier to train and 
parse semantic dependencies, and the results are 
shown in Table 3, in which all criterions are la-
beled. 
 
 P R F1 
English 76.57 60.45 67.56 
Chinese 75.45 69.92 72.58 
Japanese 91.93 43.15 58.73 
Czech 68.83 57.78 62.82 
German 62.96 47.75 54.31 
Spanish 40.11 39.50 39.80 
Catalan 41.34 40.66 41.00 
Table 3. Performance of semantic dependency parsing 
 
As shown in Table 3, the scores of the latter 
five languages are quite lower than those of the 
former two languages, and the main reason could 
be inferred from the scores of Table 2 that the drop 
of the performance of semantic dependency pars-
ing comes from the low performance of syntactic 
dependency parsing. Another reason is that, mor-
phological features are not be utilized in the classi-
fier. Our post experiments after submission show 
that average performance could improve the per-
formance after adding morphological and some 
combined features. In addition, difference between 
precision and recall indicates that the classification 
procedure works better than the identification 
procedure in semantic role labeling.  
For Chinese, semantic role of some words with 
part-of-speech VE have been mislabeled. It?s 
mainly because that these words in Chinese have 
multiple part-of-speech. The errors of POS and 
PRED greatly influence the system to perform 
these words. Another main problem occurs on the 
pairs NN + A0/A1. Identification of the two pairs 
are much lower than VA/VC/VE/VV + A0/A1 
pairs. The reason is that the identification of nomi-
nal predicates have more errors than that of verbal 
predicates due to the combination of SRL for these 
two kinds of predicates. For further study, verbal 
predicates and nominal predicates should be han-
dled respectively so that the overall performance 
can be improved.  
3.3 Overall Performance 
The average performance of our system for all lan-
guages achieves 67.81% of macro F1 Score, 
78.01% of syntactic accuracy, 56.69% of semantic 
labeled F1, 71.66% of macro precision and 64.66% 
of micro recall. 
4 Conclusion 
In this paper, we propose a pipelined approach for 
CoNLL-09 shared task on joint learning of syntac-
tic and semantic dependencies, and describe our 
system that can handle multiple languages. Our 
system focuses on improving the performance of 
syntactic and semantic dependency respectively. 
Experimental results show that the overall per-
formance can be improved for multiple languages 
by long distance dependency algorithm and ex-
tended history-based features. Besides, the system 
fits for verbal predicates than nominal predicates 
and the classification procedure works better than 
identification procedure in semantic role labeling. 
For further study, respective process should be 
handled between these two kinds of predicates, and 
argument identification should be improved by 
using more discriminative features for a better 
overall performance. 
101
Acknowledgments 
This work is supported by the Natural Science 
Foundation of China under Grant Nos.60773011, 
90820005, and Independent Research Foundation 
of Wuhan University. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2006). Genoa, Italy. 
Jason M. Eisner. 1996. Three new probabilistic models 
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pp.340?345. 
Kadri Hacioglu. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics 
(COLING). 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie Mikulo-
v? and Zden?k ?abokrtsk?. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data 
Consortium, Philadelphia, Pennsylvania, USA. ISBN 
1-58563-370-4. LDC Cat. No. LDC2006T01. URL: 
http://ldc.upenn.edu. 
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL 2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th 
Conference on Computational Natural Language 
Learning (CoNLL-2009). Boulder, Colorado, USA. 
June 4-5. pp.3-22. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp.2008-2013. 
Ryan McDonald, Koby Crammer, and Fernando Pereira.  
2005. Online large-margin training of dependency 
parsers. In Proceedings of the 43rd Annual Meeting of 
the Association for Computational Linguistics (ACL), 
pp.91?98. 
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. 
Memory-based dependency parsing. In Proceedings 
of the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL), pp.49?56. 
Joakim Nivre. 2004. Incrementality in Deterministic 
Dependency Parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop 
at ACL-2004, Barcelona, Spain, pp.50-57. 
Joakim Nivre and Johan Hall. 2005. MaltParser: A lan-
guage-independent system for data-driven depend-
ency parsing. In Proceedings of the Fourth Workshop 
on Treebanks and Linguistic Theories (TLT). 
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, 
Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov 
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural language Engineering, Volume 13, Is-
sue 02, pp.95-135. 
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin and Daniel Jurafsky. 
2005. Support Vector Learning for Semantic Argu-
ment classification. Machine Learning Journal, 2005, 
60(3): 11?39. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of 
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008). 
Mariona Taul?, Maria Ant?nia Mart? and Marta Reca-
sens. 2008. AnCora: Multilevel Annotated Corpora 
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC-2008). Marrakech, Morocco. 
Liu Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and 
Huaijun Liu. 2005. Semantic role labeling system us-
ing maximum entropy classifier. In Proceedings of 
the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL). 
Nianwen Xue. 2008. Labeling Chinese Predicates with 
Semantic roles. Computational Linguistics, 34(2): 
225-255. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank.  Natural Language 
Engineering, 15(1):143-172. 
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector ma-
chines. In Proceedings of the 8th International 
Workshop on Parsing Technologies (IWPT), pp.195?
206. 
 
102
Document Re-ranking Based on Automatically Acquired  
Key Terms in Chinese Information Retrieval 
Yang Lingpeng, Ji Donghong, Tang Li 
Institute for Infocomm Research 
21, Heng Mui Keng Terrace 
Singapore, 119613 
{lpyang,dhji,tangli}@i2r.a-star.edu.sg 
 
Abstract 
For Information Retrieval, users are more 
concerned about the precision of top ranking 
documents in most practical situations. In this 
paper, we propose a method to improve the 
precision of top N ranking documents by 
reordering the retrieved documents from the 
initial retrieval. To reorder documents, we first 
automatically extract Global Key Terms from 
document set, then use extracted Global Key 
Terms to identify Local Key Terms in a single 
document or query topic, finally we make use 
of Local Key Terms in query and documents to 
reorder the initial ranking documents. The 
experiment with NTCIR3 CLIR dataset shows 
that an average 10%-11% improvement and 
2%-5% improvement in precision can be 
achieved at top 10 and 100 ranking documents 
level respectively.  
1 Introduction 
Information retrieval (IR) is used to retrieve 
relevant documents from a large document set 
for a given query where the query is a simple 
description by natural language. In most 
practical situations, users concern more on the 
precision of top ranking documents than recall 
because users want to acquire relevant 
information from the top ranking documents.   
Traditionally, IR system uses a one-stage or 
a two-stage mechanism to retrieve relevant 
documents from document set. For one stage 
mechanism, IR system only does an initial 
retrieval. For two-stage mechanism, besides 
the initial retrieval, IR system will make use of 
the initial ranking documents to automatically 
do query expansion to form a new query and 
then use the new query to retrieve again to get 
the final ranking documents. The effectiveness 
of query expansion mainly depends on the 
precision of top N (N<50) ranking documents 
in initial retrieval because almost all proposed 
automatic query expansion algorithms make 
use of the information in the top N retrieved. 
Figure 1 demonstrates the general processes of 
a two-stage IR system. 
In this paper, we propose a method to 
improve the precision of top N ranking 
documents by reordering the initially retrieved 
documents in the initial retrieval. To reorder 
documents, we first automatically extract 
Global Key Terms from the document set, then 
use the extracted Global Key Terms to identify 
Local Key Terms in a single document or query 
topic, finally we make use of the Local Key 
Terms in queries and documents to reorder the 
initial ranking documents.  
Although our method is general and can 
apply to any languages, in this paper we?ll only 
focus on the research on Chinese IR system. 
 
F i g .  1  T r a d i t i o n a l  P r o c e s s  o f  t w o - s t a g e s  I R  
O r i g i n a l  Q u e r y  
E x p a n d e d   Q u e r y  
I n i t i a l  R e t r i e v a l  
F i n a l  R e t r i e v a l  
Q u e r y   E x p a n s i o n  
D o c u m e n t  
S e t  
I n i t i a l  R a n k i n g  
D o c u m e n t s  
F i n a l  R a n k i n g  
D o c u m e n t s  
 
 The rest of this paper is organized as 
following. In section 2, we give an overall 
introduction of our proposed method.  In 
section 3, we talk about what are Global Key 
Terms and what are Local Key Terms and how 
to acquire them. In section 4, we describe how 
these terms apply to Chinese IR system to 
improve the precision and quality of IR 
system. In section 5, we evaluate the 
performance of our proposed method and give 
some result analysis. In section 6, we present 
the conclusion and some future work. 
2 Overview of Document Reordering 
in Chinese IR 
For Chinese IR, many retrieval models, 
indexing strategies and query expansion 
strategies have been studied and successfully 
used in IR. Chinese Character, bi-gram, n-
gram (n>2) and word are the most used 
indexing units. (Li. P. 1999) gives out many 
research results on the effectiveness of single 
Chinese Character as   indexing unit and how 
to improve the effectiveness of single Chinese 
Character as indexing unit. (K.L. Kwok. 1997) 
compares three kinds of indexing units (single 
Character, bigram and short-words) and their 
effectiveness. It reports that single character 
indexing is good but not sufficiently 
competitive, while bi-gram indexing works 
surprisingly well and it?s as good as short-
word indexing in precision. (J.Y. Nie, J. Gao, 
J. Zhang and M. Zhou. 2000) suggests that 
word indexing and bi-gram indexing can 
achieve comparable performance but if we 
consider the time and space factors, then it is 
preferable to use words (and characters) as 
indexes. It also suggests that a combination of 
the longest-matching algorithm with single 
character is a good method for Chinese and if 
there is unknown word detection, the 
performance can be further improved. Many 
other papers in literature (Palmer, D. and 
Burger, J, 1997; Chien, L.F, 1995) give similar 
conclusions. Although there are still different 
voices on if bi-gram or word is the best 
indexing unit, bi-gram and word are both 
considered as the most important top two 
indexing units in Chinese IR and they are used 
in many reported Chinese IR systems and 
experiences.   
There are mainly two kinds of retrieval 
models: Vector Space Model (G. Salton and 
M. McGill, 1983) and Probabilistic Retrieval 
(N. Fuhr, 1992). They are both used in a lot of 
experiments and applications. 
For query expansion, almost all of the 
proposed strategies make use of the top N 
documents in initial ranking documents in the 
initial retrieval. Generally, query expansion 
strategy selects M indexing units (M<50) from 
the top N (N<25) documents in initial ranking 
documents according to some kind of measure 
and add these M indexing units to original 
query to form a new query. In such process of 
query expansion, it?s supposed that the top N 
documents are related with original query, but 
in practice, such an assumption is not always 
true. The Okapi approach (S.E. Roberson and 
S.Walker, 2001) supposes that the top R 
documents are related with query and it selects 
N indexing unit from the top R documents to 
form a new query, for example, R=10 and 
N=25. (M. Mitra., Amit. S. and Chris. B, 1998) 
did an experiment on different query topics 
and it is reported the effectiveness of query 
expansion mainly depends on the precision of 
the top N ranking documents. If the top N 
ranking documents are highly related with the 
original query, then query expansion can 
improve the final result. But if the top N 
documents are less related with the original 
query, query expansion cannot improve the 
final result or even reduces the precision of 
final result. These researches conclude that 
whether query expansion is successful or not 
mainly depends on the quality of top N ranking 
documents in the initial retrieval. 
The precision of top N documents in the 
initial ranking documents depends on indexing 
unit and retrieval models and mainly depends 
on indexing unit. As discussed above, bi-gram 
and word both are the most effective indexing 
units in Chinese IR.  
Other effort has been done to improve the 
precision of top N documents. (Qu. Y, 2002) 
proposed a method to re-rank initial relevant 
documents by using individual thesaurus but 
the thesaurus must be constructed manually 
and depends on each query topic.  
In this paper, we propose a new method to 
improve the precision of top N ranking 
documents in initial ranking documents by 
reordering the top M (M > N and M < 1000) 
ranking documents in initially retrieved 
documents. To reorder documents, we try to 
find long terms (more than 2 Chinese 
characters) that generally represent some 
complete concepts in query and documents, 
then we make use of these long terms to re-
weight the top M documents in initial ranking 
documents and reorder them by re-weighted 
value. We adopt a two-stage approach to 
acquire such kinds of long terms. Firstly, we 
acquire Global Key Terms from the whole 
document set; secondly, we use Global Key 
Terms to acquire Local Key Terms in a query 
or a document. After we have acquired Local 
Key Terms, we use them to re-weight the top M 
documents in initial ranking documents. Figure 
2 demonstrates the processes of an IR system 
that integrates with this new method.  
 
F i g .  2  E n h a n c e d  P r o c e s s  o f  I R  
O r i g i n a l  Q u e r y  
E x p a n d e d  Q u e r y  
I n i t i a l  R e t r i e v a l  
F i n a l  R e t r i e v a l  
Q u e r y  E x p a n s i o n  
D o c u m e n t  
S e t  
I n i t i a l  R a n k i n g  
D o c u m e n t s  
F i n a l  R a n k i n g  
D o c u m e n t s  
D o c u m e n t   
R e - O r d e r  
K e y  T e r m   
E x t r a c t i o n  
K e y  T e r m s  R e - O r d e r e d  
D o c u m e n t s  
 
3 Global/Local Key Term Extraction 
The Global /Local Key Term extraction 
concerns the problem of what is a key term. 
Intuitively, key terms in a document are some 
conceptual terms that are prominent in 
document and play main roles in 
discriminating the document from other 
documents. In other words, a key term in a 
document can represent part of the content of 
the document. Generally, from the point of the 
view of conventional linguistic studies, Key 
Terms may be some NPs, NP-Phrases or some 
kind of VPs, adjectives that can represent some 
specific concepts in document content 
representation.  
We define two kinds of Key Terms: Global 
Key Terms which are acquired from the whole 
document set and Local Key Terms which are 
acquired from a single document or a query.  
We adopt a two-stage approach to 
automatically acquire Global Key Terms and 
Local Key Terms. In the first stage, we acquire 
Global Key Terms from document set by using 
a seeding-and-expansion method. In the second 
stage, we make use of acquired Global Key 
Terms to find Local Key Terms in a single 
document or a query. 
3.1  Global Key Terms  
Global Key Terms are terms which are 
extracted from the whole document set and 
they can be regarded to represent the main 
concepts of document set.  
Although the definition of Global Key 
Terms is difficult, we try to give some 
assumptions about a Global Key Term. Before 
we give these assumptions, we first give out 
the definition of Seed and Key Term in a 
document (or document cluster) d. 
The concept Seed is given to reflect the 
prominence of a Chinese Character in a 
document (or document cluster) in some way. 
Suppose r is the reference document set 
(reference document set including document 
set and other statistical large document 
collection), d is a document (or a document 
set), w is an individual Chinese Character in d, 
let Pr(w) and Pd(w) be the probability of w 
occurring in r and d respectively, we adopt 1), 
relative probability or salience of w in d with 
respect to r (Schutze. 1998), as the criteria for 
evaluation of Seed.  
1) Pd(w) / Pr(w) 
We call w a Seed if Pd(w) / Pr(w)?? (?>1).  
   Now we give out the assumptions about a 
Key Terms in document d. 
 
i) a Key Term contains at least a Seed. 
ii) a Key Term occurs at least N (N>1) times in d. 
iii) the length of a Key Term is less than L (L<30). 
iv) a maximal character string meeting i), ii) and iii) is a 
Key Term. 
v) for a Key Term, a real maximal substring meeting i), 
ii) and iiI) without considering their occurrence in all 
those Key Terms containing it is also a Key Terms.  
 
Here a maximal character string meeting i), ii) 
and iii) refers to a adjacent Chinese character 
string meeting i), ii) and iii) while no other 
longer Chinese character strings containing it 
meet i), ii) and iii). A real maximal substring 
meeting i), ii) and iii) refers to a real substring 
meeting i), ii), and iii) while no other longer 
real substrings containing it meet i), ii) and iii). 
We use a kind of seeding-and-expansion-
based statistical strategy to acquire Key Terms 
in document (or document cluster), in which 
we first identify seeds for a Key Term then 
expand from it to get the whole Key Term. 
Fig. 3 describes the procedure to extract 
Key Terms from a document (or document 
cluster) d. 
 
let Fd(t) represents the frequency of t in d; 
let N is a given threshold (N>1); 
K = {}; 
collect Seeds in d into S; 
for all c?S 
{ 
    let Q = {t: t contains c and Fd(t)?N}; 
   while Q ? NIL 
   { 
    max-t  ? the longest string in Q; 
    K ? K + { max-t }; 
    Remove max-t  from Q; 
   for all other t in Q  
   {  
        if t is a substring of max-t  
       {    Fd(t)? Fd(t)- Fd(max-t); 
    if Fd(t)<N 
       removing t from Q; 
    } 
   } 
 } 
} 
return K as Key Terms in document d; 
 
Fig. 3 Key Term Extraction from document d
 
To acquire Global Key Terms, we first 
roughly cluster the whole document set r into 
K (K<2000) document clusters, then we regard 
each document cluster as a large document and  
apply our proposed Key Term Extraction 
algorithm (see Fig. 3) on each document 
cluster and respectively get Key Terms in each 
document cluster. All these Key Terms from 
document clusters form Global Key Terms.  
There are many document clustering 
approaches to cluster document set. K-Means 
and hierarchical clustering are the two usually 
used approaches. In our algorithm, we don?t 
need to use complicated clustering approaches 
because we only need to roughly cluster 
document set r into K document clusters. Here 
we use a simple K-Means approach to cluster 
document set. Firstly, we pick up randomly 
10*K documents from document set r; 
secondly, we use K-Means approach to cluster 
these 10*K documents into K document 
clusters; finally, we insert every other 
document into one of the K document clusters. 
Fig. 4 describes the general process to cluster 
document set r into K document clusters.  
 
let K is the number of documnet clusters to get;  
T?10*K documents randomly pickuped from r;
  
    
cluster T into K clusters {Kj} by using K-Means; 
for any document d in {r-T} 
{ 
    Ki? document cluster which has the maximal 
similarity with d; 
  insert d to document cluster Ki; 
} 
return K document clusters {Kj|1<=j<=K}; 
 
Fig. 4 Cluster document set r into K clusters 
 
Fig. 5 describes the procedure to acquire Global Key 
Terms from document set r. 
 
roughly cluster document set r to K document clusters 
{Kj|1<=j<=K} (See Fig. 4); 
 G = {}; 
 for each Kj  
 
{ 
     extract Key Terms g from Kj ; (See Fig. 3) 
     G ? G + g; 
} 
return G as Global Key Terms in document set r; 
 
Fig. 5 Global Key Terms Acquisition 
 
 In the processing of Global Key Terms 
acquisition, the frequency of each Global Key 
Term is also recorded for further use in 
identifying Local Key Terms - terms in a single 
document or query.  
 
3.2 Local Key Terms 
Unlike Global Key Terms, Local Key Terms 
are not extracted by using Key Term extraction 
algorithm from single document or query, they 
are identified based on Global Key Terms and 
their frequencies.  
Fig.6 describes the procedure of Local Key 
Terms acquisition from a single document or 
query d. 
 
  Given threshold M (M>10), N (N>100) and document d; 
L = {}; 
collect Global Key Terms occurred in d and their 
frequency in document set r into S = <c, tf>; 
for all <c, tf>?S 
{ 
      if  tf  < M  
             remove <c, tf> from S; 
  }; 
for all <c, tf>?S 
{ 
      if  c = c1c2  and  <c1, tf1>?S and <c2, tf2>?S 
 if (tf1 > tf *N  and tf2 >> tf*N)  
                     remove <c, tf> from S; 
  }; 
  while S ? NIL 
  { 
     let Q = {<t, tf>: t is the longest string is S}; 
        find <max-c,max-tf>
 
in Q where max-tf has the 
maximum value; 
  remove <max-t, max-tf>  from S; 
  if max-t occurs in d 
 {  L ? L + max-t; 
     remove all occurrance of max_t in d; 
    for all <b, tf-b>?S where b is a substring of  max-t;  
          if tf-b < max-tf  remove <b,tf-b>  from S; 
 } 
 }; 
 return L as Local Key Terms in document d; 
 
Fig. 6 Local Key Terms Acquisition 
 
Following are some examples of Global Key 
Terms and Local Key Terms in a query. 
 
Example: 
 
Query:  	

 
  
(Find information of the exhibition "Art and Culture of 
the Han Dynasty" in the National Palace Museum) 
Global Key Terms occurred in Query and their 
frequencies in document set: 
 
 (Cha2 Xun2)? 4948 

 (Gu4 Gong1)? 3456 
(Gu4 Gong1 Bo2 Wu4 Yuan4)? 727 
(Bo2 Wu4 Yuan4) ? 772 
 (Yuan4 Suo3) ? 2991 
	 (Zhu3 Ban4) ? 38698 
 (Qian1 Xi3)? 11510 
 (Han4 Dai4) ? 411 
 (Han4 Dai4 Wen3 Wu4) - 173 
 (Han4 Dai4 Wen3 Wu4 Da4 Zhan3) ? 
133 
 (Wen3 Wu4) ? 7088 
 (Wen3 Wu4 Da4 Zhan3) ? 158 
 (Da4 Zhan3) ? 2270 
 (Xiang3 Guan3) ? 67990 
  (Xiang3 Guan3 Nei3 Rong2) ? 148 
 (Nei3 Rong2) ? 31165 
Local Key Terms in Query: 
 (Han4 Dai4 Wen3 Wu4 Da4 Zhan3) 
 (Han4 Dai4 Wen3 Wu4) 
 (Wen3 Wu4) 
 (Da4 Zhan3) 
(Gu4 Gong1 Bo2 Wu4 Yuan4)  
(Bo2 Wu4 Yuan4) 
(Gu4 Gong1) 
 (Xiang3 Guan3) 
 (Nei3 Rong2) 
	 (Zhu3 Ban4) 
 (Qian1 Xi3) 
  (Cha2 Xun2) 
 
From the example, we can see the difference 
between Global Key Terms and Local Key 
Terms. For example,  (Yuan4 Suo3)  and
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 728?736, Prague, June 2007. c?2007 Association for Computational Linguistics
Tree Kernel-based Relation Extraction  
with Context-Sensitive Structured Parse Tree Information 
  
    GuoDong ZHOU12     Min ZHANG 2     Dong Hong JI 2     QiaoMing ZHU 1 
         1School of Computer Science & Technology         2  Institute for Infocomm Research 
                      Soochow Univ.                                              Heng Mui Keng Terrace 
         Suzhou, China 215006                                           Singapore 119613 
                 Email: {gdzhou,qmzhu}@suda.edu.cn      Email: {zhougd, mzhang, dhji}@i2r.a-star.edu.sg 
  
Abstract 
This paper proposes a tree kernel with context-
sensitive structured parse tree information for re-
lation extraction. It resolves two critical problems 
in previous tree kernels for relation extraction in 
two ways. First, it automatically determines a dy-
namic context-sensitive tree span for relation ex-
traction by extending the widely-used Shortest 
Path-enclosed Tree (SPT) to include necessary 
context information outside SPT. Second, it pro-
poses a context-sensitive convolution tree kernel, 
which enumerates both context-free and context-
sensitive sub-trees by considering their  ancestor 
node paths as their contexts. Moreover, this paper 
evaluates the complementary nature between our 
tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that 
our dynamic context-sensitive tree span is much 
more suitable for relation extraction than SPT and 
our tree kernel outperforms the state-of-the-art 
Collins and Duffy?s convolution tree kernel. It 
also shows that our tree kernel achieves much bet-
ter performance than the state-of-the-art linear 
kernels . Finally, it shows that feature-based and 
tree kernel-based methods much complement each 
other and the composite kernel can well integrate 
both flat and structured features.  
1 Introduction 
Relation extraction is to find various predefined se-
mantic relations between pairs of entities in text. The 
research in relation extraction has been promoted by 
the Message Understanding Conferences (MUCs) 
(MUC, 1987-1998) and the NIST Automatic Content 
Extraction (ACE) program (ACE, 2002-2005). Ac-
cording to the ACE Program, an entity is an object or 
a set of objects in the world and a relation is an ex-
plicitly or implicitly stated relationship among enti-
ties. For example, the sentence ?Bill Gates is the 
chairman and chief software architect of Microsoft 
Corporation.? conveys the ACE-style relation 
?EMPLOYMENT.exec? between the entities ?Bill 
Gates? (person name) and ?Microsoft Corporation? 
(organization name). Extraction of semantic relations 
between entities can be very useful in many applica-
tions such as question answering, e.g. to answer the 
query ?Who is the president of the United States??, 
and information  retrieval, e.g. to expand the query 
?George W. Bush? with ?the president of the United 
States? via his relationship with ?the United States?. 
Many researches have been done in relation extrac-
tion. Among them, feature-based methods (Kamb-
hatla 2004; Zhou et al, 2005) achieve certain success 
by employing a large amount of diverse linguistic 
features, varying from lexical knowledge, entity-
related information to syntactic parse trees, depend-
ency trees and semantic information. However, it is 
difficult for them to effectively capture structured 
parse tree information (Zhou et al2005), which is 
critical for further performance improvement in rela-
tion extraction.  
As an alternative to feature-based methods, tree 
kernel-based methods provide an elegant solution to 
explore implicitly structured features by directly 
computing the similarity between two trees. Although 
earlier researches (Zelenko et al2003; Culotta and 
Sorensen 2004; Bunescu and Mooney 2005a) only 
achieve success on simple tasks and fail on complex 
tasks, such as the ACE RDC task, tree kernel-based 
methods achieve much progress recently. As the 
state-of-the-art, Zhang et al(2006) applied the convo-
lution tree kernel (Collins and Duffy 2001) and 
achieved comparable performance with a state-of-the-
art linear kernel (Zhou et al2005) on the 5 relation  
types in the ACE RDC 2003 corpus.  
However, there are two problems in Collins and 
Duffy?s convolution tree kernel for relation extraction.  
The first is that the sub-trees enumerated in the tree 
kernel computation are context-free. That is, each 
sub-tree enumerated in the tree kernel computation 
728
does not consider the context information outside the 
sub-tree. The second is to decide a proper tree span in 
relation extraction. Zhang et al(2006) explored five 
tree spans in relation extraction and it was  a bit sur-
prising to find that the Shortest Path-enclosed Tree 
(SPT, i.e. the sub-tree enclosed by the shortest path 
linking two involved entities in the parse tree) per-
formed best. This is contrast to our intuition. For ex-
ample, ?got married? is critical to determine the 
relationship between ?John? and ?Mary? in the sen-
tence ?John and Mary got married? ? as shown in 
Figure 1(e). It is obvious that the information con-
tained in SPT (?John and Marry?) is not enough to 
determine their relationship. 
This paper proposes a context-sensitive convolu-
tion tree kernel for relation extraction to resolve the 
above two problems. It first automatically determines 
a dynamic context-sensitive tree span for relation ex-
traction by extending the Shortest Path-enclosed Tree 
(SPT) to include necessary context information out-
side SPT. Then it proposes a context-sensitive convo-
lution tree kernel, whic h not only enumerates context-
free sub-trees but also context-sensitive sub-trees by 
considering their ancestor node paths as their contexts. 
Moreover, this paper evaluates the complementary 
nature of different linear kernels and tree kernels via a 
composite kernel.  
The layout of this paper is as follows. In Section 2, 
we review related work in more details. Then, the 
dynamic context-sensitive tree span and the context-
sensitive convolution tree kernel are proposed in Sec-
tion 3 while Section 4 shows the experimental results. 
Finally, we conclude our work in Sec tion 5.  
2 Related Work 
The relation extraction task was first introduced as 
part of the Template Element task in MUC6 and then 
formulated as the Template Relation task in MUC7. 
Since then, many methods, such as feature-based 
(Kambhatla 2004; Zhou et al2005, 2006), tree ker-
nel-based (Zelenko et al2003; Culotta and Sorensen 
2004; Bunescu and Mooney 2005a; Zhang et al2006) 
and composite kernel-based (Zhao and Gris hman 
2005; Zhang et al2006), have been proposed in lit-
erature. 
For the feature-based methods, Kambhatla (2004) 
employed Maximum Entropy models to combine di-
verse lexical, syntactic and semantic features in rela-
tion extraction, and achieved the F-measure of 52.8 
on the 24 relation subtypes in the ACE RDC 2003 
corpus. Zhou et al(2005) further systematically ex-
plored diverse features through a linear kernel and 
Support Vector Machines, and achieved the F-
measures of 68.0 and 55.5 on the 5 relation types and 
the 24 relation subtypes in the ACE RDC 2003 cor-
pus respectively. One problem with the feature-based 
methods is that they need extensive feature engineer-
ing. Another problem is that, although they can ex-
plore some structured information in the parse tree 
(e.g. Kambhatla (2004) used the non-terminal path 
connecting the given two entities in a parse tree while 
Zhou et al (2005) introduced additional chunking 
features to enhance the performance), it is found dif-
ficult to well preserve structured information in the 
parse trees using the feature-based methods. Zhou et 
al (2006) further improved the performance by ex-
ploring the commonality among related classes in a 
class hierarchy using hierarchical learning strategy. 
As an alternative to the feature-based methods, the 
kernel-based methods (Haussler, 1999) have been 
proposed to implicitly explore various features in a 
high dimensional space by employing a kernel to cal-
culate the similarity between two objects directly. In 
particular, the kernel-based methods could be very 
effective at reducing the burden of feature engineer-
ing for structured objects in NLP researches, e.g. the 
tree structure in relation extraction.   
Zelenko et al (2003) proposed a kernel between 
two parse trees, which recursively matches nodes 
from roots to leaves in a top-down manner. For each 
pair of matched nodes, a subsequence kernel on their 
child nodes is invoked. They achieved quite success 
on two simple relation extraction tasks. Culotta and 
Sorensen (2004) extended this work to estimate simi-
larity between augmented dependency trees and 
achieved the F-measure of 45.8 on the 5 relation 
types in the ACE RDC 2003 corpus. One problem 
with the above two tree kernels is that matched nodes 
must be at the same height and have the same path to 
the root node. Bunescu and Mooney (2005a) pro-
posed a shortest path dependency tree kernel, which 
just sums up the number of common word classes 
at each position in the two paths, and achieved the 
F-measure of 52.5 on the 5 relation types in the ACE 
RDC 2003 corpus. They argued that the information 
to model a relationship between two entities can be 
typically captured by the shortest path between them 
in the dependency graph. While the shortest path 
may not be able to well preserve structured de-
pendency tree information, another problem with 
their kernel is that the two paths should have same 
length. This makes it suffer from the similar behavior 
with that of Culotta and Sorensen (2004): high preci-
sion but very low recall.  
As the state-of-the-art tree kernel-based method, 
Zhang et al(2006) explored various structured feature 
729
spaces and used the convolution tree kernel over 
parse trees (Collins and Duffy 2001) to model syntac-
tic structured information for relation extraction. 
They achieved the F-measures of 61.9 and 63.6 on the 
5 relation types of the ACE RDC 2003 corpus and the 
7 relation types of the ACE RDC 2004 corpus respec-
tively without entity-related information while the F-
measure on the 5 relation types in the ACE RDC 
2003 corpus reached 68.7 when entity-related infor-
mation was included in the parse tree. One problem 
with Collins and Duffy?s convolution tree kernel is 
that the sub-trees involved in the tree kernel computa-
tion are context-free, that is, they do not consider the 
information outside the sub-trees. This is different 
from the tree kernel in Culota and Sorensen (2004), 
where the sub-trees involved in the tree kernel com-
putation are context-sensitive (that is, with the path 
from the tree root node to the sub-tree root node in 
consideration). Zhang et al(2006) also showed that 
the widely-used Shortest Path-enclosed Tree (SPT) 
performed best. One problem with SPT is that it fails 
to capture the contextual information outside the 
shortest path, which is important for relation extrac-
tion in many cases. Our random selection of 100 pos i-
tive training instances from the ACE RDC 2003 
training corpus shows that ~25% of the cases need 
contextual information outside the shortest path. 
Among other kernels, Bunescu and Mooney (2005b) 
proposed a subsequence kernel and applied it in pro-
tein interaction and ACE relation extraction tasks. 
In order to integrate the advantages of feature-
based and tree kernel-based methods, some research-
ers have turned to composite kernel-based methods. 
Zhao and Grishman (2005) defined several feature-
based composite kernels to integrate diverse features 
for relation extraction and achieved the F-measure of 
70.4 on the 7 relation types of the ACE RDC 2004 
corpus. Zhang et al(2006) proposed two composite 
kernels to integrate a linear kernel and Collins and 
Duffy?s convolution tree kernel. It achieved the F-
measure of 70.9/57.2 on the 5 relation types/24 rela-
tion subtypes in the ACE RDC 2003 corpus and the 
F-measure of 72.1/63.6 on the 7 relation types/23 
relation subtypes in the ACE RDC 2004 corpus. 
The above discussion suggests that structured in-
formation in the parse tree may not be fully utilized in 
the previous works, regardless of feature-based, tree 
kernel-based or composite kernel-based methods. 
Compared with the previous works, this paper pro-
poses a dynamic context-sensitive tree span trying to 
cover necessary structured information and a context-
sensitive convolution tree kernel considering both 
context-free and context-sensitive sub-trees. Further-
more, a composite kernel is applied to combine our 
tree kernel and a state-of-the-art linear kernel for in-
tegrating both flat and structured features in relation 
extraction as well as validating their complementary 
nature. 
3 Context Sensitive Convolution Tree 
Kernel for Relation Extraction 
In this section, we first propose an algorithm to dy-
namically determine a proper context-sensitive tree 
span and then a context-sensitive convolution tree 
kernel for relation extraction.  
3.1 Dynamic Context-Sensitive Tree Span in 
Relation Extraction 
A relation instance between two entities is encaps u-
lated by a parse tree. Thus, it is critical to understand 
which portion of a parse tree is important in the tree 
kernel calculation. Zhang et al(2006) systematically 
explored seven different tree spans, including the 
Shortest Path-enclosed Tree (SPT) and a Context-
Sensitive Path-enclosed Tree1 (CSPT), and found that 
SPT per formed best. That is, SPT even outperforms 
CSPT. This is contrary to our intuition. For example, 
?got married? is critical to determine the relationship 
between ?John? and ?Mary? in the sentence ?John 
and Mary got married? ? as shown in Figure 1(e), 
and the information contained in SPT (?John and 
Mary?) is not enough to determine their relationship. 
Obviously, context-sensitive tree spans should have 
the potential for better performance. One problem 
with the context-sensitive tree span explored in Zhang 
et al(2006) is that it only considers the availability of 
entities? siblings and fails to consider following two 
factors: 
1) Whether is the information contained in SPT 
enough to determine the relationship between 
two entities? It depends. In the embedded cases, 
SPT is enough. For example, ?John?s wife? is 
enough to determine the relationship between 
?John? and ?John?s wife? in the sentence ?John?s 
wife got a good job? ? as shown in Figure 1(a) . 
However, SPT is not enough in the coordinated 
cases, e.g. to determine the relationship between 
?John? and ?Mary? in the sentence ?John and 
Mary got married? ? as shown in Figure 1(e). 
                                                               
1 CSPT means SPT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the node 
of entity 2.  In the case of no available  sibling, it moves 
to the parent of current node and repeat the same proc-
ess until a sibling is available or the root is reached. 
730
2) How can we extend SPT to include necessary 
context information if there is no enough infor-
mation in SPT for relation extraction?  
To answer the above two questions, we randomly 
chose 100 positive instances from the ACE RDC 
2003 training data and studied their necessary tree 
spans. It was observed that we can classify them into 
5 categories: 1) embedded (37 instances), where one 
entity is embedded in another entity, e.g. ?John? and 
?John?s wife? as shown in Figure 1(a); 2) PP-linked 
(21 instances), where one entity is linked to another 
entity via PP attachment, e.g. ?CEO? and ?Microsoft? 
in the sentence ?CEO of Microsoft announced ? ? as 
shown in Figure 1(b); 3) semi-structured (15 in-
stances), where the sentence consists of a sequence of 
noun phrases (including the two given entities), e.g. 
?Jane? and ?ABC news? in the sentence ?Jane, ABC 
news, California.? as shown in Figure 1(c); 4) de-
scriptive (7 instances), e.g. the citizenship between 
?his mother? and ?Lebanese? in the sentence ?his 
mother Lebanese landed at ?? as shown in Figure 
1(d); 5) predicate-linked and others (19 instances, 
including coordinated cases), where the predicate 
information is necessary to determine the relationship 
between two entities, e.g.  ?John? and ?Mary? in the 
sentence ?John and Mary got married?? as shown in 
Figure 1(e); 
Based on the above observations, we implement an 
algorithm to determine the necessary tree span for the 
relation extract task. The idea behind the algorithm is 
that the necessary tree span for a relation should be 
determined dynamically according to its tree span 
category and context. Given a parsed tree and two 
entities in consideration, it first determin es the tree 
span category and then extends the tree span accord-
ingly. By default, we adopt the Shortest Path-
enclosed Tree (SPT) as our tree span. We only ex-
pand the tree span when the tree span belongs to the 
?predicate-linked? category. This is based on our ob-
servation that the tree spans belonging to the ?predi-
cate-linked? category vary much syntactically and 
majority (~70%) of them need information outside 
SPT while it is quite safe (>90%) to use SPT as the 
tree span for the remaining categories. In our algo-
rithm, the expansion is done by first moving up until 
a predicate-headed phrase is found and then moving 
down along the predicated-headed path to the predi-
cate terminal node. Figure 1(e) shows an example for 
the ?predicate-linked? category where the lines with 
arrows indicate the expansion path.  
 
   
 
e) predicate-linked: SPT and the dynamic context-sensitive tree span  
Figure 1: Different tree span categories with SPT (dotted circle) and an ex-
ample of the dynamic context-sensitive tree span (solid circle) 
  
 
Figure 2: Examples of context-
free and context-sensitive sub-
trees related with Figure 1(b). 
Note: the bold node is the root 
for a sub-tree. 
A problem with our algorithm is how to deter-
mine whether an entity pair belongs to the ?predi-
cate-linked? category. In this paper, a simple 
method is applied by regarding the ?predicate-
linked? category as the default category. That is, 
those entity pairs, which do not belong to the four 
well defined and easily detected categories (i.e. 
embedded, PP-liked, semi-structured and descrip-
tive), are classified into the ?predicate-linked? cate-
gory. 
His mother Lebanese  landed 
PRP$ NNP VBD IN 
NP-E1-PER NP-E2-GPE PP 
S 
d)  descriptive 
NP 
NN 
at 
?  
VP 
Jane ABC news ,  
NNP , NNP NNS , NNP . 
NP NP-E1-PER NP-E2-ORG
NP 
c) semi-structured  
California . . 
, 
, 
, 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
PP(IN)-subroot 
b) context -sensitive 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
S(VBD) 
PP(IN)-subroot 
c) context -sensitive 
PP(IN)-subtoot 
NP-E2-ORG 
of Microsoft 
IN NNP 
a) context -free 
?  
NP 
John and Mary  got 
NNP CC NNP VBD 
married  
NP-E1-PER NP-E2-PER VP 
S 
VP 
VBN ?  
John and Mary  got 
NNP CC NNP VBD  
married 
NP-E1-PER NP-E2-PER VP 
 
NP VP 
 
?  
NP 
CEO of Microsoft announced 
NN IN NNP VBD ?  
NP-E1-PER NP-E2-ORG 
VP 
S 
b)  PP -linked  
PP 
?  
John ?s wife found a  job 
NNP POS NN VBD DT JJ NN 
NP NP-E1-PER 
NP-E2-PER VP 
S 
a) embedded  
good 
731
Since ?predicate -linked? instances only occupy 
~20% of cases, this explains why SPT performs 
better than the Context-Sensitive Path-enclosed 
Tree (CSPT) as described in Zhang et al(2006): 
consistently adopting CSPT may introduce too 
much noise/unnecessary information in the tree 
kernel. 
3.2 Context-Sensitive Convolution Tree Kernel 
Given any tree span, e.g. the dynamic context-
sensitive tree span in the last subsection, we now 
study how to measure the similarity between two 
trees, using a convolution tree kernel.A convolution 
kernel (Haussler D., 1999) aims to capture structured 
information in terms of substructures . As a special-
ized convolution kernel, Collins and Duffy?s convolu-
tion tree kernel ),( 21 TTKC  (?C? for convolution) 
counts the number of common sub-trees (sub-
structures) as the syntactic structure similarity be-
tween two parse trees T1 and T2 (Collins and Duffy 
2001): 
?
??
D=
2211 ,
2121 ),(),(
NnNn
C nnTTK    (1) 
where Nj is the set of nodes in tree Tj , and 1 2( , )n nD  
evaluates the common sub-trees rooted at n1 and n2 2 
and is computed recursively as follows:  
1) If the context-free productions (Context-Free 
Grammar(CFG) rules) at 1n  and 2n  are different, 
1 2( , ) 0n nD = ; Otherwise go to 2. 
2) If both 1n  and 2n  are POS tags, 1 2( , ) 1n n lD = ? ; 
Otherwise go to 3. 
3)  Calculate 1 2( , )n nD recursively as: 
?
=
D+=D
)(#
1
2121
1
)),(),,((1(),(
nch
k
knchknchnn l  (2) 
where )(# nch is the number of children of node n , 
),( knch  is the k th child of node n  andl (0< l <1) is 
the decay factor in order to make the kernel value less 
variable with respect to different sub-tree sizes.  
This convolution tree kernel has been successfully 
applied by Zhang et al(2006) in relation extraction. 
However, there is one problem with this tree kernel: 
the sub-trees involved in the tree kernel computation 
are context-free (That is, they do not consider the 
information outside the sub-trees). This is contrast to 
                                                               
2 That is, each node n encodes the identity of a sub-
tree rooted at n and, if there are two nodes in the 
tree with the same label, the summation will go over 
both of them. 
the tree kernel proposed in Culota and Sorensen 
(2004) which is context-sensitive, that is, it considers 
the path from the tree root node to the sub-tree root 
node. In order to integrate the advantages of both tree 
kernels and resolve the problem in Collins and 
Duffy?s convolution tree kernel, this paper proposes a 
context-sensitive convolution tree kernel. It works by 
taking ancestral information (i.e. the root node path) 
of sub-trees into consideration: 
? ?
= ??
D=
m
i NnNn
ii
C
iiii
nnTTK
1 ]2[]2[],1[]1[
11
1111
])2[],1[(])2[],1[(  (3) 
Where 
? ][1 jN i is the set of root node paths with length i 
in tree T[j] while the maximal length of a root 
node path is defined by m.  
? ])[...(][ 211 jnnnjn ii = is a root node path with 
length i in tree T[j] , which takes into account the 
i-1 ancestral nodes in2 [j] of 1n [j] in T[j]. Here, 
][1 jn k+  is the parent of ][ jn k and ][1 jn  is the 
root node of a context-free sub-tree in T[j]. For 
better differentiation, the label of each ancestral 
node in in1 [j] is augmented with the POS tag of 
its head word.  
? ])2[],1[( 11 ii nnD  measures the common context-
sensitive sub-trees rooted at root node paths 
]1[1in  and ]2[1in
3. In our tree kernel, a sub-tree 
becomes context-sensitive with its dependence on 
the root node path instead of the root node itself. 
Figure 2 shows a few examples of context-
sensitive sub-trees with comparison to context-
free sub-trees. 
Similar to Collins and Duffy (2001),   our tree ker-
nel computes ])2[],1[( 11 ii nnD recursively as follows:  
1) If the context-sensitive productions (Context-
Sensitive Grammar (CSG) rules with root node 
paths as their left hand sides) rooted at ]1[1in  and 
]2[1
in  are different, return ])2[],1[( 11
ii nnD =0; 
Otherwise go to Step 2. 
2) If both ]1[1n  and ]2[1n  are POS tags, 
l=D ])2[],1[( 11 ii nn ; Otherwise go to Step 3. 
                                                               
3 That is, each root node path in1  encodes the identity 
of a context-sensitive sub-tree rooted at in1  and, if 
there are two root node paths in the tree with the 
same label sequence, the summation will go over 
both of them.  
732
3) Calculate ])2[],1[( 11 ii nnD  recursively as: 
?
=
D+=
D
])1[(#
1
11
11
1
))],2[(),],1[((1(
])2[],1[(
inch
k
ii
ii
knchknch
nn
l
 (4) 
where ])],[( 1 kjnch i  is the k
th context-sensitive 
child of the context-sensitive sub-tree rooted at 
][1 jn i  with ])[(# 1 jnch i the number of the con-
text-sensitive children. Here, l (0< l <1) is the 
decay factor in order to make the kernel value 
less variable with respect to different sizes of the 
context-sensitive sub-trees. 
It is worth comparing our tree kernel with previous 
tree kernels. Obviously, our tree kernel is an exten-
sion of Collins and Duffy?s convolution tree kernel, 
which is a special case of our tree kernel (if m=1 in 
Equation (3)). Our tree kernel not only counts the 
occurrence of each context-free sub-tree, which does 
not consider its ancestors, but also counts the occur-
rence of each context-sensitive sub-tree, which con-
siders its ancestors. As a result, our tree kernel is not 
limited by the constraints in previous tree kernels (as 
discussed in Section 2), such as Collins and Duffy 
(2001), Zhang et al(2006), Culotta and Sorensen 
(2004) and Bunescu and Mooney (2005a). Finally, 
let?s study the computational issue with our tree ker-
nel. Although our tree kernel takes the context-
sensitive sub-trees into consideration, it only slightly 
increases the computational burden, compared with 
Collins and Duffy?s convolution tree kernel. This is 
due to that 0])2[],1[( 11 =D nn  holds for the major-
ity of context-free sub-tree pairs (Collins and Duffy 
2001) and that computation for context-sensitive sub-
tree pairs is necessary only when 
0])2[],1[( 11 ?D nn  and the context-sensitive sub-
tree pairs have the same root node path(i.e. 
]2[]1[ 11 ii nn =  in Equation (3)). 
4 Experimentation 
This paper uses the ACE RDC 2003 and 2004 cor-
pora provided by LDC in all our experiments. 
4.1 Experimental Setting  
The ACE RDC corpora are gathered from various 
newspapers, newswire and broadcasts. In the 2003 
corpus , the training set consists of 674 documents and 
9683 positive relation instances w hile the test set con-
sists of 97 documents and 1386 positive relation in-
stances. The 2003 corpus defines 5 entity types, 5 
major relation types and 24 relation subtypes. All the 
reported performances in this paper on the ACE RDC 
2003 corpus are evaluated on the test data. The 2004 
corpus  contains 451 documents and 5702 positive 
relation instances. It redefines 7 entity types, 7 major 
relation types and 23 relation subtypes. For compari-
son, we use the same setting as Zhang et al(2006) by 
applying a 5-fold cross-validation on a subset of the 
2004 data, containing 348 documents and 4400 rela-
tion instances. That is, all the reported performances 
in this paper on the ACE RDC 2004 corpus are evalu-
ated using 5-fold cross validation on the entire corpus . 
Both corpora are parsed using Charniak?s parser 
(Charniak, 2001) with the boundaries of all the entity 
mentions kept 4 . We iterate over all pairs of entity 
mentions occurring in the same sentence to generate 
potential relation instances5. In our experimentation, 
SVM (SVMLight, Joachims(1998)) is selected as our 
classifier. For efficiency, we apply the one vs. others 
strategy, which builds K classifiers so as to separate 
one class from all others. The training parameters are 
chosen using cross-validation on the ACE RDC 2003 
training data.  In particular, l  in our tree kernel is 
fine-tuned to 0.5. This suggests that about 50% dis-
count is done as our tree kernel moves down one 
level in computing ])2[],1[( 11 ii nnD .  
4.2 Experimental Results  
First, we systematically evaluate the context-sensitive 
convolution tree kernel and the dynamic context-
sensitive tree span proposed in this paper. 
Then, we evaluate the complementary nature be-
tween our tree kernel and a state-of-the-art linear ker-
nel via a composite kernel. Generally different 
feature-based methods and tree kernel-based methods 
have their own merits. It is usually easy to build a 
system using a feature-based method and achieve the 
state-of-the-art performance, while tree kernel-based 
methods  hold the potential for further performance 
improvement. Therefore, it is always a good idea to 
integrate them via a composite kernel.  
                                                               
4 This can be done by first representing all entity men-
tions with their head words and then restoring all the 
entity mentions after parsing. Moreover, please note 
that the final performance of relation extraction may 
change much with different range of parsing errors. 
We will study this issue in the near future. 
5 In this paper, we only measure the performance of rela-
tion extraction on ?true? mentions with ?true? chain-
ing of co-reference (i.e. as annotated by LDC 
annotators ). Moreover, we only model explicit relations and 
explicitly model the argument order of the two mentions in-
volved. 
733
Finally, we compare our system with the state-of-
the-art systems in the literature.  
Context-Sensitive Convolution Tree Kernel 
In this paper, the m parameter of our context-sensitive 
convolution tree kernel as shown in Equation (3) 
indicates the maximal length of root node paths and is 
optimized to 3 using 5-fold cross validation on the 
ACE RDC 2003 training data. Table 1 compares the 
impact of different m in context-sensitive convolution 
tree kernels using the Shortest Path-enclosed Tree 
(SPT) (as described in Zhang et al(2006)) on the 
major relation types of the ACE RDC 2003 and 2004 
corpora, in details. It also shows that our tree kernel 
achieves best performance on the test data using SPT 
with m = 3, which outperforms the one with m = 1 by 
~2.3 in F-measure. This suggests the parent and 
grandparent nodes of a sub-tree  contains much 
information for relation extraction while considering 
more ancestral nodes may not help. This may be due 
to that, although our experimentation on the 
training data indicates that  more than 80% (on 
average) of subtrees has a root node path longer 
than 3 (since most of the subtrees are deep from the 
root node and more than 90% of the parsed trees in 
the training data are deeper than 6 levels), 
including a root node path longer than 3 may be 
vulnerable to the full parsing errors and have 
negative impact. Table 1 also evaluates the impact of 
entity-related information in our tree kernel by 
attaching entity type information (e.g. ?PER? in the 
entity node 1 of Figure 1(b)) into both entity nodes. 
It shows that such information can significantly 
improve the performance by ~6.0 in F-measure. In all 
the following experiments, we will apply our tree 
kernel with m=3 and entity-related information by 
default. 
Table 2 compares the dynamic context-sensitive 
tree span with SPT using our tree kernel. It shows that 
the dynamic tree span can futher improve the 
performance by ~1.2 in F-measure6. This suggests the 
usefulness of extending the tree span beyond SPT for 
the ?predicate-linked? tree span category. In the 
future work, we will further explore expanding the 
dynamic tree span beyond SPT for the remaining tree 
span categories. 
  
  
  
                                                               
6 Significance test shows that the dynamic tree span per-
forms s tatistically significantly better than SPT with p-
values smaller than 0.05. 
m P(%) R(%) F 
1 72.3(72.7)  56.6(53.8) 63.5(61.8)  
2 74.9(75.2)  57.9(54.7) 65.3(63.5)  
3 75.7(76.1)  58.3(55.1) 65.9(64.0)  
4 76.0(75.9)  58.3(55.3) 66.0(63.9)  
a) without entity-related information 
m P(%) R(%) F 
1 77.2(76.9)  63.5(60.8) 69.7(67.9)  
2 79.1(78.6)  65.0(62.2) 71.3(69.4)  
3 79.6(79.4)  65.6(62.5) 71.9(69.9)  
4 79.4(79.1)  65.6(62.3) 71.8(69.7)  
b) with entity-related information 
Table 1: Evaluation of context-sensitive convolution 
tree kernels using SPT on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 
Tree Span P(%) R(%) F 
Shortest Path-  
enclosed Tree 
79.6 
(79.4) 
65.6 
(62.5) 
71.9 
(69.9) 
Dynamic Context- 
Sensitive Tee 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Table 2: Comparison of dynamic context-sensitive 
tree span with SPT using our context-sensitive 
convolution tree kernel on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 18% of positive 
instances in the ACE RDC 2003 test data belong to 
the predicate-linked category. 
  
Composite Kernel 
In this paper, a composite kernel via polynomial in-
terpolation, as described Zhang et al(2006), is ap-
plied to integrate the proposed context-sensitive 
convolution tree kernel with a state-of-the-art linear 
kernel (Zhou et al2005) 7: 
),()1(),(),(1 ???-+???=?? CPL KKK aa  (5) 
Here, ),( ??LK  and ),( ??CK  indicates the normal-
ized linear kernel and context-sensitive convolution 
tree kernel respectively while  ( , )pK ? ?  is the poly-
nomial expansion of ( , )K ? ?  with degree d=2, i.e. 
2( , ) ( ( , ) 1)pK K? ? ? ?= +  and a  is the coefficient (a  is 
set to 0.3 using cross-validation). 
                                                               
7 Here, we use the same set of flat features (i.e. word, 
entity type, mention level, overlap, base phrase chunk-
ing, dependency tree, parse tree and semantic informa-
tion) as Zhou et al(2005). 
734
Table 3 evaluates the performance of the 
composite kernel. It shows that the composite kernel 
much further improves the performance beyond that 
of either the state-of-the-art linear kernel or our tree 
kernel and achieves the F-measures of 74.1 and 75.8 
on the major relation types of the ACE RDC 2003 
and 2004 corpora respectively. This suggests that our 
tree kernel and the state-of-the-art linear kernel are 
quite complementary, and that our composite kernel 
can effectively integrate both flat and structured 
features. 
System P(%) R(%) F 
Linear Kernel 78.2 (77.2) 
63.4 
(60.7) 
70.1 
(68.0) 
Context-Sensitive Con-
volution Tree Kernel 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Composite Kernel 82.2 (80.8) 
70.2 
(68.4) 
75.8 
(74.1) 
Table 3: Performance of the compos ite kernel via 
polynomial interpolation on the major relation types 
of the ACE RDC 2003 (inside the parentheses) and 
2004 (outside the parentheses) corpora 
  
Comparison with Other Systems  
ACE RDC 2003 P(%) R(%) F 
Ours:  
composite kernel 
80.8 
(65.2) 
68.4 
(54.9) 
74.1 
(59.6) 
Zhang et al(2006):  
composite kernel 
77.3 
(64.9) 
65.6 
(51.2) 
70.9 
(57.2) 
Ours: context-sensitive  
convolution tree kernel 
80.1 
(63.4) 
63.8 
(51.9) 
71.0 
(57.1) 
Zhang et al(2006):  
convolution tree kernel 
76.1 
(62.4) 
62.6 
(48.5) 
68.7 
(54.6) 
Bunescu et al(2005):  
shortest path  
dependency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
Culotta et al(2004):  
dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
Zhou et al (2005):  
feature-based 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5) 
Kambhatla (2004):  
feature-based 
- 
(63.5) 
- 
(45.2) 
- 
(52.8) 
Table 4: Comparison of difference systems on the 
ACE RDC 2003 corpus over both 5 types (outside the 
parentheses) and 24 subtypes (inside the parentheses) 
  
  
  
ACE RDC 2004 P(%) R(%) F 
Ours:  
composite kernel 
82.2 
(70.3) 
70.2 
(62.2) 
75.8 
(66.0) 
Zhang et al(2006):  
composite kernel 
76.1 
(68.6) 
68.4 
(59.3) 
72.1 
(63.6) 
Zhao et al(2005):8  
composite kernel 
69.2 
(-) 
70.5 
(-) 
70.4 
(-) 
Ours: context-sensitive  
convolution tree kernel 
81.1 
(68.8) 
66.7 
(60.3) 
73.2 
(64.3) 
Zhang et al(2006):  
convolution tree kernel 
72.5 
(-) 
56.7 
(-) 
63.6 
(-) 
Table 5: Comparison of difference systems on the 
ACE RDC 2004 corpus over both 7 types (outside the 
parentheses) and 23 subtypes (inside the parentheses) 
  
Finally, Tables 4 and 5 compare our system with 
other state-of-the-art systems9 on the ACE RDC 2003 
and 2004 corpora, respectively. They show that our 
tree kernel-based system outperforms previous tree 
kernel-based systems. This is largely due to the con-
text-sensitive nature of our tree kernel which resolves 
the limitations of the previous tree kernels. They also 
show that our tree kernel-based system outperforms 
the state-of-the-art feature-based system. This proves 
the great potential inherent in the parse tree structure 
for relation extraction and our tree kernel takes a big 
stride towards the right direction. Finally, they also 
show that our composite kernel-based system outper-
forms other composite kernel-based systems. 
5 Conclusion 
Structured parse tree information holds great potential 
for relation extraction. This paper proposes a context-
sensitive convolution tree kernel to resolve two criti-
cal problems in previous tree kernels for relation ex-
traction by first automatically determining a dynamic 
context-sensitive tree span and then applying a con-
text-sensitive convolution tree kernel. Moreover, this 
paper evaluates the complementary nature between 
our tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that our 
dynamic context-sensitive tree span is much more 
suitable for relation extraction than the widely -used 
Shortest Path-enclosed Tree and our tree kernel out-
performs the state-of-the-art Collins and Duffy?s con-
volution tree kernel. It also shows that feature-based 
                                                               
8 There might be some typing errors for the performance 
reported in Zhao and Grishman(2005) since P, R and F 
do not match. 
9 All the state-of-the-art systems apply the entity-related 
information. It is not supervising: our experiments 
show that using the entity-related information gives a 
large performance improvement.  
735
and tree kernel-based methods well complement each 
other and the composite kernel can effectively inte-
grate both flat and structured features.  
To our knowledge, this is the first research to dem-
onstrate that, without extensive feature engineer ing, 
an individual tree kernel can achieve much better per-
formance than the state-of-the-art linear kernel in re-
lation extraction. This shows the great potential of 
structured parse tree information for relation extrac-
tion and our tree kernel takes a big stride towards the 
right direction.  
For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by ex-
ploring more useful context information. Moreover, 
we will explore more entity-related information in the 
parse tree. Our preliminary work of including the en-
tity type information significantly improves the per-
formance. Finally, we will study how to resolve the 
data imbalance and sparseness issues from the learn-
ing algorithm viewpoint.  
Acknowledgement 
This research is supported by Project 60673041 under 
the National Natural Science Foundation of China 
and Project 2006AA01Z147 under the ?863? National 
High-Tech Research and Development of China. We 
would also like to thank the critical and insightful 
comments from the four anonymous reviewers. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Bunescu R. & Mooney R.J. (2005a). A shortest path 
dependency kernel for relation extraction. 
HLT/EMNLP?2005 : 724-731. 6-8 Oct 2005. Van-
cover, B.C. 
Bunescu R. & Mooney R.J. (2005b). Subsequence Ker-
nels for Relation Extraction  NIPS?2005. Vancouver, 
BC, December 2005  
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137. Toulouse, France 
Collins M. and Duffy N. (2001). Convolution Ke rnels 
for Natural Language. NIPS?2001: 625-632. Ca m-
bridge, MA 
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. ACL?2004 . 423-429. 
21-26 July 2004. Ba rcelona, Spain. 
Haussler D. (1999). Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz 
Joachims T. (1998). Text Categorization with Su pport 
Vector Machine: learning with many relevant fea-
tures. ECML-1998 : 137-142.  Chemnitz, Germany 
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. ACL?2004(Poster). 178-181. 21-
26 July 2004. Barcelona, Spain. 
MUC. (1987-1998). The NIST MUC website: http: 
//www.itl.nist.gov/iaui/894.02/related_projects/muc/ 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083-1106. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). A 
Composite Kernel to Extract Relations between Enti-
ties with both Flat and Structured Features . COLING-
ACL-2006: 825-832. Sydney, Australia 
Zhao S.B. and Grishman R. (2005). Extracting relations 
with integrated information using kernel methods. 
ACL?2005: 419-426. Univ of Michigan-Ann Arbor,  
USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Ex-
ploring various knowledge in relation extraction. 
ACL?2005. 427-434. 25-30 June, Ann Arbor, Mich-
gan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling com-
monality among related classes in relation extraction, 
COLING-ACL?2006: 121-128. Sydney, Australia. 
736
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 907?914, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Semi-Supervised Feature Clustering Algorithm
with Application to Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
In this paper we investigate an applica-
tion of feature clustering for word sense
disambiguation, and propose a semi-
supervised feature clustering algorithm.
Compared with other feature clustering
methods (ex. supervised feature cluster-
ing), it can infer the distribution of class
labels over (unseen) features unavailable
in training data (labeled data) by the use of
the distribution of class labels over (seen)
features available in training data. Thus,
it can deal with both seen and unseen fea-
tures in feature clustering process. Our ex-
perimental results show that feature clus-
tering can aggressively reduce the dimen-
sionality of feature space, while still main-
taining state of the art sense disambigua-
tion accuracy. Furthermore, when com-
bined with a semi-supervised WSD algo-
rithm, semi-supervised feature clustering
outperforms other dimensionality reduc-
tion techniques, which indicates that using
unlabeled data in learning process helps to
improve the performance of feature clus-
tering and sense disambiguation.
1 Introduction
This paper deals with word sense disambiguation
(WSD) problem, which is to assign an appropriate
sense to an occurrence of a word in a given context.
Many corpus based statistical methods have been
proposed to solve this problem, including supervised
learning algorithms (Leacock et al, 1998; Towel and
Voorheest, 1998), weakly supervised learning algo-
rithms (Dagan and Itai, 1994; Li and Li, 2004; Mi-
halcea, 2004; Niu et al, 2005; Park et al, 2000;
Yarowsky, 1995), unsupervised learning algorithms
(or word sense discrimination) (Pedersen and Bruce,
1997; Schu?tze, 1998), and knowledge based algo-
rithms (Lesk, 1986; McCarthy et al, 2004).
In general, the most common approaches start by
evaluating the co-occurrence matrix of features ver-
sus contexts of instances of ambiguous word, given
sense-tagged training data for this target word. As
a result, contexts are usually represented in a high-
dimensional sparse feature space, which is far from
optimal for many classification algorithms. Further-
more, processing data lying in high-dimensional fea-
ture space requires large amount of memory and
CPU time, which limits the scalability of WSD
model to very large datasets or incorporation of
WSD model into natural language processing sys-
tems.
Standard dimentionality reduction techniques in-
clude (1) supervised feature selection and super-
vised feature clustering when given labeled data, (2)
unsupervised feature selection, latent semantic in-
dexing, and unsupervised feature clustering when
only unlabeled data is available. Supervised fea-
ture selection improves the performance of an ex-
amplar based learning algorithm over SENSEVAL-
2 data (Mihalcea, 2002), Naive Bayes and deci-
sion tree over SENSEVAL-1 and SENSEVAL-2 data
(Lee and Ng, 2002), but feature selection does not
improve SVM and Adaboost over SENSEVAL-1
and SENSEVAL-2 data (Lee and Ng, 2002) for
word sense disambiguation. Latent semantic in-
dexing (LSI) studied in (Schu?tze, 1998) improves
the performance of sense discrimination, while un-
supervised feature selection also improves the per-
formance of word sense discrimination (Niu et al,
2004). But little work is done on using feature clus-
tering to conduct dimensionality reduction for WSD.
This paper will describe an application of feature
907
clustering technique to WSD task.
Feature clustering has been extensively studied
for the benefit of text categorization and document
clustering. In the context of text categorization, su-
pervised feature clustering algorithms (Baker and
McCallum, 1998; Bekkerman et al, 2003; Slonim
and Tishby, 2001) usually cluster words into groups
based on the distribution of class labels over fea-
tures, which can compress the feature space much
more aggressively while still maintaining state of
the art classification accuracy. In the context of
document clustering, unsupervised feature cluster-
ing algorithms (Dhillon, 2001; Dhillon et al, 2002;
Dhillon et al, 2003; El-Yaniv and Souroujon, 2001;
Slonim and Tishby, 2000) perform word clustering
by the use of word-document co-occurrence matrix,
which can improve the performance of document
clustering by clustering documents over word clus-
ters.
Supervised feature clustering algorithm groups
features into clusters based on the distribution of
class labels over features. But it can not group un-
seen features (features that do not occur in labeled
data) into meaningful clusters since there are no
class labels associated with these unseen features.
On the other hand, while given labeled data, un-
supervised feature clustering method can not uti-
lize class label information to guide feature cluster-
ing procedure. While, as a promising classification
strategy, semi-supervised learning methods (Zhou et
al., 2003; Zhu and Ghahramani, 2002; Zhu et al,
2003) usually utilize all the features occurring in la-
beled data and unlabeled data. So in this paper we
propose a semi-supervised feature clustering algo-
rithm to overcome this problem. Firstly, we try to
induce class labels for unseen features based on the
similarity among seen features and unseen features.
Then all the features (including seen features and
unseen features) are clustered based on the distrib-
ution of class labels over them.
This paper is organized as follows. First, we
will formulate a feature clustering based WSD prob-
lem in section 2. Then in section 3 we will de-
scribe a semi-supervised feature clustering algo-
rithm. Section 4 will provide experimental results
of various dimensionality reduction techniques with
combination of state of the art WSD algorithms on
SENSEVAL-3 data. Section 5 will provide a review
of related work on feature clustering. Finally we will
conclude our work and suggest possible improve-
ment in section 6.
2 Problem Setup
Let X = {xi}ni=1 be a set of contexts of occur-
rences of an ambiguous word w, where xi repre-
sents the context of the i-th occurrence, and n is
the total number of this word?s occurrences. Let
S = {sj}cj=1 denote the sense tag set of w. The first
l examples xg(1 ? g ? l) are labeled as yg (yg ? S)
and other u (l+u = n) examples xh(l+1 ? h ? n)
are unlabeled. The goal is to predict the sense of w
in context xh by the use of label information of xg
and similarity information among examples in X .
We use F? to represent feature clustering result
into NF? clusters when F is a set of features. After
feature clustering, any context xi in X can be repre-
sented as a vector over feature clusters F? . Then we
can use supervised methods (ex. SVM) (Lee and
Ng, 2002) or semi-supervised methods (ex. label
propagation algorithm) (Niu et al, 2005) to perform
sense disambiguation on unlabeled instances of tar-
get word.
3 Semi-Supervised Feature Clustering
Algorithm
In supervised feature clustering process, F consists
of features occurring in the first l labeled examples,
which can be denoted as FL. But in the setting of
transductive learning, semi-supervised learning al-
gorithms will utilize not only the features in labeled
examples (FL), but also unseen features in unlabeled
examples (denoted as FL). FL consists of the fea-
tures that occur in unlabeled data, but never appear
in labeled data.
Supervised feature clustering algorithm usually
performs clustering analysis over feature-class ma-
trix, where each entry (i, j) in this matrix is the num-
ber of times of the i-th feature co-occurring with the
j-th class. Therefore it can not group features in FL
into meaningful clusters since there are no class la-
bels associated with these features. We overcome
this problem by firstly inducing class labels for un-
seen features based on the similarity among features
in FL and FL, then clustering all the features (in-
cluding FL and FL) based on the distribution of class
908
labels over them.
This semi-supervised feature clustering algorithm
is defined as follows:
Input:
Feature set F = FL
?FL (the first |FL| features
in F belong to FL, and the remaining |FL| features
belong to FL), context set X , the label information
of xg(1 ? g ? l), NF? (the number of clusters in F? );
Output:
Clustering solution F? ;
Algorithm:
1. Construct |F | ? |X| feature-example matrix
MF,X , where entry MF,Xi,j is the number of times of
fi co-occurring with example xj (1 ? j ? n).
2. Form |F | ? |F | affinity matrix W defined by
Wij = exp(?
d2ij
?2 ) if i 6= j and Wii = 0 (1 ?
i, j ? |F |), where dij is the distance (ex. Euclid-
ean distance) between fi (the i-th row in MF,X ) and
fj (the j-th row in MF,X ), and ? is used to control
the weight Wij .
3. Construct |FL| ? |S| feature-class matrix
Y FL,S , where the entry Y FL,Si,j is the number of
times of feature fi (fi ? FL) co-occurring with
sense sj .
4. Obtain hard label matrix for features in FL
(denoted as Y FL,Shard ) based on Y FL,S , where entry
Y F,Shard i,j = 1 if the hard label of fi is sj , otherwise
zero. Obtain hard labels for features in FL using
a classifier based on W and Y FL,Shard . In this paper
we use label propagation (LP) algorithm (Zhu and
Ghahramani, 2002) to get hard labels for FL.
5. Construct |F | ? |S| feature-class matrix Y F,Shard,
where entry Y F,Shard i,j = 1 if the hard label of fi is
sj , otherwise zero.
6. Construct the matrix L = D?1/2WD?1/2 in
which D is a diagonal matrix with its (i, i)-element
equal to the sum of the i-th row of W .
7. Label each feature in F as soft label Y? F,Si , the
i-th row of Y? F,S , where Y? F,S = (I ? ?L)?1Y F,Shard.
8. Obtain the feature clustering solution F? by
clustering the rows of Y? F,Si into NF? groups. In
this paper we use sequential information bottleneck
(sIB) algorithm (Slonim and Tishby, 2000) to per-
form clustering analysis.
End
Step 3 ? 5 are the process to obtain hard la-
bels for features in F , while the operation in step 6
and 7 is a local and global consistency based semi-
supervised learning (LGC) algorithm (Zhou et al,
2003) that smooth the classification result of LP al-
gorithm to acquire a soft label for each feature.
At first sight, this semi-supervised feature cluster-
ing algorithm seems to make little sense. Since we
run feature clustering in step 8, why not use LP algo-
rithm to obtain soft label matrix Y FL,S for features
in FL by the use of Y FL,S and W , then just apply
sIB directly to soft label matrix Y? F,S (constructed
by catenating Y FL,S and Y FL,S)?
The reason for using LGC algorithm to acquire
soft labels for features in F is that in the context
of transductive learning, the size of labeled data is
rather small, which is much less than that of un-
labeled data. This makes it difficult to obtain re-
liable estimation of class label?s distribution over
features from only labeled data. This motivates
us to use raw information (hard labels of features
in FL) from labeled data to estimate hard labels
of features in FL. Then LGC algorithm is used
to smooth the classification result of LP algorithm
based on the assumption that a good classification
should change slowly on the coherent structure ag-
gregated by a large amount of unlabeled data. This
operation makes our algorithm more robust to the
noise in feature-class matrix Y FL,S that is estimated
from labeled data.
In this paper, ? is set as the average distance be-
tween labeled examples from different classes, and
NF? = |F |/10. Latent semantic indexing technique
(LSI) is used to perform factor analysis in MF,X be-
fore calculating the distance between features in step
2.
4 Experiments and Results
4.1 Experiment Design
For empirical study of dimensionality reduction
techniques on WSD task, we evaluated five dimen-
sionality reduction algorithms on the data in English
lexical sample (ELS) task of SENSEVAL-3 (Mihal-
cea et al, 2004)(including all the 57 English words
) 1: supervised feature clustering (SuFC) (Baker and
McCallum, 1998; Bekkerman et al, 2003; Slonim
1Available at http://www.senseval.org/senseval3
909
and Tishby, 2001), iterative double clustering (IDC)
(El-Yaniv and Souroujon, 2001), semi-supervised
feature clustering (SemiFC) (our algorithm), super-
vised feature selection (SuFS) (Forman, 2003), and
latent semantic indexing (LSI) (Deerwester et. al.,
1990) 2.
We used sIB algorithm 3 to cluster features in
FL into groups based on the distribution of class la-
bels associated with each feature. This procedure
can be considered as our re-implementation of su-
pervised feature clustering. After feature clustering,
examples can be represented as vectors over feature
clusters.
IDC is an extension of double clustering method
(DC) (Slonim and Tishby, 2000), which performs it-
erations of DC. In the transductive version of IDC,
they cluster features in F as distributions over class
labels (given by the labeled data) during the first
stage of the IDC first iteration. This phase results in
feature clusters F? . Then they continue as usual; that
is, in the second phase of the first IDC iteration they
group X into NX? clusters, where X is represented
as distribution over F? . Subsequent IDC iterations
use all the unlabeled data. This IDC algorithm can
result in two clustering solutions: F? and X? . Follow-
ing (El-Yaniv and Souroujon, 2001), the number of
iterations is set as 15, and NX? = |S| (the number of
senses of target word) in our re-implementation of
IDC. After performing IDC, examples can be repre-
sented as vectors over feature clusters F? .
Supervised feature selection has been extensively
studied for text categorization task (Forman, 2003).
Information gain (IG) is one of state of the art cri-
teria for feature selection, which measures the de-
crease in entropy when the feature is given vs. ab-
sent. In this paper, we calculate IG score for each
feature in FL, then select top |F |/10 features with
highest scores to form reduced feature set. Then
examples can be represented as vectors over the re-
duced feature set.
LSI is an unsupervised factor analysis technique
based on Singular Value Decomposition of a |X| ?
|F | example-feature matrix. The underlying tech-
nique for LSI is to find an orthogonal basis for the
2Following (Baker and McCallum, 1998), we use LSI as a
representative method for unsupervised dimensionality reduc-
tion.
3Available at http://www.cs.huji.ac.il/?noamm/
feature-example space for which the axes lie along
the dimensions of maximum variance. After using
LSI on the example-feature matrix, we can get vec-
tor representation for each example in X in reduced
feature space.
For each ambiguous word in ELS task of
SENSEVAL-3, we used three types of features to
capture contextual information: part-of-speech of
neighboring words with position information, un-
ordered single words in topical context, and local
collocations (as same as the feature set used in (Lee
and Ng, 2002) except that we did not use syntactic
relations). We removed the features with occurrence
frequency (counted in both training set and test set)
less than 3 times.
We ran these five algorithms for each ambiguous
word to reduce the dimensionality of feature space
from |F | to |F |/10 no matter which training data is
used (ex. full SENSEVAL-3 training data or sam-
pled SENSEVAL-3 training data). Then we can ob-
tain new vector representation of X in new feature
space acquired by SuFC, IDC, SemiFC, and LSI or
reduced feature set by SuFS.
Then we used SVM 4 and LP algorithm to per-
form sense disambiguation on vectors in dimension-
ality reduced feature space. SVM and LP were eval-
uated using accuracy 5 (fine-grained score) on test
set of SENSEVAL-3. For LP algorithm, the test set
in SENSEVAL-3 data was also used as unlabeled
data in tranductive learning process.
We investigated two distance measures for LP: co-
sine similarity and Jensen-Shannon (JS) divergence
(Lin, 1991). Cosine similarity measures the angle
between two feature vectors, while JS divergence
measures the distance between two probability dis-
tributions if each feature vector is considered as
probability distribution over features.
For sense disambiguation on SENSEVAL-3 data,
we constructed connected graphs for LP algorithm
following (Niu et al, 2005): two instances u, v will
be connected by an edge if u is among v?s k nearest
neighbors, or if v is among u?s k nearest neighbors
4We used SV M light with linear kernel function, available
at http://svmlight.joachims.org/.
5If there are multiple sense tags for an instance in training
set or test set, then only the first tag is considered as correct
answer. Furthermore, if the answer of the instance in test set is
?U?, then this instance will be removed from test set.
910
as measured by cosine or JS distance measure. k is
5 in later experiments.
4.2 Experiments on Full SENSEVAL-3 Data
In this experiment, we took the training set in
SENSEVAL-3 as labeled data, and the test set as un-
labeled data. In other words, all of dimensionality
reduction methods and classifiers can use the label
information in training set, but can not access the
label information in test set. We evaluated differ-
ent sense disambiguation processes using test set in
SENSEVAL-3.
We use features with occurrence frequency no less
than 3 in training set and test set as feature set F for
each ambiguous word. F consists of two disjoint
subsets: FL and FL. FL consists of features occur-
ring in training set of target word in SENSEVAL-3,
while FL consists of features that occur in test set,
but never appear in training set.
Table 1 lists accuracies of SVM and LP
without or with dimensionality reduction on full
SENSEVAL-3 data. From this table, we have some
findings as follows:
(1) If without dimensionality reduction, the best
performance of sense disambiguation is 70.3%
(LPJS), while if using dimensionality reduction,
the best two systems can achieve 69.8% (SuFS +
LPJS) and 69.0% (SemiFC + LPJS) accuracies.
It seems that feature selection and feature clustering
can significantly reduce the dimensionality of fea-
ture space while losing only about 1.0% accuracy.
(2) Furthermore, LPJS algorithm performs bet-
ter than SVM when combined with the same dimen-
sionality reduction technique (except IDC). Notice
that LP algorithm uses unlabelled data during its dis-
ambiguation phase while SVM doesn?t. This indi-
cates that using unlabeled data helps to improve the
performance of sense disambiguation.
(3) When using LP algorithm for sense disam-
biguation, SemiFC performs better than other fea-
ture clustering algorithms, such as SuFC, IDC.
This indicates that clustering seen and unseen fea-
tures can satisfy the requirement of semi-supervised
learning algorithm, which does help the classifica-
tion process.
(4) When using SuFC, IDC, SuFS, or SemiFC for
dimensionality reduction, the performance of sense
disambiguation is always better than that using LSI
as dimensionality reduction method. SuFC, IDC,
SuFS, and SemiFC use label information to guide
feature clustering or feature selection, while LSI is
an unsupervised factor analysis method that can con-
duct dimensionality reduction without the use of la-
bel information from labeled data. This indicates
that using label information in dimensionality re-
duction procedure can cluster features into better
groups or select better feature subsets, which results
in better representation of contexts in reduced fea-
ture space.
4.3 Additional Experiments on Sampled
SENSEVAL-3 Data
For investigating the performance of various dimen-
sionality reduction techniques with very small train-
ing data, we ran them with only lw examples from
training set of each word in SENSEVAL-3 as la-
beled data. The remaining training examples and
all the test examples were used as unlabeled data
for SemiFC or LP algorithm. Finally we evaluated
different sense disambiguation processes using test
set in SENSEVAL-3. For each labeled set size lw,
we performed 20 trials. In each trial, we randomly
sampled lw labeled examples for each word from
training set. If any sense was absent from the sam-
pled labeled set, we redid the sampling. lw is set as
Nw,train ? 10%, where Nw,train is the number of
examples in training set of word w. Other settings
of this experiment is as same as that of previous one
in section 4.2.
In this experiment, feature set F is as same as that
in section 4.2. FL consists of features occurring in
sampled training set of target word in SENSEVAL-
3, while FL consists of features that occur in unla-
beled data (including unselected training data and all
the test set), but never appear in labeled data (sam-
pled training set).
Table 2 lists accuracies of SVM and LP with-
out or with dimensionality reduction on sampled
SENSEVAL-3 training data 6. From this table, we
have some findings as follows:
(1) If without dimensionality reduction, the best
performance of sense disambiguation is 54.9%
(LPJS), while if using dimensionality reduction, the
6We can not obtain the results of IDC over 20 trials since it
costs about 50 hours for each trial (Pentium 1.4 GHz CPU/1.0
GB memory).
911
Table 1: This table lists the accuracies of SVM and LP without or with dimensionality reduction on full
SENSEVAL-3 data. There is no result for LSI + LPJS , since the vectors obtained by LSI may contain
negative values, which prohibits the application of JS divergence for measuring the distance between these
vectors.
Without With various dimensionality
dimensionality reduction techniques
Classifier reduction SuFC IDC SuFS LSI SemiFC
SVM 69.7% 66.4% 65.1% 65.2% 59.1% 64.0%
LPcosine 68.4% 66.7% 64.9% 66.0% 60.7% 67.6%
LPJS 70.3% 67.2% 64.0% 69.8% - 69.0%
Table 2: This table lists the accuracies of SVM and LP without or with dimensionality reduction on sam-
pled SENSEVAL-3 training data. For each classifier, we performed paired t-test between the system using
SemiFC for dimensionality reduction and any other system with or without dimensionality reduction. ? (or
?) means p-value ? 0.01, while > (or <) means p-value falling into (0.01, 0.05]. Both ? (or ?) and >
(or <) indicate that the performance of current WSD system is significantly better (or worse) than that using
SemiFC for dimensionality reduction, when given same classifier.
Without With various dimensionality
dimensionality reduction techniques
Classifier reduction SuFC SuFS LSI SemiFC
SVM 53.4?1.1% (?) 50.4?1.1% (?) 52.2?1.2% (>) 49.8?0.8% (?) 51.5?1.0%
LPcosine 54.4?1.2% (?) 49.5?1.1% (?) 51.1?1.0% (?) 49.8?1.0% (?) 52.9?1.0%
LPJS 54.9?1.1% (?) 52.0?0.9% (?) 52.5?1.0% (?) - 54.1?1.2%
best performance of sense disambiguation is 54.1%
(SemiFC + LPJS). Feature clustering can signif-
icantly reduce the dimensionality of feature space
while losing only 0.8% accuracy.
(2) LPJS algorithm performs better than SVM
when combined with most of dimensionality reduc-
tion techniques. This result confirmed our previous
conclusion that using unlabeled data can improve
the sense disambiguation process. Furthermore,
SemiFC performs significantly better than SuFC and
SuFS when using LP as the classifier for sense dis-
ambiguation. The reason is that when given very
few labeled examples, the distribution of class labels
over features can not be reliably estimated, which
deteriorates the performance of SuFC or SuFS. But
SemiFC uses only raw label information (hard label
of each feature) estimated from labeled data, which
makes it robust to the noise in very small labeled
data.
(3) SuFC, SuFS and SemiFC perform better than
LSI no matter which classifier is used for sense dis-
ambiguation. This observation confirmed our previ-
ous conclusion that using label information to guide
dimensionality reduction process can result in bet-
ter representation of contexts in feature subspace,
which further improves the results of sense disam-
biguation.
5 Related Work
Feature clustering has been extensively studied for
the benefit of text categorization and document clus-
tering, which can be categorized as supervised fea-
ture clustering, semi-supervised feature clustering,
and unsupervised feature clustering.
Supervised feature clustering algorithms (Baker
and McCallum, 1998; Bekkerman et al, 2003;
Slonim and Tishby, 2001) usually cluster words into
groups based on the distribution of class labels over
features. Baker and McCallum (1998) apply super-
vised feature clustering based on distributional clus-
tering for text categorization, which can compress
the feature space much more aggressively while still
912
maintaining state of the art classification accuracy.
Slonim and Tishby (2001) and Bekkerman et. al.
(2003) apply information bottleneck method to find
word clusters. They present similar results with the
work by Baker and McCallum (1998). Slonim and
Tishby (2001) goes further to show that when the
training sample is small, word clusters can yield sig-
nificant improvement in classification accuracy.
Unsupervised feature clustering algorithms
(Dhillon, 2001; Dhillon et al, 2002; Dhillon et al,
2003; El-Yaniv and Souroujon, 2001; Slonim and
Tishby, 2000) perform word clustering by the use
of word-document co-occurrence matrix, which do
not utilize class labels to guide clustering process.
Slonim and Tishby (2000), El-Yaniv and Souroujon
(2001) and Dhillon et. al. (2003) show that word
clusters can improve the performance of document
clustering.
El-Yaniv and Souroujon (2001) present an itera-
tive double clustering (IDC) algorithm, which per-
forms iterations of double clustering (Slonim and
Tishby, 2000). Furthermore, they extend IDC algo-
rithm for semi-supervised learning when given both
labeled and unlabeled data.
Our algorithm belongs to the family of semi-
supervised feature clustering techniques, which can
utilize both labeled and unlabeled data to perform
feature clustering.
Supervised feature clustering can not group un-
seen features (features that do not occur in labeled
data) into meaningful clusters since there are no
class labels associated with these unseen features.
Our algorithm can overcome this problem by induc-
ing class labels for unseen features based on the sim-
ilarity among seen features and unseen features, then
clustering all the features (including both seen fea-
tures and unseen features) based on the distribution
of class labels over them.
Compared with the semi-supervised version of
IDC algorithm, our algorithm is more efficient, since
we perform feature clustering without iterations.
The difference between our algorithm and unsu-
pervised feature clustering is that our algorithm de-
pends on both labeled and unlabeled data, but unsu-
pervised feature clustering requires only unlabeled
data.
O?Hara et. al. (2004) use semantic class-
based collocations to augment traditional word-
based collocations for supervised WSD. Three sep-
arate sources of word relatedness are used for
these collocations: 1) WordNet hypernym rela-
tions; 2) cluster-based word similarity classes; and
3) dictionary definition analysis. Their system
achieved 56.6% fine-grained score on ELS task of
SENSEVAL-3. In contrast with their work, our data-
driven method for feature clustering based WSD
does not require external knowledge resource. Fur-
thermore, our SemiFC+LPJS method can achieve
69.0% fine-grained score on the same dataset, which
shows the effectiveness of our method.
6 Conclusion
In this paper we have investigated feature clustering
techniques for WSD, which usually group features
into clusters based on the distribution of class labels
over features. We propose a semi-supervised fea-
ture clustering algorithm to satisfy the requirement
of semi-supervised classification algorithms for di-
mensionality reduction in feature space. Our ex-
perimental results on SENSEVAL-3 data show that
feature clustering can aggressively reduce the di-
mensionality of feature space while still maintaining
state of the art sense disambiguation accuracy. Fur-
thermore, when combined with a semi-supervised
WSD algorithm, semi-supervised feature cluster-
ing outperforms supervised feature clustering and
other dimensionality reduction techniques. Our ad-
ditional experiments on sampled SENSEVAL-3 data
indicate that our semi-supervised feature clustering
method is robust to the noise in small labeled data,
which achieves better performance than supervised
feature clustering.
In the future, we may extend our work by using
more datasets to empirically evaluate this feature
clustering algorithm. This semi-supervised feature
clustering framework is quite general, which can be
applied to other NLP tasks, ex. text categorization.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments.
Z.Y. Niu is supported by A*STAR Graduate Schol-
arship.
References
Baker L. & McCallum A.. 1998. Distributional Clus-
tering of Words for Text Classification. ACM SIGIR
913
1998.
Bekkerman, R., El-Yaniv, R., Tishby, N., & Winter, Y..
2003. Distributional Word Clusters vs. Words for
Text Categorization. Journal of Machine Learning Re-
search, Vol. 3: 1183-1208.
Dagan, I. & Itai A.. 1994. Word Sense Disambigua-
tion Using A Second Language Monolingual Corpus.
Computational Linguistics, Vol. 20(4), pp. 563-596.
Deerwester, S.C., Dumais, S.T., Landauer, T.K., Furnas,
G.W., & Harshman, R.A.. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
of Information Science, Vol. 41(6), pp. 391-407.
Dhillon I.. 2001. Co-Clustering Documents and Words
Using Bipartite Spectral Graph Partitioning. ACM
SIGKDD 2001.
Dhillon I., Mallela S., & Kumar R.. 2002. Enhanced
Word Clustering for Hierarchical Text Classification.
ACM SIGKDD 2002.
Dhillon I., Mallela S., & Modha, D.. 2003. Information-
Theoretic Co-Clustering. ACM SIGKDD 2003.
El-Yaniv, R., & Souroujon, O.. 2001. Iterative Dou-
ble Clustering for Unsupervised and Semi-Supervised
Learning. NIPS 2001.
Forman, G.. 2003. An Extensive Empirical Study of Fea-
ture Selection Metrics for Text Classification. Journal
of Machine Learning Research 3(Mar):1289?1305.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24:1, 147?
165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. EMNLP 2002, (pp. 41-
48).
Lesk M.. 1986. Automated Word Sense Disambiguation
Using Machine Readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. ACM SIGDOC
1986.
Li, H. & Li, C.. 2004. Word Translation Disambiguation
Using Bilingual Bootstrapping. Computational Lin-
guistics, 30(1), 1-22.
Lin, J. 1991. Divergence Measures Based on the Shan-
non Entropy. IEEE Transactions on Information The-
ory, 37:1, 145?150.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J..
2004. Finding Predominant Word Senses in Untagged
Text. ACL 2004.
Mihalcea R.. 2002. Instance Based Learning with Au-
tomatic Feature Selection Applied to Word Sense Dis-
ambiguation. COLING 2002.
Mihalcea R.. 2004. Co-Training and Self-Training for
Word Sense Disambiguation. CoNLL 2004.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004. The
SENSEVAL-3 English Lexical Sample Task. SENSE-
VAL 2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2004. Learning Word
Senses With Feature Selection and Order Identification
Capabilities. ACL 2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2005. Word Sense
Disambiguation Using Label Propagation Based Semi-
Supervised Learning. ACL 2005.
O?Hara, T., Bruce, R., Donner, J., & Wiebe, J..
2004. Class-Based Collocations for Word-Sense Dis-
ambiguation. SENSEVAL 2004.
Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000. Word
Sense Disambiguation by Learning from Unlabeled
Data. ACL 2000.
Pedersen. T., & Bruce, R.. 1997. Distinguishing Word
Senses in Untagged Text. EMNLP 1997.
Schu?tze, H.. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:1, 97?123.
Slonim, N. & Tishby, N.. 2000. Document Clustering
Using Word Clusters via the Information Bottleneck
Method. ACM SIGIR 2000.
Slonim, N. & Tishby, N.. 2001. The Power of Word
Clusters for Text Classification. The 23rd European
Colloquium on Information Retrieval Research.
Towel, G. & Voorheest, E.M.. 1998. Disambiguating
Highly Ambiguous Words. Computational Linguis-
tics, 24:1, 125?145.
Yarowsky, D.. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. ACL 1995,
pp. 189-196.
Zhou D., Bousquet, O., Lal, T.N., Weston, J., &
Scho?lkopf, B.. 2003. Learning with Local and Global
Consistency. NIPS 16,pp. 321-328.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003. Semi-
Supervised Learning Using Gaussian Fields and Har-
monic Functions. ICML 2003.
914
  	
   	

	 	
	  	 	

Unsupervised Feature Selection for Relation Extraction
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised re-
lation extraction algorithm, which in-
duces relations between entity pairs by
grouping them into a ?natural? num-
ber of clusters based on the similarity
of their contexts. Stability-based crite-
rion is used to automatically estimate
the number of clusters. For removing
noisy feature words in clustering proce-
dure, feature selection is conducted by
optimizing a trace based criterion sub-
ject to some constraint in an unsuper-
vised manner. After relation clustering
procedure, we employ a discriminative
category matching (DCM) to find typi-
cal and discriminative words to repre-
sent different relations. Experimental
results show the effectiveness of our al-
gorithm.
1 Introduction
Relation extraction is the task of finding rela-
tionships between two entities from text contents.
There has been considerable work on supervised
learning of relation patterns, using corpora which
have been annotated to indicate the information to
be extracted (e.g. (Califf and Mooney, 1999; Ze-
lenko et al, 2002)). A range of extraction mod-
els have been used, including both symbolic rules
and statistical rules such as HMMs or Kernels.
These methods have been particularly success-
ful in some specific domains. However, manu-
ally tagging of large amounts of training data is
very time-consuming; furthermore, it is difficult
for one extraction system to be ported across dif-
ferent domains.
Due to the limitation of supervised methods,
some weakly supervised (or semi-supervised) ap-
proaches have been suggested (Brin, 1998; Eu-
gene and Luis, 2000; Sudo et al, 2003). One
common characteristic of these algorithms is that
they need to pre-define some initial seeds for any
particular relation, then bootstrap from the seeds
to acquire the relation. However, it is not easy
to select representative seeds for obtaining good
results.
Hasegawa, et al put forward an unsuper-
vised approach for relation extraction from large
text corpora (Hasegawa et al, 2004). First, they
adopted a hierarchical clustering method to clus-
ter the contexts of entity pairs. Second, after con-
text clustering, they selected the most frequent
words in the contexts to represent the relation
that holds between the entities. However, the ap-
proach exists its limitation. Firstly, the similar-
ity threshold for the clusters, like the appropriate
number of clusters, is somewhat difficult to pre-
defined. Secondly, the representative words se-
lected by frequency tends to obscure the clusters.
For solving the above problems, we present a
novel unsupervised method based on model or-
der selection and discriminative label identifica-
tion. For achieving model order identification,
stability-based criterion is used to automatically
estimate the number of clusters. For removing
noisy feature words in clustering procedure, fea-
ture selection is conducted by optimizing a trace
based criterion subject to some constraint in an
262
unsupervised manner. Furthermore, after relation
clustering, we employ a discriminative category
matching (DCM) to find typical and discrimina-
tive words to represent different relations types.
2 Proposed Method
Feature selection for relation extraction is the task
of finding important contextual words which will
help to discriminate relation types. Unlike su-
pervised learning, where class labels can guide
feature search, in unsupervised learning, it is ex-
pected to define a criterion to assess the impor-
tance of the feature subsets. Due to the interplay
between feature selection and clustering solution,
we should define an objective function to evaluate
both feature subset and model order.
In this paper, the model selection capability is
achieved by resampling based stability analysis,
which has been successfully applied to several un-
supervised learning problems (e.g. (Levine and
Domany, 2001), (Lange et al, 2002), (Roth and
Lange et al, 2003), (Niu et al, 2004)). We extend
the cluster validation strategy further to address
both feature selection and model order identifica-
tion.
Table 1 presents our model selection algorithm.
The objective function MFk,k is relevant with
both feature subset and model order. Clustering
solution that is stable against resampling will give
rise to a local optimum of MFk,k, which indicates
both important feature subset and the true cluster
number.
2.1 Entropy-based Feature Ranking
Let P = {p1, p2, ...pN} be a set of local context
vectors of co-occurrences of entity pair E1 and
E2. Here, the context includes the words occur-
ring between, before and after the entity pair. Let
W = {w1, w2, ..., wM} represent all the words
occurred in P . To select a subset of important
features from W , words are first ranked accord-
ing to their importance on clustering. The im-
portance can be assessed by the entropy criterion.
Entropy-based feature ranking is based on the as-
sumption that a feature is irrelevant if the presence
of it obscures the separability of data set(Dash et
al., 2000).
We assume pn, 1 ? n ? N , lies in feature
space W , and the dimension of feature space is
Table 1: Model Selection Algorithm for Relation Extrac-
tion
Input: Corpus D tagged with Entities(E1, E2);
Output: Feature subset and Model Order (number of
relation types);
1. Collect the contexts of all entity pairs in the document
corpus D, namely P ;
2. Rank features using entropy-based method described
in section 2.1;
3. Set the range (Kl,Kh) for the possible number of
relation clusters;
4. Set estimated model order k = Kl;
5. Conduct feature selection using the algorithm pre-
sented in section 2.2;
6. Record F?k,k and the score of the merit of both of
them, namely MF,k;
7. If k < Kh, k = k + 1, go to step 5; otherwise, go to
Step 7;
8. Select k and feature subset F?k which maximizes the
score of the merit MF,k;
M . Then the similarity between i-th data point
pi and j-th data point pj is given by the equa-
tion: Si,j = exp(?? ? Di,j), where Di,j is the
Euclidean distance between pi and pj , and ? is a
positive constant, its value is ? ln 0.5D , where D is
the average distance among the data points. Then
the entropy of data set P with N data points is
defined as:
E = ?
N?
i=1
N?
j=1
(Si,j logSi,j + (1? Si,j) log(1? Si,j))
(1)
For ranking of features, the importance of each
word I(wk) is defined as entropy of the data af-
ter discarding feature wk. It is calculated in this
way: remove each word in turn from the feature
space and calculate E of the data in the new fea-
ture space using the Equation 1. Based on the
observation that a feature is the least important if
the removal of it results in minimum E, we can
obtain the rankings of the features.
2.2 Feature Subset Selection and Model
Order Identification
In this paper, for each specified cluster number,
firstly we perform K-means clustering analysis on
each feature subset and adopts a scattering cri-
terion ?Invariant Criterion? to select an optimal
feature subset F from the feature subset space.
Here, trace(P?1W PB) is used to compare the clus-
ter quality for different feature subsets 1, which
1trace(P?1W PB) is trace of a matrix which is the sum
of its diagonal elements. PW is the within-cluster scatter
263
Table 2: Unsupervised Algorithm for Evaluation of Fea-
ture Subset and Model Order
Function: criterion(F, k, P, q)
Input: feature subset F , cluster number k, entity pairs
set P , and sampling frequency q;
Output: the score of the merit of F and k;
1. With the cluster number k as input, perform k-means
clustering analysis on pairs set PF ;
2. Construct connectivity matrix CF,k based on above
clustering solution on full pairs set PF ;
3. Use a random predictor ?k to assign uniformly drawn
labels to each entity pair in PF ;
4. Construct connectivity matrix CF,?k based on above
clustering solution on full pairs set PF ;
5. Construct q sub sets of the full pairs set, by randomly
selecting ?N of the N original pairs, 0 ? ? ? 1;
6. For each sub set, perform the clustering analysis in
Step 2, 3, 4, and result C?F,k, C?F,?k ;
7. Compute MF,k to evaluate the merit of k using Equa-
tion 3;
8. Return MF,k;
measures the ratio of between-cluster to within-
cluster scatter. The higher the trace(P?1W PB), the
higher the cluster quality.
To improve searching efficiency, features are
first ranked according to their importance. As-
sume Wr = {f1, ..., fM} is the sorted feature list.
The task of searching can be seen in the feature
subset space: {(f1, ..., fk),1 ? k ? M}.
Then the selected feature subset F is eval-
uated with the cluster number using the ob-
jective function, which can be formulated as:
F?k = argmaxF?Wr{criterion(F, k)}, subject
to coverage(P, F ) ? ? 2. Here, F?k is the opti-
mal feature subset, F and k are the feature subset
and the value of cluster number under evaluation,
and the criterion is set up based on resampling-
based stability, as Table 2 shows.
Let P? be a subset sampled from full entity
pairs set P with size ?|P | (? set as 0.9 in this
paper.), C(C?) be |P | ? |P |(|P?| ? |P?|) con-
nectivity matrix based on the clustering results on
P (P?). Each entry cij(c?ij) of C(C?) is calculated
in the following: if the entity pair pi ? P (P?),
pj ? P (P?) belong to the same cluster, then
cij(c?ij) equals 1, else 0. Then the stability is de-
matrix as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xj ? mj)
t
and PB is the between-cluster scatter matrix as: PB =?c
j=1(mj ?m)(mj ?m)t, where m is the total mean vec-
tor and mj is the mean vector for jth cluster and (Xj?mj)t
is the matrix transpose of the column vector (Xj ?mj).
2let coverage(P, F ) be the coverage rate of the feature
set F with respect to P . In practice, we set ? = 0.9.
fined in Equation 2:
M(C?, C) =
?
i,j 1{C?i,j = Ci,j = 1, pi ? P?, pj ? P?}?
i,j 1{Ci,j = 1, pi ? P?, pj ? P?}
(2)
Intuitively, M(C?, C) denotes the consistency
between the clustering results on C? and C. The
assumption is that if the cluster number k is actu-
ally the ?natural? number of relation types, then
clustering results on subsets P? generated by
sampling should be similar to the clustering re-
sult on full entity pair set P . Obviously, the above
function satisfies 0 ? M ? 1.
It is noticed that M(C?, C) tends to decrease
when increasing the value of k. Therefore for
avoiding the bias that small value of k is to be
selected as cluster number, we use the cluster
validity of a random predictor ?k to normalize
M(C?, C). The random predictor ?k achieved
the stability value by assigning uniformly drawn
labels to objects, that is, splitting the data into k
clusters randomly. Furthermore, for each k, we
tried q times. So, in the step 7 of the algorithm
of Table 2, the objective function M(C?F,k, CF,k)
can be normalized as equations 3:
MnormF,k = 1q
q?
i=1
M(C?iF,k, CF,k)?
1
q
q?
i=1
M(C?iF,?k , CF,?k )
(3)
Normalizing M(C?, C) by the stability of the
random predictor can yield values independent of
k.
After the number of optimal clusters and the
feature subset has been chosen, we adopted the
K-means algorithm for the clustering phase. The
output of context clustering is a set of context
clusters, each of them is supposed to denote one
relation type.
2.3 Discriminative Feature identification
For labelling each relation type, we use DCM
(discriminative category matching) scheme to
identify discriminative label, which is also used
in document classification (Gabriel et al, 2002)
and weights the importance of a feature based on
their distribution. In this scheme, a feature is not
important if the feature appears in many clusters
and is evenly distributed in these clusters, other-
wise it will be assigned higher importance.
To weight a feature fi within a category, we
take into account the following information:
264
Table 3: Three domains of entity pairs: frequency distribution for different relation types
PER-ORG # of pairs:786 ORG-GPE # of pairs:262 ORG-ORG # of pairs:580
Relation types Percentage Relation types Percentage Relation types Percentage
Management 36.39% Based-In 46.56% Member 27.76%
General-staff 29.90% Located 35.11% Subsidiary 19.83%
Member 19.34% Member 11.07% Part-Of 18.79%
Owner 4.45% Affiliate-Partner 3.44% Affiliate-Partner 17.93%
Located 3.28% Part-Of 2.29% Owner 8.79%
Client 1.91% Owner 1.53% Client 2.59%
Other 1.91% Management 2.59%
Affiliate-Partner 1.53% Other 1.21%
Founder 0.76% Other 0.52%
? The relative importance of fi within a cluster is de-
fined as: WCi,k = log2(pfi,k+1)log2(Nk+1) , where pfi,k is the
number of those entity pairs which contain feature fi
in cluster k. Nk is the total number of term pairs in
cluster k.
? The relative importance of fi across clusters is given
by: CCi = log N?maxk?Ci{WCi,k}?N
k=1 WCi,k
? 1logN , where Ci
is the set of clusters which contain feature fi. N is the
total number of clusters.
Here, WCi,k and CCi are designed to capture
both local information within a cluster and global
information about the feature distribution across
clusters respectively. Combining both WCi,k and
CCi we define the weight Wi,k of fi in cluster k
as: Wi,k = WC
2
i,k?CC2i?
WC2i,k+CC2i
? ?2, 0 ? Wi,k ? 1.
3 Experiments and Results
3.1 Data
We constructed three subsets for domains PER-
ORG, ORG-GPE and ORG-ORG respectively
from ACE corpus3 The details of these subsets
are given in Table 3, which are broken down by
different relation types. To verify our proposed
method, we only extracted those pairs of entity
mentions which have been tagged relation types.
And the relation type tags were used as ground
truth classes to evaluate.
3.2 Evaluation method for clustering result
Since there was no relation type tags for each
cluster in our clustering results, we adopted a
permutation procedure to assign different rela-
tion type tags to only min(|EC|,|TC|) clusters,
where |EC| is the estimated number of clusters,
and |TC| is the number of ground truth classes
3http://www.ldc.upenn.edu/Projects/ACE/
(relation types). This procedure aims to find an
one-to-one mapping function ? from the TC to
EC. To perform the mapping, we construct a
contingency table T , where each entry ti,j gives
the number of the instances that belong to both
the i-th cluster and j-th ground truth class. Then
the mapping procedure can be formulated as:?? =
argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the index
of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we
can define the evaluation measure as follows:
Accuracy(P ) =
?
j t??(j),j?
i,j ti,j
. Intuitively, it reflects
the accuracy of the clustering result.
3.3 Evaluation method for relation labelling
For evaluation of the relation labeling, we need
to explore the relatedness between the identified
labels and the pre-defined relation names. To do
this, we use one information-content based mea-
sure (Lin, 1997), which is provided in Wordnet-
Similarity package (Pedersen et al, 2004) to eval-
uate the similarity between two concepts in Word-
net. Intuitively, the relatedness between two con-
cepts in Wordnet is captured by the information
content of their lowest common subsumer (lcs)
and the information content of the two concepts
themselves , which can be formalized as follows:
Relatednesslin(c1, c2) = 2?IC(lcs(c1,c2))IC(c1)+IC(c2) . This
measure depends upon the corpus to estimate in-
formation content. We carried out the experi-
ments using the British National Corpus (BNC)
as the source of information content.
3.4 Experiments and Results
For comparison of the effect of the outer and
within contexts of entity pairs, we used five dif-
265
Table 4: Automatically determined the number of relation types using different feature ranking methods.
Domain Context
Window
Size
# of real
relation
types
Model Or-
der Base-
line
Model
Order with
?2
Model
Order with
Freq
Model Or-
der with
Entropy
PER-ORG 0-5-0 9 7 7 7 7
2-5-2 9 8 6 7 8
0-10-0 9 8 6 8 8
2-10-2 9 6 7 6 8
5-10-5 9 5 5 6 7
ORG-GPE 0-5-0 6 3 3 3 4
2-5-2 6 2 3 4 4
0-10-0 6 6 4 5 6
2-10-2 6 4 3 4 5
5-10-5 6 2 3 3 3
ORG-ORG 0-5-0 9 7 7 7 7
2-5-2 9 7 5 6 7
0-10-0 9 9 8 9 9
2-10-2 9 6 6 6 7
5-10-5 9 8 5 7 9
ferent settings of context window size (WINpre-
WINmid-WINpost) for each domain.
Table 4 shows the results of model order iden-
tification without feature selection (Baseline) and
with feature selection based on different feature
ranking criterion( ?2 , Frequency and Entropy).
The results show that the model order identifica-
tion algorithm with feature selection based on en-
tropy achieve best results: estimate cluster num-
bers which are very close to the true values. In ad-
dition, we can find that with the context setting, 0-
10-0, the estimated number of the clusters is equal
or close to the ground truth value. It demonstrates
that the intervening words less than 10 are appro-
priate features to reflect the structure behind the
contexts, while the intervening words less than 5
are not enough to infer the structure. For the con-
textual words beyond (before or after) the enti-
ties, they tend to be noisy features for the relation
estimation, as can be seen that the performance
deteriorates when taking them into consideration,
especially for the case without feature selection.
Table 5 gives a comparison of the aver-
age accuracy over five different context win-
dow size settings for different clustering settings.
For each domain, we conducted five cluster-
ing procedures: Hasegawa?s method, RLBaseline,
RLFS?2 , RLFSFreq and RLFSEntropy. For
Hasegawa?s method (Hasegawa et al, 2004), we
set the cluster number to be identical with the
number of ground truth classes. For RLBaseline,
we use the estimated cluster number to clus-
ter contexts without feature selection. For
RLFS?2 ,RLFSFreq and RLFSEntropy, we use
the selected feature subset and the estimated clus-
ter number to cluster the contexts, where the fea-
ture subset comes from ?2, frequency and entropy
criterion respectively. Comparing the average ac-
curacy of these clustering methods, we can find
that the performance of feature selection methods
is better than or comparable with the baseline sys-
tem without feature selection. Furthermore, it is
noted that RLFSEntropy achieves the highest av-
erage accuracy in three domains, which indicates
that entropy based feature pre-ranking provides
useful heuristic information for the selection of
important feature subset.
Table 6 gives the automatically estimated labels
for relation types for the domain PER-ORG. We
select two features as labels of each relation type
according to their DCM scores and calculate the
average (and maximum) relatedness between our
selected labels (E) and the predefined labels (H).
Following the same strategy, we also extracted re-
lation labels (T) from the ground truth classes and
provided the relatedness between T and H. From
the column of relatedness (E-H), we can see that it
is not easy to find the hand-tagged relation labels
exactly, furthermore, the identified labels from the
ground-truth classes are either not always compa-
rable to the pre-defined labels in most cases (T-
H). The reason may be that the pre-defined rela-
tion names tend to be some abstract labels over
the features, e.g., ?management? vs. ?president?,
266
Table 5: Performance of the clustering algorithms over three domains: the average accuracy over 5 different context window
size.
Domain Hasegawa?s
method
RLBaseline RLFS?2 RLFSFreq RLFSEntropy
PER-ORG 32.4% 34.3% 33.9% 36.6% 41.3%
ORG-GPE 43.7% 47.4% 47.1% 48.4% 50.6%
ORG-ORG 26.5% 36.2% 36.0% 38.7% 42.4%
Table 6: Relation Labelling using DCM strategy for the domain PER-ORG. Here, (T) denotes the identified relation labels
from ground truth classes. (E) is the identified relation labels from our estimated clusters. ?Ave (T-H)? denotes the average
relatedness between (T) and (H). ?Max (T-H)? denotes the maximum relatedness between (T) and (H).
Hand-tagged La-
bel (H)
Identified Label
(T)
Identified Label
(E)
Ave
(T-H)
Max
(T-H)
Ave
(E-H)
Max
(E-H)
Ave
(E-T)
Max
(E-T)
management head,president president,control 0.3703 0.4515 0.3148 0.3406 0.7443 1.0000
general-staff work,fire work,charge 0.6254 0.7823 0.6411 0.7823 0.6900 1.0000
member join,communist become,join 0.394 0.4519 0.1681 0.3360 0.3366 1.0000
owner bond,bought belong,house 0.1351 0.2702 0.0804 0.1608 0.2489 0.4978
located appear,include lobby,appear 0.0000 0.0000 0.1606 0.3213 0.2500 1.0000
client hire,reader bought,consult 0.4378 0.8755 0.0000 0.0000 0.1417 0.5666
affiliate-partner affiliate,associate assist,affiliate 0.9118 1.0000 0.5000 1.0000 0.5000 1.0000
founder form,found invest,set 0.1516 0.3048 0.3437 0.6875 0.4376 0.6932
?head? or ?control?; ?member? vs. ?join?, ?be-
come?, etc., while the abstract words and the fea-
tures are located far away in Wordnet. Table 6
also lists the relatedness between (E) and (T). We
can see that the labels are comparable by their
maximum relatedness(E-T).
4 Conclusion and Future work
In this paper, we presented an unsupervised ap-
proach for relation extraction from corpus. The
advantages of the proposed approach includes
that it doesn?t need any manual labelling of the re-
lation instances, it can identify an important fea-
ture subset and the number of the context clusters
automatically, and it can avoid extracting those
common words as characterization of relations.
References
Mary Elaine Califf and Raymond J.Mooney. 1999. Rela-
tional Learning of Pattern-Match Rules for Information
Extraction, AAAI99.
Sergey Brin. 1998. Extracting patterns and relations from
world wide web. In Proc. of WebDB?98. pages 172-183.
Kiyoshi Sudo, Satoshi Sekine and Ralph Grishman. 2003.
An Improved Extraction Pattern Representation Model
for Automatic IE Pattern Acquisition. Proceedings of ACL
2003; Sapporo, Japan.
Eugene Agichtein and Luis Gravano. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections, In
Proc. of the 5th ACM International Conference on Digi-
tal Libraries (ACMDL?00).
Takaaki Hasegawa, Satoshi Sekine and Ralph Grishman.
2004. Discovering Relations among Named Entities from
Large Corpora, ACL2004. Barcelona, Spain.
Dmitry Zelenko, Chinatsu Aone and Anthony Richardella.
2002. Kernel Methods for Relation Extraction,
EMNLP2002. Philadelphia.
Lange,T., Braun,M.,Roth, V., and Buhmann,J.M.. 2002.
Stability-Based Model Selection, Advances in Neural In-
formation Processing Systems 15.
Levine,E. and Domany,E.. 2001. Resampling Method
for Unsupervised Estimation of Cluster Calidity, Neural
Computation, Vol.13, 2573-2593.
Zhengyu Niu, Donghong Ji and Chew Lim Tan. 2004. Doc-
ument Clustering Based on Cluster Validation, CIKM?04.
November 8-13, 2004, Washington, DC, USA.
Volker Roth and Tilman Lange. 2003. Feature Selection in
Clustering Problems, NIPS2003 workshop.
Manoranjan Dash and Huan Liu. 2000. Feature Selection
for Clustering, Proceedings of Pacific-Asia Conference
on Knowledge Discovery and Data Mining.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun
Lu. 2002. Discriminative Category Matching: Effi-
cient Text Classification for Huge Document Collections,
ICDM2002. December 09-12, 2002, Japan.
D.Lin. 1997. Using syntactic dependency as a local context
to resolve word sense ambiguity. In Proceedings of the
35th Annual Meeting of ACL,. Madrid, July 1997.
Ted Pedersen, Siddharth Patwardhan and Jason Michelizzi.
2004. WordNet::Similarity-Measuring the Relatedness of
Concepts, AAAI2004.
267
 Sentence Ordering based on Cluster Adjacency in Multi-Document 
Summarization  
 
Ji Donghong, Nie Yu 
Institute for Infocomm Research 
Singapore, 119613 
{dhji, ynie}@i2r.a-star.edu.sg 
 
ABSTRACT 
In this paper, we propose a cluster-adjacency based method to 
order sentences for multi-document summarization tasks. 
Given a group of sentences to be organized into a summary, 
each sentence was mapped to a theme in source documents by 
a semi-supervised classification method, and adjacency of 
pairs of sentences is learned from source documents based on 
adjacency of clusters they belong to. Then the ordering of the 
summary sentences can be derived with the first sentence 
determined. Experiments and evaluations on DUC04 data 
show that this method gets better performance than other 
existing sentence ordering methods. 
1. Introduction 
The issue of how to extract information from source 
documents is one main topic of summarization area. Being 
the last step of multi-document summarization tasks, 
sentence ordering attracts less attention up to now. But since 
a good summary should be fluent and readable to human 
being, sentence ordering which organizes texts into the final 
summary could not be ignored. 
   Sentence ordering is much harder for multi-document 
summarization than for single-document summarization 
(McKeown et al, 2001; Barzilay and Lapata, 2005). The 
main reason is that unlike single document, multi-documents 
don?t provide a natural order of texts to be the basis of 
sentence ordering judgment. This is more obvious for 
sentence extraction based summarization systems. 
   Majority ordering is one way of sentence ordering 
(McKeown et al, 2001; Barzilay et al, 2002). This method 
groups sentences in source documents into different themes 
or topics based on summary sentences to be ordered, and the 
order of summary sentences is determined based on the order 
of themes. The idea of this method is reasonable since the 
summary of multi-documents usually covers several topics in 
source documents to achieve representative, and the theme 
ordering can suggest sentence ordering somehow. However, 
there are two challenges for this method. One is how to 
cluster sentences into topics, and the other is how to order 
sentences belonging to the same topic. Barzilay et al (2002) 
combined topic relatedness and chronological ordering 
together to order sentences. Besides chronological ordering, 
sentences were also grouped into different themes and 
ordered by the order of themes learned from source 
documents. The results show that topic relatedness can help 
chronological ordering to improve the performance.  
    Probabilistic model was also used to order sentences. 
Lapata (2003) ordered sentences based on conditional 
probabilities of sentence pairs. The conditional probabilities 
of sentence pairs were learned from a training corpus. With 
conditional probability of each sentence pairs, the 
approximate optimal global ordering was achieved with a 
simple greedy algorithm. The conditional probability of a 
pair of sentences was calculated by conditional probability of 
feature pairs occurring in the two sentences. The experiment 
results show that it gets significant improvement compared 
with randomly sentence ranking.  
   Bollegala et al (2005) combined chronological ordering, 
probabilistic ordering and topic relatedness ordering together. 
They used a machine learning approach to learn the way of 
combination of the three ordering methods. The combined 
system got better results than any of the three individual 
methods.  
   Nie et al (2006) used adjacency of sentence pairs to order 
sentences. Instead of the probability of a sentence sequence 
used in probabilistic model, the adjacency model used 
adjacency value of sentence pairs to order sentences. 
Sentence adjacency is calculated based on adjacency of 
feature pairs within the sentence pairs. Adjacency between 
two sentences means how closely they should be put together 
in a set of summary sentences. Although there is no ordering 
information provided by sentence adjacency, an optimal 
ordering of summary sentences can be derived by use of 
adjacency information of all sentence pairs if the first 
sentence is properly selected.  
    In this paper, we propose a new sentence ordering method 
named cluster-adjacency based ordering. Like the feature-
adjacency based ordering mentioned above, the ordering 
process still depends on sentence adjacency. But we cluster 
sentences first and use cluster adjacency instead of feature 
adjacency to calculate sentence adjacency. The advantage of 
this change is to avoid the sensitivity of the adjacency 
745
information to limited number of individual features, which 
usually needs manual intervention.   
The remainder of this paper is organized as follows. In 
section 2, we specify the motivation of this method. In 
section 3, we talk about the sentence classification using a 
semi-supervised method. In section 4, we discuss the 
procedure for sentence ordering. In section 5, we present 
experiments and evaluation. In section 6, we give the 
conclusion and future work.  
2. Motivation 
Majority ordering assumes that sentences in the summary 
belong to different themes or topics, and the ordering of 
sentences in the summary can be determined by the occurring 
sequence of themes in source documents. In order to derive 
the order of themes, Barzilay et al (2002) presented themes 
and their relations as a directed graph. In the graph, nodes 
denote themes; an edge from one node to another denotes the 
occurring of one theme before another in a source document, 
and the weight of an edge is set to be the frequency of the 
theme pair co-occurring in the texts. Each theme is given a 
weight that equals to the difference between its outgoing 
edges and incoming edges. By finding and removing a theme 
with the biggest weight in the graph recursively, an ordering 
of themes is determined. 
    Probabilistic ordering method treats the ordering as a task 
of finding the sentence sequence with the biggest probability 
(Lapata, 2003). For a sentence sequence T= S1, S2,?,Sn , 
suppose that the probability of any given sentence is 
determined only by its previous sentence, the probability of a 
sentence sequence can be generated based on the condition 
probabilities P(Si|Si-1) of all adjacent sentence pairs in the 
sequence. The condition probability P(Si|Si-1) can be further 
resolved as the product of condition probabilities of feature 
pairs P(fl|fm), where fl is the feature in Si, fm  is the feature in 
Si-1. 
    By finding the sentence with the biggest condition 
probability with the previous one recursively, an ordering of 
sentences is determined. A null sentence is normally 
introduced at the beginning of each source document to find 
the first sentence (Lapata, 2003). 
    Both majority ordering and probabilistic ordering 
determine text sequences in the summary based on those in 
the source documents. The intuition behind the idea is that 
the ordering of summary sentences tends to be consistent 
with those of document sentences. However, we notice that 
some important information might be lost in the process. 
Consider examples below: 
Example 1: Source Document  = ??ABA?? 
Example 2: Source Document 1 = ??AB?? 
                  Source Document 2 = ??BA?? 
Here A and B denote two themes. Let?s assume that A and B 
are both denoted by the summary sentences. In both 
examples, the frequency of A preceding B equals to that of B 
preceding A, thus no sequence preference could be learned 
from the two examples, and we can only estimate a 
probability of 0.5 following one by another. With such 
estimation, the intuition that A and B shall be put adjacently 
although their ordering is not clear would be difficult to 
capture.  
   An adjacency based ordering (Nie et al, 2006) was 
proposed to capture such adjacency information between 
texts during sentence ordering. It uses adjacency of sentence 
pairs to order summary sentences. Adjacency between two 
sentences can be seen as how closely they should be put 
together in an output summary. In general, sentence 
adjacency is derived from that of feature pairs within 
sentences. Note that there is no clue to decide the sequence of 
two sentences purely based on their adjacency value. 
However, if the first sentence has been decided, the total 
sentence sequence can be derived according to the adjacency 
values by recursively selecting one having the biggest 
adjacency value with the most recently selected.  
    For adjacency based ordering, a problem is how to 
calculate the adjacency value between two sentences. For 
feature-adjacency based ordering, the sentence adjacency is 
calculated based on that of feature pairs within the two 
sentences. But a sentence may contain many single word 
features, and there may exist many noisy features, especially 
for longer sentences. To eliminate the impact of noisy 
features, one simple method is to select top n most adjacent 
feature pairs among the two sentences (Nie et al, 2006). 
However, the parameter heavily influences the performance, 
as shown in Table 1, where each row gives the result of a run 
with the same window range and different top n adjacent 
feature pairs.  
 
Win_ 
range 
?( ??top-
n=1) 
?( ??top-
n=2) 
?( ??top
-n=3) 
?( ??top-
n=4) 
?( ??top-
n=5) 
?( ??top-
n=10) 
2 0.184 0.213 0.253 0.262 0.261 0.224 
3 0.251 0.252 0.273 0.274 0.257 0.213 
4 0.201 0.253 0.268 0.316 0.272 0.248 
Table 1.  Feature-Adjacency Based Ordering 
 
The heavy reliance on the manually pre-defined parameter 
is an obstacle for implementation of the feature-adjacency 
based ordering, since it?s hard to determine the most suitable 
value for the parameter across different tasks. More generally, 
the feature-adjacency method depends on limited number of 
individual features, which normally needs very strong feature 
selection techniques to be effective. To avoid the sensitivity 
to individual features, we propose a cluster-adjacency based 
sentence ordering. Although the clustering will also use 
individual features, the noisy ones would be lower weighted 
via appropriate weighting schemes.  
Assuming there are n summary sentences to be ordered, 
we cluster sentences in source documents into n clusters 
based on the n summary sentences. Each cluster represents a 
summary sentence. Then we use the cluster adjacency instead 
of feature adjacency to produce sentence adjacency. Since 
features are not directly used in calculating sentence 
746
adjacency, the setting of the parameter to remove noisy 
features is no more needed. In addition, we expect the 
clustering to determine the themes properly and reduce the 
affect of noisy features.  
3. Sentence Clustering 
Assume there are K summary sentences to be ordered, and 
there are N sentences in source documents, we cluster the N 
sentences into K clusters using a semi-supervised 
classification method, Label Propagation (Zhu and 
Ghahramani, 2003). The advantage of this method is that it 
can exploit the closeness between unlabeled data during 
classification, thus ensuring a better classification result even 
with very fewer labeled data. This is exactly the situation 
here, where each summary sentence can be seen as the only 
one labeled data for the class.   
Following are some notations for the label propagation 
algorithm in sentence classification: 
{rj} (1?j?K): the K summary sentences 
{mj} (1?j?N): the N document sentences to be classified  
X = {xi} (1?i?K+N) refers to the union set of the above 
two categories of sentences, i.e. xi (1?i?K) represents the K 
summary sentences, xi (K+1?i?K+N+1) represents the N 
sentences to be classified. That is, the first K sentences are 
labeled sentences while the remaining N sentences are to be 
re-ranked. C = {cj} (1?j?K) denotes the class set of 
sentences, each one in which is labeled by a summary 
sentence. Y0 ?Hs?K (s=K+N) represents initial soft labels 
attached to each sentence, where Yij0= 1 if xi is cj and 0 
otherwise. Let YL0 be top l=K rows of Y0, which corresponds 
to the labeled data, and YU0 be the remaining N rows, which 
corresponds to the unlabeled data. Here, each row in YU0 is 
initialized according to the similarity of a sentence with the 
summary sentences. 
In the label propagation algorithm, the manifold structure 
in X is represented as a connected graph and the label 
information of any vertex in the graph is propagated to 
nearby vertices through weighted edges until the propagation 
process converges. Here, each vertex corresponds to a 
sentence, and the edge between any two sentences xi and xj is 
weighted by wij to measure their similarity. Here wij is defined 
as follows: wij = exp(-dij2/ ? 2) if i ? j and wii = 0 (1?i,j?l+u), 
where dij is the distance between xi and xj, and ? is a scale to 
control the transformation. In this paper, we set ? as the 
average distance between summary sentences. Moreover, the 
weight wij between two sentences xi and xj is transformed to a 
probability tij = P(j?i) =wij/(?sk=1wkj), where tij is the 
probability to propagate a label from sentence xj to sentence 
xi. In principle, larger weights between two sentences mean 
easy travel and similar labels between them according to the 
global consistency assumption applied in this algorithm. 
Finally, tij is normalized row by row as in (1), which is to 
maintain the class probability interpretation of Y. The s ? s 
matrix is denoted asT as in (1). 
During the label propagation process, the label distribution 
of the labeled data is clamped in each loop and acts like 
forces to push out labels through unlabeled data. With this 
push originates from labeled data, the label boundaries will 
be pushed much faster along edges with larger weights and 
settle in gaps along those with lower weights. Ideally, we can 
expect that wij across different classes should be as small as 
possible and wij within a same class as big as possible. In this 
way, label propagation happens within a same class most 
likely. 
 
? == sk ikijij ttt 1)1(
  (2)  YTTIY tY LuluuUtU
01)(lim? ??? ?== . 
 
???
?
???
?=
uuul
lull
TT
TT
T)3(
This algorithm has been shown to converge to a unique 
solution (Zhu and Ghahramani, 2003) with u=M and l=K as 
in (2), where I is u ? u identity matrix. uuT  and ulT  are 
acquired by splitting matrix T after the l-th row and the l-th 
column into 4 sub-matrices as in (3). 
 
    In theory, this solution can be obtained without iteration 
and the initialization of YU0 is not important, since YU0 
does not affect the estimation of UY . However, the 
initialization of Y
?
U0 helps the algorithm converge quickly 
in practice. In this paper, each row in YU0 is initialized 
according the similarity of a sentence with the summary 
sentences. Fig. 1 gives the classification procedure.  
INPUT 
{xi} (1?i?K): set of summary sentences as labeled data; 
{xi} (K+1?i?K+N+1): set of document sentences;  
Algorithm: Label_Propagation({rj}, {mj}) 
BEGIN 
    Set the iteration index t=0 
    BEGIN DO Loop 
      Propagate the label by Yt+1 = T Yt; 
  Clamp labeled data by replacing top l row of Yt+1 with YL0   
END DO Loop when Yt converges; 
END 
Fig. 1 Label propagation for sentence classification 
 
   The output of the classification is a set of sentence clusters, 
and the number of the clusters equals to the number of 
summary sentences. In each cluster, the members can be 
ordered by their membership probabilities. In fact, the semi-
supervised classification is a kind of soft labeling (Tishby 
and Slonim, 2000; Zhou et al, 2003), in which each sentence 
belongs to different clusters, but with different probabilities. 
For sentence ordering task here, we need to get hard clusters, 
in which each sentence belongs to only one cluster. Thus, we 
need to cut the soft clusters to hard ones. To do that, for each 
cluster, we consider every sentence inside according to their 
decreasing order of their membership probabilities. If a 
sentence belongs to the current cluster with the highest 
probability, then it is selected and kept. The selection repeats 
until a sentence belongs to another cluster with higher 
probability.  
4. Sentence Ordering 
Given a set of summary sentences {S1,?,SK}, sentences of 
the source documents are clustered into K groups G1,?,GK, 
747
where Si is corresponding with Gi. For each pair of sentences 
Si and Sj, the adjacency of Si and Sj can be defined as the 
adjacency of Gi and Gj, defined in (4). 
)()(
),( 2
,
ji
ji
ji GfGf
GGf
C =  (4)
Here f(Gi) and f(Gj) respectively denote the frequency of 
cluster Gi and Gj in source documents, f(Gi, Gj)  denotes the 
frequency of Gi and Gj co-occurring in the source documents 
within a limited window range.  
    The first sentence S1 can be determined according to (5) 
based on the adjacency between null clusters (containing 
only the null sentence) and any sentence clusters.  
)max(arg ,1 joS C
TS j ?
=
 
(5)
Here C0,j denotes how close the sentence Sj and a null 
sentence are. By adding a null sentence at the beginning of 
each source document as S0 , and assuming it contains one 
null sentence, C0,j can be calculated with equation (4). 
   Given an already ordered sentence sequence, S1, S2,?,Si, 
whose sentence set R is subset of the whole sentence set T, 
the task of finding the (i+1)th sentence can be described as: 
)max(arg ,1 jiS C
RTS
i
j ??
+ =
 
(6) 
Now the sentence sequence become S1, S2,?,Si, Si+1. By 
repeating the step the whole sequence can be derived. 
5. Experiments and Evaluation 
In this section, we describe the experiments with cluster-
adjacency based ordering, and compared it with majority 
ordering, probability-based ordering and feature-adjacency 
based ordering respectively. Some methods [e.g., 8] tested 
ordering models using external training corpus and extracted 
sentence features such as nouns, verbs and dependencies 
from parsed tress. In this paper, we only used the raw input 
data, i.e., source input documents, and didn?t use any 
grammatical knowledge. For feature-adjacency based model, 
we used single words except stop words as features to 
represent sentences. For cluster-adjacency based model, we 
used the same features to produce vector representations for 
sentences. 
5.1 Test Set and Evaluation Metrics 
Regarding test data, we used DUC04 data. DUC 04 provided 
50 document sets and four manual summaries for each 
document set in its Task2. Each document set consists of 10 
documents. Sentences of each summary were taken as inputs 
to ordering models, with original sequential information 
being neglected. The output ordering of various models were 
to be compared with that specified in manual summaries.  
A number of metrics can be used to evaluate the difference 
between two orderings. In this paper, we used Kendall?s ? ? 
[9], which is defined as: 
 
2/)1(
)__(2
1 ??= NN
inversionsofnumber?
 
(7)
Here N is the number of objects to be ordered (i.e., 
sentences). Number_of_inversions is the minimal number of 
interchanges of adjacent objects to transfer an ordering into 
another. Intuitively, ? can be considered as how easily an 
ordering can be transferred to another. The value of ? ranges 
from -1 to 1, where 1 denotes the best situation ---- two 
orderings are the same, and -1 denotes the worst situation---
completely converse orderings. Given a standard ordering, 
randomly produced orderings of the same objects would get 
an average ? of 0. For examples, Table 2 gives three number 
sequences, their natural sequences and the corresponding ? 
values. 
Examples  Natural sequences ? values 
1  2  4  3 1 2 3 4 0.67 
1  5  2  3  4 1 2 3 4 5 0.4 
2  1  3 1 2 3 0.33 
Table 2. Ordering Examples 
5.2 Results 
In the following, we used Rd, Mo, Pr, Fa and Ca to denote 
random ordering, majority ordering, probabilistic model, 
feature-adjacency based model and cluster-adjacency based 
model respectively. Normally, for Fa and Ca, the window 
size is set as 3 sentences, and for Fa, the noise elimination 
parameter ( ?top-n) is set as 4.   
Table 3 gives automatic evaluation results. We can see that 
Mo and Pr got very close ? values (0.143 vs. 0.144). 
Meanwhile, Fa got better results (0.316), and the Ca achieved 
the best performance (0.415). The significance tests suggest 
that the difference between the ? values of Fa and Mo or Pr is 
significant, and so is the difference between the values of Ca 
and Fa, where *, **, ~ represent p-values <=0.01, (0.01, 
0.05], and >0.05. 
Models ? Significance SVM 
Rd -0.007   
Mo 0.143  0.153~ 
Pr 0.144   
Fa 0.316 **  
Ca 0.415 * 0.305** 
Table 3. Automatic evaluation results  
Both Mo and Ca use the themes acquired by the 
classification. In comparison, we also used SVM to do the 
classification, and Table 3 lists the ? values for Mo and Ca. 
SVM is a typical supervised classification, which only uses 
the comparison between labeled data and unlabeled data. So, 
it generally requires a large number of training data to be 
effective. The results show that the difference between the 
performance of Mo with LP (0.143) and SVM (0.153) is not 
significant, while the difference between the performance of 
Ca with LP (0.415) and SVM (0.305) is significant.    
In general, if an ordering gets a positive ? value, the 
ordering can be considered to be better than a random one. 
748
On the contrary, for a negative ? value, the ordering can be 
considered to be worse than a random one. For a zero ? 
value, the ordering is in fact close to a random one. So, 
percentage of ? values reflects quality of the orderings to 
some extent. Table 4 shows the percentage of positive 
ordering, negative orderings and median orderings for 
different models. It demonstrates that the cluster-adjacency 
based model produced the most positive orderings and the 
least negative orderings.  
Models Positive 
Orderings 
Negative 
Orderings 
Median 
Orderings 
Rd 99  (49.5%) 90 (45.0%) 11  (5.5%) 
Mo 123  (61.5%) 64  (32.0%) 13  (6.5%) 
Pr 125  (62.5%) 59  (29.5%) 16  (8.0%) 
Fa 143  (71.5%) 38  (19.0%) 19  (9.5%) 
Ca 162  (81.0%) 31  (15.5%) 7  (3.5%) 
          Table 4. Positive, Negative and Median Orderings 
To see why the cluster-adjacency model achieved better 
performance, we checked about the determination of the first 
sentence between different models, since that it is extremely 
important for Pr, Fa and Ca, and it will influence later 
selections. Either in Pr or in Fa and Ca, it was assumed that 
there is one null sentence at the beginning of each source 
document. In Pr, to determine the first sentence is to find one 
which is the most likely to follow the assumed null sentence, 
while in the two adjacency models, to determine the first 
sentence means to select one which is the closest to the null 
sentence. Table 5 shows the comparison.  
Models Correct selection of 1st sentences 
Rd 22 (14.0%) 
Mo 53 (26.5%) 
Pr 81 (41.5%) 
Fa 119 (59.5%) 
Ca 131 (65.5%) 
Table 5. First sentence determination 
Table 5 indicates that cluster-adjacency model performed 
best in selection of the first sentence in the summaries.  
  Another experiment we did is about how likely the k+1th 
sentence can be correctly selected if assuming that top k 
sentences have been successfully acquired. This is also useful 
to explain why a model performs better than others. Fig. 2 
shows the comparison of the probabilities of correct 
determination of the k+1th sentence between different 
models. Fig. 2 demonstrates that the probabilities of the 
correct k+1th sentence selection in cluster-adjacency model 
are generally higher than those in other methods, which 
indicates that the cluster-adjacency model is more 
appropriate for the data.   
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7
Ca
Fa
MO
Pr
 
                  Fig. 2. k+1th sentence determination 
Table 6 gives the experiment results of the cluster-
adjacency model with varying window ranges. In general, the 
cluster-adjacency model got better performance than feature-
adjacency model without requirement of setting the noise 
elimination parameters. This can be seen as an advantage of 
Ca over Fa. However, we can see that the adjacency window 
size still influenced the performance as it did for Fa. 
Window size ? values 
2 0.314 
3 0.415 
4 0.398 
5 0.356  
Table 6. Ca performance with different window size 
   As a concrete example, consider a summary (D31050tG) in 
Fig. 3, which includes 6 sentences as the following.  
0. After 2 years of wooing the West by signing international accords, 
apparently relaxing controls on free speech, and releasing and exiling 
three dissenters, China cracked down against political dissent in Dec 
1998. 
1. Leaders of the China Democracy Party (CDP) were arrested and three 
were sentenced to jail terms of 11 to 13 years. 
2. The West, including the US, UK and Germany, reacted strongly. 
3. Clinton's China policy of engagement was questioned. 
4. China's Jiang Zemin stated economic reform is not a prelude to 
democracy and vowed to crush any challenges to the Communist Party 
or "social stability". 
5. The CDP vowed to keep working, as more leaders awaited arrest. 
Fig. 3. A sample summary 
Table 7 gives the ordering generated by various models. 
Models Output ? values 
Pr 4 0 1 3 5 2 0.20 
Mo 1 4 3 0 2 5 0.20 
Fa 0 1 4 3 5 2 0.47 
Ca 1 2 0 3 4 5 0.73 
Table 7. Comparison: an example 
From Table 7, we have several findings. First, sentence 3, 4 
and 5 were close in the sequence in terms of their adjacency 
values, so in both Fa and Ca, once one of them was selected, 
the other two would follow. However, the closeness between 
them was not reflected in both Pr and Mo. Second, while Ca 
correctly made 1 followed by 2, Fa didn?t. The reason may be 
that although sentence 1 and 2 had higher cluster-adjacency 
value, their feature-adjacency value may be lower than that 
between sentence 1 and 4, since sentence 1 and 4 shared 
more features, and only considering a limited number of 
features may make them get higher feature-adjacency value. 
At the same time, during classification in Ca, other different 
features (other than ?China?, ?democracy?, etc) would come 
to distinguish between sentence 1 and 4, so cluster centers of 
sentence 1 and 4 would have bias toward the distinguishing 
features. Thus, their adjacency value tended to be lower in 
Ca, and in fact, they fell apart in the sequence. Third, Fa 
successfully got the first sentence, while Ca didn?t. To see 
the reason, we checked the summaries, and found that some 
summaries started with theme 0 and some more with theme 1, 
since theme 1 had part of the features in theme 0 and they 
may have contribution to feature-adjacency value, topic 1 
749
tended to have higher feature-adjacency value. This is not 
contradicting with higher cluster-adjacency value between 
theme Null and theme 1. In fact, we found putting sentence 1 
at the beginning was also acceptable subjectively.    
   In manual evaluation, the number of inversions was defined 
as the minimal number of interchanges of adjacent objects to 
transfer the output ordering to an acceptable ordering judged 
by human. We have three people participating in the 
evaluation, and the minimal, maximal and average numbers 
of interchanges for each summary among the three persons 
were selected for evaluation respectively. The Kendall?s ? of 
all 5 runs are listed in Table 8. 
 ? values 
Models Average  Minimal  Maximal 
Rd 0.106 0.202 0.034 
Mo 0.453 0.543 0.345 
Pr 0.465 0.524 0.336 
Fa 0.597 0.654 0.423 
Ca 0.665 0.723 0.457 
Table 8. Manual evaluation results on 10 summaries 
From table 7, we can find that all models get higher 
Kendall?s ? values than in automatic evaluation, and the two 
adjacency models achieved better results than Pr and Mo 
according to the three measures. As example, Table 9 lists the 
subjective evaluation for the sample summary in Fig. 3.  
Models Output Subjective 
ordering 
? values 
Pr 4 0 1 3 5 2 401235 0.73 
Mo 1 4 3 0 2 5 140235 0.73 
Fa 0 1 4 3 5 2 014235 0.73 
Ca 1 2 0 3 4 5 120345 1.0 
Table 9. Subjective evaluation: an example 
6. Conclusion and Future Work 
In this paper we propose a cluster-adjacency based model for 
sentence ordering in multi-document summarization. It learns 
adjacency information of sentences from the source 
documents and order sentences accordingly. Compared with 
the feature-adjacency model, the cluster-adjacency method 
produces sentence adjacency from cluster adjacency. The 
major advantage of this method is that it focuses on a kind of 
global adjacency (cluster on the whole), and avoids 
sensitivity to limited number of features, which in general is 
difficult. In addition, with semi-supervised classification, this 
method is expected to determine appropriate themes in source 
documents and achieve better performance. 
Although the cluster-adjacency based ordering model 
solved the problem of noise elimination required by the 
feature-adjacency based ordering, how to set another 
parameter properly, i.e., the window range, is still unclear. 
We guess it may depend on length of source documents. The 
longer the source documents are, the bigger adjacency 
window size may be expected. But more experiments are 
needed to prove it.  
In addition, the adjacency based model mainly uses only 
adjacency information to order sentences. Although it 
appears to perform better than models using only sequential 
information on DUC2004 data set, if some sequential 
information could be learned definitely from the source 
documents, it should be better to combine the adjacency 
information and sequential information.  
Reference 
 
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2001. 
Sentence ordering in multidocument summarization. Proceedings of the 
First International Conference on Human Language Technology Research 
(HLT-01), San Diego, CA, 2001, pp. 149?156.. 
Barzilay, R N. Elhadad, and K. McKeown. 2002. Inferring strategies for 
sentence ordering in multidocument news summarization. Journal of 
Artificial Intelligence Research, 17:35?55. 
Sasha Blair-Goldensohn, David Evans. Columbia University at DUC 2004. 
In Proceedings of the 4th Document Understanding Conference (DUC 
2004). May, 2004. 
Danushka Bollegala, Naoaki Okazaki, Mitsuru Ishizuka. 2005. A machine 
learning approach to sentence ordering for multidocument summarization 
and it?s evaluation. IJCNLP 2005, LNAI 3651, pages 624-635, 2005. 
McKeown K., Barzilay R. Evans D., Hatzivassiloglou V., Kan M., Schiffman 
B., &Teufel, S. (2001). Columbia multi-document summarization: 
Approach and evaluation. In Proceedings of DUC. 
Mirella Lapata. Probabilistic text structuring: Experiments with sentence 
ordering. Proceedings of the annual meeting of ACL, 2003., pages 545?
552, 2003. 
Nie Yu, Ji Donghong and Yang Lingpeng. An adjacency model for sentence 
ordering in multi-document Asian Information Retrieval Symposium 
(AIRS2006), Singapore., Oct. 2006. 
Advaith Siddharthan, Ani Nenkova and Kathleen McKeown. Syntactic 
Simplication for Improving Content Selection in Multi-Document 
Summarization. In Proceeding of COLING 2004, Geneva, Switzerland. 
Tishby, N, Slonim, N. (2000) Data clustering by Markovian relaxation and 
the Information Bottleneck Method. NIPS 13. 
Szummer M. and T. Jaakkola. (2001) Partially labeled classification with 
markov random walks. NIPS14. 
Zhu, X., Ghahramani, Z., & Lafferty, J. (2003) Semi-Supervised Learning 
Using Gaussian Fields and Harmonic Functions. ICML-2003. 
Zhou D., Bousquet, O., Lal, T.N., Weston J. & Schokopf B. (2003). Learning 
with local and Global Consistency. NIPS 16. pp: 321-328 
750
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 25?28,
New York, June 2006. c?2006 Association for Computational Linguistics
Semi-supervised Relation Extraction with Label Propagation
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
To overcome the problem of not hav-
ing enough manually labeled relation in-
stances for supervised relation extraction
methods, in this paper we propose a label
propagation (LP) based semi-supervised
learning algorithm for relation extraction
task to learn from both labeled and unla-
beled data. Evaluation on the ACE corpus
showed when only a few labeled examples
are available, our LP based relation extrac-
tion can achieve better performance than
SVM and another bootstrapping method.
1 Introduction
Relation extraction is the task of finding relation-
ships between two entities from text. For the task,
many machine learning methods have been pro-
posed, including supervised methods (Miller et al,
2000; Zelenko et al, 2002; Culotta and Soresen,
2004; Kambhatla, 2004; Zhou et al, 2005), semi-
supervised methods (Brin, 1998; Agichtein and Gra-
vano, 2000; Zhang, 2004), and unsupervised method
(Hasegawa et al, 2004).
Supervised relation extraction achieves good per-
formance, but it requires a large amount of manu-
ally labeled relation instances. Unsupervised meth-
ods do not need the definition of relation types and
manually labeled data, but it is difficult to evaluate
the clustering result since there is no relation type
label for each instance in clusters. Therefore, semi-
supervised learning has received attention, which
can minimize corpus annotation requirement.
Current works on semi-supervised resolution for
relation extraction task mostly use the bootstrap-
ping algorithm, which is based on a local consis-
tency assumption: examples close to labeled ex-
amples within the same class will have the same
labels. Such methods ignore considering the simi-
larity between unlabeled examples and do not per-
form classification from a global consistency view-
point, which may fail to exploit appropriate mani-
fold structure in data when training data is limited.
The objective of this paper is to present a label
propagation based semi-supervised learning algo-
rithm (LP algorithm) (Zhu and Ghahramani, 2002)
for Relation Extraction task. This algorithm works
by representing labeled and unlabeled examples as
vertices in a connected graph, then propagating the
label information from any vertex to nearby vertices
through weighted edges iteratively, finally inferring
the labels of unlabeled examples after the propaga-
tion process converges. Through the label propaga-
tion process, our method can make the best of the
information of labeled and unlabeled examples to re-
alize a global consistency assumption: similar ex-
amples should have similar labels. In other words,
the labels of unlabeled examples are determined by
considering not only the similarity between labeled
and unlabeled examples, but also the similarity be-
tween unlabeled examples.
2 The Proposed Method
2.1 Problem Definition
Let X = {xi}ni=1 be a set of contexts of occurrences
of all entity pairs, where xi represents the contexts
of the i-th occurrence, and n is the total number of
occurrences of all entity pairs. The first l examples
are labeled as yg ( yg ? {rj}Rj=1, rj denotes relation
type and R is the total number of relation types).
And the remaining u(u = n? l) examples are unla-
beled.
Intuitively, if two occurrences of entity pairs have
25
the similar contexts, they tend to hold the same re-
lation type. Based on this assumption, we create a
graph where the vertices are all the occurrences of
entity pairs, both labeled and unlabeled. The edge
between vertices represents their similarity. Then
the task of relation extraction can be formulated as
a form of propagation on a graph, where a vertex?s
label propagates to neighboring vertices according
to their proximity. Here, the graph is connected with
the weights: Wij = exp(? s
2
ij
?2 ), where sij is the sim-
ilarity between xi and xj calculated by some simi-
larity measures. In this paper,two similarity mea-
sures are investigated, i.e. Cosine similarity measure
and Jensen-Shannon (JS) divergence (Lin, 1991).
And we set ? as the average similarity between la-
beled examples from different classes.
2.2 Label Propagation Algorithm
Given such a graph with labeled and unlabeled ver-
tices, we investigate the label propagation algorithm
(Zhu and Ghahramani, 2002) to help us propagate
the label information of any vertex in the graph
to nearby vertices through weighted edges until a
global stable stage is achieved.
Define a n ? n probabilistic transition matrix T
Tij = P (j ? i) = wij?n
k=1 wkj
, where Tij is the prob-
ability to jump from vertex xj to vertex xi. Also de-
fine a n?R label matrix Y , where Yij representing
the probabilities of vertex yi to have the label rj .
Then the label propagation algorithm consists the
following main steps:
Step1: Initialization Firstly, set the iteration in-
dex t = 0. Then let Y 0 be the initial soft labels at-
tached to each vertex and Y 0L be the top l rows of Y 0,
which is consistent with the labeling in labeled data
(Y 0ij = 1 if yi is label rj and 0 otherwise ). Let Y 0U
be the remaining u rows corresponding to unlabeled
data points and its initialization can be arbitrary.
Step 2: Propagate the label by Y t+1 = TY t,
where T is the row-normalized matrix of T , i.e.
Tij = Tij/
?
k Tik, which can maintain the class
probability interpretation.
Step 3: Clamp the labeled data, i.e., replace the
top l row of Y t+1 with Y 0L . In this step, the labeled
data is clamped to replenish the label sources from
these labeled data. Thus the labeled data act like
sources to push out labels through unlabeled data.
Table 1: Frequency of Relation SubTypes in the ACE training
and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
Step 4: Repeat from step 2 until Y converges.
Step 5: Assign xh(l + 1 ? h ? n) with a label:
yh = argmaxjYhj .
3 Experiments and Results
3.1 Data
Our proposed graph-based method is evaluated on
the ACE corpus 1, which contains 519 files from
sources including broadcast, newswire, and news-
paper. A break-down of the tagged data by different
relation subtypes is given in Table 1.
3.2 Features
We extract the following lexical and syntactic fea-
tures from two entity mentions, and the contexts be-
fore, between and after the entity pairs. Especially,
we set the mid-context window as everything be-
tween the two entities and the pre- and post- context
as up to two words before and after the correspond-
ing entity. Most of these features are computed from
the parse trees derived from Charniak Parser (Char-
niak, 1999) and the Chunklink script 2 written by
Sabine Buchholz from Tilburg University.
1 http://www.ldc.upenn.edu/Projects/ACE/
2Software available at http://ilk.uvt.nl/?sabine/chunklink/
26
Table 2: Performance of Relation Detection: SVM and LP algorithm with different size of labeled data. The LP algorithm is
performed with two similarity measures: Cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 35.9 32.6 34.4 58.3 56.1 57.1 58.5 58.7 58.5
10% 51.3 41.5 45.9 64.5 57.5 60.7 64.6 62.0 63.2
25% 67.1 52.9 59.1 68.7 59.0 63.4 68.9 63.7 66.1
50% 74.0 57.8 64.9 69.9 61.8 65.6 70.1 64.1 66.9
75% 77.6 59.4 67.2 71.8 63.4 67.3 72.4 64.8 68.3
100% 79.8 62.9 70.3 73.9 66.9 70.2 74.2 68.2 71.1
Table 3: Performance of Relation Classification on Relation Subtype: SVM and LP algorithm with different size of labeled data.
The LP algorithm is performed with two similarity measures: Cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 31.6 26.1 28.6 39.6 37.5 38.5 40.1 38.0 39.0
10% 39.1 32.7 35.6 45.9 39.6 42.5 46.2 41.6 43.7
25% 49.8 35.0 41.1 51.0 44.5 47.3 52.3 46.0 48.9
50% 52.5 41.3 46.2 54.1 48.6 51.2 54.9 50.8 52.7
75% 58.7 46.7 52.0 56.0 52.0 53.9 56.1 52.6 54.3
100% 60.8 48.9 54.2 56.2 52.3 54.1 56.3 52.9 54.6
Words: Surface tokens of the two entities and
three context windows.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZATION,
FACILITY, LOCATION and GPE.
POS: Part-Of-Speech tags corresponding to all
tokens in the two entities and three context windows.
Chunking features: Chunk tag information and
Grammatical function of the two entities and three
context windows. IOB-chains of the heads of the
two entities are also considered. IOB-chain notes
the syntactic categories of all the constituents on the
path from the root node to this leaf node of tree.
We combine the above features with their position
information in the context to form the context vec-
tor. Before that, we filter out low frequency features
which appeared only once in the entire set.
3.3 Experimental Evaluation
3.3.1 Relation Detection
We collect all entity mention pairs which co-occur
in the same sentence from the training and devtest
corpus into two set C1 and C2 respectively. The set
C1 includes annotated training data AC1 and un-
related data UC1. We randomly sample l examples
from AC1 as labeled data and add a ?NONE? class
into labeled data for the case where the two entity
mentions are not related. The data of the ?NONE?
Table 4: Comparison of performance on individual relation
type of Zhang (2004)?s method and our method. For Zhang
(2004)?s method, feature sampling probability is set to 0.3 and
agreement threshold is set to 9 out of 10.
Bootstrapping LPJS
Rel-Type P R F P R F
ROLE 78.5 69.7 73.8 81.0 74.7 77.7
PART 65.6 34.1 44.9 70.1 41.6 52.2
AT 61.0 84.8 70.9 74.2 79.1 76.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0
NEAR undef 0 undef 13.7 12.5 13.0
class is resulted by sampling l examples from UC1.
Moreover, we combine the rest examples of C1 and
the whole set C2 as unlabeled data.
Given labeled and unlabeled data,we can perform
LP algorithm to detect possible relations, which
are those entity pairs that are not classified to the
?NONE? class but to the other 24 subtype classes.
In addition,we conduct experiments with different
sampling set size l, including 1% ? Ntrain,10% ?
Ntrain,25%?Ntrain,50%?Ntrain,75%?Ntrain,
100% ? Ntrain (Ntrain = |AC1|). If any major
subtype was absent from the sampled labeled set,we
redo the sampling. For each size,we perform 20 tri-
als and calculate an average of 20 random trials.
3.3.2 SVM vs. LP
Table 2 reports the performance of relation detec-
tion by using SVM and LP with different sizes of
27
labled data. For SVM, we use LIBSVM tool with
linear kernel function 3. And the same sampled la-
beled data used in LP is used to train SVM mod-
els. From Table 2, we see that both LPCosine and
LPJS achieve higher Recall than SVM. Especially,
with small labeled dataset (percentage of labeled
data ? 25%), this merit is more distinct. When
the percentage of labeled data increases from 50%
to 100%, LPCosine is still comparable to SVM in F-
measure while LPJS achieves better F-measure than
SVM. On the other hand, LPJS consistently outper-
forms LPCosine.
Table 3 reports the performance of relation classi-
fication, where the performance describes the aver-
age values over major relation subtypes. From Table
3, we see that LPCosine and LPJS outperform SVM
by F-measure in almost all settings of labeled data,
which is due to the increase of Recall. With smaller
labeled dataset, the gap between LP and SVM is
larger. On the other hand, LPJS divergence consis-
tently outperforms LPCosine.
3.3.3 LP vs. Bootstrapping
In (Zhang, 2004), they perform relation classifi-
cation on ACE corpus with bootstrapping on top of
SVM. To compare with their proposed Bootstrapped
SVM algorithm, we use the same feature stream set-
ting and randomly selected 100 instances from the
training data as the size of initial labeled data.
Table 4 lists the performance on individual rela-
tion type. We can find that LP algorithm achieves
6.8% performance improvement compared with the
(Zhang, 2004)?s bootstrapped SVM algorithm aver-
age on all five relation types. Notice that perfor-
mance reported on relation type ?NEAR? is low, be-
cause it occurs rarely in both training and test data.
4 Conclusion and Future work
This paper approaches the task of semi-supervised
relation extraction on Label Propagation algorithm.
Our results demonstrate that, when only very few
labeled examples are available, this manifold learn-
ing based algorithm can achieve better performance
than supervised learning method (SVM) and boot-
strapping based method, which can contribute to
3LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
minimize corpus annotation requirement. In the fu-
ture we would like to investigate how to select more
useful feature stream and whether feature selection
method can improve the performance of our graph-
based semi-supervised relation extraction.
References
Agichtein E. and Gravano L. 2000. Snowball: Extracting Rela-
tions from large Plain-Text Collections, In Proceeding of the
5th ACM International Conference on Digital Libraries.
Brin Sergey. 1998. Extracting patterns and relations from
world wide web. In Proceeding of WebDB Workshop at 6th
International Conference on Extending Database Technol-
ogy. pages 172-183.
Charniak E. 1999. A Maximum-entropy-inspired parser. Tech-
nical Report CS-99-12. Computer Science Department,
Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels for
relation extraction, In Proceedings of 42th ACL conference.
Hasegawa T., Sekine S. and Grishman R. 2004. Discover-
ing Relations among Named Entities from Large Corpora,
In Proceeding of Conference ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and semantic
features with Maximum Entropy Models for extracting rela-
tions, In Proceedings of 42th ACL conference. Spain.
Lin,J. 1991. Divergence Measures Based on the Shannon En-
tropy. IEEE Transactions on Information Theory. 37:1,145-
150.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000. A novel
use of statistical parsing to extract information from text.
In Proceedings of 6th Applied Natural Language Processing
Conference 29 April-4 may 2000, Seattle USA.
Yarowsky D. 1995. Unsupervised Word Sense Disambiguation
Rivaling Supervised Methods. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational Linguis-
tics. pp.189-196.
Zelenko D., Aone C. and Richardella A. 2002. Kernel Meth-
ods for Relation Extraction, In Proceedings of the EMNLP
Conference. Philadelphia.
Zhang Zhu. 2004. Weakly-supervised relation classification for
Information Extraction, In proceedings of ACM 13th con-
ference on Information and Knowledge Management. 8-13
Nov 2004. Washington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min. 2005.
Combining lexical, syntactic and semantic features with
Maximum Entropy Models for extracting relations, In pro-
ceedings of 43th ACL conference. USA.
Zhu Xiaojin and Ghahramani Zoubin. 2002. Learning from
Labeled and Unlabeled Data with Label Propagation. CMU
CALD tech report CMU-CALD-02-107.
28
Learning Word Senses With Feature Selection and Order Identification
Capabilities
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised word sense
learning algorithm, which induces senses of target
word by grouping its occurrences into a ?natural?
number of clusters based on the similarity of their
contexts. For removing noisy words in feature set,
feature selection is conducted by optimizing a clus-
ter validation criterion subject to some constraint in
an unsupervised manner. Gaussian mixture model
and Minimum Description Length criterion are used
to estimate cluster structure and cluster number.
Experimental results show that our algorithm can
find important feature subset, estimate model or-
der (cluster number) and achieve better performance
than another algorithm which requires cluster num-
ber to be provided.
1 Introduction
Sense disambiguation is essential for many lan-
guage applications such as machine translation, in-
formation retrieval, and speech processing (Ide and
Ve?ronis, 1998). Almost all of sense disambigua-
tion methods are heavily dependant on manually
compiled lexical resources. However these lexical
resources often miss domain specific word senses,
even many new words are not included inside.
Learning word senses from free text will help us
dispense of outside knowledge source for defining
sense by only discriminating senses of words. An-
other application of word sense learning is to help
enriching or even constructing semantic lexicons
(Widdows, 2003).
The solution of word sense learning is closely re-
lated to the interpretation of word senses. Different
interpretations of word senses result in different so-
lutions to word sense learning.
One interpretation strategy is to treat a word sense
as a set of synonyms like synset in WordNet. The
committee based word sense discovery algorithm
(Pantel and Lin, 2002) followed this strategy, which
treated senses as clusters of words occurring in sim-
ilar contexts. Their algorithm initially discovered
tight clusters called committees by grouping top
n words similar with target word using average-
link clustering. Then the target word was assigned
to committees if the similarity between them was
above a given threshold. Each committee that the
target word belonged to was interpreted as one of
its senses.
There are two difficulties with this committee
based sense learning. The first difficulty is about
derivation of feature vectors. A feature for target
word here consists of a contextual content word and
its grammatical relationship with target word. Ac-
quisition of grammatical relationship depends on
the output of a syntactic parser. But for some lan-
guages, ex. Chinese, the performance of syntactic
parsing is still a problem. The second difficulty with
this solution is that two parameters are required to
be provided, which control the number of commit-
tees and the number of senses of target word.
Another interpretation strategy is to treat a word
sense as a group of similar contexts of target word.
The context group discrimination (CGD) algorithm
presented in (Schu?tze, 1998) adopted this strategy.
Firstly, their algorithm selected important contex-
tual words using ?2 or local frequency criterion.
With the ?2 based criterion, those contextual words
whose occurrence depended on whether the am-
biguous word occurred were chosen as features.
When using local frequency criterion, their algo-
rithm selected top n most frequent contextual words
as features. Then each context of occurrences of
target word was represented by second order co-
occurrence based context vector. Singular value de-
composition (SVD) was conducted to reduce the di-
mensionality of context vectors. Then the reduced
context vectors were grouped into a pre-defined
number of clusters whose centroids corresponded to
senses of target word.
Some observations can be made about their fea-
ture selection and clustering procedure. One ob-
servation is that their feature selection uses only
first order information although the second order co-
occurrence data is available. The other observation
is about their clustering procedure. Similar with
committee based sense discovery algorithm, their
clustering procedure also requires the predefinition
of cluster number. Their method can capture both
coarse-gained and fine-grained sense distinction as
the predefined cluster number varies. But from a
point of statistical view, there should exist a parti-
tioning of data at which the most reliable, ?natural?
sense clusters appear.
In this paper, we follow the second order repre-
sentation method for contexts of target word, since
it is supposed to be less sparse and more robust than
first order information (Schu?tze, 1998). We intro-
duce a cluster validation based unsupervised fea-
ture wrapper to remove noises in contextual words,
which works by measuring the consistency between
cluster structures estimated from disjoint data sub-
sets in selected feature space. It is based on the
assumption that if selected feature subset is impor-
tant and complete, cluster structure estimated from
data subset in this feature space should be stable
and robust against random sampling. After deter-
mination of important contextual words, we use a
Gaussian mixture model (GMM) based clustering
algorithm (Bouman et al, 1998) to estimate cluster
structure and cluster number by minimizing Min-
imum Description Length (MDL) criterion (Ris-
sanen, 1978). We construct several subsets from
widely used benchmark corpus as test data. Experi-
mental results show that our algorithm (FSGMM )
can find important feature subset, estimate cluster
number and achieve better performance compared
with CGD algorithm.
This paper is organized as follows. In section
2 we will introduce our word sense learning al-
gorithm, which incorporates unsupervised feature
selection and model order identification technique.
Then we will give out the experimental results of
our algorithm and discuss some findings from these
results in section 3. Section 4 will be devoted to
a brief review of related efforts on word sense dis-
crimination. In section 5 we will conclude our work
and suggest some possible improvements.
2 Learning Procedure
2.1 Feature selection
Feature selection for word sense learning is to find
important contextual words which help to discrim-
inate senses of target word without using class la-
bels in data set. This problem can be generalized
as selecting important feature subset in an unsuper-
vised manner. Many unsupervised feature selection
algorithms have been presented, which can be cate-
gorized as feature filter (Dash et al, 2002; Talav-
era, 1999) and feature wrapper (Dy and Brodley,
2000; Law et al, 2002; Mitra et al, 2002; Modha
and Spangler, 2003).
In this paper we propose a cluster valida-
tion based unsupervised feature subset evaluation
method. Cluster validation has been used to solve
model order identification problem (Lange et al,
2002; Levine and Domany, 2001). Table 1 gives
out our feature subset evaluation algorithm. If some
features in feature subset are noises, the estimated
cluster structure on data subset in selected feature
space is not stable, which is more likely to be the
artifact of random splitting. Then the consistency
between cluster structures estimated from disjoint
data subsets will be lower. Otherwise the estimated
cluster structures should be more consistent. Here
we assume that splitting does not eliminate some of
the underlying modes in data set.
For comparison of different clustering structures,
predictors are constructed based on these clustering
solutions, then we use these predictors to classify
the same data subset. The agreement between class
memberships computed by different predictors can
be used as the measure of consistency between clus-
ter structures. We use the stability measure (Lange
et al, 2002) (given in Table 1) to assess the agree-
ment between class memberships.
For each occurrence, one strategy is to construct
its second order context vector by summing the vec-
tors of contextual words, then let the feature selec-
tion procedure start to work on these second order
contextual vectors to select features. However, since
the sense associated with a word?s occurrence is al-
ways determined by very few feature words in its
contexts, it is always the case that there exist more
noisy words than the real features in the contexts.
So, simply summing the contextual word?s vectors
together may result in noise-dominated second or-
der context vectors.
To deal with this problem, we extend the feature
selection procedure further to the construction of
second order context vectors: to select better feature
words in contexts to construct better second order
context vectors enabling better feature selection.
Since the sense associated with a word?s occur-
rence is always determined by some feature words
in its contexts, it is reasonable to suppose that the
selected features should cover most of occurrences.
Formally, let coverage(D,T ) be the coverage rate
of the feature set T with respect to a set of con-
texts D, i.e., the ratio of the number of the occur-
rences with at least one feature in their local con-
texts against the total number of occurrences, then
we assume that coverage(D,T ) ? ? . In practice,
we set ? = 0.9.
This assumption also helps to avoid the bias to-
ward the selection of fewer features, since with
fewer features, there are more occurrences without
features in contexts, and their context vectors will
be zero valued, which tends to result in more stable
cluster structure.
Let D be a set of local contexts of occurrences of
target word, then D = {di}Ni=1, where di represents
local context of the i-th occurrence, and N is the
total number of this word?s occurrences.
W is used to denote bag of words occurring in
context set D, then W = {wi}Mi=1, where wi de-
notes a word occurring in D, and M is the total
number of different contextual words.
Let V denote a M ? M second-order co-
occurrence symmetric matrix. Suppose that the i-th
, 1 ? i ? M , row in the second order matrix corre-
sponds to word wi and the j-th , 1 ? j ? M , col-
umn corresponds to word wj , then the entry speci-
fied by i-th row and j-th column records the number
of times that word wi occurs close to wj in corpus.
We use v(wi) to represent the word vector of con-
textual word wi, which is the i-th row in matrix V .
HT is a weight matrix of contextual word subset
T , T ? W . Then each entry hi,j represents the
weight of word wj in di, wj ? T , 1 ? i ? N . We
use binary term weighting method to derive context
vectors: hi,j = 1 if word wj occurs in di, otherwise
zero.
Let CT = {cTi }Ni=1 be a set of context vectors in
feature space T , where cTi is the context vector of
the i-th occurrence. cTi is defined as:
cTi =
?
j
(hi,jv(wj)), wj ? T, 1 ? i ? N. (1)
The feature subset selection in word set W can be
formulated as:
T? = argmax
T
{criterion(T,H, V, q)}, T ? W, (2)
subject to coverage(D,T ) ? ? , where T? is the op-
timal feature subset, criterion is the cluster valida-
tion based evaluation function (the function in Ta-
ble 1), q is the resampling frequency for estimate
of stability, and coverage(D,T ) is the proportion
of contexts with occurrences of features in T . This
constrained optimization results in a solution which
maximizes the criterion and meets the given con-
straint at the same time. In this paper we use se-
quential greedy forward floating search (Pudil et al,
1994) in sorted word list based on ?2 or local fre-
quency criterion. We set l = 1, m = 1, where l is
plus step, and m is take-away step.
2.2 Clustering with order identification
After feature selection, we employ a Gaussian mix-
ture modelling algorithm, Cluster (Bouman et al,
Table 1: Unsupervised Feature Subset Evaluation Algorithm.
Intuitively, for a given feature subset T , we iteratively split data
set into disjoint halves, and compute the agreement of cluster-
ing solutions estimated from these sets using stability measure.
The average of stability over q resampling is the estimation of
the score of T .
Function criterion(T , H , V , q)
Input parameter: feature subset T , weight matrix H ,
second order co-occurrence matrix V , resampling
frequency q;
(1) ST = 0;
(2) For i = 1 to q do
(2.1) Randomly split CT into disjoint halves, denoted
as CTA and CTB ;
(2.2) Estimate GMM parameter and cluster number on CTA
using Cluster, and the parameter set is denoted as ??A;
The solution ??A can be used to construct a predictor
?A;
(2.3) Estimate GMM parameter and cluster number on CTB
using Cluster, and the parameter set is denoted as ??B ,
The solution ??B can be used to construct a predictor
?B ;
(2.4) Classify CTB using ?A and ?B ;
The class labels assigned by ?A and ?B are denoted
as LA and LB ;
(2.5) ST+ = maxpi 1|CTB |
?
i 1{pi(LA(cTBi)) = LB(cTBi)},
where pi denotes possible permutation relating indices
between LA and LB , and cTBi ? CTB ;
(3) ST = 1qST ;
(4) Return ST ;
1998), to estimate cluster structure and cluster num-
ber. Let Y = {yn}Nn=1 be a set of M dimen-
sional vectors to be modelled by GMM. Assuming
that this model has K subclasses, let pik denote the
prior probability of subclass k, ?k denote the M di-
mensional mean vector for subclass k, Rk denote
the M ?M dimensional covariance matrix for sub-
class k, 1 ? k ? K. The subclass label for pixel
yn is represented by xn. MDL criterion is used
for GMM parameter estimation and order identifi-
cation, which is given by:
MDL(K, ?) = ?
N?
n=1
log (pyn|xn(yn|?)) +
1
2L log (NM),
(3)
pyn|xn(yn|?) =
K?
k=1
pyn|xn(yn|k, ?)pik, (4)
L = K(1 +M + (M + 1)M2 )? 1, (5)
The log likelihood measures the goodness of fit of
a model to data sample, while the second term pe-
nalizes complex model. This estimator works by at-
tempting to find a model order with minimum code
length to describe the data sample Y and parameter
set ?.
If the cluster number is fixed, the estimation of
GMM parameter can be solved using EM algorithm
to address this type of incomplete data problem
(Dempster et al, 1977). The initialization of mix-
ture parameter ?(1) is given by:
pi(1)k =
1
Ko (6)
?(1)k = yn, where n = b(k? 1)(N ? 1)/(Ko? 1)c+1 (7)
R(1)k =
1
N ?
N
n=1ynytn (8)
Ko is a given initial subclass number.
Then EM algorithm is used to estimate model pa-
rameters by minimizing MDL:
E-step: re-estimate the expectations based on pre-
vious iteration:
pxn|yn(k|yn, ?(i)) =
pyn|xn(yn|k, ?(i))pik?K
l=1(pyn|xn(yn|l, ?(i))pil)
, (9)
M-step: estimate the model parameter ?(i) to
maximize the log-likelihood in MDL:
Nk =
N?
n=1
pxn|yn(k|yn, ?(i)) (10)
pik = NkN (11)
?k =
1
Nk
N?
n=1
ynpxn|yn(k|yn, ?(i)) (12)
Rk = 1Nk
N?
n=1
(yn ? ?k)(yn ? ?k)tpxn|yn(k|yn, ?(i))
(13)
pyn|xn(yn|k, ?(i)) =
1
(2pi)M/2 |Rk|
?1/2 exp{?} (14)
? = ?12(yn ? ?k)
tR?1k (yn ? ?k) (15)
The EM iteration is terminated when the change
of MDL(K, ?) is less than ?:
? = 1100(1 +M +
(M + 1)M
2 )log(NM) (16)
For inferring the cluster number, EM algorithm
is applied for each value of K, 1 ? K ? Ko, and
the value K? which minimizes the value of MDL
is chosen as the correct cluster number. To make
this process more efficient, two cluster pair l and m
are selected to minimize the change in MDL crite-
ria when reducing K to K ? 1. These two clusters
l and m are then merged. The resulting parameter
set is chosen as an initial condition for EM iteration
with K ? 1 subclasses. This operation will avoid a
complete minimization with respect to pi, ?, and R
for each value of K.
Table 2: Four ambiguous words, their senses and frequency
distribution of each sense.
Word Sense Percentage
hard not easy (difficult) 82.8%
(adjective) not soft (metaphoric) 9.6%
not soft (physical) 7.6%
interest money paid for the use of money 52.4%
a share in a company or business 20.4%
readiness to give attention 14%
advantage, advancement or favor 9.4%
activity that one gives attention to 3.6%
causing attention to be given to 0.2%
line product 56%
(noun) telephone connection 10.6%
written or spoken text 9.8%
cord 8.6%
division 8.2%
formation 6.8%
serve supply with food 42.6%
(verb) hold an office 33.6%
function as something 16%
provide a service 7.8%
3 Experiments and Evaluation
3.1 Test data
We constructed four datasets from hand-tagged cor-
pus 1 by randomly selecting 500 instances for each
ambiguous word - ?hard?, ?interest?, ?line?, and
?serve?. The details of these datasets are given in
Table 2. Our preprocessing included lowering the
upper case characters, ignoring all words that con-
tain digits or non alpha-numeric characters, remov-
ing words from a stop word list, and filtering out
low frequency words which appeared only once in
entire set. We did not use stemming procedure.
The sense tags were removed when they were used
by FSGMM and CGD. In evaluation procedure,
these sense tags were used as ground truth classes.
A second order co-occurrence matrix for English
words was constructed using English version of
Xinhua News (Jan. 1998-Dec. 1999). The win-
dow size for counting second order co-occurrence
was 50 words.
3.2 Evaluation method for feature selection
For evaluation of feature selection, we used mutual
information between feature subset and class label
set to assess the importance of selected feature sub-
set. Our assessment measure is defined as:
M(T ) = 1|T |
?
w?T
?
l?L
p(w, l)log p(w, l)p(w)p(l) , (17)
where T is the feature subset to be evaluated, T ?
W , L is class label set, p(w, l) is the joint distri-
bution of two variables w and l, p(w) and p(l) are
marginal probabilities. p(w, l) is estimated based
1http://www.d.umn.edu/?tpederse/data.html
on contingency table of contextual word set W and
class label set L. Intuitively, if M(T1) > M(T2),
T1 is more important than T2 since T1 contains more
information about L.
3.3 Evaluation method for clustering result
When assessing the agreement between clustering
result and hand-tagged senses (ground truth classes)
in benchmark data, we encountered the difficulty
that there was no sense tag for each cluster.
In (Lange et al, 2002), they defined a permu-
tation procedure for calculating the agreement be-
tween two cluster memberships assigned by differ-
ent unsupervised learners. In this paper, we applied
their method to assign different sense tags to only
min(|U |, |C|) clusters by maximizing the accuracy,
where |U | is the number of clusters, and |C| is the
number of ground truth classes. The underlying as-
sumption here is that each cluster is considered as
a class, and for any two clusters, they do not share
same class labels. At most |C| clusters are assigned
sense tags, since there are only |C| classes in bench-
mark data.
Given the contingency table Q between clusters
and ground truth classes, each entry Qi,j gives the
number of occurrences which fall into both the i-
th cluster and the j-th ground truth class. If |U | <
|C|, we constructed empty clusters so that |U | =
|C|. Let ? represent a one-to-one mapping function
from C to U . It means that ?(j1) 6= ?(j2) if j1 6=
j2 and vice versa, 1 ? j1, j2 ? |C|. Then ?(j)
is the index of the cluster associated with the j-th
class. Searching a mapping function to maximize
the accuracy of U can be formulated as:
?? = argmax
?
|C|?
j=1
Q?(j),j . (18)
Then the accuracy of solution U is given by
Accuracy(U) =
?
j Q??(j),j?
i,j Qi,j
. (19)
In fact,
?
i,j Qi,j is equal to N , the number of
occurrences of target word in test set.
3.4 Experiments and results
For each dataset, we tested following procedures:
CGDterm:We implemented the context group
discrimination algorithm. Top max(|W | ?
20%, 100) words in contextual word list was se-
lected as features using frequency or ?2 based rank-
ing. Then k-means clustering2 was performed on
context vector matrix using normalized Euclidean
distance. K-means clustering was repeated 5 times
2We used k-means function in statistics toolbox of Matlab.
and the partition with best quality was chosen as fi-
nal result. The number of clusters used by k-means
was set to be identical with the number of ground
truth classes. We tested CGDterm using various
word vector weighting methods when deriving con-
text vectors, ex. binary, idf , tf ? idf .
CGDSV D: The context vector matrix was de-
rived using same method in CGDterm. Then k-
means clustering was conducted on latent seman-
tic space transformed from context vector matrix,
using normalized Euclidean distance. Specifically,
context vectors were reduced to 100 dimensions us-
ing SVD. If the dimension of context vector was
less than 100, all of latent semantic vectors with
non-zero eigenvalue were used for subsequent clus-
tering. We also tested it using different weighting
methods, ex. binary, idf , tf ? idf .
FSGMM : We performed cluster validation
based feature selection in feature set used by CGD.
Then Cluster algorithm was used to group target
word?s instances using Euclidean distance measure.
? was set as 0.90 in feature subset search procedure.
The random splitting frequency is set as 10 for es-
timation of the score of feature subset. The initial
subclass number was 20 and full covariance matrix
was used for parameter estimation of each subclass.
For investigating the effect of different context
window size on the performance of three proce-
dures, we tested these procedures using various con-
text window sizes: ?1, ?5, ?15, ?25, and all of
contextual words. The average length of sentences
in 4 datasets is 32 words before preprocessing. Per-
formance on each dataset was assessed by equation
19.
The scores of feature subsets selected by
FSGMM and CGD are listed in Table 3 and
4. The average accuracy of three procedures with
different feature ranking and weighting method is
given in Table 5. Each figure is the average over 5
different context window size and 4 datasets. We
give out the detailed results of these three proce-
dures in Figure 1. Several results should be noted
specifically:
From Table 3 and 4, we can find that FSGMM
achieved better score on mutual information (MI)
measure than CGD over 35 out of total 40 cases.
This is the evidence that our feature selection pro-
cedure can remove noise and retain important fea-
tures.
As it was shown in Table 5, with both ?2 and
freq based feature ranking, FSGMM algorithm
performed better than CGDterm and CGDSV D if
we used average accuracy to evaluate their per-
formance. Specifically, with ?2 based feature
ranking, FSGMM attained 55.4% average accu-
racy, while the best average accuracy of CGDterm
and CGDSV D were 40.9% and 51.3% respec-
tively. With freq based feature ranking, FSGMM
achieved 51.2% average accuracy, while the best av-
erage accuracy of CGDterm and CGDSV D were
45.1% and 50.2%.
The automatically estimated cluster numbers by
FSGMM over 4 datasets are given in Table 6.
The estimated cluster number was 2 ? 4 for ?hard?,
3 ? 6 for ?interest?, 3 ? 6 for ?line?, and 2 ? 4
for ?serve?. It is noted that the estimated cluster
number was less than the number of ground truth
classes in most cases. There are some reasons for
this phenomenon. First, the data is not balanced,
which may lead to that some important features can-
not be retrieved. For example, the fourth sense of
?serve?, and the sixth sense of ?line?, their corre-
sponding features are not up to the selection criteria.
Second, some senses can not be distinguished using
only bag-of-words information, and their difference
lies in syntactic information held by features. For
example, the third sense and the sixth sense of ?in-
terest? may be distinguished by syntactic relation of
feature words, while the bag of feature words occur-
ring in their context are similar. Third, some senses
are determined by global topics, rather than local
contexts. For example, according to global topics, it
may be easier to distinguish the first and the second
sense of ?interest?.
Figure 2 shows the average accuracy over three
procedures in Figure 1 as a function of context
window size for 4 datasets. For ?hard?, the per-
formance dropped as window size increased, and
the best accuracy(77.0%) was achieved at win-
dow size 1. For ?interest?, sense discrimination
did not benefit from large window size and the
best accuracy(40.1%) was achieved at window size
5. For ?line?, accuracy dropped when increas-
ing window size and the best accuracy(50.2%) was
achieved at window size 1. For ?serve?, the per-
formance benefitted from large window size and the
best accuracy(46.8%) was achieved at window size
15.
In (Leacock et al, 1998), they used Bayesian ap-
proach for sense disambiguation of three ambiguous
words, ?hard?, ?line?, and ?serve?, based on cues
from topical and local context. They observed that
local context was more reliable than topical context
as an indicator of senses for this verb and adjective,
but slightly less reliable for this noun. Compared
with their conclusion, we can find that our result
is consistent with it for ?hard?. But there is some
differences for verb ?serve? and noun ?line?. For
Table 3: Mutual information between feature subset and class
label with ?2 based feature ranking.
Word Cont. Size of MI Size of MI
wind. feature ?10?2 feature ?10?2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4018 80 0.4300
15 100 0.1362 80 0.1416
25 133 0.0997 102 0.1003
all 145 0.0937 107 0.0890
interest 1 64 1.9697 55 2.0639
5 100 0.3234 89 0.3355
15 157 0.1558 124 0.1531
25 190 0.1230 138 0.1267
all 200 0.1163 140 0.1191
line 1 39 4.2089 32 4.6456
5 100 0.4628 84 0.4871
15 183 0.1488 128 0.1429
25 263 0.1016 163 0.0962
all 351 0.0730 192 0.0743
serve 1 22 6.8169 20 6.7043
5 100 0.5057 85 0.5227
15 188 0.2078 164 0.2094
25 255 0.1503 225 0.1536
all 320 0.1149 244 0.1260
Table 4: Mutual information between feature subset and class
label with freq based feature ranking.
Word Cont. Size of MI Size of MI
wind. feature ?10?2 feature ?10?2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4194 80 0.4832
15 100 0.1647 80 0.1774
25 133 0.1150 102 0.1259
all 145 0.1064 107 0.1269
interest 1 64 1.9697 55 2.7051
5 100 0.6015 89 0.8309
15 157 0.2526 124 0.3495
25 190 0.1928 138 0.2982
all 200 0.1811 140 0.2699
line 1 39 4.2089 32 4.4606
5 100 0.6895 84 0.7816
15 183 0.2301 128 0.2929
25 263 0.1498 163 0.2181
all 351 0.1059 192 0.1630
serve 1 22 6.8169 20 7.0021
5 100 0.7045 85 0.8422
15 188 0.2763 164 0.3418
25 255 0.1901 225 0.2734
all 320 0.1490 244 0.2309
?serve?, the possible reason is that we do not use
position of local word and part of speech informa-
tion, which may deteriorate the performance when
local context(? 5 words) is used. For ?line?, the
reason might come from the feature subset, which
is not good enough to provide improvement when
Table 5: Average accuracy of three procedures with various
settings over 4 datasets.
Algorithm Feature Feature Average
ranking weighting accuracy
method method
FSGMM ?2 binary 0.554
CGDterm ?2 binary 0.404
CGDterm ?2 idf 0.407
CGDterm ?2 tf ? idf 0.409
CGDSVD ?2 binary 0.513
CGDSVD ?2 idf 0.512
CGDSVD ?2 tf ? idf 0.508
FSGMM freq binary 0.512
CGDterm freq binary 0.451
CGDterm freq idf 0.437
CGDterm freq tf ? idf 0.447
CGDSVD freq binary 0.502
CGDSVD freq idf 0.498
CGDSVD freq tf ? idf 0.485
Table 6: Automatically determined mixture component num-
ber.
Word Context Model Model
window order order
size with ?2 with freq
hard 1 3 4
5 2 2
15 2 3
25 2 3
all 2 3
interest 1 5 4
5 3 4
15 4 6
25 4 6
all 3 4
line 1 5 6
5 4 3
15 5 4
25 5 4
all 3 4
serve 1 3 3
5 3 4
15 3 3
25 3 3
all 2 4
context window size is no less than 5.
4 Related Work
Besides the two works (Pantel and Lin, 2002;
Schu?tze, 1998), there are other related efforts on
word sense discrimination (Dorow and Widdows,
2003; Fukumoto and Suzuki, 1999; Pedersen and
Bruce, 1997).
In (Pedersen and Bruce, 1997), they described an
experimental comparison of three clustering algo-
rithms for word sense discrimination. Their feature
sets included morphology of target word, part of
speech of contextual words, absence or presence of
particular contextual words, and collocation of fre-
0 1 5 15 25 all0.4
0.5
0.6
0.7
0.8
0.9
Hard dataset
Acc
ura
cy
0 1 5 15 25 all0.2
0.3
0.4
0.5
0.6
Acc
ura
cy
Interest dataset
0 1 5 15 25 all0.2
0.3
0.4
0.5
0.6
0.7
Line dataset
Acc
ura
cy
0 1 5 15 25 all0.3
0.35
0.4
0.45
0.5
0.55
0.6
Serve dataset
Acc
ura
cy
Figure 1: Results for three procedures over 4 datases. The
horizontal axis corresponds to the context window size. Solid
line represents the result of FSGMM + binary, dashed line
denotes the result of CGDSVD + idf , and dotted line is the
result of CGDterm + idf . Square marker denotes ?2 based
feature ranking, while cross marker denotes freq based feature
ranking.
0 1 5 15 25 all0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Ave
rage
 Acc
urac
y
Hard datasetInterest datasetLine datasetServe dataset
Figure 2: Average accuracy over three procedures in Figure
1 as a function of context window size (horizontal axis) for 4
datasets.
quent words. Then occurrences of target word were
grouped into a pre-defined number of clusters. Sim-
ilar with many other algorithms, their algorithm also
required the cluster number to be provided.
In (Fukumoto and Suzuki, 1999), a term weight
learning algorithm was proposed for verb sense dis-
ambiguation, which can automatically extract nouns
co-occurring with verbs and identify the number of
senses of an ambiguous verb. The weakness of their
method is to assume that nouns co-occurring with
verbs are disambiguated in advance and the number
of senses of target verb is no less than two.
The algorithm in (Dorow and Widdows, 2003)
represented target noun word, its neighbors and
their relationships using a graph in which each node
denoted a noun and two nodes had an edge between
them if they co-occurred with more than a given
number of times. Then senses of target word were
iteratively learned by clustering the local graph of
similar words around target word. Their algorithm
required a threshold as input, which controlled the
number of senses.
5 Conclusion and Future Work
Our word sense learning algorithm combined two
novel ingredients: feature selection and order iden-
tification. Feature selection was formalized as a
constrained optimization problem, the output of
which was a set of important features to determine
word senses. Both cluster structure and cluster num-
ber were estimated by minimizing a MDL crite-
rion. Experimental results showed that our algo-
rithm can retrieve important features, estimate clus-
ter number automatically, and achieve better per-
formance in terms of average accuracy than CGD
algorithm which required cluster number as input.
Our word sense learning algorithm is unsupervised
in two folds: no requirement of sense tagged data,
and no requirement of predefinition of sense num-
ber, which enables the automatic discovery of word
senses from free text.
In our algorithm, we treat bag of words in lo-
cal contexts as features. It has been shown that
local collocations and morphology of target word
play important roles in word sense disambiguation
or discrimination (Leacock et al, 1998; Widdows,
2003). It is necessary to incorporate these more
structural information to improve the performance
of word sense learning.
References
Bouman, C. A., Shapiro, M., Cook, G. W., Atkins,
C. B., & Cheng, H. (1998) Cluster: An
Unsupervsied Algorithm for Modeling Gaus-
sian Mixtures. http://dynamo.ecn.purdue.edu/
?bouman/software/cluster/.
Dash, M., Choi, K., Scheuermann, P., & Liu, H. (2002)
Feature Selection for Clustering - A Filter Solution.
Proc. of IEEE Int. Conf. on Data Mining(pp. 115?
122).
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977)
Maximum likelihood from incomplete data using the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(B).
Dorow, B, & Widdows, D. (2003) Discovering Corpus-
Specific Word Senses. Proc. of the 10th Conf. of the
European Chapter of the Association for Computa-
tional Linguistics, Conference Companion (research
notes and demos)(pp.79?82).
Dy, J. G., & Brodley, C. E. (2000) Feature Subset Selec-
tion and Order Identification for Unsupervised Learn-
ing. Proc. of the 17th Int. Conf. on Machine Learn-
ing(pp. 247?254).
Fukumoto, F., & Suzuki, Y. (1999) Word Sense Disam-
biguation in Untagged Text Based on Term Weight
Learning. Proc. of the 9th Conf. of European Chapter
of the Association for Computational Linguistics(pp.
209?216).
Ide, N., & Ve?ronis, J. (1998) Word Sense Disambigua-
tion: The State of the Art. Computational Linguistics,
24:1, 1?41.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M. (2002)
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Law, M. H., Figueiredo, M., & Jain, A. K. (2002) Fea-
ture Selection in Mixture-Based Clustering. Advances
in Neural Information Processing Systems 15.
Leacock, C., Chodorow, M., & Miller A. G. (1998) Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24:1, 147?
165.
Levine, E., & Domany, E. (2001) Resampling Method
for Unsupervised Estimation of Cluster Validity. Neu-
ral Computation, Vol. 13, 2573?2593.
Mitra, P., Murthy, A. C., & Pal, K. S. (2002) Unsu-
pervised Feature Selection Using Feature Similarity.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 24:4, 301?312.
Modha, D. S., & Spangler, W. S. (2003) Feature Weight-
ing in k-Means Clustering. Machine Learning, 52:3,
217?237.
Pantel, P. & Lin, D. K. (2002) Discovering Word Senses
from Text. Proc. of ACM SIGKDD Conf. on Knowl-
edge Discovery and Data Mining(pp. 613-619).
Pedersen, T., & Bruce, R. (1997) Distinguishing Word
Senses in Untagged Text. Proceedings of the 2nd
Conference on Empirical Methods in Natural Lan-
guage Processing(pp. 197?207).
Pudil, P., Novovicova, J., & Kittler, J. (1994) Floating
Search Methods in Feature Selection. Pattern Recog-
nigion Letters, Vol. 15, 1119-1125.
Rissanen, J. (1978) Modeling by Shortest Data Descrip-
tion. Automatica, Vol. 14, 465?471.
Schu?tze, H. (1998) Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:1, 97?123.
Talavera, L. (1999) Feature Selection as a Preprocessing
Step for Hierarchical Clustering. Proc. of the 16th Int.
Conf. on Machine Learning(pp. 389?397).
Widdows, D. (2003) Unsupervised methods for devel-
oping taxonomies by combining syntactic and statisti-
cal information. Proc. of the Human Language Tech-
nology / Conference of the North American Chapter
of the Association for Computational Linguistics(pp.
276?283).
Proceedings of the 43rd Annual Meeting of the ACL, pages 395?402,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Word Sense Disambiguation Using Label Propagation Based
Semi-Supervised Learning
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
Shortage of manually sense-tagged data is
an obstacle to supervised word sense dis-
ambiguation methods. In this paper we in-
vestigate a label propagation based semi-
supervised learning algorithm for WSD,
which combines labeled and unlabeled
data in learning process to fully realize
a global consistency assumption: simi-
lar examples should have similar labels.
Our experimental results on benchmark
corpora indicate that it consistently out-
performs SVM when only very few la-
beled examples are available, and its per-
formance is also better than monolingual
bootstrapping, and comparable to bilin-
gual bootstrapping.
1 Introduction
In this paper, we address the problem of word sense
disambiguation (WSD), which is to assign an appro-
priate sense to an occurrence of a word in a given
context. Many methods have been proposed to deal
with this problem, including supervised learning al-
gorithms (Leacock et al, 1998), semi-supervised
learning algorithms (Yarowsky, 1995), and unsuper-
vised learning algorithms (Schu?tze, 1998).
Supervised sense disambiguation has been very
successful, but it requires a lot of manually sense-
tagged data and can not utilize raw unannotated data
that can be cheaply acquired. Fully unsupervised
methods do not need the definition of senses and
manually sense-tagged data, but their sense cluster-
ing results can not be directly used in many NLP
tasks since there is no sense tag for each instance in
clusters. Considering both the availability of a large
amount of unlabelled data and direct use of word
senses, semi-supervised learning methods have re-
ceived great attention recently.
Semi-supervised methods for WSD are character-
ized in terms of exploiting unlabeled data in learning
procedure with the requirement of predefined sense
inventory for target words. They roughly fall into
three categories according to what is used for su-
pervision in learning process: (1) using external re-
sources, e.g., thesaurus or lexicons, to disambiguate
word senses or automatically generate sense-tagged
corpus, (Lesk, 1986; Lin, 1997; McCarthy et al,
2004; Seo et al, 2004; Yarowsky, 1992), (2) exploit-
ing the differences between mapping of words to
senses in different languages by the use of bilingual
corpora (e.g. parallel corpora or untagged monolin-
gual corpora in two languages) (Brown et al, 1991;
Dagan and Itai, 1994; Diab and Resnik, 2002; Li and
Li, 2004; Ng et al, 2003), (3) bootstrapping sense-
tagged seed examples to overcome the bottleneck of
acquisition of large sense-tagged data (Hearst, 1991;
Karov and Edelman, 1998; Mihalcea, 2004; Park et
al., 2000; Yarowsky, 1995).
As a commonly used semi-supervised learning
method for WSD, bootstrapping algorithm works
by iteratively classifying unlabeled examples and
adding confidently classified examples into labeled
dataset using a model learned from augmented la-
beled dataset in previous iteration. It can be found
that the affinity information among unlabeled ex-
amples is not fully explored in this bootstrapping
process. Bootstrapping is based on a local consis-
tency assumption: examples close to labeled exam-
ples within same class will have same labels, which
is also the assumption underlying many supervised
learning algorithms, such as kNN.
Recently a promising family of semi-supervised
learning algorithms are introduced, which can ef-
fectively combine unlabeled data with labeled data
395
in learning process by exploiting cluster structure
in data (Belkin and Niyogi, 2002; Blum et al,
2004; Chapelle et al, 1991; Szummer and Jaakkola,
2001; Zhu and Ghahramani, 2002; Zhu et al, 2003).
Here we investigate a label propagation based semi-
supervised learning algorithm (LP algorithm) (Zhu
and Ghahramani, 2002) for WSD, which works by
representing labeled and unlabeled examples as ver-
tices in a connected graph, then iteratively propagat-
ing label information from any vertex to nearby ver-
tices through weighted edges, finally inferring the
labels of unlabeled examples after this propagation
process converges.
Compared with bootstrapping, LP algorithm is
based on a global consistency assumption. Intu-
itively, if there is at least one labeled example in each
cluster that consists of similar examples, then unla-
beled examples will have the same labels as labeled
examples in the same cluster by propagating the la-
bel information of any example to nearby examples
according to their proximity.
This paper is organized as follows. First, we will
formulate WSD problem in the context of semi-
supervised learning in section 2. Then in section
3 we will describe LP algorithm and discuss the
difference between a supervised learning algorithm
(SVM), bootstrapping algorithm and LP algorithm.
Section 4 will provide experimental results of LP al-
gorithm on widely used benchmark corpora. Finally
we will conclude our work and suggest possible im-
provement in section 5.
2 Problem Setup
Let X = {xi}ni=1 be a set of contexts of occur-
rences of an ambiguous word w, where xi repre-
sents the context of the i-th occurrence, and n is
the total number of this word?s occurrences. Let
S = {sj}cj=1 denote the sense tag set of w. The first
l examples xg(1 ? g ? l) are labeled as yg (yg ? S)
and other u (l+u = n) examples xh(l+1 ? h ? n)
are unlabeled. The goal is to predict the sense of w
in context xh by the use of label information of xg
and similarity information among examples in X .
The cluster structure in X can be represented as a
connected graph, where each vertex corresponds to
an example, and the edge between any two examples
xi and xj is weighted so that the closer the vertices
in some distance measure, the larger the weight as-
sociated with this edge. The weights are defined as
follows: Wij = exp(?
d2ij
?2 ) if i 6= j and Wii = 0
(1 ? i, j ? n), where dij is the distance (ex. Euclid-
ean distance) between xi and xj , and ? is used to
control the weight Wij .
3 Semi-supervised Learning Method
3.1 Label Propagation Algorithm
In LP algorithm (Zhu and Ghahramani, 2002), label
information of any vertex in a graph is propagated
to nearby vertices through weighted edges until a
global stable stage is achieved. Larger edge weights
allow labels to travel through easier. Thus the closer
the examples, more likely they have similar labels
(the global consistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 ? Nn?c represent initial soft labels at-
tached to vertices, where Y 0ij = 1 if yi is sj and 0
otherwise. Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent with the
labeling in labeled data, and the initialization of Y 0U
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set ? as the aver-
age distance between labeled examples from differ-
ent classes.
Define n ? n probability transition matrix Tij =
P (j ? i) = Wij?n
k=1 Wkj
, where Tij is the probability
to jump from example xj to example xi.
Compute the row-normalized matrix T by T ij =
Tij/
?n
k=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
396
?2 ?1 0 1 2 3 4
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
labeled +1
unlabeled
labeled ?1
(a) Dataset with Two?Moon Pattern (b) SVM 
(c) Bootstrapping (d) Ideal Classification 
A 8
A 9
B8 
B9
A10 
B10 
A0 
B0 
Figure 1: Classification result on two-moon pattern dataset.
(a) Two-moon pattern dataset with two labeled points, (b) clas-
sification result by SVM, (c) labeling procedure of bootstrap-
ping algorithm, (d) ideal classification.
Then LP algorithm is defined as follows:
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Y t+1 = TY t;
3. Clamp labeled data by replacing the top l row
of Y t+1 with Y 0L . Repeat from step 2 until Y t con-
verges;
4. Assign xh(l + 1 ? h ? n) with a label sj? ,
where j? = argmaxjYhj .
This algorithm has been shown to converge to
a unique solution, which is Y?U = limt?? Y tU =
(I ? T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of Y 0U is not im-
portant, since Y 0U does not affect the estimation of
Y?U . I is u ? u identity matrix. T uu and T ul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
3.2 Comparison between SVM, Bootstrapping
and LP
For WSD, SVM is one of the state of the art super-
vised learning algorithms (Mihalcea et al, 2004),
while bootstrapping is one of the state of the art
semi-supervised learning algorithms (Li and Li,
2004; Yarowsky, 1995). For comparing LP with
SVM and bootstrapping, let us consider a dataset
with two-moon pattern shown in Figure 1(a). The
upper moon consists of 9 points, while the lower
moon consists of 13 points. There is only one la-
beled point in each moon, and other 20 points are un-
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
(a) Minimum Spanning Tree (b) t=1 
(c) t=7 (d) t=10
(e) t=12 (f) t=100
B 
A 
C 
Figure 2: Classification result of LP on two-moon pattern
dataset. (a) Minimum spanning tree of this dataset. The conver-
gence process of LP algorithm with t varying from 1 to 100 is
shown from (b) to (f).
labeled. The distance metric is Euclidian distance.
We can see that the points in one moon should be
more similar to each other than the points across the
moons.
Figure 1(b) shows the classification result of
SVM. Vertical line denotes classification hyper-
plane, which has the maximum separating margin
with respect to the labeled points in two classes. We
can see that SVM does not work well when labeled
data can not reveal the structure (two moon pattern)
in each class. The reason is that the classification
hyperplane was learned only from labeled data. In
other words, the coherent structure (two-moon pat-
tern) in unlabeled data was not explored when infer-
ring class boundary.
Figure 1(c) shows bootstrapping procedure using
kNN (k=1) as base classifier with user-specified pa-
rameter b = 1 (the number of added examples from
unlabeled data into classified data for each class in
each iteration). Termination condition is that the dis-
tance between labeled and unlabeled points is more
than inter-class distance (the distance between A0
and B0). Each arrow in Figure 1(c) represents
one classification operation in each iteration for each
class. After eight iterations, A1 ? A8 were tagged
397
as +1, and B1 ? B8 were tagged as ?1, while
A9 ? A10 and B9 ? B10 were still untagged. Then
at the ninth iteration, A9 was tagged as +1 since the
label of A9 was determined only by labeled points in
kNN model: A9 is closer to any point in {A0 ? A8}
than to any point in {B0 ? B8}, regardless of the
intrinsic structure in data: A9 ? A10 and B9 ? B10
are closer to points in lower moon than to points in
upper moon. In other words, bootstrapping method
uses the unlabeled data under a local consistency
based strategy. This is the reason that two points A9
and A10 are misclassified (shown in Figure 1(c)).
From above analysis we see that both SVM and
bootstrapping are based on a local consistency as-
sumption.
Finally we ran LP on a connected graph-minimum
spanning tree generated for this dataset, shown in
Figure 2(a). A, B, C represent three points, and
the edge A ? B connects the two moons. Figure
2(b)- 2(f) shows the convergence process of LP with
t increasing from 1 to 100. When t = 1, label in-
formation of labeled data was pushed to only nearby
points. After seven iteration steps (t = 7), point B
in upper moon was misclassified as ?1 since it first
received label information from point A through the
edge connecting two moons. After another three it-
eration steps (t=10), this misclassified point was re-
tagged as +1. The reason of this self-correcting be-
havior is that with the push of label information from
nearby points, the value of YB,+1 became higher
than YB,?1. In other words, the weight of edge
B ? C is larger than that of edge B ? A, which
makes it easier for +1 label of point C to travel to
point B. Finally, when t ? 12 LP converged to a
fixed point, which achieved the ideal classification
result.
4 Experiments and Results
4.1 Experiment Design
For empirical comparison with SVM and bootstrap-
ping, we evaluated LP on widely used benchmark
corpora - ?interest?, ?line? 1 and the data in English
lexical sample task of SENSEVAL-3 (including all
57 English words ) 2.
1Available at http://www.d.umn.edu/?tpederse/data.html
2Available at http://www.senseval.org/senseval3
Table 1: The upper two tables summarize accuracies (aver-
aged over 20 trials) and paired t-test results of SVM and LP on
SENSEVAL-3 corpus with percentage of training set increasing
from 1% to 100%. The lower table lists the official result of
baseline (using most frequent sense heuristics) and top 3 sys-
tems in ELS task of SENSEVAL-3.
Percentage SVM LPcosine LPJS
1% 24.9?2.7% 27.5?1.1% 28.1?1.1%
10% 53.4?1.1% 54.4?1.2% 54.9?1.1%
25% 62.3?0.7% 62.3?0.7% 63.3?0.9%
50% 66.6?0.5% 65.7?0.5% 66.9?0.6%
75% 68.7?0.4% 67.3?0.4% 68.7?0.3%
100% 69.7% 68.4% 70.3%
Percentage SVM vs. LPcosine SVM vs. LPJS
p-value Sign. p-value Sign.
1% 8.7e-004 ? 8.5e-005 ?
10% 1.9e-006 ? 1.0e-008 ?
25% 9.2e-001 ? 3.0e-006 ?
50% 1.9e-006 ? 6.2e-002 ?
75% 7.4e-013 ? 7.1e-001 ?
100% - - - -
Systems Baseline htsa3 IRST-Kernels nusels
Accuracy 55.2% 72.9% 72.6% 72.4%
We used three types of features to capture con-
textual information: part-of-speech of neighboring
words with position information, unordered sin-
gle words in topical context, and local collocations
(as same as the feature set used in (Lee and Ng,
2002) except that we did not use syntactic relations).
For SVM, we did not perform feature selection on
SENSEVAL-3 data since feature selection deterio-
rates its performance (Lee and Ng, 2002). When
running LP on the three datasets, we removed the
features with occurrence frequency (counted in both
training set and test set) less than 3 times.
We investigated two distance measures for LP: co-
sine similarity and Jensen-Shannon (JS) divergence
(Lin, 1991).
For the three datasets, we constructed connected
graphs following (Zhu et al, 2003): two instances
u, v will be connected by an edge if u is among v?s
k nearest neighbors, or if v is among u?s k nearest
neighbors as measured by cosine or JS distance mea-
sure. For ?interest? and ?line? corpora, k is 10 (fol-
lowing (Zhu et al, 2003)), while for SENSEVAL-3
data, k is 5 since the size of dataset for each word
in SENSEVAL-3 is much less than that of ?interest?
and ?line? datasets.
398
4.2 Experiment 1: LP vs. SVM
In this experiment, we evaluated LP and SVM
3 on the data of English lexical sample task in
SENSEVAL-3. We used l examples from training
set as labeled data, and the remaining training ex-
amples and all the test examples as unlabeled data.
For each labeled set size l, we performed 20 trials.
In each trial, we randomly sampled l labeled exam-
ples for each word from training set. If any sense
was absent from the sampled labeled set, we redid
the sampling. We conducted experiments with dif-
ferent values of l, including 1%?Nw,train, 10%?
Nw,train, 25%?Nw,train, 50%?Nw,train, 75%?
Nw,train, 100%?Nw,train (Nw,train is the number
of examples in training set of word w). SVM and LP
were evaluated using accuracy 4 (fine-grained score)
on test set of SENSEVAL-3.
We conducted paired t-test on the accuracy fig-
ures for each value of l. Paired t-test is not run when
percentage= 100%, since there is only one paired
accuracy figure. Paired t-test is usually used to esti-
mate the difference in means between normal pop-
ulations based on a set of random paired observa-
tions. {?, ?}, {<, >}, and ? correspond to p-
value ? 0.01, (0.01, 0.05], and > 0.05 respectively.
? (or ?) means that the performance of LP is sig-
nificantly better (or significantly worse) than SVM.
< (or >) means that the performance of LP is better
(or worse) than SVM.?means that the performance
of LP is almost as same as SVM.
Table 1 reports the average accuracies and paired
t-test results of SVM and LP with different sizes
of labled data. It also lists the official results of
baseline method and top 3 systems in ELS task of
SENSEVAL-3.
From Table 1, we see that with small labeled
dataset (percentage of labeled data ? 10%), LP per-
forms significantly better than SVM. When the per-
centage of labeled data increases from 50% to 75%,
the performance of LPJS and SVM become almost
same, while LPcosine performs significantly worse
than SVM.
3we used linear SVM light, available at
http://svmlight.joachims.org/.
4If there are multiple sense tags for an instance in training
set or test set, then only the first tag is considered as correct
answer. Furthermore, if the answer of the instance in test set is
?U?, then this instance will be removed from test set.
Table 2: Accuracies from (Li and Li, 2004) and average ac-
curacies of LP with c ? b labeled examples on ?interest? and
?line? corpora. Major is a baseline method in which they al-
ways choose the most frequent sense. MB-D denotes monolin-
gual bootstrapping with decision list as base classifier, MB-B
represents monolingual bootstrapping with ensemble of Naive
Bayes as base classifier, and BB is bilingual bootstrapping with
ensemble of Naive Bayes as base classifier.
Ambiguous Accuracies from (Li and Li, 2004)
words Major MB-D MB-B BB
interest 54.6% 54.7% 69.3% 75.5%
line 53.5% 55.6% 54.1% 62.7%
Ambiguous Our results
words #labeled examples LPcosine LPJS
interest 4?15=60 80.2?2.0% 79.8?2.0%
line 6?15=90 60.3?4.5% 59.4?3.9%
4.3 Experiment 2: LP vs. Bootstrapping
Li and Li (2004) used ?interest? and ?line? corpora
as test data. For the word ?interest?, they used its
four major senses. For comparison with their re-
sults, we took reduced ?interest? corpus (constructed
by retaining four major senses) and complete ?line?
corpus as evaluation data. In their algorithm, c is
the number of senses of ambiguous word, and b
(b = 15) is the number of examples added into clas-
sified data for each class in each iteration of boot-
strapping. c ? b can be considered as the size of
initial labeled data in their bootstrapping algorithm.
We ran LP with 20 trials on reduced ?interest? cor-
pus and complete ?line? corpus. In each trial, we
randomly sampled b labeled examples for each sense
of ?interest? or ?line? as labeled data. The rest
served as both unlabeled data and test data.
Table 2 summarizes the average accuracies of LP
on the two corpora. It also lists the accuracies of
monolingual bootstrapping algorithm (MB), bilin-
gual bootstrapping algorithm (BB) on ?interest? and
?line? corpora. We can see that LP performs much
better than MB-D and MB-B on both ?interest? and
?line? corpora, while the performance of LP is com-
parable to BB on these two corpora.
4.4 An Example: Word ?use?
For investigating the reason for LP to outperform
SVM and monolingual bootstrapping, we used the
data of word ?use? in English lexical sample task of
SENSEVAL-3 as an example (totally 26 examples
in training set and 14 examples in test set). For data
399
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
(a) Initial Setting (b) Ground?truth
(c) SVM (d) Bootstrapping
(e) Bootstrapping (f) LP
B A 
C 
Figure 3: Comparison of sense disambiguation results be-
tween SVM, monolingual bootstrapping and LP on word ?use?.
(a) only one labeled example for each sense of word ?use?
as training data before sense disambiguation (? and ? denote
the unlabeled examples in SENSEVAL-3 training set and test
set respectively, and other five symbols (+, ?, ?, ?, and ?)
represent the labeled examples with different sense tags sam-
pled from SENSEVAL-3 training set.), (b) ground-truth re-
sult, (c) classification result on SENSEVAL-3 test set by SVM
(accuracy= 314 = 21.4%), (d) classified data after bootstrap-
ping, (e) classification result on SENSEVAL-3 training set and
test set by 1NN (accuracy= 614 = 42.9% ), (f) classifica-
tion result on SENSEVAL-3 training set and test set by LP
(accuracy= 1014 = 71.4% ).
visualization, we conducted unsupervised nonlinear
dimensionality reduction5 on these 40 feature vec-
tors with 210 dimensions. Figure 3 (a) shows the
dimensionality reduced vectors in two-dimensional
space. We randomly sampled only one labeled ex-
ample for each sense of word ?use? as labeled data.
The remaining data in training set and test set served
as unlabeled data for bootstrapping and LP. All of
these three algorithms are evaluated using accuracy
on test set.
From Figure 3 (c) we can see that SVM misclassi-
5We used Isomap to perform dimensionality reduction by
computing two-dimensional, 39-nearest-neighbor-preserving
embedding of 210-dimensional input. Isomap is available at
http://isomap.stanford.edu/.
fied many examples from class + into class ? since
using only features occurring in training set can not
reveal the intrinsic structure in full dataset.
For comparison, we implemented monolingual
bootstrapping with kNN (k=1) as base classifier.
The parameter b is set as 1. Only b unlabeled ex-
amples nearest to labeled examples and with the
distance less than dinter?class (the minimum dis-
tance between labeled examples with different sense
tags) will be added into classified data in each itera-
tion till no such unlabeled examples can be found.
Firstly we ran this monolingual bootstrapping on
this dataset to augment initial labeled data. The re-
sulting classified data is shown in Figure 3 (d). Then
a 1NN model was learned on this classified data and
we used this model to perform classification on the
remaining unlabeled data. Figure 3 (e) reports the
final classification result by this 1NN model. We can
see that bootstrapping does not perform well since it
is susceptible to small noise in dataset. For example,
in Figure 3 (d), the unlabeled example B 6 happened
to be closest to labeled example A, then 1NN model
tagged example B with label ?. But the correct label
of B should be + as shown in Figure 3 (b). This
error caused misclassification of other unlabeled ex-
amples that should have label +.
In LP, the label information of example C can
travel to B through unlabeled data. Then example A
will compete with C and other unlabeled examples
around B when determining the label of B. In other
words, the labels of unlabeled examples are deter-
mined not only by nearby labeled examples, but also
by nearby unlabeled examples. Using this classifi-
cation strategy achieves better performance than the
local consistency based strategy adopted by SVM
and bootstrapping.
4.5 Experiment 3: LPcosine vs. LPJS
Table 3 summarizes the performance comparison
between LPcosine and LPJS on three datasets. We
can see that on SENSEVAL-3 corpus, LPJS per-
6In the two-dimensional space, example B is not the closest
example to A. The reason is that: (1) A is not close to most
of nearby examples around B, and B is not close to most of
nearby examples around A; (2) we used Isomap to maximally
preserve the neighborhood information between any example
and all other examples, which caused the loss of neighborhood
information between a few example pairs for obtaining a glob-
ally optimal solution.
400
Table 3: Performance comparison between LPcosine and
LPJS and the results of three model selection criteria are re-
ported in following two tables. In the lower table, < (or >)
means that the average value of function H(Qcosine) is lower
(or higher) than H(QJS), and it will result in selecting cosine
(or JS) as distance measure. Qcosine (or QJS) represents a ma-
trix using cosine similarity (or JS divergence). ? and ? denote
correct and wrong prediction results respectively, while ?means
that any prediction is acceptable.
LPcosine vs. LPJS
Data p-value Significance
SENSEVAL-3 (1%) 1.1e-003 ?
SENSEVAL-3 (10%) 8.9e-005 ?
SENSEVAL-3 (25%) 9.0e-009 ?
SENSEVAL-3 (50%) 3.2e-010 ?
SENSEVAL-3 (75%) 7.7e-013 ?
SENSEVAL-3 (100%) - -
interest 3.3e-002 >
line 8.1e-002 ?
H(D) H(W ) H(YU )
Data cos. vs. JS cos. vs. JS cos. vs. JS
SENSEVAL-3 (1%) > (?) > (?) < (?)
SENSEVAL-3 (10%) < (?) > (?) < (?)
SENSEVAL-3 (25%) < (?) > (?) < (?)
SENSEVAL-3 (50%) > (?) > (?) > (?)
SENSEVAL-3 (75%) > (?) > (?) > (?)
SENSEVAL-3 (100%) < (?) > (?) < (?)
interest < (?) > (?) < (?)
line > (?) > (?) > (?)
forms significantly better than LPcosine, but their
performance is almost comparable on ?interest? and
?line? corpora. This observation motivates us to au-
tomatically select a distance measure that will boost
the performance of LP on a given dataset.
Cross-validation on labeled data is not feasi-
ble due to the setting of semi-supervised learning
(l ? u). In (Zhu and Ghahramani, 2002; Zhu et
al., 2003), they suggested a label entropy criterion
H(YU ) for model selection, where Y is the label
matrix learned by their semi-supervised algorithms.
The intuition behind their method is that good para-
meters should result in confident labeling. Entropy
on matrix W (H(W )) is a commonly used measure
for unsupervised feature selection (Dash and Liu,
2000), which can be considered here. Another pos-
sible criterion for model selection is to measure the
entropy of c ? c inter-class distance matrix D cal-
culated on labeled data (denoted as H(D)), where
Di,j represents the average distance between the i-
th class and the j-th class. We will investigate three
criteria, H(D), H(W ) and H(YU ), for model se-
lection. The distance measure can be automatically
selected by minimizing the average value of function
H(D), H(W ) or H(YU ) over 20 trials.
Let Q be the M ?N matrix. Function H(Q) can
measure the entropy of matrix Q, which is defined
as (Dash and Liu, 2000):
Si,j = exp (?? ?Qi,j), (1)
H(Q) = ?
M?
i=1
N?
j=1
(Si,j logSi,j + (1? Si,j) log (1? Si,j)),
(2)
where ? is positive constant. The possible value of ?
is? ln 0.5I? , where I? =
1
MN
?
i,j Qi,j . S is introduced
for normalization of matrix Q. For SENSEVAL-
3 data, we calculated an overall average score of
H(Q) by ?w
Nw,test?
w Nw,test
H(Qw). Nw,test is the
number of examples in test set of word w. H(D),
H(W ) and H(YU ) can be obtained by replacing Q
with D, W and YU respectively.
Table 3 reports the automatic prediction results
of these three criteria.
From Table 3, we can see that using H(W )
can consistently select the optimal distance measure
when the performance gap between LPcosine and
LPJS is very large (denoted by? or?). But H(D)
and H(YU ) fail to find the optimal distance measure
when only very few labeled examples are available
(percentage of labeled data ? 10%).
H(W ) measures the separability of matrix W .
Higher value of H(W ) means that distance mea-
sure decreases the separability of examples in full
dataset. Then the boundary between clusters is ob-
scured, which makes it difficult for LP to locate this
boundary. Therefore higher value of H(W ) results
in worse performance of LP.
When labeled dataset is small, the distances be-
tween classes can not be reliably estimated, which
results in unreliable indication of the separability
of examples in full dataset. This is the reason that
H(D) performs poorly on SENSEVAL-3 corpus
when the percentage of labeled data is less than 25%.
For H(YU ), small labeled dataset can not reveal
intrinsic structure in data, which may bias the esti-
mation of YU . Then labeling confidence (H(YU ))
can not properly indicate the performance of LP.
This may interpret the poor performance of H(YU )
on SENSEVAL-3 data when percentage ? 25%.
401
5 Conclusion
In this paper we have investigated a label propaga-
tion based semi-supervised learning algorithm for
WSD, which fully realizes a global consistency as-
sumption: similar examples should have similar la-
bels. In learning process, the labels of unlabeled ex-
amples are determined not only by nearby labeled
examples, but also by nearby unlabeled examples.
Compared with semi-supervised WSD methods in
the first and second categories, our corpus based
method does not need external resources, includ-
ing WordNet, bilingual lexicon, aligned parallel cor-
pora. Our analysis and experimental results demon-
strate the potential of this cluster assumption based
algorithm. It achieves better performance than SVM
when only very few labeled examples are avail-
able, and its performance is also better than mono-
lingual bootstrapping and comparable to bilingual
bootstrapping. Finally we suggest an entropy based
method to automatically identify a distance measure
that can boost the performance of LP algorithm on a
given dataset.
It has been shown that one sense per discourse
property can improve the performance of bootstrap-
ping algorithm (Li and Li, 2004; Yarowsky, 1995).
This heuristics can be integrated into LP algorithm
by setting weight Wi,j = 1 if the i-th and j-th in-
stances are in the same discourse.
In the future we may extend the evaluation of LP
algorithm and related cluster assumption based al-
gorithms using more benchmark data for WSD. An-
other direction is to use feature clustering technique
to deal with data sparseness and noisy feature prob-
lem.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments.
Z.Y. Niu is supported by A*STAR Graduate Schol-
arship.
References
Belkin, M., & Niyogi, P.. 2002. Using Manifold Structure for Partially Labeled
Classification. NIPS 15.
Blum, A., Lafferty, J., Rwebangira, R., & Reddy, R.. 2004. Semi-Supervised
Learning Using Randomized Mincuts. ICML-2004.
Brown P., Stephen, D.P., Vincent, D.P., & Robert, Mercer.. 1991. Word Sense
Disambiguation Using Statistical Methods. ACL-1991.
Chapelle, O., Weston, J., & Scho?lkopf, B. 2002. Cluster Kernels for Semi-
supervised Learning. NIPS 15.
Dagan, I. & Itai A.. 1994. Word Sense Disambiguation Using A Second Lan-
guage Monolingual Corpus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Dash, M., & Liu, H.. 2000. Feature Selection for Clustering. PAKDD(pp. 110?
121).
Diab, M., & Resnik. P.. 2002. An Unsupervised Method for Word Sense Tagging
Using Parallel Corpora. ACL-2002(pp. 255?262).
Hearst, M.. 1991. Noun Homograph Disambiguation using Local Context in
Large Text Corpora. Proceedings of the 7th Annual Conference of the UW
Centre for the New OED and Text Research: Using Corpora, 24:1, 1?41.
Karov, Y. & Edelman, S.. 1998. Similarity-Based Word Sense Disambiguation.
Computational Linguistics, 24(1): 41-59.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998. Using Corpus Statistics and
WordNet Relations for Sense Identification. Computational Linguistics, 24:1,
147?165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Evaluation of Knowledge Sources and
Learning Algorithms for Word Sense Disambiguation. EMNLP-2002, (pp.
41-48).
Lesk M.. 1986. Automated Word Sense Disambiguation Using Machine Read-
able Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. Pro-
ceedings of the ACM SIGDOC Conference.
Li, H. & Li, C.. 2004. Word Translation Disambiguation Using Bilingual Boot-
strapping. Computational Linguistics, 30(1), 1-22.
Lin, D.K.. 1997. Using Syntactic Dependency as Local Context to Resolve Word
Sense Ambiguity. ACL-1997.
Lin, J. 1991. Divergence Measures Based on the Shannon Entropy. IEEE Trans-
actions on Information Theory, 37:1, 145?150.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J.. 2004. Finding Predominant
Word Senses in Untagged Text. ACL-2004.
Mihalcea R.. 2004. Co-training and Self-training for Word Sense Disambigua-
tion. CoNLL-2004.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004. The SENSEVAL-3 English
Lexical Sample Task. SENSEVAL-2004.
Ng, H.T., Wang, B., & Chan, Y.S.. 2003. Exploiting Parallel Texts for Word
Sense Disambiguation: An Empirical Study. ACL-2003, pp. 455-462.
Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000. Word Sense Disambiguation by
Learning from Unlabeled Data. ACL-2000.
Schu?tze, H.. 1998. Automatic Word Sense Discrimination. Computational Lin-
guistics, 24:1, 97?123.
Seo, H.C., Chung, H.J., Rim, H.C., Myaeng. S.H., & Kim, S.H.. 2004. Unsu-
pervised Word Sense Disambiguation Using WordNet Relatives. Computer,
Speech and Language, 18:3, 253?273.
Szummer, M., & Jaakkola, T.. 2001. Partially Labeled Classification with Markov
Random Walks. NIPS 14.
Yarowsky, D.. 1995. Unsupervised Word Sense Disambiguation Rivaling Super-
vised Methods. ACL-1995, pp. 189-196.
Yarowsky, D.. 1992. Word Sense Disambiguation Using Statistical Models of
Roget?s Categories Trained on Large Corpora. COLING-1992, pp. 454-460.
Zhu, X. & Ghahramani, Z.. 2002. Learning from Labeled and Unlabeled Data
with Label Propagation. CMU CALD tech report CMU-CALD-02-107.
Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003. Semi-Supervised Learning Using
Gaussian Fields and Harmonic Functions. ICML-2003.
402
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 129?136,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Relation Extraction Using Label Propagation Based Semi-supervised
Learning
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
Shortage of manually labeled data is an
obstacle to supervised relation extraction
methods. In this paper we investigate a
graph based semi-supervised learning al-
gorithm, a label propagation (LP) algo-
rithm, for relation extraction. It represents
labeled and unlabeled examples and their
distances as the nodes and the weights of
edges of a graph, and tries to obtain a la-
beling function to satisfy two constraints:
1) it should be fixed on the labeled nodes,
2) it should be smooth on the whole graph.
Experiment results on the ACE corpus
showed that this LP algorithm achieves
better performance than SVM when only
very few labeled examples are available,
and it also performs better than bootstrap-
ping for the relation extraction task.
1 Introduction
Relation extraction is the task of detecting and
classifying relationships between two entities from
text. Many machine learning methods have been
proposed to address this problem, e.g., supervised
learning algorithms (Miller et al, 2000; Zelenko et
al., 2002; Culotta and Soresen, 2004; Kambhatla,
2004; Zhou et al, 2005), semi-supervised learn-
ing algorithms (Brin, 1998; Agichtein and Gravano,
2000; Zhang, 2004), and unsupervised learning al-
gorithms (Hasegawa et al, 2004).
Supervised methods for relation extraction per-
form well on the ACE Data, but they require a large
amount of manually labeled relation instances. Un-
supervised methods do not need the definition of
relation types and manually labeled data, but they
cannot detect relations between entity pairs and its
result cannot be directly used in many NLP tasks
since there is no relation type label attached to
each instance in clustering result. Considering both
the availability of a large amount of untagged cor-
pora and direct usage of extracted relations, semi-
supervised learning methods has received great at-
tention.
DIPRE (Dual Iterative Pattern Relation Expan-
sion) (Brin, 1998) is a bootstrapping-based sys-
tem that used a pattern matching system as clas-
sifier to exploit the duality between sets of pat-
terns and relations. Snowball (Agichtein and Gra-
vano, 2000) is another system that used bootstrap-
ping techniques for extracting relations from un-
structured text. Snowball shares much in common
with DIPRE, including the employment of the boot-
strapping framework as well as the use of pattern
matching to extract new candidate relations. The
third system approaches relation classification prob-
lem with bootstrapping on top of SVM, proposed by
Zhang (2004). This system focuses on the ACE sub-
problem, RDC, and extracts various lexical and syn-
tactic features for the classification task. However,
Zhang (2004)?s method doesn?t actually ?detect? re-
laitons but only performs relation classification be-
tween two entities given that they are known to be
related.
Bootstrapping works by iteratively classifying un-
labeled examples and adding confidently classified
examples into labeled data using a model learned
from augmented labeled data in previous iteration. It
129
can be found that the affinity information among un-
labeled examples is not fully explored in this boot-
strapping process.
Recently a promising family of semi-supervised
learning algorithm is introduced, which can effec-
tively combine unlabeled data with labeled data in
learning process by exploiting manifold structure
(cluster structure) in data (Belkin and Niyogi, 2002;
Blum and Chawla, 2001; Blum et al, 2004; Zhu
and Ghahramani, 2002; Zhu et al, 2003). These
graph-based semi-supervised methods usually de-
fine a graph where the nodes represent labeled and
unlabeled examples in a dataset, and edges (may be
weighted) reflect the similarity of examples. Then
one wants a labeling function to satisfy two con-
straints at the same time: 1) it should be close to the
given labels on the labeled nodes, and 2) it should be
smooth on the whole graph. This can be expressed
in a regularization framework where the first term
is a loss function, and the second term is a regu-
larizer. These methods differ from traditional semi-
supervised learning methods in that they use graph
structure to smooth the labeling function.
To the best of our knowledge, no work has been
done on using graph based semi-supervised learning
algorithms for relation extraction. Here we inves-
tigate a label propagation algorithm (LP) (Zhu and
Ghahramani, 2002) for relation extraction task. This
algorithm works by representing labeled and unla-
beled examples as vertices in a connected graph,
then propagating the label information from any ver-
tex to nearby vertices through weighted edges itera-
tively, finally inferring the labels of unlabeled exam-
ples after the propagation process converges. In this
paper we focus on the ACE RDC task1.
The rest of this paper is organized as follows. Sec-
tion 2 presents related work. Section 3 formulates
relation extraction problem in the context of semi-
supervised learning and describes our proposed ap-
proach. Then we provide experimental results of our
proposed method and compare with a popular su-
pervised learning algorithm (SVM) and bootstrap-
ping algorithm in Section 4. Finally we conclude
our work in section 5.
1 http://www.ldc.upenn.edu/Projects/ACE/, Three tasks of
ACE program: Entity Detection and Tracking (EDT), Rela-
tion Detection and Characterization (RDC), and Event Detec-
tion and Characterization (EDC)
2 The Proposed Method
2.1 Problem Definition
The problem of relation extraction is to assign an ap-
propriate relation type to an occurrence of two entity
pairs in a given context. It can be represented as fol-
lows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 denote the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity mention pairs. In this pa-
per, we set the mid-context window as the words be-
tween the two entity mentions and the pre- and post-
context as up to two words before and after the cor-
responding entity mention.
Let X = {xi}ni=1 be a set of contexts of occur-
rences of all the entity mention pairs, where xi rep-
resents the contexts of the i-th occurrence, and n is
the total number of occurrences. The first l exam-
ples (or contexts) are labeled as yg ( yg ? {rj}Rj=1,
rj denotes relation type and R is the total number of
relation types). The remaining u(u = n ? l) exam-
ples are unlabeled.
Intuitively, if two occurrences of entity mention
pairs have the similarity context, they tend to hold
the same relation type. Based on the assumption, we
define a graph where the vertices represent the con-
texts of labeled and unlabeled occurrences of entity
mention pairs, and the edge between any two ver-
tices xi and xj is weighted so that the closer the ver-
tices in some distance measure, the larger the weight
associated with this edge. Hence, the weights are de-
fined as follows:
Wij = exp(?
s2ij
?2 ) (2)
where sij is the similarity between xi and xj calcu-
lated by some similarity measures, e.g., cosine sim-
ilarity, and ? is used to scale the weights. In this
paper, we set ? as the average similarity between la-
beled examples from different classes.
2.2 A Label Propagation Algorithm
In the LP algorithm, the label information of any
vertex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage is
achieved. Larger edge weights allow labels to travel
130
through easier. Thus the closer the examples are, the
more likely they have similar labels.
We define soft label as a vector that is a proba-
bilistic distribution over all the classes. In the la-
bel propagation process, the soft label of each initial
labeled example is clamped in each iteration to re-
plenish label sources from these labeled data. Thus
the labeled data act like sources to push out labels
through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. Hopefully, the val-
ues of Wij across different classes would be as small
as possible and the values of Wij within the same
class would be as large as possible. This will make
label propagation to stay within the same class. This
label propagation process will make the labeling
function smooth on the graph.
Define an n? n probabilistic transition matrix T
Tij = P (j ? i) = wij?n
k=1 wkj
(3)
where Tij is the probability to jump from vertex xj
to vertex xi. We define a n ? R label matrix Y ,
where Yij representing the probabilities of vertex yi
to have the label rj .
Then the label propagation algorithm consists the
following main steps:
Step1 : Initialization
? Set the iteration index t = 0;
? Let Y 0 be the initial soft labels attached to
each vertex, where Y 0ij = 1 if yi is label rj
and 0 otherwise.
? Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent
with the labeling in labeled data and the
initialization of Y 0U can be arbitrary.
Step 2 : Propagate the labels of any vertex to
nearby vertices by Y t+1 = TY t , where
T is the row-normalized matrix of T , i.e.
Tij = Tij/
?
k Tik, which can maintain the
class probability interpretation.
Step 3 : Clamp the labeled data, that is, replace the
top l row of Y t+1 with Y 0L .
Step 4 : Repeat from step 2 until Y converges.
Step 5 : Assign xh(l + 1 ? h ? n) with a label:
yh = argmaxjYhj .
The above algorithm can ensure that the labeled
data YL never changes since it is clamped in Step 3.
Actually we are interested in only YU . This algo-
rithm has been shown to converge to a unique solu-
tion Y?U = limt?? Y tU = (I ? T?uu)?1T?ulY 0L (Zhu
and Ghahramani, 2002). Here, T?uu and T?ul are ac-
quired by splitting matrix T? after the l-th row and
the l-th column into 4 sub-matrices. And I is u? u
identity matrix. We can see that the initialization of
Y 0U in this solution is not important, since Y 0U does
not affect the estimation of Y?U .
3 Experiments and Results
3.1 Feature Set
Following (Zhang, 2004), we used lexical and syn-
tactic features in the contexts of entity pairs, which
are extracted and computed from the parse trees de-
rived from Charniak Parser (Charniak, 1999) and the
Chunklink script 2 written by Sabine Buchholz from
Tilburg University.
Words: Surface tokens of the two entities and
words in the three contexts.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZA-
TION, FACILITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all tokens in the two entities and words in
the three contexts.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two enti-
ties and words in the three contexts. The
?0? tag means that the word is not in any
chunk. The ?I-XP? tag means that this
word is inside an XP chunk. The ?B-XP?
by default means that the word is at the
beginning of an XP chunk.
? Grammatical function of the two enti-
ties and words in the three contexts. The
2Software available at http://ilk.uvt.nl/?sabine/chunklink/
131
last word in each chunk is its head, and
the function of the head is the function of
the whole chunk. ?NP-SBJ? means a NP
chunk as the subject of the sentence. The
other words in a chunk that are not the
head have ?NOFUNC? as their function.
? IOB-chains of the heads of the two enti-
ties. So-called IOB-chain, noting the syn-
tactic categories of all the constituents on
the path from the root node to this leaf
node of tree.
The position information is also specified in the
description of each feature above. For example,
word features with position information include:
1) WE1 (WE2): all words in e1 (e2)
2) WHE1 (WHE2): head word of e1 (e2)
3) WMNULL: no words in Cmid
4) WMFL: the only word in Cmid
5) WMF, WML, WM2, WM3, ...: first word, last
word, second word, third word, ...in Cmid when at
least two words in Cmid
6) WEL1, WEL2, ...: first word, second word, ...
before e1
7) WER1, WER2, ...: first word, second word, ...
after e2
We combine the above lexical and syntactic features
with their position information in the contexts to
form context vectors. Before that, we filter out low
frequency features which appeared only once in the
dataset.
3.2 Similarity Measures
The similarity sij between two occurrences of entity
pairs is important to the performance of the LP al-
gorithm. In this paper, we investigated two similar-
ity measures, cosine similarity measure and Jensen-
Shannon (JS) divergence (Lin, 1991). Cosine sim-
ilarity is commonly used semantic distance, which
measures the angle between two feature vectors. JS
divergence has ever been used as distance measure
for document clustering, which outperforms cosine
similarity based document clustering (Slonim et al,
2002). JS divergence measures the distance between
two probability distributions if feature vector is con-
sidered as probability distribution over features. JS
divergence is defined as follows:
Table 1: Frequency of Relation SubTypes in the ACE training
and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
JS(q, r) = 12 [DKL(q?p?) +DKL(r?p?)] (4)
DKL(q?p?) =
?
y
q(y)(log q(y)p?(y) ) (5)
DKL(r?p?) =
?
y
r(y)(log r(y)p?(y) ) (6)
where p? = 12(q + r) and JS(q, r) represents JS
divergence between probability distribution q(y) and
r(y) (y is a random variable), which is defined in
terms of KL-divergence.
3.3 Experimental Evaluation
3.3.1 Experiment Setup
We evaluated this label propagation based rela-
tion extraction method for relation subtype detection
and characterization task on the official ACE 2003
corpus. It contains 519 files from sources including
broadcast, newswire, and newspaper. We dealt with
only intra-sentence explicit relations and assumed
that all entities have been detected beforehand in the
EDT sub-task of ACE. Table 1 lists the types and
subtypes of relations for the ACE Relation Detection
and Characterization (RDC) task, along with their
132
Table 2: The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes.
The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 35.9 32.6 34.4 58.3 56.1 57.1 58.5 58.7 58.5
10% 51.3 41.5 45.9 64.5 57.5 60.7 64.6 62.0 63.2
25% 67.1 52.9 59.1 68.7 59.0 63.4 68.9 63.7 66.1
50% 74.0 57.8 64.9 69.9 61.8 65.6 70.1 64.1 66.9
75% 77.6 59.4 67.2 71.8 63.4 67.3 72.4 64.8 68.3
100% 79.8 62.9 70.3 73.9 66.9 70.2 74.2 68.2 71.1
Table 3: The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classification
on relation subtypes. The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 31.6 26.1 28.6 39.6 37.5 38.5 40.1 38.0 39.0
10% 39.1 32.7 35.6 45.9 39.6 42.5 46.2 41.6 43.7
25% 49.8 35.0 41.1 51.0 44.5 47.3 52.3 46.0 48.9
50% 52.5 41.3 46.2 54.1 48.6 51.2 54.9 50.8 52.7
75% 58.7 46.7 52.0 56.0 52.0 53.9 56.1 52.6 54.3
100% 60.8 48.9 54.2 56.2 52.3 54.1 56.3 52.9 54.6
frequency of occurrence in the ACE training set and
test set. We constructed labeled data by randomly
sampling some examples from ACE training data
and additionally sampling examples with the same
size from the pool of unrelated entity pairs for the
?NONE? class. We used the remaining examples in
the ACE training set and the whole ACE test set as
unlabeled data. The testing set was used for final
evaluation.
3.3.2 LP vs. SVM
Support Vector Machine (SVM) is a state of the
art technique for relation extraction task. In this ex-
periment, we use LIBSVM tool 3 with linear kernel
function.
For comparison between SVM and LP, we ran
SVM and LP with different sizes of labeled data
and evaluate their performance on unlabeled data
using precision, recall and F-measure. Firstly, we
ran SVM or LP algorithm to detect possible rela-
tions from unlabeled data. If an entity mention pair
is classified not to the ?NONE? class but to the other
24 subtype classes, then it has a relation. Then con-
struct labeled datasets with different sampling set
size l, including 1%?Ntrain, 10%?Ntrain, 25%?
Ntrain, 50%?Ntrain, 75%?Ntrain, 100%?Ntrain
(Ntrain is the number of examples in the ACE train-
3LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
ing set). If any relation subtype was absent from the
sampled labeled set, we redid the sampling. For each
size, we performed 20 trials and calculated average
scores on test set over these 20 random trials.
Table 2 reports the performance of SVM and LP
with different sizes of labled data for relation detec-
tion task. We used the same sampled labeled data in
LP as the training data for SVM model.
From Table 2, we see that both LPCosine and
LPJS achieve higher Recall than SVM. Specifically,
with small labeled dataset (percentage of labeled
data ? 25%), the performance improvement by LP
is significant. When the percentage of labeled data
increases from 50% to 100%, LPCosine is still com-
parable to SVM in F-measure while LPJS achieves
slightly better F-measure than SVM. On the other
hand, LPJS consistently outperforms LPCosine.
Table 3 reports the performance of relation clas-
sification by using SVM and LP with different sizes
of labled data. And the performance describes the
average values of Precision, Recall and F-measure
over major relation subtypes.
From Table 3, we see that LPCosine and LPJS out-
perform SVM by F-measure in almost all settings
of labeled data, which is due to the increase of Re-
call. With smaller labeled dataset (percentage of la-
beled data ? 50%), the gap between LP and SVM
is larger. When the percentage of labeled data in-
133
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
1% 10% 25% 50% 75% 100%
Percentage of Labeled Examples
F-m
ea
su
re SVM
LP_Cosine
LP_JS
 
Figure 1: Comparison of the performance of SVM
and LP with different sizes of labeled data
creases from 75% to 100%, the performance of LP
algorithm is still comparable to SVM. On the other
hand, the LP algorithm based on JS divergence con-
sistently outperforms the LP algorithm based on Co-
sine similarity. Figure 1 visualizes the accuracy of
three algorithms.
As shown in Figure 1, the gap between SVM
curve and LPJS curves is large when the percentage
of labeled data is relatively low.
3.3.3 An Example
In Figure 2, we selected 25 instances in train-
ing set and 15 instances in test set from the ACE
corpus,which covered five relation types. Using
Isomap tool 4, the 40 instances with 229 feature di-
mensions are visualized in a two-dimensional space
as the figure. We randomly sampled only one la-
beled example for each relation type from the 25
training examples as labeled data. Figure 2(a) and
2(b) show the initial state and ground truth result re-
spectively. Figure 2(c) reports the classification re-
sult on test set by SVM (accuracy = 415 = 26.7%),
and Figure 2(d) gives the classification result on both
training set and test set by LP (accuracy = 1115 =
73.3%).
Comparing Figure 2(b) and Figure 2(c), we find
that many examples are misclassified from class ?
to other class symbols. This may be caused that
SVMs method ignores the intrinsic structure in data.
For Figure 2(d), the labels of unlabeled examples
are determined not only by nearby labeled examples,
but also by nearby unlabeled examples, so using LP
4The tool is available at http://isomap.stanford.edu/.
     




	
      




	

     




	
      




	

Figure 2: An example: comparison of SVM and LP
algorithm on a data set from ACE corpus. ? and
4 denote the unlabeled examples in training set and
test set respectively, and other symbols (?,?,2,+
and 5) represent the labeled examples with respec-
tive relation type sampled from training set.
strategy achieves better performance than the local
consistency based SVM strategy when the size of
labeled data is quite small.
3.3.4 LP vs. Bootstrapping
In (Zhang, 2004), they perform relation classifi-
cation on ACE corpus with bootstrapping on top of
SVM. To compare with their proposed Bootstrapped
SVM algorithm, we use the same feature stream set-
ting and randomly selected 100 instances from the
training data as the size of initial labeled data.
Table 4 lists the performance of the bootstrapped
SVM method from (Zhang, 2004) and LP method
with 100 seed labeled examples for relation type
classification task. We can see that LP algorithm
outperforms the bootstrapped SVM algorithm on
four relation type classification tasks, and perform
comparably on the relation ?SOC? classification
task.
4 Discussion
In this paper,we have investigated a graph-based
semi-supervised learning approach for relation ex-
traction problem. Experimental results showed that
the LP algorithm performs better than SVM and
134
Table 4: Comparison of the performance of the bootstrapped SVM method from (Zhang, 2004) and LP method with 100 seed
labeled examples for relation type classification task.
Bootstrapping LPJS
Relation type P R F P R F
ROLE 78.5 69.7 73.8 81.0 74.7 77.7
PART 65.6 34.1 44.9 70.1 41.6 52.2
AT 61.0 84.8 70.9 74.2 79.1 76.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0
NEAR ? ? ? 13.7 12.5 13.0
Table 5: Comparison of the performance of previous methods on ACE RDC task.
Relation Dectection Relation Detection and Classification
on Types on Subtypes
Method P R F P R F P R F
Culotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -
Kambhatla (2004) Feature based, Maxi-
mum Entropy
- - - - - - 63.5 45.2 52.8
Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5
bootstrapping. We have some findings from these
results:
The LP based relation extraction method can use
the graph structure to smooth the labels of unlabeled
examples. Therefore, the labels of unlabeled exam-
ples are determined not only by the nearby labeled
examples, but also by nearby unlabeled examples.
For supervised methods, e.g., SVM, very few la-
beled examples are not enough to reveal the struc-
ture of each class. Therefore they can not perform
well, since the classification hyperplane was learned
only from few labeled data and the coherent struc-
ture in unlabeled data was not explored when in-
ferring class boundary. Hence, our LP-based semi-
supervised method achieves better performance on
both relation detection and classification when only
few labeled data is available. Bootstrapping
Currently most of works on the RDC task of
ACE focused on supervised learning methods Cu-
lotta and Soresen (2004; Kambhatla (2004; Zhou
et al (2005). Table 5 lists a comparison on re-
lation detection and classification of these meth-
ods. Zhou et al (2005) reported the best result as
63.1%/49.5%/55.5% in Precision/Recall/F-measure
on the relation subtype classification using feature
based method, which outperforms tree kernel based
method by Culotta and Soresen (2004). Compared
with Zhou et al?s method, the performance of LP al-
gorithm is slightly lower. It may be due to that we
used a much simpler feature set. Our work in this
paper focuses on the investigation of a graph based
semi-supervised learning algorithm for relation ex-
traction. In the future, we would like to use more ef-
fective feature sets Zhou et al (2005) or kernel based
similarity measure with LP for relation extraction.
5 Conclusion and Future Work
This paper approaches the problem of semi-
supervised relation extraction using a label propaga-
tion algorithm. It represents labeled and unlabeled
examples and their distances as the nodes and the
weights of edges of a graph, and tries to obtain a
labeling function to satisfy two constraints: 1) it
should be fixed on the labeled nodes, 2) it should
be smooth on the whole graph. In the classifica-
tion process, the labels of unlabeled examples are
determined not only by nearby labeled examples,
but also by nearby unlabeled examples. Our exper-
imental results demonstrated that this graph based
algorithm can achieve better performance than SVM
when only very few labeled examples are available,
and also outperforms the bootstrapping method for
relation extraction task.
In the future, we would like to investigate more
effective feature set or use feature selection to im-
prove the performance of this graph-based semi-
supervised relation extraction method.
135
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries (ACMDL?00).
Belkin M. and Niyogi P.. 2002. Using Manifold Struc-
ture for Partially Labeled Classification. Advances in
Neural Infomation Processing Systems 15.
Blum A. and Chawla S. 2001. Learning from Labeled
and Unlabeled Data Using Graph Mincuts. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning.
Blum A., Lafferty J., Rwebangira R. and Reddy R. 2004.
Semi-Supervised Learning Using Randomized Min-
cuts. In Proceedings of the 21th International Confer-
ence on Machine Learning..
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proceedings of WebDB Work-
shop at 6th International Conference on Extending
Database Technology (WebDB?98). pages 172-183.
Charniak E. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In Proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Hasegawa T., Sekine S. and Grishman R. 2004. Dis-
covering Relations among Named Entities from Large
Corpora, In Proceeding of Conference ACL2004.
Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In Proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics.. 21-26 July 2004. Barcelona, Spain.
Lin J. 1991. Divergence Measures Based on the Shan-
non Entropy. IEEE Transactions on Information The-
ory. Vol 37, No.1, 145-150.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In Proceedings of 6th Applied Natural Lan-
guage Processing Conference 29 April-4 may 2000,
Seattle USA.
Slonim, N., Friedman, N., and Tishby, N. 2002. Un-
supervised Document Classification Using Sequential
Information Maximization. In Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Yarowsky D. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics. pp.189-196.
Zelenko D., Aone C. and Richardella A. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In Proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction. In Proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
Zhu Xiaojin and Ghahramani Zoubin. 2002. Learning
from Labeled and Unlabeled Data with Label Propa-
gation. CMU CALD tech report CMU-CALD-02-107.
Zhu Xiaojin, Ghahramani Zoubin, and Lafferty J. 2003.
Semi-Supervised Learning Using Gaussian Fields and
Harmonic Functions. In Proceedings of the 20th Inter-
national Conference on Machine Learning.
136
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 89?96,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Relation Disambiguation Using Spectral Clustering
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised learn-
ing approach to disambiguate various rela-
tions between name entities by use of vari-
ous lexical and syntactic features from the
contexts. It works by calculating eigen-
vectors of an adjacency graph?s Laplacian
to recover a submanifold of data from a
high dimensionality space and then per-
forming cluster number estimation on the
eigenvectors. Experiment results on ACE
corpora show that this spectral cluster-
ing based approach outperforms the other
clustering methods.
1 Introduction
In this paper, we address the task of relation extrac-
tion, which is to find relationships between name en-
tities in a given context. Many methods have been
proposed to deal with this task, including supervised
learning algorithms (Miller et al, 2000; Zelenko et
al., 2002; Culotta and Soresen, 2004; Kambhatla,
2004; Zhou et al, 2005), semi-supervised learn-
ing algorithms (Brin, 1998; Agichtein and Gravano,
2000; Zhang, 2004), and unsupervised learning al-
gorithm (Hasegawa et al, 2004).
Among these methods, supervised learning is usu-
ally more preferred when a large amount of la-
beled training data is available. However, it is
time-consuming and labor-intensive to manually tag
a large amount of training data. Semi-supervised
learning methods have been put forward to mini-
mize the corpus annotation requirement. Most of
semi-supervised methods employ the bootstrapping
framework, which only need to pre-define some ini-
tial seeds for any particular relation, and then boot-
strap from the seeds to acquire the relation. How-
ever, it is often quite difficult to enumerate all class
labels in the initial seeds and decide an ?optimal?
number of them.
Compared with supervised and semi-supervised
methods, Hasegawa et al (2004)?s unsupervised ap-
proach for relation extraction can overcome the dif-
ficulties on requirement of a large amount of labeled
data and enumeration of all class labels. Hasegawa
et al (2004)?s method is to use a hierarchical cluster-
ing method to cluster pairs of named entities accord-
ing to the similarity of context words intervening be-
tween the named entities. However, the drawback of
hierarchical clustering is that it required providing
cluster number by users. Furthermore, clustering is
performed in original high dimensional space, which
may induce non-convex clusters hard to identified.
This paper presents a novel application of spec-
tral clustering technique to unsupervised relation ex-
traction problem. It works by calculating eigenvec-
tors of an adjacency graph?s Laplacian to recover a
submanifold of data from a high dimensional space,
and then performing cluster number estimation on
a transformed space defined by the first few eigen-
vectors. This method may help us find non-convex
clusters. It also does not need to pre-define the num-
ber of the context clusters or pre-specify the simi-
larity threshold for the clusters as Hasegawa et al
(2004)?s method.
The rest of this paper is organized as follows. Sec-
tion 2 formulates unsupervised relation extraction
and presents how to apply the spectral clustering
89
technique to resolve the task. Then section 3 reports
experiments and results. Finally we will give a con-
clusion about our work in section 4.
2 Unsupervised Relation Extraction
Problem
Assume that two occurrences of entity pairs with
similar contexts, are tend to hold the same relation
type. Thus unsupervised relation extraction prob-
lem can be formulated as partitioning collections of
entity pairs into clusters according to the similarity
of contexts, with each cluster containing only entity
pairs labeled by the same relation type. And then, in
each cluster, the most representative words are iden-
tified from the contexts of entity pairs to induce the
label of relation type. Here, we only focus on the
clustering subtask and do not address the relation
type labeling subtask.
In the next subsections we will describe our pro-
posed method for unsupervised relation extraction,
which includes: 1) Collect the context vectors in
which the entity mention pairs co-occur; 2) Cluster
these Context vectors.
2.1 Context Vector and Feature Design
Let X = {xi}ni=1 be the set of context vectors of oc-
currences of all entity mention pairs, where xi repre-
sents the context vector of the i-th occurrence, and n
is the total number of occurrences of all entity men-
tion pairs.
Each occurrence of entity mention pairs can be
denoted as follows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 represents the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity mention pairs respectively.
We extracted features from e1, e2, Cpre, Cmid,
Cpost to construct context vectors, which are com-
puted from the parse trees derived from Charniak
Parser (Charniak, 1999) and the Chunklink script 1
written by Sabine Buchholz from Tilburg University.
Words: Words in the two entities and three context
windows.
1 Software available at http://ilk.uvt.nl/ sabine/chunklink/
Entity Type: the entity type of both entities, which
can be PERSON, ORGANIZATION, FACIL-
ITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all words in the two entities and three con-
text windows.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two enti-
ties and three context windows. The ?0?
tag means that the word is outside of any
chunk. The ?I-XP? tag means that this
word is inside an XP chunk. The ?B-XP?
by default means that the word is at the
beginning of an XP chunk.
? Grammatical function of the two entities
and three context windows. The last word
in each chunk is its head, and the function
of the head is the function of the whole
chunk. ?NP-SBJ? means a NP chunk as
the subject of the sentence. The other
words in a chunk that are not the head have
?NOFUNC? as their function.
? IOB-chains of the heads of the two enti-
ties. So-called IOB-chain, noting the syn-
tactic categories of all the constituents on
the path from the root node to this leaf
node of tree.
We combine the above lexical and syntactic fea-
tures with their position information in the context
to form the context vector. Before that, we filter out
low frequency features which appeared only once in
the entire set.
2.2 Context Clustering
Once the context vectors of entity pairs are prepared,
we come to the second stage of our method: cluster
these context vectors automatically.
In recent years, spectral clustering technique has
received more and more attention as a powerful ap-
proach to a range of clustering problems. Among
the efforts on spectral clustering techniques (Weiss,
1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,
2001; Zha et al, 2001), we adopt a modified version
90
Table 1: Context Clustering with Spectral-based Clustering
technique.
Input: A set of context vectors X = {x1, x2, ..., xn},
X ? <n?d;
Output: Clustered data and number of clusters;
1. Construct an affinity matrix by Aij = exp(? s
2
ij
?2 ) if i 6=j, 0 if i = j. Here, sij is the similarity between xi and
xj calculated by Cosine similarity measure. and the free
distance parameter ?2 is used to scale the weights;
2. Normalize the affinity matrix A to create the matrix L =
D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)
element is the sum of A?s ith row;
3. Set q = 2;
4. Compute q eigenvectors of L with greatest eigenvalues.
Arrange them in a matrix Y .
5. Perform elongated K-means with q + 1 centers on Y ,
initializing the (q + 1)-th mean in the origin;
6. If the q+1-th cluster contains any data points, then there
must be at least an extra cluster; set q = q + 1 and go
back to step 4. Otherwise, algorithm stops and outputs
clustered data and number of clusters.
(Sanguinetti et al, 2005) of the algorithm by Ng et
al. (2001) because it can provide us model order se-
lection capability.
Since we do not know how many relation types
in advance and do not have any labeled relation
training examples at hand, the problem of model
order selection arises, i.e. estimating the ?opti-
mal? number of clusters. Formally, let k be the
model order, we need to find k in Equation: k =
argmaxk{criterion(k)}. Here, the criterion is de-
fined on the result of spectral clustering.
Table 1 shows the details of the whole algorithm
for context clustering, which contains two main
stages: 1) Transformation of Clustering Space (Step
1-4); 2) Clustering in the transformed space using
Elongated K-means algorithm (Step 5-6).
2.3 Transformation of Clustering Space
We represent each context vector of entity pair as a
node in an undirected graph. Each edge (i,j) in the
graph is assigned a weight that reflects the similarity
between two context vectors i and j. Hence, the re-
lation extraction task for entity pairs can be defined
as a partition of the graph so that entity pairs that
are more similar to each other, e.g. labeled by the
same relation type, belong to the same cluster. As a
relaxation of such NP-hard discrete graph partition-
ing problem, spectral clustering technique computes
eigenvalues and eigenvectors of a Laplacian matrix
related to the given graph, and construct data clus-
ters based on such spectral information.
Thus the starting point of context clustering is to
construct an affinity matrix A from the data, which
is an n ? n matrix encoding the distances between
the various points. The affinity matrix is then nor-
malized to form a matrix L by conjugating with the
the diagonal matrix D?1/2 which has as entries the
square roots of the sum of the rows of A. This is to
take into account the different spread of the various
clusters (points belonging to more rarified clusters
will have lower sums of the corresponding row of
A). It is straightforward to prove that L is positive
definite and has eigenvalues smaller or equal to 1,
with equality holding in at least one case.
Let K be the true number of clusters present in
the dataset. If K is known beforehand, the first K
eigenvectors of L will be computed and arranged as
columns in a matrix Y . Each row of Y corresponds
to a context vector of entity pair, and the above pro-
cess can be considered as transforming the original
context vectors in a d-dimensional space to new con-
text vectors in the K-dimensional space. Therefore,
the rows of Y will cluster upon mutually orthogonal
points on the K dimensional sphere,rather than on
the coordinate axes.
2.4 The Elongated K-means algorithm
As the step 5 of Table 1 shows, the result of elon-
gated K-means algorithm is used to detect whether
the number of clusters selected q is less than the true
number K, and allows one to iteratively obtain the
number of clusters.
Consider the case when the number of clusters q
is less than the true cluster number K present in the
dataset. In such situation, taking the first q < K
eigenvectors, we will be selecting a q-dimensional
subspace in the clustering space. As the rows of the
K eigenvectors clustered along mutually orthogo-
nal vectors, their projections in a lower dimensional
space will cluster along radial directions. Therefore,
the general picture will be of q clusters elongated in
the radial direction, with possibly some clusters very
near the origin (when the subspace is orthogonal to
some of the discarded eigenvectors).
Hence, the K-means algorithm is modified as
the elongated K-means algorithm to downweight
distances along radial directions and penalize dis-
91
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(a) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(b) 
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
(c) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(d) 
Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Three
elongated clusters in the 2D clustering space using
Spectral clustering: two dominant eigenvectors; (d)
The clustering result using Spectral-based clustering
(?2=0.05). (4,? and + denote examples in different
clusters)
tances along transversal directions. The elongated
K-means algorithm computes the distance of point
x from the center ci as follows:
? If the center is not very near the origin, cTi ci > ? (? is a
parameter to be fixed by the user), the distances are cal-
culated as: edist(x, ci) = (x ? ci)TM(x ? ci), where
M = 1? (Iq ?
cicTi
cTi ci
) + ? cic
T
i
cTi ci
, ? is the sharpness param-
eter that controls the elongation (the smaller, the more
elongated the clusters) 2.
? If the center is very near the origin,cTi ci < ?, the dis-
tances are measured using the Euclidean distance.
In each iteration of procedure in Table 1, elon-
gated K-means is initialized with q centers corre-
sponding to data points in different clusters and one
center in the origin. The algorithm then will drag the
center in the origin towards one of the clusters not
accounted for. Compute another eigenvector (thus
increasing the dimension of the clustering space to
q + 1) and repeat the procedure. Eventually, when
one reach as many eigenvectors as the number of
clusters present in the data, no points will be as-
signed to the center at the origin, leaving the cluster
empty. This is the signal to terminate the algorithm.
2.5 An example
Figure 1 visualized the clustering result of three cir-
cle dataset using K-means and Spectral-based clus-
tering. From Figure 1(b), we can see that K-means
can not separate the non-convex clusters in three cir-
cle dataset successfully since it is prone to local min-
imal. For spectral-based clustering, as the algorithm
described, initially, we took the two eigenvectors of
L with largest eigenvalues, which gave us a two-
dimensional clustering space. Then to ensure that
the two centers are initialized in different clusters,
one center is set as the point that is the farthest from
the origin, while the other is set as the point that
simultaneously farthest the first center and the ori-
gin. Figure 1(c) shows the three elongated clusters in
the 2D clustering space and the corresponding clus-
tering result of dataset is visualized in Figure 1(d),
which exploits manifold structure (cluster structure)
in data.
2 In this paper, the sharpness parameter ? is set to 0.2
92
Table 2: Frequency of Major Relation SubTypes in the ACE
training and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
3 Experiments and Results
3.1 Data Setting
Our proposed unsupervised relation extraction is
evaluated on ACE 2003 corpus, which contains 519
files from sources including broadcast, newswire,
and newspaper. We only deal with intra-sentence
explicit relations and assumed that all entities have
been detected beforehand in the EDT sub-task of
ACE. To verify our proposed method, we only col-
lect those pairs of entity mentions which have been
tagged relation types in the given corpus. Then the
relation type tags were removed to test the unsuper-
vised relation disambiguation. During the evalua-
tion procedure, the relation type tags were used as
ground truth classes. A break-down of the data by
24 relation subtypes is given in Table 2.
3.2 Evaluation method for clustering result
When assessing the agreement between clustering
result and manually annotated relation types (ground
truth classes), we would encounter the problem that
there was no relation type tags for each cluster in our
clustering results.
To resolve the problem, we construct a contin-
gency table T , where each entry ti,j gives the num-
ber of the instances that belong to both the i-th es-
timated cluster and j-th ground truth class. More-
over, to ensure that any two clusters do not share
the same labels of relation types, we adopt a per-
mutation procedure to find an one-to-one mapping
function ? from the ground truth classes (relation
types) TC to the estimated clustering result EC.
There are at most |TC| clusters which are assigned
relation type tags. And if the number of the esti-
mated clusters is less than the number of the ground
truth clusters, empty clusters should be added so that
|EC| = |TC| and the one-to-one mapping can be
performed, which can be formulated as the function:
?? = argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the in-
dex of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we adopt
Precision, Recall and F-measure to evaluate the
clustering result.
3.3 Experimental Design
We perform our unsupervised relation extraction on
the devtest set of ACE corpus and evaluate the al-
gorithm on relation subtype level. Firstly, we ob-
serve the influence of various variables, including
Distance Parameter ?2, Different Features, Context
Window Size. Secondly, to verify the effectiveness
of our method, we further compare it with other two
unsupervised methods.
3.3.1 Choice of Distance Parameter ?2
We simply search over ?2 and pick the value
that finds the best aligned set of clusters on the
transformed space. Here, the scattering criterion
trace(P?1W PB) is used to compare the cluster qual-
ity for different value of ?2 3, which measures the ra-
tio of between-cluster to within-cluster scatter. The
higher the trace(P?1W PB), the higher the cluster
quality.
In Table 3 and Table 4, with different settings of
feature set and context window size, we find out the
3 trace(P?1W PB) is trace of a matrix which is the sum of
its diagonal elements. PW is the within-cluster scatter matrix
as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xi ? mj)
t and PB
is the between-cluster scatter matrix as: PB =
?c
j=1(mj ?
m)(mj ? m)t, where m is the total mean vector and mj is
the mean vector for jth cluster and (Xj ? mj)t is the matrix
transpose of the column vector (Xj ?mj).
93
Table 3: Contribution of Different Features
Features ?2 cluster number trace value Precison Recall F-measure
Words 0.021 15 2.369 41.6% 30.2% 34.9%
+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%
+POS 0.017 18 3.206 37.8% 46.9% 41.8%
+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%
Table 4: Different Context Window Size Setting
Context Window Size ?2 cluster number trace value Precision Recall F-measure
0 0.016 18 3.576 37.6% 48.1% 42.2%
2 0.015 19 3.900 43.5% 49.4% 46.3%
5 0.020 21 2.225 29.3% 34.7% 31.7%
corresponding value of ?2 and cluster number which
maximize the trace value in searching for a range of
value ?2.
3.3.2 Contribution of Different Features
As the previous section presented, we incorporate
various lexical and syntactic features to extract rela-
tion. To measure the contribution of different fea-
tures, we report the performance by gradually in-
creasing the feature set, as Table 3 shows.
Table 3 shows that all of the four categories of fea-
tures contribute to the improvement of performance
more or less. Firstly,the addition of entity type fea-
ture is very useful, which improves F-measure by
6.6%. Secondly, adding POS features can increase
F-measure score but do not improve very much.
Thirdly, chunking features also show their great use-
fulness with increasing Precision/Recall/F-measure
by 5.7%/2.5%/4.5%.
We combine all these features to do all other eval-
uations in our experiments.
3.3.3 Setting of Context Window Size
We have mentioned in Section 2 that the context
vectors of entity pairs are derived from the contexts
before, between and after the entity mention pairs.
Hence, we have to specify the three context window
size first. In this paper, we set the mid-context win-
dow as everything between the two entity mentions.
For the pre- and post- context windows, we could
have different choices. For example, if we specify
the outer context window size as 2, then it means that
the pre-context (post-context)) includes two words
before (after) the first (second) entity.
For comparison of the effect of the outer context
of entity mention pairs, we conducted three different
Table 5: Performance of our proposed method (Spectral-
based clustering) compared with other unsupervised methods:
((Hasegawa et al, 2004))?s clustering method and K-means
clustering.
Precision Recall F-measure
Hasegawa?s Method1 38.7% 29.8% 33.7%
Hasegawa?s Method2 37.9% 36.0% 36.9%
Kmeans 34.3% 40.2% 36.8%
Our Proposed Method 43.5% 49.4% 46.3%
settings of context window size (0, 2, 5) as Table 4
shows. From this table we can find that with the con-
text window size setting, 2, the algorithm achieves
the best performance of 43.5%/49.4%/46.3% in
Precision/Recall/F-measure. With the context win-
dow size setting, 5, the performance becomes worse
because extending the context too much may include
more features, but at the same time, the noise also
increases.
3.3.4 Comparison with other Unsupervised
methods
In (Hasegawa et al, 2004), they preformed un-
supervised relation extraction based on hierarchical
clustering and they only used word features between
entity mention pairs to construct context vectors. We
reported the clustering results using the same clus-
tering strategy as Hasegawa et al (2004) proposed.
In Table 5, Hasegawa?s Method1 means the test used
the word feature as Hasegawa et al (2004) while
Hasegawa?s Method2 means the test used the same
feature set as our method. In both tests, we specified
the cluster number as the number of ground truth
classes.
We also approached the relation extraction prob-
lem using the standard clustering technique, K-
94
means, where we adopted the same feature set de-
fined in our proposed method to cluster the con-
text vectors of entity mention pairs and pre-specified
the cluster number as the number of ground truth
classes.
Table 5 reports the performance of our proposed
method comparing with the other two unsupervised
methods. Table 5 shows our proposed spectral based
method clearly outperforms the other two unsuper-
vised methods by 12.5% and 9.5% in F-measure re-
spectively. Moreover, the incorporation of various
lexical and syntactic features into Hasegawa et al
(2004)?s method2 makes it outperform Hasegawa et
al. (2004)?s method1 which only uses word feature.
3.4 Discussion
In this paper, we have shown that the modified spec-
tral clustering technique, with various lexical and
syntactic features derived from the context of entity
pairs, performed well on the unsupervised relation
extraction problem. Our experiments show that by
the choice of the distance parameter ?2, we can esti-
mate the cluster number which provides the tightest
clusters. We notice that the estimated cluster num-
ber is less than the number of ground truth classes
in most cases. The reason for this phenomenon may
be that some relation types can not be easily distin-
guished using the context information only. For ex-
ample, the relation subtypes ?Located?, ?Based-In?
and ?Residence? are difficult to disambiguate even
for human experts to differentiate.
The results also show that various lexical and
syntactic features contain useful information for the
task. Especially, although we did not concern the
dependency tree and full parse tree information as
other supervised methods (Miller et al, 2000; Cu-
lotta and Soresen, 2004; Kambhatla, 2004; Zhou et
al., 2005), the incorporation of simple features, such
as words and chunking information, still can provide
complement information for capturing the character-
istics of entity pairs. This perhaps dues to the fact
that two entity mentions are close to each other in
most of relations defined in ACE. Another observa-
tion from the result is that extending the outer con-
text window of entity mention pairs too much may
not improve the performance since the process may
incorporate more noise information and affect the
clustering result.
As regards the clustering technique, the spectral-
based clustering performs better than direct cluster-
ing, K-means. Since the spectral-based algorithm
works in a transformed space of low dimension-
ality, data can be easily clustered so that the al-
gorithm can be implemented with better efficiency
and speed. And the performance using spectral-
based clustering can be improved due to the reason
that spectral-based clustering overcomes the draw-
back of K-means (prone to local minima) and may
find non-convex clusters consistent with human in-
tuition.
Generally, from the point of view of unsu-
pervised resolution for relation extraction, our
approach already achieves best performance of
43.5%/49.4%/46.3% in Precision/Recall/F-measure
compared with other clustering methods.
4 Conclusion and Future work
In this paper, we approach unsupervised relation ex-
traction problem by using spectral-based clustering
technique with diverse lexical and syntactic features
derived from context. The advantage of our method
is that it doesn?t need any manually labeled relation
instances, and pre-definition the number of the con-
text clusters. Experiment results on the ACE corpus
show that our method achieves better performance
than other unsupervised methods, i.e.Hasegawa et
al. (2004)?s method and Kmeans-based method.
Currently we combine various lexical and syn-
tactic features to construct context vectors for clus-
tering. In the future we will further explore other
semantic information to assist the relation extrac-
tion problem. Moreover, instead of cosine similar-
ity measure to calculate the distance between con-
text vectors, we will try other distributional similar-
ity measures to see whether the performance of re-
lation extraction can be improved. In addition, if we
can find an effective unsupervised way to filter out
unrelated entity pairs in advance, it would make our
proposed method more practical.
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proc. of the 5th ACM International Conference on
Digital Libraries (ACMDL?00).
95
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proc. of WebDB Workshop at
6th International Conference on Extending Database
Technology (WebDB?98). pages 172-183.
Charniak E.. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12.. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Defense Advanced Research Projects Agency. 1995.
Proceedings of the Sixth Message Understanding Con-
ference (MUC-6) Morgan Kaufmann Publishers, Inc.
Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.
2004. Discovering Relations among Named Enti-
ties from Large Corpora, Proceeding of Conference
ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Kannan R., Vempala S., and Vetta A.. 2000. On cluster-
ing: Good,bad and spectral. In Proceedings of the 41st
Foundations of Computer Science. pages 367-380.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In proceedings of 6th Applied Natural Lan-
guage Processing Conference. 29 April-4 may 2000,
Seattle USA.
Ng Andrew.Y, Jordan M., and Weiss Y.. 2001. On spec-
tral clustering: Analysis and an algorithm. In Pro-
ceedings of Advances in Neural Information Process-
ing Systems. pages 849-856.
Sanguinetti G., Laidler J. and Lawrence N.. 2005. Au-
tomatic determination of the number of clusters us-
ing spectral algorithms.In: IEEE Machine Learning
for Signal Processing. 28-30 Sept 2005, Mystic, Con-
necticut, USA.
Shi J. and Malik.J. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence. 22(8):888-905.
Weiss Yair. 1999. Segmentation using eigenvectors: A
unifying view. ICCV(2). pp.975-982.
Zelenko D., Aone C. and Richardella A.. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001. Spec-
tral Relaxation for k-means clustering. In Neural In-
formation Processing Systems (NIPS2001). pages
1057-1064, 2001.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction, In proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
96
Optimizing Feature Set for Chinese Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This article describes the implementation of I2R
word sense disambiguation system (I2R ?WSD)
that participated in one senseval3 task: Chinese lex-
ical sample task. Our core algorithm is a supervised
Naive Bayes classifier. This classifier utilizes an op-
timal feature set, which is determined by maximiz-
ing the cross validated accuracy of NB classifier on
training data. The optimal feature set includes part-
of-speech with position information in local con-
text, and bag of words in topical context.
1 Introduction
Word sense disambiguation (WSD) is to assign ap-
propriate meaning to a given ambiguous word in
a text. Corpus based method is one of the suc-
cessful lines of research on WSD. Many supervised
learning algorithms have been applied for WSD,
ex. Bayesian learning (Leacock et al, 1998), ex-
emplar based learning (Ng and Lee, 1996), decision
list (Yarowsky, 2000), neural network (Towel and
Voorheest, 1998), maximum entropy method (Dang
et al, 2002), etc.. In this paper, we employ Naive
Bayes classifier to perform WSD.
Resolving the ambiguity of words usually relies
on the contexts of their occurrences. The feature
set used for context representation consists of lo-
cal and topical features. Local features include part
of speech tags of words within local context, mor-
phological information of target word, local collo-
cations, and syntactic relations between contextual
words and target word, etc.. Topical features are
bag of words occurred within topical context. Con-
textual features play an important role in providing
discrimination information for classifiers in WSD.
In other words, an informative feature set will help
classifiers to accurately disambiguate word senses,
but an uninformative feature set will deteriorate the
performance of classifiers. In this paper, we opti-
mize feature set by maximizing the cross validated
accuracy of Naive Bayes classifier on sense tagged
training data.
2 Naive Bayes Classifier
Let C = {c1, c2, ..., cL} represent class labels,
F = {f1, f2, ..., fM} be a set of features. The
value of fj , 1 ? j ? M , is 1 if fj is present in
the context of target word, otherwise 0. In classi-
fication process, the Naive Bayes classifier tries to
find the class that maximizes P (ci|F ), the proba-
bility of class ci given feature set F , 1 ? i ? L.
Assuming the independence between features, the
classification procedure can be formulated as:
i? = arg max
1?i?L
p(ci)
?M
j=1 p(fj |ci)?M
j=1 p(fj)
, (1)
where p(ci), p(fj |ci) and p(fj) are estimated using
maximum likelihood method. To avoid the effects
of zero counts when estimating p(fj |ci), the zero
counts of p(fj |ci) are replaced with p(ci)/N , where
N is the number of training examples.
3 Feature Set
For Chinese WSD, there are two strategies to extract
contextual information. One is based on Chinese
characters, the other is to utilize Chinese words and
related morphological or syntactic information. In
our system, context representation is based on Chi-
nese words, since words are less ambiguous than
characters.
We use two types of features for Chinese WSD:
local features and topical features. All of these fea-
tures are acquired from data at senseval3 without
utilization of any other knowledge resource.
3.1 Local features
Two sets of local features are investigated, which
are represented by LocalA and LocalB. Let nl de-
note the local context window size.
LocalA contains only part of speech tags
with position information: POS?nl , ...,
POS?1, POS0, POS+1, ..., POS+nl , where
POS?i (POS+i) is the part of speech (POS) of the
i-th words to the left (right) of target word w, and
POS0 is the POS of w.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
LocalB enriches the local context by including
the following features: local words with position in-
formation (W?nl , ..., W?1, W+1, ..., W+nl), bigram
templates ((W?nl , W?(nl?1)), ..., (W?1, W+1),
..., (W+(nl?1), W+nl)), local words with POS tags(W POS) (position information is not considered),
and part of speech tags with position information.
All of these POS tags, words, and bigrams are
gathered and each of them contributed as one fea-
ture. For a training or test example, the value of
some feature is 1 if it occurred in local context, oth-
erwise it is 0. In this paper, we investigate two val-
ues of nl for LocalA and LocalB, 1 and 2, which
results in four feature sets.
3.2 Topical features
We consider all Chinese words within a context
window size nt as topical features. For each training
or test example, senseval3 data provides one sen-
tence as the context of ambiguous word. In sense-
val3 Chinese training data, all contextual sentences
are segmented into words and tagged with part of
speech.
Words which contain non-Chinese character are
removed, and remaining words occurred within
context window size nt are gathered. Each remain-
ing word is considered as one feature. The value of
topical feature is 1 if it occurred within window size
nt, otherwise it is 0.
In later experiment, we set different values for nt,
ex. 1, 2, 3, 4, 5, 10, 20, 30, 40, 50. Our experimen-
tal result indicated that the accuracy of sense dis-
ambiguation is related to the value of nt. For differ-
ent ambiguous words, the value of nt which yields
best disambiguation accuracy is different. It is de-
sirable to determine an optimal value, n?t, for each
ambiguous word by maximizing the cross validated
accuracy.
4 Data Set
In Chinese lexical sample task, training data con-
sists of 793 sense-tagged examples for 20 ambigu-
ous Chinese words. Test data consists of 380 un-
tagged examples for the same 20 target words. Ta-
ble 1 shows the details of training data and test data.
5 Criterion for Evaluation of Feature Sets
In this paper, five fold cross validation method was
employed to estimate the accuracy of our classi-
fier, which was the criterion for evaluation of fea-
ture sets. All of the sense tagged examples of some
target word in senseval3 training data were shuf-
fled and divided into five equal folds. We used four
folds as training set and the remaining fold as test
set. This procedure was repeated five times under
different division between training set and test set.
The average accuracy over five runs is defined as the
accuracy of our classifier.
6 Evaluation of Feature Sets
Four feature sets were investigated:
FEATUREA1: LocalA with nl = 1, and topical
feature within optimal context window size n?t;
FEATUREA2: LocalA with nl = 2, and topical
feature within optimal context window size n?t;
FEATUREB1: LocalB with nl = 1, and topical
feature within optimal context window size n?t;
FEATUREB2: LocalB with nl = 2, and topical
feature within optimal context window size n?t.
We performed training and test procedure using
exactly same training and test set for each feature
set. For each word, the optimal value of topical con-
text window size n?t was determined by selecting a
minimal value of nt which maximized the cross val-
idated accuracy.
Table 2 summarizes the results of Naive Bayes
classifier using four feature sets evaluated on sen-
seval3 Chinese training data. Figure 1 shows the
accuracy of Naive Bayes classifier as a function of
topical context window size on four nouns and three
verbs. Several results should be noted specifically:
If overall accuracy over 20 Chinese charac-
ters is used as evaluation criterion for feature
set, the four feature sets can be sorted as fol-
lows: FEATUREA1 > FEATUREA2 ?
FEATUREB1 > FEATUREB2. This indi-
cated that simply increasing local window size or
enriching feature set by incorporating bigram tem-
plates, local word with position information, and lo-
cal words with POS tags did not improve the perfor-
mance of sense disambiguation.
In table 2, it showed that with FEATUREA1, the
optimal topical context window size was less than
10 words for 13 out of 20 target words. Figure
1 showed that for most of nouns and verbs, Naive
Bayes classifier achieved best disambiguation accu-
racy with small topical context window size (<10
words). This gives the evidence that for most of
Chinese words, including nouns and verbs, the near
distance context is more important than the long dis-
tance context for sense disambiguation.
7 Experimental Result
The empirical study in section 6 showed that FEA-
TUREA1 performed best among all the feature sets.
A Naive Bayes classifier with FEATUREA1 as fea-
ture set was learned from all the senseval3 Chinese
training data for each target word. Then we used
Table 1: Details of training data and test data in Chinese lexical sample task.
POS occurred # senses occurred
Ambiguous word in training data # training examples in training data # test examples
ba3wo4 n v vn 31 4 15
bao1 n nr q v 76 8 36
cai2liao4 n 20 2 10
chong1ji1 v vn 28 3 13
chuan1 v 28 3 14
di4fang1 b n 36 4 17
fen1zi3 n 36 2 16
huo2dong4 a v vn 36 5 16
lao3 Ng a an d j 57 6 26
lu4 n nr q 57 6 28
mei2you3 d v 30 3 15
qi3lai2 v 40 4 20
qian2 n nr 40 4 20
ri4zi5 n 48 3 21
shao3 Ng a ad j v 42 5 20
tu1chu1 a ad v 30 3 15
yan2jiu1 n v vn 30 3 15
yun4dong4 n nz v vn 54 3 27
zou3 v vn 49 5 24
zuo4 v 25 3 12
this classifier to determine the senses of occurrences
of target words in test data. The official result of
I2R?WSD system in Chinese lexical sample task
is listed below:
Precision: 60.40% (229.00 correct of 379.00 at-
tempted).
Recall: 60.40% (229.00 correct of 379.00 in to-
tal).
Attempted: 100.00% (379.00 attempted of
379.00 in total).
8 Conclusion
In this paper, we described the implementation of
I2R ? WSD system that participated in one sen-
seval3 task: Chinese lexical sample task. An op-
timal feature set was selected by maximizing the
cross validated accuracy of supervised Naive Bayes
classifier on sense-tagged data. The senses of occur-
rences of target words in test data were determined
using Naive Bayes classifier with optimal feature
set learned from training data. Our system achieved
60.40% precision and recall in Chinese lexical sam-
ple task.
References
Dang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D.
(2002) Simple Features for Chinese Word Sense
Disambiguation. In Proc. of COLING.
Leacock, C., Chodorow, M., & Miller G. A. (1998)
Using Corpus Statistics and WordNet Relations
for Sense Identification. Computational Linguis-
tics, 24:1, 147?165.
Mooney, R. J. (1996) Comparative Experiments on
Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. In Proc.
of EMNLP, pp. 82-91, Philadelphia, PA.
Ng, H. T., & Lee H. B. (1996) Integrating Multi-
ple Knowledge Sources to Disambiguate Word
Sense: An Exemplar-Based Approach. In Proc.
of ACL, pp. 40-47.
Pedersen, T. (2001) A Decision Tree of Bigrams is
an Accurate Predictor of Word Sense. In Proc. of
NAACL.
Towel, G., & Voorheest, E. M. (1998) Disambiguat-
ing Highly Ambiguous Words. Computational
Linguistics, 24:1, 125?146.
Yarowsky, D. (2000) Hierarchical Decision Lists
for Word Sense Disambiguation. Computers and
the Humanities, 34(1-2), 179?186.
Table 2: Accuracy of Naive Bayes classifier with different feature sets on Senseval3 Chinese training data.
FEATUREA1 FEATUREA2 FEATUREB1 FEATUREB2
Ambiguous word n?t Accuracy n?t Accuracy n?t Accuracy n?t Accuracy
ba3wo4 5 30.0 4 23.3 4 30.0 3 30.0
bao1 2 30.7 20 34.0 2 33.3 20 32.0
cai2liao4 2 85.0 2 80.0 2 75.0 2 60.0
chong1ji1 20 40.0 3 40.0 30 36.0 1 28.0
chuan1 3 72.0 5 68.0 3 56.0 5 64.0
di4fang1 2 74.3 1 62.9 1 71.4 1 65.7
fen1zi3 20 91.4 50 91.4 20 88.6 20 85.7
huo2dong4 5 40.0 20 51.4 10 42.9 4 40.0
lao3 3 49.1 4 47.3 3 52.7 20 52.7
lu4 1 83.6 2 78.2 2 81.8 1 76.4
mei2you3 20 50.0 20 47.9 4 43.3 3 50.0
qi3lai2 4 75.0 1 75.0 1 80.0 1 77.5
qian2 3 57.5 4 57.5 3 60.0 5 57.5
ri4zi5 4 62.2 4 57.8 10 55.6 4 55.6
shao3 4 45.0 3 50.0 10 42.5 20 50.0
tu1chu1 10 83.3 10 80.0 10 80.0 10 76.7
yan2jiu1 20 43.3 20 46.7 10 50.0 20 36.7
yun4dong4 10 64.0 10 66.0 10 62.0 10 58.0
zou3 5 44.4 5 44.4 4 51.1 4 51.1
zuo4 20 64.0 30 60.0 20 64.0 20 64.0
Overall 57.7 56.9 57.0 55.1
0 1 2 3 4 5 10 20 30 40 500.4
0.5
0.6
0.7
0.8
0.9
1
nt
Ac
cu
rac
y
0 1 2 3 4 5 10 20 30 40 500.3
0.4
0.5
0.6
0.7
0.8
nt
Ac
cu
rac
y
chuan1 
qi3lai2
zuo4   
cai2liao4
fen1zi3  
qian2    
ri4zi5   
Figure 1: Accuracy of Naive Bayes classifier with the optimal feature set FEATUREA1 on four nouns (top
figure) and three verbs (bottom figure). The horizontal axis represents the topical context window size.
Chinese Text Summarization Based on Thematic Area Detection 
Po Hu 
Department of Computer 
Science 
Central China Normal 
University 
Wuhan, China, 430079 
geminihupo@163.com 
Tingting He 
Department of Computer 
Science 
Central China Normal 
University 
Wuhan, China, 430079 
 hett@163.net 
Donghong Ji 
Institute for Infocomm 
Research 
Heng Mui Keng Terrace, 
Singapore, 119613 
dhji@i2r.a-star.edu.sg 
 
Abstract 
Automatic summarization is an active research 
area in natural language processing. This paper has 
proposed a special method that produces text 
summary by detecting thematic areas in Chinese 
document. The specificity of the method is that the 
produced summary can both cover many different 
themes and reduce its redundancy obviously at the 
same time. In this method, the detection of latent 
thematic areas is realized by adopting K-medoids 
clustering method as well as a novel clustering 
analysis method, which can be used to determine 
automatically K, the number of clusters.. In 
addition, a novel parameter, which is known as 
representation entropy, is used for summarization 
redundancy evaluation. Experimental results 
indicate a clear superiority of the proposed method  
over the traditional non-thematic-area-detection 
method under the proposed evaluation scheme 
when dealing with different genres of text 
documents with free style and flexible theme 
distribution. 
1 Introduction 
With the approaching information explosion, 
people begin to feel at a loss about the mass of 
information. Because the effectiveness  of the 
existing information retrieval technology is still 
unsatisfactory, it becomes a problem to efficiently 
find the information mostly related to the needs of 
customers retrieval results so that customers can 
easily accept or reject the retrieved information 
without needing to look at the original retrieval 
results. This paper has proposed a new 
summarization method, where K-medoid 
clustering method is applied to detect all possible 
partitions of thematic areas, and a novel clustering 
analysis method, which is based on a self-defined 
objective function, is applied to automatically 
determine K, the number of latent thematic areas in a 
document  
This method consists of three main stages: 1) Find 
out the thematic areas in the document by adopting the 
K-medoid clustering method (Kaufmann and 
Rousseeuw, 1987as well as a novel clustering analysis 
method. 2) From each thematic area, find a sentence 
which has the maximum semantic similarity value 
with this area as the representation. 3) Output the 
selected sentences to form the final summary 
according to their pos itions in the original document. 
To validate the effectiveness of the proposed 
method, use this method as well as the traditional 
non-thematic -areas-detection method on our 
experimental samples to generate two groups of 
summaries. Next, make a comparison between them. 
The final results show a clear superiority of our 
method over the traditional one in the scores of the 
evaluation parameters. 
The remainder of this paper is organized as 
follows. In the next section, we review related 
methods that are commonly discussed in the 
automatic summarization literature. Section 3 
describes our method in detail. The evaluation 
methodology and experimental results are presented 
in Section 4. Finally, we conclude with a discussion 
and future work. 
2 Related Work  
The research of automatic summarization begins with 
H.P.Luhn?s work. By far, a large number of scholars 
have taken part in the research and had many 
achievements. Most of the researchers have 
concentrated on the sentence-extraction 
summarization method (the so-called shallower 
approach) (Wang et al, 2003; Nomoto and 
Matsumoto, 2001; Gong and Liu, 2001), but not the 
sentence-generation method (the so-called deeper 
approach)(Yang and Zhong., 1998). On the one hand, 
it is caused by the high complexity and the severe 
limitation of practical fields of rational natural 
language processing technology and knowledge 
engineering technology. On the other hand, it is 
closely associated with the great achievements in 
many fields of natural language processing by 
statistical research methods, machine learning 
methods and pattern recognition methods in recent 
years (Mani, 2001). 
The summarization method of sentence-
extraction can roughly be divided into two kinds: 
supervised and unsupervised (Nomoto and 
Matsumoto, 2001). Generally, the realization of the 
former relies on plenty of manual summaries, that 
is so-called ?Gold Standards? which help 
determining the relevant parameters of the 
statistical model for summarization. However, not 
all people believe that manual summaries are 
reliable, so the researchers have begun to 
investigate the general unsupervised method, 
which can avoid the requirement of support of 
manual summaries. Nevertheless it is soon 
discovered that the summaries produced by this 
method can?t cover all the themes and have great 
redundancy at the same time. Usually, it can only 
cover those intensively distributed themes while 
neglects others. So researchers in Nanjing 
University proposed a summarization method 
based on the analysis of the discourse structure to 
overcome these problems (Wang et al, 2003). By 
making statistics of the reduplicated words in the 
adjacent paragraphs of the document, the semantic 
distances among them can be worked out. Then 
analyse the thematic structure of the document and 
extract sentences from each theme to form a 
summary. It is ideal to employ this method while 
dealing with those documents with standard 
discourse structure, because it can effectively 
avoid the problems caused by the summarization 
method without discourse structure analysis. Yet 
when the writing style of a document is rather free 
and the distribution of the themes is variable, that 
is the same theme can be distributed in several 
paragraphs not adjacent to each other, then the use 
of this method can?t be equally effective. 
To deal with a lot of Chinese documents which 
have free style of writing and flexible themes, a 
sentence-extraction summarization method created 
by detecting thematic areas is tried following such 
work as (Nomoto and Matsumoto, 2001; Salton et 
al., 1996; Salton et al, 1997; Carbonell and 
Goldstein, 1998; Lin and Hovy, 2000). The 
thematic areas detection in a document is obtained 
through the adaptive clustering of paragraphs (cf. 
Moens et al 1999), so it can overcome in a certain 
degree the defects of the above methods in dealing 
with the documents with rather flexible theme 
distribution. 
3 The Algorithm 
In this section, the proposed method will be 
introduced in detail. The method consists of the 
following three main stages: 
Stage 1: Find the different thematic areas in the 
document through paragraph clustering and 
clustering analysis. 
Stage 2: Select the most suitable sentence from 
each thematic area as the representative 
one. 
Stage 3: Make the representative sentences form 
the final summary according to certain 
requirements. 
3.1 Stage 1: Thematic Area Detection 
The process of thematic area detection is displayed in 
Figure 1. 
The each step of Figure 1 is explained in the 
following subsections. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: The process of thematic area detection (4 
steps in all) 
 
 
 
3.1.1 Step 1: Term Extraction 
 
Different from the general word segmentation 
operation adopted in the traditional Chinese 
automatic summarization research, we do not take the 
general operation when pre-processing the original 
Original document 
Term extraction 
Vector representation of paragraph 
Weight calculation of paragraph 
Paragraph clustering 
Clustering analysis 
Thematic area detection 
1 
2 
3 
4 
document, but make use of the method introduced 
by (Liu et al, 2003) to extract terms 
from the document and then express its content by 
such metadata elements as terms.  
The greatest advantage of term extraction 
technology is that it needs no support of fixed 
thesaurus, only through the continuous updating 
and making statistics of a real corpus. We can 
dynamically establish and update a term bank and 
improve the extraction quality through continuous 
correcting of the parameters for extraction. Thus it 
is of wide practical prospects for natural language 
processing. In addition, the terms can represent a 
relative specific meaning, because most of them 
are phrases, which consist of multi-characters. 
 
3.1.2 Step 2: Vector Representation and Weight 
Calculation of Paragraph 
 
The advantage of the vector space model (VSM) is 
that it successfully makes the unstructured 
documents structured which makes it possible to 
handle the massive real documents by adopting the 
existing mathematical instruments. All the terms 
extracted from the document are considered as the 
features of a vector, while the values of the 
features are statistics of the terms. According to 
this, we can set up the VSM of paragraphs, that is 
each paragraph Pi (i:1~M,M is the number of all 
paragraphs in a document) is represented as the 
vector of weights of terms, VPi, VPi =
( WPi1,WPi2,?,WPiN)  
Where N is the total number of terms, WPij 
denotes the weight of the j-th term in the i-th 
paragraph. There are many methods of calculating 
WPij, such as tf, tf*idf, mutual information (Patrick 
Pantel and Lin, 2002), etc. The method adopted 
here (Gong and Liu, 2001) is shown as follows: 
 
WPi j= log(1+TF(Ti j))*log(M/Mj ) (1) 
 
Where TF(Tij) denotes the number of occurrence 
of the j-th term in the i-th paragraph, M/Mj denotes 
the inverse paragraph frequency of term j, and Mj 
denotes the number of paragraphs in which term j 
occurs. In accordance, on the basis of defining 
WPij, we can further define the weight of 
paragraph P i, W(P i), by the follwing formula: 
 (2) 
 
In formula (2), n represents the total number of 
different terms occurring in the i-th paragraph. 
3.1.3 Step 3: Paragraph Clustering and 
Clustering Analysis  
 
1) Paragraph clustering 
The existing clustering algorithms can be 
categorized as hierarchical (e.g. agglomerative etc) 
and partitional (e.g. K-means, K-medoids, etc) 
(Pantel and Lin, 2002). 
The complexity of the hierarchical clustering 
algorithm is O(n2Log(n)) , where n is the number 
of elements to be clustered, which is usually 
greater than that of the partitional method. For 
example, the complexity of K-means is linear in n. 
So in order to achieve high efficiency of algorithm, 
we choose the latter to cluster paragraphs. 
K-means clustering algorithm is a fine choice in 
many circumstances, because it is simple and 
effective. But in the process of clustering by means 
of K-means, the quality of clustering is greatly 
affected by the elements that marginally belong to 
the cluster, and the centroid can?t represent the real 
element in the cluster, So while choosing the 
paragraphs clustering algorithm, we adopt K-
medoids (Kaufmann and Rousseeuw, 1987; Moens 
et al 1999) which is less sensitive to the effect of 
marginal elements than K-means. 
Suppose that every sample point in the N-
dimensional sample space respectively represent a 
paragraph vector, and the clustering of paragraphs 
can be visualized as that of the M sample points in 
the sample space. Here N is the number of terms in 
the document and M is the number of paragraphs. 
Table 1 shows the formal description of the 
paragraph clustering process based on K-medoids 
method. 
2) Clustering analysis 
A classical problem when adopting K-medoid 
clustering method and many other clustering 
methods is the determination of K, the number of 
clusters. In traditional K-medoid method, K must 
be offered by the user in advance. In many cases, 
it?s impractical. As to clustering of paragraphs, 
customers can?t predict the latent thematic number 
in the document, so it?s impossible to offer K 
correctly. 
In view of the problem, the authors put forward a 
new clustering analysis method to automatically 
determine the value of K according to the 
distribution of values of the self-defined objective 
function. The basic idea is that if K, the number of 
clusters,  is  determined  with  each  value of K, and   
  
  
 
Input: <a, b>, they respectively denote the 
paragraph matrix composed by all the 
paragraph vectors in the document and 
the number of clusters, k (the range of k 
is set to 2~M). 
 
Step 1: randomly select k paragraph vectors as 
the initial medoids of the clusters (here, 
the medoids denote the representative 
paragraphs of k clusters). 
Step 2: assign each paragraph vector to a cluster 
according to the medoid X closest to it. 
Step 3: calculate the Euclidean distance between 
all the paragraph vectors and their closest 
medoids. 
Step  4: randomly select a paragraph vector Y. 
Step 5: to all the X, if it can reduce the 
Euclidean distance between all the 
paragraph vectors and their closest 
medoids by interchanging X and Y, then 
change their positions, otherwise keep as 
the original. 
Step 6: repeat from step 2 to 5 until no changes 
take place. 
 
Output: <A, B, C>, they respectively denote the 
cluster id, the representative paragraph 
vector and all the paragraph vectors of 
each cluster under the k clusters. 
Table 1: Paragraph clustering process based on 
K-medoid method 
suitably, then the corresponding clustering 
results can well distinguish the different themes 
in the document, and correspondingly the 
average of the sum of the weight of the 
representative paragraph under each theme will 
tend to maximize. We call this the maximum 
property of the objective function. 
 
Correspondingly, we define the following 
objective function Objf(K) to reflect clustering 
quality and determine the number of clusters, K. 
1
( )
( )
K
j
j
W P
O b j f K
K
==
?
 (3) 
Where W(Pj) denotes the weight of the 
selected representative paragraph in the j-th 
cluster, here the selected representative 
paragraph Pj can be regarded as the medoid in 
the j-th cluster which is determined by the final 
output of the presented K-medoid paragraph 
clustering process, and the weight of Pj is 
calculated by formula (2). Put the objective 
function in K clustering results corresponding 
then make good use of the maximum property of 
the objective function to adaptively determine the 
final number of clusters, K. 
Figure 2 shows the concrete distribution of the 
values of objective function obtained in the 
example document ?On the Situation and Measures 
That Face Fishing in the Sea in Da Lian City? 
when adopting the proposed clustering analysis 
method. According to the maximum property of 
objective function, that is take the value of K when 
the values of the objective function take maximum 
as the final number of clusters. From the results in 
Figure 2, we can know that K equals to six, that is 
we find six latent thematic areas from nine 
paragraphs in the document with this method. 
1
1.05
1 . 1
1.15
1 . 2
1.25
1 . 3
k=2 k=3 k=4 k=5 k=6 k=7 k=8 k=9
k?
Figure 2: The distribution of the values of the 
objective function when K takes different values 
 
 
Figure 3 displays the paragraph clustering results 
when K equals six in the process of adopting K-
medoid clustering method on the example 
document. 
 
 
Component 1
C
om
po
ne
nt
 2
-10 -5 0 5
-5
0
5
These two components explain 54.44 % of the point variability.  
Figure 3: The paragraphs clustering result when K 
equals to six 
 
 
 
3.1.4 Step 4: Thematic Area Detection 
 
Output the complete information table of each 
thematic area in the form of the representative 
paragraph and all the paragraphs and sentences 
covered by the thematic area. 
 
3.2 Stage 2: Selection of the Thematic 
Representative Sentences 
 
To select a most suitable representative sentence 
from each thematic area, the author proposes the 
following method. This is in contrast  with a 
method proposed by Radev (Radev et al, 2000 ), 
where the centroid of a cluster is selected as the 
representative one. 
Method: select the sentence which is most 
similar to the thematic area semantically as 
representative one. 
Before carrying out the method in detail, there 
are two problems to be solved: 
1) The vector representation of sentence and 
thematic area 
The vector representation of sentence and 
thematic area is similar to that of paragraph 
introduced before. We only need to change the 
weight calculation field of the terms from the 
interior of paragraph to the interior of sentence 
or thematic area. Accordingly, we can describe 
the sentence vector and thematic area vector as 
follows 
VSj= ( WSj1,WSj2,?,WSjN)  
VAk= ( WAk1,WAk2,?,WAkN)  
 
2) The semantic similarity calculation between 
sentence and thematic area 
The calculation of semantic similarity of 
sentence and thematic area can be achieved by 
calculating the vector distance between sentence 
vector and thematic area vector. Here we adopt 
the traditional cosine method for vector distance 
calculation. Correspondingly, the distance 
between the sentence vector VSj and the thematic 
area vector VAk is calculated by the following 
formula: 
( ) ,2 2
1 1 1
( , )
N N N
j k ji ki ji ki
i i i
WS WA WS WAVS VACos
= = =
? ? ? ?= ? ?? ? ? ?? ? ? ?
? ? ?  (4) 
 
Principles of evaluating summarization redundancy 
 
At the premise of the same number of 
summarization sentences selected out by 
different summarization methods: 
 
The higher the value of RE calculated by the 
covariance matrix of the summarization sentence 
vectors. 
The lower the summarization redundancy. 
 
Table 2: The evaluation principles of the 
summarization redundancy based on RE 
 
3.3 Stage 3: The Creation of the Summary 
 
Ouput the selected representative sentences from 
each thematic area according to their postions in the 
original document to form the final summary.  
 
4 Experimental Results and Performance 
Evaluation 
4.1 Evaluation Methodology 
It is challenging to objectively evaluate the qua lity of 
different automatic summarization methods. Methods 
for evaluation can be broadly classified into two 
categories: intrinsic and extrinsic (Mani, 2001). We 
adopt the former to evaluate the quality of 
summarization by defining the following parameters 
for evaluation. 
1) Theme coverage (TC) 
The definition of TC is the percentage of the 
thematic contents covered by the selected 
summarization sentences. The value of the 
parameter can be got by means of the works of 
some experts. 
2) Representation entropy (RE) 
In order to effectively and objectively evaluate the 
redundancy of the produced summary, we refer to 
the parameter which was initially proposed by 
(Mitra et al, 2002) for evaluating the feature 
redundancy in the process of feature selection and 
transform it into the novel parameter to evaluate 
the summarization redundancy. 
According to this, some important notations are 
defined as follows: 
 
 
N Number of terms in the original 
document ;  
Nz Number of sentences in the 
produced summary ;  
Lz 
Nz-by-N matrix composed by 
all the sentence vectors in the 
produced summary ;  
?z 
Nz-by-Nz covariance matrix 
composed by all the sentence 
vectors in the produced 
summary ;  
l i Eigenvalues of ?z i:1~Nz ;  
? i ? i= l i /
1
Nz
i =
l  ? i ;  
 
 
 
 
 
 
 
Theme coverage (TC) Representation 
entropy (RE) 
Genre Sample ID Number 
of 
characters  
Number 
of 
paragraphs 
Number 
of 
detected 
thematic 
areas  
Method1 Method2 Method1 Method2 
d10000801 1461 11 5 0.6 0.56 1.44 1.25 
d10000901 1192 7 5 0.64 0.6 1.36 1.35 
d10100101 1936 14 9 0.66 0.64 2.14 2.06 
d10100201 1778 12 6 0.8 0.5 1.62 1.54 
d10100301 2472 4 3 0.64 0.4 0.81 1.05 
d10100601 1553 11 7 0.9 0.64 1.79 1.83 
d29600501 2400 6 4 0.7 0.56 1.33 1.01 
d29800101 670 4 3 0.64 0.6 1.06 1.01 
d40000301 2026 8 5 0.56 0.52 1.45 1.54 
Economy 
d40100101 1529 7 4 0.6 0.58 1.19 1.31 
e10000101 907 4 2 0.72 0.56 0.64 0.24 
e10000201 845 5 3 0.9 0.6 1.06 0.89 
e29600201 2035 5 4 0.72 0.5 1.36 1.21 
Art 
e29800201 1831 7 2 0.56 0.52 0.67 0.57 
f20000101 2354 12 7 0.58 0.5 1.92 1.79 Prose 
f20000201 1769 9 6 0.64 0.52 1.72 1.50 
g00000201 1163 5 4 0.84 0.56 1.34 1.21 
g00000501 790 6 4 0.64 0.54 1.31 1.26 
g00001201 425 5 5 0.92 0.62 1.45 1.49 
g00100101 1629 10 3 0.84 0.6 0.93 0.82 
g00100301 817 6 4 0.76 0.7 1.32 1.26 
g00100501 1355 4 4 0.84 0.5 1.31 1.12 
g09600901 2179 7 6 0.72 0.62 1.75 1.73 
Military 
g09601601 1271 5 3 0.7 0.52 1.03 0.98 
h00000401 1224 6 6 0.72 0.54 1.75 1.60 
h00000601 1331 15 7 0.6 0.5 1.88 1.80 
h00000901 1507 7 3 0.64 0.68 1.05 0.83 
h00001801 1604 8 6 0.68 0.64 1.73 1.66 
h00100301 960 6 3 0.9 0.4 1.04 1.05 
Life 
h00100601 1228 6 3 0.8 0.6 1.06 0.89 
 
Table 3: Experimental data 
 
 
 
 
Mean of theme 
coverage ( TC ) 
Mean of representation 
entropy ( R E ) 
Ratio of 
information and 
noise (F) 
Genre Number 
of samples 
Method 
1 
Method 
2 
Method 
1 
Method 
2 
Method 
1 
Method 
2 
Economy 10 0.68 0.56 1.42 1.40 2.81 2.27 
Art 4 0.72 0.54 0.93 0.73 1.82 1.12 
Prose 2 0.62 0.52 1.82 1.65 3.83 2.71 
Military  8 0.78 0.58 1.31 1.23 2.89 1.98 
Life 6 0.72 0.56 1.42 1.31 2.98 2.08 
Table 4: Evaluation results of parameters 
 
 
 
The value of RE (Mitra et al, 2002) is calculated 
as follows: 
RE= -
1
N z
i =
?  ?  ? i * il o g  (5) 
The evaluation principles of the summarization 
redundancy based on RE  are demonstrated in 
Table 2. 
 
3) Ratio of information and noise (F) 
F=TC/e ?RE (6) 
 
The novel evaluation parameter proposed by us 
can objectively evaluate the quality of the produced 
summary by effectively combining the above two 
parameters. The more the value of F, the better the 
quality of the produced summary. 
 
4.2 Experimental Results 
We randomly extract 200 documents of different 
genres from the Modern Chinese Corpus of State 
Language Commission to form the experimental 
corpus. Because summarizing short documents 
doesn?t make much sense in real applications (Gong 
and Liu, 2001), we select 30 documents of more 
than 400 characters from the corpus as the samples 
which are summarized by the proposed 
summarization method (method 1 for abbreviation) 
and the traditional non-thematic -area-detection 
method (method 2 for abbreviation), that is the 
method of determining the weights of sentences in a 
document, sorting them in a decreasing order, and 
selecting the top sentences in the end. The specific 
experimental data and evaluation results of 
parameters are given in table 3 and table 4. 
The synthetic evaluation of the 30 samples proves 
that our method under the above evaluation 
parameters is superior to the traditional non-
thematic-area-detection  summarization method 
when dealing with different genres of text 
documents with free style and flexible theme 
distribution, and the results we have achieved are 
encouraging. 
 
5 Conclusions 
In this paper, we have proposed a new 
summarization method based on thematic areas 
detection. By adopting a novel clustering analysis 
method, it can adaptively detect the different 
thematic areas in the document, and automatically 
determine K, the number of thematic areas. So the 
produced summary can both cover as many as 
different themes and reduce its redundancy 
obviously at the same time. 
 For our experiment, we used three different 
parameters to evaluate the quality of the produced 
summaries in theme coverage and summarization 
redundancy. We achieved a better performance than 
the traditional non-thematic -areas-detection method 
in the proposed evaluation scheme. As a future 
work , we need the additional research for testing 
the proposed method on la rger-scale real corpora , 
and have the further comparison with earlier similar 
works such as MMR, etc. In addition, we?ll 
improve our summarization system by considering 
the structure of thematic areas and user?s 
requirement. 
 
References  
Jaime Carbonell and Jade Goldstein. 1998. The use 
of MMR, diversity-based reranking for reordering 
documents and producing summaries. In 
Proceedings of the 21th Annual International 
ACM SIGIR Conference on Research and 
Development in Information Retrieval. ACM, 
New York. 
Yihong Gong, Xin Liu. 2001. Generic text 
summarization using relevance measure and 
latent semantic analysis. In Proceedings of ACM 
SIGIR?01, pages 19-25, ACM, New York. 
L. Kaufmann and P.J. Rousseeuw. 1987. Clustering 
by means of medoids. In Statistical Data Analysis 
Based on the L1 Norm,Y.Dodge,Ed,Amsterdam, 
405-416. 
Chin-Yew Lin and Eduard Hovy. 2000. The 
automatic acquisition of topic signatures for text 
summarization. In Proceedings of the 18th 
International Conference of Computational 
Linguistics (COLING 2000). 
Jian-Zhou Liu, Ting-Ting He, and Dong-Hong Ji. 
2003. Extracting Chinese term based on open 
corpus. In Proceedings of the 20th International 
Conference on Computer Processing of Oriental 
Languages,pages 43-49. ACM, New York.  
Inderjeet Mani. 2001. Summarization evaluation: an 
overview. In Proceedings of the NTCIR 
Workshop 2 Meeting on Evaluation of Chinese 
and Japanese Text Retrieval and Text 
Summarization. 
Inderjeet Mani. 2001. Recent developments in text 
summarization. In Proceedings of CIKM?01, 529-
531. 
Pabitra Mitra, C.A. Murthy, Sankar and K.Pal. 
2002. Unsupervised feature selection using 
feature similarity. IEEE Transactions of Pattern 
Analysis and Machine Intelligence: 1-13. 
Marie-Francine Moens, Caroline Uyttendaele and 
Jos Dumortier. 1999. Abstracting of legal cases: 
The potential of clustering based on the selection 
of representative objects. Journal of the American 
Society for Information Science, 50 (2): 151-161.  
Tadashi Nomoto, Yuji Matsumoto. 2001. A new 
approach to unsupervised text summarization. In 
Proceedings of ACM SIGIR?01, pages 26-34. 
ACM, New York.  
Patrick Pantel and Dekang Lin. 2002. Document 
clustering with committees. In Proceedings of 
ACM SIGIR?02, pages 199-206. ACM, New 
York.  
Dragomir R. Radev, Hongyan Jing, and Malgorzata 
Budzikowska. 2000. Centroid-based 
summarization of multiple documents: sentence 
extraction, utility-based evaluation, and user 
studies. In ANLP/NAACL Workshop on 
Summarization.  
Gerard Salton, Amit Singhal, Chris Buckley and 
Mandar Mitra. 1996. Automatic text 
decomposition using text segments and text 
themes. Hypertext 1996: 53-65. 
Gerard Salton, Amit Singhal, Mandar Mitra and 
Chris Buckley. 1997. Automatic text structuring 
and summarization. In Information Processing 
and Management, 33(2):193-208. 
Ji-Cheng Wang, Gang-Shan Wu, Yuan-Yuan Zhou, 
Fu-Yan Zhang. 2003. Research on automatic 
summarization of web document guided by 
discourse. Journal of Computer Research and 
Development, 40(3):398-405. 
Xiao-Lan Yang and Yi-Xin Zhong. 1998. Study and 
realization for text interpretation and automatic 
abstracting. Acta Electronica Sinica, 26(7):155-
158. 
'RFXPHQW5HUDQNLQJ%DVHGRQ*OREDODQG/RFDO7HUPV
<DQJ/LQJSHQJ,QVWLWXWHIRU,QIRFRPP5HVHDUFK+HQJ0XL.HQJ7HUUDFH6LQJDSRUHOS\DQJ#LUDVWDUHGXVJ
-L'RQJKRQJ7DQJ/L,QVWLWXWHIRU,QIRFRPP5HVHDUFK+HQJ0XL.HQJ7HUUDFH6LQJDSRUH^GKMLWDQJOL`#LUDVWDUHGXVJ
$EVWUDFW
,Q WKLV SDSHUZH SURSRVH DPHWKRG WR LPSURYHWKHSUHFLVLRQRI WRS1 GRFXPHQWVE\ UHRUGHULQJWKH UHWULHYHG GRFXPHQWV LQ WKH LQLWLDO UHWULHYDO7RUHRUGHUWKHGRFXPHQWVZHILUVWDXWRPDWLFDOO\H[WUDFWJOREDONH\WHUPVIURPGRFXPHQWVHWWKHQXVH WKHVH WHUPV DQG WKHLU IUHTXHQF\ WR LGHQWLI\ORFDO NH\ WHUPV LQ D VLQJOH TXHU\ RU GRFXPHQWILQDOO\ZHPDNHXVHRIORFDONH\WHUPVWRUHRUGHUWKH LQLWLDOO\ UHWULHYHG GRFXPHQWV ,Q RXUH[SHULPHQWVEDVHGRQ17&,5&/,5GDWDVHWDQDYHUDJHLPSURYHPHQWFDQEHPDGHIRUWRS  GRFXPHQWV DQG DQ DYHUDJH LPSURYHPHQW FDQ EH PDGH IRU WRS GRFXPHQWV
 ,QWURGXFWLRQ
7R ILQG RXW ZKDW ZH UHDOO\ ZDQW IURP D ODUJHGRFXPHQW VHW LV D KHDGDFKH ,QIRUPDWLRQ UHWULHYDO,5LVXVHGWRUHWULHYHUHOHYDQWGRFXPHQWVIURPDODUJH GRFXPHQW VHW IRU D JLYHQ TXHU\ ZKHUH WKHTXHU\ LVD VLPSOHGHVFULSWLRQE\QDWXUDO ODQJXDJH,QPRVWSUDFWLFDOVLWXDWLRQVXVHUVFRQFHUQPXFKRQWKHSUHFLVLRQRIWRSUDQNLQJGRFXPHQWVWKDQUHFDOOEHFDXVHXVHUVZDQWWRDFTXLUHUHOHYDQWLQIRUPDWLRQIURP WKH WRS UDQNLQJ GRFXPHQWV WR VDYH WKHLUYDOXDEOHWLPH7UDGLWLRQDOO\ ,5 V\VWHP XVHV D RQHVWDJH RU DWZRVWDJH PHFKDQLVP WR UHWULHYH UHOHYDQWGRFXPHQWV IURP GRFXPHQW VHW )RU RQH VWDJHPHFKDQLVP ,5 V\VWHP RQO\ GRHV DQ LQLWLDOUHWULHYDO )RU WZRVWDJH PHFKDQLVP H[FHSW WKHLQLWLDO UHWULHYDO ,5 V\VWHP ZLOO PDNH XVH RI WKHLQLWLDO UDQNLQJ GRFXPHQWV WR DXWRPDWLFDOO\ GRTXHU\H[SDQVLRQWRIRUPDQHZTXHU\DQGWKHQXVHWKH QHZ TXHU\ WR UHWULHYH DJDLQ WR JHW WKH ILQDOUDQNLQJ GRFXPHQWV 7KH HIIHFWLYHQHVV RI TXHU\H[SDQVLRQPDLQO\GHSHQGVRQ WKHSUHFLVLRQRI WRS5 5 UDQNLQJ GRFXPHQWV LQ LQLWLDO UHWULHYDOEHFDXVH DOPRVW DOO SURSRVHG DXWRPDWLF TXHU\H[SDQVLRQDOJRULWKPVPDNHXVHRIWKHLQIRUPDWLRQLQ WKH WRS 5 GRFXPHQWV LQ LQLWLDO UDQNLQJGRFXPHQWV )LJXUH  GHPRQVWUDWHV WKH JHQHUDOSURFHVVHVRIDWZRVWDJH,5V\VWHP
7R LPSURYH WKH SUHFLVLRQ RI WRS 1 UDQNLQJGRFXPHQWV LQ LQLWLDO UHWULHYDO PDQ\ UHVHDUFKHVKDYH EHHQ GRQH RQ WKH UHWULHYDO PRGDOV DQGLQGH[LQJXQLWV
)LJ7UDGLWLRQDO3URFHVVRIWZRVWDJHV,5
2ULJLQDO4XHU\
([SDQGHG4XHU\
,QLWLDO5HWULHYDO
)LQDO5HWULHYDO
4XHU\([SDQVLRQ
'RFXPHQW6HW
,QLWLDO5DQNLQJ'RFXPHQWV
)LQDO5DQNLQJ'RFXPHQWV
 ,Q WKLV SDSHU ZH SURSRVH D PHWKRG WRLPSURYHWKHSUHFLVLRQRIWRS1UDQNLQJGRFXPHQWVLQ WKH LQLWLDO UHWULHYDO E\ UHRUGHULQJ WKH LQLWLDOUDQNLQJ GRFXPHQWV LQ WKH LQLWLDO UHWULHYDO 7RUHRUGHU GRFXPHQWV ZH ILUVW DXWRPDWLFDOO\ H[WUDFWJOREDO NH\ WHUPV IURP GRFXPHQW VHW WKHQ XVHH[WUDFWHGJOREDONH\WHUPVDQGWKHLUIUHTXHQFLHVWRLGHQWLI\ ORFDO NH\ WHUPV LQ D VLQJOH GRFXPHQW RUTXHU\WRSLFILQDOO\ZHPDNHXVHRIORFDONH\WHUPVLQ TXHU\ DQG GRFXPHQWV WR UHRUGHU WKH LQLWLDOUDQNLQJ GRFXPHQWV%\ GRLQJ VR RXUPHWKRG FDQLPSURYH WKH SUHFLVLRQ RI WRS GRFXPHQWV LQ LQLWLDOUDQNLQJ GRFXPHQWV DQG KHOS WR LPSURYH WKHHIIHFWLYHQHVVRITXHU\H[SDQVLRQ$OWKRXJKRXUPHWKRG LV JHQHUDODQGFDQDSSO\WRDQ\ODQJXDJHVLQWKLVSDSHUZH?OORQO\IRFXVRQWKHUHVHDUFKRQ&KLQHVH,5V\VWHP
7KHUHVWRIWKLVSDSHULVRUJDQL]HGDVIROORZLQJ,QVHFWLRQZHJLYHDQRYHUDOOLQWURGXFWLRQRIRXUSURSRVHGPHWKRG  ,Q VHFWLRQZHGHVFULEHZKDWDUHJOREDONH\WHUPVDQGZKDWDUHORFDONH\WHUPVDQGKRZWRDFTXLUHWKHP,QVHFWLRQZHGHVFULEHKRZJOREDONH\WHUPVDQGORFDONH\WHUPVDSSO\WR&KLQHVH ,5 V\VWHP WR LPSURYH WKH SUHFLVLRQ DQGTXDOLW\RI,5V\VWHP,QVHFWLRQZHHYDOXDWHWKHSHUIRUPDQFH RI RXU SURSRVHG PHWKRG DQG JLYHVRPH UHVXOW DQDO\VLV ,Q VHFWLRQ ZH SUHVHQW WKHFRQFOXVLRQDQGVRPHIXWXUHZRUN
 2YHUYLHZ RI 'RFXPHQW 5HRUGHULQJ LQ&KLQHVH,5
)RU &KLQHVH ,5 PDQ\ UHWULHYDO PRGHOV LQGH[LQJVWUDWHJLHVDQGTXHU\H[SDQVLRQVWUDWHJLHVKDYHEHHQVWXGLHG DQG VXFFHVVIXOO\ XVHG LQ ,5 &KLQHVH&KDUDFWHUELJUDPQJUDPQ!DQGZRUGDUHWKHPRVWXVHG LQGH[LQJXQLWV /L3JLYHVRXWPDQ\UHVHDUFKUHVXOWVRQWKHHIIHFWLYHQHVVRIVLQJOH&KLQHVH &KDUDFWHU DV   LQGH[LQJ XQLW DQG KRZ WRLPSURYH WKH HIIHFWLYHQHVV RI VLQJOH &KLQHVH&KDUDFWHU DV LQGH[LQJ XQLW ./ .ZRN A Large-Scale Semantic Structure for Chinese Sentences 
Tang Li 
Institutue for Infocomm Research  
21 Heng Mui Keng Terrace 
Singapore119613 
Tangli@I2R.a-star.edu.sg 
Ji Donghong, Yang Lingpeng 
Institutue for Infocomm Research  
21 Heng Mui Keng Terrace 
Singapore119613 
{dhji, lpyang}@I2R.a-star.edu.sg 
 
Abstract 
Motivated by a systematic analysis of 
Chinese semantic relationships, we 
constructed a Chinese semantic framework 
based on surface syntactic relationships, deep 
semantic relationships and feature structure to 
express dependencies between lexical 
meanings and conceptual structures, and 
relations that underlie those lexical meanings. 
Analyzing the semantic representations of 
10000 Chinese sentences, we provide a model 
of semantically and syntactically annotated 
sentences from which reliable information on 
combinatorial possibilities of each semantic 
item targeted for analysis can be displayed. 
We also propose a semantic argument ? head 
relation, ?basic conceptual structure? and the 
?Head-Driven Principle?. Our results show that 
we can successfully disambiguate some 
troublesome sentences, and minimize the 
redundancy in language knowledge 
descriptions for natural language processing.  
1 Introduction 
To enable computer-based analysis of Chinese 
sentences in natural language texts we have 
developed a semantic framework, taking into 
account concepts used in the Berkeley FrameNet 
Project (Baker, Fillmore, & Lowe 1998; Fillmore 
& Baker 2001) and the Penn Chinese Tree Bank 
(Nianwen Xue; Fei Xia et al 2000). The FrameNet 
Project, as a computational project, is creating a 
lexical resource for English, based on the principle 
of  semantic frames. It has tried to concentrate on 
frames which help to explain the meanings of 
groups of words, rather than frames that cover just 
one word. The representation of the valences of its 
target words and descriptions of the semantic 
frames underlying the meanings of the words 
described are the mainly part of the database. The 
Penn Chinese Tree Bank analyzed the syntactic 
structure of a phrase or sentence for  selected text, 
based on the current research in Chinese syntax 
and the linguistic expertise of those involved in 
this project. Different from Pan?s syntactic 
structures and FrameNet?s semantic frames, our 
object is to record exactly how the semantic 
features relates frames to those syntactic 
constituents. The key task is to determine the 
relationship between the two direct constituents in 
terms of the semantic relationship. The grammar 
functions are also considered for primarily 
identifying the relation. Here, we use methods 
developed for the analysis of semantic 
relationships to produce a framework based on the 
direct component link. Our framework is largely a 
semantic one, but it has adopted some crucial 
principles of syntactic analysis in the semantic 
structure analysis. 
In this paper, we present our model of 
semantically and synactically annotated 10000 
Chinese sentences. The focus is on the analysis of 
the semantic relationships between one word to 
another in a sentence. We also briefly discuss the 
annotation process. 
2 Theoretical Framework and Case Study 
The basic assumption of Frame Semantics 
(Fillmore 1976;1977; Fillmore & Atkins 1992; 
Petruck 1996) as it applies to the description of 
lexical meanings is that each word (in a given 
meaning) evokes a particular frame and possibly 
profiles some element or aspect of that frame. By 
being linked to frames, each word is directly 
connected with other words in its frame(s). where 
word dependence association are needed from 
surface syntactic structures which actually reflect 
the grammatical relationship to the deep semantics 
structure whereby semantic content are put into 
natural language. The meaning of a word, in most 
cases, is best demonstrated by reference to a 
semantic network. Referential meaning on its own 
is insufficient. Word meaning would include the 
other dimensions concerning the structure and 
function of words. Unlike English, in which there 
are two major types of evidence that help to 
determine the syntactic structure of a phrase or 
sentence: morphological information and 
distributional information (such as word order) ,  in 
Chinese the lack of conclusive morphological cues 
makes ambiguity analyses for one sentence more 
likely. Moreover, most Chinese sentences order are 
very flexible. Phrase omission, word  movement, 
ellipsis and binding also make it difficult to 
characterize their grammatical relation. So the 
semantic information provides important clues for 
Chinese sentence analyse. We have to rely on 
semantic knowledge to guide role assignment. 
Thus,  we propose a method allowing a syntactic 
and semantic-based analysis of sequences and 
relationship of semantic items to obtain the 
common distribution of the relationship order. 
3 Method 
The analysis method that will be presented here 
is logically equivalent to the parsing of syntax and 
semantic dependency  with feature constraints. 
The key idea in our method is to avoid the 
complexity hierarchical tree sturcture. We are 
concerned with building structures that reflect  
basic relationships between one word and other in 
a single sentence. We use methods developed for 
the analysis of semantic relationships to produce a 
framework based on the order link. We started 
from an initial analysis based on the surface 
syntactics, then we analyzed deep semantic 
relationships, and attempted to improve it by 
removing the semantic order from the syntactic 
structure and reconnecting them in different places. 
Since many word phrase patterns are difficult for 
computers to recognize, trying to compromise 
between linguistic correctness and engineering 
convenience, we link the difference semantic roles 
on the flat level, while employing a few template 
rules. All semantic words are linked on the same 
level. They are non-hierarchical constructs. This 
flatted representation allows access to various 
levels of syntactic description tree simultaneously. 
In fact, the purpose of generalization is to get a 
regular expression from the original sentence.  
We manually tagged two kind of relationship 
among our large-scale frameworks: 1. syntax-
semantic relationship; 2. semantic feature 
relationship. 
Our framework consists of a set of nodes and a 
set of arcs that join the nodes, with each word or 
concept corresponding to a node and links between 
any two nodes that are directly associated. The 
basic links in the framework are between one word 
item to another based on immediate semantic 
deperdency order. We summarized the immediate 
semantic relationship through a variety of semantic 
relation features such as agent, reason, result and 
so on. The feature of relationship between two 
nodes are labeled on the arc.  
We developed the first fully instantiated 
semantic structure by manually labeling semantic 
representations in a machine-readable format. To 
make sure that our model can deal with various 
kinds of texts in real life situations, we have 
analysed 10000 sentences from large Web site 
corpora based on our formal model. Our aim is not 
to describe in detail any specific, but to capture at 
an abstract level the semantic relations between the 
direct components in a sentence. Our model?s most 
important domain of application is to Chinese 
sentence analysis, but it may also be applicable to 
different languages. This semantic framwork 
constructs a model on the basis of a few rules. 
The present paper indicates how situation types 
are represented, how these representations are 
composed from semantic representations of 
linguistic constituents, and how these type 
differences affect the expression of sentences. 
3.1 Syntax-Semantics Relationship Labeling 
This work flow includes linking and labeling of 
each relation between direct semantic items in 
single sentences, which reflects different semantic 
representation, and descriptions of the relations of 
each frame?s basic conceptual structure in terms of 
semantic actions. A semantic representation is a 
feature that allows one word in the sentence to 
point at some other word to which it is related. A 
word in a sentence may have much direct 
representation, these are differentiated by the 
semantic action. By analyzing the direct se mantic 
representation, we can capture semantic 
relationships between words, reconstructing a  
framework for the order of Chinese sentences. 
In most cases syntactic relationships are 
consistent with semantic relationships. The 
following framework shows show some important 
similarities between the structure of syntactic and 
semantic structure. For example, in 
     ?????. ??I am watching TV.?? 
Syntactically,  ?? ?? I? is subject, directly 
relating to the verbal predicate ????watch?, ??
? ??TV? is object , also links to the verbal 
predicate directly. ????be doing?as a adverb is 
an adjoined predicate ????watch?, there is a 
direct relationship between the two nodes.  
Semantically,  ????I?is the agent and ????
?TV?is the recipients, both of them have a direct 
relationship with the activity ????watch?. So 
we link the different nodes as follows: 
 
In cases where the relationship between syntax 
and semantics is inconsistent, by syntactic analysis, 
if there are multiple syntactic analyses among a 
sentence, we always choose the analysis 
 3.2 ?Head? Determination relationship that is consistent with the semantic 
relationship. For example, the Chinese sentence The basic link is the direct link between two 
semantic units. In addition, a set of general rules 
for determining the directions has been identified. 
???????? 
?many people sit beside the street.? 
The above sentence can be analyzed either of the 
following two syntactic structures.  
1. That between  Head and Its Modifier as a 
Case of Direct Relationship 
type 1: The head (see below), and the modifiers that 
come before it, constitute a type of modification 
relationship, which is one of the typical cases of 
direct relationships, e.g,  
 A. Gao zige de ren 
type 2:      tall  body DE person 
 
     the person with tall body 
B.   (to be compared with the above sentence) 
      ren de gezi gao 
      person DE body tall The two syntactic structures are analyzed with 
difference in the first node and the second node. In 
type 1, ?????beside of the street?is analyzed 
as subject, for type2, the linguist also analyzed it as 
adverb modifier, adjuncting to the predicate ???
?seat?. But when this sentence is analyzed in 
terms of semantics, there is only one relationship 
structure similar as type 2. ?? ?? people? is 
analyzed as agent,  ?????beside of the street?
as localizer, attached to the activity ????seat?. 
This semantic structure is consistent with the 
syntactic structure type 2. Only one structure can 
display both syntax and semantic relationship 
simultaneously. So we choose the second analysis. 
      ?The person?s body is tall.? 
In the above sentence, ren ?person? and gezi 
?body? hold a modification relationship, but gao 
?tall? and ren?person? are related indirectly as the 
relationship between the two words is realized 
through that of gezi ?body?. Therefore, we say that 
the relationship that ren ?person? holds with gezi 
?body? is a direct one, but that with gao is a rather 
indirect one. 
2. That between An Action Verb and Its Patient 
as a Case of a Direct relationship 
In case a head noun is an AGENT of an action 
verb within a modifying phrase, then the 
relationship between the Head none and the action 
verb is a direct one. The following sentences 
illustrate the point. 
If the syntactic relationship is different from the 
semantic relationship, we take no account of the 
syntactic order. In the Chinese sentence  C. chi pingguo de nuhai. 
     Eat apples DE   girl ??????? 
    ?the girl who is eating apples.?     ?she cry so much that her eyes become red.? 
D. (to be compared with the above sentence) Within the surface syntactic structure, adjective 
??? ?red?will be analyzed as  complementation 
and directly associated with main verb  ?? ? 
? cry?  , which indicate  result of predicate. 
Underlying the syntactic structure, ??? ?red?
actually point to ??? ?? eyes? in semantic 
representation. There is no direct semantic 
relationship between   ?? ? ? cry? and ?? ? 
?red?. The semantic network can be analyzed as: 
she cry + her eyes become red, the immediate 
relationship between ?he? as a possessor and 
?belly? as a possession and that between ?belly? as 
entity and ?painful? as description. In this case we 
link the node ?? ? ? red? to ??? ??eyes?
directly based on semantic relationship. 
      nuhai chi pingguo 
      girl    eat apples 
     ?The girl is eating apples.? 
In the above sentence, nuhai ?girl? is an AGENT 
of the action verb chi ?eat?, the two words have a 
direct semantic relationship, therefore we link 
them directly and annotate ?girl? as a head. In 
contrary, the relationship between nuhai ?girl? and 
pingguo ?apples? is of an indirect type. 
3. Other Cases of Direct Relationships 
In case there is neither a modification nor an 
AGENT/PATIENT relationship, the whole phrase, 
which is still directly related to a following 
describing phrase, has to be embedded. E.g.,  
E. ban shiqing yinggai guquan daju. 
Handle problem should care-about overall  
 
situation 
 ?People should care about the overall situation  
when they handle problems.? 
F. chouyan hai shenti.  
    Smoke   harm health 
    ?Smoking harms health.? 
 
G. ta neng daying de shiqing wo ye neng daying. 
     He can accept DE event I also can accept 
The above three head semantic structures clearly 
show us the different relationships among sentence 
and noun phrases with different meaning. The head 
words are connected  to  their modifier through 
arrow arcs. The first SVO relationship is also 
represented by non-head tagging.  
  ?The event that he can accept are also 
acceptable to me.? 
3.3 ?Head? Determination 
Since Chinese lacks morphological cues, the  
grammatical markers (such as ?????) and 
word order are comparatively important cues for 
the relationship determination. We have to rely on 
grammatical and semantic knowledge to guide role 
assignment. 
3.4 Feature Abstracting and Labeling 
Based on the analysis of semantic relationships, 
we have been parsing feature structures to express 
dependencies between semantic features. In our 
analysis model, semantic feature means a variety 
of  detailed semantic relationships. Most of the 
time, semantic features are not so easy to define. 
Some feature typologies have been provided, but 
there is still much discussions about the nature of a 
re in a text. To avoid the confusion of feature 
ification, we p d a method bstract 
the semantic featur tly from se s that 
contain the natura
sentences without s
labeling the sema
categorys include in
the relationship ar
semantic framewo
dimension. For exam
In this study, we have proposed an approach that 
combines ?basic conceptual structure? and our 
?Head-Driven Principle?.  According to the ?Head-
Driven Principle?, most structures are analyzed as 
having a ?Head? which is connected to various 
types of modifiers, such as Head-NP (adjective-
noun, noun-adverbial pairs ???  ), Head-VP 
(adverbial-verb, verb-adverbial, adjective-verb?). 
In our framework, modification is represented by 
attaching tags with arrows to the core semantic 
item whereve the type of modification can be 
clearly identified. Since the SVO is the basic order 
in Chinese, there is no modifier relationship among 
the level of SVO. In our model, ?Subject-Predicate 
Structures? and ?Verb-Object Structures? are 
represented as non-head. In above example, the 
relation linking the ?core? noun and verb with their 
?adjunct? is tagged with an arrow to indicate that it 
is a ?head?. Both A and B label the ?head? as the 
core noun. E labels the ?head? as  the core verb. 
Employing the ?Head-Driven Principle? for the 
construction of semantic models. Some ambiguous 
sentences can be clearly represented. The different 
meaning among sentence or phrase  containing 
same words can also be described . Conside the 
following sentence and phrases: 
       Ta  gezi   bu g
       His stature isn
       He isn?t tall. 
 
              
 
 
 
 
 
statu
he
In traditional anal
constituent in a sen
meaning of the sent
is semantic feature l
thus in our semantic
?tall? semantically, 
marking a semanti
immediate constitu
structure, after featu
to its English co
translation from one
??????? 
?The students like the teachers.? 
??????? 
(the students who like the teachers) 
??????? 
(the teachers who the students like) 
All of above examples containing same meaning 
words can have very different meaning, depending 
on the different word order and grammatical 
marker ?? ??DE? . We use head tagging to 
construct different frameworks for these structures:  In some sentence
features but also th
Similarly we abstrac
features. Thus we ca
to express this level 
    
 
Ta  liang   mi     
he   two   metersropose
e direc
l feature word.
emantic features i
ntic features re
 other sentences,
cs. Thus we co
rk based on 
ple: 
ao. 
?t tall. 
re 
not 
ysis, ?stature? is ju
tence. However, 
ence is ?he is not t
inking ?he? and ?t
 analysis we link o
?stature? is take
c relationship, ra
ent. This Chine
re abstraction, is 
unterpart. It fa
 language into ano
s , there are not o
eir particular valu
ted the values atta
n expand the featu
of detail. For exam
  gao. 
  tall   to a
ntencefeatu
class For those 
nsert , we?ll 
fer to the 
 attached on 
nstructed a 
multi-profile 
tall
st a syntactic 
the essential 
all?, ?stature? 
all? together, 
nly ?he? and 
n as feature 
ther than an 
se semantic 
very similar 
cilitates the 
ther. 
nly semantic 
es included. 
ched on  the 
re structures 
ple 
?He is two meters tall.?  
 Several different sentences which should be 
analyzed as having the same syntactic structure 
may have fundamentally different semantic 
structures. The following three sentences S1, 
S2and S3, for example, should be analyzed as 
having the syntactic structure, but their semantic 
structures are nevertheless represented as S1?, S2? 
and S3? respectively in our framework. 
 
 
 
 
he tall
two 
meters 
In the above framework, ?tall? is the semantic 
feature describing staturs of the agent ?he? , and 
?two meters? express the value of the feature. They 
provid different information at different level, 
constructing a feature sturcture.  NP + V + Adj + NP   
  S1?Ta xiao-tong le duzi 4 The Advantages of Our Semantic Model     
           he laugh-painful ASP belly 
In developing our semantic frameworks, we also 
have articulated a framework of ?Noun-Centrality? 
as a supplement to the widely assumed ?Verb-
Centrality? practice. We can successfully 
disambiguate some troublesome sentences, and 
minimize the redundancy in language knowledge 
description for natural language processing. We 
automatically learn a simpler, less redundant 
representation of the same information.  
          ?He laughed so much that his belly was 
painful.? 
  S2?Wo kan-tou le ni 
         I  see- through ASP you 
         ?I understand you thoroughly.? 
    S3?Ta da po-le beizi 
            She broke up the cup First, comparing syntactic order and semantic 
order, we used the reconstructed original order, 
giving some different order sentences similar 
results. Thus, variations of order in the same 
sentence can reveal the same relationships. 
            She broke up the cup. 
                                                    
S1?? NP         V        Adj     NP                  One semantic structure may correspond to more 
syntactic structures in Chinese, and this 
correspondence can be made specifically clear 
using our approach.  
 
1.Ta da-le 
wo 
 She beat me 
 ?She beat 
me.? 
2.Ta ba wo da-
le 
She BA me beat  
?She beat me.? 
      
3.Wo BEI Ta da-
le 
I BEI she beat 
?I have been 
beaten by her.? 
The above three sentences, their syntactic 
structures are clearly different from each other. 
That is, the direct object wo ?me? appears right 
after the main verb in (1) whereas the same logical 
object has moved to a pre-verbal position with the 
help of a special Chinese preposition BA in (2) and 
to a sentence-initial position with the help of BEI 
in (3). But underlying the difference syntactic 
structures, they share the same basic semantic 
structure, using semantic represented expression, 
the three sentences of above example can be 
described in below. 
AGENT Ta   ?she? 
PATIENT Wo?me? 
ACTION Da  ?beat? 
 
 
S2??NP            V       Adj     NP                  
 
                                                                                                  
 
S3??NP         V   Adj        NP 
                                        
 
On the other hand, many structural ambiguities 
in Chinese sentences are one of the major problems 
in Chinese syntactic analyses. One syntactic 
structure may correspond to two or more semantic 
structures, that is, various forms of structural 
ambiguity are widely observed in Chinese. 
Disregarding the semantic types will cause 
syntactic ambiguity. If this type of information is 
not available during parsing, important clues will 
be missing, and loss of accuracy will result. 
Consider the Chinese sentence 
Ta de yifu zuo de piaoliang. 
   Her cloth do DE beautiful 
 Reading 1:  ?She has made the cloth beautifully 
b) minimal redundancy in language knowledge 
description for natural language processing. 
Reading 2: (Somebody) has made her cloth 
beautifully.? 
We hope to use the minimum analysis method to 
find the semantic order with equal relationship 
among new sentence. We then used the partition 
relationship as a training database to recognize 
new order as similar as these order structures. 
Syntactically, the sentence, with either one of 
the above two semantic interpretations, should be 
analyzed as 
                          S 
We also have been creating feature sets parsing 
feature structures to expressing dependencies 
between semantic features. Furthermore, we 
abstracted the values attached to the features. Thus 
we can expand the feature structures to express this 
level of detail. 
                      /         \ 
               NP               VP 
            /        \            /      \ 
      NP           N      V         Adj (Complement) 
       |               |        |              | 
References  
      Ta de     yifu    zuo  de  piaoliang 
Baker C, Fillmore C, Lower J 1998 The 
Berkeley                Her    cloth      make DE beautiful 
   FrameNet Project, In Proc. of ACL/COLING 
1998.  
But the two semantic structures have to be 
properly represented in a semantics-oriented 
framework. We do so as in type A and type B 
respectively. 
Daniel Gildea and Daniel Jurafsky 2002 Auto 
matic Labeling of Semantic Roles. In Proc. of 
ACL 2000. 
Type A:   Ta de yifu zuo de piaoliang.                Nianwen Xue, Fei Xia 2000 The Bracketing 
Guidelines for the Pann Chinese Treebank, IRCS 
Report 00-08 University of Pennsylvania, Oct 
2000 
                 Her cloth do DE beautiful         
            
Dominique Dutoit, Thierry Poibeau 2002 Inferring 
Knowledge from a Large Semantic Network, In 
Proc. of COLING 2000 
 Type B: Ta     de yifu zuo de piaoliang. James Henderson, Paola Merlo, Ivan Petroff  2002 
Using Syntactic Analysis to Increase Efficiency 
in Visualizing Text Collections, In Proc. of 
ACL/COLING 2002. 
               Her cloth do DE beautiful 
    
 
So under our proposal, the above two different 
types of semantic relations can be clearly 
represented.. 
5 Conclusion 
In this paper we have demonstrated how our 
semantic model can be created to analyze and 
represent the semantic relationships of Chinese 
sentence structures. The semantic model project is 
producing a structured tree bank with a richer set 
of semantic and syntactic relationships of different 
words on the basis of the analysis of lexical 
meanings and conceptual structures that underlie 
those lexical meanings. We developed some 
methods for determining the relationship between 
direct semantic items based on the analysis of 
syntactic and semantic order. The key advantages 
of our semantic model are:  
a) many ambiguous sentences can be clearly 
represented. 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 154?157,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese word segmentation and named entity recognition based 
on a context-dependent Mutual Information Independence Model 
 
 
Zhang Min    Zhou GuoDong    Yang LingPeng    Ji DongHong 
 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
Email: (mzhang, zhougd, lpyang, dhji)@i2r.a-star.edu.sg 
 
Abstract  
This paper briefly describes our system in the 
third SIGHAN bakeoff on Chinese word 
segmentation and named entity recognition. 
This is done via a word chunking strategy 
using a context-dependent Mutual 
Information Independence Model. 
Evaluation shows that our system performs 
well on all the word segmentation closed 
tracks and achieves very good scalability 
across different corpora. It also shows that 
the use of the same strategy in named entity 
recognition shows promising performance 
given the fact that we only spend less than 
three days in total on extending the system in 
word segmentation to incorporate named 
entity recognition, including training and 
formal testing. 
1  Introduction 
Word segmentation and named entity recognition 
aim at recognizing the implicit word boundaries 
and proper nouns, such as names of persons, 
locations and organizations, respectively in plain 
Chinese text, and are critical in Chinese 
information processing.  However, there exist two 
problems when developing a practical word 
segmentation or named entity recognition system 
for large open applications, i.e. the resolution of 
ambiguous segmentations and the identification 
of OOV words or OOV entity names.  
In order to resolve above problems, we 
developed a purely statistical Chinese word 
segmentation system and a named entity 
recognition system using a three-stage strategy 
under an unified framework.  
The first stage is called known word 
segmentation, which aims to segment an input 
sequence of Chinese characters into a sequence of 
known words (called word atoms in this paper). In 
this paper, all Chinese characters are regarded as 
known words and a word unigram model is 
applied to perform this task for efficiency.  Also, 
for convenience, all the English characters are 
transformed into the Chinese counterparts in 
preprocessing, which will be recovered just 
before outputting results. 
The second stage is the word and/or named 
entity identification and classification on the 
sequence of atomic words in the first step. Here, a 
word chunking strategy is applied to detect words 
and/or entity names by chunking one or more 
atomic words together according to the word 
formation patterns of the word atoms and optional 
entity name formation patterns for named entity 
recognition. The problem of word segmentation 
and/or entity name recognition are re-cast as 
chunking one or more word atoms together to 
form a new word and/or entity name, and a 
discriminative Markov model, named Mutual 
Information Independence Model (MIIM), is 
adopted in chunking. Besides, a SVM plus 
sigmoid model is applied to integrate various 
types of contexts and implement the 
discriminative modeling in MIIM. 
The third step is post processing, which tries 
to further resolve ambiguous segmentations and 
unknown word segmentation. Due to time limit, 
this is only done in Chinese word segmentation. 
No post processing is done on Chinese named 
entity recognition. 
The rest of this paper is as follows: Section 2 
describes the context-dependent Mutual 
Information Independence Model in details while 
purely statistical post-processing in Chinese word 
segmentation is presented in Section 3. Finally, 
we report the results of our system in Chinese 
word segmentation and named entity recognition 
in Section 4 and conclude our work in Section 5. 
154
2 Mutual Information Independence 
Model  
In this paper, we use a discriminative Markov 
model, called Mutual Information Independence 
Model (MIIM) as proposed by Zhou et al(2002), 
for Chinese word segmentation and named entity 
recognition. MIIM is derived from a conditional 
probability model. Given an observation sequence 
n
n oooO L211 = , MIIM finds a stochastic optimal 
state(tag) sequence nn sssS L211 =  that 
maximizes: 
??
==
? +=
n
i
n
i
n
i
i
i
nn OsPSsPMIOSP
1
1
2
1
111 )|(log),()|(log  
We call the above model the Mutual 
Information Independence Model due to its 
Pair-wise Mutual Information (PMI) assumption 
(Zhou et al2002). The above model consists of 
two sub-models: the state transition model 
?
=
?n
i
i
i SsPMI
2
1
1 ),( , which can be computed by 
applying ngram modeling, and the output model 
?
=
n
i
n
i OsP
1
1 )|(log , which can be estimated by any 
probability-based classifier, such as a maximum 
entropy classifier or a SVM plus sigmoid 
classifier (Zhou et al2006).  In this competition, 
the SVM plus sigmoid classifier is used in 
Chinese word segmentation while a simple 
backoff  approach as described in Zhou et al
(2002) is used in named entity recognition. 
Here, a variant of the Viterbi algorithm 
(Viterbi 1967) in decoding the standard Hidden 
Markov Model (HMM) (Rabiner 1989) is 
implemented to find the most likely state 
sequence by replacing the state transition model 
and the output model of the standard HMM with 
the state transition model and the output model of 
the MIIM, respectively. The above MIIM has 
been successfully applied in many applications, 
such as text chunking (Zhou 2004), Chinese word 
segmentation ( Zhou 2005), English named entity 
recognition in the newswire domain (Zhou et al
2002) and the biomedical domain (Zhou et al
2004; Zhou et al2006). 
For Chinese word segmentation and named 
entity recognition by chunking, a word or a entity 
name is regarded as a chunk of one or more word 
atoms and we have: 
? >=< iii wpo , ; iw is the thi ?  word atom in 
the sequence of word atoms nn wwwW L211 = ; 
ip  is the word formation pattern of the word 
atom iw . Here ip  measures the word 
formation power of the word atom iw  and 
consists of: 
o The percentage of iw  occurring as a whole 
word (round to 10%) 
o The percentage of iw  occurring at the 
beginning of other words (round to 10%) 
o The percentage of iw  occurring at the end 
of other words (round to 10%) 
o The length of iw  
o Especially for named entity recognition, 
the percentages of a word occurring in 
different entity types (round to 10%). 
? is : the states are used to bracket and 
differentiate various types of words and 
optional entity types for named entity 
recognition. In this way, Chinese word 
segmentation and named entity recognition 
can be regarded as a bracketing and 
classification process. is  is structural and 
consists of two parts: 
o Boundary category (B): it includes four 
values: {O, B, M, E}, where O means that 
current word atom is a whOle word or 
entity name and B/M/E means that current 
word atom is at the Beginning/in the 
Middle/at the End of a word or entity name. 
o Unit category (W): It is used to denote the 
type of the word or entity name.  
Because of the limited number of boundary 
and unit categories, the current word atom 
formation pattern ip  described above is added 
into the state transition model in MIIM. This 
makes the above MIIM context dependent as 
follows: 
??
==
?
? +=
n
i
n
i
n
i
ii
i
i
nn
OsPppSsPMI
OSP
1
1
2
1
1
1
11
)|(log)|,(
)|(log
 
3 Post Processing in Word 
Segmentation 
The third step is post processing, which tries to 
resolve ambiguous segmentations and false 
unknown word generation raised in the second 
step. Due to time limit, this is only done in 
Chinese word segmentation, i.e. no post 
processing is done on Chinese named entity 
recognition. 
155
A simple pattern-based method is employed to 
capture context information to correct the 
segmentation errors generated in the second steps. 
The pattern is designed as follows: 
<Ambiguous Entry (AE)> | <Left Context, 
Right Context> => <Proper Segmentation> 
The ambiguity entry (AE) means ambiguous 
segmentations or forced-generated unknown 
words. We use the 1st and 2nd words before AE as 
the left context and the 1st and 2nd words after AE 
as the right context. To reduce sparseness, we also 
only use the 1st left and right words as context. 
This means that there are two patterns generated 
for the same context. All the patterns are 
automatically learned from training corpus using 
the following algorithm. 
 
LearningPatterns() 
// Input: training corpus 
// Output: patterns 
BEGIN 
(1) Training a MIIM model using training 
corpus 
(2) Using the MIIM model to segment training 
corpus 
(3) Aligning the training corpus with the 
segmented training corpus 
(4) Extracting error segmentations 
(5) Generating disambiguation patterns using 
the left and right context 
(6) Removing the conflicting entries if two 
patterns have the same left hand side but 
different right hand side. 
END 
 
4 Evaluation 
We first develop our system using the PKU data 
released in the Second SIGHAN Bakeoff last 
year. Then, we train and evaluate it on the Third 
SIGHAN Bakeoff corpora without any 
fine-tuning. We only carry out our evaluation on 
the closed tracks. It means that we do not use any 
additional knowledge beyond the training corpus. 
Precision (P), Recall (R), F-measure (F), OOV 
Recall and IV Recall are adopted to measure the 
performance of word segmentation. Accuracy 
(A), Precision (P), Recall (R) and F-measure (F) 
are adopted to measure the performance of NER. 
Tables 1, 2 and 3 in the next page report the 
performance of our algorithm on different corpus 
in the SIGHAN Bakeoff 02 and Bakeoff 03, 
respectively. For the performance of other 
systems, please refer to 
http://sighan.cs.uchicago.edu/bakeoff2005/data/r
esults.php.htm for the Chinese bakeoff 2005 and 
http://sighan.cs.uchicago.edu/bakeoff2006/longst
ats.html for the Chinese bakeoff 2006.  
Comparison against other systems shows that 
our system achieves the state-of-the-art 
performance on all Chinese word segmentation 
closed tracks and shows good scalability across 
different corpora. The small performance gap 
should be able to overcome by replacing the word 
unigram model with the more powerful word 
bigram model. Due to very limited time of less 
than three days, although our NER system under 
the unified framework as Chinese word 
segmentation does not achieve the 
state-of-the-art, its performance in NER is quite 
promising and provides a good platform for 
further improvement. Error analysis reveals that 
OOV is still an open problem that is far from to 
resolve. In addition, different corpus defines 
different segmentation principles. This will stress 
OOV handling in the extreme. Therefore a system 
trained on one genre usually performances worse 
when faced with text from a different register. 
5 Conclusion 
This paper proposes a purely unified statistical 
three-stage strategy in Chinese word 
segmentation and named entity recognition, 
which are based on a context-dependent Mutual 
Information Independence Model. Evaluation 
shows that our system achieves the 
states-of-the-art segmentation performance and 
provides a good platform for further performance 
improvement of Chinese NER. 
References  
Rabiner L. 1989. A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition. IEEE 77(2), pages257-285.  
Viterbi A.J. 1967. Error Bounds for 
Convolutional Codes and an Asymptotically 
Optimum Decoding Algorithm. IEEE 
Transactions on Information Theory, IT 13(2), 
260-269. 
Zhou GuoDong and Su Jain. 2002. Named Entity 
Recognition Using a HMM-based Chunk 
Tagger, Proceedings of the 40th Annual Meeting 
of the Association for Computational 
Linguistics  (ACL?2002). Philadelphia. July 
2002. pp473-480.  
156
Zhou GuoDong, Zhang Jie, Su Jian, Shen Dan and 
Tan ChewLim. 2004. Recognizing Names in 
Biomedical Texts: a Machine Learning 
Approach. Bioinformatics. 20(7): 1178-1190. 
DOI: 10.1093/bioinformatics/bth060. 2004. 
ISSN: 1460-2059 
Zhou GuoDong. 2004. Discriminative hidden 
Markov modeling with long state dependence 
using a kNN ensemble. Proceedings of 20th 
International Conference on Computational 
Linguistics (COLING?2004).  23-27 Aug, 2004, 
Geneva, Switzerland.  
Zhou GuoDong. 2005. A chunking strategy 
towards unknown word detection in Chinese 
word segmentation. Proceedings of 2nd 
International Joint Conference on Natural 
Language Processing (IJCNLP?2005), Lecture 
Notes in Computer Science (LNCS 3651) 
Zhou GuoDong. 2006. Recognizing names in 
biomedical texts using Mutual Information 
Independence Model and SVM plus Sigmod. 
International Journal of Medical Informatics 
(Article in Press). ISSN 1386-5056 
Tables  
Task P R F OOV Recall IV Recall 
CityU 0.9 38 0.952 94.5 0.578 0.967 
MSRA 0.952 0.962 95.7 0.51 0.98 
CKIP 0.94 0.957 94.8 0.502 0.976 
PKU 0.952 0.952 95.2 0.71 0.967 
Table 1: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 02  
 
Task P R F OOV Recall IV Recall 
CityU 0.968 0.961 96.5 0.633 0.983 
MSRA 0.961 0.953 95.7 0.499 0.977 
CKIP 0.958 0.941 94.9 0.554 0.976 
UPUC 0.936 0.917 92.6 0.617 0.966 
Table 2: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 03 
 
Task A P R F 
MSRA 0.9743 0.8150 0.7882 79.92 
CityU 0.9725 0.8466 0.8061 82.59 
     Table 3: Performance of NER on Closed Tracks in the SIGHAN Bakeoff 03 
 
 
 
157
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 415?422,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Partially Supervised Sense Disambiguation by Learning Sense Number
from Tagged and Untagged Corpora
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
Supervised and semi-supervised sense dis-
ambiguation methods will mis-tag the in-
stances of a target word if the senses of
these instances are not defined in sense in-
ventories or there are no tagged instances
for these senses in training data. Here we
used a model order identification method
to avoid the misclassification of the in-
stances with undefined senses by discov-
ering new senses from mixed data (tagged
and untagged corpora). This algorithm
tries to obtain a natural partition of the
mixed data by maximizing a stability cri-
terion defined on the classification result
from an extended label propagation al-
gorithm over all the possible values of
the number of senses (or sense number,
model order). Experimental results on
SENSEVAL-3 data indicate that it outper-
forms SVM, a one-class partially super-
vised classification algorithm, and a clus-
tering based model order identification al-
gorithm when the tagged data is incom-
plete.
1 Introduction
In this paper, we address the problem of partially
supervised word sense disambiguation, which is
to disambiguate the senses of occurrences of a tar-
get word in untagged texts when given incomplete
tagged corpus 1.
Word sense disambiguation can be defined as
associating a target word in a text or discourse
1?incomplete tagged corpus? means that tagged corpus
does not include the instances of some senses for the target
word, while these senses may occur in untagged texts.
with a definition or meaning. Many corpus based
methods have been proposed to deal with the sense
disambiguation problem when given definition for
each possible sense of a target word or a tagged
corpus with the instances of each possible sense,
e.g., supervised sense disambiguation (Leacock et
al., 1998), and semi-supervised sense disambigua-
tion (Yarowsky, 1995).
Supervised methods usually rely on the infor-
mation from previously sense tagged corpora to
determine the senses of words in unseen texts.
Semi-supervised methods for WSD are charac-
terized in terms of exploiting unlabeled data in
the learning procedure with the need of prede-
fined sense inventories for target words. The in-
formation for semi-supervised sense disambigua-
tion is usually obtained from bilingual corpora
(e.g. parallel corpora or untagged monolingual
corpora in two languages) (Brown et al, 1991; Da-
gan and Itai, 1994), or sense-tagged seed examples
(Yarowsky, 1995).
Some observations can be made on the previous
supervised and semi-supervised methods. They
always rely on hand-crafted lexicons (e.g., Word-
Net) as sense inventories. But these resources may
miss domain-specific senses, which leads to in-
complete sense tagged corpus. Therefore, sense
taggers trained on the incomplete tagged corpus
will misclassify some instances if the senses of
these instances are not defined in sense invento-
ries. For example, one performs WSD in informa-
tion technology related texts using WordNet 2 as
sense inventory. When disambiguating the word
?boot? in the phrase ?boot sector?, the sense tag-
ger will assign this instance with one of the senses
of ?boot? listed in WordNet. But the correct sense
2Online version of WordNet is available at
http://wordnet.princeton.edu/cgi-bin/webwn2.0
415
?loading operating system into memory? is not in-
cluded in WordNet. Therefore, this instance will
be associated with an incorrect sense.
So, in this work, we would like to study the
problem of partially supervised sense disambigua-
tion with an incomplete sense tagged corpus.
Specifically, given an incomplete sense-tagged
corpus and a large amount of untagged examples
for a target word 3, we are interested in (1) label-
ing the instances in the untagged corpus with sense
tags occurring in the tagged corpus; (2) trying to
find undefined senses (or new senses) of the target
word 4 from the untagged corpus, which will be
represented by instances from the untagged cor-
pus.
We propose an automatic method to estimate
the number of senses (or sense number, model or-
der) of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification result over all
the possible values of sense number. At the same
time, we can obtain a classification of the mixed
data with the optimal number of groups. If the es-
timated sense number in the mixed data is equal
to the sense number of the target word in tagged
corpus, then there is no new sense in untagged
corpus. Otherwise new senses will be represented
by groups in which there is no instance from the
tagged corpus.
This partially supervised sense disambiguation
algorithm may help enriching manually compiled
lexicons by inducing new senses from untagged
corpora.
This paper is organized as follows. First, a
model order identification algorithm will be pre-
sented for partially supervised sense disambigua-
tion in section 2. Section 3 will provide experi-
mental results of this algorithm for sense disam-
biguation on SENSEVAL-3 data. Then related
work on partially supervised classification will be
summarized in section 4. Finally we will conclude
our work and suggest possible improvements in
section 5.
2 Partially Supervised Word Sense
Disambiguation
The partially supervised sense disambiguation
problem can be generalized as a model order iden-
3Untagged data usually includes the occurrences of all the
possible senses of the target word
4?undefined senses? are the senses that do not appear in
tagged corpus.
tification problem. We try to estimate the sense
number of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification results over all
the possible values of sense number. If the esti-
mated sense number in the mixed data is equal to
the sense number in the tagged corpus, then there
is no new sense in the untagged corpus. Other-
wise new senses will be represented by clusters in
which there is no instance from the tagged corpus.
The stability criterion assesses the agreement be-
tween classification results on full mixed data and
sampled mixed data. A partially supervised clas-
sification algorithm is used to classify the full or
sampled mixed data into a given number of classes
before the stability assessment, which will be pre-
sented in section 2.1. Then we will provide the
details of the model order identification procedure
in section 2.2.
2.1 An Extended Label Propagation
Algorithm
Table 1: Extended label propagation algorithm.
Function: ELP(DL, DU , k, Y 0DL+DU )Input: labeled examples DL, unlabeled
examples DU , model order k, initial
labeling matrix Y 0DL+DU ;Output: the labeling matrix YDU on DU ;
1 If k < kXL then
YDU =NULL;
2 Else if k = kXL then
Run plain label propagation algorithm
on DU with YDU as output;
3 Else then
3.1 Estimate the size of tagged data set
of new classes;
3.2 Generate tagged examples from DU
for (kXL + 1)-th to k-th new classes;
3.3 Run plain label propagation algorithm
on DU with augmented tagged dataset
as labeled data;
3.4 YDU is the output from plain label
propagation algorithm;
End if
4 Return YDU ;
Let XL+U = {xi}ni=1 be a set of contexts of
occurrences of an ambiguous word w, where xi
represents the context of the i-th occurrence, and n
is the total number of this word?s occurrences. Let
416
SL = {sj}cj=1 denote the sense tag set of w in XL,
where XL denotes the first l examples xg(1 ? g ?
l) that are labeled as yg (yg ? SL). Let XU denote
other u (l + u = n) examples xh(l + 1 ? h ? n)
that are unlabeled.
Let Y 0XL+U ? N |XL+U |?|SL| represent initialsoft labels attached to tagged instances, where
Y 0XL+U ,ij = 1 if yi is sj and 0 otherwise. Let Y 0XL
be the top l rows of Y 0XL+U and Y 0XU be the remain-
ing u rows. Y 0XL is consistent with the labeling inlabeled data, and the initialization of Y 0XU can bearbitrary.
Let k denote the possible value of the number
of senses in mixed data XL+U , and kXL be the
number of senses in initial tagged data XL. Note
that kXL = |SL|, and k ? kXL .
The classification algorithm in the order identi-
fication process should be able to accept labeled
data DL 5, unlabeled data DU 6 and model order k
as input, and assign a class label or a cluster index
to each instance in DU as output. Previous super-
vised or semi-supervised algorithms (e.g. SVM,
label propagation algorithm (Zhu and Ghahra-
mani, 2002)) cannot classify the examples in DU
into k groups if k > kXL . The semi-supervised k-
means clustering algorithm (Wagstaff et al, 2001)
may be used to perform clustering analysis on
mixed data, but its efficiency is a problem for clus-
tering analysis on a very large dataset since multi-
ple restarts are usually required to avoid local op-
tima and multiple iterations will be run in each
clustering process for optimizing a clustering so-
lution.
In this work, we propose an alternative method,
an extended label propagation algorithm (ELP),
which can classify the examples in DU into k
groups. If the value of k is equal to kXL , then
ELP is identical with the plain label propagation
algorithm (LP) (Zhu and Ghahramani, 2002). Oth-
erwise, if the value of k is greater than kXL , we
perform classification by the following steps:
(1) estimate the dataset size of each new class as
sizenew class by identifying the examples of new
classes using the ?Spy? technique 7 and assuming
5DL may be the dataset XL or a subset sampled from XL.
6DU may be the dataset XU or a subset sampled from
XU .
7The ?Spy? technique was proposed in (Liu et al, 2003).
Our re-implementation of this technique consists of three
steps: (1) sample a small subset DsL with the size 15%?|DL|from DL; (2) train a classifier with tagged data DL ? DsL;(3) classify DU and DsL, and then select some examples from
DU as the dataset of new classes, which have the classifica-
that new classes are equally distributed;
(2) D?L = DL, D?U = DU ;
(3) remove tagged examples of the m-th new
class (kXL + 1 ? m ? k) from D?L 8 and train a
classifier on this labeled dataset without the m-th
class;
(4) the classifier is then used to classify the ex-
amples in D?U ;
(5) the least confidently unlabeled point
xclass m ? D
?
U , together with its label m, is added
to the labeled data D?L = D?L + xclass m, and
D?U = D
?
U ? xclass m;
(6) steps (3) to (5) are repeated for each new
class till the augmented tagged data set is large
enough (here we try to select sizenew class/4 ex-
amples with their sense tags as tagged data for
each new class);
(7) use plain LP algorithm to classify remaining
unlabeled data D?U with D?L as labeled data.
Table 1 shows this extended label propagation
algorithm.
Next we will provide the details of the plain la-
bel propagation algorithm.
Define Wij = exp(?d
2
ij
?2 ) if i 6= j and Wii = 0(1 ? i, j ? |DL + DU |), where dij is the distance
(e.g., Euclidean distance) between the example xi
and xj , and ? is used to control the weight Wij .
Define |DL + DU | ? |DL + DU | probability
transition matrix Tij = P (j ? i) = Wij?n
k=1 Wkj
,
where Tij is the probability to jump from example
xj to example xi.
Compute the row-normalized matrix T by
T ij = Tij/
?n
k=1 Tik.
The classification solution is obtained by
YDU = (I ? T uu)?1T ulY 0DL . I is |DU | ? |DU |
identity matrix. T uu and T ul are acquired by split-
ting matrix T after the |DL|-th row and the |DL|-th
column into 4 sub-matrices.
2.2 Model Order Identification Procedure
For achieving the model order identification (or
sense number estimation) ability, we use a clus-
ter validation based criterion (Levine and Domany,
2001) to infer the optimal number of senses of w
in XL+U .
tion confidence less than the average of that in DsL. Classifi-cation confidence of the example xi is defined as the absolute
value of the difference between two maximum values from
the i-th row in labeling matrix.
8Initially there are no tagged examples for the m-th class
in D?L. Therefore we do not need to remove tagged examples
for this new class, and then directly train a classifier with D?L.
417
Table 2: Model order evaluation algorithm.
Function: CV(XL+U , k, q, Y 0XL+U )
Input: data set XL+U , model order k,
and sampling frequency q;
Output: the score of the merit of k;
1 Run the extended label propagation
algorithm with XL, XU , k and Y 0XL+U ;
2 Construct connectivity matrix Ck based
on above classification solution on XU ;
3 Use a random predictor ?k to assign
uniformly drawn labels to each vector
in XU ;
4 Construct connectivity matrix C?k using
above classification solution on XU ;
5 For ? = 1 to q do
5.1 Randomly sample a subset X?L+U with
the size ?|XL+U | from XL+U , 0 < ? < 1;
5.2 Run the extended label propagation
algorithm with X?L, X?U , k and Y 0?;
5.3 Construct connectivity matrix C?k using
above classification solution on X?U ;
5.4 Use ?k to assign uniformly drawn labels
to each vector in X?U ;
5.5 Construct connectivity matrix C??k usingabove classification solution on X?U ;
Endfor
6 Evaluate the merit of k using following
formula:
Mk = 1q
?
?(M(C?k , Ck) ? M(C??k , C?k)),
where M(C?, C) is given by equation (2);
7 Return Mk;
Then this model order identification procedure
can be formulated as:
k?XL+U = argmaxKmin?k?Kmax{CV (XL+U , k, q, Y 0XL+U )}.(1)
k?XL+U is the estimated sense number in XL+U ,
Kmin (or Kmax) is the minimum (or maximum)
value of sense number, and k is the possible value
of sense number in XL+U . Note that k ? kXL .
Then we set Kmin = kXL . Kmax may be set as a
value greater than the possible ground-truth value.
CV is a cluster validation based evaluation func-
tion. Table 2 shows the details of this function.
We set q, the resampling frequency for estimation
of stability score, as 20. ? is set as 0.90. The ran-
dom predictor assigns uniformly distributed class
labels to each instance in a given dataset. We
run this CV procedure for each value of k. The
value of k that maximizes this function will be se-
lected as the estimation of sense number. At the
same time, we can obtain a partition of XL+U with
k?XL+U groups.
The function M(C?, C) in Table 2 is given by
(Levine and Domany, 2001):
M(C?, C) =
?
i,j 1{C
?
i,j = Ci,j = 1, xi, xj ? X?U}
?
i,j 1{Ci,j = 1, xi, xj ? X
?
U}
,
(2)
where X?U is the untagged data in X?L+U , X?L+U
is a subset with the size ?|XL+U | (0 < ? < 1)
sampled from XL+U , C or C? is |XU | ? |XU | or
|X?U | ? |X
?
U | connectivity matrix based on classi-
fication solutions computed on XU or X?U respec-
tively. The connectivity matrix C is defined as:
Ci,j = 1 if xi and xj belong to the same cluster,
otherwise Ci,j = 0. C? is calculated in the same
way.
M(C?, C) measures the proportion of example
pairs in each group computed on XU that are also
assigned into the same group by the classification
solution on X?U . Clearly, 0 ? M ? 1. Intu-
itively, if the value of k is identical with the true
value of sense number, then classification results
on the different subsets generated by sampling
should be similar with that on the full dataset. In
the other words, the classification solution with the
true model order as parameter is robust against re-
sampling, which gives rise to a local optimum of
M(C?, C).
In this algorithm, we normalize M(C?k , Ck) by
the equation in step 6 of Table 2, which makes
our objective function different from the figure of
merit (equation ( 2)) proposed in (Levine and Do-
many, 2001). The reason to normalize M(C?k , Ck)
is that M(C?k , Ck) tends to decrease when increas-
ing the value of k (Lange et al, 2002). Therefore
for avoiding the bias that the smaller value of k
is to be selected as the model order, we use the
cluster validity of a random predictor to normalize
M(C?k , Ck).
If k?XL+U is equal to kXL , then there is no new
sense in XU . Otherwise (k?XL+U > kXL) new
senses of w may be represented by the groups in
which there is no instance from XL.
3 Experiments and Results
3.1 Experiment Design
We evaluated the ELP based model order iden-
tification algorithm on the data in English lexi-
cal sample task of SENSEVAL-3 (including all
418
Table 3: Description of The percentage of official
training data used as tagged data when instances
with different sense sets are removed from official
training data.
The percentage of official
training data used as tagged data
Ssubset = {s1} 42.8%
Ssubset = {s2} 76.7%
Ssubset = {s3} 89.1%
Ssubset = {s1, s2} 19.6%
Ssubset = {s1, s3} 32.0%
Ssubset = {s2, s3} 65.9%
the 57 English words ) 9, and further empirically
compared it with other state of the art classifi-
cation methods, including SVM 10 (the state of
the art method for supervised word sense disam-
biguation (Mihalcea et al, 2004)), a one-class par-
tially supervised classification algorithm (Liu et
al., 2003) 11, and a semi-supervised k-means clus-
tering based model order identification algorithm.
The data for English lexical samples task in
SENSEVAL-3 consists of 7860 examples as offi-
cial training data, and 3944 examples as official
test data for 57 English words. The number of
senses of each English word varies from 3 to 11.
We evaluated these four algorithms with differ-
ent sizes of incomplete tagged data. Given offi-
cial training data of the word w, we constructed
incomplete tagged data XL by removing the all
the tagged instances from official training data that
have sense tags from Ssubset, where Ssubset is a
subset of the ground-truth sense set S for w, and S
consists of the sense tags in official training set for
w. The removed training data and official test data
of w were used as XU . Note that SL = S?Ssubset.
Then we ran these four algorithm for each target
word w with XL as tagged data and XU as un-
tagged data, and evaluated their performance us-
ing the accuracy on official test data of all the 57
words. We conducted six experiments for each tar-
get word w by setting Ssubset as {s1}, {s2}, {s3},
{s1, s2}, {s1, s3}, or {s2, s3}, where si is the i-th
most frequent sense of w. Ssubset cannot be set as
{s4} since some words have only three senses. Ta-
ble 3 lists the percentage of official training data
used as tagged data (the number of examples in in-
9Available at http://www.senseval.org/senseval3
10we used a linear SV M light, available at
http://svmlight.joachims.org/.
11Available at http://www.cs.uic.edu/?liub/LPU/LPU-
download.html
complete tagged data divided by the number of ex-
amples in official training data) when we removed
the instances with sense tags from Ssubset for all
the 57 words. If Ssubset = {s3}, then most of
sense tagged examples are still included in tagged
data. If Ssubset = {s1, s2}, then there are very few
tagged examples in tagged data. If no instances are
removed from official training data, then the value
of percentage is 100%.
Given an incomplete tagged corpus for a target
word, SVM does not have the ability to find the
new senses from untagged corpus. Therefore it la-
bels all the instances in the untagged corpus with
sense tags from SL.
Given a set of positive examples for a class and
a set of unlabeled examples, the one-class partially
supervised classification algorithm, LPU (Learn-
ing from Positive and Unlabeled examples) (Liu
et al, 2003), learns a classifier in four steps:
Step 1: Identify a small set of reliable negative
examples from unlabeled examples by the use of a
classifier.
Step 2: Build a classifier using positive ex-
amples and automatically selected negative exam-
ples.
Step 3: Iteratively run previous two steps until
no unlabeled examples are classified as negative
ones or the unlabeled set is null.
Step 4: Select a good classifier from the set of
classifiers constructed above.
For comparison, LPU 12 was run to perform
classification on XU for each class in XL. The
label of each instance in XU was determined by
maximizing the classification score from LPU out-
put for each class. If the maximum score of an
instance is negative, then this instance will be la-
beled as a new class. Note that LPU classifies
XL+U into kXL + 1 groups in most of cases.
The clustering based partially supervised sense
disambiguation algorithm was implemented by re-
placing ELP with a semi-supervised k-means clus-
tering algorithm (Wagstaff et al, 2001) in the
model order identification procedure. The label
information in labeled data was used to guide the
semi-supervised clustering on XL+U . Firstly, the
labeled data may be used to determine initial clus-
ter centroids. If the cluster number is greater
12The three parameters in LPU were set as follows: ?-s1
spy -s2 svm -c 1?. It means that we used the spy technique for
step 1 in LPU, the SVM algorithm for step 2, and selected the
first or the last classifier as the final classifier. It is identical
with the algorithm ?Spy+SVM IS? in Liu et al (2003).
419
than kXL , the initial centroids of clusters for new
classes will be assigned as randomly selected in-
stances. Secondly, in the clustering process, the
instances with the same class label will stay in
the same cluster, while the instances with different
class labels will belong to different clusters. For
better clustering solution, this clustering process
will be restarted three times. Clustering process
will be terminated when clustering solution con-
verges or the number of iteration steps is more than
30. Kmin = kXL = |SL|, Kmax = Kmin + m. m
is set as 4.
We used Jensen-Shannon (JS) divergence (Lin,
1991) as distance measure for semi-supervised
clustering and ELP, since plain LP with JS diver-
gence achieves better performance than that with
cosine similarity on SENSEVAL-3 data (Niu et al,
2005).
For the LP process in ELP algorithm, we con-
structed connected graphs as follows: two in-
stances u, v will be connected by an edge if u is
among v?s 10 nearest neighbors, or if v is among
u?s 10 nearest neighbors as measured by cosine or
JS distance measure (following (Zhu and Ghahra-
mani, 2002)).
We used three types of features to capture the
information in all the contextual sentences of tar-
get words in SENSEVAL-3 data for all the four
algorithms: part-of-speech of neighboring words
with position information, words in topical con-
text without position information (after removing
stop words), and local collocations (as same as the
feature set used in (Lee and Ng, 2002) except that
we did not use syntactic relations). We removed
the features with occurrence frequency (counted
in both training set and test set) less than 3 times.
If the estimated sense number is more than the
sense number in the initial tagged corpus XL, then
the results from order identification based meth-
ods will consist of the instances from clusters of
unknown classes. When assessing the agreement
between these classification results and the known
results on official test set, we will encounter the
problem that there is no sense tag for each instance
in unknown classes. Slonim and Tishby (2000)
proposed to assign documents in each cluster with
the most dominant class label in that cluster, and
then conducted evaluation on these labeled docu-
ments. Here we will follow their method for as-
signing sense tags to unknown classes from LPU,
clustering based order identification process, and
ELP based order identification process. We as-
signed the instances from unknown classes with
the dominant sense tag in that cluster. The result
from LPU always includes only one cluster of the
unknown class. We also assigned the instances
from the unknown class with the dominant sense
tag in that cluster. When all instances have their
sense tags, we evaluated the their results using the
accuracy on official test set.
3.2 Results on Sense Disambiguation
Table 4 summarizes the accuracy of SVM, LPU,
the semi-supervised k-means clustering algorithm
with correct sense number |S| or estimated sense
number k?XL+U as input, and the ELP algorithm
with correct sense number |S| or estimated sense
number k?XL+U as input using various incomplete
tagged data. The last row in Table 4 lists the av-
erage accuracy of each algorithm over the six ex-
perimental settings. Using |S| as input means that
we do not perform order identification procedure,
while using k?XL+U as input is to perform order
identification and obtain the classification results
on XU at the same time.
We can see that ELP based method outperforms
clustering based method in terms of average accu-
racy under the same experiment setting, and these
two methods outperforms SVM and LPU. More-
over, using the correct sense number as input helps
to improve the overall performance of both clus-
tering based method and ELP based method.
Comparing the performance of the same sys-
tem with different sizes of tagged data (from the
first experiment to the third experiment, and from
the fourth experiment to the sixth experiment), we
can see that the performance was improved when
given more labeled data. Furthermore, ELP based
method outperforms other methods in terms of ac-
curacy when rare senses (e.g. s3) are missing in
the tagged data. It seems that ELP based method
has the ability to find rare senses with the use of
tagged and untagged corpora.
LPU algorithm can deal with only one-class
classification problem. Therefore the labeled data
of other classes cannot be used when determining
the positive labeled data for current class. ELP
can use the labeled data of all the known classes to
determine the seeds of unknown classes. It may
explain why LPU?s performance is worse than
ELP based sense disambiguation although LPU
can correctly estimate the sense number in XL+U
420
Table 4: This table summarizes the accuracy of SVM, LPU, the semi-supervised k-means clustering al-
gorithm with correct sense number |S| or estimated sense number k?XL+U as input, and the ELP algorithm
with correct sense number |S| or estimated sense number k?XL+U as input on the official test data of ELS
task in SENSEVAL-3 when given various incomplete tagged corpora.
Clustering algorithm ELP algorithm Clustering algorithm ELP algorithm
SVM LPU with |S| as input with |S| as input with k?XL+U as input with k?XL+U as input
Ssubset =
{s1} 30.6% 22.3% 43.9% 47.8% 40.0% 38.7%
Ssubset =
{s2} 59.7% 54.6% 44.0% 62.4% 48.5% 62.6%
Ssubset =
{s3} 67.0% 53.4% 48.7% 67.2% 52.4% 69.1%
Ssubset =
{s1, s2} 14.6% 13.1% 44.4% 40.2% 35.6% 33.0%
Ssubset =
{s1, s3} 25.7% 21.1% 48.5% 37.9% 39.8% 31.0%
Ssubset =
{s2, s3} 56.2% 53.1% 47.3% 59.4% 46.6% 58.7%
Average accuracy 42.3% 36.3% 46.1% 52.5% 43.8% 48.9%
Table 5: These two tables provide the mean and
standard deviation of absolute values of the differ-
ence between ground-truth results |S| and sense
numbers estimated by clustering or ELP based or-
der identification procedure respectively.
Clustering based method ELP based method
Ssubset =
{s1} 1.3?1.1 2.2?1.1
Ssubset =
{s2} 2.4?0.9 2.4?0.9
Ssubset =
{s3} 2.6?0.7 2.6?0.7
Ssubset =
{s1, s2} 1.2?0.6 1.6?0.5
Ssubset =
{s1, s3} 1.4?0.6 1.8?0.4
Ssubset =
{s2, s3} 1.8?0.5 1.8?0.5
when only one sense is missing in XL.
When very few labeled examples are avail-
able, the noise in labeled data makes it difficult
to learn the classification score (each entry in
YDU ). Therefore using the classification confi-
dence criterion may lead to poor performance of
seed selection for unknown classes if the classifi-
cation score is not accurate. It may explain why
ELP based method does not outperform cluster-
ing based method with small labeled data (e.g.,
Ssubset = {s1}).
3.3 Results on Sense Number Estimation
Table 5 provides the mean and standard devia-
tion of absolute difference values between ground-
truth results |S| and sense numbers estimated by
clustering or ELP based order identification pro-
cedures respectively. For example, if the ground
truth sense number of the word w is kw, and the es-
timated value is k?w, then the absolute value of the
difference between these two values is |kw ? k?w|.
Therefore we can have this value for each word.
Then we calculated the mean and deviation on this
array of absolute values. LPU does not have the
order identification capability since it always as-
sumes that there is at least one new class in un-
labeled data, and does not further differentiate the
instances from these new classes. Therefore we do
not provide the order identification results of LPU.
From the results in Table 5, we can see that esti-
mated sense numbers are closer to ground truth re-
sults when given less labeled data for clustering or
ELP based methods. Moreover, clustering based
method performs better than ELP based method in
terms of order identification when given less la-
beled data (e.g., Ssubset = {s1}). It seems that
ELP is not robust to the noise in small labeled data,
compared with the semi-supervised k-means clus-
tering algorithm.
4 Related Work
The work closest to ours is partially supervised
classification or building classifiers using positive
examples and unlabeled examples, which has been
studied in machine learning community (Denis et
al., 2002; Liu et al, 2003; Manevitz and Yousef,
2001; Yu et al, 2002). However, they cannot
421
group negative examples into meaningful clusters.
In contrast, our algorithm can find the occurrence
of negative examples and further group these neg-
ative examples into a ?natural? number of clusters.
Semi-supervised clustering (Wagstaff et al, 2001)
may be used to perform classification by the use
of labeled and unlabeled examples, but it encoun-
ters the same problem of partially supervised clas-
sification that model order cannot be automatically
estimated.
Levine and Domany (2001) and Lange et al
(2002) proposed cluster validation based criteria
for cluster number estimation. However, they
showed the application of the cluster validation
method only for unsupervised learning. Our work
can be considered as an extension of their methods
in the setting of partially supervised learning.
In natural language processing community, the
work that is closely related to ours is word sense
discrimination which can induce senses by group-
ing occurrences of a word into clusters (Schu?tze,
1998). If it is considered as unsupervised meth-
ods to solve sense disambiguation problem, then
our method employs partially supervised learning
technique to deal with sense disambiguation prob-
lem by use of tagged and untagged texts.
5 Conclusions
In this paper, we present an order identification
based partially supervised classification algorithm
and investigate its application to partially super-
vised word sense disambiguation problem. Exper-
imental results on SENSEVAL-3 data indicate that
our ELP based model order identification algo-
rithm achieves better performance than other state
of the art classification algorithms, e.g., SVM,
a one-class partially supervised algorithm (LPU),
and a semi-supervised k-means clustering based
model order identification algorithm.
References
Brown P., Stephen, D.P., Vincent, D.P., & Robert, Mer-
cer.. 1991. Word Sense Disambiguation Using Sta-
tistical Methods. Proceedings of ACL.
Dagan, I. & Itai A.. 1994. Word Sense Disambigua-
tion Using A Second Language Monolingual Cor-
pus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Denis, F., Gilleron, R., & Tommasi, M.. 2002. Text
Classification from Positive and Unlabeled Exam-
ples. Proceedings of the 9th International Confer-
ence on Information Processing and Management of
Uncertainty in Knowledge-Based Systems.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M.
2002. Stability-Based Model Selection. NIPS 15.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998.
Using Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24:1, 147?165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Eval-
uation of Knowledge Sources and Learning Algo-
rithms for Word Sense Disambiguation. Proceed-
ings of EMNLP, (pp. 41-48).
Levine, E., & Domany, E. 2001. Resampling Method
for Unsupervised Estimation of Cluster Validity.
Neural Computation, Vol. 13, 2573?2593.
Lin, J. 1991. Divergence Measures Based on the
Shannon Entropy. IEEE Transactions on Informa-
tion Theory, 37:1, 145?150.
Liu, B., Dai, Y., Li, X., Lee, W.S., & Yu, P.. 2003.
Building Text Classifiers Using Positive and Unla-
beled Examples. Proceedings of IEEE ICDM.
Manevitz, L.M., & Yousef, M.. 2001. One Class
SVMs for Document Classification. Journal of Ma-
chine Learning, 2, 139-154.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004.
The SENSEVAL-3 English Lexical Sample Task.
SENSEVAL-2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2005. Word Sense
Disambiguation Using Label Propagation Based
Semi-Supervised Learning. Proceedings of ACL.
Schu?tze, H.. 1998. Automatic Word Sense Discrimi-
nation. Computational Linguistics, 24:1, 97?123.
Wagstaff, K., Cardie, C., Rogers, S., & Schroedl, S..
2001. Constrained K-Means Clustering with Back-
ground Knowledge. Proceedings of ICML.
Yarowsky, D.. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Pro-
ceedings of ACL.
Yu, H., Han, J., & Chang, K. C.-C.. 2002. PEBL: Pos-
itive example based learning for web page classifi-
cation using SVM. Proceedings of ACM SIGKDD.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
422
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 568?575,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Relation Disambiguation with Order Identification
Capabilities
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
We present an unsupervised learning ap-
proach to disambiguate various relations
between name entities by use of various
lexical and syntactic features from the
contexts. It works by calculating eigen-
vectors of an adjacency graph?s Lapla-
cian to recover a submanifold of data
from a high dimensionality space and
then performing cluster number estima-
tion on the eigenvectors. This method
can address two difficulties encoutered
in Hasegawa et al (2004)?s hierarchical
clustering: no consideration of manifold
structure in data, and requirement to pro-
vide cluster number by users. Experiment
results on ACE corpora show that this
spectral clustering based approach outper-
forms Hasegawa et al (2004)?s hierarchi-
cal clustering method and a plain k-means
clustering method.
1 Introduction
The task of relation extraction is to identify vari-
ous semantic relations between name entities from
text. Prior work on automatic relation extraction
come in three kinds: supervised learning algorithms
(Miller et al, 2000; Zelenko et al, 2002; Culotta
and Soresen, 2004; Kambhatla, 2004; Zhou et al,
2005), semi-supervised learning algorithms (Brin,
1998; Agichtein and Gravano, 2000; Zhang, 2004),
and unsupervised learning algorithm (Hasegawa et
al., 2004).
Among these methods, supervised learning is usu-
ally more preferred when a large amount of la-
beled training data is available. However, it is
time-consuming and labor-intensive to manually tag
a large amount of training data. Semi-supervised
learning methods have been put forward to mini-
mize the corpus annotation requirement. Most of
semi-supervised methods employ the bootstrapping
framework, which only need to pre-define some ini-
tial seeds for any particular relation, and then boot-
strap from the seeds to acquire the relation. How-
ever, it is often quite difficult to enumerate all class
labels in the initial seeds and decide an ?optimal?
number of them.
Compared with supervised and semi-supervised
methods, Hasegawa et al (2004)?s unsupervised ap-
proach for relation extraction can overcome the dif-
ficulties on requirement of a large amount of labeled
data and enumeration of all class labels. Hasegawa
et al (2004)?s method is to use a hierarchical cluster-
ing method to cluster pairs of named entities accord-
ing to the similarity of context words intervening be-
tween the named entities. However, the drawback of
hierarchical clustering is that it required providing
cluster number by users. Furthermore, clustering is
performed in original high dimensional space, which
may induce non-convex clusters hard to identified.
This paper presents a novel application of spec-
tral clustering technique to unsupervised relation ex-
traction problem. It works by calculating eigenvec-
tors of an adjacency graph?s Laplacian to recover a
submanifold of data from a high dimensional space,
and then performing cluster number estimation on
a transformed space defined by the first few eigen-
vectors. This method may help us find non-convex
clusters. It also does not need to pre-define the num-
ber of the context clusters or pre-specify the simi-
larity threshold for the clusters as Hasegawa et al
568
(2004)?s method.
The rest of this paper is organized as follows. Sec-
tion 2 formulates unsupervised relation extraction
and presents how to apply the spectral clustering
technique to resolve the task. Then section 3 reports
experiments and results. Finally we will give a con-
clusion about our work in section 4.
2 Unsupervised Relation Extraction
Problem
Assume that two occurrences of entity pairs with
similar contexts, are tend to hold the same relation
type. Thus unsupervised relation extraction prob-
lem can be formulated as partitioning collections of
entity pairs into clusters according to the similarity
of contexts, with each cluster containing only entity
pairs labeled by the same relation type. And then, in
each cluster, the most representative words are iden-
tified from the contexts of entity pairs to induce the
label of relation type. Here, we only focus on the
clustering subtask and do not address the relation
type labeling subtask.
In the next subsections we will describe our pro-
posed method for unsupervised relation extraction,
which includes: 1) Collect the context vectors in
which the entity mention pairs co-occur; 2) Cluster
these Context vectors.
2.1 Context Vector and Feature Design
Let X = {xi}ni=1 be the set of context vectors of oc-
currences of all entity mention pairs, where xi repre-
sents the context vector of the i-th occurrence, and n
is the total number of occurrences of all entity pairs.
Each occurrence of entity mention pairs can be
denoted as follows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 represents the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity pairs respectively.
We extracted features from e1, e2, Cpre, Cmid,
Cpost to construct context vectors, which are com-
puted from the parse trees derived from Charniak
Parser (Charniak, 1999) and the Chunklink script 1
written by Sabine Buchholz from Tilburg University.
1 Software available at http://ilk.uvt.nl/ sabine/chunklink/
Words: Words in the two entities and three context
windows.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZA-
TION, FACILITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all words in the two entities and three con-
text windows.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two entities and
three context windows. The ?0? tag means that
the word is outside of any chunk. The ?I-XP? tag
means that this word is inside an XP chunk. The
?B-XP? by default means that the word is at the be-
ginning of an XP chunk.
? Grammatical function of the two entities and
three context windows. The last word in each chunk
is its head, and the function of the head is the func-
tion of the whole chunk. ?NP-SBJ? means a NP
chunk as the subject of the sentence. The other
words in a chunk that are not the head have ?NO-
FUNC? as their function.
? IOB-chains of the heads of the two entities. So-
called IOB-chain, noting the syntactic categories of
all the constituents on the path from the root node
to this leaf node of tree.
We combine the above lexical and syntactic fea-
tures with their position information in the context
to form the context vector. Before that, we filter out
low frequency features which appeared only once in
the entire set.
2.2 Context Clustering
Once the context vectors of entity pairs are prepared,
we come to the second stage of our method: cluster
these context vectors automatically.
In recent years, spectral clustering technique has
received more and more attention as a powerful ap-
proach to a range of clustering problems. Among
the efforts on spectral clustering techniques (Weiss,
1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,
2001; Zha et al, 2001), we adopt a modified version
(Sanguinetti et al, 2005) of the algorithm by Ng et
al. (2001) because it can provide us model order se-
lection capability.
Since we do not know how many relation types
in advance and do not have any labeled relation
569
Table 1: Context Clustering with Spectral-based Clustering
technique.
Input: A set of context vectors X = {x1, x2, ..., xn},
X ? <n?d;
Output: Clustered data and number of clusters;
1. Construct an affinity matrix by Aij = exp(? s
2
ij
?2 ) if i 6=j, 0 if i = j. Here, sij is the similarity between xi and
xj calculated by Cosine similarity measure. and the free
distance parameter ?2 is used to scale the weights;
2. Normalize the affinity matrix A to create the matrix L =
D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)
element is the sum of A?s ith row;
3. Set q = 2;
4. Compute q eigenvectors of L with greatest eigenvalues.
Arrange them in a matrix Y .
5. Perform elongated K-means with q + 1 centers on Y ,
initializing the (q + 1)-th mean in the origin;
6. If the q+1-th cluster contains any data points, then there
must be at least an extra cluster; set q = q + 1 and go
back to step 4. Otherwise, algorithm stops and outputs
clustered data and number of clusters.
training examples at hand, the problem of model
order selection arises, i.e. estimating the ?opti-
mal? number of clusters. Formally, let k be the
model order, we need to find k in Equation: k =
argmaxk{criterion(k)}. Here, the criterion is de-
fined on the result of spectral clustering.
Table 1 shows the details of the whole algorithm
for context clustering, which contains two main
stages: 1) Transformation of Clustering Space (Step
1-4); 2) Clustering in the transformed space using
Elongated K-means algorithm (Step 5-6).
2.3 Transformation of Clustering Space
We represent each context vector of entity pair as a
node in an undirected graph. Each edge (i,j) in the
graph is assigned a weight that reflects the similarity
between two context vectors i and j. Hence, the re-
lation extraction task for entity pairs can be defined
as a partition of the graph so that entity pairs that
are more similar to each other, e.g. labeled by the
same relation type, belong to the same cluster. As a
relaxation of such NP-hard discrete graph partition-
ing problem, spectral clustering technique computes
eigenvalues and eigenvectors of a Laplacian matrix
related to the given graph, and construct data clus-
ters based on such spectral information.
Thus the starting point of context clustering is to
construct an affinity matrix A from the data, which
is an n ? n matrix encoding the distances between
the various points. The affinity matrix is then nor-
malized to form a matrix L by conjugating with the
the diagonal matrix D?1/2 which has as entries the
square roots of the sum of the rows of A. This is to
take into account the different spread of the various
clusters (points belonging to more rarified clusters
will have lower sums of the corresponding row of
A). It is straightforward to prove that L is positive
definite and has eigenvalues smaller or equal to 1,
with equality holding in at least one case.
Let K be the true number of clusters present in
the dataset. If K is known beforehand, the first K
eigenvectors of L will be computed and arranged as
columns in a matrix Y . Each row of Y corresponds
to a context vector of entity pair, and the above pro-
cess can be considered as transforming the original
context vectors in a d-dimensional space to new con-
text vectors in the K-dimensional space. Therefore,
the rows of Y will cluster upon mutually orthogonal
points on the K dimensional sphere,rather than on
the coordinate axes.
2.4 The Elongated K-means algorithm
As the step 5 of Table 1 shows, the result of elon-
gated K-means algorithm is used to detect whether
the number of clusters selected q is less than the true
number K, and allows one to iteratively obtain the
number of clusters.
Consider the case when the number of clusters q
is less than the true cluster number K present in the
dataset. In such situation, taking the first q < K
eigenvectors, we will be selecting a q-dimensional
subspace in the clustering space. As the rows of the
K eigenvectors clustered along mutually orthogo-
nal vectors, their projections in a lower dimensional
space will cluster along radial directions. Therefore,
the general picture will be of q clusters elongated in
the radial direction, with possibly some clusters very
near the origin (when the subspace is orthogonal to
some of the discarded eigenvectors).
Hence, the K-means algorithm is modified as
the elongated K-means algorithm to downweight
distances along radial directions and penalize dis-
tances along transversal directions. The elongated
K-means algorithm computes the distance of point
x from the center ci as follows:
? If the center is not very near the origin, cTi ci > ? (? is a
parameter to be fixed by the user), the distances are cal-
570
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(a) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(b) 
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
(c) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(d) 
Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Three
elongated clusters in the 2D clustering space using
Spectral clustering: two dominant eigenvectors; (d)
The clustering result using Spectral-based clustering
(?2=0.05). (4,? and + denote examples in different
clusters)
culated as: edist(x, ci) = (x ? ci)TM(x ? ci), where
M = 1? (Iq ?
cicTi
cTi ci
) + ? cic
T
i
cTi ci
, ? is the sharpness param-
eter that controls the elongation (the smaller, the more
elongated the clusters) 2.
? If the center is very near the origin,cTi ci < ?, the dis-
tances are measured using the Euclidean distance.
In each iteration of procedure in Table 1, elon-
gated K-means is initialized with q centers corre-
sponding to data points in different clusters and one
center in the origin. The algorithm then will drag the
center in the origin towards one of the clusters not
accounted for. Compute another eigenvector (thus
increasing the dimension of the clustering space to
q + 1) and repeat the procedure. Eventually, when
one reach as many eigenvectors as the number of
clusters present in the data, no points will be as-
signed to the center at the origin, leaving the cluster
empty. This is the signal to terminate the algorithm.
2.5 An example
Figure 1 visualized the clustering result of three cir-
cle dataset using K-means and Spectral-based clus-
tering. From Figure 1(b), we can see that K-means
can not separate the non-convex clusters in three cir-
cle dataset successfully since it is prone to local min-
imal. For spectral-based clustering, as the algorithm
described, initially, we took the two eigenvectors of
L with largest eigenvalues, which gave us a two-
dimensional clustering space. Then to ensure that
the two centers are initialized in different clusters,
one center is set as the point that is the farthest from
the origin, while the other is set as the point that
simultaneously farthest the first center and the ori-
gin. Figure 1(c) shows the three elongated clusters in
the 2D clustering space and the corresponding clus-
tering result of dataset is visualized in Figure 1(d),
which exploits manifold structure (cluster structure)
in data.
3 Experiments and Results
3.1 Data Setting
Our proposed unsupervised relation extraction is
evaluated on ACE corpus, which contains 519 files
from sources including broadcast, newswire, and
newspaper. We only deal with intra-sentence ex-
plicit relations and assumed that all entities have
2 In this paper, the sharpness parameter ? is set to 0.2
571
Table 2: Frequency of Major Relation SubTypes in the ACE
training and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
been detected beforehand in the EDT sub-task of
ACE. To verify our proposed method, we only col-
lect those pairs of entity mentions which have been
tagged relation types in the given corpus. Then the
relation type tags were removed to test the unsuper-
vised relation disambiguation. During the evalua-
tion procedure, the relation type tags were used as
ground truth classes. A break-down of the data by
24 relation subtypes is given in Table 2.
3.2 Evaluation method for clustering result
When assessing the agreement between clustering
result and manually annotated relation types (ground
truth classes), we would encounter the problem that
there was no relation type tags for each cluster in our
clustering results.
To resolve the problem, we construct a contin-
gency table T , where each entry ti,j gives the num-
ber of the instances that belong to both the i-th es-
timated cluster and j-th ground truth class. More-
over, to ensure that any two clusters do not share
the same labels of relation types, we adopt a per-
mutation procedure to find an one-to-one mapping
function ? from the ground truth classes (relation
types) TC to the estimated clustering result EC.
There are at most |TC| clusters which are assigned
relation type tags. And if the number of the esti-
mated clusters is less than the number of the ground
truth clusters, empty clusters should be added so that
|EC| = |TC| and the one-to-one mapping can be
performed, which can be formulated as the function:
?? = argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the in-
dex of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we adopt
Precision, Recall and F-measure to evaluate the
clustering result.
3.3 Experimental Design
We perform our unsupervised relation extraction on
the devtest set of ACE corpus and evaluate the al-
gorithm on relation subtype level. Firstly, we ob-
serve the influence of various variables, including
Distance Parameter ?2, Different Features, Context
Window Size. Secondly, to verify the effectiveness
of our method, we further compare it with super-
vised method based on SVM and other two unsuper-
vised methods.
3.3.1 Choice of Distance Parameter ?2
We simply search over ?2 and pick the value
that finds the best aligned set of clusters on the
transformed space. Here, the scattering criterion
trace(P?1W PB) is used to compare the cluster qual-
ity for different value of ?2 3, which measures the ra-
tio of between-cluster to within-cluster scatter. The
higher the trace(P?1W PB), the higher the cluster
quality.
In Table 3 and Table 4, with different settings of
feature set and context window size, we find out the
corresponding value of ?2 and cluster number which
maximize the trace value in searching for a range of
value ?2.
3.3.2 Contribution of Different Features
As the previous section presented, we incorporate
various lexical and syntactic features to extract rela-
3 trace(P?1W PB) is trace of a matrix which is the sum of
its diagonal elements. PW is the within-cluster scatter matrix
as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xi ? mj)
t and PB
is the between-cluster scatter matrix as: PB =
?c
j=1(mj ?
m)(mj ? m)t, where m is the total mean vector and mj is
the mean vector for jth cluster and (Xj ? mj)t is the matrix
transpose of the column vector (Xj ?mj).
572
Table 3: Contribution of Different Features
Features ?2 cluster number trace value Precison Recall F-measure
Words 0.021 15 2.369 41.6% 30.2% 34.9%
+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%
+POS 0.017 18 3.206 37.8% 46.9% 41.8%
+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%
Table 4: Different Context Window Size Setting
Context Window Size ?2 cluster number trace value Precision Recall F-measure
0 0.016 18 3.576 37.6% 48.1% 42.2%
2 0.015 19 3.900 43.5% 49.4% 46.3%
5 0.020 21 2.225 29.3% 34.7% 31.7%
tion. To measure the contribution of different fea-
tures, we report the performance by gradually in-
creasing the feature set, as Table 3 shows.
Table 3 shows that all of the four categories of fea-
tures contribute to the improvement of performance
more or less. Firstly,the addition of entity type fea-
ture is very useful, which improves F-measure by
6.6%. Secondly, adding POS features can increase
F-measure score but do not improve very much.
Thirdly, chunking features also show their great use-
fulness with increasing Precision/Recall/F-measure
by 5.7%/2.5%/4.5%.
We combine all these features to do all other eval-
uations in our experiments.
3.3.3 Setting of Context Window Size
We have mentioned in Section 2 that the context
vectors of entity pairs are derived from the contexts
before, between and after the entity mention pairs.
Hence, we have to specify the three context window
size first. In this paper, we set the mid-context win-
dow as everything between the two entity mentions.
For the pre- and post- context windows, we could
have different choices. For example, if we specify
the outer context window size as 2, then it means that
the pre-context (post-context)) includes two words
before (after) the first (second) entity.
For comparison of the effect of the outer context
of entity mention pairs, we conducted three different
settings of context window size (0, 2, 5) as Table 4
shows. From this table we can find that with the con-
text window size setting, 2, the algorithm achieves
the best performance of 43.5%/49.4%/46.3% in
Precision/Recall/F-measure. With the context win-
dow size setting, 5, the performance becomes worse
Table 5: Performance of our proposed method (Spectral-
based clustering) compared with supervised method (SVM) and
unsupervised methods((Hasegawa et al, 2004))?s method and
K-means clustering.
Precision Recall F-measure
SVM 61.2% 49.6% 54.8%
Hasegawa?s Method1 38.7% 29.8% 33.7%
Hasegawa?s Method2 37.9% 36.0% 36.9%
Kmeans 34.3% 40.2% 36.8%
Our Proposed Method 43.5% 49.4% 46.3%
because extending the context too much may include
more features, but at the same time, the noise also
increases.
3.3.4 Comparison with Supervised methods
and other Unsupervised methods
To explore the effectiveness of our unsupervised
method compared to supervised method, we perform
SVM technique with the same feature set defined in
our proposed method. The LIBSVM tool is used in
this test 4. The kernel function we used is linear
and SVM models are trained using the training set
of ACE corpus.
In (Hasegawa et al, 2004), they preformed un-
supervised relation extraction based on hierarchical
clustering and they only used word features between
entity mention pairs to construct context vectors. We
reported the clustering results using the same clus-
tering strategy as Hasegawa et al (2004) proposed.
In Table 5, Hasegawa?s Method1 means the test used
the word feature as Hasegawa et al (2004) while
Hasegawa?s Method2 means the test used the same
feature set as our method. In both tests, we specified
4 LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/ cjlin/libsvm. It
supports multi-class classification.
573
Table 6: Comparison of the existing efforts on ACE RDC task.
Relation Dectection Relation Classification
on Types on Subtypes
Method P R F P R F P R F
Culotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -
Kambhatla (2004) Feature based, Maxi-
mum Entropy
- - - - - - 63.5 45.2 52.8
Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5
the cluster number as the number of ground truth
classes.
We also approached the relation extraction prob-
lem using the standard clustering technique, K-
means, where we adopted the same feature set de-
fined in our proposed method to cluster the con-
text vectors of entity mention pairs and pre-specified
the cluster number as the number of ground truth
classes.
Table 5 reports the performance of our pro-
posed method comparing with SVM-based super-
vised method and the other two unsupervised meth-
ods. As the result shows, SVM-based method by us-
ing the same feature set in our proposed method can
achieve 61.2%/49.6%/54.8% in Precision/Recall/F-
measure. Table 5 also shows our proposed spec-
tral based method clearly outperforms the other
two unsupervised methods by 12.5% and 9.5% in
F-measure respectively. Moreover, the incorpora-
tion of various lexical and syntactic features into
Hasegawa et al (2004)?s method2 makes it outper-
form Hasegawa et al (2004)?s method1 which only
uses word feature.
3.4 Discussion
In this paper, we have shown that the modified spec-
tral clustering technique, with various lexical and
syntactic features derived from the context of en-
tity pairs, performed well on the unsupervised re-
lation disambiguation problem. Our experiments
show that by the choice of the distance parameter
?2, we can estimate the cluster number which pro-
vides the tightest clusters. We notice that the es-
timated cluster number is less than the number of
ground truth classes in most cases. The reason for
this phenomenon may be that some relation types
can not be easily distinguished using the context in-
formation only. For example, the relation subtypes
?Located?, ?Based-In? and ?Residence? are difficult
to disambiguate even for human experts to differen-
tiate.
The results also show that various lexical and
syntactic features contain useful information for the
task. Especially, although we did not concern the
dependency tree and full parse tree information as
other supervised methods (Miller et al, 2000; Cu-
lotta and Soresen, 2004; Kambhatla, 2004; Zhou et
al., 2005), the incorporation of simple features, such
as words and chunking information, still can provide
complement information for capturing the charac-
teristics of entity pairs. Another observation from
the result is that extending the outer context window
of entity mention pairs too much may not improve
the performance since the process may incorporate
more noise information and affect the clustering re-
sult.
As regards the clustering technique, the spectral-
based clustering performs better than direct cluster-
ing, K-means. Since the spectral-based algorithm
works in a transformed space of low dimension-
ality, data can be easily clustered so that the al-
gorithm can be implemented with better efficiency
and speed. And the performance using spectral-
based clustering can be improved due to the reason
that spectral-based clustering overcomes the draw-
back of K-means (prone to local minima) and may
find non-convex clusters consistent with human in-
tuition.
Currently most of works on the RDC task of ACE
focused on supervised learning methods. Table 6
lists a comparison of these methods on relation de-
tection and relation classification. Zhou et al (2005)
reported the best result as 63.1%/49.5%/55.5% in
Precision/Recall/F-measure on the extraction of
ACE relation subtypes using feature based method,
which outperforms tree kernel based method by
Culotta and Soresen (2004). Although our unsu-
pervised method still can not outperform these su-
574
pervised methods, from the point of view of un-
supervised resolution for relation extraction, our
approach already achieves best performance of
43.5%/49.4%/46.3% in Precision/Recall/F-measure
compared with other clustering methods.
4 Conclusion and Future work
In this paper, we approach unsupervised relation dis-
ambiguation problem by using spectral-based clus-
tering technique with diverse lexical and syntactic
features derived from context. The advantage of our
method is that it doesn?t need any manually labeled
relation instances, and pre-definition the number of
the context clusters. Experiment results on the ACE
corpus show that our method achieves better perfor-
mance than other unsupervised methods.
Currently we combine various lexical and syn-
tactic features to construct context vectors for clus-
tering. In the future we will further explore other
semantic information to assist the relation extrac-
tion problem. Moreover, instead of cosine similar-
ity measure to calculate the distance between con-
text vectors, we will try other distributional similar-
ity measures to see whether the performance of re-
lation extraction can be improved. In addition, if we
can find an effective unsupervised way to filter out
unrelated entity pairs in advance, it would make our
proposed method more practical.
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proc. of the 5th ACM International Conference on
Digital Libraries (ACMDL?00).
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proc. of WebDB Workshop at
6th International Conference on Extending Database
Technology (WebDB?98). pages 172-183.
Charniak E.. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12.. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Defense Advanced Research Projects Agency. 1995.
Proceedings of the Sixth Message Understanding Con-
ference (MUC-6) Morgan Kaufmann Publishers, Inc.
Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.
2004. Discovering Relations among Named Enti-
ties from Large Corpora, Proceeding of Conference
ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Kannan R., Vempala S., and Vetta A.. 2000. On cluster-
ing: Good,bad and spectral. In Proceedings of the 41st
Foundations of Computer Science. pages 367-380.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In proceedings of 6th Applied Natural Lan-
guage Processing Conference. 29 April-4 may 2000,
Seattle USA.
Ng Andrew.Y, Jordan M., and Weiss Y.. 2001. On spec-
tral clustering: Analysis and an algorithm. In Pro-
ceedings of Advances in Neural Information Process-
ing Systems. pages 849-856.
Sanguinetti G., Laidler J. and Lawrence N.. 2005. Au-
tomatic determination of the number of clusters us-
ing spectral algorithms.In: IEEE Machine Learning
for Signal Processing. 28-30 Sept 2005, Mystic, Con-
necticut, USA.
Shi J. and Malik.J. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence. 22(8):888-905.
Weiss Yair. 1999. Segmentation using eigenvectors: A
unifying view. ICCV(2). pp.975-982.
Zelenko D., Aone C. and Richardella A.. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001. Spec-
tral Relaxation for k-means clustering. In Neural In-
formation Processing Systems (NIPS2001). pages
1057-1064, 2001.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction, In proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
575
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 177?182,
Prague, June 2007. c?2007 Association for Computational Linguistics
I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense
Disambiguation, and English Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
niu zy@hotmail.com
dhji@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This paper describes the implementation
of our three systems at SemEval-2007, for
task 2 (word sense discrimination), task 5
(Chinese word sense disambiguation), and
the first subtask in task 17 (English word
sense disambiguation). For task 2, we ap-
plied a cluster validation method to esti-
mate the number of senses of a target word
in untagged data, and then grouped the in-
stances of this target word into the esti-
mated number of clusters. For both task 5
and task 17, We used the label propagation
algorithm as the classifier for sense disam-
biguation. Our system at task 2 achieved
63.9% F-score under unsupervised evalua-
tion, and 71.9% supervised recall with su-
pervised evaluation. For task 5, our sys-
tem obtained 71.2% micro-average preci-
sion and 74.7% macro-average precision.
For the lexical sample subtask for task
17, our system achieved 86.4% coarse-
grained precision and recall.
1 Introduction
SemEval-2007 launches totally 18 tasks for evalua-
tion exercise, covering word sense disambiguation,
word sense discrimination, semantic role labeling,
and sense disambiguation for information retrieval,
and other topics in NLP. We participated three tasks
in SemEval-2007, which are task 2 (Evaluating
Word Sense Induction and Discrimination Systems),
task 5 (Multilingual Chinese-English Lexical Sam-
ple Task) and the first subtask at task 17 (English
Lexical Sample, English Semantic Role Labeling
and English All-Words Tasks).
The goal for SemEval-2007 task 2 (Evaluat-
ing Word Sense Induction and Discrimination Sys-
tems)(Agirre and Soroa, 2007) is to automatically
discriminate the senses of English target words by
the use of only untagged data. Here we address this
word sense discrimination problem by (1) estimat-
ing the number of word senses of a target word in
untagged data using a stability criterion, and then (2)
grouping the instances of this target word into the
estimated number of clusters according to the simi-
larity of contexts of the instances. No sense-tagged
data is used to help the clustering process.
The goal of task 5 (Chinese Word Sense Disam-
biguation) is to create a framework for the evaluation
of word sense disambiguation in Chinese-English
machine translation systems. Each participates of
this task will be provided with sense tagged train-
ing data and untagged test data for 40 Chinese pol-
ysemous words. The ?sense tags? for the ambigu-
ous Chinese target words are given in the form of
their English translations. Here we used a semi-
supervised classification algorithm (label propaga-
tion algorithm) (Niu, et al, 2005) to address this
Chinese word sense disambiguation problem.
The lexical sample subtask of task 17 (English
Word Sense Disambiguation) provides sense-tagged
training data and untagged test data for 35 nouns and
65 verbs. This data includes, for each target word:
OntoNotes sense tags (these are groupings of Word-
Net senses that are more coarse-grained than tradi-
177
tional WN entries), as well as the sense inventory for
these lemmas. Here we used only the training data
supplied in this subtask for sense disambiguation in
test set. The label propagation algorithm (Niu, et al,
2005) was used to perform sense disambiguation by
the use of both training data and test data.
This paper will be organized as follows. First, we
will provide the feature set used for task 2, task 5
and task 17 in section 2. Secondly, we will present
the word sense discrimination method used for task
2 in section 3. Then, we will give the label propa-
gation algorithm for task 5 and task 17 in section 4.
Section 5 will provide the description of data sets at
task 2, task 5 and task 17. Then, we will present the
experimental results of our systems at the three tasks
in section 6. Finally we will give a conclusion of our
work in section 7.
2 Feature Set
In task 2, task 5 and task 17, we used three types of
features to capture contextual information: part-of-
speech of neighboring words (no more than three-
word distance) with position information, unordered
single words in topical context (all the contextual
sentences), and local collocations (including 11 col-
locations). The feature set used here is as same as
the feature set used in (Lee and Ng, 2002) except
that we did not use syntactic relations.
3 The Word Sense Discrimination Method
for Task 2
Word sense discrimination is to automatically dis-
criminate the senses of target words by the use of
only untagged data. So we can employ clustering
algorithms to address this problem. Another prob-
lem is that there is no sense inventories for target
words. So the clustering algorithms should have the
ability to automatically estimate the sense number
of a target word.
Here we used the sequential Information Bottle-
neck algorithm (sIB) (Slonim, et al, 2002) to esti-
mate cluster structure, which measures the similarity
of contexts of instances of target words according to
the similarity of their contextual feature conditional
distribution. But sIB requires the number of clus-
ters as input. So we used a cluster validation method
to automatically estimate the sense number of a tar-
Table 1: Sense number estimation procedure for
word sense discrimination.
1 Set lower bound Kmin and upper bound Kmax
for sense number k;
2 Set k = Kmin;
3 Conduct the cluster validation process
presented in Table 2 to evaluate the merit of k;
4 Record k and the value of Mk;
5 Set k = k + 1. If k ? Kmax, go to step 3,
otherwise go to step 6;
6 Choose the value k? that maximizes Mk,
where k? is the estimated sense number.
get word before clustering analysis. Cluster valida-
tion (or stability based approach)is a commonly used
method to the problem of model order identification
(or cluster number estimation) (Lange, et al, 2002;
Levine and Domany, 2001). The assumption of this
method is that if the model order is identical with the
true value, then the cluster structure estimated from
the data is stable against resampling, otherwise, it is
more likely to be the artifact of sampled data.
3.1 The Sense Number Estimation Procedure
Table 1 presents the sense number estimation pro-
cedure. Kmin was set as 2, and Kmax was set as 5 in
our system. The evaluation function Mk (described
in Table 2) is relevant with the sense number k. q
is set as 20 here. Clustering solution which is stable
against resampling will give rise to a local optimum
of Mk, which indicates the true value of sense num-
ber. In the cluster validation procedure, we used the
sIB algorithm to perform clustering analysis (de-
scribed in section 3.2).
The function M(C?, C) in Table 2 is given by
(Levine and Domany, 2001):
M(C?, C) =
?
i,j 1{C
?
i,j = Ci,j = 1, di ? D?, dj ? D?}?
i,j 1{Ci,j = 1, di ? D?, dj ? D?}
,
(1)
where D? is a subset with size ?|D| sampled from
full data set D, C and C? are |D|? |D| connectivity
matrixes based on clustering solutions computed on
D and D? respectively, and 0 ? ? ? 1. The con-
nectivity matrix C is defined as: Ci,j = 1 if di and
dj belong to the same cluster, otherwise Ci,j = 0.
C? is calculated in the same way. ? is set as 0.90 in
this paper.
178
Table 2: The cluster validation method for evalua-
tion of values of sense number k.
Function: Cluster Validation(k, D, q)
Input: cluster number k, data set D,
and sampling frequency q;
Output: the score of the merit of k;
1 Perform clustering analysis using sIB on
data set D with k as input;
2 Construct connectivity matrix Ck based on
above clustering solution on D;
3 Use a random predictor ?k to assign
uniformly drawn labels to instances in D;
4 Construct connectivity matrix C?k
using above clustering solution on D;
5 For ? = 1 to q do
5.1 Randomly sample a subset (D?) with size
?|D| from D, 0 ? ? ? 1;
5.2 Perform clustering analysis using sIB on
(D?) with k as input;
5.3 Construct connectivity matrix C?k using
above clustering solution on (D?);
5.4 Use ?k to assign uniformly drawn labels
to instances in (D?);
5.5 Construct connectivity matrix C??kusing above clustering solution on (D?);
Endfor
6 Evaluate the merit of k using following
objective function:
Mk = 1q
?
? M(C?k , Ck) ? 1q
?
? M(C??k , C?k),
where M(C?, C) is given by equation (1);
7 Return Mk;
M(C?, C) measures the proportion of document
pairs in each cluster computed on D that are also as-
signed into the same cluster by clustering solution
on D?. Clearly, 0 ? M ? 1. Intuitively, if clus-
ter number k is identical with the true value, then
clustering results on different subsets generated by
sampling should be similar with that on full data set,
which gives rise to a local optimum of M(C?, C).
In our algorithm, we normalize M(C?F,k, CF,k)using the equation in step 6 of Table 2, which
makes our objective function different from the fig-
ure of merit (equation ( 1)) proposed in (Levine
and Domany, 2001). The reason to normalize
M(C?F,k, CF,k) is that M(C?F,k, CF,k) tends to de-
crease when increasing the value of k. Therefore for
avoiding the bias that smaller value of k is to be se-
lected as cluster number, we use the cluster validity
of a random predictor to normalize M(C?F,k, CF,k).
3.2 The sIB Clustering Algorithm
Here we used the sIB algorithm (Slonim, et al,
2002) to estimate cluster structure, which measures
the similarity of contexts of instances according to
the similarity of their feature conditional distribu-
tion. sIB is a simplified ?hard? variant of informa-
tion bottleneck method (Tishby, et al, 1999).
Let d represent a document, and w represent a fea-
ture word, d ? D, w ? F . Given the joint distri-
bution p(d,w), the document clustering problem is
formulated as looking for a compact representation
T for D, which preserves as much information as
possible about F . T is the document clustering so-
lution. For solving this optimization problem, sIB
algorithm was proposed in (Slonim, et al, 2002),
which found a local maximum of I(T, F ) by: given
an initial partition T , iteratively drawing a d ? D
out of its cluster t(d), t ? T , and merging it into
tnew such that tnew = argmaxt?Td(d, t). d(d, t) is
the change of I(T, F ) due to merging d into cluster
tnew, which is given by
d(d, t) = (p(d) + p(t))JS(p(w|d), p(w|t)). (2)
JS(p, q) is the Jensen-Shannon divergence, which
is defined as
JS(p, q) = pipDKL(p?p) + piqDKL(q?p), (3)
DKL(p?p) =
?
y
plog pp, (4)
DKL(q?p) =
?
y
qlog qp, (5)
{p, q} ? {p(w|d), p(w|t)}, (6)
{pip, piq} ? {
p(d)
p(d) + p(t) ,
p(t)
p(d) + p(t)}, (7)
p = pipp(w|d) + piqp(w|t). (8)
179
4 The Label Propagation Algorithm for
Task 5 and Task 17
In the label propagation algorithm (LP) (Zhu and
Ghahramani, 2002), label information of any ver-
tex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage
is achieved. Larger edge weights allow labels to
travel through easier. Thus the closer the examples,
more likely they have similar labels (the global con-
sistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 ? Nn?c represent initial soft labels at-
tached to vertices, where Y 0ij = 1 if yi is sj and 0
otherwise. Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent with the
labeling in labeled data, and the initialization of Y 0U
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set ? as the aver-
age distance between labeled examples from differ-
ent classes.
Define n ? n probability transition matrix Tij =
P (j ? i) = Wij?n
k=1 Wkj
, where Tij is the probability
to jump from example xj to example xi.
Compute the row-normalized matrix T by T ij =
Tij/
?n
k=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
Then LP algorithm is defined as follows:
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Y t+1 = TY t;
3. Clamp labeled data by replacing the top l row
of Y t+1 with Y 0L . Repeat from step 2 until Y t con-
verges;
4. Assign xh(l + 1 ? h ? n) with a label sj? ,
where j? = argmaxjYhj .
This algorithm has been shown to converge to
a unique solution, which is Y?U = limt?? Y tU =
(I ? T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of Y 0U is not im-
portant, since Y 0U does not affect the estimation of
Y?U . I is u ? u identity matrix. T uu and T ul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
For task 5 and 17, we constructed connected
graphs as follows: two instances u, v will be con-
nected by an edge if u is among v?s k nearest neigh-
bors, or if v is among u?s k nearest neighbors as mea-
sured by cosine or JS distance measure. k is set 10
in our system implementation.
5 Data Sets of Task 2, Task 5 and Task 17
The test data for task 2 includes totally 27132 un-
tagged instances for 100 ambiguous English words.
There is no training data for task 2.
There are 40 ambiguous Chinese words in task
5. The training data for this task consists of 2686
instances, while the test data includes 935 instances.
There are 100 ambiguous English words in the
first subtask of task 17. The training data for this
task consists of 22281 instances, while the test data
includes 4851 instances.
6 Experimental Results of Our Systems at
Task 2, Task 5 and Task 17
Table 3: The best/worst/average F-score of all the
systems at task 2 and the F-score of our system at
task 2 for all target words, nouns and verbs with un-
supervised evaluation.
All words Nouns Verbs
Best 78.7% 80.8% 76.3%
Worst 56.1% 65.8% 45.1%
Average 65.4% 69.0% 61.4%
Our system 63.9% 68.0% 59.3%
Table 3 lists the best/worst/average F-score of all
the systems at task 2 and the F-score of our system
at task 2 for all target words, nouns and verbs with
180
Table 4: The best/worst/average supervised recall of
all the systems at task 2 and the supervised recall of
our system at task 2 for all target words, nouns and
verbs with supervised evaluation.
All words Nouns Verbs
Best 81.6% 86.8% 75.7%
Worst 78.5% 81.4% 75.2%
Average 79.6% 83.0% 75.7%
Our system 81.6% 86.8% 75.7%
Table 5: The best/worst/average micro-average pre-
cision and macro-average precision of all the sys-
tems at task 5 and the micro-average precision and
macro-average precision of our system at task 5.
Micro-average Macro-average
Best 71.7% 74.9%
Worst 33.7% 39.6%
Average 58.5% 62.7%
Our system 71.2% 74.7%
unsupervised evaluation. Our system obtained the
fourth place among six systems with unsupervised
evaluation. Table 4 shows the best/worst/average
supervised recall of all the systems at task 2 and the
supervised recall of our system at task 2 for all tar-
get words, nouns and verbs with supervised evalu-
ation. Our system is ranked as the first among six
systems with supervised evaluation. Table 7 lists
the estimated sense numbers by our system for all
the words at task 2. The average of all the estimated
sense numbers is 3.1, while the average of all the
ground-truth sense numbers is 3.6 if we consider the
sense inventories provided in task 17 as the answer.
It seems that our estimated sense numbers are close
to the ground-truth ones.
Table 5 provides the best/worst/average micro-
average precision and macro-average precision of all
the systems at task 5 and the micro-average preci-
sion and macro-average precision of our system at
task 5. Our system obtained the second place among
six systems for task 5.
Table 6 shows the best/worst/average coarse-
grained score (precision) of all the systems the lexi-
cal sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sample
Table 6: The best/worst/average coarse-grained
score (precision) of all the systems at the lexical
sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sam-
ple subtask of task 17.
Coarse-grained score (precision)
Best 88.7%
Worst 52.1%
Average 70.0%
Our system 86.4%
subtask of task 17. The attempted rate of all the sys-
tems is 100%. So the precision value is equal to the
recall value for all the systems. Here we listed only
the precision for the 13 systems at this subtask. Our
system is ranked as the third one among 13 systems.
7 Conclusion
In this paper, we described the implementation of
our I2R systems that participated in task 2, task 5,
and task 17 at SemEval-2007. Our systems achieved
63.9% F-score and 81.6% supervised recall for task
2, 71.2% micro-average precision and 74.7% macro-
average precision for task 5, and 86.4% coarse-
grained precision and recall for the lexical sample
subtask of task 17. The performance of our system
is very good under supervised evaluation. It may
be explained by that our system has the ability to
find some minor senses so that it can outperforms
the baseline system that always uses the most fre-
quent sense as the answer.
References
Agirre E. , & Soroa A. 2007. SemEval-2007 Task 2:
Evaluating Word Sense Induction and Discrimination
Systems. Proceedings of SemEval-2007, Association
for Computational Linguistics.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M. 2002.
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Lee, Y.K., & Ng, H.T. 2002. An Empirical Evalua-
tion of Knowledge Sources and Learning Algorithms
for Word Sense Disambiguation. Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, (pp. 41-48).
181
Levine, E., & Domany, E. 2001. Resampling Method for
Unsupervised Estimation of Cluster Validity. Neural
Computation, Vol. 13, 2573?2593.
Niu, Z.Y., Ji, D.H., & Tan, C.L. 2005. Word Sense
Disambiguation Using Label Propagation Based Semi-
Supervised Learning. Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics.
Slonim, N., Friedman, N., & Tishby, N. 2002. Un-
supervised Document Classification Using Sequential
Information Maximization. Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Tishby, N., Pereira, F., & Bialek, W. (1999) The Infor-
mation Bottleneck Method. Proc. of the 37th Allerton
Conference on Communication, Control and Comput-
ing.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Table 7: The estimated sense numbers by our system
for all the words at task 2.
explain 2 move 3
position 3 express 4
buy 2 begin 2
hope 3 prepare 3
feel 5 policy 2
hold 2 attempt 2
work 5 recall 3
people 4 find 2
system 2 join 2
bill 2 build 2
hour 5 base 3
value 4 management 2
job 5 turn 4
rush 2 kill 2
ask 2 area 5
approve 4 affect 4
capital 4 keep 5
purchase 2 improve 2
propose 2 do 2
see 3 drug 5
president 3 come 5
power 3 disclose 4
effect 2 avoid 3
part 5 plant 2
exchange 4 share 2
state 2 carrier 2
care 5 complete 2
promise 3 maintain 3
estimate 2 development 4
rate 2 space 5
say 2 raise 3
remove 5 future 3
grant 4 network 3
remember 3 announce 5
cause 2 start 3
point 5 order 2
occur 4 defense 5
authority 3 set 3
regard 2 chance 2
go 3 produce 2
allow 4 negotiate 2
describe 2 enjoy 4
prove 3 exist 4
claim 4 replace 3
fix 2 examine 3
end 5 lead 3
receive 3 source 2
complain 3 report 2
need 2 believe 2
condition 2 contribute 3
182
Semantic Annotation of Chinese Phrases Using Recursive-Graph 
Ji Donghong 
Kent Ridge Digital Labs, 
Singapore, 119613 
dhj i@krdl.org.sg 
Abstract 
In this paper, we propose a recursive graph 
based scheme for semantic annotation of 
Chinese phrases. Compared with others, this 
scheme can fully differentiate hose Chinese 
phrases that comprise the same content 
words but hold different meanings due to 
their different word order or some involved 
function words, and capture the hierarchical 
conceptual structure of Chinese phrases, 
which underlies their main semantic 
information. We also give the guidelines for 
annotating various commonly used types of 
Chinese phrases. 
1 Introduction 
Semantically annotated linguistic data are 
important resources for natural language 
processing, and have been used in many 
NLP areas, e.g., parsing, word sense 
disambiguation, co-reference r solution and 
information extraction, etc. But due to huge 
efforts needed in building them and general 
difficulties in dealing with semantic 
information, such resources are scarcely seen 
for most languages including Chinese, if not 
all. 
In this paper, we focus on Chinese 
phrases and present a recursive graph based 
scheme for their semantic annotation. 
Compared with the same task for sentences, 
it only involves a relatively small data set 
and simple semantic information but has 
potential generality and application. 
One is at which goal the annotation is aimed. 
In our case, the goal is established to be to 
help disclose the correspondence b tween 
linguistic forms and meaning, which is also 
the primary goal in both traditional linguistic 
research (Chomsky, 1968) and natural 
language understanding (Allen, 1995). 
The other problem is whether the 
annotation can significantly contribute to the 
established goal. In our case, the answer is of 
course positive. First, the phrases themselves 
are a specific kind of linguistic forms, thus 
their semantic annotation directly provides 
the correspondence b tween them and their 
meanings. Second, by some kinds of analogy 
rules, these annotated phrases can be 
examples for deriving the correspondence 
between new phrases and their meanings. 
Third, as compared with western languages, 
Chinese language has a specific feature that 
its sentences form in roughly the same way 
as its phrases (Zhu, 1982), which shows that 
by some kinds of combinatory rules, the 
mapping between sentences and their 
meanings can also be determined based on 
the examples. 
The remainder of this paper is organised 
as the following. In section 2, we describe 
the motivation of introducing recursive 
graph. In section 3, we formally define what 
is a recursive graph. In section 4, we specify 
how commonly used types of Chinese 
phrases are annotated using recursive graph. 
In section 5, we give the conclusion and 
discuss ome future work. 
2 Motivation 
As a specific semantic annotation task, 
two problems hould be made clear at first. 
In general, semantic annotation of linguistic 
forms is to associate with them their 
101 
semantic information being represented in 
some formal languages or diagrams. With 
the semantic information involved varying, 
the formal languages or diagrams may be 
different. 
One commonly used diagram for 
semantic annotation of linguistic forms is 
dependent tree, in which the dependence or
control relationship between constituents of
a linguistic form is depicted (Langacker, 
1997). But such trees may be not powerful 
enough to differentiate those Chinese 
phrases that comprise the same content 
words but hold different meanings due to 
their word order or involved function words. 
As an example, consider 1) and 2) 1. 
1) 
/zousi/ /qiche/ 
smuggle car 
2) 
/qiche/ /zousi/ 
car smuggle 
Notice 1) and 2) contain the same content 
words, but hold different word order. 
Regarding their meanings, 1) is an 
ambiguous phrase, corresponding with two 
English translation phrases as 3) and 4). 
3) to smuggle cars 
4) smuggled cars 
The translation phrase for 2) is 5). 
5) the smuggling of cars 
So, there are altogether three meanings held 
by the two phrases. But the two content 
words can only form two dependent trees, 
listed in 6) and 7). 
1 In this paper, whenever listing a Chinese word, we 
always list its Pinyin included within two symbols 7', 
and its English translation. For a Chinese phrase, we 
furthermore list its English translation when 
necessary. 
6) ~ (/qiche/, car) 
t 
~L(/zousi/, smuggle) 
7) ;~ ~ (/zousi/, smuggle) 
t 
(/qich , car) 
Obviously, these two dependent trees cannot 
code the three meanings listed in 3), 4) and 
5). Only if we could see the same word 
;~1~ L(/zousi/, smuggle) semantically 
different in 1) and 2), we could add another 
dependent tree 8) to code 5), with 6) and 7) 
corresponding with 3) and 4) respectively. 
8) j~t~ L'(/zousi/, smuggle) 2 
t 
~ :~i~: (Iqichel, car) 
But this view is quite unintuitive, and will 
lead to contradictory. Consider other two 
phrases 9) and 10). 
9) 
/zousi/ /jituan/ 
smuggle bloc 
smuggling bloc 
10) 
/qiche/ /zousi/ /jituan/ 
car smuggle bloc 
car smuggling bloc 
Intuitively, the word :~l~/L(/zousi/, smuggle) 
in 2) holds the same meaning as the word in 
10), which subsequently is equivalent with 
the same word in 9). On the other hand, there 
is no reason to treat the same word 
~L (/zousi/, smuggle) semantically 
differently in 1) and 9), two typical noun 
2 To differentiate the two words, we add one quotation 
mark. 
102 
phrases. Particularly, they are both followed 
by a typical noun in the two phrases. 
The second problem for dependent tree to 
semantically annotate linguistic forms is that 
due to its tree nature, it cannot represent 
multi-dependence relationship, in which one 
node is controlled by several nodes. For 
example, consider 11). 
11) 
/xihuan/ /ziji/ /de/ /ren/ 
like self of people 
the people who like oneself 
Conventionally, its dependent tree should be 
12). 
12) ~ (/ren/, people) 
gX (/xihuan/, like) 
~ (/ziji/, self) 
But intuitively, there should be some 
dependence r lationship between )k(/ren/, 
people) and fl ~ (/ziji/, self). If we add this 
relationship, it will become agraph. 
2.2 Conceptual Graph 
Conceptual graph is another diagram for 
semantic annotation of linguistic forms, 
which comprises concepts and conceptual 
relationship denoted by linguistic forms 
(Eklund, 1996). Although it is claimed to 
be a directed graph in its original form, it is 
equivalent to an undirected graph in nature, 
with its relationship nodes and their directed 
edges replaced with an undirected edge to 
directly denote the relationship. 
One problem with this diagram for 
semantic annotation is that it cannot code the 
information about head, if any, in a linguistic 
form, which intuitively specifies the main 
information carried by a linguistic form. This 
will lead to severe problems when using the 
graph to represent linguistic forms. For 
example, both phrases 1) and 2) with all 
three meanings 3), 4) and 5) would be 
represented by the same diagram as in 13) 3 . 
13) ~/~ ~:~ 
(/zousi/, smuggle) (/qiche/, car) 
To differentiate the three meanings held 
by the two phrases, we suggest using the 
following two weighted graphs and one 
unweighted graph, i.e., 14), 15) and 16) to 
represent 3), 4) and 5) respectively. 
14) ~$L ~ 
(/zousi/, smuggle) (/qiche/, car) 
15, I I 
(/zousi/, smuggle) (/qiche/, car) 
16) \[ ~t~L ~ 
(/zousi/, smuggle) (/qiche/, car) 
Here we basically use undirected graphs to 
annotate phrases, and introduce a rectangle 
to denote the head of a linguistic phrase, if 
any. Notice that we don't mark a head in 14), 
which means that we don't take the verb 
~L(/zousi/ ,  smuggle) as the head of the 
verb phrase as usual. In general, for most 
verb phrases like 14) in Chinese, they 
correspond with two modifier-center phrases 
like 15) and 16) that comprise the same 
content words but with different meanings. 
For such phrases, we generally use a 
headword to differentiate between the verb 
phrase and the two modifier-center phrases, 
and then use different headword to 
distinguish the two modifier-center phrases. 
Another problem with conceptual 
structure is concerned with its ability to deal 
with hierarchical structures. Although nested 
conceptual structure is introduced to 
3 Unless necessary, we don't list relationship in the 
conceptual graph. 
103 
describe nested belief, one particular kind of 
hierarchical structures (Genevirve, 1998), 
some simple hierarchical structures cannot 
be distinguished or annotated appropriately. 
As an example, consider 17) and 18). 
17) ~ ~ ~L  ~ 
/piaoliang/ /de/ /zousi/ /qiche/ 
beautiful of smuggle car 
beautiful smuggled cars 
18) ~I~L ~. ,  ~ ~ 
/zousi/ /piaoliang/ /de/ /qiche/ 
smuggle beautiful of car 
to smuggle beautiful cars 
Using conceptual graph, we can annotate 
both of them as 19), in which case the two 
phrases cannot be distinguished. 
19) ~t~L~ ~ ~ ~  
If based on undirected graph plus head, 17) 
can be annotated as 20). 
20, 
But there will be no appropriate annotation 
for 18), because on the one hand, 
~( /q iche/ ,car )  should be coded as a head 
due to its relationship with ~g~(/piaoliang/, 
beautiful), on the other hand, it's role as the 
object of the verb ~L(/zousi/ ,  smuggle) 
makes it illegal to be a head. 
To differentiate 17) and 18), we further 
suggest specifying the embedded structures 
in linguistic forms in some way, and use 
circles to denote them. In such opinion, 17) 
can be annotated as 21) and 22) respectively. 
21) ~ 
In this diagram, the smaller rectangle 
denotes the head of the modifier-center 
phrase ~$/L~ (/zousiqiche/, smuggled 
cars), the circle codes the phrase as an 
embedded structure of the whole phrase, 
while bigger rectangle denotes the head of 
the whole phrase. 
22) :xT~ L ~  
In this diagram, the circle denotes the 
embedded structure $~, ;~~( /p iao f iang  
de qiche/, beautiful cars), while the rectangle 
denotes the head of the embedded structure. 
Notice the phrase on the whole is a verb 
phrase, so there is no head coded here. 
3. Recursive Graph 
One major concern for semantic annotation 
of linguistic forms is what semantic 
information will be coded, or more 
generally, what is their semantic 
information, which has long been a quite 
controversial question. From the point of 
view of concepts, linguistic forms including 
phrases emantically refer to concepts, which 
we think generally fall within four 
categories: 
a) preliminary concept. For example, we 
may see the word tl/~(/wu/, thing) 
denotes aprefiminary concept. 
b) compositional concept or situtm'on, 
which consists of some concepts and 
their relation 4. For example, phrase 1) 
with the meaning 3) denotes a 
situation including the concepts the 
two content words, :i~l~L(/zousi/, 
smuggle) and ~( /q iche / ,  car), 
denote respectively, and their 
relationship, ~llJ{:(/shoushi/, patient). 
4 relation is also a specific kind of concopts, e.g., 
~$(/shoushi/, patient) is a relation between 
~L(/zousi/, smuggle) and ~(/qicheJ, car). 
104 
c) subordinate concept or specific 
concept within a situation, for 
example, phrase 1) with the meaning 
4) refers to the concept denoted by 
~( /q iche / ,  car) in the situation 
mentioned in b). 
d) subordinate feature, or specific 
feature of a situation or a concept, 
which generally stands for the 
relationship between a situation or a 
concept and another unknown 
concept. Consider 23). 
23) ~r~i~l~ ~ ~b~ 
/guniang/ /de/ /waimao/ 
girl of appearance 
the appearance of girls 
In this phrase, the word ~t'~ 
(/waimao/, appearance) denotes a 
feature of the concept the word ~i~i~l~ 
(/guniang/, girl) denotes, and the 
value of this feature doesn't occur in 
this phrase. 
Notice that the concepts within one situation 
themselves may be compositional concepts, 
subordinate concepts or subordinate f atures, 
so the concepts that linguistic forms 
including phrases denote generally presents a 
kind of hierarchical structure. In fact, this 
hierarchical structure in return represents he 
main semantic information of linguistic 
forms. 
We introduce recursive graph as formal 
diagram to represent the hierarchical 
structure of linguistic forms. Let Po be the 
set of preliminary points, we call pc Po a O- 
level graph. Suppose Pi~-Po, Ei(~-Po) is the 
set of the edges between points in P1, 
Ri(~(PlX El)) is the set of relations between 
points in PI and edges in Et s, then: 
s Here, Edges are also points. An edge point 
connecting two other points here equals an edge in a 
traditional definition of graph. 
i) <P1, El, Ri> is a I-level 
compositional graph; 
ii) <<Pi, El, R~>, p> (peP9 is a 1- 
level point-headed graph; 
iii) <<P1, El, R/>, e> (e~E1) is a 1- 
level edge-headed graph; 
iv) 1-level concepts comprise 1-1evel 
compositional graphs, l-level 
point-headed graphs, and 1-level 
edge-headed graphs. 
Let ,_o=Po, Zn-1 be the set of (n-1)-leveI 
graphs, suppose Pn~(,.ouZiu .... ,Za4), 
(PnnXn.1)?NIL, En(C-Pn) is the set of the 
edges between points in P~, Rn(C(P=x En)) is 
the set of relations between points in P= and 
edges in En, then: 
v) 
vi) 
vii) 
viii) 
<P~, E~, Rn> is a n-level 
compositional graph; 
<<en,  En, Rn>, p> (P~Pn) is a n- 
level point-headed graph; 
<<P~, E~, Rn>, e> (e~En) is a n- 
level edge-headed graph; 
n-level concepts comprise n-level 
compositional graphs, n-level 
point-headed graphs, and n-level 
edge-headed graphs. 
Intuitively, O-level graph corresponds with 
preliminary points, compositional graph 
corresponds with situation, point-headed 
graph corresponds with subordinate concept, 
and edge-headed graph corresponds with 
subordinate f ature. 
4. Annotation Guidelines 
In general, Chinese phrases can roughly be 
classified into five categories, i.e., sub- 
predicate, verb-object, modifier-center, verb- 
complement, and coordinate. We give some 
examples in the following for each category. 
4.1 Sub-predicate 
105 
In general, the phrase :in this category 
denotes a compositional concept. For 
example, 24) can be annotated as 25). 
24) ~t/~ .l~ ~ 
/guniang/ /piaoliang/ 
girl beautiful 
Girls are beautiful 
25) ~ti~i l~ ~ 
Intuitively, the concept the word 
Jti!i~l~(/guniang/, irl) denotes has a feature, 
~t '~ (/waimao/, appearance), and its value 
is ~( /p iao l iang / ,  beautiful). In other 
words, there exists a relationship, i.e., 
~l'~(/waimao/, appearance), between the 
two concepts denoted by ~rdi~l~(/guniang/, 
girl) and ~-~ (/piaoliang/, beautiful) 
respectively 6. 
4.2 Verb-object 
Similar with sub-predicate phrases, the 
phrase in this category also denotes a 
compositional concept. Phrase 1) with the 
meaning 3) is an example, and can be 
annotated as 26). 
26) ~$~ ~ 
Intuitively, there exists a relationship, i.e., 
~l : ( /shoushi / ,  patient), between the two 
concepts denoted by ;~t~L (/zousi/, smuggle) 
and ~ (/qiche/, car) respectively. 
4.3 Modifier-center 
The phrase in this category generally denotes 
a subordinate concept. The center here in the 
phrase can be a verb, a noun or an adjective. 
As examples, 27) and 28) are annotated as 
29) and 30). 
27) 
28) 
/piaoliang/ /de/ /guniang/ 
beautiful of girl 
beautiful girls 
/guniang/ /de/ /piaoliang/ 
girl of beautiful 
the beauty of girls 
29) 
9b~ 
I 
Intuitively, the two concepts denoted by the 
two content words ~t  (/guniang/, girl) and 
~g~ (/piaoliang/, beautiful) in 27) and 28) 
hold the same relationship, i.e., 
?t'~(/waimao/, appearance) as in 24). The 
difference lies in that the phrases 27) and 28) 
both have a head, i.e., ~i~i~l~ (/guniang/, girl) 
and ~-~ (/piaoliang/, beautiful) 
respectively. 
Another example is 23), it denotes an 
subordinate f ature, annotated as 31) 7. 
I 
Intuitively, the concept denoted by 
~t ( /gun iang/ ,  girl) has a feature 
~b~ (/waimao/, appearance), which is the 
head of this phrase. 
4.4 Verb-complement 
For the phrase in this category, the semantic 
relationship between its two parts, i.e., verb 
and complement, is very complicated (Ma, 
1987); even there is no direct semantic links 
6 Unless necessary, we don't list the link between 
concepts and relation ames. 
7 The link between ~i~l~(/guniang/, girl) and 
~b~(/waimao/, appearance) denotes a relation 
between concepts and relationships. 
106 
between them sometimes. Consider 32) and 
33). 
32) 
/chi/ /bao/ 
eat full. 
to eat and be full 
33) ~ 't~ 
/pao/ /kuai/ 
run fast 
to run fast 
In 32), {~(/bao/, full) has a direct link with 
the agent of ~(/chi/, eat), in which sense the 
two concepts denoted by ~\[g(/bao/, full) and 
~(/chi/, eat) has no direct semantic link. We 
don't consider such phrases in our annotation 
temporarily. In contrast, in 33), 'I~(/kuai/, 
fast) acts as the value of one feature 
:~.~(/sudu/, speed) of ~(/pao/, run), in 
which sense it has a direct semantic link with 
~?~(/pao/, run). They form a subordinate 
concept, annotated as 34). 
34) ~-q  '\[~ 
4.5 Coordinate 
In general, the phrase in this category 
denotes a compositional concept. As an 
example, consider 35). 
35) 
/shi/ /sheng/ 
teacher student 
teacher and student 
Intuitively, there is a relationship, i.e., 
9-f (/bing/, and), between the two concepts 
denoted by the two words. So, it can be 
annotated as 36). 
36) ~ 
5 Conclusion and Future 
Work 
In this paper, we propose a recursive graph 
based scheme for semantic annotation of 
Chinese phrases. This scheme can fully 
differentiate those Chinese phrases that 
comprise the same content words but with 
different meaning due to their different word 
order or some involved function words. It 
can also capture the hierarchical conceptual 
structures of Chinese phrases, which 
determine their main semantic information. 
Now, we have annotated about 5,000 
Chinese phrases using this scheme. One 
methodological issue is that we also choose 
multi-character Chinese words as candidate 
annotation phrases. The reason is that unlike 
western languages, there is no clear-cut 
between words and phrases in Chinese, for 
most multi-character Chinese words, their 
components may also be words, and the 
meaning of the multi-character words tends 
to be strongly related with that of their 
component words. In this sense, the words 
can be treated as basic phrases. More 
importantly, such basic annotation examples 
can be more complete in the sense of 
coverage. 
Future work will extend to 50,000 
Chinese phrases. Another future work is to 
learn the rules that dete~ne the mapping 
between linguistic forms and meanings 
based on this annotation, and apply the rules 
to new phrases and sentences. If this can be 
successful, we can develop a semantic 
analyzer for Chinese without any syntactic 
analysis. In particular, we can avoid part-of- 
speeches or syntactic structures that have 
long been difficult notions for Chinese 
languages. 
Acknowledgments 
I would like to thank Prof. Dong Zhendong 
and Prof. Huang Changning for their 
107 
insightful discussions, and the anonymous 
reviewers for their comments. 
References 
J.Allen, Natural 'Language Understanding, Second 
Edition, the Benj arnirgCurnming Publishing 
Company, INC, 1995. 
Chomsky, 1968, Language and Mind, Enlarged 
edition, Harcourt Brace Jovanovich, Publishers. 
Eklnnd, Peter W., Gerard Ellis, & Graham Mann, eds. 
(1996) Conceptual Structures: Knowledge 
Representation as Interlingua, Lecture Notes in 
Artificial Intelligence I 115, Springer-Verlag, Berlin. 
GeneviEve Simonet, 1998, Two FOL Semantics for 
Simple and Nested Conceptual Graphs, in 
proceedings of international conference on 
Conceptual Graph. 
R. W. Langaeker, 1997, Constituency, dependency, 
and concept grouping, Cognitive Linguistics, 8(1): 1- 
32. 
Ma Xiwen, 1987, Some sentential patterns related 
with verb-result verbs, Zhong Guo Yu Wen (Chinese 
in China), 201 (6): 424-441. (in Chinese) 
Zhu Dexi, Yu Fa Da Wen (in Chinese), Beijing 
University Press, 1982. 
108 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1601?1611, Dublin, Ireland, August 23-29 2014.
 
 Word Sense Induction Using Lexical Chain based Hypergraph Model 
 
Tao Qian1,3, Donghong Ji*1, Mingyao Zhang2, Chong Teng1, and Congling Xia1 
(1) Computer School, Wuhan University, Wuhan, China 
?2?College of Foreign Languages and Literature, Wuhan University, Wuhan, China 
 (3) College of Computer Science and Technology, Hubei University of  
Science and Technology,XianNing, China 
{taoqian, dhji, myzhang, tengchong, clxia}@whu.edu.cn 
 
 
Abstract 
Word Sense Induction is a task of automatically finding word senses from large scale texts. It is general-
ly considered as an unsupervised clustering problem. This paper introduces a hypergraph model in 
which nodes represent instances of contexts where a target word occurs and hyperedges represent high-
er-order semantic relatedness among instances. A lexical chain based method is used for discovering the 
hyperedges, and hypergraph clustering methods are used for finding word senses among the context in-
stances. Experiments show that this model outperforms other methods in supervised evaluation and 
achieves comparable performance with other methods in unsupervised evaluation.  
1 Introduction 
Word sense induction (WSI) aims to automatically find senses of a given target word (Yarowsky, 
1995) from large scale texts. Compared with existing manual word sense resources, WSI techniques 
use clustering algorithms to determine the possible senses for a word. 
Word sense induction is generally considered as an unsupervised clustering problem. The input for 
the clustering algorithm is context instances of a target word, represented by word bags or co-
occurrence vectors, and the output is a grouping of these instances into classes, each corresponding to 
an induced sense.  
Traditional methods in WSI tend to adopt the vector space model, in which the context of each in-
stance of a target word is represented as a vector of features based on frequency statistics and proba-
bility distributions, e.g., first-order or second-order vector (Sch?tze, 1998; Purandare and Pedersen, 
2004; Cruys et al., 2011). These vectors are clustered and the resulting clusters represent the induced 
senses. Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; 
V?ronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope 
and Keller, 2013), which have been recently explored successfully to some extent. Graph-based me-
thods are considering the notion of a co-occurrence graph, assuming a binary relatedness between co-
occurring words. 
One of the key challenges in WSI is learning the higher-order semantic relatedness among multiple 
context instances. Previous approaches (Klapaftis and Manandhar. 2007; Bordag, 2006) for WSI are 
used to construct higher-order relatedness by counting co-occurrence frequency or collocation of muti-
words, regardless of global semantic similarity. 
Lexical chain (Morris and Hirst, 1991) is defined as a sequence of semantically related words in text 
and provides important clues about the text structure and topic. It can be viewed as a global counter-
part of the measures of semantic similarity (Navigli, 2009). For example, Figure 1 gives three context 
instances containing Apple.  
                                                 
* Corresponding author E-mail: dhji@whu.edu.cn. 
 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings foo-
ter are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
1601
  Figure 1. Context instances of Apple 
 
Obviously, four Apples in Figure 1 all refer to the Apple Company. We can directly group three in-
stances by the lexical chain: iPod-iTunes-hardware and software product-Inc. This lexical chain 
represents a higher-order semantic relatedness among the three instances. 
In this paper, we propose a hypergraph model from a global perspective, in which nodes represent 
instances of contexts where a target word occurs and hyperedges denote higher-order semantic rela-
tedness among instances. A lexical chain based method is used for identifying the hyperedges. This 
method for lexical chain extraction is a knowledge-free method based on LDA topic model (Remus 
and Biemann, 2013).  
The remainder of this paper is structured as follows. Section 2 presents an overview of the related 
work. Section 3 describes our model in details. Section 4 provides a quantitative evaluation and com-
parison with other algorithms in the SemEval-2013 word sense induction task. Finally, section 5 draws 
conclusions and lays out some future research directions.  
2 Related Work 
2.1 Word sense induction 
A number of diverse approaches to WSI have been proposed so far. Context features are often 
represented in a variety of forms such as co-occurrence of words within phrases (Pantel and Lin, 2002; 
Dorow and Widdows, 2003), parts of speeches (Purandare and Pedersen, 2004), and grammatical rela-
tions (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, 
such as two words before and after the target word, the sentence or even larger paragraph within which 
contains the target word. 
Most of the work in WSI is the vector space model, such as context-based vector algorithm 
(Sch?tze, 1998; Ide et al., 2001; Van de Cruys et al., 2011), substitute-based vector algorithm (Yatbaz 
et al., 2012; Baskaya et al., 2013). In this model, the context of each instance of a target word is 
represented as a vector of features based on frequency statistics or probability distributions (e.g., first-
order or second-order vector). These vectors are clustered by various algorithms and the resulting clus-
ters represent the induced senses.  
Another family of employed approach is graph-based methods, which have been successfully ap-
plied in the sense induction task with some better results achieved. In this framework words are 
represented as nodes in the graph and vertices are drawn between the target word and its co-
occurrences. The co-occurrences between words can be obtained on the basis of grammatical (Wid-
dows and Dorow, 2002) or collocational relations (V?ronis, 2004). Senses are induced by identifying 
highly dense sub-graphs (hubs) in the co-occurrence graph.  
Klapaftis (2007) uses hypergraph model for WSI, in which co-occurrences of two or more words 
are represented by using weighted hyperedges. This model fully exploits the existence of collocations 
or terms consisting of more than two words. In fact, the method converts the sense induction problem 
to the clustering of the contextual words, and the result relies on local word co-occurrence frequency.  
Our hypergraph model is constructed from a global perspective, where the whole context instance is 
regarded as a node.  
WSI evaluation also is an important issue in WSI tasks. Previous WSI evaluations in SemEval 
(Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding 
the single most salient sense of a target word given its context. However, as shown in Erk and McCar-
thy (2009), multiple senses of the target word may be perceived by readers from different angles and a 
graded notion of sense labeling may be considered as the most appropriate. The SemEval-2013 WSI 
evaluation is designed to explore the possibility of finding all perceived senses of a target word in a 
single context instance. Our model is evaluated and verified on the SemEval-2013 WSI task. 
1602
  
 
2.2 Lexical chain extraction 
The Lexical chain method is an important technique in natural language processing. A lexical chain is 
a sequence of semantic related words in text and provides important clues about the text structure and 
topic. It has formed a theoretically well-founded building block in a lot of applications, such as word 
sense disambiguation (Manabu and Takeo, 1994), malapropism detection and correction (Hirst and St-
Onge, 1998), summarization (Barzilay et al., 1997), topic tracking (Carthy, 2004), text segmentation 
(Stokes et al., 2004), and others.  
There are mainly two approaches for lexical chain extraction. One focuses on the use of knowledge 
resources like WordNet (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background 
information in order to quantify semantic relations between words. A major disadvantage of this strat-
egy is that it relies on the resource, which has a direct impact on the quality of lexical chains. Another 
approach is based on statistical methods (Remus and Biemann, 2013).  In this paper, we follow Remus 
and Biemann (2013) to automatically extract lexical chain by using LDA topic model.  
3  Hypergraph model 
In general, lexical chain based hypergraph model contains the following steps:  
i) Automatically extracting lexical chains based on topic model;  
ii) Constructing hypergraph with lexical chains;  
iii) Inducing word sense by hypergraph clustering. 
3.1 Lexical chain extraction 
The extraction technique of lexical chains is based on LDA topic model. LDA topic model (Blei et al., 
2003) is a probabilistic model of text generation designed for revealing some hidden structure in large 
data collections. The key idea is that each document can be represented as a probability distribution 
over a fixed set of topics where each topic can be represented as a probability distribution over words. 
We use LDA topic model for estimating the semantic closeness of lexical terms, and explore a way of 
utilizing LDA?s topic information in constructing lexical chains automatically. In our model, docu-
ment is replaced with context instance of a target word.  
We adopt the idea of interpreting lexical chains as topics and placing all word tokens that share the 
same topic into the same chain. Lexical chains are usually extracted from the same paragraph or text, 
whose topic distributions are identical. However, in our experiment the context instances of a target 
word for WSI are derived from different articles, whose topic distributions are varied. Therefore both 
lexical and contextual topics are modeled. After training the LDA model, we use the information of 
the per-document topic distribution ?d= p(z|d), the per-topic word distribution?w=p(w|z) and the 
sampling topic of a word zw.  
The key work lies in how to assign a word to a topic in training LDA model. Since single samples 
of topics per word may exhibit a large variance (Riedl and Biemann, 2012), we sample several times 
and use the mode (most frequently assigned) topic ID per word as the topic assignment. 
Algorithm 1. lexical chains extraction algorithm 
Input: training set D of target word, hyper-parameters of LDA model;  semantic threshold ?. 
Output: lexical chain set S 
1  ?,?,Z          LDA (D) 
2  for each topic z 
3     lc =""                 //  lc denotes a lexical chain 
4     for each doc d 
5         for each word w in doc d 
6             if ( zw = z  and p(w,d|z) > ? )  
7                lc.add (w) 
8   S.add (lc)  
9   return S 
1603
   
The extraction algorithm is shown in algorithm 1. In order to improve the quality of identified lexi-
cal chains, a threshold ? is set to filter those invalid words whose generating probability of sampling 
topics in the document is lower than ?.  
( , | ) ( | ) ( | )p w d z p z d p w z ?? >                 (1) The threshold ? is essential for the quality of lexical chains, which directly impacts on the performance 
of the model. Detailed analysis for the threshold ? will be given in section 4.5. 
3.2 Hypergraph creation 
A hypergraph H = (V, E) is a generalization of a graph whose edge can connect more than two vertices. 
Just as graphs represent many kinds of information in mathematical and computer science problems, 
hypergraphs also arise in important practical problems, including circuit layout, boolean satisfiability, 
numerical linear algebra, complex network and article co-citation, etc.  
Figure 2 shows an example of hypergraph creation in our model. We represent each context in-
stance as a vertex and connect those context instances with a lexical chain across them by a hyperedge. 
A hyperedge weight equals to the weight of the corresponding lexical chain, defined as follows: 
( | d ) (w | )
( ) (2)| |
i
i i
w C
p z p z
w e C
?
=                       
?
   
where lexical chain C corresponds to hyperedge e, |C| is the number of words in C, and z is the sam-
pling topic of C. 
3.3 Hypergraph clustering 
For hypergraph clustering, the hypergraph is usually transformed into induced graph. There are two 
transformation strategies: one is vertex expansions (2006; Zhou et al., 2006), i.e., clique expansion or 
star expansion, in which a hyperedge is transformed into a clique; the other is called hyperedge expan-
sion (Pu and Faltings, 2012) based on a network flow technique, in which hyperedges are projected 
back to vertices through the adjacency information between hyperedges and vertices. 
Hypergraph clustering algorithm can be divided into two classes: one is based on minimal norma-
lized cut, and the other is based on maximal density. We use three general hypergraph clustering algo-
rithms to identify the context instance clusters. The three algorithms are simply shown in figure 3 and 
described as the follows. 
1) Normalized Hypergraph Cut (NHC) 
 
1604
  The NHC algorithm (Zhou et al., 2006) is a typical approach based on vertex expansion. The objec-
tive is to obtain a partition in which the connection among the vertices in the same cluster is dense 
while the connection between two clusters is sparse. The main steps include transforming the hyper-
graph into an induced graph first, and then adopting the normalized Laplacian to spectral partitioning. 
2) Hyperedge Expansion Clustering (HEC) 
Some works (e.g., Shashua et al., 2006; Bulo and Pelillo, 2012) have shown that the pairwise affini-
ty relations after the projection to the induced graph would introduce information loss, and working 
directly on the hypergraph could produce better performance.  
The hyperedge expansion works as follows. It constructs a directed graph G = (V, E) that includes 
two vertices e+ and e- for each hyperedge e in the original hypergraph. Note that the vertices in G cor-
respond to the hyperedges, but not the vertices in the original hypergraph. A directed edge is placed 
from e+ to e- with weight w(e) where w is the weighting function in the hypergraph. For every pair of 
overlapping hyperedges e1 and e2, two directed edges 1 2(e ,e )? +  and 2 1(e ,e )? +  are added to G with weights 
w(e2) and w(e1). After hypergraph expansion, it adopts spectral method for clustering.  
3) Schype  
The Schype (Michoel and Nachtergaele, 2012) is a maximal density cluster algorithm. According to 
the generalization of the Perron-Frobenius theorem, there exists a unique, positive vector, called the 
dominant eigenvector, over the set of vertices of the hypergraph, which produces a maximal density 
sub-graph with linear time. The procedure is as follow:  
i) Finding maximal density sub-graph by computing the dominant eigenvector. 
ii) Removing all vertices and hyperedges of the sub-graph from hypergraph. 
iii) Repeating above steps until no vertex in hypergraph occurs.  
This algorithm tends to generate many fine-grained clusters. We follow Tan and Kumar (2006) to 
merge clusters using two measures: cohesion and separation. The cohesion of a cluster Ci is defined as: 
,
#( | , )
(C ) | C |
i ix C y C
i
i
e x y e
cohesion ? ?
?
=
?
                 (3) 
where #( | , )e x y e?  is the number of hyperedges containing nodes x and y in C and |Ci| is the number of 
vertices in Ci. Separation between two clusters Ci, Cj is defined as: 
,
#( | , )
(C ,C ) 1 ( )| C | | C |
i jx C y C
i j
i j
e x y e
separation ? ?
?
= ?
?
?
       (4) 
1605
 We merge cluster pairs with high cohesion and low separation. The intuition is that context in-
stances in such pairs will maintain a relatively high degree of semantic similarity. High cohesion is 
defined as greater than average cohesion of all clusters. Low separation is defined as a reciprocal rela-
tionship between two clusters: if a cluster Ci has the lowest separations to a cluster Cj and Cj has the 
lowest separation to Ci, then the two (high cohesion) clusters are merged. This merging process is ite-
rated until it converges. 
4 Experiment and Evaluation 
4.1 Dataset 
Our WSI evaluation is based on the dataset provided by the SemEval-2013 shared 13th task. Test data 
was drawn from the Open American National Corpus (OANC) (Ide and Suderman, 2004) across a va-
riety of genres and from both the spoken and written portions of the corpus. It consists of 4,806 in-
stances of 50 target words: 20 verbs, 20 nouns and 10 adjectives. Due to the unsupervised nature of the 
task, participants were not provided with sense-labeled training data.  However, WSI systems were 
provided with the ukWac corpus (Baroni et al., 2009) to use in inducing senses. Additionally, we used 
the SemEval-2013 lexical trial data sets as development sets to tune parameters. 
4.2 Implementation details 
The training data is extracted from uKWac corpus. For each target word, we extracted 10K context 
instances, and each instance is a sentence window containing the target word. Additionally, we ran-
domly selected 10K sentences as common auxiliary corpus, including none of the target word. The 
training data are tagged with POS tags and lemmatized with TreeTagger (Schmid, 1994).  Removing 
stop words, nouns are taken as features. Meanwhile, we also removed words that co-occur with the 
target of word less than 50 times over the whole ukWac data.  
The training data in the model contains 20K instances: 10K instances of target word, 10K auxiliary 
instances. Specifically, we used the JGibbLDA1 framework for topic model estimation and inference, 
and examined the following LDA parameters: number of topics K, dirichlet hyperparameters for doc-
ument-topic distribution ? and topic-term distribution ?. We tested combinations in the ranges 
K=1000?1500?2000,  ?=5/K..50/K and ?=0.001..0.1. The highest performance of the WSI system 
was found for K = 2000, ? =0.025, ? = 0.001. Similar to tuning the dirichlet hyperparameters of LDA, 
the best parameter ? in lexical chain extraction is 0.0001 in the ranges ? =0.01.. 0.000001. 
We adopt the three clustering algorithms to cluster hypergraph2. The number of clusters is set as 10 
for NHC and HEC, while Schype algorithm generates the number of the clusters (but requiring the 
edge-vertex ratio to be pre-defined), whose average number of senses is 31.8 after clusters are merged. 
Additionally, For Schype algorithm, we used the default values of parameters, except that the ?min-
clustscore? parameter, a minimal score to output a cluster, being tuned to 0.1. 
The sense inventory acquired from the induction step can be used for disambiguation of individual 
instances. Each sense is represented as a vector, whose element is a word and the value of element is 
co-occurrence frequency with target word in the training set. Each test instance is also represented as a 
vector. The similarity between the instance and the induced sense is computed by using cosine func-
tion. For each test instance, it is compared with each sense separately, and finally the sense is selected 
if the cosine value is greater than a certain threshold ?. In experiment, ? is 0.04 for NHC and HEC, 
and is 0.1 for schype. 
4.3 Evaluation measures 
Evaluation in the SemEval-2013 WSI task can be divided into two categories:  
1. A traditional WSD task for unsupervised WSD and WSI systems, 
2. A clustering comparison setting that evaluates the similarity of the sense inventories for WSI sys-
tems.  
                                                 
1 http://sourceforge.net/projects/jgibblda/ 
2  The Hypergraph Analysis Toolbox (HAT) for NHC and HEC: http://lia.epfl.ch/index.php/research/relational-learning 
and the Schype?s code: http://www.roslin.ed.ac.uk/tom-michoel/software/ 
1606
 In the first evaluation, we adopt a WSD task with three objectives: (1) detecting which senses are 
applicable, (2) ranking senses by their applicability, and (3) measuring agreement in applicability rat-
ings with human annotators. Each objective uses a specific measurement:  
i):  Jaccard Index: given two sets of sense labels for an instance, X and Y, the Jaccard Index is used 
to measure the agreement: X Y
X Y
?
?
. The Jaccard Index is maximized when X and Y use identical labels, 
and is minimized when the sets of sense labels are disjoint. 
ii):  Positionally-weighted Kendall?s ? similarity: for graded sense evaluation, we consider an rank-
ing scoring based on Kumar and Vassilvitskii(2010), which weights the penalty of reordering the low-
er positions less than the penalty of reordering the first ranks. 
iii):  Weighted Normalized Discounted Cumulative Gain (WNDCG): NDCG (Moffat and Zobel, 
2008) normally compares the rankings of two lists. It is extended to weighting the DCG by consider-
ing the relative difference in the two weights.  
Because the induced senses will likely vary in number and nature between systems, the WSD evalu-
ation has to incorporate a sense alignment step, in which it performs by splitting the test in-stances into 
two sets: a mapping set and an evaluation set. The optimal mapping from induced senses to gold-
standard senses is learned from the mapping set, and the sense alignment is used to map the predic-
tions of the WSI system to pre-defined senses for the evaluation set. The particular split we use to cal-
culate WSD effectiveness in this paper is 80%/20% (mapping/test), averaged across 5 random splits. 
In the clustering evaluation, similarity between participant?s clusters and the gold standard clusters 
is measured by way of two metrics. 
i): Fuzzy Normalised Mutual information (NMI): it extends the method of (Lancichinetti et al., 2009) 
to compute NMI between overlapping clusters. Fuzzy NMI captures the alignment of the two clusters 
independent of the cluster sizes and therefore serves as an effective measure of the ability of an ap-
proach to accurately model rare senses. 
ii): Fuzzy B-Cubed: it adapts the overlapping B-Cubed measured defined in Amigo et al. (2009) to 
the fuzzy clustering setting, and provides an item-based evaluation that is sensitive to the cluster size 
skew and effectively captures the expected performance of the system on a dataset where the cluster 
distribution would be equivalent.  
4.4 Results 
We compared our models with four baselines and three benchmark systems from the SemEval-2013 
task. Four baselines are described as follows. 
? Baseline MFS ? most frequent sense baseline, assigning all test instances to the MFS in the 
test data (regardless of what applicability rating it was given).  
? One sense ? labels each instance with the same induced sense. 
? One sense per instance (1clinst) ? labels each instance with its own induced. 
? Baseline Random-n ? randomly assigns each test instance to one of n randomly selected in-
duced senses, where n is the number of senses for the target word in WordNet 3.1.  
Three benchmark systems as the following are those which achieved better results in the original 
SemEval-2013 task. 
? AI-KU is based on a lexical substitution method. 
? UoS uses dependency-parsed features from the corpus, which are then clustered into senses us-
ing the MaxMax algorithm (Hope and Keller, 2013). 
? Unimelb is a non-parameter topic model which uses a Hierarchical Dirichlet Process (Teh et al., 
2006) to automatically infer the number of senses from contextual and positional features. 
4.4.1 Supervised evaluation 
In the supervised evaluation, the automatically induced clusters are mapped to gold standard senses, 
using one part of the test set. The obtained mapping is used to label the other part of test set with gold 
standard tags, which means that the methods are evaluated in a standard WSD task. In experiment, we 
follow the 80/20 setup: 80% for mapping and 20% for test. 
Table 1 shows the results of our systems on test data using all instances (including verbs, nouns and 
adjectives) for the WSD evaluation. As in previous WSD tasks, the MFS baseline on Jaccard Index 
measures is quite competitive, outperforming all systems in detecting which senses are applicable.  
1607
  
System JI-F1 WKT-F1 
 WNDCG-
F1 
NHC 0.325 0.692 0.375 
HEC 0.327 0.693 0.376 
Schype 0.376 0.753 0.345 
AI-KU  0.244 0.642 0.332 
UNIMELB 0.213 0.62 0.371 
UoS 0.232 0.625 0.374 
MFS 0.455 0.465 0.339 
One sense 0.192 0.609 0.288 
1c1inst 0.0 0.095 0.0 
Random-n 0.29 0.638 0.286 
 
Table 1. The supervised results over the SemEv-
al-2013 dataset. 
System FNMI FBC 
NHC 0.046  0.406  
HEC 0.037  0.400  
Schype 0.042  0.377  
AI-KU 0.039 0.451 
UNIMELB 0.056 0.459 
UoS 0.045 0.448 
MFS - - 
One sense 0 0.632 
1c1inst 0.071 0 
Random-n 0.016 0.245 
 
Table 2.  The unsupervised results over the Se-
mEval-2013 dataset. 
 
However, most systems in this task were able to outperform the MFS baseline in ranking senses and 
quantifying their applicability. 
On the other hand, it also indicates that our three systems achieve better or comparable scores And 
the Schype gets the highest scores in detecting senses and ranking senses over all systems.  
4.4.2 Unsupervised evaluation 
Unsupervised evaluation aims to measure the similarity of the induced sense inventories for WSI sys-
tems. Unlike supervised metrics, it avoids potential loss of sense information since this setting does 
not require any sense mapping procedure to convert induced senses to WordNet senses.  
Table 2 shows the performance of our systems, benchmarks and baselines. It shows that the NMI-
measure is biased towards the one sense per instance baseline and the FBC-measure one sense base-
line. However, systems are capable of performing well in both the Fuzzy NMI and Fuzzy B-Cubed 
measures, thereby avoiding the extreme performance of either baseline. Generally, the performance of 
our model gets balanced scores. 
4.5 Discussion 
Topic models, such as LDA and HDP (Brody and Lapata, 2009; Lau et al., 2012), have been success-
fully adopted for WSI, in which one topic is viewed as one sense. Our work is motivated by lexical 
chain that represents the intrinsic semantic relatedness among context instances on the viewpoint of 
linguistics. Topic model is used to find lexical chains which are interpreted as topics. We have com-
pared the Unimelb (Lau et al., 2013), a HDP topic model, with our model in the experiments. Addi-
tionally, we also follow Lau et al. (2012) to train a LDA model with a fixed number of topics based on 
our training data for WSI3. Table 3 shows the supervised result compared to the Schype. 
These experiments show promising performance for our model, which captures richer semantic re-
latedness by using lexical chains. Lexical chains play a key role for the performance of our model. 
Intuitively, when lexical chains are too long, the higher-order relatedness would be mixed with some 
noises, while when lexical chains are too short, some higher-order relatedness will be missed. In order 
to verify the effectiveness of lexical chains, we tune the parameter ? in lexical chain extraction proce-
dure and the results are shown in Figure 4.  
Another issue is the impact of POS labels of word for WSI task. The test data in SemeVal-2013 
WSI task contain nouns, verbs and adjectives. We also test the performance based on different POS 
labels. Table 4 gives the supervised evaluation performance of our three systems on adjectives, verbs 
and nouns respectively. We found that the performance for adjectives in sense detection is the best, 
verbs followed and nouns worst, whereas it?s reversed in sense ranking. The probable interpretation is  
                                                 
3 The LDA model parameters are set as follows: K=10, ? =0.025, ? = 0.001. 
1608
   
Figure 4.  Performance analysis on Jaccard index 
measure for different threshold parameter ? in 
lexical chains extraction procedure. 
 
Type LDAK=10 Schype 
JI-F1 0.318 0.376 
WKT-F1 0.692 0.753 
 WNDCG-F1 0.334 0.345 
 
Table 3. The supervised results over the SemEv-
al-2013 dataset between LDAk=10 and Schype for 
WSI. 
 
  NHC HEC Schype 
POS JI WKT WNDCG JI WKT WNDCG JI WKT WNDCG
nouns 0.306  0.697  0.370  0.313 0.702 0.367  0.363 0.767  0.246  
verbs 0.336  0.686  0.374  0.336 0.690 0.380  0.384 0.749  0.245  
adjs 0.347  0.697  0.396  0.342 0.678 0.390  0.394 0.733  0.248  
 
Table 4. The supervised performance of three algorithms respectively on nouns, verbs and adjectives. 
 
that adjective?s average sense number is the lowest, and the sense granularity is greater than verbs and 
nouns over the test data4. 
5 Conclusions and future work 
In this paper, we present a hypergraph model in which a node represents an instance and a hyperedge 
represents higher-order semantic relatedness among instances. Compared with other strategies based 
on binary local comparison, the model captures complex semantic relatedness among the instances 
from a global perspective.  
The evaluation results indicate that our model outperforms or reaches competitive performance 
comparable to other systems for the SemEval-2013 word sense induction task. Additionally, the expe-
riments also show that both sense number and sense granularity of a target word affect the perfor-
mance of WSI. 
For future work, we would like to explore better ways to extract and evaluate lexical chain for WSI 
task. In addition, for the three clustering algorithms, they generally require the number of clusters or 
edge-vertex ratio to be pre-defined, so we will seek more effective hypergraph clustering algorithms to 
automatically determine the parameters. Finally, the hypergraph model proposed in this work is not 
specific to the sense induction task, and can be adapted for other applications, such as document clas-
sification and clustering, information retrieval, etc. 
Acknowledgements 
This work is supported by the National Natural Science Foundation of China (No. 61173062, 61373108, 
61133012), the major program of the National Social Science Foundation of China (No. 11&ZD189), and the 
High Performance Computing Center of Computer School, Wuhan University. 
Reference 
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task 02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7?12.  
Eneko Agirre, David Martnez, Oier L?pez de Lacalle, and Aitor Soroa. 2006. Two graph-based algorithms for 
state-of-the-art WSD. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language 
                                                 
4 In the test data, the average number of senses of nouns, verbs, adjectives respectively is 7.15, 6.85, 5.9 respectively. 
1609
 Processing, pages 585?593.  
Enrique Amig?, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering 
evaluation metrics based on formal constraints. Information retrieval, 12(4):461?486. 
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collec-
tion of very large linguistically processed web-crawled corpora. Language resources and evaluation, 
43(3):209?226. 
Regina Barzilay, Michael Elhadad, et al. 1997. Using lexical chains for text summarization. In Proceedings of 
the ACL workshop on Intelligent scalable text summarization, volume 17, pages 10?17.  
Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz Yuret. 2013. AI-kU: Using substitute vectors and co-
occurrence modeling for word sense induction and disambiguation. In Proceedings of SemEval-2013.  
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of Machine 
learning research, 3:993?1022. 
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference 
of the European Chapter of the Association for Computational Linguistics.  
Stefan Bordag. 2006. Word sense induction: Triplet-based clustering and automatic evaluation. In Proceedings 
of the 11th Conference of the European Chapter of the Association for Computational Linguistics 
Joe Carthy. 2004. Lexical chains versus keywords for topic tracking. In Computational Linguistics and Intelli-
gent Text Processing, pages 507?510.  
Antonio Di Marco and Roberto Navigli. 2011. Clustering web search results with maximum spanning trees. In 
AI* IA 2011: Artificial Intelligence Around Man and Beyond, pages 201?212. 
Beate Dorow and Dominic Widdows. 2003. Discovering corpus-specific word senses. In Proceedings of the 
tenth Conference on European chapter of the Association for Computational Linguistics-Volume 2, pages 79?
82.  
Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In 
Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint 
Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 10?18.  
Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and cor-
rection of malapropisms. WordNet: An electronic lexical database, 305:305?332. 
David Hope and Bill Keller. 2013. Maxmax: a graph-based soft clustering algorithm applied to word sense in-
duction. In Computational Linguistics and Intelligent Text Processing, pages 368?381.  
Nancy Ide and Keith Suderman. 2004. The american national corpus first release. In LREC.  
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2001. Automatic sense tagging using parallel corpora. In NLPRS, 
pages 83?90. 
David Jurgens and Ioannis Klapaftis. 2013. SemEval-2013 Task 13: Word Sense Induction for Graded and Non-
Graded Senses. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 
2013). 
Ioannis P Klapaftis and Suresh Manandhar. 2007. Uoy: a hypergraph model for word sense induction & disam-
biguation. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 414?417.  
Andrea Lancichinetti, Santo Fortunato, and J?nos Kert?sz. 2009. Detecting the overlapping and hierarchical 
community structure in complex networks. New Journal of Physics, 11(3):033015. 
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013. unimelb: Topic Modelling-based Word Sense Induction. 
In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). 
Jey Han Lau, Paul Cook and Diana McCarthy, David Newman and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Associa-
tion for Computational Linguistics. 
Okumura Manabu and Honda Takeo. 1994. Word sense disambiguation and text segmentation based on lexical 
cohesion. In Proceedings of the 15th conference on Computational linguistics-Volume 2, pages 755?761.  
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, and Sameer S Pradhan. 2010. SemEval-2010 task 14: 
Word sense induction & disambiguation. In Proceedings of the 5th International Workshop on Semantic 
1610
 Evaluation, pages 63?68.  
Tom Michoel and Bruno Nachtergaele. 2012. Alignment and integration of complex networks by hypergraph-
based spectral clustering. Physical Review E, 86(5):056111. 
Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM 
Transactions on Information Systems (TOIS), 27(1):2. 
Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the 
structure of text. Computational linguistics, 17(1):21?48. 
Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR),41(2):10. 
Michael Steinbach Pang-Ning Tan and Vipin Kumar. 2006. Introduction to Data Mining. Pearson Addison Wes-
ley. 
Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the eighth ACM 
SIGKDD, pages 613?619.  
Li Pu and Boi Faltings. 2012. Hypergraph learning with hyperedge expansion. In Machine Learning and Know-
ledge Discovery in Databases, pages 410?425. 
Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and simi-
larity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41?48. 
Boston. 
Kumar, Ravi and Vassilvitskii, Sergei. 2010. Generalized distances between rankings. In Proceedings of the 19th 
international conference on World wide web, pages 571-580.  
Steffen Remus and Chris Biemann. 2013. Three knowledge-free methods for automatic lexical chain extraction. 
In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational 
Linguistics: Human Language Technologies, pages 989?999, Atlanta, Georgia, June.  
Martin Riedl and Chris Biemann. 2012. Sweeping through the topic space: bad luck? roll again! In Proceedings 
of the Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP, pages 19?27.  
Samuel Rota Bulo and Marcello Pelillo. 2012. A game-theoretic approach to hypergraph clustering. 
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international 
conference on new methods in language processing, volume 12, pages 44?49.  
Hinrich Sch?tze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123. 
Amnon Shashua, Ron Zass, and Tamir Hazan. 2006. Multi-way clustering using super-symmetric non-negative 
tensor factorization. In Computer Vision?ECCV 2006, pages 595?608.  
Nicola Stokes, Joe Carthy, and Alan F Smeaton. 2004. Select: a lexical cohesion based news story segmentation 
system. AI Communications, 17(1):3?12. 
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes. 
Journal of the american statistical association, 101(476). 
Tim Van de Cruys, Marianna Apidianaki, et al. 2011. Latent semantic word sense induction and disambiguation. 
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-
guage Technologies (ACL/HLT), pages 1476?1485. 
Jean V?ronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech & Language, 
18(3):223?252. 
Dominic Widdows and Beate Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proceedings 
of the 19th International Conference on Computational linguistics-Volume 1, pages 1?7.  
David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings 
of the 33rd annual meeting on Association for Computational Linguistics, pages 189?196.  
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012. Learning syntactic categories using paradigmatic repre-
sentations of word context. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural Language Learning, pages 940?951.  
Dengyong Zhou, Jiayuan Huang, and Bernhard Sch?lkopf. 2006. Learning with hypergraphs: Clustering, classi-
fication, and embedding. In Advances in Neural Information Processing Systems, pages 1601?1608. 
1611
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 488?498,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Positive Unlabeled Learning for Deceptive Reviews Detection
Yafeng Ren Donghong Ji Hongbin Zhang
Computer School
Wuhan University
Wuhan 430072, China
{renyafeng,dhji,zhanghongbin}@whu.edu.cn
Abstract
Deceptive reviews detection has attract-
ed significant attention from both business
and research communities. However, due
to the difficulty of human labeling need-
ed for supervised learning, the problem re-
mains to be highly challenging. This pa-
per proposed a novel angle to the prob-
lem by modeling PU (positive unlabeled)
learning. A semi-supervised model, called
mixing population and individual proper-
ty PU learning (MPIPUL), is proposed.
Firstly, some reliable negative examples
are identified from the unlabeled dataset.
Secondly, some representative positive ex-
amples and negative examples are gener-
ated based on LDA (Latent Dirichlet Al-
location). Thirdly, for the remaining un-
labeled examples (we call them spy ex-
amples), which can not be explicitly iden-
tified as positive and negative, two simi-
larity weights are assigned, by which the
probability of a spy example belonging to
the positive class and the negative class
are displayed. Finally, spy examples and
their similarity weights are incorporated
into SVM (Support Vector Machine) to
build an accurate classifier. Experiments
on gold-standard dataset demonstrate the
effectiveness of MPIPUL which outper-
forms the state-of-the-art baselines.
1 Introduction
The Web has dramatically changed the way peo-
ple express themselves and interact with others,
people frequently write reviews on e-commerce
sites, forums and blogs to achieve these purpos-
es. For NLP (Natural Language Processing), these
user-generated contents are of great value in that
they contain abundant information related to peo-
ple?s opinions on certain topics. Currently, on-
line reviews on products and services are used
extensively by consumers and businesses to con-
duct decisive purchase, product design and mar-
keting strategies. Hence, sentiment analysis and
opinion mining based on product reviews have
become a popular topic of NLP (Pang and Lee,
2008; Liu, 2012). However, since reviews infor-
mation can guide people?s purchase behavior, pos-
itive reviews can result in huge economic benefit-
s and fame for organizations or individuals. This
leaves room for promoting the generation of re-
view spams. Through observations and studies of
the predecessors (Jindal and Liu, 2008; Ott et al.,
2011), review spams are divided into the following
two classes:
? Deceptive Reviews: Those deliberately mis-
lead readers by giving undeserving positive
reviews to some target objects in order to pro-
mote the objects, or by giving unjust nega-
tive reviews to some target objects in order to
damage their reputation.
? Disruptive Reviews: Those are non-reviews,
which mainly include advertisements and
other irrelevant reviews containing no opin-
ion.
Disruptive reviews pose little threat to peo-
ple, because human can easily identify and ignore
them. In this paper, we focus on the more chal-
lenging ones: deceptive reviews. Generally, de-
ceptive reviews detection is deemed to be a classi-
fication problem (Ott et al., 2011; Li et al., 2011;
Feng et al., 2012). Based on the positive and neg-
ative examples annotated by people, supervised
learning is utilized to build a classifier, and then an
unlabeled review can be predicted as deceptive re-
view or truthful one. But the work from Ott et al.
(2011) shows that human cannot identify decep-
tive reviews from their prior knowledge, which in-
dicates that human-annotated review datasets must
488
include some mislabeled examples. These exam-
ples will disturb the generation ability of the clas-
sifiers. So simple supervised learning is regarded
as unsuitable for this task.
It is difficult to come by human labeling need-
ed for supervised learning and evaluation, we can-
not obtain the datasets containing deceptive re-
views. However, we can get some truthful reviews
with high confidence by heuristic rules and prior
knowledge. Meanwhile, a lot of unlabeled reviews
are available. The problem thus is this: based on
some truthful reviews and a lot of unlabeled re-
views, can we build an accurate classifier to iden-
tify deceptive reviews.
PU (positive unlabeled) learning can be utilized
to deal with the above situation (Liu et al., 2002;
Liu et al., 2003). Different from traditional super-
vised learning, PU learning can still build an ac-
curate classifier even without the negative training
examples. Several PU learning techniques have
been applied successfully in document classifica-
tion with promising results (Zhang, 2005; Elkan
and Noto, 2008; Li et al., 2009; Xiao et al., 2011),
while they have yet to be applied in detecting de-
ceptive reviews. Here, we will study how to design
PU learning to detect deceptive reviews.
An important challenge is how to deal with
spy examples (easily mislabeled) of unlabeled re-
views, which is not easily handled by the previous
PU learning techniques. In this paper, we propose
a novel approach, mixing population and individ-
ual property PU learning (MPIPUL), by assigning
similarity weights and incorporating weights into
SVM learning phase. This paper makes the fol-
lowing contributions:
? For the first time, PU learning is defined in
the environment of identifying deceptive re-
views.
? A novel PU learning is proposed based on L-
DA and SVM.
? Experimental results demonstrate that our
proposed method outperforms the curren-
t baselines.
2 Related Work
2.1 Deceptive Reviews Detection
Spam has historically been investigated in the con-
texts of e-mail (Drucker et al., 1999; Gyongyi et
al., 2004) and the Web (Ntoulas et al., 2006). In
recent years, researchers have started to look at de-
ceptive reviews.
Jindal and Liu (2008) found that opinion s-
pam was widespread and different from e-mail
and Web spam in essence (Jindal and Liu, 2008).
They trained models using product review data,
by defining features to distinguish duplicate opin-
ion and non-duplicate based on the review tex-
t, reviewers and product information. Wu et al.
(2010) proposed an alternative strategy of popu-
larity rankings (Wu et al., 2010).
Ott et al. (2011) developed the first dataset con-
taining gold-standard deceptive reviews by crowd-
sourcing (Ott et al., 2011), and presented three su-
pervised learning methods to detect deceptive re-
views by integrating knowledge from psycholin-
guistics and computational linguistics. This gold-
standard dataset will be used in the paper. Li et al.
(2011) manually built a review dataset from their
crawled reviews (Li et al., 2011), and exploited
semi-supervised co-training algorithm to identify
deceptive reviews.
Feng et al. (2012) verified the connection be-
tween the deceptive reviews and the abnormal dis-
tributions (Feng et al., 2012a). Later, they (Feng et
al., 2012b) demonstrated that features driven from
CFG (Context Free Grammar) parsing trees con-
sistently improve the detection performance.
Mukherjee et al. (2012) proposed detect-
ing group spammers (a group of reviewers who
work collaboratively to write deceptive reviews) in
product reviews (Mukherjee et al., 2012). The pro-
posed method first used frequent itemset mining
to find a set of candidate groups. Then GSRank
was presented which can consider relationships a-
mong groups, individual reviewers and products
they reviewed to detect spammer groups. Later,
they also proposed exploiting observed reviewing
behaviors to detect opinion spammers in an unsu-
pervised Bayesian inference framework (Mukher-
jee et al., 2013).
Ren et al. (2014) assumed that there must be
some difference on language structure and sen-
timent polarity between deceptive reviews and
truthful ones (Ren et al., 2014a), then they de-
fined the features related to the review text and
used genetic algorithm for feature selection, fi-
nally they combined two unsupervised clustering
algorithm to identify deceptive reviews. Later,
they (Ren et al., 2014b) present a new approach,
from the viewpoint of correcting the mislabeled
489
examples, to find deceptive reviews. Firstly, they
partition a dataset into several subsets.Then they
construct a classifier set for each subset and s-
elect the best one to evaluate the whole dataset.
Meanwhile, error variables are defined to compute
the probability that the examples have been mis-
labeled. Finally, the mislabeled examples are cor-
rected based on two threshold schemes, majority
and non-objection.
Unlike previous studies, PU learning is imple-
mented to identify deceptive reviews.
2.2 Positive Unlabeled Learning
According to the use of the unlabeled data, PU
learning can be divided into two classes.
One family of methods built the final classifier
by using positive examples dataset and some ex-
amples of the unlabeled dataset (Liu et al., 2002;
Liu et al., 2003). The basic idea is to find a set
of reliable negative examples from the unlabeled
data firstly, and then to learn a classifier using EM
(Expectation Maximization) or SVM. The perfor-
mance is limited for neglecting the rest examples
of unlabeled dataset.
Another family of methods learned the final
classifier by using positive examples dataset and
all examples of the unlabeled dataset. Li et al.
(Li et al., 2009) studied PU learning in the data
stream environment, they proposed a PU learn-
ing LELC (PU Learning by Extracting Likely
positive and negative micro-Clusters) for docu-
ment classification, they assume that the exam-
ples close together shared the same labels. Xi-
ao et al. (Xiao et al., 2011) proposed a method,
called SPUL (similarity-based PU learning), the
local similarity-based and global similarity-based
mechanisms are proposed to generate the similar-
ity weights for the easily mislabeled examples,
respectively. Experimental results show global
SPUL generally performs better than local SPUL.
In this paper, a novel PU learning (MPIPUL) is
proposed to identify deceptive reviews.
3 Preliminary
Before we introduce the proposed method, we
briefly review SVM, which has proven to be an
effective classification algorithm (Vapnik, 1998).
Let T = {(x
(1)
, y
(1)
), (x
(2)
, y
(2)
), . . . , (x
(|T |)
, y
(|T |)
)} be a training set, where x
(i)
? R
d
and
y
(i)
? {+1,?1}. SVM aims to seek an optimal
separating hyperplane w
T
x
(i)
+ b = 0, the hyper-
plane can be obtained by solving the following
optimization problem:
min F (w, b, ?
i
) =
1
2
||w||
2
+ C
|T |
?
i=1
?
i
s.t. y
(i)
(w
T
x
(i)
+ b) ? 1 ? ?
i
, i = 1, . . . , |T |
?
i
? 0, i = 1, . . . , |T |
(1)
where w
T
represents the transpose of w, C is a
parameter to balance the classification errors and
?
i
are variables to relax the margin constraints.
The optimal classifier can be achieved by using
the Lagrange function. For a test example x, if
w
T
x+b < 0, it is classified into the negative class;
otherwise, it is positive.
In the following, SVM is extended to incorpo-
rate the spy examples and their weights, such that
the spy examples can contribute differently to the
classifier construction.
4 The Proposed Method
In this section, we will introduce the proposed ap-
proach in details. In our PU learning (MPIPUL),
truthful reviews are named positive examples, and
deceptive reviews are called negative examples. P
is defined as a set which contains all positive ex-
amples. U is a set for all unlabeled examples. PU
learning aims at building a classifier using P and
U . MPIPUL adopts the following four steps:
? Step 1: Extract the reliable negative exam-
ples;
? Step 2: Compute the representative positive
and negative examples;
? Step 3: Generate the similarity weights for
the spy examples;
? Step 4: Build the final SVM classifier;
4.1 Extracting Reliable Negative Examples
Considering only positive and unlabeled examples
are available in PU learning, some negative ex-
amples need to be extracted firstly. These exam-
ples will influence the performance of the follow-
ing three steps. So high-quality negative examples
must be guaranteed. Previous works solved the
problem with the Spy technique (Liu et al., 2002)
or the Rocchio technique (Liu et al., 2003), we in-
tegrate them in order to get reliable negative ex-
amples. Let subsets NS
1
and NS
2
contain the
490
corresponding reliable negative examples extract-
ed by the two techniques, respectively. Examples
are considered to be a reliable negative only if both
techniques agree that they are negative. That is,
NS = NS
1
? NS
2
, where NS contains the reli-
able negative examples.
After reliable negative examples are extracted,
there are still some unlabeled examples (we call
spy examples) in set U , let subset US = U ?NS,
which stores all the spy examples. It is crucial to
determine how to deal with these spy examples.
4.2 Computing Representative Positive and
Negative Examples
Generally, a classifier can be constructed to pre-
dict deceptive reviews based on the positive ex-
amples set P and the reliable negative examples
set NS. But the classifier is not accurate enough
for lacking of making full use of unlabeled dataset
U . In order to utilize spy examples in subset US,
some representative positive and negative exam-
ples are calculated firstly. Since the examples have
different styles in sentiment polarity and topic dis-
tribution, for every class, computing one repre-
sentative example is not suitable. For the posi-
tive class or the negative class, to ensure there is
a big difference between the different representa-
tive examples. This paper proposes clustering re-
liable negative examples into several groups based
on LDA (Latent Dirichlet Allocation) topic mod-
el and K-means, and then multiple representative
examples can be obtained.
LDA topic model is known as a parametric
Bayesian clustering model (Blei et al., 2003), and
assumes that each document can be represented
as the distribution of several topics, each docu-
ment is associated with common topics. LDA can
well capture the relationship between internal doc-
uments.
In our experiments based on LDA model, we
can get the topic distribution for the reliable neg-
ative examples, then some reliable negative exam-
ples which are similar in topic distribution will be
clustered into a group by K-means. Finally, these
reliable negative examples can be clustered into n
micro-clusters (NS
1
, NS
2
, . . . , NS
n
). Here,
n = 30 ? |NS|/(|US| + |NS|) (2)
Here, according to the suggestion of previous
work (Xiao et al., 2011), we examine the impact
of the different parameter (from 10 to 60) on over-
all performance, and select the best value 30.
Based on the modified Rocchio formula (Buck-
ley et al., 1999), n representative positive exam-
ples (p
k
) and n negative ones (n
k
) can be obtained
using the following formula:
p
k
= ?
1
|P |
|P |
?
i=1
x
(i)
? x
(i)
?
? ?
1
|NS
k
|
|NS
k
|
?
i=1
x
(i)
? x
(i)
?
n
k
= ?
1
|NS
k
|
|NS
k
|
?
i=1
x
(i)
? x
(i)
?
? ?
1
|P |
|P |
?
i=1
x
(i)
? x
(i)
?
k = 1, . . . , n
(3)
According to previous works (Buckley et al.,
1994), where the value of ? and ? are set to 16
and 4 respectively. The research from Buckley et
al. demonstrate that this combination emphasizes
occurrences in the relevant documents as opposed
to non-relevant documents.
4.3 Generating Similarity Weights
For a spy example x, since we do not know which
class it should belong to, enforcing x to the posi-
tive class or the negative class will lead to some
mislabeled examples, which disturbs the perfor-
mance of final classifier. We represent a spy ex-
ample x using the following probability model:
{x, (p
+
(x), p
?
(x))}, p
+
(x) + p
?
(x) = 1 (4)
Where p
+
(x) and p
?
(x) are similarity weight-
s which represent the probability of x belonging
to the positive class and the negative class, re-
spectively. For example, {x, (1, 0)} means that x
is positive, while {x, (0, 1)} indicates that x is i-
dentified to be negative. For {x, (p
+
(x), p
?
(x))},
where 0 < p
+
(x) < 1 and 0 < p
?
(x) < 1, it
implies that the probability of x belonging to the
positive class and the negative class are both con-
sidered.
In this section, similarity weights are decided by
mixing global information (population property)
and local information (individual property). Then
all spy examples and their similarity weights are
incorporated into a SVM-based learning model.
4.3.1 Population Property
Population property means that the examples in
each micro-cluster share the similarity in sen-
timent polarity and topic distribution, and they
belong to the same category with a high pos-
sibility. In our framework, in order to com-
pare with the representative examples, all spy ex-
amples are firstly clustered into n micro-clusters
491
(US
1
, US
2
, . . . , US
n
) based on LDA and K-
means. Then, for every spy example x in one
micro-cluster US
i
, we tags with temporary label
by finding its most similar representative example.
Finally, we can get the similarity weights for a spy
example x in micro-cluster US
i
, their probability
pertaining to the positive class and negative class
can be represented by the following formula:
p pop(x) =
|positive|
|US
i
|
n pop(x) =
|negative|
|US
i
|
(5)
where |US
i
| represents the number of all examples
in micro-cluster US
i
, |positive| means the num-
ber of the examples which is called temporary pos-
itive in US
i
, and |negative| means the number of
the examples which is called temporary negative
in US
i
.
For example, Figure 1 shows the part (C1, C2,
C3, C4) of the clustering results for the spy exam-
ples based on LDA and K-means, the examples
x in C4 are assigned with weights p pop(x) =
4
9
, n pop(x) =
5
9
, the examples x in C1 are as-
signed with weights p pop(x) = 1, n pop(x) = 0.
Figure 1: Illustration of population property
The advantage of population property lies in the
fact that it considers the similar relationship be-
tween the examples, from which the same micro-
cluster are assigned the same similarity weight.
However, it cannot distinguish the difference of
examples in one micro-cluster. In fact, the simi-
larity weights of examples from the same micro-
cluster can be different, since they are located
physically different. For example, for the spy ex-
ample y and z in micro-cluster C4, it is apparent-
ly unreasonable that we assign the same similarity
weights to them. So we should join the local in-
formation (individual property) when we are com-
puting the similarity weights for a spy example.
4.3.2 Individual Property
Individual property is taken into account to mea-
sure the relationship between every spy example
and all representative ones. Specifically, for ex-
ample x, we firstly compute its similarity to each
of the representative examples, and then the prob-
ability of the example x belonging to the positive
class and negative class can be calculated using the
following formula:
p ind(x) =
?
n
k=1
sim(x, p
k
)
?
n
k=1
(sim(x, p
k
) + sim(x, n
k
))
n ind(x) =
?
n
k=1
sim(x, n
k
)
?
n
k=1
(sim(x, p
k
) + sim(x, n
k
))
(6)
In the above formula,
sim(x, y) =
x ? y
||x|| ? ||y||
4.3.3 Similarity Weights
A scheme mixing population and individual prop-
erty is designed to generate the similarity weights
of spy examples. Specifically, for spy example x,
their similarity weights can be obtained by the fol-
lowing formula:
p
+
(x) = ? ? p pop(x) + (1 ? ?) ? p ind(x)
p
?
(x) = ? ? n pop(x) + (1 ? ?) ? n ind(x)
(7)
Where ? is a parameter to balance the informa-
tion from population property and individual prop-
erty. In the remaining section, we will examine
the impact of the parameter ? on overall perfor-
mance. Meanwhile, it can be easily proved that
p
+
(x) + p
?
(x) = 1.
4.4 Constructing SVM Classifier
After performing the third step, each spy example
x is assigned two similarity weights: p
+
(x) and
p
?
(x). In this section, we will extend the formu-
lation of SVM by incorporating the examples in
positive set P , reliable negative set NS, spy ex-
amples set US and their similarity weights into a
SVM-based learning model.
4.4.1 Primal Problem
Since the similarity weights p
+
(x) and p
?
(x) in-
dicate the probability for a spy example x belong-
ing to the positive class and the negative class, re-
spectively. The optimization formula (1) can be
492
rewritten as the following optimization problem:
min F (w, b, ?) =
1
2
||w||
2
+ C
1
|P |
?
i=1
?
i
+ C
2
?
|US|
?
j=1
p
+
(x
(j)
)?
j
+ C
3
|US|
?
m=1
p
?
(x
(m)
)?
m
+C
4
|NS|
?
n=1
?
n
s.t. y
(i)
(w
T
x
(i)
+ b) ? 1 ? ?
i
, x
(i)
? P
y
(j)
(w
T
x
(j)
+ b) ? 1 ? ?
j
, x
(j)
? US
y
(m)
(w
T
x
(m)
+ b) ? 1 ? ?
m
, x
(m)
? US
y
(n)
(w
T
x
(n)
+ b) ? 1 ? ?
n
, x
(n)
? NS
?
i
? 0, ?
j
? 0, ?
m
? 0, ?
n
? 0
(8)
Where C
1
, C
2
, C
3
and C
4
are penalty factors con-
trolling the tradeoff between the hyperplane mar-
gin and the errors, ?
i
, ?
j
, ?
m
and ?
n
are the error
terms. p
+
(x
(j)
)?
j
and p
?
(x
(m)
)?
m
can be consid-
ered as errors with different weights. Note that,
a bigger value of p
+
(x
(j)
) can increase the effect
of parameter ?
j
, so that the corresponding example
x
(j)
becomes more significant towards the positive
class. In the following, we will find the dual form
to address the above optimization problem.
4.4.2 Dual Problem
Assume ?
i
and ?
j
are Lagrange multipliers. To
simplify the presentation, we redefine some nota-
tions as follows:
C
+
i
=
{
C
1
, x
(i)
? P
C
2
p
+
(x
(j)
), x
(j)
? US
C
?
j
=
{
C
3
p
?
(x
(m)
), x
(m)
? US
C
4
, x
(n)
? NS
Based on the above definitions, we let T
+
=
P ? US, T
?
= US ? NS and T
?
= T
+
? T
?
.
The Wolfe dual of primal formulation can be ob-
tained as follows (Appendix A for the calculation
process):
max W (?) =
|T
?
|
?
i=1
?
i
?
1
2
|T
?
|
?
i=1,j=1
?
i
?
j
y
(i)
?
y
(j)
< x
(i)
, x
(j)
>
s.t. C
+
i
? ?
i
? 0, x
(i)
? T
+
C
?
j
? ?
j
? 0, x
(j)
? T
?
|T
+
|
?
i=1
?
i
?
|T
?
|
?
j=1
?
j
= 0
(9)
where < x
(i)
, x
(j)
> is the inner product of x
(i)
and x
(j)
. In order to get the better performance, we
can replace them by using kernel function ?(x
(i)
)
and ?(x
(j)
), respectively. The kernel track can
convert the input space into a high-dimension fea-
ture space. It can solve the uneven distribution of
dataset and complex problem from heterogeneous
data sources, which allows data to get a better ex-
pression in the new space (Lanckriet et al., 2004;
Lee et al., 2007).
After solving the above problem, w can be ob-
tained, then b can also be obtained by using KKT
(Karush-Kuhn-Tucker) conditions. For a test ex-
ample x, if w
T
x+ b > 0, it belongs to the positive
class. Otherwise, it is negative.
5 Experiments
We aim to evaluate whether our proposed PU
learning can identify deceptive reviews properly.
We firstly describe the gold-standard dataset, and
then introduce the way to generate the positive
examples P and unlabeled examples U . Finally
we present human performance in gold-standard
dataset.
5.1 Datasets
There is very little progress in detection of de-
ceptive reviews, one reason is the lack of stan-
dard dataset for algorithm evaluation. The gold-
standard dataset is created based on crowdsourc-
ing platform (Ott et al., 2011), which is also adopt-
ed as the experimental dataset in this paper.
5.1.1 Deceptive Reviews
Crowdsourcing services can carry out massive da-
ta collection and annotation; it defines the task in
the network platform, and paid for online anony-
mous workers to complete the task.
493
Humans cannot be precisely distinguish decep-
tive ones from existing reviews, but they can create
deceptive reviews as one part of the dataset. Ott et
al. (2011) accomplish this work by AMT (Ama-
zon Mechanical Turk). They set 400 tasks for 20
hotels, in which each hotel gets 20 tasks. Specif-
ic task is: If you are a hotel market department
employee, for each positive review you wrote for
the benefit for hotel development, you may get one
dollar. They collect 400 deceptive reviews.
5.1.2 Truthful Reviews
For the collection of truthful reviews, they get
6977 reviews from TripAdvisor
1
based on the
same 20 Chicago hotels, and remove some reviews
on the basis of the following constraints:
? Delete all non-five star reviews;
? Delete all non-English reviews;
? Delete all reviews which are less than 75
characters;
? Delete all reviews written by first-time au-
thors;
2124 reviews are gathered after filtering. 400 of
them are chosen as truthful ones for balancing the
number of deceptive reviews, as well as maintain-
ing consistent with the distribution of the length of
deceptive reviews. 800 reviews constitute whole
gold-standard dataset at last.
5.2 Experiment Setup
We conduct 10-fold cross-validation: the dataset
is randomly split into ten folds, where nine fold-
s are selected for training and the tenth fold for
test. In training dataset, it contains 360 truthful
reviews and 360 deceptive ones. This paper is in-
tended to apply PU learning to identify deceptive
reviews. We specially make the following setting:
take 20% of the truthful reviews in training set as
positive examples dataset P , all remaining truthful
and deceptive reviews in training set as the unla-
beled dataset U . Therefore, during one round of
the algorithm, the training set contains 720 exam-
ples including 72 positive examples (set P ) and
648 unlabeled examples (set U ), and the test set
contains 80 examples including 40 positive and 40
negative ones. In order to verify the stability of
the proposed method, we also experiment anoth-
er two different settings, which account for 30%
1
http://www.tripadvisor.com
and 40% of the truthful reviews in training set as
positive examples dataset P respectively.
5.3 Human Performance
Human performance reflects the degree of difficul-
ty to address this task. The rationality of PU learn-
ing is closely related to human performance.
We solicit the help of three volunteer students,
who were asked to make judgments on test sub-
set (corresponding to the tenth fold of our cross-
validation experiments, contains 40 deceptive re-
views and 40 truthful reviews). Additionally, to
test the extent to which the individual human
judges are biased, we evaluate the performance of
two virtual meta-judges: one is the MAJORITY
meta-judge when at last two out of three human
judge believe the review to be deceptive, and the
other is the SKEPTIC when any human judge be-
lieves the review to be deceptive. It is apparent
from the results that human judges are not par-
ticularly effective at this task (Table 1). Inter-
annotator agreement among the three judges, com-
puted using Fleiss? kappa, is 0.09. Landis and
Koch (Landis and Koch, 1977) suggest that s-
cores in the range (0.00, 0.20) correspond to ?s-
light agreemen? between annotators. The largest
pairwise Cohen?s kappa is 0.11 between JUDGE-
1 and JUDGE-3, far below generally accepted
pairwise agreement levels. We can infer that the
dataset which are annotated by people will include
a lot of mislabeled examples. Identifying decep-
tive reviews by simply using supervised learning
methods is not appropriate. So we propose ad-
dressing this issue by using PU learning.
Table 1: Human performance
Methods Accuracy (%)
Human
JUDGE-1 57.9
JUDGE-2 55.4
JUDGE-3 61.7
META
MAJORITY 58.3
SKEPTIC 62.4
6 Results and Analysis
In order to verify the effectiveness of our proposed
method, we perform two PU learning (LELC and
SPUL) in the gold-standard dataset.
494
6.1 Experimental Results
Table 2 shows that the experimental results com-
pared with different PU learning techniques. In
Table 2, P (20%) means that we randomly select
20 percentages of truthful reviews to form the pos-
itive examples subset P . In our MPIPUL frame-
work, we set ? = 0.3. We can see that our pro-
posed method can obtain 83.91%, 85.43% and
86.69% in accuracy from different experimental
settings, respectively. Compared to the curren-
t best method (SPUL-global), the accuracy can be
improved 2.06% on average. MPUPUL can im-
prove 3.21% on average than LELC. The above
discussion shows our proposed methods consis-
tently outperform the other PU baselines.
Table 2: Accuracy on the different PU learning
Baselines P(20%) P(30%) P(40%)
LELC 81.12 82.08 83.21
SPUL-local 81.43 82.71 84.09
SPUL-global 81.89 83.24 84.73
MPIPUL (0.3) 83.91 85.43 86.69
PU learning framework in this paper can obtain
the better performance. Two factors contribute to
the improved performance. Firstly, LDA can cap-
ture the deeper information of the reviews in topic
distribution. Secondly, strategies of mixing pop-
ulation and individual property can generate the
similarity weights for spy examples, and these ex-
amples and their similarity weights are extended
into SVM, which can build a more accurate clas-
sifier.
6.2 Parameter Sensitivity
For the spy examples, the similarity weights are
generated by population property and individual
property. Should we select the more population
information or individual information? In MPIP-
UL, parameter ? is utilized to adjust this process.
So we experiment with the different value of the
parameter ? on MPUPUL performance (Figure 2).
As showed in Figure 2, for P (20%), if ? < 0.3,
the performance increases linearly, if ? > 0.3,
the performance will decrease linearly. Mean-
while, we can get the same trends for P (30%) and
P (40%). Based on the above discussion, MPIP-
UL can get the best performance when ? ? 0.3.
Figure 2: Algorithm performance on different pa-
rameter
7 Conclusions and Future Work
This paper proposes a novel PU learning (MPIP-
UL) technique to identify deceptive reviews based
on LDA and SVM. Firstly, the spy examples are
assigned similarity weights by integrating the in-
formation from the population property and in-
dividual property. Then the spy examples and
their similarity weights are incorporated into SVM
learning phase to build an accurate classifier. Ex-
perimental results on gold-standard dataset show
the effectiveness of our method.
In future work, we will discuss the application
of our proposed method in the massive dataset.
Acknowledgments
We are grateful to the anonymous reviewer-
s for their thoughtful comments. This work
is supported by the State Key Program of
National Natural Science Foundation of China
(Grant No.61133012), the National Natural Sci-
ence Foundation of China (Grant No.61173062,
61373108) and the National Philosophy Social
Science Major Bidding Project of China (Grant
No. 11&ZD189).
References
Alexandros Ntoulas, Marc Najork, Mark Manasse, and
Dennis Fetterly. 2006. Detecting spam web pages
through content analysis. In Proceedings of the 15th
International Conference on World Wide Web, page
83-92, Edinburgh, Scotland.
Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui
Wang, Meichun Hsu, Malu Castellanos, and Riddhi-
man Ghosh. 2013. Spotting opinion spammers us-
ing behavioral footprints. In Proceeding of the 19th
495
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Ming, page 632-640, Ly-
on, France.
Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012.
Spotting fake reviewer groups in consumer reviews.
In Proceeding of the 21st International Conference
on World Wide Web, page 191-200, New York, US-
A.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Morgan & Claypool Publishers. San Rafael,
USA.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.
2002. Partially supervised classification of text doc-
uments. In Proceedings of the 19th International
Conference on Machine Learning, page 387-394,
San Francisco, USA.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip
S. Yu. 2003. Building text classifiers using positive
and unlabeled examples. In Proceedings of the 3rd
IEEE International Conference on Data Ming, page
179-182, Washington, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1-135.
Charles Elkan and Keith Noto. 2008. Learning clas-
sifiers from only positive and unlabeled data. In
Proceedings of the 14th ACM SIGKDD Internation-
al Conference on Knowledge Discovery and Data
Ming, page 213-220, Las Vegas, USA.
Chirs Buckley, Bgrard Salton, and James Allan. 1994.
The effect of adding relevance information in a rele-
vance feedback environment. In Proceedings of the
17th Annual International SIGIR Conference on Re-
search and Development Retrieval, page 292-300,
Dublin, Ireland.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993-1022.
Dell Zhang. 2005. A simple probabilistic approach
to learning from positive and unlabeled examples.
In Proceedings of the 5th Annual UK Workshop on
Computational Intelligence, page 83-87.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan
Zhu. 2011. Learning to identify review spam. In
Proceeding of the 22nd International Joint Confer-
ence on Artificial Intelligence, page 2488-2493,
Barcelona, Spain.
Fang Wu and Bernardo A. Huberman. 2010. Opinion
formation under costly express. ACM Transactions
on Intelligence System Technology, 1(5):1-13.
Gert R. G. Lanckeriet, Nello Cristianini, Peter Bartlet-
t, Laurent EI Ghaoui, and Michael I.Jordan. 2004.
Learning the kernel matrix with seim-difinit pro-
gramming. Journal of Machine Learning Research,
5:27-72.
Harris Drucker, Donghui Wu, and Vladimir N. Vap-
nik. 1999. Support vector machines for spam cate-
gorization. IEEE Transactions on Neural Networks,
10(5):1048-1054.
Kumar Ankita and Sminchisescu Cristian. 2006. Sup-
port kernel machines for object recognition. In Pro-
ceedings of the IEEE 11th International Conference
on Computer Vision, page 1-8, Rio de Janeiro, Brza-
il.
Myle Ott, Yelin Choi, Claire Caridie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceeding
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
noloies, page 309-319, Portland, USA.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceeding of the 1st ACM Interna-
tional Conference on Web Search and Data Mining,
page 137-142, California, USA.
Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159-174.
Song Feng, Longfei Xing, Anupam Gogar, and Yejin
Choi. 2012. Distributional footprints of deceptive
product reviews. In Proceeding of the 6th Inter-
national AAAI Conference on WebBlogs and Social
Media, page 98-105, Dublin, Ireland.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceeding of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, page 171-
175, Jeju Island, Korea.
Vladimir N. Vapnik. 1998. Statistical learning theory.
Springer. New York, USA.
Wanjui Lee, Sergey Verzakov, and Robert P. Duin.
2007. Kernel combination versus classifier com-
bination. In Proceedings of the 7th International
Workshop on Multiple Classifier Systems, page 22-
31, Rrague, Czech Republic.
Xiaoli Li, Philip S. Yu, Bing Liu, and See Kiong Ng.
2009. Positive unlabeled learning for data stream
classification. In Proceedings of the SIAM Inter-
national Conference on Data Ming, page 257-268,
Nevada, USA.
Yafeng Ren, Donghong Ji, Lan Yin, and Hongbin
Zhang. 2014. Finding deceptive opinion spam by
correcting the mislabled instances. Chinese Journal
of Electronics, 23(4):702-707.
Yafeng Ren, Lan Yin, and Donghong Ji. 2014. De-
ceptive reviews detection based on language struc-
ture and sentiment polarity. Journal of Frontiers of
Computer Science and Technology, 8(3):313-320.
496
Yanshan Xiao, Bing Liu, Jie Yin, Longbing Cao,
Chengqi Zhang, and Zhifeng Hao. 2011.
Similarity-based approach for positive and unla-
beled learning. In Proceeding of the 22nd Inter-
national Joint Conference on Artifical Intelligence,
page 1577-1582, Barcelona, Spain.
Zoltan Gyongyi, Hector Garcia-Molina, and Jan
Pedesen. 2004. Combating web spam web with
trustrank. In Proceedings of the 30th International
Conference on Very Large Data Bases, page 576-
587, Toronto, Canada.
Appendix A
The optimization problem is as follows:
min F (w, b, ?) =
1
2
||w||
2
+ C
1
|P |
?
i=1
?
i
+ C
2
?
|US|
?
j=1
p
+
(x
(j)
)?
j
+ C
3
|US|
?
m=1
p
?
(x
(m)
)?
m
+
C
4
|NS|
?
n=1
?
n
s.t. y
(i)
(w
T
x
(i)
+ b) ? 1 ? ?
i
, x
(i)
? P
y
(j)
(w
T
x
(j)
+ b) ? 1 ? ?
j
, x
(j)
? US
y
(m)
(w
T
x
(m)
+ b) ? 1 ? ?
m
, x
(m)
? US
y
(n)
(w
T
x
(n)
+ b) ? 1 ? ?
n
, x
(n)
? NS
?
i
? 0, ?
j
? 0, ?
m
? 0, ?
n
? 0
(10)
We construct the Lagrangian function for the
above optimization problem, we have:
L(w, b, ?, ?, ?) = F (w, b, ?) +
|P |
?
i=1
?
i
[?y
(i)
?
(w
T
x
(i)
+ b) + 1 ? ?
i
] +
|US|
?
j=1
?
j
[?y
(j)
(w
T
x
(j)
+
b) + 1 ? ?
j
] +
|US|
?
m=1
?
m
[?y
(m)
(w
T
x
(m)
+ b) + 1
??
m
] +
|NS|
?
n=1
?
n
[?y
(n)
(w
T
x
(n)
+ b) + 1 ? ?
n
]?
|P |
?
i=1
?
i
?
i
?
|US|
?
j=1
?
j
?
j
?
|US|
?
m=1
?
m
?
m
?
|NS|
?
n=1
?
n
?
n
(11)
Here, the ? and ? are Lagrange multipliers. To
find the dual form of the problem, we need to first
minimize L(w, b, ?, ?, ?) with respect to w and b,
we will do by setting the derivatives of L with re-
spect to w and b to zero, we have:
?L(w, b, ?, ?, ?)
?w
= w ?
|P |
?
i=1
?
i
y
(i)
x
(i)
?
|US|
?
j=1
?
j
y
(j)
x
(j)
?
|US|
?
m=1
?
m
y
(m)
x
(m)
?
|NS|
?
n=1
?
n
y
(n)
?
x
(n)
= 0
(12)
This implies that
w =
|P |
?
i=1
?
i
y
(i)
x
(i)
+
|US|
?
j=1
?
j
y
(j)
x
(j)
+
|US|
?
m=1
?
m
?
y
(m)
x
(m)
+
|NS|
?
n=1
?
n
y
(n)
x
(n)
(13)
Here, to simplify the presentation, we redefine
some notations in the following:
T
+
= P ? US, T
?
= US ?NS, T
?
= T
+
? T
?
C
+
i
=
{
C
1
, x
(i)
? P
C
2
p
+
x
(j)
, x
(j)
? US
C
?
j
=
{
C
3
p
?
x
(m)
, x
(m)
? US
C
4
, x
(n)
? NS
so we obtain
w =
|T
?
|
?
i=1
?
i
y
(i)
x
(i)
(14)
As for the derivative with respect to b, we obtain
?L(w, b, ?, ?, ?)
?b
= ?
|P |
?
i=1
?
i
y
(i)
?
|US|
?
j=1
?
j
y
(j)
?
|US|
?
m=1
?
m
y
(m)
?
|NS|
?
n=1
?
n
y
(n)
= 0
(15)
We get:
|T
?
|
?
i=1
?
i
y
(i)
= 0 (16)
497
If we take Equation (14) and (16) back into the
Lagrangian function (Equation 11), and simplify,
we get
L(w, b, ?, ?, ?) =
|T
?
|
?
i=1
?
i
?
1
2
|T
?
|
?
i,j=1
y
(i)
y
(j)
?
i
?
?
j
< x
(i)
, x
(j)
>
(17)
To the primal optimization formula (10), we can
obtain the following dual optimization problem:
max W (?) =
|T
?
|
?
i=1
?
i
?
1
2
|T
?
|
?
i=1,j=1
?
i
?
j
y
(i)
?
y
(j)
< x
(i)
, x
(j)
>
s.t. C
+
i
? ?
i
? 0, x
(i)
? T
+
C
?
j
? ?
j
? 0, x
(j)
? T
?
|T
+
|
?
i=1
?
i
?
|T
?
|
?
j=1
?
j
= 0
(18)
where < x
(i)
, x
(j)
> is the inner product of x
(i)
and x
(j)
, we can replace them by using kernel
function ?(x
(i)
) and ?(x
(j)
), respectively.
498
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 113?118
Manchester, August 2008
Automatic Chinese Catchword Extraction 
Based on Time Series Analysis 
Han Ren1, Donghong Ji1, Jing Wan2 and Lei Han1
1 School of Computer Science, Wuhan University 430079, China 
2 Center for Study of Language & Information, Wuhan University 430072, China 
cslotus@mail.whu.edu.cn, donghong_ji@yahoo.com, 
jennifer.wanj@gmail.com, hattason@mail.whu.edu.cn 
 
Abstract 
Catchwords refer to those popular words 
or phrases in a time period. In this paper, 
we propose a novel approach for 
automatic extraction of Chinese 
catchwords. By analyzing features of 
catchwords, we define three aspects to 
describe Popular Degree of catchwords. 
Then we use curve fitting in Time Series 
Analysis to build Popular Degree Curves 
of the extracted terms. Finally we give a 
formula that can calculate Popular 
Degree values of catchwords and get a 
ranking list of catchword candidates. 
Experiments show that the method is 
effective. 
1 Introduction 
Generally, a catchword is a term which 
represents a hot social phenomenon or an 
important incident, and is paid attention by 
public society within certain time period. On the 
one hand, catchwords represent the mass value 
orientation for a period. On the other hand, they 
have a high timeliness. Currently, there are quiet 
a few ranking and evaluations of catchwords 
every year in various kinds of media. Only in 
year 2005, tens of Chinese organizations 
published their ranking list of Chinese 
catchwords.  
Catchwords contain a great deal of 
information from any particular area, and such 
words truly and vividly reflect changes of our 
lives and our society. By monitoring and analysis 
of catchwords, we can learn the change of public 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
attention in time. In addition, we may detect the 
potential changes of some linguistic rules, which 
can help establish and adjust state language 
policies. 
Currently, two kinds of approaches are 
adopted to evaluate catchwords. One is by CTR 
(Click-Through Rate) or retrieval times, but the 
limitation is that it is just based on frequency, 
which is only one feature of catchwords. The 
other is by manual evaluation, but it depends on 
their subjective judgment to a large extent. In this 
paper, we propose a novel approach that can 
automatically analyze and extract Chinese 
catchwords. By analyzing sample catchwords 
and finding out their common features, we 
provide a method to evaluate the popular degree. 
After ranking, terms that have high values are 
picked out as catchword candidates.  
The rest of the paper is organized as follows. 
In Section 2, we discuss about the linguistic basis 
of catchword judgment. In Section 3, we describe 
the extraction method in detail. In Section 4, we 
present the experimental results as well as some 
discussions. Finally, we give the conclusion and 
future work in Section 5. 
2 Linguistic basis 
The popularity of a word or phrase contains two 
factors: time and area, namely how long it lasts 
and how far it spreads. But neither of them have 
definite criterion. 
2.1 Linguistic definition of catchword 
Many researches of catchwords come from pure 
linguistic areas. Wang (1997) proposed that 
catchwords, which include words, phrases, 
sentences or special patterns, are a language form 
in certain times and among certain groups or 
communities. Guo (1999) specified that 
catchwords are popular words, which are widely 
113
used in certain period of time among certain 
groups of people. To sum up, catchwords are a 
language form spreading quickly within certain 
area in certain period of time. 
According to Zipf?s Law (Zipf, 1949), the 
word that has a higher usage frequency is shorter 
than others. Catchwords also follow this 
principle: most catchwords are words and 
phrases instead of sentences and longer language 
units, which are more difficult to extract 
automatically. In the paper, we focus on 
catchwords as words and phrases. 
2.2 Features of catchword 
Some features of catchwords have been proposed, 
but there have been few research to quantify and 
weigh the features. Zhang (1999) proposed a 
method to judge catchwords by weighing 
Circulating Degree of catchwords, which are 
based on Dynamic Circulating Corpus. But the 
corpus construction and the judgment still 
depend on manual efforts. 
By analyzing usage frequency of catchwords, 
we find that being a language phenomenon 
within a period of time, a catchword has two 
features: one is high usage frequency, namely a 
catchword is frequently used in certain period of 
time; the other is timeliness, namely this 
situation will lasts for some time. Our 
quantification method is based on these features. 
3 Extraction Method 
In this section, the extraction method is described 
in detail. After term extraction, the features of 
terms are weighed by time series analysis. The 
algorithm in section 3.4 shows the process to 
extract catchword candidates. 
3.1 Term Extraction 
Catchwords are words or phrases with maximal 
meanings, most of which are multi-character 
words or phrases. Word segmentation has a low 
discrimination for long phrases, while term 
extraction has a better way to extract them. 
Zhang (2006) proposed a new ATE algorithm, 
which is based on the decomposition of prime 
string. The algorithm evaluates the probability of 
a long string to be a term by weighing relation 
degree among sub-strings within the long string. 
The algorithm can raise the precision in 
extracting multi-character words and long 
phrases. In this paper, we use this method to 
extract terms. 
3.2 Popular Degree Curve 
For extracted terms, a time granularity should be 
defined to describe their features. We select 
?day? as the time granularity and get every day?s 
usage frequency for each term in one year. These 
can be described as a time series like below: 
{ }1 2, ,..., ,...,w w w wt wC c c c c= n           (1) 
wC  is the time series of term w.  is the 
usage frequency of term w in the day t. n is the 
number of observation days.  
wtc
As a latent knowledge, two features of 
catchwords mentioned in section 2.2 exist in 
their time series. The effective method to find out 
the latent knowledge in the time series is Time 
Series Analysis, which includes linear analysis 
and nonlinear analysis. As the time series of 
terms belong to nonlinear series, we use 
nonlinear analysis to deal with them. 
After getting usage frequency, we use SMA 
(Simple Moving Average) method to eliminate 
the random fluctuation of series . The 
formula is as follows: 
wC
( )
1
m
w t m j
j
wt
c
c
m
? +
==
?
                   (2) 
wtc  is the smoothed usage frequency of term 
w in the day t and m is the interval. In SMA 
method, a short interval has a little effect, while a 
long one may result in low accuracy. So we 
should specify a proper interval. Through 
experiments we find that an appropriate interval 
is between 10 and 20. Smoothed time series is as 
follows: 
{ }1 2, ,..., ,...,w w w wt wC c c c c= n          (3) 
Smoothed time series of terms can be 
described as curves, in which the coordinate x is 
day t and coordinate y is wtc . Through these 
curves we can see that, catchwords appear in 
certain period of time and its usage frequency 
increases in this period. After reaching the 
highest point, usage frequency of catchwords 
decrease slowly. We call this process Popular 
Degree, which contains three aspects: 
1) Popular Trend: the increasing process of 
usage frequency; the more obviously the popular 
trend changes, the higher the popular degree is. 
2) Peak Value: maximum usage frequency 
within certain period of time; the larger the peak 
value is, the higher the popular degree is. 
114
3) Popular Keeping: the decreasing process of 
usage frequency; the more gently the popular 
keeping changes, the higher the popular degree is. 
Three aspects above determine popular degree 
of catchwords. Figure 1 shows the smoothed 
time series curve of the catchword ???? 2 ?  
evaluated in year 2005: 
 
Figure 1. Smoothed time series curve of 
the catchword ????? 
To the catchword ?????, its Popular Trend 
changes obviously and its Popular Keeping 
changes gently. Meanwhile, its Peak Value is 
relatively higher than those of most catchwords. 
So the catchword ????? has a high Popular 
Degree. 
According to three aspects of Popular Degree, 
smoothed time series curve is separated into two 
parts: one is ascending period, namely Popular 
Trend process; the other is descending period, 
namely Popular Keeping process. We use conic 
fitting to deal with two parts of series. A conic?s 
formula is like below: 
2Y a bt ct= + +                             
According to least square method, a standard 
equation that can deduce three parameters a, b 
and c is as follows: 
2
2
2 2 3
 
 
 
Y na b t c t
tY a t b t c t
t Y a t b t c t
? = + +?? = + +?? = + +??
? ? ?
? ? ? ?
? ? ? ?
3
4
M
              
Assume TS is the starting time, TE is the ending 
time, and TM is the time that time series curve 
reaches the highest point. According to conic 
fitting method we can get curves of ascending 
and descending period. Formulas of two conics 
are as follows: 
2
2
( )           
( )        
S
M E
u a bu cu T t T
v a b v c v T t T
?
?
? = + + ? ??? ? ? ?= + + ? ???
     (4)  
                                                 
2 ??? means Sudan red in English. 
Variable u and v are usage frequency of a term 
in a day, ( )u?  is the formula of ascending curve, 
and ( )v?  is the formula of descending curve. The 
curve described by equation (4) is called Popular 
Degree Curve. Figure 2 shows the Popular 
Degree Curve of the catchword ?????: 
 Figure 2. Popular Degree Curve of 
the catchword ????? 
3.3 Popular Degree Value 
The decision of catchwords is based on three 
aspects of Popular Degree described in section 
3.2. We propose a formula to calculate Popular 
Degree values of terms. After getting the values, 
a ranking list by inverse order is established. The 
Popular Degree of a catchword is in the direct 
ratio to its place in the ranking list. The formula 
is as follows: 
( ) ( ) ( ) ( )PD w PT w PV w PK w= ? ?        (5) 
PD(w) is the Popular Degree value of the 
catchword w. PT(w) is the Popular Trend value 
of w: 
( ) ( )
( )
( )
M S
M
T T
PT w
T
? ?? ?
?= i                  (6) 
? is the adjusting parameter of Popular Trend. 
The formula indicates that PT(w) is related to 
changing process of Popular Degree Curve. 
PV(w) is the Peak Value of w: 
{ }
{ } { }
max
( )
1
max max
wt
wt wt
ww
c
PV w
c c
N
?=
+?i
 (7) 
? is the adjusting parameter of Peak Value. 
The formula indicates that PV(w) is related to the 
maximum usage frequency of w. PK(w) is the 
Popular Keeping value of w: 
( ) ( )
( ) 1
( )
M E
M
T T
PK w
T
? ?? ?
? ??= ?? ?? ?
i            (8) 
? is the adjusting parameter of Popular 
Keeping. The formula indicates that PK(w) is 
related to changing process of Popular Degree 
Curve. Parameter ?, ? and ? control proportion 
of three aspects in Popular Degree value. 
115
All extracted terms are ranked according to 
their Popular Degree values. Terms that have 
high scores are picked out as catchword 
candidates. 
3.4 Algorithm 
The algorithm of automatic catchwords 
extraction is described below: 
 
Algorithm Extracting catchwords 
Input text collections 
Output ranking list of catchword candidates 
Method 
1) use ATE algorithm mentioned in section 3.1 to 
extract terms 
2) filter terms that contains numbers and 
punctuations 
3) foreach term 
4)   calculate its smoothed time series by formula 
(2) 
5)   use conic fitting method in section 3.2 to get 
its Popular Degree Curve like equation (4) 
6)   use formula (5) ~ (8) to calculate its Popular 
Degree value 
7) rank all Popular Degree values from high to 
low 
4 Experimental Results and Analysis 
4.1 Text Collection 
In the experiment, we use 136,191 web pages 
crawled from Sina3?s news reports in year 2005 
including six categories: economy, science, 
current affairs, military, sports and entertainment. 
For the experimental purpose, we extract body 
content in every web page by using Noise 
Reducing algorithm (Shianhua Lin & Janming 
Ho, 2002). Totally, the extracted subset includes 
129,328 documents. 
4.2 Experiment settings 
In the experiment, several parameters should be 
settled to perform the catchwords extraction. 
?n  
A large time granularity may result in low 
accuracy for conic fitting. In this paper, we 
select ?day? as the time granularity. 
?m 
For the interval m in formula (2), a proper 
value should be specified to not only 
eliminate random fluctuation but also keep 
                                                 
3 http://www.sina.com.cn/ 
accuracy of data. In the experiment we find 
that the proper interval is between 10 and 20.  
?TS and TE
Catchwords have a high timeliness, so we 
should specify a time domain. By analysis of 
sample catchwords, we find that popular 
time domain for most of them approximately 
last for not more than 6 months. So we 
specify the time domain is n / 2. Thus the 
relationship among the starting time TS and 
the ending time TE is below: 
2S E
n
T T= ?  
As a proper example, the starting point can 
be 60 days away from the highest point. 
Thus the Popular Trend process and the 
Popular Keeping process both last for nearly 
3 months. So the relationship can be 
described as formulas below: 
4S M
n
T T ? ?= ? ? ?? ?
,  
4E M
n
T T ? ?= + ? ?? ?
                
??, ?, ? 
To keep the Popular Degree values of 
catchwords within [0, 1], three adjusting 
parameters are satisfied to the inequation: 
0 , , 1? ? ?< ? . 
Table 1 shows proper values of parameters as 
schema 1. We also give other schemas, which 
contain different values of parameters, to 
compare with the schema 1. In schema 2 to 
schema 4, default values of parameters are the 
same with schema 1. 
 
parameter Value 
n 365 
t [1, 365] 
m 15 
TS TM ? ? n / 4? 
TE TM + ? n / 4? 
? 1 
? 1 
? 1 
Table 1. parameters in schema 1 
 
schema 2: different m values 
schema 3: different values of TS and TE
schema 4: different values of ?, ? and ? 
4.3 Evaluation Measure 
Currently, there is no unified standard for 
catchword evaluation. In year 2005, NLRMRC 
116
(National Language Resources Monitoring and 
Research Centre, held by MOE of China) had 
published their top 100 Chinese catchwords. We 
use co-occurrence ratio of catchwords for the 
evaluation. The formula of co-occurrence ratio is 
as follows: 
CNr
N
=  
N is the number of ranking catchwords. NC is 
the co-occurrence of catchwords, namely the 
number of catchwords which appear both in our 
approach and NLRMRC in top N. 
4.4 Results 
We use algorithm described in section 3.4 to get 
a ranking list of catchword candidates. 
According to ATE algorithm mentioned in 
section 3.1, we extract 966,532 terms. After 
filtering invalid terms we get 892,184 terms and 
calculate each term?s Popular Degree value. 
Table 2 - 5 shows the co-occurrence ratio with 
schema 1 - 4. 
N=20 N=40 N=60 N=80 N=100
7% 18% 36% 53% 66%
Table 2. Co-occurrence ratio using schema 1 
m N=20 N=40 N=60 N=80 N=100
5 3% 7% 16% 29% 45%
10 4% 11% 25% 44% 59%
20 7% 15% 32% 49% 63%
25 6% 14% 29% 46% 60%
Table 3. Co-occurrence ratio using schema 2 
TM - TS : 
TE - TM
N=20 N=40 N=60 N=80 N=100
1 : 4 0% 3% 8% 15% 22%
2 : 3 4% 14% 30% 49% 64%
3 : 2 5% 15% 33% 51% 63%
4 : 1 2% 5% 12% 21% 26%
Table 4. Co-occurrence ratio using schema 3 
 N=20 N=40 N=60 N=80 N=100
?=0.5 3% 9% 24% 42% 55%
?=0.8 6% 15% 31% 50% 64%
?=0.5 2% 6% 16% 37% 52%
?=0.8 5% 13% 29% 47% 59%
?=0.5 3% 11% 26% 43% 57%
?=0.8 6% 15% 32% 51% 62%
Table 5. Co-occurrence ratio using schema 4 
Table 2 shows the co-occurrence ratio of the 
catchwords extracted by our approach and 
NLRMRC in top N catchwords ranking list. It 
indicates that, when N is 100, co-occurrence of 
the catchwords reaches 66%; when N is lower, 
the ratio is also lower. On the one hand, we can 
see that our approach has a good effect on 
automatically extracting catchwords, closing to 
the result of manual evaluation with the 
increment of N. On the other hand, it proves that 
divergence exists between our approach and 
manual evaluation in high-ranking catchwords. 
Table 3 indicates that, the condition of m = 20 
has a better co-occurrence ratio in contrast with 
others in schema 2. It is because a short interval 
has a little effect, while a long one may result in 
low accuracy in SMA. 
Table 4 indicates that a better performance can 
be made when the proportion of TM - TS and TE - 
TM is close to 1:1. It proves that Popular Trend 
process is just as important as Popular Keeping 
process. Therefore the best time domain of these 
two processes are both n / 4.  
Three parameters can adjust the weights of PD, 
PV and PK in formula (5). Table 5 indicates that 
three factors above are all important for weighing 
a catchword, while ?  is a little more important 
than ? and ?. Therefore, maximum usage 
frequency of a catchword is a little more 
important than two other factors. 
From Table 2 ? 5 we can see that, parameters 
in schema 1 is most appropriate for the 
evaluation. 
Table 6 shows the ranking list of top 10 
catchword candidates according to their Popular 
Degree values: 
candidates4 PD value 
??? 0.251262 
???? 0.220975 
?? 0.213843 
????? 0.196326 
TD-SCDMA 0.185691 
???? 0.166730 
??? 0.154803 
??? 0.137211 
???? 0.121738 
???? 0.120667 
Table 6. Popular Degree values of Top 10 
catchword candidates 
                                                 
4  ???? means a talent show by Hunan Satellite. 
?? means petroleum price 
????? means textile negotiation 
???? means a famous girl called sister lotus 
??? means STS Discovery OV-103 
??? means a billiards player named Junhui Ding 
???? means Six-Party Talks 
???? means swine streptococcus suis 
117
4.5 Analysis 
In our experiment, Popular Values of some 
catchwords by manual evaluation are lower. By 
analyzing their time series curves, we find that 
usage frequencies of these terms are not high. 
We also find that these catchwords mostly have 
other expressions. Such as the catchword ???
???? 5 ? can be also called ????? 6 ?. 
These two synonyms are treated as one term in 
manual evaluation that corresponds to promote 
usage frequency. However, relationship between 
the two synonyms is not concerned in automatic 
extraction. They are treated as separate terms. So 
the Popular Degree Values of these two 
synonyms are not high either. It proves that parts 
of catchwords by manual evaluation are collected 
and generalized. A catchword should be treated 
not only as a separate word or a phrase, but also 
as a part of a word-cluster, which consist of 
synonymous words or phrases. Through word 
clustering method, we can get an increasing 
quantity of the co-occurrence of catchwords 
between our approach and manual evaluations. 
5 Conclusions 
Being as one aspect of dynamic language 
research, catchwords have a far-reaching 
significance for the development of linguistics. 
The paper proposes an approach that can 
automatically detect and extract catchwords. By 
analyzing evaluated catchwords and finding out 
their common feature called popular degree, the 
paper provides a method of popular degree 
quantification and gives a formula to calculate 
term?s popular degree value. After ranking, terms 
that have high values are picked out as 
catchword candidates. The result can be provided 
as a reference for catchword evaluation. 
Experiments show that automatic catchword 
extraction can promote the precision and 
objectivity, and mostly lighten difficulties and 
workload of evaluation. 
In the experiment, we also find that some 
catchwords are not isolated, but have a strong 
relationship and express the same meaning. In 
the future, we can unite all synonymous 
catchwords to a word cluster and calculate the 
cluster?s popular degree value. Thus we would 
be able to achieve a better performance for 
extraction. 
                                                 
5 ?????? means social security system 
6 ???? is the abbreviation of ?????? 
Acknowledgement 
This work is supported by the Natural Science 
Foundation of China under Grant Nos.60773011, 
60703008. 
References 
G.E.P.Box, G.M.Jenkins and G.C.Reinsel. 1994. Time 
Series Analysis, Forecasting and Control. Third 
Edition, Prentice-Hall. 
Richard L. Burden and J.Douglas Faires. 2001. 
Numerical Analysis. Seventh Edition, Brooks/Cole, 
Thomson Learning, Inc., pp. 186-226. 
Xi Guo. 1999. China Society Linguistics. Nanjing : 
Nanjing University Press. 
H. Kantz and T. Schreiber. 1997. Nonlinear Time 
Series Analysis. Cambridge University Press, 1997 
Shianhua Lin, Janming Ho. 2002. Discovering 
informative content blocks from Web documents. In: 
SIGKDD. 
Dechun Wang 1997. Introduction to Linguistics. 
Shanghai: Shanghai Foreign Language Education 
Press. 
George K.Zipf 1949. Human Behavior and Principle 
of Least Effort: an Introduction to Human Ecology. 
Addison Wesley, Cambridge, Massachusetts. 
Pu Zhang 1999. On thinking of language sense and 
Circulating Degree. Beijing: Language Teaching 
and Linguistic Studies, (1). 
Yong Zhang 2006. Automatic Chinese Term 
Extraction Based on Decomposition of Prime 
String. Beijing: Computer Engineering, (23). 
 
118
