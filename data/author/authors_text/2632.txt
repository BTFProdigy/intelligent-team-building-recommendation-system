Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 102?111, Prague, June 2007. c?2007 Association for Computational Linguistics
Incremental generation of plural descriptions: Similarity and partitioning
Albert Gatt and Kees van Deemter
Department of Computing Science
University of Aberdeen
{agatt,kvdeemte}@csd.abdn.ac.uk
Abstract
Approaches to plural reference generation
emphasise descriptive brevity, but often lack
empirical backing. This paper describes
a corpus-based study of plural descrip-
tions, and proposes a psycholinguistically-
motivated algorithm for plural reference
generation. The descriptive strategy is based
on partitioning and incorporates corpus-
derived heuristics. An exhaustive evaluation
shows that the output closely matches hu-
man data.
1 Introduction
Generation of Referring Expressions (GRE) is a
well-studied sub-task of microplanning in Natural
Language Generation. Most algorithms in this area
view GRE as a content determination problem, that
is, their emphasis is on the construction of a se-
mantic representation which is eventually mapped
to a linguistic realisation (i.e. a noun phrase). Con-
tent Determination for GRE starts from a Knowledge
Base (KB) consisting of a set of entities U and a set
of properties P represented as attribute-value pairs,
and searches for a description D ? P which distin-
guishes a referent r ? U from its distractors. Under
this view, reference is mainly about identification of
an entitiy in a given context (represented by the KB),
a well-studied pragmatic function of definite noun
phrases in both the psycholinguistic and the compu-
tational literature (Olson, 1970).
For example, the KB in Table 1 represents 8 en-
tities in a 2D visual domain, each with 6 attributes,
including their location, represented as a combina-
tion of horizontal (X) and vertical (Y) numerical co-
TYPE COLOUR ORIENTATION SIZE X Y
e1 desk red back small 3 1
e2 sofa blue back small 5 2
e3 desk red back large 1 1
e4 desk red front large 2 3
e5 desk blue right large 2 4
e6 sofa red back large 4 1
e7 sofa red front large 3 3
e8 sofa blue back large 3 2
Table 1: A visual domain
ordinates. To refer to an entity an algorithm searches
through values of the different attributes.
GRE has been dominated by Dale and Reiter?s
(1995) Incremental Algorithm (IA), one version
of which, generalised to deal with non-disjunctive
plural references, is shown in Algorithm 1 (van
Deemter, 2002). A non-disjunctive reference to a
set R is possible just in case all the elements of R
can be distinguished using the same attribute-value
pairs. Such a description is equivalent to the logical
conjunction of the properties in question. This al-
gorithm, IAplur, initialises a description D and a set
of distractors C [1.1?1.2], and traverses an ordered
list of properties, called the preference order (PO)
[1.3], which reflects general or domain-specific pref-
Algorithm 1 IAplur(R,U,PO
1: D ? ?
2: C ? U ?R
3: for ?A : v? ? PO do
4: if R ? [[ ?A : v? ]] ? [[ ?A : v? ]]? C 6= ? then
5: D ? D ? {?A : v?}
6: C ? C ? [[ ?A : v? ]]
7: if [[ D ]] = R then
8: return D
9: end if
10: end if
11: end for
12: return D
102
erences for attributes. For instance, with the PO in
the top row of the Table, the algorithm first consid-
ers values of TYPE, then COLOUR, and so on, adding
a property to D if it is true of the intended referents
R, and has some contrastive value, that is, excludes
some distractors [1.4]. The description and the dis-
tractor set C are updated accordingly [1.5?1.6], and
the description returned if it is distinguishing [1.7].
Given R = {e1, e2}, this algorithm would return the
following description:
(1) ?ORIENTATION : back? ? ?SIZE : small?
This description is overspecified, because ORI-
ENTATION is not strictly necessary to distinguish
the referents (?SIZE : small? suffices). Moreover,
the description does not include TYPE, though it
has been argued that this is always required, as it
maps to the head noun of an NP (Dale and Re-
iter, 1995). We will adopt this assumption here, for
reasons explained below. Due to its hillclimbing
nature, the IA avoids combinatorial search, unlike
some predecessors which searched exhaustively for
the briefest possible description of a referent (Dale,
1989), based on a strict interpretation of the Gricean
Maxim of Quantity (Grice, 1975). Given that, un-
der the view proposed by Olson (1970) among oth-
ers, the function of a referential NP is to identify, a
strict Gricean interpretation holds that it should con-
tain no more information than necessary to achieve
this goal.
The Incremental Algorithm constitutes a depar-
ture from this view given that it can overspecify
through its use of a PO. This has been justified
on psycholinguistic grounds. Speakers overspecify
their descriptions because they begin their formula-
tion of a reference without exhaustively scanning a
domain (Pechmann, 1989; Belke and Meyer, 2002).
They prioritise the basic-level category (TYPE) of an
object, and salient, absolute properties like COLOUR
(Pechmann, 1989; Eikmeyer and Ahlse`n, 1996), as
well as locative properties in the vertical dimen-
sion (Arts, 2004). Relative attributes like SIZE
are avoided unless absolutely required for identi-
fication (Belke and Meyer, 2002). This evidence
suggests that speakers conceptualise referents as
gestalts (Pechmann, 1989) whose core is their basic-
level TYPE (Murphy, 2002) and some other salient
attributes like COLOUR. For instance, according to
Schriefers and Pechmann (1988), an NP such as the
large black triangle reflects a conceptualisation of
the referent as a black triangle, of which the SIZE
property is predicated. Thus, the TYPE+COLOUR
combination is not mentally represented as two sep-
arable dimensions.
In what follows, we will sometimes refer to this prin-
ciple as theConceptual Gestalts Principle. Note that
the IA does not fully mirror these human tendencies,
since it only includes preferred attributes in a de-
scription if they remove some distractors given the
current state of the algorithm, whereas psycholin-
guistic research suggests that people include them
irrespective of contrastiveness (but cf. van der Sluis
and Krahmer (2005)).
More recent research on plural GRE has de-
emphasised these issues, especially in case of dis-
junctive plural reference. Disjunction is required
whenever elements of a set of referents R do not
have identical distinguishing properties. For exam-
ple, {e1, e3} can be distinguished by the following
Conjunctive Normal Form (CNF) description1:
(2) ?TYPE : desk??
`
?COLOUR : red?? ?COLOUR : blue?
?
?
`
?ORIENTATION : right? ? ?ORIENTATION : back?
?
Such a description would be returned by a gen-
eralised version of Algorithm 1 proposed by van
Deemter (2002). This generalisation, IAbool (so
called because it handles all Boolean operators, such
as negation and disjunction), first tries to find a non-
disjunctive description using Algorithm 1. Failing
this, it searches through disjunctions of properties
of increasing length, conjoining them to the descrip-
tion. This procedure has three consequences:
1. Efficiency: Searching through disjunctive
combinations results in a combinatorial explo-
sion (van Deemter, 2002).
2. Gestalts and content: The notion of a ?pre-
ferred attribute? is obscured, since it is dif-
ficult to apply the same reasoning that moti-
vated the PO in the IA to combinations like
(COLOUR ? SIZE).
1Note that logical disjunction is usually rendered as linguis-
tic coordination using and. Thus, the table and the desk is the
union of things which are desks or tables.
103
3. Form: Descriptions can become logically very
complex (Gardent, 2002; Horacek, 2004).
Proposals to deal with (3) include Gardent?s
(2002) non-incremental, constraint-based algorithm
to generate the briefest available description of a
set, an approach extended in Gardent et al (2004).
An alternative, by Horacek (2004), combines best-
first search with optimisation to reduce logical com-
plexity. Neither approach benefits from empiri-
cal grounding, and both leave open the question of
whether previous psycholinguistic research on sin-
gular reference is applicable to plurals.
This paper reports a corpus-based analysis of plu-
ral descriptions elicited in well-defined domains, of
which Table 1 is an example. This study falls within
a recent trend in which empirical issues in GRE have
begun to be tackled (Gupta and Stent, 2005; Jordan
andWalker, 2005; Viethen and Dale, 2006). We then
propose an efficient algorithm for the generation of
references to arbitrary sets, which combines corpus-
derived heuristics and a partitioning-based proce-
dure, comparing this to IAbool. Unlike van Deemter
(2002), we only focus on disjunction, leaving nega-
tion aside. Our starting point is the assumption that
plurals, like singulars, evince preferences for certain
attributes as predicted by the Conceptual Gestalts
Principle. Based on previous work in Gestalt per-
ception (Wertheimer, 1938; Rock, 1983), we pro-
pose an extension of this to sets, whereby plural de-
scriptions are preferred if (a) they maximise the sim-
ilarity of their referents, using the same attributes to
describe them as far as possible; (b) prioritise salient
(?preferred?) attributes which are central to the con-
ceptual representation of an object. We address (3)
above by investigating the logical form of plurals in
the corpus. One determinant of logical form is the
basic-level category of objects. For example, to re-
fer to {e1, e2} in the Table, an author has at least the
following options:
(3) (a) the small desk and sofa
(b) the small red desk and the small blue sofa
(c) the small desk and the small blue sofa
(d) the small objects
These descriptions exemplify three possible sources
of variation:
Disjunctive/Non-disjunctive: The last description,
(3d), is non-disjunctive (i.e. it is logically a conjunc-
tion of properties). This, however, is only achiev-
able through the use of a non-basic level value for
the TYPE of the entities (objects). Using the basic-
level would require the disjunction (?TYPE : desk??
?TYPE : sofa?), which is the case in (3a?c). Given
that basic-level categories are preferred on indepen-
dent grounds (Rosch et al, 1976), we would expect
examples like (3d) to be relatively infrequent.
Aggregation: If a description is disjunctive, it may
be aggregated, with properties common to all ob-
jects realised as wide-scope modifiers. For instance,
in (3a), small modifies desk and sofa. By contrast,
(3b) is non-aggregated: small occurs twice (modi-
fying each coordinate in the NP). Non-aggregated,
disjunctive descriptions are logically equivalent to a
partition of a set. For instance, (3c) partitions the
set R = {e1, e2} into {{e1}, {e2}}, describing each
element separately. Descriptions like (3b) are more
overspecified than their aggregated counterparts due
to the repetition of information.
Paralellism/Similarity: Non-aggregated, disjunc-
tive descriptions (partitions) may exhibit semantic
parallelism: In (3b), elements of the partition are
described using exactly the same attributes (that is,
TYPE, COLOUR, and SIZE). This is not the case in
(3c), which does represent a partition but is non-
parallel. Parallel structures maximise the similarity
of elements of a partition, using the same attributes
to describe both. The likelihood of propagation of an
attribute across disjuncts is probably dependent on
its degree of salience or preference (e.g. COLOUR is
expected to be more likely to be found in a parallel
structure than SIZE).
2 The data
The data for our study is a subset of the TUNA Cor-
pus (Gatt et al, 2007), consisting of 900 references
to furniture and household items, collected via a
controlled experiment involving 45 participants. In
addition to their TYPE, objects in the domains have
COLOUR, ORIENTATION and SIZE (see Table 1). For
each subset of these three attributes, there was an
equal number of domains in which the minimally
distinguishing description (MD) consisted of values
of that subset. For example, Table 1 represents a do-
main in which the intended referents, {e1, e2}, can
104
<DESCRIPTION num=?pl?>
<DESCRIPTION num=?sg?>
<ATTRIBUTE name=?size? value=?small?>small</ATTRIBUTE>
<ATTRIBUTE name=?colour? value=?red?>red</ATTRIBUTE>
<ATTRIBUTE name=?type? value=?desk?>desk</ATTRIBUTE>
</DESCRIPTION>
and
<DESCRIPTION num=?sg?>
<ATTRIBUTE name=?size? value=?small?>small</ATTRIBUTE>
<ATTRIBUTE name=?colour? value=?blue?>blue</ATTRIBUTE>
<ATTRIBUTE name=?type? value=?sofa?>sofa</ATTRIBUTE>
</DESCRIPTION>
</DESCRIPTION>
(?SIZE : small? ? ?COLOUR : red? ? ?TYPE : desk?)
?
(?SIZE : small? ? ?COLOUR : blue? ? ?TYPE : sofa?)
Figure 1: Corpus annotation examples
be minimally distinguished using only SIZE2. Thus,
overspecified usage of attributes can be identified
in authors? descriptions. Domain objects were ran-
domly placed in a 3 (row) ? 5 (column) grid, rep-
resented by X and Y in Table 1. These are relevant
for a subset of descriptions which contain locative
expressions.
Corpus descriptions are paired with an explicit
XML domain representation, and annotated with se-
mantic markup which makes clear which attributes
a description contains. This markup abstracts away
from differences in lexicalisation, making it an ideal
resource to evaluate content determination algo-
rithms, because it is semantically transparent, in
the sense of this term used by van Deemter et al
(2006). This markup scheme also enables the com-
positional derivation of a logical form from a natural
language description. For example, the XML repre-
sentation of (3b) is shown in Figure 1, which also
displays the LF derived from it. Each constituent NP
in (3b) is annotated as a set of attributes enclosed by
a DESCRIPTION tag, which is marked up as singular
(sg). The two coordinates are further enclosed in
a plural DESCRIPTION; correspondingly, the LF is a
disjunction of (the LFs of) the two internal descrip-
tions.
Descriptions in the corpus were elicited in 7 do-
mains with one referent, and 13 domains with 2
referents. Plural domains represented levels of a
Value Similarity factor. In 7 Value-Similar (VS)
domains, referents were identifiable using identical
values of the minimally distinguishing attributes. In
the remaining 6 Value-Dissimilar (VDS) domains,
the minimally distinguishing values were different.
Table 1 represents a VS domain, where {e1, e2} can
2TYPE was not included in the calculation of MD.
VS VDS
+Disj ?Disj +Disj ?Disj
+aggr 20.2 15.5 2.4 3.7
?aggr 64.3 ? 93.9 ?
% overall 84.5 15.5 96.3 3.7
Table 2: % disjunctive and non-disjunctive plurals
be minimally distinguished using the same value of
SIZE (small).
In terms of our introductory discussion, referents
in Value-Similar conditions could be minimally dis-
tinguished using a conjunction of properties, while
Value-Dissimilar referents required a disjunction
since, if two referents could be minimally distin-
guished by different values v and v? of an attribute
A, then MD had the form ?A : v? ? ?A : v??. How-
ever, even in the VS condition, referents had differ-
ent basic-level types. Thus, an author faced with a
domain like Table 1 had at least the descriptive op-
tions in (3a?d). If they chose to refer to entities using
basic-level values of TYPE, their description would
be disjunctive (e.g. 3a). A non-disjunctive descrip-
tion would require the use of a superordinate value,
as in (3d).
Our analysis will focus on a stratified random
sample of 180 plural descriptions, referred to as PL1,
generated by taking 4 descriptions from each author
(2 each from VS and VDS conditions). We also use
the singular data (SG; N = 315). The remaining
plural descriptions (PL2; N = 405) are used for
evaluation.
3 The logical form of plurals
Descriptions in PL1 were first classified according to
whether they were non-disjunctive (cf. 3d) or dis-
junctive (3a?c). The latter were further classified
into aggregated (3a) and non-aggregated (3b). Ta-
ble 2 displays the percentage of descriptions in each
of the four categories, within each level of Value
Similarity. Disjunctive descriptions were a major-
ity in either condition, and most of these were non-
aggregated. As noted in ?1, these descriptions cor-
respond to partitions of the set of referents.
Since referents in VS had identical properties ex-
cept for TYPE values, the most likely reason for the
majority of disjunctives in VS is that people?s de-
scriptions represented a partition of a set of refer-
ents induced by the basic-level category of the ob-
105
Non-Parallel Parallel ?2 (p ? .001)
overspec. 24.6 75.4 92.467
underspec. 5.3 94.7 42.217
well-spec. 11 89 26
Table 3: Parallelism: % per description type
jects. This is strengthened by the finding that the
likelihood of a description being disjunctive or non-
disjunctive did not differ as a function of Value Sim-
ilarity (?2 = 2.56, p > .1). A ?2 test on overall fre-
quencies of aggregated versus non-aggregated dis-
junctives showed that the non-aggregated descrip-
tions (?true? partitions) were a significant major-
ity (?2 = 83.63, p < .001). However, the
greater frequency of aggregation in VS compared
to VDS turned out to be significant (?2 = 15.498,
p < .001). Note that the predominance of non-
aggregated descriptions in VS implies that proper-
ties are repeated in two disjuncts (resp. coordinate
NPs), suggesting that authors are likely to redun-
dantly propagate properties across disjuncts. This
evidence goes against some recent proposals for plu-
ral reference generation which emphasise brevity
(Gardent, 2002).
3.1 Conceptual gestalts and similarity
Allowing for the independent motivation for set par-
titioning based on TYPE values, we suggested in ?1
that parallel descriptions such as (3b) may be more
likely than non-parallel ones (3c), since the latter
does not use the same properties to describe the two
referents. Similarity, however, should also interact
with attribute preferences.
For this part of the analysis, we focus exclusively
on the disjunctive descriptions in PL1 (N = 150) in
both VS and VDS. The descriptions were categorised
according to whether they had parallel or non-
parallel semantic structure. Evidence for Similarity
interacting with attribute preferences is strongest if
it is found in those cases where an attribute is over-
specified (i.e. used when not required for a distin-
guishing description). In those cases where corpus
descriptions do not contain locative expressions (the
X and/or Y attributes), such an overspecified usage
is straightforwardly identified based on the MD of
a domain. This is less straightforward in the case of
locatives, since the position of objects was randomly
determined in each domain. Therefore, we divided
Actual Predicted
p(A, SG) p(A, PPS) p(A, PPS)
COLOUR .680 .835 .61
SIZE .290 .359 .28
ORIENTATION .280 .269 .26
X-DIMENSION .440 .517 .52
Y-DIMENSION .630 .647 .65
Table 4: Actual and predicted usage probabilities
descriptions into three classes, whereby a descrip-
tion is considered to be:
1. underspecified if it does not include a locative
expression and omits some MD attributes;
2. overspecified if either (a) it does not omit any
MD attributes, but includes locatives and/or
non-required visual attributes; or (b) it omits
some MD attributes, but includes both a locative
expression and other, non-required attributes;
3. well-specified otherwise.
Proportions of Parallel and Non-Parallel descrip-
tions for each of the three classes are are shown
in Table 3. In all three description types, there is
an overwhelming majority of Parallel descriptions,
confirmed by a ?2 analysis. The difference in pro-
portions of description types did not differ between
VS and VDS (?2 < 1, p > .8), suggesting that the
tendency to redundantly repeat attributes, avoiding
aggregation, is independent of whether elements of
a set can be minimally distinguished using identical
values.
Our second prediction was that the likelihood
with which an attribute is used in a parallel structure
is a function of its overall ?preference?. Thus, we
expect attributes such as COLOUR to feature more
than once (perhaps redundantly) in a parallel de-
scription to a greater extent than SIZE. To test this,
we used the SG sample, estimating the overall prob-
ability of occurrence of a given attribute in a singu-
lar description (denoted p(A, SG)), and using this in
a non-linear regression model to predict the likeli-
hood of usage of an attribute in a plural partitioned
description with parallel semantic structure (denoted
p(A, PPS)). The data was fitted to a regression equa-
tion of the form p(A, PPS) = k? p(A, SG)S . The re-
sulting equation, shown in (4), had a near-perfect fit
106
to the data (R2 = .910)3. This is confirmed by com-
paring actual probability of occurrence in the second
column of Table 4, to the predicted probabilities in
the third column, which are estimated from singular
probabilities using (4).
p(A, PPS) = .713 p(A, SG).912 (4)
Note that the probabilities in the Table con-
firm previous psycholinguistic findings. To the ex-
tent that probability of occurrence reflects salience
and/or conceptual importance, an order over the
three attributes COLOUR, SIZE and ORIENTATION
can be deduced (C>>O>>S), which is compatible
with the findings of Pechmann (1989), Belke and
Meyer (2002) and others. The locative attributes
are also ordered (Y>>X), confirming the findings
of Arts (2004) that vertical location is preferred. Or-
derings deducible from the SG data in turn are ex-
cellent predictors of the likelihood of ?propagating?
an attribute across disjuncts in a plural description,
something which is likely even if an attribute is re-
dundant, modulo the centrality or salience of the at-
tribute in the mental gestalt corresponding to the set.
Together with the earlier findings on logical form,
the data evinces a dual strategy whereby (a) sets
are partitioned based on basic-level conceptual cat-
egory; (b) elements of the partitions are described
using the same attributes if they are easily perceived
and conceptualised. Thus, of the descriptions in (3)
above, it is (3b) that is the norm among authors.
4 Content determination by partitioning
In this section we describe IApart, a partitioning-
based content determination algorithm. Though pre-
sented as a version of the IA, the basic strategy is
generalisable beyond it. For our purposes, the as-
sumption of a preference order will be maintained.
IApart is distinguished from the original IA and
IAbool (cf. ?1) in two respects. First, it induces par-
titions opportunistically based on KB information,
and this is is reflected in the way descriptions are
represented. Second,, the criteria whereby a prop-
erty is added to a description include a consideration
of the overall salience or preference of an attribute,
and its contribution to the conceptual cohesiveness
3A similar analysis using linear regression gave essentially
the same results.
of the description. Throughout the following discus-
sion, we maintain a running example from Table 1,
in which R = {e1, e2, e5}.
4.1 Partitioned descriptions
IApart generates a partitioned description (Dpart) of
a set R, corresponding to a formula in Disjunctive
Normal Form. Dpart is a set of Description Frag-
ments (DFs). A DF is a triple ?RDF, TDF,MDF?, where
RDF ? R, TDF is a value of TYPE, and MDF is a pos-
sibly empty set of other properties. DFs refer to dis-
joint subsets of R. As the representation suggests,
TYPE is given a special status. IApart starts by se-
lecting the basic-level values of TYPE, partitioning
R and creating a DF for each element of the partition
on this basis. In our example, the selection of TYPE
results in two DFs, with MDF initialised to empty:
(5) DF1
?
{e1, e5}, ?TYPE : desk?, ?
?
DF2
?
{e2}, ?TYPE : sofa?, ?
?
Although neither DF is distinguishing, RDF indicates
which referents a fragment is intended to identify.
In this way, the algorithm incorporates a ?divide-
and-conquer? strategy, splitting up the referential in-
tention into ?sub-intentions? to refer to elements of
a partition. Following the initial step of selecting
TYPE, the algorithm considers other properties in
PO. Suppose ?COLOUR : blue? is considered first.
This property is true of e2 and e5. Since DF2 refers to
e2, the new property can be added to MDF2 . Since e5
is not the sole referent of DF1, the property induces
a further partitioning of this fragment, resulting in a
new DF. This is identical to DF1 except that it refers
only to e5 and contains ?COLOUR : blue?. DF1 it-
self now refers only to e1. Once ?COLOUR : red? is
considered, it is added to the latter, yielding (6).
(6) DF1
?
{e1}, ?TYPE : desk?, {?COLOUR : red?}
?
DF2
?
{e2}, ?TYPE : sofa?, {?COLOUR : blue?}
?
DF3
?
{e5}, ?TYPE : desk?, {?COLOUR : blue?}
?
The procedure updateDescription, which cre-
ates and updates DFs, is formalised in Algorithm 2.
When some property ?A : v? is found to be ?use-
ful? in relation to R (in a sense to be made precise),
this function is called with two arguments: ?A : v?
itself, and R? = [[ ?A : v? ]] ? R, the referents of
which ?A : v? is true. The procedure iterates through
107
Algorithm 2 updateDescription(?A : v?, R?)
1: for ?RDF, TDF,MDF? ? Dpart do
2: if R? = ? then
3: return
4: else if RDF ? R? then
5: MDF ?MDF ?
?
?A : v?
?
6: R? ? R? ?RDF
7: else if RDF ?R? 6= ? then
8: Rnew ? RDF ?R?
9: DFnew ?
?
Rnew, TDF,MDF ? {?A : v?}
?
10: Dpart ? Dpart ?
?
DFnew
?
11: RDF ? RDF ?Rnew
12: R? ? R? ?Rnew
13: end if
14: end for
15: if A = TYPE then
16: Dpart ? Dpart ?
??
R?, ?A : v?, ?
??
17: else
18: Dpart ? Dpart ?
?
?R?,?, {?A : v?}?
?
19: end if
the DFs in Dpart, adding the property to any DF such
thatRDF?R? 6= ?, untilR? is empty and all referents
in it have been accounted for [2.2]. As indicated in
the informal discussion, there are two cases to con-
sider for each DF:
1. RDF ? R? [2.4]. This corresponds to our exam-
ple involving ?COLOUR : blue? and DF2. The
property is simply added to MDF [2.5] and R?
is updated by removing the elements thus ac-
counted for [2.6].
2. Suppose RDF 6? R?. If RDF ? R? is empty, then
?A : v? is not useful. Suppose on the other hand
that RDF ? R? 6= ? [2.7]. This occurred with
?COLOUR : red? in relation to DF1. The proce-
dure initialises Rnew, a set holding those refer-
ents in RDF which are also in R? [2.8]. A new
DF (DFnew) is created, which is a copy of the
old DF, except that (a) it contains the new prop-
erty; and (b) its intended referents are Rnew
[2.9]. The new DF is included in the description
[2.10], while the old DF is altered by removing
Rnew from RDF [2.11]. This ensures that DFs
denote disjoint subsets of R.
Two special cases arise when Dpart is empty, or
there are some elements of R? for which no DF ex-
ists. Both cases result in the construction of a new
DF. An example of the former case is the initial state
of the algorithm, when TYPE is added. As in exam-
ple (5), the TYPE results in a new DF [2.16]. If a
property is not a TYPE, the new DF has T set to null
(?) and the property is included in M [2.18]4. Note
that this procedure easily generalises to the singular
case, where Dpart would only contain one DF.
4.2 Property selection criteria
IApart?s content determination strategy maximises
the similarity of a set by generating semantically
parallel structures. Though contrastiveness plays a
role in property selection, the ?preference? or con-
ceptual salience of an attribute is also considered in
the decision to propagate it across DFs.
Candidate properties for addition need only be
true of at least one element of R. Because of the
partitioning strategy, properties are not equally con-
strastive for all referents. For instance, in (5), e2
needs to be distinguished from the other sofas in Ta-
ble 1, while {e1, e5} need to be distinguished from
the desks. Therefore, distractors are held in an as-
sociative array C, such that for all r ? R, C[r] is
the set of distractors for that referent at a given stage
in the procedure. Contrastiveness is defined via the
following Boolean function:
contrastive(?A : v?, R) ?
?r ? R : C[r] ? [[ ?A : v? ]] 6= ? (7)
We turn next to salience and similarity. Let
A(Dpart) be the set of attributes included in Dpart.
A property is salient with respect to Dpart if it satis-
fies the following:
salient(?A : v?, Dpart) ?
A ? A(Dpart) ? (.713 p(A, SG)
.912 > 0.5) (8)
that is, the attribute is already included in the de-
scription, and the predicted probability of its be-
ing propagated in more than one fragment of a de-
scription is greater than chance. A potential prob-
lem arises here. Consider the description in (5)
once more. At this stage, IApart begins to consider
COLOUR. The value red is true of e1, but non-
contrastive (all the desks which are not inR are red).
If this is the first value of COLOUR considered, (8)
returns false because the attribute has not been
used in any part of the description. On later con-
sidering ?COLOUR : blue?, the algorithm adds it to
4This only occurs if the KB is incomplete, that is, there some
entities have no TYPE, so that R is not fully covered by the
intended referents of the DFs when TYPE is initially added.
108
Dpart, since it is contrastive for {e2, e5}, but will
have failed to propagate COLOUR across fragments.
As a result, IApart considers values of an attribute in
order of discriminatory power (Dale, 1989), defined
in the present context as follows:
|[[ ?A : v? ]] ?R| + |[[ ?A : v? ]] ? (U ?R)|
|[[ ?A : v? ]]|
(9)
Discriminatory power depends on the number of ref-
erents a property includes in its extension, and the
number of distractors (U?R) it removes. By priori-
tising discriminatory values, the algorithm first con-
siders and adds ?COLOUR : blue?, and subsequently
will include red because (8) returns true.
To continue with the example, at the stage repre-
sented by (6), only e5 has been distinguished. ORI-
ENTATION, the next attribute considered, is not con-
trastive for any referent. On considering SIZE, small
is found to be contrastive for e1 and e2, and added to
DF1 and DF2. However, SIZE is not added to DF3, in
spite of being present in two other fragments. This
is because the probability function p(SIZE, PPS) re-
turns a value below 0.5 (see Table 4, reflecting the
relatively low conceptual salience of this attribute.
The final description is the blue desk, the small red
desk and the small blue sofa. This example illus-
trates the limits set on semantic parallelism and sim-
ilarity: only attributes which are salient enough are
redundantly propagated across DFs.
4.3 Complexity
An estimate of the complexity of IApart must ac-
count for the way properties are selected (?4.2) and
the way descriptions are updated (Algorithm 2).
Property selection involves checking properties
for contrastive value and salience, and updating the
ordering of values of each attribute based on dis-
criminatory power (9). Clearly, the number of times
this is carried out is bounded by the number of prop-
erties in the KB, which we denote np. Every time a
property is selected, the discriminatory power of val-
ues changes (since the number of remaining distrac-
tors changes). Now, in the worst case, all np proper-
ties are selected by the algorithm 5. Each time, the
algorithm must compare the remaining properties
5Only unique properties need to be considered, as each prop-
erty is selected at most once, though it can be included in more
than one DF.
Mean Mode PRP
IAbool
+ LOC 7.716 7 .7
? LOC 8.335 7 3.5
IApart
+ LOC 4.345 4 6.8
? LOC 1.93 0 44.7
Table 5: Edit distance scores
pairwise for discriminatory power, a quadratic op-
eration with complexity O(n2p). With respect to the
procedure updateDescription, we need to consider
the number of iterations in the for loop starting at
line [2.1]. This is bounded by nr = |R| (there can be
no more DFs than there are referents). Once again,
if at most np properties are selected, then the algo-
rithm makes at most nr iterations np times, yield-
ing complexity O(npnr). Overall, then, IApart has a
worst-case runtime complexity O(n3pnr).
5 Evaluation
IApart was compared to van Deemter?s IAbool (?1)
against human output in the evaluation sub-corpus
PL2 (N = 405). This was considered an ade-
quate comparison, since IAbool shares with the cur-
rent framework a genetic relationship with the IA.
Other approaches, such as Gardent?s (2002) brevity-
oriented algorithm, would perform poorly on our
data. As shown in ?3, overspecification is extremely
common in plural descriptions, suggesting that such
a strategy is on the wrong track (but see ?6).
IApart and IAbool were each run over the domain
representation paired with each corpus description.
The output logical form was compared to the LF
compiled from the XML representation of an au-
thor?s description (cf. Figure 1). LFs were repre-
sented as and-or trees, and compared using the tree
edit distance algorithm of Shasha and Zhang (1990).
On this measure, a value of 0 indicates identity.
Because only a subset of descriptions con-
tain locative expressions, PL2 was divided into
a +LOC dataset (N = 148) and a ?LOC
dataset (N = 257). The preference orders for
both algorithms were (C>>O>>S) for ?LOC and
(Y>>C>>X>>S>>O) for +LOC. These are sug-
gested by the attribute probabilities in Table 4. Ta-
ble 5 displays the mean Edit score obtained by
each algorithm on the two datasets, the modal (most
frequent) value, and the perfect recall percentage
(PRP), the proportion of Edit scores of 0, indicating
109
perfect agreement with an author.
As the means and modes indicate, IApart outper-
formed IAbool on both datasets, with a consistently
higher PRP (this coincides with the modal score in
the case of ?LOC). Pairwise t?tests showed that
the trends were significant in both +LOC (t(147) =
9.28, p < .001) and ?LOC (t(256) = 10.039,
p < .001).
IAbool has a higher (worse) mean on ?LOC, but a
better PRP than on+LOC. This apparent discrepancy
is partly due to variance in the edit distance scores.
For instance, because the Y attribute was highest in
the preference order for +LOC, there were occasions
when both referents could be identified using the
same value of Y, which was therefore included by
IAbool at first pass, before considering disjunctions.
Since Y was highly preferred by authors (see Table
4), there was higher agreement on these cases, com-
pared to those where the values of Y were different
for the two referents. In the latter case, Y was only
when disjunctions were considered, if at all. The
worse performance of IApart on +LOC is due to a
larger choice of attributes, also resulting in greater
variance, and occasionally incurring higher Edit cost
when the algorithm overspecified more than a hu-
man author. This is a potential shortcoming of the
partitioning strategy outlined here, when it is applied
to more complex domains.
Some example outputs are given below, in a do-
main where COLOUR sufficed to distinguish the ref-
erents, which had different values of this attribute
(i.e. an instance of the VDS condition). The formula
returned by IApart (10a) is identical to the (LF of)
the human-authored description (with Edit score of
0). The output of IAbool is shown in (10b).
(10) (a)
`
fan ? green
?
?
`
sofa ? blue
?
?the green fan and the big sofa?
(b)
`
sofa ? fan
?
? small ? front ?
`
blue ? green
?
?the small, blue and green sofa and fan?
As a result of IAbool?s requiring a property or dis-
junction to be true of the the entire set of refer-
ents, COLOUR is not included until disjunctions are
considered, while values of SIZE and ORIENTATION
are included at first pass. By contrast, IApart in-
cludes COLOUR before any other attribute apart from
TYPE. Though overspecification is common in our
data, IAbool overspecifies with the ?wrong? attributes
(those which are relatively dispreferred). The ratio-
nale in IApart is to overspecify only if a property
will enhance referent similarity, and is sufficiently
salient. As for logical form, the Conjunctive Nor-
mal Form output of IAbool increases the Edit score,
given the larger number of logical operators in (10b)
compared to (10a).
6 Summary and conclusions
This paper presented a study of plural reference,
showing that people (a) partition sets based on the
basic level TYPE or category of their elements and
(b) redundantly propagate attributes across disjuncts
in a description, modulo their salience. Our algo-
rithm partitions a set opportunistically, and incor-
porates a corpus-derived heuristic to estimate the
salience of a property. Evaluation results showed
that these principles are on the right track, with sig-
nificantly better performance over a previous model
(van Deemter, 2002). The partitioning strategy is
related to a proposal by van Deemter and Krah-
mer (2007), which performs exhaustive search for
a partition of a set whose elements can be described
non-disjunctively. Unlike the present approach, this
algorithm is non-incremental and computationally
costly.
IApart initially performs partitioning based on the
basic-level TYPE of objects, in line with the evi-
dence. However, later partitions can be induced by
other properties, possible yielding partitions even
with same-TYPE referents (e.g. the blue chair and
the red chair). Aggregation (the blue and red chairs)
may be desirable in such cases, but limits on syntac-
tic complexity of NPs are bound to play a role (Ho-
racek, 2004). Another possible limitation of IApart
is that, despite strong evidence for overspecifica-
tion, complex domains could yield very lengthy out-
puts. Strategies to avoid them include the utilisation
of other boolean operators like negation (the desks
which are not red) (Horacek, 2004). These issues
are open to future empirical research.
7 Acknowledgements
Thanks to Ehud Reiter and Ielka van der Sluis for
useful comments. This work forms part of the TUNA
project (www.csd.abdn.ac.uk/research/tuna/),
supported by EPSRC grant GR/S13330/01.
110
References
A. Arts. 2004. Overspecification in Instructive Texts.
Ph.D. thesis, Univiersity of Tilburg.
E. Belke and A. Meyer. 2002. Tracking the time course
of multidimensional stimulus discrimination: Analy-
sis of viewing patterns and processing times during
same-different decisions. European Journal of Cog-
nitive Psychology, 14(2):237?266.
R. Dale and E. Reiter. 1995. Computational interpreta-
tion of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(8):233?263.
Robert Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL-89.
H. J. Eikmeyer and E. Ahlse`n. 1996. The cognitive pro-
cess of referring to an object: A comparative study of
german and swedish. In Proceedings of the 16th Scan-
dinavian Conference on Linguistics.
C. Gardent, H. Manue?lian, K. Striegnitz, and M. Amoia.
2004. Generating definite descriptions: Non-
incrementality, inference, and data. In T. Pechman
and C. Habel, editors, Multidisciplinary Approaches
to Language Production. Mouton de Gruyter.
C. Gardent. 2002. Generating minimal definite descrip-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, ACL-
02.
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proceedings
of the 11th European Workshop on Natural Language
Generation, ENLG-07. To appear.
H.P. Grice. 1975. Logic and conversation. In P. Cole and
J.L. Morgan, editors, Syntax and Semantics: Speech
Acts., volume III. Academic Press.
S. Gupta and A. J. Stent. 2005. Automatic evaluation
of referring expression generation using corpora. In
Proceedings of the 1st Workshop on Using Corpora in
NLG, Birmingham, UK.
H. Horacek. 2004. On referring to sets of objects natu-
rally. In Proceedings of the 3rd International Confer-
ence on Natural Language Generation, INLG-04.
P. W. Jordan and M. Walker. 2005. Learning content se-
lection rules for generating object descriptions in di-
alogue. Journal of Artificial Intelligence Research,
24:157?194.
G. L. Murphy. 2002. The big book of concepts. MIT
Press, Cambridge, Ma.
D. R. Olson. 1970. Language and thought: Aspects of a
cognitive theory of semantics. Psychological Review,
77:257?273.
Thomas Pechmann. 1989. Incremental speech pro-
duction and referential overspecification. Linguistics,
27:89?110.
I. Rock. 1983. The Logic of Perception. MIT Press,
Cambridge, Ma.
E. Rosch, C. B. Mervis, W. Gray, D. Johnson, and
P. Boyes-Braem. 1976. Basic objects in natural cat-
egories. Cognitive Psychology, 8:382?439.
H. Schriefers and T. Pechmann. 1988. Incremental pro-
duction of referential noun phrases by human speak-
ers. In M. Zock and G. Sabah, editors, Advances in
Natural Language Generation, volume 1. Pinter, Lon-
don.
D. Shasha and K. Zhang. 1990. Fast algorithms for unit
cost editing distance between trees. Journal of Algo-
rithms, 11:581?621.
K. van Deemter and E. Krahmer. 2007. Graphs and
booleans: On the generation of referring expressions.
In H. Bunt and R. Muskens, editors, Computing Mean-
ing, volume III. Springer, Berlin.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the
generation of referring expressions. In Proceedings
of the 4th International Conference on Natural Lan-
guage Generation (Special Session on Data Sharing
and Evaluation), INLG-06.
K. van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
I. van der Sluis and E. Krahmer. 2005. Towards the gen-
eration of overspecified multimodal referring expres-
sions. In Proceedings of the Symposium on Dialogue
Modelling and Generation, 15th Annual Meeting of
the Society for Text and Discourse, STD-05.
J. Viethen and R. Dale. 2006. Algorithms for generat-
ing referring expressions: Do they do what people do?
In Proceedings of the 4th International Conference on
Natural Language Generation, INLG-06.
M. Wertheimer. 1938. Laws of organization in per-
ceptual forms. In W. Ellis, editor, A Source Book of
Gestalt Psychology.Routledge &Kegan Paul, London.
111
Structuring Knowledge for Reference Generation:
A Clustering Algorithm
Albert Gatt
Department of Computing Science
University of Aberdeen
Scotland, United Kingdom
agatt@csd.abdn.ac.uk
Abstract
This paper discusses two problems that arise
in the Generation of Referring Expressions:
(a) numeric-valued attributes, such as size or
location; (b) perspective-taking in reference.
Both problems, it is argued, can be resolved
if some structure is imposed on the available
knowledge prior to content determination. We
describe a clustering algorithm which is suffi-
ciently general to be applied to these diverse
problems, discuss its application, and evaluate
its performance.
1 Introduction
The problem of Generating Referring Expressions
(GRE) can be summed up as a search for the prop-
erties in a knowledge base (KB) whose combination
uniquely distinguishes a set of referents from their dis-
tractors. The content determination strategy adopted
in such algorithms is usually based on the assump-
tion (made explicit in Reiter (1990)) that the space of
possible descriptions is partially ordered with respect
to some principle(s) which determine their adequacy.
Traditionally, these principles have been defined via
an interpretation of the Gricean maxims (Dale, 1989;
Reiter, 1990; Dale and Reiter, 1995; van Deemter,
2002)1. However, little attention has been paid to con-
textual or intentional influences on attribute selection
(but cf. Jordan and Walker (2000); Krahmer and The-
une (2002)). Furthermore, it is often assumed that
all relevant knowledge about domain objects is repre-
sented in the database in a format (e.g. attribute-value
pairs) that requires no further processing.
This paper is concerned with two scenarios which
raise problems for such an approach to GRE:
1. Real-valued attributes, e.g. size or spatial coor-
dinates, which represent continuous dimensions.
The utility of such attributes depends on whether
a set of referents have values that are ?sufficiently
1For example, the Gricean Brevity maxim (Grice, 1975)
has been interpreted as a directive to find the shortest possible
description for a given referent
close? on the given dimension, and ?sufficiently
distant? from those of their distractors. We dis-
cuss this problem in ?2.
2. Perspective-taking The contextual appropriate-
ness of a description depends on the perspective
being taken in context. For instance, if it is known
of a referent that it is a teacher, and a sportsman, it
is better to talk of the teacher in a context where
another referent has been introduced as the stu-
dent. This is discussed further in ?3.
Our aim is to motivate an approach to GRE where
these problems are solved by pre-processing the infor-
mation in the knowledge base, prior to content deter-
mination. To this end, ?4 describes a clustering algo-
rithm and shows how it can be applied to these different
problems to structure the KB prior to GRE.
2 Numeric values: The case of location
Several types of information about domain entities,
such as gradable properties (van Deemter, 2000) and
physical location, are best captured by real-valued at-
tributes. Here, we focus on the example of location as
an attribute taking a tuple of values which jointly de-
termine the position of an entity.
The ability to distinguish groups is a well-
established feature of the human perceptual appara-
tus (Wertheimer, 1938; Treisman, 1982). Representing
salient groups can facilitate the task of excluding dis-
tractors in the search for a referent. For instance, the
set of referents marked as the intended referential tar-
get in Figure 1 is easily distinguishable as a group and
warrants the use of a spatial description such as the ob-
jects in the top left corner, possibly with a collective
predicate, such as clustered or gathered. In case of
reference to a subset of the marked set, although loca-
tion would be insufficient to distinguish the targets, it
would reduce the distractor set and facilitate reference
resolution2.
In GRE, an approach to spatial reference based
on grouping has been proposed by Funakoshi et al
2Location has been found to significantly facilitate reso-
lution, even when it is logically redundant (Arts, 2004)
321
e1
e4 e3
e2
e8
e9
e13e12
e10e11
e6 e7
e5
Figure 1: Spatial Example
(2004). Given a domain and a target referent, a se-
quence of groups is constructed, starting from the
largest group containing the referent, and recursively
narrowing down the group until only the referent is
identified. The entire sequence is then rendered lin-
guistically. The algorithm used for identifying percep-
tual groups is the one proposed by Thorisson (1994),
the core of which is a procedure which takes as input
a list of pairs of objects, ordered by the distance be-
tween the entities in the pairs. The procedure loops
through the list, finding the greatest difference in dis-
tance between two adjacent pairs. This is determined
as a cutoff point for group formation. Two problems
are raised by this approach:
P1 Ambiguous clusters A domain entity can be
placed in more than one group. If, say, the in-
put list is
?
{a, b}, {c, e}, {a, f}
?
and the great-
est difference after the first iteration is between
{c, e} and {a, f}, then the first group to be formed
will be {a, b, c, e} with {a, f} likely to be placed
in a different group after further iterations. This
may be confusing from a referential point of view.
The problem arises because grouping or cluster-
ing takes place on the basis of pairwise proxim-
ity or distance. This problem can be partially cir-
cumvented by identifying groups on several per-
ceptual dimensions (e.g. spatial distance, colour,
and shape) and then seeking to merge identical
groups determined on the basis of these differ-
ent qualities (see Thorisson (1994)). However, the
grouping strategy can still return groups which do
not conform to human perceptual principles. A
better strategy is to base clustering on the Near-
est Neighbour Principle, familiar from computa-
tional geometry (Prepaarata and Shamos, 1985),
whereby elements are clustered with their nearest
neighbours, given a distance function. The solu-
tion offered below is based on this principle.
P2 Perceptual proximity Absolute distance is not
sufficient for cluster identification. In Figure 1,
for example, the pairs {e1, e2} and {e5, e6} could
easily be consecutively ranked, since the distance
between e1 and e2 is roughly equal to that be-
tween e5 and e6. However, they would not natu-
rally be clustered together by a human observer,
because grouping of objects also needs to take
into account the position of the surrounding ele-
ments. Thus, while e1 is as far away from e2 as
e5 is from e6, there are elements which are closer
to {e1, e2} than to {e5, e6}.
The proposal in ?4 represents a way of getting
around these problems, which are expected to arise in
any kind of domain where the information given is the
pairwise distance between elements. Before turning to
the framework, we consider another situation in GRE
where the need for clustering could arise.
3 Perspectives and semantic similarity
In real-world discourse, entities can often be talked
about from different points of view, with speakers
bringing to bear world and domain-specific knowledge
to select information that is relevant to the current
topic. In order to generate coherent discourse, a gener-
ator should ideally keep track of how entities have been
referred to, and maintain consistency as far as possible.
type profession nationality
e1 man student englishman
e2 woman teacher italian
e3 man chef greek
Table 1: Semantic Example
Suppose e1 in Table 1 has been introduced into the
discourse via the description the student and the next
utterance requires a reference to e2. Any one of the
three available attributes would suffice to distinguish
the latter. However, a description such as the woman
or the italianwould describe this entity from a different
point of view relative to e1. By hypothesis, the teacher
is more appropriate, because the property ascribed to
e2 is more similar to that ascribed to e1.
A similar case arises with plural disjunctive descrip-
tions of the form ?x[p(x)?q(x)], which are usually re-
alised as coordinate constructions of the form the N?1
and the N?2. For instance a reference to {e1, e2} such
as the woman and the student, or the englishman and
the teacher, would be odd, compared to the alterna-
tive the student and the teacher. The latter describes
these entities under the same perspective. Note that
?consistency? or ?similarity? is not guaranteed simply
by attempting to use values of the same attribute(s) for
a given set of referents. The description the student
322
and the chef for {e1, e3} is relatively odd compared to
the alternative the englishman and the greek. In both
kinds of scenarios, a GRE algorithm that relied on a
rigid preference order could not guarantee that a coher-
ent description would be generated every time it was
available.
The issues raised here have never been systemati-
cally addressed in the GRE literature, although support
for the underlying intuitions can be found in various
quarters. Kronfeld (1989) distinguishes between func-
tionally and conversationally relevant descriptions. A
description is functionally relevant if it succeeds in dis-
tinguishing the intended referent(s), but conversational
relevance arises in part from implicatures carried by
the use of attributes in context. For example, describ-
ing e1 as the student carries the (Gricean) implicature
that the entity?s academic role or profession is some-
how relevant to the current discourse. When two enti-
ties are described using contrasting properties, say the
student and the italian, the listener may find it harder
to work out the relevance of the contrast. In a related
vein, Aloni (2002) formalises the appropriateness of an
answer to a question of the form Wh x? with reference
to the ?conceptual covers? or perspectives under which
x can be conceptualised, not all of which are equally
relevant given the hearer?s information state and the
discourse context.
With respect to plurals, Eschenbach et al (1989) ar-
gue that the generation of a plural anaphor with a split
antecedent is more felicitous when the antecedents
have something in common, such as their ontological
category. This constraint has been shown to hold psy-
cholinguistically (Kaup et al, 2002; Koh and Clifton,
2002; Moxey et al, 2004). Gatt and van Deemter
(2005a) have shown that people?s perception of the ad-
equacy of plural descriptions of the form, the N1 and
(the) N2 is significantly correlated with the seman-
tic similarity of N1 and N2, while singular descrip-
tions are more likely to be aggregated into a plural if
semantically similar attributes are available (Gatt and
Van Deemter, 2005b).
The two kinds of problems discussed here could be
resolved by pre-processing the KB in order to iden-
tify available perspectives. One way of doing this is
to group available properties into clusters of seman-
tically similar ones. This requires a well-defined no-
tion of ?similarity? which determines the ?distance? be-
tween properties in semantic space. As with spatial
clustering, the problem is then of how to get from
pairwise distance to well-formed clusters or groups,
while respecting the principles underlying human per-
ceptual/conceptual organisation. The next section de-
scribes an algorithm that aims to achieve this.
4 A framework for clustering
In what follows, we assume the existence of a set of
clusters C in a domain S of objects (entities or proper-
ties), to be ?discovered? by the algorithm. We further
assume the existence of a dimension, which is char-
acterised by a function ? that returns the pairwise dis-
tance ?(a, b), where ?a, b? ? S?S. In case an attribute
is characterised bymore than one dimension, say ?x, y?
coordinates in a 2D plane, as in Figure 1, then ? is de-
fined as the Euclidean distance between pairs:
? =
?
?
?x,y??D
|xab ? yab|2 (1)
where D is a tuple of dimensions, xab = ?(a, b) on di-
mension x. ? satisfies the axioms of minimality (2a),
symmetry (2b), and the triangle inequality (2c), by
which it determines a metric space on S:
?(a, b) ? 0 ?
(
?(a, b) = 0? a = b
) (2a)
?(a, b) = ?(b, a) (2b)
?(a, b) + ?(b, c) ? ?(a, c) (2c)
We now turn to the problems raised in ?2. P1 would
be avoided by a clustering algorithm that satisfies (3).
?
Ci?C
= ? (3)
It was also suggested above that a potential solution
to P1 is to cluster using the Nearest Neighbour Princi-
ple. Before considering a solution to P2, i.e. the prob-
lem of discovering clusters that approximate human
intuitions, it is useful to recapitulate the classic prin-
ciples of perceptual grouping proposed by Wertheimer
(1938), of which the following two are the most rele-
vant:
1. Proximity The smaller the distance between ob-
jects in the cluster, the more easily perceived it is.
2. Similarity Similar entities will tend to be more
easily perceived as a coherent group.
Arguably, once a numeric definition of (semantic)
similarity is available, the Similarity Principle boils
down to the Proximity principle, where proximity is
defined via a semantic distance function. This view
is adopted here. How well our interpretation of these
principles can be ported to the semantic clustering
problem of ?3 will be seen in the following subsec-
tions.
To resolve P2, we will propose an algorithm that
uses a context-sensitive definition of ?nearest neigh-
bour?. Recall that P2 arises because, while ? is a mea-
sure of ?objective? distance on some scale, perceived
323
proximity (resp. distance) of a pair ?a, b? is contingent
not only on ?(a, b), but also on the distance of a and
b from all other elements in S. A first step towards
meeting this requirement is to consider, for a given
pair of objects, not only the absolute distance (prox-
imity) between them, but also the extent to which they
are equidistant from other objects in S. Formally, a
measure of perceived proximity prox(a, b) can be ap-
proximated by the following function. Let the two sets
Pab, Dab be defined as follows:
Pab =
{
x|x ? S ? ?(x, a) ? ?(x, b)
}
Dab =
{
y|y ? S ? ?(y, a) 6? ?(y, b)
}
Then:
prox(a, b) = F (?(a, b), |Pab|, |Dab|) (4)
that is, prox(a, b) is a function of the absolute dis-
tance ?(a, b), the number of elements in S ? {a, b}
which are roughly equidistant from a and b, and the
number of elements which are not equidistant. One
way of conceptualising this is to consider, for a given
object a, the list of all other elements of S, ranked by
their distance (proximity) to a. Suppose there exists an
object b whose ranked list is similar to that of a, while
another object c?s list is very different. Then, all other
things being equal (in particular, the pairwise absolute
distance), a clusters closer to b than does c.
This takes us from a metric, distance-based concep-
tion, to a broader notion of the ?similarity? between two
objects in a metric space. Our definition is inspired
by Tversky?s feature-based Contrast Model (1977), in
which the similarity of a, b with feature sets A,B is
a linear function of the features they have in com-
mon and the features that pertain only to A or B, i.e.:
sim(a, b) = f(A ? B) ? f(A ?B). In (4), the dis-
tance of a and b from every other object is the relevant
feature.
4.1 Computing perceived proximity
The computation of pairwise perceived proximity
prox(a, b), shown in Algorithm 1, is the first step to-
wards finding clusters in the domain.
Following Thorisson (1994), the procedure uses
the absolute distance ? to calculate ?absolute proxim-
ity? (1.7), a value in (0, 1), with 1 corresponding to
?(a, b) = 0, i.e. identity (cf. axiom (2a) ). The proce-
dure then visits each element of the domain, and com-
pares its rank with respect to a and b (1.9?1.13)3, in-
crementing a proximity score s (1.10) if the ranks are
3We simplify the presentation by assuming the function
rank(x, a) that returns the rank of x with respect to a. In
practice, this is achieved by creating, for each element of the
input pair, a totally ordered list La such that La[r] holds the
set of elements ranked at r with respect to ?(x, a)
Algorithm 1 prox(a,b)
Require: ?(a, b)
Require: k (a constant)
1: maxD ? max?x,y??S?S ?(x, y)
2: if a = b then
3: return 1
4: end if
5: s? 0
6: d? 0
7: p(a, b)? 1? ?(a,b)maxD
8: for all x ? S ? {a, b} do
9: if |rank(x, a)? rank(x, b)| ? k then
10: s? s + 1
11: else
12: d? d + 1
13: end if
14: end for
15: return p(a, b)? sd
approximately equal, or a distance score d otherwise
(1.12). Approximate equality is determined via a con-
stant k (1.1), which, based on our experiments is set to
a tenth the size of S. The procedure returns the ratio of
proximity and distance scores, weighted by the abso-
lute proximity p(a, b) (1.15). Algorithm 1 is called for
all pairs in S ? S yielding, for each element a ? S, a
list of elements ordered by their perceived proximity to
a. The entity with the highest proximity to a is called
its anchor. Note that any domain object has one, and
only one anchor.
4.2 Creating clusters
The procedure makeClusters(S,Anchors),
shown in its basic form in Algorithm 2, uses the
notion of an anchor introduced above. The rationale
behind the algorithm is captured by the following
declarative principle, where C ? C is any cluster, and
anchor(a, b) means ?b is the anchor of a?:
a ? C ? anchor(a, b)? b ? C (5)
A cluster is defined as the transitive closure of the
anchor relation, that is, if it holds that anchor(a, b)
and anchor(b, c), then {a, b, c} will be clustered to-
gether. Apart from satisfying (5), the procedure also in-
duces a partition on S, satisfying (3). Given these pri-
mary aims, no attempt is made, once clusters are gen-
erated, to further sub-divide them, although we briefly
return to this issue in ?5. The algorithm initialises a
set Clusters to empty (2.1), and iterates through the
list of objects S (2.5). For each object a and its anchor
b (2.6), it first checks whether they have already been
clustered (e.g. if either of them was the anchor of an
object visited earlier) (2.7, 2.12). If this is not the case,
then a provisional cluster is initialised for each element
324
Algorithm 2 makeClusters(S, Anchors)
Ensure: S 6= ?
1: Clusters? ?
2: if |S| = 1 then
3: return S
4: end if
5: for all a ? S do
6: b? Anchors[a]
7: if ?C ? Clusters : a ? C then
8: Ca ? C
9: else
10: Ca ? {a}
11: end if
12: if ?C ? Clusters : b ? C then
13: Cb ? C
14: Clusters? Clusters? {Cb}
15: else
16: Cb ? {b}
17: end if
18: Ca ? Ca ? Cb
19: Clusters? Clusters ? {Ca}
20: end for
21: return Clusters
(2.10, 2.16). The procedure simply merges the cluster
containing a with that of its b (2.18), having removed
the latter from the cluster set (2.14).
This algorithm is guaranteed to induce a partition,
since no element will end up in more than one group.
It does not depend on an ordering of pairs a` la Tho-
risson. However, problems arise when elements and
anchors are clustered na?ively. For instance, if an el-
ement is very distant from every other element in the
domain, prox(a, b) will still find an anchor for it, and
makeClusters(S,Anchors) will place it in the same
cluster as its anchor, although it is an outlier. Before
describing how this problem is rectified, we introduce
the notion of a family (F ) of elements. Informally, this
is a set of elements of S that have the same anchor, that
is:
?a, b ? F : anchor(a, x) ? anchor(b, y)? x = y
(6)
The solution to the outlier problem is to calculate a
centroid value for each family found after prox(a, b).
This is the average proximity between the common an-
chor and all members of its family, minus one stan-
dard deviation. Prior to merging, at line (2.18), the
algorithm now checks whether the proximity value be-
tween an element and its anchor falls below the cen-
troid value. If it does, the the cluster containing an
object and that containing its anchor are not merged.
4.3 Two applications
The algorithm was applied to the two scenarios de-
scribed in ?2 and ?3. In the spatial domain, the al-
gorithm returns groups or clusters of entities, based on
their spatial proximity. This was tested on domains like
Figure 1 in which the input is a set of entities whose
position is defined as a pair of x/y coordinates. Fig-
ure 1 illustrates a potential problem with the proce-
dure. In that figure, it holds that anchor(e8, e9) and
anchor(e9, e8), making e8 and e9 a reciprocal pair.
In such cases, the algorithm inevitably groups the two
elements, whatever their proximity/distance. This may
be problematic when elements of a reciprocal pair are
very distant from eachother, in which case they are un-
likely to be perceived as a group. We return to this
problem briefly in ?5.
The second domain of application is the cluster-
ing of properties into ?perspectives?. Here, we use
the information-theoretic definition of similarity de-
veloped by Lin (1998) and applied to corpus data by
Kilgarriff and Tugwell (Kilgarriff and Tugwell, 2001).
This measure defines the similarity of two words as a
function of the likelihood of their occurring in the same
grammatical environments in a corpus. This measure
was shown experimentally to correlate highly with hu-
man acceptability judgments of disjunctive plural de-
scriptions (Gatt and van Deemter, 2005a), when com-
pared with a number of measures that calculate the
similarity of word senses in WordNet. Using this as
the measure of semantic distance between words, the
algorithm returns clusters such as those in Figure 2.
input: { waiter, essay, footballer, article, servant,
cricketer, novel, cook, book, maid,
player, striker, goalkeeper }
output:
1 { essay, article, novel, book }
2 { footballer, cricketer }
3 { waiter, cook, servant, maid }
4 { player, goalkeeper, striker }
Figure 2: Output on a Semantic Domain
If the words in Figure 2 represented properties of
different entities in the domain of discourse, then
the clusters would represent perspectives or ?covers?,
whose extension is a set of entities that can be talked
about from the same point of view. For example, if
some entity were specified as having the property foot-
baller, and the property striker, while another entity
had the property cricketer, then according to the output
of the algorithm, the description the footballer and the
cricketer is the most conceptually coherent one avail-
able. It could be argued that the units of representation
325
spatial semantic
1 0.94 0.58
2 0.86 0.36
3 0.62 0.76
4 0.93 0.52
mean 0.84 0.64
Table 2: Proportion of agreement among participants
in GRE are not words but ?properties? (e.g. values of
attributes) which can be realised in a number of differ-
ent ways (if, for instance, there are a number of syn-
onyms corresponding roughly to the same intension).
This could be remedied by defining similarity as ?dis-
tance in an ontology?; conversely, properties could be
viewed as a set of potential (word) realisations.
5 Evaluation
The evaluation of the algorithm was based on a com-
parison of its output against the output of human beings
in a similar task.
Thirteen native or fluent speakers of English volun-
teered to participate in the study. The materials con-
sisted of 8 domains, 4 of which were graphical repre-
sentations of a 2D spatial layout containing 13 points.
The pictures were generated by plotting numerical x/y
coordinates (the same values are used as input to the
algorithm). The other four domains consisted of a
set of 13 arbitrarily chosen nouns. Participants were
presented with an eight-page booklet with spatial and
semantic domains on alternate pages. They were in-
structed to draw circles around the best clusters in the
pictures, or write down the words in groups that were
related according to their intuitions. Clusters could be
of arbitrary size, but each element had to be placed in
exactly one cluster.
5.1 Participant agreement
Participant agreement on each domain was measured
using kappa. Since the task did not involve predefined
clusters, the set of unique groups (denoted G) gener-
ated by participants in every domain was identified,
representing the set of ?categories? available post hoc.
For each domain element, the number of times it oc-
curred in each group served as the basis to calculate
the proportion of agreement among participants for the
element. The total agreement P (A) and the agreement
expected by chance, P (E) were then used in the stan-
dard formula
k = P (A)? P (E)1? P (E)
Table 2 shows a remarkable difference between the
two domain types, with very high agreement on spa-
tial domains and lower values on the semantic task.
The difference was significant (t = 2.54, p < 0.05).
Disagreement on spatial domains was mostly due to
the problem of reciprocal pairs, where participants dis-
agreed on whether entities such as e8 and e9 in Figure 1
gave rise to a well-formed cluster or not. However, all
the participants were consistent with the version of the
Nearest Neighbour Principle given in (5). If an element
was grouped, it was always grouped with its anchor.
The disagreement in the semantic domains seemed
to turn on two cases4:
1. Sub-clusters Whereas some proposals included
clusters such as { man, woman, boy, girl, infant,
toddler, baby, child } , others chose to group {
infant, toddler, baby,child } separately.
2. Polysemy For example, liver was in some cases
clustered with { steak, pizza } , while others
grouped it with items like { heart, lung } .
Insofar as an algorithm should capture the whole range
of phenomena observed, (1) above could be accounted
for by making repeated calls to the Algorithm to sub-
divide clusters. One problem is that, in case only one
cluster is found in the original domain, the same cluster
will be returned after further attempts at sub-clustering.
A possible solution to this is to redefine the parameter
k in Algorithm (1), making the condition for proximity
more strict. As for the second observation, the desider-
atum expressed in (3) may be too strong in the semantic
domain, since words can be polysemous. As suggested
above, one way to resolve this would be to measure
distance between word senses, as opposed to words.
5.2 Algorithm performance
The performance of the algorithm (hereafter the target)
against the human output was compared to two base-
line algorithms. In the spatial domains, we used an
implementation of the Thorisson algorithm (Thorisson,
1994) described in ?2. In our implementation, the pro-
cedure was called iteratively until all domain objects
had been clustered in at least one group.
For the semantic domains, the baseline was a simple
procedure which calculated the powerset of each do-
main S. For each subset in pow(S) ? {?, S}, the pro-
cedure calculates the mean pairwise similarity between
words, returning an ordered list of subsets. This partial
order is then traversed, choosing subsets until all ele-
ments had been grouped. This seemed to be a reason-
able baseline, because it corresponds to the intuition
that the ?best cluster? from a semantic point of view is
the one with the highest pairwise similarity among its
elements.
4The conservative strategy used here probably amplifies
disagreements; disregarding clusters which are subsumed by
other clusters would control at least for case (1)
326
The output of the target and baseline algorithms was
compared to human output in the following ways:
1. By item In each of the eight test domains, an
agreement score was calculated for each domain
element e (i.e. 13 scores in each domain). Let
Us be the set of distinct groups containing e pro-
posed by the experimental participants, and let Ua
be the set of unique groups containing e proposed
by the algorithm (|Ua| = 1 in case of the target
algorithm, but not necessarily for the baselines,
since they do not impose a partition). For each
pair ?Uai , Usj ? of algorithm-human clusters, the
agreement score was defined as
|Uai ? Usj |
|Uai ? Usj |+ |Uai ? Usi |
,
i.e. the ratio of the number of elements on which
the human/algorithm agree, and the number of el-
ements on which they do not agree. This returns a
number in (0, 1) with 1 indicating perfect agree-
ment. The maximal such score for each entity was
selected. This controlled for the possible advan-
tage that the target alorithm might have, given
that it, like the human participants, partitions the
domain.
2. By participant An overall mean agreement score
was computed for each participant using the
above formula for the target and baseline algo-
rithms in each domain.
Results by item Table 3 shows the mean and modal
agreement scores obtained for both target and base-
line in each domain type. At a glance, the target alo-
rithm performed better than the baseline on the spatial
domains, with a modal score of 1, indicating perfect
agreement on 60% of the objects. The situation is dif-
ferent in the semantic domains, where target and base-
line performed roughly equally well; in fact, the modal
score of 1 accounts for 75% baseline scores.
target baseline
spatial mean 0.84 0.72
mode 1 (60%) 0.67 (40%)
semantic mean 0.86 0.86
mode 1 (65%) 1 (75%)
Table 3: Mean and modal agreement scores
Unsurprisingly, the difference between target and
baseline algorithmswas reliable on the spatial domains
(t = 2.865, p < .01), but not on the semantic domains
(t < 1, ns). This was confirmed by a one-way Analysis
of Variance (ANOVA), testing the effect of algorithm
(target/baseline) and domain type (spatial/semantic) on
agreement results. There was a significant main ef-
fect of domain type (F = 6.399, p = .01), while
the main effect of algorithm was marginally significant
(F = 3.542, p = .06). However, there was a reliable
type ? algorithm interaction (F = 3.624, p = .05),
confirming the finding that the agreement between tar-
get and human output differed between domain types.
Given the relative lack of agreement between partic-
ipants in the semantic clustering task, this is unsur-
prising. Although the analysis focused on maximal
scores obtained per entity, if participants do not agree
on groupings, then the means which are statistically
compared are likely to mask a significant amount of
variance. We now turn to the analysis by participants.
Results by participant The difference between tar-
get and baselines in agreement across participants was
significant both for spatial (t = 16.6, p < .01)
and semantic (t = 5.759, t < .01) domain types.
This corroborates the earlier conclusion: once par-
ticipant variation is controlled for by including it in
the statistical model, the differences between target
and baseline show up as reliable across the board. A
univariate ANOVA corroborates the results, showing
no significant main effect of domain type (F < 1,
ns), but a highly significant main effect of algorithm
(F = 233.5, p < .01) and a significant interaction
(F = 44.3, p < .01).
Summary The results of the evaluation are encour-
aging, showing high agreement between the output of
the algorithm and the output that was judged by hu-
mans as most appropriate. They also suggest frame-
work of ?4 corresponds to human intuitions better than
the baselines tested here. However, these results should
be interpretedwith caution in the case of semantic clus-
tering, where there was significant variability in human
agreement. With respect to spatial clustering, one out-
standing problem is that of reciprocal pairs which are
too distant from eachother to form a perceptually well-
formed cluster. We are extending the empirical study
to new domains involving such cases, in order to infer
from the human data a threshold on pairwise distance
between entities, beyond which they are not clustered.
6 Conclusions and future work
This paper attempted to achieve a dual goal. First, we
highlighted a number of scenarios in which the perfor-
mance of a GRE algorithm can be enhanced by an ini-
tial step which identifies clusters of entities or proper-
ties. Second, we described an algorithm which takes as
input a set of objects and returns a set of clusters based
on a calculation of their perceived proximity. The def-
inition of perceived proximity seeks to take into ac-
count some of the principles of human perceptual and
conceptual organisation.
In current work, the algorithm is being applied to
327
two problems in GRE, namely, the generation of spatial
references involving collective predicates (e.g. gath-
ered), and the identification of the available perspec-
tives or conceptual covers, under which referents may
be described.
References
M. Aloni. 2002. Questions under cover. In D. Barker-
Plummer, D. Beaver, J. van Benthem, and P. Scotto
de Luzio, editors, Words, Proofs, and Diagrams.
CSLI.
Anja Arts. 2004. Overspecification in Instructive
Texts. Ph.D. thesis, Univiersity of Tilburg.
Robert Dale and Ehud Reiter. 1995. Computational
interpretation of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(8):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, ACL-89.
C. Eschenbach, C. Habel, M. Herweg, and K. Rehkam-
per. 1989. Remarks on plural anaphora. In Pro-
ceedings of the 4th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL-89.
K. Funakoshi, S. Watanabe, N. Kuriyama, and T. Toku-
naga. 2004. Generating referring expressions using
perceptual groups. In Proceedings of the 3rd Inter-
national Conference on Natural Language Genera-
tion, INLG-04.
A. Gatt and K. van Deemter. 2005a. Semantic simi-
larity and the generation of referring expressions: A
first report. In Proceedings of the 6th International
Workshop on Computational Semantics, IWCS-6.
A. Gatt and K. Van Deemter. 2005b. Towards a
psycholinguistically-motivated algorithm for refer-
ring to sets: The role of semantic similarity. Techni-
cal report, TUNA Project, University of Aberdeen.
H.P. Grice. 1975. Logic and conversation. In P. Cole
and J.L. Morgan, editors, Syntax and Semantics:
Speech Acts., volume III. Academic Press.
P. Jordan and M. Walker. 2000. Learning attribute
selections for non-pronominal expressions. In Pro-
ceedings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL-00.
B. Kaup, S. Kelter, and C. Habel. 2002. Represent-
ing referents of plural expressions and resolving plu-
ral anaphors. Language and Cognitive Processes,
17(4):405?450.
A. Kilgarriff and D. Tugwell. 2001. Word sketch: Ex-
traction and display of significant collocations for
lexicography. In Proceedings of the Collocations
Workshop in Association with ACL-2001.
S. Koh and C. Clifton. 2002. Resolution of the an-
tecedent of a plural pronoun: Ontological categories
and predicate symmetry. Journal of Memory and
Language, 46:830?844.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
Kees van Deemter and Rodger Kibble, editors, In-
formation Sharing: Reference and Presupposition in
Language Generation and Interpretation. Stanford:
CSLI.
A. Kronfeld. 1989. Conversationally relevant descrip-
tions. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
ACL89.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the International Con-
ference on Machine Learning.
L. Moxey, A. J. Sanford, P. Sturt, and L. I Morrow.
2004. Constraints on the formation of plural refer-
ence objects: The influence of role, conjunction and
type of description. Journal of Memory and Lan-
guage, 51:346?364.
F. P. Prepaarata and M. A. Shamos. 1985. Computa-
tional Geometry. Springer.
E. Reiter. 1990. The computational complexity of
avoiding conversational implicatures. In Proceed-
ings of the 28th Annual Meeting of the Association
for Computational Linguistics, ACL-90.
K. R. Thorisson. 1994. Simulated perceptual group-
ing: An application to human-computer interaction.
In Proceedings of the 16th Annual Conference of the
Cognitive Science Society.
A. Treisman. 1982. Perceptual grouping and attention
in visual search for features and objects. Journal of
Experimental Psychology: Human Perception and
Performance, 8(2):194?214.
A. Tversky. 1977. Features of similarity. Psychologi-
cal Review, 84(4):327?352.
K. van Deemter. 2000. Generating vague descriptions.
In Proceedings of the First International Conference
on Natural Language Generation, INLG-00.
Kees van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
M. Wertheimer. 1938. Laws of organization in per-
ceptual forms. In W. Ellis, editor, A Source Book of
Gestalt Psychology. Routledge & Kegan Paul.
328
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 255?262,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Conceptual Coherence in the Generation of Referring Expressions
Albert Gatt
Department of Computing Science
University of Aberdeen
agatt@csd.abdn.ac.uk
Kees van Deemter
Department of Computing Science
University of Aberdeen
kvdeemte@csd.abdn.ac.uk
Abstract
One of the challenges in the automatic
generation of referring expressions is to
identify a set of domain entities coher-
ently, that is, from the same conceptual
perspective. We describe and evaluate
an algorithm that generates a conceptually
coherent description of a target set. The
design of the algorithm is motivated by the
results of psycholinguistic experiments.
1 Introduction
Algorithms for the Generation of Referring Ex-
pressions (GRE) seek a set of properties that dis-
tinguish an intended referent from its distractors
in a knowledge base. Much of the GRE litera-
ture has focused on developing efficient content
determination strategies that output the best avail-
able description according to some interpretation
of the Gricean maxims (Dale and Reiter, 1995),
especially Brevity. Work on reference to sets has
also proceeded within this general framework (van
Deemter, 2002; Gardent, 2002; Horacek, 2004).
One problem that has not received much atten-
tion is that of conceptual coherence in the genera-
tion of plural references, i.e. the ascription of re-
lated properties to elements of a set, so that the
resulting description constitutes a coherent cover
for the plurality. As an example, consider a ref-
erence to {e1, e3} in Table 1 using the Incremen-
tal Algorithm (IA) (Dale and Reiter, 1995). IA
searches along an ordered list of attributes, select-
ing properties of the intended referents that re-
move some distractors. Assuming the ordering in
the top row, IA would yield the postgraduate and
the chef, which is fine in case occupation is the
relevant attribute in the discourse, but otherwise is
arguably worse than an alternative like the italian
and the maltese, because it is more difficult to see
what a postgraduate and a chef have in common.
type occupation nationality
e1 man postgraduate maltese
e2 man undergraduate greek
e3 man chef italian
Table 1: Example domain
Such examples lead us to hypothesise the follow-
ing constraint:
Conceptual Coherence Constraint
(CC): As far as possible, describe
objects using related properties.
Related issues have been raised in the formal
semantics literature. Aloni (2002) argues that an
appropriate answer to a question of the form ?Wh
x?? must conceptualise the different instantiations
of x using a perspective which is relevant given the
hearer?s information state and the context. Kron-
feld (1989) distinguishes a description?s functional
relevance ? i.e. its success in distinguishing a ref-
erent ? from its conversational relevance, which
arises in part from implicatures. In our example,
describing e1 as the postgraduate carries the im-
plicature that the entity?s academic role is relevant.
When two entities are described using contrasting
properties, say the student and the italian, the con-
trast may be misleading for the listener.
Any attempt to port these observations to the
GRE scenario must do so without sacrificing logi-
cal completeness. While a GRE algorithm should
attempt to find the most coherent description avail-
able, it should not fail in the absence of a coher-
ent set of properties. This paper aims to achieve
a dual goal. First (?2), we will show that the CC
can be explained and modelled in terms of lexi-
cal semantic forces within a description, a claim
supported by the results of two experiments. Our
focus on ?low-level?, lexical, determinants of ad-
equacy constitutes a departure from the standard
Gricean view. Second, we describe an algorithm
255
motivated by the experimental findings (?3) which
seeks to find the most coherent description avail-
able in a domain according to CC.
2 Empirical evidence
We take as paradigmatic the case where a plural
reference involves disjunction/union, that is, has
the logical form ?x (p(x) ? q(x)), realised as a
description of the form the N1 and the N2. By hy-
pothesis, the case where all referents can be de-
scribed using identical properties (logically, a con-
junction), is a limiting case of CC.
Previous work on plural anaphor processing has
shown that pronoun resolution is easier when an-
tecedents are ontologically similar (e.g. all hu-
mans) (Kaup et al, 2002; Koh and Clifton, 2002).
Reference to a heterogeneous set increases pro-
cessing difficulty.
Our experiments extended these findings to full
definite NP reference. Throughout, we used a dis-
tributional definition of similarity, as defined by
Lin (1998), which was found to be highly corre-
lated to people?s preferences for disjunctive de-
scriptions (Gatt and van Deemter, 2005). The sim-
ilarity of two arbitrary objects a and b is a function
of the information gained by giving a joint descrip-
tion of a and b in terms of what they have in com-
mon, compared to describing a and b separately.
The relevant data in the lexical domain is the
grammatical environment in which words occur.
This information is represented as a set of triples
?rel, w,w??, where rel is a grammatical relation,
w the word of interest and w? its co-argument
in rel (e.g. ? premodifies, dog, domestic ?). Let
F (w) be a list of such triples. The information
content of this set is defined as mutual information
I(F (w)) (Church and Hanks, 1990). The similar-
ity of two words w1 and w2, of the same grammat-
ical category, is:
?(w1, w2) =
2 ? I(F (w1) ? F (w2))
I(F (w1)) + I(F (w2))
(1)
For example, if premodifies is one of the rele-
vant grammatical relations, then dog and cat might
occur several times in a corpus with the same pre-
modifiers (tame, domestic, etc). Thus, ?(dog, cat)
is large because in a corpus, they often occur in
the same contexts and there is considerable infor-
mation gain in a description of their common data.
Rather than using a hand-crafted ontology to in-
fer similarity, this definition looks at real language
Condition a b c distractor
HDS spanner chisel plug thimble
LDS toothbrush knife ashtray clock
Figure 1: Conditions in Experiment 1
use. It covers ontological similarity to the extent
that ontologically similar objects are talked about
in the same contexts, but also cuts across ontolog-
ical distinctions (for example newspaper and jour-
nalist might turn out to be very similar).
We use the information contained in the
SketchEngine database1 (Kilgarriff, 2003), a
largescale implementation of Lin?s theory based
on the BNC, which contains grammatical triples
in the form of Word Sketches for each word, with
each triple accompanied by a salience value in-
dicating the likelihood of occurrence of the word
with its argument in a grammatical relation. Each
word also has a thesaurus entry, containing a
ranked list of words of the same category, ordered
by their similarity to the head word.
2.1 Experiment 1
In Experiment 1, participants were placed in a sit-
uation where they were buying objects from an on-
line store. They saw scenarios containing four pic-
tures of objects, three of which (the targets) were
identically priced. Participants referred to them by
completing a 2-sentence discourse:
S1 The object1 and the object 2 cost amount.
S2 The object3 also costs amount.
If similarity is a constraint on referential coher-
ence in plural references, then if two targets are
similar (and dissimilar to the third), a plural refer-
ence to them in S1 should be more likely, with the
third entity referred to in S2.
Materials, design and procedure All the pic-
tures were artefacts selected from a set of draw-
ings normed in a picture-naming task with British
English speakers (Barry et al, 1997).
Each trial consisted of the four pictures ar-
ranged in an array on a screen. Of the three targets
(a, b, c), c was always an object whose name in
the norms was dissimilar to that of a and b. The
semantic similarity of (nouns denoting) a and b
was manipulated as a factor with two levels: High
Distributional Similarity (HDS) meant that b oc-
curred among the top 50 most similar items to a in
its Sketchengine thesaurus entry. Low DS (LDS))
1http://www.sketchengine.co.uk
256
meant that b did not occur in the top 500 entries
for a. Examples are shown in Figure 2.1.
Visual Similarity (VS) of a and b was also con-
trolled. Pairs of pictures were first normed with a
group who rated them on a 10-point scale based
on their visual properties. High-VS (HVS) pairs
had a mean rating ? 6; Low-VS LVS) pairs had
mean ratings ? 2. Two sets of materials were con-
structed, for a total of 2 (DS) ? 2 (V S) ? 2 = 8
trials.
29 self-reported native or fluent speakers of En-
glish completed the experiment over the web. To
complete the sentences, participants clicked on the
objects in the order they wished to refer to them.
Nouns appeared in the next available space2.
Results and discussion Responses were coded
according to whether objects a and b were referred
to in the plural subject of S1 (a + b responses) or
not (a? b responses). If our hypothesis is correct,
there should be a higher proportion of a + b re-
sponses in the HDS condition. We did not expect
an effect of VS. In what follows, we report by-
subjects Friedman analyses (?21); by-items analy-
ses (?22); and by-subjects sign tests (Z) on propor-
tions of responses for pairwise comparisons.
Response frequencies across conditions differed
reliably by subjects (?21 = 46.124, p < .001).
The frequency of a + b responses in S1 was re-
liably higher than that of a ? b in the HDS condi-
tion (?22 = 41.371, p < .001), but not the HVS
condition (?22 = 1.755, ns). Pairwise compar-
isons between HDS and LDS showed a signif-
icantly higher proportion of a + b responses in
the former (Z = 4.48, p < .001); the differ-
ence was barely significant across VS conditions
(Z = 1.9, p = .06).
The results show that, given a clear choice of
entities to refer to in a plurality, people are more
likely to describe similar entities in a plural de-
scription. However, these results raise two further
questions. First, given a choice of distinguishing
properties for individuals making up a target set,
will participants follow the predictions of the CC?
(In other words, is distributional similarity rele-
vant for content determination?) Second, does the
similarity effect carry over to modifiers, such as
adjectives, or is the CC exclusively a constraint on
types?
2Earler replications involving typing yielded parallel re-
sults and high conformity between the words used and those
predicted by the picture norms.
Three millionaires with a passion for antiques were spotted
dining at a London restaurant.
e1 One of the men, a Rumanian, is a dealeri .
e2 The second, a princej , is a collectori .
e3 The third, a dukej , is a bachelor.
The XXXX were both accompanied by servants, but the
bachelor wasn?t .
Figure 2: Example discourses
2.2 Experiment 2
Experiment 2 was a sentence continuation task,
designed to closely approximate content determi-
nation in GRE. Participants saw a series of dis-
courses, in which three entities (e1, e2, e3) were
introduced, each with two distinguishing proper-
ties. The final sentence in each discourse had a
missing plural subject NP referring to two of these.
The context made it clear which of the three en-
tities had to be referred to. Our hypothesis was
that participants would prefer to use semantically
similar properties for the plural reference, even if
dissimilar properties were also available.
Materials, design and procedure Materials
consisted of 24 discourses, such as those in Fig-
ure 2.2. After an initial introductory sentence, the
3 entities were introduced in separate sentences.
In all discourses, the pairs {e1, e2} and {e2, e3}
could be described using either pairwise similar or
dissimilar properties (similar pairs are coindexed
in the figure). In half the discourses, the dis-
tinguishing properties of each entity were nouns;
thus, although all three entities belonged to the
same ontological category (e.g. all human), they
had distinct types (e.g. duke, prince, bachelor). In
the other half, entities were of the same type, that
is the NPs introducing them had the same nominal
head, but had distinguishing adjectival modifiers.
For counterbalancing, two versions of each dis-
course were constructed, such that, if {e1, e2} was
the target set in Version 1, then {e2, e3} was the
target in Version 2. Twelve filler items requiring
singular reference in the continuation were also in-
cluded. The order in which the entities were intro-
duced was randomised across participants, as was
the order of trials. The experiment was completed
by 18 native speakers of English, selected from the
Aberdeen NLG Group database. They were ran-
domly assigned to either Version 1 or 2.
Results and discussion Responses were coded
1 if the semantically similar properties were used
(e.g. the prince and the duke in Fig. 2.2); 2 if the
257
similar properties were used together with other
properties (e.g. the prince and the bachelor duke);
3 if a superordinate term was used to replace the
similar properties (e.g. the noblemen); 4 otherwise
(e.g. The duke and the collector).
Response types differed significantly in the
nominal condition both by subjects (?21 =
45.89, p < .001) and by items (?22 = 287.9, p <
.001). Differences were also reliable in the mod-
ifier condition (?21 = 36.3, p < .001, ?22 =
199.2, p < .001). However, the trends across con-
ditions were opposed, with more items in the 1 re-
sponse category in the nominal condition (53.7%)
and more in the 4 category in the modifier condi-
tion (47.2%). Recoding responses as binary (?sim-
ilar? = 1,2,3; ?dissimilar? = 4) showed a significant
difference in proportions for the nominal category
(?2 = 4.78, p = .03), but not the modifier cate-
gory. Pairwise comparisons showed a significantly
larger proportion of 1 (Z = 2.7, p = .007) and
2 responses (Z = 2.54, p = .01) in the nominal
compared to the modifier condition.
The results suggest that in a referential task, par-
ticipants are likely to conform to the CC, but that
the CC operates mainly on nouns, and less so on
(adjectival) modifiers. Nouns (or types, as we shall
sometimes call them) have the function of cate-
gorising objects; thus similar types facilitate the
mental representation of a plurality in a concep-
tually coherent way. According to the definition
in (1), this is because similarity of two types im-
plies a greater likelihood of their being used in
the same predicate-argument structures. As a re-
sult, it is easier to map the elements of a plural-
ity to a common role in a sentence. A related
proposal has been made by Moxey and Sanford
(1995), whose Scenario Mapping Principle holds
that a plural reference is licensed to the extent that
the elements of the plurality can be mapped to a
common role in the discourse. This is influenced
by how easy it is to conceive of such a role for the
referents. Our results can be viewed as providing
a handle on the notion of ?ease of conception of a
common role?; in particular we propose that likeli-
hood of occurrence in the same linguistic contexts
directly reflects the extent to which two types can
be mapped to a single plural role.
As regards modifiers, while it is probably pre-
mature to suggest that CC plays no role in modifier
selection, it is likely that modifiers play a different
role from nouns. Previous work has shown that
id base type occupation specialisation girth
e1 woman professor physicist plump
e2 woman lecturer geologist thin
e3 man lecturer biologist plump
e4 man chemist thin
Table 2: An example knowledge base
restrictions on the plausibility of adjective-noun
combinations exist (Lapata et al, 1999), and that
using unlikely combinations (e.g. the immaculate
kitchen rather than the spotless kitchen) impacts
processing in online tasks (Murphy, 1984). Unlike
types, which have a categorisation function, mod-
ifiers have the role of adding information about an
element of a category. This would partially ex-
plain the experimental results: When elements of
a plurality have identical types (as in the modifier
version of our experiment), the CC is already satis-
fied, and selection of modifiers would presumably
depend on respecting adjective-noun combination
restrictions. Further research is required to ver-
ify this, although the algorithm presented below
makes use of the Sketch Engine database to take
modifier-noun combinations into account.
3 An algorithm for referring to sets
Our next task is to port the results to GRE. The
main ingredient to achieve conceptual coherence
will be the definition of semantic similarity. In
what follows, all examples will be drawn from the
domain in Table 3.
We make the following assumptions. There is
a set U of domain entities, properties of which
are specified in a KB as attribute-value pairs. We
assume a distinction between types, that is, any
property that can be realised as a noun; and modi-
fiers, or non-types. Given a set of target referents
R ? U , the algorithm described below generates a
description D in Disjunctive Normal Form (DNF),
having the following properties:
1. Any disjunct in D contains a ?type? property,
i.e. a property realisable as a head noun.
2. If D has two or more disjuncts, each a con-
junction containing at least one type, then the
disjoined types should be as similar as pos-
sible, given the information in the KB and
the completeness requirement: that the algo-
rithm find a distinguishing description when-
ever one exists.
258
We first make our interpretation of the CC more
precise. Let T be the set of types in the KB, and
let ?(t, t?) be the (symmetrical) similarity between
any two types t and t?. These determine a seman-
tic space S = ?T, ??. We define the notion of a
perspective as follows.
Definition 1. Perspective
A perspective P is a convex subset of S, i.e.:
?t, t?, t?? ? T :
{t, t?} ? P ? ?(t, t??) ? ?(t, t?) ? t?? ? P
The aims of the algorithm are to describe ele-
ments of R using types from the same perspective,
failing which, it attempts to minimise the distance
between the perspectives from which types are se-
lected in the disjunctions of D. Distance between
perspectives is defined below.
3.1 Finding perspectives
The system makes use of the SketchEngine
database as its primary knowledge source. Since
the definition of similarity applies to words, rather
than properties, the first step is to generate all pos-
sible lexicalisations of the available attribute-value
pairs in the domain. In this paper, we simplify by
assuming a one-to-one mapping between proper-
ties and words.
Another requirement is to distinguish between
type properties (the set T ), and non-types (M )3.
The Thesaurus is used to find pairwise similarity
of types in order to group them into related clus-
ters. Word Sketches are used to find, for each type,
the modifiers in the KB that are appropriate to the
type, on the basis of the associated salience values.
For example, in Table 3, e3 has plump as the value
for girth, which combines more felicitously with
man, than with biologist.
Types are clustered using the algorithm de-
scribed in Gatt (2006). For each type t, the al-
gorithm finds its nearest neighbour nt in seman-
tic space. Clusters are then found by recursively
grouping elements with their nearest neighbours.
If t, t? have a common nearest neighbour n, then
{t, t?, n} is a cluster. Clearly, the resulting sets are
convex in the sense of Definition 1. Each modi-
fier is assigned to a cluster by finding in its Word
Sketch the type with which it co-occurs with the
greatest salience value. Thus, a cluster is a pair
3This is determined using corpus-derived information.
Note that T and M need not be disjoint, and entities can have
more than one type property
T: {lecturer, professor}
T: {woman, man}
M: {plump, thin}
T: {geologist, physicist,
biologist, chemist}32
1
1 0.6
1
Figure 3: Perspective Graph
?P,M ?? where P is a perspective, and M ? ? M .
The distance ?(A,B) between two clusters A and
B is defined straightforwardly in terms of the dis-
tance between their perspectives PA and PB:
?(A,B) = 1
1 +
P
x?PA,y?PB
?(x,y)
|PA?PB |
(2)
Finally, a weighted, connected graph G =
?V,E, ?? is created, where V is the set of clus-
ters, and E is the set of edges with edge weights
defined as the semantic distance between perspec-
tives. Figure 3.1 shows the graph constructed for
the domain in Table 3.
We now define the coherence of a description
more precisely. Given a DNF description D, we
shall say that a perspective P is realised in D if
there is at least one type t ? P which is in D.
Let PD be the set of perspectives realised in D.
Since G is connected, PD determines a connected
subgraph of G. The total weight of D, w(D) is the
sum of weights of the edges in PD.
Definition 2. Maximal coherence
A description D is maximally coherent iff there
is no description D? coextensive with D such that
w(D) > w(D?).
(Note that several descriptions of the same ref-
erent may all be maximally coherent.)
3.2 Content determination
The core of the content determination procedure
maintains the DNF description D as an associa-
tive array, such that for any r ? R, D[r] is a con-
junction of properties true of r. Given a cluster
?P,M?, the procedure searches incrementally first
through P, and then M , selecting properties that
are true of at least one referent and exclude some
distractors, as in the IA (Dale and Reiter, 1995).
By Definition 2, the task of the algorithm is
to minimise the total weight w(D). If PD is the
259
set of perspectives represented in D on termina-
tion, then maximal coherence would require PD
to be the subgraph of G with the lowest total cost
from which a distinguishing description could be
constructed. Under this interpretation, PD corre-
sponds to a Shortest Connection, or Steiner, Net-
work. Finding such networks is known to be NP-
Hard. Therefore, we adopt a weaker (greedy) in-
terpretation. Under the new definition, if D is
the only description for R, then it trivially satis-
fies maximal coherence. Otherwise, the algorithm
aims to maximise local coherence.
Definition 3. Local coherence
A description D is locally coherent iff:
a. either D is maximally coherent or
b. there is no D? coextensive with D, obtained
by replacing types from some perspective in
PD with types from another perspective such
that w(D) > w(D?).
Our implementation of this idea begins the
search for distinguishing properties by identifying
the vertex of G which contains the greatest num-
ber of referents in its extension. This constitutes
the root node of the search path. For each node
of the graph it visits, the algorithm searches for
properties that are true of some subset of R, and
removes some distractors, maintaining a set N of
the perspectives which are represented in D up to
the current point. The crucial choice points arise
when a new node (perspective) needs to be visited
in the graph. At each such point, the next node n
to be visited is the one which minimises the total
weight of N , that is:
min
n?V
?
u?N
w(u, n) (3)
The results of this procedure closely approxi-
mate maximal coherence, because the algorithm
starts with the vertex most likely to distinguish
the referents, and then greedily proceeds to those
nodes which minimise w(D) given the current
state, that is, taking all previously used nodes into
account.
As an example of the output, we will take
R = {e1, e3, e4} as the intended referents in Table
3. First, the algorithm determines the cluster with
the greatest number of referents in its extension.
In this case, there is a tie between clusters 2 and
3 in Figure 3.1, since all three entities have type
properties in these clusters. In either case, the
entities are distinguishable from a single cluster.
If cluster 3 is selected as the root, the output is
?x [physicist(x) ? biologist(x) ? chemist(x)].
In case the algorithm selects cluster 2 as the
root node the final output is the logical form
?x [man(x) ? (woman(x) ? plump(x))].
There is an alternative description that the
algorithm does not consider. An algorithm
that aimed for conciseness would generate
?x [professor(x) ?man(x)] (the professor and
the men), which does not satisfy local coherence.
These examples therefore highlight the possible
tension between the avoidance of redundancy and
achieving coherence. It is to an investigation of
this tension that we now turn.
4 Evaluation
It has been known at least since Dale and Reiter
(1995) that the best distinguishing description is
not always the shortest one. Yet, brevity plays a
part in all GRE algorithms, sometimes in a strict
form (Dale, 1989), or by letting the algorithm ap-
proximate the shortest description (for example, in
the Dale and Reiter?s IA). This is also true of refer-
ences to sets, the clearest example being Gardent?s
constraint based approach, which always finds the
description with the smallest number of logical op-
erators. Such proposals do not take coherence (in
our sense of the word) into account. This raises
obvious questions about the relative importance of
brevity and coherence in reference to sets.
The evaluation took the form of an experiment
to compare the output of our Coherence Model
with the family of algorithms that have placed
Brevity at the centre of content determination. Par-
ticipants were asked to compare pairs of descrip-
tions of one and the same target set, selecting the
one they found most natural. Each description
could either be optimally brief or not (?b) and also
either optimally coherent or not (?c). Non-brief
descriptions, took the form the A, the B and the C.
Brief descriptions ?aggregated? two disjuncts into
one (e.g. the A and the D?s where D comprises the
union of B and C). We expected to find that:
H1 +c descriptions are preferred over ?c.
H2 (+c,?b) descriptions are preferred over ones
that are (?c,+b).
H3 +b descriptions are preferred over ?b.
Confirmation of H1 would be interpreted as ev-
idence that, by taking coherence into account, our
260
Three old manuscripts were auctioned at Sotheby?s.
e1 One of them is a book, a biography of a composer.
e2 The second, a sailor?s journal, was published
in the form of a pamphlet. It is a record of a voyage.
e3 The third, another pamphlet, is an essay by Hume.
(+c,?b) The biography, the journal and the essay were sold to a col-
lector.
(+c, +b) The book and the pamphlets were sold to a collector.
(?c, +b) The biography and the pamphlets were sold to a collector.
(?c,?b) The book, the record and the essay were sold to a collector.
Figure 4: Example domain in the evaluation
algorithm is on the right track. If H3 were con-
firmed, then earlier algorithms were (also) on the
right track by taking brevity into account. Con-
firmation of H2 would be interpreted as meaning
that, in references to sets, conceptual coherence is
more important than brevity (defined as the num-
ber of disjuncts in a disjunctive reference to a set).
Materials, design and procedure Six dis-
courses were constructed, each introducing three
entities. Each set of three could be described
using all 4 possible combinations of ?b ? ?c
(see Figure 4). Entities were human in two of
the discourses, and artefacts of various kinds in
the remainder. Properties of entities were intro-
duced textually; the order of presentation was ran-
domised. A forced-choice task was used. Each
discourse was presented with 2 possible continua-
tions consisting of a sentence with a plural subject
NP, and participants were asked to indicate the one
they found most natural. The 6 comparisons cor-
responded to 6 sub-conditions:
C1. Coherence constant
a. (+c,?b) vs. (+c,+b)
b. (?c,?b) vs. (?c,+b)
C2. Brevity constant
a. (+c,?b) vs. (?c,?b)
b. (+c,+b) vs. (?c,+b)
C3. Tradeoff/control
a. (+c,?b) vs. (?c,+b)
b. (?c,?b) vs. (+c,+b)
Participants saw each discourse in a single con-
dition. They were randomly divided into six
groups, so that each discourse was used for a dif-
ferent condition in each group. 39 native English
speakers, all undergraduates at the University of
Aberdeen, took part in the study.
Results and discussion Results were coded ac-
cording to whether a participant?s choice was ?b
C1a C1b C2a C2b C3a C3b
+b 51.3 43.6 ? ? 30.8 76.9
+c ? ? 82.1 79.5 69.2 76.9
Table 3: Response proportions (%)
and/or ?c. Table 4 displays response propor-
tions. Overall, the conditions had a significant
impact on responses, both by subjects (Friedman
?2 = 107.3, p < .001) and by items (?2 =
30.2, p < .001). When coherence was kept con-
stant (C1a and C1b), the likelihood of a response
being +b was no different from ?b (C1a: ?2 =
.023, p = .8; C1b: ?2 = .64, p = .4); the con-
ditions C1a and C1b did not differ significantly
(?2 = .46, p = .5). By contrast, conditions
where brevity was kept constant (C2a and C2b)
resulted in very significantly higher proportions of
+c choices (C2a: ?2 = 16.03, p < .001; C2b:
?2 = 13.56, p < .001). No difference was ob-
served between C2a and C2b (?2 = .08, p = .8).
In the tradeoff case (C3a), participants were much
more likely to select a +c description than a +b
one (?2 = 39.0, p < .001); a majority opted
for the (+b,+c) description in the control case
(?2 = 39.0, p < .001).
The results strongly support H1 and H2, since
participants? choices are impacted by Coherence.
They do not indicate a preference for brief de-
scriptions, a finding that echoes Jordan?s (2000),
to the effect that speakers often relinquish brevity
in favour of observing task or discourse con-
straints. Since this experiment compared our al-
gorithm against the current state of the art in ref-
erences to sets, these results do not necessarily
warrant the affirmation of the null hypothesis in
the case of H3. We limited Brevity to number of
disjuncts, omitting negation, and varying only be-
tween length 2 or 3. Longer or more complex de-
scriptions might evince different tendencies. Nev-
ertheless, the results show a strong impact of Co-
herence, compared to (a kind of) brevity, in strong
support of the algorithm presented above, as a re-
alisation of the Coherence Model.
5 Conclusions and future work
This paper started with an empirical investigation
of conceptual coherence in reference, which led
to a definition of local coherence as the basis for
a new greedy algorithm that tries to minimise the
semantic distance between the perspectives repre-
261
sented in a description. The evaluation strongly
supports our Coherence Model.
We are extending this work in two directions.
First, we are investigating similarity effects across
noun phrases, and their impact on text readabil-
ity. Finding an impact of such factors would make
this model a useful complement to current theories
of discourse, which usually interpret coherence in
terms of discourse/sentential structure.
Second, we intend to relinquish the assumption
of a one-to-one correspondence between proper-
ties and words (cf. Siddharthan and Copestake
(2004)), making use of the fact that words can be
disambiguated by nearby words that are similar.
To use a well-worn example: the ?financial institu-
tion? sense of bank might not make the river and
its bank lexically incoherent as a description of a
piece of scenery, since the word river might cause
the hearer to focus on the aquatic reading of the
word anyway.
6 Acknowledgements
Thanks to Ielka van der Sluis, Imtiaz
Khan, Ehud Reiter, Chris Mellish, Graeme
Ritchie and Judith Masthoff for useful com-
ments. This work is part of the TUNA
project (http://www.csd.abdn.ac.uk/
research/tuna), supported by EPSRC grant
no. GR/S13330/01
References
M. Aloni. 2002. Questions under cover. In D. Barker-
Plummer, D. Beaver, J. van Benthem, and P. Scotto
de Luzio, editors, Words, Proofs, and Diagrams.
CSLI, Stanford, Ca.
C. Barry, C. M. Morrison, and A. W. Ellis. 1997.
Naming the snodgrass and vanderwart pictures.
Quarterly Journal of Experimental Psychology,
50A(3):560?585.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information and lexicography. Com-
putational Linguistics, 16(1):22?29.
R. Dale and E. Reiter. 1995. Computational interpre-
tation of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(8):233?
263.
Robert Dale. 1989. Cooking up referring expressions.
In Proc. 27th Annual Meeting of the Association for
Computational Linguistics.
C. Gardent. 2002. Generating minimal definite de-
scriptions. In Proc. 40th Annual Meeting of the As-
sociation for Computational Linguistics.
A. Gatt and K. van Deemter. 2005. Semantic simi-
larity and the generation of referring expressions: A
first report. In Proceedings of the 6th International
Workshop on Computational Semantics, IWCS-6.
A. Gatt. 2006. Structuring knowledge for reference
generation: A clustering algorithm. In Proc. 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
H. Horacek. 2004. On referring to sets of objects natu-
rally. In Proc. 3rd International Conference on Nat-
ural Language Generation.
P. W. Jordan. 2000. Can nominal expressions achieve
multiple goals? In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics.
B. Kaup, S. Kelter, and C. Habel. 2002. Represent-
ing referents of plural expressions and resolving plu-
ral anaphors. Language and Cognitive Processes,
17(4):405?450.
A. Kilgarriff. 2003. Thesauruses for natural language
processing. In Proc. NLP-KE, Beijing.
S. Koh and C. Clifton. 2002. Resolution of the an-
tecedent of a plural pronoun: Ontological categories
and predicate symmetry. Journal of Memory and
Language, 46:830?844.
A. Kronfeld. 1989. Conversationally relevant descrip-
tions. In Proc. 27th Annual Meeting of the Associa-
tion for Computational Linguistics.
M. Lapata, S. McDonald, and F. Keller. 1999. Deter-
minants of adjective-noun plausibility. In Proc. 9th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
D. Lin. 1998. An information-theoretic definition
of similarity. In Proc. International Conference on
Machine Learning.
L. Moxey and A. Sanford. 1995. Notes on plural refer-
ence and the scenario-mapping principle in compre-
hension. In C.Habel and G.Rickheit, editors, Focus
and cohesion in discourse. de Gruyter, Berlin.
G.L. Murphy. 1984. Establishing and accessing refer-
ents in discourse. Memory and Cognition, 12:489?
497.
A. Siddharthan and A. Copestake. 2004. Generat-
ing referring expressions in open domains. In Proc.
42nd Annual Meeting of the Association for Compu-
tational Linguistics.
K. van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
262
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 197?200,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Intrinsic vs. Extrinsic Evaluation Measures for
Referring Expression Generation
Anja Belz
Natural Language Technology Group
University of Brighton
Brighton BN2 4GJ, UK
a.s.belz@brighton.ac.uk
Albert Gatt
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Abstract
In this paper we present research in which we
apply (i) the kind of intrinsic evaluation met-
rics that are characteristic of current compara-
tive HLT evaluation, and (ii) extrinsic, human
task-performance evaluations more in keeping
with NLG traditions, to 15 systems implement-
ing a language generation task. We analyse
the evaluation results and find that there are no
significant correlations between intrinsic and
extrinsic evaluation measures for this task.
1 Introduction
In recent years, NLG evaluation has taken on a more
comparative character. NLG now has evaluation re-
sults for comparable, but independently developed
systems, including results for systems that regener-
ate the Penn Treebank (Langkilde, 2002) and sys-
tems that generate weather forecasts (Belz and Re-
iter, 2006). The growing interest in comparative
evaluation has also resulted in a tentative interest in
shared-task evaluation events, which led to the first
such event for NLG (the Attribute Selection for Gen-
eration of Referring Expressions, or ASGRE, Chal-
lenge) in 2007 (Belz and Gatt, 2007), with a second
event (the Referring Expression Generation, or REG,
Challenge) currently underway.
In HLT in general, comparative evaluations (and
shared-task evaluation events in particular) are dom-
inated by intrinsic evaluation methodologies, in con-
trast to the more extrinsic evaluation traditions of
NLG. In this paper, we present research in which we
applied both intrinsic and extrinsic evaluation meth-
ods to the same task, in order to shed light on how
the two correlate for NLG tasks. The results show a
surprising lack of correlation between the two types
of measures, suggesting that intrinsic metrics and
extrinsic methods can represent two very different
views of how well a system performs.
2 Task, Data and Systems
Referring expression generation (REG) is concerned
with the generation of expressions that describe en-
tities in a given piece of discourse. REG research
goes back at least to the 1980s (Appelt, Grosz, Joshi,
McDonald and others), but the field as it is today
was shaped in particular by Dale and Reiter?s work
(Dale, 1989; Dale and Reiter, 1995). REG tends to be
divided into the stages of attribute selection (select-
ing properties of entities) and realisation (convert-
ing selected properties into word strings). Attribute
selection in its standard formulation was the shared
task in the ASGRE Challenge: given an intended ref-
erent (?target?) and the other domain entities (?dis-
tractors?) each with possible attributes, select a set
of attributes for the target referent.
The ASGRE data (which is now publicly available)
consists of all 780 singular items in the TUNA corpus
(Gatt et al, 2007) in two subdomains, consisting of
descriptions of furniture and people. Each data item
is a paired attribute set (as derived from a human-
produced RE) and domain representation (target and
distractor entities represented as possible attributes
and values).
ASGRE participants were asked to submit the out-
puts produced by their systems for an unseen test
data set. The outputs from 15 of these systems,
shown in the left column of Table 1, were used in
197
the experiments reported below. Systems differed
in terms of whether they were trainable, performed
exhaustive search and hardwired use of certain at-
tributes types, among other algorithmic properties
(see the ASGRE papers for full details). In the case
of one system (IS-FBS), a buggy version was origi-
nally submitted and used in Exp 1. It was replaced in
Exp 2 by a corrected version; the former is marked
by a * in what follows.
3 Evaluation Methods
1. Extrinsic evaluation measures: We conducted
two task-performance evaluation experiments (the
first was part of the ASGRE Challenge, the second
is new), in which participants identified the referent
denoted by a description by clicking on a picture in
a visual display of target and distractor entities. To
enable subjects to read the outputs of peer systems,
we converted them from the attribute-value format
described above to something more readable, using
a simple attribute-to-word converter.
Both experiments used a Repeated Latin Squares
design, and involved 30 participants and 2,250 indi-
vidual trials (see Belz & Gatt (2007) for full details).
In Exp 1, subjects were shown the domain on
the same screen as the description. Two depen-
dent measures were used: (i) combined reading and
identification time (RIT), measured from the point at
which the description and pictures appeared on the
screen to the point at which a picture was selected
by mouse-click; and (ii) error rate (ER-1).
In Exp 2, subjects first read the description and
then initiated the presentation of domain entities.
We computed: (i) reading time (RT), measured from
the presentation of a description to the point where
a subject requested the presentation of the domain;
(ii) identification time (IT), measured from the pre-
sentation of the domain to the point where a subject
clicked on a picture; and (iii) error rate (ER-2).
2. REG-specific intrinsic measures: Unique-
ness is the proportion of attribute sets generated by
a system which identify the referent uniquely (i.e.
none of the distractors). Minimality is the propor-
tion of attribute sets which are minimal as well as
unique (i.e. there is no smaller unique set of at-
tributes). These measures were included because
they are commonly named as desiderata for attribute
selection algorithms in the REG field (Dale, 1989).
The minimality check used in this paper treats refer-
ent type as a simple attribute, as the ASGRE systems
tended to do.1
3. Set-similarity measures: The Dice similarity
coefficient computes the similarity between a peer
attribute set A1 and a (human-produced) reference
attribute set A2 as 2?|A1?A2||A1|+|A2| . MASI (Passonneau,
2006) is similar but biased in favour of similarity
where one set is a subset of the other.
4. String-similarity measures: In order to apply
string-similarity metrics, peer and reference outputs
were converted to word-strings by the method de-
scribed under 1 above. String-edit distance (SE) is
straightforward Levenshtein distance with a substi-
tution cost of 2 and insertion/deletion cost of 1. We
also used the version of string-edit distance (?SEB?)
of Bangalore et al (2000) which normalises for
length. BLEU computes the proportion of word n-
grams (n ? 4 is standard) that a peer output shares
with several reference outputs. The NIST MT eval-
uation metric (Doddington, 2002) is an adaptation
of BLEU which gives more importance to less fre-
quent (hence more informative) n-grams. We also
used two versions of the ROUGE metric (Lin and
Hovy, 2003), ROUGE-2 and ROUGE-SU4 (based on
non-contiguous, or ?skip?, n-grams), which were of-
ficial scores in the DUC 2005 summarization task.
4 Results
Results for all evaluation measures and all systems
are shown in Table 1. Uniqueness results are not
included, as all systems scored 100%.
We ran univariate analyses of variance (ANOVAs)
using SYSTEM as the independent variable (15
levels), testing its effect on the extrinsic task-
performance measures. For error rate (ER), we used
a Kruskal-Wallis ranks test to compare identifica-
tion accuracy rates across systems2. The main effect
of SYSTEM was significant on RIT (F (14, 2249) =
6.401, p < .001), RT (F (14, 2249) = 2.56, p <
.01), and IT (F (14, 2249) = 1.93, p < .01). In nei-
ther experiment was there a significant effect on ER.
1As a consequence, the Minimality results we report here
look different from those in the ASGRE report.
2A non-paramteric test was more appropriate given the large
number of zero values in ER proportions, and a high dependency
of variance on the mean.
198
extrinsic REG string-similarity set-similarity
RIT RT IT ER-1 ER-2 Min RSU4 R-2 NIST BLEU SE SEB Dice MASI
CAM-B 2784.80 1309.07 1952.39 9.33 5.33 8.11 .673 .647 2.70 .309 4.42 .307 .620 .403
CAM-BU 2659.37 1251.32 1877.95 9.33 4 10.14 .663 .638 2.61 .317 4.23 .359 .630 .420
CAM-T 2626.02 1475.31 1978.24 10 5.33 0 .698 .723 3.50 .415 3.67 .496 .725 .560
CAM-TU 2572.82 1297.37 1809.04 8.67 4 0 .677 .691 3.28 .407 3.71 .494 .721 .557
DIT-DS 2785.40 1304.12 1859.25 10.67 2 0 .651 .679 4.23 .457 3.55 .525 .750 .595
GR-FP 2724.56 1382.04 2053.33 8.67 3.33 4.73 .65 .649 3.24 .358 3.87 .441 .689 .480
GR-SC 2811.09 1349.05 1899.59 11.33 2 4.73 .644 .644 2.42 .305 4 .431 .671 .466
IS-FBN 3570.90 1837.55 2188.92 15.33 6 1.35 .771 .772 4.75 .521 3.15 .438 .770 .601
IS-FBS ? 1461.45 2181.88 ? 7.33 100 .485 .448 2.11 .166 5.53 .089 .368 .182
*IS-FBS 4008.99 ? ? 10 ? 39.86 ? ? ? ? ? ? .527 .281
IS-IAC 2844.17 1356.15 1973.19 8.67 6 0 .612 .623 3.77 .442 3.43 .559 .746 .597
NIL 1960.31 1482.67 1960.31 10 5.33 20.27 .525 .509 3.32 .32 4.12 .447 .625 .477
T-AS+ 2652.85 1321.20 1817.30 9.33 4.67 0 .671 .684 2.62 .298 4.24 .37 .660 .452
T-AS 2864.93 1229.42 1766.35 10 4.67 0 .683 .692 2.99 .342 4.10 .393 .645 .422
T-RS+ 2759.76 1278.01 1814.93 6.67 1.33 0 .677 .697 2.85 .303 4.32 .36 .669 .459
T-RS 2514.37 1255.28 1866.94 8.67 4.67 0 .694 .711 3.16 .341 4.18 .383 .655 .432
Table 1: Results for all systems and evaluation measures (ER-1 = error rate in Exp 1, ER-2 = error rate in Exp 2). (R =
ROUGE; system IDs as in the ASGRE papers, except GR = GRAPH; T = TITCH).
Table 2 shows correlations between the automatic
metrics and the task-performance measures from
Exp 1. RIT and ER-1 are not included because of
the presence of *IS-FBS in Exp 1 (but see individual
results below). For reasons of space, we refer the
reader to the table for individual correlation results.
We also computed correlations between the task-
performance measures across the two experiments
(leaving out the IS-FBS system). Correlation be-
tween RIT and RT was .827**; between RIT and IT
.675**; and there was no significant correlation be-
tween the error rates. The one difference evident
between RT and IT is that ER correlates only with IT
(not RT) in Exp 2 (see Table 2).
5 Discussion
In Table 2, the four broad types of metrics we have
investigated (task-performance, REG-specific, string
similarity, set similarity) are indicated by vertical
and horizontal lines. The results within each of the
resulting boxes are very homogeneous. There are
significant (and mostly strong) correlations not only
among the string-similarity metrics and among the
set-similarities, but also across the two types. There
are also significant correlations between the three
task-performance measures.
However, the correlation figures between the task-
performance measures and all others are weak and
not significant. The one exception is the correlation
between NIST and RT which is actually in the wrong
direction (better NIST implies worse reading times).
This is an unambiguous result and it shows clearly
that similarity to human-produced reference texts is
not necessarily indicative of quality as measured by
human task performance.
The emergence of comparative evaluation in NLG
raises the broader question of how systems that gen-
erate language should be compared. In MT and sum-
marisation it is more or less taken as read that sys-
tems which generate more human-like language are
better systems. However, it has not been shown
that more human-like outputs result in better per-
formance from an extrinsic perspective. Intuitively,
it might be expected that higher humanlikeness en-
tails better task-performance (here, shorter read-
ing/identification times, lower error). The lack of
significant covariation between intrinsic and extrin-
sic measures in our experiments suggests otherwise.
6 Conclusions
Our aim in this paper was to shed light on how
the intrinsic evaluation methodologies that dominate
current comparative HLT evaluations correlate with
human task-performance evaluations more in keep-
ing with NLG traditions. We used the data and sys-
tems from the recent ASGRE Challenge, and com-
pared a total of 17 different evaluation methods for
15 different systems implementing the ASGRE task.
Our most striking result is that none of the met-
rics that assess humanlikeness correlate with any of
the task-performance measures, while strong corre-
lations are observed within the two classes of mea-
199
extrinsic REG string-similarity set-similarity
RT IT ER-2 Min R-SU4 R-2 NIST BLEU SE SEB Dice MASI
RT 1 .8** .46 .18 .10 .05 .54* .39 -.30 .02 .12 .23
IT .8** 1 .59* .56* -.24 -.33 .22 .04 .09 -.31 -.28 -.17
ER-2 .46 .59* 1 .51 -.29 -.36 .03 -.08 .22 -.34 -.39 -.29
Min .18 .56* .51 1 -.76** -.81** -.46 -.66** .79** -.8** -.90** -.79**
R-SU4 .10 -.24 -.29 -.76** 1 .98** .45 .63* -.63* .42 .72** .57*
R-2 .05 -.33 -.36 -.81** .98** 1 .51 .68** -.69** .53* .78** .65**
NIST .54* .22 .03 -.46 .45 .51 1 .94** -.84** .68** .74** .82**
BLEU .39 .04 -.08 -.66** .63* .68** .94** 1 -.96** .82** .89** .93**
SE -.30 .09 .22 .79** -.63* -.69** -.84** -.96** 1 -.92** -.96** -.97**
SEB .02 -.31 -.34 -.8** .42 .53* .68** .82** -.92** 1 .92** .95**
Dice .12 -.28 -.39 -.90** .72** .78** .74** .89** -.96** .92** 1 .97**
MASI .23 -.17 -.29 -.79** .57* .65** .82** .93** -.97** .95** .97** 1
Table 2: Pairwise correlations between all automatic measures and the task-performance results from Exp 2. (? =
significant at .05; ?? at .01). R = ROUGE.
sures ? intrinsic and extrinsic. Somewhat worry-
ingly, our results show that a system?s ability to pro-
duce human-like outputs may be completely unre-
lated to its effect on human task-performance.
Our main conclusions for REG evaluation are that
we need to be cautious in relying on humanlikeness
as a quality criterion, and that we leave extrinsic
evaluation behind at our peril as we move towards
more comparative forms of evaluation.
Given that the intrinsic metrics that dominate in
competetive HLT evaluations are not assessed in
terms of correlation with extrinsic notions of qual-
ity, our results sound a more general note of caution
about using intrinsic measures (and humanlikeness
metrics in particular) without extrinsic validation.
Acknowledgments
We gratefully acknowledge the contribution made to
the evaluations by the faculty and staff at Brighton
University who participated in the identification ex-
periments. Thanks are also due to Robert Dale, Kees
van Deemter, Ielka van der Sluis and the anonymous
reviewers for very helpful comments. The biggest
contribution was, of course, made by the participants
in the ASGRE Challenge who created the systems in-
volved in the evaluations.
References
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proceedings of
the 1st International Conference on Natural Language
Generation (INLG ?00), pages 1?8.
A. Belz and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proceedings of the 2nd UCNLG Workshop: Language
Generation and Machine Translation (UCNLG+MT),
pages 75?83.
A. Belz and E. Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proc. EACL?06,
pages 313?320.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233?263.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proceedings
of the 11th European Workshop on Natural Language
Generation (ENLG?07), pages 49?56.
I. Langkilde. 2002. An empirical verification of cov-
erage and correctness for a general-purpose sentence
generator. In Proceedings of the 2nd International
Natural Language Generation Conference (INLG ?02).
C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL 2003, pages 71?78.
R. Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic an-
notation. In Proceedings of the 5th Language Re-
sources and Evaluation Converence (LREC?06).
200
Proceedings of the Fourth International Natural Language Generation Conference, pages 130?132,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Building a semantically transparent corpus
for the generation of referring expressions
Kees van Deemter and Ielka van der Sluis and Albert Gatt
Department of Computing Science
University of Aberdeen
{kvdeemte,ivdsluis,agatt}@csd.abdn.ac.uk
Abstract
This paper discusses the construction of
a corpus for the evaluation of algorithms
that generate referring expressions. It is
argued that such an evaluation task re-
quires a semantically transparent corpus,
and controlled experiments are the best
way to create such a resource. We address
a number of issues that have arisen in an
ongoing evaluation study, among which is
the problem of judging the output of GRE
algorithms against a human gold standard.
1 Creating and using a corpus for GRE
A decade ago, Dale and Reiter (1995) published
a seminal paper in which they compared a num-
ber of GRE algorithms. These algorithms included
a Full Brevity (FB) algorithm which generates de-
scriptions of minimal length, a greedy algorithm
(GA), and an Incremental Algorithm (IA). The
authors argued that the latter was the best model
of human referential behaviour, and versions of
the IA have since come to represent the state
of the art in GRE. Dale and Reiter?s hypothe-
sis was motivated by psycholinguistic findings,
notably that speakers tend to initiate references
before they have completely scanned a domain.
However, this finding affords different algorithmic
interpretations. Similarly, the finding that basic-
level terms in referring expressions allow hearers
to form a psychological gestalt could be incorpo-
rated into practically any GRE algorithm.1
We decided to put Dale and Reiter?s hypothesis
to the test by an evaluation of the output of dif-
1A separate argument for IA involves tractability, but al-
though some alternatives (such as FB) are intractable, others
(such as GA) are only polynomial, and can therefore not eas-
ily be dismissed on purely computational grounds.
ferent GRE algorithms against human production.
However, it is notoriously difficult to obtain suit-
able corpora for a task that is as semantically in-
tensive as Content Determination (for GRE). Al-
though existing corpora are valuable resources,
NLG often requires information that is not avail-
able in text. Suppose, for example, that a corpus
contained articles about politics, how would the
output of a GRE algorithm be evaluated against the
corpus? It would be difficult to infer from an ar-
ticle exactly which representatives in the British
House of Commons are Liberal Democrats, or
Scottish. Combining multiple texts is hazardous,
since facts could alter across sources and time.
Moreover, the conditions under which such texts
were produced (e.g. fault-critical or not, as ex-
plained below) are hard to determine.
A recent GRE evaluation by Gupta and Stent
(2005) focused on dialogue corpora, using MAP-
TASK and COCONUT, both of which have an as-
sociated domain. Their results show that referent
identification in MAPTASK often requires no more
than a TYPE attribute, so that none of the algo-
rithms performed better than a baseline. In con-
trast to MAPTASK, COCONUT has a more elabo-
rate domain, but it is characterised by a collabora-
tive task, and references frequently go beyond the
identification criterion that is typically invoked in
GRE2. Mindful of the limitations of existing cor-
pora, and of the extent to which evaluation de-
pends on the corpus under study, we are using
controlled experiments to create a corpus whose
construction will ensure that existing algorithms
can be adequately differentiated on an identifica-
tion task.
2Jordan and Walker (2000) have demonstrated a signifi-
cantly better match to the human data when task-related con-
straints are taken into account.
130
2 Setup of the experiment
Like Dale and Reiter (1995), we focused on first-
mention descriptions. However, we decided to in-
clude simple ?disjunctive? references to sets (as
in ?the red chair and the black table?), in addi-
tion to conjunctions of atomic properties, since
these can be handled by essentially the same al-
gorithms (van Deemter, 2002). For generality, we
looked at two very different domains. One of these
involved artificially constructed pictures of furni-
ture, where the available attributes and values are
relatively easy to determine. The other involved
real photographs of individuals, which provide a
richer range of options to subjects. To date, data
has been collected from 19 participants, and anal-
ysis is in progress.
Our first challenge was to make the experiment
naturalistic. Subjects were shown 38 randomised
trials, each depicting a set of objects, one or two
of which were the targets, surrounded by 6 dis-
tractors (Figure 1). In each case, a minimal distin-
guishing description of the targets was available.
Subjects were led to believe that they would be
describing the targets for an interlocutor. Once a
description was typed, the system removed from
the screen what it took to be the referents.
Figure 1: A stimulus example from the furniture domain.
Three groups performed the task in different
conditions, namely: ??FaultCritical?, where
half the subjects in the ?+FaultCritical? case
could use location (?in the top left corner?). The
?+FaultCritical? group was told: ?Our program
will eventually be used in situations where it is
crucial that it understands descriptions accurately.
In these situations, there will often be no option to
correct mistakes. Therefore, (...) you will not get
the chance to revise (your description)?. By con-
trast, the ??FaultCritical? subjects were given
the opportunity to revise their description should
the system have got it wrong. Subjects in the
??Location? condition were told that their inter-
locutor could see exactly the same pictures as they
could, but these had been jumbled up; by con-
trast, ?+Location? subjects were led to believe
that their addressee could see the pictures in ex-
actly the same position.
The second main challenge was to create tri-
als that would distinguish between all the algo-
rithms. For instance, if trials involved only one at-
tribute, say an object?s TYPE (e.g., chair or table),
they would not allow us to distinguish IA from
FB, as both would always generate the shortest de-
scription. Subtler issues arise with local brevity
(Reiter, 1990), an optimisation strategy which re-
quires sufficiently complex trials to make a differ-
ence.
3 How to analyse the data?
Our semantically transparent corpus can be
used for testing various hypotheses, for in-
stance about when an algorithm should
overspecify descriptions (e.g. more in
?+FaultCritical,+Location? (Arts, 2004),
and/or when the target is a set). Here, we focus on
the issue raised in Section 1, namely, which of the
algorithms discussed in Dale and Reiter (1995)
matches human behaviour best.
The first problem is determining the relevant al-
gorithms. The IA comes in different flavours, be-
cause its output depends on the order in which
the different properties are attempted (commonly
called the preference order). It is possible to
consider all different IAs (trying every conceiv-
able preference order), but this would increase the
number of statistical hypotheses to be tested, im-
pacting the validity of the results and requiring a
Bonferroni correction. Instead, we are using a pre-
test to find the optimal version of IA, comparing
only that version to the other algorithms.
The second question is how to assess algorithm
performance. Since our production experiment
does not yield a single gold standard (GS), an al-
gorithm might match subjects better in one con-
dition (e.g. ?+FaultCritical), or perform bet-
ter in one domain (e.g. furniture). Moreover, it
might match subjects poorly overall due to sam-
ple variation, while evincing a perfect match with
a single individual. Using both a by-subjects and a
by-items analysis will partially control for sample
131
dispersion.
How should we calculate the match between an
algorithm and a GS? Once again, there are two
facets to this problem. Since we are focusing on
Content Determination, each human description
could be viewed as associating, with the relevant
trial, a set of properties. Our approach will be to
annotate each human description with the set of at-
tributes it contains. However, the real data is often
messy. For example, when one subject called an
object ?the non-coloured table?, and another called
it ?the grey desk?, both may be expressing the same
attributes (i.e. TYPE and COLOUR). Also, while it
is often assumed that the output of GRE is a def-
inite noun phrase, this is not always the case in
our corpus, which contains indefinite distinguish-
ing descriptions such as ?a red chair, facing to
the right?, and telegraphic messages such as ?red,
right-facing?.
The second aspect to the problem concerns the
actual human-algorithm comparison. Suppose the
GS equals the output of one subject, and we are
comparing two algorithms, x and y. Suppose our
subject produced ?the two huge red sofas?, which
the GS associates with the set {sofa, red, large}.
Suppose our algorithms describe the target as:
Output from x : {sofa, red, top}
Output from y : {sofa, red, large, top}
Which of these algorithms matches the GS best?
Algorithm y adds a property (perhaps overspecify-
ing even more than the GS). Algorithm x has the
same length as the GS, but replaces one property
by another. Several reasonable ways of assess-
ing the differences can be devised, one of which is
Levenshtein distance (which suggests preferring y
over x, since the latter involves a deletion and an
addition) (Levenshtein, 1966). We also intend to
examine how often the GS over- or underspecifies
where the algorithm does not.
4 Conclusion
Corpora can be an invaluable resource for NLG
as long as the necessary contextual information
and the conditions under which the texts in a cor-
pus were produced are known. We believe that
controlled and balanced experiments are needed
for building semantically transparent resources,
whose construction we have discussed. As shown
in this paper, evaluation of algorithms against the
number of gold standards obtained with such a
corpus needs careful consideration.
Evaluation of GRE ? and NLG systems more
generally ? would benefit from more investiga-
tion of the differences between readers and pro-
ducers. In future work, we intend to follow up
with a reader-oriented experiment in which we test
the speed and/or accuracy with which the output
of different GRE algorithms is understood by sub-
jects. The dependent variables here will be non-
linguistic (perhaps involving subjects clicking on
pictures of presumed target referents). This illus-
trates a more general issue in this area, namely
that corpora should, in our view, only be a start-
ing point, with which data of different kinds can
be associated.
5 Acknowledgments
Thanks to Ehud Reiter, Richard Power
and Emiel Krahmer for useful comments.
This work is part of the TUNA project
(http://www.csd.abdn.ac.uk/
research/tuna/), funded by the EPSRC
in the UK (GR/S13330/01).
References
[Arts2004] A. Arts. 2004. Overspecification in Instruc-
tive Texts. Ph.D. thesis, Tilburg University.
[Dale and Reiter1995] R. Dale and E. Reiter. 1995.
Computational interpretations of the Gricean max-
ims in the generation of referring expressions. Cog-
nitive Science, 18:233?263.
[van Deemter2002] K. van Deemter. 2002. Generat-
ing referring expressions: Boolean extensions of the
incremental algorithm. Computational Linguistics,
28(1):37?52.
[Gupta and Stent2005] S. Gupta and A. J. Stent. 2005.
Automatic evaluation of referring expression gener-
ation using corpora. In Proceedings of the 1st Work-
shop on Using Corpora in NLG, Birmingham, UK.
[Jordan and Walker2000] P. Jordan and M. Walker.
2000. Learning attribute selections for non-
pronominal expressions. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics.
[Levenshtein1966] V. Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and rever-
sals. Soviet Physics Doklady, 10(8):707?710.
[Reiter1990] E. Reiter. 1990. The computational com-
plexity of avoiding conversational implicatures. In
Proceedings of the 28th ACL Meeting, pages 97?
104. MIT Press.
132
Proceedings of the 12th European Workshop on Natural Language Generation, pages 90?93,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
SimpleNLG: A realisation engine for practical applications
Albert Gatt and Ehud Reiter
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
{a.gatt,e.reiter}@abdn.ac.uk
Abstract
This paper describes SimpleNLG, a re-
alisation engine for English which aims
to provide simple and robust interfaces to
generate syntactic structures and linearise
them. The library is also flexible in al-
lowing the use of mixed (canned and non-
canned) representations.
1 Introduction
Over the past several years, a significant consensus
has emerged over the definition of the realisation
task, through the development of realisers such as
REALPRO (Lavoie and Rambow, 1997), ALETH-
GEN (Coch, 1996), KPML (Bateman, 1997),
FUF/SURGE (Elhadad and Robin, 1996), HALO-
GEN (Langkilde, 2000), YAG (McRoy et al, 2000),
and OPENCCG (White, 2006).
Realisation involves two logically distinguish-
able tasks. Tactical generation involves making
appropriate linguistic choices given the semantic
input. However, once tactical decisions have been
taken, building a syntactic representation, apply-
ing the right morphological operations, and lin-
earising the sentence as a string are comparatively
mechanical tasks. With the possible exception
of template-based realisers, such as YAG, exist-
ing wide-coverage realisers usually carry out both
tasks. By contrast, a realisation engine focuses on
the second of the two tasks, making no commit-
ments as to how semantic inputs are mapped to
syntactic outputs. This leaves the (tactical) prob-
lem of defining mappings from semantic inputs
to morphosyntactic structures entirely up to the
developer, something which may be attractive in
those applications where full control of the out-
put of generation is required. Such control is not
always easily available in wide-coverage tactical
generators, for a number of reasons:
1. Many such realisers define an input formal-
ism, which effectively circumscribes the (se-
mantic) space of possibilities that the realiser
handles. The developer needs to ensure that
the input to realisation is mapped to the req-
uisite formalism.
2. Since the tactical problem involves search
through a space of linguistic choices, the
broader the coverage, the more efficiency
may be compromised. Where real-time de-
ployment is a goal, this may be an obstacle.
3. Many application domains have sub-
language requirements. For example, the
language used in summaries of weather data
(Reiter et al, 2005) or patient information
(Portet et al, to appear) differs from standard
usage, and does not always allow variation
to the same extent. Since realisers don?t
typically address such requirements, their
use in a particular application may require
the alteration of the realiser?s rule-base or,
in the case of statistical realisers, re-training
on large volumes of appropruately annotated
data.
This paper describes SimpleNLG, a realisa-
tion engine which grew out of recent experiences
in building large-scale data-to-text NLG systems,
whose goal is to summarise large volumes of nu-
meric and symbolic data (Reiter, 2007). Sub-
language requirements and efficiency are impor-
tant considerations in such systems. Although
meeting these requirements was the initial motiva-
tion behind SimpleNLG, it has since been devel-
oped into an engine with significant coverage of
English syntax and morphology, while at the same
time providing a simple API that offers users di-
rect programmatic control over the realisation pro-
cess.
90
Feature Values Applicable classes
lexical ADJPOSITION Attrib1/2/3, PostNominal, Predicative ADJ
ADVPOSITION Sentential, PostVerbal, Verbal ADV
AGRTYPE Count, Mass, Group, Inv-Pl, Inv-Sg N
COMPLTYPE AdjP, AdvP, B-Inf, WhFin, WhInf, . . . V
VTYPE Aux, Main, Modal V
phrasal FUNCTION Subject, Obj, I-Obj, Prep-Obj, Modifier all
SFORM B-Inf, Gerund, Imper, Inf, Subj S
INTERROGTYPE Yes/No, How, What, . . . S
NUMBERAGR Plural, Singular NP
TENSE Pres, Past, Fut VP
TAXIS (boolean) true (=perfective), false VP
POSSESSIVE (boolean) true (=possessive), false NP
PASSIVE (boolean) true, false VP
Table 1: Features and values available in SimpleNLG
2 Overview of SimpleNLG
SimpleNLG is a Java library that provides inter-
faces offering direct control over the realisation
process, that is, over the way phrases are built and
combined, inflectional morphological operations,
and linearisation. It defines a set of lexical and
phrasal types, corresponding to the major gram-
matical categories, as well as ways of combining
these and setting various feature values. In con-
structing a syntactic structure and linearising it as
text with SimpleNLG, the following steps are un-
dertaken:
1. Initialisation of the basic constituents re-
quired, with the appropriate lexical items;
2. Using the operations provided in the API to
set features of the constituents, such as those
in bottom panel of Table 1;
3. Combining constituents into larger struc-
tures, again using the operations provided in
the API which apply to the constituents in
question;
4. Passing the resulting structure to the lin-
eariser, which traverses the constituent struc-
ture, applying the correct inflections and lin-
ear ordering depending on the features, be-
fore returning the realised string.
Constituents in SimpleNLG can be a mixture
of canned and non-canned representations. This
is useful in applications where certain inputs can
be mapped to an output string in a deterministic
fashion, while others require a more flexible map-
ping to outputs depending, for example, on seman-
tic features and context. SimpleNLG tries to meet
these needs by providing significant syntactic cov-
erage with the added option of combining canned
and non-canned strings.
Another aim of the engine is robustness: struc-
tures which are incomplete or not well-formed will
not result in a crash, but typically will yield infe-
licitous, though comprehensible, output. This is a
feature that SimpleNLG shares with YAG (McRoy
et al, 2000). A third design criterion was to
achieve a clear separation between morphological
and syntactic operations. The lexical component
of the library, which includes a wide-coverage
morphological generator, is distinct from the syn-
tactic component. This makes it useful for applica-
tions which do not require complex syntactic op-
erations, but which need output strings to be cor-
rectly inflected.
2.1 Lexical operations
The lexical component provides interfaces that de-
fine a Lexicon, a MorphologicalRule, and
a LexicalItem, with subtypes for different lex-
ical classes (Noun, Preposition etc). Mor-
phological rules, a re-implementation of those in
MORPHG (Minnen et al, 2001), cover the full
range of English inflection, including regular and
irregular forms1. In addition to the range of mor-
phological operations that apply to them, various
features can be specified for lexical items. For ex-
ample, as shown in the top panel of Table 1, ad-
jectives and adverbs can be specified for their typ-
ical syntactic positions. Thus, an adjective such
as red would have the values Attrib2, indicating
that it usually occurs in attribute position 2 (fol-
lowing Attrib1 adjectives such as large), and Pred-
icative. Similarly, nouns are classified to indicate
1Thanks are due to John Carroll at the University of Sus-
sex for permission to re-use these rules.
91
their agreement features (count, mass, etc), while
verbs can be specified for the range of syntactic
complement types they allow (e.g. bare infinitives
and WH-complements).
A typical development scenario involves the
creation of a Lexicon, the repository of the rel-
evant items and their properties. Though this
can be done programmatically, the current distri-
bution of SimpleNLG provides an interface to a
database constructed from the NIH Specialist Lexi-
con2, a large (> 300,000 entries) repository of lex-
ical items in the medical and general English do-
mains, which incorporates information about lexi-
cal features such as those in Table 1.
2.2 Syntactic operations
The syntactic component of SimpleNLG de-
fines interfaces for HeadedPhrase and
CoordinatePhrase. Apart from various
phrasal subtypes (referred to as PhraseSpecs)
following the usage in Reiter and Dale (2000)),
several grammatical features are defined, includ-
ing Tense, Number, Person and Mood (see
Table 1). In addition, a StringPhraseSpec
represents a piece of canned text of arbitrary
length.
A complete syntactic structure is achieved by
initialising constituents with the relevant fea-
tures, and combining them using the operations
specified by the interface. Any syntactic struc-
ture can consist of a mixture of Phrase or
CoordinatePhrase types and canned strings.
The input lexical items to phrase constructors can
themselves be either strings or lexical items as de-
fined in the lexical component. Once syntactic
structures have been constructed, they are passed
to a lineariser, which also handles basic punctua-
tion and other orthographic conventions (such as
capitalisation).
The syntactic component covers the full range
of English verbal forms, including participals,
compound tenses, and progressive aspect. Sub-
types of CoordinatePhrase allow for fully
recursive coordination. As shown in the bottom
panel of Figure 1, subjunctive forms and different
kinds of interrogatives are also handled using the
same basic feature-setting mechanism.
The example below illustrates one way of con-
structing the phrase the boys left the house, ini-
2http://lexsrv3.nlm.nih.gov/
SPECIALIST/index.html
tialising a sentence with the main verb leave
and setting a Tense feature. Note that the
SPhraseSpec interface allows the setting of the
main verb, although this is internally represented
as the head of a VPPhraseSpec dominated by
the clause. An alternative would be to construct
the verb phrase directly, and set it as a constituent
of the sentence. Similarly, the direct object, which
is specified directly as a constituent of the sen-
tence, is internally represented as the object of the
verb phrase. In this example, the direct object
is an NPPhraseSpec consisting of two words,
passed as arguments and internally rendered as
lexical items of type Determiner and Noun re-
spectively. By contrast, the subject is defined as a
canned string.
(1) Phrase s1 =
new SPhraseSpec(?leave?);
s1.setTense(PAST);
s1.setObject(
new NPPhraseSpec(?the?, ?house?));
Phrase s2 =
new StringPhraseSpec(?the boys?);
s1.setSubject(s2);
Setting the INTERROGATIVETYPE feature of
sentence (1) turns it into a question. Two exam-
ples, are shown below. While (2) exemplifies a
simple yes/no question, in (3), a WH-constituent
is specified as establishing a dependency with the
direct object (the house).
(2) s1.setInterrogative(YES NO);
(Did the boys leave home?)
(3) s1.setInterrogative(WHERE, OBJECT);
(Where did the boys leave?)
In summary, building syntactic structures in
SimpleNLG is largely a question of feature setting,
with no restrictions on whether representations are
partially or exclusively made up of canned strings.
2.2.1 Interaction of lexicon and syntax
The phrasal features in the bottom panel of Table 1
determine the form of the output, since they are
automatically interpreted by the realiser as instruc-
tions to call the correct morphological operations
on lexical items. Hence, the syntactic and morpho-
logical components are closely integrated (though
distinct). Currently, however, lexical features such
as ADJPOSITION are not fully integrated with the
syntactic component. For example, although ad-
jectives in the lexicon are specified for their po-
sition relative to other modifiers, and nouns are
92
specified for whether they take singular or plural
agreement, this informaiton is not currently used
automatically by the realiser. Full integration of
lexical features and syntactic realisation is cur-
rently the focus of ongoing development.
2.3 Efficiency
As an indication of efficiency, we measured the
time taken to realise 26 summaries with an aver-
age text length of 160.8 tokens (14.4 sentences),
and sentences ranging in complexity from simple
declaratives to complex embedded clauses3. The
estimates, shown below, average over 100 itera-
tions per text (i.e. a total of 2600 runs of the re-
aliser) on a Dell Optiplex GX620 machine running
Windows XP with a 3.16 GHz Pentium proces-
sor. Separate times are given for the initialisation
of constituents based on semantic representations,
along the lines shown in (1), (SYN), and linearisa-
tion (LIN). These figures suggest that a medium-
length, multiparagraph text can be rendered in un-
der a second in most cases.
MEAN (ms) SD MIN MAX
SYN 280.7 229.7 13.8 788.34
LIN 749.38 712.6 23.26 2700.38
3 Conclusions and future work
This paper has described SimpleNLG, a realisa-
tion engine which differs from most tactical gen-
erators in that it provides a transparent API to carry
out low-level tasks such as inflection and syntac-
tic combination, while making no commitments
about input specifications or input-output map-
pings.
The simplicity of use of SimpleNLG is reflected
in its community of users. The currently avail-
able public distribution4, has been used by several
groups for three main purposes: (a) as a front-end
to NLG systems in projects where realisation is not
the primary research focus; (b) as a simple natu-
ral language component in user interfaces for other
kinds of systems, by researchers who do not work
in NLG proper; (c) as a teaching tool in advanced
undergraduate and postgraduate courses on Natu-
ral Language Processing.
SimpleNLG remains under continuous develop-
ment. Current work is focusing on the inclusion of
output formatting and punctuation modules, which
3The system that generates these summaries is fully de-
scribed by Portet et al (to appear).
4SimpleNLG is available, with exhaus-
tive documentation, at the following URL:
http://www.csd.abdn.ac.uk/?ereiter/simplenlg/.
are currently handled using simple defaults. More-
over, an enhanced interface to the lexicon is being
developed to handle derivational morphology and
a fuller integration of complementation frames of
lexical items with the syntactic component.
References
J. A. Bateman. 1997. Enabling technology for multi-
lingual natural language generation: the KPML de-
velopment environment. Natural Language Engi-
neering, 3(1):15?55.
J. Coch. 1996. Overview of AlethGen. In Proceedings
of the 8th International Natural Language Genera-
tion Workshop.
M. Elhadad and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic realiza-
tion component. In Proceedings of the 8th Interna-
tional Natural Language Generation Workshop.
I. Langkilde. 2000. Forest-based statistical language
generation. In Proceedings of the 1st Meeting of
the North American Chapter of the Association for
Computational Linguistics.
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on Applied Natural Language
Processing.
S.W. McRoy, S. Channarukul, and S. Ali. 2000. YAG:
A template-based generator for real-time systems.
In Proceedings of the 1st International Conference
on Natural Language Generation.
G. Minnen, J. J. Carroll, and D. Pearce. 2001. Ap-
plied morphological processing of English. Natural
Language Engineering, 7(3):207?223.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. to appear. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press, Cambridge, UK.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
E. Reiter. 2007. An architecture for Data-to-Text sys-
tems. In Proceedings of the 11th European Work-
shop on Natural Language Generation.
M. White. 2006. Chart realization from disjunctive
inputs. In Proceedings of the 4th International Con-
ference on Natural Language Generation.
93
Proceedings of the 12th European Workshop on Natural Language Generation, pages 98?101,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Hearer-oriented Evaluation of Referring Expression Generation ?
Imtiaz H. Khan, Kees van Deemter, Graeme Ritchie, Albert Gatt, Alexandra A. Cleland
University of Aberdeen, Aberdeen, Scotland, United Kingdom
{i.h.khan,k.vdeemter,g.ritchie,a.gatt,a.cleland}@abdn.ac.uk
Abstract
This paper discusses the evaluation of a
Generation of Referring Expressions algo-
rithm that takes structural ambiguity into
account. We describe an ongoing study
with human readers.
1 Introduction
In recent years, the NLG community has seen a
substantial number of studies to evaluate Gener-
ation of Referring Expressions (GRE) algorithms,
but it is still far from clear what would constitute
an optimal evaluation method. Two limitations
stand out in the bulk of existing work. Firstly,
most existing evaluations are essentially speaker-
oriented, focussing on the degree of ?human-
likeness? of the generated descriptions, disre-
garding their effectiveness (e.g. Mellish and Dale
(1998), Gupta and Stent (2005), van Deemter et al
(2006), Belz and Kilgarriff (2006), Belz and Re-
iter (2006), Paris et al (2006), Viethen and Dale
(2006), Gatt and Belz (2008)). The limited num-
ber of exceptions to this rule indicate that the dif-
ferences between the two approaches to evaluation
can be substantial (Gatt and Belz, 2008). Sec-
ondly, most evaluations have focussed on the se-
mantic content of the generated descriptions, as
produced by the Content Determination stage of
a GRE algorithm; this means that linguistic re-
alisation (i.e. the choice of words and linguistic
constructions) is usually not addressed (exceptions
are: Stone and Webber (1998), Krahmer and The-
une (2002), Siddharthan and Copestake (2004)).
Our aim is to build GRE algorithms that produce
referring expressions that are of optimal benefit to
a hearer. That is, we are interested in generating
descriptions that are easy to read and understand.
But the readability and intelligibility of a descrip-
tion can crucially depend on the way in which it is
? This work is supported by a University of Aberdeen
Sixth Century Studentship, and EPSRC grant EP/E011764/1.
worded. This happens particularly when there is
potential for misunderstanding, as can happen in
the case of attachment and scope ambiguities.
Suppose, for example, one wants to make it
clear that all radical students and all radical teach-
ers are in agreement with a certain idea. It might
be risky to express this as ?the radical students and
teachers are agreed?, since the reader1 might be
inclined to interpret this as pertaining to all teach-
ers rather than only the radical ones. For this rea-
son, a GRE program might opt for the longer noun
phrase ?the radical students and the radical teach-
ers?. But because this expression is lengthier, the
choice involves a compromise between compre-
hensibiliity and brevity, a special case of a diffi-
cult trade-off that is typical of generation as well
as interpretation of language (van Deemter, 2004).
We previously reported the design of an algo-
rithm (based on an earlier work on expressions re-
ferring to sets (Gatt, 2007)), which was derived
from experiments in which readers were asked to
express their preference between different descrip-
tions and to respond to instructions which used a
variety of phrasings (Khan et al, 2008). Here we
discuss the issues that arise when such an algo-
rithm is evaluated in terms of its benefits for read-
ers.
2 Summary of the algorithm
In order to study specific data, we have focussed
on the construction illustrated in Section 1 above:
potentially ambiguous Noun Phrases of the gen-
eral form the Adj Nouni and Nounj . For such
phrases, there are potentially two interpretations:
wide scope (Adj modifies both Nouni and Nounj)
or narrow scope (Adj modifies Nouni but not
Nounj).
Our algorithm starts from an unambiguous set-
theoretic formula over lexical items (i.e. words
1In this paper, we use the word reader and hearer inter-
changeably.
98
have already been chosen), and thus has to choose
between a number of different realisations. The
possible phrasings for the wide scope meaning are:
(1) the Adj Noun1 and Noun2, (2) the Adj Noun2
and Noun1, (3) the Adj Noun1 and the Adj Noun2,
and (4) the Adj Noun2 and the Adj Noun1. For nar-
row scope, the possibilities are: (1) the Adj Noun1
and Noun2, (2) the Noun2 and Adj Noun1, (3) the
Adj Noun1 and the Noun2, and (4) the Noun2 and
the Adj Noun1. For our purposes, (1) and (2) are
designated as ?brief?, (3) and (4) as ?non-brief?
(that is, ?brevity? has a specialised sense involv-
ing the presence/absence of ?the? and possibly Adj
before the second Noun). Importantly, the ?non-
brief? expressions are syntactically unambiguous,
but the ?brief? NPs are potentially ambiguous, and
hence are the focus of attention in this work.
Our algorithm is based on certain specific hy-
potheses (from the earlier experiments) which
make crucial use of corpus data concerning the
frequency of two types of collocations: the col-
location between an adjective and a noun, and the
collocation between two nouns. At a broader level,
we hypothesise: the most likely reading of an NP
can be predicted using corpus data (Word Sketches
(Kilgarriff, 2003)). The more specific hypotheses
derive from earlier work by Kilgarriff (2003) and
Chantree et al (2006), and were further developed
and tested in our previous experiments. The cen-
tral idea is that this statistical information can be
used to predict a ?most likely? scoping (and hence
interpretation) for the adjective in the ?brief? (i.e.
potentially ambiguous) NPs. We define an NP to
be predictable if our model predicts a single read-
ing for it; otherwise it is unpredictable. Hence, all
?non-brief? NPs are predictable (being unambigu-
ous), but only some of the ?brief? ones are pre-
dictable.
In a nutshell, the model underlying our algo-
rithm prefers predictable expressions to unpre-
dictable ones, but if several of the expressions are
predictable then brief expressions are preferred
over non-brief.
3 Aims of the study
We want to find out whether our generator
makes the best possible choices (for hearers) from
amongst the different ways in which a given de-
scription can be realised. But although our al-
gorithm uses sophisticated strategies for avoiding
noun phrases that it believes to be liable to mis-
understanding, misunderstandings cannot be ruled
out, and if a hearer misunderstands a noun phrase
then secondary aspects such as reading (and/or
comprehension) speed are of little consequence.
We therefore plan first to find out the likelihood of
misunderstanding. For this reason, we will report
on the degree of accuracy, as a percentage of times
that a participant?s understanding of an expression
that we label as predictable fails to match the in-
terpretation assigned by our model. Additionally,
we shall statistically test two hypotheses:
Comprehension Accuracy 1: Predictable ex-
pressions are more often interpreted in
agreement than in disagreement with the
model.
Comprehension Accuracy 2: There is more
agreement among participants on the inter-
pretation of predictable expressions than of
unpredictable expressions.
We will not only test the comprehensibility of the
expressions generated by our algorithm, but their
readability and intelligibility as well. This is nec-
essary because the experiments which led to the
algorithm design considered only certain aspects
of the hearer?s reaction to NPs (e.g. metalinguistic
judgements about a participant?s preferences) and
we wish to check these comprehensibility/brevity
facets from a different, perhaps psycholinguisti-
cally more valid, perspective. It is also necessary
because avoidance of misunderstandings is not the
only decisive factor: if several of the expressions
are predictable then our algorithm chooses be-
tween them by preferring brevity. But why is brief
better than non-brief? Taking readability and intel-
ligibility together as ?processing speed?, our third
hypothesis is:
Processing speed: Subjects process
predictable brief expressions more
quickly than predictable non-brief ones.
Confirmation of this hypothesis would be a strong
indication that our algorithm is on the right track,
particularly if the degree of accuracy (see above)
turns out to be high. Processing speed is a com-
plex concept, but we could decompose it as ?read-
ing speed? and ?comprehension speed?, permitting
us to examine reading and comprehension sepa-
rately. We intend to see what evidence there is for
the following additional propositions, which will
be tested solely to aid our understanding.
99
Reading Speed:
RS1: Subjects read predictable brief NPs more
quickly than unpredictable brief ones.
RS2: Subjects read unpredictable brief NPs more
quickly than predictable non-brief ones.
RS3: Subjects read predictable brief NPs more
quickly than predictable non-brief ones.
Comprehension Speed:
CS1: Subjects comprehend predictable brief NPs
more quickly than unpredictable brief ones.
CS2: Subjects comprehend predictable non-brief
NPs more quickly than unpredictable brief ones.
CS3: Subjects do not comprehend predictable
non-brief NPs more quickly than predictable brief
ones.
(Remember that, in our restricted set of NPs, a
phrase cannot be both ?unpredictable? and ?non-
brief?.) Rejection of any of these statements will
not count against our algorithm.
4 Sketch of experimental procedure
Participants will be presented with a sequence of
trials (on a computer screen), each of which con-
sists of a lead-in sentence followed by a target sen-
tence and a comprehension question that relates to
the two sentences together. The target sentence
might for example say ?the radical students and
teachers were waving their hands?. The compre-
hension question in this case could be ?Were the
moderate teachers waving their hands??. As both
the target sentence and the comprehension ques-
tion make use of definite NPs (e.g. ?the moderate
teachers?), it is necessary to ensure any presuppo-
sitions about the existence of the referent set are
met, without biasing the answer. For this reason,
the target sentence is preceded by a lead-in sen-
tence to establish the existence of the sets within
the discourse (here, ?there were radical and mod-
erate people in a rally?).
Given this set-up we are confident that we
can identify, from a participant?s yes/no answer,
whether the NP in the target sentence was assigned
a narrow-scope or a wide-scope reading for the ad-
jective. The computer will record the participant?s
response as well as the length of time that the par-
ticipant took to answer the question. We will use
Linger2 for presentation of stimuli. Pilots sug-
gest that the complexity of the trials makes it ad-
visable to use masked sentence-based self-paced
2http://tedlab.mit.edu/?dr/Linger/
reading, in which every press of the space bar re-
veals the next sentence and the previous sentence
is replaced by dashes.
The choice of nouns and adjectives (to construct
NPs) is motivated by the fact that there is a bal-
anced distribution of NPs in each of the follow-
ing three classes. Wide scope class is the one for
which our model predicts a wide-scope reading;
narrow scope class is the one for which our model
predicts a narrow-scope reading; and ambiguous
class is the one for which our model fails to pre-
dict a single reading (Khan et al, 2008).
5 Issues emerging from this study
The design of this experiment raised some difficult
questions, some quite unexpected:
1. The quality of the output of a generation al-
gorithm might appear to be a simple and well-
understood concept. However, output quality is
multi-faceted, because an expression may be easy
to read but difficult to process semantically, or the
other way round. A thorough output evaluation
should address both aspects of quality, in our view.
2. If both reading and understanding are ad-
dressed, this raises the question of how these
two dimensions should be traded off against each
other. If one algorithm?s output was read more
quickly than that of another, but understood more
slowly than the second, which of the two should be
preferred? Perhaps there is a legitimate role here
for metalinguistic judgments after all, in which
participants are asked to express their preference
between expressions (see Paraboni et al (2006) for
discussion)? An alternative point of view is that
these questions are impossible to answer indepen-
dent of a realistic setting in which participants ut-
ter sentences with a concrete communicative pur-
pose in mind. If utterances were made in order to
accomplish a concrete task (e.g., to win a game)
then task-based evaluation would be possible.
3. Even though this paper has not focussed on de-
tails of experimental design and analysis, one diffi-
culty is worth mentioning: given the grammatical
options between which the generator is choosing,
only three types of situations are represented: a de-
scription can be brief and predictable (e.g. using
?the old men and women? to convey wide scope,
since the adjective is predicted by our algorithm
to have wide scope), brief and unpredictable (e.g.
?the rowing boats and ships? for wide scope, given
100
a prediction of narrow scope), or non-brief and
predictable (e.g. ?the old men and the old women?
for wide scope). It might appear that there exists
a fourth option: non-brief and unpredictable. But
this is ruled out by our technical sense of ?non-
brief?: as noted earlier, ?non-brief? NPs do not
have the scope ambiguity. Because of this ?miss-
ing cell?, it will not be possible to analyse our data
using an ANOVA test, which would have automat-
ically taken care of all possible interactions be-
tween comprehensibility and brevity. A number
of different tests will be used instead, with Bon-
ferroni corrections where necessary.
6 Conclusion
Human-based evaluation is gaining considerable
popularity in the NLG community. Whereas eval-
uation of GRE has mostly been speaker-oriented,
the present paper has explored a plan for an ex-
perimental hearer-oriented evaluation. The main
conclusion is that hearer-based evaluation is diffi-
cult because the quality of a generated expression
can be measured in different ways, whose results
cannot be assumed to match. One factor we have
not examined is the notion of fluency: it is possible
that our algorithm will sometimes choose a word
order (e.g. ?the women and old men?) that is rela-
tively infrequent, and therefore lacking in fluency.
Such situations might lead to longer reading times.
References
A. Belz and A. Kilgarriff. 2006. Shared-task evalu-
ations in HLT: Lessons for NLG. In Proceedings
of the 4th International Conference on Natural Lan-
guage Generation, pages 133?135.
A. Belz and E. Reiter. 2006. Comparing automatic
and human evaluation of NLG systems. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 313?320, Trento, Italy, 3-7 April.
F. Chantree, B. Nuseibeh, A. de Roeck, and A. Willis.
2006. Identifying nocuous ambiguities in require-
ments specifications. In Proceedings of 14th IEEE
International Requirements Engineering conference
(RE?06), Minneapolis/St. Paul, Minnesota, U.S.A.
A. Gatt and A. Belz. 2008. Attribute selection for re-
ferring expression generation: New algorithms and
evaluation methods. In Proceedings of the 5th Inter-
national Conference on NLG.
A. Gatt. 2007. Generating Coherent References to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, Aberdeen, Scotland.
S. Gupta and A. Stent. 2005. Automatic evaluation
of referring expression generation using corpora. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation, pages 1?6.
I. H. Khan, K. van Deemter, and G. Ritchie. 2008.
Generation of referring expressions: Managing
structural ambiguities. In Proceedings of the 22nd
International Conference on Computational Lin-
guistics (COLING-8), pages 433?440, Manchester.
A. Kilgarriff. 2003. Thesauruses for natural language
processing. In Proceedings of NLP-KE, pages 5?13,
Beijing, China.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Information
Sharing: Reference and Presupposition in Language
Generation and Interpretation, CSLI Publications,
pages 223?264.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12(4):349?373.
I. Paraboni, J. Masthoff, and K. van Deemter. 2006.
Overspecified reference in hierarchical domain:
measuring the benefits for readers. In Proceedings
of the Fourth International Conference on Natural
Language Generation(INLG), pages 55?62.
C. Paris, N. Colineau, and R. Wilkinson. 2006. Eval-
uations of NLG systems: Common corpus and tasks
or common dimensions and metrics? In Proceed-
ings of the 4th International Conference on Natural
Language Generation, pages 127?129.
A. Siddharthan and A. Copestake. 2004. Generating
referring expressions in open domains. In Proceed-
ings of the 42nd Meeting of the Association for Com-
putational Linguistics Annual Conference (ACL-04).
M. Stone and B. Webber. 1998. Textual economy
through close coupling of syntax and semantics. In
Proceedings of the Ninth International Workshop on
Natural Language Generation, pages 178?187, New
Brunswick, New Jersey.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the
generation of referring expressions. In Proceedings
of the 4th International Conference on Natural Lan-
guage Generation, pages 130?132.
K. van Deemter. 2004. Towards a probabilistic version
of bidirectional OT syntax and semantics. Journal
of Semantics, 21(3):251?281.
J. Viethen and R. Dale. 2006. Towards the evaluation
of referring expression generation. In Proceedings
of the 4th Australasian Language Technology Work-
shop, pages 115?122, Sydney, Australia.
101
Proceedings of the 12th European Workshop on Natural Language Generation, pages 174?182,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
The TUNA-REG Challenge 2009: Overview and Evaluation Results
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Anja Belz Eric Kow
Natural Language Technology Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Abstract
The TUNA-REG?09 Challenge was one
of the shared-task evaluation competitions
at Generation Challenges 2009. TUNA-
REG?09 used data from the TUNA Cor-
pus of paired representations of enti-
ties and human-authored referring expres-
sions. The shared task was to create sys-
tems that generate referring expressions
for entities given representations of sets
of entities and their properties. Four
teams submitted six systems to TUNA-
REG?09. We evaluated the six systems and
two sets of human-authored referring ex-
pressions using several automatic intrinsic
measures, a human-assessed intrinsic eval-
uation and a human task performance ex-
periment. This report describes the TUNA-
REG task and the evaluation methods used,
and presents the evaluation results.
1 Introduction
This year?s run of the TUNA-REG Shared-Task
Evaluation Competition (STEC) is the third, and
final, competition to involve the TUNA Corpus of
referring expressions. The TUNA Corpus was first
used in the Pilot Attribute Selection for Gener-
ating Referring Expressions (ASGRE) Challenge
(Belz and Gatt, 2007) which took place between
May and September 2007; and again for three of
the shared tasks in Referring Expression Genera-
tion (REG) Challenges 2008, which ran between
September 2007 and May 2008 (Gatt et al, 2008).
This year?s TUNA Task replicates one of the three
tasks from REG?08, the TUNA-REG Task. It uses
the same test data, to enable direct comparison
against the 2008 results. Four participating teams
submitted 6 different systems this year; teams and
their affiliations are shown in Table 1.
Team ID Affiliation
GRAPH Macquarie, Tilburg and Twente Universities
IS ICSI, University of California
NIL-UCM Universidad Complutense de Madrid
USP University of Sa?o Paolo
Table 1: TUNA-REG?09 Participants.
2 Data
Each file in the TUNA corpus1 consists of a sin-
gle pairing of a domain (a representation of 7 en-
tities and their attributes) and a human-authored
description for one of the entities (the target refer-
ent). Some domains represent sets of people, some
represent items of furniture (see also Table 2). The
descriptions were collected in an online elicita-
tion experiment which was advertised mainly on
a website hosted at the University of Zurich Web
Experimentation List2 (a web service for recruit-
ing subjects for experiments), and in which partic-
ipation was not controlled or monitored. In the
experiment, participants were shown pictures of
the entities in the given domain and were asked to
type a description of the target referent (which was
highlighted in the visual display). The main condi-
tion3 manipulated in the experiment was +/?LOC:
in the +LOC condition, participants were told that
they could refer to entities using any of their prop-
erties (including their location on the screen). In
the ?LOC condition, they were discouraged from
doing so, though not prevented.
The XML format we have been using in the
TUNA-REG STECs, shown in Figure 1, is a vari-
ant of the original format of the TUNA corpus.
The root TRIAL node has a unique ID and an
indication of the +/ ? LOC experimental condi-
1http://www.csd.abdn.ac.uk/research/tuna/
2http://genpsylab-wexlist.unizh.ch
3The elicitation experiment had an additional independent
variable, manipulating whether descriptions were elicited in a
?fault-critical? or ?non-fault-critical? condition. For the shared
tasks this was ignored by collapsing all the data in these two
conditions.
174
tion. The DOMAIN node contains 7 ENTITY nodes,
which themselves contain a number of ATTRIBUTE
nodes defining the possible properties of an en-
tity in attribute-value notation. The attributes in-
clude properties such as an object?s colour or
a person?s clothing, and the location of the im-
age in the visual display which the DOMAIN rep-
resents. Each ENTITY node indicates whether it
is the target referent or one of the six distrac-
tors, and also has a pointer to the image that it
represents. The WORD-STRING is the actual de-
scription typed by one of the human authors, the
ANNOTATED-WORD-STRING is the description with
substrings annotated with the attributes they re-
alise, while the ATTRIBUTE-SET contains the set of
attributes only. The ANNOTATED-WORD-STRING and
ATTRIBUTE-SET nodes were provided in the train-
ing and development data only, to show how sub-
strings of a human-authored description mapped
to attributes.
<TRIAL CONDITION="+/-LOC" ID="...">
<DOMAIN>
<ENTITY ID="..." TYPE="target" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
<ENTITY ID="..." TYPE="distractor" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
...
</DOMAIN>
<WORD-STRING>
string describing the target referent
</WORD-STRING>
<ANNOTATED-WORD-STRING>
string in WORD-STRING annotated
with attributes in ATTRIBUTE-SET
</ANNOTATED-WORD-STRING>
<ATTRIBUTE-SET>
set of domain attributes in the description
</ATTRIBUTE-SET>
</TRIAL>
Figure 1: XML format of corpus items.
Apart from differences in the XML format, the
data used in the TUNA-REG Task also differs from
the original TUNA corpus in that it has only the sin-
gular referring expressions from the original cor-
pus, and in that we have added to it the files of
images of entities that the XML mark-up points to.
The test set, which was constructed for the
2008 run of the TUNA-REG Task, consists of 112
items, each with a different domain paired with
two human-authored descriptions. The items are
distributed equally between furniture items and
people, and between both experimental conditions
(+/ ? LOC). In the following sections, the two
sets of human descriptions will be referred to as
HUMAN-1 and HUMAN-2.4 The numbers of files
in the training, development and test sets, as well
as in the people and furniture subdomains, are
shown in Table 2.
Furniture People All
Training 319 274 593
Development 80 68 148
Test 56 56 112
All 455 398 853
Table 2: TUNA-REG data: subset sizes.
3 The TUNA-REG Task
Referring Expression Generation (REG) has been
the subject of intensive research in the NLG com-
munity, giving rise to substantial consensus on the
problem definition, as well as the nature of the in-
puts and outputs of REG algorithms. Typically,
such algorithms take as input a domain, consist-
ing of entities and their attributes, together with an
indication of which is the intended referent, and
output a set of attributes true of the referent which
distinguish it from other entities in the domain.
The TUNA-REG task adds an additional stage (re-
alisation) in which selected attributes are mapped
to a natural language expression (usually a noun
phrase). Realisation has received far less attention
among REG researchers than attribute selection.
The TUNA-REG task is an ?end-to-end? refer-
ring expression generation task, in the sense that
it takes as input a representation of a set of enti-
ties and their properties, and outputs a word string
which describes the target entity. Participating
systems were not constrained to have attribute se-
lection as a separate module from realisation.
In terms of the XML format, the items in
the test set distributed to participants consisted
of a DOMAIN node and ATTRIBUTE-SET, and par-
ticipating systems had to generate appropriate
WORD-STRINGs.
As with previous STECs involving the TUNA
data, we deliberately refrained from including in
the task definition any aim that would imply as-
sumptions about quality (as would be the case if
we had asked participants to aim to produce, say,
minimal or uniquely distinguishing referring ex-
pressions), and instead we simply listed the evalu-
ation criteria that were going to be used (described
in Section 5).
4Descriptions in each set are not all by the same author.
175
Evaluation criterion Type of evaluation Evaluation technique
Humanlikeness Intrinsic/automatic Accuracy, String-edit distance, BLEU-3, NIST
Adequacy/clarity Intrinsic/human Judgment of adequacy as rated by native speakers
Fluency Intrinsic/human Judgment of fluency as rated by native speakers
Referential clarity Extrinsic/human Speed and accuracy in identification experiment
Table 3: Overview of evaluation methods.
4 Participating Teams and Systems
This section briefly describes this year?s submis-
sions. Full descriptions of participating systems
can be found in the participants? reports included
in this volume.
IS: The submission of the IS team, IS-FP-GT, is
based on the idea that different writers use differ-
ent styles of referring expressions, and that, there-
fore, knowing the identity of the writer helps gen-
erate REs similar to those in the corpus. The
attribute-selection algorithm is an extended full-
brevity algorithm which uses a nearest neighbour
technique to select the attribute set (AS) most sim-
ilar to a given writer?s previous ASs, or, in a case
where no ASs by the given writer have previously
been seen, to select the AS that has the highest de-
gree of similarity with all previously seen ASs by
any writer. If multiple ASs remain, the algorithm
first selects the shortest, then the most represen-
tative of the remaining REs, then the AS with the
highest-frequency attributes. Individualised statis-
tical models are used to convert the selected AS
into a surface-syntactic dependency tree which is
then converted to a word stirng with an existing
realiser.
GRAPH: The GRAPH team reused their existing
graph-based attribute selection component, which
represents a domain as a weighted graph, and uses
a cost function for attributes. The team devel-
oped a new realiser which uses a set of templates
derived from the descriptions in the TUNA cor-
pus. In order to build templates, certain subsets
of attributes were grouped together, individual at-
tributes were replaced by their type, and a pre-
ferred order for attributes was determined based
on frequencies of orderings. During realisation,
if a matching template exists, types are replaced
with the most frequent word string for each given
attribute; if no match exists, realisation is done by
a simple rule-based method.
NIL-UCM: The three systems submitted by this
group use a standard evolutionary algorithm for
attribute selection where genotypes consist of
binary-valued genes each representing the pres-
ence or absence of a given attribute. Realisation
is done with a case-based reasoning (CBR) method
which retrieves the most similar previously seen
ASs for an input AS, in order of their similarity
to the input. (Sub)strings are then copied from
the preferred retrieved case to create the output
word string. One system, NIL-UCM-EvoCBR uses
both components as described above. The other
two systems, NIL-UCM-ValuesCBR and NIL-UCM-
EvoTAP, replace one of the components with the
team?s corresponding component from REG?08.
USP: The system submitted by this group, USP-
EACH, is a frequency-based greedy attribute se-
lection strategy which takes into account the +/?
LOC attribute in the TUNA data. Realisation was
done using the surface realiser supplied to partici-
pants in the ASGRE?07 Challenge.
5 Evaluation Methods and Results
We used a range of different evaluation methods,
including intrinsic and extrinsic,5 automatically
computed and human-evaluated, as shown in the
overview in Table 3. Participants computed auto-
matic intrinsic evaluation scores on the develop-
ment set (using the teval program provided by
us). We performed all of the evaluations shown
in Table 3 on the test data set. For all measures,
results were computed both (a) overall, using the
entire test data set, and (b) by entity type, that is,
computing separate values for outputs in the furni-
ture and in the people domain. Evaluation meth-
ods for each evaluation type and corresponding
evaluation results are presented in the following
three sections.
5.1 Automatic intrinsic evaluations
Humanlikeness, by which we mean the similar-
ity of system outputs to sets of human-produced
reference ?outputs?, was assessed using Accuracy,
5Intrinsic evaluations assess properties of peer systems in
their own right, whereas extrinsic evaluations assess the effect
of a peer system on something that is external to it, such as its
effect on human performance at some given task or the added
value it brings to an application.
176
All development data People Furniture
Accuracy SE BLEU Accuracy SE BLEU Accuracy SE BLEU
IS-FP-GT 9.71% 4.313 0.297 4.41% 4.764 0.2263 15% 3.863 0.3684
GRAPH ? 5.03 0.30 ? 5.15 0.33 ? 4.94 0.27
NIL-UCM-EvoTAP 6% 5.41 0.20 3% 6.04 0.15 8% 4.87 0.24
NIL-UCM-ValuesCBR 1% 5.86 0.19 1% 5.80 0.17 1% 5.91 0.20
USP-EACH ? 6.03 0.19 ? 7.50 0.04 ? 4.78 0.31
NIL-UCM-EvoCBR 3% 6.31 0.17 1% 6.94 0.16 4% 5.77 0.18
Table 4: Participating teams? self-reported automatic intrinsic scores on development data set with single
human-authored reference description (listed in order of overall mean SE score).
All test data People Furniture
Acc SE BLEU NIST Acc SE BLEU NIST Acc SE BLEU NIST
GRAPH 12.50 6.41 0.47 2.57 8.93 7.04 0.43 2.16 16.07 5.79 0.51 2.26
IS-FP-GT 3.57 6.74 0.28 0.75 3.57 7.04 0.37 0.94 3.57 6.45 0.13 0.36
NIL-UCM-EvoTAP 6.25 7.28 0.26 0.90 3.57 8.07 0.20 0.45 8.93 6.48 0.34 1.22
USP-EACH 7.14 7.59 0.27 1.33 0.00 9.04 0.11 0.46 14.29 6.14 0.41 2.28
NIL-UCM-ValuesCBR 2.68 7.71 0.27 1.69 3.57 8.07 0.23 0.94 1.79 7.34 0.28 1.99
NIL-UCM-EvoCBR 2.68 8.02 0.26 1.97 0.00 9.07 0.19 1.65 5.36 6.96 0.35 1.69
HUMAN-2 2.68 9.68 0.12 1.78 3.57 10.64 0.12 1.50 1.79 8.71 0.13 1.57
HUMAN-1 2.68 9.68 0.12 1.68 3.57 10.64 0.12 1.41 1.79 8.71 0.12 1.49
Table 5: Automatic intrinsic scores on test data set with two human-authored reference descriptions
(listed in order of overall mean SE score).
string-edit distance, BLEU-3 and NIST-5. Accu-
racy measures the percentage of cases where a
system?s output word string was identical to the
corresponding description in the corpus. String-
edit distance (SE) is the classic Levenshtein dis-
tance measure and computes the minimal number
of insertions, deletions and substitutions required
to transform one string into another. We set the
cost for insertions and deletions to 1, and that for
substitutions to 2. If two strings are identical, then
this metric returns 0 (perfect match). Otherwise
the value depends on the length of the two strings
(the maximum value is the sum of the lengths). As
an aggregate measure, we compute the mean of
pairwise SE scores.
BLEU-x is an n-gram based string comparison
measure, originally proposed by Papineni et al
(2001; 2002) for evaluation of Machine Transla-
tion systems. It computes the proportion of word
n-grams of length x and less that a system out-
put shares with several reference outputs. Setting
x = 4 (i.e. considering all n-grams of length ? 4)
is standard, but because many of the TUNA de-
scriptions are shorter than 4 tokens, we compute
BLEU-3 instead. BLEU ranges from 0 to 1.
NIST is a version of BLEU, but where BLEU
gives equal weight to all n-grams, NIST gives more
importance to less frequent n-grams, which are
taken to be more informative. The maximum NIST
score depends on the size of the test set.
Unlike string-edit distance, BLEU and NIST are
by definition aggregate measures (i.e. a single
score is obtained for a peer system based on the
entire set of items to be compared, and this is not
generally equal to the average of scores for indi-
vidual items).
Because the test data has two human-authored
reference descriptions per domain, the Accuracy
and SE scores had to be computed slightly differ-
ently to obtain test data scores (whereas BLEU and
NIST are designed for multiple reference texts).
For the test data only, therefore, Accuracy ex-
presses the percentage of a system?s outputs that
match at least one of the reference outputs, and SE
is the average of the two pairwise scores against
the reference outputs.
Results: Table 4 is an overview of the self-
reported scores on the development set included in
the participants? reports (not all participants report
Accuracy scores). The corresponding scores for
the test data set as well as NIST scores for the test
data (all computed by us), are shown in Table 5.
The table also includes the result of comparing
the two sets of human descriptions, HUMAN-1 and
HUMAN-2, to each other using the same metrics
(their scores are distinct only for non-commutative
measures, i.e. NIST and BLEU).
We ran6 a one-way ANOVA for the SE scores.
6We used SPSS for all statistical analyses and tests.
177
There was a main effect of SYSTEM on SE (F =
10.938, p < .001). A post-hoc Tukey HSD test
with ? = .05 revealed a number of significant dif-
ferences: all systems were significantly better than
the human-authored descriptions, and GRAPH was
furthermore significantly better than NIL-UCM-
EvoCBR.
We also computed the Kruskal-Wallis H value
for the systems? individual Accuracy scores, using
a chi square test to establish significance. By this
test, the observed aggregate difference among the
seven systems is significant at the .01 level (?27 =
20.169).
5.2 Human intrinsic evaluation
The TUNA?09 Challenge was the first TUNA
shared-task competition to include an intrinsic
evaluation involving human judgments of quality.
Design: The intrinsic human evaluation in-
volved descriptions for all 112 test data items from
all six submitted systems, as well as from the two
sets of human-authored descriptions.7 Thus, each
of the 112 test set items was associated with 8
different descriptions. We used a Repeated Latin
Squares design which ensures that each subject
sees descriptions from each system and for each
domain the same number of times. There were
fourteen 8 ? 8 squares, and a total of 896 indi-
vidual judgments in this evaluation, each system
receiving 112 judgments (14 from each subject).
Procedure: In each of the 112 trials, par-
ticipants were shown a system output (i.e. a
WORD-STRING), together with its corresponding
domain, displayed as the set of corresponding im-
ages on the screen.8 The intended (target) referent
was highlighted by a red frame surrounding it on
the screen. They were asked to give two ratings
in answer to the following questions (the first for
Adequacy, the second for Fluency):
1. How clear is this description? Try to imagine
someone who could see the same grid with
the same pictures, but didn?t know which of
the pictures was the target. How easily would
they be able to find it, based on the phrase
given?
7Note that we refer to all outputs, whether human or
system-generated, as system outputs in what follows.
8The on-screen display of images was very similar, al-
though not identical, to that in the original TUNA elicitation
experiments.
2. How fluent is this description? Here your
task is to judge how well the phrase reads.
Is it good, clear English?
We did not use a rating scale (where integers
correspond to different assessments of quality),
because it is not generally considered appropriate
to apply parametric methods of analysis to ordinal
data. Instead, we asked subjects to give their judg-
ments for Adequacy and Fluency for each item by
manipulating a slider like this:
The slider pointer was placed in the center at the
beginning of each trial, as shown above. The posi-
tion of the slider selected by the subject mapped to
an integer value between 1 and 100. However, the
scale was not visible to participants, whose task
was to move the pointer to the left or right. The
further to the right, the more positive the judgment
(and the higher the value returned); the further to
the left, the more negative.
Following instructions, subjects did two prac-
tice examples, followed by the 112 test items in
random order. Subjects carried out the experi-
ment over the internet, at a time and place of their
choosing, and were allowed to interrupt and re-
sume the experiment. According to self-reported
timings, subjects took between 25 and 60 minutes
to complete the experiment (not counting breaks).
Participants: We recruited eight native speak-
ers of English from among post-graduate students
currently doing a Masters degree in a linguistics-
related subject.9
We recorded subjects? gender, level of educa-
tion, field of study, proficiency in English, vari-
ety of English and colour vision. Since all sub-
jects were native English speakers, had normal
colour vision, and had comparable levels of ed-
ucation and academic backgrounds, as indicated
above, these variables are not included in the anal-
yses reported below.
Results: Table 6 displays the mean Fluency and
Adequacy judgments obtained by each system.
We conducted two separate 8 (SYSTEM) ? 2 (DO-
MAIN) Univariate Analyses of Variance (ANOVAs)
on Adequacy and Fluency, where DOMAIN ranges
9MA Linguistics and MRes Speech, Language and Cog-
nition at UCL; MA Applied Linguistics and MRes Psychol-
ogy at Sussex; and MA Media-assisted Language Teaching
at Brighton.
178
All test data People Furniture
Adequacy Fluency Adequacy Fluency Adequacy Fluency
Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD
GRAPH 84.11 21.07 85.81 17.52 85.30 18.10 87.70 14.42 82.91 23.78 83.93 20.11
USP-EACH 77.72 28.33 84.20 20.27 81.04 26.48 81.82 24.47 74.41 29.93 86.57 14.79
NIL-UCM-EvoTAP 76.16 28.34 61.95 26.13 78.66 27.48 59.13 29.78 73.66 29.22 64.77 21.79
HUMAN-2 74.63 34.77 73.38 27.63 80.93 31.83 73.16 30.88 68.34 36.68 73.59 24.23
NIL-UCM-ValuesCBR 72.34 33.93 59.41 33.94 68.18 37.37 46.23 34.92 76.50 29.86 72.59 27.43
HUMAN-1 70.38 34.92 71.52 30.79 83.39 24.27 72.39 28.55 57.36 39.08 70.64 33.13
NIL-UCM-EvoCBR 63.65 37.19 55.38 35.32 56.61 40.20 41.45 37.38 70.70 32.76 69.30 26.93
IS-FP-GT 59.46 40.94 66.21 30.97 88.79 19.26 65.27 32.22 30.14 35.51 67.16 29.94
Table 6: Human-assessed intrinsic scores on test data set, including the two sets of human-authored
reference descriptions (listed in order of overall mean Adequacy score).
Adequacy Fluency
GRAPH A GRAPH A
USP-EACH A B USP-EACH A B
NIL-UCM-EvoTAP A B HUMAN-2 B C
HUMAN-2 A B C HUMAN-1 C D
NIL-UCM-ValuesCBR A B C IS-FP-GT C D E
HUMAN-1 B C D NIL-UCM-EvoTAP D E
NIL-UCM-EvoCBR C D NIL-UCM-ValuesCBR E
IS-FP-GT D NIL-UCM-EvoCBR E
Table 7: Homogeneous subsets for Adequacy and Fluency. Systems which do not share a letter are
significantly different at ? = .05.
over People and Furniture Items. On Adequacy,
there were main effects of SYSTEM (F (7, 880) =
7.291, p < .001) and DOMAIN (F (1, 880) =
29.133, p < .001), with a significant interac-
tion between the two (F (7, 880) = 15.30, p <
.001). On Fluency, there were main effects of
SYSTEM (F (7, 880) = 18.14) and of DOMAIN
(F (7, 880) = 17.20), again with a significant
SYSTEM ? DOMAIN interaction (F (7, 880) =
5.60), all significant at p < .001. Post-hoc Tukey
comparisons on both dependent measures yielded
the homogeneous subsets displayed in Table 7.
5.3 Extrinsic task-performance evaluation
As for earlier shared tasks involving the TUNA
data, we carried out a task-performance experi-
ment in which subjects have the task of identifying
intended referents.
Design: The extrinsic human evaluation in-
volved descriptions for all 112 test data items from
all six submitted systems, as well as from the two
sets of human-authored descriptions. We used a
Repeated Latin Squares design with fourteen 8?8
squares, so again there were a total of 896 individ-
ual judgments and each system received 112 judg-
ments, however this time it was 7 from each sub-
ject, as there were 16 participants; so half the par-
ticipants did the first 56 items (the first 7 squares),
and the other half the second 56 (the remaining 7
squares).
Procedure: In each of their 5 practice trials and
56 real trials, participants were shown a system
output (i.e. a WORD-STRING), together with its cor-
responding domain, displayed as the set of corre-
sponding images on the screen. In this experiment
the intended referent was not highlighted in the on-
screen display, and the participants? task was to
identify the intended referent among the pictures
by mouse-clicking on it.10
In previous TUNA identification experiments
(Belz and Gatt, 2007; Gatt et al, 2008), sub-
jects had to read the description before identify-
ing the intended referent. In ASGRE?07 both de-
scription and pictures were displayed at the same
time, yielding a single time measure that com-
bined reading and identification times. In REG?08,
subjects first read the description and then called
up the pictures on the screen when they had fin-
ished reading the description, which yielded sepa-
rate reading and identification times.
10Due to limitations related to the stimulus presentation
software, the images in this experiment were displayed in
strict rows and columns, whereas the display grid in the web-
based TUNA elicitation experiment and the intrinsic human
evalution experiment were slightly distorted. This may have
affected timings in those (very rare) cases where a description
explicitly referenced the column a target referent was located
in, as in the chair in column 1.
179
This year we tried out a version of the experi-
ment where subjects listened to descriptions read
out by a synthetic voice11 over headphones while
looking at the pictures displayed on the screen.
Stimulus presentation was carried out using
DMDX, a Win-32 software package for psycholin-
guistic experiments involving time measurements
(Forster and Forster, 2003). Participants initiated
each trial, which consisted of an initial warning
bell and a fixation point flashed on the screen for
1000ms. Following this, the visual domain was
displayed, and the voice reading the description
was initiated after a delay of 500ms. We recorded
time in milliseconds from the start of display to the
mouse-click whereby a participant identified the
target referent. This is hereafter referred to as the
identification speed. The analysis reported below
also uses identification accuracy, the percentage
of correctly identified target referents, as an addi-
tional dependent variable. Trials timed out after
15, 000ms.
Participants: The experiment was carried out
by 16 participants recruited from among the fac-
ulty and administrative staff of the University of
Brighton. All participants carried out the experi-
ment under supervision in the same quiet room on
the same laptop, in the same ambient conditions,
with no interruptions. All participants were native
speakers, and we recorded type of post, whether
they had normal colour vision and hearing, and
whether they were left or right-handed.
Timeouts and outliers: None of the trials
reached time-out stage during the experiment.
Outliers were defined as those identification times
which fell outside the mean ?2SD (standard de-
viation) range. 44 data points (4.9%) out of a to-
tal of 896 were identified as outliers by this defi-
nition; these were replaced with the series mean
(Ratliff, 1993). The results reported for identi-
fication speed below are based on these adjusted
times.
Results: Table 8 displays mean identification
speed and identification accuracy per system. A
univariate ANOVA on identification speed revealed
significant main effects of SYSTEM (F (7, 880) =
4.04, p < .001) and DOMAIN (F (1, 880) =
11We used the University of Edinburgh?s Festival speech
generation system (Black et al, 1999) in combination
with the nitech us slt arctic hts voice, a high-quality female
American voice.
USP-EACH A
GRAPH A
NIL-UCM-EvoTAP A B
IS-FP-GT A B
NIL-UCM-ValuesCBR A B
NIL-UCM-EvoCBR A B
HUMAN-2 B
HUMAN-1 B
Table 9: Homogeneous subsets for Identification
Speed. Systems which do not share a letter are
significantly different at ? = .05.
11.53, p < .001), with a significant interaction
(F (7, 880) = 6.02, p < .001). Table 9 displays
homogeneous subsets obtained following pairwise
comparisons using a post-hoc Tukey HSD analysis.
We treated identification accuracy as an indica-
tor variable (indicating whether a participant cor-
rectly identified a target referent or not in a given
trial). A Kruskal-Wallis test showed a significant
difference between systems (?27 = 44.98; p <
.001).
5.4 Correlations
Table 10 displays the correlations between the
eight evaluation measures we used. The num-
bers are Pearson product-moment correlation co-
efficients, calculated on the means (1 mean per
system on each measure).
As regards the human-assessed intrinsic scores,
there is no significant correlation between Ad-
equacy and Fluency. Among the automatically
computed intrinsic measures, the only significant
correlation is between Accuracy and BLEU. For
the extrinsic identification performance measures,
there is no significant correlation between Identi-
fication Accuracy and Identification Speed.
As for correlations across the two types
(human-assessed and automatically computed) of
intrinsic measures, the only significant correla-
tions are between Fluency and Accuracy, and be-
tween Adequacy and Accuracy. So, a system
with a higher percentage of human-like outputs
(as measured by Accurach) also tends to be scored
more highly in terms of Fluency and Adequacy by
humans.
We also found significant correlations between
intrinsic and extrinsic measures: there was a
strong and significant correlation between Iden-
tification Accuracy and Adequacy, implying that
more adequate system outputs allowed people to
identify target referents more correctly; there was
also a significant (negative) correlation between
180
All test data People Furniture
ID acc. ID. speed ID acc. ID. speed ID acc. ID. speed
% Mean SD % Mean SD % Mean SD
GRAPH 0.96 3069.16 878.89 0.95 3081.01 767.62 0.96 3057.31 984.60
HUMAN-1 0.91 3517.58 1028.83 0.95 3323.76 764.59 0.88 3711.41 1214.55
USP-EACH 0.90 3067.16 821.00 0.86 3262.79 865.61 0.95 2871.53 730.15
NIL-UCM-EvoTAP 0.88 3159.41 910.65 0.88 3375.17 948.46 0.89 2943.65 824.17
NIL-UCM-ValuesCBR 0.87 3262.53 974.55 0.80 3447.50 1003.21 0.93 3077.56 916.87
HUMAN-2 0.83 3463.88 1001.29 0.89 3647.41 1045.95 0.77 3280.35 927.79
NIL-UCM-EvoCBR 0.81 3362.22 892.45 0.75 3779.64 831.91 0.88 2944.80 748.69
IS-FP-GT 0.68 3167.11 964.45 0.89 2980.30 750.78 0.46 3353.91 1114.68
Table 8: Identification speed and accuracy per system. Systems are displayed in descending order of
overall identification accuracy.
Human-assessed, intrinsic Extrinsic Auto-assessed, intrinsic
Fluency Adequacy ID Acc. ID Speed Acc. SE BLEU NIST
Fluency 1 0.68 0.50 -0.89* .85* -0.57 0.66 0.30
Adequacy 0.68 1 0.95** -0.65 .83* -0.29 0.60 0.48
Identification Accuracy 0.50 0.95** 1 -0.39 0.68 -0.01 0.49 0.60
Identification Speed 0.89* -0.65 -0.39 1 -0.79 0.68 -0.51 0.06
Accuracy 0.85* 0.83* 0.68 -0.79 1.00 -0.68 .859* 0.49
SE -0.57 -0.29 -0.01 0.68 -0.68 1 -0.75 -0.07
BLEU 0.66 0.60 0.49 -0.51 .86* -0.75 1 0.71
NIST 0.30 0.48 0.60 0.06 0.49 -0.07 0.71 1
Table 10: Correlations (Pearson?s r) between all evaluation measures. (?significant at p ? .05;
??significant at p ? .01)
Fluency and Identification Speed, implying that
more fluent descriptions led to faster identifica-
tion. While these results differ from previous find-
ings (Belz and Gatt, 2008), in which no significant
correlations were found between extrinsic mea-
sures and automatic intrinsic metrics, it is worth
noting that significance in the results reported here
was only observed between human-assessed in-
trinsic measures and the extrinsic ones.
6 Concluding Remarks
The three editions of the TUNA STEC have at-
tracted a substantial amount of interest. In addi-
tion to a sizeable body of new work on referring
expression generation, as another tangible out-
come of these STECs we now have a wide range
of different sets of system outputs for the same set
of inputs. A particularly valuable resource is the
pairing of these outputs from the submitted sys-
tems in each edition with evaluation data.
As this was the last time we are running a STEC
with the TUNA data, we will now make all data
sets, documentation and evaluation software from
all TUNA STECs available to researchers. We are
planning to add to these as many system outputs
as we can, so that other researchers can perform
evaluations involving these.
We are also planning to complete our evalua-
tions of the evaluation methods we have devel-
oped. Among such experiments will be direct
comparisons between the results of the three vari-
ants of the identification experiment we have tried
out, and a direct comparison between different
designs for human-assessed intrinsic evaluations
(e.g. comparing the slider design reported here to
preference judgments and rating scales).
Apart from the technological progress in REG
which we hope the TUNA STECs have helped
achieve, perhaps the single most important scien-
tific result is strong evidence for the importance
of extrinsic evaluations, as these do not necessar-
ily agree with the results of much more commonly
used intrinsic types of evaluations.
Acknowledgments
We thank our colleagues at the University of
Brighton who participated in the identification ex-
periment, and the Masters students at UCL, Sus-
sex and Brighton who participated in the qual-
ity assessment experiment. The evaluations were
funded by EPSRC (UK) grant EP/G03995X/1.
References
A. Belz and A. Gatt. 2007. The attribute selection for
gre challenge: Overview and evaluation results. In
181
Proceedings of UCNLG+MT: Language Generation
and Machine Translation.
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?08),
pages 197?200.
A. Black, P. Taylor, and R. Caley, 1999. The Festi-
val Speech Synthesis System: System Documenta-
tion. University of Edinburgh, 1.4 edition.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy.
Behavior Research Methods, Instruments, & Com-
puters, 35(1):116?124.
A. Gatt, A. Belz, and Eric Kow. 2008. The tuna chal-
lenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Conference on
Natural Language Generation (INLG?08).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: A method for automatic evaluation of ma-
chine translation. IBM research report, IBM Re-
search Division.
S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002.
Bleu: a. method for automatic evaluation of machine
translation. In Proc. 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-02),
pages 311?318.
R. Ratliff. 1993. Methods for dealing with reaction
time outliers. Psychological Bulletin, 114(3):510?
532.
182
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 79?87,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
The GREC Main Subject Reference Generation Challenge 2009:
Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Abstract
The GREC-MSR Task at Generation Chal-
lenges 2009 required participating systems
to select coreference chains to the main
subject of short encyclopaedic texts col-
lected from Wikipedia. Three teams sub-
mitted one system each, and we addition-
ally created four baseline systems. Sys-
tems were tested automatically using ex-
isting intrinsic metrics. We also evaluated
systems extrinsically by applying corefer-
ence resolution tools to the outputs and
measuring the success of the tools. In ad-
dition, systems were tested in an intrinsic
evaluation involving human judges. This
report describes the GREC-MSR Task and
the evaluation methods applied, gives brief
descriptions of the participating systems,
and presents the evaluation results.
1 Introduction
The GREC-MSR Task is about how to generate ap-
propriate references to an entity in the context of a
piece of discourse longer than a sentence. Rather
than requiring participants to generate referring
expressions from scratch, the GREC-MSR data pro-
vides sets of possible referring expressions for se-
lection. This was the second time we ran a shared
task using the GREC-MSR data (following a first
run in 2008). The task definition was again kept
fairly simple, but in the 2009 round the main aim
for participating systems was to select an appro-
priate word string to serve as a referring expres-
sion, whereas in 2008 it was to select an appropri-
ate type of referring expression (name, common
noun, pronoun, or empty reference).
The immediate motivating application context
for the GREC-MSR Task is the improvement of ref-
erential clarity and coherence in extractive sum-
maries by regenerating referring expressions in
them. There has recently been a small flurry
of work in this area (Steinberger et al, 2007;
Nenkova, 2008). In the longer term, the GREC-
MSR Task is intended to be a step in the direction
of the more general task of generating referential
expressions in discourse context.
The GREC-MSR data is an extension of the
GREC 1.0 Corpus which had about 1,000 texts in
the subdomains of cities, countries, rivers and peo-
ple (Belz and Varges, 2007a). For the purpose of
the GREC-MSR shared task, an additional 1,000
texts in the new subdomain of mountain texts were
obtained and a new XML annotation scheme (Sec-
tion 2.2) was developed.
Team System Name
University of Delaware UDel
ICSI, Berkeley ICSI-CRF
Jadavpur University JUNLG
Table 1: GREC-MSR?09 participating teams.
Nine teams from seven countries registered for
GREC-MSR?09, of which three teams (Table 1)
submitted one system each.1 Participants had to
submit their system reports before downloading
test data inputs, and had to submit test data out-
puts within 48 hours of downloading the test data
inputs. In addition to the participants? systems,
we also used the corpus texts themselves as ?sys-
tem? outputs, and created 4 baseline systems; we
evaluated the resulting 8 systems using a range of
intrinsic and extrinsic evaluation methods (for de-
tails see Sections 5 and 6). This report presents the
results of all evaluations (Section 6), along with
descriptions of GREC-MSR data and task (Sec-
tion 2), test sets (Section 3), evaluation methods
(Section 4), and participating systems (Section 5).
2 Data and Task
The GREC Corpus (version 2.0) consists of about
2,000 texts in total, all collected from introduc-
1One team submitted by the original deadline (Jan. 2009),
one by the revised deadline (1 June 2009), one slightly later.
79
tory sections in Wikipedia articles, in five different
subdomains (cities, countries, rivers, people and
mountains). In each text, three broad categories
of Main Subject Reference (MSR)2 have been an-
notated, resulting in a total of about 13,000 anno-
tated REs. The GREC-MSR shared task version of
the corpus was randomly divided into 90% train-
ing data (of which 10% were randomly selected as
development data) and 10% test data. Participants
used the training data in developing their systems,
and (as a minimum requirement) reported results
on the development data.
2.1 Types of referential expression annotated
Three broad categories of main subject referring
expressions (MSREs) are annotated in the GREC
corpus3 ? subject NPs, object NPs, and geni-
tive NPs and pronouns which function as subject-
determiners within their matrix NP. These cate-
gories of referring expressions (RE) are relatively
straightforward to identify and to achieve high
inter-annotator agreement on (complete agree-
ment among four annotators in 86% of MSRs), and
account for most cases of overt main subject refer-
ence in the GREC texts. The annotators were asked
to identify subject, object and genitive subject-
determiners and decide whether or not they refer
to the main subject of the text. More detail is pro-
vided in Belz and Varges (2007b).
In addition to the above, relative pronouns in
supplementary relative clauses (as opposed to in-
tegrated relative clauses, Huddleston and Pullum,
2002, p. 1058) were annotated, e.g.:
(1) Stoichkov is a football manager and former striker
who was a member of the Bulgaria national team that
finished fourth at the 1994 FIFA World Cup.
We also annotated ?non-realised? subject MSREs
in those cases of VP coordination where an MSRE
is the subject of the coordinated VPs, e.g.:
(2) He stated the first version of the Law of conservation
of mass, introduced the Metric system, and
helped to reform chemical nomenclature.
The motivation for annotating the approximate
place where the subject NP would be if it were
realised (the gap-like underscores above) is that
from a generation perspective there is a choice to
be made about whether to realise the subject NP in
the second and third coordinates or not.
2The main subject of a Wikipedia article is simply taken to
be given by its title, e.g. in the cities domain the main subject
(and title) of one text is London.
3In terminology and view of grammar the annotations rely
heavily on Huddleston and Pullum (2002).
2.2 XML format
Figure 1 is one of the texts distributed in the
GREC-MSR training/development data set. The
REF element indicates a reference, in the sense of
?an instance of referring? (which could, in princi-
ple, be realised by gesture or graphically, as well
as by a string of words, or a combination of these).
REFs have three attributes: ID, a unique refer-
ence identifier; SEMCAT, the semantic category of
the referent, ranging over city, country, river,
person, mountain; and SYNCAT, the syntactic cate-
gory required of referential expressions for the ref-
erent in this discourse context (np-obj, np-subj,
subj-det). A REF is composed of one REFEX ele-
ment (the ?selected? referential expression for the
given reference; in the training/development data
texts it is simply the referential expression found
in the corpus) and one ALT-REFEX element which
in turn is a list of REFEXs which are possible alter-
native referential expressions (see following sec-
tion).
REFEX elements have four attributes. The
HEAD attribute has the possible values nominal,
pronoun, and rel-pron; the CASE attribute has
the possible values nominative, accusative and
genitive for pronouns, and plain and genitive
for nominals. The binary-valued EMPHATIC at-
tribute indicates whether the RE is emphatic; in
the GREC-MSR corpus, the only type of RE that
has EMPHATIC=yes is one which incorporates a re-
flexive pronoun used emphatically (e.g. India it-
self ). The REG08-TYPE attribute indicates basic RE
type. The choice of types is motivated by the hy-
pothesis that one of the most basic decisions to be
taken in RE selection for named entities is whether
to use an RE that includes a name, such as Mod-
ern India (the corresponding REG08-TYPE value
is name); whether to go for a common-noun RE,
i.e. with a category noun like country as the head
(common); whether to use a pronoun (pronoun); or
whether it can be left unrealised (empty).
2.3 The GREC-MSR Task
The task for participating systems was to develop
a method for selecting one of the REFEXs in the
ALT-REFEX list, for each REF in each TEXT in the
test sets. The test data inputs were identical to
the training/development data, except that REF el-
ements contained only an ALT-REFEX list, not the
preceding ?selected? REFEX. ALT-REFEX lists are
generated for each text by an automatic method
80
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TEXT SYSTEM "reg08-grec.dtd">
<TEXT ID="36">
<TITLE>Jean Baudrillard</TITLE>
<PARAGRAPH>
<REF ID="36.1" SEMCAT="person" SYNCAT="np-subj">
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX>
<REFEX REG08-TYPE="empty">_</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX>
</ALT-REFEX>
</REF>
(born June 20, 1929) is a cultural theorist, philosopher, political commentator,
sociologist, and photographer.
<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det">
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX>
</ALT-REFEX>
</REF>
work is frequently associated with postmodernism and post-structuralism.
</PARAGRAPH>
</TEXT>
Figure 1: Example text from the GREC-MSR Training Data.
which collects all the (manually annotated) MSREs
in a text including the title, and adds several de-
faults: pronouns and reflexive pronouns in all sub-
domains; and category nouns (e.g. the river), in
all subdomains except people. The main objec-
tive in the 2009 GREC-MSR Task was to get the
word strings contained in REFEXs right (whereas
in REG?08 it was the REG08-TYPE attributes).
3 Test Data
1. Test Set C-1: a randomly selected 10% sub-
set (183 texts) of the GREC corpus (with the same
proportions of texts in the 5 subdomains as in the
training/testing data).
2. Test Set C-2: the same subset of texts as in C-
1; however, for C-2 we did not use the MSREs in
the corpus, but replaced them with human-selected
alternatives. These were obtained in an online ex-
periment as described in Belz & Varges (2007a)
where subjects selected MSREs in a setting that du-
plicated the conditions in which the participating
systems in the GREC-MSR Task make selections.4
We obtained three versions of each text, where in
each version all MSREs were selected by the same
person. The motivation for this version of Test Set
C was that having several human-produced chains
of MSREs to compare the outputs of participating
(?peer?) systems against is more reliable than hav-
ing one only; and that Wikipedia texts are edited
4The experiment can be tried out here: http://www.nltg.
brighton.ac.uk/home/Anja.Belz/TESTDRIVE/
by multiple authors which sometimes adversely
affects MSR chains; we wanted to have additional
reference texts where all references are selected by
a single author.
3. Test Set L: 74 Wikipedia introductory texts
from the subdomain of lakes (there were no lake
texts in the training/development set).
4. Test Set P: 31 short encyclopaedic texts in
the same 5 subdomains as in the GREC corpus,
in approximately the same proportions as in the
training/testing data, but of different origin. We
transcribed these texts from printed encyclopae-
dias published in the 1980s which are not avail-
able in electronic form. The texts in this set are
much shorter and more homogeneous than the
Wikipedia texts, and the sequences of MSRs fol-
low very similar patterns. It seems likely that it is
these properties that have resulted in better scores
overall for Test Set P than for the other test sets
in both the 2008 and 2009 runs of the GREC-MSR
task (for the latter, see Section 6).
Each test set was designed to test peer systems
for generalisation to different kinds of unseen data.
Test Set C tests for generalisation to unseen ma-
terial from the same corpus and the same subdo-
mains as the training set; Test Set L tests for gen-
eralisation to unseen material from the same cor-
pus but different subdomain; and Test Set P for
generalisation to a different corpus but the same
subdomains.
81
4 Evaluation methods
4.1 Automatic intrinsic evaluations5
Accuracy of REFEX word strings: when com-
puted against test sets (C-1, L and P), Word String
Accuracy is simply the proportion of REFEX word
strings selected by a participating system that are
identical to the one in the corpus. When computed
against test set C-2, which has three versions of
each text, Word String Accuracy is computed as
follows: first the number of correct REFEX word
strings is computed at the text level for each of the
three versions of a text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total number
of REFs in all the texts, which gives the global
Word String Accuracy score. The rationale be-
hind computing the Word String Accuracy scores
in this way for multiple-RE test sets (maximising
scores on RE chains rather than individual REs) is
that an RE is not good or bad in its own right, but
depends on other MSREs in the same text.
Accuracy of REG08-Type: similarly to Word
String Accuracy above, when computed against
test sets C-1, L and P, REG08-Type Accuracy is the
proportion of REFEXs selected by a participating
system that have a REG08-TYPE value identical to
the one in the corpus. When computed against test
set C-2, first the number of correct REG08-Types is
computed at the text level for each of the three ver-
sions of a corpus text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total num-
ber of REFs in all the texts, which gives the global
REG08-Type Accuracy score.
String-edit distance metrics: String-edit dis-
tance (SE) is straightforward Levenshtein distance
with a substitution cost of 2 and insertion/deletion
cost of 1. We also used a length-normalised ver-
sion of string-edit distance (denoted ?norm. SE? in
results tables below). For test sets C-1, L and P,
the global score is simply the mean of all RE-level
scores. For Test Set C-2, the global score is the
mean of the mean of the three text-level scores.
Other metrics: BLEU is a precision metric
from machine translation that assesses peer trans-
lations in terms of the proportion of word n-grams
5For GREC-MSR?09 we updated the tool that computes all
automatic intrinsic scores and in the course of this eliminated
a character encoding issue; as a result the results for baseline
systems and corpus texts reported here are on the whole very
slightly higher than those reported for GREC-MSR?08.
(n ? 4 is standard) they share with several ref-
erence translations. We used BLEU-3 rather than
the more standard BLEU-4 because most REs in
the corpus are less than 4 tokens long. We also
used the NIST version of BLEU which weights in
favour of less frequent n-grams. In both cases,
we assessed just the MSREs selected by peer sys-
tems (leaving out the surrounding text), and com-
puted scores globally (rather than averaging over
RE-level scores), as this is standard for these met-
rics. BLEU, and NIST are designed to work with
one or multiple reference texts, so we did not need
to use a different method for Test Set C-2.
4.2 Automatic extrinsic evaluation
As in GREC-MSR?08, we used an automatic ex-
trinsic evaluation method based on coreference
resolution performance.6 The basic idea is that it
seems likely that badly chosen reference chains af-
fect the ability to resolve REs in automatic coref-
erence resolution tools which will tend to perform
worse with poorly selected MSR reference chains.
To counteract the possibility of results being a
function of a specific coreference resolution algo-
rithm or tool, we used two different resolvers?
those included in LingPipe7 and OpenNLP (Mor-
ton, 2005)?and averaged results.
There does not appear to be a single standard
evaluation metric in the coreference resolution
community, so we opted to use three: MUC-6
(Vilain et al, 1995), CEAF (Luo, 2005), and B-
CUBED (Bagga and Baldwin, 1998), which seem
to be the most widely accepted metrics. All three
metrics compute Recall, Precision and F-Scores
on aligned gold-standard and resolver-tool coref-
erence chains. They differ in how the alignment
is obtained and what components of coreference
chains are counted for calculating scores. Results
for the automatic extrinsic evaluations are reported
below in terms of the F-Scores from these three
metrics, as well as in terms of their mean.
4.3 Human intrinsic evaluation
The intrinsic human evaluation involved 24 ran-
domly selected items from Test Set C and outputs
for these produced by peer and basline systems as
6However, for GREC?09 we overhauled the tool; the cur-
rent version no longer uses JavaRAP, and uses the most recent
versions of the other resolvers; the GREC-MSR?08 and GREC-
MSR?09 results for this method are not entirely comparable
for this reason.
7http://alias-i.com/lingpipe/
82
Figure 2: Example of text presented in human intrinsic evaluation of GREC-MSR systems.
well as those found in the original corpus texts
(8 systems in total). We used a Repeated Latin
Squares design which ensures that each subject
sees the same number of outputs from each sys-
tem and for each test set item. There were three
8x8 squares, and a total of 576 individual judg-
ments in this evaluation (72 per system: 3 criteria
x 3 articles x 8 evaluators).
We recruited 8 native speakers of English from
among post-graduate students currently doing a
linguistics-related degree at University College
London (UCL) and University of Sussex.
Following detailed instructions, subjects did
two practice examples, followed by the 24 texts
to be evaluated, in random order. Subjects carried
out the evaluation over the internet, at a time and
place of their choosing. They were allowed to in-
terrupt and resume the experiment (though discor-
ouged from doing so). According to self-reported
timings, subjects took between 25 and 45 minutes
to complete the evaluation (not counting breaks).
Figure 2 shows what subjects saw during the
evaluation of an individual text. All references to
the MS are highlighted in yellow, and the task is to
evaluate the quality of the REs in terms of three cri-
teria which were explained in the introduction as
follows (the wording of the explanations of Crite-
ria 1 and 3 were taken from the DUC evaluations):
1. Referential Clarity: It should be easy to identify who
or what the referring expressions in the text are refer-
ring to. If a person or other entity is mentioned, it
should be clear what their role in the story is. So, a ref-
erence would be unclear if an entity is referenced, but
their identity or relation to the story remains unclear.
2. Fluency: A referring expression should ?read well?, i.e.
it should be written in good, clear English, and the use
of titles and names etc. should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
3. Structure and Coherence: The text should be well
structured and well organised. The text should not just
be a heap of related information, but should build from
sentence to sentence to a coherent body of information
about a topic. This criterion too is independent of the
others.
Subjects selected evaluation scores by moving
sliders (see Figure 2) along scales ranging from 1
to 5. Slider pointers started out in the middle of
the scale (3). These were continuous scales and
we recorded scores with one decimal place (e.g.
3.2). The meaning of the numbers was explained
in terms of integer scores (1=very poor, 2=poor,
3=neither poor nor good, 4=good, 5=very good).
5 Systems
Base-rand, Base-freq, Base-1st, Base-name:
Baseline system Base-rand selects one of the
REFEXs at random. Base-freq selects the REFEX
that is the overall most frequent given the SYNCAT
and SEMCAT of the reference. Base-1st al-
ways selects the REFEX which appears first in
the ALT-REFEX list; and Base-name selects the
shortest REFEX with attributes REG08-TYPE=name,
HEAD=nominal and EMPHATIC=no.8
8Attributes are considered in this order. If for one at-
tribute, the right value is not found, the process ignores that
attribute and moves on the next one.
83
UDel: The UDel system consists of a prepro-
cessing component performing sentence segmen-
tation and identification of non-referring occur-
rences of main subject (MS) names, an RE type
selection component (two C5.0 decision trees, one
optimised for people and mountains, the other for
the other subdomains), and a word string selec-
tion component. The RE type selection decision
trees use the following features: is the MS the sub-
ject of the current, preceding and preceding but
one sentence; was the last MSR in subject position;
are there interfering references to other entities be-
tween the current and the previous MSR; distance
to preceding non-referring occurrences of an MS
name; sentence and reference IDs; other features
indicating whether the reference occurred before
and after certain words and punctuation marks.
Given a selected RE type, the word-string selec-
tion component selects the longest non-emphatic
name for the first named reference in an article,
and the shortest for subsequent named references;
for other types, the first matching word-string is
used, backing off to pronoun or name.
ICSI-CRF: The ICSI-CRF system construes the
GREC-MSR task as a sequence labelling task and
determines the most likely current label given pre-
ceding labels using a Conditional Random Field
model trained using the follow features for the cur-
rent, preceding and preceding but one MSR: pre-
ceding and following word unigram and bigram;
suffix of preceding and following word; preceding
and following punctuation; reference ID; is this is
the beginning of a paragraph. If more than one la-
bel remains, the last in the list of possible REs in
the GREC-MSR data is selected.
JUNLG: The JUNLG system is based on co-
occurrence statistics between REF feature sets and
REFEX feature sets as found in the GREC-MSR data.
REF feature sets were augmented by a paragraph
counter and a within-paragraph REF counter. For
each given set of REF features, the system selects
the most frequent REFEX feature set (as determined
from co-occurrence counts in the training data). If
the current set of possible REFEXs does not include
a REFEX with the selected feature set, then the sec-
ond most likely feature set is selected. Several
hand-coded default rules override the frequency-
based selections, e.g. if the preceding word is a
conjunction, and the current SYNCAT is np-subj,
then the REG08-Type is empty.
6 Results
This section presents the results of all evalua-
tion methods described in Section 4. We start
with Word String Accuracy, the intrinsic auto-
matic metric which participating teams were told
was going to be the chief evaluation method, fol-
lowed by REG08-Type Accuracy and other intrin-
sic automatic metrics (Section 6.2), the intrinsic
human evaluation (Section 6.3) and the extrinsic
automatic evaluation (Section 6.4).
System Word String Acc. REG08-Type Acc. Norm. Edit Dist.
ICSI-CRF 0.67 0.75 0.28
UDel 0.6357 0.7027 0.3383
JUNLG 0.532 0.62 0.421
Table 2: Self-reported evaluation scores for devel-
opment set.
6.1 Word String Accuracy
Participants computed Word String Accuracy for
the development set (97 texts) themselves, using
an evaluation tool provided by us. These scores
are shown in column 2 of Table 2, and are also
included in the participants? reports in this vol-
ume. Corresponding results for test set C-1 are
shown in column 2 of Table 3. Surprisingly, Word
String Accuracy results on the test data are better
(than on the development data) for the UDel and
JUNLG systems. Also included in this table are re-
sults for the four baseline systems, and it is clear
that selecting the most frequent word string given
SEMCAT and SYNCAT (as done by the Base-freq sys-
tem) provides a strong baseline.
The other two parts of Table 3 contain results for
test sets L and P. As expected, results for Test Set L
are lower than for Test Set C-1, because in addition
to consisting of unseen texts (like C-1), Test Set L
is also from an unseen subdomain (unlike C-1).
The Word String Accuracy results for Test Set P
are higher than for any other set, probably for the
reasons discussed at the end of Section 3.
For each test set in Table 3 we carried out a
univariate ANOVA with System as the fixed factor,
?Number of REFEXs in a text? as a random factor,
and Word String Accuracy as the dependent vari-
able. We found significant main effects of Sys-
tem on Word String Accuracy at p < .001 in the
case of all three test sets (C-1: F(7,1272) = 90.058;
L: F(7,440) = 44.139; P: F(7,168) = 21.991).9
The columns containing capital letters in Table 3
9We included the corpus texts themselves in the analysis,
hence 7 degrees of freedom (8 systems).
84
Test Set C-1 Test Set L Test Set P
UDel 67.68 A UDel 52.89 A UDel 77.16 A
ICSI-CRF 62.98 A JUNLG 50.80 A ICSI-CRF 72.22 A
JUNLG 61.94 A ICSI-CRF 49.20 A JUNLG 71.60 A
Base-freq 47.05 B Base-name 21.06 B Base-freq 53.09 B
Base-name 28.74 C Base-freq 20.74 B Base-name 27.78 C
Base-1st 28.26 C Base-1st 20.74 B Base-1st 27.16 C
Base-rand 18.95 D Base-rand 15.11 B Base-rand 18.52 C
Table 3: Word String Accuracy scores against Test Sets C-1, L and P; homogeneous subsets (Tukey HSD,
alpha = .05) for each test set (systems that do not share a letter are significantly different).
System Word String Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People Mountains
Corpus 71.58 A 65.25 69.11 76.47 80.40 66.87
UDel 70.22 A B 68.09 71.20 76.47 76.63 64.84
JUNLG 64.57 B C 54.61 51.83 73.53 71.86 65.85
ICSI-CRF 63.69 C 58.87 56.54 64.71 72.11 60.98
Base-freq 57.01 D 51.06 57.07 58.82 63.82 53.05
Base-name 40.21 E 51.06 46.07 29.41 29.90 43.90
Base-1st 39.65 E 47.52 41.88 38.24 25.63 47.97
Base-rand 26.99 F 28.37 29.32 23.53 21.61 30.28
Table 4: Word String Accuracy scores against Test Set C-2 for complete set and for subdomains; homo-
geneous subsets (Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are
significantly different).
show the homogeneous subsets of systems as de-
termined by post-hoc Tukey HSD comparisons of
means. Systems whose Word String Accuracy
scores are not significantly different (at the .05
level) share a letter.
The results for Word String Accuracy com-
puted against Test Set C-2 are shown in Table 4.
These should be considered the chief results of the
GREC-MSR?09 Task evaluations, as stated in the
participants? guidelines. Here too we performed
a univariate ANOVA with System as the fixed fac-
tor, Number of REFEXs as the random factor and
Word String Accuracy as the dependent variable.
There was a significant main effect of System
(F(7,1272) = 74.892, p < .001). We compared the
mean scores with Tukey?s HSD. As can be seen
from the resulting homogeneous subsets, there is
no significant difference between the corpus texts
(C-1) and the UDel system, but also there is no
significant difference between the latter and the
JUNLG system. In this analysis, all peer systems
outperform all baselines; the Base-freq baseline
outperforms all other baselines; and Base-name
and Base-1st outperform the random baseline.
Overall, there is a marked improvement in Word
String Accuracy compared to GREC-MSR?08
where peer systems? scores ranged from 50.72 to
65.61.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure re-
ported on in the preceding section, we computed
REG08-Type Accuracy and the string similarity
metrics described in Section 4.1. The resulting
scores for Test Set C-2 are shown in Table 5 (re-
call that in Test Set C-2 corpus texts are evalu-
ated against 3 texts with human-selected alterna-
tive REs). The corpus texts again receive the best
scores across the board. Ranks for peer systems
are very similar to those reported in the last sec-
tion.
We performed a univariate ANOVA with Sys-
tem as the fixed factor, Number of REFEXs as the
random factor, and REG08-Type Accuracy as the
dependent variable. The main effect of System
was F(7,1272) = 75.040, p < .001; the homoge-
neous subsets resulting from the Tukey HSD post-
hoc analysis are shown in columns 3?5 of Table 5.
The differences between the scores of the peer sys-
tems and the corpus texts were not found to be sig-
nificant.
6.3 Human-assessed intrinsic measures
Table 6 shows the results of the human intrinsic
evaluation. In each of the three parts of the ta-
ble (showing the results for Fluency, Clarity and
Coherence, respectively) systems are ordered in
terms of their mean scores (shown in the second
column of each part of the table). We first es-
tablished that the main effect of Evaluator was
weak (F between 2.1 and 2.6) on Fluency, Clar-
ity and Coherence, and only of borderline signifi-
cance (just below .05); and that the interaction be-
tween System and Evaluator was very weak and
85
System Other similarity measures for Triple-RE Test Set C-2
REG08-Type BLEU-3 NIST SE norm. SE
Corpus 79.30 A 0.77 5.60 1.04 0.34
UDel 77.71 A 0.74 5.32 1.11 0.37
JUNLG 75.40 A 0.53 4.69 1.34 0.40
ICSI-CRF 75.16 A 0.54 4.68 1.32 0.41
Base-freq 62.50 B 0.54 4.30 1.93 0.50
Base-name 51.04 C 0.46 4.76 1.80 0.63
Base-1st 50.32 C 0.39 4.42 1.93 0.63
Base-rand 48.09 C 0.26 3.02 2.30 0.72
Table 5: REG08-Type Accuracy, BLEU, NIST and string-edit scores, computed on test set C-2 (systems
in order of REG08-Type Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for REG08-Type
Accuracy only (systems that do not share a letter are significantly different).
Fluency Clarity Coherence
Corpus 4.43 A Base-name 4.62 A Corpus 4.40 A
UDel 4.27 A Corpus 4.56 A JUNLG 4.33 A
JUNLG 4.26 A JUNLG 4.50 A UDel 4.27 A B
ICSI-CRF 4.15 A B ICSI-CRF 4.45 A ICSI-CRF 4.02 A B
Base-freq 3.33 B C UDel 4.35 A Base-freq 3.96 A B
Base-name 2.84 C D Base-1st 4.27 A Base-name 3.85 A B
Base-1st 2.76 C D Base-freq 4.10 A Base-1st 3.7 A B
Base-rand 2.15 D Base-rand 3.18 B Base-rand 3.46 B
Table 6: Clarity, Fluency and Coherence scores (with homogeneous subsets) for all systems.
not significant in the case of Clarity and Coher-
ence, and borderline significant in the case of Flu-
ency. We then ran a (non-factorial) multivariate
ANOVA, with Fluency, Coherence and Clarity as
the dependent variables, and (just) System as the
fixed factor. The main effect of System was as
follows: Fluency: F(7,128) = 20.444, p < 0.001;
Clarity: F(7,128) = 5.248, p < 0.001; Coherence:
F(7,128) = 2.680, p < 0.012. The homogeneous
subsets resulting from a post-hoc Tukey analysis
are shown in the letter columns in Table 6.
The effect of System was strongest on Fluency;
here, the system ranks are also the same as for
Word String Accuracy and REG08-Type Accuracy
for Test Set C-2. This, together with the fair
amount of significant differences found, indicates
that the evaluators were able to make sense of the
Fluency criterion and that there were interesting
differences between systems under this criterion.
However, differences between the three peer sys-
tems were not significant.
For Clarity, there were no significant differ-
ences among the peer systems and non-random
baseline systems; all of these were significantly
better than the random baseline. Base-name had
the highest mean Clarity score, possibly because
always chosing the name of an entity when refer-
ring to it ensures high referential clarity.
The Coherence results are perhaps the most dif-
ficult to interpret. Both the main effect of System
on Coherence and its significance were weaker
than for Fluency and Clarity. Only two signifi-
cant pairwise differences were found: Corpus and
JUNLG were better than the random baseline. The
system ranks are roughly the same as for Fluency,
but the mean scores cover a smaller range (from
3.46 to 4.4) than in the case of either of the other
two criteria. Overall, the Coherence results proba-
bly indicate that the evaluators found it somewhat
difficult to make sense of the Coherence criterion.
Computing Pearson?s r for the three criteria
on individual (text-level) scores showed that there
were only moderate correlations between them (all
around r = 0.5) which were all significant at
? = 0.05. This gives some indication that the
evaluators were able to assess the three criteria in-
dependently from each other.
6.4 Automatic extrinsic measures
We fed the outputs of all eight systems through
the two coreference resolvers, and computed mean
MUC, CEAF and B-CUBED F-Scores as described
in Section 4.2. The second column in Table 7
shows the mean of these three F-Scores, to give
a single overall result for this evaluation method.
A univariate ANOVA with mean F-Score as the de-
pendent variable and System as the fixed factor
revealed a significant main effect of System on
mean F-Score (F(7,1456) = 73.061, p < .001).
A post-hoc comparison of the means (Tukey HSD,
alpha = .05) found the significant differences in-
dicated by the homogeneous subsets in columns
3?4 (Table 7). The numbers shown in the last
three columns are the separate MUC, CEAF and B-
CUBED F-Scores for each system, averaged over
the two resolver tools. ANOVAs revealed the fol-
86
lowing effects of System on the separate scoring
methods: on CEAF F(7,1456) = 43.471, p < .001;
on MUC: F(7,1456) =, p < .001; on B-CUBED:
F(7,1456) = 38.574, p < .001. All three scor-
ing methods separately and their mean yielded the
same significant differences (as shown in columns
3?4 of Table 7).
The three F-Score measures (MUC, CEAF and B-
CUBED) are all significantly correlated (p < .001,
2-tailed). However it is not a strong correlation,
with Pearson?s correlation coefficient around 0.5.
System (MUC+CEAF+B3)/3 MUC CEAF B3
Base-name 65.19 A 62.35 63.14 70.06
Base-1st 63.77 A 59.95 62.08 69.28
Base-freq 63.14 A 59.08 62.04 68.3
UDel 46.19 B 34.85 46.86 56.86
ICSI-CRF 44.47 B 31.61 45.58 56.21
JUNLG 44.19 B 31.27 45.21 56.10
Base-rand 42.99 B 30.24 43.04 55.7
Corpus 42.52 B 29.53 43.57 54.47
Table 7: MUC, CEAF and B-CUBED F-Scores for
all systems; homogeneous subsets (Tukey HSD),
alpha = .05, for mean of F-Scores.
6.5 Correlations
When assessed on the system-level scores and us-
ing Pearson?s r, all evaluation methods above were
strongly and significantly correlated with each
other (at the 0.01 level, 2-tailed), with the fol-
lowing exceptions. Clarity was not significantly
correlated with any of the other methods except
NIST (r = .902, p < .01); apart from this, NIST
was only correlated with Word String Accuracy on
test set C-2, with non-normalised string-edit dis-
tance, Fluency and Coherence, moreover all at the
weaker 0.05 level. Finally, the extrinsic method
was not correlated with any of the intrinsic meth-
ods (and in fact showed signs of being negatively
correlated with all of them except Clarity).
7 Concluding Remarks
The GREC-MSR Task is still a relatively new task
not only for an NLG shared-task challenge, but also
as a research task in general (post-processing ex-
tractive summaries in order to improve their qual-
ity seems to be just taking off as a research sub-
field). There was substantial interest in the GREC-
MSR Task this year (as indicated by the nine teams
that originally registered). However, only three
teams were ultimately able to participate.
We continued the traditions of previous NLG
shared tasks in that we used a wide range of eval-
uation metrics to obtain a well-rounded view of
the quality of the participating systems. This in-
cluded intrinsic human evaluations for the first
time. However, we decided against an extrinsic
human evaluation this year, given time constraints
as well as the fact that this evaluation type yielded
barely any significant results last year.
Overall, there was an improvement in system
performance compared to last year, to the point
where the performance of the top system was
barely distinguishable from the human topline.
We are not currently planning to run the GREC-
MSR task again next year.
Acknowledgments
Many thanks to the UCL and Sussex students who
participated in the intrinsic evaluation experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC?98, pages
563?566.
A. Belz and S. Varges. 2007a. Generation of repeated
references to discourse entities. In Proceedings of
ENLG?07, pages 9?16.
A. Belz and S. Varges. 2007b. The GREC corpus:
Main subject reference in context. Technical Report
NLTG-07-01, University of Brighton.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of
Pensylvania.
A. Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Proceedings of IJC-
NLP?08.
L. Qiu, M. Kan, and T.-S. Chua. 2004. A public refer-
ence implementation of the rap anaphora resolution
algorithm. In Proceedings of LREC?04, pages 291?
294.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management:
Special issue on Summarization, 43(6):1663?1680.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. Proceedings of MUC-6, pages
45?52.
87
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2007?2017, Dublin, Ireland, August 23-29 2014.
Learning when to point: A data-driven approach
Albert Gatt
Institute of Linguistics
University of Malta
albert.gatt@um.edu.mt
Patrizia Paggio
Institute of Linguistics, Uni of Malta
Centre for Language Technology, Uni of Copenhagen
patrizia.paggio@um.edu.mt
Abstract
The relationship between how people describe objects and when they choose to point is complex
and likely to be influenced by factors related to both perceptual and discourse context. In this
paper, we explore these interactions using machine-learning on a dialogue corpus, to identify
multimodal referential strategies that can be used in automatic multimodal generation. We show
that the decision to use a pointing gesture depends on features of the accompanying description
(especially whether it contains spatial information), and on visual properties, especially distance
or separation of a referent from its previous referent.
1 Introduction
The automatic generation of multimodal referring actions is a relatively under-studied phenomenon in
Natural Language Generation (NLG). While there has been extensive research on Referring Expression
Generation (REG) focusing on the choice of content in expressions such as (1) below (Dale, 1989; Dale
and Reiter, 1995; Krahmer and van Deemter, 2012), their multimodal counterpart ? exemplified in (2) ?
raises questions that go beyond these choices.
(1) the group of five large red circles
(2) there?s a group of five large red ones [+pointing gesture with arm extended]
One important question concerns the appropriateness of a pointing gesture under different conditions.
The relevant conditions here include both the physical or perceptual common ground shared by interlocu-
tors (for example, what other objects are in the vicinity of the target referent, and therefore potentially
confusable with it), the discursive common ground (for example, whether this object has been referred
to before) and the content of the interlocutor?s speech act, that is, what she chooses to say in addition to
pointing. For example in (2), the speaker, who is engaged in a dialogue in which she needs to guide her
interlocutor through a route on an abstract map (see Section 3 below), has chosen to use the cardinality
of the referent (it is a group made up of five circles), its size, and its colour. Her choice of properties
may be sufficient to distinguish it from all its distractors in the current context. However, unlike (1), (2)
is a composite utterance consisting of two communicative modalities, each of which contributes to the
communicative intention (Enfield, 2009).
This paper addresses the question of when a pointing gesture is appropriate as part of a composite,
multimodal referring action. This is an important component of many multimodal generation systems,
including those that communicate through embodied agents. We address this question in a data-driven
manner, using a corpus of dialogues in which references have been annotated at both the level of speech
and gesture. Our aim is to learn strategies for combinations of pointing and describing, as a function of
perceptual and discursive features. We first summarise some relevant psycholinguistic and computational
work (Section 2), before describing our corpus data (Section 3) and reporting on the machine-learning
experiments conducted (Section 4). Section 5 concludes with some remarks on future work.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2007
2 Pointing and reference
The idea that gesture and speech are planned separately, incorporated in early work on multimodal gen-
eration (Andr?e and Rist, 1996) is contradicted by more recent psycholinguistic research, in which gesture
and language are increasingly viewed as tightly coupled (Kita and
?
Ozy?urek, 2003; McNeill, 1985; Mc-
Neill and Duncan, 2000), contributing jointly to the composite utterances (Enfield, 2009). This view
has also influenced recent work in multimodal NLG. For example, Kopp et al. (2008) use ?multimodal
concepts?, combining propositional and gestural or perceptual information.
In the case of referring expressions, pointing has been treated as a property, on a par with an object?s
colour or size. Thus, van der Sluis and Krahmer (2007) propose an algorithm in a graph-based framework
(Krahmer et al., 2003) which selects pointing gestures of varying degrees of precision based on their cost
when compared to other linguistically realisable features. Similarly, Kranstedt and Wachsmuth (2005)
propose an extension of Dale and Reiter?s (1995) Incremental Algorithm, which initially considers the
possibility of producing an unambiguous pointing gesture. If this fails, a pointing gesture that is less
precise may be generated, together with descriptive features of an object.
Both of these approaches assume that the choice of modality in a referring action ultimately hinges on a
trade-off between what can be said and what is easiest to produce, a view that has some empirical support
(Beun and Cremers, 1998; Bangerter, 2004; Piwek, 2007). On the other hand de Ruiter et al. (2012)
found that likelihood of pointing was unaffected by the difficulty of using descriptive features. From
a computational perspective, our earlier work (Gatt and Paggio, 2013) also found evidence, based on a
machine-learning study on dialogue data, for the co-occurrence of pointing with descriptive (especially
spatial) features, suggesting that pointing gestures may be planned in tandem (and not in competition)
with these features.
The present paper uses the same corpus data as Gatt and Paggio (2013); however, that paper focused
on the relationship between descriptive features (in the spoken part of the utterance) and pointing. In
contrast, here we take a much broader view, also addressing the impact of the physical/perceptual features
of the objects under discussion, and aspects of the discourse history.
3 Data used in this study
Matcher's mapwithoutitinerary
Director's mapwith itinerary
Director Follower
Low screensto hide maps
(a) Experiment setup
START
1/5
16
9
2/1817
4/ 1514
12 7/11
8/13
10
16
16
16
16
14 141414
4/ 154
/ 15 4/ 154/ 15
12 12
12
12
1010
10 1
0
7/117/11
7/11 7/11
8/138/1
3 8/138/13
1/5
1/51/5
1/5
9
99 9
17 1717
17
2/18
2/18 2/18 2/183/63/63/63/63/6
(b) Group circles map (numbers indicate the order in
which landmarks are visited along the itinerary)
Figure 1: MREDI dialogue setup (reproduced from Gatt and Paggio (2013)).
We use the MREDI (Multimodal REference in DIalogue) corpus (van der Sluis et al., 2008; Gatt and
Paggio, 2013), a collection of MapTask-like dialogues (Anderson et al., 1991). Dialogues in MREDI
were conducted by dyads consisting of a Director and a Follower. The Director?s task was to guide the
Follower along a route through a visually shared ?map?, located approximately one metre away, directly
in front of them, blown up to roughly A0 size. The Director also had a private map on which the route
was indicated, while the Follower?s private map was used to mark the route as it unfolded in the course
of the conversation. Figure 1a displays the basic setup.
2008
There were no restrictions on what interlocutors could say. Participants in the study were told in
advance that they could use both speech and gestures, but were not explicitly instructed to point. The
maps consisted of collections of shapes of different colours and sizes and were very densely populated
(see Figure 1b). Four maps were used in the study: in two of these, landmarks consisted of individual
circles or squares, while in the other two they consisted of groups or clusters of five circles or squares
(Figure 1b is a group circle map). In the group maps, all elements of a group of five were of the same
colour and size.
On each map, there were 18 ?landmarks?; these were the milestones along the itinerary and were
marked on the Director?s private map, but not visible on the large map that constituted the common
ground. For example, the landmarks (groups of 5 circles) in Figure 1b are numbered from 1 to 18. Each
dyad did all four maps; the order was randomised for each pair of participants. Participants switched roles
between one map and another. In addition to the difference between group and individual landmarks, the
maps were designed to manipulate a number of independent variables:
1. Distinguishing Properties (DistProps): Landmarks on the itinerary differed from their distractors
? the objects in their immediate vicinity (the focus area) ? in colour, or in size, or in both colour
and size. The focus area was defined as the set of objects immediately surrounding a target. This
means that different landmarks required different combinations of properties to ensure that they
could be unambiguously identified by a description. For example, in Figure 1b, the group marked
17 consists of a landmark where size is the distinguishing feature, since all five circles in the group
are small, and the objects in their immediate vicinity are either large or medium-sized. There were
equal numbers of landmarks on each map that could be distinguished by colour only, size only, or
both.
2. Prior reference (Discourse): Some of the landmarks were visited twice in the itinerary; these are
indicated using two numbers in Figure 1b. Thus, landmark 8 in this map was also visited later as
landmark 13. There were 6 landmarks on each map that were revisited in this way. This is the
primary manipulation related to discourse history.
3. Shift of domain focus (Distance): Landmarks were located either near to or far away from the
previous target. For example, in Figure 1b, landmark 17 and landmark 18 are adjacent (?near?
condition), but landmark 17 is far from the preceding landmark 16.
In what follows, we use data from 8 dyads. Similar to Gatt and Paggio (2013), we only consider
utterances by Directors. These were transcribed and split up according to the landmark to which they
corresponded. In case a landmark was described over multiple turns in the dialogue, each turn was
annotated as a separate utterance. Our dataset consists of a total of 2255 such utterances, of which 370
(16.4%) contain a pointing gesture. This is a relatively low proportion of such gestures, compared to
some previous studies, such as Beun and Cremers (1998), who found that 48% of referential acts in
their task-oriented dialogue corpus included a pointing gesture. However, Beun and Cremers focussed
exclusively on first-mention referring expressions. Furthermore, the low proportion of pointing gestures
in MREDI may be due to the fact that under our definition, the identification of a landmark may be spread
over several turns, with possible interruptions by the Follower. Each such turn constitutes a separate
utterance. This raises the likelihood that certain features of the composite utterance, including pointing,
will only occur on some of the turns.
3.1 Features
Utterances in MREDI were annotated with the features displayed in Table 1. These codify aspects of the
descriptive content of a referential act, as well as the presence or absence of a pointing gesture.
The features originally encoded in the MREDI corpus had frequency values; Gatt and Paggio (2013)
used these frequencies in their study. However, for our experiments, we collapsed the features related
to descriptive content ? hereafter referred to as descriptive features ? into boolean features. This signif-
icantly reduces the feature set and makes the rules acquired in our machine-learning experiments easier
2009
Feature Name Definition Example
Visual
S Size mention of the target size the group of small circles
Sh Shape mention of the target shape the circles at the bottom
C Colour mention of the target colour The blue square near the red square
Deictic/anaphoric
ID Identity Statement of identity between
the current and a previous or later target
the red square,
the same one we saw at number 5
D Deixis Use of a deictic reference those squares
Locative
RP Relative position Position of the target landmark relative
to another object on the map
the blue square
just below the red square
AP Absolute position Target position based on absolute
frame of reference
The blue circle down at the bottom
FP Path references References to non-targets on the
path leading to the target.
go east to the first tiny square,
past the blue one
DIR Directions Direction-giving. take a right, go across
and straight down
Action
GZ Gaze Gaze at the shared map (boolean).
Point Pointing Use of a pointing gesture (boolean).
1
Table 1: Features annotated in the dialogues. All features have frequency values, except for the Action
features, which are boolean.
to interpret. Further, it enables us to test our hypothesis that the presence or absence of a type of feature
(descriptive, physical or discursive) impacts the decision to point. The boolean descriptive features are
as follows:
1. Deixis: this has the value true if the utterance contains a demonstrative pronoun (such as that), or
a reference to the landmark that identifies it with the previous landmark. Thus, this feature is true
if ID > 0 or D > 0 in Table 1;
2. Locative: this has the value true if the utterance contains any of the spatial properties in Table 1.
Thus, the feature is true if AP > 0 or RP > 0 or FP > 0 or DIR > 0.;
3. Visual: this has the value true if the utterance contains at least one mention of the landmark?s
visual properties. This, the feature is true if C > 0 or Sh > 0 or S > 0.
In addition to these features, our experiments also made use of the physical features (Distance and
DistProps) manipulated as part of the MREDI data collection study (see above), as well as the feature
Discourse, which encodes prior reference.
Finally, we added a new feature to the dataset, MapConfl, which indicates the type of map on which
utterances were produced, namely, individual or group circles or squares. This feature was included
because the larger size of group landmarks, compared to individuals, may have influenced the decision
to point, since groups are more visually salient.
The feature Gaze is present whenever a pointing gesture is made; hence, it is not used in the machine
learning experiments reported below.
4 Experiments
In our earlier study on the MREDI corpus (Gatt and Paggio, 2013), investigating the relationship between
pointing and descriptive features, we found that the latter could indeed be used as predictors of pointing
gestures with an accuracy of 0.833 (F-score). The study also concluded that among the descriptive
features it was locative properties that were most useful in guiding the decision of whether or not to
point, compared to features describing visual characteristics of the objects.
However, in much of the work reviewed in Section 2, especially work arguing in favour of a trade-off
in cost between pointing and describing, the occurrence of pointing is made to depend on the physical
properties of referents. Therefore, in the present study we want to test whether the occurrence of pointing
2010
gestures can be predicted more accurately as a function of (i) the descriptive features that speakers use
to refer to landmarks; (ii) the physical/perceptual context in which they are found and (iii) whether
or not they have been referred to earlier in the discourse. Furthermore, we want to investigate which
combinations of physical and descriptive features provide the best results.
Two sets of experiments were conducted on different versions of the MREDI dataset. The first dataset
(referred to as the complete dataset) is the same one used in the Gatt and Paggio (2013) study. It includes
all of the 2255 Director?s utterances from the eight dyads in the corpus, including those that did not
contain any references at all, linguistic or gestural. Such utterances might, for example, be confirmations
or feedback produced in the course of the dialogue.
We also report results on a second dataset (referred to as the referential dataset), consisting of all
utterances that contain a reference, either using descriptive features, pointing, or both. This dataset
consisted of 1542 utterances. Note that the number of utterances with a pointing gesture is still 370 in
the pruned dataset.
The task in the experiments was to classify the binary feature Point. As mentioned earlier, 370 of these
utterances contain a pointing gesture. In other words, there are 370 occurrences of Point=1.
All the experiments were run using the Weka tool (Witten and Frank, 2005), which gives access
to many different algorithms, using 10-fold cross-validation throughout. In the experiments with the
complete dataset, the ZeroR and OneR classifiers were first run on the data to establish a baseline.
ZeroR always chooses the most frequent value of the class that is being predicted; in the present case,
it consistently classifies all utterances as Point = 0, since the majority of utterances do not containg
pointing gestures. OneR identifies a single feature, on the basis of which all classifications are made.
On the MREDI data, OneR always assigned Point = 0 to all utterances, based on a single rule using the
MapConfl feature (i.e. the type of map or domain in which the dialogue was being carried out). Note
that both baseline classifiers were trained using all features.
Various combinations of descriptive and physical features were then tested using different classifiers
in Weka, including NaiveBayes, Support Vector Machines, Maximum Entropy (Logistic in Weka) and
the J48 Decision Tree classifier. The present paper will report results for the last two of these, for the fol-
lowing reasons. First, these were the ones which performed best. In addition, the decision trees built by
J48 provide an analysis tool to understand how the various features interact, given their transparency; on
the other hand, MaxEnt sometimes outperforms J48 and provides a ?ceiling?, in addition to the baselines
described above.
The strategy used in testing feature combinations was essentially ablative. We tested first using all
features, and then compared the performance of the classifiers when they use only descriptive features
(Visual, Locative and Deixis), or only Discourse together with the physical features (DistProps and
Distance). Omitting descriptive features and using only physical features with Discourse invariably
performed near or below baseline (see below). Thus, we experimented with combinations of descriptive
features and each physical feature, as well as Discourse, individually.
4.1 Results on the complete dataset
The results for the complete dataset are shown in Table 2 in terms of Precision, Recall and F-measure for
each of the classifiers. The top rows display the results using all features, while the baseline results are
in the bottom rows. The remaining results for different combinations of features are in descending order
of F-score.
Interestingly, using all features ? i.e. MapConfl, DistProps, Discourse, Distance, Visual, Locative and
Deictic ? with or without MapConfl, results in worse overall performance than using a combination of de-
scriptive features (Locative, Deictic and Visual) with Distance. This combination is closely matched for
accuracy by the combination involving descriptive features, Distance and DistProps. However, dropping
Distance (using only descriptive features and DistProps) results in worse performance.
The addition of Distance and/or DistProps clearly improves the predictive accuracy of a classifier that
uses descriptive features. However, the worst combination is found when the descriptive features are
excluded. This is in line with the results reported by Gatt and Paggio (2013), who found that features of
2011
Classifier P R F Features
J48 0.827 0.847 0.832 All
Logistic 0.831 0.854 0.828 All
J48 0.833 0.851 0.838 All - MapConfl
Logistic 0.832 0.851 0.837 All - MapConfl
Logistic 0.839 0.853 0.844 Descriptive + Distance
J48 0.839 0.853 0.844 Descriptive minus Deictic + Distance
Logistic 0.839 0.853 0.844 Descriptive minus Deictic + Distance
J48 0.836 0.851 0.84 Descriptive+DistProps + Distance
J48 0.839 0.853 0.84 Descriptive+Distance
Logistic 0.833 0.851 0.838 Descriptive+DistProps + Distance
J48 0.821 0.847 0.824 Descriptive+DistProps
Logistic 0.809 0.842 0.794 Only Descriptive
Logistic 0.809 0.842 0.794 Descriptive + DistProps
Logistic 0.809 0.842 0.793 Descriptive + Discourse
J48 0.803 0.84 0.787 Only Descriptive
J48 0.795 0.838 0.781 Descriptive + Discourse
J48 0.699 0.836 0.761 Physical + Discourse
Logistic 0.699 0.836 0.761 Physical + and Discourse
ZeroR 0.699 0.836 0.761 All
OneR 0.699 0.836 0.761 All
Table 2: Predicting pointing gestures with different feature combinations in the complete MREDI dataset.
the descriptions produced by speakers were good predictors of pointing.
Adding only DistProps to the descriptive features improved the accuracy of the Logistic classifier
somewhat, though it had a greater impact on J48. However, Distance seems to have the greatest impact
of the two physical features. Discourse does not appear to play an important role: incorporating this
feature does not result in much improvement over using only descriptive features; indeed, in the case of
J48, it decreases accuracy.
We also tested one of the best combinations involving descriptive features and Distance but excluding
the Deictic feature from the set of descriptive features. This was done because pointing in referential
acts is frequently viewed on a par with deictic expressions, insofar as they are both indexical (Bangerter,
2004). This raises the question whether, out of all the descriptive features, Deixis could be considered
a somewhat redundant predictor. The results suggest that removing Deixis from the descriptive features
does not alter the accuracy of the classifier. We return to the role of Deixis in the discussion in Section
4.3.
4.2 Results on the referential dataset
Exactly the same combinations of features were tested, using 10-fold cross-validation, in separate ex-
periments on the referential dataset. This was done in order to compare the results on a dataset which
contains less ?noise?, that is, fewer utterances which were non-referential. Such utterances may compro-
mise the predictive validity of certain features, as they inflate the number of utterances in which Point=0.
Table 3 contains the results obtained on the reduced dataset. The accuracy is in general lower due to
the fact that predicting the absence of pointing is easier in the complete dataset, where many utterances
contain no reference at all, descriptive or gestural.
Contrary to the findings on the complete dataset, using the complete set of features as predictors of
pointing gives slightly better results than using either descriptive or physical features alone, at least in
2012
Classifier P R F Features
J48 0.783 0.799 0.785 All - MapConfl
Logistic 0.726 0.764 0.679 All - MapConfl
J48 0.774 0.793 0.776 All
Logistic 0.704 0.760 0.681 All
J48 0.781 0.797 0.784 Descriptive + DistProps + distance
J48 0.766 0.785 0.770 Descriptive + DistProps
J48 0.748 0.777 0.745 Descriptive + Distance
J48 0.758 0.783 0.744 Only descriptive
J48 0.774 0.788 0.740 Descriptive + Discourse
Logistic 0.720 0.762 0.675 Descriptive + DistProps + distance
Logistic 0.688 0.759 0.662 Descriptive + Discourse
Logistic 0.699 0.760 0.661 Descriptive + DistProps
Logistic 0.759 0.761 0.660 Descriptive + Distance
J48 0.577 0.760 0.656 Only physical + Discourse
Logistic 0.577 0.760 0.656 Only descriptive
Logistic 0.577 0.760 0.656 Only physical + Discourse
ZeroR 0.577 0.76 0.656 All
ONeR 0.577 0.76 0.656 All
Table 3: Predicting pointing gestures with different feature combinations in the referential MREDI
dataset.
the case of the decision tree classifier. This combination also exceeds the combination of descriptives,
DistProps and Distance, though only marginally. However, this does remain the next best combination
for J48, consistent with the results on the complete dataset. However, this combination performs quite
badly in the case of the Logistic classifier.
The fact that using all features performs better this time is probably due to the fact that there are fewer
non-referential utterances in this dataset. Once again, the role of Discourse seems marginal.
4.3 Analysis and discussion
Figure 2 shows the decision trees built by J48 for the two datasets when descriptive features are used
together with DistProps and Distance.
Visual
0 (1556/117)
Locative
0 (310/57)
Distance
0 (116/40) 1 (273/117)
=TRUE
=FALSE
=FALSE
=TRUE
=CLOSE =FAR
(a) Complete MREDI dataset
Visual
Locative
0 (310/57)
Deictic
0 (290/28)
Locative
0 (519/56) 1 (33)
Deictic
1 (68/27) DistProps
1 (109/50)
0 (118/48)
Distance
0 (23/8) 1 (71/31)
=TRUE =FALSE
=TRUE
=FALSE =TRUE
=FALSE
=TRUE =FALSE=TRUE
=FALSE
=COLOUR
=SIZE
=BOTH
=CLOSE =FAR
(b) Referential MREDI dataset
Figure 2: J48 Decision trees from the complete and referential datasets
Our main findings can be summarised as follows. First, descriptive features play an important role in
2013
the prediction of pointing; this replicates previous observations (Gatt and Paggio, 2013). Second, and
more importantly, the prediction accuracy improves when physical features, representing aspects of the
visual/perceptual context, are taken into account. This is especially true of Distance, suggesting that
a sizeable shift of perceptual focus, from one landmark to another further away, motivates a pointing
gesture, as shown in both trees in Figure 2. Once again, it is worth comparing this to the results of Beun
and Cremers (1998), who find that shifts of perceptual focus play a role in increasing the amount of
(descrpitive) information speakers include in a referring expression. However, they find no impact of
focus shifts on pointing gestures; our results, by contrast, suggest that such shifts do play a role.
There are a number of striking features in the trees in the figure. First, the descriptive feature Locative
plays a crucial role. All cases of pointing involve the presence of a Locative, with one exception: on the
referential dataset (Figure 2b), in case no Visual, Deictic or Locative features are used, the tree predicts
a pointing gesture. However, this case covers a very small number of instances (33), with 0% error rate.
All of these turn out to be utterances where there is no descriptive reference at all and speakers rely
exclusively on pointing. Example (3) below is typical of these.
(3) D: And a slightly bigger green to the right of that
M: M-hm
D: In the center of those like pack
M: Yeah
D: is number 9. [+pointing]
Clearly, these are cases in which the pointing gesture occurs as part of an extended sequence of ut-
terances which jointly identify a landmark. Descriptive features have already been uttered; the pointing
comes at the very end. In summary, the one case where Locatives don?t feature in predicting a pointing
gesture turns out to be a rather special case.
A second striking aspect of the trees is that while Deixis plays a predictive role in the tree based
on the referential dataset, it doesn?t in the case of the complete dataset. This is interesting in view of
the relationship that has often been noted between referential pointing gestures and deictic expressions
(Bangerter, 2004). Note, however, that there is no inconsistency between the two trees: the single path
through the tree in Figure 2a that results in pointing is subsumed by the path in Figure 2b which specifies
in addition that Deixis should be false, and DistProps should have the value colour. This still leaves
open the question why Deixis plays no role in the full dataset, despite being included as part of the
descriptive features that resulted in this tree. Indeed, we have already shown that, among the descriptive
features, Deixis doesn?t contribute much predictive power on the full dataset (see Section 4.1).
One possibility is that Deixis is generally under-represented in the corpus. However, there are propor-
tionately fewer utterances in the full dataset containing Deixis (20%), compared to the referential dataset
(30%). Furthermore, it may be partially dependent on the Locative features. There may be a priori
reasons to assume this as a working hypothesis: Deixis anchors parts of the speech signal to physical
properties of the common ground, potentially making it redundant with respect to location (which has
already specified the relevant physical/spatial features of the common ground).
Complete Dataset Referential Dataset
Locative Deictic Deictic
false true false true
false 74 26 42 58
true 88 12 88 12
overall 80 20 70 30
Table 4: Deictic features (D and ID) relative to Locatives. All figures are percentages.
Table 4 displays the distribution of Deictic expressions with respect to Locatives, that is, the proportion
of utterances containing a Deictic expression as a function of whether the utterances also contain a
Locative expression. The tables shows proportions both for the full and the referential dataset.
2014
Note that when Deictics are used, it is mostly in the absence of a Locative expression. A chi-square
test of independence suggests that the frequency of use of Locative and Deictic expressions are not
independent (complete dataset: ?
2
1
= 63.044, p < .001; referential: ?
2
= 358.21, p < .001). However,
there is a higher proportion of Deictic expressions in the referential dataset (30% overall); this may
account for the use of this feature in the decision tree for this dataset (it is more informative). Crucially,
the trend in the use of deictic expressions is reversed in the two datasets: when Locative is false on
the referential dataset, most utterances involve a deictic expression; the reverse is true on the complete
dataset.
There is one path through the tree in Figure 2b which seems to contradict the hypothesis that deictic
expressions are used in the absence of locatives. There are 68 cases where pointing is used when both
Deictic and Locative are true. One possibility is that this is caused by our having defined the Deictic
feature as true whenever there is an actual deictic expression (variable D in Table 1; e.g. those squares)
or an identity expression (variable ID; e.g. the same one we saw). To investigate this further, Table 5
shows a breakdown of the frequencies of the presence or absence of a locative expression, as a function of
whether a true deictic (D) or an identity expression (ID) is used in an utterance. Once again, proportions
are displayed for both datasets.
Complete dataset Referential dataset
True Deictic Locative Locative
false true false true
false 54 46 26 74
true 78 22 76 24
(a) True deictic expressions (D)
Complete dataset Referential dataset
Identity Locative Locative
false true false true
false 56 44 31 69
true 68 32 67 33
(b) Identity expressions (ID)
Table 5: Identity (ID) and actual Deictic (D) expressions relative to Locatives. All figures are percent-
ages.
There are two observations that stem from these proportions: First, in line with our earlier observa-
tions, there is a greater proportion of true deictic (D) expressions in utterances that contain no locative
expression. For example, 78% of utterances in the complete dataset that have no locatives contain a
deictic; the corresponding figure in the referential dataset is 76%. The same pattern holds for identity
(ID) expressions. Second, out of the utterances that do not contain a locative, the proportion containing
a true deictic (D) is greater than the proportion containing an identity expression (ID). This may explain
the apparent exception ? represented by the path in Figure 2b ? to our generalisation that locatives and
deictics are redundant with respect to each other, and locatives tend to be avoided if deictics are used.
The explanation may lie in the conflation, in the boolean Deictic feature used in our experiments, of true
deictics and identity expressions. The path in the decision tree where both Locative and Deictic are true
may be accounting for utterances in which an identity expression is used, rather than a true deictic.
5 Conclusions and future work
This paper addressed the question of when pointing gestures should be generated, as a function of the
features a speaker uses to identify a referent, as well as the features of the context in which an utterance
is being produced. The best predictors of pointing are descriptive features, especially locatives, and
features of the perceptual context, especially distance from the last referent. The latter is a marker of
a shift of perceptual focus, akin to the focus shifts identified by Beun and Cremers (1998). Our study
also sheds light on the relationship between pointing and the use of deictic expressions, suggesting that,
while the two are often used together, deictics tend to be used more in the absence of locative features.
We also note some limitations of our methodology. Inspection of the results in Tables 2 and 3 shows
that the best performing classifiers, though they exceed baselines, do not do so by a wide margin. We
believe that one of the main reasons for this is the relative scarcity of pointing gestures in our dataset (as
discussed in Section 3), which may have resulted in a sizeable subset of utterances where pointing was
relatively straightforward to predict (e.g. based on one feature, as in the OneR baseline classifier). This
is a limitation we intend to investigate in future work, through a more diverse dataset where pointing
2015
features more strongly. In addition, it is worth noting that the ablative testing reported here does suggest
that certain features play a greater role in determining when speakers choose to point.
Our work addresses an important question in Natural Language Generation systems that seek to gen-
erate multimodal referring acts, namely, how pointing and describing should be combined and when. In
future work, we intend to extend this research in two ways: first, by extending our focus to incorporate
the interactive features of a dialogue and their impact on referential success; and second, by focusing on
other domains with a view to testing the generalisability of the results.
Acknowledgements
Thanks to Ielka van der Sluis, Adrian Bangerter and Paul Piwek, who collaborated on the development
of the MREDI corpus. Thanks to the anonymous reviewers of COLING 2014 for helpful comments.
References
A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Doherty, S. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller,
C. Sotillo, H. S. Thompson, and R. Weinert. 1991. The HCRC Map Task corpus. Language and Speech,
34:351?366.
E. Andr?e and T. Rist. 1996. Coping with temporal constraints in multimedia presentation planning. In Proceedings
of the 13th National Conference on Artificial Intelligence (AAAI?96).
A. Bangerter. 2004. Using pointing and describing to achieve joint focus of attention in dialogue. Psychological
Science, 15(6):415?419.
R.J. Beun and A. Cremers. 1998. Object reference in a shared domain of conversation. Pragmatics and Cognition,
6(1-2):121?152.
R. Dale and E. Reiter. 1995. Computational interpretation of the Gricean maxims in the generation of referring
expressions. Cognitive Science, 19(8):233?263.
R. Dale. 1989. Cooking up referring expressions. In Proceedings of the 27th annual meeting of the Association
for Computational Linguistics (ACL?89), pages 68?75.
J.P. de Ruiter, A. Bangerter, and P. Dings. 2012. The interplay between gesture and speech in the production of
referring expressions: Investigating the tradeoff hypothesis. Topics in Cognitive Science, 4:232?248.
N.J. Enfield. 2009. The Anatomy of Meaning: Speech, Gesture and Composite Utterances. Cambridge University
Press, Cambridge.
A. Gatt and P. Paggio. 2013. What and where: An empirical investigation of pointing gestures and descriptions in
multimodal referring actions. In Proceedings of the 14th European Workshop on Natural Language Generation
(ENLG?13).
S. Kita and A.
?
Ozy?urek. 2003. What does cross-linguistic variation in semantic coordination of speech and
gesture reveal?: Evidence for an interface representation of spatial thinking and speaking. Journal of Memory
and Language, 48:16?32.
S. Kopp, K. Bergmann, and I. Wachsmuth. 2008. Multimodal communication from multimodal thinking: Towards
an integrated model of speech and gesture production. International Journal of Semantic Computing, 2(1):115?
136.
E. Krahmer and K. van Deemter. 2012. Computational generation of referring expressions: A survey. Computa-
tional Linguistics, 38(1):173?218.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-based generation of referring expressions. Computational
Linguistics, 29(1):53?72.
A. Kranstedt and I. Wachsmuth. 2005. Incremental generation of multimodal deixis referring to objects. In
Proceedings of the 10th European Workshop on Natural Language Generation (ENLG?05).
D. McNeill and S.D. Duncan. 2000. Growth points in thinking for speaking. In D. McNeill, editor, Language and
Gesture, pages 141?161. Cambridge University Press.
2016
D. McNeill. 1985. So you think gestures are nonverbal? Psychological Review, 92(3):350?371.
P. Piwek. 2007. Modality choice for generation of referring acts: Pointing vs describing. In Proceedings of the
Workshop on Multimodal Output Generation (MOG?07)., pages 129?139.
I. van der Sluis and E. Krahmer. 2007. Generating multimodal referring expressions. Discourse Processes,
44(3):145?174.
I. van der Sluis, P. Piwek, A. Gatt, and A. Bangerter. 2008. Towards a balanced corpus of multimodal referring
expressions in dialogue. In Proceedings of the Symposium on Multimodal Output Generation (MOG?08).
I.H. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kauf-
mann, San Francisco, second edition.
2017
Attribute Selection for Referring Expression Generation:
New Algorithms and Evaluation Methods
Albert Gatt
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Anja Belz
Natural Language Technology Group
University of Brighton
Brighton BN2 4GJ, UK
a.s.belz@brighton.ac.uk
Abstract
Referring expression generation has recently
been the subject of the first Shared Task Chal-
lenge in NLG. In this paper, we analyse the
systems that participated in the Challenge in
terms of their algorithmic properties, compar-
ing new techniques to classic ones, based on
results from a new human task-performance
experiment and from the intrinsic measures
that were used in the Challenge. We also con-
sider the relationship between different eval-
uation methods, showing that extrinsic task-
performance experiments and intrinsic evalu-
ation methods yield results that are not signif-
icantly correlated. We argue that this high-
lights the importance of including extrinsic
evaluation methods in comparative NLG eval-
uations.
1 Introduction and Background
The Generation of Referring Expressions (GRE) is
one of the most intensively studied sub-tasks of
Natural Language Generation (NLG). Much re-
search has focused on content determination in GRE,
which is typically framed as an attribute selection
task in which properties are selected in order to
describe an intended referent. Since such proper-
ties are typically defined as attribute-value pairs,
in what follows, we will refer to this as the AS-
GRE (Attribute Selection for Generating Referring
Expressions) task.
1.1 Approaches to ASGRE
Since early work on ASGRE, which focused on prag-
matic motivations behind different types of refer-
ence (Appelt, 1985; Appelt and Kronfeld, 1987), the
focus has increasingly been on definite descriptions
and identification, where the set of attributes se-
lected should uniquely distinguish the intended ref-
erent from other entities (its ?distractors?). Unique
Reference in this sense is a dominant criterion for
selecting attribute sets in classic ASGRE algorithms.
Following Dale (1989), and especially Dale and Re-
iter (1995), several contributions have extended the
remit of ASGRE algorithms to handle relations (Dale
and Haddock, 1991; Kelleher and Kruijff, 2006) and
gradable attributes (van Deemter, 2006); and also to
guarantee logical completeness of algorithms (van
Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt
and van Deemter, 2007).
Much of this work has incorporated the prin-
ciple of brevity. Based on the Gricean Quantity
maxim (Grice, 1975), and originally discussed by
Appelt (1985), and further by Dale (1989), Reiter
(1990) and Gardent (2002), this principle holds that
descriptions should contain no more information
than is necessary to distinguish an intended refer-
ent. In ASGRE, this has been translated into a crite-
rion which determines the adequacy of an attribute
set, implemented in its most straightforward form in
Full Brevity algorithms which select the smallest at-
tribute set that uniquely refers to the intended refer-
ent (Dale, 1989).
Another frequent property of ASGRE algorithms is
Incrementality which involves selection of attributes
one at a time (rather than exhaustive search for a
distinguishing set), and was initially motivated by
algorithmic complexity considerations. The Incre-
mental Algorithm of Dale and Reiter (1995) was
also justified with reference to psycholinguistic find-
ings that (a) humans overspecify their references
(i.e. they are not brief when they produce a refer-
50
ence); and (b) this tends to occur with properties
which seem to be very salient or in some sense pre-
ferred (Pechmann, 1989; Belke and Meyer, 2002;
Arts, 2004). In the Incremental Algorithm (Dale
and Reiter, 1995), incrementality took the form of
hillclimbing along an attribute order which reflects
the preference that humans manifest for certain at-
tributes (such as COLOUR). This Modelling of Hu-
man Preferences in the Incremental Algorithm has
proven influential. Another feature proposed by
Dale and Reiter is to hardwire the inclusion of the
type of an entity in a description, reflecting the hu-
man tendency to always include the category of an
object (e.g. chair or man), even if it has no dis-
criminatory value. A compromise may be reached
between incrementality and brevity; for example,
Dale?s (1989) Greedy Algorithm selects attributes
one at a time, and computes the Discriminatory
Power of attributes, that is, it bases selection on the
extent to which an attribute helps distinguish an en-
tity from its distractors.
In the remainder of this paper, we uniformly use
the term ?algorithmic property? for the selection cri-
teria and other properties of ASGRE algorithms de-
scribed above, and refer to them by the following
short forms: Full Brevity, Uniqueness, Discrimina-
tory Power, Hardwired Type Selection, Human Pref-
erence Modelling and Incrementality.1
1.2 ASGRE and Evaluation
Though ASGRE evaluations have been carried out
(Gupta and Stent, 2005; Viethen and Dale, 2006;
Gatt et al, 2007), these have focused on ?classic? al-
gorithms, and have been corpus-based. The absence
of task-performance evaluations is surprising, con-
sidering the well-defined nature of the ASGRE task,
and the predominance of task-performance studies
elsewhere in the NLG evaluation literature (Reiter et
al., 2003; Karasimos and Isard, 2004).
Given the widespread agreement on task defini-
tion and input/output specifications, ASGRE was an
ideal candidate for the first NLG shared task evalu-
ation challenge. The challenge was first discussed
1While terms like ?Unique Reference? and ?Brevity? are
characteristics of outputs of ASGRE algorithms, we use them as
shorthand here for those properties of algorithms which guar-
antee that they will achieve these characteristics in the attribute
sets generated.
during a workshop held at Arlington, Va. (Dale and
White, 2007), and eventually organised as part of
the UCNLG+MT Workshop in September 2007 (Belz
and Gatt, 2007).
The ASGRE Shared Task provided an opportunity
to (a) assess the extent to which the field has diver-
sified since its inception; (b) carry out a compar-
ative evaluation involving both automatic methods
and human task-performance methods.
1.3 Overview
In the ASGRE Challenge report (Belz and Gatt,
2007) we presented the results of the ASGRE Chal-
lenge evaluations objectively and with little inter-
pretation. In this paper, we present the results of a
new task-performance evaluation and a new intrinsic
measure involving the same 15 systems and compare
the new results with the earlier ones. We also exam-
ine all results in the light of the algorithmic proper-
ties of the participating systems. We thus focus on
two issues in ASGRE and its evaluation. We exam-
ine the similarities and differences among the sys-
tems submitted to the ASGRE Challenge and com-
pare them to classic approaches (Section 2.1). We
look at the results of intrinsic and extrinsic evalu-
ations (Section 4) and examine how these relate to
algorithmic properties (Section 4.1 and 4.2). Finally
we look at how intrinsic and extrinsic evaluations
correlate with each other (Section 4.3).
2 The ASGRE Challenge
The ASGRE Challenge used the TUNA Corpus (Gatt
et al, 2007), a set of human-produced referring ex-
pressions (REs) for entities in visual domains of
pictures of furniture or people. The corpus was
collected during an online elicitation experiment in
which subjects typed descriptions of a target referent
in a DOMAIN in which there were also 6 other enti-
ties (?distractors?). Each RE in the corpus is paired
with a domain representation consisting of the tar-
get referent and the set of distractors, each with their
possible attributes and values; Figure 1(a) shows an
example of an entity representation.
In each human-produced RE, substrings are an-
notated with the attribute(s) they express (?realise?).
For the Challenge, the attributes in each RE were
extracted to produce a DESCRIPTION, i.e. a set of
51
<ENTITY ID="121" TYPE="target">
<ATTRIBUTE NAME="colour" VALUE="blue" />
<ATTRIBUTE NAME="orientation" VALUE="left" />
<ATTRIBUTE NAME="type" VALUE="fan" />
<ATTRIBUTE NAME="size" VALUE="small" />
<ATTRIBUTE NAME="x-dimension" VALUE="1" />
<ATTRIBUTE NAME="y-dimension" VALUE="3" />
</ENTITY>
(a) Entity representation
<DESCRIPTION>
<ATTRIBUTE NAME="colour" VALUE="blue" />
<ATTRIBUTE NAME="orientation" VALUE="left" />
<ATTRIBUTE NAME="type" VALUE="fan" />
</DESCRIPTION>
(b) Description: the blue fan facing left
Figure 1: Example of the input and output data in the ASGRE Challenge
attribute-value pairs to describe the target referent.
An example, corresponding to a human-produced
RE for Figure 1(a), is shown in Figure 1(b).
For the ASGRE Challenge, the 780 singular de-
scriptions in the corpus were used, and divided
into 60% training data, 20% development data and
20% test data. Participants were given both input
(DOMAIN) and output (DESCRIPTION) parts in the
training and development data, but just inputs in the
test data. They were asked to submit the correspond-
ing outputs for test data inputs.
2.1 Systems
The evaluations reported below included 15 of
the 22 submitted ASGRE systems. Table 1 is an
overview of these systems in terms of five of the
classic algorithmic properties described in Section 1,
and one property that has emerged in more recent
ASGRE algorithms: Trainability, or automatic adapt-
ability of systems from data. This classification of
systems is based on the reports submitted by their
developers to the ASGRE Challenge. Properties are
indicated in the first column (abbreviations are ex-
plained in the table caption).
The version of the IS-FBS system that was origi-
nally submitted to ASGRE contained a bug and did
not actually output minimal attribute sets (but added
an arbitrary attribute to each set). Unlike the ASGRE
Challenge task-performance evaluation, the analysis
presented in this paper uses the corrected version of
this system.
3 Evaluation methods
Evaluation methods can be characterised as either
intrinsic or extrinsic. While intrinsic methods eval-
uate the outputs of algorithms in their own right, ei-
ther relative to a corpus or based on absolute evalu-
ation metrics, extrinsic methods assess the effect of
an algorithm on something external to it, such as its
effect on human performance on some external task.
3.1 Intrinsic measures
In the evaluation results reported below, we use two
of the intrinsic methods used in the ASGRE Chal-
lenge. The first of these is Minimality, defined as
the proportion of descriptions produced by a system
that are maximally brief, as per the original defini-
tion in Dale (1989). The second is the Dice coeffi-
cient, used to compare the description produced by
a system (DS) to the human-produced description
(DH ) on the same input domain. Dice is estimated
as follows:
2? |DS ?DH |
|DS |+ |DH |
(1)
For this paper, we also computed MASI, a ver-
sion of the Jaccard similarity coefficient proposed
by Passonneau (2006) which multiplies the simi-
larity value by a monotonicity coefficient, biasing
the measure towards those cases where DS and
DH have an empty set difference. Intuitively, this
means that those system-produced descriptions are
preferred which do not include attributes that are
omitted by a human. Thus, two of our intrinsic mea-
sures assess Humanlikeness (Dice and MASI), while
Minimality reflects the extent to which an algorithm
conforms to brevity, one of the principles that has
emerged from the ASGRE literature.
3.2 Extrinsic measures
In the ASGRE Challenge, the extrinsic evaluation
was performed via an experiment in which partici-
52
CAM GRAPH IS TITCH
T TU B BU DIT-DS FP SC FBS FBN IAC NIL RS RS+ AS AS+
Incr Y Y Y Y Y n n n n Y Y Y Y Y Y
DP Y Y Y Y n n n n n n n Y Y Y Y
Train Y Y n n Y Y Y n Y Y Y/n Y Y Y Y
Type Y Y Y Y Y Y Y n n n n Y Y Y Y
Hum Y Y Y Y n Y n n n n n n n n n
FB n n n n n n n Y n n n n n n n
Table 1: Overview of properties of systems submitted to the ASGRE Challenge. All systems produce outputs with
unique reference. The meaning of the abbreviations is as follows (for definitions see Section 1): Incr = Incrementality;
DP = Discriminatory Power; Train = Trainability; Type = Hardwired Type Selection; Hum = Human Preference
Modelling; FB = Full Brevity
.
pants were shown visual representations of the do-
mains that were used as inputs in the test data,
coupled with a system-generated description on the
same screen. Their task was to identify the intended
referent in the domain by means of a mouse click.
The rationale behind the experiment was to assess
the degree to which an algorithm achieved its stated
purpose of generating a description that facilitates
identification of the intended referent.
Since system outputs were sets of attributes (see
Figure 1(b)), they were first mapped to NL strings
using a deterministic, template-based method which
always maps each attribute to word(s) in the same
way and in the same position regardless of context2.
The present analysis is based on results from
a new evaluation experiment which replicated the
original methodology with a slight difference.
While participants in the ASGRE experiment were
shown visual domains and descriptions at the same
time, this experiment sought to distinguish reading
and identification time. Thus, for each system out-
put, participants first saw the description, using the
mouse to call up the visual domain once they had
read it. This yields three dependent measures: (a)
reading time (RT); (b) identification time (IT); (c) er-
ror rate (ER), defined as the proportion of trials per
system for which participants identified the wrong
referent.
One of the possible complications with the RT
measure is that, while it depends on the semantics
of a description (the attributes that an algorithm se-
lects), the syntactic complexity of the description
will also impact the time it takes to read it. In our
2The template-based string-mapping algorithm was created
by Irene Langkilde-Geary at the University of Brighton.
setup, the adoption of a deterministic, one-to-one
mapping between an attribute and its linguistic rep-
resentation controls for this to some extent. Since
every attribute in any semantic representation will
invariably map to the same surface form, there is no
variation between systems in terms of how a partic-
ular attribute is realised.
Design: As in the original experiment, we used a
Repeated Latin Squares design in which each com-
bination of peer system and test set item is allocated
one trial. Because there were 148 items in the test
set, but 15 peer systems, 2 test set items were ran-
domly selected and duplicated to give a test set size
of 150, and 10 Latin Squares. The 150 test set items
were divided into two sets of 75; 15 of the 30 partici-
pants did the first 75 items (the first 5 Latin Squares),
while the other 15 did the rest. This resulted in 2250
trials (each corresponding to a combination of test
set item and system) in all. For the purposes of this
paper, the duplicate items were treated as fillers, that
is, every item was included only once in the analysis.
Participants and procedure: The experiment
was carried out by 30 participants recruited from
among the faculty and administrative staff of the
University of Brighton. Participants carried out the
experiment under supervision in a quiet room on a
laptop. Stimulus presentation was carried out us-
ing DMDX, a Win-32 software package for psy-
cholinguistic experiments involving time measure-
ments (Forster and Forster, 2003). Participants initi-
ated each trial, which consisted of an initial warning
bell and a fixation point flashed on the screen for
1000ms. They then read the description and called
up the visual domain to identify the referent. Identi-
53
fication was carried out by clicking on the image that
a participant thought was the target referent of the
description they had read. RT was measured from
the point at which the description was presented, to
the point at which a participant called up the next
screen via mouse click. IT was measured from the
point at which pictures (the visual domain) were pre-
sented on the screen to the point where a participant
identified a referent by clicking on it. In addition
to the time taken to identify a referent, the program
recorded the location of a participant?s mouse click,
in order to assess whether the object identified was
in fact the correct one. Trials timed out after 15 sec-
onds; this only occurred in 2 trials overall (out of
2250).
4 Results and discussion
Table 2 displays aggregate (mean or percentage)
scores for each of the intrinsic and extrinsic mea-
sures. As an initial test for differences between
the 15 systems separate univariate ANOVAs were
conducted using SYSTEM as the independent vari-
able (15 levels), testing its effect on the extrinsic
task-performance measures (RT and IT). For er-
ror rate (ER), we used a Kruskall-Wallis ranks test
to compare identification accuracy rates across sys-
tems3. The main effect of SYSTEM was significant
on RT (F (14, 2219) = 2.58, p < .01), and IT
(F (14, 2219) = 1.90, p < .05). No significant main
effect was found on ER (?2 = 13.88, p > .4).4
Systems also differed significantly on Dice
(F (14, 2219) = 18.66, p < .001) and MASI scores
(F (14, 2219) = 15.94, p < .001).
In the remainder of this section, we first consider
the differences between systems based on the algo-
rithmic properties listed in Table 1 (and described
in Section 1). For this part of the analysis, we con-
sider intrinsic and extrinsic evaluation measures sep-
arately (Section 4.1 and 4.2). Subsequently, we ex-
plicitly compare the intrinsic and extrinsic methods
(Section 4.3).
3The large number of zero values in ER proportions, and a
high dependency of variance on the mean, meant that a non-
parametric test was more appropriate.
4We left out the duplicated items in this analysis.
4.1 Differences between system types
As Table 1 indicates, there are some similarities
between the new systems submitted for the AS-
GRE tasks, and the classic approaches discussed in
Section 1. Eleven of the systems are incremen-
tal, although only a minority (5 systems) explic-
itly hardwire human preferences. Furthermore, 8
systems (all variations on two basic systems, CAM
and TITCH) compute the discriminatory value of the
properties selected. Just one system adopts the Full
Brevity approach of Dale (1989), while the major-
ity (11 systems) adopt the Dale and Reiter conven-
tion of always adding TYPE. Perhaps the greatest
point of divergence between the ASGRE Challenge
systems and the classic algorithms discussed in Sec-
tion 1 is the predominance of data-driven approaches
(12 systems are trainable), a characteristic that mir-
rors trends elsewhere in HLT.
It is worth asking how the particular algorithmic
properties impact on performance, as measured with
the evaluation methods used. Table 3 displays means
computed with all the extrinsic and intrinsic mea-
sures, for all system properties except full brevity
and discriminatory power. Since the latter are re-
lated to Minimality, which was a separate evaluation
measure in this study, they are treated separately be-
low (Section 4.2). Note that the aggregate scores in
Table 3 are based on unequal-sized samples, since
this is a post-hoc analysis which the evaluation stud-
ies described above did not directly target.
Table 3 indicates that systems that were directly
trained on data display higher agreement (Dice and
MASI) scores with the human data. This is rela-
tively unsurprising; what is less straightforwardly
predictable is that trainable systems have better task-
performance scores (IT and RT). Systems which al-
ways include TYPE and those which are incremen-
tal apparently score better overall. Somewhat sur-
prisingly, incorporating human attribute preferences
results in smaller improvements than incremental-
ity and TYPE inclusion. This is likely due to the
fact that the attributes preferred by humans are not
straightforwardly determinable in the people subdo-
main (van der Sluis et al, 2007). In contrast, those
in the furniture domain, which include COLOUR,
SIZE and ORIENTATION, are fairly predictable as far
as human preference goes, based on previous psy-
54
Minimality Dice MASI RT IT ER
IS-FBN 1.35 0.770 0.601 1837.546 2188.923 6
DIT-DS 0 0.750 0.595 1304.119 1859.246 2
IS-IAC 0 0.746 0.597 1356.146 1973.193 6
CAM-T 0 0.725 0.560 1475.313 1978.237 5.33
CAM-TU 0 0.721 0.557 1297.372 1809.044 4
GRAPH-FP 4.73 0.689 0.479 1382.039 2053.326 3.33
GRAPH-SC 4.73 0.671 0.466 1349.047 1899.585 2
TITCH-RS+ 0 0.669 0.459 1278.008 1814.933 1.33
TITCH-AS+ 0 0.660 0.452 1321.204 1817.303 4.67
TITCH-RS 0 0.655 0.432 1255.278 1866.935 4.67
TITCH-AS 0 0.645 0.422 1229.417 1766.350 4.67
CAM-BU 10.14 0.630 0.420 1251.316 1877.948 4
NIL 20.27 0.625 0.477 1482.665 1960.314 5.33
CAM-B 8.11 0.620 0.403 1309.070 1952.394 5.33
IS-FBS 100 0.368 0.182 1461.453 2181.883 7.33
Table 2: Results for systems and evaluation measures (in order of Dice).
trainable includes type incremental human preferences
no yes no yes no yes no yes
Dice 0.561 0.700 0.627 0.676 0.625 0.677 0.656 0.677
MASI 0.370 0.511 0.464 0.477 0.432 0.488 0.468 0.484
RT 1376.13 1371.41 1534.45 1313.83 1507.52 1323.63 1387.49 1343.02
IT 1993.13 1911.55 2076.08 1881.39 2080.93 1879.63 1932.87 1934.19
ER 5.2% 4.2% 6.3% 3.8% 4.7% 4.4% 4.5% 4.5%
Table 3: Means on intrinsic and extrinsic measures, by system type.
Dice MASI IT RT
Trainable 171.67? 145.63? .002 4.36
Incremental 47.90? 45.61? 7.43 1.64
Includes type 18.42? 29.38? 6.89 11.27
Human pref. 3.27 5.12 2.11 .83
Incr. ? Train. 40.82? 30.99? .001 7.08
Table 4: Multivariate tests examining impact of system
type on evaluation measures. Cells indicate F?values
with 4 numerator and 2201 error degrees of freedom. ? :
p ? .05 after Bonferroni correction.
cholinguistic results (Pechmann, 1989; Arts, 2004).
As a further test of these differences, a multivari-
ate ANOVA was conducted, comparing scores on the
intrinsic and extrinsic measures, as a function of the
presence or absence of the 4 algorithmic properties
being considered here. The results are shown in
Table 4 which displays figures for all main effects
and for the one interaction that was significant (in-
crementality ? trainability, bottom row). Since this
is a multiple-test post-hoc analysis on unequal-sized
samples, all significance values are based on a Bon-
ferroni correction5 The table does not include ER, as
5The Bonferroni correction adjusts for the increased like-
no main effect of SYSTEM was found on this mea-
sure.
The results show that trainability, followed by in-
crementality and inclusion of TYPE had the strongest
impact on system quality, as far as this is measured
by Dice and MASI.
The incrementality ? trainability interaction is
mostly due to the huge effect that trainability has
on how non-incremental systems perform. In the
case of incremental systems, trainability gives rise to
marginally better performance: the mean Dice score
for non-trainable incremental systems was .625, as
compared to .696 for trainable incremental systems.
Similar patterns are observable with MASI (.433
vs .439). However, the most significant feature
of the interaction is the large benefit that training
on data confers on non-incremental systems. Un-
trained non-incremental systems obtained a mean
Dice score of .368 (MASI = .182), while the mean
Dice score on trained non-incremental systems was
.710, with a mean MASI score of .515. Another,
lihood of finding results statistically significant when multiple
tests are applied by lowering the alpha value (the significance
threshold) by dividing it by the number of tests involved.
55
Intrinsic measures Extrinsic measures
Dice MASI ER RT IT
-.904?? -.789?? .505 .183 .560?
Table 5: Correlations between Minimality and other eval-
uation measures. Legend: ??: p ? .01, ?: p ? .05
more tentative, conclusion that could be made by
looking at these figures is that trainable systems per-
form better if they are non-incremental; however,
the difference (Dice of .710 vs .696) seems rather
marginal and would require further testing. What
is also striking in this table is the absence of any
significant impact of these properties on the task-
performance measures of reading and identification
time, despite initial impressions based on means (Ta-
ble 3).
In summary, at least one of the principles that has
been widely adopted in ASGRE ? incrementality ?
seems to be validated by the evaluation results for
these new algorithms. However, results show that
improvement on an automatically computed human-
likeness metric such as MASI or Dice does not neces-
sarily imply an improvement on a task-performance
measure. This is a theme to which we return in Sec-
tion 4.3.
4.2 The role of minimality
Apart from incrementality, the other dominant prin-
ciple that has emerged from nearly three decades of
ASGRE research is brevity. While psycholinguistic
research has shown (Pechmann, 1989; Belke and
Meyer, 2002; Arts, 2004) that the strict interpre-
tation of the Gricean Quantity Maxim, adopted for
example by Dale (1989), is not observed by speak-
ers, brevity has remained a central concern in recent
approaches (Gardent, 2002). Only one algorithm
adopted a full-brevity approach in the ASGRE Chal-
lenge; however, several algorithms emphasised dis-
criminatory power, and some of these, as a compar-
ison between Tables 1 and 2 shows, tended to have
higher Minimality scores overall.
Since Minimality was part of the overall design
of the evaluation, it is possible to see the extent
to which it covaries significantly with other perfor-
mance measures. The relevant correlations are dis-
played in Table 5. There are highly significant neg-
ative correlations between Minimality and the two
intrinsic humanlikeness measures (Dice and MASI).
The significant positive correlation with IT implies
that participants in our evaluation experiment tended
to take longer to identify referents in the case of sys-
tems which produce more minimal descriptions.
The negative correlation with the humanlikeness
measures corroborates previous findings that peo-
ple do not observe a strict interpretation of the
Gricean Quantity Maxim (Pechmann, 1989; Belke
and Meyer, 2002; Arts, 2004). On the other hand,
the direction of the correlation with IT is somewhat
more surprising. One possible explanation is that
minimal descriptions often do not include a TYPE
attribute, since this seldom has any discriminatory
power in the TUNA Corpus domains.
The role of Minimality remains a topic of some
debate in the psycholinguistic literature. A recent
study (Engelhardt et al, 2006) showed that identifi-
cation of objects in a visual domain could be delayed
if a description of the target referent was overspeci-
fied, suggesting a beneficial effect from Minimality.
However, Engelhardt et al also found, in a separate
study, that overspecified descriptions tended to be
rated as no worse than brief, or minimal ones. In-
deed, one of the points that emerges from this study
is that intrinsic and extrinsic assessments can yield
contrasting results. It is to this issue that we now
turn.
4.3 Intrinsic vs. extrinsic methods
Since humanlikeness (intrinsic) and task perfor-
mance (extrinsic) are different perspectives on the
outputs of an ASGRE algorithm, it is worth ask-
ing to what extent they agree. This is an impor-
tant question for comparative HLT evaluation more
generally, which has become dominated by corpus-
based humanlikeness metrics. To obtain an indi-
cation of agreement, we computed correlations be-
tween the two humanlikeness measures and the task-
performance measures; these are displayed in Table
6.
The two time measures (RT and IT) are strongly
correlated, implying that when subjects took longer
to read descriptions, they also took longer to iden-
tify a referent. This may seem surprising in view
of the positive correlation between Minimality and
IT (shorter descriptions imply longer IT) which was
reported in Section 4.2, where Minimality was also
56
RT IT ER Dice MASI
RT 1 .8** 0.46 0.12 .23
IT .8** 1 .59* -0.28 -.17
ER 0.46 .59* 1 -0.39 -.29
Dice 0.12 -0.28 -0.39 1 .97**
MASI 0.23 -0.17 -0.29 .97** 1
Table 6: Pairwise correlations between humanlikeness
and task-performance measures (?: p ? .05; ??: p ? .01)
shown not to correlate with RT (shorter descriptions
do not imply longer or shorter RT). What this result
indicates is that rather than the number of attributes
in a description, as measured by Minimality, it is the
content that influences identification latency. Part of
the effect may be due to the lack of TYPE in minimal
descriptions noted earlier.
Table 6 also shows that error rate is significantly
correlated with IT (but not with RT), i.e. where sub-
jects took longer to identify referents, the identifi-
cation was more likely to be wrong. This points to
a common underlying cause for slower reading and
identification, possibly arising from the use of at-
tributes (such as SIZE) which impose a greater cog-
nitive load on the reader.
The very strong correlation (0.97) between Dice
and MASI is to be expected, given the similarity in
the way they are defined.
Another unambiguous result emerges: none of the
similarity-based metrics covary significantly with
any of the task-performance measures. An extended
analysis involving a larger range of intrinsic met-
rics confirmed this lack of significant covariation
for string-based similarity metrics as well as set-
similarity metrics across two task-performance ex-
periments (Belz and Gatt, 2008). This indicates that
at least for some areas of HLT, task-performance
evaluation is vital: without the external reality check
provided by extrinsic evaluations, intrinsic evalua-
tions may end up being too self-contained and dis-
connected from notions of usefulness to provide a
meaningful assessment of systems? quality.
5 Conclusion
Comparative evaluation can be of great benefit, es-
pecially in an area as mature and diverse as GRE.
A shared-task evaluation like the ASGRE Challenge
can help identify the strengths and weaknesses of al-
ternative approaches and techniques, as measured by
different evaluation criteria. Perhaps even more im-
portantly, such a challenge helps evaluate the evalu-
ation methods themselves and reveals which (com-
binations of) evaluation methods are appropriate for
a given evaluation purpose.
As far as the first issue is concerned, this paper
has shown that trainability, incrementality and not
aiming for minimality are the algorithmic proper-
ties most helpful in achieving high humanlikeness
scores. This result is in line with psycholinguistic
research on attribute selection in reference by hu-
mans. Less clear-cut is the relationship between
these properties and task-performance measures that
assess how efficiently a referent can be identified
based on a description.
The second part of the analysis presented here
showed that intrinsic and extrinsic perspectives on
the quality of system outputs quality can yield un-
correlated sets of results, making it difficult to pre-
dict quality as measured by one based on quality
as measured by the other. Furthermore, while in-
tuitively it might be expected that higher human-
likeness entailed better task performance, our results
suggest that this is not necessarily the case.
Our main conclusions for ASGRE evaluation are
that (a) while humanlikeness evaluation may provide
a measure of one aspect of systems but we need to be
cautious in relying on humanlikeness as a criterion
of overall quality (standard in evaluation of MT and
summarisation); and (b) that we must not leave the
NLG tradition of task-performance evaluations be-
hind as we move towards more comparative forms
of evaluation.
Acknowledgements
We gratefully acknowledge the contribution made to
the evaluations by the faculty and staff at Brighton
University who participated in the identification ex-
periments. The biggest contribution was, of course,
made by the participants in the ASGRE Challenge
who created the systems involved in the evalua-
tions: Bernd Bohnet, Ann Copestake, Pablo Gerva?s,
Raquel Herva?s, John Kelleher, Emiel Krahmer,
Takahiro Kurosawa, Advaith Siddharthan, Philipp
Spanger, Takenobu Tokunaga, Mariet Theune, Pas-
cal Touset and Jette Viethen.
57
References
D. Appelt and A. Kronfeld. 1987. A computa-
tional model of referring. In Proc. 10th International
Joint Conference on Artificial Intelligence (IJCAI-87),
pages 640?647.
D. Appelt. 1985. Planning English referring expressions.
Artificial Intelligence, 26(1):1?33.
A. Arts. 2004. Overspecification in Instructive Texts.
Ph.D. thesis, University of Tilburg.
E. Belke and A. Meyer. 2002. Tracking the time course
of multidimensional stimulus discrimination: Analy-
sis of viewing patterns and processing times during
same-different decisions. European Journal of Cog-
nitive Psychology, 14(2):237?266.
A. Belz and A. Gatt. 2007. The Attribute Selection for
GRE Challenge: Overview and evaluation results. In
Proc. 2nd UCNLG Workshop: Language Generation
and Machine Translation (UCNLG+MT), pages 75?
83.
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic evalu-
ation measures for referring expression generation. In
Proc. 46th Annual Meeting of the Association for Com-
putational Linguistics (ACL-08). To appear.
R. Dale and N. Haddock. 1991. Generating refer-
ring expressions containing relations. In Proc. 5th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-91)., pages
161?166.
R. Dale and E. Reiter. 1995. Computational interpreta-
tion of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(8):233?263.
R. Dale and M. White, editors. 2007. Shared tasks and
comparative evaluation in Natural Language Genera-
tion: Workshop Report.
Robert Dale. 1989. Cooking up referring expressions.
In Proc. 27th Annual Meeting of the Association for
Computational Linguistics (ACL-89)., pages 68?75.
P. E. Engelhardt, K.G.D Bailey, and F. Ferreira. 2006.
Do speakers and listeners observe the Gricean Maxim
of Quantity? Journal of Memory and Language,
54:554?573.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy. Be-
havior Research Methods, Instruments, & Computers,
35(1):116?124.
C. Gardent. 2002. Generating minimal definite descrip-
tions. In Proc. 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-02)., pages
96?103.
A. Gatt and K. van Deemter. 2007. Incremental genera-
tion of plural descriptions: Similarity and partitioning.
In Proc. 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP/CONLL-
07), pages 102?111.
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proc. 11th
European Workshop on Natural Language Generation
(ENLG-07)., pages 49?56.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. L. Morgan, editors, Syntax and Semantics:
Speech Acts., volume III. Academic Press.
S. Gupta and A. J. Stent. 2005. Automatic evalua-
tion of referring expression generation using corpora.
In Proc. 1st Workshop on Using Corpora in NLG
(UCNLG-05)., pages 1?6.
H. Horacek. 2004. On referring to sets of objects natu-
rally. In Proc. 3rd International Conference on Natu-
ral Language Generation (INLG-04)., pages 70?79.
A. Karasimos and A. Isard. 2004. Multilingual evalua-
tion of a natural language generation system. In Proc.
4th International Conference on Language Resources
and Evaluation (LREC-04).
J. D. Kelleher and G-J Kruijff. 2006. Incremental gen-
eration of spatial referring expressions in situated di-
alog. In Proc. joint 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(ACL/COLING-06)., pages 1041?1048.
R. Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic anno-
tation. In Proc. 5th International Conference on Lan-
guage Resources and Evaluation (LREC-06)., pages
831?836.
Thomas Pechmann. 1989. Incremental speech pro-
duction and referential overspecification. Linguistics,
27:89?110.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessation
letters. Artificial Intelligence, 144:41?58.
E. Reiter. 1990. The computational complexity of avoid-
ing conversational implicatures. In Proc. 28th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-90)., pages 97?104.
K. van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
K. van Deemter. 2006. Generating referring expressions
that involve gradable properties. Computational Lin-
guistics, 32(2):195?222.
I. van der Sluis, A. Gatt, and K. van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions: Going beyond toy domains. In Proc. In-
ternational Conference on Recent Advances in Natural
Language Processing (RANLP-07).
J. Viethen and R. Dale. 2006. Algorithms for generat-
ing referring expressions: Do they do what people do?
In Proc. 4th International Conference on Natural Lan-
guage Generation (INLG-06)., pages 63?72.
58
The Importance of Narrative and Other Lessons from an Evaluation of an
NLG System that Summarises Clinical Data
Ehud Reiter, Albert Gatt, Franc?ois Portet
Dept of Computing Science
University of Aberdeen, UK
{e.reiter,a.gatt,fportet}@
abdn.ac.uk
Marian van der Meulen?
Dept of Psychology
University of Edinburgh, UK
m.a.van-der-meulen@
sms.ed.ac.uk
Abstract
The BABYTALK BT-45 system generates tex-
tual summaries of clinical data about babies in
a neonatal intensive care unit. A recent task-
based evaluation of the system suggested that
these summaries are useful, but not as effec-
tive as they could be. In this paper we present
a qualitative analysis of problems that the
evaluation highlighted in BT-45 texts. Many
of these problems are due to the fact that BT-
45 does not generate good narrative texts; this
is a topic which has not previously received
much attention from the NLG research com-
munity, but seems to be quite important for
creating good data-to-text systems.
1 Introduction
Data-to-text NLG systems produce textual output
based on the analysis and interpretation of non-
linguistic data (Reiter, 2007). Systems which pro-
duce short summaries of small amounts of data, such
as weather-forecast generators (Reiter et al, 2005),
have been one of the most successful applications of
NLG, and there is growing interest in creating sys-
tems which produce longer summaries of larger data
sets.
We have recently carried out an evaluation of one
such system, BT-45 (Portet et al, 2007), which gen-
erates multi-paragraph summaries of clinical data
from a Neonatal Intensive Care Unit (NICU). The
summaries cover a period of roughly 45 minutes,
and describe both sensor data (heart rate, blood oxy-
gen saturation, etc, sampled at 1 sec intervals) as
well as discrete events such as drug administration;
?Now at the Department of Clinical Neurosciences, Univer-
sity Hospital, Geneva, Switzerland
they are intended to help medical staff make treat-
ment decisions. This evaluation showed that from a
decision-support perspective, the BT-45 texts were
as effective as visualisations of the data, but less ef-
fective than human-written textual summaries.
In addition to quantitative performance data,
which is presented elsewhere (van der Meulen et
al., submitted), the evaluation also gave us valuable
clues about what aspects of data-to-text technology
need to be improved in order to make texts gener-
ated by such systems more effective as decision sup-
port aids; this is the subject of this paper. Somewhat
to our surprise, many of the problems identified in
the evaluation relate to the fact that BT-45 could not
produce a good narrative describing the data. Gen-
eration of non-fictional narratives is not something
which has been the focus of much NLG research in
the past, but our results suggest it is important, at
least in the context of producing texts which are ef-
fective decision-support aids.
1.1 Background: Data-to-Text
Data-to-text systems are motivated by the belief that
(brief) linguistic summaries of datasets may in some
cases be more effective than more traditional pre-
sentations of numeric data, such as tables, statistical
analyses, and graphical visualisations (even simple
visual/graphical displays require relatively complex
cognitive processing (Carpenter and Shah, 1998)).
Also linguistic summaries can be delivered in some
contexts where visualisations are not possible, such
as text messages on a mobile phone, or when the
user is visually impaired (Ferres et al, 2006). In the
NICU domain, Law et al (2005) conducted an ex-
periment which showed that medical professionals
were more likely to make the correct treatment deci-
147
sion when shown a human-written textual summary
of the data than when they were shown a graphical
visualisation of the data.
A number of data-to-text systems have been de-
veloped and indeed fielded, especially in the domain
of weather forecasts (Goldberg et al, 1994; Reiter
et al, 2005). Most of these systems have gener-
ated short (paragraph-length or smaller) summaries
of relatively small data sets (less than 1KB). Some
research has been on systems that summarise larger
data sets (Yu et al, 2007; Turner et al, 2008), but
these systems have also generated paragraph-length
summaries; we are not aware of any previous re-
search on generating multi-paragraph summaries in
a data-to-text system.
Data-to-texts systems have been evaluated in a
number of ways, including human ratings (the most
common technique) (Reiter et al, 2005), BLEU-like
scores against human texts (Belz and Reiter, 2006),
post-edit analyses (Sripada et al, 2005), and per-
suasive effectiveness (Carenini and Moore, 2006).
However, again to the best of our knowledge no pre-
vious data-to-text system has been evaluated by ask-
ing users to make decisions based on the generated
texts, and measuring the quality of these decisions.
2 BabyTalk and BT-45
Law et al (2005) showed that human-written textual
summaries were effective decision-support aids in
NICU, but of course it is not practical to expect medi-
cal professionals to routinely write such summaries,
especially considering that the summaries used by
Law et al in some cases took several hours to write.
The goal of the BABYTALK research project is to
use NLG and data-to-text technology to automati-
cally generate textual summaries of NICU data, for a
variety of audiences and purposes. The first system
developed in BABYTALK, and the subject of this pa-
per, is BT-45 (Portet et al, 2007), which generates
summaries of 30-60 minute chunks of clinical data,
for the purpose of helping nurses and doctors make
appropriate treatment decisions.
An example of BABYTALK input data is shown in
Figures 1 (sensor data) and 2 (selected event data).
Figure 3 shows the human-written corpus text for
this scenario, and Figure 4 shows the BT-45 text gen-
erated for this scenario. Note that for the purposes of
Figure 1: Example Babytalk Input Data: Sensors
HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 =
blood CO2 level; SaO2 = oxygen saturation; T1 = chest
temperature; T2 = toe temperature; mean BP = blood
pressure. The bars and triangles at the bottom show the
time of discrete events (Figure 2).
event time
Blood transfusion 13.35
Intermittent crying 13.38
FiO2 (oxygen level) changed to 50% 13.51
Incubator temperature changed to 36.4 13.52
Attempt to insert line 13.53
Line removed 13.57
Attempt to insert line 13.58
Line removed 14.00
FiO2 (oxygen level) changed to 45% 14.03
Attempt to insert line 14.08
Line removed 14.09
Figure 2: Example Babytalk Input Data: Selected Dis-
crete Events
148
You saw the baby between 13:30 and 14:10.
Over the course of the monitoring period the HR in-
creases steadily from 140 to 165; the BP varies between
41 and 49.
At the start of the period T1 is 37 and T2 is 35.8C.
During the first 15 minutes the pO2 is 7.8-9.2 and the
pCO2 is 5.9-7.3.
At 13:35 a blood transfusion is commenced.
At 13:38 the baby is crying and there are a few up-
ward spikes in the pO2 trace corresponding to downward
spikes in the pCO2 trace. At 13.45 the humidity on the
incubator walls is wiped away and T1 and T2 fall to 36.3
and 35.4 respectively
At 13:50 the baby is examined. There is a desaturation to
72% and the FiO2 is changed to 50%. Between now and
14.10 there are several attempts to site a peripheral can-
nula. The pO2 and pCO2 both have spikes in the traces,
pO2 reaching 19.2 and pCO2 reaching 0.4. There are sev-
eral episodes of artefact in the oxygen saturation trace.
T1 and T2 fall to 36.2 and 35.7 and the oxygen saturation
falls to 65%. The FiO2 is adjusted to 50%. Also at this
time the incubator temperature is adjusted to 36.4C.
At 14:03 with the pO2 at 13.9 and oxygen saturation at
100%, the FiO2 is reduced to 45
At 14:04 T1 rises sharply to 40, then drops fairly steeply
to 28.5C. Between 14.06 and 14.10 there are several
spikes in the pO2 and pCO2 traces but by 14.10 the pO2
is 8, pCO2 is 3.7, the oxygen saturation is 71%, the HR
is 163, the BP 39, T1 29C and T2 35.4C.
Figure 3: Corpus Text for Fig 1, 2 data
this paper, we have deliberately selected a relatively
poor quality BT-45 output text.
The processing performed by BT-45 is described
by Portet et al (2007). Very briefly, BT-45 generates
texts in four stages:
? signal analysis: for example, detecting spikes
in the sensor channels; this is done using adap-
tations of standard pattern detection and recog-
nition algorithms.
? data abstraction: for example, identifying
three line-insertion-attempt and line-removal
events within a short span of time and
grouping these into the higher-level con-
cept LINE-INSERTION-PROCESS-FAILED (BT-
You saw the baby between 13:30 and 14:10. Heart Rate
(HR) = 149. Core Temperature (T1) = 37.0. Periph-
eral Temperature (T2) = 35.8. Transcutaneous Oxygen
(TcPO2 ) = 9.5. Transcutaneous CO2 (TcPCO2) = 6.7.
Mean Blood Pressure (mean BP) = 41. Oxygen Satura-
tion (SaO2) = 94.
Over the next 39 minutes SaO2 decreased to 81, T2 de-
creased to 34.5, HR stayed at around 151, T1 decreased
to 36.2 and mean BP stayed at around 40.
A blood transfusion was given to the baby at 13:35.
At 13:50 there was a desaturation down to 65. As a result,
Fraction of Inspired Oxygen (FIO2) was set to 50
There were 3 failed attempts to insert a peripheral venous
line at 13:53. TcPO2 suddenly decreased to 8.1. SaO2
suddenly increased to 92. TcPO2 suddenly decreased to
9.3. There was a spike in TcPO2 up to 14.8. There had
been another spike in T1 up to 40.5. FIO2 had been low-
ered to 45%. Previously the baby had been examined.
Figure 4: BT-45 Text for Fig 1, 2 data
45 includes a domain ontology of such con-
cepts); this is done using knowledge-based
techniques.
? document planning: for example, deciding not
to mention most of the spikes in O2 and CO2;
this is primarily done in a bottom-up fashion,
using information (computed by the data ab-
straction module) on the medical importance of
events, and also on causal and other relation-
ships between events.
? microplanning and realisation: producing the
actual text shown in Figure 4; this is mostly
done using relatively standard NLG techniques,
although we have developed new techniques
for communicating temporal information and
relationships.
3 Evaluation of BT-45
BT-45 was evaluated by asking medical staff to
make decisions about what actions they should take
with regard to a baby, after viewing either a BT-45
text, a human-written textual summary, or a visual-
isation of the baby?s data; this was similar in gen-
eral terms to the experiment described by Law et al
(2005). van der Meulen et al (submitted) gives de-
149
tails about the evaluation design and quantitative re-
sults of the evaluation; in this paper we just briefly
summarise these aspects of the evaluation
Material: Our medical collaborators selected 24
scenarios (data sets), and defined 18 types of ac-
tions. For each of the data sets, they specified which
of the 18 actions were appropriate, inappropriate, or
neutral (neither appropriate nor inappropriate); one
appropriate action was identified as the main target
action. For the data set shown in Figures 1 and 2, for
example:
? Main target action: Adjust monitoring equip-
ment
? Other appropriate actions: calm/comfort baby,
manage temperature, analyse blood sample
? Neutral actions: adjust CPAP (ventilation) set-
tings, baby care (e.g., nappy change)
? Inappropriate actions: all other actions (e.g.
blood transfusion, order X-Ray) (12 in all)
For each scenario, we created three presentations: a
visualisation (similar to Figure 1), a human-written
text summary written by our collaborators, and the
summary produced by BT-45. Our collaborators
were asked not to include any explicit medical in-
terpretation of the data in their human-written sum-
maries. For each scenario, our collaborators also
prepared a text which gave background information
about the baby.
When developing BT-45, we had access to the
data collection that the scenario data sets were taken
from (which includes several months of data), but
did not know ahead of time which specific scenarios
would be used in the experiment.
Subjects: 35 medical professionals, including ju-
nior nurses, senior nurses, junior doctors, and senior
doctors.
Procedure: Each subject was shown 8 scenarios
in each condition (visualisation, human text, BT-45
text) in a Latin Square design; all subjects were also
shown the background texts. Subjects were asked to
specify which actions should be taken for this baby,
selecting actions from a fixed set of check-boxes;
they were given three minutes to make this deci-
sion. Subjects were not explicitly asked for free-text
comments, but any comments spontaneously made
by subjects were recorded. Subject responses were
scored by computing the percentage of appropriate
actions they selected, and subtracting from this the
percentage of inappropriate actions.
Results: The highest score was achieved by the
human texts (mean score of 0.39); there was no sig-
nificant difference between the BT-45 texts (mean
score of 0.34) and the visualisations (mean score of
0.33). The largest differences occurred in the ju-
nior nurses group. van der Meulen et al (submitted)
present a detailed statistical analysis of the results.
Discussion: This shows that BT-45 texts were as
effective as visualisation, but less effective than the
human texts. This suggests that data-to-text technol-
ogy as it stands could be useful as a supplement to
visualisations (since some individuals do better with
texts and some with graphics; also some data sets are
visualised effectively and some are not), and in con-
texts where visualisation is not possible. But it also
suggests that if we can improve the technology so
that computer-generated texts are as effective as hu-
man texts, we should have a very effective decision-
support technology.
4 Quantitative Comparison of BT-45 and
Corpus Texts
In addition to the task-based evaluation described
above, we also quantitatively compared the BT-45
and human texts, and qualitatively analysed prob-
lems in the BT-45 texts. Quantitative comparison
was done by annotating the BT-45 and human texts
to identify which events they mentioned. For each
scenario, we computed the MASI coefficient (Pas-
sonneau, 2006) between the set of events mentioned
in the BT-45 and human texts. The average MASI
score was 0.21 (SD = 0.13), which is low; this
suggests that BT-45 and the human writers choose
different content. We also checked whether similar
human and BT-45 texts (as judged by MASI score)
obtained similar evaluation scores; in fact there was
no significant correlation between MASI similarity
of human and BT-45 texts and the difference be-
tween their evaluation scores.
We performed a second analysis based on com-
paring the structure (e.g., number and size of para-
graphs) of the BT-45 and human texts, using a
tree-edit-distance metric to compare text structures.
150
Again this showed that there were large differences
between the structure of the BT-45 and human texts,
and that these differences did correlate with differ-
ences in evaluation scores.
In other words, simple metrics of content and
structural differences do not seem to be good pre-
dictors of text effectiveness; this is perhaps not sur-
prising given the complexity of the texts and the in-
formation they are communicating.
5 Qualitative Analysis of Problems in
BT-45 texts
The final step in our evaluation was to qualitatively
analyse the BT-45 texts and the results of the task-
based evaluation, in order to highlight problems in
the BT-45 texts. Of course we were aware of numer-
ous ways in which the software could be improved,
but the evaluation gave us information about which
of these mattered most in terms of overall effective-
ness. We report this analysis below, including issues
identified from subjects? comments, issues identi-
fied from scenarios where BT-45 texts did poorly,
and problems identified via manual inspection of the
texts. We do not distinguish between ?linguistic? and
?reasoning? problems, in part because it is usually
difficult (and indeed somewhat artificial) to separate
these aspects of BT-45.
5.1 Problems Identified by Subjects
Subjects made a number of comments during the ex-
periment. Two aspects of BT-45 were repeatedly
criticised in these comments.
5.1.1 Layout and bullet lists
Subjects wanted better layout and formatting, in
the human texts as well as the BT-45 texts (BT-45
texts do not currently include any visual formatting).
In particular, they wanted bullet lists to be used, es-
pecially for lab results. Such issues have been exten-
sively discussed by other researchers (e.g., (Power et
al., 2003)), we will not further discuss them here.
5.1.2 Continuity
BT-45 sometimes described changes in signals (or
other events) which didn?t make sense because they
omitted intermediate events. For example, consider
the last paragraph in the BT-45 text shown in Fig-
ure 4 (with italics added):
There were 3 failed attempts to insert a pe-
ripheral venous line at 13:53. TcPO2 sud-
denly decreased to 8.1. SaO2 suddenly in-
creased to 92. TcPO2 suddenly decreased
to 9.3. There was a spike in TcPO2 up
to 14.8. There had been another spike in
T1 up to 40.5. FIO2 had been lowered to
45%. Previously the baby had been exam-
ined.
Subjects complained that it made no sense for
TcPO2 to decrease to 9.3 when the last value men-
tioned for this parameter was 8.1
In this case (and in many others like it), BT-45
had identified the decrease events as being medically
important, but had not assigned as much importance,
and hence not mentioned, the increase event (TcPO2
went up to 19) between these decrease events. This
is partially because BT-45 believed that a TcPO2 of
19 is a sensor artefact (not a real reading of blood
oxygen), since 19kPa is a very high value for this
channel. In fact this is a correct inference on BT-
45?s part, but the text is still confusing for readers.
We call this problem continuity, making an anal-
ogy to the problems that film-makers have in ensur-
ing that scenes in a film (which maybe shot in very
different times and locations) fit together in the eyes
of the viewer. It is interesting to note that some of
the human texts also seem to have continuity prob-
lems (for example, the text in Figure 3 says T2 falls
to 35.4, and then says T2 falls to 35.7), but none of
the subjects complained about continuity problems
in the human texts. So some kinds of continuity vio-
lations seem more problematical to readers than oth-
ers. Perhaps this depends on the proximity of the
events both in the document structure and in time;
we hope to empirically explore this hypothesis.
Continuity is just one aspect of the broader prob-
lem of deciding which events need to be explicitly
mentioned in the text, and which can be omitted.
Making such decisions is perhaps one of the hard-
est aspects of data-to-text.
5.2 Scenarios Where BT-45 did Badly
When analysing the results of the experiment, we
noticed that BT-45 texts did as well as the human
texts for scenarios based on five of the eight target
actions; however they did significantly worse than
151
main target action human BT-45 diff
Adjust CPAP 0.37 0.37 0
Adjust monitoring equip 0.59 0.22 0.37
Adjust ventilation 0.22 0.23 -0.01
Extubate 0.14 0.12 0.02
Manage temperature 0.55 0.33 0.22
No action 0.61 0.43 0.18
Suction 0.34 0.42 -0.08
Support blood pressure 0.45 0.55 -0.10
Table 1: Average evaluation score by main target action
the human texts on the scenarios based on the other
three actions (Adjust Monitoring Equipment, Man-
age Temperature, and No Action). Details are shown
in Table 1; an ANOVA confirms that there is a sig-
nificant effect of main target action on scores (p <
.001). We have identified a number of reasons why
we believe this is the case, which we discuss below.
5.2.1 Too much focus on medically important
events
Content-selection in BT-45 is largely driven by
rules that assess the medical importance of events
and patterns. In particular, BT-45 tends to give low
importance to events which it believes are due to
sensor artefacts. While this strategy makes sense
in many cases, it leads to poor performance in sce-
narios where the target action is Adjust Monitor-
ing Equipment, when sensor problems need to be
pointed out to the reader.
This can be seen in the example scenario used in
this paper. The TcPO2 and TcPCO2 traces shown in
Figure 1 are full of sensor artefacts (such as the im-
plausibly high values of TcPO2 mentioned above).
The human text shown in Figure 3 explicitly men-
tions these, for example (italics added)
At 13:50 the baby is examined. There
is a desaturation to 72% and the FiO2 is
changed to 50%. Between now and 14.10
there are several attempts to site a periph-
eral cannula. The pO2 and pCO2 both
have spikes in the traces, pO2 reaching
19.2 and pCO2 reaching 0.4. There are
several episodes of artefact in the oxygen
saturation trace.
The BT-45 text shown in Figure 4, in contrast, only
mentions one spike in TcPO2, and does not mention
any artefacts.
This is a difficult problem to solve, because in
a context where medical intervention was needed,
BT-45 would be correct to ignore the sensor prob-
lems. One solution would be for BT-45 to perform
a top-level diagnosis itself, and adjust its texts based
on whether it believed staff should focus on medi-
cal intervention or adjusting sensors. Whether this
is desirable or even feasible is unclear; it relates to
the more general issue of how a data-summarisation
system such as BT-45 should be integrated with
the kind of diagnosis systems developed by the
AI/Medicine community.
5.2.2 Poor description of related channels
BT-45 essentially describes each channel inde-
pendently. For temperature, however, it is often bet-
ter to describe the two temperature channels together
and even contrast them, which is what the human
texts do; this contributes to BT-45?s poor perfor-
mance in Manage Temperature scenarios.
For example, in one of the Manage Temperature
scenarios, the BT-45 text says
Core Temperature (T1) = 36.4. Peripheral
Temperature (T2) = 34.0. . . .
(new paragraph)Over the next 44 minutes
T2 decreased to 33.4.
The human text says
He is warm centrally but T2 drifts down
over the 45 minutes from 34 to 33.3C.
The information content of the two texts is quite
similar, but the human text describes temperature
in an integrated fashion. Similar problems occur in
other scenarios. In fact, over the 24 scenarios as
a whole, the human texts include only three para-
graphs which mention just one of the temperatures
(T1 or T2, but not both), while the BT-45 texts in-
clude 18 such paragraphs.
BT-45?s document planner is mostly driven by
medical importance and causal relationships; al-
though it does try to group together information
about related channels, this is done as a secondary
optimisation, not as a primary organising principle.
The human texts place a much higher priority on
152
grouping ?physiological systems? (to use NICU ter-
minology) of related channels and events together,
including the respiratory and cardiac systems as well
as the temperature system. We suspect that BT-45
should place more emphasis on systems in its docu-
ment planning.
5.2.3 Poor long-term overview
BT-45 does not do a good job of summarising a
channel?s behaviour over the entire scenario. This
isn?t a problem in eventful scenarios, where the key
is to describe the events; but it does reduce the effec-
tiveness of texts in uneventful scenarios where the
main target action is No Action (i.e., do nothing).
This problem can be seen in the text extracts
shown in the previous section. Even at the level of
individual channels, He is warm centrally is a better
overview than Core Temperature (T1) = 36.4; and
T2 drifts down over the 45 minutes from 34 to 33.3C
is better than Peripheral Temperature (T2) = 34.0.
. . . Over the next 44 minutes T2 decreased to 33.4.
At a signal analysis level, BT-45 also does not
do a good job of detecting patterns (such as spikes)
with a duration of minutes instead of seconds. This
contributes to the system?s poor performance in
Manage Temperature scenarios, because tempera-
ture changes relatively slowly.
We believe these problems can be solved, by
putting more emphasis on analysis and reporting of
long time-scale events in the BT-45 modules.
5.3 Other Problems
We manually examined the texts, looking for cases
where the BT-45 texts did not seem clear. This high-
lighted a number of additional issues.
5.3.1 Describing events at different temporal
time-scales
BT-45 does not always do a good job of correctly
identifying long-term trends in a context where there
are also short-term patterns such as spikes. In fact
accurately detecting simultaneous events at different
time-scales is one of the major signal analysis chal-
lenges in BT-45. There are linguistic issues as well
as signal analysis ones; for example, should long-
duration and short-duration events be described in
separate paragraphs?
5.3.2 Poor communication of time
BT-45 texts often did not communicate time well.
This is for a number of reasons, of which the
most fundamental is problems describing the time
of long-duration events. For instance, in our ex-
ample scenario, the sequence of insert/remove line
events in Figure 2 is analysed by the data-abstraction
module as the abstract event LINE-INSERTION-
PROCESS-FAILED, with a start time of 13.53 (first
insertion event) and an end time of 14.09 (last re-
moval event). BT-45 expresses this as There were 3
failed attempts to insert a peripheral venous line at
13:53; the time given is the time the abstract event
started, which is reasonable in this case. Now, if the
final insertion attempt at 14.08 had been successful,
the BT-45 data abstraction module would have in-
stead produced the abstract event LINE-INSERTION-
PROCESS-SUCCEEDED, with similar times, and BT-
45 would have produced the text
After three attempts, at 13.53 a peripheral
venous line was inserted successfully.
In other words, the time given would still be the time
that the abstract event started; but this is mislead-
ing, because readers of the above text expect that
the stated time is the time of the successful inser-
tion (14.08), not the time at which the sequence of
insert/remove events started.
We need a much better model of how to communi-
cate time, and how this communication depends on
the semantics and linguistic expression of the events
being described. An obvious first step, which we are
currently working on, is to include a linguistically-
motivated temporal ontology (Moens and Steedman,
1988), which will be separate from the existing do-
main ontology. We also need better techniques for
communicating the temporal relationships between
events in cases where they are not listed in chrono-
logical order (Oberlander and Lascarides, 1992).
6 Discussion
Two discourse analysts from Edinburgh University,
Dr. Andy McKinlay and Dr Chris McVittie, kindly
examined and compared some of the human and BT-
45 texts. Their top-level comment was that the hu-
man texts had much better narrative structures than
the BT-45 texts. They use the term ?narrative? in
153
the sense of Labov (1972, Chapter 9); that is story-
like structures which describe real experiences, and
which go beyond just describing the events and in-
clude information that helps listeners make sense of
what happened, such as abstracts, evaluatives, cor-
relatives, and explicatives.
Dr McKinlay and Dr. McVittie pointed out many
of the problems mentioned above, but they also
pointed out a number of other narrative deficiencies
in the BT-45 texts. The most fundamental was that
the human texts did a much better job of linking re-
lated events into a coherent whole. Other deficien-
cies include the lack of any kind of conclusion in the
BT-45 texts.
We agree with this analysis; it is striking that
many of the specific problems identified are related
to the problem of generating narratives. Continu-
ity, description of related channels, overview of be-
haviour over time, and communication of time are
all aspects of narrative in the broad sense; they are
things we need to get right in order to turn a text
into a story. This point is especially significant in
light of the fact that many of our medical collabora-
tors at Edinburgh have informally told us that they
believe stories are valuable when presenting infor-
mation about the babies, and indeed that a major
problem with data visualisation systems compared
to written notes (which they used many years ago) is
that the visualisation systems do not tell stories.
Unfortunately, we are not aware of any previ-
ous research in the NLG community about these is-
sues. Researchers in the creativity community have
looked at issues such as plot and character develop-
ment in systems that generate fictional stories (Perez
y Perez and Sharples, 2004); but this is not relevant
to our problem, which is presenting non-fictional
events as a narrative. Callaway and Lester (2002)
looked at microplanning issues in narrative genera-
tion, including reference, lexical variation, and ag-
gregation; but none of these were identified in our
evaluation as major problems in text quality.
7 Future Work
The BABYTALK project continues until August
2010, and during this period we hope to investigate
most of the issues identified above, especially the
ones related to narrative. We are currently conduct-
ing experiments to improve the way we communi-
cate time, and we have started redoing the docu-
ment planner to do a better job of describing sys-
tems of related channels in a unified manner. We
are also investigating top-down data abstraction and
document planning approaches which we hope will
address continuity problems, and which may assist
in better overviews and narrative structures. We are
also working on many issues not directly related to
narrative, such as reasoning about and communicat-
ing uncertainty, use of vague language, generation
of texts for non-specialists (e.g., parents), and HCI
issues.
We would welcome interest by other researchers
in these topics (there is more that needs investigat-
ing than we can do on our own!), and we would be
happy to assist such people, for example by sharing
some of our code and data resources.
8 Conclusion
We believe that there is enormous potential in sys-
tems such as BABYTALK which generate textual
summaries of data; the world desperately needs bet-
ter techniques to help people understand data sets,
and our experiments suggest that good textual sum-
maries really can help communicate data sets, at
least in some contexts. However, building good
data summarisation systems requires the NLG re-
search community to address a number of problems
which it has not traditionally focused on, many of
which have to do with generating good narratives.
We intend to focus much of our energy on these
issues, and would welcome research contributions
from other members of the community.
Acknowledgements
Many thanks to our colleagues in the BabyTalk
project, and to the doctors and nurses who partic-
ipated in the evaluation; this work would not have
been possible without them. Special thanks to Dr
McKinlay and Dr McVittie for agreeing to anal-
yse the texts for us. We are also grateful to our
colleagues in the Aberdeen NLG group, and to the
anonymous reviewers, for their helpful comments.
This research was funded by the UK Engineer-
ing and Physical Sciences Research Council, under
grant EP/D049520/1.
154
References
A Belz and E Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of
EACL-2006, pages 313?320.
C Callaway and J Lester. 2002. Narrative prose genera-
tion. Artificial Intelligence, 139:213?252.
G Carenini and J Moore. 2006. Generating and eval-
uating evaluative arguments. Artificial Intelligence,
170:925?952.
P Carpenter and P Shah. 1998. A model of the percep-
tual and conceptual processes in graph comprehension.
Journal of Experimental Psychology: Applied, 4:74?
100.
L Ferres, A Parush, S Roberts, and G Lindgaard. 2006.
Helping people with visual impairments gain access to
graphical information through natural language: The
iGraph system. In Proceedings of ICCHP-2008.
E Goldberg, N Driedger, and R Kittredge. 1994. Using
natural-language processing to produce weather fore-
casts. IEEE Expert, 9(2):45?53.
W Labov. 1972. Language in the Inner City. University
of Pennsylvania Press.
A Law, Y Freer, J Hunter, R Logie, N McIntosh, and
J Quinn. 2005. A comparison of graphical and textual
presentations of time series data to support medical de-
cision making in the neonatal intensive care unit. Jour-
nal of Clinical Monitoring and Computing, 19:183?
194.
M Moens and M Steedman. 1988. Temporal ontology
and temporal reference. Computational Linguistics,
14(2):15?28.
J Oberlander and A Lascarides. 1992. Preventing false
temporal implicatures: Interactive defaults for text
generation. In Proceedings of COLING-1992, pages
721?727.
R Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic an-
notation. In Proceedings of LREC-2006.
R Perez y Perez and M Sharples. 2004. Three computer-
based models of storytelling: Brutus, Minstrel, and
Mexica. Knowledge-Based Systems, 17:15?29.
F Portet, E Reiter, J Hunter, and S Sripada. 2007. Auto-
matic generation of textual summaries from neonatal
intensive care data. In Proceedings of AIME 2007.
R Power, D Scott, and N Bouayad-Agha. 2003. Doc-
ument structure. Computational Linguistics, 29:211?
260.
E Reiter, S Sripada, J Hunter, J Yu, and I Davy. 2005.
Choosing words in computer-generated weather fore-
casts. Artificial Intelligence, 167:137?169.
E Reiter. 2007. An architecture for Data-to-Text sys-
tems. In Proceedings of ENLG-07, pages 97?104.
S Sripada, E Reiter, and L Hawizy. 2005. Evaluation of
an NLG system using post-edit data: Lessons learned.
In Proceedings of ENLG-2005, pages 133?139.
R Turner, S Sripada, E Reiter, and I Davy. 2008. Using
spatial reference frames to generate grounded textual
summaries of georeferenced data. In Proceedings of
INLG-2008.
M van der Meulen, R Logie, Y Freer, C Sykes, N McIn-
tosh, and J Hunter. submitted. When a graph is poorer
than 100 words: A comparison of computerised natu-
ral language generation, human generated descriptions
and graphical displays in neonatal intensive care.
J Yu, E Reiter, J Hunter, and C Mellish. 2007. Choosing
the content of textual summaries of large time-series
data sets. Natural Language Engineering, 13:25?49.
155
Preface
Referring Expression Generation Challenges 2008 (REG?08) was the second
shared-task evaluation challenge (STEC) in the field of Natural Language Gen-
eration (NLG) and took place between September 2007, when REG?08 was first
announced, and May 2008, when the evaluation of the participating systems was
completed.
REG?08 follows on from the Attribute Selection for Referring Expression Gen-
eration Challenge in 2007 (ASGRE?07) which was conceived as a pilot event for
shared-task evaluation in NLG. Shared tasks in NLG are themselves a natural con-
tinuation of a growing interest in more comparative forms of evaluation among
NLG researchers, and mirror trends in other HLT fields.
Since the foundational work of authors such as Appelt, Kronfeld, Grosz, Joshi,
Dale and Reiter, Referring Expression Generation (REG) has been the subject of
intensive research in NLG, and has ? unusually for NLG ? led to significant
consensus on the REG problem definition, as well as the nature of the inputs and
outputs of REG algorithms. This is particularly true of the attribute selection sub-
task, perhaps the most widely researched NLG subtask, which was the shared task
in ASGRE?07. REG?08 included the same task (TUNA-AS), and added two more
tasks based on the TUNA corpus: realisation (TUNA-R) and the complete referring
expression generation task (TUNA-REG). REG?08 also introduced a new data set
of short introductory sections from Wikipedia articles on geographic entities and
people (the GREC corpus) and a new task based on it: generation of referring ex-
pressions for named entities in the context of a discourse longer than a sentence.
The intended application context for this task is improvement of referential clarity
in extractive summaries.
In addition to the four shared tasks, REG?08 offered, for each of the two
datasets, (i) an open submission track in which participants could submit any work
involving the data while opting out of the competetive element, and (ii) an eval-
uation track, in which proposals for new evaluation methods for the shared task
could be submitted. We believe that these two types of open-access tracks are im-
portant because they allow the wider research community to shape the focus and
methodologies of STECs directly.
We successfully applied (with the help of support letters from many of last
year?s participants and other HLT colleagues) for funding from the Engineering
and Physical Sciences Research Council (EPSRC), the main funding body for HLT
in the UK. This support enabled us to double the size of the GREC corpus and
to carry out extensive human task performance evaluations, as well as employ a
dedicated research fellow (Eric Kow) to help with all aspects of REG?08. It also
enabled us to enlist the help of Jette Viethen from Macquarie University with the
GREC evaluations.
REG?08 got underway with a first announcement in September 2007. We re-
leased samples for both datasets and invited preliminary registrations in January
2008, and released the full Participants? Pack including instructions and train-
ing/development data on 22nd February to registered participants. Twelve teams
registered for one or more of the TUNA tasks, and five for the GREC Task. Among
the participants were teams from Australia, Spain, Ireland, UK, USA, Brazil, Bel-
gium, Netherlands, India and Germany.
181
By the deadline of 7th April (which was extended slightly), 8 teams submitted
13 systems for TUNA-AS, 4 teams submitted 5 systems for TUNA-R, and 6 teams
submitted 15 systems for TUNA-REG. Three teams submitted 6 systems for the
brand new GREC task, to which we added four baseline systems. We also received
one submission in the TUNA Open Track which applies Portuguese surface realisa-
tion to TUNA attribute sets.
The submission process required participants to first submit a report describing
their approach and reporting results on the development data for the task(s) they
had participated in. For this purpose, they were supplied with programs to compute
the relevant evaluation metrics. On submission of the report, they could download
the test data, and had 48 hours to submit their outputs on the test set.
All submissions are described in the participants? reports in this volume. We
are pleased to say that several of the contributions came from students, as well as
from researchers entirely new to the field of NLG.
We had a total of 33 TUNA systems and 10 GREC systems to evaluate. We
conducted task-performance experiments for all TUNA-REG systems and the 6 sub-
mitted GREC systems. We computed a range of automatic intrinsic measures for
all task tracks. For the GREC task, we also tried out a new evaluation idea: auto-
matic extrinsic evaluation using coreference resolution tools. All these evaluation
methods are described in detail in the two evaluation reports in this volume.
Preparations are already underway for a third NLG shared-task evaluation event
in 2009, Generation Challenges 2009, which will include a TUNA-REG progress
check task, a GREC task and a new task on instruction giving in virtual environ-
ments (the GIVE Challenge). Results will be presented at ENLG?09.
Like all STECs, REG?08 would not have been possible without the contributions
of many different people. We would like to thank the faculty, staff and students
of Brighton University and the friends who participated in the evaluation exper-
iments; the INLG?08 organisers, in particular Mike White; the research support
team at Brighton University and the EPSRC for help with obtaining funding; Jason
Baldridge and Pascal Denis for assistance with the coreference resolvers; and last
but not least, the REG?08 participants for making the most of the short available
time to build their systems (and for never once complaining, even when they had
good reason to). Special thanks are due to Eric Kow and Jette Viethen who worked
extremely hard especially during the four weeks of the evaluation period.
Anja Belz and Albert Gatt
182
The GREC Challenge: Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Abstract
The GREC Task at REG?08 required partici-
pating systems to select coreference chains to
the main subject of short encyclopaedic texts
collected from Wikipedia. Three teams sub-
mitted a total of 6 systems, and we addition-
ally created four baseline systems. Systems
were tested automatically using a range of ex-
isting intrinsic metrics. We also evaluated
systems extrinsically by applying coreference
resolution tools to the outputs and measuring
the success of the tools. In addition, systems
were tested in a reading/comprehension exper-
iment involving human subjects. This report
describes the GREC Task and the evaluation
methods, gives brief descriptions of the par-
ticipating systems, and presents the evaluation
results.
1 Introduction
The GREC task is about how to generate appropri-
ate references to an entity in the context of a piece
of discourse longer than a sentence. Rather than
requiring participants to generate referring expres-
sions from scratch, the GREC data provides sets of
possible referring expressions for selection. As this
is a new referring expression generation (REG) task,
the shared task definition was kept fairly simple
and the aim for participating systems was to select
the appropriate type of referring expression (more
specifically, its REG08-TYPE, full details below).
The immediate motivating application context for
the GREC Task is the improvement of referential
clarity and coherence in extractive summarisation
by regenerating referring expressions in summaries.
There has recently been a small flurry of work in
this area (Steinberger et al, 2007; Nenkova, 2008).
In the longer term, the GREC Task is intended to be a
step in the direction of the more general task of gen-
erating referential expressions in discourse context.
The GREC Task Corpus is an extension of GREC
1.0 which had about 1,000 texts in the subdomains
of cities, countries, rivers and people (Belz and
Varges, 2007a). for the purpose of the REG?08 GREC
Task, we obtained an additional 1,000 texts in the
new subdomain of mountain texts and developed a
new XML annotation scheme (Section 2.2).
Five teams from four countries registered for the
GREC Task, of which three teams eventually submit-
ted 6 systems. We also used the corpus texts them-
selves as ?system? outputs, and created four base-
line systems. We evaluated the resulting 10 sys-
tems using a range of intrinsic and extrinsic evalu-
ation methods. This report presents the results of all
evaluations (Section 6), along with descriptions of
the GREC data and task (Section 2), test sets (Sec-
tion 3), evaluation methods (Section 4), and partici-
pating systems (Section 5).
2 Data and Task
The GREC Corpus (version 2.0) consists of about
2,000 texts in total, all collected from introductory
sections in Wikipedia articles, in five different do-
mains (cities, countries, rivers, people and moun-
tains). In each text, three broad categories of Main
Subject Reference (MSR)1 have been annotated, re-
1The main subject of a Wikipedia article is simply taken to
be given by its title, e.g. in the cities domain the main subject
183
sulting in a total of about 13,000 annotated REs.
The corpus was randomly divided into 90% train-
ing data (of which 10% were randomly selected as
development data) and 10% test data. Participants
used the training data in developing their systems,
and (as a minimum requirement) reported results on
the development data. Participants had 48 hours to
submit outputs for the (previously unseen) test data.
2.1 Types of referential expression annotated
Three broad categories of main subject referring ex-
pression (MSREs) are annotated in the GREC corpus2
? subject NPs, object NPs, and genitive NPs and pro-
nouns which function as subject-determiners within
their matrix NP. These categories of referring ex-
pression (RE) are relatively straightforward to iden-
tify and achieve high inter-annotator agreement on
(complete agreement among four annotators in 86%
of MSRs), and account for most cases of overt main
subject reference (MSR) in the GREC texts. The an-
notators were asked to identify subject, object and
genitive subject-determiners and decide whether or
not they refer to the main subject of the text. More
detail is provided in Belz and Varges (2007b).
In addition to the above, relative pronouns in sup-
plementary relative clauses (as opposed to integrated
relative clauses, Huddleston and Pullum, 2002, p.
1058) were annotated, e.g.:
(1) Stoichkov is a football manager and former striker who
was a member of the Bulgaria national team that
finished fourth at the 1994 FIFA World Cup.
We also annotated ?non-realised? subject MSREs
in a restricted set of cases of VP coordination where
an MSRE is the subject of the coordinated VPs, e.g.:
(2) He stated the first version of the Law of conservation of
mass, introduced the Metric system, and helped to
reform chemical nomenclature.
The motivation for annotating the approximate
place where the subject NP would be if it were re-
alised (the gap-like underscores above) is that from
a generation perspective there is a choice to be made
about whether to realise the subject NP in the second
and third coordinates or not.
(and title) of one text is London.
2In terminology and view of grammar the annotations rely
heavily on Huddleston and Pullum (2002).
2.2 XML format
Figure 1 is one of the texts distributed in the GREC
data sample for the REG Challenge. The REF el-
ement indicates a reference, in the sense of ?an
instance of referring? (which could, in principle,
be realised by gesture or graphically, as well as
by a string of words, or a combination of these).
REFs have three attributes: ID, a unique refer-
ence identifier; SEMCAT, the semantic category of
the referent, ranging over city, country, river,
person, mountain; and SYNCAT, the syntactic cat-
egory required of referential expressions for the ref-
erent in this discourse context (np-obj, np-subj,
subj-det). A REF is composed of one REFEX el-
ement (the ?selected? referential expression for the
given reference; in the corpus texts it is simply
the referential expression found in the corpus) and
one ALT-REFEX element which in turn is a list of
REFEXs which are alternative referential expressions
obtained by other means (see following section).
REFEX elements have four attributes. The
HEAD attribute has the possible values nominal,
pronoun, and rel-pron; the CASE attribute has
the possible values nominative, accusative and
genitive for pronouns, and plain and genitive
for nominals. The binary-valued EMPHATIC at-
tribute indicates whether the RE is emphatic; in the
present version of the GREC corpus, the only type of
RE that has this attribute is one which incorporates
a reflexive pronoun used emphatically (e.g. India it-
self ). The REG08-TYPE attribute indicates basic RE
type as required for the REG?08 GREC task defini-
tion. The choice of types is motivated by the hy-
pothesis that one of the most basic decisions to be
taken in RE selection for named entities is whether to
use an RE that includes a name, such as Modern In-
dia (the corresponding REG08-TYPE value is name);
whether to go for a common-noun RE, i.e. with a
category noun like country as the head (common);
whether to pronominalise the RE (pronoun); or
whether it can be left unrealised (empty).
2.3 The REG?08 GREC Task
The task for participating systems was to develop
a method for selecting one of the REFEXs in the
ALT-REFEX list, for each REF in each TEXT in the
test sets. The test data inputs were identical to the
184
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TEXT SYSTEM "reg08-grec.dtd">
<TEXT ID="36">
<TITLE>Jean Baudrillard</TITLE>
<PARAGRAPH>
<REF ID="36.1" SEMCAT="person" SYNCAT="np-subj">
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX>
<REFEX REG08-TYPE="empty">_</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX>
</ALT-REFEX>
</REF>
(born June 20, 1929) is a cultural theorist, philosopher, political commentator,
sociologist, and photographer.
<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det">
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX>
</ALT-REFEX>
</REF>
work is frequently associated with postmodernism and post-structuralism.
</PARAGRAPH>
</TEXT>
Figure 1: Example text from REG?08 Training Data.
training/development data, except that REF elements
contained only an ALT-REFEX list, not the preced-
ing ?selected? REFEX. ALT-REFEX lists are gener-
ated for each text by an automatic method which
collects all the (manually annotated) MSREs in a text
including the title and adds several defaults: pro-
nouns and reflexive pronouns in all subdomains; and
category nouns (e.g. the river), in all subdomains
except people. The main objective in the REG?08
GREC Task was to get the REG08-TYPE attribute of
REFEXs right.
3 Test Data
1. GREC Test Set C-1: a randomly selected 10%
subset (183 texts) of the GREC corpus (with the same
proportions of texts in the 5 subdomains as in the
training/testing data).
2. GREC Test Set C-2: the same subset of texts as
in C-1; however, for C-2 we did not use the MSREs
in the corpus, but replaced each of them with three
human-selected alternatives. These were obtained in
an online experiment as described in Belz & Varges
(2007a) where subjects selected MSREs in a setting
that duplicated the conditions in which the partici-
pating systems in the REG?08 GREC Task made se-
lections.3 We obtained three versions of each text,
where in each version all MSREs were selected by
the same person. The motivation for creating this
version of Test Set C was firstly that having sev-
eral human-produced chains of MSREs to compare
the outputs of participating (?peer?) systems against
is more reliable than having one only; and secondly
that Wikipedia texts are edited by multiple authors
and so MSR chains may sometimes be adversely af-
fected by this; we wanted to have additional refer-
ence texts without this characteristic.
3. GREC Test Set L: 74 Wikipedia introductory
texts from the subdomain of lakes; participants did
not know what this subdomain was until they re-
ceived the test data (there were no lake texts in the
training/development set).
4. GREC Test Set P: 31 short encyclopaedic texts
in the same 5 subdomains as in the GREC corpus,
in approximately the same proportions as in the
training/testing data, but from a source other than
3The experiment can be tried out here: http://www.nltg.
brighton.ac.uk/home/Anja.Belz/TESTDRIVE/
185
Wikipedia. We transcribed these texts from printed
encyclopaedias published in the 1980s which are
not available in electronic form, and this provenance
was not revealed to participants. The texts in this set
are much shorter and more homogeneous than the
Wikipedia texts, and the sequences of MSRs follow
very similar patterns. It seems likely that it is these
properties that have resulted in better scores overall
for Test Set P (see Section 6).
Each test set was designed to test peer systems for
a different aspect of generalisation. Test Set C tests
for generalisation to unseen material from the same
corpus and the same subdomains as the training set;
Test Set L tests for generalisation to unseen material
from the same corpus but different subdomain; and
Test Set P tests generalisation to a different corpus
but same subdomains.
4 Evaluation methods
4.1 Automatic intrinsic evaluations
Accuracy of REG08-Type: when computed against
the single-RE test sets (C-1, L and P), REG08-Type
Accuracy is the proportion of REFEXs selected by a
participating system that have a REG08-TYPE value
identical to the one in the corpus.
When computed against the triple-RE test set (C-
2), first the number of correct REG08-Types is com-
puted at the text level for each of the three ver-
sions of a corpus text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total number of
REFs in all the texts, which gives the global REG08-
Type Accuracy score. The rationale behind com-
puting the REG08-Type Accuracy scores in this way
for multiple-RE test sets (maximising scores on RE
chains rather than individual REs) is that an RE is
not good or bad in its own right, but depends on the
other MSRs in the same text.4
String Accuracy: This is defined just like
REG08-Type Accuracy, except here what is deter-
mined is identity between REFEX word strings (the
MSREs themselves), not between REG08-Types.
String-edit distance metrics: String-edit dis-
tance (SE) is straightforward Levenshtein distance
with a substitution cost of 2 and insertion/deletion
4This definition is also slightly different from the one given
in the Participants? Pack.
cost of 1. We also used the version of string-edit
distance described by Bangalore et al (2000) which
normalises for length. This version is denoted ?SEB?
below. For the single-RE test sets, the global score
is simply the average of all RE-level scores. For Test
Set C-2, we used an approach analogous to that de-
scribed above for REG08-Type Accuracy. We first
computed the best string-edit distance at the text
level (here, just the sum of RE-level distances) and
then obtained the global distance by dividing the
sum of best text-level distances by the number of
REFs in all the texts.
Other metrics: BLEU is a precision metric from
MT that assesses the quality of a peer translation
in terms of the proportion of its word n-grams
(n ? 4 is standard) that it shares with several ref-
erence translations. We used BLEU-3 rather than
the more standard BLEU-4 because most REs in the
corpus are less than 4 tokens long. We also used
the NIST version of BLEU which weights in favour
of less frequent n-grams, as well as ROUGE-2 and
ROUGE-SU4 (the two official automatic scores from
the DUC summarisation competitions). In all cases,
we assessed just the MSREs selected by peer systems
(leaving out the surrounding text), and computed
scores globally (rather than averaging over RE-level
scores), as this is standard for these metrics.
BLEU, NIST and ROUGE are designed to work
with either one or multiple reference texts, so we did
not need to use a different method for Test Set C-2.
4.2 Human extrinsic evaluation
We designed a reading/comprehension experiment
in which the task for subjects was to read texts
one sentence at a time and then to answer three
brief multiple-choice comprehension questions after
reading each text. The basic idea was that it seemed
likely that badly chosen MSR reference chains would
adversely affect ease of comprehension, and that this
might in turn affect reading speed and accuracy in
answering comprehension questions.
We used a randomly selected subset of 21 texts
from Test Set C, and recruited 21 subjects from
among the staff, faculty and students of Brighton
and Sussex universities. We used a Repeated Latin
Squares design in which each combination of text
and system was allocated three trials. During the
experiment we recorded SRTime, the time subjects
186
took to read sentences (from the point when the sen-
tence appeared on the screen to the point at which
the subject requested the next sentence).
We also recorded the speed and accuracy with
which subjects answered the questions at the end (Q-
Time and Q-Acc). The role of the comprehension
questions was to encourage subjects to read the texts
properly, rather than skimming through them, and
we did not necessarily expect any significant results
from the associated measures.
The questions were designed to be of varying de-
grees of difficulty and predictability. There was one
set of three questions (each with five possible an-
swers) associated with each text, and questions fol-
lowed the same pattern across the texts: the first
question was always about the subdomain of a text;
the second about the location of the main subject; the
third question was designed not to be predictable.
The order of the answers was randomised for each
question and each subject. The order of texts (with
associated questions) was randomised for each sub-
ject. We used the DMDX package for presentation
of sentences and measuring reading times and ques-
tion answering accuracy (Forster and Forster, 2003).
Subjects did the experiment in a quiet room, under
supervision.
4.3 Automatic extrinsic evaluation
As a new and highly experimental method, we tried
out an automatic approach to extrinsic evaluation.
The basic idea was similar to that in the human-
based experiments described above: badly chosen
reference chains seem likely to affect the reader?s
ability to resolve REs. In the automatic version, the
role of the reader is played by an automatic coref-
erence resolution tool and the expectation is that the
tool performs worse (are less able to identify coref-
erence chains correctly) with worse MSR reference
chains.
To counteract the potential problem of results be-
ing a function of a specific coreference resolution
algorithm or tool, we decided to use three differ-
ent resolvers?those included in LingPipe,5 JavaRap
(Qiu et al, 2004) and OpenNLP (Morton, 2005)?
and to average results.
There does not appear to be a single standard eval-
5http://alias-i.com/lingpipe/
uation metric in the coreference resolution commu-
nity, so we opted to use three: MUC-6 (Vilain et al,
1995), CEAF (Luo, 2005), and B-CUBED (Bagga and
Baldwin, 1998), which seem to be the most widely
accepted metrics.
All three metrics compute Recall, Precision and
F-Scores on aligned gold-standard and resolver-tool
coreference chains. They differ in how the align-
ment is obtained and what components of corefer-
ence chains are counted for calculating scores. Re-
sults for the automatic extrinsic evaluations are re-
ported below in terms of the F-Scores from these
three metrics, as well as in terms of their average.
5 Systems
Base-rand, Base-freq, Base-1st, Base-name: We
created four baseline systems. Base-rand selects
one of the REFEXs at random. Base-freq selects
the REFEX that is the overall most frequent given
the SYNCAT and SEMCAT of the reference. Base-
1st always selects the REFEX which appears first
in the list of REFEXs; and Base-name selects the
shortest REFEX with attributes REG08-TYPE=name,
HEAD=nominal and EMPHATIC=no.6
CNTS-Type-g, CNTS-Prop-s: The CNTS sys-
tems are trained using memory-based learning with
automatic parameter optimisation. They use a set of
14 features obtained by various kinds of syntactic
preprocessing and named-entity recognition as well
as from the corpus annotations: SEMCAT, SYNCAT,
position of RE in text, neighbouring words and POS-
tags, distance to previous mention, SYNCATs of
three preceding REFEXs, binary feature indicating
whether the most recent named entity was the main
subject (MS), main verb of the sentence. For Type-
g, a single classifier was trained to predict just the
REG08-TYPE property of REFEXs. For Prop-s, four
classifiers were trained, one for each subdomain, to
predict all four properties of REFEXs (rather than just
REG08-TYPE).
OSU-b-all, OSU-b-nonRE, OSU-n-nonRE: The
OSU-2 systems are maximum-entropy classifiers
trained on a range of features obtained by prepro-
6Attributes are tried in this order. If for one attribute, the
right value is not found, the process ignores that attribute and
moves on the next one.
187
System REG08-Type Accuracy for Development SetAll Cities Coun Riv Peop Moun
CNTS-Type-g 76.52 64.65 75 65 85.37 75.42
CNTS-Prop-s 73.93 65.66 69.57 70 79.51 74.58
IS-G 66 54.5 64 80 66.8 65
OSU-n-nonRE 62.50 53.54 63.04 65 67.32 61.67
OSU-b-all 58.54 53.54 57.61 75 65.85 49.58
OSU-b-nonRE 51.07 51.52 53.26 40 57.07 45.83
Table 1: Self-reported REG08-Type Accuracy scores for
development set.
cessing the text, as well as from the corpus anno-
tations: SEMCAT, SYNCAT, position of RE in text,
presence of contrasting discourse entity, distance be-
tween current and preceding reference to the MS,
string similarity measures between REFEXs and ti-
tle of text. OSU-b-all and OSU-b-nonRE are binary
classifiers which give the likelihood of selecting a
given REFEX vs. not selecting it, whereas OSU-n-
nonRE is a 4-class classifier giving the likelihoods
of selecting each of the four REG08-TYPEs. OSU-
b-all also uses the REFEX attributes as features.
IS-G: The IS-G system is a multi-layer percep-
tron which uses four features obtained by prepro-
cessing texts as well as from the corpus annota-
tions: SYNCAT, distance between current and pre-
ceding reference to the MS, position of RE in text,
REG08-TYPE of preceding reference to the MS, fea-
ture indicating whether the preceding MSR is in the
same sentence.
6 Results
This section presents the results of all the evalua-
tion methods described in Section 4. We start with
REG08-Type Accuracy, an intrinsic automatic met-
ric which participating teams were told was going
to be the chief evaluation method, followed by other
intrinsic automatic metrics (Section 6.2), the extrin-
sic human evaluation (Section 6.3) and the extrinsic
automatic evaluation (Section 6.4).
6.1 REG08-Type Accuracy
Participants computed REG08-Type Accuracy for
the development set (97 texts) themselves, using a
tool provided by us. These scores are shown in
Table 1, and are also included in the participants?
reports elsewhere in this volume. Systems are or-
dered in terms of their overall REG08-Type Accu-
racy (column 1), and scores for each subdomain are
also shown. Scores are highly consistent across the
subdomains, except for the river subdomain which
was the smallest set (containing only 4 texts), and
results for it may be idiosyncratic for this reason.
Corresponding results for the (unseen) test set C-1
are shown in column 2 of Table 2. As would be ex-
pected, results are slightly worse than for the (seen)
development set (although some systems managed
to improve over their development set scores). Also
included in this table are results for the four base-
line systems, and it is clear that selecting the most
frequent REG08-Type given SEMCAT and SYNCAT
(as done by the Base-freq system) provides a strong
baseline.
Other columns in Table 2 contain results for test
sets L and P. Again as expected, results for Test Set
L are lower than for Test Set C-1, because in ad-
dition to consisting of unseen texts (like C-1), Test
Set L is also from an unseen subdomain (unlike C-
1). The results for Test Set P are higher and on a par
with those for the development set, probably for the
reasons discussed at the end of Section 3.
For each test set in Table 2 we carried out a uni-
variate ANOVA with System as the fixed factor. We
found significant main effects at p < .001 in all
three cases (C-1: F = 95.426; L: F = 63.758;
P: F = 21.188). The columns containing capital
letters in Table 2 show the homogeneous subsets of
systems as determined by post-hoc Tukey HSD com-
parisons of means. Systems whose REG08-Type Ac-
curacy scores are not significantly different (at the
.05 level) share a letter.
The results for REG08-Type Accuracy computed
against the triple-RE Test Set C-2 are shown in Ta-
ble 3. These should be considered as the chief results
of the GREC Task evaluations, as stated in the guide-
lines. Here too we performed a univariate ANOVA
with System as the fixed factor and REG08-Type
as the dependent variable. Having established by
ANOVA that there was a significant main effect of
System (F = 86.946, p < .001), we compared the
mean scores with Tukey?s HSD. As can be seen from
the resulting homogeneous subsets, there is no sig-
nificant difference between the corpus texts (C-1)
and system CNTS-Type-g, but also there is no sig-
188
single-RE Test Set C-1 Test Set L Test Set P
CNTS-Type-g 68.15 A CNTS-Type-g 62.06 A CNTS-Type-g 75.31 A
CNTS-Prop-s 67.04 A CNTS-Prop-s 62.06 A CNTS-Prop-s 72.84 A B
IS-G 66.48 A IS-G 60.93 A IS-G 67.90 A B C
OSU-n-nonRE 63.69 A OSU-n-nonRE 41.80 B OSU-n-nonRE 66.67 A B C
OSU-b-nonRE 53.11 B OSU-b-nonRE 39.23 B OSU-b-all 57.41 B C D
OSU-b-all 52.39 B OSU-b-all 37.62 B C OSU-b-nonRE 56.17 C D
Base-freq 43.47 C Base-freq 35.53 B C Base-freq 44.44 D F
Base-name 39.49 C Base-rand 23.63 C D Base-rand 33.95 F
Base-1st 39.17 C Base-name 23.63 D Base-name 32.10 F
Base-rand 32.72 D Base-1st 29.74 D Base-rand 32.10 F
Table 2: REG08-Type Accuracy scores and homogeneous subsets (Tukey HSD, alpha = .05) for single-RE test sets.
Systems that do not share a letter are significantly different.
System REG08-Type Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People Mountains
Corpus 78.58 A 70.92 77.49 85.29 84.67 75.81
CNTS-Type-g 72.61 A B 65.96 71.73 73.53 77.64 70.73
CNTS-Prop-s 71.34 B 64.54 67.02 70.59 75.38 71.75
IS-G 70.78 B 69.50 65.45 76.47 76.88 67.89
OSU-n-nonRE 69.82 B 65.25 64.92 79.41 78.14 65.65
OSU-b-nonRE 58.76 C 52.48 60.73 50.00 59.80 59.55
OSU-b-all 57.48 C 53.90 58.64 47.06 59.05 57.52
Base-name 50.00 D 53.19 54.45 35.29 43.22 53.86
Base-1st 49.28 D 53.19 49.21 38.24 43.22 53.86
Base-freq 48.17 D 43.97 42.41 55.88 56.78 44.11
Base-rand 41.24 E 41.84 36.13 32.35 44.47 41.06
Table 3: REG08-Type Accuracy scores against Test Set C-2 for complete set and for subdomains; homogeneous subsets
(Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are significantly different).
nificant difference between the latter and systems
CNTS-Prop-s, IS-G and OSU-n-nonRE. In this anal-
ysis, all systems outperform the random baseline;
all peer systems outperform all of the baselines; and
the four best peer systems outperform the remaining
two.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure reported
on in the preceding section, we computed the string
similarity metrics described in Section 4.1 for all
four test sets. Results were very similar to those
for REG08-Type Accuracy, so we are reporting only
scores for Test Set C-2 (Table 4). The corpus texts
again receive the best scores across the board (SE is
the odd one out, because here lower scores are bet-
ter). Ranks for peer systems are very similar to the
results reported in the last section.
We performed an ANOVA (F = 138.159, p <
.001) and Tukey HSD post-hoc analysis for String
Accuracy. The resulting homogeneous subsets (Ta-
ble 4, columns 3?8) reveal significant differences
similar to those for REG08-Type Accuracy. We also
computed Pearson product-moment correlation co-
efficients between all automatic intrinsic evaluation
measures we used. All pairwise correlations were
significant at the .01 level (using a two-tailed test).
One of the strongest correlations (.961) was between
REG08-Type Accuracy and String Accuracy, imply-
ing that getting REG08-Type right gets you some
way towards getting the actual RE right.
6.3 Human-based extrinsic measures
As a result of the experiment described in Sec-
tion 4.2 we had SRTime measures (sentence reading
times) for each sentence in each of the 21 texts that
were included in the experiment. Table 5 shows the
resulting SRTimes in milliseconds averaged per sys-
tem. None of the differences were statistically sig-
nificant. We also analysed SRTimes normalised by
sentence length; SRTimes only from sentences that
contained MRSs; and SRTimes normalised for sub-
ject reading speed. There were no significant differ-
ences under any of these analyses.
Much of the variance in SRTimes was due to sub-
jects? very different average reading speeds: means
of SRTime normalised for sentence length ranged
from 188.45ms to 426.10ms for individual subjects.
189
System Word string similarity for Triple-RE Test Set C-2String Accuracy BLEU-3 NIST ROUGE-2 ROUGE-SU4 SE SEB
Corpus 71.18 A 0.7792 7.5080 0.66102 0.70991 0.7229 0.5136
CNTS-Type-g 65.61 A B 0.7377 6.1288 0.60280 0.64998 0.8838 0.3627
CNTS-Prop-s 65.29 A B 0.6760 5.9338 0.60103 0.64963 0.9068 0.3835
OSU-n-nonRE 63.85 B C 0.6715 5.7745 0.53395 0.57459 0.9666 0.0164
IS-G 58.20 C 0.5107 5.6102 0.50270 0.57052 1.1616 0.1818
OSU-b-nonRE 51.11 D 0.4964 5.5363 0.38255 0.42969 1.2834 0.0247
OSU-b-all 50.72 D 0.5050 5.6058 0.35133 0.39570 1.2994 0.3402
Base-freq 41.32 E 0.2684 3.0155 0.27727 0.33007 1.54299 -0.3250
Base-name 39.41 E 0.4641 5.9372 0.20730 0.25379 1.5175 -0.1912
Base-1st 39.09 E 0.3932 5.1597 0.21443 0.24037 1.6449 -0.0751
Base-rand 17.99 F 0.2182 2.9327 0.36056 0.41847 2.3217 -0.7937
Table 4: String Accuracy, BLEU, NIST, ROUGE and string-edit scores, computed on single-RE and triple-RE test
sets (systems in order of String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy only
(systems that do not share a letter are significantly different).
Mean SRTime (msecs)
CNTS-Prop-s 6305.8551
IS-G 6340.5131
OSU-n-nonRE 6422.5073
CNTS-Type-g 6435.6574
OSU-b-all 6451.7624
OSU-b-nonRE 6454.6749
Corpus 6548.2734
Table 5: Mean SRTimes for each system.
There was also variance from Text, i.e. some of the
texts appear to be harder to read than others.
The other two measures from the task-
performance experiment were Q-Acc (question
answering accuracy) and Q-Time (question answer-
ing speed). ANOVAs revealed no significant main
effect of System on Q-Time. For Q-Acc, we looked
at each of the three question types Q1, Q2, Q3
(see Section 4.2) separately. ANOVAs showed no
significant effect of System on Q-Acc for Q2 and
Q3; there was a slight effect (F = 2.193, p < .05)
of System on Q-Acc for Q1 (the easiest of the
questions which simply asked for the subdomain
of a text). Table 6 shows Q-Acc for Q1 and Q2,
and the results of a post-hoc analysis (Tukey HSD)
which revealed two homogeneous subsets with a lot
of overlap (columns 2 and 3).
Table 6 shows the results of this analysis: there
was
6.4 Automatic extrinsic measures
We used the same 21 texts as in the human extrin-
sic experiments, fed the outputs of each peer sys-
Question 1 Q2 Q3
Corpus 1.00 A .78 .63
CNTS-Type-g 1.00 A .83 .71
CNTS-Prop-s .98 A B .86 .75
OSU-b-nonRE .97 A B .83 .67
OSU-b-all .95 A B .75 .62
IS-G .95 A B .81 .63
OSU-n-nonRE .90 B .76 .76
Table 6: Question types 1?3, proportions correct; homo-
geneous subsets for Q1 (Tukey HSD, alpha = .05).
tem as well as the corpus texts through the three
coreference resolvers, and computed average MUC,
CEAF and B-CUBED F-Scores as described in Sec-
tion 4.3. The second column Table 7 shows the av-
erage of these three F-Scores, to give a single over-
all result for this evaluation method. A univariate
ANOVA with the average F-Score (column 2) as the
dependent variable and System as the single fixed
factor revealed a significant main effect of System
on average F-Score (F = 5.051, p < .001). A
post-hoc comparison of the means (Tukey HSD, al-
pha = .05) found the significant differences indi-
cated by the homogeneous subsets in columns 3?
5 (Table 7). The numbers shown in the last three
columns are the separate MUC, CEAF and B-CUBED
F-Scores for each system, averaged over the three
resolver tools. ANOVAs revealed the following ef-
fects of System: on CEAF F = 9.984, p < .001;
on MUC: F = 10.07, p < .001; on B-CUBED:
F = 8.446, p < .001.
The three F-Score measures (MUC, CEAF and B-
CUBED) are all strongly and highly significantly cor-
190
related: Pearson?s correlation coefficient is .947 for
B-CUBED and CEAF, .917 for B-CUBED and MUC,
and .951 for CEAF and MUC (p < .01, 2-tailed).
System (MUC+CEAF+B3)/3 MUC CEAF B3
Base-1st 53.50 A 47.59 52.64 60.28
Base-name 52.84 A 45.99 51.73 60.81
OSU-n-nonRE 51.39 A 46.92 49.8 57.45
OSU-b-nonRE 51.27 A 47.68 48.62 57.50
OSU-b-all 50.87 A 47.06 48.40 57.14
CNTS-Type-g 48.64 A B 43.77 46.32 55.82
IS-G 48.05 A B 43.25 46.24 54.66
CNTS-Prop-s 46.35 A B 42.82 43.36 52.88
Corpus 43.32 A B 37.89 41.6 50.47
Base-freq 41.41 B C 34.48 40.28 49.46
Base-rand 35.13 C 21.24 35.60 48.55
Table 7: MUC, CEAF and B-CUBED F-Scores for all sys-
tems; homogeneous subsets (Tukey HSD), alpha = .05,
for average of F-Scores.
7 Concluding Remarks
The GREC Task is a new task not only for an NLG
shared-task challenge, but also as a research task in
general (improving referential clarity in extractive
summaries seems to be just taking off as a research
subfield). It was therefore not unexpected that only
three teams were able to participate in this task.
We continued the traditions of the ASGRE?07
Challenge in that we used a wide range of evalu-
ation metrics to obtain a well-rounded view of the
quality of the participating systems. It had been our
intention to use evaluation methods in all four possi-
ble extrinsic/intrinsic and automatic/human combi-
nations. However, the combination intrinsic/human
is missing from this report and will have to be left to
future research.
There was no indication in the human task perfor-
mance experiment that the different reference chains
selected by different systems had any impact on sub-
jects? reading speeds, and the evidence that there
is an effect on comprehension was scant. This
means that we will need to investigate alternative
task-performance measures. Because of the lack of
significant results from the human extrinsic experi-
ment, we were also unable to validate the automatic
extrinsic experiment against it, and so at this point
we do not really know how useful it is (despite some
correlation with intrinsic measures), something we
will seek to establish in future research.
Acknowledgments
Many thanks to Jason Baldridge and Pascal De-
nis for help with selecting coreference resolution
tools and metrics, and to the colleagues and students
who helped with the task-performance experiment.
Thanks are also due to the members of the Corpora
and SIGGEN mailing lists, colleagues, friends and
friends of friends who helped with the online MSRE
selection experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Proceedings of the Linguistic
Coreference Workshop at LREC?98, pages 563?566.
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proceedings of
INLG?00, pages 1?8.
A. Belz and S. Varges. 2007a. Generation of repeated
references to discourse entities. In Proceedings of
ENLG?07, pages 9?16.
A. Belz and S. Varges. 2007b. The GREC corpus: Main
subject reference in context. Technical Report NLTG-
07-01, University of Brighton.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy. Be-
havior Research Methods, Instruments, & Computers,
35(1):116?124.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of Pen-
sylvania.
A. Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Proceedings of IJC-
NLP?08.
L. Qiu, M. Kan, and T.-S. Chua. 2004. A public ref-
erence implementation of the rap anaphora resolution
algorithm. In Proceedings of LREC?04, pages 291?
294.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.
2007. Two uses of anaphora resolution in summariza-
tion. Information Processing and Management: Spe-
cial issue on Summarization, 43(6):1663?1680.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. Proceedings of MUC-6, pages 45?52.
191
The TUNA Challenge 2008: Overview and Evaluation Results
Albert Gatt
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Anja Belz Eric Kow
Natural Language Technology Group
University of Brighton
Brighton BN2 4GJ, UK
{asb, eykk10}@brighton.ac.uk
Abstract
The TUNA Challenge was a set of three shared
tasks at REG?08, all of which used data from
the TUNA Corpus. The three tasks covered
attribute selection for referring expressions
(TUNA-AS), realisation (TUNA-R) and end-to-
end referring expression generation (TUNA-
REG). 8 teams submitted a total of 33 systems
to the three tasks, with an additional submis-
sion to the Open Track. The evaluation used
a range of automatically computed measures.
In addition, an evaluation experiment was car-
ried out using the peer outputs for the TUNA-
REG task. This report describes each task and
the evaluation methods used, and presents the
evaluation results.
1 Introduction
The TUNA Challenge 2008 built on the foundations
laid in the ASGRE 2007 Challenge (Belz and Gatt,
2007), which consisted of a single shared task, based
on a subset of the TUNA Corpus (Gatt et al, 2007).
The TUNA Corpus is a collection of human-authored
descriptions of a referent, paired with a represen-
tation of the domain in which that description was
elicited.
The 2008 Challenge expanded the scope of the
previous edition in a variety of ways. This year,
there were three shared tasks. TUNA-AS is the At-
tribute Selection task piloted in the 2007 ASGRE
Challenge, which involves the selection of a set of
attributes which are true of a target referent, and
help to distinguish it from its distractors in a do-
main. TUNA-R is a realisation task, involving the
mapping from attribute sets to linguistic descrip-
tions. TUNA-REG is an ?end to end? referring ex-
pression generation task, involving a mapping from
an input domain to a linguistic description of a target
referent. In addition, there was an Open Submission
Track, where participants were invited to submit a
report on any interesting research that involved the
shared task data, and an Evaluation Track, for which
submissions were invited on proposals for evalua-
tion methods. This year?s TUNA Challenge also ex-
panded considerably on the evaluation methods used
in the various tasks. The measures can be divided
into intrinsic, automatically computed methods, and
extrinsic measures obtained through a task-oriented
experiment involving human participants.
The training and development data for the Chal-
lenge included the full dataset used in the ASGRE
Challenge, that is, all of the 2007 training, develop-
ment and test data. For the 2008 edition, two new
test sets were constructed. Test Set 1 was used for
TUNA-R, Test Set 2 was used for both TUNA-AS and
TUNA-REG.
1.1 Overview of submissions
Overall, 8 research groups submitted 33 systems by
the deadline. Table 1 provides a summary of the sub-
missions. The extrinsic evaluation experiment was
carried out on peer outputs in the TUNA-REG task
only, using outputs from at most 4 systems per par-
ticipating group. The 10 systems included are indi-
cated in boldface in the table. An additional submis-
sion was made by the USP team to the Open Track.
No submissions were made to the Evaluation Track.
Given the number of submissions, space restrictions
do not permit us to give an overview of the charac-
teristics of the various systems; these can be found
in the reports authored by each participating group,
which are included in this volume.
198
Group Organisation TUNA-AS TUNA-R TUNA-REG
ATT AT&T Labs Research Inc.
ATT-DR-b ATT-R ATT-TemplateS-ws
ATT-DR-sf ATT-TemplateS-drws
ATT-FB-f ATT-Template-ws
ATT-FB-m ATT-Template-drws
ATT-FB-sf ATT-PermuteRank-ws
ATT-FB-sr ATT-PermuteRank-drws
ATT-Dependency-drws
ATT-Dependency-ws
DIT Dublin Institute of Technology
DIT-FBI DIT-CBSR DIT-FBI-CBSR
DIT-TVAS DIT-RBR DIT-TVAS-RBR
GRAPH University of Tilbug etc GRAPH-FP GRAPH-4+B?
IS University of Stuttgart IS-FP IS-GT IS-FP-GT
JUCSENLP Jadavpur University JU-PTBSGRE
NIL-UCM Universidad Complutense de Madrid NIL-UCM-MFVF NIL-UCM-BSC NIL-UCM-FVBS
OSU Ohio State University OSU-GP OSU-GP?
USP University of Sao Paolo USP-EACH-FREQ
Table 1: Overview of participating teams and systems, by task. TUNA-REG peer systems whose outputs were included
in the extrinsic, task-based evaluation are shown in boldface. Systems marked ? were submissions to TUNA-AS which
made use of the off-the-shelf ASGRE realiser for their entries to TUNA-REG.
Participants in TUNA-AS and TUNA-R were also
given the opportunity to submit peer outputs for
TUNA-REG, and having them included in the ex-
trinsic evaluation, by making the use of off-the-
shelf modules. For systems in TUNA-AS, we
made available a template-based realiser, written by
Irene Langkilde-Geary at the University of Brighton.
Originally used in the 2007 ASGRE Challenge, this
was re-used by some TUNA-AS participants to re-
alise their outputs. Systems which made use of this
facility are marked by a (*) in Table 1.
In the rest of this report, we first give an overview
of the tasks and the data used for the Challenge (Sec-
tion 2), followed by a description of the evaluation
methods (Section 3). Section 4 gives the compar-
ative evaluation results for each task, followed by
a few concluding remarks in Section 5. In what
follows, we will use the following terminology, in
keeping with their usage in Belz and Gatt (2007): a
peer system is a system submitted to the shared-task
challenge, while peer output is an attribute set or a
description (in the form of a word string) produced
by a peer system. We will refer to a description in
the TUNA corpus as a reference output.
2 Data and task overview
2.1 The TUNA Data
The TUNA corpus was constructed via an elicita-
tion experiment as part of the TUNA project1. Each
file in the data consists of a single pairing of a do-
main (representation of entities and their attributes)
and a human-authored description (reference output)
1http://www.csd.abdn.ac.uk/research/tuna/
<TRIAL CONDITION="+/-LOC" ID="...">
<DOMAIN>
<ENTITY ID="..." TYPE="target" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
<ENTITY ID="..." TYPE="distractor" IMAGE="...">
<ATTRIBUTE NAME="..." VALUE="..." />
...
</ENTITY>
...
</DOMAIN>
<WORD-STRING>
the string describing the target referent
</WORD-STRING>
<ANNOTATED-WORD-STRING>
the string in WORD-STRING annotated
with attributes in ATTRIBUTE-SET
</ANNOTATED-WORD-STRING>
<ATTRIBUTE-SET>
the set of domain attributes in the description
</ATTRIBUTE-SET>
</TRIAL>
Figure 1: Format of corpus items
which is intended to describe the target referent in
the domain. Only the singular descriptions in the
corpus were used for the TUNA Challenge.
The descriptions in the corpus are subdivided by
entity type: there are references to people, and refer-
ences to furniture items. In addition, the elicitation
experiment manipulated a single condition, ?LOC.
In the +LOC condition, experimental participants
were told that they could refer to entities using any
of their properties, including their location. In the
?LOC condition, they were discouraged from doing
so, though not prevented.
Figure 1 is an outline of the XML format used in
the Challenge. Each file has a root TRIAL node
with a unique ID and an indication of the experi-
mental condition. The DOMAIN node subsumes 7
199
ENTITY nodes, which themselves subsume a num-
ber of ATTRIBUTE nodes defining the properties of
an entity in attribute-value notation. The attributes
include properties such as an object?s colour or a
person?s clothing, and the location of the image in
the visual display which the DOMAIN represents.
Each ENTITY node indicates whether it is the target
referent or one of the six distractors, and also has a
pointer to the image that it represents. Images were
made available to the TUNA Challenge participants.
The WORD-STRING is the actual de-
scription typed by a human author, and the
ATTRIBUTE-SET is the set of attributes belonging
to the referent that the description includes. The
ANNOTATED-WORD-STRING node was only
provided in the training and development data,
to display how substrings of a human-authored
description were mapped to attributes to determine
the ATTRIBUTE-SET.
Training and development data: For the TUNA
Challenge, the 780 singular corpus instances were
divided into 80% training data and 20% develop-
ment data. This data consists of all the training,
development and test data used in the 2007 ASGRE
Challenge.
Test data: Two new test sets were constructed by
replicating the original TUNA elicitation experi-
ment. The new experiment was designed to ensure
that each DOMAIN in the new test sets had two
reference outputs. Thus, this year?s corpus-based
evaluations are conducted against multiple instances
of each input DOMAIN. Both sets consisted of 112
items, divided equally into furniture and people
descriptions, sampled from both experimental
conditions (?LOC). Test Set 1 was used for the
TUNA-R Task. Participants in this task received a
version of the test set whose items consisted of a
DOMAIN node and an ATTRIBUTE-SET node.
There were 56 unique DOMAINs, each represented
twice in the test set, with two attribute sets from two
different human authors. Because each DOMAIN
and ATTRIBUTE-SET combination in this test
set is unique, the results for this task are reported
below over the whole of Test Set 1. Test Set 2
was used for the TUNA-AS and TUNA-REG Tasks.
For these tasks, the test items given to participants
consisted of a DOMAIN node only. There were
112 unique DOMAINs; the evaluations on these
tasks were conducted by comparing each peer
output to two different reference outputs for each
of these domains. Therefore, in the TUNA-AS and
TUNA-REG tasks, the data presented here averages
over the two outputs per DOMAIN.
2.2 The tasks
Task 1: Attribute Selection (TUNA-AS): The
TUNA-AS task focused on content determination for
referring expressions, and follows the basic prob-
lem definition used in much previous work in the
area: given a domain and a target referent, select a
subset of the attributes of that referent which will
help to distinguish it from its distractors. The inputs
for this task consisted of a TRIAL node enclosing a
DOMAIN node (a representation of entities and prop-
erties). A peer output was a TRIAL node enclos-
ing just an ATTRIBUTE-SET node whose children
were the attributes selected by a peer system for the
target entity.
Task 2: Realisation (TUNA-R): The TUNA-R task
focussed on realisation. The aim was to map an
ATTRIBUTE-SET node to a word string which de-
scribes the ENTITY that is marked as the target such
that the entity can be identified in the domain. The
inputs for this task consisted of a TRIAL node en-
closing a DOMAIN and an ATTRIBUTE-SET node.
A peer output for this task consisted of a TRIAL
node enclosing just a WORD-STRING node.
Task 3: ?End-to-end? Referring Expression Gen-
eration (TUNA-REG): For the TUNA-REG task, the
input consisted of a DOMAIN, and a peer output was
a word string which described the entity marked as
the target such that the entity could be identified in
the domain. The input for this task was identical to
that for TUNA-AS, i.e. a TRIAL node enclosing just
a DOMAIN node. A peer output for this task was
identical in format to that for the TUNA-R task, i.e. a
TRIAL enclosing just a WORD-STRING node.
3 Evaluation methods
The evaluation methods used in each task, and the
quality criteria that they assess, are summarised in
Table 2. Peer outputs from all tasks were evalu-
ated using intrinsic methods. All of these were au-
tomatically computed, and are subdivided into (a)
200
Task Criterion Type Methods
TUNA-AS Humanlikeness Intrinsic Accuracy, Dice, MASI
Minimality Intrinsic Proportion of minimal outputs
Uniqueness Intrinsic Proportion of unique outputs
TUNA-R Humanlikeness Intrinsic Accuracy, BLEU, NIST, string-edit distance
TUNA-REG Humanlikeness Intrinsic Accuracy, BLEU, NIST string-edit distance
Ease of comprehension Extrinsic Self-paced reading in identification experiment
Referential Clarity Extrinsic Speed and accuracy in identification experiment
Table 2: Evaluation methods used per task
those measures that assess humanlikeness, i.e. the
degree of similarity between a peer output and a ref-
erence output; and (b) measures that assess intrin-
sic properties of peer outputs. Peer outputs from
the TUNA-REG task were also included in a human,
task-oriented evaluation, which is extrinsic insofar
as it measures the adequacy of a peer output in terms
of its utility in an externally defined task. In the re-
mainder of this section, we summarise the properties
of the intrinsic methods. Section 3.1 describes the
experiment conducted for the extrinsic evaluation.
Dice coefficient (TUNA-AS): This is a set-
comparison metric, ranging between 0 and 1, where
1 indicates a perfect match between sets. For two
attribute sets A and B, Dice is computed as follows:
Dice(A,B) =
2? |A ?B|
|A|+ |B|
(1)
MASI (TUNA-AS): The MASI score (Passonneau,
2006) is an adaptation of the Jaccard coefficient
which biases it in favour of similarity where one set
is a subset of the other. Like Dice, it ranges between
0 and 1, where 1 indicates a perfect match. It is com-
puted as follows:
MASI(A,B) = ? ?
|A ?B|
|A ?B|
(2)
where ? is a monotonicity coefficient defined as fol-
lows:
? =
?
???
???
0 if A ?B = ?
1 if A = B
2
3 if A ? B or B ? A
1
3 otherwise
(3)
Accuracy (all tasks): This is computed as the pro-
portion of the peer outputs of a system which have
an exact match to a reference output. In TUNA-AS,
Accuracy was computed as the proportion of times a
system returned an ATTRIBUTE-SET identical to
the reference ATTRIBUTE-SET produced by a hu-
man author for the same DOMAIN. In TUNA-R and
TUNA-REG, Accuracy was computed as the propor-
tion of times a peer WORD-STRING was identical
to the reference WORD-STRING produced by an au-
thor for the same DOMAIN.
String-edit distance (TUNA-R, TUNA-REG): This
is the classic Levenshtein distance measure, used to
compare the difference between a peer output and a
reference output in the corpus, as the minimal num-
ber of insertions, deletions and/or substitutions of
words required to transform one string into another.
The cost for insertions and deletions was set to 1,
that for substitutions to 2. Edit distance is an integer
bounded by the length of the longest description in
the pair being compared.
BLEU (TUNA-R, TUNA-REG): This is an n-gram
based string comparison measure, originally pro-
posed by Papineni et al (2002) for evaluation of
Machine Translation systems. It evaluates a system
based on the proportion of word n-grams (consid-
ering all n-grams of length n ? 4 is standard) that
it shares with several reference translations. Unlike
Dice, MASI and String-edit, BLEU is by definition an
aggregate measure (i.e. a single BLEU score is ob-
tained for a system based on the entire set of items
to be compared, and this is generally not equal to the
average of BLEU scores for individual items). BLEU
ranges between 0 and 1.
NIST (TUNA-R, TUNA-REG): This is a version of
BLEU, which gives more importance to less frequent
(hence more informative) n-grams. The range of
NIST scores depends on the size of the test set. Like
BLEU, this is an aggregate measure.
Uniqueness (TUNA-AS): This measure was in-
cluded for backwards comparability with the ASGRE
Challenge 2007. It is defined as the proportion of
peer ATTRIBUTE-SETs which identify the target
referent uniquely, i.e. whose (logical conjunction of)
201
attributes are true of the target, and of no other entity
in the DOMAIN.
Minimality (TUNA-AS): This measure was defined
as the proportion of peer ATTRIBUTE-SETs which
are minimal, where ?minimal? means that there is
no attribute-set which uniquely identifies the target
referent in the domain which is smaller. Note that
this definition includes Uniqueness as a prerequisite,
since the description must identify the target entity
uniquely in order to qualify for Minimality.
All intrinsic evaluation methods except for BLEU
and NIST were computed (a) overall, using the entire
test data set (i.e. Test Set 1 or 2 as appropriate); and
(b) by object type, that is, computing separate values
for outputs referring to targets of type furniture and
people.
3.1 Extrinsic evaluation in TUNA-REG
The experiment for the extrinsic evaluation of
TUNA-REG peer outputs combined a self-paced
reading and identification paradigm, comparing the
peer outputs from 10 of the TUNA-REG systems
shown in Table 1, as well as the two sets of human-
authored reference outputs for Test Set 2. We refer
to the latter as HUMAN-1 and HUMAN-2 in what fol-
lows2.
In the task given to experimental subjects, a trial
consisted of a description paired with a visual do-
main representation corresponding to an item in Test
Set 2. Each trial was split into two phases: (a) in an
initial reading phase, subjects were presented with
the description only. This phase was terminated by
subjects once they had read the description. (b) In
the second, identification phase, subjects saw the vi-
sual domain in which the description had been pro-
duced, consisting of images of the domain entities
in the same spatial configuration as that in the test
set DOMAIN. They clicked on the object that they
thought was the intended referent of the description
they had read.
The experiment yielded three dependent mea-
sures: (a) reading time (RT), measured from the
point at which the description was presented, to the
2Note that HUMAN-1 and HUMAN-2 were both sets of de-
scriptions randomly sampled from the data collected in the ex-
periment. Each set of human descriptions contains output from
different human authors.
point at which a participant called up the next screen
via mouse click; (b) identification time (IT), mea-
sured from the point at which pictures (the visual
domain) were presented on the screen to the point
where a participant identified a referent by clicking
on it; (c) error rate (ER), the proportion of times the
wrong referent was identified.
This design differs from that used in the 2007
ASGRE Challenge, in which descriptions and visual
domains were presented in a single phase (on the
same screen), so that RT and IT were conflated. The
new experiment replicates the methodology reported
in Gatt and Belz (2008), in a follow-up study on
the ASGRE 2007 data. Another difference between
the two experiments is that the current one is based
on peer outputs which are themselves realisations,
whereas the ASGRE experiment involved attribute
sets which had to be realised before they could be
used.
Design: We used a Repeated Latin Squares design,
in which each combination of SYSTEM3 and test set
item is allocated one trial. Since there were 12 lev-
els of SYSTEM, but 112 test set items, 8 randomly
selected items (4 furniture and 4 people) were du-
plicated, yielding 120 items and 10 12 ? 12 latin
squares. The items were divided into two sets of 60.
Half of the participants did the first 60 items (the first
5 latin squares), and the other half the second 60.
Participants and procedure: The experiment was
carried out by 24 participants recruited from among
the faculty and administrative staff of the Univer-
sity of Brighton, as well as from among the au-
thors? acquaintances. Participants carried out the
experiment under supervision in a quiet room on a
laptop. Stimulus presentation was carried out us-
ing DMDX, a Win-32 software package for psy-
cholinguistic experiments involving time measure-
ments (Forster and Forster, 2003). Participants initi-
ated each trial, which consisted of an initial warning
bell and a fixation point flashed on the screen for
1000ms. They then read the description and called
up the visual domain to identify the referent. Trials
timed out after 15000ms.
Treatment of outliers and timeouts: Trials which
3The SYSTEM independent variable in this experiment in-
cludes HUMAN-1 and HUMAN-2.
202
timed out with no response were discounted from
the analysis. Out of a total of (24 ? 60 =) 1440
trials, there were 4 reading timeouts (0.3%) and 7
identification timeouts (0.5%). Outliers for RT and
IT were defined as those exceeding a threshold of
mean ?2SD. There were 64 outliers on RT (4.4%)
and 191 on IT (13.3%). Outliers were replaced by
the overall mean for RT and IT (see Ratliff (1993)
for discussion of this method).
4 Evaluation results
This section presents results for each of the tasks.
For all measures, except BLEU and NIST, we present
separate descriptive statistics by entity type (people
vs. furniture subsets of the relevant test set), and
overall.
4.1 Results for TUNA-AS
Descriptive statistics are displayed for all systems in
Table 3. This includes the Accuracy and Minimal-
ity scores (proportions), and mean MASI and Dice
scores. Values are displayed by entity type and over-
all. The standard deviation for Dice and MASI is
displayed overall. Scores average over both sets of
reference outputs in Test Set 2. All systems scored
100% on Uniqueness, and either 0 or 100% on Min-
imality. These measures are therefore not included
in the significance testing, though Minimality is in-
cluded in the correlations reported below.
Two 15 (SYSTEM) ? 2 (ENTITY TYPE) uni-
variate ANOVAs were conducted on the Dice and
MASI scores. We report significant effects at p ?
.001. There were main effects of SYSTEM (Dice:
F (13, 1540) = 193.08; MASI: F (13, 1540) =
93.45) and ENTITY TYPE (Dice: F (1, 1540) =
91.75; MASI: F (1, 1540) = 168.12), as well
as a significant interaction between the two (Dice:
F (13, 1540) = 7.45, MASI: F (13, 1540) = 7.35).
Post-hoc Tukey?s comparisons on both Dice and
MASI yielded the homogeneous subsets displayed in
Table 4.
Differences among systems on Accuracy were
analysed by coding this as an indicator variable: for
each peer output, the variable indicated whether it
achieved perfect match with at least one of the two
reference outputs on the same DOMAIN. A Kruskall-
Wallis test showed that the difference between sys-
tems was significant (?2 = 275.01, p < .001).
Minimality Accuracy Dice MASI
Minimality -0.877 -0.959 -0.901
Accuracy -0.877 0.973 0.998
Dice -0.959 0.973 0.985
MASI -0.901 0.998 0.985
Table 5: Correlations for TUNA-AS; all values are signif-
icant at p ? .05
Pairwise correlations using Pearson?s r are shown
in Table 5, for all measures except Uniqueness. All
correlations are positive and significant, with the ex-
ception of those involving Minimality, which cor-
relates negatively with all other measures (i.e. the
higher the proportion of minimal descriptions of a
system, the lower its score on humanlikeness, as
measured by Dice, MASI and Accuracy). This re-
sult corroborates a similar finding in the 2007 AS-
GRE Challenge.
4.2 Results for TUNA-R
Table 6 shows descriptives for the 5 participating
systems in TUNA-R. Once again, mean Edit scores
and Accuracy proportions are shown both overall
and by entity type, while BLEU and NIST are overall
aggregate scores.
A 15 (SYSTEM) ? 2 (ENTITY TYPE) univariate
ANOVA was conducted on the Edit Distance scores.
There was no main effect of SYSTEM, and no in-
teraction, but ENTITY TYPE exhibited a main effect
(F (1, 550) = 19.99, p < .001). Given the lack of a
main effect, no post-hoc comparisons between sys-
tems were conducted. A Kruskall-Wallis test also
showed no difference between systems on Accu-
racy. Pairwise correlations between all measures are
shown in Table 7; this time, the only significant cor-
relation is between NIST and BLEU.
Edit Accuracy NIST BLEU
Edit 0.195 -0.095 0.099
Accuracy 0.195 0.837 0.701
NIST -0.095 0.837 0.900?
BLEU 0.099 0.701 0.900?
Table 7: Correlations for the TUNA-R task (? indicates
p ? .05).
4.3 Results for TUNA-REG
4.3.1 Tests on the intrinsic measures
Results for the intrinsic measures on the TUNA-
REG task are shown in Table 8. As in Section 4.1,
203
Dice MASI Accuracy Minimality
furniture people both SD furniture people both SD furniture people both both
GRAPH 0.858 0.729 0.794 0.160 0.705 0.465 0.585 0.272 0.53 0.56 0.40 0.00
JU-PTBSGRE 0.858 0.762 0.810 0.152 0.705 0.501 0.603 0.251 0.55 0.58 0.41 0.00
ATT-DR-b 0.852 0.722 0.787 0.154 0.663 0.441 0.552 0.283 0.52 0.54 0.36 0.00
ATT-DR-sf 0.852 0.722 0.787 0.154 0.663 0.441 0.552 0.283 0.50 0.52 0.36 0.00
DIT-FBI 0.850 0.731 0.791 0.153 0.661 0.451 0.556 0.280 0.50 0.53 0.36 0.00
IS-FP 0.828 0.723 0.776 0.165 0.641 0.475 0.558 0.278 0.52 0.54 0.37 0.00
NIL-UCM-MFVF 0.821 0.684 0.753 0.169 0.601 0.383 0.492 0.290 0.44 0.46 0.31 0.00
USP-EACH-FREQ 0.820 0.663 0.742 0.176 0.616 0.404 0.510 0.291 0.46 0.48 0.33 0.00
DIT-TVAS 0.814 0.684 0.749 0.166 0.580 0.383 0.482 0.285 0.43 0.46 0.29 0.00
OSU-GP 0.640 0.443 0.541 0.226 0.352 0.114 0.233 0.227 0.17 0.20 0.06 0.00
ATT-FB-m 0.357 0.263 0.310 0.245 0.164 0.119 0.141 0.125 0.13 0.14 0.00 1.00
ATT-FB-f 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00
ATT-FB-sf 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00
ATT-FB-sr 0.231 0.307 0.269 0.215 0.093 0.138 0.116 0.104 0.13 0.12 0.00 1.00
Table 3: Descriptives for the TUNA-AS task. All means are shown by entity type; standard deviations are displayed
overall.
Dice MASI
ATT-FB-f A ATT-FB-f A
ATT-FB-sf A ATT-FB-sf A
ATT-FB-sr A ATT-FB-sr A
ATT-FB-m A ATT-FB-m A B
OSU-GP B OSU-GP B
USP-EACH-FREQ C DIT-TVAS C
DIT-TVAS C NIL-UCM-MFVF C D
NIL-UCM-MFVF C USP-EACH-FREQ C D E
IS-FP C ATT-DR-b C D E
ATT-DR-b C ATT-DR-sf C D E
ATT-DR-sf C DIT-FBI C D E
DIT-FBI C IS-FP C D E
GRAPH C GRAPH D E
JU-PTBSGRE C JU-PTBSGRE E
Table 4: Homogeneous subsets for systems in TUNA-AS. Systems which do not share a common letter are significantly
different at p ? .05
Edit Accuracy NIST BLEU
furniture people both SD furniture people both both both
IS-GT 7.750 9.768 8.759 6.319 0.02 0.00 0.01 0.4526 0.0415
NIL-UCM-BSC 7.411 9.143 8.277 6.276 0.05 0.04 0.04 1.7034 0.0784
ATT-1-R 7.143 9.268 8.205 6.140 0.02 0.00 0.01 0.1249 0
DIT-CBSR 7.054 10.286 8.670 6.873 0.09 0.02 0.05 1.1623 0.0686
DIT-RBR 6.929 9.857 8.393 6.668 0.04 0.00 0.02 0.9151 0.0694
Table 6: Descriptives for the TUNA-R task.
Edit Accuracy BLEU NIST
furniture people both SD furniture people both both both
ATT-PermuteRank-ws 8.339 8.304 8.321 3.283 0.00 0 0 0.007 0.0288
ATT-Template-ws 8.304 8.161 8.232 3.030 0.00 0 0 0 0.0059
ATT-Dependency-ws 8.232 8.000 8.116 3.023 0.00 0 0 0.0001 0.0139
ATT-TemplateS-ws 8.214 8.161 8.188 3.063 0.00 0 0 0 0.0057
OSU-GP 7.964 13.232 10.598 4.223 0.00 0 0 1.976 0.0236
ATT-PermuteRank-drws 7.464 8.411 7.938 3.431 0.02 0.04 0.03 0.603 0.0571
DIT-TVAS-RBR 6.893 8.161 7.527 3.358 0.05 0 0.03 1.0233 0.0659
ATT-TemplateS-drws 6.786 7.679 7.232 3.745 0.07 0.02 0.04 0.6786 0.0958
ATT-Template-drws 6.768 7.696 7.232 3.757 0.07 0.02 0.04 0.6083 0.0929
NIL-UCM-FVBS 6.643 8.411 7.527 3.618 0.07 0.04 0.05 1.8277 0.0684
IS-FP-GT 6.607 7.304 6.955 3.225 0.05 0.02 0.04 0.8708 0.1086
DIT-FBI-CBSR 6.536 7.643 7.089 3.889 0.16 0.05 0.11 0.8804 0.1259
ATT-Dependency-drws 6.482 7.446 6.964 3.349 0.07 0 0.04 0.3427 0.0477
GRAPH 5.946 9.018 7.482 3.541 0.18 0 0.09 1.141 0.0696
Table 8: Descriptives for TUNA-REG on the intrinsic measures.
means for the intrinsic measures average over both
sets of reference outputs in Test Set 2.
A 15 (SYSTEM) ?2 (ENTITY TYPE) univariate
ANOVA was conducted on the Edit Distance scores.
There were significant main effects of SYSTEM
(F (13, 1540) = 8.6, p < .001) and ENTITY TYPE
(F (1, 1540) = 47.5, p < .001), as well as a signif-
icant interaction (F (13, 1540) = 5.77, p < .001).
A Kruskall-Wallis test on Accuracy, coded as an in-
dicator variable (see Section 4.2), showed that sys-
tems differed significantly on this measure as well
(?2 = 26.27, p < .05).
Post-hoc Tukey?s comparisons were conducted on
Edit Distance; the homogeneous subsets are shown
in Table 9. The table suggests that the main effect
of Edit Distance may largely have been due to the
204
difference between OSU-GP and all other systems.
Correlations between these measures are shown
in Table 10. Contrary to the results in Section 4.2,
the correlation between BLEU and NIST does not
reach significance. The negative correlations be-
tween Edit distance and Accuracy, and between Edit
and BLEU are as expected, since higher Edit cost im-
plies greater distance from a reference output.
IS-FP-GT A
ATT-Dependency-drws A
DIT-FBI-CBSR A
ATT-Template-drws A
ATT-TemplateS-drws A
GRAPH-4+B A
DIT-TVAS-RBR A
NIL-UCM-FVBS A
ATT-PermuteRank-drws A
ATT-Dependency-ws A
ATT-TemplateS-ws A
ATT-Template-ws A
ATT-PermuteRank-ws A
OSU-GP B
Table 9: Homogeneous subsets for systems in TUNA-
REG, Edit Distance measure. Systems which do not share
a common letter are significantly different at p ? .05
Edit Accuracy NIST BLEU
Edit -0.584? 0.250 -0.636?
Accuracy -0.584? 0.383 0.807??
NIST 0.250 0.383 0.371
BLEU -0.636? 0.807?? 0.371
Table 10: Correlations for TUNA-REG (? indicates p ?
.05; ?? indicates p ? .01).
4.3.2 Tests on the extrinsic measures
Table 11 displays the results for the extrinsic mea-
sures. Reading time (RT), identification time (IT)
and error rate (ER), are displayed only for the sys-
tems that participated in the evaluation experiment,
as well as for the two sets of reference outputs
HUMAN-1 and HUMAN-2.
Separate univariate ANOVAs were conducted test-
ing the effect of SYSTEM and ENTITY TYPE on IT
and RT. For IT, there was a significant main effect
of SYSTEM (F (11, 1409) = 5.66, p < .001) and
ENTITY TYPE (F (1, 1409) = 23.507, p < .001),
as well as a significant interaction (F (11, 1409) =
2.378, p < .05). The same pattern held for RT, with
a main effect of SYSTEM (F (11, 1412) = 9.95, p <
.001) and ENTITY TYPE (F (1, 1412) = 9.74, p <
.05) and a significant interaction (F (11, 1412) =
2.064, p < .05). A Kruskall-Wallis test conducted
on ER showed a significant impact of SYSTEM on the
extent to which experimental participants identified
the wrong referents (?2 = 35.45, p < .001). The
homogeneous subsets yielded by post-hoc Tukey?s
comparisons among systems, on both RT and IT, are
displayed in Table 12.
Finally, pairwise correlations were estimated be-
tween all three extrinsic measures. The only sig-
nificant correlation was between RT and IT (r =
.784, p < .05), suggesting that the longer experi-
mental subjects took to read a description, the longer
they also took to identify the target referent.
5 Conclusion
The first ASGRE Challenge, held in 2007, was re-
garded and presented as a pilot event, for a research
community in which there was growing interest in
comparative evaluation on shared datasets. Refer-
ring Expression Generation was an ideal starting
point, because of its relatively long history within
the NLG community, and the widespread agreement
on inputs, outputs and task definitions.
The tasks described and evaluated in this report
constitute a broadening of scope over the 2007 Chal-
lenge. Like the previous Challenge, the 2008 edition
emphasised diversity in terms of the measures of
quality used. This year, there was also an increased
emphasis on broadening the range of tasks, with the
inclusion of realisation and end-to-end referring ex-
pressions generation. This extends the scope of the
REG problem, which has traditionally been focussed
on content determination (attribute selection) for the
most part. As for evaluation, the diversity of mea-
sures can shed light on different aspects of quality
in these tasks. The fact that the correlation among
measures based on different quality criteria is not
straightforward is in itself an argument for maintain-
ing this diversity, particularly as comparative evalu-
ation exercises such as this one provide the oppor-
tunity for further investigation of the nature of these
relationships.
Another indicator of the growing diversity in this
year?s Challenge is the range of algorithmic solu-
tions in the three tasks, ranging from new models
based on classical algorithms, to data-driven meth-
ods, evolutionary algorithms, and graph- and tree-
based frameworks. The body of work represented
by submissions to the TUNA-R and TUNA-REG tasks
is also interesting for its exploration of how to apply
205
RT IT ER
furniture people both SD furniture people both SD furniture people both
HUMAN-1 2155.376 2187.737 2171.693 2036.462 1973.369 1911.742 1942.297 809.5139 11.864 6.780 9.322
OSU-GP 2080.532 3204.198 2637.644 1555.003 2063.441 2274.690 2167.275 682.8325 6.667 18.966 12.712
HUMAN-2 1823.553 2298.467 2061.010 1475.005 1873.621 1945.880 1909.750 761.3386 16.667 5.000 10.833
ATT-PremuteRank-drws 1664.911 1420.087 1543.528 1392.729 1765.731 1719.456 1742.788 675.3462 10.000 8.475 9.244
DIT-FBI-CBSR 1581.535 1521.799 1551.667 1170.031 1528.119 1932.806 1732.163 694.9878 10.169 10.000 10.084
NIL-UCM-FVBS 1561.291 1933.833 1747.562 1428.490 1531.378 1723.148 1627.263 672.9894 6.667 3.333 5.000
GRAPH 1499.582 1516.804 1508.193 952.158 1706.153 2026.268 1866.211 704.0210 5.000 5.000 5.000
DIT-TVAS-RBR 1485.149 1442.573 1463.861 998.332 1559.953 1734.853 1647.403 588.4615 8.333 13.333 10.833
ATT-Dependency-drws 1460.152 1583.887 1522.019 1177.817 1505.059 2078.336 1791.697 725.9459 1.667 18.333 10.000
ATT-TemplateS-drws 1341.245 1641.539 1490.130 1098.304 1656.401 1720.365 1687.841 650.8357 3.333 10.345 6.780
IS-FT-GT 1292.754 1614.712 1453.733 1374.652 1616.855 1884.557 1750.706 732.4362 6.667 1.667 4.167
ATT-PermuteRank-ws 1218.136 1450.603 1334.369 1203.975 1876.680 1831.485 1854.082 688.3493 31.667 13.333 22.500
Table 11: Descriptives for the extrinsic measures in TUNA-REG.
IT RT
NIL-UCM-FVBS A ATT-PermuteRank-ws A
DIT-TVAS-RBR A IS-FT-GT A
ATT-TemplateS-drws A B DIT-TVAS-RBR A
DIT-FBI-CBSR A B ATT-TemplateS-drws A
ATT-PremuteRank-drws A B GRAPH-4+B A B
IS-FT-GT A B ATT-Dependency-drws A B
ATT-Dependency-drws A B ATT-PremuteRank-drws A B
ATT-PermuteRank-ws A B DIT-FBI-CBSR A B
GRAPH-4+B A B NIL-UCM-FVBS A B C
HUMAN-2 A B C HUMAN-2 B C
HUMAN-1 B C HUMAN-1 C D
OSU-GP C OSU-GP D
Table 12: Homogeneous subsets for systems in TUNA-REG, extrinsic time measures. Systems which do not share a
common letter are significantly different at p ? .05
realisation techniques to the specific problem posed
by referring expressions.
The outcomes of this evaluation exercise are ob-
viously not intended to be a ?final word? on the right
way to carry out evaluation in referring expressions
generation. Rather, comparative results open up the
possibility of improvement and change. Another im-
portant aspect of a shared task of this nature is that
it results in an archive of data that can be further
exploited, either through follow-up studies, or for
the provision of baselines against which to compare
novel approaches. We have already used the data
from ASGRE 2007 for further investigation, particu-
larly in the area of extrinsic evaluation. We plan to
carry out more such studies in the future.
Acknowledgements
Our heartfelt thanks to the participants who helped
to make this event a success. Thanks to Advaith Sid-
dharthan, who proposed MASI for TUNA-AS.
References
A. Belz and A. Gatt. 2007. The attribute selection for
gre challenge: Overview and evaluation results. In
Proc. UCNLG+MT: Language Generation and Ma-
chine Translation.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy. Be-
havior Research Methods, Instruments, & Computers,
35(1):116?124.
A. Gatt and A. Belz. 2008. Attribute selection for refer-
ring expression generation: New algorithms and evalu-
ation methods. In Proceedings of the 5th International
Conference on Natural Language Generation (INLG-
08).
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proc. 11th
European Workshop on Natural Language Generation
(ENLG-07).
S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002.
Bleu: a. method for automatic evaluation of machine
translation. In Proc. 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-02), pages
311?318.
R. Passonneau. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic anno-
tation. In Proc. 5th International Conference on Lan-
guage Resources and Evaluation (LREC-06).
R. Ratliff. 1993. Methods for dealing with reaction time
outliers. Psychological Bulletin, 114(3):510?532.
206
Proceedings of the 12th European Workshop on Natural Language Generation, pages 162?164,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Generation Challenges 2009
162
Preface
Generation Challenges 2009 was the third round of shared-task evaluation
competitions (STECs) that involve the generation of natural language, and
followed the Pilot Attribute Selection for Generating Referring Expressions
Challenge in 2007 (ASGRE?07) and Referring Expression Generation Chal-
lenges in 2008 (REG?08). More information about all these NLG STEC ac-
tivities can be found via the links on the Generation Challenges homepage:
http://www.nltg.brighton.ac.uk/research/genchal09
Generation Challenges 2009 brought together four STECs: the TUNA Refer-
ring Expression Generation Task (TUNA-REG) organised by Albert Gatt, Anja Belz
and Eric Kow; the two GREC Challenges, GREC Main Subject Reference Genera-
tion (GREC-MSR) and GREC Named Entity Generation (GREC-NEG), organised by
Anja Belz, Eric Kow, Jette Viethen and Albert Gatt; and the Giving Instructions in
Virtual Environments Challenge (GIVE) organised by Donna Byron, Justine Cas-
sell, Robert Dale, Alexander Koller, Johanna Moore, Jon Oberlander, and Kristina
Striegnitz.
In the GIVE Challenge, participating teams developed systems which generate
natural-language instructions to users navigating a virtual 3D environment and per-
forming computer-game-like tasks. The four participating systems were evaluated
by measuring how quickly, accurately and efficiently users were able to perform
tasks with a given system?s instructions. The evaluation report for the GIVE Chal-
lenge can be found in this volume; the participants? reports will be made publicly
available at a later stage.
The TUNA-REG Task was the end-to-end referring expression generation task
(combining the attribute selection and realisation subtasks) which was first intro-
duced in REG?08, and which used the TUNA corpus of paired descriptions and
pictures of entities. This year?s TUNA-REG Task had an open call for participation,
but it was also organised in the spirit of a progress check which would give partic-
ipants from TUNA-REG?08 an opportunity to submit improved systems, the results
for which could be compared to last year?s results. Of five registered teams from
five countries, four teams submitted a total of 6 systems to TUNA-REG. These,
along with two sets of human outputs, were evaluated by automatic intrinsic and
human-based intrinsic and extrinsic evaluations. The results report and the partici-
pants? reports can be found in this volume.
The GREC-MSR Task was the same as in REG?08 and used a corpus of intro-
ductory sections from Wikipedia articles on geographic entities and people. The
task was to generate referring expressions for mentions of the main subject of the
article in the context of the full text of the article. The new GREC-NEG Task used
a separate corpus of introductory sections from Wikipedia articles on people, and
the task was to generate referring expressions for all mentions of all people in an
article.
Eight teams from seven countries registered for each of the GREC-MSR and
GREC-NEG tasks. As the system submission deadline approached, it became clear
that just two teams were certain that they were going to complete their systems in
time. For this reason, and also because of a moving camera-ready deadline, we de-
cided, after careful consideration and consultation with participants, to extend the
system development period for the GREC Tasks and to hold the GREC?09 results
163
meeting at the ACL-IJCNLP?09 Workshop on Language Generation and Summari-
sation in Singapore on 6 August 2009, and to publish all GREC?09 reports in the
proceedings of that workshop.
In addition to the four shared tasks, Generation Challenges 2009 offered (i) an
open submission track in which participants could submit any work involving the
data from any of the shared tasks, while opting out of the competetive element, (ii)
an evaluation track, in which proposals for new evaluation methods for the shared
task could be submitted, and (iii) a task proposal track in which proposals for new
shared tasks could be submitted. We believe that these types of open-access tracks
are important because they allow the wider research community to shape the focus
and methodologies of STECs directly. We received one submission in the open
submission track, involving the TUNA data, and none in the other tracks.
We successfully applied (with the help of support letters from many of last
year?s participants and other HLT colleagues) for funding from the Engineering
and Physical Sciences Research Council (EPSRC), the main funding body for HLT
in the UK. This support helped with all aspects of organising Generation Chal-
lenges 2009, and enabled us to create the new GREC-People corpus and to carry
out extensive human evaluations, as well as to employ a dedicated research fellow
(Eric Kow) to help with all aspects of Generation Challenges 2009.
Preparations are already underway for a fourth NLG shared-task evaluation
event next year, Generation Challenges 2010, which is likely to include a further
run of the GREC-NEG Task with an extended training/development corpus, a new
task which links GREC-NEG to a named-entity recognition preprocessing stage, and
a second run of the GIVE Challenge. We are hoping that results will be presented
at INLG?10.
Like our previous STECs, Generation Challenges 2009 would not have been
possible without the contributions of many different people. Wewould like to thank
the faculty and staff of Brighton University, and the students of UCL, Brighton
and Sussex Universities who participated in the evaluation experiments as well
as all other participants in our online data elicitation and evaluation exercises; the
ENLG?09 organisers, Mariet Theune and Emiel Krahmer; the research support team
at Brighton University and the EPSRC for help with obtaining funding; and last but
not least, the participants in the shared tasks for making the most of the short
available time to build some very successful systems.
February 2009 Anja Belz and Albert Gatt
164
Textual Properties and Task Based Evaluation: Investigating the Role of
Surface Properties, Structure and Content.
Albert Gatt
Institute of Linguistics
University of Malta
albert.gatt@um.edu.mt
Franc?ois Portet
Laboratoire d?Informatique de Grenoble
Grenoble Institute of Technology
francois.portet@imag.fr
Abstract
This paper investigates the relationship be-
tween the results of an extrinsic, task-
based evaluation of an NLG system and
various metrics measuring both surface
and deep semantic textual properties, in-
cluding relevance. The latter rely heav-
ily on domain knowledge. We show that
they correlate systematically with some
measures of performance. The core ar-
gument of this paper is that more domain
knowledge-based metrics shed more light
on the relationship between deep semantic
properties of a text and task performance.
1 Introduction
Evaluation methodology in NLG has generated a
lot of interest. Some recent work suggested that
the relationship between various intrinsic and ex-
trinsic evaluation methods (Spa?rck-Jones and Gal-
liers, 1996) is not straightforward (Reiter and
Belz, 2009; Gatt and Belz, to appear), leading to
some arguments for more domain-specific intrin-
sic metrics (Foster, 2008). One reason why these
issues are important is that reliable intrinsic eval-
uation metrics that correlate with performance in
an extrinsic, task-based setting can inform system
development. Indeed, this is often the stated pur-
pose of evaluation metrics such as BLEU (Papineni
et al, 2002) and ROUGE (Lin and Hovy, 2003),
which were originally characterised as evaluation
?understudies?.
In this paper we take up these questions in the
context of a knowledge-based NLG system, BT-45
(Portet et al, 2009), which summarises medical
data for decision support purposes in a Neona-
tal Intensive Care Unit (NICU). Our extrinsic
data comes from an experiment involving com-
plex medical decision making based on automati-
cally generated and human-authored texts (van der
Meulen et al, 2009). This gives us the oppor-
tunity to directly compare the textual character-
istics of generated and human-written summaries
and their relationship to decision-making perfor-
mance. The present work uses data from an ear-
lier study (Gatt and Portet, 2009), which presented
some preliminary results along these lines for the
system in question. We extend this work in a num-
ber of ways. Our principal aim is to test the va-
lidity not only of general-purpose metrics which
measure surface properties of text, but also of met-
rics which make use of domain knowledge, in the
sense that they attempt to relate the ?deep seman-
tics? of the texts to extrinsic factors, based on an
ontology for the BT-45 domain.
After an overview of related work in section 2,
the BT-45 system, its domain ontology and the ex-
trinsic evaluation are described in section 3. The
ontology plays an important role in the evaluation
metrics presented in Section 5. Finally, the eval-
uation of the methods is presented in Section 6,
before discussing and concluding in Section 7.
2 Related Work
In NLG evaluation, extrinsic, task-based methods
play a significant role (Reiter et al, 2003; Karasi-
mos and Isard, 2004; Stock et al, 2007). De-
pending on the study design, these studies often
leave open the question of precisely which as-
pects of a system (and of the text it generates)
contribute to success or failure. Intrinsic NLG
evaluations often involve ratings of text quality
or responses to questionnaires (Lester and Porter,
1997; Callaway and Lester, 2002; Foster, 2008),
with some studies using post-editing by human ex-
perts (Reiter et al, 2005). Automatically com-
puted metrics exploiting corpora, such as BLEU,
NIST and ROUGE, have mainly been used in eval-
uations of the coverage and quality of morphosyn-
tactic realisers (Langkilde-Geary, 2002; Callaway,
2003), though they have recently also been used
for subtasks such as Referring Expression Gener-
ation (Gatt and Belz, to appear) as well as end-to-
end weather forecasting systems (Reiter and Belz,
2009). The widespread use of these metrics in
NLP partly rests on the fact that they are quick
and cheap, but there is controversy about their re-
liability both in MT (Calliston-Burch et al, 2006)
and summarisation (Dorr et al, 2005; Liu and Liu,
2008). As noted in Section 1, similar questions
have been raised in NLG. One of the problems
associated with these metrics is that they rely on
the notion of a ?gold standard?, which is not al-
ways precisely definable given multiple solutions
to the same generation, summarisation or transla-
tion task. These observations underlie recent de-
velopments in Summarisation evaluation such as
the Pyramid method (Nenkova and Passonneau,
2004), which in addition also emphasises content
overlap with a set of reference summaries, rather
than n-gram matches.
It is interesting to note that, with some excep-
tions (Foster, 2008), most of the methodologi-
cal studies on intrinsic evaluation cited here have
focused on ?generic? metrics (corpus-based au-
tomatic measures being foremost among them),
none of which use domain knowledge to quantify
those aspects of a text related to its content. There
is some work in Summarisation that suggests that
incorporating more knowledge improves results.
For example, Yoo and Song (Yoo et al, 2007)
used the Medical Subject Headings (MeSH) to
construct graphs representing the high-level con-
tent of documents, which are then used to clus-
ter documents by topic, each cluster being used to
produce a summary. In (Plaza et al, 2009), the
authors have proposed a summarisation method
based on WordNet concepts and showed that this
higher level representation improves the summari-
sation task.
The principal aim of this paper is to develop
metrics with which to compare texts using domain
knowledge ? in the form of the ontology used in
the BT-45 system ? and to correlate results to hu-
man decision-making performance. The resulting
metrics focus on aspects of content, structure and
relevance that are shown to correlate meaningfully
with task performance, in contrast to other, more
surface-oriented ones (such as ROUGE).
3 The BT-45 System
BT-45(Portet et al, 2009) was designed to gen-
erate a textual summary of 45 minutes of patient
data in a Neonatal Intensive Care Unit (NICU), of
the kind shown in Figure 1(a). The corresponding
summary for the same data shown in Figure 1(b)
is a two-step consensus summary written by two
expert neonatologists. These two summaries cor-
respond to two of the conditions in the task-based
evaluation experiment described below.
In BT-45, summaries such as Figure 1(a) were
generated from raw input data consisting of (a)
physiological signals measured using sensors for
various parameters (such as heart rate); and (b)
discrete events logged by medical staff (e.g. drug
administration). The system was based on a
pipeline architecture which extends the standard
NLG tasks such as document planning and mi-
croplanning with preliminary stages for data anal-
ysis and reasoning. The texts generated were de-
scriptive, that is, they kept interpretation to a min-
imum (for example, the system did not make di-
agnoses). Nor were they generated with a bias to-
wards specific problems or actions that could be
considered desirable for a clinician to take in a par-
ticular context.
Every stage of the generation process made use
of a domain-specific ontology of around 550 con-
cepts, an excerpt of which is shown in Figure 1(c).
The ontology classified objects of type EVENT
and ENTITY into several subtypes; for example,
a DRUG ADMINISTRATION is an INTERVENTION,
which means it involves an agent and a patient.
The ontology functioned as a repository of declar-
ative knowledge, on the basis of which produc-
tion rules were defined to support reasoning in or-
der to make abstractions and to identify relations
(such as causality) between events detected in the
data based on their ontological class and their spe-
cific properties. In addition to the standard IS-A
links, the ontology contains functional relation-
ships which connect events to concepts represent-
ing physiological systems (such as the respiratory
or cardiovascular systems); these are referred to as
functional concepts. For example, in Figure 1(c), a
FEED event is linked to NUTRITION, meaning that
it is primarily relevant to the nutritional system.
These links were included in the ontology follow-
ing consultation with a senior neonatal consultant
after the development of BT-45 was completed.
Their inclusion was motivated by the knowledge-
based evaluation metrics developed for the pur-
poses of the present study, and discussed further
Over the next 38 minutes T1 stayed at around
37.4.
By 13:33 TcPO2 had rapidly decreased to 2.7.
Previously HR had increased to 173.
By 13:35 there had been 2 successive desatura-
tions down to 56. Previously T2 had increased
to 35.5.
By 13:40 SaO2 had rapidly decreased to 79.
(a) BT-45 summary
At the start of the monitoring period: HR
baseline is 145-155, oxygen saturation is
99%, pO2 = 4.9 and CO2 = 10.3 Mean BP is
37-47; T1 and T2 are 37.3degC and 34.6degC
respectively.
At 13:33 there is a desaturation to 59%, which
is accompanied by a drop in pO2 to 1.3 and
a decrease in HR to 122. The blood pressure
rises toward the end of this episode to 49. These
parameters return to their baselines by 13:37.
(b) Human summary (c) Ontology excerpt
Figure 1: Excerpts from Human and BT-45 summaries, and ontology example.
in Section 5.
The task-based experiment to evaluate BT-45
was conducted off-ward and involved a group of
35 clinicians, who were exposed to 24 scenarios,
each covering approximately 45 minutes of patient
data, together with a short introductory text which
gave some background about the patient. The pa-
tient data was then presented in one of three condi-
tions: graphically using a time-series plot, and tex-
tually in the form of a consensus summary written
by human experts (H; Figure 1(b)) and one gener-
ated automatically by BT-45(C; Figure 1(a)). Like
the BT-45 texts, the H texts did not give interpre-
tations or diagnoses and every effort was made not
to bias a reader in favour of certain courses of ac-
tion. A Latin Square design was used to ensure
that each scenario was shown to an equal number
of participants in each condition, while no partici-
pant saw the same scenario in more than one con-
dition.
For each scenario, the task was to select one
or more appropriate clinical actions from a prede-
fined set of 18, one of which was ?no action?. Se-
lections had to be made within three minutes, after
which the scenario timed out. The same choice of
18 actions was given in each scenario s, but for
each one, two neonatal experts identified the sub-
sets of appropriate (APs), inappropriate/potentially
harmful (INAPs) and neutral actions. One of the
appropriate actions was also deemed to be the ?tar-
get?, that is, the most important action to take.
In three scenarios, the ?target? was ?no action?.
For each participant p and scenario s, the perfor-
mance score P ps was based on the proportion PAPs
of actions selected out of APs, and the proportion
PINAPs selected out of the set of inappropriate ac-
tions INAPs: P
p
s = PAPs ? PINAPs ? [?1, 1].
Overall, decision making in the H condition was
better (Ps = .45SD=.10) than either C (Ps =
.41SD=.13) or G (Ps = .40SD=.15). No sig-
nificant difference was found between the latter
two, but the H texts were significantly better than
the C texts, as revealed in a by-subjects ANOVA
(F (1, 31) = 5.266, p < 0.05). We also performed
a post-hoc analysis, comparing the proportions of
appropriate actions selected, PAP and that of inap-
propriate actions PINAP in the H and C conditions
across scenarios. In addition, we computed a dif-
ferent score SPAP, defined as the proportion of ap-
propriate actions selected by a participant within
a scenario out of the total number of actions se-
lected (effectively a measure of ?precision?). A
comparison between means for these three scores
obtained across scenarios showed no significant
differences.
In the analysis reported in Section 6, we com-
pare our textual metrics to both the global score
P as well as to these three other performance in-
dicators. In various follow-up analyses (van der
Meulen et al, 2009; Reiter et al, 2008), it was
found that the three scenarios in which the tar-
get action was ?no action? may have misled some
participants, insofar as this option was included
among a set of other actions, some of which were
themselves deemed appropriate or at least neutral
(in the sense that they could be carried out without
harming the patient). We shall therefore exclude
these scenarios from our analyses.
<P>
At 14:15 hours
<EVENT TYPE="HEEL_PRICK" ID="e11">
a heel prick is done.
</EVENT>
<EVENT TYPE="TREND" SOURCE="HR" DIRECTION="increasing" ID="e12">
The HR increases
</EVENT>
at this point and for 7 minutes from the start of this procedure
<EVENT CARDINALITY="3" SOURCE="SaO2" TYPE="ARTIFACT" ID="e13">
there is a lot of artefact in the oxygen saturation trace.
</EVENT>
</P>
<TREL ARG0="e11" ARG1="TIMESTAMP" RELATION="at" />
<TREL ARG0="e12" ARG1="e11" RELATION="starts" />
<TREL ARG0="e13" ARG1="e11" RELATION="starts" />
(a) Annotation (b) Normalised tree
Figure 2: Fragment of an annotated summary and normalised tree representation.
4 Corpus Annotation
For this study, we annotated the H and C texts
from our experiment using the ontology, in or-
der to make both their semantic content and struc-
ture explicit. Figure 2(a) shows an excerpt from
an annotated text. Every paragraph of the text is
marked up explicitly. All segments of the text cor-
responding to an ontology EVENT are marked up
with a TYPE (the name of the concept in the on-
tology) and other properties, such as DIRECTION
and SOURCE in the case of trends in physiolog-
ical parameters. The CARDINALITY attribute is
used to indicate that a single text segment abstracts
over several occurrences in the data; for example,
the statement about artefacts in the example corre-
sponds to three such episodes in the data.
In addition to events, the markup also includes
separate nodes for all the temporal (TREL) and
discourse (DREL) relations which are explicitly
mentioned in the text, typically using adverbial or
prepositional phrases or verbs of causality. Ev-
ery TREL and DREL points to two arguments and
has a RELATION attribute. In the case of a TREL,
the value is one of the temporal relations de-
fined by (Allen, 1983). For DRELs, values were
restricted to CAUSE and CONTRAST (Mann and
S.Thompson, 1988). One of the arguments of a
TREL can be a timestamp, rather than an event.
This is the case for the first sentence in the frag-
ment, where event e11 is specified as having oc-
curred at a specific time (at 14:15). By contrast,
r4 is a relation between e11 and e12, where
the RELATION is STARTS, indicating that the text
specifies that e11 is used by the author as the an-
chor point to specify the start of e12, as reflected
by the expression at this point.
The markup provided the basis on which many
of the metrics described in the following section
were computed. Based on the annotation, we
used a normalised structural representation of the
texts as shown in Figure 2(b), consisting of PARA-
GRAPH (P) nodes which subsume events and rela-
tions. Relations dominate their event arguments.
For example, the starts TREL holding between
e12 and e11 is represented by a STARTS node
subsuming the two events. In case an event is
dominated by more than one relation (for exam-
ple, it is temporally related to two events, as e11
is in Figure 2(a), we maintain the tree structure by
creating two copies of the event, which are sub-
sumed by the two relations. Thus, the normalised
tree representation is a ?compiled out? version of
the graph representing all events and their rela-
tions. The tree representation is better suited to
our needs, given the complexity of comparing two
graphs.
5 Metrics
The evaluation metrics used to score texts written
by domain experts and those generated by the BT-
45 system fall into three main classes, described
below.
Semantic content and structure To compare
both the content and the structure of texts, we used
three measures. The first quantifies the number
of EVENT nodes in an annotated text, defined as
?
e?E c, where E is the set of events mentioned,
and c is the value of the CARDINALITY attribute
of an event e ? E. Similarly, we computed the
number of temporal (TREL) and discourse (DREL)
relations mentioned in a text. We also used the
Tree Edit Distance metric to compute the distance
between the tree representations of the H and C
texts (see Figure 2(b)). This measure computes
the minimum number of node insertions, dele-
tions and substitutions required to transform one
tree into another and therefore takes into account
not only the content (events and relations) but
also its structural arrangement in the text. The
edit distance between two trees is computed using
the standard Levenshtein edit distance algorithm,
computed over a string that represents the preorder
traversal of the two trees, using a cost of 1 for in-
sertions and deletions, and 2 for substitutions.
N-gram overlap As a measure of n-gram over-
lap, we use ROUGE-n, which measures simple
n?gram overlap (in the present paper we use
n = 4). We also use ROUGE-SU, in which over-
lap is computed using skip-bigrams while also ac-
counting for unigrams that a text has in common
with its reference (in order to avoid bias against
texts which share several unigrams but few skip
bigrams).
Domain-dependent relevance metrics The
metrics described so far make use of domain
knowledge only to the extent that this is reflected
in the textual markup. We now consider a family
of metrics which are much more heavily reliant
on domain-specific knowledge structures and
reasoning. In our domain, the relevance of a
text in a given experimental scenario s can be
defined in terms of whether the events it mentions
have some relationship to the appropriate clinical
actions (APs). We attempt to model some aspects
of this using a weighting strategy and reasoning
rules.
Recall from Section 3 that fc?s represent the
various physiological systems to which an event
or action can be related. Therefore, each event e
mentioned in a text can be related to a set of pos-
sible actions using the functional concepts fc(e)
to which that event is linked in the ontology. Let
Es,t be the set of events mentioned in text t for
scenario s. An event e ? Es,t references an action
a iff FC(e) ? FC(a) 6= ?. Our hypothesis is that
an appropriate action is more likely to be taken if
there are events which reference it in the text ? that
is, if the text mentions things which are directly or
indirectly relevant to the action. For instance, if a
text mentions events related to the RESPIRATION
fc, a clinician might be more likely to make a de-
cision to manage a patient?s ventilation support.
It is worth emphasising that, since both the BT-
45 and human-authored texts were descriptive and
were not written or generated with the appropriate
actions in mind, the hypothesis that the relevance
of the content to the appropriate actions might in-
crease the likelihood of these actions being chosen
is far from a foregone conclusion.
Part of the novelty in this way of quantifying
relevance lies in its use of the knowledge (i.e. the
ontology) that is already available to the system,
rather than asking human experts to rate the rele-
vance of a text, a time-consuming process which
could be subject to experts? personal biases. How-
ever, this way of conceptualising relevance gener-
ates links to too many actions for one event. It is
often the case that an event, through its association
with a functional concept, references more than
one action, but not all of these are appropriate.
For example, a change in oxygen saturation can
be related to RESPIRATION, which itself is related
to several respiration-related actions in a scenario,
only some of which are appropriate. Clearly, rele-
vance depends not only on a physiological connec-
tion between an event and a phsiological system
(functional concept), but also on the context, that
is, the other events and their relative importance
in a given scenario. Another factor that needs to
be taken into account is the overall probability of
an action. Some actions are performed routinely,
while others tend to be associated with emergen-
cies (e.g. a nappy change is much more frequent
over all than resuscitating a patient). This means
that some actions ? even appropriate ones ? may
have been less likely to be selected even though
they were referenced by the text and were appro-
priate.
We prune unwarranted connections between
events and actions by taking into account (a) a pa-
tient?s current status (described in the text and in
the background information given to experimental
participants); (b) the fact that some actions have
much higher prior probabilities than others be-
cause they are performed more routinely; (c) the
fact that some events may be more important than
others (e.g. resuscitation is much more important
than a nappy change). Based on this, we define the
weight of an action a as follows:
Wa =
P
e?E
Pr(a)?e.importance
P
a?Ae
Pr(a)
P
e?E e.importance
(1)
Where E is the set of events in the text, Ae the
set of actions related to event e, e.importance ?
N+ the importance of the event e and Pr(a) the
prior probability of action a. All weights are nor-
malised so that the following inequalities hold:
X
a?Ae
Pr(a) ? e.importance
P
a?Ae
Pr(a)
= e.importance (2)
X
a?A
Wa = 1 (3)
where A is the set of all possible actions. The idea
is that an event e makes some contribution (pos-
sibly 0) to the relevance of some actions Ae, and
the total weight of the event is distributed among
all actions related to it using (a) the prior probabil-
ity Pr(a) of each action (the most frequent action
will have more weight) and (b) the importance of
the event. At the end of the process each action
would be assigned a score representing the accu-
mulated weights of the events, which is then nor-
malised, so that
?
a?A Wa = 1.
The prior probability in the equation is meant
to reflect our earlier observation that clinical ac-
tions differ in the frequency with which they are
performed and this may bias their selection. Pri-
ors were computed using maximum likelihood es-
timates from a large database containing exhaus-
tive annotations of clinical actions recorded by an
on-site research nurse over a period of 4 months in
a NICU, which contains a total of 43,889 records
of actions (Hunter et al, 2003).
The importance value in equation (1) is meant
to reflect the fact that events in the text do not
attract the attention of a reader to the same ex-
tent, since they do not have the same degree
of ?severity? or ?surprise?. We operationalise
this by identifying the superconcepts in the on-
tology (PATHOLOGICAL-FUNCTION, DISORDER,
SURGICAL-INTERVENTION, etc.) which could be
thought of as representing ?drastic? occurrences.
To these we added the concept of a TREND which
corresponds to a change in a physiological param-
eter (such as an increase in heart rate), based on the
rationale that the primary aim of NICU staff is to
keep a patient stable, so that any physiological in-
stability warrants an intervention. The importance
of events subsumed by these superconcepts was
then set to be three times that of ?normal? events.
Finally, we apply knowledge-based rules to
prune the number of actions Ae related to an event
e. As an example, a decision to intubate a baby
depends not only on events in the text which ref-
erence this action, but also on whether the baby is
already intubated. This can be assessed by check-
ing whether s/he is on CMV (a type of ventila-
tion which is only used after intubation). The rule
is represented as INTUBATE ? ?on(baby, CMV).
Although such rules are extremely rough, they do
help to prune inconsistencies.
Two scores were computed for both human
and computer texts using equation (1). RELs,t
is the sum of weights of actions referenced in
a text t for scenario s which are appropriate:
RELs,t =
?
a?Aap Wa. Conversely, IRRELs,t
quantifies the weights of actions referenced in t
for scenario s which are inappropriate: IRRELs,t =?
a?Ainap
Wa.
6 Results
In what follows, we report two-tailed Pearson?s r
correlations to compare our metrics to the three
performance measures discussed in Section 3: P ,
the global performance score; PAPP and PINAPP, the
proportion of appropriate (resp. inappropriate) ac-
tions selected from the subsets of in/appropriate
(resp. inappropriate) actions in a scenario; and
SPAPP, the proportion of appropriate actions se-
lected by a participant out of the set of actions se-
lected. The last three are included because they
shed light more directly on the extent to which
experimental participants chose correctly or incor-
rectly. In case a metric measures similarity or dif-
ference between texts, the correlation reported is
with the difference between the H scores and the C
scores. Where relevant, we also report correlations
with the absolute mean performance scores within
the H and/or C conditions. Correlations exclude
the three scenarios which had ?no action? as the
target appropriate action, though where relevant,
we will indicate whether the correlations change
when these scenarios are also included.
6.1 Content and Structure
Overall, the C texts mentioned significantly fewer
events than the H texts (t20 = 2.44, p = .05),
and also mentioned fewer temporal and discourse
relations explicitly (t20 = 3.70, p < .05). In
P (H-C) PAPP (H-C) SPAPP (H-C) PINAP (H-C)
Events (H-C) .43? .42? .02 -.09
Relations (H-C) .34 .30 0 -.15
Tree Edit .36 .33 .09 -.14
Table 1: Correlations between performance differences and content/structure measures. ?significant at
p = .05; ?approaches significance at p = .06
the case of the H texts, the number of events
and relations did not correlate significantly with
any of the performance scores. In the case of
the C texts, the number of relations mentioned
was significantly negatively correlated to PINAP
(r = ?.49, p < .05), and positively correlated
to SPAPP (r = .7, p < .001). This suggests
that temporal and discourse relations made texts
more understandable and resulted in more appro-
priate actions being taken. More unexpectedly, the
number of events mentioned was negatively cor-
related to PAPP (r = ?.53, p < .05) and to P
(r = ?.5, p < .05). This may have been due to the
C texts mentioning a number of events that were
relatively unimportant and/or irrelevant to the ap-
propriate actions.
Table 1 displays correlations between perfor-
mance differences between H and C, and differ-
ences in number of events and relations, as well
as Tree Edit Distance. The positive correlation
between the number of events mentioned and P
suggests that a larger amount of content in the
H texts is partially responsible for the difference
in decision-making accuracy by experimental par-
ticipants. This is further supported by the fact
that the correlation with the difference in PAPP ap-
proaches significance. It is worth noting that none
of these correlations are significant when means
from the three ?no action? scenarios are included
in the computation. This further supports our ear-
lier conclusion that these three scenarios are out-
liers. Somewhat surprisingly, Tree Edit Distance
does not correlate significantly with any of the per-
formance differences, though the correlations go
in the expected directions (positive in the case of
P , SPAPP and PAPP, negative in the case of PINAP).
This may be due to the high variance in the Edit
Distance scores (mean: 66.5; SD: 34.8).
Overall, these results show that differences in
both content and structure made the H texts supe-
rior and human texts did a much better job at ex-
plicitly relating events or situating them in time,
which is crucial for comprehension and correct
decision-making. This point has previously been
Absolute Scores (C) Differences (H-C)
P PAP PINAP SPAP P PAP PINAP SPAP
R-4 .33 .38 .2 -.03 -.19 -.2 -.01 -.1
R-SU -.03 -.02 .05 -.31 .04 .01 -.1 .13
Table 2: Correlations between ROUGE and perfor-
mance scores in the C condition. ?significant at
p = .05.
made in relation to the same data on the basis of a
qualitative study (Reiter et al, 2008).
6.2 N-gram Overlap
Correlations with ROUGE-4 and ROUGE-SU are
shown in Table 2 both for absolute performance
scores on the C texts, and for the differences be-
tween H and C. This is because ROUGE can be
interpreted in two ways: on the one hand, it mea-
sures the ?quality? of C texts relative to the ref-
erence human texts; on the other it also indicates
similarity between C and H.
There are no significant correlations between
ROUGE and any of our performance measures. Al-
though this leaves open the question of whether a
different set of performance measures, or a differ-
ent experiment, would evince a more systematic
covariation, the results suggest that it is not sur-
face similarity (to the extent that this is measured
by ROUGE) that is contributing to better decision
making. It is however worth noting that some cor-
relations with ROUGE-4, namely those involving
P and PAPP, do turn out significant when the ?no
action? scenarios are included. This turns out to
be solely due to one of the ?no action? scenar-
ios, which had a much higher ROUGE-4 score than
the others, possibly because the corresponding hu-
man text was comparatively brief and the number
of events mentioned in the two texts was roughly
equal (11 for the C text, 12 for the H text).
6.3 Knowledge Based Relevance Metrics
Finally, we compare our knowledge-based mea-
sures of the relevance of the content to appropri-
ate actions (REL) and to inappropriate actions (IR-
REL). The correlations between each measure and
Human (H) BT-45 (C)
P PAP PINAP SPAP P PAP PINAP SPAP
REL .14 .11 -.14 .60? .33 .24 -.49? .7?
IRREL -.25 -.22 .1 -.56? -.34 -.26 .43 -.62?
Table 3: Correlations between knowledge-based relevance scores and absolute performance scores in the
C and H conditions. ?significant at p ? .05.
the absolute performance scores in each condition
are displayed in Table 3.
The absolute scores in Table 3 show that both
REL and IRREL are significantly correlated to
SPAPP, the proportion of appropriate actions out
of the actions selected by participants. The cor-
relations are in the expected direction: there is a
strong tendency for participants to choose more
appropriate actions when REL is high, and the re-
verse is true for IRREL. In the case of the C texts,
there is also a negative correlation (as expected)
between REL and PINAP, though this is the only
one that reaches significance with this variable. It
therefore appears that the knowledge-based rele-
vance measures evince a meaningful relationship
with at least some of the more ?direct? measures of
performance (those assessing the relative prefer-
ence of participants for appropriate actions based
on a textual summary), though not with the global
preference score P . One possible reason for the
low correlations with the latter is that the two mea-
sures attempt to quantify directly the relevance of
the content units in a text to in/appropriate courses
of action; hence, they have a more direct relation-
ship to measures of proportions of the courses of
actions chosen.
7 Discussion and Conclusions
We conclude this paper with some observations
about the relative merit of different measures of
textual characteristics. ?Standard?, surface-based
measures such as (ROUGE) do not display any sys-
tematic relationship with our extrinsic measures
of performance, recalling similar observations in
the NLG literature (Gatt and Belz, to appear) and
in MT and Summarisation (Calliston-Burch et al,
2006; Dorr et al, 2005). Some authors have also
reported that ROUGE does not correlate well with
human judgements of NLG texts (Reiter and Belz,
2009). On the other hand, we do find some evi-
dence that the amount of content in texts, and the
extent to which they explicitly relate content el-
ements temporally and rhetorically, may impact
decision-making. The significant correlations ob-
served between the number of relations in a text
and the extrinsic measures are worth emphasis-
ing, as they suggest a significant role not only for
content, but also rhetorical and temporal structure,
something that many metrics do not take into ac-
count.
Perhaps the most important contribution of this
paper has been to emphasise knowledge-based as-
pects of textual evaluation, not only by measur-
ing content units and structure, but also by de-
veloping a motivated relevance metric, the cru-
cial assumption being that the utility of a sum-
mary is contingent on its managing to convey in-
formation that will motivate a reader to take the
?right? course of action. The strong correlations
between the relevance measures and the extent to
which people chose the correct actions (or more
accurately, chose more correct actions) vindicates
this assumption.
Some of the correlations which turned out not to
be significant may be due to ?noise? in the data, in
particular, high variance in the performance scores
(as suggested by the standard deviations for P
given in Section 3). They therefore do not war-
rant the conclusion that no relationship exists be-
tween a particular measure and extrinsic task per-
formance; nevertheless, where other studies have
noted similar gaps, the trends in question may be
systematic and general. This, however, can only
be ascertained in further follow-up studies.
This paper has investigated the relationship be-
tween a number of intrinsic measures of text qual-
ity and decision-making performance based on an
external task. Emphasis was placed on metrics
that quantify aspects of semantics, relevance and
structure. We have also compared generated texts
to their human-authored counterparts to identify
differences which can motivate further system im-
provements. Future work will focus on further ex-
ploring metrics that reflect the relevance of a text,
as well as the role of temporal and discourse struc-
ture in conveying the intended meaning.
References
J. F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Charles B. Callaway and James C. Lester. 2002.
Narrative prose generation. Artificial Intelligence,
139(2):213?252.
C. B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proc. IJCAI?03.
C. Calliston-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In Proc. EACL?06.
B. J. Dorr, C. Monz, S. President, R. Schwartz, and
D. Zajic. 2005. A methodology for extrinsic evalu-
ation of text summarization: Does ROUGE correlate?
In Proc. Workshop on Intrinsic and Extrinsic Evalu-
ation Measures.
M.E. Foster. 2008. Automated metrics that agree with
human judgements on generated output for an em-
bodied conversational agent. In Proc. INLG?08.
A. Gatt and A. Belz. to appear. Introducing shared task
evaluation to NLG: The TUNA shared task evalua-
tion challenges. In E. Krahmer and M. Theune, ed-
itors, Empirical Methods in Natural Language Gen-
eration. Springer.
A. Gatt and F. Portet. 2009. Text content and task
performance in the evaluation of a natural language
generation system. In Proc. RANLP?09.
J. Hunter, G. Ewing, L. Ferguson, Y. Freer, R. Logie,
P. McCue, and N. McIntosh. 2003. The NEONATE
database. In Proc. IDAMAP?03.
A. Karasimos and A. Isard. 2004. Multilingual eval-
uation of a natural language generation system. In
Proc. LREC?04.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG?02.
J.C. Lester and B.W. Porter. 1997. Developing and
empirically evaluating robust explanation genera-
tors: The KNIGHT experiments. Computational Lin-
guistics, 23(1):65?101.
C-Y Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. in.
In Proc. of HLT-NAACL?03.
F. Liu and Y. Liu. 2008. Correlation between rouge
and human evaluation of extractive meeting sum-
maries. In Proc. ACL?08.
W. C. Mann and S.Thompson. 1988. Rhetorical struc-
ture theory: Towards a functional theory of text or-
ganisation. Text, 8(3):243?281.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarisation: The Pyramid
method. In Proc. NAACL-HLT?04.
S. Papineni, T. Roukos, W. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proc. ACL?02.
L Plaza, A D??az, and P Gerva?s P. 2009. Auto-
matic summarization of news using wordnet concept
graphs. best paper award. In Proc. IADIS?09.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic generation
of textual summaries from neonatal intensive care
data. Artificial Intelligence, 173(7?8):789?816.
E. Reiter and A. Belz. 2009. An investigation into the
validity of some metrics for automatically evaluat-
ing Natural Language Generation systems. Compu-
tational Linguistics, 35(4):529?558.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessa-
tion letters. Artificial Intelligence, 144:41?58.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
E. Reiter, A. Gatt, F. Portet, and M. van der Meulen.
2008. The importance of narrative and other lessons
from an evaluation of an NLG system that sum-
marises clinical data. In Proc. INLG?08.
K. Spa?rck-Jones and J. R. Galliers. 1996. Evaluating
natural language processing systems: An analysis
and review. Springer, Berlin.
O. Stock, M. Zancanaro, P. Busetta, C. Callaway,
A. Krueger, M. Kruppa, T. Kuflik, E. Not, and
C. Rocchi. 2007. Adaptive, intelligent presen-
tation of information for the museum visitor in
PEACH. User Modeling and User-Adapted Interac-
tion, 17(3):257?304.
M. van der Meulen, R. H. Logie, Y. Freer, C. Sykes,
N. McIntosh, and J. Hunter. 2009. When a graph is
poorer than 100 words. Applied Cognitive Psychol-
ogy, 24(1):77?89.
I. Yoo, X. Hu, and I-Y Song. 2007. A coherent
graph-based semantic clustering and summarization
approach for biomedical literature and a new sum-
marization evaluation method. BMC Bioinformat-
ics, 8(9).
Preface
Generation Challenges 2010 was the fourth round of shared-task evaluation compe-
titions (STECs) that involve the generation of natural language; it followed the Pilot
Attribute Selection for Generating Referring Expressions Challenge in 2007 (AS-
GRE?07) and Referring Expression Generation Challenges in 2008 (REG?08), and
Generation Challenges 2009 (GenChal?09). More information about all these NLG
STEC activities can be found via the links on the Generation Challenges homepage
(http://www.nltg.brighton.ac.uk/research/genchal10).
Generation Challenges 2010 brought together three sets of STECs: the three
GREC Challenges, GREC Named Entity Generation (GREC-NEG), Named Entity
Reference Detection (GREC-NER), and Named Entity Reference Regeneration
(GREC-Full), organised by Anja Belz and Eric Kow; the Challenge on Generat-
ing Instructions in Virtual Environments (GIVE) organised by Donna Byron, Jus-
tine Cassell, Robert Dale, Alexander Koller, Johanna Moore, Jon Oberlander, and
Kristina Striegnitz; and the new Question Generation (QG) tasks, organised by
Vasile Rus, Brendan Wyse, Mihai Lintean, Svetlana Stoyanchev and Paul Piwek.
In the GIVE Challenge, participating teams developed systems which gener-
ate natural-language instructions to users navigating a virtual 3D environment and
performing computer-game-like tasks. The seven participating systems were eval-
uated by measuring how quickly, accurately and efficiently users were able to per-
form tasks with a given system?s instructions, as well as on subjective measures.
Unlike the first GIVE Challenge, this year?s challenge allowed users to move and
turn freely in the virtual environment, rather than in discrete steps, making the NLG
task much harder. The evaluation report for the GIVE Challenge can be found in
this volume; the participants? reports will be made available on the GIVE website
(http://www.give-challenge.org/research) at a later stage.
The GREC Tasks used the GREC-People corpus of introductory sections from
Wikipedia articles on people. In GREC-NEG, the task was to select referring ex-
pressions for all mentions of all people in an article from given lists of alternatives
(this was the same task as at GenChal?09). The GREC-NER task combines named-
entity recognition and coreference resolution, restricted to people entities; the aim
for participating systems is to identify all those types of mentions of people that
are annotated in the GREC-People corpus. The aim for GREC-Full systems was to
improve the referential clarity and fluency of input texts. Participants were free to
do this in whichever way they chose. Participants were encouraged, though not
required, to create systems which replace referring expressions as and where nec-
essary to produce as clear and fluent a text as possible. This task could be viewed
as combining the GREC-NER and GREC-NEG tasks.
The first Question Generation challenge consisted of three tasks: Task A re-
quired questions to be generated from paragraphs of texts; Task B required systems
to generate questions from sentences, and Task C was an Open Task track in which
any QG research involving evaluation could be submitted. At the time of going to
press, the QG tasks are still running; this volume contains a preliminary report from
the organisers.
In addition to the four shared tasks, Generation Challenges 2010 offered (i) an
open submission track in which participants could submit any work involving the
data from any of the shared tasks, while opting out of the competetive element, (ii)
an evaluation track, in which proposals for new evaluation methods for the shared
task could be submitted, and (iii) a task proposal track in which proposals for new
shared tasks could be submitted. We believe that these types of open-access tracks
are important because they allow the wider research community to shape the focus
and methodologies of STECs directly.
We received three submissions in the Task Proposals track: an outline proposal
for tasks involving language generation under uncertainty (Lemon et al); a pro-
posal for a shared task on improving text written by non-native speakers (Dale and
Kilgarriff); and a proposal for a surface realisation task (White et al).
Once again, we successfully applied (with the help of support letters frommany
of last year?s participants and other HLT colleagues) for funding from the Engineer-
ing and Physical Sciences Research Council (EPSRC), the main funding body for
HLT in the UK. This support helped with all aspects of organising Generation Chal-
lenges 2010, and enabled us to create the new GREC-People corpus and to carry out
extensive human evaluations, as well as to employ a dedicated research fellow (Eric
Kow) to help with all aspects of Generation Challenges 2010.
Preparations are already underway for a fifth NLG shared-task evaluation event
next year, Generation Challenges 2011, which is likely to include a further run of
the GIVE Task, a second run of the QG Challenge, and a pilot surface realisation
task. We expect that results will be presented at ENLG?11.
Just like our previous STECs, Generation Challenges 2010 would not have been
possible without the contributions of many different people. Wewould like to thank
the students of Oxford University, KCL, UCL, Brighton and Sussex Universities
who participated in the evaluation experiments, as well as all other participants in
our online data elicitation and evaluation exercises; the INLG?10 organisers, Ielka
van der Sluis, John Kelleher and Brian MacNamee; the research support team at
Brighton University and the EPSRC for help with obtaining funding; and last but
not least, the participants in the shared tasks themselves.
July 2010 Anja Belz, Albert Gatt and Alexander Koller
Proceedings of the 14th European Workshop on Natural Language Generation, pages 82?91,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
What and where: An empirical investigation of pointing gestures and
descriptions in multimodal referring actions
Albert Gatt
Institute of Linguistics
University of Malta
albert.gatt@um.edu.mt
Patrizia Paggio
Institute of Linguistics
University of Malta
patrizia.paggio@um.edu.mt
Abstract
Pointing gestures are pervasive in human
referring actions, and are often combined
with spoken descriptions. Combining ges-
ture and speech naturally to refer to objects
is an essential task in multimodal NLG
systems. However, the way gesture and
speech should be combined in a referring
act remains an open question. In particu-
lar, it is not clear whether, in planning a
pointing gesture in conjunction with a de-
scription, an NLG system should seek to
minimise the redundancy between them,
e.g. by letting the pointing gesture indi-
cate locative information, with other, non-
locative properties of a referent included
in the description. This question has a
bearing on whether the gestural and spo-
ken parts of referring acts are planned sep-
arately or arise from a common underly-
ing computational mechanism. This paper
investigates this question empirically, us-
ing machine-learning techniques on a new
corpus of dialogues involving multimodal
references to objects. Our results indi-
cate that human pointing strategies inter-
act with descriptive strategies. In partic-
ular, pointing gestures are strongly asso-
ciated with the use of locative features in
referring expressions.
1 Introduction
Referring Expression Generation (REG) is consid-
ered a core task in many NLG systems (Krahmer
and van Deemter, 2012). Typically, the REG task is
defined in terms of identification: a referent needs
to be unambiguously identified in a discourse, en-
abling the reader or listener to pick it out from
among its potential distractors. Most work in this
area has focused on algorithms that select the con-
tent for definite descriptions (Dale, 1989; Dale and
Reiter, 1995), or on the best form for a referring
expression given the discourse context, for exam-
ple, whether it should be a full definite description,
a reduced one, or a pronoun (McCoy and Strube,
1999; Callaway and Lester, 2002; Krahmer and
Theune, 2002).
Less attention has been payed to the role of
gestures in referring actions and the way these
can be coupled with discursive strategies for ref-
erent identification. This question becomes partic-
ularly important in the context of multimodal sys-
tems, for example, those involving embodied con-
versational agents, where the ?naturalness? of an
interaction hinges in part on the appropriate use
of embodied actions, including referring actions.
Multimodal strategies can also make communica-
tion more efficient. For example, Louwerse and
Bangerter (2010) found that the use of pointing
gestures resulted in significantly faster resolution
of ambiguous referring expressions; crucially, this
result was replicated when the pointing gesture
was artificially generated, rather than made by a
human.
Like human communicators, embodied agents
need the ability to plan multimodal referring acts,
combining both linguistic reference and pointing.
An important question is whether these two com-
ponents of a referring act should be planned in or-
der to minimise redundancy between them or not.
For example, given that a pointing gesture can ef-
ficiently locate a target referent in a visual do-
main, should an accompanying description avoid
mentioning locative properties, thereby minimis-
ing redundancy? This question is the main focus
of this paper. However, it bears on a deeper is-
sue, of relevance to the architecture of multimodal
systems (and the cognitive architectures whose be-
haviours such systems seek to emulate): Should
gestural and descriptive strategies be viewed as
separate (implying that a REG module can plan
its linguistic referring expressions more or less in-
82
dependently of whether a pointing gesture is also
used) or should they be viewed as tightly coupled?
If they are indeed coupled, are there any features
of a linguistic description (for example, an object?s
location) which are excluded when a pointing ges-
ture is used, or are linguistic features always re-
dundant with pointing?
The present paper addresses these questions in a
data-driven fashion, using a multimodal corpus of
dialogues collected specifically to study referring
actions at both the linguistic and gestural levels.
We focus on pointing (that is, deictic) gestures di-
rected at an intended referent (as opposed to, say,
iconic gestures) and investigate the extent to which
pointing interacts with linguistic means for refer-
ent identification. Following an overview of pre-
vious work on pointing and reference (Section 2)
and a description of the corpus (Section 3), we de-
scribe a number of machine-learning experiments
that address the main empirical question (Section
4), concluding with a discussion.
2 Background: Pointing and describing
There is a growing consensus in the psycholin-
guistic literature, especially following the work
of McNeill (McNeill, 1985), that gesture and
language share a number of underlying mental
processes and are therefore coupled to a signif-
icant degree. This view is in part based on the
observation that gestures are temporally coupled
with speech and contribute meaningfully to the
achievement of a communicative intention (Mc-
Neill and Duncan, 2000). For instance, in the ex-
ample below, extracted from our corpus (see Sec-
tion 3), a speaker identifies a landmark (composed
of a collection of five circles) on a map through a
combination of a pointing gesture and the mention
of the size and colour of the elements making up
the landmark.
(1) there?s a group of five large red ones [points]
In this case, the pointing gesture further con-
tributes to the communicative aim of identifying
the cluster of five objects, in tandem with the vi-
sual features mentioned in the description. Mc-
Neill?s proposal (McNeill and Duncan, 2000) is
that speech and gesture should be considered as
the joint outcome of the language production pro-
cess, rather than as outcomes of separate pro-
cesses. Various models have been proposed which
are more or less congruent with this view. For
example, de Ruiter (2000) proposes that the two
modalities are planned together at early stages of
conceptualisation during speech production, while
Kita and O?zyu?rek (2003) suggest that gestures are
planned by spatio-motoric processes which differ
from the planning of speech production, but inter-
act with it at particular points.
Recent computational work has also taken these
ideas on board. For example, Kopp et. al. (2008)
describe a system for the concurrent planning and
generation of gesture and speech, whose archi-
tecture is inspired by Kita and O?zyu?rek (2003)
and which makes use of ?multimodal concepts?
(inspired by McNeill?s ?growth points?) combin-
ing both propositional and visuo-spatial proper-
ties. This contrasts with earlier architectures, such
as that proposed by Andre? and Rist (1996), where
generation of text and gesture is undertaken by
separate modules communicating with a central
planner.
The idea that the planning of language is tightly
coupled with that of gesture raises the possibility
that the two modalities may overlap to different
degrees. Gesture may be completely redundant
with speech, or may encode aspects of the com-
municative intention that are not included in the
linguistic message itself. This raises an interesting
question for multimodal REG: are there features of
objects that tend to be mentioned in tandem with
a pointing gesture; if so, which are they? For ex-
ample, the reference in (1) mentions the size and
colour of the landmark, but not its location, pos-
sibly suggesting that the speaker relied on point-
ing to convey the ?where? of the target referent, as
opposed to the ?what?, which is conveyed by the
description. This, however, is not the case in the
example below, where pointing is accompanied by
a mention of the referent?s location.
(2) [...] the red ones directly to the left [...]
[points]
There are at least two views on the relationship
between pointing and describing (de Ruiter et al,
2012). On the one hand, the trade-off hypothesis
holds that the decision to use a pointing gesture de-
pends on the effort or ?cost? involved (the further
away from the speaker and the smaller a referent
is, the more costly it would be to point at it), com-
pared to the effort involved in describing a referent
linguistically.
On the other hand, pointing and (some aspects
of) describing might proceed hand in hand, so that
83
there is some degree of redundancy between the
two modalities. Under this view, pointing may be
chosen not based on (low) cost assessment but as
part of a specifically multimodal cognitive strat-
egy.
Evidence for the trade-off hypothesis is reported
by Bangerter (2004), who found that, as pointing
became easier in a task-oriented dialogue (because
the distance between the speaker and the referent
was shorter), there was a decrease in verbal effort,
as measured by the number of words produced, as
well as a decrease in the use of locative and visual
features such as colour. Piwek (2007) also found
that referring acts accompanied by pointing tended
to include descriptions containing fewer properties
than those which were not. These results are com-
patible with a view of the speaker/generator as es-
sentially seeking to minimise effort in the commu-
nicative act, adopting the easiest available strategy
that will not compromise communicative success
(Beun and Cremers, 1998).
Similar results are reported by van der Sluis and
Krahmer (2007), who model the trade-off hypoth-
esis in a multimodal REG algorithm based on the
graph-based framework of Krahmer et. al. (2003).
The algorithm chooses to use pointing gestures,
with various degrees of precision, depending on
their cost relative to that of features that can be
used in a linguistic description.
There is also evidence against the trade-off
model. Recent experimental work by de Ruiter
et. al. (2012) showed that the tendency for speak-
ers to point was unaffected by the difficulty of re-
ferring to an object using linguistic features, al-
though pointing did decrease with repeated refer-
ence to the same entities. Interestingly, the authors
observed a correlation between the rate of pointing
and the use of locative properties of objects. This
would appear to favour a model in which the lin-
guistically describable features of objects are dif-
ferentiated: speakers may be using locative prop-
erties and pointing together as part of a strategy to
identify the ?where? of an object. This is in line
with the observation by Louwerse and Bangerter
(2010) that, in visual domains, using pointing ges-
tures with locative expressions increases the speed
with which references are resolved.
The evidence from de Ruiter et. al would seem
to contradict the assumptions underlying current
multimodal REG models. As we have seen, van der
Sluis and Krahmer (van der Sluis and Krahmer,
2007) assume a trade-off between speech and ges-
ture. A similar assumption is made by Kranstedt
and Wachsmuth (2005), who view pointing ges-
turs as mainly concerned with the ?where? of an
object. Their algorithm, which underlies the plan-
ning of multimodal references by a virtual agent,
extends the Incremental Algorithm (Dale and Re-
iter, 1995) as follows. Given an object in a 3D
space, the algorithm first considers the possibil-
ity of producing an unambiguous pointing ges-
ture; failing this, a pointing gesture covering the
intended referent and some of its surrounding dis-
tractors may be planned. In the latter case, the al-
gorithm then integrates other features of the ob-
ject (e.g. its colour), in an effort to exclude the
distractors that remain within the scope of the am-
biguous point. One of the claims underlying this
model is that ?absolute? location, which is covered
by pointing, is given first preference after pointing
itself, with other features of a referent being con-
sidered afterwards, in a preference order that will
only use relative location if all other options (such
as colour) are exhausted.
In summary, the empirical evidence for the
relationship between pointing and describing is
mixed. While the view that the planning of lan-
guage in different modalities should be tightly
coupled has proven useful and productive, the pre-
cise way in which the two interact in a referring act
is still an open question, especially where the re-
lationship between location and the other features
of a target referent is concerned. In the remain-
der of this paper, we report on an empirical study
that used machine learning methods with a view to
establishing the relationship between descriptive
features and pointing in multimodal references.
Our study is not committed to a specific architec-
ture for multimodal reference planning; rather, our
aim is to establish whether pointing and describ-
ing can partly overlap in the information that they
convey about a referent. Specifically, we are in-
terested in whether the use of a description that
includes spatial or locative information excludes a
pointing gesture.
3 Corpus and data
The data used in this study comes from the MREDI
(Multimodal REference in DIalogue) corpus (van
der Sluis et al, 2008)2, a new collection of dia-
2We intend to make this corpus publicly available in the
near future.
84
Matcher's mapwithoutitinerary
Director's mapwith itinerary
Director Follower
Low screensto hide maps
(a) Experiment setup
START
1/5
16
9
2/1817
4/ 1514
12 7/11
8/13
10
16
16
16
16
14 141414
4/ 154
/ 15 4/ 154/ 15
12 12
12
12
1010
10 1
0
7/117/11
7/11 7/11
8/138/1
3 8/138/13
1/5
1/51/5
1/5
9
99 9
17 1717
17
2/18
2/18 2/18 2/183/63/63/63/63/6
(b) Group circles map (numbers indicate the order
in which landmarks are visited along the itinerary)
Figure 1: MREDI dialogue setup
Feature Name Definition Example
Visual
S Size mention of the target size the group of small circles
Sh Shape mention of the target shape the circles at the bottom
C Colour mention of the target colour The blue square near the red square
Deictic/anaphoric
I Identity Statement of identity between
the current and a previous or later target
the red square,
the same one we saw at number 5
D Deixis Use of a deictic reference those squares
Locative
RP Relative position Position of the target landmark relative
to another object on the map
the blue square
just below the red square
AP Absolute position Target position based on absolute
frame of reference
The blue circle down at the bottom
FP Path references References to non-targets on the
path leading to the target.
go east to the first tiny square,
past the blue one
DIR Directions Direction-giving. take a right, go across
and straight down
Action GZ Gaze Gaze at the shared map (boolean).
Point Pointing Use of a pointing gesture (boolean).1
Table 1: Features annotated in the dialogues. All features have frequency values, except for the Action
features, which are boolean.
logues elicited using a task similar to the Map-
Task (Anderson et al, 1991), in which a director
and a follower talked about a map displayed on a
wall in front of them, approximately 1 metre away.
Each also had a private copy of the map; the di-
rector?s map had an itinerary on it, and her task
was to communicate the itinerary to the follower,
who marked it on his own private map. Partici-
pants were free to interact using speech and ges-
ture, without touching the shared map or standing
up. They could see each other, but could not see
each other?s private maps. Figure 1(a) displays the
basic experimental setup.
The maps consisted of shapes (squares or cir-
cles), with a sequence of landmarks constituting
the itinerary (initially known only to the director).
The maps were designed to manipulate a number
of independent variables, in a balanced design:
? Cardinality The target destinations in the
itineraries were either individual landmarks
(in 2 of the maps) or sets of 5 landmarks with
the same attributes (e.g., all green squares);
? Visual Attributes: Targets on the itinerary
differed from their distractors ? the objects
in their immediate vicinity (the focus area)
? in colour, or in size, or in both colour and
size. The focus area was defined as the set of
objects immediately surrounding a target;
? Prior reference: Some of the targets were
visited twice in the itinerary;
? Shift of domain focus: Targets were located
near to or far away from the previous target.
Note that if two targets t1 and t2 were in the
near condition, then t1 is one of the distrac-
tors of t2 and vice versa.
Each participant dyad did all four maps (single-
ton squares and circles; group squares and circles),
85
in a pseudo-random order, alternating in the di-
rector/matcher role so that each was director for
two of the maps. Figure 1(b) displays the direc-
tor?s map consisting of group circles. Note that the
itinerary is marked by numbering the target land-
marks. Landmarks with two numbers are visited
twice (for example, the first landmark is marked
1, but is also marked 5, meaning that it is the first
and the fifth landmark in the itinerary). During the
experiment, the map was mounted on a wall and
blown up to A0 size; this significantly reduced the
impression of visual clutter.
Data was collected from 8 pairs of participants3.
In the present study, we focus exclusively on the
directors? utterances. These were transcribed and
split up according to the landmark to which they
corresponded. In case a landmark was described
over multiple turns in the dialogue, each turn was
annotated as a separate utterance. Utterances were
annotated with the features displayed in Table 1.
Broadly, features are divided into four types: (a)
Deictic/Anaphoric, pertaining to the use of de-
ictic demonstratives, and/or references to previ-
ously identified entities; (ii) Visual, that is, cor-
responding to a landmark?s perceptual properties;
(iii) Locative, involving a description of the ob-
ject?s location; and (iv) Action, pertaining to ges-
ture and gaze. All features are frequencies per
utterance, except for Action features, which are
boolean.
Feature Frequency Mean SD
S 510 0.23 0.48
Sh 252 0.10 0.40
C 603 0.30 0.50
I 249 0.10 0.40
D 375 0.17 0.43
RP 529 0.13 0.40
AP 293 0.13 0.40
FP 989 0.40 0.70
DIR 251 0.11 0.37
GZ 836
Point 370
Table 2: Descriptive statistics for features in the
corpus
The corpus consists of a total of 2255 director?s
3A number of other dialogues were recorded, but were not
included in the corpus because participants focused on their
own private maps and never used pointing gestures, making it
impossible to study the conditions under which such gestures
are produced.
utterances. The frequency of each feature in the
corpus, as well as the per-utterance mean and stan-
dard deviation (where relevant), are indicated in
Table 2; note that, with the exception of Action
features, all feature values are frequencies per ut-
terance.
Type No point (#) Point (#) Total
Group 907 201 1108
Singleton 978 169 1147
Total 1885 370 2255
Table 3: Frequency of occurrence of pointing ges-
tures relative to different object types.
As expected, linguistic features are much more
frequent than pointing gestures. In fact only
16.4% of the utterances in the corpus are accompa-
nied by pointing gestures. Previous studies, such
as that by Beun and Cremers (Beun and Cremers,
1998) report a higher incidence of pointing (48%
overall). Note, however, that Beun and Cremers
focussed exclusively on first mention descriptions
(which numbered 145 in all), while our corpus in-
cludes subsequent mentions, as well as multiple
consecutive references to the same object divided
over several utterances (which are counted sepa-
rately in our totals).
Table 3 shows frequency figures for the pointing
gestures in the corpus relative to the type of object
they refer to (group vs. singleton): in accordance
with the trade-off theory, which predicts that larger
objects should be easier to point at, we see a sig-
nificant difference (?2(1) = 4.769, p = 0.028)
between the two types, with more pointing occur-
ring with group objects (that is, in group maps).
4 Experiments
In much of the work discussed in Section 2, the
generation of pointing gestures is viewed as de-
pendent on physical characteristics of the refer-
ents, in other words on their being suitable for
pointing. This is especially true of work related
to the trade-off hypothesis, in which the costs of
pointing gestures are calculated as a function of
the referent object?s size and its distance from the
speaker. In the present paper, by contrast, we
are interested in investigating the relation between
pointing and linguistic means of referent identi-
fication. More specifically, we address the ques-
tion to what degree the different linguistic expres-
sions used by the speaker to refer to objects in
86
the MREDI dialogues, can be used to predict the
occurrence of pointing gestures. Note that this
question addresses the correlation between prop-
erties in a description and the occurrence of point-
ing, rather than the issue of how pointing and de-
scribing should be planned. Nevertheless, as we
have emphasised in Section 2, the question of co-
occurrence of the two referential strategies does
have a bearing on architectural issues.
A first set of experiments were run in order to
test the general trade-off hypothesis. We tested
a number of classifiers on the task of classifying
the binary feature point, given all the linguistic
features in the corpus. More specifically, the at-
tributes used for the classification were MapConfl,
DIR, RP, AP, FP, S, Sh, C, D, I, Point. They are
all explained and exemplified in Table 1 with the
exception of MapConfl, which indicates whether a
specific case in the data comes from a group or a
singleton map. This feature was included because,
as noted in the previous section, whether a target
landmark was a singleton or a group made a dif-
ference, presumably because groups are larger and
more visually salient. Note further that one of the
Action features, GZ (gaze), is ignored in the ex-
periments because it is an almost univocal predic-
tor of pointing. Indeed, gazing is involved roughly
every time Point has the value y (yes) (but not the
other way round).
The experiments were run using the Weka (Wit-
ten and Frank, 2005) tool, which gives access
to many different algorithms, and 10-fold cross-
validation was used throughout. The results are
shown in Table (4) in terms of Precision, Recall
and F-measure for each of the classifiers.
Classifier P R F
Baseline 1 (ZeroR) 0.699 0.836 0.761
Baseline 2 (OneR) 0.762 0.834 0.765
SMO 0.699 0.836 0.761
NaiveBayes 0.795 0.811 0.802
Logistic 0.806 0.84 0.808
J48 0.829 0.85 0.833
Table 4: Predicting pointing gestures given all the
linguistic features in the corpus: classification re-
sults.
Two baselines were created to evaluate the re-
sults. The first one is provided by the ZeroR clas-
sifier, which always chooses the most frequent
class, in this case n (no pointing gesture). The
F-measure obtained by this method is somewhat
high at 0.761, because there are relatively few
pointing gestures in the data. The second base-
line, which provides a slightly more interesting re-
sult against which to evaluate the other classifiers,
is provided by OneR. It achieves an F-measure of
0.765 by predicting a pointing gesture if DIR >=
2.5, in other words if there are at least 2.5 occur-
rences of direction expressions in the utterance.
Using this rule has the effect of predicting a few
of the pointing gestures, with an F-measure on the
y class (occurrence of pointing gestures) of 0.031.
The other four sets of results were obtained
by running four different classification algorithms
with the same set of attributes. Apart from SMO
(an algorithm using support vector machines), all
the classifiers perform better than the baseline.
The best results are produced by the decision tree
classifier J48, which obtains an overall F-measure
of 0.833, and an F-measure of 0.421 on the y class.
The confusion matrix generated by J48 on this
data-set is shown in Table (5)
a b ? classified as
1794 91 a = n
247 123 b = y
Table 5: Predicting pointing given all the linguistic
features in the corpus: confusion matrix.
The model created by the decision tree classi-
fier (J48) is quite complex (size=57 and no. of
leaves=29). The first branching, which corre-
sponds to no AP (Absolute Position) and no C
(Colour), assigns n to as many as 1571 instances
(with 115 errors). The tree is shown in Fig-
ure (2). The tree also shows that certain combina-
tions of features are more likely to be associated
with pointing gestures. These are predominantly
combinations including occurrences of AP, or, in
the absence of absolute position, combinations in-
cluding positive values for FP (Frequency of ref-
erence on Path) and DIR (Direction).
The maximum entropy model, built by the lo-
gistic regression algorithm (Logistic), shows sim-
ilar tendencies in that the attributes that are as-
signed the highest weights are AP, C and DIR.
These results confirm the general hypothesis
that there is a strong relationship between linguis-
tic features used in a description and pointing ges-
tures. Indeed, it is possible to predict pointing ges-
tures on the basis of the linguistic features used.
87
Figure 2: J48 decision tree
Classifier P R F Features
Exp1: J48 0.829 0.85 0.833 All features
Exp3: Logistic 0.806 0.84 0.808 Loc+D+I
Exp2: J48 0.835 0.851 0.806 MapConfl+Loc+D+I
Exp6: NaiveBayes 0.793 0.825 0.802 Loc
Exp4: NaiveBayes 0.764 0.804 0.779 MapConfl+Visual+D+I
Exp5: J48 0.761 0.808 0.777 MapConfl+Visual
Exp8: NaiveBayes 0.761 0.808 0.777 Visual
Exp9: NaiveBayes 0.761 0.801 0.775 Visual+D+I
Baseline 2: OneR 0.762 0.834 0.765 Dir
Exp7: F48 0.699 0.836 0.761 MapConfl+D+I
Baseline 1: ZeroR 0.699 0.836 0.761 Most freq class
Table 6: Predicting pointing gestures with different feature combinations: classification results.
In particular, the results suggest a difference be-
tween features that express locative properties and
those having to do with the visual description of
the same object (its colour, size and shape). More
specifically, it would seem that locative features
are more useful to the classifiers than visual prop-
erties.
To test this second hypothesis, we ran a series
of experiments where the task was still to predict
pointing gestures, but different subsets of the lin-
guistic features were tested one at the time. For
each feature combination, we run the classification
using J48, Naive Bayes and the Logistic regression
algorithm. In Table (6), we show the best result
obtained for each feature combination. The classi-
fiers are ordered from the most accurate to the least
accurate, and the combination of features used by
each of them is listed in the last column. The best
results and the two baselines from the previous set
of experiments are included for the sake of com-
parison. Note that the term Loc is used to refer to
all the locative attributes AP, DIR, RP, AP and FP,
88
while Visual refers to S, Sh and C.
The best results are those obtained when the
complete feature set is used in the training. How-
ever, the next best results are achieved by the clas-
sifiers using the locative features, either alone or
together with features concerning the map type,
identity with a previously mentioned object and
deictic reference, with an F-measure in the range
0.802?0.808. If visual features are used instead,
the F-measure is in the range 0.775?0.779. The
worst results are obtained if neither location nor
visual description are used. Thus, although the dif-
ferences between the best and the worst classifiers
are not dramatic, in this data we see a tendency for
the locative features to be slightly better predictors
of pointing gestures than features corresponding to
visual descriptions.
5 Discussion and conclusions
The automatic classification experiments de-
scribed above show that to a certain extent, the
pointing gestures occurring in the MREDI corpus
can be predicted based on the linguistic expres-
sions used by the speaker in conjunction with
pointing. More precisely, linguistic descriptions
can be used to predict about one third of the point-
ing gestures that speakers have produced in the
corpus. This is an interesting and novel result,
which not only supports the general notion that
gestures and speech should be seen as tightly cou-
pled, but also suggests that this coupling does not
result in a minimisation of redundancy between
the two modalities. Rather, it appears that a num-
ber of pointing gestures accompanied descriptions
containing locative properties, something that con-
tradicts the predictions of models based on the
trade-off hypothesis (Kranstedt and Wachsmuth,
2005; van der Sluis and Krahmer, 2007).
There are a number of limitations of the present
study, which we plan to address in future work.
First, pointing gestures in our corpus were rela-
tively scarce (16.4% of utterances were accompa-
nied by pointing). This in part explains the relative
accuracy of our baselines: predicting the major-
ity class (that is, no pointing) in every case will
clearly yield reasonable results given that the size
of the class is so large. On the other hand, the
relative scarcity of pointing may also indicate that
pointing is somewhat more costly than linguistic
description, in cognitive and physical terms. In
fact, the difference we see in the number of point-
ing gestures between singleton and group maps
also seems to confirm this assumption: in the
group maps, where objects are larger, and thus
more easily pointed at according to the trade-off
model, there are in fact significantly more pointing
gestures. The incidence of pointing may also have
been affected by the nature of the domains used:
although the shared maps in the experiments were
large and quite close to the interlocutors, the pres-
ence of objects of the same shape may have added
to the general visual clutter of the maps, making
pointing less likely.
Another aspect of the data that we have not
investigated is the presence of individual strate-
gies. We know that speakers differ a lot in their
use of gesturing as regards e.g. frequency, type
of gesture and representation techniques. Recent
models of gesture production for embodied agents
are taking such differences into account (Neff et
al., 2008; Bergmann and Kopp, 2009). Similarly,
some speakers might have a greater preference for
pointing than others. For example, Beun and Cre-
mers (1998) note that certain speakers in their cor-
pus explicitly stated that they had attempted to per-
form the task in their dialogues without pointing,
in spite of their having been told that they could
point. Recent data-driven experiments on referen-
tial descriptions by Dale and Viethen (Dale and Vi-
ethen, 2010), In a domain quite similar to the one
used here, suggest that speakers do indeed clus-
ter according to their preferred referential strat-
egy. Similar assumptions have informed REG al-
gorithms trained on the TUNA Corpus, in the con-
text of the Generation Challenges (Gatt and Belz,
2010) (Bohnet, 2008; Di Fabbrizio et al, 2008).
In future work, we plan to address this question
in a multimodal context, where results by Piwek
(2007) have already suggested that such individ-
ual strategies may play an important role.
The hypothesis that specific combinations of
pointing and linguistic descriptions (for example,
an object?s colour or size) can be excluded, is
clearly not borne out by the data. There is, how-
ever, a tendency for locative features to act as
stronger predictors of pointing gestures. Although
the trend is not very strong, it is an interesting
one since it confirms the experimental results by
de Ruiter et. al. reviewed earlier (de Ruiter et al,
2012). This may suggest that a pointing gesture
may ultimately be planned within the same system
as locative features (i.e. the decision of whether or
89
not to point is not dependent on the decision of
whether or not to describe inherent, visual proper-
ties of the object, but on whether the object?s lo-
cation is to be indicated). Another feature that is
worth exploring further is deixis, specifically the
difference between proximal and distal deictic ex-
pressions and their interaction with pointing ges-
tures. For example, Piwek et al (2007) found that
proximal deictic expressions tend to be associated
with a more intensive attentional focusing mecha-
nism, while Bangerter (2004) also observes an as-
sociation between pointing and the use of deictic
expressions.
From an NLG perspective, our results suggest
that decisions to generate a pointing gesture and
those to select visual attributes might take place
independently (perhaps in parallel, perhaps in dif-
ferent modules). From a cognitive perspective, it
suggests two types of interaction between atten-
tion/vision and language/gesture, related to the de-
scription of the ?what? of an object and its ?where?
(Landau and Jackendoff, 1993).
Finally, our study focused on the relationship
between the two modalities involved in a referen-
tial act, addressing the question of redundancy be-
tween them. We have not addressed the impact of
the visual properties of a target referent in relation
to its surrounding objects, on the choices speakers
make in these two modalities. This is a priority for
future work, given that the corpus was designed to
balance the presence or absence of various visual
properties of an object (see Section 3). Taking this
even further, it remains to be investigated, for ex-
ample, whether there would be interesting differ-
ences in the relationship betwene pointing and de-
scribing between 2D scenes of the kind used here,
and 3D environments of the sort used by Kranst-
edt and Wachsmuth (2005). Another priority is
to take into account the interactive nature of the
dialogues, with particular focus on the follower?s
feedback to the director, as an indicator of the suc-
cess of referential expressions. This is another as-
pect of the dialogue situation that may have an im-
pact on planning multimodal referential acts.
Acknowledgements
Special thanks are due to Ielka van der Sluis,
Adrian Bangerter and Paul Piwek, who were in-
volved in every step of the design, collection and
annotation of the MREDI corpus, and who also
commented on preliminary drafts of this paper.
References
A. Anderson, M. Bader, E. Bard, E. Boyle, G. M. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, C. Sotillo, H. S. Thompson, and R. Wein-
ert. 1991. The HCRC Map Task corpus. Language
and Speech, 34:351?366.
E. Andre? and T. Rist. 1996. Coping with temporal
constraints in multimedia presentation planning. In
Proceedings of the 13th National Conference on Ar-
tificial Intelligence (AAAI?96).
A. Bangerter. 2004. Using pointing and describing to
achieve joint focus of attention in dialogue. Psycho-
logical Science, 15(6):415?419.
K. Bergmann and S. Kopp. 2009. GNetIc - using
bayesian decision networks for iconic gesture gen-
eration. In A. Nijholt and H. Vilhja?lmsson, editors,
Proceedings of the 9th International Conference on
Intelligent Virtual Agents (LNAI 5773), pages 76?89.
Springer.
R.J. Beun and A. Cremers. 1998. Object reference in
a shared domain of conversation. Pragmatics and
Cognition, 6(1-2):121?152.
B. Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion (INLG?08).
C. Callaway and J. C. Lester. 2002. Pronominalization
in generated discourse and dialogue. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02).
R. Dale and E. Reiter. 1995. Computational interpre-
tation of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(8):233?
263.
R. Dale and J. Viethen. 2010. Attribute-centric re-
ferring expression generation. In E. Krahmer and
M. Theune, editors, Empirical Methods in Natu-
ral Language Generation, volume 5790 of LNAI.
Springer, Berlin and Heidelberg.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th annual meeting of the As-
sociation for Computational Linguistics (ACL?89),
pages 68?75.
J.P. de Ruiter, A. Bangerter, and P. Dings. 2012. The
interplay between gesture and speech in the produc-
tion of referring expressions: Investigating the trade-
off hypothesis. Topics in Cognitive Science, 4:232?
248.
J.P. de Ruiter. 2000. The production of gesture and
speech. In D. McNeill, editor, Language and Ges-
ture, pages 284?311. Cambridge University Press.
90
G. Di Fabbrizio, A. J. Stent, and S. Bangalore.
2008. Trainable speaker-based referring expression
generation. In Proceedings of the 12th Confer-
ence on Computational Natural Language Learning
(CONLL?08), pages 151?158.
A. Gatt and A. Belz. 2010. Introducing shared task
evaluation to nlg: The TUNA shared task evaluation
challenges. In E. Krahmer and M. Theune, editors,
Empirical Methods in Natural Language Genera-
tion. Springer.
S. Kita and A. O?zyu?rek. 2003. What does cross-
linguistic variation in semantic coordination of
speech and gesture reveal?: Evidence for an inter-
face representation of spatial thinking and speaking.
Journal of Memory and Language, 48:16?32.
S. Kopp, K. Bergmann, and I. Wachsmuth. 2008. Mul-
timodal communication from multimodal thinking:
Towards an integrated model of speech and gesture
production. International Journal of Semantic Com-
puting, 2(1):115?136.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Information
Sharing: Reference and Presupposition in Language
Generation and Interpretation. CSLI Publications,
Stanford.
E. Krahmer and K. van Deemter. 2012. Computational
generation of referring expressions: A survey. Com-
putational Linguistics, 38(1):173?218.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
A. Kranstedt and I. Wachsmuth. 2005. Incremental
generation of multimodal deixis referring to objects.
In Proceedings of the 10th European Workshop on
Natural Language Generation (ENLG?05).
B. Landau and R. Jackendoff. 1993. what and where in
spatial language and spatial cognition. Behavioral
and Brain Sciences, 16:217?238.
M. Louwerse and A. Bangerter. 2010. Effects of am-
biguous gestures and language on the time-course of
reference resolution. Cognitive Science, 34:1517?
1529.
K.F. McCoy and M. Strube. 1999. Generating
anaphoric expressions: Pronoun or definite descrip-
tion? In Proceedings of the Workshop on the Rela-
tion of Discourse/Dialogue Structure and Reference.
D. McNeill and S.D. Duncan. 2000. Growth points in
thinking for speaking. In D. McNeill, editor, Lan-
guage and Gesture, pages 141?161. Cambridge Uni-
versity Press.
D. McNeill. 1985. So you think gestures are nonver-
bal? Psychological Review, 92(3):350?371.
M. Neff, M. Kipp, I. Albrecht, and H.-P. Seidel. 2008.
Gesture modeling and animation based on a proba-
bilistic recreation of speaker style. ACM Transac-
tions on Graphics, 27(1):1?24.
P. Piwek, R-J. Beun, and A. Cremers. 2007. proximal
and distal in language and cognition: Evidence from
deictic demonstratives in dutch. Journal of Prag-
matics, 40(4):694?718.
P. Piwek. 2007. Modality choice for generation of re-
ferring acts: Pointing vs describing. In Proceedings
of the Workshop on Multimodal Output Generation
(MOG?07)., pages 129?139.
I. van der Sluis and E. Krahmer. 2007. Generating
multimodal referring expressions. Discourse Pro-
cesses, 44(3):145?174.
I. van der Sluis, P. Piwek, A. Gatt, and A. Bangerter.
2008. Towards a balanced corpus of multimodal re-
ferring expressions in dialogue. In Proceedings of
the Symposium on Multimodal Output Generation
(MOG?08).
I.H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, second edition.
91
