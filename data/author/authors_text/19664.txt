Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 895?904, Dublin, Ireland, August 23-29 2014.
An Off-the-shelf Approach to Authorship Attribution
Jamal Abdul Nasir
Dep. of Computer Science
LUMS Lahore
Pakistan
jamaln@lums.edu.pk
Nico G
?
ornitz
Dep. of Computer Science
TU Berlin
Germany
goernitz@tu-berlin.de
Ulf Brefeld
Dep. of Computer Science
TU Darmstadt
Germany
brefeld@cs.tu-darmstadt.de
Abstract
Authorship detection is a challenging task due to many design choices the user has to decide
on. The performance highly depends on the right set of features, the amount of data, in-sample
vs. out-of-sample settings, and profile- vs. instance-based approaches. So far, the variety of
combinations renders off-the-shelf methods for authorship detection inappropriate. We propose a
novel and generally deployable method that does not share these limitations. We treat authorship
attribution as an anomaly detection problem where author regions are learned in feature space.
The choice of the right feature space for a given task is identified automatically by representing
the optimal solution as a linear mixture of multiple kernel functions (MKL). Our approach allows
to include labelled as well as unlabelled examples to remedy the in-sample and out-of-sample
problems. Empirically, we observe our proposed novel technique either to be better or on par
with baseline competitors. However, our method relieves the user from critical design choices
(e.g., feature set) and can therefore be used as an off-the-shelf method for authorship attribution.
1 Introduction
Automatically attributing a piece of text to its author is one of the oldest problems studied in linguis-
tics (Mendenhall, 1887). Despite being an old problem, authorship attribution is still highly topical
and todays applications range from plagiarism detection (Maurer et al., 2006), identifying the origin of
anonymous harassments in emails, blogs, and chat rooms (Tan et al., 2013) to copyright and estate is-
sues as well as resolving historical questions of disputed authorship (Mosteller and Wallace, 1964; Fung,
2003).
Intrinsically, the goal of authorship detection is to identify the characteristic traits of an author. The
idea is that, these traits distinguish an author from others in terms of writing style, use of words, etc. Thus,
prior work often focuses on designing and extracting features from text to capture these traits. There is a
great deal of features proposed for authorship detection, including word or character n-grams (Burrows,
1987; Houvardas and Stamatatos, 2006), part-of-speech (Stamatatos et al., 2001), probabilistic context-
free grammars (Raghavan et al., 2010), or linguistic features (Koppel et al., 2006). However, indicative
features for one author do not necessarily help to characterise another. A major problem in authorship
detection is therefore to find the right set of features for a given task at hand (Forman, 2003).
Algorithmically, a variety of different models have been studied in the context of authorship detection,
ranging from probabilistic approaches (Seroussi et al., 2011) and similarity-based methods (Koppel et
al., 2011) to vector space models (Fung, 2003; Li et al., 2006). The approaches either treat documents as
independent (instance-based) or concatenate documents by the same author (profile-based). Intuitively,
the latter is helpful if an author has a concise way of expressing herself so that the concatenated document
allows to extract a statistic that is sufficient for capturing her style. On the other hand, instance-based
approaches are better suited for expressive authors and have advantages in sparse data scenarios.
Another aspect in authorship attribution is the application scenario of the final model. In transductive
(in-sample) settings, the unlabelled documents of interest are already included in the training process
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
895
?4 ?2 0 2 4
?2
?1
0
1
2
3
4
5
6
?4 ?2 0 2 4
?2
?1
0
1
2
3
4
5
6
?4 ?2 0 2 4
?2
?1
0
1
2
3
4
5
6
Figure 1: Three solutions of an anomaly detection problem where data is represented by RBF kernels
with different band-width parameters. Combining anomaly detection with multiple kernel learning al-
lows to include all three kernels simultaneously in the optimisation and to find the optimal linear mixture
of the three (or more) kernels together automatically for a given task.
and the model does not necessarily perform well on new and unseen texts. By contrast, inductive (out-
of-sample) scenarios generally allow to learn models that can be applied to any future text but require
larger training samples to achieve accurate performances.
In this paper, we propose a general machine learning-based approach to authorship detection. Our
approach remedies the above mentioned problems by fusing existing techniques: (i) We cast authorship
attribution as an anomaly detection problem where one model is learned for every author. The idea is to
identify a concise region in feature space that contains (most of) the documents of the author of interest
while other documents are considered outliers. Thus, the model can be viewed as a profile-based ap-
proach in feature space while the data is treated on an instance-based level. (ii) We remedy the in-sample
/ out-of-sample problem by providing a semi-supervised extension of the commonly unsupervised outlier
detection framework. By doing so, we may include authorship labels for the already known documents
and leave the disputed ones unlabelled. (iii) Finally, we devise our model consequentially as a member
of the multiple kernel learning family to automatically include a mathematically well founded feature
selection framework that renders the method generally applicable. The optimal solution is given by a
(possibly sparse) linear mixture of kernel functions.
Empirically, we observe that our approach consistently outperforms baseline competitors or confirms
common knowledge with respect to the authorship of disputed articles. The main advantage of the
method however lies in its simplicity. Practitioners do not need to take critical design choices in terms of
which features to use and which not. By contrast, all features (kernels) can be used in the optimisation
and the method itself finds the optimal combination for the problem at hand.
The remainder of this paper is structured as follows. Section 2 reviews related work. Our main contri-
bution is presented in Section 3. We report on empirical results in Section 4 and Section 5 concludes.
2 Related Work
Authorship attribution using linguistic and stylistic features has a long tradition and can be dated back
to the nineteenth century. As a first attempt, Mendenhall (Mendenhall, 1887) uses features based on
word lengths to characterise the plays of Shakespeare. Later in the first half of the 20th century, different
textual statistics, such as Zipf?s distribution (Zipf, 1932) and Yule?s k-statistic (Yule, 1944) have been
proposed to quantify textual style. A study conducted by (Mosteller and Wallace, 1964) is one of the most
influential modern work in authorship attribution. They use a Bayesian approach to analyse frequencies
of a small set of function words for The Federalist Papers, a series of 85 political essays written by John
Jay, Alexander Hamilton, and James Madison. Until the late 1990s, research in stylometry has been
dominated by feature engineering to quantify writing style (Holmes, 1998) and about 1,000 different
measures have been proposed (Rudman, 1997).
896
Document representation is essential for author attribution tasks. Features aim to capture character-
istic traits of authors that persist across topics. Traditional stylometric features include function and
high-frequency words, hapax legomena, Yules k-statistic, syllable distributions, sentence length, word
length and word frequencies, vocabulary richness functions as well as syntactic features. Many stud-
ies combine features of different types using multivariate analyses. Some researchers use punctuation
symbols while others experiment with n-grams (Diederich et al., 2003). Grammatical style markers with
natural language processing techniques are also used to extract features from the documents.
Also in terms of technical approaches, authorship attribution has been studied with a wide range of
different approaches. The deployed techniques can be broadly divided into three categories: machine
learning (Diederich et al., 2003), multivariate/cluster analysis (Khmelev, 2000), and natural language
processing (Stamatatos et al., 2000). Principal components analysis (PCA) is one of the widely used
techinques for authorship studies, for instance, (Holmes and Crofts, 2010) apply PCA to identify the
authorship of unknown articles that have been attributed to Stephen Crane. In addition, machine learning-
based approaches, including neural networks (Neme et al., 2011) and support vector machines (SVMs)
(Diederich et al., 2003), are frequently used to discriminate documents by different authors. An excellent
survey on the diversity of approaches for authorship detection is provided by (Stamatatos, 2009).
Density level set estimation, also known as one-class learning (Tax and Duin, 1999; Sch?olkopf et
al., 1999), is the problem of learning a representation of a single class of interest, rejecting data points
that deviate from the learned model of normality. Thus, it has been proven very successful in anomaly
detection scenarios such as network intrusion detection (G?ornitz et al., 2009). Various extensions have
been proposed, i.e. to incorporate prior and additional knowledge (G?ornitz et al., 2013; Blanchard et al.,
2010) in a semi-supervised fashion (Chapelle et al., 2006) and to learn linear combinations of kernels
(Kloft et al., 2011; Rakotomamonjy et al., 2008) which is especially usefull whenever the right choice of
representation is unknown.
3 Methodology
In this section, we cast semi-supervised anomaly detection as an instance of multiple kernel learning.
The rational for this idea is shown in Figure 1. The figure shows three solutions for an anomaly de-
tection task. The data is represented by RBF kernels with different band-width parameters. As shown
in the figure, the choice of the band width parameter is crucial and leads to very different solutions.
Usually, kernel parameters are thus included in the model selection and their optimisation is often time
consuming. Fusing the anomaly detection with multiple kernel learning allows to include all three ker-
nels simultaneously in the optimisation and to find the best linear mixture of the three (or more) kernels
together with model parameters for the data at hand.
We briefly review anomaly detection and semi-supervised anomaly detection in Sections 3.1 and 3.2,
respectively, and present our main contribution, multiple kernel learning for anomaly detection, in Sec-
tion 3.3.
3.1 Preliminaries
Anomaly detection is often cast as an unsupervised one-class problem where the goal is to find a hyper-
plane that separates the majority of the input examples from the origin with maximum margin, so that, by
definition, points not exceeding the hyperplane are considered outliers. Analogously, we aim to learn a
separating hyperplane for articles by an author of interest, such that documents not exceeding the learned
hyperplane are written by other authors.
Given a n articles x
1
, . . . ,x
n
of possibly different authors, a straight forward optimisation problem
that identifies the hyperplane in terms of its normal vector w and threshold ? is known as one-class
support vector machine (Tax and Duin, 1999; Sch?olkopf et al., 1999) and given by
min
w,?,??0
1
2
?w ?
2
2
? ?+ ?
u
n
?
i=1
?
i
s.t. ?
n
i=1
: ?w, ?(x
i
)? ? ?? ?
i
.
897
The hyperplane is realised by the decision function
f(x) = ?w, ?(x)? ? ?
and new articles are credited to the author if f(x) > 0 and are considered work by someone else if
f(x) ? 0. The threshold ? can be interpreted as a measure of expressiveness of an author. E.g., authors
who have a very clear and concise style realise smaller thresholds than expressive authors that may adopt
to different writing styles.
3.2 Semi-supervised Anomaly Detection
Using only unlabelled data is usually leading to inaccurate models in the presence of only
a few data points. We therefore extend the problem setting to include m labeled examples
(x
n+1
, y
n+1
), . . . , (x
n+m
, y
n+m
) in addition to the n unlabelled ones. Labels y
i
? {+1,?1} are con-
sidered binary, that is in case y
i
= +1, document x
i
belongs to the author of interest. To combine sums
and hence, improve readability, we introduce labels y
i
= +1 ? i = 1, . . . , n for all unlabelled examples
and an indicator function I
c
? [c > n] to mask labeled examples; the function I
c
simply returns 1 if
c > n and 0 otherwise.
A semi-supervised generalisation of the hypersphere model of the previous section is the convex semi-
supervised anomaly detection (SSAD) (G?ornitz et al., 2013) which uses an L
2
-regularizer together with
the hinge-loss. Let ? be the margin for the labeled examples and ?, ?
u
, and ?
l
trade-off parameters, the
optimisation problem becomes
min
w,?,??0,??0
1
2
?w ?
2
2
? ?? ?? +
n+m
?
i=1
(I
i
?
l
+ (1? I
i
)?
u
)?
i
s.t. ?
n+m
i=1
: y
i
?w, ?(x
i
)? ? y
i
?+ I
i
? ? ?
i
.
The solution w admits a dual representation and can be written as
w =
n+m
?
i=1
?
i
y
i
?(x
i
)
and hence, the decision function depends only on inner products of the input examples wich paves the
way for kernel functions K
?
(x,x
?
) = ??(x), ?(x
?
)? (see (M?uller et al., 2001) for an introduction to
kernels). It holds
f(x) =
n+m
?
i=1
?
i
y
i
??(x
i
), ?(x)? ? ? =
n+m
?
i=1
?
i
y
i
K
?
(x
i
,x)? ?.
We omit the subscript ? in the remainder to not clutter notation unnecessarily.
3.3 Multiple Kernel Learning for Anomaly Detection
Learning linear combinations of multiple kernels is an appealing strategy when the right choice of rep-
resentations is unknown. We therefore generalise the semi-supervised anomaly detection of the previous
section as a member of the multiple kernel learning framework (Lanckriet et al., 2004). Thus, we aim to
learn a weighted combination of T kernels with mixing coefficients ? = (?
1
, . . . , ?
T
):
K
MKL
(x,x
?
) :=
T
?
t=1
?
t
K
t
(x,x
?
) =
T
?
t=1
?
t
??
t
(x), ?
t
(x
?
)?
=
T
?
t=1
?
?
?
t
?
t
(x),
?
?
t
?
t
(x
?
)?.
898
In general, properties of the mixing coefficients include (i) non-negativity, hence ?
t
? 0 and (ii) normal-
isation ?? ?
p
= 1. Recent work (Kloft et al., 2011) suggests to use the more general p-norm instead of
a common 1-norm (Lanckriet et al., 2004; Bach et al., 2004; Rakotomamonjy et al., 2008). The latter
usually leads to sparse mixing coefficients which is not appealing in every situation whereas p-norm with
1 ? p ? inf admits sparsity adjustments for the problem at hand and thus adds flexibility. Incorporating
multiple feature representations in our model introduced in Section 3.1 leads to
f
MKL
(x) =
T
?
t=1
?
?
w
t
,
?
?
t
?
t
(x)? ? ? =
T
?
t=1
?
?
t
?
?
w
t
, ?
t
(x)? ? ?. (1)
Due to technical reasons, i.e. to preserve convexity, we substitute the model parameters w
t
=
?
?
t
?
w
t
and arrive at the revised primal MKL-SSAD optimisation problem:
min
{w
t
},?,??0,??0,??0
?
2
?? ?
2
p
+
1
2
T
?
t=1
1
?
t
?w
t
?
2
2
? ?? ?? +
n+m
?
i=1
(I
i
?
l
+ (1? I
i
)?
u
)?
i
s.t. ?
n+m
i=1
: y
i
T
?
t=1
?w
t
, ?
t
(x
i
)? ? y
i
?+ I
i
? ? ?
i
. (2)
(Kloft et al., 2011) prove the equivalence of Tikhonov and Ivanov regularisation which allows to move
the regulariser on the mixing coefficients in the objective function. We will exploit this relation on various
occasions in this section. Deriving the Lagrange dual problem, we arrive at the intermediate saddle point
problem
max
?
min
{w
t
},??0
?
2
?? ?
2
p
+
1
2
T
?
t=1
1
?
t
?w
t
?
2
2
?
n+m
?
i=1
?
i
y
i
?
t
?w
t
, ?
t
(x
i
)?
s.t. ? ?
n+m
?
i=1
I
i
?
i
and 0 ? ?
i
? I
i
?
l
+ (1? I
i
)?
u
? i
We are solving the optimisation problem in a block-coordinate descent fashion by alternating between
w and ?. This enables us to compute the latter analytically assuming fixed variables w and setting the
partial derivative to zero:
??
t
p?1
?? ?
2?p
p
?
?w
t
?
2
2
?
t
2
= 0.
Therefore, given ? ? 0 we get
?
t
= ??w
t
?
2
p+1
2
.
Furthermore, it holds that at any optimal point ?? ?
p
= 1 and solving for ? gives ? =
1/(
?
T
t=1
?w
t
?
2p
p+1
2
)
1
p
. Putting things together, gives the analytical update rule
?
t
=
?w
t
?
2
p+1
2
(
?
T
t=1
?w
t
?
2p
p+1
2
)
1
p
(3)
which, since only norms are involved, ensures non-negativity for the mixing coefficients. Substituting
w
t
using the representer theorem w
t
= ?
t
?
n+m
i=1
?
i
y
i
?
t
(x
i
) yields the final optimisation problem for
899
Algorithm 1 Proposed optimization algorithm for MKL-SSAD (2)
Require: x,y, ?
u
, ?
l
, ? & p? norm
Initialize kernel mixture coefficients such that ??
z=0
?
p
= 1
while Until Convergence do
Step 1: solve the convex SSAD problem as stated in Eqn. (4)
?
z+1
= argmax?:0??
i
?I
i
?
l
+(1?I
i
)?
u
J(?,?
z
) s.t. ? ?
?
n+m
i=1
I
i
?
i
Step 2: optimize the weights according to Eqn. (3)
?
z+1
= argmin??0 J(?
z+1
,?) s.t. ?? ?
2
p
? 1
z=z+1
end while
return Trained parameter vector ?
?
, weights ?
?
MKL-SSAD:
max
?
min
?:?? ?2
p
?1
=:J(?,?)
? ?? ?
?
1
2
?
i,j
?
i
?
j
y
i
y
j
T
?
t=1
?
t
K
t
(x
i
,x
j
) (4)
s.t. ? ?
n+m
?
i=1
I
i
?
i
and 0 ? ?
i
? I
i
?
l
+ (1? I
i
)?
u
? i
As a block-coordinate descent method, we can iteratively alternate between the two optimisation blocks
and every limit point of Algorithm 1 is a globally optimal point (cmp. also (Kloft et al., 2011)). Algo-
rithm 1 summarises the proposed optimisation procedure.
4 Empirical Results
In this section, we empirically evaluate the benefit of fusing semi-supervised anomaly detection with
multiple kernel learning. We experiment on two data sets, the Reuters-50-50 corpus in Section 4.1 and
the Federalist Papers in Section 4.2
4.1 Reuters 50-50
We use a subset of the Reuters 50-50 data set
1
to evaluate the performance of the aforementioned
approaches. The reduced data contains 1000 articles written by 10 authors, Aaron Pressman, Alan
Crosby, Alexander Smith, Benjamin Kang Lim, Bernard Hickey, Brad Dorfman, Darren Schuettler,
David Lawder, Edna Fernandes, and Eric Auchard.
We deploy the following four kernels to represent documents: the first kernel is made of 484 func-
tion words taken from (Koppel and Schler, 2003), the second contains part-of-speech (POS) tags, the
third is assembled by 3-letter suffixes, the last one simply a bag-of-words (BOW) kernel. We split the
data into training (90%) and test (10%) sets and conduct a 10-fold cross-validation on the training set
for model selection. The best performing models are then evaluated on the test set. In this set of ex-
periments, we use a transductive setting where all training instances are labeled and only holdout and
test articles are unlabelled. We compare the performance of our approach with different p-norms to
the SSAD which uses one kernel at a time. For our MKL-based approach we use p-norms in the set
p ? {1, 1.7783, 3.1623, 5.6234, 10}.
The results in terms of averaged micro- and macro-F
1
measures are shown in Table 1. MKL con-
sistently outperforms the single-kernel baseline for all p-norms. That is, instead of extensively experi-
menting with SSAD and different kernel functions and parameter selections, a single run with our MKL
already leads to better performances in both metrics. The rightmost column in the table shows the result
for SSAD using a sum of the input kernels. Apparently, the performance is worse than using a bag-
of-words kernel alone. This result underlines the necessity of effective feature selection techniques for
1
https://archive.ics.uci.edu/ml/datasets/Reuter 50 50
900
Classifier 1
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 2
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 3
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 4
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 5
 
 
Func POS Suf BOW
1
1.7783
3.1623
5.6234
10
Classifier 6
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 7
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 8
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 9
Func POS Suf BOW
1
1.7783
3.16235.6234
10
Classifier 10
Func POS Suf BOW
1
1.7783
3.16235.6234
10 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 2: Kernel mixture coefficients for the 10 classes
authorship attribution. Note that our method can actually be viewed as an ensemble method that com-
bines several models as shown in Equation (1). However, compared to traditional ensembles, our method
uses a convex combination and hence returns the optimal ensemble given the data.
Table 1: F-scores for the subset of Reuters 50-50
p-norm MKL SSAD
1 1.7783 3.1623 5.6234 10 func-w POS Suffix-3 BOW
?
F
micro
73.46 73.08 73.84 73.89 74.23 63.08 54.62 70.01 72.85 61.76
F
macro
79.23 78.86 79.63 79.76 80.07 68.66 58.03 74.01 78.09 70.93
Figure 2 visualises the resulting mixing coefficients for the 10 authors/classifiers. While the models
are very similar at first sight, small deviations indicate differences in the style of the authors. Consider
for instance the top-left matrix. The contribution of the part-of-speech tag kernel (second column) to the
final solution is less than for the other authors. By contrast, the importance of the Suffix-3 kernel has
(slightly) more impact than for the remaining authors. This result shows that author-dependent mixtures
are found that help to capture characteristic traits of the respective writing styles.
4.2 Revisiting the Federalist Papers
The Federalist Papers are a series of 85 articles and essays written during 1787?1788. They were pub-
lished anonymously to persuade the citizens of the State of New York to ratify the Constitution. Later,
these papers were credited to Alexander Hamilton, John Jay, and James Madison; 73 of the documents
are uniquely associated with one of the three authors while the remaining 12, also known as the disputed
papers, have been claimed by both, Hamilton and Madison. Three of the 73 articles are considered joint
work by Hamilton and Madison. Previous studies often assign all 12 disputed papers to Madison which
we assume as ground-truth in the remainder (Mosteller and Wallace, 1964; Fung, 2003).
To confirm or refuse these previous findings, we conduct an experiment using same four kernels as
in the previous section, that is, a function words kernel (Koppel and Schler, 2003), a part-of-speech
(POS) tag kernel, a Suffix-3 kernel, and a bag-of-words (BOW) kernel. We compare the performance
of our approach (MKL) with semi-supervised anomaly detection (SSAD) (G?ornitz et al., 2013), support
vector data description (SVDD) (Tax and Duin, 1999), and the one-class SVM (OCSVM) (Sch?olkopf et
al., 1999). As before, the baselines cannot use all kernels at a time and are evaluated on every kernel
separately. For simplicity, we show only the MKL results for parameter p = 2 as all other p-norms that
we tried out lead to the same result.
901
We randomly divide the undisputed papers into training (80%) and holdout (20%) and use the 12
disputed papers for testing. We make sure that training sets contain at least three examples of every
author and two articles written jointly by Hamilton and Madison. Otherwise we discard and draw again.
We repeat experiments five times with randomly drawn training and holdout sets and report on averaged
accuracies for the disputed test set.
Table 2: Results for the disputed articles of the Federalist papers.
kernel H&M M J H
MKL (all) 0 12 0 0
SSAD
484fw 0 12 0 0
POS 9 0 3 0
Suffix3 0 12 0 0
BoW 0 0 0 12
SVDD
484fw 12 0 0 0
POS 12 0 0 0
Suffix3 12 0 0 0
BoW 12 0 0 0
OCSVM
484fw 12 0 0 0
POS 12 0 0 0
Suffix3 12 0 0 0
BoW 12 0 0 0
The results are shown in Table 2. The one-class SVM and SVDD constantly credit the 12 disputed
articles as joint work by Hamilton and Madison. The outcome of SSAD highly depends on the kernel
function; while the part-of-speech kernel distributes the papers on Jay (3) and Hamilton and Madison
(9), respectively, the bag-of-words kernel assigns all documents to Hamilton. By contrast, SVDD with
function word and Suffix-3 kernels attribute the articles to Madison. The same outcome is observed
for our novel MKL that also credits the 12 papers to Madison. Thus, MKL and SSAD with function
words and BoW kernel confirm todays assumption that all 12 papers have been written by Madison.
However, choosing SSAD as the base classifier in the absence of prior knowledge leaves much room for
interpretations and the user in the need of deciding between three solutions, depending on which kernel
she prefers. By using our MKL, selecting features and or kernel functions is no longer necessary as the
learning algorithm itself picks the right combination of kernels for the problem at hand. Thus, the more
kernels are thrown into, the richer the decision space for the MKL.
5 Conclusion
We proposed a universal method for authorship detection. Our approach built upon semi-supervised
anomaly detection and generalised existing techniques to utilise multiple kernels; a requirement which
is particularly beneficial for authorship attribution as features are usually tailored to tasks at hand and do
not necessarily translate well to other authors. Our method is proven to converge to the optimal solution
and simple to implement. Our empirical results show the robustness of our approach as it consistently
outperforms baseline competitors on a subset of Reuters 50-50 or confirms common knowledge wrt the
authorship of disputed articles of the Federalist Papers. The main advantage of the method however lies
in its simplicity. Practitioners do not need to take critical design choices in terms of which features to use
and which not. By contrast, all features (kernels) can be used in the optimisation and the method itself
finds the optimal combination for the problem at hand.
Acknowledgements
Jamal Abdul Nasir is supported by a grant from the Higher Education Commission, H-9 Islamabad,
Pakistan. Nico G?ornitz is supported by the German Bundesministerium f?ur Bildung und Forschung
902
(BMBF FKZ 01GQ0850 and 01IB001A). Ulf Brefeld is also affiliated with the German Institute for
Educational Research (DIPF), Frankfurt/Main, Germany.
References
F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. 2004. Multiple kernel learning, conic duality, and the SMO
algorithm. In Proc. of the International Conference on Machine Learning.
Gilles Blanchard, Gyemin Lee, and Clayton Scott. 2010. Semi-Supervised Novelty Detection. Journal of Machine
Learning Research, pages 2973?2973?3009?3009, December.
J. F. Burrows. 1987. Computation into Criticism: A Study of Jane Austen?s Novels and an Experiment in Method.
Oxford: Clarendon Press.
Olivier Chapelle, Bernhard Sch?olkopf, and Alexander Zien. 2006. Semi-supervised learning. {MIT} Press.
J. Diederich, J. Kindermann, E. Leopold, and G. Paass. 2003. Authorship attribution with support vector machines.
Applied Intelligence, 19(1):109?123.
G. Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Glenn Fung. 2003. The disputed federalist papers: Svm feature selection via concave minimization. In Richard
Tapia Celebration of Diversity in Computing Conference.
Nico G?ornitz, Marius Kloft, and Ulf Brefeld. 2009. Active and semi-supervised data domain description. In
ECML, pages 407?422. Springer.
Nico G?ornitz, Marius Kloft, Konrad Rieck, and Ulf Brefeld. 2013. Toward Supervised Anomaly Detection.
Journal of Artificial Intelligence Research (JAIR), 46:235?262.
David I. Holmes and Daniel W. Crofts. 2010. The diary of a public man: a case study in traditional and non-
traditional authorship attribution. LLC, 25(2):179?197.
David I. Holmes. 1998. The Evolution of Stylometry in Humanities Scholarship. Literary and Linguistic Com-
puting, 13(3):111?117, September.
J. Houvardas and E. Stamatatos. 2006. N-gram feature selection for authorship identification. In AIMSA.
Dmitry V. Khmelev. 2000. Disputed authorship resolution through using relative empirical entropy for markov
chains of letters in human language texts. Journal of Quantitative Linguistics, 7(3):201?207.
M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. 2011. lp-Norm Multiple Kernel Learning. JMLR, 12:953?997.
M. Koppel and J. Schler. 2003. Exploiting stylistic idiosyncrasies for authorship attribution. In Proceedings of the
International Joint Conference on Artificial Ingelligence.
M. Koppel, N. Akiva, and I. Dagan. 2006. Feature instability as a criterion for selecting potential style markers:
Special topic section on computational analysis of style. Journal of the American Society for Information
Science and Technology, 57(11):1519?1525.
M. Koppel, J. Schler, and S. Argamon. 2011. Authorship attribution in the wild. Language Resources & Evalua-
tion, 45(1):83?94.
G. Lanckriet, N. Cristianini, L. E. Ghaoui, P. Bartlett, and M. I. Jordan. 2004. Learning the kernel matrix with
semi-definite programming. JMLR, 5:27?72.
J. Li, R. Zheng, and H. Chen. 2006. From fingerprint to writeprint. Communications of the ACM, 49(4):76?82.
Hermann Maurer, Frank Kappe, and Bilal Zaka. 2006. Plagiarism ? a survey. Journal of Universal Computer
Science, 12(8)8:1050?1084.
T. C. Mendenhall. 1887. The characteristic curves of composition. Science, ns-9(214S):237?246.
Frederick Mosteller and David L. Wallace. 1964. Inference and Disputed Authorship: The Federalist. Springer-
Verlag, New York. 2nd Edition appeared in 1984 and was called Applied Bayesian and Classical Inference.
903
Klaus-Robert M?uller, Sebastian Mika, Gunnar R?atsch, Koji Tsuda, and Bernhard Sch?olkopf. 2001. An introduc-
tion to kernel-based learning algorithms. IEEE Transactions on Neural Networks, 12(2):181?201.
Antonio Neme, Blanca Lugo, and Alejandra Cervera. 2011. Authorship attribution as a case of anomaly detection:
A neural network model. Int. J. Hybrid Intell. Syst., 8(4):225?235.
S. Raghavan, A. Kovashka, and R. Mooney. 2010. Authorship attribution using probabilistic context-free gram-
mars. In Proceedings of the ACL 2010 Conference.
Alain Rakotomamonjy, Francis R. Bach, Stephan Canu, and Yves Grandvalet. 2008. SimpleMKL. JMLR,
9:2491?2521.
Joseph Rudman. 1997. The state of authorship attribution studies: Some problems and solutions. Computers and
the Humanities, 31(4):351?365.
B Sch?olkopf, J C Platt, J Shawe-Taylor, a J Smola, and R C Williamson. 1999. Estimating the support of a
high-dimensional distribution. Technical report, July.
Y. Seroussi, I. Zukerman, and F. Bohnert. 2011. Authorship attribution with latent dirichlet allocation. In Pro-
ceedings of the 15th International Conference on Computational Natural Language Learning.
Efstathios Stamatatos, Nikos Fakotakis, and George K. Kokkinakis. 2000. Automatic text categorization in terms
of genre and author. Computational Linguistics, 26(4):471?495.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 2001. Computer-based authorship attribution without lexical
measures. In Computers and the Humanities.
Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. Journal of the American Society
for Information Science and Technology, 60(3):538?556.
Enhua Tan, Lei Guo, Songqing Chen, Xiaodong Zhang, and Yihong (Eric) Zhao. 2013. Unik: Unsupervised social
network spam detection. In Proceedings of CIKM.
D. Tax and R. Duin. 1999. Data domain description using support vectors. In Proceedings of the European
Symposium on Artificial Neural Networks, volume 256, pages 251?256. Citeseer.
G. Udnv Yule. 1944. The Statistical Study of Literary Vocabulary. Cambridge University Press.
G. K. Zipf. 1932. Selective Studies and the Principle of Relative Frequency in Language. Harvard University
Press.
904
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1636?1647, Dublin, Ireland, August 23-29 2014.
Learning to Summarise Related Sentences
Emmanouil Tzouridis
?
?
Dep. of Computer Science
TU Darmstadt
Germany
{tzouridis,brefeld}@kma.
informatik.tu-darmstadt.de
Jamal Abdul Nasir
Dep. of Computer Science
LUMS Lahore
Pakistan
jamaln@lums.edu.pk
Ulf Brefeld
??
?
Inform. Center f. Education
DIPF Frankfurt/Main
Germany
brefeld@dipf.de
Abstract
We cast multi-sentence compression as a structured prediction problem. Related sentences are
represented by a word graph so that summaries constitute paths in the graph (Filippova, 2010).
We devise a parameterised shortest path algorithm that can be written as a generalised linear
model in a joint space of word graphs and compressions. We use a large-margin approach to
adapt parameterised edge weights to the data such that the shortest path is identical to the desired
summary. Decoding during training is performed in polynomial time using loss augmented infer-
ence. Empirically, we compare our approach to the state-of-the-art in graph-based multi-sentence
compression and observe significant improvements of about 7% in ROUGE F-measure and 8%
in BLEU score, respectively.
1 Introduction
Automatic text summarisation is one of oldest forms of natural language processing (Luhn, 1958; Bax-
endale, 1958). The goal is to extract the most important part of the content from either a single document
or a collection of documents (Mani, 2001; Roussinov and Chen, 2001; McKeown et al., 2005).
Frequently, the information of interest is contained in only a part of a sentence or may be distributed
across parts of several sentences. Identifying the content carrying part(s) constitutes an essential tech-
nique not only for single- and multi-document extractive summarisation but also text simplification in
general. Generating a simplified version of a text traditionally has many applications in question answer-
ing (Hermjakob et al., 2002) and speech synthesis (Kaji et al., 2004). Due to limited display sizes of
mobile devices, recent applications also deal with summarising/simplifying news articles, social media,
emails, or websites (Corston-Oliver, 2001).
Multi-sentence compression (MSC) unifies many of the mentioned characteristics and challenges and
can be seen as a key to text summarisation and simplification (Jing and McKeown, 2000). The task in
multi-sentence compression is to map a collection of related sentences to a grammatical short sentence
that preserves the most important part of the content. Sentence compression methods have been devised
using manually crafted rules (Dorr et al., 2003), language models (Hori et al., 2003; Clarke and Lapata,
2008), or syntactical representations (Barzilay and Lee, 2003; Galley and McKeown, 2007; Filippova and
Strube, 2008). Filippova (2010) introduces an elegant graph-based approach to multi-sentence compres-
sion that simply relies on the words of the sentences and efficient dynamic programming. Her approach
implements the observation that the frequency of words influences their appearance in human summaries
(Nenkova et al., 2006). Although being an intuitive rule that does work well in practice, frequency-based
strategies often remain heuristic.
In this paper we propose a structured learning-based approach to multi-sentence compression. In
analogy to Filippova (2010), related sentences are represented by a word graph (the input). Words are
identified with vertices and directed edges connect adjacent words in at least one sentence, so that the
summarising sentence (the output) is contained as a path in the graph. Generally, learning mappings
between complex structured and interdependent inputs and outputs challenges the standard model of
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1636
learning a mapping from independently drawn instances to a small set of labels. To capture the involved
dependencies we represent input graphs G and output paths p jointly by a (possibly rich) feature repre-
sentation ?(G, p). The goal is to find a linear function f(G, p) = ?
>
?(G, p) in joint space such that
p = argmin
p?
f(G, p?) (1)
is the desired summary for the collection G. Our approach can therefore be seen as translating the work
by Filippova (2010) into the structured prediction framework (Tsochantaridis et al., 2005; Taskar et al.,
2004). Instead of applying heuristics, we adapt the decoding machinery to the data by parameterising
a shortest path algorithm. The latter admits a representation as a generalised linear model in joint in-
put output space. We devise a structural support vector machine (SVM) (Tsochantaridis et al., 2005)
to learn the shortest path in possibly high dimensional joint feature spaces and propose a generalised,
loss-augmented decoding algorithm that is solved exactly by an integer linear program in polynomial
time. Empirically, we evaluate the structural support vector machine on a real world news headline
summarisation task. Our experiments show that a very rudimentary set of five features already suffices
to significantly improve the state-of-the-art in graph-based multi-sentence compression. We observe an
increase of 7% in in ROUGE F-measure and 8% in BLEU score, respectively.
The remainder of the paper is organised as follows. Section 2 reviews related work and Section 3
introduces word graphs and shortest paths. Our technical contribution is presented in Section 4. We
report on empirical results in Section 5 and Section 6 concludes.
2 Related Work
The goal of automatic text summarisation is to produce a summary of a given text (or text collection)
that preserves the most important information (Luhn, 1958; Edmundson, 1969). Summarisation systems
usually rely on clues or features that help to identify key elements such as the main topic of a document
(Salton et al., 1994). Such features may be extracted from sentences (e.g., the length of a sentence, its
position in the text), words (e.g., frequency of a word, relative position in sentence) as well as from style
and structure elements (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1997).
A special case of text summarisation is sentence compression; given a sentence, the task is to produce
a summary of the input that preserves the most important information and is grammatically correct (Jing,
2000). Sentence compression is thus relevant for many NLP tasks including question answering, machine
translation, text simplification, speech synthesis applications and multi-sentence compression (e.g., Lin
(2003)).
Multi-sentence compression extends sentence compression to collections of related sentences that are
to be summarised in a single output sentence. Traditionally, contributions to multi-sentence compression
exploit linguistic properties based on lexical information and syntactic dependencies. Dorr et al. (2003)
for instance propose a headline generation system based on linguistically-motivated, hand-crafted heuris-
tics. Barzilay and Lee (2003) study sentence compression with dependency trees. The aligned trees are
represented by a lattice from which a summary is extracted by an entropy-based criterion over all possi-
ble traversals of the lattice. Similarly, Barzilay and McKeown (2005) combine syntactic trees of similar
sentences by a multi-sequence alignment candidate selection and summary generation. Wan (2007) de-
ploys a language model in combination with maximum spanning trees to rank candidate aggregations
satisfying grammatical constraints. Hori et al. (2003) propose a statistical model for automatic speech
summarisation without using parallel data or syntactic information. Instead they focus on language mod-
els to provide a scoring function and use dynamic programming for searching the compression with the
highest score. Clarke and Lapata (2008) cast sentence compression as an optimisation problem. They
use linguistically motivated constraints and integer linear programming to infer globally optimal com-
pressions.
Recently, graph-based approaches to multi-sentence compression have been proposed. The underly-
ing idea is that syntax may help to find important content. Thus, instead of using hand-crafted rules,
parsers, or language models, a simple and robust graph-based method can be used to generate reason-
able summaries. Graph-based multi-sentence compression approaches identify the summary with the
1637
shortest path in word graphs (Filippova, 2010). Shortest paths of unweighted word graphs however do
not necessarily lead to satisfying summaries. As a remedy, Filippova (2010) introduces heuristic edge
weights based on normalised frequencies of the connected words. Boudin and Morin (2013) propose an
additional re-ranking scheme to identify summarisations that contain key phrases. The underlying idea
is that particular key phrases give rise to certain topics and thus lead to more informative aggregations.
In this paper, we parameterise the graph-based framework by Filippova (2010) such that the short-
est path algorithm is adapted to labeled data at hand. Adapting the dynamic programming to the data
renders the use of heuristics unnecessary. Instead, word graphs and compressions are embedded in a
(possibly high-dimensional) joint feature space where a generalised linear scoring function learns to sep-
arate between compressions of different quality. We develop a generalised, loss-augmented shortest path
algorithm that is solved exactly by a (relaxed) integer linear program in polynomial time.
3 Preliminaries
3.1 Word Graphs
In a nutshell, word graphs represent collections of sentences efficiently in a graph by mapping identical
words to a single vertex while the graph structure preserves the local neighbourhood of words.
From a collection of related sentences a word graph is constructed as follows: Initially, every sentence
is augmented by a preceding start token ?S? and a terminal end symbol ?E? so that beginning and end
of the sentences are preserved in the final graph. Starting with the empty graph, sentences are added
one after another. The first word of the first sentence is the auxiliary ?S? that is converted into the first
vertex v
?S?
. The second word of the first sentence also becomes a vertex v and the two vertices are
connected with a directed edge v
?S?
? v. The procedure continues with the third word and so on until
the end symbol ?E? is reached. The other sentences are incorporated analogously. A special case arises
if the graph already contains a vertex v that is identical to the word that is just to be added. Instead
of adding a redundant vertex, the already existing vertex v is used and, if v 6= v
?S?
, connected to the
respective predecessor as before. In that case, the vertex v has an in-degree of (at least) two and is used
as the predecessor for the next word to be added. The procedure continues until all n sentences are
incorporated in the graph.
Note that merging nodes to the same vertex requires an appropriate preprocessing of the sentences.
Simple lower- or upper-case representations of words often suffice but more complex preprocessing
schemas are also possible such as merging vertices carrying synonyms or words possessing small Word-
Net distances (Miller, 1995; Fellbaum, 1998). As word graphs are a condensed representation of the
input sentences, word graphs are also known as compression graphs. The described construction gives
us a directed graph G = (V, E), where V is the set of unique words in the sentences and E the set of
neighbouring words. An exemplary word graph is shown in Figure 1.
3.2 Shortest Path Algorithms
Given a directed weighted graph G = (V, E , cost) where V is the set of vertices and E ? V ? V the set
of edges. The function cost : (v, v
?
) 7? <
+
assigns positive weights to every edge (v, v
?
) ? E . A path
p from a vertex v
s
? V to a vertex v
e
? V is a sequence of edges connecting vertices of G. We write
P(v
s
, v
e
) to denote the set of all possible paths starting in v
s
and terminating in v
e
. The cost of a path is
given by the sum of the weights of the edges on the path.
The shortest path from a start vertex v
s
? V to an end vertex v
e
? V is defined as the path in G from
v
s
to v
e
with the lowest costs. Introducing auxiliary binary variables p
(v,v
?
)
indicating whether an edge
(v, v
?
) ? E lies on the path (p
v,v
?
= 1) or not (p
v,v
?
= 0) the shortest path can be computed by the
following optimisation problem
p
?
= argmin
p
?
(v,v
?
)?E
p
v,v
?
cost(v, v
?
) s.t. p ? P(v
s
, v
e
). (2)
There exist many algorithms for computing shortest paths efficiently (Bellman, 1958; Ford, 1956; Dijk-
stra, 1959). Usually, these methods are based on dynamic programming or (relaxed) integer program-
1638
Figure 1: The word graph constructed from the sentences: ?Yahoo in rumoured $1.1bn bid to buy white-
hot Tumblr?, ?Yahoo buys Tumblr as David Karp stays as CEO?, ?Yahoo to buy Tumblr for $1.1bn?.
The shortest path is highlighted.
ming, where an approximation of the exact quantity is iteratively updated until it converges to the correct
solution, which is achieved in polynomial time. A prominent algorithm for computing the k-shortest
paths is Yen?s algorithm (Yen, 1971). Intuitively, the approach recursively computes the second best
solution by considering deviations from the shortest path, the third best solution from the previous two
solutions, and so on. Figure 1 visualises the shortest path for the displayed compression graph.
4 Learning to Summarise Related Sentences
4.1 Problem Setting
Given a word graph G, we aim to find a ranking function f(G, p) that assigns the lowest score to the
best summary p
?
, that is, p
?
!
= argmin
p
f(G, p). Note that f is defined jointly on G and p to allow for
exploiting dependencies between word graph and summary. Our approach can thus be seen as an instance
of structured prediction models. The quality of f is measured by the Hamming loss ?, ?(p
?
, p?) =
1
2
?
(v
i
,v
j
)?V
[[p
?
ij
6= p?
ij
]], that details differences between the best summary p
?
and the prediction p?,
where [[z]] is the indicator function returning one if z is true and zero otherwise. The generalisation error
is given by
R[f ] =
?
?
(
p, argmin
p?
f(G, p?)
)
dP (G, p)
and approximated by its empirical counterpart
?
R[f ] =
m
?
i=1
?
(
p
i
, argmin
p?
f(G
i
, p?)
)
(3)
on a finite m-sample of pairs {(G
i
, p
i
)}
m
i=1
where G
i
is a word graph and p
i
the best summarising sen-
tence. However, minimising the empirical risk directly leads to an ill-posed optimisation problem as
1639
there generally exist many indistinguishable but equally well solutions realising an empirical loss of
zero. We thus focus on the minimisation of the regularised empirical risk
?
R
reg
[f ] = ?(f) +
m
?
i=1
?
(
p
i
, argmin
p?
f(G
i
, p?)
)
.
The additive regularisation ?(f) acts like a prior on f , e.g. to enforce smooth solutions. In the remainder
we use ?(f) = ?f?
2
.
4.2 Representation
The idea of our approach is as follows: We adapt the cost function of the graph to the training sample such
that the shortest path of the compression graph is identical to the desired summary. Recall the general
form of the cost function of Section 3.2. Instead of a constant or hand-crafted function (Filippova, 2010),
we deploy a linear mixture of features ?
i
, parameterised by ?,
cost(v, v
?
) =
?
i
?
i
?
i
(v, v
?
) = ?
>
?(v, v
?
).
Features ?
i
(v, v
?
) are drawn from adjacent vertices v, v
?
in the word graph to capture local dependencies
of the connecting edge. Examples for feature functions are frequency-based counts or indicators such as
POS-transitions of the form ?
234
(v, v
?
) = [[v is a noun?v
?
is a verb]]. Note that complex features using
the context of the edge are straight forward by extending the feature representation to the input graph
?(v, v
?
,G). The final feature vector is obtained by stacking-up all feature functions, that is, ?(v, v
?
) =
(. . . , ?
i
(v, v
?
), . . .)
>
.
Using the parameterised costs in the computation of the shortest path in Equation (2) yields the fol-
lowing objective function (ignoring the constraints for a moment) that can be rewritten as a generalised
linear model in joint input output space
?
(v
i
,v
j
)?V
p
ij
?
>
?(v
i
, v
j
) = ?
>
?
?
?
(v
i
,v
j
)?V
p
ij
?(v
i
, v
j
)
?
?
? ?? ?
??(G,p)
= ?
>
?(G, p) = f(G, p)
where the joint feature representation is given by
?(G, p) ?
?
?
?
(v
i
,v
j
)?V
p
ij
?(v
i
, v
j
)
?
?
.
Decoding the shortest path p? for a fixed parameter vector ? can now be computed by
p? = argmin
p
f(G, p) s.t. p ? P(?S?, ?E?)
using standard shortest path algorithms (Yen, 1971). In addition, reformulating the objective as a gener-
alised linear model allows to adapt the parameters ? to the data to identify shortest paths with summaries.
4.3 Optimisation
Recall that the goal of the optimisation is to find the ranking function f(G, p) that takes the smallest value
for the best summary. That is, for the i-th training instance (G
i
, p
i
), we aim at fulfilling the constraints
?
>
?(G
i
, p)? ?
>
?(G
i
, p
i
) > 0 (4)
for all p ? P(?S?, ?E?)\p
i
. We extend the constraints in Equation (4) by a term that induces a margin
between the best path p
i
and all alternative paths. A common technique is called margin-rescaling and
implies to scale the margin with the actual loss that is induced by decoding p? instead of p
i
. Thus,
1640
rescaling the margin by the loss implements the intuition that the confidence of rejecting a mistaken
output is proportional to its error. In the context of learning shortest paths, margin-rescaling gives us the
following constraints
?
>
?(G
i
, p?)? ?
>
?(G
i
, p
i
) > ?(p
i
, p?)? ?
i
for all p ? P(?S?, ?E?)\p
i
. The non-negative ?
i
? 0 is a slack-variable that allows point-wise relaxations
of the margin. Solving the equation for ?
i
shows that margin rescaling also effects the hinge loss that
now augments the structural loss ?,
`
?
(G
i
, p
i
, f) = max
[
min
p?
[?(p
i
, p?)? f(G
i
, p?) + f(G
i
, p
i
)]
]
.
The effective hinge loss upper bounds the structural loss ? for every pair (G
i
, p
i
) and trivially also
m
?
i=1
`
?
(G
i
, p
i
, f) ?
m
?
i=1
?(p
i
, argmin
p?
f(G
i
, p?))
holds. A max-margin approach to learning shortest paths therefore leads to the following optimisation
problem that is also known as structural support vector machine (Tsochantaridis et al., 2005)
min
?,??0
???
2
+ C
m
?
i=1
?
i
s.t. ?i ?p? ? P\p
i
: f(p?)? f(p
i
) > ?(p
i
, p?)? ?
i
. (5)
The parameter C trades-off margin maximisation and error minimisation and needs to be adjusted by
the user. The above optimisation problem can be solved efficiently by cutting plane methods. The idea
behind cutting planes is to instantiate only a minimal subset of the exponentially many constraints. That
is, for the i-th training example, we decode the shortest path p? given our current model and consider two
cases: (i) For p? 6= p
i
the prediction is erroneous and p? is called the most strongly violated constraint
as it realises the smallest function value and f(G
i
, p?) < f(G
i
, p) holds for all p 6= p?. Consequentially,
the respective constraint of the above optimisation problem is instantiated and influences the subsequent
iterations. (ii) If instead the prediction is correct, that is p? = p
i
, we need to verify that the runner-up p?
(2)
fulfils the margin constraint. If so, we proceed with the next training example, otherwise we instantiate
the corresponding constraint, analogously to case (i). Luckily, we do not need to rely on an expensive
two-best shortest path algorithm but can compute the most strongly violated constraint directly via the
cost function
Q(p?) = ?(p
i
, p?)? ?
>
?(G
i
, p?) + ?
>
?(G
i
, p
i
) (6)
that has to be maximised wrt p?. The following proposition shows that we can equivalently solve a shortest
path problem for finding the maximiser of Q.
Proposition 1. The argmax p? of Q in Equation (6) can be computed by minimising a shortest path
problem with cost function cost(v
i
, v
j
) = p
ij
+ ?
>
?(v
i
, v
j
).
Proof. We treat the ground truth paths p as graphs and write V(p) for the set of nodes on the path and
E(p) to denote the set of edges that lie on the path. If, for instance, an element of the binary adjacency
matrix representing path p equals one, e.g., p
ij
= 1, we write p
i
, p
j
? V(p) and (p
i
, p
j
) ? E(p). First,
note that the Hamming loss can be rewritten as
?(p, p?) =
?
(p
i
,p
j
)?E(p)
(1? p
ij
p?
ij
) .
1641
We have
p? = argmax
p?
?(p, p?) + ?
>
?(G
i
, p)? ?
>
?(G
i
, p?)
= argmax
p?
?(p, p?)? ?
>
?(G
i
, p?)
= argmax
p?
?
(p
i
,p
j
)?E(p)
(1? p
ij
p?
ij
)? ?
>
?(G
i
, p?)
= argmax
p?
?
?
(p
i
,p
j
)?E(p)
p
ij
p?
ij
? ?
>
?(G
i
, p?)
= argmin
p?
?
(p
i
,p
j
)?E(p)
p
ij
p?
ij
+ ?
>
?(G
i
, p?)
= argmin
p?
?
(p
i
,p
j
)?E(p)
p
ij
p?
ij
+ ?
>
[
?
(x
i
,x
j
)?E(G)
p?
ij
?(v
i
, v
j
)
]
= argmin
p?
?
(v
i
,v
j
)?E(G)
p
ij
p?
ij
+ ?
>
[
?
(x
i
,x
j
)?E(G)
p?
ij
?(v
i
, v
j
)
]
= argmin
p?
?
(v
i
,v
j
)?E(G)
[
p
ij
+ ?
>
?(v
i
, v
j
)
]
p?
ij
The output p? is the shortest path with costs given by p
ij
+ ?
>
?(v
i
, v
j
).
Using this result, the following lemma shows that we can compute the most strongly violated constraint
directly by a linear program.
Lemma 1. The maximizer p? of function Q in Equation (6) and thus the shortest path of Proposition 1
can be computed in polynomial time by the following linear program
min
p?
?
ij
(
p
ij
+ ?
>
?(v
i
, v
j
)
)
p?
ij
subject to the constraints
?k ? V(G)\{?S?, ?E?} :
?
j
p?
kj
?
?
i
p?
ik
? 0 ? ?
?
j
p?
kj
+
?
i
p?
ik
? 0
?
j
p?
?S?,j
?
?
i
p?
i,?S?
? 1 ? ?
?
j
p?
?S?,j
+
?
i
p?
i,?S?
? ?1
?
i
p?
i,?E?
?
?
j
p?
?E?,j
? 1 ? ?
?
i
p?
i,?E?
+
?
j
p?
?E?,j
? ?1
?(i, j) : p?
ij
? G
(i,j)
? ?(i, j) : p?
ij
? {0, 1}.
Proof. For lack of space, we only motivate the constraints. The first line of constraints guarantees that
every inner node of the path has exactly as many incoming as outgoing edges, the second line forces the
path to start in v
?S?
and, analogously, the third line ensures that it terminates in v
?E?
. The last line of
constraints requires the edges of the path p? to adhere to existing paths of G.
4.4 Parallelisation
Using the result by Zinkevich et al. (2011) the proposed approach can easily be distributed on several
machines. Note that cutting planes treat one input (G, p) at a time. Thus, several models can be trained
independently in parallel on disjoint subsets of the data. A subsequent merging process aggregates the
models where each models impact is proportional to the amount of data it has been trained on. Note that
the described parallelisation can easily be realised by the MapReduce/Hadoop framework. Processing
training instances and updating local models is performed by (one or more) mappers while the merge
operation is carried out by a reduce task.
1642
Table 1: Left: Collection of related sentences. Right: Candidate compressions and number of votes.
related sentences
White House: Hong Kong had ?plenty of time? to stop Snowden live coverage
Edward Snowden leaves reporters chasing shadows around an airport
US warns Moscow not to let Edward Snowden escape Russia
WikiLeaks forced to defend Ecuador as Edward Snowden seeks asylum
Snowden is ?not on plane? to Cuba
summary #
snowden seeks asylum 5
snowden live coverage 5
snowden escape russia 1
edward snowden seeks asylum 3
wikileaks forced to cuba 1
5 Empirical Results
5.1 Data Preparation
We crawl RSS feeds of 6 major news sites and harvest news headlines of a predefined set of categories
including sports, technology, and business. The headlines are processed automatically by a spectral clus-
tering. The data is thus transformed into a fully connected graph where vertices correspond to headlines
and edges are weighted by the number of shared non-stopwords. The clustering is performed for each
category on a daily basis. Resulting clusters are headlines that belong (with high probability) to the same
event and form our related input sentences. Groups with less than five sentences are discarded.
To identify the best summaries, we conduct a crowd sourcing experiment on Crowdflower
1
. Every
annotator is given a group of related sentences together with 10 possible summaries generated by a 10-
best Yen?s algorithm (Yen, 1971). The task of the annotator is to pick the best summary or mark the
collection as inappropriate. Each collection is labeled by at least 10 annotators. The group is discarded
if the majority of the annotators mark the group as inappropriate. Otherwise, the three most frequent
summaries are extracted, ties are broken by the authors. The most frequent summary is used as the
ground-truth annotation in the learning phase, the other two are used additionally in the evaluation. The
described process leaves us with 1024 sentences that are divided into 164 annotated groups of related
sentences. Table 1 shows an exemplary collection of related sentences (left) and a selection of summaries
together with the number of votes from the annotators. The overall distribution of votes is displayed in
Figure 2. The figure shows the mean value per rank of all 164 normalised and sorted histograms. The
best summary receives on average 8% more votes than the runner-up (not shown).
Figure 2: Distribution of annotations.
5.2 Baselines and Features
We compare our learning approach to graph-based sentence compression techniques proposed by Filip-
pova (2010), Boudin and Morin (2013). The two baselines construct word graphs as presented in Section
3.1 and output the weighted shortest path. Filippova (2010) uses a frequency-based heuristic for edges
weights and Boudin and Morin incorporate a keyphrase detection framework to re-rank summaries ac-
cording to the number and importance of keyphrases found. In addition, we also include an unweighted
shortest path strategy which is a straight forward application of Yen?s algorithm (Yen, 1971) and trivially
returns the shortest path in terms of the number of edges. Additional straw men are a random (Random)
input sentence and the shortest input sentence (Shortest).
In our learning-based approach, every edge between vertices v and v
?
is associated with a feature
vector. Let w = #(v) the frequency of word v, w
?
the analogue for v
?
, e = #(v, v
?
) the fre-
quency of the edge, and n = |V| the number of vertices in the graph. The feature vector ?(v, v
?
) of
1
http://crowdflower.com
1643
Table 2: ROUGE F-measure scores
training set size
22 35 48 61 74 87 100
R1 Random 46.72 46.82 46.41 46.20 46.39 46.53 46.88
Shortest 45.93 45.77 46.39 46.56 47.01 47.59 48.04
Yen 45.14 44.47 45.12 45.13 45.63 46.14 46.39
Filippova 52.70 52.94 52.16 52.02 52.22 52.45 51.81
Boudin 52.72 53.12 53.43 53.52 53.10 52.81 52.35
SVM 48.39 50.30 55.09 54.59 57.39 54.89 57.66
R2 Random 30.43 30.63 30.56 30.31 30.38 30.64 31.09
Shortest 27.65 27.43 27.90 27.93 28.64 29.47 30.10
Yen 31.38 30.82 31.16 31.40 31.90 32.30 32.56
Filippova 36.12 36.52 35.56 35.49 35.75 35.98 35.64
Boudin 36.71 37.01 37.79 37.65 36.97 36.75 36.31
SVM 33.64 35.40 40.46 40.68 43.44 40.45 43.58
RW1.2 Random 35.91 35.97 35.74 35.58 35.80 35.93 36.07
Shortest 34.47 34.29 34.77 34.85 35.32 35.9 36.16
Yen 34.85 34.26 34.74 34.83 35.22 35.62 35.77
Filippova 40.30 40.53 39.88 39.70 39.94 40.12 39.56
Boudin 40.79 40.99 41.37 41.31 40.92 40.83 40.36
SVM 37.94 39.06 42.61 42.33 44.63 42.90 45.00
the edge v ? v
?
consists of the normalised joint frequency ?
1
(w,w
?
) =
e
n
, the maximal word fre-
quency ?
2
(w,w
?
) = max
{
w
n
,
w
?
n
}
, the lexical relevance ?
3
(w,w
?
) =
2
n
w?w
?
w+w
?
, the normalised PMI
?
4
(w,w
?
) = (log
e
w?w
?
)/ ? log
e
n
(Bouma, 2009), and ?
5
captures the average location of the phrase in
the input sentences (Turney, 2000),
?
5
(w, w?) =
?
?
?
?
?
1.0 : [0? 10]%
0.4 : [10? 30]%
0.8 : [30? 60]%
0.6 : [60? 80]%
1.0 : [80? 100]%.
Note that ?
i
? [0, 1] holds for 1 ? i ? 5. Also note that ? denotes a rudimentary set of features.
Elaborate representations could for instance also contain POS-tags or named entities.
5.3 Experimental Setup and Results
For the news headline experiment, we draw m ? {22, 35, 48, 61, 74, 87, 100} training instances without
replacement at random from the collected data. The remaining instances are split randomly into equally
sized holdout and test sets. We perform model selection for adjusting the trade-off parameter of the
support vector machine on the interval C ? [2
?10
, 2
12
]. We report average ROUGE F-measures (Lin,
2004) and BLEU scores (Papineni et al., 2012) over 10 repetitions with distinct training, holdout, and
test sets. In each repetition, all algorithms are trained and/or evaluated on identical data splits.
ROUGE measures the concordance of system and human generated summaries by determining n-
gram, word sequence, and word pair matches. We use unigrams (R1), bigrams (R2), and the weighted
longest common subsequence (RW1.2) to evaluate compressions. Note that R1 has been found to corre-
late well with human evaluations based on various statistical metrics (Lin and Hovy, 2003). Moreover,
R1 and R2 emulate human pyramid and responsiveness scores (Owczarzak et al., 2012).
Table 2 shows the resulting ROUGE scores for the news headline experiment. Significant results are
marked in bold face according to a paired t-test using a significance level of 5%. For small training sets,
the structural support vector machine performs only slightly better than the unweighted application of
Yen?s algorithm and is clearly outperformed by the unsupervised baselines. However, the SVM improves
1644
Table 3: BLEU scores
training set size
22 35 48 61 74 87 100
B1 Random 38.56 38.40 36.49 36.77 36.00 36.87 37.35
Shortest 37.45 38.37 38.46 37.25 37.28 37.17 36.64
Yen 29.46 28.3 29.39 29.98 29.99 31.20 30.64
Filippova 44.26 43.29 44.66 44.57 45.21 43.10 43.52
Boudin 44.00 42.54 44.75 44.39 44.80 43.22 43.96
SVM 39.60 41.96 48.44 47.10 50.20 46.90 50.39
B2 Random 34.85 34.80 33.12 33.48 32.65 33.77 34.12
Shortest 33.34 34.27 34.54 33.39 33.39 33.43 33.45
Yen 28.51 27.27 28.34 28.74 29.06 30.05 29.73
Filippova 39.92 39.36 40.05 40.27 41.14 39.26 39.60
Boudin 39.43 38.45 39.99 40.02 40.52 39.20 39.84
SVM 36.37 38.63 45.31 44.15 47.40 43.75 47.44
B3 Random 35.91 35.97 35.74 35.58 35.80 35.93 36.07
Shortest 34.47 34.29 34.77 34.85 35.32 35.90 36.16
Yen 27.85 26.64 27.61 27.84 28.38 29.34 28.93
Filippova 36.07 35.88 35.86 36.42 37.37 35.55 35.97
Boudin 35.05 34.39 35.40 35.76 36.17 34.93 35.85
SVM 33.26 35.39 42.31 41.05 44.54 40.52 44.51
continuously with increasing training set sizes and outperforms the baselines significantly for more than
50 training examples. The unsupervised baselines cannot utilise the valuable annotations of the data and
remain constant. For 100 training instances, we observe performance improvements of about 5-7% for
all three ROUGE F-measures.
The BLEU metric computes scores for individual segments, then averages these scores over the whole
corpus for a final score. For our experiments we use BLEU-1, BLEU-2 and BLEU-3 to evaluate com-
pressions. Table 3 shows the corresponding results, significant results are again marked in bold face
according to a paired t-test with a significance level of 5%. The table draws a similar picture than the
previous one. The SVM continuously improves the performance with increasing training set sizes and
beats the baselines again at about 50 training examples significantly. For 100 training instances, all three
BLEU scores are improved by about 7-8%, respectively.
5.4 Analysis
The Pearson correlation between BLEU scores per instance and the number of vertices is -0.1886. The
negative correlation implies that summarising larger word-graphs is more challenging. A negative corre-
lation of -0.1267 is also observed for the lexical diversity of the collection; diverse groups of sentences
are thus more difficult to summarise. A possible remedy could be features that are not frequency-based,
such as POS-transitions. By contrast, the density of the graph given by |E|/|V|(|V| ? 1) shows a positive
correlation of 0.1851. The more dense a graph, the more edges interconnect vertices and there exist more
paths. These paths however frequently pass through the same vertices and as a consequence the lexical
diversity is low. A positive correlation of graph density is therefore closely connected to a negative
correlation of lexical diversity.
6 Conclusion
We proposed to learn shortest paths in word graphs for multi-sentence compression. A shortest path
algorithm is parameterised and adapted to labeled data at hand using the structured prediction frame-
work. Word graphs and summaries are embedded in a joint feature space where a generalised linear
scoring function learns to separate between compressions of different quality. Decoding is performed
1645
by a generalised, loss-augmented shortest path algorithm that can be solved by an integer linear pro-
gram in polynomial time. Empirically, we observe that a very rudimentary set of five features suffices to
significantly improve the state-of-the-art in graph-based multi-sentence compression.
Acknowledgments
Jamal Abdul Nasir is supported by a grant from the Higher Education Commission, H-9 Islamabad,
Pakistan.
References
R. Barzilay and L. Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence
Alignment, Proceedings of NAACL-HLT.
R. Barzilay and K. R. McKeown. 2005. Sentence Fusion for Multidocument News Summarization, Comput.
Linguist. 31(3), 297?328.
P. Baxendale 1958. Machine-made index for technical literature - an experiment, IBM Journal of Research
Development, 2(4):354?361.
R. Bellman 1958. On a routing problem, Quarterly of Applied Mathematics 16:87?90.
F. Boudin and E. Morin. 2013 Keyphrase Extraction for N-best reranking in multi-sentence compression, Pro-
ceedigs of NAACL-HLT
G. Bouma. 2009. Normalized (pointwise) Mutual information in collocation extraction, Proceedings of GSCL.
J. Clarke and M. Lapata. 2008. Global inference for sentence compression: An integer linear programming
approach, Journal of Artificial Intelligence Research, 31:399?429.
S. H. Corston-Oliver 2001. Text compaction for display on very small screens, Proceedings of the NAACL
Workshop on Automatic Summarization.
E. W. Dijkstra 1959. A note on two problems in connexion with graphs, Numerische Mathematik 1:269?271.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation,
Proceedings of the HLT-NAACL Workshop on Text Summarization.
H. P. Edmundson 1969. New methods in automatic extracting, Journal of the ACM, 16(2):264?285.
C. Fellbaum (Ed.). 1998. WordNet: An Electronic Lexical Database, Cambridge, MA: MIT Press.
K. Filippova. 2010. Multi-sentence compression: Finding shortest paths in word graphs, Proceedings of COLING.
K. Filippova and M. Strube. 2008. Dependency tree based sentence compression, Proceedings of INLG.
J. Ford, R. Lester 1956. Network Flow Theory, Paper P-923, Santa Monica, California: RAND Corporation.
M. Galley and K. R. McKeown. 2007. Lexicalized Markov grammars for sentence compression, Proceedings of
NAACL-HLT
U. Hermjakob, A. Echihabi, and D. Marcu. 2002. Natural language based reformulation resource and wide
exploitation for question answering, Proceedings of the Text Retrieval Conference.
C. Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel. 2003. A statistical approach to automatic speech summariza-
tion, EURASIP Journal on Applied Signal Processing, 2:128?139.
H. Jing 2000. Sentence reduction for automatic text summarization, Proc. of ANLP.
H. Jing and K. McKeown. 2000. Cut and paste based text summarization, Proc. of NAACL.
N. Kaji, M. Okamoto, and S. Kurohashi,. 2004. Paraphrasing predicates from written language to spoken
language using the web, Proceedings of HLT-NAACL.
J. Kupiec, J. Pedersen, and F. Chen 1995 A trainable document summarizer, Proceedings of SIGIR.
1646
C. Lin. 2003. Improving summarization performance by sentence compression - a pilot study, Proceedings of the
Int. Workshop on Information Retrieval with Asian Language.
C. Lin. 2004. Rouge: A package for automatic evaluation of summaries, Proceedings of the ACL Workshop on
Text Summarization Branches Out.
C.-Y. Lin and E. H. Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics,
Proceedings of HLT-NAACL.
H.P. Luhn. 1958. The Automatic Creation of Literature Abstracts, IBM Journal of Research and Development
2(2), 159?165.
I. Mani. 2001. Automatic Summarization, Amsterdam, Philadelphia: John Benjamins.
D. Marcu. 1997 The Rhetorical Parsing of Natural Language Texts, Proceedings of ACL/EACL.
K. R. McKeown, J. Hirschberg, M. Galley, and S. Maskey. 2005. From Text to Speech Summarization, Proceedings
of ICASSP.
G. A. Miller. 1995. WordNet: a lexical database for English, Communications of the ACM Vol. 38, No. 11:
39-41.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document sum-
marizer: exploring the factors that influence summarization, Proceedings of SIGIR.
K. Owczarzak, J. M. Conroy, H. T. Dang, and A. Nenkova. 2012. An assessment of the accuracy of automatic
evaluation in summarization, Proceedings of the Workshop on Evaluation Metrics and System Comparison for
Automatic Summarization.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation, Proceedings of ACL.
D. Roussinov and H. Chen. 2001 Information Navigation on the Web by Clustering and Summarizing Query
Results, Information Processing and Management, 37 (6), 789?816.
G. Salton, J. Allan, C. Buckley, and A. Singhal, 1994 Automatic Analysis, Theme Generation, and Summarization
of Machine-Readable Texts, Science 264(5164), 1421?1426.
B. Taskar and D. Klein and M. Collins and D. Koller and C. Manning. 2004. Max-margin parsing, Proceedings of
EMNLP, 2004.
S. Teufel and M. Moens. 1997 sentence extraction as a classification task, Proceedings of the ACL/EACL
Workshop on Intelligent and Scalable Text Summarization.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. 2005. Large margin methods for structured and
interdependent output variables, JMLR, 6 (Sep):1453-1484.
P. D. Turney. 2000. Learning algorithms for keyphrase extraction, Information Retrieval 2(4), 303?336.
S. Wan, R. Dale, M. Dras, C. Paris. 2007. Global revision in summarisation : generating novel sentences with
Prim?s algorithm, Proceedings of PACLING.
J. Y. Yen. 1971. Finding the k shortest loopless paths in a network, Management Science 17 (11): 712?716
M. Zinkevich, M. Weimer, A. Smola, and L. Li. 2011. Parallelized stochastic gradient descent, Proceedings of
NIPS.
1647
