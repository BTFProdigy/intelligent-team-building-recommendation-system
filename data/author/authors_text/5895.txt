Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1005?1014, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning to Merge Word Senses
Rion Snow Sushant Prakash
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{rion,sprakash}@cs.stanford.edu
Daniel Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305 USA
jurafsky@stanford.edu
Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305 USA
ang@cs.stanford.edu
Abstract
It has been widely observed that different NLP appli-
cations require different sense granularities in order to
best exploit word sense distinctions, and that for many
applications WordNet senses are too fine-grained. In
contrast to previously proposed automatic methods for
sense clustering, we formulate sense merging as a su-
pervised learning problem, exploiting human-labeled
sense clusterings as training data. We train a discrimi-
native classifier over a wide variety of features derived
from WordNet structure, corpus-based evidence, and
evidence from other lexical resources. Our learned
similarity measure outperforms previously proposed
automatic methods for sense clustering on the task of
predicting human sense merging judgments, yielding
an absolute F-score improvement of 4.1% on nouns,
13.6% on verbs, and 4.0% on adjectives. Finally, we
propose a model for clustering sense taxonomies us-
ing the outputs of our classifier, and we make avail-
able several automatically sense-clustered WordNets
of various sense granularities.
1 Introduction
Defining a discrete inventory of senses for a word is
extremely difficult (Kilgarriff, 1997; Hanks, 2000;
Palmer et al, 2005). Perhaps the greatest obstacle is
the dynamic nature of sense definition: the correct
granularity for word senses depends on the appli-
cation. For language learners, a fine-grained set of
word senses may help in learning subtle distinctions,
while coarsely-defined senses are probably more
useful in NLP tasks like information retrieval (Gon-
zalo et al, 1998), query expansion (Moldovan and
Mihalcea, 2000), and WSD (Resnik and Yarowsky,
1999; Palmer et al, 2005).
Lexical resources such as WordNet (Fellbaum,
1998) use extremely fine-grained notions of word
sense, which carefully capture even minor distinc-
tions between different possible word senses (e.g.,
the 8 noun senses of bass shown in Figure 1). Pro-
ducing sense-clustered inventories of arbitrary sense
granularity is thus crucial for tasks which depend on
lexical resources like WordNet, and is also impor-
tant for the task of automatically constructing new
WordNet-like taxonomies. A solution to this prob-
lem must also deal with the constraints of the Word-
Net taxonomy itself; for example when clustering
two senses, we need to consider the transitive effects
of merging synsets.
The state of the art in sense clustering is insuffi-
cient to meet these needs. Current sense clustering
algorithms are generally unsupervised, each relying
on a different set of useful features or hand-built
rules. But hand-written rules have little flexibility
to produce clusterings of different granularities, and
previously proposed methods offer little in the di-
rection of intelligently combining and weighting the
many proposed features.
In response to these challenges, we propose a
new algorithm for clustering large-scale sense hier-
archies like WordNet. Our algorithm is based on a
supervised classifier that learns to make graduated
judgments corresponding to the estimated probabil-
ity that each particular sense pair should be merged.
This classifier is trained on gold standard sense clus-
tering judgments using a diverse feature space. We
are able to use the outputs of our classifier to produce
a ranked list of sense merge judgments by merge
probability, and from this create sense-clustered in-
ventories of arbitrary sense granularity.1
In Section 2 we discuss past work in sense cluster-
1We have made sense-clustered Wordnets using the al-
gorithms discussed in this paper available for download at
http://ai.stanford.edu/?rion/swn.
1005
INSTRUMENT 7: ...the lowest range of a family of musical instruments
FISH
4: the lean flesh of a saltwater fish of the family Serranidae
5: any of various North American freshwater fish with lean flesh
8: nontechnical name for any of numerous... fishes
SINGER
3: an adult male singer with the lowest voice
6: the lowest adult male singing voice
PITCH
1: the lowest part of the musical range
2: the lowest part in polyphonic music
Figure 1: Sense clusters for the noun bass; the eight
WordNet senses as clustered into four groups in the
SENSEVAL-2 coarse-grained evaluation data
ing, and the gold standard datasets that we use in our
work. In Section 3 we introduce our battery of fea-
tures; in Section 4 we show how to extend our sense-
merging model to cluster full taxonomies like Word-
Net. In Section 5 we evaluate our classifier against
thirteen previously proposed methods.
2 Background
A wide number of manual and automatic techniques
have been proposed for clustering sense inventories
and mapping between sense inventories of different
granularities. Much work has gone into methods for
measuring synset similarity; early work in this direc-
tion includes (Dolan, 1994), which attempted to dis-
cover sense similarities between dictionary senses.
A variety of synset similarity measures based on
properties of WordNet itself have been proposed;
nine such measures are discussed in (Pedersen et al,
2004), including gloss-based heuristics (Lesk, 1986;
Banerjee and Pedersen, 2003), information-content
based measures (Resnik, 1995; Lin, 1998; Jiang and
Conrath, 1997), and others. Other approaches have
used specific cues from WordNet structure to inform
the construction of semantic rules; for example, (Pe-
ters et al, 1998) suggest clustering two senses based
on a wide variety of structural cues from Word-
Net, including if they are twins (if two synsets share
more than one word in their synonym list) or if
they represent an example of autohyponymy (if one
sense is the direct descendant of the other). (Mihal-
cea and Moldovan, 2001) implements six semantic
rules, using twin and autohyponym features, in addi-
tion to other WordNet-structure-based rules such as
whether two synsets share a pertainym, antonym, or
are clustered together in the same verb group.
A large body of work has attempted to capture
corpus-based estimates of word similarity (Pereira
et al, 1993; Lin, 1998); however, the lack of
large sense-tagged corpora prevent most such tech-
niques from being used effectively to compare dif-
ferent senses of the same word. Some corpus-based
attempts that are capable of estimating similarity
between word senses include the topic signatures
method; here, (Agirre and Lopez, 2003) collect con-
texts for a polysemous word based either on sense-
tagged corpora or by using a weighted agglomera-
tion of contexts of a polysemous word?s monose-
mous relatives (i.e., single-sense synsets related by
hypernym, hyponym, or other relations) from some
large untagged corpus. Other corpus-based tech-
niques developed specifically for sense clustering
include (McCarthy, 2006), which uses a combina-
tion of word-to-word distributional similarity com-
bined with the JCN WordNet-based similarity mea-
sure, and work by (Chugur et al, 2002) in find-
ing co-occurrences of senses within documents in
sense-tagged corpora. Other attempts have exploited
disagreements between WSD systems (Agirre and
Lopez, 2003) or between human labelers (Chklovski
and Mihalcea, 2003) to create synset similarity
measures; while promising, these techniques are
severely limited by the performance of the WSD
systems or the amount of available labeled data.
Some approaches for clustering have made use of
regular patterns of polysemy among words. (Pe-
ters et al, 1998) uses the COUSIN relation defined
in WordNet 1.5 to cluster hyponyms of categorically
related noun synsets, e.g., ?container/quantity? (e.g.,
for clustering senses of ?cup? or ?barrel?) or ?or-
ganization/construction? (e.g., for the building and
institution senses of ?hospital? or ?school?); other
approaches based on systematic polysemy include
the hand-constructed CORELEX database (Buite-
laar, 1998), and automatic attempts to extract pat-
terns of systematic polysemy based on minimal de-
scription length principles (Tomuro, 2001).
Another family of approaches has been to
use either manually-annotated or automatically-
constructed mappings to coarser-grained sense in-
ventories; an attempt at providing coarse-grained
sense distinctions for the SENSEVAL-1 exercise in-
cluded a mapping between WordNet and the Hec-
tor lexicon (Palmer et al, 2005). Other attempts in
1006
this vein include mappings between WordNet and
PropBank (Palmer et al, 2004) and mappings to
Levin classes (Levin, 1993; Palmer et al, 2005).
(Navigli, 2006) presents an automatic approach for
mapping between sense inventories; here similari-
ties in gloss definition and structured relations be-
tween the two sense inventories are exploited in or-
der to map between WordNet senses and distinc-
tions made within the coarser-grained Oxford En-
glish Dictionary. Other work has attempted to ex-
ploit translational equivalences of WordNet senses
in other languages, for example using foreign lan-
guage WordNet interlingual indexes (Gonzalo et al,
1998; Chugur et al, 2002).
2.1 Gold standard sense clustering data
Our approach for learning how to merge senses
relies upon the availability of labeled judgments
of sense relatedness. In this work we focus on
two datasets of hand-labeled sense groupings for
WordNet: first, a dataset of sense groupings over
nouns, verbs, and adjectives provided as part of
the SENSEVAL-2 English lexical sample WSD task
(Kilgarriff, 2001), and second, a corpus-driven map-
ping of nouns and verbs in WordNet 2.1 to the
Omega Ontology (Philpot et al, 2005), produced as
part of the ONTONOTES project (Hovy et al, 2006).
A wide variety of semantic and syntactic criteria
were used to produce the SENSEVAL-2 groupings
(Palmer et al, 2004; Palmer et al, 2005); this data
covers all senses of 411 nouns, 519 verbs, and 257
adjectives, and has been used as gold standard sense
clustering data in previous work (Agirre and Lopez,
2003; McCarthy, 2006)2. The number of judgments
within this data (after mapping to WordNet 2.1) is
displayed in Table 1.
Due to a lack of interannotator agreement data for
this dataset, (McCarthy, 2006) performed an anno-
tation study using three labelers on a 20-noun sub-
set of the SENSEVAL-2 groupings; the three label-
ers were given the task of deciding whether the 351
potentially-related sense pairs were ?Related?, ?Un-
related?, or ?Don?t Know?.3 In this task the pair-
2In order to facilitate future work in this area, we
have made cleaned versions of these groupings available at
http://ai.stanford.edu/?rion/swn along with a ?diff? with the
original files.
3McCarthy?s gold standard data is available at
SENSEVAL-2
POS Total Pairs Merged Pairs Proportion
Nouns 16403 2593 0.1581
Verbs 30688 3373 0.1099
Adjectives 8368 2209 0.2640
ONTONOTES
POS Total Pairs Merged Pairs Proportion
Nouns 3552 347 0.0977
Verbs 4663 1225 0.2627
Table 1: Gold standard datasets for sense merging;
only sense pairs that share a word in common are
included; proportion refers to the fraction of synsets
sharing a word that have been merged
POS Overlap ON-True ON-False F-Score
S-T S-F S-T S-F
Nouns 2116 121 55 181 1759 0.5063
Verbs 3297 351 503 179 2264 0.5072
Table 2: Agreement data for gold standard datasets
wise interannotator F-scores were (0.4874, 0.5454,
0.7926), for an average F-score of 0.6084.
The ONTONOTES dataset4 covers a smaller set
of nouns and verbs, but it has been created with a
more rigorous corpus-based iterative annotation pro-
cess. For each of the nouns and verbs in question, a
50-sentence sample of instances is annotated using
a preliminary set of sense distinctions; if the word
sense interannotator agreement for the sample is less
than 90%, then the sense distinctions are revised and
the sample is re-annotated, and so forth, until an in-
terannotator agreement of at least 90% is reached.
We construct a combined gold standard set from
these SENSEVAL-2 and ONTONOTES groupings,
removing disagreements. The overlap and agree-
ment/disagreement data between the two groupings
is given in Table 2; here, for example, the column
with ON-True and S-F indicates the count of senses
that ONTONOTES judged as positive examples of
sense merging, but that SENSEVAL-2 data did not
merge. We also calculate the F-score achieved by
considering only one of the datasets as a gold stan-
dard, and computing precision and recall for the
other. Since the two datasets were created indepen-
dently, with different annotation guidelines, we can-
ftp://ftp.informatics.susx.ac.uk/pub/users/dianam/relateGS/.
4The OntoNotes groupings will be available through the
LDC at http://www.ldc.upenn.edu.
1007
not consider this as a valid estimate of interannota-
tor agreement; nonetheless the F-score for the two
datasets on the overlapping set of sense judgments
(50.6% for nouns and 50.7% for verbs) is roughly
in the same range as those observed in (McCarthy,
2006).
3 Learning to merge word senses
3.1 WordNet-based features
Here we describe the feature space we construct for
classifying whether or not a pair of synsets should be
merged; first, we employ a wide variety of linguistic
features based on information derived from Word-
Net. We use eight similarity measures implemented
within the WordNet::Similarity package5, described
in (Pedersen et al, 2004); these include three mea-
sures derived from the paths between the synsets
in WordNet: HSO (Hirst and St-Onge, 1998), LCH
(Leacock and Chodorow, 1998), and WUP (Wu and
Palmer, 1994); three measures based on information
content: RES (Resnik, 1995), LIN (Lin, 1998), and
JCN (Jiang and Conrath, 1997); the gloss-based Ex-
tended Lesk Measure LESK, (Banerjee and Peder-
sen, 2003), and finally the gloss vector similarity
measure VECTOR (Patwardan, 2003). We imple-
ment the TWIN feature (Peters et al, 1998), which
counts the number of shared synonyms between
the two synsets. Additionally we produce pair-
wise features indicating whether two senses share an
ANTONYM, PERTAINYM, or derivationally-related
forms (DERIV). We also create the verb-specific
features of whether two verb synsets are linked in
a VERBGROUP (indicating semantic similarity) or
share a VERBFRAME, indicating syntactic similar-
ity. Also, we encode a generalized notion of sib-
linghood in the MN features, recording the distance
of the synset pair?s nearest least common subsumer
(i.e., closest shared hypernym) from the two synsets,
and, separately, the maximum of those distances (in
the MAXMN feature.
Previous attempts at categorizing systematic pol-
ysemy patterns within WordNet has resulted in the
COUSIN feature6; we create binary features which
5We choose not to use the PATH measure due to its negligible
difference from the LCH measure.
6This data is included in the WordNet 1.6 distribution as the
?cousin.tops? file.
indicate whether a synset pair belong to hypernym
ancestries indicated by one or more of these COUSIN
features, and the specific cousin pair(s) involved.
Finally we create sense-specific features, including
SENSECOUNT, the total number of senses associ-
ated with the shared word between the two synsets
with the highest number of senses, and SENSENUM,
the specific pairing of senses for the shared word
with the highest number of senses (which might al-
low us to learn whether the most frequent sense of a
word has a higher chance of having similar deriva-
tive senses with lower frequency).
3.2 Features derived from corpora and other
lexical resources
In addition to WordNet-based features, we use
a number of features derived from corpora and
other lexical resources. We use the publicly avail-
able topic signature data7 described in (Agirre and
Lopez, 2004), yielding representative contexts for
all nominal synsets from WordNet 1.6. These topic
signatures were obtained by weighting the contexts
of monosemous relatives of each noun synset (i.e.,
single-sense synsets related by hypernym, hyponym,
or other relations); the text for these contexts were
extracted from snippets using the Google search en-
gine. We then create a sense similarity feature by
taking a thresholded cosine similarity between pairs
of topic signatures for these noun synsets.
Additionally, we use the WordNet domain dataset
described in (Magnini and Cavaglia, 2000; Ben-
tivogli et al, 2004). This dataset contains one or
more labels indicating of 164 hierarchically orga-
nized ?domains? or ?subject fields? for each noun,
verb, and adjective synset in WordNet; we derive a
set of binary features from this data, with a single
feature indicating whether or not two synsets share
a domain, and one indicator feature per pair of do-
mains indicating respective membership of the sense
pair within those domains.
Finally, we use as a feature the mappings pro-
duced in (Navigli, 2006) of WordNet senses to Ox-
ford English Dictionary senses. This OED dataset
was used as the coarse-grained sense inventory in the
Coarse-grained English all-words task of SemEval-
7The topic signature data is available for download at
http://ixa.si.ehu.es/Ixa/resources/sensecorpus.
1008
20078; we specify a single binary feature for each
pair of synsets from this data; this feature is true if
the words are clustered in the OED mapping, and
false otherwise.
3.3 Classifier, training, and feature selection
For each part of speech, we split the merged gold
standard data into a part-of-speech-specific train-
ing set (70%) and a held-out test set (30%). For
every synset pair we use the binary ?merged? or
?not-merged? labels to train a support vector ma-
chine classifier9 (Joachims, 2002) for each POS-
specific training set. We perform feature selection
and regularization parameter optimization using 10-
fold cross-validation.
4 Clustering Senses in WordNet
The previous section describes a classifier which
predicts whether two synsets should be merged; we
would like to use the pairwise judgments of this
classifier to cluster the senses within a sense hierar-
chy. In this section we present the challenge implicit
in applying sense merging to full taxonomies, and
present our model for clustering within a taxonomy.
4.1 Challenges of clustering a sense taxonomy
The task of clustering a sense taxonomy presents
certain challenges not present in the problem of clus-
tering the senses of a word; in order to create a
consistent clustering of a sense hierarchy an algo-
rithm must consider the transitive effects of merging
synsets. This problem is compounded in sense tax-
onomies like WordNet, where each synset may have
additional structured relations, e.g., hypernym (IS-
A) or holonym (is-part-of) links. In order to consis-
tently merge two noun senses with different hyper-
nym ancestries within WordNet, for example, an al-
gorithm must decide whether to have the new sense
inherit both hypernym ancestries, or whether to in-
herit only one, and if so it must decide which ances-
try is more relevant for the merged sense.
Without strict checking, human labelers will
likely find it difficult to label a sense inventory with
8http://lcl.di.uniroma1.it/coarse-grained-aw/index.html
9We use the SV Mperf package, freely available for non-
commercial use from http://svmlight.joachims.org; we use the
default settings in v2.00, except for the regularization parameter
(set in 10-fold cross-validation).
Clusering based on ??need??
Clustering based on ??require??
need#v#1
require#v#1 require as useful, just, or proper
need#v#2
require#v#4 have need of
need#v#3 have or feel a need for
require#v#1
need#v#1 require as useful, just, or proper
require#v#4
need#v#2 have need of
require#v#2 consider obligatory; request and expect
require#v#3 make someone do something
Figure 2: Inconsistent sense clusters for the verbs
require and need from SENSEVAL-2 judgments
transitively-consistent judgments. As an example,
consider the SENSEVAL-2 clusterings of the verbs
require and need, as shown in Figure 2. In WN 2.1
require has four verb senses, of which the first has
synonyms {necessitate, ask, postulate, need, take,
involve, call for, demand}, and gloss ?require as use-
ful, just, or proper?; and the fourth has synonyms
{want, need}, and gloss ?have need of.?
Within the word require, the SENSEVAL-2 dataset
clusters senses 1 and 4, leaving the rest unclustered.
In order to make a consistent clustering with respect
to the sense inventory, however, we must enforce
the transitive closure by merging the synset corre-
sponding to the first sense (necessitate, ask, need
etc.), with the senses of want and need in the fourth
sense. In particular, these two senses correspond
to WordNet 2.1 senses need#v#1 and need#v#2, re-
spectively, which are not clustered according to
the SENSEVAL-2 word-specific labeling for need ?
need#v#1 is listed as a singleton (i.e., unclustered)
sense, though need#v#2 is clustered with need#v#3,
?have or feel a need for.?
While one might hope that such disagreements
between sense clusterings are rare, we found
178 such transitive closure disagreements in the
SENSEVAL-2 data. The ONTONOTES data is much
cleaner in this respect, most likely due to the
stricter annotation standard (Hovy et al, 2006);
we found only one transitive closure disagreement
1009
in the OntoNotes data, specifically WordNet 2.1
synsets (head#n#2, lead#n#7: ?be in charge of?) and
(head#n#3, lead#v#4: ?travel in front of?) are clus-
tered under head but not under lead.
4.2 Sense clustering within a taxonomy
As a solution to the previously mentioned chal-
lenges, in order to produce taxonomies of different
sense granularities with consistent sense distinctions
we propose to apply agglomerative clustering over
all synsets in WordNet 2.1. While one might con-
sider recalculating synset similarity features after
each synset merge operation, depending on the fea-
ture set this could be prohibitively expensive; for our
purposes we use average-link agglomerative cluster-
ing, in effect approximating the the pairwise similar-
ity score between a given synset and a merged sense
as the average of the similarity scores between the
given synset and the clustered sense?s component
synsets. Further, for the purpose of sense cluster-
ing we assume a zero sense similarity score between
synsets with no intersecting words.
Without exploiting additional hypernym or
coordinate-term evidence, our algorithm does
not distinguish between judgments about which
hypernym ancestry or other structured relationships
to keep or remove upon merging two synsets. In
lieu of additional evidence, for our experiments
we choose to retain only the hypernym ancestry of
the sense with the highest frequency in SEMCOR,
breaking frequency ties by choosing the first-listed
sense in WordNet. We add every other relationship
(meronyms, entailments, etc.) to the new merged
sense (except in the rare case where adding a
relation would cause a cycle in acyclic relations like
hypernymy or holonymy, in which case we omit
it). Using this clustering method we have produced
several sense-clustered WordNets of varying sense
granularity, which we evaluate in Section 5.3.
5 Evaluation
We evaluate our classifier in a comparison with thir-
teen previously proposed similarity measures and
automatic methods for sense clustering. We conduct
a feature ablation study to explore the relevance of
the different features in our system. Finally, we eval-
uate the sense-clustered taxonomies we create on
the problem of providing improved coarse-grained
sense distinctions for WSD evaluation.
5.1 Evaluation of automatic sense merging
We evaluate our classifier on two held-out test
sets; first, a 30% sample of the sense judgments
from the merged gold standard dataset consisting
of both the SENSEVAL-2 and ONTONOTES sense
judgments; and, second, a test set consisting of only
the ONTONOTES subset of our first held-out test set.
For comparison we implement thirteen of the meth-
ods discussed in Section 2. First, we evaluate each
of the eight WordNet::Similarity measures individu-
ally. Next, we implement cosine similarity of topic
signatures (TOPSIG) built from monosemous rela-
tives (Agirre and Lopez, 2003), which provides a
real-valued similarity score for noun synset pairs.
Additionally, we implement the two methods
proposed in (Peters et al, 1998), namely using
metonymy clusters (MetClust) and generalization
clusters (GenClust) based on the COUSIN relation-
ship in WordNet. While (Peters et al, 1998) only
considers four cousin pairs, we re-implement their
method for general purpose sense clustering by us-
ing all 226 cousin pairs defined in WordNet 1.6,
mapped to WordNet 2.1 synsets. These methods
each provide a single clustering of noun synsets.
Next, we implement the set of semantic rules de-
scribed in (Mihalcea and Moldovan, 2001) (MIMO);
this algorithm for merging senses is based on 6 se-
mantic rules, in effect using a subset of the TWIN,
MAXMN, PERTAINYM, ANTONYM, and VERB-
GROUP features; in our implementation we set the
parameter for when to cluster based on number of
twins to K = 2; this results in a single clustering
for each of nouns, verbs, and adjectives. Finally, we
compare against the mapping from WordNet to the
Oxford English Dictionary constructed in (Navigli,
2006), equivalent to clustering based solely on the
OED feature.
Considering merging senses as a binary classifi-
cation task, Table 3 gives the F-score performance
of our classifier vs. the thirteen other classifiers and
an uninformed ?merge all synsets? baseline on our
held-out gold standard test set. This table shows that
our SVM classifier outperforms all implemented
methods on the basis of F-score on both datasets
1010
SENSEVAL-2 + ONTONOTES
ONTONOTES
Method Nouns Verbs Adj Nouns Verbs
SVM 0.4228 0.4319 0.4727 0.3698 0.4545
RES 0.3817 0.2703 ? 0.2807 0.3156
WUP 0.3763 0.2782 ? 0.3036 0.3451
LCH 0.3700 0.2440 ? 0.2857 0.3396
OED 0.3310 0.2878 0.3712 0.2183 0.3962
LESK 0.3174 0.2956 0.4323 0.2914 0.3774
HSO 0.3090 0.2784 0.4312 0.3025 0.3156
TOPSIG 0.3072 ? ? 0.2581 ?
VEC 0.2960 0.2315 0.4321 0.2454 0.3420
JCN 0.2818 0.2292 ? 0.2222 0.3156
LIN 0.2759 0.2464 ? 0.2056 0.3471
Baseline 0.2587 0.2072 0.4312 0.1488 0.3156
MIMO 0.0989 0.2142 0.0759 0.1833 0.2157
GenClust 0.0973 ? ? 0.0264 ?
MetClust 0.0876 ? ? 0.0377 ?
Table 3: F-score sense merging evaluation on hand-
labeled testsets
for all parts of speech. In Figure 3 we give a pre-
cision/recall plot for noun sense merge judgments
for the SENSEVAL-2 + ONTONOTES dataset. For
sake of simplicity we plot only the two best mea-
sures (RES and WUP) of the eight WordNet-based
similarity measures; we see that our classifier, RES,
and WUP each have higher precision all levels of
recall compared to the other tested measures.
Of the methods we compare against, only the
WordNet-based similarity measures, (Mihalcea and
Moldovan, 2001), and (Navigli, 2006) provide a
method for predicting verb similarities; our learned
measure widely outperforms these methods, achiev-
ing a 13.6% F-score improvement over the LESK
similarity measure. In Figure 4 we give a pre-
cision/recall plot for verb sense merge judgments,
plotting the performance of the three best WordNet-
based similarity measures; here we see that our clas-
sifier has significantly higher precision than all other
tested measures at nearly every level of recall.
Only the measures provided by LESK, HSO,
VEC, (Mihalcea and Moldovan, 2001), and (Nav-
igli, 2006) provide a method for predicting adjective
similarities; of these, only LESK and VEC outper-
form the uninformed baseline on adjectives, while
our learned measure achieves a 4.0% improvement
over the LESK measure on adjectives.
5.2 Feature analysis
Next we analyze our feature space. Table 4 gives the
ablation analysis for all features used in our system
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Nouns
SVM Classifier
Resnik Measure
Wu & Palmer Measure
Topic Signatures
OED Mapping
Generalization Clusters
Metonym Clusters
Semantic Rules
Figure 3: Precision/Recall plot for noun sense merge
judgments
as evaluated on our held-out test set; here the quan-
tity listed in the table is the F-score loss obtained by
removing that single feature from our feature space,
and retraining and retesting our classifiers, keeping
everything else the same. Here negative scores cor-
respond to an improvement in classifier performance
with the removal of the feature.
For noun classification, the three features that
yield the highest gain in testset F-score are the
topic signature, OED, and derivational link features,
yielding a 4.0%, 3.6%, and 3.5% gain, respectively.
For verb classification, we find that three features
yield more than a 5% F-score gain; by far the largest
single-feature performance gain for verb classifica-
tion found in our ablation study was the DERIV fea-
ture, i.e., the count of shared derivational links be-
tween the two synsets; this single feature improves
our maximum F-score by 9.8% on the testset. This
is a particularly interesting discovery, as none of the
referenced automatic techniques for sense clustering
presently make use of this very useful feature. We
also achieve large gains with the LIN and LESK sim-
ilarity features, with F-score improvement of 7.4%
and 5.4% gain respectively.
For adjective classification again the DERIV fea-
ture proved very helpful, with a 3.5% gain on the
testset. Interestingly, only the DERIV feature and
the SENSECNT features helped across all parts of
speech; in many cases a feature which proved to be
1011
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Verbs
SVM Classifier
Lesk Measure
Hirst & St?Onge
Wu & Palmer
OED Mapping
Semantic Rules
Figure 4: Precision/Recall plot for verb sense merge
judgments
very helpful for one part of speech actually hurt per-
formance on another part of speech (e.g., LIN on
nouns and OED on adjectives).
5.3 Evaluation of sense-clustered Wordnets
Our goal in clustering a sense taxonomy is to pro-
duce fully sense-clustered WordNets, and to be able
to produce coarse-grained Wordnets at many differ-
ent levels of resolution. In order to evaluate the en-
tire sense-clustered taxonomy, we have employed an
evaluation method inspired by Word Sense Disam-
biguation (this is similar to an evaluation used in
Navigli, 2006, however we do not remove monose-
mous clusters). Given past system responses in the
SENSEVAL-3 English all-words task, we can eval-
uate past systems on the same corpus, but using
the coarse-grained sense hierarchy provided by our
sense-clustered taxonomy. We may then compare
the scores of each system on the coarse-grained task
against their scores given a random clustering at the
same resolution. Our expectation is that, if our sense
clustering is much better than a random sense clus-
tering (and, of course, that the WSD algorithms per-
form better than random guessing), we will see a
marked improvement in the performance of WSD
algorithms using our coarse-grained sense hierarchy.
We consider the outputs of the top 3 all-
words WSD systems that participated in Senseval-3:
Gambl (Decadt et al, 2004), SenseLearner (Mihal-
cea and Faruque, 2004), and KOC University (Yuret,
Nouns Verbs Adjectives
F-SCORE 0.4228 0.4319 0.4727
Feature F-Score Ablation Difference
TOPSIG 0.0403 ? ?
OED 0.0355 0.0126 -0.0124
DERIV 0.0351 0.0977 0.0352
RES 0.0287 0.0147 ?
TWIN 0.0285 0.0109 -0.0130
MN 0.0188 0.0358 ?
LESK 0.0183 0.0541 -0.0250
SENSENUM 0.0155 0.0146 -0.0147
SENSECNT 0.0121 0.0160 0.0168
DOMAIN 0.0119 0.0082 -0.0265
LCH 0.0099 0.0068 ?
WUP 0.0036 0.0168 ?
JCN 0.0025 0.0190 ?
ANTONYM 0.0000 0.0295 0.0000
MAXMN -0.0013 0.0179 ?
VEC -0.0024 0.0371 -0.0062
HSO -0.0073 0.0112 -0.0246
LIN -0.0086 0.0742 ?
COUSIN -0.0094 ? ?
VERBGRP ? 0.0327 ?
VERBFRM ? 0.0102 ?
PERTAINYM ? ? -0.0029
Table 4: Feature ablation study; F-score difference
obtained by removal of the single feature
2004). A guess by a system is given full credit if it
was either the correct answer or if it was in the same
cluster as the correct answer.
Clearly any amount of clustering will only in-
crease WSD performance. Therefore, to account for
this natural improvement and consider only the ef-
fect of our particular clustering, we also calculate
the expected score for a random clustering of the
same granularity, as follows: Let C represent the set
of clusters over the possible N synsets containing a
given word; we then calculate the expectation that an
incorrectly-chosen sense and the actual correct sense
would be clustered together in the random clustering
as
P
c?C |c|(|c|?1)
N(N?1) .
Our sense clustering algorithm provides little im-
provement over random clustering when too few or
too many clusters are chosen; however, with an ap-
propriate threshold for average-link clustering we
find a maximum of 3.55% F-score improvement in
WSD over random clustering (averaged over the de-
cisions of the top 3 WSD algorithms). Table 5 shows
the improvement of the three top WSD algorithms
given a sense clustering created by our algorithm vs.
a random clustering at the same granularity.
1012
0 0.5 1 1.5 2 2.5 3 3.5
x 104
0.65
0.7
0.75
0.8
Sense Merge Iterations
W
SD
 F
?S
co
re
Group Average Agglomerative Clustering
Random Clustering
Figure 5: WSD Improvement with coarse-grained
sense hierarchies
System F-score Avg-link Random Impr.
Gambl 0.6516 0.7702 0.7346 0.0356
SenseLearner 0.6458 0.7536 0.7195 0.0341
KOC Univ. 0.6414 0.7521 0.7153 0.0368
Table 5: Improvement in SENSEVAL-3 WSD perfor-
mance using our average-link agglomerative cluster-
ing vs. random clustering at the same granularity
6 Conclusion
We have presented a classifier for automatic sense
merging that significantly outperforms previously
proposed automatic methods. In addition to its novel
use of supervised learning and the integration of
many previously proposed features, it is interest-
ing that one of our new features, the DERIV count
of shared derivational links between two synsets,
proved an extraordinarily useful new cue for sense-
merging, particularly for verbs.
We also show how to integrate this sense-merging
algorithm into a model for sense clustering full sense
taxonomies like WordNet, incorporating taxonomic
constraints such as the transitive effects of merging
synsets. Using this model, we have produced several
WordNet taxonomies of various sense granularities;
we hope these new lexical resources will be useful
for NLP applications that require a coarser-grained
sense hierarchy than that already found in WordNet.
Acknowledgments
Thanks to Marie-Catherine de Marneffe, Mona
Diab, Christiane Fellbaum, Thad Hughes, and Ben-
jamin Packer for useful discussions. Rion Snow is
supported by an NSF Fellowship. This work was
supported in part by the Disruptive Technology Of-
fice (DTO)?s Advanced Question Answering for In-
telligence (AQUAINT) Phase III Program.
References
Eneko Agirre and Oier Lopez de Lacalle. 2003. Cluster-
ing WordNet word senses. In Proceedings of RANLP
2003.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nomi-
nal senses. In Proceedings of LREC 2004.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
Gloss Overlaps as a Measure of Semantic Relatedness.
In Proceedings of IJCAI 2003.
Lisa Bentivogli, Pamela Forner, Bernardo Magnini, and
Emanuele Pianta. 2004. Revising the WordNet Do-
mains Hierarchy: Semantics, Coverage, and Balanc-
ing. In Proceedings of COLING Workshop on Multi-
lingual Linguistic Resources, 2004.
Timothy Chklovski and Rada Mihalcea. 2003. Exploit-
ing Agreement and Disagreement of Human Annota-
tors for Word Sense Disambiguation. In Proceedings
of RANLP 2003.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and Sense Proximity in the Senseval-2 Test
Suite. In Proceedings of ACL 2002 WSD Workshop.
Bart Decadt, Veronique Hoste, Walter Daelemans, and
Antal van den Bosch. 2004. Gamble, genetic algo-
rithm optimization of memory-based wsd. In Proceed-
ings of ACL/SIGLEX Senseval-3.
William Dolan. 1994. Word Sense Ambiguation: Clus-
tering Related Senses. In Proceedings of ACL 1994.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Julio Gonzalo, Felia Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings of COLING-
ACL 1998 Workshop on WordNet in NLP Systems.
Patrick Hanks. 2000. Do word meanings exist? Com-
puters and the Humanities, 34(1-2): 171-177.
1013
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. In WordNet: An Electronic
Lexical Database.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. Proceedings of HLT-NAACL 2006.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics, 19-33.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines. Dissertation, Kluwer,
2002.
Adam Kilgariff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(1-2): 1-13.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the SENSEVAL-2 work-
shop, 17-20.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. In WordNet: An Electronic Lexi-
cal Database.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of SIG-
DOC 1986.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML 1998.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998.
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC 2000.
Diana McCarthy. 2006. Relating WordNet Senses for
Word Sense Disambiguation. In Proceedings of ACL
Workshop on Making Sense of Sense, 2006.
Rada Mihalcea and Dan I. Moldovan. 2001. Automatic
Generation of a Coarse Grained WordNet. In Proceed-
ings of NAACL Workshop on WordNet and Other Lex-
ical Resources.
Rada Mihalcea and Ehsanul Faruque. 2004. Sense-
learner: Minimally supervised word sense disam-
biguation for all words in open text. In Proceedings
of ACL/SIGLEX Senseval-3.
Dan I. Moldovan and Rada Mihalcea. 2000. Using
WordNet and lexical operators to improve Internet
searches. IEEE Internet Computing, 4(1):34-43.
Roberto Navigli. 2006. Meaningful Clustering of
Senses Helps Boost Word Sense Disambiguation Per-
formance. In Proceedings of COLING-ACL 2006.
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang.
2004. Different Sense Granularities for Different Ap-
plications. In Proceedings of Workshop on Scalable
Natural Language Understanding.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005. Making fine-grained and coarse-grained
sense distinctions. Journal of Natural Language Engi-
neering.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector measure
of semantic relatedness. Master?s thesis, Univ. of Min-
nesota, Duluth.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of NAACL
2004.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional Clustering of English Words. In Pro-
ceedings of ACL 1993.
Wim Peters, Ivonne Peters, and Piek Vossen. 1998. Au-
tomatic Sense Clustering in EuroWordNet. In Pro-
ceedings of LREC 1998.
Andrew Philpot, Eduard Hovy, and Patrick Pantel. 2005.
The Omega Ontology. In Proceedings of the ON-
TOLEX Workshop at IJCNLP 2005.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the IJCAI 1995, 448-453.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2):113-134.
Noriko Tomuro. 2001. Tree-cut and A Lexicon based
on Systematic Polysemy. In Proceedings of NAACL
2001.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of ACL 1994.
Deniz Yuret. 2004. Some experiments with a naive
bayes wsd system. In Proceedings of ACL/SIGLEX
Senseval-3.
1014
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254?263,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Language Tasks
Rion Snow? Brendan O?Connor? Daniel Jurafsky? Andrew Y. Ng?
?Computer Science Dept.
Stanford University
Stanford, CA 94305
{rion,ang}@cs.stanford.edu
?Dolores Labs, Inc.
832 Capp St.
San Francisco, CA 94110
brendano@doloreslabs.com
?Linguistics Dept.
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Human linguistic annotation is crucial for
many natural language processing tasks but
can be expensive and time-consuming. We ex-
plore the use of Amazon?s Mechanical Turk
system, a significantly cheaper and faster
method for collecting annotations from a
broad base of paid non-expert contributors
over the Web. We investigate five tasks: af-
fect recognition, word similarity, recognizing
textual entailment, event temporal ordering,
and word sense disambiguation. For all five,
we show high agreement between Mechani-
cal Turk non-expert annotations and existing
gold standard labels provided by expert label-
ers. For the task of affect recognition, we also
show that using non-expert labels for training
machine learning algorithms can be as effec-
tive as using gold standard annotations from
experts. We propose a technique for bias
correction that significantly improves annota-
tion quality on two tasks. We conclude that
many large labeling tasks can be effectively
designed and carried out in this method at a
fraction of the usual expense.
1 Introduction
Large scale annotation projects such as TreeBank
(Marcus et al, 1993), PropBank (Palmer et
al., 2005), TimeBank (Pustejovsky et al, 2003),
FrameNet (Baker et al, 1998), SemCor (Miller et
al., 1993), and others play an important role in
natural language processing research, encouraging
the development of novel ideas, tasks, and algo-
rithms. The construction of these datasets, how-
ever, is extremely expensive in both annotator-hours
and financial cost. Since the performance of many
natural language processing tasks is limited by the
amount and quality of data available to them (Banko
and Brill, 2001), one promising alternative for some
tasks is the collection of non-expert annotations.
In this work we explore the use of Amazon Me-
chanical Turk1 (AMT) to determine whether non-
expert labelers can provide reliable natural language
annotations. We chose five natural language under-
standing tasks that we felt would be sufficiently nat-
ural and learnable for non-experts, and for which
we had gold standard labels from expert labelers,
as well as (in some cases) expert labeler agree-
ment information. The tasks are: affect recogni-
tion, word similarity, recognizing textual entailment,
event temporal ordering, and word sense disam-
biguation. For each task, we used AMT to annotate
data and measured the quality of the annotations by
comparing them with the gold standard (expert) la-
bels on the same data. Further, we compare machine
learning classifiers trained on expert annotations vs.
non-expert annotations.
In the next sections of the paper we introduce
the five tasks and the evaluation metrics, and offer
methodological insights, including a technique for
bias correction that improves annotation quality.2
1 http://mturk.com
2 Please see http://blog.doloreslabs.com/?p=109
for a condensed version of this paper, follow-ups, and on-
going public discussion. We encourage comments to be di-
rected here in addition to email when appropriate. Dolores
Labs Blog, ?AMT is fast, cheap, and good for machine learning
data,? Brendan O?Connor, Sept. 9, 2008. More related work at
http://blog.doloreslabs.com/topics/wisdom/.
254
2 Related Work
The idea of collecting annotations from volunteer
contributors has been used for a variety of tasks.
Luis von Ahn pioneered the collection of data via
online annotation tasks in the form of games, includ-
ing the ESPGame for labeling images (von Ahn and
Dabbish, 2004) and Verbosity for annotating word
relations (von Ahn et al, 2006). The Open Mind
Initiative (Stork, 1999) has taken a similar approach,
attempting to make such tasks as annotating word
sense (Chklovski and Mihalcea, 2002) and common-
sense word relations (Singh, 2002) sufficiently ?easy
and fun? to entice users into freely labeling data.
There have been an increasing number of experi-
ments using Mechanical Turk for annotation. In (Su
et al, 2007) workers provided annotations for the
tasks of hotel name entity resolution and attribute
extraction of age, product brand, and product model,
and were found to have high accuracy compared
to gold-standard labels. Kittur et al (2008) com-
pared AMT evaluations of Wikipedia article qual-
ity against experts, finding validation tests were im-
portant to ensure good results. Zaenen (Submitted)
studied the agreement of annotators on the problem
of recognizing textual entailment (a similar task and
dataset is explained in more detail in Section 4).
At least several studies have already used AMT
without external gold standard comparisons. In
(Nakov, 2008) workers generated paraphrases of
250 noun-noun compounds which were then used
as the gold standard dataset for evaluating an au-
tomatic method of noun compound paraphrasing.
Kaisser and Lowe (2008) use AMT to help build a
dataset for question answering, annotating the an-
swers to 8107 questions with the sentence contain-
ing the answer. Kaisser et al (2008) examines the
task of customizing the summary length of QA out-
put; non-experts from AMT chose a summary length
that suited their information needs for varying query
types. Dakka and Ipeirotis (2008) evaluate a docu-
ment facet generation system against AMT-supplied
facets, and also use workers for user studies of the
system. Sorokin and Forsyth (2008) collect data for
machine vision tasks and report speed and costs sim-
ilar to our findings; their summaries of worker be-
havior also corroborate with what we have found.
In general, volunteer-supplied or AMT-supplied
data is more plentiful but noisier than expert data.
It is powerful because independent annotations can
be aggregated to achieve high reliability. Sheng et
al. (2008) explore several methods for using many
noisy labels to create labeled data, how to choose
which examples should get more labels, and how to
include labels? uncertainty information when train-
ing classifiers. Since we focus on empirically val-
idating AMT as a data source, we tend to stick to
simple aggregation methods.
3 Task Design
In this section we describe Amazon Mechanical
Turk and the general design of our experiments.
3.1 Amazon Mechanical Turk
We employ the Amazon Mechanical Turk system
in order to elicit annotations from non-expert label-
ers. AMT is an online labor market where workers
are paid small amounts of money to complete small
tasks. The design of the system is as follows: one is
required to have an Amazon account to either sub-
mit tasks for annotations or to annotate submitted
tasks. These Amazon accounts are anonymous, but
are referenced by a unique Amazon ID. A Requester
can create a group of Human Intelligence Tasks (or
HITs), each of which is a form composed of an arbi-
trary number of questions. The user requesting an-
notations for the group of HITs can specify the num-
ber of unique annotations per HIT they are willing
to pay for, as well as the reward payment for each
individual HIT. While this does not guarantee that
unique people will annotate the task (since a single
person could conceivably annotate tasks using mul-
tiple accounts, in violation of the user agreement),
this does guarantee that annotations will be collected
from unique accounts. AMT also allows a requester
to restrict which workers are allowed to annotate a
task by requiring that all workers have a particular
set of qualifications, such as sufficient accuracy on
a small test set or a minimum percentage of previ-
ously accepted submissions. Annotators (variously
referred to as Workers or Turkers) may then annotate
the tasks of their choosing. Finally, after each HIT
has been annotated, the Requester has the option of
approving the work and optionally giving a bonus
to individual workers. There is a two-way commu-
255
nication channel between the task designer and the
workers mediated by Amazon, and Amazon handles
all financial transactions.
3.2 Task Design
In general we follow a few simple design principles:
we attempt to keep our task descriptions as succinct
as possible, and we attempt to give demonstrative
examples for each class wherever possible. We have
published the full experimental design and the data
we have collected for each task online3. We have
restricted our study to tasks where we require only
a multiple-choice response or numeric input within
a fixed range. For every task we collect ten inde-
pendent annotations for each unique item; this re-
dundancy allows us to perform an in-depth study of
how data quality improves with the number of inde-
pendent annotations.
4 Annotation Tasks
We analyze the quality of non-expert annotations on
five tasks: affect recognition, word similarity, rec-
ognizing textual entailment, temporal event recogni-
tion, and word sense disambiguation. In this section
we define each annotation task and the parameters
of the annotations we request using AMT. Addition-
ally we give an initial analysis of the task results,
and summarize the cost of the experiments.
4.1 Affective Text Analysis
This experiment is based on the affective text an-
notation task proposed in Strapparava and Mihalcea
(2007), wherein each annotator is presented with a
list of short headlines, and is asked to give numeric
judgments in the interval [0,100] rating the headline
for six emotions: anger, disgust, fear, joy, sadness,
and surprise, and a single numeric rating in the inter-
val [-100,100] to denote the overall positive or nega-
tive valence of the emotional content of the headline,
as in this sample headline-annotation pair:
Outcry at N Korea ?nuclear test?
(Anger, 30), (Disgust,30), (Fear,30), (Joy,0),
(Sadness,20), (Surprise,40), (Valence,-50).
3All tasks and collected data are available at
http://ai.stanford.edu/
?
rion/annotations/.
For our experiment we select a 100-headline sample
from the original SemEval test set, and collect 10
affect annotations for each of the seven label types,
for a total of 7000 affect labels.
We then performed two comparisons to evaluate
the quality of the AMT annotations. First, we asked
how well the non-experts agreed with the experts.
We did this by comparing the interannotator agree-
ment (ITA) of individual expert annotations to that
of single non-expert and averaged non-expert anno-
tations. In the original experiment ITA is measured
by calculating the Pearson correlation of one anno-
tator?s labels with the average of the labels of the
other five annotators. For each expert labeler, we
computed this ITA score of the expert against the
other five; we then average these ITA scores across
all expert annotators to compute the average expert
ITA (reported in Table 1 as ?E vs. E?. We then do the
same for individual non-expert annotations, averag-
ing Pearson correlation across all sets of the five ex-
pert labelers (?NE vs. E?). We then calculate the ITA
for each expert vs. the averaged labels from all other
experts and non-experts (marked as ?E vs. All?) and
for each non-expert vs. the pool of other non-experts
and all experts (?NE vs. All?). We compute these
ITA scores for each emotion task separately, aver-
aging the six emotion tasks as ?Avg. Emo? and the
average of all tasks as ?Avg. All?.
Emotion E vs. E E vs. All NE vs. E NE vs. All
Anger 0.459 0.503 0.444 0.573
Disgust 0.583 0.594 0.537 0.647
Fear 0.711 0.683 0.418 0.498
Joy 0.596 0.585 0.340 0.421
Sadness 0.645 0.650 0.563 0.651
Surprise 0.464 0.463 0.201 0.225
Valence 0.759 0.767 0.530 0.554
Avg. Emo 0.576 0.603 0.417 0.503
Avg. All 0.580 0.607 0.433 0.510
Table 1: Average expert and non-expert ITA on test-set
The results in Table 1 conform to the expectation
that experts are better labelers: experts agree with
experts more than non-experts agree with experts,
although the ITAs are in many cases quite close. But
we also found that adding non-experts to the gold
standard (?E vs. All?) improves agreement, suggest-
ing that non-expert annotations are good enough to
increase the overall quality of the gold labels. Our
256
first comparison showed that individual experts were
better than individual non-experts. In our next com-
parison we ask how many averaged non-experts it
would take to rival the performance of a single ex-
pert. We did this by averaging the labels of each pos-
sible subset of n non-expert annotations, for value
of n in {1, 2, . . . , 10}. We then treat this average as
though it is the output of a single ?meta-labeler?, and
compute the ITA with respect to each subset of five
of the six expert annotators. We then average the
results of these studies across each subset size; the
results of this experiment are given in Table 2 and in
Figure 1. In addition to the single meta-labeler, we
ask: what is the minimum number of non-expert an-
notations k from which we can create a meta-labeler
that has equal or better ITA than an expert annotator?
In Table 2 we give the minimum k for each emotion,
and the averaged ITA for that meta-labeler consist-
ing of k non-experts (marked ?k-NE?). In Figure 1
we plot the expert ITA correlation as the horizontal
dashed line.
Emotion 1-Expert 10-NE k k-NE
Anger 0.459 0.675 2 0.536
Disgust 0.583 0.746 2 0.627
Fear 0.711 0.689 ? ?
Joy 0.596 0.632 7 0.600
Sadness 0.645 0.776 2 0.656
Surprise 0.464 0.496 9 0.481
Valence 0.759 0.844 5 0.803
Avg. Emo. 0.576 0.669 4 0.589
Avg. All 0.603 0.694 4 0.613
Table 2: Average expert and averaged correlation over
10 non-experts on test-set. k is the minimum number of
non-experts needed to beat an average expert.
These results show that for all tasks except ?Fear?
we are able to achieve expert-level ITA with the
held-out set of experts within 9 labelers, and fre-
quently within only 2 labelers. Pooling judgments
across all 7 tasks we find that on average it re-
quires only 4 non-expert annotations per example to
achieve the equivalent ITA as a single expert anno-
tator. Given that we paid US$2.00 in order to collect
the 7000 non-expert annotations, we may interpret
our rate of 3500 non-expert labels per USD as at
least 875 expert-equivalent labels per USD.
4.2 Word Similarity
This task replicates the word similarity task used in
(Miller and Charles, 1991), following a previous
2 4 6 8 100
.4
5
0.
55
0.
65
co
rr
e
la
tio
n
anger
2 4 6 8 10
0.
55
0.
65
0.
75
co
rr
e
la
tio
n
disgust
2 4 6 8 100
.4
0
0.
50
0.
60
0.
70
co
rr
e
la
tio
n
fear
2 4 6 8 10
0.
35
0.
45
0.
55
0.
65
co
rr
e
la
tio
n
joy
2 4 6 8 100
.5
5
0.
65
0.
75
annotators
co
rr
e
la
tio
n
sadness
2 4 6 8 100
.2
0
0.
30
0.
40
0.
50
annotators
co
rr
e
la
tio
n
surprise
Figure 1: Non-expert correlation for affect recognition
task initially proposed by (Rubenstein and Good-
enough, 1965). Specifically, we ask for numeric
judgments of word similarity for 30 word pairs on
a scale of [0,10], allowing fractional responses4 .
These word pairs range from highly similar (e.g.,
{boy, lad}), to unrelated (e.g., {noon, string}). Nu-
merous expert and non-expert studies have shown
that this task typically yields very high interannota-
tor agreement as measured by Pearson correlation;
(Miller and Charles, 1991) found a 0.97 correla-
tion of the annotations of 38 subjects with the an-
notations given by 51 subjects in (Rubenstein and
Goodenough, 1965), and a following study (Resnik,
1999) with 10 subjects found a 0.958 correlation
with (Miller and Charles, 1991).
In our experiment we ask for 10 annotations each
of the full 30 word pairs, at an offered price of $0.02
for each set of 30 annotations (or, equivalently, at
the rate of 1500 annotations per USD). The most
surprising aspect of this study was the speed with
which it was completed; the task of 300 annotations
was completed by 10 annotators in less than 11 min-
4(Miller and Charles, 1991) and others originally used a
numerical score of [0,4].
257
utes from the time of submission of our task to AMT,
at the rate of 1724 annotations / hour.
As in the previous task we evaluate our non-
expert annotations by averaging the numeric re-
sponses from each possible subset of n annotators
and computing the interannotator agreement with
respect to the gold scores reported in (Miller and
Charles, 1991). Our results are displayed in Figure
2, with Resnik?s 0.958 correlation plotted as the hor-
izontal line; we find that at 10 annotators we achieve
a correlation of 0.952, well within the range of other
studies of expert and non-expert annotations.
2 4 6 8 10
0.
84
0.
90
0.
96
annotations
co
rr
e
la
tio
n
Word Similarity ITA
Figure 2: ITA for word similarity experiment
4.3 Recognizing Textual Entailment
This task replicates the recognizing textual entail-
ment task originally proposed in the PASCAL Rec-
ognizing Textual Entailment task (Dagan et al,
2006); here for each question the annotator is pre-
sented with two sentences and given a binary choice
of whether the second hypothesis sentence can be
inferred from the first. For example, the hypothesis
sentence ?Oil prices drop? would constitute a true
entailment from the text ?Crude Oil Prices Slump?,
but a false entailment from ?The government an-
nounced last week that it plans to raise oil prices?.
We gather 10 annotations each for all 800 sen-
tence pairs in the PASCAL RTE-1 dataset. For this
dataset expert interannotator agreement studies have
been reported as achieving 91% and 96% agreement
over various subsections of the corpus. When con-
sidering multiple non-expert annotations for a sen-
tence pair we use simple majority voting, breaking
ties randomly and averaging performance over all
possible ways to break ties. We collect 10 annota-
tions for each of 100 RTE sentence pairs; as dis-
played in Figure 3, we achieve a maximum accu-
racy of 89.7%, averaging over the annotations of 10
workers5.
2 4 6 8 100
.7
0
0.
80
0.
90
annotations
a
cc
u
ra
cy
RTE ITA
Figure 3: Inter-annotator agreement for RTE experiment
4.4 Event Annotation
This task is inspired by the TimeBank corpus (Puste-
jovsky et al, 2003), which includes among its anno-
tations a label for event-pairs that represents the tem-
poral relation between them, from a set of fourteen
relations (before, after, during, includes, etc.). We
implement temporal ordering as a simplified version
of the TimeBank event temporal annotation task:
rather than annotating all fourteen event types, we
restrict our consideration to the two simplest labels:
?strictly before? and ?strictly after?. Furthermore,
rather than marking both nouns and verbs in the text
as possible events, we only consider possible verb
events. We extract the 462 verb event pairs labeled
as ?strictly before? or ?strictly after? in the Time-
Bank corpus, and we present these pairs to annota-
tors with a forced binary choice on whether the event
described by the first verb occurs before or after the
second. For example, in a dialogue about a plane
explosion, we have the utterance: ?It just blew up in
the air, and then we saw two fireballs go down to the,
5It might seem pointless to consider an even number of an-
notations in this circumstance, since the majority voting mech-
anism and tie-breaking yields identical performance for 2n + 1
and 2n + 2 annotators; however, in Section 5 we will consider
methods that can make use of the even annotations.
258
to the water, and there was a big small, ah, smoke,
from ah, coming up from that?. Here for each anno-
tation we highlight the specific verb pair of interest
(e.g., go/coming, or blew/saw) and ask which event
occurs first (here, go and blew, respectively).
The results of this task are presented in Figure 4.
We achieve high agreement for this task, at a rate
of 0.94 with simple voting over 10 annotators (4620
total annotations). While an expert ITA of 0.77 was
reported for the more general task involving all four-
teen labels on both noun and verb events, no expert
ITA numbers have been reported for this simplified
temporal ordering task.
2 4 6 8 100
.7
0
0.
80
0.
90
annotators
a
cc
u
ra
cy
Temp. Ordering ITA
Figure 4: ITA for temporal ordering experiment
4.5 Word Sense Disambiguation
In this task we consider a simple problem on which
machine learning algorithms have been shown to
produce extremely good results; here we annotate
part of the SemEval Word Sense Disambiguation
Lexical Sample task (Pradhan et al, 2007); specif-
ically, we present the labeler with a paragraph of
text containing the word ?president? (e.g., a para-
graph containing ?Robert E. Lyons III...was ap-
pointed president and chief operating officer...?) and
ask the labeler which one of the following three
sense labels is most appropriate:
1) executive officer of a firm, corporation, or university
2) head of a country (other than the U.S.)
3) head of the U.S., President of the United States
We collect 10 annotations for each of 177 examples
of the noun ?president? for the three senses given in
SemEval. As shown in Figure 5, performing simple
majority voting (with random tie-breaking) over an-
notators results in a rapid accuracy plateau at a very
high rate of 0.994 accuracy. In fact, further analy-
sis reveals that there was only a single disagreement
between the averaged non-expert vote and the gold
standard; on inspection it was observed that the an-
notators voted strongly against the original gold la-
bel (9-to-1 against), and that it was in fact found to
be an error in the original gold standard annotation.6
After correcting this error, the non-expert accuracy
rate is 100% on the 177 examples in this task. This
is a specific example where non-expert annotations
can be used to correct expert annotations.
Since expert ITA was not reported per word on
this dataset, we compare instead to the performance
of the best automatic system performance for dis-
ambiguating ?president? in SemEval Task 17 (Cai et
al., 2007), with an accuracy of 0.98.
2 4 6 8 10
0.
98
0
0.
99
0
1.
00
0
annotators
a
cc
u
ra
cy
WSD ITA
Figure 5: Inter-annotator agreement for WSD experiment
4.6 Summary
Cost Time Labels Labels
Task Labels (USD) (hrs) per USD per hr
Affect 7000 $2.00 5.93 3500 1180.4
WSim 300 $0.20 0.174 1500 1724.1
RTE 8000 $8.00 89.3 1000 89.59
Event 4620 $13.86 39.9 333.3 115.85
WSD 1770 $1.76 8.59 1005.7 206.1
Total 21690 25.82 143.9 840.0 150.7
Table 3: Summary of costs for non-expert labels
6The example sentence began ?The Egyptian president said
he would visit Libya today...? and was mistakenly marked as
the ?head of a company? sense in the gold annotation (example
id 24:0@24@wsj/23/wsj 2381@wsj@en@on).
259
0 200 400 600 800
0.
4
0.
6
0.
8
1.
0
number of annotations
a
cc
u
ra
cy
Figure 6: Worker accuracies on the RTE task. Each point
is one worker. Vertical jitter has been added to points on
the left to show the large number of workers who did the
minimum amount of work (20 examples).
In Table 3 we give a summary of the costs asso-
ciated with obtaining the non-expert annotations for
each of our 5 tasks. Here Time is given as the to-
tal amount of time in hours elapsed from submitting
the group of HITs to AMT until the last assignment
is submitted by the last worker.
5 Bias correction for non-expert
annotators
The reliability of individual workers varies. Some
are very accurate, while others are more careless and
make mistakes; and a small few give very noisy re-
sponses. Furthermore, for most AMT data collec-
tion experiments, a relatively small number of work-
ers do a large portion of the task, since workers may
do as much or as little as they please. Figure 6 shows
accuracy rates for individual workers on one task.
Both the overall variability, as well as the prospect
of identifying high-volume but low-quality workers,
suggest that controlling for individual worker qual-
ity could yield higher quality overall judgments.
In general, there are at least three ways to enhance
quality in the face of worker error. More work-
ers can be used, as described in previous sections.
Another method is to use Amazon?s compensation
mechanisms to give monetary bonuses to highly-
performing workers and deny payments to unreli-
able ones; this is useful, but beyond the scope of
this paper. In this section we explore a third alterna-
tive, to model the reliability and biases of individual
workers and correct for them.
A wide number of methods have been explored to
correct for the bias of annotators. Dawid and Skene
(1979) are the first to consider the case of having
multiple annotators per example but unknown true
labels. They introduce an EM algorithm to simul-
taneously estimate annotator biases and latent label
classes. Wiebe et al (1999) analyze linguistic anno-
tator agreement statistics to find bias, and use a sim-
ilar model to correct labels. A large literature in bio-
statistics addresses this same problem for medical
diagnosis. Albert and Dodd (2004) review several
related models, but argue they have various short-
comings and emphasize instead the importance of
having a gold standard.
Here we take an approach based on gold standard
labels, using a small amount of expert-labeled train-
ing data in order to correct for the individual biases
of different non-expert annotators. The idea is to re-
calibrate worker?s responses to more closely match
expert behavior. We focus on categorical examples,
though a similar method can be used with numeric
data.
5.1 Bias correction in categorical data
Following Dawid and Skene, we model labels and
workers with a multinomial model similar to Naive
Bayes. Every example i has a true label xi. For sim-
plicity, assume two labels {Y,N}. Several differ-
ent workers give labels yi1, yi2, . . . yiW . A worker?s
conditional probability of response is modeled as
multinomial, and we model each worker?s judgment
as conditionally independent of other workers given
the true label xi, i.e.:
P (yi1, . . . , yiW , xi) =
(
?
w
P (yiw|xi)
)
p(xi)
To infer the posterior probability of the true label
for a new example, worker judgments are integrated
via Bayes rule, yielding the posterior log-odds:
log
P (xi = Y |yi1 . . . yiW )
P (xi = N |yi1 . . . yiW )
=
?
w
log
P (yiw|xi = Y )
P (yiw|xi = N)
+ log
P (xi = Y )
P (xi = N)
260
The worker response likelihoods P (yw|x = Y )
and P (yw|x = N) can be directly estimated from
frequencies of worker performance on gold standard
examples. (If we used maximum likelihood esti-
mation with no Laplace smoothing, then each yw|x
is just the worker?s empirical confusion matrix.)
For MAP label estimation, the above equation de-
scribes a weighted voting rule: each worker?s vote is
weighted by their log likelihood ratio for their given
response. Intuitively, workers who are more than
50% accurate have positive votes; workers whose
judgments are pure noise have zero votes; and an-
ticorrelated workers have negative votes. (A simpler
form of the model only considers accuracy rates,
thus weighting worker votes by log accw1?accw . But we
use the full unconstrained multinomial model here.)
5.1.1 Example tasks: RTE-1 and event
annotation
We used this model to improve accuracy on the
RTE-1 and event annotation tasks. (The other cate-
gorical task, word sense disambiguation, could not
be improved because it already had maximum accu-
racy.) First we took a sample of annotations giving
k responses per example. Within this sample, we
trained and tested via 20-fold cross-validation across
examples. Worker models were fit using Laplace
smoothing of 1 pseudocount; label priors were uni-
form, which was reasonably similar to the empirical
distribution for both tasks.
annotators
a
cc
u
ra
cy
0.
7
0.
8
0.
9
RTE
annotators
0.
7
0.
8
0.
9
before/after
Gold calibrated
Naive voting
Figure 7: Gold-calibrated labels versus raw labels
Figure 7 shows improved accuracy at different
numbers of annotators. The lowest line is for the
naive 50% majority voting rule. (This is equivalent
to the model under uniform priors and equal accu-
racies across workers and labels.) Each point is the
data set?s accuracy against the gold labels, averaged
across resamplings each of which obtains k annota-
tions per example. RTE has an average +4.0% ac-
curacy increase, averaged across 2 through 10 anno-
tators. We find a +3.4% gain on event annotation.
Finally, we experimented with a similar calibration
method for numeric data, using a Gaussian noise
model for each worker: yw|x ? N(x + ?w, ?w).
On the affect task, this yielded a small but consis-
tent increases in Pearson correlation at all numbers
of annotators, averaging a +0.6% gain.
6 Training a system with non-expert
annotations
In this section we train a supervised affect recogni-
tion system with expert vs. non-expert annotations.
6.1 Experimental Design
For the purpose of this experiment we create a sim-
ple bag-of-words unigram model for predicting af-
fect and valence, similar to the SWAT system (Katz
et al, 2007), one of the top-performing systems on
the SemEval Affective Text task.7 For each token
t in our training set, we assign t a weight for each
emotion e equal to the average emotion score ob-
served in each headline H that t participates in. i.e.,
if Ht is the set of headlines containing the token t,
then:
Score(e, t) =
?
H?Ht Score(e,H)
|Ht|
With these weights of the individual tokens we
may then compute the score for an emotion e of a
new headline H as the average score over the set of
tokens t ? H that we?ve observed in the training set
(ignoring those tokens not in the training set), i.e.:
Score(e,H) =
?
t?H
Score(e, t)
|H|
Where |H| is simply the number of tokens in
headline H , ignoring tokens not observed in the
training set.
7 Unlike the SWAT system we perform no lemmatization,
synonym expansion, or any other preprocessing of the tokens;
we simply use whitespace-separated tokens within each head-
line.
261
6.2 Experiments
We use 100 headlines as a training set (examples
500-599 from the test set of SemEval Task 14), and
we use the remaining 900 headlines as our test set.
Since we are fortunate to have the six separate ex-
pert annotations in this task, we can perform an ex-
tended systematic comparison of the performance of
the classifier trained with expert vs. non-expert data.
Emotion 1-Expert 10-NE k k-NE
Anger 0.084 0.233 1 0.172
Disgust 0.130 0.231 1 0.185
Fear 0.159 0.247 1 0.176
Joy 0.130 0.125 ? ?
Sadness 0.127 0.174 1 0.141
Surprise 0.060 0.101 1 0.061
Valence 0.159 0.229 2 0.146
Avg. Emo 0.116 0.185 1 0.135
Avg. All 0.122 0.191 1 0.137
Table 4: Performance of expert-trained and non-expert-
trained classifiers on test-set. k is the minimum number
of non-experts needed to beat an average expert.
For this evaluation we compare the performance
of systems trained on expert and non-expert annota-
tions. For each expert annotator we train a system
using only the judgments provided by that annota-
tor, and then create a gold standard test set using the
average of the responses of the remaining five label-
ers on that set. In this way we create six indepen-
dent expert-trained systems and compute the aver-
age across their performance, calculated as Pearson
correlation to the gold standard; this is reported in
the ?1-Expert? column of Table 4.
Next we train systems using non-expert labels;
for each possible subset of n annotators, for n ?
{1, 2, . . . , 10} we train a system, and evaluate by
calculating Pearson correlation with the same set of
gold standard datasets used in the expert-trained sys-
tem evaluation. Averaging the results of these stud-
ies yields the results in Table 4.
As in Table 2 we calculate the minimum number
of non-expert annotations per example k required on
average to achieve similar performance to the ex-
pert annotations; surprisingly we find that for five
of the seven tasks, the average system trained with a
single set of non-expert annotations outperforms the
average system trained with the labels from a sin-
gle expert. One possible hypothesis for the cause
of this non-intuitive result is that individual labelers
(including experts) tend to have a strong bias, and
since multiple non-expert labelers may contribute to
a single set of non-expert annotations, the annotator
diversity within the single set of labels may have the
effect of reducing annotator bias and thus increasing
system performance.
7 Conclusion
We demonstrate the effectiveness of using Amazon
Mechanical Turk for a variety of natural language
annotation tasks. Our evaluation of non-expert la-
beler data vs. expert annotations for five tasks found
that for many tasks only a small number of non-
expert annotations per item are necessary to equal
the performance of an expert annotator. In a detailed
study of expert and non-expert agreement for an af-
fect recognition task we find that we require an av-
erage of 4 non-expert labels per item in order to em-
ulate expert-level label quality. Finally, we demon-
strate significant improvement by controlling for la-
beler bias.
Acknowledgments
Thanks to Nathanael Chambers, Annie Zaenen,
Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Car-
penter, David Vickrey, William Morgan, and Lukas
Biewald for useful discussions, and for the gener-
ous support of Dolores Labs. This work was sup-
ported in part by the Disruptive Technology Office
(DTO)?s Advanced Question Answering for Intelli-
gence (AQUAINT) Phase III Program.
References
Paul S. Albert and Lori E. Dodd. 2004. A Cautionary
Note on the Robustness of Latent Class Models for
Estimating Diagnostic Error without a Gold Standard.
Biometrics, Vol. 60 (2004), pp. 427-435.
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
COLING-ACL 1998.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proc. of ACL-2001.
Junfu Cai, Wee Sun Lee and Yee Whye Teh. 2007. Im-
proving Word Sense Disambiguation Using Topic Fea-
tures. In Proc. of EMNLP-2007 .
262
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proc. of the Workshop on ?Word Sense Disam-
biguation: Recent Successes and Future Directions?,
ACL 2002.
Timothy Chklovski and Yolanda Gil. 2005. Towards
Managing Knowledge Collection from Volunteer Con-
tributors. Proceedings of AAAI Spring Symposium
on Knowledge Collection from Volunteer Contributors
(KCVC05).
Ido Dagan, Oren Glickman and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. Machine Learning Challenges. Lecture
Notes in Computer Science, Vol. 3944, pp. 177-190,
Springer, 2006.
Wisam Dakka and Panagiotis G. Ipeirotis. 2008. Au-
tomatic Extraction of Useful Facet Terms from Text
Documents. In Proc. of ICDE-2008.
A. P. Dawid and A. M. Skene. 1979. Maximum Like-
lihood Estimation of Observer Error-Rates Using the
EM Algorithm. Applied Statistics, Vol. 28, No. 1
(1979), pp. 20-28.
Michael Kaisser and John B. Lowe. 2008. A Re-
search Collection of QuestionAnswer Sentence Pairs.
In Proc. of LREC-2008.
Michael Kaisser, Marti Hearst, and John B. Lowe.
2008. Evidence for Varying Search Results Summary
Lengths. In Proc. of ACL-2008.
Phil Katz, Matthew Singleton, Richard Wicentowski.
2007. SWAT-MP: The SemEval-2007 Systems for
Task 5 and Task 14. In Proc. of SemEval-2007.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI-2008.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics 19:2, June 1993.
George A. Miller and William G. Charles. 1991. Con-
textual Correlates of Semantic Similarity. Language
and Cognitive Processes, vol. 6, no. 1, pp. 1-28, 1991.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunke. 1993. A semantic concordance. In
Proc. of HLT-1993.
Preslav Nakov. 2008. Paraphrasing Verbs for Noun
Compound Interpretation. In Proc. of the Workshop
on Multiword Expressions, LREC-2008.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics, 31:1.
Sameer Pradhan, Edward Loper, Dmitriy Dligach and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Proc.
of SemEval-2007 .
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proc. of Corpus Linguistics 2003, 647-656.
Philip Resnik. 1999. Semantic Similarity in a Taxon-
omy: An Information-Based Measure and its Applica-
tion to Problems of Ambiguity in Natural Language.
JAIR, Volume 11, pages 95-130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get Another Label? Improving Data Qual-
ity and Data Mining Using Multiple, Noisy Labelers.
In Proc. of KDD-2008.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, 2002.
Alexander Sorokin and David Forsyth. 2008. Util-
ity data annotation with Amazon Mechanical Turk.
To appear in Proc. of First IEEE Workshop on
Internet Vision at CVPR, 2008. See also:
http://vision.cs.uiuc.edu/annotation/
David G. Stork. 1999. The Open Mind Initiative.
IEEE Expert Systems and Their Applications pp. 16-
20, May/June 1999.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text In Proc. of SemEval-
2007.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C.
Baker. 2007. Internet-Scale Collection of Human-
Reviewed Data. In Proc. of WWW-2007.
Luis von Ahn and Laura Dabbish. 2004. Labeling Im-
ages with a Computer Game. In ACM Conference on
Human Factors in Computing Systems, CHI 2004.
Luis von Ahn, Mihir Kedia and Manuel Blum. 2006.
Verbosity: A Game for Collecting Common-Sense
Knowledge. In ACM Conference on Human Factors
in Computing Systems, CHI Notes 2006.
Ellen Voorhees and Hoa Trang Dang. 2006. Overview of
the TREC 2005 question answering track. In Proc. of
TREC-2005.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proc. of ACL-1999.
Annie Zaenen. Submitted. Do give a penny for their
thoughts. International Journal of Natural Language
Engineering (submitted).
263
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 387?394, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Robust Textual Inference via Graph Matching
Aria D. Haghighi
Dept. of Computer Science
Stanford University
Stanford, CA
aria42@stanford.edu
Andrew Y. Ng
Dept. of Computer Science
Stanford University
Stanford, CA
ang@cs.stanford.edu
Christopher D. Manning
Dept. of Computer Science
Stanford University
Stanford, CA
manning@cs.stanford.edu
Abstract
We present a system for deciding whether
a given sentence can be inferred from
text. Each sentence is represented as a
directed graph (extracted from a depen-
dency parser) in which the nodes repre-
sent words or phrases, and the links repre-
sent syntactic and semantic relationships.
We develop a learned graph matching ap-
proach to approximate entailment using
the amount of the sentence?s semantic
content which is contained in the text. We
present results on the Recognizing Textual
Entailment dataset (Dagan et al, 2005),
and show that our approach outperforms
Bag-Of-Words and TF-IDF models. In ad-
dition, we explore common sources of er-
rors in our approach and how to remedy
them.
1 Introduction
A fundamental stumbling block for several NLP ap-
plications is the lack of robust and accurate seman-
tic inference. For instance, question answering sys-
tems must be able to recognize, or infer, an answer
which may be expressed differently from the query.
Information extraction systems must also be able
recognize the variability of equivalent linguistic ex-
pressions. Document summarization systems must
generate succinct sentences which express the same
content as the original document. In Machine Trans-
lation evaluation, we must be able to recognize legit-
imate translations which structurally differ from our
reference translation.
One sub-task underlying these applications is the
ability to recognize semantic entailment; whether
one piece of text follows from another. In contrast
to recent work which has successfully utilized logic-
based abductive approaches to inference (Moldovan
et al, 2003; Raina et al, 2005b), we adopt a graph-
based representation of sentences, and use graph
matching approach to measure the semantic over-
lap of text. Graph matching techniques have proven
to be a useful approach for tractable approximate
matching in other domains including computer vi-
sion. In the domain of language, graphs provide
a natural way to express the dependencies between
words and phrases in a sentence. Furthermore,
graph matching also has the advantage of providing
a framework for structural matching of phrases that
would be difficult to resolve at the level of individual
words.
2 Task Definition and Data
We describe our approach in the context of the 2005
Recognizing Textual Entailment (RTE) Challenge
(Dagan et al, 2005), but note that our approach eas-
ily extends to other related inference tasks. The sys-
tem presented here was one component of our re-
search group?s 2005 RTE submission (Raina et al,
2005a) which was the top-ranking system according
to one of the two evaluation metrics.
In the 2005 RTE domain, we are given a set of
pairs, each consisting of two parts: 1) the text, a
387
SNP-Bezos
NNP
Bezos
VP-established
VBD
established
NP-company
DT
a
NN
company
Bezos
(person)
company
(organization)
establish
(VBD)
Subj (Agent) Obj (Patient)
Figure 1: An example parse tree and the correspond-
ing dependency graph. Each phrase of the parse tree
is annotated with its head word, and the parentheti-
cal edge labels in the dependency graph correspond
to semantic roles.
small passage,1 and the hypothesis, a single sen-
tence. Our task is to decide if the hypothesis is ?en-
tailed? by the text. Here, ?entails? does not mean
strict logical implication, but roughly means that
a competent speaker with basic world-knowledge
would be happy to conclude the hypothesis given the
text. This criterion has an aspect of relevance logic
as opposed to material implication: while various
additional background information may be needed
for the hypothesis to follow, the text must substan-
tially support the hypothesis.
Despite the informality of the criterion and the
fact that the available world knowledge is left
unspecified, human judges show extremely good
agreement on this task ? 3 human judges indepen-
dent of the organizers calculated agreement rates
with the released data set ranging from 91?96% (Da-
gan et al, 2005). We believe that this in part reflects
that the task is fairly natural to human beings. For
a flavor of the nature (and difficulty) of the task, see
Table 1.
We give results on the data provided for the RTE
task which consists of 567 development pairs and
800 test pairs. In both sets the pairs are divided into
7 tasks ? each containing roughly the same number
of entailed and not-entailed instances ? which were
used as both motivation and means for obtaining and
constructing the data items. We will use the follow-
ing toy example to illustrate our representation and
matching technique:
Text: In 1994, Amazon.com was founded by Jeff Bezos.
Hypothesis: Bezos established a company.
1Usually a single sentence, but occasionally longer.
3 Semantic Representation
3.1 The Need for Dependencies
Perhaps the most common representation of text for
assessing content is ?Bag-Of-Words? or ?Bag-of-N-
Grams? (Papineni et al, 2002). However, such rep-
resentations lose syntactic information which can
be essential to determining entailment. Consider a
Question Answer system searching for an answer
to When was Israel established? A representation
which did not utilize syntax would probably enthusi-
astically return an answer from (the 2005 RTE text):
The National Institute for Psychobiology in Israel
was established in 1979.
In this example, it?s important to try to match rela-
tionships as well as words. In particular, any answer
to the question should preserve the dependency be-
tween Israel and established. However, in the pro-
posed answer, the expected dependency is missing
although all the words are present.
Our approach is to view sentences as graphs be-
tween words and phrases, where dependency rela-
tionships, as in (Lin and Pantel, 2001), are charac-
terized by the path between vertices.
Given this representation, we judge entailment by
measuring not only how many of the hypothesis ver-
tices are matched to the text but also how well the
relationships between vertices in the hypothesis are
preserved in their textual counterparts. For the re-
mainder of the section we outline how we produce
graphs from text, and in the next section we intro-
duce our graph matching model.
3.2 From Text To Graphs
Starting with raw English text, we use a version of
the parser described in (Klein and Manning, 2003),
to obtain a parse tree. Then, we derive a dependency
tree representation of the sentence using a slightly
modified version of Collins? head propagation rules
(Collins, 1999), which make main verbs not auxil-
iaries the head of sentences. Edges in the depen-
dency graph are labeled by a set of hand-created
tgrep expressions. These labels represent ?sur-
face? syntax relationships such as subj for subject
and amod for adjective modifier, similar to the rela-
tions in Minipar (Lin and Pantel, 2001). The depen-
dency graph is the basis for our graphical represen-
tation, but it is enhanced in the following ways:
388
Task Text Hypothesis Entailed
Question An-
swer (QA)
Prince Charles was previously married to Princess
Diana, who died in a car crash in Paris in August
1997.
Prince Charles and Princess Diana got
married in August 1997.
False
Machine
Translation
(MT)
Sultan Al-Shawi, a.k.a the Attorney, said during a
funeral held for the victims, ?They were all chil-
dren of Iraq killed during the savage bombing.?.
The Attorney, said at the funeral, ?They
were all Iraqis killed during the brutal
shelling.?.
True
Comparable
Documents
(CD)
Napster, which started as an unauthorized song-
swapping Web site, has transformed into a legal
service offering music downloads for a monthly
fee.
Napster illegally offers music down-
loads.
False
Paraphrase
Recognition
(PP)
Kerry hit Bush hard on his conduct on the war in
Iraq.
Kerry shot Bush. False
Information
Retrieval (IR)
The country?s largest private employer, Wal-Mart
Stores Inc., is being sued by a number of its female
employees who claim they were kept out of jobs in
management because they are women.
Wal-Mart sued for sexual discrimina-
tion.
True
Table 1: Some Textual Entailment examples. The last three demonstrate some of the harder instances.
1. Collapse Collocations and Named-Entities: We
?collapse? dependency nodes which represent
named entities (e.g., Jeff Bezos in Figure fig-
example) and also collocations listed in Word-
Net, including verbs and their adjacent particles
(e.g., blow off in He blew off his work) .
2. Dependency Folding: As in (Lin and Pan-
tel, 2001), we found it useful to fold cer-
tain dependencies (such as modifying preposi-
tions) so that modifiers became labels connect-
ing the modifier?s governor and dependent di-
rectly. For instance, in the text graph in Figure
2, we have changed in from a word into a rela-
tion between its head verb and the head of its
NP complement.
3. Semantic Role Labeling: We also augment
the graph representation with Probank-style
semantic roles via the system described in
(Toutanova et al, 2005). Each predicate adds
an arc labeled with the appropriate seman-
tic role to the head of the argument phrase.
This helps to create links between words which
share a deep semantic relation not evident in
the surface syntax. Additionally, modifying
phrases are labeled with their semantic types
(e.g., in 1991 is linked by a Temporal edge in
the text graph of Figure 2), which should be
useful in Question Answering tasks.
4. Coreference Links: Using a co-rereference res-
olution tagger, coref links are added through-
out the graph. These links allowed connecting
the referent entity to the vertices of the referring
vertex. In the case of multiple sentence texts, it
is our only ?link? in the graph between entities
in the two sentences.
For the remainder of the paper, we will refer to
the text as T and hypothesis as H , and will speak
of them in graph terminology. In addition we will
use HV and HE to denote the vertices and edges,
respectively, of H .
4 Entailment by Graph Matching
We take the view that a hypothesis is entailed from
the text when the cost of matching the hypothesis
graph to the text graph is low. For the remainder of
this section, we outline a general model for assign-
ing a match cost to graphs.
For hypothesis graph H , and text graph T , a
matching M is a mapping from the vertices of H to
those of T . For vertex v in H , we will use M(v) to
denote its ?match? in T . As is common in statistical
machine translation, we allow nodes in H to map to
fictitious NULL vertices in T if necessary. Suppose
the cost of matching M is Cost(M). If M is the set
of such matchings, we define the cost of matching
H to T to be
MatchCost(H,T ) = min
M?M
Cost(M) (1)
Suppose we have a model, VertexSub(v,M(v)),
which gives us a cost in [0, 1], for substituting ver-
tex v in H for M(v) in T . One natural cost model
389
is to use the normalized cost for each of the vertex
substitutions in M :
VertexCost(M) = 1Z
?
v?HV
w(v)VertexSub(v,M(v))
(2)
Here, w(v) represents the weight or relative im-
portance for vertex v, and Z = ?v?HV w(v) is
a normalization constant. In our implementation,
the weight of each vertex was based on the part-of-
speech tag of the word or the type of named entity,
if applicable. However, there are several other pos-
sibilities including using TF-IDF weights for words
and phrases.
Notice that when Cost(M) takes the form of
(2), computing MatchCost(H,T ) is equivalent to
finding the minimal cost bipartite graph-matching,
which can be efficiently computed using linear pro-
gramming.
We would like our cost-model to incorporate
some measure of how relationships in H are pre-
served in T under M . Ideally, a matching should
preserve all local relationships; i.e, if v ? v? ? HE ,
then M(v) ? M(v?) ? TE . When this condition
holds for all edges in H , H is isomorphic to a sub-
graph of T .
What we would like is an approximate notion of
isomorphism, where we penalize the distortion of
each edge relation in H . Consider an edge e =
(v, v?) ? HE , and let ?M (e) be the path from M(v)
to M(v?) in T .
Again, suppose we have a model,
PathSub(e, ?M (e)) for assessing the ?cost? of
substituting a direct relation e ? HE for its coun-
terpart, ?M (e), under the matching. This leads to
a formulation similar to (2), where we consider the
normalized cost of substituting each edge relation
in H with a path in T :
RelationCost(M) = 1Z
?
e?HE
w(e)PathSub(e, ?M (e))
(3)
where Z = ?e?HE w(e) is a normalization con-
stant. As in the vertex case, we have weights
for each hypothesis edge, w(e), based upon the
edge?s label; typically subject and object relations
are more important to match than others. Our fi-
nal matching cost is given by a convex mixture of
Subj (Agent)
establish
(VBD)
Bezos
(person)
Company
(organization)
Obj (Patient) 
Subj (Agent)
found
(VBD)
Jeff Bezos
(person)
Amazon.com
(organization)
Obj (Patient)
In (Temporal)
1991
(date)
Synonym 
Match
Cost: 0.4
Hyponym
Match
Cost: 0.0
Exact
Match
Cost: 0.0
Vertex Cost: (0.0 + 0.2 + 0.4)/3 = 0.2
Relation Cost: 0  (Graphs Isomorphic)  
Match Cost: 0.55 (0.2) + (.45) 0.0 = 0.11
Figure 2: Example graph matching (? = 0.55) for
example pair. Dashed lines represent optimal match-
ing.
the vertex and relational match costs: Cost(M) =
?VertexCost(M) + (1 ? ?)RelationCost(M).
Notice that minimizing Cost(M) is computa-
tionally hard since if our PathSub model as-
signs zero cost only for preserving edges, then
RelationCost(M) = 0 if and only if H is isomorphic
to a subgraph of T . Since subgraph isomophism is
an NP-complete problem, we cannot hope to have an
efficient exact procedure for minimizing the graph
matching cost. As an approximation, we can ef-
ficiently find the matching M? which minimizes
VertexCost(?); we then perform local greedy hill-
climbing search, beginning from M?, to approxi-
mate the minimal matching. The allowed operations
are changing the assignment of any hypothesis ver-
tex to a text one, and, to avoid ridges, swapping two
hypothesis assignments
5 Node and Edge Substitution Models
In the previous section we described our graph
matching model in terms of our VertexSub model,
which gives a cost for substituting one graph vertex
for another, and PathSub, which gives a cost for sub-
stituting the path relationship between two paths in
one graph for that in another. We now outline these
models.
5.1 Vertex substitution cost model
Our VertexSub(v,M(v)) model is based upon a
sliding scale, where progressively higher costs are
390
given based upon the following conditions:
? Exact Match: v and M(v) are identical words/
phrases.
? Stem Match: v and M(v)?s stems match or one
is a derivational form of the other; e.g., matching
coaches to coach.
? Synonym Match: v and M(v) are synonyms ac-
cording to WordNet (Fellbaum, 1998). In particu-
lar we use the top 3 senses of both words to deter-
mine synsets.
? Hypernym Match: v is a ?kind of? M(v), as
determined by WordNet. Note that this feature is
asymmetric.
? WordNet Similarity: v and M(v) are similar ac-
cording to WordNet::Similarity (Peder-
sen et al, 2004). In particular, we use the measure
described in (Resnik, 1995). We found it useful
to only use similarities above a fixed threshold to
ensure precision.
? LSA Match: v and M(v) are distributionally
similar according to a freely available Latent Se-
mantic Indexing package,2 or for verbs similar
according to VerbOcean (Chklovski and Pantel,
2004).
? POS Match: v and M(v) have the same part of
speech.
? No Match: M(v) is NULL.
Although the above conditions often produce rea-
sonable matchings between text and hypothesis, we
found the recall of these lexical resources to be far
from adequate. More robust lexical resources would
almost certainly boost performance.
5.2 Path substitution cost model
Our PathSub(v ? v?,M(v) ? M(v?)) model is
also based upon a sliding scale cost based upon the
following conditions:
? Exact Match: M(v) ? M(v?) is an en edge in
T with the same label.
? Partial Match: M(v) ? M(v?) is an en edge in
T , not necessarily with the same label.
? Ancestor Match: M(v) is an ancestor of M(v?).
We use an exponentially increasing cost for longer
distance relationships.
2Available at http://infomap.stanford.edu
? Kinked Match: M(v) and M(v?) share a com-
mon parent or ancestor in T . We use an exponen-
tially increasing cost based on the maximum of
the node?s distances to their least common ances-
tor in T .
These conditions capture many of the common
ways in which relationships between entities are dis-
torted in semantically related sentences. For in-
stance, in our system, a partial match will occur
whenever an edge type differs in detail, for instance
use of the preposition towards in one case and to in
the other. An ancestor match will occur whenever an
indirect relation leads to the insertion of an interven-
ing node in the dependency graph, such as matching
John is studying French farming vs. John is studying
French farming practices.
5.3 Learning Weights
Is it possible to learn weights for the relative impor-
tance of the conditions in the VertexSub and PathSub
models? Consider the case where match costs are
given only by equation (2) and vertices are weighted
uniformly (w(v) = 1). Suppose that ?(v,M(v))
is a vector of features3 indicating the cost accord-
ing to each of the conditions listed for matching v
to M(v). Also let w be weights for each element
of ?(v,M(v)). First we can model the substitution
cost for a given matching as:
VertexSub(v,M(v)) = exp (w
T ?(v,M(v)))
1 + exp (wT ?(v,M(v)))
Letting s(?) be the 1-sigmoid function used in the
right hand side of the equation above, our final
matching cost as a function of w is given by
c(H,T ;w) = min
M?M
1
|HV |
?
v?H
s(wT ?(v,M(v)))
(4)
Suppose we have a set of text/hypothesis pairs,
{(T (1),H(1)), . . . , (T (n),H(n))}, with labels y(i)
which are 1 if H(i) is entailed by T (i) and 0
otherwise. Then we would like to choose w to
minimize costs for entailed examples and maximize
it for non-entailed pairs:
3In the case of our ?match? conditions, these features will
be binary.
391
?(w) =
?
i:y(i)=1
log c(H(i), T (i);w) +
?
i:y(i)=0
log(1 ? c(H(i), T (i);w))
Unfortunately, ?(w) is not a convex function. No-
tice that the cost of each matching, M , implicitly
depends on the current setting of the weights w. It
can be shown that since each c(H,T ;w) involves
minimizing M ? M, which depends on w, it is not
convex. Therefore, we can?t hope to globally opti-
mize our cost functions over w and must settle for
an approximation.
One approach is to use coordinate ascent over M
and w. Suppose that we begin with arbitrary weights
and given these weights choose M (i) to minimize
each c(H(i), T (i);w). Then we use a relaxed form of
the cost function where we use the matchings found
in the last step:
c?(H(i), T (i);w) = 1|HV |
?
v?H
s(wT?(v,M (i)(v)))
Then we maximize w with respect to ?(w) with
each c(?) replaced with the cost-function c?(?). This
step involves only logistic regression. We repeat this
procedure until our weights converge.
To test the effectiveness of the above procedure
we compared performance against baseline settings
using a random split on the development set. Picking
each weight uniformly at random resulted in 53%
accuracy. Setting all weights identically to an arbi-
trary value gave 54%. The procedure above, where
the weights are initialized to the same value, resulted
in an accuracy of 57%. However, we believe there
is still room for improvement since carefully-hand
chosen weights results in comparable performance
to the learned weights on the final test set. We be-
lieve this setting of learning under matchings is a
rather general one and could be beneficial to other
domains such as Machine Translation. In the future,
we hope to find better approximation techniques for
this problem.
6 Checks
One systematic source of error coming from our ba-
sic approach is the implicit assumption of upwards
monotonicity of entailment; i.e., if T entails H then
adding more words to T should also give us a sen-
tence which entails H . This assumption, also made
by other recent abductive approaches (Moldovan et
al., 2003), does not hold for several classes of exam-
ples. Our formalism does not at present provide a
general solution to this issue, but we include special
case handling of the most common types of cases,
which we outline below.4 These checks are done af-
ter graph matching and assume we have stored the
minimal cost matching.
Negation Check
Text: Clinton?s book is not a bestseller
Hypothesis: Clinton?s book is a bestseller
To catch such examples, we check that each hy-
pothesis verb is not matched to a text word which
is negated (unless the verb pairs are antonyms) and
vice versa. In this instance, the is in H , denoted by
isH , is matched to isT which has a negation modifier,
notT , absent for isH . So the negation check fails.
Factive Check
Text: Clonaid claims to have cloned 13 babies worldwide.
Hypothesis: Clonaid has cloned 13 babies.
Non-factive verbs (claim, think, charged, etc.) in
contrast to factive verbs (know, regret, etc.) have
sentential complements which do not represent true
propositions. We detect such cases, by checking that
each verb in H that is matched in T does not have a
non-factive verb for a parent.
Superlative Check
Text: The Osaka World Trade Center is the tallest building in
Western Japan.
Hypothesis: The Osaka World Trade Center is the tallest build-
ing in Japan.
In general, superlative modifiers (most, biggest,
etc.) invert the typical monotonicity of entailment
and must be handled as special cases. For any
noun n with a superlative modifier (part-of-speech
JJS) in H , we must ensure that all modifier relations
of M(n) are preserved in H . In this example, build-
ingH has a superlative modifier tallestH , so we must
ensure that each modifier relation of JapanT , a noun
4All the examples are actual, or slightly altered, RTE exam-
ples.
392
Method Accuracy CWS
Random 50.0% 0.500
Bag-Of-Words 49.5% 0.548
TF-IDF 51.8% 0.560
GM-General 56.8% 0.614
GM-ByTask 56.7% 0.620
Table 2: Accuracy and confidence weighted score
(CWS) for test set using various techniques.
dependent of buildingT , has a WesternT modifier not
in H . So its fails the superlative check.
Additionally, during error analysis on the devel-
opment set, we spotted the following cases where
our VertexSub function erroneously labeled vertices
as similar, and required special case consideration:
? Antonym Check: We consistently found that the
WordNet::Similarity modules gave high-
similarity to antonyms.5 We explicitly check
whether a matching involved antonyms and reject
unless one of the vertices had a negation modifier.
? Numeric Mismatch: Since numeric expressions
typically have the same part-of-speech tag (CD),
they were typically matched when exact matches
could not be found. However, mismatching nu-
merical tokens usually indicated that H was not
entailed, and so pairs with a numerical mismatch
were rejected.
7 Experiments and Results
For our experiments we used the devolpement and
test sets from the Recognizing Textual Entailment
challenge (Dagan et al, 2005). We give results for
our system as well as for the following systems:
? Bag-Of-Words: We tokenize the text and hypoth-
esis and strip the function words, and stem the re-
sulting words. The cost is given by the fraction of
the hypothesis not matched in the text.
? TF-IDF: Similar to Bag-Of-Words except that
there is a tf.idf weight associated with each hy-
pothesis word so that more ?important? words are
higher weight for matching.
5This isn?t necessarily incorrect, but is simply not suitable
for textual inference.
Task GM-General GM-ByTask
Accuracy CWS Accuracy CWS
CD 72.0% 0.742 76.0% 0.771
IE 55.9% 0.583 55.8% 0.595
IR 52.2% 0.564 51.1% 0.572
MT 50.0% 0.497 43.3% 0.489
PP 58.0% 0.741 58.0% 0.746
QA 53.8% 0.537 55.4% 0.556
RC 52.1% 0.539 52.9% 0.523
Table 3: Accuracy and confidence weighted score
(CWS) split by task on the RTE test set.
We also present results for two graph matching
(GM) systems. The GM-General system fits a sin-
gle global threshold from the development set. The
GM-ByTask system fits a different threshold for
each of the tasks.
Our results are summarized in Table 2. As the re-
sult indicates, the task is particularly hard; all RTE
participants scored between 50% and 60% in terms
of overall accuracy (Dagan et al, 2005). Nevever-
theless, both GM systems perform better than either
Bag-Of-Words or TF-IDF. CWS refers to Confi-
dence Weighted Score (also known as average pre-
cision). This measure is perhaps a more insightful
measure, since it allows the inclusion of a ranking
of answers by confidence and assesses whether you
are correct on the pairs that you are most confident
that you know the answer to. To assess CWS, our
n answers are sorted in decreasing order by the con-
fidence we return, and then for each i, we calculate
ai, our accuracy on our i most confident predictions.
Then CWS = 1n
?n
i=1 ai.
We also present results on a per-task basis in Ta-
ble 3. Interestingly, there is a large variation in per-
formance depending on the task.
8 Conclusion
We have presented a learned graph matching ap-
proach to approximating textual entailment which
outperforms models which only match at the word
level, and is competitive with recent weighed ab-
duction models (Moldovan et al, 2003). In addition,
we explore problematic cases of nonmonotonicity in
entailment, which are not naturally handled by ei-
ther subgraph matching or the so-called ?logic form?
393
Text Hypothesis True Ans. Our Ans. Conf Comments
A Filipino hostage in Iraq was re-
leased.
A Filipino hostage
was freed in Iraq.
True True 0.84 Verb rewrite is handled.
Phrasal ordering does not
affect cost.
The government announced last
week that it plans to raise oil
prices.
Oil prices drop. False False 0.95 High cost given for substituting
word for its antonym.
Shrek 2 rang up $92 million. Shrek 2 earned $92
million.
True False 0.59 Collocation ?rang up? is
not known to be similar to
?earned?.
Sonia Gandhi can be defeated in
the next elections in India by BJP.
Sonia Gandhi is de-
feated by BJP.
False True 0.77 ?can be? does not indicate the
complement event occurs.
Fighters loyal to Moqtada al-Sadr
shot down a U.S. helicopter Thurs-
day in the holy city of Najaf.
Fighters loyal to
Moqtada al-Sadr
shot down Najaf.
False True 0.67 Should recognize non-Location
cannot be substituted for Loca-
tion.
C and D Technologies announced
that it has closed the acquisition of
Datel, Inc.
Datel Acquired C
and D technologies.
False True 0.64 Failed to penalize switch in se-
mantic role structure enough
Table 4: Analysis of results on some RTE examples along with out guesses and confidence probabilities
inference of (Moldovan et al, 2003) and have pro-
posed a way to capture common cases of this phe-
nomenon. We believe that the methods employed
in this work show much potential for improving the
state-of-the-art in computational semantic inference.
9 Acknowledgments
Many thanks to Rajat Raina, Christopher Cox,
Kristina Toutanova, Jenny Finkel, Marie-Catherine
de Marneffe, and Bill MacCartney for providing us
with linguistic modules and useful discussions. This
work was supported by the Advanced Research and
Development Activity (ARDA)?s Advanced Ques-
tion Answering for Intelligence (AQUAINT) pro-
gram.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In EMNLP.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognizing textual entailment
challenge. In Proceedings of the PASCAL Challenges
Workshop Recognizing Textual Entailment.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining, pages 323?328.
Dan I. Moldovan, Christine Clark, Sanda M. Harabagiu,
and Steven J. Maiorano. 2003. Cogex: A logic prover
for question answering. In HLT-NAACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL.
Ted Pedersen, Siddharth Parwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity ? measuring the relat-
edness of concepts. In AAAI.
Rajat Raina, Aria Haghighi, Christopher Cox, Jenny
Finkel, Jeff Michels, Kristina Toutanova, Bill Mac-
Cartney, Marie-Catherine de Marneffe, Christopher D.
Manning, and Andrew Y. Ng. 2005a. Robust textual
inference using diverse knowledge sources. In Pro-
ceedings of the First PASCAL Challenges Workshop.
Southampton, UK.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005b. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI 2005.
AAAI Press.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In IJCAI, pages
448?453.
Kristina Toutanova, Aria Haghighi, and Cristiopher Man-
ning. 2005. Joint learning improves semantic role la-
beling. In Association of Computational Linguistics
(ACL).
394
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801?808,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Taxonomy Induction from Heterogenous Evidence
Rion Snow
Computer Science Department
Stanford University
Stanford, CA 94305
rion@cs.stanford.edu
Daniel Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305
ang@cs.stanford.edu
Abstract
We propose a novel algorithm for inducing seman-
tic taxonomies. Previous algorithms for taxonomy
induction have typically focused on independent
classifiers for discovering new single relationships
based on hand-constructed or automatically discov-
ered textual patterns. By contrast, our algorithm
flexibly incorporates evidence from multiple clas-
sifiers over heterogenous relationships to optimize
the entire structure of the taxonomy, using knowl-
edge of a word?s coordinate terms to help in deter-
mining its hypernyms, and vice versa. We apply our
algorithm on the problem of sense-disambiguated
noun hyponym acquisition, where we combine the
predictions of hypernym and coordinate term clas-
sifiers with the knowledge in a preexisting seman-
tic taxonomy (WordNet 2.1). We add 10, 000 novel
synsets to WordNet 2.1 at 84% precision, a rela-
tive error reduction of 70% over a non-joint algo-
rithm using the same component classifiers. Fi-
nally, we show that a taxonomy built using our al-
gorithm shows a 23% relative F-score improvement
over WordNet 2.1 on an independent testset of hy-
pernym pairs.
1 Introduction
The goal of capturing structured relational knowl-
edge about lexical terms has been the motivating
force underlying many projects in lexical acquisi-
tion, information extraction, and the construction
of semantic taxonomies. Broad-coverage seman-
tic taxonomies such as WordNet (Fellbaum, 1998)
and CYC (Lenat, 1995) have been constructed by
hand at great cost; while a crucial source of knowl-
edge about the relations between words, these tax-
onomies still suffer from sparse coverage.
Many algorithms with the potential for auto-
matically extending lexical resources have been
proposed, including work in lexical acquisition
(Riloff and Shepherd, 1997; Roark and Charniak,
1998) and in discovering instances, named enti-
ties, and alternate glosses (Etzioni et al, 2005;
Pasc?a, 2005). Additionally, a wide variety of
relationship-specific classifiers have been pro-
posed, including pattern-based classifiers for hy-
ponyms (Hearst, 1992), meronyms (Girju, 2003),
synonyms (Lin et al, 2003), a variety of verb re-
lations (Chklovski and Pantel, 2004), and general
purpose analogy relations (Turney et al, 2003).
Such classifiers use hand-written or automatically-
induced patterns like Such NPy as NPx or NPy
like NPx to determine, for example that NPy is a
hyponym of NPx (i.e., NPy IS-A NPx). While
such classifiers have achieved some degree of suc-
cess, they frequently lack the global knowledge
necessary to integrate their predictions into a com-
plex taxonomy with multiple relations.
Past work on semantic taxonomy induction in-
cludes the noun hypernym hierarchy created in
(Caraballo, 2001), the part-whole taxonomies in
(Girju, 2003), and a great deal of recent work de-
scribed in (Buitelaar et al, 2005). Such work has
typically either focused on only inferring small
taxonomies over a single relation, or as in (Cara-
ballo, 2001), has used evidence for multiple rela-
tions independently from one another, by for ex-
ample first focusing strictly on inferring clusters
of coordinate terms, and then by inferring hyper-
nyms over those clusters.
Another major shortfall in previous techniques
for taxonomy induction has been the inability to
handle lexical ambiguity. Previous approaches
have typically sidestepped the issue of polysemy
altogether by making the assumption of only a sin-
gle sense per word, and inferring taxonomies ex-
plicitly over words and not senses. Enforcing a
false monosemy has the downside of making po-
tentially erroneous inferences; for example, col-
lapsing the polysemous term Bush into a single
sense might lead one to infer by transitivity that
a rose bush is a kind of U.S. president.
Our approach simultaneously provides a solu-
tion to the problems of jointly considering evi-
dence about multiple relationships as well as lexi-
cal ambiguity within a single probabilistic frame-
work. The key contribution of this work is to offer
a solution to two crucial problems in taxonomy in-
801
duction and hyponym acquisition: the problem of
combining heterogenous sources of evidence in a
flexible way, and the problem of correctly identi-
fying the appropriate word sense of each new word
added to the taxonomy.1
2 A Probabilistic Framework for
Taxonomy Induction
In section 2.1 we introduce our definitions for tax-
onomies, relations, and the taxonomic constraints
that enforce dependencies between relations; in
section 2.2 we give a probabilistic model for defin-
ing the conditional probability of a set of relational
evidence given a taxonomy; in section 2.3 we for-
mulate a local search algorithm to find the taxon-
omy maximizing this conditional probability; and
in section 2.4 we extend our framework to deal
with lexical ambiguity.
2.1 Taxonomies, Relations, and Taxonomic
Constraints
We define a taxonomy T as a set of pairwise re-
lations R over some domain of objects DT. For
example, the relations in WordNet include hyper-
nymy, holonymy, verb entailment, and many oth-
ers; the objects of WordNet between which these
relations hold are its word senses or synsets. We
define that each relation R ? R is a set of ordered
or unordered pairs of objects (i, j) ? DT; we de-
fine Rij ? T if relationship R holds over objects
(i, j) in T.
Relations for Hyponym Acquisition
For the case of hyponym acquisition, the ob-
jects in our taxonomy are WordNet synsets. In
this paper we focus on two of the many possible
relationships between senses: the hypernym rela-
tion and the coordinate term relation. We treat the
hypernym or ISA relation as atomic; we use the
notation Hnij if a sense j is the n-th ancestor of a
sense i in the hypernym hierarchy. We will sim-
ply use Hij to indicate that j is an ancestor of i
at some unspecified level. Two senses are typi-
cally considered to be ?coordinate terms? or ?tax-
onomic sisters? if they share an immediate parent
in the hypernym hierarchy. We generalize this no-
tion of siblinghood to state that two senses i and
j are (m,n)-cousins if their closest least common
1The taxonomies discussed in this paper are available for
download at http://ai.stanford.edu/?rion/swn.
subsumer (LCS)2 is within exactly m and n links,
respectively.3 We use the notation Cmnij to denote
that i and j are (m,n)-cousins. Thus coordinate
terms are (1, 1)-cousins; technically the hypernym
relation may also be seen as a specific case of this
representation; an immediate parent in the hyper-
nym hierarchy is a (1, 0)-cousin, and the k-th an-
cestor is a (k, 0)-cousin.
Taxonomic Constraints
A semantic taxonomy such as WordNet en-
forces certain taxonomic constraints which disal-
low particular taxonomies T. For example, the
ISA transitivity constraint in WordNet requires
that each synset inherits the hypernyms of its hy-
pernym, and the part-inheritance constraint re-
quires that each synset inherits the meronyms of
its hypernyms.
For the case of hyponym acquisition we enforce
the following two taxonomic constraints on the
hypernym and (m,n)-cousin relations:
1. ISA Transitivity:
Hmij ?Hnjk ? Hm+nik .
2. Definition of (m,n)-cousinhood:
Cmnij ? ?k.k = LCS(i, j) ?Hmik ?Hnjk.
Constraint (1) requires that the each synset inherits
the hypernyms of its direct hypernym; constraint
(2) simply defines the (m,n)-cousin relation in
terms of the atomic hypernym relation.
The addition of any new hypernym relation to a
preexisting taxonomy will usually necessitate the
addition of a set of other novel relations as implied
by the taxonomic constraints. We refer to the full
set of novel relations implied by a new link Rij as
I(Rij); we discuss the efficient computation of the
set of implied links for the purpose of hyponym
acquisition in Section 3.4.
2.2 A Probabilistic Formulation
We propose that the event Rij ? T has some
prior probability P (Rij ? T), and P (Rij ?
2A least common subsumer LCS(i, j) is defined as a
synset that is an ancestor in the hypernym hierarchy of both
i and j which has no child that is also an ancestor of both i
and j. When there is more than one LCS (due to multiple
inheritance), we refer to the closest LCS, i.e.,the LCS that
minimizes the maximum distance to i and j.
3An (m,n)-cousin for m ? 2 corresponds to the English
kinship relation ?(m?1)-th cousin |m?n|-times removed.?
802
T) + P (Rij 6? T) = 1. We define the probability
of the taxonomy as a whole as the joint probability
of its component relations; given a partition of all
possible relations R = {A,B} where A ? T and
B 6? T, we define:
P (T) = P (A ? T, B 6? T).
We assume that we have some set of observed evi-
dence E consisting of observed features over pairs
of objects in some domain DE; we?ll begin with
the assumption that our features are over pairs of
words, and that the objects in the taxonomy also
correspond directly to words.4 Given a set of fea-
tures ERij ? E, we assume we have some model
for inferring P (Rij ? T|ERij), i.e., the posterior
probability of the event Rij ? T given the corre-
sponding evidence ERij for that relation. For exam-
ple, evidence for the hypernym relation EHij might
be the set of all observed lexico-syntactic patterns
containing i and j in all sentences in some corpus.
For simplicity we make the following indepen-
dence assumptions: first, we assume that each
item of observed evidence ERij is independent of
all other observed evidence given the taxonomyT,
i.e., P (E|T) = ?ERij?E P (E
R
ij |T).
Further, we assume that each item of observed
evidence ERij depends on the taxonomy T only by
way of the corresponding relation Rij , i.e.,
P (ERij |T) =
{ P (ERij |Rij ? T) if Rij ? T
P (ERij |Rij 6? T) if Rij 6? T
For example, if our evidence EHij is a set of ob-
served lexico-syntactic patterns indicative of hy-
pernymy between two words i and j, we assume
that whatever dependence the relations in T have
on our observations may be explained entirely by
dependence on the existence or non-existence of
the single hypernym relation H(i, j).
Applying these two independence assumptions
we may express the conditional probability of our
evidence given the taxonomy:
P (E|T) =
?
Rij?T
P (ERij |Rij ? T)
?
?
Rij 6?T
P (ERij |Rij 6? T).
Rewriting the conditional probability in terms
of our estimates of the posterior probabilities
4In section 2.4 we drop this assumption, extending our
model to manage lexical ambiguity.
P (Rij |ERij) using Bayes Rule, we obtain:
P (E|T) =
?
Rij?T
P (Rij ? T|ERij)P (ERij)
P (Rij ? T)
?
?
Rij 6?T
P (Rij 6? T|ERij)P (ERij)
P (Rij 6? T) .
Within our model we define the goal of taxon-
omy induction to be to find the taxonomy T? that
maximizes the conditional probability of our ob-
servations E given the relationships of T, i.e., to
find
T? = argmax
T
P (E|T).
2.3 Local Search Over Taxonomies
We propose a search algorithm for finding T? for
the case of hyponym acquisition. We assume we
begin with some initial (possibly empty) taxon-
omy T. We restrict our consideration of possible
new taxonomies to those created by the single op-
eration ADD-RELATION(Rij ,T), which adds the
single relation Rij to T.
We define the multiplicative change ?T(Rij)
to the conditional probability P (E|T) given the
addition of a single relation Rij :
?T(Rij) = P (E|T?)/P (E|T)
= P (Rij ? T|E
R
ij)P (ERij)
P (Rij 6? T|ERij)P (ERij)
? P (Rij 6? T)P (Rij ? T)
= k
?
? P
(
Rij ? T|ERij
)
1? P
(
Rij ? T|ERij
)
?
? .
Here k is the inverse odds of the prior on the event
Rij ? T; we consider this to be a constant inde-
pendent of i, j, and the taxonomy T.
To enforce the taxonomic constraints in T, for
each application of the ADD-RELATION operator
we must add all new relations in the implied set
I(Rij) not already in T.5 Thus we define the mul-
tiplicative change of the full set of implied rela-
tions as the product over all new relations:
?T(I(Rij)) =
?
R?I(Rij)
?T(R).
5For example, in order to add the new synset
microsoft under the noun synset company#n#1
in WordNet 2.1, we must necessarily add the
new relations H2(microsoft, institution#n#1)
C11(microsoft, dotcom#n#1), and so on.
803
This definition leads to the following best-first
search algorithm for hyponym acquisition, which
at each iteration defines the new taxonomy as the
union of the previous taxonomy T and the set of
novel relations implied by the relation Rij that
maximizes ?T(I(Rij)) and thus maximizes the
conditional probability of the evidence over all
possible single relations:
WHILE max
Rij 6?T
?T(I(Rij)) > 1
T ? T ? I(arg max
Rij 6?T
?T(I(Rij))).
2.4 Extending the Model to Manage Lexical
Ambiguity
Since word senses are not directly observable, if
the objects in the taxonomy are word senses (as in
WordNet), we must extend our model to allow for
a many-to-many mapping (e.g., a word-to-sense
mapping) between DE and DT. For this setting
we assume we know the function senses(i), map-
ping from the word i to all of i?s possible corre-
sponding senses.
We assume that each set of word-pair evidence
ERij we possess is in fact sense-pair evidence ERkl
for a specific pair of senses k0 ? senses(i), l0 ?
senses(j). Further, we assume that a new relation
between two words is probable only between the
correct sense pair, i.e.:
P (Rkl|ERij) = 1{k = k0, l = l0} ? P (Rij |ERij).
When computing the conditional probability of a
specific new relation Rkl ? I(Rab), we assume
that the relevant sense pair k0, l0 is the one which
maximizes the probability of the new relation, i.e.
for k ? senses(i), l ? senses(j),
(k0, l0) = argmaxk,l P (Rkl ? T|E
R
ij).
Our independence assumptions for this exten-
sion need only to be changed slightly; we now as-
sume that the evidence ERij depends on the taxon-
omy T via only a single relation between sense-
pairs Rkl. Using this revised independence as-
sumption the derivation for best-first search over
taxonomies for hyponym acquisition remains un-
changed. One side effect of this revised indepen-
dence assumption is that the addition of the single
?sense-collapsed? relation Rkl in the taxonomy T
will explain the evidence ERij for the relation over
words i and j now that such evidence has been re-
vealed to concern only the specific senses k and l.
3 Extending WordNet
We demonstrate the ability of our model to use
evidence from multiple relations to extend Word-
Net with novel noun hyponyms. While in prin-
ciple we could use any number of relations, for
simplicity we consider two primary sources of ev-
idence: the probability of two words in WordNet
being in a hypernym relation, and the probability
of two words in WordNet being in a coordinate re-
lation.
In sections 3.1 and 3.2 we describe the construc-
tion of our hypernym and coordinate classifiers,
respectively; in section 3.3 we outline the efficient
algorithm we use to perform local search over
hyponym-extended WordNets; and in section 3.4
we give an example of the implicit structure-based
word sense disambiguation performed within our
framework.
3.1 Hyponym Classification
Our classifier for the hypernym relation is derived
from the ?hypernym-only? classifier described in
(Snow et al, 2005). The features used for pre-
dicting the hypernym relationship are obtained by
parsing a large corpus of newswire and encyclo-
pedia text with MINIPAR (Lin, 1998). From the
resulting dependency trees the evidence EHij for
each word pair (i, j) is constructed; the evidence
takes the form of a vector of counts of occurrences
that each labeled syntactic dependency path was
found as the shortest path connecting i and j in
some dependency tree. The labeled training set is
constructed by labeling the collected feature vec-
tors as positive ?known hypernym? or negative
?known non-hypernym? examples using WordNet
2.0; 49,922 feature vectors were labeled as pos-
itive training examples, and 800,828 noun pairs
were labeled as negative training examples. The
model for predicting P (Hij |EHij ) is then trained
using logistic regression, predicting the noun-pair
hypernymy label from WordNet from the feature
vector of lexico-syntactic patterns.
The hypernym classifier described above pre-
dicts the probability of the generalized hypernym-
ancestor relation over words P (Hij |EHij ). For
the purposes of taxonomy induction, we would
prefer an ancestor-distance specific set of clas-
sifiers over senses, i.e., for k ? senses(i), l ?
senses(j), the set of classifiers estimating
{P (H1kl|EHij ), P (H2kl|EHij ), . . . }.
804
One problem that arises from directly assign-
ing the probability P (Hnij |EHij ) ? P (Hij |EHij ) for
all n is the possibility of adding a novel hyponym
to an overly-specific hypernym, which might still
satisfy P (Hnij |EHij ) for a very large n. In or-
der to discourage unnecessary overspecification,
we penalize each probability P (Hkij |EHij ) by a
factor ?k?1 for some ? < 1, and renormalize:
P (Hkij |EHij ) ? ?k?1P (Hij |EHij ). In our experi-
ments we set ? = 0.95.
3.2 (m,n)-cousin Classification
The classifier for learning coordinate terms relies
on the notion of distributional similarity, i.e., the
idea that two words with similar meanings will be
used in similar contexts (Hindle, 1990). We ex-
tend this notion to suggest that words with similar
meanings should be near each other in a seman-
tic taxonomy, and in particular will likely share a
hypernym as a near parent.
Our classifier for (m,n)-cousins is derived
from the algorithm and corpus given in (Ravichan-
dran et al, 2005). In that work an efficient ran-
domized algorithm is derived for computing clus-
ters of similar nouns. We use a set of more than
1000 distinct clusters of English nouns collected
by their algorithm over 70 million webpages6,
with each noun i having a score representing its
cosine similarity to the centroid c of the cluster to
which it belongs, cos(?(i, c)).
We use the cluster scores of noun pairs as input
to our own algorithm for predicting the (m,n)-
cousin relationship between the senses of two
words i and j. If two words i and j appear in
a cluster together, with cluster centroid c, we set
our single coordinate input feature to be the mini-
mum cluster score min(cos(?(i, c)), cos(?(j, c))),
and zero otherwise. For each such noun pair fea-
ture, we construct a labeled training set of (m,n)-
cousin relation labels from WordNet 2.1. We de-
fine a noun pair (i, j) to be a ?known (m,n)-
cousin? if for some senses k ? senses(i), l ?
senses(j), Cmnij ? WordNet; if more than one
such relation exists, we assume the relation with
smallest sum m + n, breaking ties by smallest
absolute difference |m ? n|. We consider all
such labeled relationships from WordNet with 0 ?
m,n ? 7; pairs of words that have no correspond-
ing pair of synsets connected in the hypernym hi-
6As a preprocessing step we hand-edit the clusters to re-
move those containing non-English words, terms related to
adult content, and other webpage-specific clusters.
erarchy, or with min(m,n) > 7, are assigned to
a single class C?. Further, due to the symme-
try of the similarity score, we merge each class
Cmn = Cmn ? Cnm; this implies that the result-
ing classifier will predict, as expected given a sym-
metric input, P (Cmnkl |ECij ) = P (Cnmkl |ECij ).
We find 333,473 noun synset pairs in our train-
ing set with similarity score greater than 0.15. We
next apply softmax regression to learn a classifier
that predicts P (Cmnij |ECij ), predicting the Word-
Net class labels from the single similarity score
derived from the noun pair?s cluster similarity.
3.3 Details of our Implementation
Hyponym acquisition is among the simplest and
most straightforward of the possible applications
of our model; here we show how we efficiently
implement our algorithm for this problem. First,
we identify the set of all the word pairs (i, j) over
which we have hypernym and/or coordinate ev-
idence, and which might represent additions of
a novel hyponym to the WordNet 2.1 taxonomy
(i.e., that has a known noun hypernym and an un-
known hyponym, or has a known noun coordi-
nate term and an unknown coordinate term). This
yields a list of 95,000 single links over threshold
P (Rij) > 0.12.
For each unknown hyponym i we may have
several pieces of evidence; for example, for the
unknown term continental we have 21 relevant
pieces of hypernym evidence, with links to possi-
ble hypernyms {carrier, airline, unit, . . .}; and we
have 5 pieces of coordinate evidence, with links to
possible coordinate terms {airline, american ea-
gle, airbus, . . .}.
For each proposed hypernym or coordinate link
involved with the novel hyponym i, we compute
the set of candidate hypernyms for i; in practice
we consider all senses of the immediate hypernym
j for each potential novel hypernym, and all senses
of the coordinate term k and its first two hypernym
ancestors for each potential coordinate.
In the continental example, from the 26 individ-
ual pieces of evidence over words we construct the
set of 99 unique synsets that we will consider as
possible hypernyms; these include the two senses
of the word airline, the ten senses of the word car-
rier, and so forth.
Next, we iterate through each of the possi-
ble hypernym synsets l under which we might
add the new word i; for each synset l we com-
805
pute the change in taxonomy score resulting from
adding the implied relations I(H1il) required by
the taxonomic constraints of T. Since typically
our set of all evidence involving i will be much
smaller than the set of possible relations in I(H1il),
we may efficiently check whether, for each sense
s ? senses(w), for all words where we have
some evidence ERiw, whether s participates in
some relation with i in the set of implied rela-
tions I(H1il).7 If there is more than one sense
s ? senses(w), we add to I(H1il) the single re-
lationship Ris that maximizes the taxonomy like-
lihood, i.e. argmaxs?senses(w) ?T(Ris).
3.4 Hypernym Sense Disambiguation
A major strength of our model is its ability to cor-
rectly choose the sense of a hypernym to which
to add a novel hyponym, despite collecting ev-
idence over untagged word pairs. In our algo-
rithm word sense disambiguation is an implicit
side-effect of our algorithm; since our algorithm
chooses to add the single link which, with its im-
plied links, yields the most likely taxonomy, and
since each distinct synset in WordNet has a differ-
ent immediate neighborhood of relations, our al-
gorithm simply disambiguates each node based on
its surrounding structural information.
As an example of sense disambiguation in prac-
tice, consider our example of continental. Sup-
pose we are iterating through each of the 99 pos-
sible synsets under which we might add conti-
nental as a hyponym, and we come to the synset
airline#n#2 in WordNet 2.1, i.e. ?a commer-
cial organization serving as a common carrier.?
In this case we will iterate through each piece
of hypernym and coordinate evidence; we find
that the relation H(continental, carrier) is satis-
fied with high probability for the specific synset
carrier#n#5, the grandparent of airline#n#2; thus
the factor ?T(H3(continental, carrier#n#5)) is
included in the factor of the set of implied rela-
tions ?T
(I(H1(continental, airline#n#2))).
Suppose we instead evaluate the first synset
of airline, i.e., airline#n#1, with the gloss ?a
hose that carries air under pressure.? For this
synset none of the other 20 relationships di-
rectly implied by hypernym evidence or the
5 relationships implied by the coordinate ev-
7Checking whether or not Ris ? I(H1il) may be effi-
ciently computed by checking whether s is in the hypernym
ancestors of l or if it shares a least common subsumer with l
within 7 steps.
idence are implied by adding the single link
H1(continental,airline#n#1); thus the resulting
change in the set of implied links given by the cor-
rect ?carrier? sense of airline is much higher than
that of the ?hose? sense. In fact it is the largest of
all the 99 considered hypernym links for continen-
tal; H1(continental, airline#n#2) is link #18,736
added to the taxonomy by our algorithm.
4 Evaluation
In order to evaluate our framework for taxonomy
induction, we have applied hyponym acquisition
to construct several distinct taxonomies, starting
with the base of WordNet 2.1 and only adding
novel noun hyponyms. Further, we have con-
structed taxonomies using a baseline algorithm,
which uses the identical hypernym and coordinate
classifiers used in our joint algorithm, but which
does not combine the evidence of the classifiers.
In section 4.1 we describe our evaluation
methodology; in sections 4.2 and 4.3 we analyze
the fine-grained precision and disambiguation pre-
cision of our algorithm compared to the baseline;
in section 4.4 we compare the coarse-grained pre-
cision of our links (motivated by categories de-
fined by the WordNet supersenses) against the
baseline algorithm and against an ?oracle? for
named entity recognition.
Finally, in section 4.5 we evaluate the tax-
onomies inferred by our algorithm directly against
the WordNet 2.1 taxonomy; we perform this eval-
uation by testing each taxonomy on a set of human
judgments of hypernym and non-hypernym noun
pairs sampled from newswire text.
4.1 Methodology
We evaluate the quality of our acquired hy-
ponyms by direct judgment. In four sep-
arate annotation sessions, two judges labeled
{50,100,100,100} samples uniformly generated
from the first {100,1000,10000,20000} single
links added by our algorithm.
For the direct measure of fine-grained precision,
we simply ask for each link H(X,Y ) added by the
system, is X a Y ? In addition to the fine-grained
precision, we give a coarse-grained evaluation, in-
spired by the idea of supersense-tagging in (Cia-
ramita and Johnson, 2003). The 26 supersenses
used in WordNet 2.1 are listed in Table 1; we label
a hyponym link as correct in the coarse-grained
evaluation if the novel hyponym is placed under
the appropriate supersense. This evaluation task
806
1 Tops 8 communication 15 object 22 relation
2 act 9 event 16 person 23 shape
3 animal 10 feeling 17 phenomenon 24 state
4 artifact 11 food 18 plant 25 substance
5 attribute 12 group 19 possession 26 time
6 body 13 location 20 process
7 cognition 14 motive 21 quantity
Table 1: The 26 WordNet supersenses
is similar to a fine-grained Named Entity Recog-
nition (Fleischman and Hovy, 2002) task with 26
categories; for example, if our algorithm mistak-
enly inserts a novel non-capital city under the hy-
ponym state capital, it will inherit the correct su-
persense location. Finally, we evaluate the abil-
ity of our algorithm to correctly choose the ap-
propriate sense of the hypernym under which a
novel hyponym is being added. Our labelers cate-
gorize each candidate sense-disambiguated hyper-
nym synset suggested by our algorithm into the
following categories:
c1: Correct sense-disambiguated hypernym.
c2: Correct hypernym word, but incorrect sense of
that word.
c3: Incorrect hypernym, but correct supersense.
c4: Any other relation is considered incorrect.
A single hyponym/hypernym pair is allowed to be
simultaneously labeled 2 and 3.
4.2 Fine-grained evaluation
Table 2 displays the results of our evaluation of
fine-grained precision for the baseline non-joint
algorithm (Base) and our joint algorithm (Joint),
as well as the relative error reduction (ER) of our
algorithm over the baseline. We use the mini-
mum of the two judges? scores. Here we define
fine-grained precision as c1/total. We see that
our joint algorithm strongly outperforms the base-
line, and has high precision for predicting novel
hyponyms up to 10,000 links.
4.3 Hypernym sense disambiguation
Also in Table 2 we compare the sense dis-
ambiguation precision of our algorithm and the
baseline. Here we measure the precision of
sense-disambiguation among all examples where
each algorithm found a correct hyponym word;
our calculation for disambiguation precision is
c1/ (c1 + c2). Again our joint algorithm outper-
forms the baseline algorithm at all levels of re-
call. Interestingly the baseline disambiguation
precision improves with higher recall; this may
Fine-grained Pre. Disambiguation Pre.
#Links Base Joint ER Base Joint ER
100 0.60 1.00 100% 0.86 1.00 100%
1000 0.52 0.93 85% 0.84 1.00 100%
10000 0.46 0.84 70% 0.90 1.00 100%
20000 0.46 0.68 41% 0.94 0.98 68%
Table 2: Fine-grained and disambiguation preci-
sion and error reduction for hyponym acquisition
# Links NER Base Joint ER vs. ER vs.
Oracle NER Base
100 1.00 0.72 1.00 0% 100%
1000 0.69 0.68 0.99 97% 85%
10000 0.45 0.69 0.96 93% 70%
20000 0.54 0.69 0.92 83% 41%
Table 3: Coarse-grained precision and error reduc-
tion vs. Non-joint baseline and NER Oracle
be attributed to the observation that the highest-
confidence hypernyms predicted by individual
classifiers are likely to be polysemous, whereas
hypernyms of lower confidence are more fre-
quently monosemous (and thus trivially easy to
disambiguate).
4.4 Coarse-grained evaluation
We compute coarse-grained precision as (c1 +
c3)/total. Inferring the correct coarse-grained su-
persense of a novel hyponym can be viewed as a
fine-grained (26-category) Named Entity Recog-
nition task; our algorithm for taxonomy induction
can thus be viewed as performing high-accuracy
fine-grained NER. Here we compare against both
the baseline non-joint algorithm as well as an
?oracle? algorithm for Named Entity Recogni-
tion, which perfectly classifies the supersense of
all nouns that fall under the four supersenses
{person, group, location, quantity}, but works
only for those supersenses. Table 3 shows the
results of this coarse-grained evaluation. We see
that the baseline non-joint algorithm has higher
precision than the NER oracle as 10,000 and
20,000 links; however, both are significantly out-
performed by our joint algorithm, which main-
tains high coarse-grained precision (92%) even at
20,000 links.
4.5 Comparison of inferred taxonomies and
WordNet
For our final evaluation we compare our learned
taxonomies directly against the currently exist-
ing hypernym links in WordNet 2.1. In order to
compare taxonomies we use a hand-labeled test
807
WN +10K +20K +30K +40K
PRE 0.524 0.524 0.574 0.583 0.571
REC 0.165 0.165 0.203 0.211 0.211
F 0.251 0.251 0.300 0.309 0.307
Table 4: Taxonomy hypernym classification vs.
WordNet 2.1 on hand-labeled testset
set of over 5,000 noun pairs, randomly-sampled
from newswire corpora (described in (Snow et al,
2005)). We measured the performance of both our
inferred taxonomies and WordNet against this test
set.8 The performance and comparison of the best
WordNet classifier vs. our taxonomies is given in
Table 4. Our best-performing inferred taxonomy
on this test set is achieved after adding 30,000
novel hyponyms, achieving an 23% relative im-
provement in F-score over the WN2.1 classifier.
5 Conclusions
We have presented an algorithm for inducing se-
mantic taxonomies which attempts to globally
optimize the entire structure of the taxonomy.
Our probabilistic architecture also includes a new
model for learning coordinate terms based on
(m,n)-cousin classification. The model?s ability
to integrate heterogeneous evidence from different
classifiers offers a solution to the key problem of
choosing the correct word sense to which to attach
a new hypernym.
Acknowledgements
Thanks to Christiane Fellbaum, Rajat Raina, Bill
MacCartney, and Allison Buckley for useful dis-
cussions and assistance annotating data. Rion
Snow is supported by an NDSEG Fellowship
sponsored by the DOD and AFOSR. This work
was supported in part by the Disruptive Technol-
ogy Office (DTO)?s Advanced Question Answer-
ing for Intelligence (AQUAINT) Program.
References
P. Buitelaar, P. Cimiano and B. Magnini. 2005. Ontol-
ogy Learning from Text: Methods, Evaluation and
Applications. Volume 123 Frontiers in Artificial In-
telligence and Applications.
S. Caraballo. 2001. Automatic Acquisition of
a Hypernym-Labeled Noun Hierarchy from Text.
Brown University Ph.D. Thesis.
8We found that the WordNet 2.1 model achieving the
highest F-score used only the first sense of each hyponym,
and allowed a maximum distance of 4 edges between each
hyponym and its hypernym.
S. Cederberg and D. Widdows. 2003. Using LSA and
Noun Coordination Information to Improve the Pre-
cision and Recall of Automatic Hyponymy Extrac-
tion. Proc. CoNLL-2003, pp. 111?118.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Relations.
Proc. EMNLP-2004.
M. Ciaramita and M. Johnson. 2003. Supersense
Tagging of Unknown Nouns in WordNet. Proc.
EMNLP-2003.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised Named-Entity Extraction from
the Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA: MIT Press.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning Semantic Constraints for the Automatic
Discovery of Part-Whole Relations. Proc. HLT-03.
M. Fleischman and E. Hovy. 2002. Fine grained clas-
sification of named entities. Proc. COLING-02.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. Proc. COLING-92.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. ACL-90.
D. Lenat. 1995. CYC: A Large-Scale Investment in
Knowledge Infrastructure, Communications of the
ACM, 38:11, 33?35.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain.
D. Lin, S. Zhao, L. Qin and M. Zhou. 2003. Iden-
tifying Synonyms among Distributionally Similar
Words. Proc. IJCAI-03.
M. Pasc?a. 2005. Finding Instance Names and Alter-
native Glosses on the Web: WordNet Reloaded. CI-
CLing 2005, pp. 280-292.
D. Ravichandran, P. Pantel, and E. Hovy. 2002. Ran-
domized Algorithms and NLP: Using Locality Sen-
sitive Hash Function for High Speed Noun Cluster-
ing. Proc. ACL-2002.
E. Riloff and J. Shepherd. 1997. A Corpus-Based
Approach for Building Semantic Lexicons. Proc
EMNLP-1997.
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurerence statistics for semi-automatic-semantic
lexicon construction. Proc. ACL-1998.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym dis-
covery. NIPS 2005.
P. Turney, M. Littman, J. Bigham, and V. Shnay-
der. 2003. Combining independent modules to
solve multiple-choice synonym and analogy prob-
lems. Proc. RANLP-2003, pp. 482?489.
808
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 618?626,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Solving the Problem of Cascading Errors: Approximate Bayesian
Inference for Linguistic Annotation Pipelines
Jenny Rose Finkel, Christopher D. Manning and Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305
{jrfinkel, manning, ang}@cs.stanford.edu
Abstract
The end-to-end performance of natural
language processing systems for com-
pound tasks, such as question answering
and textual entailment, is often hampered
by use of a greedy 1-best pipeline archi-
tecture, which causes errors to propagate
and compound at each stage. We present
a novel architecture, which models these
pipelines as Bayesian networks, with each
low level task corresponding to a variable
in the network, and then we perform ap-
proximate inference to find the best la-
beling. Our approach is extremely sim-
ple to apply but gains the benefits of sam-
pling the entire distribution over labels at
each stage in the pipeline. We apply our
method to two tasks ? semantic role la-
beling and recognizing textual entailment
? and achieve useful performance gains
from the superior pipeline architecture.
1 Introduction
Almost any system for natural language under-
standing must recover hidden linguistic structure
at many different levels: parts of speech, syntac-
tic dependencies, named entities, etc. For exam-
ple, modern semantic role labeling (SRL) systems
use the parse of the sentence, and question answer-
ing requires question type classification, parsing,
named entity tagging, semantic role labeling, and
often other tasks, many of which are dependent
on one another and must be pipelined together.
Pipelined systems are ubiquitous in NLP: in ad-
dition to the above examples, commonly parsers
and named entity recognizers use part of speech
tags and chunking information, and also word seg-
mentation for languages such as Chinese. Almost
no NLP task is truly standalone.
Most current systems for higher-level, aggre-
gate NLP tasks employ a simple 1-best feed for-
ward architecture: they greedily take the best out-
put at each stage in the pipeline and pass it on to
the next stage. This is the simplest architecture to
build (particularly if reusing existing component
systems), but errors are frequently made during
this pipeline of annotations, and when a system
is given incorrectly labeled input it is much harder
for that system to do its task correctly. For ex-
ample, when doing semantic role labeling, if no
syntactic constituent of the parse actually corre-
sponds to a given semantic role, then that seman-
tic role will almost certainly be misidentified. It
is therefore disappointing, but not surprising, that
F-measures on SRL drop more than 10% when
switching from gold parses to automatic parses
(for instance, from 91.2 to 80.0 for the joint model
of Toutanova (2005)).
A common improvement on this architecture is
to pass k-best lists between processing stages, for
example (Sutton and McCallum, 2005; Wellner et
al., 2004). Passing on a k-best list gives useful
improvements (e.g., in Koomen et al (2005)), but
efficiently enumerating k-best lists often requires
very substantial cognitive and engineering effort,
e.g., in (Huang and Chiang, 2005; Toutanova et
al., 2005).
At the other extreme, one can maintain the
entire space of representations (and their proba-
bilities) at each level, and use this full distribu-
tion to calculate the full distribution at the next
level. If restricting oneself to weighted finite state
transducers (WFSTs), a framework applicable to a
number of NLP applications (as outlined in Kart-
tunen (2000)), a pipeline can be compressed down
618
into a single WFST, giving outputs equivalent
to propagating the entire distribution through the
pipeline. In the worst case there is an exponential
space cost, but in many relevant cases composition
is in practice quite practical. Outside of WFSTs,
maintaining entire probability distributions is usu-
ally infeasible in NLP, because for most intermedi-
ate tasks, such as parsing and named entity recog-
nition, there is an exponential number of possible
labelings. Nevertheless, for some models, such as
most parsing models, these exponential labelings
can be compactly represented in a packed form,
e.g., (Maxwell and Kaplan, 1995; Crouch, 2005),
and subsequent stages can be reengineered to work
over these packed representations, e.g., (Geman
and Johnson, 2002). However, doing this normally
also involves a very high cognitive and engineer-
ing effort, and in practice this solution is infre-
quently adopted. Moreover, in some cases, a sub-
sequent module is incompatible with the packed
representation of a previous module and an ex-
ponential amount of work is nevertheless required
within this architecture.
Here we present an attractive middle ground
in dealing with linguistic pipelines. Rather than
only using the 1 or k most likely labelings at each
stage, we would indeed like to take into account
all possible labelings and their probabilities, but
we would like to be able to do so without a lot of
thinking or engineering. We propose that this can
be achieved by use of approximate inference. The
form of approximate inference we use is very sim-
ple: at each stage in the pipeline, we draw a sam-
ple from the distribution of labels, conditioned on
the samples drawn at previous stages. We repeat
this many times, and then use the samples from
the last stage, which corresponds to the ultimate,
higher-level task, to form a majority vote classifier.
As the number of samples increases, this method
will approximate the complete distribution. Use of
the method is normally a simple modification to an
existing piece of code, and the method is general.
It can be applied not only to all pipelines, but to
multi-stage algorithms which are not pipelines as
well.
We apply our method to two problems: seman-
tic role labeling and recognizing textual entail-
ment. For semantic role labeling we use a two
stage pipeline which parses the input sentence, and
for recognizing textual entailment we use a three
stage pipeline which tags the sentence with named
entities and then parses it before passing it to the
entailment decider.
2 Approach
2.1 Overview
In order to do approximate inference, we model
the entire pipeline as a Bayesian network. Each
stage in the pipeline corresponds to a variable in
the network. For example, the parser stage cor-
responds to a variable whose possible values are
all possible parses of the sentence. The probabil-
ities of the parses are conditioned on the parent
variables, which may just be the words of the sen-
tence, or may be the part of speech tags output by
a part of speech tagger.
The simple linear structure of a typical linguis-
tic annotation network permits exact inference that
is quadratic in the number of possible labels at
each stage, but unfortunately our annotation vari-
ables have a very large domain. Additionally,
some networks may not even be linear; frequently
one stage may require the output from multiple
previous stages, or multiple earlier stages may be
completely independent of one another. For ex-
ample, a typical QA system will do question type
classification on the question, and from that ex-
tract keywords which are passed to the informa-
tion retreival part of the system. Meanwhile, the
retreived documents are parsed and tagged with
named entities; the network rejoins those outputs
with the question type classification to decide on
the correct answer. We address these issues by
using approximate inference instead of exact in-
ference. The structure of the nodes in the network
permits direct sampling based on a topological sort
of the nodes. Samples are drawn from the condi-
tional distributions of each node, conditioned on
the samples drawn at earlier nodes in the topolog-
ical sort.
2.2 Probability of a Complete Labeling
Before we can discuss how to sample from these
Bayes nets, we will formalize how to move from
an annotation pipeline to a Bayes net. Let A be
the set of n annotators A1, A2, ..., An (e.g., part
of speech tagger, named entity recognizer, parser).
These are the variables in the network. For annota-
tor ai, we denote the set of other annotators whose
input is directly needed as Parents(Ai) ? A
and a particular assignment to those variables is
parents(Ai). The possible values for a particu-
619
lar annotator Ai are ai (e.g., a particular parse tree
or named entity tagging). We can now formulate
the probability of a complete annotation (over all
annotators) in the standard way for Bayes nets:
PBN(a1, a2, ..., an) =
N
?
i=1
P (ai|parents(Ai))
(1)
2.3 Approximate Inference in Bayesian
Networks
This factorization of the joint probability distri-
bution facilitates inference. However, exact in-
ference is intractable because of the number of
possible values for our variables. Parsing, part of
speech tagging, and named entity tagging (to name
a few) all have a number of possible labels that is
exponential in the length of the sentence, so we
use approximate inference. We chose Monte Carlo
inference, in which samples drawn from the joint
distribution are used to approximate a marginal
distribution for a subset of variables in the dis-
tribution. First, the nodes are sorted in topologi-
cal order. Then, samples are drawn for each vari-
able, conditioned on the samples which have al-
ready been drawn. Many samples are drawn, and
are used to estimate the joint distribution.
Importantly, for many language processing
tasks our application only needs to provide the
most likely value for a high-level linguistic an-
notation (e.g., the guessed semantic roles, or an-
swer to a question), and other annotations such as
parse trees are only present to assist in performing
that task. The probability of the final annotation is
given by:
PBN(an) =
?
a1,a2,...,an?1
PBN(a1, a2, ..., an) (2)
Because we are summing out all variables other
than the final one, we effectively use only the sam-
ples drawn from the final stage, ignoring the labels
of the variables, to estimate the marginal distribu-
tion over that variable. We then return the label
which had the highest number of samples. For
example, when trying to recognize textual entail-
ment, we count how many times we sampled ?yes,
it is entailed? and how many times we sampled
?no, it is not entailed? and return the answer with
more samples.
When the outcome you are trying to predict is
binary (as is the case with RTE) or n-ary for small
n, the number of samples needed to obtain a good
estimate of the posterior probability is very small.
This is true even if the spaces being sampled from
during intermediate stages are exponentially large
(such as the space of all parse trees). Ng and
Jordan (2001) show that under mild assumptions,
with only N samples the relative classification er-
ror will be at most O( 1N ) higher than the error of
the Bayes optimal classifier (in our case, the clas-
sifier which does exact inference). Even if the out-
come space is not small, the sampling technique
we present can still be very useful, as we will see
later for the case of SRL.
3 Generating Samples
The method we have outlined requires the ability
to sample from the conditional distributions in the
factored distribution of (1): in our case, the prob-
ability of a particular linguistic annotation, condi-
tioned on other linguistic annotations. Note that
this differs from the usual annotation task: taking
the argmax. But for most algorithms the change is
a small and easy change. We discuss how to ob-
tain samples efficiently from a few different anno-
tation models: probabilistic context free grammars
(PCFGs), and conditional random fields (CRFs).
3.1 Sampling Parses
Bod (1995) discusses parsing with probabilistic
tree substitution grammars, which, unlike simple
PCFGs, do not have a one-to-one mapping be-
tween output parse trees and a derivation (a bag of
rules) that produced it, and hence the most-likely
derivation may not correspond to the most likely
parse tree. He therefore presents a bottom-up ap-
proach to sampling derivations from a derivation
forest, which does correspond to a sample from the
space of parse trees. Goodman (1998) presents a
top-down version of this algorithm. Although we
use a PCFG for parsing, it is the grammar of (Klein
and Manning, 2003), which uses extensive state-
splitting, and so there is again a many-to-one cor-
respondence between derivations and parses, and
we use an algorithm similar to Goodman?s in our
work.
PCFGs put probabilities on each rule, such as
S ? NP VP and NN ? ?dog?. The probability of
a parse is the product of the probabilities of the
rules used to construct the parse tree. A dynamic
programing algorithm, the inside algorithm, can
be used to find the probability of a sentence. The
620
inside probability ?k(p, q) is the probability that
words p through q, inclusive, were produced by
the non-terminal k. So the probability of the sen-
tence The boy pet the dog. is equal to the inside
probability ?S(1, 6), where the first word, w1 is
The and the sixth word, w6, is [period]. It is also
useful for our purposes to view this quantity as the
sum of the probabilities of all parses of the sen-
tence which have S as the start symbol. The prob-
ability can be defined recursively (Manning and
Schu?tze, 1999) as follows:
?k(p, q) =
?
?
?
?
?
?
?
?
?
P (Nk ? wp) if p = q
?
r,s
q?1
?
d=p
P (Nk ? N rN s)?r(p, d)?s(d + 1, q)
otherwise
(3)
where Nk, N r and N s are non-terminal symbols
and wp is the word at position p. We have omit-
ted the case of unary rules for simplicity since it
requires a closure operation.
These probabilities can be efficiently computed
using a dynamic program. or memoization of each
value as it is calculated. Once we have computed
all of the inside probabilities, they can be used to
generate parses from the distribution of all parses
of the sentence, using the algorithm in Figure 1.
This algorithm is called after all of the inside
probabilities have been calculated and stored, and
take as parameters S, 1, and length(sentence). It
works by building the tree, starting from the root,
and recursively generating children based on the
posterior probabilities of applying each rule and
each possible position on which to split the sen-
tences. Intuitively, the algorithm is given a non-
terminal symbol, such as S or NP, and a span of
words, and has to decide (a) what rule to apply to
expand the non-terminal, and (b) where to split the
span of words, so that each non-terminal result-
ing from applying the rule has an associated word
span, and the process can repeat. The inside prob-
abilities are calculated just once, and we can then
generate many samples very quickly; DrawSam-
ples is linear in the number of words, and rules.
3.2 Sampling Named Entity Taggings
To do named entity recognition, we chose to use
a conditional random field (CRF) model, based on
Lafferty et al (2001). CRFs represent the state of
function DRAWSAMPLE(Nk, r, s)
if r = s
tree.label = Nk
tree.child = word(r)
return (tree)
for each rule m ? {m? : head(m?) = Nk}
N i ? lChild(m)
Nj ? rChild(m)
for q ? r to s? 1
scores(m,q)? P (m)?i(r, q)?j(q + 1, s)
(m, q)? SAMPLEFROM(scores)
tree.label = head(m)
tree.lChild = DRAWSAMPLE(lChild(m), r, q)
tree.rChild = DRAWSAMPLE(rChild(m), q + 1, s)
return (tree)
Figure 1: Pseudo-code for sampling parse trees from a PCFG.
This is a recursive algorithm which starts at the root of the
tree and expands each node by sampling from the distribu-
tion of possible rules and ways to split the span of words. Its
arguments are a non-terminal and two integers corresponding
to word indices, and it is initially called with arguments S, 1,
and the length of the sentence. There is a call to sampleFrom,
which takes an (unnormalized) probability distribution, nor-
malizes it, draws a sample and then returns the sample.
the art in sequence modeling ? they are discrimi-
natively trained, and maximize the joint likelihood
of the entire label sequence in a manner which
allows for bi-directional flow of information. In
order to describe how samples are generated, we
generalize CRFs in a way that is consistent with
the Markov random field literature. We create a
linear chain of cliques, each of which represents
the probabilistic relationship between an adjacent
set of n states using a factor table containing |S|n
values. These factor tables on their own should
not be viewed as probabilities, unnormalized or
otherwise. They are, however, defined in terms of
exponential models conditioned on features of the
observation sequence, and must be instantiated for
each new observation sequence. The probability
of a state sequence is then defined by the sequence
of factor tables in the clique chain, given the ob-
servation sequence:
PCRF(s|o) =
1
Z(o)
N
?
i=1
Fi(si?n . . . si) (4)
where Fi(si?n . . . si) is the element of the fac-
tor table at position i corresponding to states si?n
through si, and Z(o) is the partition function
which serves to normalize the distribution.1 To in-
1To handle the start condition properly, imagine also that
we define a set of distinguished start states s?(n?1) . . . s0.
621
fer the most likely state sequence in a CRF it is
customary to use the Viterbi algorithm.
We then apply a process called clique tree cal-
ibration, which involves passing messages be-
tween the cliques (see Cowell et al (2003) for
a full treatment of this topic). After this pro-
cess has completed, the factor tables can be
viewed as unnormalized probabilities, which can
be used to compute conditional probabilities,
PCRF(si|si?n . . . si?1, o). Once these probabili-
ties have been calculated, generating samples is
very simple. First, we draw a sample for the label
at the first position,2 and then, for each subsequent
position, we draw a sample from the distribution
for that position, conditioned on the label sampled
at the previous position. This process results in
a sample of a complete labeling of the sequence,
drawn from the posterior distribution of complete
named entity taggings.
Similarly to generating sample parses, the ex-
pensive part is calculating the probabilities; once
we have them we can generate new samples very
quickly.
3.3 k-Best Lists
At first glance, k-best lists may seem like they
should outperform sampling, because in effect
they are the k best samples. However, there are
several important reasons why one might prefer
sampling. One reason is that the k best paths
through a word lattice, or the k best derivations in
parse forest do not necessarily correspond to the
k best sentences or parse trees. In fact, there are
no known sub-exponential algorithms for the best
outputs in these models, when there are multiple
ways to derive the same output.3 This is not just a
theoretical concern ? the Stanford parser uses such
a grammar, and we found that when generating a
50-best derivation list that on average these deriva-
tions corresponded to about half as many unique
parse trees. Our approach circumvents this issue
entirely, because the samples are generated from
the actual output distribution.
Intuition also suggests that sampling should
give more diversity at each stage, reducing the
likelihood of not even considering the correct out-
put. Using the Brown portion of the SRL test
set (discussed in sections 4 and 6.1), and 50-
samples/50-best, we found that on average the 50-
2Conditioned on the distinguished start states.
3Many thanks to an anonymous reviewer for pointing out
this argument.
samples system considered approximately 25%
more potential SRL labelings than the 50-best sys-
tem.
When pipelines have more than two stages, it
is customary to do a beam search, with a beam
size of k. This means that at each stage in the
pipeline, more and more of the probability mass
gets ?thrown away.? Practically, this means that
as pipeline length increases, there will be in-
creasingly less diversity of labels from the earlier
stages. In a degenerate 10-stage, k-best pipeline,
where the last stage depends mainly on the first
stage, it is probable that all but a few labelings
from the first stage will have been pruned away,
leaving something much smaller than a k-best
sample, possibly even a 1-best sample, as input to
the final stage. Using approximate inference to es-
timate the marginal distribution over the last stage
in the pipeline, such as our sampling approach, the
pipeline length does not have this negative impact
or affect the number of samples needed. And un-
like k-best beam searches, there is an entire re-
search community, along with a large body of lit-
erature, which studies how to do approximate in-
ference in Bayesian networks and can provide per-
formance bounds based on the method and the
number of samples generated.
One final issue with the k-best method arises
when instead of a linear chain pipeline, one is us-
ing a general directed acyclic graph where a node
can have multiple parents. In this situation, doing
the k-best calculation actually becomes exponen-
tial in the size of the largest in-degree of a node ?
for a node with n parents, you must try all kn com-
binations of the values for the parent nodes. With
sampling this is not an issue; each sample can be
generated based on a topological sort of the graph.
4 Semantic Role Labeling
4.1 Task Description
Given a sentence and a target verb (the predicate)
the goal of semantic role labeling is to identify and
label syntactic constituents of the parse tree with
semantic roles of the predicate. Common roles
are agent, which is the thing performing the ac-
tion, patient, which is the thing on which the ac-
tion is being performed, and instrument, which is
the thing with which the action is being done. Ad-
ditionally, there are modifier arguments which can
specify the location, time, manner, etc. The fol-
lowing sentence provides an example of a predi-
622
cate and its arguments:
[The luxury auto maker]agent [last
year]temp [sold]pred [1,214 cars]patient
in [the U.S]location.
Semantic role labeling is a key component for
systems that do question answering, summariza-
tion, and any other task which directly uses a se-
mantic interpretation.
4.2 System Description
We modified the system described in Haghighi
et al (2005) and Toutanova et al (2005) to test
our method. The system uses both local models,
which score subtrees of the entire parse tree inde-
pendently of the labels of other nodes not in that
subtree, and joint models, which score the entire
labeling of a tree with semantic roles (for a partic-
ular predicate).
First, the task is separated into two stages, and
local models are learned for each. At the first
stage, the identification stage, a classifier labels
each node in the tree as either ARG, meaning that
it is an argument (either core or modifier) to the
predicate, or NONE, meaning that it is not an argu-
ment. At the second stage, the classification stage,
the classifier is given a set of arguments for a pred-
icate and must label each with its semantic role.
Next, a Viterbi-like dynamic algorithm is used
to generate a list of the k-best joint (identification
and classification) labelings according to the lo-
cal models. The algorithm enforces the constraint
that the roles should be non-overlapping. Finally,
a joint model is constructed which scores a com-
pletely labeled tree, and it is used to re-rank the k-
best list. The separation into local and joint mod-
els is necessary because there are an exponential
number of ways to label the entire tree, so using
the joint model alone would be intractable. Ide-
ally, we would want to use approximate inference
instead of a k-best list here as well. Particle fil-
tering would be particularly well suited - particles
could be sampled from the local model and then
reweighted using the joint model. Unfortunately,
we did not have enough time modify the code of
(Haghighi et al, 2005) accordingly, so the k-best
structure remained.
To generate samples from the SRL system, we
take the scores given to the k-best list, normalize
them to sum to 1, and sample from them. One
consequence of this, is that any labeling not on the
k-best list has a probability of 0.
5 Recognizing Textual Entailment
5.1 Task Description
In the task of recognizing textual entailment
(RTE), also commonly referred to as robust textual
inference, you are provided with two passages, a
text and a hypothesis, and must decide whether the
hypothesis can be inferred from the text. The term
robust is used because the task is not meant to be
domain specific. The term inference is used be-
cause this is not meant to be logical entailment, but
rather what an intelligent, informed human would
infer. Many NLP applications would benefit from
the ability to do robust textual entailment, includ-
ing question answering, information retrieval and
multi-document summarization. There have been
two PASCAL workshops (Dagan et al, 2005) with
shared tasks in the past two years devoted to RTE.
We used the data from the 2006 workshop, which
contains 800 text-hypothesis pairs in each of the
test and development sets4 (there is no training
set). Here is an example from the development
set from the first RTE challenge:
Text: Researchers at the Harvard School of Pub-
lic Health say that people who drink coffee
may be doing a lot more than keeping them-
selves awake ? this kind of consumption ap-
parently also can help reduce the risk of dis-
eases.
Hypothesis: Coffee drinking has health benefits.
The positive and negative examples are bal-
anced, so the baseline of guessing either all yes
or all no would score 50%. This is a hard task ? at
the first challenge no system scored over 60%.
5.2 System Description
MacCartney et al (2006) describe a system for do-
ing robust textual inference. They divide the task
into three stages ? linguistic analysis, graph align-
ment, and entailment determination. The first of
these stages, linguistic analysis is itself a pipeline
of parsing and named entity recognition. They use
the syntactic parse to (deterministically) produce
a typed dependency graph for each sentence. This
pipeline is the one we replace. The second stage,
graph alignment consists of trying to find good
alignments between the typed dependency graphs
4The dataset and further information from both
challenges can be downloaded from http://www.pascal-
network.org/Challenges/RTE2/Datasets/
623
NER parser RTE
Figure 2: The pipeline for recognizing textual entailment.
for the text and hypothesis. Each possible align-
ment has a score, and the alignment with the best
score is propagated forward. The final stage, en-
tailment determination, is where the decision is
actually made. Using the score from the align-
ment, as well as other features, a logistic model
is created to predict entailment. The parameters
for this model are learned from development data.5
While it would be preferable to sample possible
alignments, their system for generating alignment
scores is not probabilistic, and it is unclear how
one could convert between alignment scores and
probabilities in a meaningful way.
Our modified linguistic analysis pipeline does
NER tagging and parsing (in their system, the
parse is dependent on the NER tagging because
some types of entities are pre-chunked before
parsing) and treats the remaining two sections of
their pipeline, the alignment and determination
stages, as one final stage. Because the entailment
determination stage is based on a logistic model, a
probability of entailment is given and sampling is
straightforward.
6 Experimental Results
In our experiments we compare the greedy
pipelined approach with our sampling pipeline ap-
proach.
6.1 Semantic Role Labeling
For the past two years CoNLL has had shared
tasks on SRL (Carreras and Ma`rquez (2004) and
Carreras and Ma`rquez (2005)). We used the
CoNLL 2005 data and evaluation script. When
evaluating semantic role labeling results, it is com-
mon to present numbers on both the core argu-
ments (i.e., excluding the modifying arguments)
and all arguments. We follow this convention and
present both sets of numbers. We give precision,
5They report their results on the first PASCAL dataset,
and use only the development set from the first challenge for
learning weights. When we test on the data from the second
challenge, we use all data from the first challenge and the
development data from the second challenge to learn these
weights.
SRL Results ? Penn Treebank Portion
Core Args Precision Recall F-measure
Greedy 79.31% 77.7% 78.50%
K-Best 80.05% 78.45% 79.24%
Sampling 80.13% 78.25% 79.18%
All Args Precision Recall F-measure
Greedy 78.49% 74.77% 76.58%
K-Best 79.58% 74.90% 77.16%
Sampling 79.81% 74.85% 77.31%
SRL Results ? Brown Portion
Core Args Precision Recall F-measure
Greedy 68.28% 67.72% 68.0%
K-Best 69.25% 69.02% 69.13%
Sampling 69.35% 68.93% 69.16%
All Args Precision Recall F-measure
Greedy 66.6% 60.45% 63.38%
K-Best 68.82% 61.03% 64.69%
Sampling 68.6% 61.11% 64.64%
Table 1: Results for semantic role labeling task. The sampled
numbers are averaged over several runs, as discussed.
recall and F-measure, which are based on the num-
ber of arguments correctly identified. For an argu-
ment to be correct both the span and the classifica-
tion must be correct; there is no partial credit.
To generate sampled parses, we used the Stan-
ford parser (Klein and Manning, 2003). The
CoNLL data comes with parses from Charniak?s
parser (Charniak, 2000), so we had to re-parse
the data and retrain the SRL system on these new
parses, resulting in a lower baseline than previ-
ously presented work. We choose to use Stan-
ford?s parser because of the ease with which we
could modify it to generate samples. Unfortu-
nately, its performance is slightly below that of the
other parsers.
The CoNLL data has two separate test sets; the
first is section 23 of the Penn Treebank (PTB),
and the second is ?fresh sentences? taken from the
Brown corpus. For full results, please see Table 1.
On the Penn Treebank portion we saw an absolute
F-score improvement of 0.7% on both core and all
arguments. On the Brown portion of the test set we
saw an improvement of 1.25% on core and 1.16%
on all arguments. In this context, a gain of over
1% is quite large: for instance, the scores for the
top 4 systems on the Brown data at CoNLL 2005
were within 1% of each other. For both portions,
we generated 50 samples, and did this 4 times, av-
eraging the results. We most likely saw better per-
formance on the Brown portion than the PTB por-
tion because the parser was trained on the Penn
Treebank training data, so the most likely parses
will be of higher quality for the PTB portion of
the test data than for the Brown portion. We also
624
RTE Results
Accuracy Average Precision
Greedy 59.13% 59.91%
Sampling 60.88% 61.99%
Table 2: Results for recognizing textual entailment. The sam-
pled numbers are averaged over several runs, as discussed.
ran the pipeline using a 50-best list, and found the
two results to be comparable.
6.2 Textual Entailment
For the second PASCAL RTE challenge, two dif-
ferent types of performance measures were used
to evaluate labels and confidence of the labels for
the text-hypothesis pairs. The first measure is ac-
curacy ? the percentage of correct judgments. The
second measure is average precision. Responses
are sorted based on entailment confidence and then
average precision is calculated by the following
equation:
1
R
n
?
i=1
E(i)# correct up to pair ii (5)
where n is the size of the test set, R is the number
of positive (entailed) examples, E(i) is an indi-
cator function whose value is 1 if the ith pair is
entailed, and the is are sorted based on the entail-
ment confidence. The intention of this measure is
to evaluate how well calibrated a system is. Sys-
tems which are more confident in their correct an-
swers and less confident in their incorrect answers
will perform better on this measure.
Our results are presented in Table 2. We gen-
erated 25 samples for each run, and repeated the
process 7 times, averaging over runs. Accuracy
was improved by 1.5% and average precision by
2%. It does not come as a surprise that the average
precision improvement was larger than the accu-
racy improvement, because our model explicitly
estimates its own degree of confidence by estimat-
ing the posterior probability of the class label.
7 Conclusions and Future Work
We have presented a method for handling lan-
guage processing pipelines in which later stages
of processing are conditioned on the results of
earlier stages. Currently, common practice is to
take the best labeling at each point in a linguistic
analysis pipeline, but this method ignores informa-
tion about alternate labelings and their likelihoods.
Our approach uses all of the information available,
and has the added advantage of being extremely
simple to implement. By modifying your subtasks
to generate samples instead of the most likely la-
beling, our method can be used with very little ad-
ditional overhead. And, as we have shown, such
modifications are usually simple to make; further,
with only a ?small? (polynomial) number of sam-
ples k, under mild assumptions the classification
error obtained by the sampling approximation ap-
proaches that of exact inference. (Ng and Jordan,
2001) In contrast, an algorithm that keeps track
only of the k-best list enjoys no such theoretical
guarantee, and can require an exponentially large
value for k to approach comparable error. We also
note that in practice, k-best lists are often more
complicated to implement and more computation-
ally expensive (e.g. the complexity of generat-
ing k sample parses or CRF outputs is substan-
tially lower than that of generating the k best parse
derivations or CRF outputs).
The major contribution of this work is not
specific to semantic role labeling or recognizing
textual entailment. We are proposing a general
method to deal with all multi-stage algorithms. It
is common to build systems using many different
software packages, often from other groups, and to
string together the 1-best outputs. If, instead, all
NLP researchers wrote packages which can gen-
erate samples from the posterior, then the entire
NLP community could use this method as easily
as they can use the greedy methods that are com-
mon today, and which do not perform as well.
One possible direction for improvement of this
work would be to move from a Bayesian network
to an undirected Markov network. This is desir-
able because influence should be able to flow in
both directions in this pipeline. For example, the
semantic role labeler should be able to tell the
parser that it did not like a particular parse, and
this should influence the probability assigned to
that parse. The main difficulty here lies in how
to model this reversal of influence. The problem
of using parse trees to help decide good semantic
role labelings is well studied, but the problem of
using semantic role labelings to influence parses is
not. Furthermore, this requires building joint mod-
els over adjacent nodes, which is usually a non-
trivial task. However, we feel that this approach
would improve performance even more on these
pipelined tasks and should be pursued.
625
8 Acknowledgements
We would like to thank our anonymous review-
ers for their comments and suggestions. We
would also like to thank Kristina Toutanova, Aria
Haghighi and the Stanford RTE group for their as-
sistance in understanding and using their code.
This paper is based on work funded in part by a
Stanford School of Engineering fellowship and in
part by the Defense Advanced Research Projects
Agency through IBM. The content does not nec-
essarily reflect the views of the U.S. Government,
and no official endorsement should be inferred.
References
Rens Bod. 1995. The problem of computing the most proba-
ble tree in data-oriented parsing and stochastic tree gram-
mars. In Proceedings of EACL 1995.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL 2004.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL 2005.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 14th National Conference
on Artificial Intelligence.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and
David J. Spiegelhalter. 2003. Probabilistic Networks and
Expert Systems. Springer.
Richard Crouch. 2005. Packed rewriting for mapping se-
mantics to KR. In Proceedings of the 6th International
Workshop on Computational Semantics.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The PASCAL recognizing textual entailment challenge. In
Proceedings of the PASCAL Challenges Workshop on Rec-
ognizing Textual Entailment.
Stuart Geman and Mark Johnson. 2002. Dynamic program-
ming for parsing and estimation of stochastic unification-
based grammars. In Proceedings of ACL 2002.
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Aria Haghighi, Kristina Toutanova, and Christopher D. Man-
ning. 2005. A joint model for semantic role labeling. In
Proceedings of CoNLL 2005.
Liang Huang and David Chiang. 2005. Better k-best pars-
ing. In Proceedings of the 9th International Workshop on
Parsing Technologies.
Lauri Karttunen. 2000. Applications of finite-state trans-
ducers in natural-language processing. In Proceesings of
the Fifth International Conference on Implementation and
Application of Automata.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL 2003.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen tau
Yih. 2005. Generalized inference with multiple semantic
role labeling systems. In Proceedings of CoNLL 2005,
pages 181?184.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic models
for segmenting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on Ma-
chine Learning, pages 282?289.
Bill MacCartney, Trond Grenager, Marie de Marneffe, Daniel
Cer, and Christopher D. Manning. 2006. Learning to rec-
ognize features of valid textual entailments. In Proceed-
ings of NAACL-HTL 2006.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press, Cambridge, Massachusetts.
John T. Maxwell, III and Ronald M. Kaplan. 1995. A method
for disjunctive constraint satisfaction. In Mary Dalrymple,
Ronald M. Kaplan, John T. Maxwell III, and Annie Zae-
nen, editors, Formal Issues in Lexical-Functional Gram-
mar, number 47 in CSLI Lecture Notes Series, chapter 14,
pages 381?481. CSLI Publications.
Andrew Ng and Michael Jordan. 2001. Convergence rates of
the voting Gibbs classifier, with application to Bayesian
feature selection. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning.
Charles Sutton and Andrew McCallum. 2005. Joint pars-
ing and semantic role labeling. In Proceedings of CoNLL
2005, pages 225?228.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL 2005.
Kristina Toutanova. 2005. Effective statistical models for syn-
tactic and semantic disambiguation. Ph.D. thesis, Stan-
ford University.
Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael
Hay. 2004. An integrated, conditional model of informa-
tion extraction and coreference with application to citation
matching. In Proceedings of the 20th Annual Conference
on Uncertainty in Artificial Intelligence.
626
Workshop on TextGraphs, at HLT-NAACL 2006, pages 1?8,
New York City, June 2006. c?2006 Association for Computational Linguistics
A Graphical Framework for Contextual Search and Name Disambiguation
in Email
Einat Minkov
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15213
einatm@cs.cmu.edu
William W. Cohen
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Andrew Y. Ng
Computer Science Dept.
Stanford University
Stanford, CA 94305
ang@cs.stanford.edu
Abstract
Similarity measures for text have histor-
ically been an important tool for solving
information retrieval problems. In this pa-
per we consider extended similarity met-
rics for documents and other objects em-
bedded in graphs, facilitated via a lazy
graph walk. We provide a detailed in-
stantiation of this framework for email
data, where content, social networks and
a timeline are integrated in a structural
graph. The suggested framework is evalu-
ated for the task of disambiguating names
in email documents. We show that rerank-
ing schemes based on the graph-walk sim-
ilarity measures often outperform base-
line methods, and that further improve-
ments can be obtained by use of appropri-
ate learning methods.
1 Introduction
Many tasks in information retrieval can be per-
formed by clever application of textual similarity
metrics. In particular, The canonical IR problem of
ad hoc retrieval is often formulated as the task of
finding documents ?similar to? a query. In modern
IR settings, however, documents are usually not iso-
lated objects: instead, they are frequently connected
to other objects, via hyperlinks or meta-data. (An
email message, for instance, is connected via header
information to other emails in the same thread and
also to the recipient?s social network.) Thus it is
important to understand how text-based document
similarity measures can be extended to documents
embedded in complex structural settings.
Our similarity metric is based on a lazy graph
walk, and is closely related to the well-known
PageRank algorithm (Page et al, 1998). PageRank
and its variants are based on a graph walk of infi-
nite length with random resets. In a lazy graph walk,
there is a fixed probability of halting the walk at each
step. In previous work (Toutanova et al, 2004), lazy
walks over graphs were used for estimating word
dependency distributions: in this case, the graph
was one constructed especially for this task, and the
edges in the graph represented different flavors of
word-to-word similarity. Other recent papers have
also used walks over graphs for query expansion (Xi
et al, 2005; Collins-Thompson and Callan, 2005).
In these tasks, the walk propagates similarity to a
start node through edges in the graph?incidentally
accumulating evidence of similarity over multiple
connecting paths.
In contrast to this previous work, we consider
schemes for propogating similarity across a graph
that naturally models a structured dataset like an
email corpus: entities correspond to objects includ-
ing email addresses and dates, (as well as the usual
types of documents and terms), and edges corre-
spond to relations like sent-by. We view the simi-
larity metric as a tool for performing search across
this structured dataset, in which related entities that
are not directly similar to a query can be reached via
multi-step graph walk.
In this paper, we formulate and evaluate this ex-
tended similarity metric. The principal problem we
1
consider is disambiguating personal names in email,
which we formulate as the task of retrieving the per-
son most related to a particular name mention. We
show that for this task, the graph-based approach im-
proves substantially over plausible baselines. After
retrieval, learning can be used to adjust the ranking
of retrieved names based on the edges in the paths
traversed to find these names, which leads to an ad-
ditional performance improvement. Name disam-
biguation is a particular application of the suggested
general framework, which is also applicable to any
real-world setting in which structural data is avail-
able as well as text.
This paper proceeds as follows. Sections 2 and
3 formalize the general framework and its instanti-
ation for email. Section 4 gives a short summary
of the learning approach. Section 5 includes experi-
mental evaluation, describing the corpora and results
for the person name disambiguation task. The paper
concludes with a review of related work, summary
and future directions.
2 Email as a Graph
A graph G consists of a set of nodes, and a set of la-
beled directed edges. Nodes will be denoted by let-
ters like x, y, or z, and we will denote an edge from
x to y with label ` as x `?? y. Every node x has
a type, denoted T (x), and we will assume that there
are a fixed set of possible types. We will assume for
convenience that there are no edges from a node to
itself (this assumption can be easily relaxed.)
We will use these graphs to represent real-world
data. Each node represents some real-world entity,
and each edge x `?? y asserts that some binary
relation `(x, y) holds. The entity types used here
to represent an email corpus are shown in the left-
most column of Table 1. They include the tradi-
tional types in information retrieval systems, namely
file and term. In addition, however, they include the
types person, email-address and date. These enti-
ties are constructed from a collection of email mes-
sages in the obvious way?for example, a recipient of
?Einat Minkov <einat@cs.cmu.edu>? indicates the
existence of a person node ?Einat Minkov? and an
email-address node ?einat@cs.cmu.edu?. (We as-
sume here that person names are unique identifiers.)
The graph edges are directed. We will assume
that edge labels determine the source and target
node types: i.e., if x `?? z and w `?? y then
T (w) = T (x) and T (y) = T (z). However, mul-
tiple relations can hold between any particular pair
of nodes types: for instance, it could be that x `?? y
or x `
?
?? y, where ` 6= `?. (For instance, an email
message x could be sent-from y, or sent-to y.) Note
also that edges need not denote functional relations:
for a given x and `, there may be many distinct nodes
y such that x `?? y. For instance, for a file x, there
are many distinct terms y such that x has-term?? y holds.
In representing email, we also create an inverse
label `?1 for each edge label (relation) `. Note that
this means that the graph will definitely be cyclic.
Table 1 gives the full set of relations used in our
email represention scheme.
3 Graph Similarity
3.1 Edge weights
Similarity between two nodes is defined by a lazy
walk process, and a walk on the graph is controlled
by a small set of parameters ?. To walk away from
a node x, one first picks an edge label `; then, given
`, one picks a node y such that x `?? y. We assume
that the probability of picking the label ` depends
only on the type T (x) of the node x, i.e., that the
outgoing probability from node x of following an
edge type ` is:
Pr(` | x) = Pr(` | Ti) ? ?`,Ti
Let STi be the set of possible labels for an edge leav-
ing a node of type Ti. We require that the weights
over all outgoing edge types given the source node
type form a probability distribution, i.e., that
?
`?STi
?`,Ti = 1
In this paper, we will assume that once ` is picked,
y is chosen uniformly from the set of all y such that
x `?? y. That is, the weight of an edge of type l
connecting source node x to node y is:
Pr(x `?? y | `) = ?`,Ti
| y : x `?? y |
This assumption could easily be generalized, how-
ever: for instance, for the type T (x) = file and
2
source type edge type target type
file sent-from person
sent-from-email email-address
sent-to person
sent-to-email email-address
date-of date
has-subject-term term
has-term term
person sent-from inv. file
sent-to?1 file
alias email-address
has-term term
email-address sent-to-email?1 file
sent-from-email?1 file
alias-inverse person
is-email?1 term
term has-term?1 file
has subject-term?1 file
is-email email-address
has-term?1 person
date date-of?1 file
Table 1: Graph structure: Node and relation types
` = has-term, weights for terms y such that x `?? y
might be distributed according to an appropriate lan-
guage model (Croft and Lafferty, 2003).
3.2 Graph walks
Conceptually, the edge weights above define the
probability of moving from a node x to some other
node y. At each step in a lazy graph walk, there
is also some probability ? of staying at x. Putting
these together, and denoting byMxy the probability
of being at node y at time t + 1 given that one is at
x at time t in the walk, we define
Mxy =
{
(1 ? ?)
?
` Pr(x
`?? y|`) ? Pr(`|T (x)) x 6= y
? x = y
If we associate nodes with integers, and makeM
a matrix indexed by nodes, then a walk of k steps
can then be defined by matrix multiplication: specif-
ically, if V0 is some initial probability distribution
over nodes, then the distribution after a k-step walk
is proportional to Vk = V0Mk. Larger values of ?
increase the weight given to shorter paths between
x and y. In the experiments reported here, we con-
sider small values of k, and this computation is car-
ried out directly using sparse-matrix multiplication
methods.1 If V0 gives probability 1 to some node x0
1We have also explored an alternative approach based on
sampling; this method scales better but introduces some addi-
tional variance into the procedure, which is undesirable for ex-
perimentation.
and probability 0 to all other nodes, then the value
given to y in Vk can be interpreted as a similarity
measure between x and y.
In our framework, a query is an initial distribu-
tion Vq over nodes, plus a desired output type Tout ,
and the answer is a list of nodes y of type Tout ,
ranked by their score in the distribution Vk. For in-
stance, for an ordinary ad hoc document retrieval
query (like ?economic impact of recycling tires?)
would be an appropriate distribution Vq over query
terms, with Tout = file . Replacing Tout with person
would find the person most related to the query?
e.g., an email contact heavily associated with the
retread economics. Replacing Vq with a point dis-
tribution over a particular document would find the
people most closely associated with the given docu-
ment.
3.3 Relation to TF-IDF
It is interesting to view this framework in compar-
ison to more traditional IR methods. Suppose we
restrict ourselves to two types, terms and files, and
allow only in-file edges. Now consider an initial
query distribution Vq which is uniform over the two
terms ?the aardvark?. A one-step matrix multiplica-
tion will result in a distribution V1, which includes
file nodes. The common term ?the? will spread
its probability mass into small fractions over many
file nodes, while the unusual term ?aardvark? will
spread its weight over only a few files: hence the
effect will be similar to use of an IDF weighting
scheme.
4 Learning
As suggested by the comments above, this graph
framework could be used for many types of tasks,
and it is unlikely that a single set of parameter val-
ues will be best for all tasks. It is thus important to
consider the problem of learning how to better rank
graph nodes.
Previous researchers have described schemes for
adjusting the parameters ? using gradient descent-
like methods (Diligenti et al, 2005; Nie et al, 2005).
In this paper, we suggest an alternative approach of
learning to re-order an initial ranking. This rerank-
ing approach has been used in the past for meta-
search (Cohen et al, 1999) and also several natural-
3
language related tasks (Collins and Koo, 2005). The
advantage of reranking over parameter tuning is that
the learned classifier can take advantage of ?global?
features that are not easily used in walk.
Note that node reranking, while can be used as
an alternative to weight manipulation, it is better
viewed as a complementary approach, as the tech-
niques can be naturally combined by first tuning the
parameters ?, and then reranking the result using a
classifier which exploits non-local features. This hy-
brid approach has been used successfully in the past
on tasks like parsing (Collins and Koo, 2005).
We here give a short overview of the reranking ap-
proach, that is described in detail elsewhere (Collins
and Koo, 2005). The reranking algorithm is pro-
vided with a training set containing n examples. Ex-
ample i (for 1 ? i ? n) includes a ranked list of
li nodes. Let wij be the jth node for example i,
and let p(wij) be the probability assigned to wij by
the graph walk. A candidate node wij is represented
through m features, which are computed by m fea-
ture functions f1, . . . , fm. We will require that the
features be binary; this restriction allows a closed
form parameter update. The ranking function for
node x is defined as:
F (x, ??) = ?0L(x) +
m
?
k=1
?kfk(x)
where L(x) = log(p(x)) and ?? is a vector of real-
value parameters. Given a new test example, the out-
put of the model is the given node list re-ranked by
F (x, ??).
To learn the parameter weights ??, we use a boost-
ing method (Collins and Koo, 2005), which min-
imizes the following loss function on the training
data:
ExpLoss(??) =
?
i
li
?
j=2
e?(F (xi,1,??)?F (xi,j ,??))
where xi,1 is, without loss of generality, a correct
target node. The weights for the function are learned
with a boosting-like method, where in each itera-
tion the feature fk that has the most impact on the
loss function is chosen, and ?k is modified. Closed
form formulas exist for calculating the optimal ad-
ditive updates and the impact per feature (Schapire
and Singer, 1999).
5 Evaluation
We experiment with three separate corpora.
The Cspace corpus contains email messages col-
lected from a management course conducted at
Carnegie Mellon University in 1997 (Minkov et
al., 2005). In this course, MBA students, orga-
nized in teams of four to six members, ran simu-
lated companies in different market scenarios. The
corpus we used here includes the emails of all
teams over a period of four days. The Enron cor-
pus is a collection of mail from the Enron cor-
pus that has been made available for the research
community (Klimt and Yang, 2004). Here, we
used the saved email of two different users.2 To
eliminate spam and news postings we removed
email files sent from email addresses with suf-
fix ?.com? that are not Enron?s; widely distributed
email files (sent from ?enron.announcement?, to
?all.employees@enron.com? etc.). Text from for-
warded messages, or replied-to messages were also
removed from the corpus.
Table 2 gives the size of each processed corpus,
and the number of nodes in the graph representation
of it. In deriving terms for the graph, terms were
Porter-stemmed and stop words were removed. The
processed Enron corpora are available from the first
author?s home page.
corpus Person set
files nodes train test
Cspace 821 6248 26 80
Sager-E 1632 9753 11 51
Shapiro-R 978 13174 11 49
Table 2: Corpora Details
5.1 Person Name Disambiguation
5.1.1 Task definition
Consider an email message containing a common
name like ?Andrew?. Ideally an intelligent mailer
would, like the user, understand which person ?An-
drew? refers to, and would rapidly perform tasks like
retrieving Andrew?s prefered email address or home
page. Resolving the referent of a person name is also
an important complement to the ability to perform
named entity extraction for tasks like social network
analysis or studies of social interaction in email.
2Specifially, we used the ?all documents? folder, including
both incoming and outgoing files.
4
However, although the referent of the name is
unambiguous to the recipient of the email, it can
be non-trivial for an automated system to find out
which ?Andrew? is indicated. Automatically de-
termining that ?Andrew? refers to ?Andrew Y. Ng?
and not ?Andrew McCallum? (for instance) is espe-
cially difficult when an informal nickname is used,
or when the mentioned person does not appear in the
email header. As noted above, we model this prob-
lem as a search task: based on a name-mention in an
email message m, we formulate query distribution
Vq, and then retrieve a ranked list of person nodes.
5.1.2 Data preparation
Unfortunately, building a corpus for evaluating
this task is non-trivial, because (if trivial cases are
eliminated) determining a name?s referent is often
non-trivial for a human other than the intended re-
cipient. We evaluated this task using three labeled
datasets, as detailed in Table 2.
The Cspace corpus has been manually annotated
with personal names (Minkov et al, 2005). Addi-
tionally, with the corpus, there is a great deal of
information available about the composition of the
individual teams, the way the teams interact, and
the full names of the team members. Using this
extra information it is possible to manually resolve
name mentions. We collected 106 cases in which
single-token names were mentioned in the the body
of a message but did not match any name from the
header. Instances for which there was not suffi-
cient information to determine a unique person en-
tity were excluded from the example set. In addition
to names that refer to people that are simply not in
the header, the names in this corpus include people
that are in the email header, but cannot be matched
because they are referred to using: initials?this is
commonly done in the sign-off to an email; nick-
names, including common nicknames (e.g., ?Dave?
for ?David?), unusual nicknames (e.g., ?Kai? for
?Keiko?); or American names adopted in place of
a foreign name (e.g., ?Jenny? for ?Qing?).
For Enron, two datasets were generated automat-
ically. We collected name mentions which corre-
spond uniquely a names that is in the email ?Cc?
header line; then, to simulate a non-trivial matching
task, we eliminate the collected person name from
the email header. We also used a small dictionary of
16 common American nicknames to identify nick-
names that mapped uniquely to full person names
on the ?Cc? header line.
For each dataset, some examples were picked ran-
domly and set aside for learning and evaluation pur-
poses.
initials nicknames other
Cspace 11.3% 54.7% 34.0%
Sager-E - 10.2% 89.8%
Shapiro-R - 15.0% 85.0%
Table 3: Person Name Disambiguation Datasets
5.2 Results for person name disambiguation
5.2.1 Evaluation details
All of the methods applied generate a ranked list
of person nodes, and there is exactly one correct an-
swer per example.3 Figure 1 gives results4 for two
of the datasets as a function of recall at rank k, up
to rank 10. Table 4 shows the mean average preci-
sion (MAP) of the ranked lists as well as accuracy,
which we define as the percentage of correct answers
at rank 1 (i.e., precision at rank 1.)
5.2.2 Baseline method
To our knowledge, there are no previously re-
ported experiments for this task on email data. As a
baseline, we apply a reasonably sophisticated string
matching method (Cohen et al, 2003). Each name
mention in question is matched against all of the per-
son names in the corpus. The similarity score be-
tween the name term and a person name is calculated
as the maximal Jaro similarity score (Cohen et al,
2003) between the term and any single token of the
personal name (ranging between 0 to 1). In addition,
we incorporate a nickname dictionary5, such that if
the name term is a known nickname of a name, the
similarity score of that pair is set to 1.
The results are shown in Figure 1 and Table 4. As
can be seen, the baseline approach is substantially
less effective for the more informal Cspace dataset.
Recall that the Cspace corpus includes many cases
such as initials, and also nicknames that have no
literal resemblance to the person?s name (section
3If a ranking contains a block of items with the same score,
a node?s rank is counted as the average rank of the ?block?.
4Results refer to test examples only.
5The same dictionary that was used for dataset generation.
5
5.1.2), which are not handled well by the string sim-
ilarity approach. For the Enron datasets, the base-
line approach perfoms generally better (Table 4). In
all the corpora there are many ambiguous instances,
e.g., common names like ?Dave? or ?Andy? that
match many people with equal strength.
5.2.3 Graph walk methods
We perform two variants of graph walk, corre-
sponding to different methods of forming the query
distribution Vq. Unless otherwise stated, we will use
a uniform weighting of labels?i.e., ?`,T = 1/ST ;
? = 1/2; and a walk of length 2.
In the first variant, we concentrate all the prob-
ability in the query distribution on the name term.
The column labeled term gives the results of the
graph walk from this probability vector. Intuitively,
using this variant, the name term propagates its
weight to the files in which it appears. Then, weight
is propagated to person nodes which co-occur fre-
quently with these files. Note that in our graph
scheme there is a direct path between terms to per-
son names, so that they recieve weight as well.
As can be seen in the results, this leads to very
effective performance: e.g., it leads to 61.3% vs.
41.3% accuracy for the baseline approach on the
CSpace dataset. However, it does not handle am-
biguous terms as well as one would like, as the query
does not include any information of the context in
which the name occurred: the top-ranked answer for
ambiguous name terms (e.g., ?Dave?) will always
be the same person. To solve this problem, we also
used a file+term walk, in which the query Vq gives
equal weight to the name term node and the file in
which it appears.
We found that adding the file node to Vq provides
useful context for ambiguous instances?e.g., the
correct ?David? would in general be ranked higher
than other persons with this same name. On the
other hand, though, adding the file node reduces
the the contribution of the term node. Although the
MAP and accuracy are decreased, file+term has bet-
ter performance than term at higher recall levels, as
can be seen in Figure 1.
5.2.4 Reranking the output of a walk
We now examine reranking as a technique for im-
proving the results. After some preliminary exper-
imentation, we adopted the following types of fea-
tures f for a node x. The set of features are fairly
generic. Edge unigram features indicate, for each
edge label `, whether ` was used in reaching x from
Vq. Edge bigram features indicate, for each pair of
edge labels `1, `2, whether `1 and `2 were used (in
that order) in reaching x from Vq. Top edge bigram
features are similar but indicate if `1, `2 were used
in one of the two highest-scoring paths between Vq
and x (where the ?score? of a path is the product of
Pr(y `?? z) for all edges in the path.)
We believe that these features could all be com-
puted using dynamic programming methods. Cur-
rently, however, we compute features by using a
method we call path unfolding, which is simi-
lar to the back-propagation through time algorithm
(Haykin, 1994; Diligenti et al, 2005) used in train-
ing recurrent neural networks. Graph unfolding is
based on a backward breadth-first visit of the graph,
starting at the target node at time step k, and expand-
ing the unfolded paths by one layer per each time
step. This procedure is more expensive, but offers
more flexibility in choosing alternative features, and
was useful in determining an optimal feature set.
In addition, we used for this task some addi-
tional problem-specific features. One feature indi-
cates whether the set of paths leading to a node orig-
inate from one or two nodes in Vq. (We conjecture
that in the file+term walk, nodes are connected to
both the source term and file nodes are more rele-
vant comparing to nodes that are reached from the
file node or term node only.) We also form features
that indicate whether the given term is a nickname of
the person name, per the nicknames dictionary; and
whether the Jaro similarity score between the term
and the person name is above 0.8. This information
is similar to that used by the baseline method.
The results (for the test set, after training on the
train set) are shown in Table 4 and (for two represen-
tative cases) Figure 1. In each case the top 10 nodes
were reranked. Reranking substantially improves
performance, especially for the file+term walk. The
accuracy rate is higher than 75% across all datasets.
The features that were assigned the highest weights
by the re-ranker were the literal similarity features
and the source count feature.
6
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20
Cu
m
ul
at
ive
 R
at
e
CSPACE
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25
Cu
m
ul
at
ive
 R
at
e
Rank
SHAPIRO-R
baseline
term
term reranked
file+term
file+term re-ranked
Figure 1: Person name disambiguation results: Re-
call at rank k
6 Related Work
As noted above, the similarity measure we use is
based on graph-walk techniques which have been
adopted by many other researchers for several dif-
ferent tasks. In the information retrieval commu-
nity, infinite graph walks are prevalent for deter-
mining document centrality (e.g., (Page et al, 1998;
Diligenti et al, 2005; Kurland and Lee, 2005)). A
related venue of research is of spreading activa-
tion over semantic or association networks, where
the underlying idea is to propagate activation from
source nodes via weighted links through the network
(Berger et al, 2004; Salton and Buckley, 1988).
The idea of representing structured data as a
graph is widespread in the data mining community,
which is mostly concerned with relational or semi-
structured data. Recently, the idea of PageRank
MAP Accuracy
Cspace
Baseline 49.0 41.3
Graph - term 72.6 61.3
Graph - file+term 66.3 48.8
Reranking - term 85.6 72.5
Reranking - file+term 89.0 83.8
Sager-E
Baseline 67.5 39.2
Graph - term 82.8 66.7
Graph - file+term 61.7 41.2
Reranking - term 83.2 68.6
Reranking - file+term 88.9 80.4
Shapiro-R
Baseline 60.8 38.8
Graph - term 84.1 63.3
Graph - file+term 56.5 38.8
Reranking - term 87.9 65.3
Reranking - file+term 85.5 77.6
Table 4: Person Name Disambiguation Results
has been applied to keyword search in structured
databases (Balmin et al, 2004). Analysis of inter-
object relationships has been suggested for entity
disambiguation for entities in a graph (Kalashnikov
et al, 2005), where edges are unlabelled. It has been
suggested to model similarity between objects in re-
lational data in terms of structural-context similarity
(Jeh and Widom, 2002).
We propose the use of learned re-ranking schemes
to improve performance of a lazy graph walk.
Earlier authors have considered instead using hill-
climbing approaches to adjust the parameters of a
graph-walk (Diligenti et al, 2005). We have not
compared directly with such approaches; prelimi-
nary experiments suggest that the performance gain
of such methods is limited, due to their inability to
exploit the global features we used here6. Related
research explores random walks for semi supervised
learning (Zhu et al, 2003; Zhou et al, 2005).
The task of person disambiguation has been stud-
ied in the field of social networks (e.g., (Malin et
al., 2005)). In particular, it has been suggested to
perform name disambiguation in email using traf-
fic information, as derived from the email headers
(Diehl et al, 2006). Our approach differs in that it
allows integration of email content and a timeline in
addition to social network information in a unified
6For instance, re-ranking using a set of simple locally-
computable features only modestly improved performance of
the ?random? weight set for the CSpace threading task.
7
framework. In addition, we incorporate learning to
tune the system parameters automatically.
7 Conclusion
We have presented a scheme for representing a cor-
pus of email messages with a graph of typed entities,
and an extension of the traditional notions of docu-
ment similarity to documents embedded in a graph.
Using a boosting-based learning scheme to rerank
outputs based on graph-walk related, as well as other
domain-specific, features provides an additional per-
formance improvement. The final results are quite
strong: for the explored name disambiguation task,
the method yields MAP scores in the mid-to-upper
80?s. The person name identification task illustrates
a key advantage of our approach?that context can
be easily incorporated in entity disambiguation.
In future work, we plan to further explore the
scalability of the approach, and also ways of inte-
grating this approach with language-modeling ap-
proaches for document representation and retrieval.
An open question with regard to contextual (multi-
source) graph walk in this framework is whether it is
possible to further focus probability mass on nodes
that are reached from multiple source nodes. This
may prove beneficial for complex queries.
References
Andrey Balmin, Vagelis Hristidis, and Yannis Papakonstanti-
nou. 2004. ObjectRank: Authority-based keyword search in
databases. In VLDB.
Helmut Berger, Michael Dittenbach, and Dieter Merkl. 2004.
An adaptive information retrieval system. based on associa-
tive networks. In APCCM.
William W. Cohen, Robert E. Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial Intelli-
gence Research (JAIR), 10:243?270.
William W. Cohen, Pradeep Ravikumar, and Stephen Fienberg.
2003. A comparison of string distance metrics for name-
matching tasks. In IIWEB.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Kevyn Collins-Thompson and Jamie Callan. 2005. Query ex-
pansion using random walk models. In CIKM.
W. Bruce Croft and John Lafferty. 2003. Language Modeling
for Information Retrieval. Springer.
Christopher P. Diehl, Lise Getoor, and Galileo Namata. 2006.
Name reference resolution in organizational email archives.
In SIAM.
Michelangelo Diligenti, Marco Gori, and Marco Maggini.
2005. Learning web page scores by error back-propagation.
In IJCAI.
Simon Haykin. 1994. Neural Networks. Macmillan College
Publishing Company.
Glen Jeh and Jennifer Widom. 2002. Simrank: A measure of
structural-context similarity. In SIGKDD.
Dmitri Kalashnikov, Sharad Mehrotra, and Zhaoqi Chen. 2005.
Exploiting relationship for domain independent data clean-
ing. In SIAM.
Brown Klimt and Yiming Yang. 2004. The enron corpus: A
new dataset for email classification research. In ECML.
Oren Kurland and Lillian Lee. 2005. Pagerank without hyper-
links: Structural re-ranking using links induced by language
models. In SIGIR.
Bradely Malin, Edoardo M. Airoldi, and Kathleen M. Carley.
2005. A social network analysis model for name disam-
biguation in lists. Journal of Computational and Mathemat-
ical Organization Theory, 11(2).
Einat Minkov, Richard Wang, and William Cohen. 2005. Ex-
tracting personal names from emails: Applying named entity
recognition to informal text. In HLT-EMNLP.
Zaiqing Nie, Yuanzhi Zhang, Ji-Rong Wen, and Wei-Ying Ma.
2005. Object-level ranking: Bringing order to web objects.
In WWW.
Larry Page, Sergey Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the web. In
Technical Report, Computer Science department, Stanford
University.
Gerard Salton and Chris Buckley. 1988. On the use of spread-
ing activation methods in automatic information retrieval. In
SIGIR.
Robert E. Schapire and Yoram Singer. 1999. Improved boost-
ing algorithms using confidence-rated predictions. Machine
Learning, 37(3):297?336.
Kristina Toutanova, Christopher D. Manning, and Andrew Y.
Ng. 2004. Learning random walk models for inducing word
dependency distributions. In ICML.
Wensi Xi, Edward Allan Fox, Weiguo Patrick Fan, Benyu
Zhang, Zheng Chen, Jun Yan, and Dong Zhuang. 2005.
Simfusion: Measuring similarity using unified relationship
matrix. In SIGIR.
Dengyong Zhou, Bernhard Scholkopf, and Thomas Hofmann.
2005. Semi-supervised learning on directed graphs. In
NIPS.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003.
Semi-supervised learning using gaussian fields and harmonic
functions. In ICML.
8
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151?161,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Recursive Autoencoders
for Predicting Sentiment Distributions
Richard Socher Jeffrey Pennington? Eric H. Huang Andrew Y. Ng Christopher D. Manning
Computer Science Department, Stanford University, Stanford, CA 94305, USA
?SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA
richard@socher.org {jpennin,ehhuang,ang,manning}@stanford.edu ang@cs.stanford.edu
Abstract
We introduce a novel machine learning frame-
work based on recursive autoencoders for
sentence-level prediction of sentiment label
distributions. Our method learns vector space
representations for multi-word phrases. In
sentiment prediction tasks these represen-
tations outperform other state-of-the-art ap-
proaches on commonly used datasets, such as
movie reviews, without using any pre-defined
sentiment lexica or polarity shifting rules. We
also evaluate the model?s ability to predict
sentiment distributions on a new dataset based
on confessions from the experience project.
The dataset consists of personal user stories
annotated with multiple labels which, when
aggregated, form a multinomial distribution
that captures emotional reactions. Our al-
gorithm can more accurately predict distri-
butions over such labels compared to several
competitive baselines.
1 Introduction
The ability to identify sentiments about personal ex-
periences, products, movies etc. is crucial to un-
derstand user generated content in social networks,
blogs or product reviews. Detecting sentiment in
these data is a challenging task which has recently
spawned a lot of interest (Pang and Lee, 2008).
Current baseline methods often use bag-of-words
representations which cannot properly capture more
complex linguistic phenomena in sentiment analy-
sis (Pang et al, 2002). For instance, while the two
phrases ?white blood cells destroying an infection?
and ?an infection destroying white blood cells? have
the same bag-of-words representation, the former is
a positive reaction while the later is very negative.
More advanced methods such as (Nakagawa et al,
IndicesWords
Semantic Representations
Recursive Autoencoder
i         walked      into         a        parked     car
Sorry, Hugs      You Rock       Teehee    I Understand    Wow, Just Wow
Predicted Sentiment Distribution
Figure 1: Illustration of our recursive autoencoder archi-
tecture which learns semantic vector representations of
phrases. Word indices (orange) are first mapped into a
semantic vector space (blue). Then they are recursively
merged by the same autoencoder network into a fixed
length sentence representation. The vectors at each node
are used as features to predict a distribution over senti-
ment labels.
2010) that can capture such phenomena use many
manually constructed resources (sentiment lexica,
parsers, polarity-shifting rules). This limits the ap-
plicability of these methods to a broader range of
tasks and languages. Lastly, almost all previous
work is based on single, positive/negative categories
or scales such as star ratings. Examples are movie
reviews (Pang and Lee, 2005), opinions (Wiebe et
al., 2005), customer reviews (Ding et al, 2008) or
multiple aspects of restaurants (Snyder and Barzilay,
2007). Such a one-dimensional scale does not accu-
rately reflect the complexity of human emotions and
sentiments.
In this work, we seek to address three issues. (i)
Instead of using a bag-of-words representation, our
model exploits hierarchical structure and uses com-
positional semantics to understand sentiment. (ii)
Our system can be trained both on unlabeled do-
main data and on supervised sentiment data and does
not require any language-specific sentiment lexica,
151
parsers, etc. (iii) Rather than limiting sentiment to
a positive/negative scale, we predict a multidimen-
sional distribution over several complex, intercon-
nected sentiments.
We introduce an approach based on semi-
supervised, recursive autoencoders (RAE) which
use as input continuous word vectors. Fig. 1 shows
an illustration of the model which learns vector rep-
resentations of phrases and full sentences as well as
their hierarchical structure from unsupervised text.
We extend our model to also learn a distribution over
sentiment labels at each node of the hierarchy.
We evaluate our approach on several standard
datasets where we achieve state-of-the art perfor-
mance. Furthermore, we show results on the re-
cently introduced experience project (EP) dataset
(Potts, 2010) that captures a broader spectrum of
human sentiments and emotions. The dataset con-
sists of very personal confessions anonymously
made by people on the experience project website
www.experienceproject.com. Confessions are la-
beled with a set of five reactions by other users. Re-
action labels are you rock (expressing approvement),
tehee (amusement), I understand, Sorry, hugs and
Wow, just wow (displaying shock). For evaluation on
this dataset we predict both the label with the most
votes as well as the full distribution over the senti-
ment categories. On both tasks our model outper-
forms competitive baselines. A set of over 31,000
confessions as well as the code of our model are
available at www.socher.org.
After describing the model in detail, we evalu-
ate it qualitatively by analyzing the learned n-gram
vector representations and compare quantitatively
against other methods on standard datasets and the
EP dataset.
2 Semi-Supervised Recursive
Autoencoders
Our model aims to find vector representations for
variable-sized phrases in either unsupervised or
semi-supervised training regimes. These representa-
tions can then be used for subsequent tasks. We first
describe neural word representations and then pro-
ceed to review a related recursive model based on
autoencoders, introduce our recursive autoencoder
(RAE) and describe how it can be modified to jointly
learn phrase representations, phrase structure and
sentiment distributions.
2.1 Neural Word Representations
We represent words as continuous vectors of param-
eters. We explore two settings. In the first setting
we simply initialize each word vector x ? Rn by
sampling it from a zero mean Gaussian distribution:
x ? N (0, ?2). These word vectors are then stacked
into a word embedding matrix L ? Rn?|V |, where
|V | is the size of the vocabulary. This initialization
works well in supervised settings where a network
can subsequently modify these vectors to capture
certain label distributions.
In the second setting, we pre-train the word vec-
tors with an unsupervised neural language model
(Bengio et al, 2003; Collobert and Weston, 2008).
These models jointly learn an embedding of words
into a vector space and use these vectors to predict
how likely a word occurs given its context. After
learning via gradient ascent the word vectors cap-
ture syntactic and semantic information from their
co-occurrence statistics.
In both cases we can use the resulting matrix of
word vectors L for subsequent tasks as follows. As-
sume we are given a sentence as an ordered list of
m words. Each word has an associated vocabulary
index k into the embedding matrix which we use to
retrieve the word?s vector representation. Mathemat-
ically, this look-up operation can be seen as a sim-
ple projection layer where we use a binary vector b
which is zero in all positions except at the kth index,
xi = Lbk ? Rn. (1)
In the remainder of this paper, we represent a sen-
tence (or any n-gram) as an ordered list of these
vectors (x1, . . . , xm). This word representation is
better suited to autoencoders than the binary number
representations used in previous related autoencoder
models such as the recursive autoassociative mem-
ory (RAAM) model (Pollack, 1990; Voegtlin and
Dominey, 2005) or recurrent neural networks (El-
man, 1991) since sigmoid units are inherently con-
tinuous. Pollack circumvented this problem by hav-
ing vocabularies with only a handful of words and
by manually defining a threshold to binarize the re-
sulting vectors.
152
x1 x3 x4x2
y1=f(W(1)[x3;x4] + b)
y2=f(W(1)[x2;y1] + b)
y3=f(W(1)[x1;y2] + b)
Figure 2: Illustration of an application of a recursive au-
toencoder to a binary tree. The nodes which are not filled
are only used to compute reconstruction errors. A stan-
dard autoencoder (in box) is re-used at each node of the
tree.
2.2 Traditional Recursive Autoencoders
The goal of autoencoders is to learn a representation
of their inputs. In this section we describe how to
obtain a reduced dimensional vector representation
for sentences.
In the past autoencoders have only been used in
setting where the tree structure was given a-priori.
We review this setting before continuing with our
model which does not require a given tree structure.
Fig. 2 shows an instance of a recursive autoencoder
(RAE) applied to a given tree. Assume we are given
a list of word vectors x = (x1, . . . , xm) as described
in the previous section as well as a binary tree struc-
ture for this input in the form of branching triplets
of parents with children: (p ? c1c2). Each child
can be either an input word vector xi or a nontermi-
nal node in the tree. For the example in Fig. 2, we
have the following triplets: ((y1 ? x3x4), (y2 ?
x2y1), (y1 ? x1y2)). In order to be able to apply
the same neural network to each pair of children, the
hidden representations yi have to have the same di-
mensionality as the xi?s.
Given this tree structure, we can now compute the
parent representations. The first parent vector y1 is
computed from the children (c1, c2) = (x3, x4):
p = f(W (1)[c1; c2] + b(1)), (2)
where we multiplied a matrix of parameters W (1) ?
Rn?2n by the concatenation of the two children.
After adding a bias term we applied an element-
wise activation function such as tanh to the result-
ing vector. One way of assessing how well this n-
dimensional vector represents its children is to try to
reconstruct the children in a reconstruction layer:
[
c?1; c?2
]
= W (2)p+ b(2). (3)
During training, the goal is to minimize the recon-
struction errors of this input pair. For each pair, we
compute the Euclidean distance between the original
input and its reconstruction:
Erec([c1; c2]) =
1
2
????[c1; c2]?
[
c?1; c?2
]????2 . (4)
This model of a standard autoencoder is boxed in
Fig. 2. Now that we have defined how an autoen-
coder can be used to compute an n-dimensional vec-
tor representation (p) of two n-dimensional children
(c1, c2), we can describe how such a network can be
used for the rest of the tree.
Essentially, the same steps repeat. Now that y1
is given, we can use Eq. 2 to compute y2 by setting
the children to be (c1, c2) = (x2, y1). Again, after
computing the intermediate parent vector y2, we can
assess how well this vector capture the content of
the children by computing the reconstruction error
as in Eq. 4. The process repeat until the full tree
is constructed and we have a reconstruction error at
each nonterminal node. This model is similar to the
RAAM model (Pollack, 1990) which also requires a
fixed tree structure.
2.3 Unsupervised Recursive Autoencoder for
Structure Prediction
Now, assume there is no tree structure given for
the input vectors in x. The goal of our structure-
prediction RAE is to minimize the reconstruction er-
ror of all vector pairs of children in a tree. We de-
fine A(x) as the set of all possible trees that can be
built from an input sentence x. Further, let T (y) be
a function that returns the triplets of a tree indexed
by s of all the non-terminal nodes in a tree. Using
the reconstruction error of Eq. 4, we compute
RAE?(x) = argmin
y?A(x)
?
s?T (y)
Erec([c1; c2]s) (5)
We now describe a greedy approximation that con-
structs such a tree.
153
Greedy Unsupervised RAE. For a sentence with
m words, we apply the autoencoder recursively. It
takes the first pair of neighboring vectors, defines
them as potential children of a phrase (c1; c2) =
(x1;x2), concatenates them and gives them as in-
put to the autoencoder. For each word pair, we save
the potential parent node p and the resulting recon-
struction error.
After computing the score for the first pair, the
network is shifted by one position and takes as input
vectors (c1, c2) = (x2, x3) and again computes a po-
tential parent node and a score. This process repeats
until it hits the last pair of words in the sentence:
(c1, c2) = (xm?1, xm). Next, it selects the pair
which had the lowest reconstruction error (Erec) and
its parent representation p will represent this phrase
and replace both children in the sentence word list.
For instance, consider the sequence (x1, x2, x3, x4)
and assume the lowestErec was obtained by the pair
(x3, x4). After the first pass, the new sequence then
consists of (x1, x2, p(3,4)). The process repeats and
treats the new vector p(3,4) like any other input vec-
tor. For instance, subsequent states could be either:
(x1, p(2,(3,4))) or (p(1,2), p(3,4)). Both states would
then finish with a deterministic choice of collapsing
the remaining two states into one parent to obtain
(p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree
is then recovered by unfolding the collapsing deci-
sions.
The resulting tree structure captures as much of
the single-word information as possible (in order
to allow reconstructing the word vectors) but does
not necessarily follow standard syntactic constraints.
We also experimented with a method that finds bet-
ter solutions to Eq. 5 based on CKY-like beam
search algorithms (Socher et al, 2010; Socher et al,
2011) but the performance is similar and the greedy
version is much faster.
Weighted Reconstruction. One problem with
simply using the reconstruction error of both chil-
dren equally as describe in Eq. 4 is that each child
could represent a different number of previously
collapsed words and is hence of bigger importance
for the overall meaning reconstruction of the sen-
tence. For instance in the case of (x1, p(2,(3,4)))
one would like to give more importance to recon-
structing p than x1. We capture this desideratum
by adjusting the reconstruction error. Let n1, n2 be
the number of words underneath a current poten-
tial child, we re-define the reconstruction error to be
Erec([c1; c2]; ?) =
n1
n1 + n2
????c1 ? c?1
????2 + n2n1 + n2
????c2 ? c?2
????2 (6)
Length Normalization. One of the goals of
RAEs is to induce semantic vector representations
that allow us to compare n-grams of different
lengths. The RAE tries to lower reconstruction error
of not only the bigrams but also of nodes higher in
the tree. Unfortunately, since the RAE computes the
hidden representations it then tries to reconstruct, it
can just lower reconstruction error by making the
hidden layer very small in magnitude. To prevent
such undesirable behavior, we modify the hidden
layer such that the resulting parent representation al-
ways has length one, after computing p as in Eq. 2,
we simply set: p = p||p|| .
2.4 Semi-Supervised Recursive Autoencoders
So far, the RAE was completely unsupervised and
induced general representations that capture the se-
mantics of multi-word phrases.In this section, we
extend RAEs to a semi-supervised setting in order
to predict a sentence- or phrase-level target distribu-
tion t.1
One of the main advantages of the RAE is that
each node of the tree built by the RAE has associ-
ated with it a distributed vector representation (the
parent vector p) which could also be seen as fea-
tures describing that phrase. We can leverage this
representation by adding on top of each parent node
a simple softmax layer to predict class distributions:
d(p; ?) = softmax(W labelp). (7)
Assuming there are K labels, d ? RK is
a K-dimensional multinomial distribution and?
k=1 dk = 1. Fig. 3 shows such a semi-supervised
RAE unit. Let tk be the kth element of the multino-
mial target label distribution t for one entry. The
softmax layer?s outputs are interpreted as condi-
tional probabilities dk = p(k|[c1; c2]), hence the
cross-entropy error is
EcE(p, t; ?) = ?
K?
k=1
tk log dk(p; ?). (8)
1For the binary label classification case, the distribution is
of the form [1, 0] for class 1 and [0, 1] for class 2.
154
R e c o n s t r u c t i o n  e r r o r            C r o s s - e n t r o p y e r r o r
W(1)
W(2) W(l a be l )
Figure 3: Illustration of an RAE unit at a nonterminal tree
node. Red nodes show the supervised softmax layer for
label distribution prediction.
Using this cross-entropy error for the label and the
reconstruction error from Eq. 6, the final semi-
supervised RAE objective over (sentences,label)
pairs (x, t) in a corpus becomes
J = 1N
?
(x,t)
E(x, t; ?) + ?2 ||?||
2, (9)
where we have an error for each entry in the training
set that is the sum over the error at the nodes of the
tree that is constructed by the greedy RAE:
E(x, t; ?) =
?
s?T (RAE?(x))
E([c1; c2]s, ps, t, ?).
The error at each nonterminal node is the weighted
sum of reconstruction and cross-entropy errors,
E([c1; c2]s, ps, t, ?) =
?Erec([c1; c2]s; ?) + (1? ?)EcE(ps, t; ?).
The hyperparameter ? weighs reconstruction and
cross-entropy error. When minimizing the cross-
entropy error of this softmax layer, the error will
backpropagate and influence both the RAE param-
eters and the word representations. Initially, words
such as good and bad have very similar representa-
tions. This is also the case for Brown clusters and
other methods that use only cooccurrence statistics
in a small window around each word. When learn-
ing with positive/negative sentiment, the word em-
beddings get modified and capture less syntactic and
more sentiment information.
In order to predict the sentiment distribution of a
sentence with this model, we use the learned vector
representation of the top tree node and train a simple
logistic regression classifier.
3 Learning
Let ? = (W (1), b(1),W (2), b(1),W label, L) be the set
of our model parameters, then the gradient becomes:
?J
?? =
1
N
?
(x,t)
?E(x, t; ?)
?? + ??. (10)
To compute this gradient, we first greedily construct
all trees and then derivatives for these trees are com-
puted efficiently via backpropagation through struc-
ture (Goller and Ku?chler, 1996). Because the algo-
rithm is greedy and the derivatives of the supervised
cross-entropy error also modify the matrix W (1),
this objective is not necessarily continuous and a
step in the gradient descent direction may not nec-
essarily decrease the objective. However, we found
that L-BFGS run over the complete training data
(batch mode) to minimize the objective works well
in practice, and that convergence is smooth, with the
algorithm typically finding a good solution quickly.
4 Experiments
We first describe the new experience project (EP)
dataset, results of standard classification tasks on
this dataset and how to predict its sentiment label
distributions. We then show results on other com-
monly used datasets and conclude with an analysis
of the important parameters of the model.
In all experiments involving our model, we repre-
sent words using 100-dimensional word vectors. We
explore the two settings mentioned in Sec. 2.1. We
compare performance on standard datasets when us-
ing randomly initialized word vectors (random word
init.) or word vectors trained by the model of Col-
lobert and Weston (2008) and provided by Turian
et al (2010).2 These vectors were trained on an
unlabeled corpus of the English Wikipedia. Note
that alternatives such as Brown clusters are not suit-
able since they do not capture sentiment information
(good and bad are usually in the same cluster) and
cannot be modified via backpropagation.
2http://metaoptimize.com/projects/
wordreprs/
155
Corpus K Instances Distr.(+/-) Avg|W |
MPQA 2 10,624 0.31/0.69 3
MR 2 10,662 0.5/0.5 22
EP 5 31,675 .2/.2/.1/.4/.1 113
EP? 4 5 6,129 .2/.2/.1/.4/.1 129
Table 1: Statistics on the different datasets. K is the num-
ber of classes. Distr. is the distribution of the different
classes (in the case of 2, the positive/negative classes, for
EP the rounded distribution of total votes in each class).
|W | is the average number of words per instance. We use
EP? 4, a subset of entries with at least 4 votes.
4.1 EP Dataset: The Experience Project
The confessions section of the experience project
website3 lets people anonymously write short per-
sonal stories or ?confessions?. Once a story is on
the site, each user can give a single vote to one of
five label categories (with our interpretation):
1 Sorry, Hugs: User offers condolences to author.
2. You Rock: Indicating approval, congratulations.
3. Teehee: User found the anecdote amusing.
4. I Understand: Show of empathy.
5. Wow, Just Wow: Expression of surprise,shock.
The EP dataset has 31,676 confession entries, a to-
tal number of 74,859 votes for the 5 labels above, the
average number of votes per entry is 2.4 (with a vari-
ance of 33). For the five categories, the numbers of
votes are [14, 816; 13, 325; 10, 073; 30, 844; 5, 801].
Since an entry with less than 4 votes is not very well
identified, we train and test only on entries with at
least 4 total votes. There are 6,129 total such entries.
The distribution over total votes in the 5 classes
is similar: [0.22; 0.2; 0.11; 0.37; 0.1]. The average
length of entries is 129 words. Some entries con-
tain multiple sentences. In these cases, we average
the predicted label distributions from the sentences.
Table 1 shows statistics of this and other commonly
used sentiment datasets (which we compare on in
later experiments). Table 2 shows example entries
as well as gold and predicted label distributions as
described in the next sections.
Compared to other datasets, the EP dataset con-
tains a wider range of human emotions that goes far
beyond positive/negative product or movie reviews.
Each item is labeled with a multinomial distribu-
3http://www.experienceproject.com/
confessions.php
tion over interconnected response categories. This
is in contrast to most other datasets (including multi-
aspect rating) where several distinct aspects are rated
independently but on the same scale. The topics
range from generic happy statements, daily clumsi-
ness reports, love, loneliness, to relationship abuse
and suicidal notes. As is evident from the total num-
ber of label votes, the most common user reaction
is one of empathy and an ability to relate to the au-
thors experience. However, some stories describe
horrible scenarios that are not common and hence
receive more offers of condolence. In the following
sections we show some examples of stories with pre-
dicted and true distributions but refrain from listing
the most horrible experiences.
For all experiments on the EP dataset, we split the
data into train (49%), development (21%) and test
data (30%).
4.2 EP: Predicting the Label with Most Votes
The first task for our evaluation on the EP dataset is
to simply predict the single class that receives the
most votes. In order to compare our novel joint
phrase representation and classifier learning frame-
work to traditional methods, we use the following
baselines:
Random Since there are five classes, this gives 20%
accuracy.
Most Frequent Selecting the class which most fre-
quently has the most votes (the class I under-
stand).
Baseline 1: Binary BoW This baseline uses logis-
tic regression on binary bag-of-word represen-
tations that are 1 if a word is present and 0 oth-
erwise.
Baseline 2: Features This model is similar to tra-
ditional approaches to sentiment classification
in that it uses many hand-engineered resources.
We first used a spell-checker and Wordnet to
map words and their misspellings to synsets to
reduce the total number of words. We then re-
placed sentiment words with a sentiment cat-
egory identifier using the sentiment lexica of
the Harvard Inquirer (Stone, 1966) and LIWC
(Pennebaker et al, 2007). Lastly, we used tf-idf
weighting on the bag-of-word representations
and trained an SVM.156
KL Predicted&Gold V. Entry (Shortened if it ends with ...)
.03
.16 .16 .16 .33 .16
6 I reguarly shoplift. I got caught once and went to jail, but I?ve found that this was not a deterrent. I don?t buy
groceries, I don?t buy school supplies for my kids, I don?t buy gifts for my kids, we don?t pay for movies, and I
dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...
.03
.38 .04 .06 .35 .14
165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1
hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights.
i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict...
.05
.14 .28 .14 .28 .14
7 Hi there, Im a guy that loves a girl, the same old bloody story... I met her a while ago, while studying, she Is so
perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so
did I with her, she has been the first person, male or female that has ever made that bond with me,...
.07
.27 .18 .00 .45 .09
11 be kissing you right now. i should be wrapped in your arms in the dark, but instead i?ve ruined everything. i?ve
piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn?t feel because
i?ve never had you close enough. we?ve never touched, but i still feel as though a part of me is missing. ...
.05 23 Dear Love, I just want to say that I am looking for you. Tonight I felt the urge to write, and I am becoming more
and more frustrated that I have not found you yet. I?m also tired of spending so much heart on an old dream. ...
.05 5 I wish I knew somone to talk to here.
.06 24 I loved her but I screwed it up. Now she?s moved on. I?ll never have her again. I don?t know if I?ll ever stop
thinking about her.
.06 5 i am 13 years old and i hate my father he is alwas geting drunk and do?s not care about how it affects me or my
sisters i want to care but the truthis i dont care if he dies
.13 6 well i think hairy women are attractive
.35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard. I need it. It?ll make my soul
feel a bit better :)
.36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12
years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just
can?t meet men. (before you judge, no Im not terribly picky!) What is wrong with me?
.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy. Then the teacher found out
it was one of us and made us go two days without freetime. It might be a little late now, but sorry guys it was
me haha
.92 4 My paper is due in less than 24 hours and I?m still dancing round my room!
Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue,
left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The
5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher,
our model makes reasonable alternative label choices. Some entries are shortened.
Baseline 3: Word Vectors We can ignore the RAE
tree structure and only train softmax layers di-
rectly on the pre-trained words in order to influ-
ence the word vectors. This is followed by an
SVM trained on the average of the word vec-
tors.
We also experimented with latent Dirichlet aloca-
tion (Blei et al, 2003) but performance was very
low.
Table 3 shows the results for predicting the class
with the most votes. Even the approach that is based
on sentiment lexica and other resources is outper-
formed by our model by almost 3%, showing that
for tasks involving complex broad-range human sen-
timent, the often used sentiment lexica lack in cover-
age and traditional bag-of-words representations are
not powerful enough.
4.3 EP: Predicting Sentiment Distributions
We now turn to evaluating our distribution-
prediction approach. In both this and the previous
Method Accuracy
Random 20.0
Most Frequent 38.1
Baseline 1: Binary BoW 46.4
Baseline 2: Features 47.0
Baseline 3: Word Vectors 45.5
RAE (our method) 50.1
Table 3: Accuracy of predicting the class with most votes.
maximum label task, we backprop using the gold
multinomial distribution as a target. Since we max-
imize likelihood and because we want to predict a
distribution that is closest to the distribution of labels
that people would assign to a story, we evaluate us-
ing KL divergence: KL(g||p) = ?i gi log(gi/pi),
where g is the gold distribution and p is the predicted
one. We report the average KL divergence, where a
smaller value indicates better predictive power. To
get an idea of the values of KL divergence, predict-
157
Avg.Distr. BoW Features Word Vec. RAE0.6
0.7
0.8
0.83 0.81 0.72 0.73 0.70
Figure 4: Average KL-divergence between gold and pre-
dicted sentiment distributions (lower is better).
ing random distributions gives a an average of 1.2 in
KL divergence, predicting simply the average distri-
bution in the training data give 0.83. Fig. 4 shows
that our RAE-based model outperforms the other
baselines. Table 2 shows EP example entries with
predicted and gold distributions, as well as numbers
of votes.
4.4 Binary Polarity Classification
In order to compare our approach to other meth-
ods we also show results on commonly used sen-
timent datasets: movie reviews4 (MR) (Pang and
Lee, 2005) and opinions5 (MPQA) (Wiebe et al,
2005).We give statistical information on these and
the EP corpus in Table 1.
We compare to the state-of-the-art system of
(Nakagawa et al, 2010), a dependency tree based
classification method that uses CRFs with hidden
variables. We use the same training and testing regi-
men (10-fold cross validation) as well as their base-
lines: majority phrase voting using sentiment and
reversal lexica; rule-based reversal using a depen-
dency tree; Bag-of-Features and their full Tree-CRF
model. As shown in Table 4, our algorithm outper-
forms their approach on both datasets. For the movie
review (MR) data set, we do not use any hand-
designed lexica. An error analysis on the MPQA
dataset showed several cases of single words which
never occurred in the training set. Correctly classify-
ing these instances can only be the result of having
them in the original sentiment lexicon. Hence, for
the experiment on MPQA we added the same sen-
timent lexicon that (Nakagawa et al, 2010) used in
their system to our training set. This improved ac-
curacy from 86.0 to 86.4.Using the pre-trained word
vectors boosts performance by less than 1% com-
4www.cs.cornell.edu/people/pabo/
movie-review-data/
5www.cs.pitt.edu/mpqa/
Method MR MPQA
Voting with two lexica 63.1 81.7
Rule-based reversal on trees 62.9 82.8
Bag of features with reversal 76.4 84.1
Tree-CRF (Nakagawa et al?10) 77.3 86.1
RAE (random word init.) 76.8 85.7
RAE (our method) 77.7 86.4
Table 4: Accuracy of sentiment classification on movie
review polarity (MR) and the MPQA dataset.
0 0.2 0.4 0.6 0.8 10.83
0.84
0.85
0.86
0.87
Figure 5: Accuracy on the development split of the MR
polarity dataset for different weightings of reconstruction
error and supervised cross-entropy error: err = ?Erec+
(1? ?)EcE .
pared to randomly initialized word vectors (setting:
random word init). This shows that our method can
work well even in settings with little training data.
We visualize the semantic vectors that the recursive
autoencoder learns by listing n-grams that give the
highest probability for each polarity. Table 5 shows
such n-grams for different lengths when the RAE is
trained on the movie review polarity dataset.
On a 4-core machine, training time for the smaller
corpora such as the movie reviews takes around 3
hours and for the larger EP corpus around 12 hours
until convergence. Testing of hundreds of movie re-
views takes only a few seconds.
4.5 Reconstruction vs. Classification Error
In this experiment, we show how the hyperparame-
ter ? influences accuracy on the development set of
one of the cross-validation splits of the MR dataset.
This parameter essentially trade-off the supervised
and unsupervised parts of the objective. Fig. 5 shows
that a larger focus on the supervised objective is im-
portant but that a weight of ? = 0.2 for the recon-
struction error prevents overfitting and achieves the
highest performance.
158
n Most negative n-grams Most positive n-grams
1 bad; boring; dull; flat; pointless; tv; neither; pretentious; badly;
worst; lame; mediocre; lack; routine; loud; bore; barely; stupid;
tired; poorly; suffers; heavy;nor; choppy; superficial
touching; enjoyable; powerful; warm; moving; culture; flaws;
provides; engrossing; wonderful; beautiful; quiet; socio-political;
thoughtful; portrait; refreshingly; chilling; rich; beautifully; solid;
2 how bad; by bad; dull .; for bad; to bad; boring .; , dull; are bad;
that bad; boring ,; , flat; pointless .; badly by; on tv; so routine; lack
the; mediocre .; a generic; stupid ,; abysmally pathetic
the beautiful; moving,; thoughtful and; , inventive; solid and; a
beautiful; a beautifully; and hilarious; with dazzling; provides the;
provides.; and inventive; as powerful; moving and; a moving; a
powerful
3 . too bad; exactly how bad; and never dull; shot but dull; is more
boring; to the dull; dull, UNK; it is bad; or just plain; by turns
pretentious; manipulative and contrived; bag of stale; is a bad; the
whole mildly; contrived pastiche of; from this choppy; stale mate-
rial.
funny and touching; a small gem; with a moving; cuts, fast; , fine
music; smart and taut; culture into a; romantic , riveting; ... a solid;
beautifully acted .; , gradually reveals; with the chilling; cast of
solid; has a solid; spare yet audacious; ... a polished; both the
beauty;
5 boring than anything else.; a major waste ... generic; nothing i
hadn?t already; ,UNK plotting;superficial; problem ? no laughs.;
,just horribly mediocre .; dull, UNK feel.; there?s nothing exactly
wrong; movie is about a boring; essentially a collection of bits
reminded us that a feel-good; engrossing, seldom UNK,; between
realistic characters showing honest; a solid piece of journalistic;
easily the most thoughtful fictional; cute, funny, heartwarming;
with wry humor and genuine; engrossing and ultimately tragic.;
8 loud, silly, stupid and pointless.; dull, dumb and derivative horror
film.; UNK?s film, a boring, pretentious; this film biggest problem
? no laughs.; film in the series looks and feels tired; do draw easy
chuckles but lead nowhere.; stupid, infantile, redundant, sloppy
shot in rich , shadowy black-and-white , devils an escapist con-
fection that ?s pure entertainment .; , deeply absorbing piece that
works as a; ... one of the most ingenious and entertaining; film is a
riveting , brisk delight .; bringing richer meaning to the story ?s;
Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our model
predicts the most positive and most negative responses.
5 Related Work
5.1 Autoencoders and Deep Learning
Autoencoders are neural networks that learn a re-
duced dimensional representation of fixed-size in-
puts such as image patches or bag-of-word repre-
sentations of text documents. They can be used to
efficiently learn feature encodings which are useful
for classification. Recently, Mirowski et al (2010)
learn dynamic autoencoders for documents in a bag-
of-words format which, like ours, combine super-
vised and reconstruction objectives.
The idea of applying an autoencoder in a recursive
setting was introduced by Pollack (1990). Pollack?s
recursive auto-associative memories (RAAMs) are
similar to ours in that they are a connectionst, feed-
forward model. However, RAAMs learn vector
representations only for fixed recursive data struc-
tures, whereas our RAE builds this recursive data
structure. More recently, (Voegtlin and Dominey,
2005) introduced a linear modification to RAAMs
that is able to better generalize to novel combina-
tions of previously seen constituents. One of the
major shortcomings of previous applications of re-
cursive autoencoders to natural language sentences
was their binary word representation as discussed in
Sec. 2.1.
Recently, (Socher et al, 2010; Socher et al, 2011)
introduced a max-margin framework based on recur-
sive neural networks (RNNs) for labeled structure
prediction. Their models are applicable to natural
language and computer vision tasks such as parsing
or object detection. The current work is related in
that it uses a recursive deep learning model. How-
ever, RNNs require labeled tree structures and use a
supervised score at each node. Instead, RAEs learn
hierarchical structures that are trying to capture as
much of the the original word vectors as possible.
The learned structures are not necessarily syntacti-
cally plausible but can capture more of the semantic
content of the word vectors. Other recent deep learn-
ing methods for sentiment analysis include (Maas et
al., 2011).
5.2 Sentiment Analysis
Pang et al (2002) were one of the first to experiment
with sentiment classification. They show that sim-
ple bag-of-words approaches based on Naive Bayes,
MaxEnt models or SVMs are often insufficient for
predicting sentiment of documents even though they
work well for general topic-based document classi-
fication. Even adding specific negation words, bi-
grams or part-of-speech information to these mod-
els did not add significant improvements. Other
document-level sentiment work includes (Turney,
2002; Dave et al, 2003; Beineke et al, 2004; Pang
and Lee, 2004). For further references, see (Pang
and Lee, 2008).
Instead of document level sentiment classifica-
tion, (Wilson et al, 2005) analyze the contextual
polarity of phrases and incorporate many well de-
signed features including dependency trees. They
also show improvements by first distinguishing be-
159
tween neutral and polar sentences. Our model natu-
rally incorporates the recursive interaction between
context and polarity words in sentences in a unified
framework while simultaneously learning the neces-
sary features to make accurate predictions. Other ap-
proaches for sentence-level sentiment detection in-
clude (Yu and Hatzivassiloglou, 2003; Grefenstette
et al, 2004; Ikeda et al, 2008).
Most previous work is centered around a given
sentiment lexicon or building one via heuristics
(Kim and Hovy, 2007; Esuli and Sebastiani, 2007),
manual annotation (Das and Chen, 2001) or machine
learning techniques (Turney, 2002). In contrast, we
do not require an initial or constructed sentiment lex-
icon of positive and negative words. In fact, when
training our approach on documents or sentences, it
jointly learns such lexica for both single words and
n-grams (see Table 5). (Mao and Lebanon, 2007)
propose isotonic conditional random fields and dif-
ferentiate between local, sentence-level and global,
document-level sentiment.
The work of (Polanyi and Zaenen, 2006; Choi and
Cardie, 2008) focuses on manually constructing sev-
eral lexica and rules for both polar words and re-
lated content-word negators, such as ?prevent can-
cer?, where prevent reverses the negative polarity of
cancer. Like our approach they capture composi-
tional semantics. However, our model does so with-
out manually constructing any rules or lexica.
Recently, (Velikovich et al, 2010) showed how to
use a seed lexicon and a graph propagation frame-
work to learn a larger sentiment lexicon that also in-
cludes polar multi-word phrases such as ?once in a
life time?. While our method can also learn multi-
word phrases it does not require a seed set or a large
web graph. (Nakagawa et al, 2010) introduced an
approach based on CRFs with hidden variables with
very good performance. We compare to their state-
of-the-art system. We outperform them on the stan-
dard corpora that we tested on without requiring
external systems such as POS taggers, dependency
parsers and sentiment lexica. Our approach jointly
learns the necessary features and tree structure.
In multi-aspect rating (Snyder and Barzilay, 2007)
one finds several distinct aspects such as food or ser-
vice in a restaurant and then rates them on a fixed
linear scale such as 1-5 stars, where all aspects could
obtain just 1 star or all aspects could obtain 5 stars
independently. In contrast, in our method a single
aspect (a complex reaction to a human experience)
is predicted not in terms of a fixed scale but in terms
of a multinomial distribution over several intercon-
nected, sometimes mutually exclusive emotions. A
single story cannot simultaneously obtain a strong
reaction in different emotional responses (by virtue
of having to sum to one).
6 Conclusion
We presented a novel algorithm that can accurately
predict sentence-level sentiment distributions. With-
out using any hand-engineered resources such as
sentiment lexica, parsers or sentiment shifting rules,
our model achieves state-of-the-art performance on
commonly used sentiment datasets. Furthermore,
we introduce a new dataset that contains distribu-
tions over a broad range of human emotions. Our
evaluation shows that our model can more accu-
rately predict these distributions than other models.
Acknowledgments
We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of DARPA, AFRL, or
the US government. This work was also supported in part
by the DARPA Deep Learning program under contract
number FA8650-10-C-7020.
We thank Chris Potts for help with the EP data set, Ray-
mond Hsu, Bozhi See, and Alan Wu for letting us use
their system as a baseline and Jiquan Ngiam, Quoc Le,
Gabor Angeli and Andrew Maas for their feedback.
References
P. Beineke, T. Hastie, C. D. Manning, and
S. Vaithyanathan. 2004. Exploring sentiment
summarization. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning Re-
search., 3:993?1022.
160
Y. Choi and C. Cardie. 2008. Learning with composi-
tional semantics as structural inference for subsenten-
tial sentiment analysis. In EMNLP.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
S. Das and M. Chen. 2001. Yahoo! for Amazon: Ex-
tracting market sentiment from stock message boards.
In Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519?528.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings of
the Conference on Web Search and Web Data Mining
(WSDM).
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3):195?225.
A. Esuli and F. Sebastiani. 2007. Pageranking word-
net synsets: An application to opinion mining. In
Proceedings of the Association for Computational Lin-
guistics (ACL).
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of Recherche d?Information Assiste?e par Ordinateur
(RIAO).
D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura.
2008. Learning to shift the polarity of words for senti-
ment classification. In IJCNLP.
S. Kim and E. Hovy. 2007. Crystal: Analyzing predic-
tive opinions on the web. In EMNLP-CoNLL.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,
and C. Potts. 2011. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of ACL.
Y. Mao and G. Lebanon. 2007. Isotonic Conditional
Random Fields and Local Sentiment Flow. In NIPS.
P. Mirowski, M. Ranzato, and Y. LeCun. 2010. Dynamic
auto-encoders for semantic indexing. In Proceedings
of the NIPS 2010 Workshop on Deep Learning.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115?124.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In EMNLP.
J. W. Pennebaker, R.J. Booth, and M. E. Francis. 2007.
Linguistic inquiry and word count: Liwc2007 opera-
tors manual. University of Texas.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46:77?105, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the Good Grief algorithm. In HLT-NAACL.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.
2011. Parsing Natural Scenes and Natural Language
with Recursive Neural Networks. In ICML.
P. J. Stone. 1966. The General Inquirer: A Computer
Approach to Content Analysis. The MIT Press.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384?394.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of
reviews. In ACL.
L. Velikovich, S. Blair-Goldensohn, K. Hannan, and
R. McDonald. 2010. The viability of web-derived po-
larity lexicons. In NAACL, HLT.
T. Voegtlin and P. Dominey. 2005. Linear Recursive Dis-
tributed Representations. Neural Networks, 18(7).
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT/EMNLP.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
EMNLP.
161
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1201?1211, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Semantic Compositionality through Recursive Matrix-Vector Spaces
Richard Socher Brody Huval Christopher D. Manning Andrew Y. Ng
richard@socher.org, {brodyh,manning,ang}@stanford.edu
Computer Science Department, Stanford University
Abstract
Single-word vector space models have been
very successful at learning lexical informa-
tion. However, they cannot capture the com-
positional meaning of longer phrases, prevent-
ing them from a deeper understanding of lan-
guage. We introduce a recursive neural net-
work (RNN) model that learns compositional
vector representations for phrases and sen-
tences of arbitrary syntactic type and length.
Our model assigns a vector and a matrix to ev-
ery node in a parse tree: the vector captures
the inherent meaning of the constituent, while
the matrix captures how it changes the mean-
ing of neighboring words or phrases. This
matrix-vector RNN can learn the meaning of
operators in propositional logic and natural
language. The model obtains state of the art
performance on three different experiments:
predicting fine-grained sentiment distributions
of adverb-adjective pairs; classifying senti-
ment labels of movie reviews and classifying
semantic relationships such as cause-effect or
topic-message between nouns using the syn-
tactic path between them.
1 Introduction
Semantic word vector spaces are at the core of many
useful natural language applications such as search
query expansions (Jones et al 2006), fact extrac-
tion for information retrieval (Pas?ca et al 2006)
and automatic annotation of text with disambiguated
Wikipedia links (Ratinov et al 2011), among many
others (Turney and Pantel, 2010). In these mod-
els the meaning of a word is encoded as a vector
computed from co-occurrence statistics of a word
and its neighboring words. Such vectors have been
shown to correlate well with human judgments of
word similarity (Griffiths et al 2007).
?      very                        good                           movie          ...       (  a  ,  A  )                (  b  ,  B  )                     (  c  ,  C  )  
Recursive Matrix-Vector Model
f(Ba, Ab)=
 Ba=                        Ab=
- vector
- matrix...
?
Figure 1: A recursive neural network which learns se-
mantic vector representations of phrases in a tree struc-
ture. Each word and phrase is represented by a vector
and a matrix, e.g., very = (a,A). The matrix is applied
to neighboring vectors. The same function is repeated to
combine the phrase very good with movie.
Despite their success, single word vector models
are severely limited since they do not capture com-
positionality, the important quality of natural lan-
guage that allows speakers to determine the meaning
of a longer expression based on the meanings of its
words and the rules used to combine them (Frege,
1892). This prevents them from gaining a deeper
understanding of the semantics of longer phrases or
sentences. Recently, there has been much progress
in capturing compositionality in vector spaces, e.g.,
(Mitchell and Lapata, 2010; Baroni and Zamparelli,
2010; Zanzotto et al 2010; Yessenalina and Cardie,
2011; Socher et al 2011c) (see related work). We
extend these approaches with a more general and
powerful model of semantic composition.
We present a novel recursive neural network
model for semantic compositionality. In our context,
compositionality is the ability to learn compositional
vector representations for various types of phrases
and sentences of arbitrary length. Fig. 1 shows an
illustration of the model in which each constituent
(a word or longer phrase) has a matrix-vector (MV)
1201
representation. The vector captures the meaning of
that constituent. The matrix captures how it modifies
the meaning of the other word that it combines with.
A representation for a longer phrase is computed
bottom-up by recursively combining the words ac-
cording to the syntactic structure of a parse tree.
Since the model uses the MV representation with a
neural network as the final merging function, we call
our model a matrix-vector recursive neural network
(MV-RNN).
We show that the ability to capture semantic com-
positionality in a syntactically plausible way trans-
lates into state of the art performance on various
tasks. The first experiment demonstrates that our
model can learn fine-grained semantic composition-
ality. The task is to predict a sentiment distribution
over movie reviews of adverb-adjective pairs such as
unbelievably sad or really awesome. The MV-RNN
is the only model that is able to properly negate sen-
timent when adjectives are combined with not. The
MV-RNN outperforms previous state of the art mod-
els on full sentence sentiment prediction of movie
reviews. The last experiment shows that the MV-
RNN can also be used to find relationships between
words using the learned phrase vectors. The rela-
tionship between words is recursively constructed
and composed by words of arbitrary type in the
variable length syntactic path between them. On
the associated task of classifying relationships be-
tween nouns in arbitrary positions of a sentence the
model outperforms all previous approaches on the
SemEval-2010 Task 8 competition (Hendrickx et al
2010). It outperforms all but one of the previous ap-
proaches without using any hand-designed semantic
resources such as WordNet or FrameNet. By adding
WordNet hypernyms, POS and NER tags our model
outperforms the state of the art that uses significantly
more resources. The code for our model is available
at www.socher.org.
2 MV-RNN: A Recursive Matrix-Vector
Model
The dominant approach for building representations
of multi-word units from single word vector repre-
sentations has been to form a linear combination of
the single word representations, such as a sum or
weighted average. This happens in information re-
trieval and in various text similarity functions based
on lexical similarity. These approaches can work
well when the meaning of a text is literally ?the sum
of its parts?, but fails when words function as oper-
ators that modify the meaning of another word: the
meaning of ?extremely strong? cannot be captured
as the sum of word representations for ?extremely?
and ?strong.?
The model of Socher et al(2011c) provided a
new possibility for moving beyond a linear combi-
nation, through use of a matrix W that multiplied
the word vectors (a, b), and a nonlinearity function
g (such as a sigmoid or tanh). They compute the
parent vector p that describes both words as
p = g
(
W
[
a
b
])
(1)
and apply this function recursively inside a binarized
parse tree so that it can compute vectors for multi-
word sequences. Even though the nonlinearity al-
lows to express a wider range of functions, it is al-
most certainly too much to expect a single fixed W
matrix to be able to capture the meaning combina-
tion effects of all natural language operators. After
all, inside the function g, we have the same linear
transformation for all possible pairs of word vectors.
Recent work has started to capture the behavior
of natural language operators inside semantic vec-
tor spaces by modeling them as matrices, which
would allow a matrix for ?extremely? to appropri-
ately modify vectors for ?smelly? or ?strong? (Ba-
roni and Zamparelli, 2010; Zanzotto et al 2010).
These approaches are along the right lines but so
far have been restricted to capture linear functions
of pairs of words whereas we would like nonlinear
functions to compute compositional meaning repre-
sentations for multi-word phrases or full sentences.
The MV-RNN combines the strengths of both of
these ideas by (i) assigning a vector and a matrix to
every word and (ii) learning an input-specific, non-
linear, compositional function for computing vector
and matrix representations for multi-word sequences
of any syntactic type. Assigning vector-matrix rep-
resentations to all words instead of only to words of
one part of speech category allows for greater flex-
ibility which benefits performance. If a word lacks
operator semantics, its matrix can be an identity ma-
trix. However, if a word acts mainly as an operator,
1202
such as ?extremely?, its vector can become close to
zero, while its matrix gains a clear operator mean-
ing, here magnifying the meaning of the modified
word in both positive and negative directions.
In this section we describe the initial word rep-
resentations, the details of combining two words as
well as the multi-word extensions. This is followed
by an explanation of our training procedure.
2.1 Matrix-Vector Neural Word Representation
We represent a word as both a continuous vector
and a matrix of parameters. We initialize all word
vectors x ? Rn with pre-trained 50-dimensional
word vectors from the unsupervised model of Col-
lobert and Weston (2008). Using Wikipedia text,
their model learns word vectors by predicting how
likely it is for each word to occur in its context. Sim-
ilar to other local co-occurrence based vector space
models, the resulting word vectors capture syntactic
and semantic information. Every word is also asso-
ciated with a matrix X . In all experiments, we ini-
tialize matrices as X = I+ , i.e., the identity plus a
small amount of Gaussian noise. If the vectors have
dimensionality n, then each word?s matrix has di-
mensionality X ? Rn?n. While the initialization is
random, the vectors and matrices will subsequently
be modified to enable a sequence of words to com-
pose a vector that can predict a distribution over se-
mantic labels. Henceforth, we represent any phrase
or sentence of length m as an ordered list of vector-
matrix pairs ((a,A), . . . , (m,M)), where each pair
is retrieved based on the word at that position.
2.2 Composition Models for Two Words
We first review composition functions for two
words. In order to compute a parent vector p from
two consecutive words and their respective vectors
a and b, Mitchell and Lapata (2010) give as their
most general function: p = f(a, b, R,K),where R
is the a-priori known syntactic relation and K is
background knowledge.
There are many possible functions f . For our
models, there is a constraint on p which is that it
has the same dimensionality as each of the input
vectors. This way, we can compare p easily with
its children and p can be the input to a composition
with another word. The latter is a requirement that
will become clear in the next section. This excludes
tensor products which were outperformed by sim-
pler weighted addition and multiplication methods
in (Mitchell and Lapata, 2010).
We will explore methods that do not require
any manually designed semantic resources as back-
ground knowledge K. No explicit knowledge about
the type of relation R is used. Instead we want the
model to capture this implicitly via the learned ma-
trices. We propose the following combination func-
tion which is input dependent:
p = fA,B(a, b) = f(Ba,Ab) = g
(
W
[
Ba
Ab
])
,
(2)
whereA,B are matrices for single words, the global
W ? Rn?2n is a matrix that maps both transformed
words back into the same n-dimensional space. The
element-wise function g could be simply the identity
function but we use instead a nonlinearity such as
the sigmoid or hyperbolic tangent tanh. Such a non-
linearity will allow us to approximate a wider range
of functions beyond purely linear functions. We can
also add a bias term before applying g but omit this
for clarity. Rewriting the two transformed vectors as
one vector z, we get p = g(Wz) which is a single
layer neural network. In this model, the word ma-
trices can capture compositional effects specific to
each word, whereas W captures a general composi-
tion function.
This function builds upon and generalizes several
recent models in the literature. The most related
work is that of (Mitchell and Lapata, 2010; Zan-
zotto et al 2010) who introduced and explored the
composition function p = Ba + Ab for word pairs.
This model is a special case of Eq. 2 when we set
W = [II] (i.e. two concatenated identity matri-
ces) and g(x) = x (the identity function). Baroni
and Zamparelli (2010) computed the parent vector
of adjective-noun pairs by p = Ab, where A is an
adjective matrix and b is a vector for a noun. This
cannot capture nouns modifying other nouns, e.g.,
disk drive. This model too is a special case of the
above model with B = 0n?n. Lastly, the models of
(Socher et al 2011b; Socher et al 2011c; Socher et
al., 2011a) as described above are also special cases
with bothA andB set to the identity matrix. We will
compare to these special cases in our experiments.
1203
???????????????   (a , A)     (b , B)    (c , C)  
Matrix-Vector Recursive Neural Network
(p1 , P1)
( p2, P2  ) p2  = g(W                )
P2 =  WM
Cp1 P1c[   ]
P1C[   ]
Figure 2: Example of how the MV-RNN merges a phrase
with another word at a nonterminal node of a parse tree.
2.3 Recursive Compositions of Multiple Words
and Phrases
This section describes how we extend a word-pair
matrix-vector-based compositional model to learn
vectors and matrices for longer sequences of words.
The main idea is to apply the same function f to
pairs of constituents in a parse tree. For this to
work, we need to take as input a binary parse tree
of a phrase or sentence and also compute matrices at
each nonterminal parent node. The function f can
be readily used for phrase vectors since it is recur-
sively compatible (p has the same dimensionality as
its children). For computing nonterminal phrase ma-
trices, we define the function
P = fM (A,B) = WM
[
A
B
]
, (3)
where WM ? Rn?2n, so P ? Rn?n just like each
input matrix.
After two words form a constituent in the parse
tree, this constituent can now be merged with an-
other one by applying the same functions f and
fM . For instance, to compute the vectors and ma-
trices depicted in Fig. 2, we first merge words a
and b and their matrices: p1 = f(Ba,Ab), P1 =
fM (A,B). The resulting vector-matrix pair (p1, P1)
can now be used to compute the full phrase when
combining it with word c and computing p2 =
f(Cp1, P1c), P2 = fM (P1, C). The model com-
putes vectors and matrices in a bottom-up fashion,
applying the functions f, fM to its own previous out-
put (i.e. recursively) until it reaches the top node of
the tree which represents the entire sentence.
For experiments with longer sequences we will
compare to standard RNNs and the special case of
the MV-RNN that computes the parent by p = Ab+
Ba, which we name the linear Matrix-Vector Re-
cursion model (linear MVR). Previously, this model
had not been trained for multi-word sequences. Sec.
6 talks about alternatives for compositionality.
2.4 Objective Functions for Training
One of the advantages of RNN-based models is that
each node of a tree has associated with it a dis-
tributed vector representation (the parent vector p)
which can also be seen as features describing that
phrase. We train these representations by adding on
top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or
relationship classes: d(p) = softmax(W labelp). If
there are K labels, then d ? RK is a K-dimensional
multinomial distribution. For the applications below
(excluding logic), the corresponding error function
E(s, t, ?) that we minimize for a sentence s and its
tree t is the sum of cross-entropy errors at all nodes.
The only other methods that use this type of ob-
jective function are (Socher et al 2011b; Socher
et al 2011c), who also combine it with either a
score or reconstruction error. Hence, for compar-
isons to other related work, we need to merge vari-
ations of computing the parent vector p with this
classifier. The main difference is that the MV-RNN
has more flexibility since it has an input specific re-
cursive function fA,B to compute each parent. In
the following applications, we will use the softmax
classifier to predict both sentiment distributions and
noun-noun relationships.
2.5 Learning
Let ? = (W,WM ,W label, L, LM ) be our model pa-
rameters and ? a vector with regularization hyperpa-
rameters for all model parameters. L andLM are the
sets of all word vectors and word matrices. The gra-
dient of the overall objective function J becomes:
?J
??
=
1
N
?
(x,t)
?E(x, t; ?)
??
+ ??. (4)
To compute this gradient, we first compute all tree
nodes (pi, Pi) from the bottom-up and then take
derivatives of the softmax classifiers at each node
in the tree from the top down. Derivatives are com-
puted efficiently via backpropagation through struc-
ture (Goller and Ku?chler, 1996). Even though the
1204
objective is not convex, we found that L-BFGS run
over the complete training data (batch mode) mini-
mizes the objective well in practice and convergence
is smooth. For more information see (Socher et al
2010).
2.6 Low-Rank Matrix Approximations
If every word is represented by an n-dimensional
vector and additionally by an n ? n matrix, the di-
mensionality of the whole model may become too
large with commonly used vector sizes of n = 100.
In order to reduce the number of parameters, we rep-
resent word matrices by the following low-rank plus
diagonal approximation:
A = UV + diag(a), (5)
where U ? Rn?r, V ? Rr?n, a ? Rn and we set
the rank for all experiments to r = 3.
2.7 Discussion: Evaluation and Generality
Evaluation of compositional vector spaces is a com-
plex task. Most related work compares similarity
judgments of unsupervised models to those of hu-
man judgments and aims at high correlation. These
evaluations can give important insights. However,
even with good correlation the question remains
how these models would perform on downstream
NLP tasks such as sentiment detection. We ex-
perimented with unsupervised learning of general
vector-matrix representations by having the MV-
RNN predict words in their correct context. Ini-
tializing the models with these general representa-
tions, did not improve the performance on the tasks
we consider. For sentiment analysis, this is not sur-
prising since antonyms often get similar vectors dur-
ing unsupervised learning from co-occurrences due
to high similarity of local syntactic contexts. In our
experiments, the high prediction performance came
from supervised learning of meaning representations
using labeled data. While these representations are
task-specific, they could be used across tasks in a
multi-task learning setup. However, in order to fairly
compare to related work, we use only the super-
vised data of each task. Before we describe our full-
scale experiments, we analyze the model?s expres-
sive powers.
3 Model Analysis
This section analyzes the model with two proof-of-
concept studies. First, we examine its ability to learn
operator semantics for adverb-adjective pairs. If a
model cannot correctly capture how an adverb op-
erates on the meaning of adjectives, then there?s lit-
tle chance it can learn operators for more complex
relationships. The second study analyzes whether
the MV-RNN can learn simple boolean operators of
propositional logic such as conjunctives or negation
from truth values. Again, if a model did not have this
ability, then there?s little chance it could learn these
frequently occurring phenomena from the noisy lan-
guage of real texts such as movie reviews.
3.1 Predicting Sentiment Distributions of
Adverb-Adjective Pairs
The first study considers the prediction of fine-
grained sentiment distributions of adverb-adjective
pairs and analyzes different possibilities for com-
puting the parent vector p. The results show that
the MV-RNN operators are powerful enough to cap-
ture the operational meanings of various types of ad-
verbs. For example, very is an intensifier, pretty is an
attenuator, and not can negate or strongly attenuate
the positivity of an adjective. For instance not great
is still pretty good and not terrible; see Potts (2010)
for details.
We use a publicly available IMDB dataset of ex-
tracted adverb-adjective pairs from movie reviews.1
The dataset provides the distribution over star rat-
ings: Each consecutive word pair appears a certain
number of times in reviews that have also associ-
ated with them an overall rating of the movie. After
normalizing by the total number of occurrences, one
gets a multinomial distribution over ratings. Only
word pairs that appear at least 50 times are kept. Of
the remaining pairs, we use 4211 randomly sampled
ones for training and a separate set of 1804 for test-
ing. We never give the algorithm sentiment distribu-
tions for single words, and, while single words over-
lap between training and testing, the test set consists
of never before seen word pairs.
The softmax classifier is trained to minimize the
cross entropy error. Hence, an evaluation in terms of
KL-divergence is the most reasonable choice. It is
1http://compprag.christopherpotts.net/reviews.html
1205
Method Avg KL
Uniform 0.327
Mean train 0.193
p = 12(a+ b) 0.103
p = a? b 0.103
p = [a; b] 0.101
p = Ab 0.103
RNN 0.093
Linear MVR 0.092
MV-RNN 0.091
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 fairly annoying
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 fairly awesome
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 fairly sad
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 not annoying
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 not awesome
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 not sad
 
 Training Pair
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 unbelievably annoying
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 unbelievably awesome
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 unbelievably sad
 
 MV?RNNRNN
Figure 3: Left: Average KL-divergence for predicting sentiment distributions of unseen adverb-adjective pairs of the
test set. See text for p descriptions. Lower is better. The main difference in the KL divergence comes from the few
negation pairs in the test set. Right: Predicting sentiment distributions (over 1-10 stars on the x-axis) of adverb-
adjective pairs. Each row has the same adverb and each column the same adjective. Many predictions are similar
between the two models. The RNN and linear MVR are not able to modify the sentiment correctly: not awesome is
more positive than fairly awesome and not annoying has a similar shape as unbelievably annoying. Predictions of the
linear MVR model are almost identical to the standard RNN for these examples.
defined as KL(g||p) =
?
i gi log(gi/pi), where g is
the gold distribution and p is the predicted one.
We compare to several baselines and ablations of
the MV-RNN model. An (adverb,adjective) pair is
described by its vectors (a, b) and matrices (A,B).
1 p = 0.5(a+ b), vector average
2. p = a? b, element-wise vector multiplication
3. p = [a; b], vector concatenation
4. p = Ab, similar to (Baroni and Lenci, 2010)
5. p = g(W [a; b]), RNN, similar to Socher et al
6. p = Ab+Ba, Linear MVR, similar to (Mitchell
and Lapata, 2010; Zanzotto et al 2010)
7. p = g(W [Ba;Ab]), MV-RNN
The final distribution is always predicted by a
softmax classifier whose inputs p vary for each of
the models. This objective function (see Sec. 2.4)
is different to all previously published work except
that of (Socher et al 2011c).
We cross-validated all models over regulariza-
tion parameters for word vectors, the softmax clas-
sifier, the RNN parameter W and the word op-
erators (10?4, 10?3) and word vector sizes (n =
6, 8, 10, 12, 15, 20). All models performed best at
vector sizes of below 12. Hence, it is the model?s
power and not the number of parameters that deter-
mines the performance. The table in Fig. 3 shows
the average KL-divergence on the test set. It shows
that the idea of matrix-vector representations for all
words and having a nonlinearity are both impor-
tant. The MV-RNN which combines these two ideas
is best able to learn the various compositional ef-
fects. The main difference in KL divergence comes
from the few negation cases in the test set. Fig. 3
shows examples of predicted distributions. Many
of the predictions are accurate and similar between
the top models. However, only the MV-RNN has
enough expressive power to allow negation to com-
pletely shift the sentiment with respect to an adjec-
tive. A negated adjective carrying negative senti-
ment becomes slightly positive, whereas not awe-
some is correctly attenuated. All three top models
correctly capture the U-shape of unbelievably sad.
This pair peaks at both the negative and positive
spectrum because it is ambiguous. When referring
to the performance of actors, it is very negative, but,
when talking about the plot, many people enjoy sad
and thought-provoking movies. The p = Ab model
does not perform well because it cannot model the
fact that for an adjective like ?sad,? the operator of
?unbelievably? behaves differently.
1206
false
false
? false
false
true
? false
false
false
? true
true
true
? true
true
? false
false
? true
Figure 4: Training trees for the MV-RNN to learn propositional operators. The model learns vectors and operators for
? (and) and ? (negation). The model outputs the exact representations of false and true respectively at the top node.
Hence, the operators can be combined recursively an arbitrary number of times for more complex logical functions.
3.2 Logic- and Vector-based Compositionality
Another natural question is whether the MV-RNN
can, in general, capture some of the simple boolean
logic that is sometimes found in language. In other
words, can it learn some of the propositional logic
operators such as and, or, not in terms of vectors and
matrices from a few examples. Answering this ques-
tion can also be seen as a first step towards bridg-
ing the gap between logic-based, formal semantics
(Montague, 1974) and vector space models.
The logic-based view of language accounts nicely
for compositionality by directly mapping syntac-
tic constituents to lambda calculus expressions. At
the word level, the focus is on function words, and
nouns and adjectives are often defined only in terms
of the sets of entities they denote in the world. Most
words are treated as atomic symbols with no rela-
tion to each other. There have been many attempts
at automatically parsing natural language to a logi-
cal form using recursive compositional rules.
Conversely, vector space models have the attrac-
tive property that they can automatically extract
knowledge from large corpora without supervision.
Unlike logic-based approaches, these models allow
us to make fine-grained statements about the seman-
tic similarity of words which correlate well with hu-
man judgments (Griffiths et al 2007). Logic-based
approaches are often seen as orthogonal to distribu-
tional vector-based approaches. However, Garrette
et al(2011) recently introduced a combination of a
vector space model inside a Markov Logic Network.
One open question is whether vector-based mod-
els can learn some of the simple logic encountered
in language such as negation or conjunctives. To
this end, we illustrate in a simple example that our
MV-RNN model and its learned word matrices (op-
erators) have the ability to learn propositional logic
operators such as ?,?,? (and, or, not). This is a
necessary (though not sufficient) condition for the
ability to pick up these phenomena in real datasets
and tasks such as sentiment detection which we fo-
cus on in the subsequent sections.
Our setup is as follows. We train on 6 strictly
right-branching trees as in Fig. 4. We consider the 1-
dimensional case and fix the representation for true
to (t = 1, T = 1) and false to (f = 0, F = 1).
Fixing the operators to the 1 ? 1 identity matrix 1
is essentially ignoring them. The objective is then
to create a perfect reconstruction of (t, T ) or (f, F )
(depending on the formula), which we achieve by
the least squares error between the top vector?s rep-
resentation and the corresponding truth value, e.g.
for ?false: min ||ptop ? t||2 + ||Ptop ? T ||2.
As our function g (see Eq. 2), we use a linear
threshold unit: g(x) = max(min(x, 1), 0). Giving
the derivatives computed for the objective function
for the examples in Fig. 4 to a standard L-BFGS op-
timizer quickly yields a training error of 0. Hence,
the output of these 6 examples has exactly one of the
truth representations, making it recursively compati-
ble with further combinations of operators. Thus, we
can combine these operators to construct any propo-
sitional logic function of any number of inputs (in-
cluding xor). Hence, this MV-RNN is complete in
terms of propositional logic.
4 Predicting Movie Review Ratings
In this section, we analyze the model?s performance
on full length sentences. We compare to previous
state of the art methods on a standard benchmark
dataset of movie reviews (Pang and Lee, 2005; Nak-
agawa et al 2010; Socher et al 2011c). This
dataset consists of 10,000 positive and negative sin-
gle sentences describing movie sentiment. In this
and the next experiment we use binarized trees from
the Stanford Parser (Klein and Manning, 2003). We
use the exact same setup and parameters (regulariza-
tion, word vector size, etc.) as the published code of
Socher et al(2011c).2
2www.socher.org
1207
Method Acc.
Tree-CRF (Nakagawa et al 2010) 77.3
RAE (Socher et al 2011c) 77.7
Linear MVR 77.1
MV-RNN 79.0
Table 1: Accuracy of classification on full length movie
review polarity (MR).
S. C. Review sentence
1
?
The film is bright and flashy in all the right ways.
0
?
Not always too whimsical for its own good this
strange hybrid of crime thriller, quirky character
study, third-rate romance and female empowerment
fantasy never really finds the tonal or thematic glue
it needs.
0
?
Doesn?t come close to justifying the hype that sur-
rounded its debut at the Sundance film festival two
years ago.
0 x Director Hoffman, his writer and Kline?s agent
should serve detention.
1 x A bodice-ripper for intellectuals.
Table 2: Hard movie review examples of positive (1) and
negative (0) sentiment (S.) that of all methods only the
MV-RNN predicted correctly (C:
?
) or could not classify
as correct either (C: x).
Table 1 shows comparisons to the system of (Nak-
agawa et al 2010), a dependency tree based classifi-
cation method that uses CRFs with hidden variables.
The state of the art recursive autoencoder model of
Socher et al(2011c) obtained 77.7% accuracy. Our
new MV-RNN gives the highest performance, out-
performing also the linear MVR (Sec. 2.2).
Table 2 shows several hard examples that only the
MV-RNN was able to classify correctly. None of the
methods correctly classified the last two examples
which require more world knowledge.
5 Classification of Semantic Relationships
The previous task considered global classification of
an entire phrase or sentence. In our last experiment
we show that the MV-RNN can also learn how a syn-
tactic context composes an aggregate meaning of the
semantic relationships between words. In particular,
the task is finding semantic relationships between
pairs of nominals. For instance, in the sentence
?My [apartment]e1 has a pretty large [kitchen]e2.?,
we want to predict that the kitchen and apartment are
in a component-whole relationship. Predicting such
????[ m o v i e ]  s h o w e d  [ w ar s ]     ?
MV - R N N  f o r  R e l at i o n s h i p Cl as s i f i cat i o n
?
?
Cl as s i f i e r :  Me s s age - T o pi c
Figure 5: The MV-RNN learns vectors in the path con-
necting two words (dotted lines) to determine their se-
mantic relationship. It takes into consideration a variable
length sequence of various word types in that path.
semantic relations is useful for information extrac-
tion and thesaurus construction applications. Many
approaches use features for all words on the path
between the two words of interest. We show that
by building a single compositional semantics for the
minimal constituent including both terms one can
achieve a higher performance.
This task requires the ability to deal with se-
quences of words of arbitrary type and length in be-
tween the two nouns in question.Fig. 5 explains our
method for classifying nominal relationships. We
first find the path in the parse tree between the two
words whose relation we want to classify. We then
select the highest node of the path and classify the
relationship using that node?s vector as features. We
apply the same type of MV-RNN model as in senti-
ment to the subtree spanned by the two words.
We use the dataset and evaluation framework
of SemEval-2010 Task 8 (Hendrickx et al 2010).
There are 9 ordered relationships (with two direc-
tions) and an undirected other class, resulting in
19 classes. Among the relationships are: message-
topic, cause-effect, instrument-agency (etc. see Ta-
ble 3 for list). A pair is counted as correct if the
order of the words in the relationship is correct.
Table 4 lists results for several competing meth-
ods together with the resources and features used
by each method. We compare to the systems of
the competition which are described in Hendrickx
et al(2010) as well as the RNN and linear MVR.
Most systems used a considerable amount of hand-
designed semantic resources. In contrast to these
methods, the MV-RNN only needs a parser for the
tree structure and learns all semantics from unla-
beled corpora and the training data. Only the Se-
mEval training dataset is specific to this task, the re-
1208
Relationship Sentence with labeled nouns for which to predict relationships
Cause-Effect(e2,e1) Avian [influenza]e1 is an infectious disease caused by type a strains of the influenza [virus]e2.
Entity-Origin(e1,e2) The [mother]e1 left her native [land]e2 about the same time and they were married in that city.
Message-Topic(e2,e1) Roadside [attractions]e1 are frequently advertised with [billboards]e2 to attract tourists.
Product-Producer(e1,e2) A child is told a [lie]e1 for several years by their [parents]e2 before he/she realizes that ...
Entity-Destination(e1,e2) The accident has spread [oil]e1 into the [ocean]e2.
Member-Collection(e2,e1) The siege started, with a [regiment]e1 of lightly armored [swordsmen]e2 ramming down the gate.
Instrument-Agency(e2,e1) The core of the [analyzer]e1 identifies the paths using the constraint propagation [method]e2.
Component-Whole(e2,e1) The size of a [tree]e1 [crown]e2 is strongly correlated with the growth of the tree.
Content-Container(e1,e2) The hidden [camera]e1, found by a security guard, was hidden in a business card-sized [leaflet
box]e2 placed at an unmanned ATM in Tokyo?s Minato ward in early September.
Table 3: Examples of correct classifications of ordered, semantic relations between nouns by the MV-RNN. Note that
the final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketed
words. The paths vary in length and the words vary in type.
Classifier Feature Sets F1
SVM POS, stemming, syntactic patterns 60.1
SVM word pair, words in between 72.5
SVM POS, WordNet, stemming, syntactic
patterns
74.8
SVM POS, WordNet, morphological fea-
tures, thesauri, Google n-grams
77.6
MaxEnt POS, WordNet, morphological fea-
tures, noun compound system, the-
sauri, Google n-grams
77.6
SVM POS, WordNet, prefixes and other
morphological features, POS, depen-
dency parse features, Levin classes,
PropBank, FrameNet, NomLex-Plus,
Google n-grams, paraphrases, Tex-
tRunner
82.2
RNN - 74.8
Lin.MVR - 73.0
MV-RNN - 79.1
RNN POS,WordNet,NER 77.6
Lin.MVR POS,WordNet,NER 78.7
MV-RNN POS,WordNet,NER 82.4
Table 4: Learning methods, their feature sets and F1
results for predicting semantic relations between nouns.
The MV-RNN outperforms all but one method without
any additional feature sets. By adding three such features,
it obtains state of the art performance.
maining inputs and the training setup are the same
as in previous sentiment experiments.
The best method on this dataset (Rink and
Harabagiu, 2010) obtains 82.2% F1. In order to
see whether our system can improve over this sys-
tem, we added three features to the MV-RNN vec-
tor and trained another softmax classifier. The fea-
tures and their performance increases were POS tags
(+0.9); WordNet hypernyms (+1.3) and named en-
tity tags (NER) of the two words (+0.6). Features
were computed using the code of Ciaramita and Al-
tun (2006).3 With these features, the performance
improved over the state of the art system. Table 3
shows random correct classification examples.
6 Related work
Distributional approaches have become omnipresent
for the recognition of semantic similarity between
words and the treatment of compositionality has
seen much progress in recent years. Hence, we can-
not do justice to the large amount of literature. Com-
monly, single words are represented as vectors of
distributional characteristics ? e.g., their frequencies
in specific syntactic relations or their co-occurrences
with given context words (Pado and Lapata, 2007;
Baroni and Lenci, 2010; Turney and Pantel, 2010).
These representations have proven very effective in
sense discrimination and disambiguation (Schu?tze,
1998), automatic thesaurus extraction (Lin, 1998;
Curran, 2004) and selectional preferences.
There are several sophisticated ideas for com-
positionality in vector spaces. Mitchell and Lap-
ata (2010) present an overview of the most impor-
tant compositional models, from simple vector ad-
dition and component-wise multiplication to tensor
products, and convolution (Metcalfe, 1990). They
measured the similarity between word pairs such
as compound nouns or verb-object pairs and com-
pared these with human similarity judgments. Sim-
ple vector averaging or multiplication performed
best, hence our focus on related baselines above.
3sourceforge.net/projects/supersensetag/
1209
Other important models are tensor products (Clark
and Pulman, 2007), quantum logic (Widdows,
2008), holographic reduced representations (Plate,
1995) and the Compositional Matrix Space model
(Rudolph and Giesbrecht, 2010). RNNs are related
to autoencoder models such as the recursive autoas-
sociative memory (RAAM) (Pollack, 1990) or recur-
rent neural networks (Elman, 1991). Bottou (2011)
and Hinton (1990) discussed related models such as
recursive autoencoders for text understanding.
Our model builds upon and generalizes the mod-
els of (Mitchell and Lapata, 2010; Baroni and Zam-
parelli, 2010; Zanzotto et al 2010; Socher et al
2011c) (see Sec. 2.2). We compare to them in
our experiments. Yessenalina and Cardie (2011) in-
troduce a sentiment analysis model that describes
words as matrices and composition as matrix mul-
tiplication. Since matrix multiplication is associa-
tive, this cannot capture different scopes of nega-
tion or syntactic differences. Their model, is a spe-
cial case of our encoding model (when you ignore
vectors, fix the tree to be strictly branching in one
direction and use as the matrix composition func-
tion P = AB). Since our classifiers are trained on
the vectors, we cannot compare to this approach di-
rectly. Grefenstette and Sadrzadeh (2011) learn ma-
trices for verbs in a categorical model. The trained
matrices improve correlation with human judgments
on the task of identifying relatedness of subject-
verb-object triplets.
7 Conclusion
We introduced a new model towards a complete
treatment of compositionality in word vector spaces.
Our model builds on a syntactically plausible parse
tree and can handle compositional phenomena. The
main novelty of our model is the combination of
matrix-vector representations with a recursive neu-
ral network. It can learn both the meaning vectors of
a word and how that word modifies its neighbors (via
its matrix). The MV-RNN combines attractive the-
oretical properties with good performance on large,
noisy datasets. It generalizes several models in the
literature, can learn propositional logic, accurately
predicts sentiment and can be used to classify se-
mantic relationships between nouns in a sentence.
Acknowledgments
We thank for great discussions about the paper:
John Platt, Chris Potts, Josh Tenenbaum, Mihai Sur-
deanu, Quoc Le and Kevin Miller. The authors
gratefully acknowledges the support of the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181, and the DARPA Deep Learning program
under contract number FA8650-10-C-7020. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
DARPA, AFRL, or the US government.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
M. Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP.
L. Bottou. 2011. From machine learning to machine
reasoning. CoRR, abs/1102.1808.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In EMNLP.
S. Clark and S. Pulman. 2007. Combining symbolic and
distributional models of meaning. In Proceedings of
the AAAI Spring Symposium on Quantum Interaction,
pages 52?55.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: deep neural networks
with multitask learning. In ICML.
J. Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3).
G. Frege. 1892. U?ber Sinn und Bedeutung. In Zeitschrift
fu?r Philosophie und philosophische Kritik, 100.
D. Garrette, K. Erk, and R. Mooney. 2011. Integrat-
ing Logical Representations with Probabilistic Infor-
mation using Markov Logic. In Proceedings of the In-
ternational Conference on Computational Semantics.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
1210
E. Grefenstette and M. Sadrzadeh. 2011. Experimental
support for a categorical compositional distributional
model of meaning. In EMNLP.
T. L. Griffiths, J. B. Tenenbaum, and M. Steyvers. 2007.
Topics in semantic representation. Psychological Re-
view, 114.
I. Hendrickx, S.N. Kim, Z. Kozareva, P. Nakov,
D. O? Se?aghdha, S. Pado?, M. Pennacchiotti, L. Ro-
mano, and S. Szpakowicz. 2010. Semeval-2010 task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
G. E. Hinton. 1990. Mapping part-whole hierarchies into
connectionist networks. Artificial Intelligence, 46(1-
2).
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15th
international conference on World Wide Web.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING-ACL, pages
768?774.
E. J. Metcalfe. 1990. A compositive holographic asso-
ciative recall model. Psychological Review, 88:627?
661.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388?1429.
R. Montague. 1974. English as a formal language. Lin-
guaggi nella Societa e nella Tecnica, pages 189?224.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. In ACL.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115?124.
T. A. Plate. 1995. Holographic reduced representations.
IEEE Transactions on Neural Networks, 6(3):623?
641.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
B. Rink and S. Harabagiu. 2010. UTD: Classifying se-
mantic relations by combining lexical and semantic re-
sources. In Proceedings of the 5th International Work-
shop on Semantic Evaluation.
S. Rudolph and E. Giesbrecht. 2010. Compositional
matrix-space models of language. In ACL.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24:97?124.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
In NIPS. MIT Press.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
In EMNLP.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
D. Widdows. 2008. Semantic vector products: Some ini-
tial investigations. In Proceedings of the Second AAAI
Symposium on Quantum Interaction.
A. Yessenalina and C. Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
EMNLP.
F.M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-
andhar. 2010. Estimating linear models for composi-
tional distributional semantics. COLING.
1211
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng and Christopher Potts
Stanford University, Stanford, CA 94305, USA
richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu
{jeaneis,manning,cgpotts}@stanford.edu
Abstract
Semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. Further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. To remedy this, we introduce a
Sentiment Treebank. It includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-
ality. To address them, we introduce the
Recursive Neural Tensor Network. When
trained on the new treebank, this model out-
performs all previous methods on several met-
rics. It pushes the state of the art in single
sentence positive/negative classification from
80% up to 85.4%. The accuracy of predicting
fine-grained sentiment labels for all phrases
reaches 80.7%, an improvement of 9.7% over
bag of features baselines. Lastly, it is the only
model that can accurately capture the effects
of negation and its scope at various tree levels
for both positive and negative phrases.
1 Introduction
Semantic vector spaces for single words have been
widely used as features (Turney and Pantel, 2010).
Because they cannot capture the meaning of longer
phrases properly, compositionality in semantic vec-
tor spaces has recently received a lot of attention
(Mitchell and Lapata, 2010; Socher et al, 2010;
Zanzotto et al, 2010; Yessenalina and Cardie, 2011;
Socher et al, 2012; Grefenstette et al, 2013). How-
ever, progress is held back by the current lack of
large and labeled compositionality resources and
?
0
0This 0film
?
?
?
0does 0n?t
0
+care +0about ++
+
+
+cleverness 0,
0wit
0or
+
0
0any 00other +kind
+
0of ++intelligent + +humor
0.
Figure 1: Example of the Recursive Neural Tensor Net-
work accurately predicting 5 sentiment classes, very neg-
ative to very positive (? ?, ?, 0, +, + +), at every node of a
parse tree and capturing the negation and its scope in this
sentence.
models to accurately capture the underlying phe-
nomena presented in such data. To address this need,
we introduce the Stanford Sentiment Treebank and
a powerful Recursive Neural Tensor Network that
can accurately predict the compositional semantic
effects present in this new corpus.
The Stanford Sentiment Treebank is the first cor-
pus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser (Klein and Manning, 2003) and includes a
total of 215,154 unique phrases from those parse
trees, each annotated by 3 human judges. This new
dataset alows us to analyze the intricacies of senti-
ment and to capture complex linguistic phenomena.
Fig. 1 shows one of the many examples with clear
compositional structure. The granularity and size of
1631
this dataset will enable the community to train com-
positional models that are based on supervised and
structured machine learning techniques. While there
are several datasets with document and chunk labels
available, there is a need to better capture sentiment
from short comments, such as Twitter data, which
provide less overall signal per document.
In order to capture the compositional effects with
higher accuracy, we propose a new model called the
Recursive Neural Tensor Network (RNTN). Recur-
sive Neural Tensor Networks take as input phrases
of any length. They represent a phrase through word
vectors and a parse tree and then compute vectors for
higher nodes in the tree using the same tensor-based
composition function. We compare to several super-
vised, compositional models such as standard recur-
sive neural networks (RNN) (Socher et al, 2011b),
matrix-vector RNNs (Socher et al, 2012), and base-
lines such as neural networks that ignore word order,
Naive Bayes (NB), bi-gram NB and SVM. All mod-
els get a significant boost when trained with the new
dataset but the RNTN obtains the highest perfor-
mance with 80.7% accuracy when predicting fine-
grained sentiment for all nodes. Lastly, we use a test
set of positive and negative sentences and their re-
spective negations to show that, unlike bag of words
models, the RNTN accurately captures the sentiment
change and scope of negation. RNTNs also learn
that sentiment of phrases following the contrastive
conjunction ?but? dominates.
The complete training and testing code, a live
demo and the Stanford Sentiment Treebank dataset
are available at http://nlp.stanford.edu/
sentiment.
2 Related Work
This work is connected to five different areas of NLP
research, each with their own large amount of related
work to which we cannot do full justice given space
constraints.
Semantic Vector Spaces. The dominant ap-
proach in semantic vector spaces uses distributional
similarities of single words. Often, co-occurrence
statistics of a word and its context are used to de-
scribe each word (Turney and Pantel, 2010; Baroni
and Lenci, 2010), such as tf-idf. Variants of this idea
use more complex frequencies such as how often a
word appears in a certain syntactic context (Pado
and Lapata, 2007; Erk and Pado?, 2008). However,
distributional vectors often do not properly capture
the differences in antonyms since those often have
similar contexts. One possibility to remedy this is to
use neural word vectors (Bengio et al, 2003). These
vectors can be trained in an unsupervised fashion
to capture distributional similarities (Collobert and
Weston, 2008; Huang et al, 2012) but then also be
fine-tuned and trained to specific tasks such as sen-
timent detection (Socher et al, 2011b). The models
in this paper can use purely supervised word repre-
sentations learned entirely on the new corpus.
Compositionality in Vector Spaces. Most of
the compositionality algorithms and related datasets
capture two word compositions. Mitchell and La-
pata (2010) use e.g. two-word phrases and analyze
similarities computed by vector addition, multiplica-
tion and others. Some related models such as holo-
graphic reduced representations (Plate, 1995), quan-
tum logic (Widdows, 2008), discrete-continuous
models (Clark and Pulman, 2007) and the recent
compositional matrix space model (Rudolph and
Giesbrecht, 2010) have not been experimentally val-
idated on larger corpora. Yessenalina and Cardie
(2011) compute matrix representations for longer
phrases and define composition as matrix multipli-
cation, and also evaluate on sentiment. Grefen-
stette and Sadrzadeh (2011) analyze subject-verb-
object triplets and find a matrix-based categorical
model to correlate well with human judgments. We
compare to the recent line of work on supervised
compositional models. In particular we will de-
scribe and experimentally compare our new RNTN
model to recursive neural networks (RNN) (Socher
et al, 2011b) and matrix-vector RNNs (Socher et
al., 2012) both of which have been applied to bag of
words sentiment corpora.
Logical Form. A related field that tackles com-
positionality from a very different angle is that of
trying to map sentences to logical form (Zettlemoyer
and Collins, 2005). While these models are highly
interesting and work well in closed domains and
on discrete sets, they could only capture sentiment
distributions using separate mechanisms beyond the
currently used logical forms.
Deep Learning. Apart from the above mentioned
1632
work on RNNs, several compositionality ideas re-
lated to neural networks have been discussed by Bot-
tou (2011) and Hinton (1990) and first models such
as Recursive Auto-associative memories been exper-
imented with by Pollack (1990). The idea to relate
inputs through three way interactions, parameterized
by a tensor have been proposed for relation classifi-
cation (Sutskever et al, 2009; Jenatton et al, 2012),
extending Restricted Boltzmann machines (Ranzato
and Hinton, 2010) and as a special layer for speech
recognition (Yu et al, 2012).
Sentiment Analysis. Apart from the above-
mentioned work, most approaches in sentiment anal-
ysis use bag of words representations (Pang and Lee,
2008). Snyder and Barzilay (2007) analyzed larger
reviews in more detail by analyzing the sentiment
of multiple aspects of restaurants, such as food or
atmosphere. Several works have explored sentiment
compositionality through careful engineering of fea-
tures or polarity shifting rules on syntactic structures
(Polanyi and Zaenen, 2006; Moilanen and Pulman,
2007; Rentoumi et al, 2010; Nakagawa et al, 2010).
3 Stanford Sentiment Treebank
Bag of words classifiers can work well in longer
documents by relying on a few words with strong
sentiment like ?awesome? or ?exhilarating.? How-
ever, sentiment accuracies even for binary posi-
tive/negative classification for single sentences has
not exceeded 80% for several years. For the more
difficult multiclass case including a neutral class,
accuracy is often below 60% for short messages
on Twitter (Wang et al, 2012). From a linguistic
or cognitive standpoint, ignoring word order in the
treatment of a semantic task is not plausible, and, as
we will show, it cannot accurately classify hard ex-
amples of negation. Correctly predicting these hard
cases is necessary to further improve performance.
In this section we will introduce and provide some
analyses for the new Sentiment Treebank which in-
cludes labels for every syntactically plausible phrase
in thousands of sentences, allowing us to train and
evaluate compositional models.
We consider the corpus of movie review excerpts
from the rottentomatoes.com website orig-
inally collected and published by Pang and Lee
(2005). The original dataset includes 10,662 sen-
nerdy ?folks
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
phenomenal ?fantasy ?best ?sellers
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
 ?
 ?
Figure 3: The labeling interface. Random phrases were
shown and annotators had a slider for selecting the senti-
ment and its degree.
tences, half of which were considered positive and
the other half negative. Each label is extracted from
a longer movie review and reflects the writer?s over-
all intention for this review. The normalized, lower-
cased text is first used to recover, from the origi-
nal website, the text with capitalization. Remaining
HTML tags and sentences that are not in English
are deleted. The Stanford Parser (Klein and Man-
ning, 2003) is used to parses all 10,662 sentences.
In approximately 1,100 cases it splits the snippet
into multiple sentences. We then used Amazon Me-
chanical Turk to label the resulting 215,154 phrases.
Fig. 3 shows the interface annotators saw. The slider
has 25 different values and is initially set to neutral.
The phrases in each hit are randomly sampled from
the set of all phrases in order to prevent labels being
influenced by what follows. For more details on the
dataset collection, see supplementary material.
Fig. 2 shows the normalized label distributions at
each n-gram length. Starting at length 20, the ma-
jority are full sentences. One of the findings from
labeling sentences based on reader?s perception is
that many of them could be considered neutral. We
also notice that stronger sentiment often builds up
in longer phrases and the majority of the shorter
phrases are neutral. Another observation is that most
annotators moved the slider to one of the five po-
sitions: negative, somewhat negative, neutral, posi-
tive or somewhat positive. The extreme values were
rarely used and the slider was not often left in be-
tween the ticks. Hence, even a 5-class classification
into these categories captures the main variability
of the labels. We will name this fine-grained senti-
ment classification and our main experiment will be
to recover these five labels for phrases of all lengths.
1633
5 10 15 20 25 30 35 40 45
N-Gram Length
0%
20%
40%
60%
80%
100%
%
 o
f S
en
tim
en
t V
al
ue
s
Neutral
SomeZhat 3ositiYe
3ositiYe
Ver\ 3ositiYe
SomeZhat NegatiYe
NegatiYe
Ver\ NegatiYe
(a)
(a)
(b)
(b)
(c)
(c)
(d)
(d)
Distributions of sentiment values for (a) unigrams, 
(b) 10-grams, (c) 20-grams, and (d) full sentences.
Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral;
longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence
the two strongest labels and intermediate tick positions are merged into 5 classes.
4 Recursive Neural Models
The models in this section compute compositional
vector representations for phrases of variable length
and syntactic type. These representations will then
be used as features to classify each phrase. Fig. 4
displays this approach. When an n-gram is given to
the compositional models, it is parsed into a binary
tree and each leaf node, corresponding to a word,
is represented as a vector. Recursive neural mod-
els will then compute parent vectors in a bottom
up fashion using different types of compositional-
ity functions g. The parent vectors are again given
as features to a classifier. For ease of exposition,
we will use the tri-gram in this figure to explain all
models.
We first describe the operations that the below re-
cursive neural models have in common: word vector
representations and classification. This is followed
by descriptions of two previous RNN models and
our RNTN.
Each word is represented as a d-dimensional vec-
tor. We initialize all word vectors by randomly
sampling each value from a uniform distribution:
U(?r, r), where r = 0.0001. All the word vec-
tors are stacked in the word embedding matrix L ?
Rd?|V |, where |V | is the size of the vocabulary. Ini-
tially the word vectors will be random but the L ma-
trix is seen as a parameter that is trained jointly with
the compositionality models.
We can use the word vectors immediately as
parameters to optimize and as feature inputs to
a softmax classifier. For classification into five
classes, we compute the posterior probability over
    not      very       good ...
        a          b             c 
p1 =g(b,c)
p2 = g(a,p1)
0 0 +
+ +
-
Figure 4: Approach of Recursive Neural Network mod-
els for sentiment: Compute parent vectors in a bottom up
fashion using a compositionality function g and use node
vectors as features for a classifier at that node. This func-
tion varies for the different models.
labels given the word vector via:
ya = softmax(Wsa), (1)
where Ws ? R5?d is the sentiment classification
matrix. For the given tri-gram, this is repeated for
vectors b and c. The main task of and difference
between the models will be to compute the hidden
vectors pi ? Rd in a bottom up fashion.
4.1 RNN: Recursive Neural Network
The simplest member of this family of neural net-
work models is the standard recursive neural net-
work (Goller and Ku?chler, 1996; Socher et al,
2011a). First, it is determined which parent already
has all its children computed. In the above tree ex-
ample, p1 has its two children?s vectors since both
are words. RNNs use the following equations to
compute the parent vectors:
1634
p1 = f
(
W
[
b
c
])
, p2 = f
(
W
[
a
p1
])
,
where f = tanh is a standard element-wise nonlin-
earity, W ? Rd?2d is the main parameter to learn
and we omit the bias for simplicity. The bias can be
added as an extra column to W if an additional 1 is
added to the concatenation of the input vectors. The
parent vectors must be of the same dimensionality to
be recursively compatible and be used as input to the
next composition. Each parent vector pi, is given to
the same softmax classifier of Eq. 1 to compute its
label probabilities.
This model uses the same compositionality func-
tion as the recursive autoencoder (Socher et al,
2011b) and recursive auto-associate memories (Pol-
lack, 1990). The only difference to the former model
is that we fix the tree structures and ignore the re-
construction loss. In initial experiments, we found
that with the additional amount of training data, the
reconstruction loss at each node is not necessary to
obtain high performance.
4.2 MV-RNN: Matrix-Vector RNN
The MV-RNN is linguistically motivated in that
most of the parameters are associated with words
and each composition function that computes vec-
tors for longer phrases depends on the actual words
being combined. The main idea of the MV-RNN
(Socher et al, 2012) is to represent every word and
longer phrase in a parse tree as both a vector and
a matrix. When two constituents are combined the
matrix of one is multiplied with the vector of the
other and vice versa. Hence, the compositional func-
tion is parameterized by the words that participate in
it.
Each word?s matrix is initialized as a d?d identity
matrix, plus a small amount of Gaussian noise. Sim-
ilar to the random word vectors, the parameters of
these matrices will be trained to minimize the clas-
sification error at each node. For this model, each n-
gram is represented as a list of (vector,matrix) pairs,
together with the parse tree. For the tree with (vec-
tor,matrix) nodes:
(p2,P2)
(a,A) (p1,P1)
(b,B) (c,C)
the MV-RNN computes the first parent vector and its
matrix via two equations:
p1 = f
(
W
[
Cb
Bc
])
, P1 = f
(
WM
[
B
C
])
,
where WM ? Rd?2d and the result is again a d ? d
matrix. Similarly, the second parent node is com-
puted using the previously computed (vector,matrix)
pair (p1, P1) as well as (a,A). The vectors are used
for classifying each phrase using the same softmax
classifier as in Eq. 1.
4.3 RNTN:Recursive Neural Tensor Network
One problem with the MV-RNN is that the number
of parameters becomes very large and depends on
the size of the vocabulary. It would be cognitively
more plausible if there was a single powerful com-
position function with a fixed number of parameters.
The standard RNN is a good candidate for such a
function. However, in the standard RNN, the input
vectors only implicitly interact through the nonlin-
earity (squashing) function. A more direct, possibly
multiplicative, interaction would allow the model to
have greater interactions between the input vectors.
Motivated by these ideas we ask the question: Can
a single, more powerful composition function per-
form better and compose aggregate meaning from
smaller constituents more accurately than many in-
put specific ones? In order to answer this question,
we propose a new model called the Recursive Neu-
ral Tensor Network (RNTN). The main idea is to use
the same, tensor-based composition function for all
nodes.
Fig. 5 shows a single tensor layer. We define the
output of a tensor product h ? Rd via the follow-
ing vectorized notation and the equivalent but more
detailed notation for each slice V [i] ? Rd?d:
h =
[
b
c
]T
V [1:d]
[
b
c
]
;hi =
[
b
c
]T
V [i]
[
b
c
]
.
where V [1:d] ? R2d?2d?d is the tensor that defines
multiple bilinear forms.
1635
            Slices of       Standard   
                Tensor Layer          Layer
p = f             V[1:2]        +   W
Neural Tensor Layer
b
c
b
c
b
c
T
p = f                             +          
Figure 5: A single layer of the Recursive Neural Ten-
sor Network. Each dashed box represents one of d-many
slices and can capture a type of influence a child can have
on its parent.
The RNTN uses this definition for computing p1:
p1 = f
([
b
c
]T
V [1:d]
[
b
c
]
+W
[
b
c
])
,
where W is as defined in the previous models. The
next parent vector p2 in the tri-gram will be com-
puted with the same weights:
p2 = f
([
a
p1
]T
V [1:d]
[
a
p1
]
+W
[
a
p1
])
.
The main advantage over the previous RNN
model, which is a special case of the RNTN when
V is set to 0, is that the tensor can directly relate in-
put vectors. Intuitively, we can interpret each slice
of the tensor as capturing a specific type of compo-
sition.
An alternative to RNTNs would be to make the
compositional function more powerful by adding a
second neural network layer. However, initial exper-
iments showed that it is hard to optimize this model
and vector interactions are still more implicit than in
the RNTN.
4.4 Tensor Backprop through Structure
We describe in this section how to train the RNTN
model. As mentioned above, each node has a
softmax classifier trained on its vector representa-
tion to predict a given ground truth or target vector
t. We assume the target distribution vector at each
node has a 0-1 encoding. If there are C classes, then
it has length C and a 1 at the correct label. All other
entries are 0.
We want to maximize the probability of the cor-
rect prediction, or minimize the cross-entropy error
between the predicted distribution yi ? RC?1 at
node i and the target distribution ti ? RC?1 at that
node. This is equivalent (up to a constant) to mini-
mizing the KL-divergence between the two distribu-
tions. The error as a function of the RNTN parame-
ters ? = (V,W,Ws, L) for a sentence is:
E(?) =
?
i
?
j
tij log y
i
j + ????
2 (2)
The derivative for the weights of the softmax clas-
sifier are standard and simply sum up from each
node?s error. We define xi to be the vector at node
i (in the example trigram, the xi ? Rd?1?s are
(a, b, c, p1, p2)). We skip the standard derivative for
Ws. Each node backpropagates its error through to
the recursively used weights V,W . Let ?i,s ? Rd?1
be the softmax error vector at node i:
?i,s =
(
W Ts (y
i ? ti)
)
? f ?(xi),
where ? is the Hadamard product between the two
vectors and f ? is the element-wise derivative of f
which in the standard case of using f = tanh can
be computed using only f(xi).
The remaining derivatives can only be computed
in a top-down fashion from the top node through the
tree and into the leaf nodes. The full derivative for
V and W is the sum of the derivatives at each of
the nodes. We define the complete incoming error
messages for a node i as ?i,com. The top node, in
our case p2, only received errors from the top node?s
softmax. Hence, ?p2,com = ?p2,s which we can
use to obtain the standard backprop derivative for
W (Goller and Ku?chler, 1996; Socher et al, 2010).
For the derivative of each slice k = 1, . . . , d, we get:
?Ep2
?V [k]
= ?p2,comk
[
a
p1
] [
a
p1
]T
,
where ?p2,comk is just the k?th element of this vector.
Now, we can compute the error message for the two
1636
children of p2:
?p2,down =
(
W T ?p2,com + S
)
? f ?
([
a
p1
])
,
where we define
S =
d?
k=1
?p2,comk
(
V [k] +
(
V [k]
)T
)[
a
p1
]
The children of p2, will then each take half of this
vector and add their own softmax error message for
the complete ?. In particular, we have
?p1,com = ?p1,s + ?p2,down[d+ 1 : 2d],
where ?p2,down[d + 1 : 2d] indicates that p1 is the
right child of p2 and hence takes the 2nd half of the
error, for the final word vector derivative for a, it
will be ?p2,down[1 : d].
The full derivative for slice V [k] for this trigram
tree then is the sum at each node:
?E
?V [k]
=
Ep2
?V [k]
+ ?p1,comk
[
b
c
] [
b
c
]T
,
and similarly for W . For this nonconvex optimiza-
tion we use AdaGrad (Duchi et al, 2011) which con-
verges in less than 3 hours to a local optimum.
5 Experiments
We include two types of analyses. The first type in-
cludes several large quantitative evaluations on the
test set. The second type focuses on two linguistic
phenomena that are important in sentiment.
For all models, we use the dev set and cross-
validate over regularization of the weights, word
vector size as well as learning rate and minibatch
size for AdaGrad. Optimal performance for all mod-
els was achieved at word vector sizes between 25
and 35 dimensions and batch sizes between 20 and
30. Performance decreased at larger or smaller vec-
tor and batch sizes. This indicates that the RNTN
does not outperform the standard RNN due to sim-
ply having more parameters. The MV-RNN has or-
ders of magnitudes more parameters than any other
model due to the word matrices. The RNTN would
usually achieve its best performance on the dev set
after training for 3 - 5 hours. Initial experiments
Model
Fine-grained Positive/Negative
All Root All Root
NB 67.2 41.0 82.6 81.8
SVM 64.3 40.7 84.6 79.4
BiNB 71.0 41.9 82.7 83.1
VecAvg 73.3 32.7 85.1 80.1
RNN 79.0 43.2 86.1 82.4
MV-RNN 78.7 44.4 86.8 82.9
RNTN 80.7 45.7 87.6 85.4
Table 1: Accuracy for fine grained (5-class) and binary
predictions at the sentence level (root) and for all nodes.
showed that the recursive models worked signifi-
cantly worse (over 5% drop in accuracy) when no
nonlinearity was used. We use f = tanh in all ex-
periments.
We compare to commonly used methods that use
bag of words features with Naive Bayes and SVMs,
as well as Naive Bayes with bag of bigram features.
We abbreviate these with NB, SVM and biNB. We
also compare to a model that averages neural word
vectors and ignores word order (VecAvg).
The sentences in the treebank were split into a
train (8544), dev (1101) and test splits (2210) and
these splits are made available with the data release.
We also analyze performance on only positive and
negative sentences, ignoring the neutral class. This
filters about 20% of the data with the three sets hav-
ing 6920/872/1821 sentences.
5.1 Fine-grained Sentiment For All Phrases
The main novel experiment and evaluation metric
analyze the accuracy of fine-grained sentiment clas-
sification for all phrases. Fig. 2 showed that a fine
grained classification into 5 classes is a reasonable
approximation to capture most of the data variation.
Fig. 6 shows the result on this new corpus. The
RNTN gets the highest performance, followed by
the MV-RNN and RNN. The recursive models work
very well on shorter phrases, where negation and
composition are important, while bag of features
baselines perform well only with longer sentences.
The RNTN accuracy upper bounds other models at
most n-gram lengths.
Table 1 (left) shows the overall accuracy numbers
for fine grained prediction at all phrase lengths and
full sentences.
1637
    
1*UDP/HQJWK





$
F
F
X
U
D
F
\
    
1*UDP/HQJWK

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142?150,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Word Vectors for Sentiment Analysis
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
Andrew Y. Ng, and Christopher Potts
Stanford University
Stanford, CA 94305
[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu
Abstract
Unsupervised vector-based approaches to se-
mantics can model rich lexical meanings, but
they largely fail to capture sentiment informa-
tion that is central to many word meanings and
important for a wide range of NLP tasks. We
present a model that uses a mix of unsuper-
vised and supervised techniques to learn word
vectors capturing semantic term?document in-
formation as well as rich sentiment content.
The proposed model can leverage both con-
tinuous and multi-dimensional sentiment in-
formation as well as non-sentiment annota-
tions. We instantiate the model to utilize the
document-level sentiment polarity annotations
present in many online documents (e.g. star
ratings). We evaluate the model using small,
widely used sentiment and subjectivity cor-
pora and find it out-performs several previ-
ously introduced methods for sentiment clas-
sification. We also introduce a large dataset
of movie reviews to serve as a more robust
benchmark for work in this area.
1 Introduction
Word representations are a critical component of
many natural language processing systems. It is
common to represent words as indices in a vocab-
ulary, but this fails to capture the rich relational
structure of the lexicon. Vector-based models do
much better in this regard. They encode continu-
ous similarities between words as distance or angle
between word vectors in a high-dimensional space.
The general approach has proven useful in tasks
such as word sense disambiguation, named entity
recognition, part of speech tagging, and document
retrieval (Turney and Pantel, 2010; Collobert and
Weston, 2008; Turian et al, 2010).
In this paper, we present a model to capture both
semantic and sentiment similarities among words.
The semantic component of our model learns word
vectors via an unsupervised probabilistic model of
documents. However, in keeping with linguistic and
cognitive research arguing that expressive content
and descriptive semantic content are distinct (Ka-
plan, 1999; Jay, 2000; Potts, 2007), we find that
this basic model misses crucial sentiment informa-
tion. For example, while it learns that wonderful
and amazing are semantically close, it doesn?t cap-
ture the fact that these are both very strong positive
sentiment words, at the opposite end of the spectrum
from terrible and awful.
Thus, we extend the model with a supervised
sentiment component that is capable of embracing
many social and attitudinal aspects of meaning (Wil-
son et al, 2004; Alm et al, 2005; Andreevskaia
and Bergler, 2006; Pang and Lee, 2005; Goldberg
and Zhu, 2006; Snyder and Barzilay, 2007). This
component of the model uses the vector represen-
tation of words to predict the sentiment annotations
on contexts in which the words appear. This causes
words expressing similar sentiment to have similar
vector representations. The full objective function
of the model thus learns semantic vectors that are
imbued with nuanced sentiment information. In our
experiments, we show how the model can leverage
document-level sentiment annotations of a sort that
are abundant online in the form of consumer reviews
for movies, products, etc. The technique is suffi-
142
ciently general to work also with continuous and
multi-dimensional notions of sentiment as well as
non-sentiment annotations (e.g., political affiliation,
speaker commitment).
After presenting the model in detail, we pro-
vide illustrative examples of the vectors it learns,
and then we systematically evaluate the approach
on document-level and sentence-level classification
tasks. Our experiments involve the small, widely
used sentiment and subjectivity corpora of Pang and
Lee (2004), which permits us to make comparisons
with a number of related approaches and published
results. We also show that this dataset contains many
correlations between examples in the training and
testing sets. This leads us to evaluate on, and make
publicly available, a large dataset of informal movie
reviews from the Internet Movie Database (IMDB).
2 Related work
The model we present in the next section draws in-
spiration from prior work on both probabilistic topic
modeling and vector-spaced models for word mean-
ings.
Latent Dirichlet Allocation (LDA; (Blei et al,
2003)) is a probabilistic document model that as-
sumes each document is a mixture of latent top-
ics. For each latent topic T , the model learns a
conditional distribution p(w|T ) for the probability
that word w occurs in T . One can obtain a k-
dimensional vector representation of words by first
training a k-topic model and then filling the matrix
with the p(w|T ) values (normalized to unit length).
The result is a word?topic matrix in which the rows
are taken to represent word meanings. However,
because the emphasis in LDA is on modeling top-
ics, not word meanings, there is no guarantee that
the row (word) vectors are sensible as points in a
k-dimensional space. Indeed, we show in section
4 that using LDA in this way does not deliver ro-
bust word vectors. The semantic component of our
model shares its probabilistic foundation with LDA,
but is factored in a manner designed to discover
word vectors rather than latent topics. Some recent
work introduces extensions of LDA to capture sen-
timent in addition to topical information (Li et al,
2010; Lin and He, 2009; Boyd-Graber and Resnik,
2010). Like LDA, these methods focus on model-
ing sentiment-imbued topics rather than embedding
words in a vector space.
Vector space models (VSMs) seek to model words
directly (Turney and Pantel, 2010). Latent Seman-
tic Analysis (LSA), perhaps the best known VSM,
explicitly learns semantic word vectors by apply-
ing singular value decomposition (SVD) to factor a
term?document co-occurrence matrix. It is typical
to weight and normalize the matrix values prior to
SVD. To obtain a k-dimensional representation for a
given word, only the entries corresponding to the k
largest singular values are taken from the word?s ba-
sis in the factored matrix. Such matrix factorization-
based approaches are extremely successful in prac-
tice, but they force the researcher to make a number
of design choices (weighting, normalization, dimen-
sionality reduction algorithm) with little theoretical
guidance to suggest which to prefer.
Using term frequency (tf) and inverse document
frequency (idf) weighting to transform the values
in a VSM often increases the performance of re-
trieval and categorization systems. Delta idf weight-
ing (Martineau and Finin, 2009) is a supervised vari-
ant of idf weighting in which the idf calculation is
done for each document class and then one value
is subtracted from the other. Martineau and Finin
present evidence that this weighting helps with sen-
timent classification, and Paltoglou and Thelwall
(2010) systematically explore a number of weight-
ing schemes in the context of sentiment analysis.
The success of delta idf weighting in previous work
suggests that incorporating sentiment information
into VSM values via supervised methods is help-
ful for sentiment analysis. We adopt this insight,
but we are able to incorporate it directly into our
model?s objective function. (Section 4 compares
our approach with a representative sample of such
weighting schemes.)
3 Our Model
To capture semantic similarities among words, we
derive a probabilistic model of documents which
learns word representations. This component does
not require labeled data, and shares its foundation
with probabilistic topic models such as LDA. The
sentiment component of our model uses sentiment
annotations to constrain words expressing similar
143
sentiment to have similar representations. We can
efficiently learn parameters for the joint objective
function using alternating maximization.
3.1 Capturing Semantic Similarities
We build a probabilistic model of a document us-
ing a continuous mixture distribution over words in-
dexed by a multi-dimensional random variable ?.
We assume words in a document are conditionally
independent given the mixture variable ?. We assign
a probability to a document d using a joint distribu-
tion over the document and ?. The model assumes
each word wi ? d is conditionally independent of
the other words given ?. The probability of a docu-
ment is thus
p(d) =
?
p(d, ?)d? =
?
p(?)
N
?
i=1
p(wi|?)d?. (1)
Where N is the number of words in d and wi is
the ith word in d. We use a Gaussian prior on ?.
We define the conditional distribution p(wi|?) us-
ing a log-linear model with parameters R and b.
The energy function uses a word representation ma-
trix R ? R(? x |V |) where each word w (represented
as a one-on vector) in the vocabulary V has a ?-
dimensional vector representation ?w = Rw corre-
sponding to that word?s column in R. The random
variable ? is also a ?-dimensional vector, ? ? R?
which weights each of the ? dimensions of words?
representation vectors. We additionally introduce a
bias bw for each word to capture differences in over-
all word frequencies. The energy assigned to a word
w given these model parameters is
E(w; ?, ?w, bw) = ??T?w ? bw. (2)
To obtain the distribution p(w|?) we use a softmax,
p(w|?;R, b) = exp(?E(w; ?, ?w, bw))?
w??V exp(?E(w?; ?, ?w? , bw?))
(3)
= exp(?
T?w + bw)
?
w??V exp(?T?w? + bw?)
. (4)
The number of terms in the denominator?s sum-
mation grows linearly in |V |, making exact com-
putation of the distribution possible. For a given
?, a word w?s occurrence probability is related to
how closely its representation vector ?w matches the
scaling direction of ?. This idea is similar to the
word vector inner product used in the log-bilinear
language model of Mnih and Hinton (2007).
Equation 1 resembles the probabilistic model of
LDA (Blei et al, 2003), which models documents
as mixtures of latent topics. One could view the en-
tries of a word vector ? as that word?s association
strength with respect to each latent topic dimension.
The random variable ? then defines a weighting over
topics. However, our model does not attempt to
model individual topics, but instead directly models
word probabilities conditioned on the topic mixture
variable ?. Because of the log-linear formulation of
the conditional distribution, ? is a vector in R? and
not restricted to the unit simplex as it is in LDA.
We now derive maximum likelihood learning for
this model when given a set of unlabeled documents
D. In maximum likelihood learning we maximize
the probability of the observed data given the model
parameters. We assume documents dk ? D are i.i.d.
samples. Thus the learning problem becomes
max
R,b
p(D;R, b) =
?
dk?D
?
p(?)
Nk
?
i=1
p(wi|?;R, b)d?.
(5)
Using maximum a posteriori (MAP) estimates for ?,
we approximate this learning problem as
max
R,b
?
dk?D
p(??k)
Nk
?
i=1
p(wi|??k;R, b), (6)
where ??k denotes the MAP estimate of ? for dk.
We introduce a Frobenious norm regularization term
for the word representation matrix R. The word bi-
ases b are not regularized reflecting the fact that we
want the biases to capture whatever overall word fre-
quency statistics are present in the data. By taking
the logarithm and simplifying we obtain the final ob-
jective,
?||R||2F +
?
dk?D
?||??k||22 +
Nk
?
i=1
log p(wi|??k;R, b),
(7)
which is maximized with respect to R and b. The
hyper-parameters in the model are the regularization
144
weights (? and ?), and the word vector dimension-
ality ?.
3.2 Capturing Word Sentiment
The model presented so far does not explicitly cap-
ture sentiment information. Applying this algorithm
to documents will produce representations where
words that occur together in documents have sim-
ilar representations. However, this unsupervised
approach has no explicit way of capturing which
words are predictive of sentiment as opposed to
content-related. Much previous work in natural lan-
guage processing achieves better representations by
learning from multiple tasks (Collobert and Weston,
2008; Finkel and Manning, 2009). Following this
theme we introduce a second task to utilize labeled
documents to improve our model?s word representa-
tions.
Sentiment is a complex, multi-dimensional con-
cept. Depending on which aspects of sentiment we
wish to capture, we can give some body of text a
sentiment label s which can be categorical, continu-
ous, or multi-dimensional. To leverage such labels,
we introduce an objective that the word vectors of
our model should predict the sentiment label using
some appropriate predictor,
s? = f(?w). (8)
Using an appropriate predictor function f(x) we
map a word vector ?w to a predicted sentiment label
s?. We can then improve our word vector ?w to better
predict the sentiment labels of contexts in which that
word occurs.
For simplicity we consider the case where the sen-
timent label s is a scalar continuous value repre-
senting sentiment polarity of a document. This cap-
tures the case of many online reviews where doc-
uments are associated with a label on a star rating
scale. We linearly map such star values to the inter-
val s ? [0, 1] and treat them as a probability of pos-
itive sentiment polarity. Using this formulation, we
employ a logistic regression as our predictor f(x).
We use w?s vector representation ?w and regression
weights ? to express this as
p(s = 1|w;R,?) = ?(?T?w + bc), (9)
where ?(x) is the logistic function and ? ? R? is the
logistic regression weight vector. We additionally
introduce a scalar bias bc for the classifier.
The logistic regression weights ? and bc define
a linear hyperplane in the word vector space where
a word vector?s positive sentiment probability de-
pends on where it lies with respect to this hyper-
plane. Learning over a collection of documents re-
sults in words residing different distances from this
hyperplane based on the average polarity of docu-
ments in which the words occur.
Given a set of labeled documents D where sk is
the sentiment label for document dk, we wish to
maximize the probability of document labels given
the documents. We assume documents in the collec-
tion and words within a document are i.i.d. samples.
By maximizing the log-objective we obtain,
max
R,?,bc
|D|
?
k=1
Nk
?
i=1
log p(sk|wi;R,?, bc). (10)
The conditional probability p(sk|wi;R,?, bc) is
easily obtained from equation 9.
3.3 Learning
The full learning objective maximizes a sum of the
two objectives presented. This produces a final ob-
jective function of,
?||R||2F +
|D|
?
k=1
?||??k||22 +
Nk
?
i=1
log p(wi|??k;R, b)
+
|D|
?
k=1
1
|Sk|
Nk
?
i=1
log p(sk|wi;R,?, bc). (11)
|Sk| denotes the number of documents in the dataset
with the same rounded value of sk (i.e. sk < 0.5
and sk ? 0.5). We introduce the weighting 1|Sk| to
combat the well-known imbalance in ratings present
in review collections. This weighting prevents the
overall distribution of document ratings from affect-
ing the estimate of document ratings in which a par-
ticular word occurs. The hyper-parameters of the
model are the regularization weights (? and ?), and
the word vector dimensionality ?.
Maximizing the objective function with respect to
R, b, ?, and bc is a non-convex problem. We use
alternating maximization, which first optimizes the
145
word representations (R, b, ?, and bc) while leav-
ing the MAP estimates (??) fixed. Then we find the
new MAP estimate for each document while leav-
ing the word representations fixed, and continue this
process until convergence. The optimization algo-
rithm quickly finds a global solution for each ??k be-
cause we have a low-dimensional, convex problem
in each ??k. Because the MAP estimation problems
for different documents are independent, we can
solve them on separate machines in parallel. This
facilitates scaling the model to document collections
with hundreds of thousands of documents.
4 Experiments
We evaluate our model with document-level and
sentence-level categorization tasks in the domain of
online movie reviews. For document categoriza-
tion, we compare our method to previously pub-
lished results on a standard dataset, and introduce
a new dataset for the task. In both tasks we com-
pare our model?s word representations with several
bag of words weighting methods, and alternative ap-
proaches to word vector induction.
4.1 Word Representation Learning
We induce word representations with our model us-
ing 25,000 movie reviews from IMDB. Because
some movies receive substantially more reviews
than others, we limited ourselves to including at
most 30 reviews from any movie in the collection.
We build a fixed dictionary of the 5,000 most fre-
quent tokens, but ignore the 50 most frequent terms
from the original full vocabulary. Traditional stop
word removal was not used because certain stop
words (e.g. negating words) are indicative of senti-
ment. Stemming was not applied because the model
learns similar representations for words of the same
stem when the data suggests it. Additionally, be-
cause certain non-word tokens (e.g. ?!? and ?:-)? )
are indicative of sentiment, we allow them in our vo-
cabulary. Ratings on IMDB are given as star values
(? {1, 2, ..., 10}), which we linearly map to [0, 1] to
use as document labels when training our model.
The semantic component of our model does not
require document labels. We train a variant of our
model which uses 50,000 unlabeled reviews in addi-
tion to the labeled set of 25,000 reviews. The unla-
beled set of reviews contains neutral reviews as well
as those which are polarized as found in the labeled
set. Training the model with additional unlabeled
data captures a common scenario where the amount
of labeled data is small relative to the amount of un-
labeled data available. For all word vector models,
we use 50-dimensional vectors.
As a qualitative assessment of word represen-
tations, we visualize the words most similar to a
query word using vector similarity of the learned
representations. Given a query word w and an-
other word w? we obtain their vector representations
?w and ?w? , and evaluate their cosine similarity as
S(?w, ?w?) = ?
T
w?w?
||?w||?||?w? ||
. By assessing the simi-
larity of w with all other words w?, we can find the
words deemed most similar by the model.
Table 1 shows the most similar words to given
query words using our model?s word representations
as well as those of LSA. All of these vectors cap-
ture broad semantic similarities. However, both ver-
sions of our model seem to do better than LSA in
avoiding accidental distributional similarities (e.g.,
screwball and grant as similar to romantic) A com-
parison of the two versions of our model also begins
to highlight the importance of adding sentiment in-
formation. In general, words indicative of sentiment
tend to have high similarity with words of the same
sentiment polarity, so even the purely unsupervised
model?s results look promising. However, they also
show more genre and content effects. For exam-
ple, the sentiment enriched vectors for ghastly are
truly semantic alternatives to that word, whereas the
vectors without sentiment also contain some content
words that tend to have ghastly predicated of them.
Of course, this is only an impressionistic analysis of
a few cases, but it is helpful in understanding why
the sentiment-enriched model proves superior at the
sentiment classification results we report next.
4.2 Other Word Representations
For comparison, we implemented several alternative
vector space models that are conceptually similar to
our own, as discussed in section 2:
Latent Semantic Analysis (LSA; Deerwester et
al., 1990) We apply truncated SVD to a tf.idf
weighted, cosine normalized count matrix, which
is a standard weighting and smoothing scheme for
146
Our model Our model
Sentiment + Semantic Semantic only LSA
melancholy
bittersweet thoughtful poetic
heartbreaking warmth lyrical
happiness layer poetry
tenderness gentle profound
compassionate loneliness vivid
ghastly
embarrassingly predators hideous
trite hideous inept
laughably tube severely
atrocious baffled grotesque
appalling smack unsuspecting
lackluster
lame passable uninspired
laughable unconvincing flat
unimaginative amateurish bland
uninspired cliche?d forgettable
awful insipid mediocre
romantic
romance romance romance
love charming screwball
sweet delightful grant
beautiful sweet comedies
relationship chemistry comedy
Table 1: Similarity of learned word vectors. Each target word is given with its five most similar words using cosine
similarity of the vectors determined by each model. The full version of our model (left) captures both lexical similarity
as well as similarity of sentiment strength and orientation. Our unsupervised semantic component (center) and LSA
(right) capture semantic relations.
VSM induction (Turney and Pantel, 2010).
Latent Dirichlet Allocation (LDA; Blei et
al., 2003) We use the method described in sec-
tion 2 for inducing word representations from the
topic matrix. To train the 50-topic LDA model we
use code released by Blei et al (2003). We use the
same 5,000 term vocabulary for LDA as is used for
training word vector models. We leave the LDA
hyperparameters at their default values, though
some work suggests optimizing over priors for LDA
is important (Wallach et al, 2009).
Weighting Variants We evaluate both binary (b)
term frequency weighting with smoothed delta idf
(?t?) and no idf (n) because these variants worked
well in previous experiments in sentiment (Mar-
tineau and Finin, 2009; Pang et al, 2002). In all
cases, we use cosine normalization (c). Paltoglou
and Thelwall (2010) perform an extensive analysis
of such weighting variants for sentiment tasks.
4.3 Document Polarity Classification
Our first evaluation task is document-level senti-
ment polarity classification. A classifier must pre-
dict whether a given review is positive or negative
given the review text.
Given a document?s bag of words vector v, we
obtain features from our model using a matrix-
vector product Rv, where v can have arbitrary tf.idf
weighting. We do not cosine normalize v, instead
applying cosine normalization to the final feature
vector Rv. This procedure is also used to obtain
features from the LDA and LSA word vectors. In
preliminary experiments, we found ?bnn? weighting
to work best for v when generating document fea-
tures via the product Rv. In all experiments, we
use this weighting to get multi-word representations
147
Features PL04 Our Dataset Subjectivity
Bag of Words (bnc) 85.45 87.80 87.77
Bag of Words (b?t?c) 85.80 88.23 85.65
LDA 66.70 67.42 66.65
LSA 84.55 83.96 82.82
Our Semantic Only 87.10 87.30 86.65
Our Full 84.65 87.44 86.19
Our Full, Additional Unlabeled 87.05 87.99 87.22
Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58
Our Full + Bag of Words (bnc) 87.85 88.33 88.45
Our Full, Add?l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13
Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00
Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A
tf.?idf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A
Appraisal Taxonomy (Whitelaw et al, 2005) 90.20 N/A N/A
Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews
often used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB,
and the sentence subjectivity dataset alo released by (Pang and Lee, 2004). All tasks are balanced two-class problems.
from word vectors.
4.3.1 Pang and Lee Movie Review Dataset
The polarity dataset version 2.0 introduced by Pang
and Lee (2004) 1 consists of 2,000 movie reviews,
where each is associated with a binary sentiment po-
larity label. We report 10-fold cross validation re-
sults using the authors? published folds to make our
results comparable with others in the literature. We
use a linear support vector machine (SVM) classifier
trained with LIBLINEAR (Fan et al, 2008), and set
the SVM regularization parameter to the same value
used by Pang and Lee (2004).
Table 2 shows the classification performance of
our method, other VSMs we implemented, and pre-
viously reported results from the literature. Bag of
words vectors are denoted by their weighting nota-
tion. Features from word vector learner are denoted
by the learner name. As a control, we trained ver-
sions of our model with only the unsupervised se-
mantic component, and the full model (semantic and
sentiment). We also include results for a version of
our full model trained with 50,000 additional unla-
beled examples. Finally, to test whether our mod-
els? representations complement a standard bag of
words, we evaluate performance of the two feature
representations concatenated.
1http://www.cs.cornell.edu/people/pabo/movie-review-data
Our method?s features clearly outperform those of
other VSMs, and perform best when combined with
the original bag of words representation. The vari-
ant of our model trained with additional unlabeled
data performed best, suggesting the model can effec-
tively utilize large amounts of unlabeled data along
with labeled examples. Our method performs com-
petitively with previously reported results in spite of
our restriction to a vocabulary of only 5,000 words.
We extracted the movie title associated with each
review and found that 1,299 of the 2,000 reviews in
the dataset have at least one other review of the same
movie in the dataset. Of 406 movies with multiple
reviews, 249 have the same polarity label for all of
their reviews. Overall, these facts suggest that, rela-
tive to the size of the dataset, there are highly corre-
lated examples with correlated labels. This is a nat-
ural and expected property of this kind of document
collection, but it can have a substantial impact on
performance in datasets of this scale. In the random
folds distributed by the authors, approximately 50%
of reviews in each validation fold?s test set have a
review of the same movie with the same label in the
training set. Because the dataset is small, a learner
may perform well by memorizing the association be-
tween label and words unique to a particular movie
(e.g., character names or plot terms).
We introduce a substantially larger dataset, which
148
uses disjoint sets of movies for training and testing.
These steps minimize the ability of a learner to rely
on idiosyncratic word?class associations, thereby
focusing attention on genuine sentiment features.
4.3.2 IMDB Review Dataset
We constructed a collection of 50,000 reviews from
IMDB, allowing no more than 30 reviews per movie.
The constructed dataset contains an even number of
positive and negative reviews, so randomly guessing
yields 50% accuracy. Following previous work on
polarity classification, we consider only highly po-
larized reviews. A negative review has a score ? 4
out of 10, and a positive review has a score ? 7
out of 10. Neutral reviews are not included in the
dataset. In the interest of providing a benchmark for
future work in this area, we release this dataset to
the public.2
We evenly divided the dataset into training and
test sets. The training set is the same 25,000 la-
beled reviews used to induce word vectors with our
model. We evaluate classifier performance after
cross-validating classifier parameters on the training
set, again using a linear SVM in all cases. Table 2
shows classification performance on our subset of
IMDB reviews. Our model showed superior per-
formance to other approaches, and performed best
when concatenated with bag of words representa-
tion. Again the variant of our model which utilized
extra unlabeled data during training performed best.
Differences in accuracy are small, but, because
our test set contains 25,000 examples, the variance
of the performance estimate is quite low. For ex-
ample, an accuracy increase of 0.1% corresponds to
correctly classifying an additional 25 reviews.
4.4 Subjectivity Detection
As a second evaluation task, we performed sentence-
level subjectivity classification. In this task, a clas-
sifier is trained to decide whether a given sentence is
subjective, expressing the writer?s opinions, or ob-
jective, expressing purely facts. We used the dataset
of Pang and Lee (2004), which contains subjective
sentences from movie review summaries and objec-
tive sentences from movie plot summaries. This task
2Dataset and further details are available online at:
http://www.andrew-maas.net/data/sentiment
is substantially different from the review classifica-
tion task because it uses sentences as opposed to en-
tire documents and the target concept is subjectivity
instead of opinion polarity. We randomly split the
10,000 examples into 10 folds and report 10-fold
cross validation accuracy using the SVM training
protocol of Pang and Lee (2004).
Table 2 shows classification accuracies from the
sentence subjectivity experiment. Our model again
provided superior features when compared against
other VSMs. Improvement over the bag-of-words
baseline is obtained by concatenating the two feature
vectors.
5 Discussion
We presented a vector space model that learns word
representations captuing semantic and sentiment in-
formation. The model?s probabilistic foundation
gives a theoretically justified technique for word
vector induction as an alternative to the overwhelm-
ing number of matrix factorization-based techniques
commonly used. Our model is parametrized as a
log-bilinear model following recent success in us-
ing similar techniques for language models (Bengio
et al, 2003; Collobert and Weston, 2008; Mnih and
Hinton, 2007), and it is related to probabilistic latent
topic models (Blei et al, 2003; Steyvers and Grif-
fiths, 2006). We parametrize the topical component
of our model in a manner that aims to capture word
representations instead of latent topics. In our ex-
periments, our method performed better than LDA,
which models latent topics directly.
We extended the unsupervised model to incor-
porate sentiment information and showed how this
extended model can leverage the abundance of
sentiment-labeled texts available online to yield
word representations that capture both sentiment
and semantic relations. We demonstrated the util-
ity of such representations on two tasks of senti-
ment classification, using existing datasets as well
as a larger one that we release for future research.
These tasks involve relatively simple sentiment in-
formation, but the model is highly flexible in this
regard; it can be used to characterize a wide variety
of annotations, and thus is broadly applicable in the
growing areas of sentiment analysis and retrieval.
149
Acknowledgments
This work is supported by the DARPA Deep Learn-
ing program under contract number FA8650-10-C-
7020, an NSF Graduate Fellowship awarded to AM,
and ONR grant No. N00014-10-1-0109 to CP.
References
C. O. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: machine learning for text-based emotion predic-
tion. In Proceedings of HLT/EMNLP, pages 579?586.
A. Andreevskaia and S. Bergler. 2006. Mining Word-
Net for fuzzy sentiment: sentiment tag extraction from
WordNet glosses. In Proceedings of the European
ACL, pages 209?216.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
a neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155, August.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning Re-
search, 3:993?1022, May.
J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment
analysis across languages: multilingual supervised la-
tent Dirichlet alocation. In Proceedings of EMNLP,
pages 45?55.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing. In Proceedings of the
ICML, pages 160?167.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41:391?407, September.
R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and
C. J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. The Journal of Machine Learning
Research, 9:1871?1874, August.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In Proceedings of NAACL,
pages 326?334.
A. B. Goldberg and J. Zhu. 2006. Seeing stars when
there aren?t many stars: graph-based semi-supervised
learning for sentiment categorization. In TextGraphs:
HLT/NAACL Workshop on Graph-based Algorithms
for Natural Language Processing, pages 45?52.
T. Jay. 2000. Why We Curse: A Neuro-Psycho-
Social Theory of Speech. John Benjamins, Philadel-
phia/Amsterdam.
D. Kaplan. 1999. What is meaning? Explorations in the
theory of Meaning as Use. Brief version ? draft 1.
Ms., UCLA.
A. Kennedy and D. Inkpen. 2006. Sentiment clas-
sification of movie reviews using contextual valence
shifters. Computational Intelligence, 22:110?125,
May.
F. Li, M. Huang, and X. Zhu. 2010. Sentiment analysis
with global topics and local dependency. In Proceed-
ings of AAAI, pages 1371?1376.
C. Lin and Y. He. 2009. Joint sentiment/topic model for
sentiment analysis. In Proceeding of the 18th ACM
Conference on Information and Knowledge Manage-
ment, pages 375?384.
J. Martineau and T. Finin. 2009. Delta tfidf: an improved
feature space for sentiment analysis. In Proceedings
of the 3rd AAAI International Conference on Weblogs
and Social Media, pages 258?261.
A. Mnih and G. E. Hinton. 2007. Three new graphical
models for statistical language modelling. In Proceed-
ings of the ICML, pages 641?648.
G. Paltoglou and M. Thelwall. 2010. A study of informa-
tion retrieval weighting schemes for sentiment analy-
sis. In Proceedings of the ACL, pages 1386?1395.
B. Pang and L. Lee. 2004. A sentimental education:
sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
pages 271?278.
B. Pang and L. Lee. 2005. Seeing stars: exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of ACL, pages 115?
124.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learning
techniques. In Proceedings of EMNLP, pages 79?86.
C. Potts. 2007. The expressive dimension. Theoretical
Linguistics, 33:165?197.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings of
NAACL, pages 300?307.
M. Steyvers and T. L. Griffiths. 2006. Probabilistic topic
models. In T. Landauer, D McNamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road
to Meaning.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proceedings of the ACL, page
384394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
H. Wallach, D. Mimno, and A. McCallum. 2009. Re-
thinking LDA: why priors matter. In Proceedings of
NIPS, pages 1973?1981.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proceedings
of CIKM, pages 625?631.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of AAAI, pages 761?769.
150
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 873?882,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improving Word Representations via Global Context
and Multiple Word Prototypes
Eric H. Huang, Richard Socher?, Christopher D. Manning, Andrew Y. Ng
Computer Science Department, Stanford University, Stanford, CA 94305, USA
{ehhuang,manning,ang}@stanford.edu, ?richard@socher.org
Abstract
Unsupervised word representations are very
useful in NLP tasks both as inputs to learning
algorithms and as extra word features in NLP
systems. However, most of these models are
built with only local context and one represen-
tation per word. This is problematic because
words are often polysemous and global con-
text can also provide useful information for
learning word meanings. We present a new
neural network architecture which 1) learns
word embeddings that better capture the se-
mantics of words by incorporating both local
and global document context, and 2) accounts
for homonymy and polysemy by learning mul-
tiple embeddings per word. We introduce a
new dataset with human judgments on pairs of
words in sentential context, and evaluate our
model on it, showing that our model outper-
forms competitive baselines and other neural
language models. 1
1 Introduction
Vector-space models (VSM) represent word mean-
ings with vectors that capture semantic and syntac-
tic information of words. These representations can
be used to induce similarity measures by computing
distances between the vectors, leading to many use-
ful applications, such as information retrieval (Man-
ning et al, 2008), document classification (Sebas-
tiani, 2002) and question answering (Tellex et al,
2003).
1The dataset and word vectors can be downloaded at
http://ai.stanford.edu/?ehhuang/.
Despite their usefulness, most VSMs share a
common problem that each word is only repre-
sented with one vector, which clearly fails to capture
homonymy and polysemy. Reisinger and Mooney
(2010b) introduced a multi-prototype VSM where
word sense discrimination is first applied by clus-
tering contexts, and then prototypes are built using
the contexts of the sense-labeled words. However, in
order to cluster accurately, it is important to capture
both the syntax and semantics of words. While many
approaches use local contexts to disambiguate word
meaning, global contexts can also provide useful
topical information (Ng and Zelle, 1997). Several
studies in psychology have also shown that global
context can help language comprehension (Hess et
al., 1995) and acquisition (Li et al, 2000).
We introduce a new neural-network-based lan-
guage model that distinguishes and uses both local
and global context via a joint training objective. The
model learns word representations that better cap-
ture the semantics of words, while still keeping syn-
tactic information. These improved representations
can be used to represent contexts for clustering word
instances, which is used in the multi-prototype ver-
sion of our model that accounts for words with mul-
tiple senses.
We evaluate our new model on the standard
WordSim-353 (Finkelstein et al, 2001) dataset that
includes human similarity judgments on pairs of
words, showing that combining both local and
global context outperforms using only local or
global context alone, and is competitive with state-
of-the-art methods. However, one limitation of this
evaluation is that the human judgments are on pairs
873
Global ContextLocal Context
scorel scoreg
Document
he walks to the bank... ...
sum
score
river
water
shore
global semantic vector
?
play
weighted average
Figure 1: An overview of our neural language model. The model makes use of both local and global context to compute
a score that should be large for the actual next word (bank in the example), compared to the score for other words.
When word meaning is still ambiguous given local context, information in global context can help disambiguation.
of words presented in isolation, ignoring meaning
variations in context. Since word interpretation in
context is important especially for homonymous and
polysemous words, we introduce a new dataset with
human judgments on similarity between pairs of
words in sentential context. To capture interesting
word pairs, we sample different senses of words us-
ing WordNet (Miller, 1995). The dataset includes
verbs and adjectives, in addition to nouns. We show
that our multi-prototype model improves upon the
single-prototype version and outperforms other neu-
ral language models and baselines on this dataset.
2 Global Context-Aware Neural Language
Model
In this section, we describe the training objective of
our model, followed by a description of the neural
network architecture, ending with a brief description
of our model?s training method.
2.1 Training Objective
Our model jointly learns word representations while
learning to discriminate the next word given a short
word sequence (local context) and the document
(global context) in which the word sequence occurs.
Because our goal is to learn useful word representa-
tions and not the probability of the next word given
previous words (which prohibits looking ahead), our
model can utilize the entire document to provide
global context.
Given a word sequence s and document d in
which the sequence occurs, our goal is to discrim-
inate the correct last word in s from other random
words. We compute scores g(s, d) and g(sw, d)
where sw is swith the last word replaced by wordw,
and g(?, ?) is the scoring function that represents the
neural networks used. We want g(s, d) to be larger
than g(sw, d) by a margin of 1, for any other word
w in the vocabulary, which corresponds to the train-
ing objective of minimizing the ranking loss for each
(s, d) found in the corpus:
Cs,d =
?
w?V
max(0, 1? g(s, d) + g(sw, d)) (1)
Collobert and Weston (2008) showed that this rank-
ing approach can produce good word embeddings
that are useful in several NLP tasks, and allows
much faster training of the model compared to op-
timizing log-likelihood of the next word.
2.2 Neural Network Architecture
We define two scoring components that contribute
to the final score of a (word sequence, document)
pair. The scoring components are computed by two
neural networks, one capturing local context and the
other global context, as shown in Figure 1. We now
describe how each scoring component is computed.
The score of local context uses the local word se-
quence s. We first represent the word sequence s as
874
an ordered list of vectors x = (x1, x2, ..., xm) where
xi is the embedding of word i in the sequence, which
is a column in the embedding matrix L ? Rn?|V |
where |V | denotes the size of the vocabulary. The
columns of this embedding matrix L are the word
vectors and will be learned and updated during train-
ing. To compute the score of local context, scorel,
we use a neural network with one hidden layer:
a1 = f(W1[x1;x2; ...;xm] + b1) (2)
scorel = W2a1 + b2 (3)
where [x1;x2; ...;xm] is the concatenation of the
m word embeddings representing sequence s, f is
an element-wise activation function such as tanh,
a1 ? Rh?1 is the activation of the hidden layer with
h hidden nodes, W1 ? Rh?(mn) and W2 ? R1?h
are respectively the first and second layer weights of
the neural network, and b1, b2 are the biases of each
layer.
For the score of the global context, we represent
the document also as an ordered list of word em-
beddings, d = (d1, d2, ..., dk). We first compute the
weighted average of all word vectors in the docu-
ment:
c =
?k
i=1w(ti)di
?k
i=1w(ti)
(4)
where w(?) can be any weighting function that cap-
tures the importance of word ti in the document. We
use idf-weighting as the weighting function.
We use a two-layer neural network to compute the
global context score, scoreg, similar to the above:
a1
(g) = f(W (g)1 [c;xm] + b
(g)
1 ) (5)
scoreg = W
(g)
2 a
(g)
1 + b
(g)
2 (6)
where [c;xm] is the concatenation of the weighted
average document vector and the vector of the last
word in s, a1(g) ? Rh
(g)?1 is the activation of
the hidden layer with h(g) hidden nodes, W (g)1 ?
Rh
(g)?(2n) and W (g)2 ? R
1?h(g) are respectively the
first and second layer weights of the neural network,
and b(g)1 , b
(g)
2 are the biases of each layer. Note that
instead of using the document where the sequence
occurs, we can also specify a fixed k > m that cap-
tures larger context.
The final score is the sum of the two scores:
score = scorel + scoreg (7)
The local score preserves word order and syntactic
information, while the global score uses a weighted
average which is similar to bag-of-words features,
capturing more of the semantics and topics of the
document. Note that Collobert and Weston (2008)?s
language model corresponds to the network using
only local context.
2.3 Learning
Following Collobert and Weston (2008), we sample
the gradient of the objective by randomly choosing
a word from the dictionary as a corrupt example for
each sequence-document pair, (s, d), and take the
derivative of the ranking loss with respect to the pa-
rameters: weights of the neural network and the em-
bedding matrix L. These weights are updated via
backpropagation. The embedding matrix L is the
word representations. We found that word embed-
dings move to good positions in the vector space
faster when using mini-batch L-BFGS (Liu and No-
cedal, 1989) with 1000 pairs of good and corrupt ex-
amples per batch for training, compared to stochas-
tic gradient descent.
3 Multi-Prototype Neural Language
Model
Despite distributional similarity models? successful
applications in various NLP tasks, one major limi-
tation common to most of these models is that they
assume only one representation for each word. This
single-prototype representation is problematic be-
cause many words have multiple meanings, which
can be wildly different. Using one representa-
tion simply cannot capture the different meanings.
Moreover, using all contexts of a homonymous or
polysemous word to build a single prototype could
hurt the representation, which cannot represent any
one of the meanings well as it is influenced by all
meanings of the word.
Instead of using only one representation per word,
Reisinger and Mooney (2010b) proposed the multi-
prototype approach for vector-space models, which
uses multiple representations to capture different
senses and usages of a word. We show how our
875
model can readily adopt the multi-prototype ap-
proach. We present a way to use our learned
single-prototype embeddings to represent each con-
text window, which can then be used by clustering to
perform word sense discrimination (Schu?tze, 1998).
In order to learn multiple prototypes, we first
gather the fixed-sized context windows of all occur-
rences of a word (we use 5 words before and after
the word occurrence). Each context is represented
by a weighted average of the context words? vectors,
where again, we use idf-weighting as the weighting
function, similar to the document context represen-
tation described in Section 2.2. We then use spheri-
cal k-means to cluster these context representations,
which has been shown to model semantic relations
well (Dhillon and Modha, 2001). Finally, each word
occurrence in the corpus is re-labeled to its associ-
ated cluster and is used to train the word representa-
tion for that cluster.
Similarity between a pair of words (w,w?) us-
ing the multi-prototype approach can be computed
with or without context, as defined by Reisinger and
Mooney (2010b):
AvgSimC(w,w?) =
1
K2
k?
i=1
k?
j=1
p(c, w, i)p(c?, w?, j)d(?i(w), ?j(w
?))
(8)
where p(c, w, i) is the likelihood that word w is in
its cluster i given context c, ?i(w) is the vector rep-
resenting the i-th cluster centroid of w, and d(v, v?)
is a function computing similarity between two vec-
tors, which can be any of the distance functions pre-
sented by Curran (2004). The similarity measure can
be computed in absence of context by assuming uni-
form p(c, w, i) over i.
4 Experiments
In this section, we first present a qualitative analysis
comparing the nearest neighbors of our model?s em-
beddings with those of others, showing our embed-
dings better capture the semantics of words, with the
use of global context. Our model also improves the
correlation with human judgments on a word simi-
larity task. Because word interpretation in context is
important, we introduce a new dataset with human
judgments on similarity of pairs of words in senten-
tial context. Finally, we show that our model outper-
forms other methods on this dataset and also that the
multi-prototype approach improves over the single-
prototype approach.
We chose Wikipedia as the corpus to train all
models because of its wide range of topics and
word usages, and its clean organization of docu-
ment by topic. We used the April 2010 snapshot of
the Wikipedia corpus (Shaoul and Westbury, 2010),
with a total of about 2 million articles and 990 mil-
lion tokens. We use a dictionary of the 30,000 most
frequent words in Wikipedia, converted to lower
case. In preprocessing, we keep the frequent num-
bers intact and replace each digit of the uncommon
numbers to ?DG? so as to preserve information such
as it being a year (e.g. ?DGDGDGDG?). The con-
verted numbers that are rare are mapped to a NUM-
BER token. Other rare words not in the dictionary
are mapped to an UNKNOWN token.
For all experiments, our models use 50-
dimensional embeddings. We use 10-word windows
of text as the local context, 100 hidden units, and no
weight regularization for both neural networks. For
multi-prototype variants, we fix the number of pro-
totypes to be 10.
4.1 Qualitative Evaluations
In order to show that our model learns more seman-
tic word representations with global context, we give
the nearest neighbors of our single-prototype model
versus C&W?s, which only uses local context. The
nearest neighbors of a word are computed by com-
paring the cosine similarity between the center word
and all other words in the dictionary. Table 1 shows
the nearest neighbors of some words. The nearest
neighbors of ?market? that C&W?s embeddings give
are more constrained by the syntactic constraint that
words in plural form are only close to other words
in plural form, whereas our model captures that the
singular and plural forms of a word are similar in
meaning. Other examples show that our model in-
duces nearest neighbors that better capture seman-
tics.
Table 2 shows the nearest neighbors of our model
using the multi-prototype approach. We see that
the clustering is able to group contexts of different
876
Center
Word
C&W Our Model
markets firms, industries,
stores
market, firms,
businesses
American Australian,
Indian, Italian
U.S., Canadian,
African
illegal alleged, overseas,
banned
harmful, prohib-
ited, convicted
Table 1: Nearest neighbors of words based on cosine sim-
ilarity. Our model is less constrained by syntax and is
more semantic.
Center Word Nearest Neighbors
bank 1 corporation, insurance, company
bank 2 shore, coast, direction
star 1 movie, film, radio
star 2 galaxy, planet, moon
cell 1 telephone, smart, phone
cell 2 pathology, molecular, physiology
left 1 close, leave, live
left 2 top, round, right
Table 2: Nearest neighbors of word embeddings learned
by our model using the multi-prototype approach based
on cosine similarity. The clustering is able to find the dif-
ferent meanings, usages, and parts of speech of the words.
meanings of a word into separate groups, allowing
our model to learn multiple meaningful representa-
tions of a word.
4.2 WordSim-353
A standard dataset for evaluating vector-space mod-
els is the WordSim-353 dataset (Finkelstein et al,
2001), which consists of 353 pairs of nouns. Each
pair is presented without context and associated with
13 to 16 human judgments on similarity and re-
latedness on a scale from 0 to 10. For example,
(cup,drink) received an average score of 7.25, while
(cup,substance) received an average score of 1.92.
Table 3 shows our results compared to previous
methods, including C&W?s language model and the
hierarchical log-bilinear (HLBL) model (Mnih and
Hinton, 2008), which is a probabilistic, linear neu-
ral model. We downloaded these embeddings from
Turian et al (2010). These embeddings were trained
on the smaller corpus RCV1 that contains one year
of Reuters English newswire, and show similar cor-
relations on the dataset. We report the result of
Model Corpus ?? 100
Our Model-g Wiki. 22.8
C&W RCV1 29.5
HLBL RCV1 33.2
C&W* Wiki. 49.8
C&W Wiki. 55.3
Our Model Wiki. 64.2
Our Model* Wiki. 71.3
Pruned tf-idf Wiki. 73.4
ESA Wiki. 75
Tiered Pruned tf-idf Wiki. 76.9
Table 3: Spearman?s ? correlation on WordSim-353,
showing our model?s improvement over previous neural
models for learning word embeddings. C&W* is the
word embeddings trained and provided by C&W. Our
Model* is trained without stop words, while Our Model-
g uses only global context. Pruned tf-idf (Reisinger and
Mooney, 2010b) and ESA (Gabrilovich and Markovitch,
2007) are also included.
our re-implementation of C&W?s model trained on
Wikipedia, showing the large effect of using a dif-
ferent corpus.
Our model is able to learn more semantic word
embeddings and noticeably improves upon C&W?s
model. Note that our model achieves higher corre-
lation (64.2) than either using local context alone
(C&W: 55.3) or using global context alone (Our
Model-g: 22.8). We also found that correlation can
be further improved by removing stop words (71.3).
Thus, each window of text (training example) con-
tains more information but still preserves some syn-
tactic information as the words are still ordered in
the local context.
4.3 New Dataset: Word Similarity in Context
The many previous datasets that associate human
judgments on similarity between pairs of words,
such as WordSim-353, MC (Miller and Charles,
1991) and RG (Rubenstein and Goodenough, 1965),
have helped to advance the development of vector-
space models. However, common to all datasets is
that similarity scores are given to pairs of words in
isolation. This is problematic because the mean-
ings of homonymous and polysemous words depend
highly on the words? contexts. For example, in the
two phrases, ?he swings the baseball bat? and ?the
877
Word 1 Word 2
Located downtown along the east bank of the Des
Moines River ...
This is the basis of all money laundering , a track record
of depositing clean money before slipping through dirty
money ...
Inside the ruins , there are bats and a bowl with Pokeys
that fills with sand over the course of the race , and the
music changes somewhat while inside ...
An aggressive lower order batsman who usually bats at
No. 11 , Muralitharan is known for his tendency to back
away to leg and slog ...
An example of legacy left in the Mideast from these
nobles is the Krak des Chevaliers ? enlargement by the
Counts of Tripoli and Toulouse ...
... one should not adhere to a particular explanation ,
only in such measure as to be ready to abandon it if it
be proved with certainty to be false ...
... and Andy ?s getting ready to pack his bags and head
up to Los Angeles tomorrow to get ready to fly back
home on Thursday
... she encounters Ben ( Duane Jones ) , who arrives
in a pickup truck and defends the house against another
pack of zombies ...
In practice , there is an unknown phase delay between
the transmitter and receiver that must be compensated
by ? synchronization ? of the receivers local oscillator
... but Gilbert did not believe that she was dedicated
enough , and when she missed a rehearsal , she was
dismissed ...
Table 4: Example pairs from our new dataset. Note that words in a pair can be the same word and have different parts
of speech.
bat flies?, bat has completely different meanings. It
is unclear how this variation in meaning is accounted
for in human judgments of words presented without
context.
One of the main contributions of this paper is the
creation of a new dataset that addresses this issue.
The dataset has three interesting characteristics: 1)
human judgments are on pairs of words presented in
sentential context, 2) word pairs and their contexts
are chosen to reflect interesting variations in mean-
ings of homonymous and polysemous words, and 3)
verbs and adjectives are present in addition to nouns.
We now describe our methodology in constructing
the dataset.
4.3.1 Dataset Construction
Our procedure of constructing the dataset consists
of three steps: 1) select a list a words, 2) for each
word, select another word to form a pair, 3) for each
word in a pair, find a sentential context. We now
describe each step in detail.
In step 1, in order to make sure we select a diverse
list of words, we consider three attributes of a word:
frequency in a corpus, number of parts of speech,
and number of synsets according to WordNet. For
frequency, we divide words into three groups, top
2,000 most frequent, between 2,000 and 5,000, and
between 5,000 to 10,000 based on occurrences in
Wikipedia. For number of parts of speech, we group
words based on their number of possible parts of
speech (noun, verb or adjective), from 1 to 3. We
also group words by their number of synsets: [0,5],
[6,10], [11, 20], and [20, max]. Finally, we sam-
ple at most 15 words from each combination in the
Cartesian product of the above groupings.
In step 2, for each of the words selected in step
1, we want to choose the other word so that the pair
captures an interesting relationship. Similar to Man-
andhar et al (2010), we use WordNet to first ran-
domly select one synset of the first word, we then
construct a set of words in various relations to the
first word?s chosen synset, including hypernyms, hy-
ponyms, holonyms, meronyms and attributes. We
randomly select a word from this set of words as the
second word in the pair. We try to repeat the above
twice to generate two pairs for each word. In addi-
tion, for words with more than five synsets, we allow
the second word to be the same as the first, but with
different synsets. We end up with pairs of words as
well as the one chosen synset for each word in the
pairs.
In step 3, we aim to extract a sentence from
Wikipedia for each word, which contains the word
and corresponds to a usage of the chosen synset.
We first find all sentences in which the word oc-
curs. We then POS tag2 these sentences and filter out
those that do not match the chosen POS. To find the
2We used the MaxEnt Treebank POS tagger in the python
nltk library.
878
Model ?? 100
C&W-S 57.0
Our Model-S 58.6
Our Model-M AvgSim 62.8
Our Model-M AvgSimC 65.7
tf-idf-S 26.3
Pruned tf-idf-S 62.5
Pruned tf-idf-M AvgSim 60.4
Pruned tf-idf-M AvgSimC 60.5
Table 5: Spearman?s ? correlation on our new
dataset. Our Model-S uses the single-prototype approach,
while Our Model-M uses the multi-prototype approach.
AvgSim calculates similarity with each prototype con-
tributing equally, while AvgSimC weighs the prototypes
according to probability of the word belonging to that
prototype?s cluster.
word usages that correspond to the chosen synset,
we first construct a set of related words of the chosen
synset, including hypernyms, hyponyms, holonyms,
meronyms and attributes. Using this set of related
words, we filter out a sentence if the document in
which the sentence appears does not include one of
the related words. Finally, we randomly select one
sentence from those that are left.
Table 4 shows some examples from the dataset.
Note that the dataset alo includes pairs of the same
word. Single-prototype models would give the max
similarity score for those pairs, which can be prob-
lematic depending on the words? contexts. This
dataset requires models to examine context when de-
termining word meaning.
Using Amazon Mechanical Turk, we collected 10
human similarity ratings for each pair, as Snow et
al. (2008) found that 10 non-expert annotators can
achieve very close inter-annotator agreement with
expert raters. To ensure worker quality, we only
allowed workers with over 95% approval rate to
work on our task. Furthermore, we discarded all
ratings by a worker if he/she entered scores out of
the accepted range or missed a rating, signaling low-
quality work.
We obtained a total of 2,003 word pairs and their
sentential contexts. The word pairs consist of 1,712
unique words. Of the 2,003 word pairs, 1328 are
noun-noun pairs, 399 verb-verb, 140 verb-noun, 97
adjective-adjective, 30 noun-adjective, and 9 verb-
adjective. 241 pairs are same-word pairs.
4.3.2 Evaluations on Word Similarity in
Context
For evaluation, we also compute Spearman corre-
lation between a model?s computed similarity scores
and human judgments. Table 5 compares different
models? results on this dataset. We compare against
the following baselines: tf-idf represents words in
a word-word matrix capturing co-occurrence counts
in all 10-word context windows. Reisinger and
Mooney (2010b) found pruning the low-value tf-idf
features helps performance. We report the result
of this pruning technique after tuning the thresh-
old value on this dataset, removing all but the top
200 features in each word vector. We tried the
same multi-prototype approach and used spherical
k-means3 to cluster the contexts using tf-idf repre-
sentations, but obtained lower numbers than single-
prototype (55.4 with AvgSimC). We then tried using
pruned tf-idf representations on contexts with our
clustering assignments (included in Table 5), but still
got results worse than the single-prototype version
of the pruned tf-idf model (60.5 with AvgSimC).
This suggests that the pruned tf-idf representations
might be more susceptible to noise or mistakes in
context clustering.
By utilizing global context, our model outper-
forms C&W?s vectors and the above baselines on
this dataset. With multiple representations per
word, we show that the multi-prototype approach
can improve over the single-prototype version with-
out using context (62.8 vs. 58.6). Moreover, using
AvgSimC4 which takes contexts into account, the
multi-prototype model obtains the best performance
(65.7).
5 Related Work
Neural language models (Bengio et al, 2003; Mnih
and Hinton, 2007; Collobert and Weston, 2008;
Schwenk and Gauvain, 2002; Emami et al, 2003)
have been shown to be very powerful at language
modeling, a task where models are asked to ac-
curately predict the next word given previously
seen words. By using distributed representations of
3We first tried movMF as in Reisinger and Mooney (2010b),
but were unable to get decent results (only 31.5).
4probability of being in a cluster is calculated as the inverse
of the distance to the cluster centroid.
879
words which model words? similarity, this type of
models addresses the data sparseness problem that
n-gram models encounter when large contexts are
used. Most of these models used relative local con-
texts of between 2 to 10 words. Schwenk and Gau-
vain (2002) tried to incorporate larger context by
combining partial parses of past word sequences and
a neural language model. They used up to 3 previ-
ous head words and showed increased performance
on language modeling. Our model uses a similar
neural network architecture as these models and uses
the ranking-loss training objective proposed by Col-
lobert and Weston (2008), but introduces a new way
to combine local and global context to train word
embeddings.
Besides language modeling, word embeddings in-
duced by neural language models have been use-
ful in chunking, NER (Turian et al, 2010), parsing
(Socher et al, 2011b), sentiment analysis (Socher et
al., 2011c) and paraphrase detection (Socher et al,
2011a). However, they have not been directly eval-
uated on word similarity tasks, which are important
for tasks such as information retrieval and summa-
rization. Our experiments show that our word em-
beddings are competitive in word similarity tasks.
Most of the previous vector-space models use a
single vector to represent a word even though many
words have multiple meanings. The multi-prototype
approach has been widely studied in models of cat-
egorization in psychology (Rosseel, 2002; Griffiths
et al, 2009), while Schu?tze (1998) used clustering
of contexts to perform word sense discrimination.
Reisinger and Mooney (2010b) combined the two
approaches and applied them to vector-space mod-
els, which was further improved in Reisinger and
Mooney (2010a). Two other recent papers (Dhillon
et al, 2011; Reddy et al, 2011) present models
for constructing word representations that deal with
context. It would be interesting to evaluate those
models on our new dataset.
Many datasets with human similarity ratings on
pairs of words, such as WordSim-353 (Finkelstein
et al, 2001), MC (Miller and Charles, 1991) and
RG (Rubenstein and Goodenough, 1965), have been
widely used to evaluate vector-space models. Moti-
vated to evaluate composition models, Mitchell and
Lapata (2008) introduced a dataset where an intran-
sitive verb, presented with a subject noun, is com-
pared to another verb chosen to be either similar or
dissimilar to the intransitive verb in context. The
context is short, with only one word, and only verbs
are compared. Erk and Pado? (2008), Thater et al
(2011) and Dinu and Lapata (2010) evaluated word
similarity in context with a modified task where sys-
tems are to rerank gold-standard paraphrase candi-
dates given the SemEval 2007 Lexical Substitution
Task dataset. This task only indirectly evaluates sim-
ilarity as only reranking of already similar words are
evaluated.
6 Conclusion
We presented a new neural network architecture that
learns more semantic word representations by us-
ing both local and global context in learning. These
learned word embeddings can be used to represent
word contexts as low-dimensional weighted average
vectors, which are then clustered to form different
meaning groups and used to learn multi-prototype
vectors. We introduced a new dataset with human
judgments on similarity between pairs of words in
context, so as to evaluate model?s abilities to capture
homonymy and polysemy of words in context. Our
new multi-prototype neural language model outper-
forms previous neural models and competitive base-
lines on this new dataset.
Acknowledgments
The authors gratefully acknowledges the support of
the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181, and the DARPA Deep
Learning program under contract number FA8650-
10-C-7020. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of DARPA, AFRL, or the US government.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent,
Christian Jauvin, Jaz K, Thomas Hofmann, Tomaso
Poggio, and John Shawe-taylor. 2003. A neural prob-
abilistic language model. Journal of Machine Learn-
ing Research, 3:1137?1155.
880
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, ICML ?08, pages 160?167, New York, NY, USA.
ACM.
James Richard Curran. 2004. From distributional to se-
mantic similarity. Technical report.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42:143?175, January.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems (NIPS), volume 24.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 1162?1172,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmad Emami, Peng Xu, and Frederick Jelinek. 2003.
Using a connectionist model in a syntactical based lan-
guage model. In Acoustics, Speech, and Signal Pro-
cessing, pages 372?375.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. In Proceedings of the 10th international
conference on World Wide Web, WWW ?01, pages
406?414, New York, NY, USA. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Thomas L Griffiths, Kevin R Canini, Adam N Sanborn,
and Daniel J Navarro. 2009. Unifying rational models
of categorization via the hierarchical dirichlet process.
Brain, page 323328.
David J Hess, Donald J Foss, and Patrick Carroll. 1995.
Effects of global and local context on lexical process-
ing during language comprehension. Journal of Ex-
perimental Psychology: General, 124(1):62?82.
Ping Li, Curt Burgess, and Kevin Lund. 2000. The ac-
quisition of word meaning through global lexical co-
occurrences.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528, December.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach,
and Sameer S Pradhan. 2010. Semeval-2010 task
14: Word sense induction & disambiguation. Word
Journal Of The International Linguistic Association,
(July):63?68.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
George A Miller and Walter G Charles. 1991. Contextual
correlates of semantic similarity. Language & Cogni-
tive Processes, 6(1):1?28.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings of
ACL-08: HLT, pages 236?244.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of the 24th international conference on
Machine learning, ICML ?07, pages 641?648, New
York, NY, USA. ACM.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In In NIPS.
Ht Ng and J Zelle. 1997. Corpus-based approaches to
semantic interpretation in natural language processing.
AI Magazine, 18(4):45?64.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static proto-
type vectors for semantic composition. In Proceedings
of 5th International Joint Conference on Natural Lan-
guage Processing, pages 705?713, Chiang Mai, Thai-
land, November. Asian Federation of Natural Lan-
guage Processing.
Joseph Reisinger and Raymond Mooney. 2010a. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 1173?1182, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joseph Reisinger and Raymond J. Mooney. 2010b.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 109?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yves Rosseel. 2002. Mixture models of categorization.
Journal of Mathematical Psychology, 46:178?210.
881
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627?633, October.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
Holger Schwenk and Jean-luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary con-
tinuous speech recognition. In In International Con-
ference on Acoustics, Speech and Signal Processing,
pages 765?768.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Comput. Surv., 34:1?
47, March.
Cyrus Shaoul and Chris Westbury. 2010. The westbury
lab wikipedia corpus.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 254?263, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning (ICML).
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-
des, and Gregory Marton. 2003. Quantitative evalu-
ation of passage retrieval algorithms for question an-
swering. In Proceedings of the 26th Annual Interna-
tional ACM SIGIR Conference on Search and Devel-
opment in Information Retrieval, pages 41?47. ACM
Press.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: a simple and effec-
tive vector model. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, IJCNLP ?11.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 384?394, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
882
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455?465,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing with Compositional Vector Grammars
Richard Socher John Bauer Christopher D. Manning Andrew Y. Ng
Computer Science Department, Stanford University, Stanford, CA 94305, USA
richard@socher.org, horatio@gmail.com, manning@stanford.edu, ang@cs.stanford.edu
Abstract
Natural language parsing has typically
been done with small sets of discrete cat-
egories such as NP and VP, but this rep-
resentation does not capture the full syn-
tactic nor semantic richness of linguistic
phrases, and attempts to improve on this
by lexicalizing phrases or splitting cate-
gories only partly address the problem at
the cost of huge feature spaces and sparse-
ness. Instead, we introduce a Compo-
sitional Vector Grammar (CVG), which
combines PCFGs with a syntactically un-
tied recursive neural network that learns
syntactico-semantic, compositional vector
representations. The CVG improves the
PCFG of the Stanford Parser by 3.8% to
obtain an F1 score of 90.4%. It is fast
to train and implemented approximately as
an efficient reranker it is about 20% faster
than the current Stanford factored parser.
The CVG learns a soft notion of head
words and improves performance on the
types of ambiguities that require semantic
information such as PP attachments.
1 Introduction
Syntactic parsing is a central task in natural lan-
guage processing because of its importance in me-
diating between linguistic expression and mean-
ing. For example, much work has shown the use-
fulness of syntactic representations for subsequent
tasks such as relation extraction, semantic role la-
beling (Gildea and Palmer, 2002) and paraphrase
detection (Callison-Burch, 2008).
Syntactic descriptions standardly use coarse
discrete categories such as NP for noun phrases
or PP for prepositional phrases. However, recent
work has shown that parsing results can be greatly
improved by defining more fine-grained syntactic
(riding,V,       )    (a,Det,       )        (bike,NN,       )
(a bike,NP,       )
(riding a bike,VP,       )
Discrete Syntactic ? Continuous Semantic 
Representations in the Compositional Vector Grammar
Figure 1: Example of a CVG tree with (cate-
gory,vector) representations at each node. The
vectors for nonterminals are computed via a new
type of recursive neural network which is condi-
tioned on syntactic categories from a PCFG.
categories, which better capture phrases with simi-
lar behavior, whether through manual feature engi-
neering (Klein and Manning, 2003a) or automatic
learning (Petrov et al, 2006). However, subdi-
viding a category like NP into 30 or 60 subcate-
gories can only provide a very limited represen-
tation of phrase meaning and semantic similarity.
Two strands of work therefore attempt to go fur-
ther. First, recent work in discriminative parsing
has shown gains from careful engineering of fea-
tures (Taskar et al, 2004; Finkel et al, 2008). Fea-
tures in such parsers can be seen as defining effec-
tive dimensions of similarity between categories.
Second, lexicalized parsers (Collins, 2003; Char-
niak, 2000) associate each category with a lexical
item. This gives a fine-grained notion of semantic
similarity, which is useful for tackling problems
like ambiguous attachment decisions. However,
this approach necessitates complex shrinkage esti-
mation schemes to deal with the sparsity of obser-
vations of the lexicalized categories.
In many natural language systems, single words
and n-grams are usefully described by their distri-
butional similarities (Brown et al, 1992), among
many others. But, even with large corpora, many
455
n-grams will never be seen during training, espe-
cially when n is large. In these cases, one cannot
simply use distributional similarities to represent
unseen phrases. In this work, we present a new so-
lution to learn features and phrase representations
even for very long, unseen n-grams.
We introduce a Compositional Vector Grammar
Parser (CVG) for structure prediction. Like the
above work on parsing, the model addresses the
problem of representing phrases and categories.
Unlike them, it jointly learns how to parse and how
to represent phrases as both discrete categories and
continuous vectors as illustrated in Fig. 1. CVGs
combine the advantages of standard probabilistic
context free grammars (PCFG) with those of re-
cursive neural networks (RNNs). The former can
capture the discrete categorization of phrases into
NP or PP while the latter can capture fine-grained
syntactic and compositional-semantic information
on phrases and words. This information can help
in cases where syntactic ambiguity can only be re-
solved with semantic information, such as in the
PP attachment of the two sentences: They ate udon
with forks. vs. They ate udon with chicken.
Previous RNN-based parsers used the same
(tied) weights at all nodes to compute the vector
representing a constituent (Socher et al, 2011b).
This requires the composition function to be ex-
tremely powerful, since it has to combine phrases
with different syntactic head words, and it is hard
to optimize since the parameters form a very deep
neural network. We generalize the fully tied RNN
to one with syntactically untied weights. The
weights at each node are conditionally dependent
on the categories of the child constituents. This
allows different composition functions when com-
bining different types of phrases and is shown to
result in a large improvement in parsing accuracy.
Our compositional distributed representation al-
lows a CVG parser to make accurate parsing de-
cisions and capture similarities between phrases
and sentences. Any PCFG-based parser can be im-
proved with an RNN. We use a simplified version
of the Stanford Parser (Klein and Manning, 2003a)
as the base PCFG and improve its accuracy from
86.56 to 90.44% labeled F1 on all sentences of the
WSJ section 23. The code of our parser is avail-
able at nlp.stanford.edu.
2 Related Work
The CVG is inspired by two lines of research:
Enriching PCFG parsers through more diverse
sets of discrete states and recursive deep learning
models that jointly learn classifiers and continuous
feature representations for variable-sized inputs.
Improving Discrete Syntactic Representations
As mentioned in the introduction, there are several
approaches to improving discrete representations
for parsing. Klein and Manning (2003a) use
manual feature engineering, while Petrov et
al. (2006) use a learning algorithm that splits
and merges the syntactic categories in order
to maximize likelihood on the treebank. Their
approach splits categories into several dozen
subcategories. Another approach is lexicalized
parsers (Collins, 2003; Charniak, 2000) that
describe each category with a lexical item, usually
the head word. More recently, Hall and Klein
(2012) combine several such annotation schemes
in a factored parser. We extend the above ideas
from discrete representations to richer continuous
ones. The CVG can be seen as factoring discrete
and continuous parsing in one model. Another
different approach to the above generative models
is to learn discriminative parsers using many well
designed features (Taskar et al, 2004; Finkel et
al., 2008). We also borrow ideas from this line of
research in that our parser combines the generative
PCFG model with discriminatively learned RNNs.
Deep Learning and Recursive Deep Learning
Early attempts at using neural networks to de-
scribe phrases include Elman (1991), who used re-
current neural networks to create representations
of sentences from a simple toy grammar and to
analyze the linguistic expressiveness of the re-
sulting representations. Words were represented
as one-on vectors, which was feasible since the
grammar only included a handful of words. Col-
lobert and Weston (2008) showed that neural net-
works can perform well on sequence labeling lan-
guage processing tasks while also learning appro-
priate features. However, their model is lacking
in that it cannot represent the recursive structure
inherent in natural language. They partially cir-
cumvent this problem by using either independent
window-based classifiers or a convolutional layer.
RNN-specific training was introduced by Goller
and Ku?chler (1996) to learn distributed represen-
tations of given, structured objects such as logi-
cal terms. In contrast, our model both predicts the
structure and its representation.
456
Henderson (2003) was the first to show that neu-
ral networks can be successfully used for large
scale parsing. He introduced a left-corner parser to
estimate the probabilities of parsing decisions con-
ditioned on the parsing history. The input to Hen-
derson?s model consists of pairs of frequent words
and their part-of-speech (POS) tags. Both the orig-
inal parsing system and its probabilistic interpre-
tation (Titov and Henderson, 2007) learn features
that represent the parsing history and do not pro-
vide a principled linguistic representation like our
phrase representations. Other related work in-
cludes (Henderson, 2004), who discriminatively
trains a parser based on synchrony networks and
(Titov and Henderson, 2006), who use an SVM to
adapt a generative parser to different domains.
Costa et al (2003) apply recursive neural net-
works to re-rank possible phrase attachments in
an incremental parser. Their work is the first to
show that RNNs can capture enough information
to make correct parsing decisions, but they only
test on a subset of 2000 sentences. Menchetti et
al. (2005) use RNNs to re-rank different parses.
For their results on full sentence parsing, they re-
rank candidate trees created by the Collins parser
(Collins, 2003). Similar to their work, we use the
idea of letting discrete categories reduce the search
space during inference. We compare to fully tied
RNNs in which the same weights are used at every
node. Our syntactically untied RNNs outperform
them by a significant margin. The idea of untying
has also been successfully used in deep learning
applied to vision (Le et al, 2010).
This paper uses several ideas of (Socher et al,
2011b). The main differences are (i) the dual
representation of nodes as discrete categories and
vectors, (ii) the combination with a PCFG, and
(iii) the syntactic untying of weights based on
child categories. We directly compare models with
fully tied and untied weights. Another work that
represents phrases with a dual discrete-continuous
representation is (Kartsaklis et al, 2012).
3 Compositional Vector Grammars
This section introduces Compositional Vector
Grammars (CVGs), a model to jointly find syntac-
tic structure and capture compositional semantic
information.
CVGs build on two observations. Firstly, that a
lot of the structure and regularity in languages can
be captured by well-designed syntactic patterns.
Hence, the CVG builds on top of a standard PCFG
parser. However, many parsing decisions show
fine-grained semantic factors at work. Therefore
we combine syntactic and semantic information
by giving the parser access to rich syntactico-
semantic information in the form of distributional
word vectors and compute compositional semantic
vector representations for longer phrases (Costa
et al, 2003; Menchetti et al, 2005; Socher et
al., 2011b). The CVG model merges ideas from
both generative models that assume discrete syn-
tactic categories and discriminative models that
are trained using continuous vectors.
We will first briefly introduce single word vec-
tor representations and then describe the CVG ob-
jective function, tree scoring and inference.
3.1 Word Vector Representations
In most systems that use a vector representa-
tion for words, such vectors are based on co-
occurrence statistics of each word and its context
(Turney and Pantel, 2010). Another line of re-
search to learn distributional word vectors is based
on neural language models (Bengio et al, 2003)
which jointly learn an embedding of words into an
n-dimensional feature space and use these embed-
dings to predict how suitable a word is in its con-
text. These vector representations capture inter-
esting linear relationships (up to some accuracy),
such as king?man+woman ? queen (Mikolov
et al, 2013).
Collobert and Weston (2008) introduced a new
model to compute such an embedding. The idea
is to construct a neural network that outputs high
scores for windows that occur in a large unla-
beled corpus and low scores for windows where
one word is replaced by a random word. When
such a network is optimized via gradient ascent the
derivatives backpropagate into the word embed-
ding matrix X . In order to predict correct scores
the vectors in the matrix capture co-occurrence
statistics.
For further details and evaluations of these em-
beddings, see (Turian et al, 2010; Huang et al,
2012). The resulting X matrix is used as follows.
Assume we are given a sentence as an ordered list
of m words. Each word w has an index [w] = i
into the columns of the embedding matrix. This
index is used to retrieve the word?s vector repre-
sentation aw using a simple multiplication with a
binary vector e, which is zero everywhere, except
457
at the ith index. So aw = Lei ? Rn. Henceforth,
after mapping each word to its vector, we represent
a sentence S as an ordered list of (word,vector)
pairs: x = ((w1, aw1), . . . , (wm, awm)).
Now that we have discrete and continuous rep-
resentations for all words, we can continue with
the approach for computing tree structures and
vectors for nonterminal nodes.
3.2 Max-Margin Training Objective for
CVGs
The goal of supervised parsing is to learn a func-
tion g : X ? Y , where X is the set of sentences
and Y is the set of all possible labeled binary parse
trees. The set of all possible trees for a given sen-
tence xi is defined as Y (xi) and the correct tree
for a sentence is yi.
We first define a structured margin loss ?(yi, y?)
for predicting a tree y? for a given correct tree.
The loss increases the more incorrect the proposed
parse tree is (Goodman, 1998). The discrepancy
between trees is measured by counting the number
of nodes N(y) with an incorrect span (or label) in
the proposed tree:
?(yi, y?) =
?
d?N(y?)
?1{d /? N(yi)}. (1)
We set ? = 0.1 in all experiments. For a given
set of training instances (xi, yi), we search for the
function g?, parameterized by ?, with the smallest
expected loss on a new sentence. It has the follow-
ing form:
g?(x) = arg max
y??Y (x)
s(CVG(?, x, y?)), (2)
where the tree is found by the Compositional Vec-
tor Grammar (CVG) introduced below and then
scored via the function s. The higher the score of
a tree the more confident the algorithm is that its
structure is correct. This max-margin, structure-
prediction objective (Taskar et al, 2004; Ratliff
et al, 2007; Socher et al, 2011b) trains the CVG
so that the highest scoring tree will be the correct
tree: g?(xi) = yi and its score will be larger up to
a margin to other possible trees y? ? Y(xi):
s(CVG(?, xi, yi)) ? s(CVG(?, xi, y?)) + ?(yi, y?).
This leads to the regularized risk function for m
training examples:
J(?) = 1m
m?
i=1
ri(?) +
?
2 ???
2
2, where
ri(?) = max
y??Y (xi)
(
s(CVG(xi, y?)) + ?(yi, y?)
)
? s(CVG(xi, yi)) (3)
Intuitively, to minimize this objective, the score of
the correct tree yi is increased and the score of the
highest scoring incorrect tree y? is decreased.
3.3 Scoring Trees with CVGs
For ease of exposition, we first describe how to
score an existing fully labeled tree with a standard
RNN and then with a CVG. The subsequent sec-
tion will then describe a bottom-up beam search
and its approximation for finding the optimal tree.
Assume, for now, we are given a labeled
parse tree as shown in Fig. 2. We define
the word representations as (vector, POS) pairs:
((a,A), (b, B), (c, C)), where the vectors are de-
fined as in Sec. 3.1 and the POS tags come from
a PCFG. The standard RNN essentially ignores all
POS tags and syntactic categories and each non-
terminal node is associated with the same neural
network (i.e., the weights across nodes are fully
tied). We can represent the binary tree in Fig. 2
in the form of branching triplets (p ? c1c2).
Each such triplet denotes that a parent node p has
two children and each ck can be either a word
vector or a non-terminal node in the tree. For
the example in Fig. 2, we would get the triples
((p1 ? bc), (p2 ? ap1)). Note that in order
to replicate the neural network and compute node
representations in a bottom up fashion, the parent
must have the same dimensionality as the children:
p ? Rn.
Given this tree structure, we can now compute
activations for each node from the bottom up. We
begin by computing the activation for p1 using
the children?s word vectors. We first concatenate
the children?s representations b, c ? Rn?1 into a
vector
[
b
c
]
? R2n?1. Then the composition
function multiplies this vector by the parameter
weights of the RNN W ? Rn?2n and applies an
element-wise nonlinearity function f = tanh to
the output vector. The resulting output p(1) is then
given as input to compute p(2).
p(1) = f
(
W
[
b
c
])
, p(2) = f
(
W
[
a
p1
])
458
(A , a=       )        ( B , b=       )       ( C, c=       )
P(1 ), p(1 )=       
 P(2 ), p(2 )=        
Standard Recursive Neural Network
= f   W bc
= f   W ap(1 )
Figure 2: An example tree with a simple Recursive
Neural Network: The same weight matrix is repli-
cated and used to compute all non-terminal node
representations. Leaf nodes are n-dimensional
vector representations of words.
In order to compute a score of how plausible of
a syntactic constituent a parent is the RNN uses a
single-unit linear layer for all i:
s(p(i)) = vT p(i),
where v ? Rn is a vector of parameters that need
to be trained. This score will be used to find the
highest scoring tree. For more details on how stan-
dard RNNs can be used for parsing, see Socher et
al. (2011b).
The standard RNN requires a single composi-
tion function to capture all types of compositions:
adjectives and nouns, verbs and nouns, adverbs
and adjectives, etc. Even though this function is
a powerful one, we find a single neural network
weight matrix cannot fully capture the richness of
compositionality. Several extensions are possible:
A two-layered RNN would provide more expres-
sive power, however, it is much harder to train be-
cause the resulting neural network becomes very
deep and suffers from vanishing gradient prob-
lems. Socher et al (2012) proposed to give ev-
ery single word a matrix and a vector. The ma-
trix is then applied to the sibling node?s vector
during the composition. While this results in a
powerful composition function that essentially de-
pends on the words being combined, the number
of model parameters explodes and the composi-
tion functions do not capture the syntactic com-
monalities between similar POS tags or syntactic
categories.
Based on the above considerations, we propose
the Compositional Vector Grammar (CVG) that
conditions the composition function at each node
on discrete syntactic categories extracted from a
(A , a=       )        ( B , b=       )       ( C, c=       )
P(1 ), p(1 )=       
 P(2 ), p(2 )=        
Syntactically Untied Recursive Neural Network
= f   W (B ,C) bc
= f   W (A ,P  ) ap(1 )
(1 )
Figure 3: Example of a syntactically untied RNN
in which the function to compute a parent vector
depends on the syntactic categories of its children
which we assume are given for now.
PCFG. Hence, CVGs combine discrete, syntactic
rule probabilities and continuous vector composi-
tions. The idea is that the syntactic categories of
the children determine what composition function
to use for computing the vector of their parents.
While not perfect, a dedicated composition func-
tion for each rule RHS can well capture common
composition processes such as adjective or adverb
modification versus noun or clausal complementa-
tion. For instance, it could learn that an NP should
be similar to its head noun and little influenced by
a determiner, whereas in an adjective modification
both words considerably determine the meaning of
a phrase. The original RNN is parameterized by a
single weight matrixW . In contrast, the CVG uses
a syntactically untied RNN (SU-RNN) which has
a set of such weights. The size of this set depends
on the number of sibling category combinations in
the PCFG.
Fig. 3 shows an example SU-RNN that com-
putes parent vectors with syntactically untied
weights. The CVG computes the first parent vec-
tor via the SU-RNN:
p(1) = f
(
W (B,C)
[
b
c
])
,
where W (B,C) ? Rn?2n is now a matrix that de-
pends on the categories of the two children. In
this bottom up procedure, the score for each node
consists of summing two elements: First, a single
linear unit that scores the parent vector and sec-
ond, the log probability of the PCFG for the rule
that combines these two children:
s
(
p(1)
)
=
(
v(B,C)
)T p(1) + logP (P1 ? B C),
(4)
459
where P (P1 ? B C) comes from the PCFG.
This can be interpreted as the log probability of a
discrete-continuous rule application with the fol-
lowing factorization:
P ((P1, p1)? (B, b)(C, c)) (5)
= P (p1 ? b c|P1 ? B C)P (P1 ? B C),
Note, however, that due to the continuous nature
of the word vectors, the probability of such a CVG
rule application is not comparable to probabilities
provided by a PCFG since the latter sum to 1 for
all children.
Assuming that node p1 has syntactic category
P1, we compute the second parent vector via:
p(2) = f
(
W (A,P1)
[
a
p(1)
])
.
The score of the last parent in this trigram is com-
puted via:
s
(
p(2)
)
=
(
v(A,P1)
)T p(2) + logP (P2 ? A P1).
3.4 Parsing with CVGs
The above scores (Eq. 4) are used in the search for
the correct tree for a sentence. The goodness of a
tree is measured in terms of its score and the CVG
score of a complete tree is the sum of the scores at
each node:
s(CVG(?, x, y?)) =
?
d?N(y?)
s
(
pd
)
. (6)
The main objective function in Eq. 3 includes a
maximization over all possible trees maxy??Y (x).
Finding the global maximum, however, cannot be
done efficiently for longer sentences nor can we
use dynamic programming. This is due to the fact
that the vectors break the independence assump-
tions of the base PCFG. A (category, vector) node
representation is dependent on all the words in its
span and hence to find the true global optimum,
we would have to compute the scores for all bi-
nary trees. For a sentence of length n, there are
Catalan(n) many possible binary trees which is
very large even for moderately long sentences.
One could use a bottom-up beam search, keep-
ing a k-best list at every cell of the chart, possibly
for each syntactic category. This beam search in-
ference procedure is still considerably slower than
using only the simplified base PCFG, especially
since it has a small state space (see next section for
details). Since each probability look-up is cheap
but computing SU-RNN scores requires a matrix
product, we would like to reduce the number of
SU-RNN score computations to only those trees
that require semantic information. We note that
labeled F1 of the Stanford PCFG parser on the test
set is 86.17%. However, if one used an oracle to
select the best tree from the top 200 trees that it
produces, one could get an F1 of 95.46%.
We use this knowledge to speed up inference via
two bottom-up passes through the parsing chart.
During the first one, we use only the base PCFG to
run CKY dynamic programming through the tree.
The k = 200-best parses at the top cell of the
chart are calculated using the efficient algorithm
of (Huang and Chiang, 2005). Then, the second
pass is a beam search with the full CVG model (in-
cluding the more expensive matrix multiplications
of the SU-RNN). This beam search only consid-
ers phrases that appear in the top 200 parses. This
is similar to a re-ranking setup but with one main
difference: the SU-RNN rule score computation at
each node still only has access to its child vectors,
not the whole tree or other global features. This
allows the second pass to be very fast. We use this
setup in our experiments below.
3.5 Training SU-RNNs
The full CVG model is trained in two stages. First
the base PCFG is trained and its top trees are
cached and then used for training the SU-RNN
conditioned on the PCFG. The SU-RNN is trained
using the objective in Eq. 3 and the scores as ex-
emplified by Eq. 6. For each sentence, we use the
method described above to efficiently find an ap-
proximation for the optimal tree.
To minimize the objective we want to increase
the scores of the correct tree?s constituents and
decrease the score of those in the highest scor-
ing incorrect tree. Derivatives are computed via
backpropagation through structure (BTS) (Goller
and Ku?chler, 1996). The derivative of tree i has
to be taken with respect to all parameter matrices
W (AB) that appear in it. The main difference be-
tween backpropagation in standard RNNs and SU-
RNNs is that the derivatives at each node only add
to the overall derivative of the specific matrix at
that node. For more details on backpropagation
through RNNs, see Socher et al (2010)
460
3.6 Subgradient Methods and AdaGrad
The objective function is not differentiable due to
the hinge loss. Therefore, we generalize gradient
ascent via the subgradient method (Ratliff et al,
2007) which computes a gradient-like direction.
Let ? = (X,W (??), v(??)) ? RM be a vector of all
M model parameters, where we denote W (??) as
the set of matrices that appear in the training set.
The subgradient of Eq. 3 becomes:
?J
?? =
?
i
?s(xi, y?max)
?? ?
?s(xi, yi)
?? + ?,
where y?max is the tree with the highest score. To
minimize the objective, we use the diagonal vari-
ant of AdaGrad (Duchi et al, 2011) with mini-
batches. For our parameter updates, we first de-
fine g? ? RM?1 to be the subgradient at time step
? and Gt = ?t?=1 g?gT? . The parameter update at
time step t then becomes:
?t = ?t?1 ? ? (diag(Gt))?1/2 gt, (7)
where ? is the learning rate. Since we use the di-
agonal of Gt, we only have to store M values and
the update becomes fast to compute: At time step
t, the update for the i?th parameter ?t,i is:
?t,i = ?t?1,i ?
???t
?=1 g2?,i
gt,i. (8)
Hence, the learning rate is adapting differ-
ently for each parameter and rare parameters get
larger updates than frequently occurring parame-
ters. This is helpful in our setting since some W
matrices appear in only a few training trees. This
procedure found much better optima (by ?3% la-
beled F1 on the dev set), and converged more
quickly than L-BFGS which we used previously
in RNN training (Socher et al, 2011a). Training
time is roughly 4 hours on a single machine.
3.7 Initialization of Weight Matrices
In the absence of any knowledge on how to com-
bine two categories, our prior for combining two
vectors is to average them instead of performing a
completely random projection. Hence, we initial-
ize the binary W matrices with:
W (??) = 0.5[In?nIn?n0n?1] + ,
where we include the bias in the last column and
the random variable is uniformly distributed:  ?
U [?0.001, 0.001]. The first block is multiplied by
the left child and the second by the right child:
W (AB)
?
?
a
b
1
?
? =
[
W (A)W (B)bias
]
?
?
a
b
1
?
?
= W (A)a+W (B)b+ bias.
4 Experiments
We evaluate the CVG in two ways: First, by a stan-
dard parsing evaluation on Penn Treebank WSJ
and then by analyzing the model errors in detail.
4.1 Cross-validating Hyperparameters
We used the first 20 files of WSJ section 22
to cross-validate several model and optimization
choices. The base PCFG uses simplified cate-
gories of the Stanford PCFG Parser (Klein and
Manning, 2003a). We decreased the state split-
ting of the PCFG grammar (which helps both by
making it less sparse and by reducing the num-
ber of parameters in the SU-RNN) by adding
the following options to training: ?-noRightRec -
dominatesV 0 -baseNP 0?. This reduces the num-
ber of states from 15,276 to 12,061 states and 602
POS tags. These include split categories, such as
parent annotation categories like VP?S. Further-
more, we ignore all category splits for the SU-
RNN weights, resulting in 66 unary and 882 bi-
nary child pairs. Hence, the SU-RNN has 66+882
transformation matrices and scoring vectors. Note
that any PCFG, including latent annotation PCFGs
(Matsuzaki et al, 2005) could be used. However,
since the vectors will capture lexical and semantic
information, even simple base PCFGs can be sub-
stantially improved. Since the computational com-
plexity of PCFGs depends on the number of states,
a base PCFG with fewer states is much faster.
Testing on the full WSJ section 22 dev set (1700
sentences) takes roughly 470 seconds with the
simple base PCFG, 1320 seconds with our new
CVG and 1600 seconds with the currently pub-
lished Stanford factored parser. Hence, increased
performance comes also with a speed improve-
ment of approximately 20%.
We fix the same regularization of ? = 10?4
for all parameters. The minibatch size was set to
20. We also cross-validated on AdaGrad?s learn-
ing rate which was eventually set to ? = 0.1 and
word vector size. The 25-dimensional vectors pro-
vided by Turian et al (2010) provided the best
461
Parser dev (all) test? 40 test (all)
Stanford PCFG 85.8 86.2 85.5
Stanford Factored 87.4 87.2 86.6
Factored PCFGs 89.7 90.1 89.4
Collins 87.7
SSN (Henderson) 89.4
Berkeley Parser 90.1
CVG (RNN) 85.7 85.1 85.0
CVG (SU-RNN) 91.2 91.1 90.4
Charniak-SelfTrain 91.0
Charniak-RS 92.1
Table 1: Comparison of parsers with richer state
representations on the WSJ. The last line is the
self-trained re-ranked Charniak parser.
performance and were faster than 50-,100- or 200-
dimensional ones. We hypothesize that the larger
word vector sizes, while capturing more seman-
tic knowledge, result in too many SU-RNN matrix
parameters to train and hence perform worse.
4.2 Results on WSJ
The dev set accuracy of the best model is 90.93%
labeled F1 on all sentences. This model re-
sulted in 90.44% on the final test set (WSJ sec-
tion 23). Table 1 compares our results to the
two Stanford parser variants (the unlexicalized
PCFG (Klein and Manning, 2003a) and the fac-
tored parser (Klein and Manning, 2003b)) and
other parsers that use richer state representations:
the Berkeley parser (Petrov and Klein, 2007),
Collins parser (Collins, 1997), SSN: a statistical
neural network parser (Henderson, 2004), Fac-
tored PCFGs (Hall and Klein, 2012), Charniak-
SelfTrain: the self-training approach of McClosky
et al (2006), which bootstraps and parses addi-
tional large corpora multiple times, Charniak-RS:
the state of the art self-trained and discrimina-
tively re-ranked Charniak-Johnson parser combin-
ing (Charniak, 2000; McClosky et al, 2006; Char-
niak and Johnson, 2005). See Kummerfeld et al
(2012) for more comparisons. We compare also
to a standard RNN ?CVG (RNN)? and to the pro-
posed CVG with SU-RNNs.
4.3 Model Analysis
Analysis of Error Types. Table 2 shows a de-
tailed comparison of different errors. We use
the code provided by Kummerfeld et al (2012)
and compare to the previous version of the Stan-
ford factored parser as well as to the Berkeley
and Charniak-reranked-self-trained parsers (de-
fined above). See Kummerfeld et al (2012) for
details and comparisons to other parsers. One of
Error Type Stanford CVG Berkeley Char-RS
PP Attach 1.02 0.79 0.82 0.60
Clause Attach 0.64 0.43 0.50 0.38
Diff Label 0.40 0.29 0.29 0.31
Mod Attach 0.37 0.27 0.27 0.25
NP Attach 0.44 0.31 0.27 0.25
Co-ord 0.39 0.32 0.38 0.23
1-Word Span 0.48 0.31 0.28 0.20
Unary 0.35 0.22 0.24 0.14
NP Int 0.28 0.19 0.18 0.14
Other 0.62 0.41 0.41 0.50
Table 2: Detailed comparison of different parsers.
the largest sources of improved performance over
the original Stanford factored parser is in the cor-
rect placement of PP phrases. When measuring
only the F1 of parse nodes that include at least one
PP child, the CVG improves the Stanford parser
by 6.2% to an F1 of 77.54%. This is a 0.23 re-
duction in the average number of bracket errors
per sentence. The ?Other? category includes VP,
PRN and other attachments, appositives and inter-
nal structures of modifiers and QPs.
Analysis of Composition Matrices. An analy-
sis of the norms of the binary matrices reveals
that the model learns a soft vectorized notion of
head words: Head words are given larger weights
and importance when computing the parent vec-
tor: For the matrices combining siblings with cat-
egories VP:PP, VP:NP and VP:PRT, the weights in
the part of the matrix which is multiplied with the
VP child vector dominates. Similarly NPs dom-
inate DTs. Fig. 5 shows example matrices. The
two strong diagonals are due to the initialization
described in Sec. 3.7.
Semantic Transfer for PP Attachments. In this
small model analysis, we use two pairs of sen-
tences that the original Stanford parser and the
CVG did not parse correctly after training on
the WSJ. We then continue to train both parsers
on two similar sentences and then analyze if the
parsers correctly transferred the knowledge. The
training sentences are He eats spaghetti with a
fork. and She eats spaghetti with pork. The very
similar test sentences are He eats spaghetti with a
spoon. and He eats spaghetti with meat. Initially,
both parsers incorrectly attach the PP to the verb
in both test sentences. After training, the CVG
parses both correctly, while the factored Stanford
parser incorrectly attaches both PPs to spaghetti.
The CVG?s ability to transfer the correct PP at-
tachments is due to the semantic word vector sim-
ilarity between the words in the sentences. Fig. 4
shows the outputs of the two parsers.
462
(a) Stanford factored parserS
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
DT
a
NN
spoon
S
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
PRP
meat
(b) Compositional Vector GrammarS
NP
PRP
He
VP
VBZ
eats
NP
NNS
spaghetti
PP
IN
with
NP
DT
a
NN
spoon
S
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
NN
meat
Figure 4: Test sentences of semantic transfer for PP attachments. The CVG was able to transfer se-
mantic word knowledge from two related training sentences. In contrast, the Stanford parser could not
distinguish the PP attachments based on the word semantics.
 
 
10 20 30 40 50
5
10
15
20
25 ?0.2
0
0.2
0.4
0.6
0.8
DT-NP
 
 
10 20 30 40 50
5
10
15
20
25
?0.4
?0.2
0
0.2
0.4
0.6
VP-NP
 
 
10 20 30 40 50
5
10
15
20
25 ?0.2
0
0.2
0.4
0.6
ADJP-NP
Figure 5: Three binary composition matrices
showing that head words dominate the composi-
tion. The model learns to not give determiners
much importance. The two diagonals show clearly
the two blocks that are multiplied with the left and
right children, respectively.
5 Conclusion
We introduced Compositional Vector Grammars
(CVGs), a parsing model that combines the speed
of small-state PCFGs with the semantic richness
of neural word representations and compositional
phrase vectors. The compositional vectors are
learned with a new syntactically untied recursive
neural network. This model is linguistically more
plausible since it chooses different composition
functions for a parent node based on the syntac-
tic categories of its children. The CVG obtains
90.44% labeled F1 on the full WSJ test set and is
20% faster than the previous Stanford parser.
Acknowledgments
We thank Percy Liang for chats about the paper.
Richard is supported by a Microsoft Research PhD
fellowship. The authors gratefully acknowledge
the support of the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under Air Force
Research Laboratory (AFRL) prime contract no.
FA8750-13-2-0040, and the DARPA Deep Learn-
ing program under contract number FA8650-10-
C-7020. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of DARPA, AFRL, or the US govern-
ment.
463
References
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Lin-
guistics, 18.
C. Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, pages 196?205.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking.
In ACL.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of ACL, pages 132?139.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
F. Costa, P. Frasconi, V. Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language us-
ing recursive neural networks. Applied Intelligence.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12, July.
J. L. Elman. 1991. Distributed representations, sim-
ple recurrent networks, and grammatical structure.
Machine Learning, 7(2-3):195?225.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In Proceedings of ACL, pages 959?967.
D. Gildea and M. Palmer. 2002. The necessity of pars-
ing for predicate argument recognition. In Proceed-
ings of ACL, pages 239?246.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backprop-
agation through structure. In Proceedings of the In-
ternational Conference on Neural Networks.
J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
MIT.
D. Hall and D. Klein. 2012. Training factored pcfgs
with expectation propagation. In EMNLP.
J. Henderson. 2003. Neural network probability esti-
mation for broad coverage parsing. In Proceedings
of EACL.
J. Henderson. 2004. Discriminative training of a neu-
ral network statistical parser. In ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT 2005).
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2012. A
unified sentence space for categorical distributional-
compositional semantics: Theory and experiments.
Proceedings of 24th International Conference on
Computational Linguistics (COLING): Posters.
D. Klein and C. D. Manning. 2003a. Accurate un-
lexicalized parsing. In Proceedings of ACL, pages
423?430.
D. Klein and C.D. Manning. 2003b. Fast exact in-
ference with a factored model for natural language
parsing. In NIPS.
J. K. Kummerfeld, D. Hall, J. R. Curran, and D. Klein.
2012. Parser showdown at the wall street corral: An
empirical investigation of error types in parser out-
put. In EMNLP.
Q. V. Le, J. Ngiam, Z. Chen, D. Chia, P. W. Koh, and
A. Y. Ng. 2010. Tiled convolutional neural net-
works. In NIPS.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In ACL.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL.
S. Menchetti, F. Costa, P. Frasconi, and M. Pon-
til. 2005. Wide coverage natural language pro-
cessing using kernel methods and neural networks
for structured data. Pattern Recognition Letters,
26(12):1896?1906.
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguis-
tic regularities in continuous spaceword representa-
tions. In HLT-NAACL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of ACL, pages 433?440.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2007. (On-
line) subgradient methods for structured prediction.
In Eleventh International Conference on Artificial
Intelligence and Statistics (AIStats).
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learn-
ing continuous phrase representations and syntactic
parsing with recursive neural networks. In Proceed-
ings of the NIPS-2010 Deep Learning and Unsuper-
vised Feature Learning Workshop.
464
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase De-
tection. In NIPS. MIT Press.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic Compositionality Through Recur-
sive Matrix-Vector Spaces. In EMNLP.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
EMNLP, pages 1?8.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In CoNLL-X.
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In ACL.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
465
Grounded Compositional Semantics
for Finding and Describing Images with Sentences
Richard Socher, Andrej Karpathy, Quoc V. Le*, Christopher D. Manning, Andrew Y. Ng
Stanford University, Computer Science Department, *Google Inc.
richard@socher.org, karpathy@cs.stanford.edu,
qvl@google.com, manning@stanford.edu, ang@cs.stanford.edu
Abstract
Previous work on Recursive Neural Networks
(RNNs) shows that these models can produce
compositional feature vectors for accurately
representing and classifying sentences or im-
ages. However, the sentence vectors of previ-
ous models cannot accurately represent visu-
ally grounded meaning. We introduce the DT-
RNN model which uses dependency trees to
embed sentences into a vector space in order
to retrieve images that are described by those
sentences. Unlike previous RNN-based mod-
els which use constituency trees, DT-RNNs
naturally focus on the action and agents in
a sentence. They are better able to abstract
from the details of word order and syntactic
expression. DT-RNNs outperform other re-
cursive and recurrent neural networks, kernel-
ized CCA and a bag-of-words baseline on the
tasks of finding an image that fits a sentence
description and vice versa. They also give
more similar representations to sentences that
describe the same image.
1 Introduction
Single word vector spaces are widely used (Turney
and Pantel, 2010) and successful at classifying sin-
gle words and capturing their meaning (Collobert
and Weston, 2008; Huang et al., 2012; Mikolov et
al., 2013). Since words rarely appear in isolation,
the task of learning compositional meaning repre-
sentations for longer phrases has recently received a
lot of attention (Mitchell and Lapata, 2010; Socher
et al., 2010; Socher et al., 2012; Grefenstette et al.,
2013). Similarly, classifying whole images into a
fixed set of classes also achieves very high perfor-
mance (Le et al., 2012; Krizhevsky et al., 2012).
However, similar to words, objects in images are of-
ten seen in relationships with other objects which are
not adequately described by a single label.
In this work, we introduce a model, illustrated in
Fig. 1, which learns to map sentences and images
into a common embedding space in order to be able
to retrieve one from the other. We assume word and
image representations are first learned in their re-
spective single modalities but finally mapped into a
jointly learned multimodal embedding space.
Our model for mapping sentences into this space
is based on ideas from Recursive Neural Networks
(RNNs) (Pollack, 1990; Costa et al., 2003; Socher
et al., 2011b). However, unlike all previous RNN
models which are based on constituency trees (CT-
RNNs), our model computes compositional vector
representations inside dependency trees. The com-
positional vectors computed by this new dependency
tree RNN (DT-RNN) capture more of the meaning
of sentences, where we define meaning in terms of
similarity to a ?visual representation? of the textual
description. DT-RNN induced vector representa-
tions of sentences are more robust to changes in the
syntactic structure or word order than related mod-
els such as CT-RNNs or Recurrent Neural Networks
since they naturally focus on a sentence?s action and
its agents.
We evaluate and compare DT-RNN induced rep-
resentations on their ability to use a sentence such as
?A man wearing a helmet jumps on his bike near a
beach.? to find images that show such a scene. The
goal is to learn sentence representations that capture
207
Transactions of the Association for Computational Linguistics, 2 (2014) 207?218. Action Editor: Alexander Clark.
Submitted 10/2013; Revised 3/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
A man wearing a helmet jumps on his bike near a beach.
Compositional Sentence Vectors
Two airplanes parked in an airport.
A man jumping his downhill bike.
Image Vector Representation
A small child sits on a cement wall near white flower.
Multi-Modal 
Representations
Figure 1: The DT-RNN learns vector representations for sentences based on their dependency trees. We learn to
map the outputs of convolutional neural networks applied to images into the same space and can then compare both
sentences and images. This allows us to query images with a sentence and give sentence descriptions to images.
the visual scene described and to find appropriate
images in the learned, multi-modal sentence-image
space. Conversely, when given a query image, we
would like to find a description that goes beyond a
single label by providing a correct sentence describ-
ing it, a task that has recently garnered a lot of at-
tention (Farhadi et al., 2010; Ordonez et al., 2011;
Kuznetsova et al., 2012). We use the dataset intro-
duced by (Rashtchian et al., 2010) which consists of
1000 images, each with 5 descriptions. On all tasks,
our model outperforms baselines and related mod-
els.
2 Related Work
The presented model is connected to several areas of
NLP and vision research, each with a large amount
of related work to which we can only do some justice
given space constraints.
Semantic Vector Spaces and Their Composition-
ality. The dominant approach in semantic vec-
tor spaces uses distributional similarities of single
words. Often, co-occurrence statistics of a word and
its context are used to describe each word (Turney
and Pantel, 2010; Baroni and Lenci, 2010), such
as tf-idf. Most of the compositionality algorithms
and related datasets capture two-word compositions.
For instance, (Mitchell and Lapata, 2010) use two-
word phrases and analyze similarities computed by
vector addition, multiplication and others. Compo-
sitionality is an active field of research with many
different models and representations being explored
(Grefenstette et al., 2013), among many others. We
compare to supervised compositional models that
can learn task-specific vector representations such as
constituency tree recursive neural networks (Socher
et al., 2011b; Socher et al., 2011a), chain structured
recurrent neural networks and other baselines. An-
other alternative would be to use CCG trees as a
backbone for vector composition (K.M. Hermann,
2013).
Multimodal Embeddings. Multimodal embed-
ding methods project data from multiple sources
such as sound and video (Ngiam et al., 2011) or im-
ages and text. Socher et al. (Socher and Fei-Fei,
2010) project words and image regions into a com-
mon space using kernelized canonical correlation
analysis to obtain state of the art performance in an-
notation and segmentation. Similar to our work, they
use unsupervised large text corpora to learn seman-
tic word representations. Among other recent work
is that by Srivastava and Salakhutdinov (2012) who
developed multimodal Deep Boltzmann Machines.
Similar to their work, we use techniques from the
broad field of deep learning to represent images and
words.
Recently, single word vector embeddings have
been used for zero shot learning (Socher et al.,
2013c). Mapping images to word vectors enabled
their system to classify images as depicting objects
such as ?cat? without seeing any examples of this
class. Related work has also been presented at NIPS
(Socher et al., 2013b; Frome et al., 2013). This work
moves zero-shot learning beyond single categories
per image and extends it to unseen phrases and full
length sentences, making use of similar ideas of se-
mantic spaces grounded in visual knowledge.
208
Detailed Image Annotation. Interactions be-
tween images and texts is a growing research field.
Early work in this area includes generating single
words or fixed phrases from images (Duygulu et al.,
2002; Barnard et al., 2003) or using contextual in-
formation to improve recognition (Gupta and Davis,
2008; Torralba et al., 2010).
Apart from a large body of work on single object
image classification (Le et al., 2012), there is also
work on attribute classification and other mid-level
elements (Kumar et al., 2009), some of which we
hope to capture with our approach as well.
Our work is close in spirit with recent work in de-
scribing images with more detailed, longer textual
descriptions. In particular, Yao et al. (2010) describe
images using hierarchical knowledge and humans in
the loop. In contrast, our work does not require hu-
man interactions. Farhadi et al. (2010) and Kulkarni
et al. (2011), on the other hand, use a more automatic
method to parse images. For instance, the former ap-
proach uses a single triple of objects estimated for an
image to retrieve sentences from a collection written
to describe similar images. It forms representations
to describe 1 object, 1 action, and 1 scene. Kulkarni
et al. (2011) extends their method to describe an im-
age with multiple objects. None of these approaches
have used a compositional sentence vector repre-
sentation and they require specific language gener-
ation techniques and sophisticated inference meth-
ods. Since our model is based on neural networks in-
ference is fast and simple. Kuznetsova et al. (2012)
use a very large parallel corpus to connect images
and sentences. Feng and Lapata (2013) use a large
dataset of captioned images and experiments with
both extractive (search) and abstractive (generation)
models.
Most related is the very recent work of Hodosh et
al. (2013). They too evaluate using a ranking mea-
sure. In our experiments, we compare to kernelized
Canonical Correlation Analysis which is the main
technique in their experiments.
3 Dependency-Tree Recursive Neural
Networks
In this section we first focus on the DT-RNN model
that computes compositional vector representations
for phrases and sentences of variable length and syn-
tactic type. In section 5 the resulting vectors will
then become multimodal features by mapping im-
ages that show what the sentence describes to the
same space and learning both the image and sen-
tence mapping jointly.
The most common way of building representa-
tions for longer phrases from single word vectors is
to simply linearly average the word vectors. While
this bag-of-words approach can yield reasonable
performance in some tasks, it gives all the words the
same weight and cannot distinguish important dif-
ferences in simple visual descriptions such as The
bike crashed into the standing car. vs. The car
crashed into the standing bike..
RNN models (Pollack, 1990; Goller and Ku?chler,
1996; Socher et al., 2011b; Socher et al., 2011a) pro-
vided a novel way of combining word vectors for
longer phrases that moved beyond simple averag-
ing. They combine vectors with an RNN in binary
constituency trees which have potentially many hid-
den layers. While the induced vector representations
work very well on many tasks, they also inevitably
capture a lot of syntactic structure of the sentence.
However, the task of finding images from sentence
descriptions requires us to be more invariant to syn-
tactic differences. One such example are active-
passive constructions which can collapse words such
as ?by? in some formalisms (de Marneffe et al.,
2006), relying instead on the semantic relationship
of ?agent?. For instance, The mother hugged her
child. and The child was hugged by its mother.
should map to roughly the same visual space. Cur-
rent Recursive and Recurrent Neural Networks do
not exhibit this behavior and even bag of words rep-
resentations would be influenced by the words was
and by. The model we describe below focuses more
on recognizing actions and agents and has the po-
tential to learn representations that are invariant to
active-passive differences.
3.1 DT-RNN Inputs: Word Vectors and
Dependency Trees
In order for the DT-RNN to compute a vector repre-
sentation for an ordered list of m words (a phrase or
sentence), we map the single words to a vector space
and then parse the sentence.
First, we map each word to a d-dimensional vec-
tor. We initialize these word vectors with the un-
209
A man wearing a helmet jumps on his bike near a beach
det
nsubj
partmod detdobj
root
prep posspobj
prep
detpobj
Figure 2: Example of a full dependency tree for a longer sentence. The DT-RNN will compute vector representations
at every word that represents that word and an arbitrary number of child nodes. The final representation is computed
at the root node, here at the verb jumps. Note that more important activity and object words are higher up in this tree
structure.
supervised model of Huang et al. (2012) which can
learn single word vector representations from both
local and global contexts. The idea is to construct a
neural network that outputs high scores for windows
and documents that occur in a large unlabeled corpus
and low scores for window-document pairs where
one word is replaced by a random word. When
such a network is optimized via gradient descent the
derivatives backpropagate into a word embedding
matrix A which stores word vectors as columns. In
order to predict correct scores the vectors in the ma-
trix capture co-occurrence statistics. We use d = 50
in all our experiments. The embedding matrix X
is then used by finding the column index i of each
word: [w] = i and retrieving the corresponding col-
umn xw from X . Henceforth, we represent an input
sentence s as an ordered list of (word,vector) pairs:
s = ((w1, xw1), . . . , (wm, xwm)).
Next, the sequence of words (w1, . . . , wm) is
parsed by the dependency parser of de Marneffe
et al. (2006). Fig. 2 shows an example. We can
represent a dependency tree d of a sentence s as
an ordered list of (child,parent) indices: d(s) =
{(i, j)}, where every child word in the sequence
i = 1, . . . ,m is present and has any word j ?
{1, . . . ,m} ? {0} as its parent. The root word has
as its parent 0 and we notice that the same word can
be a parent between zero and m number of times.
Without loss of generality, we assume that these in-
dices form a tree structure. To summarize, the input
to the DT-RNN for each sentence is the pair (s, d):
the words and their vectors and the dependency tree.
3.2 Forward Propagation in DT-RNNs
Given these two inputs, we now illustrate how the
DT-RNN computes parent vectors. We will use the
following sentence as a running example: Students1
ride2 bikes3 at4 night5. Fig. 3 shows its tree
and computed vector representations. The depen-
Students                 bikes           night
ride 
at          x 1
x 2
x 3
x 4
x 5
h1
h2
h3
h4
h5
Figure 3: Example of a DT-RNN tree structure for com-
puting a sentence representation in a bottom up fashion.
dency tree for this sentence can be summarized by
the following set of (child, parent) edges: d =
{(1, 2), (2, 0), (3, 2), (4, 2), (5, 4)}.
The DT-RNN model will compute parent vectors
at each word that include all the dependent (chil-
dren) nodes in a bottom up fashion using a com-
positionality function g? which is parameterized by
all the model parameters ?. To this end, the algo-
rithm searches for nodes in a tree that have either
(i) no children or (ii) whose children have already
been computed and then computes the correspond-
ing vector.
In our example, the words x1, x3, x5 are leaf
nodes and hence, we can compute their correspond-
ing hidden nodes via:
hc = g?(xc) = f(Wvxc) for c = 1, 3, 5, (1)
where we compute the hidden vector at position c
via our general composition function g?. In the case
of leaf nodes, this composition function becomes
simply a linear layer, parameterized by Wv ? Rn?d,
followed by a nonlinearity. We cross-validate over
using no nonlinearity (f = id), tanh, sigmoid or
rectified linear units (f = max(0, x), but generally
find tanh to perform best.
The final sentence representation we want to com-
pute is at h2, however, since we still do not have h4,
210
we compute that one next:
h4 = g?(x4, h5) = f(Wvx4 +Wr1h5), (2)
where we use the same Wv as before to map the
word vector into hidden space but we now also have
a linear layer that takes as input h5, the only child
of the fourth node. The matrix Wr1 ? Rn?n is used
because node 5 is the first child node on the right
side of node 4. Generally, we have multiple matri-
ces for composing with hidden child vectors from
the right and left sides: Wr? = (Wr1, . . . ,Wrkr) and
Wl? = (Wl1, . . . ,Wlkl). The number of needed ma-
trices is determined by the data by simply finding
the maximum numbers of left kl and right kr chil-
dren any node has. If at test time a child appeared
at an even large distance (this does not happen in
our test set), the corresponding matrix would be the
identity matrix.
Now that all children of h2 have their hidden vec-
tors, we can compute the final sentence representa-
tion via:
h2 = g?(x2, h1, h3, h4) = (3)
f(Wvx2 +Wl1h1 +Wr1h3 +Wr2h4).
Notice that the children are multiplied by matrices
that depend on their location relative to the current
node.
Another modification that improves the mean
rank by approximately 6 in image search on the dev
set is to weight nodes by the number of words under-
neath them and normalize by the sum of words under
all children. This encourages the intuitive desidera-
tum that nodes describing longer phrases are more
important. Let `(i) be the number of leaf nodes
(words) under node i and C(i, y) be the set of child
nodes of node i in dependency tree y. The final com-
position function for a node vector hi becomes:
hi = f
?
? 1
`(i)
?
?Wvxi +
?
j?C(i)
`(j)Wpos(i,j)hj
?
?
?
? ,
(4)
where by definition `(i) = 1 + ?j?C(i) `(j) and
pos(i, j) is the relative position of child j with re-
spect to node i, e.g. l1 or r2 in Eq. 3.
3.3 Semantic Dependency Tree RNNs
An alternative is to condition the weight matrices
on the semantic relations given by the dependency
parser. We use the collapsed tree formalism of
the Stanford dependency parser (de Marneffe et al.,
2006). With such a semantic untying of the weights,
the DT-RNN makes better use of the dependency
formalism and could give active-passive reversals
similar semantic vector representation. The equation
for this semantic DT-RNN (SDT-RNN) is the same
as the one above except that the matrices Wpos(i,j)
are replaced with matrices based on the dependency
relationship. There are a total of 141 unique such
relationships in the dataset. However, most are very
rare. For examples of semantic relationships, see
Fig. 2 and the model analysis section 6.7.
This forward propagation can be used for com-
puting compositional vectors and in Sec. 5 we will
explain the objective function in which these are
trained.
3.4 Comparison to Previous RNN Models
The DT-RNN has several important differences to
previous RNN models of Socher et al. (2011a) and
(Socher et al., 2011b; Socher et al., 2011c). These
constituency tree RNNs (CT-RNNs) use the follow-
ing composition function to compute a hidden par-
ent vector h from exactly two child vectors (c1, c2)
in a binary tree: h = f
(
W
[
c1
c2
])
, where W ?
Rd?2d is the main parameter to learn. This can be
rewritten to show the similarity to the DT-RNN as
h = f(Wl1c1 +Wr1c2). However, there are several
important differences.
Note first that in previous RNN models the par-
ent vectors were of the same dimensionality to be
recursively compatible and be used as input to the
next composition. In contrast, our new model first
maps single words into a hidden space and then par-
ent nodes are composed from these hidden vectors.
This allows a higher capacity representation which
is especially helpful for nodes that have many chil-
dren.
Secondly, the DT-RNN allows for n-ary nodes in
the tree. This is an improvement that is possible even
for constituency tree CT-RNNs but it has not been
explored in previous models.
Third, due to computing parent nodes in con-
stituency trees, previous models had the problem
that words that are merged last in the tree have a
larger weight or importance in the final sentence rep-
211
Figure 4: The architecture of the visual model. This model has 3 sequences of filtering, pooling and local contrast
normalization layers. The learnable parameters are the filtering layer. The filters are not shared, i.e., the network is
nonconvolutional.
resentation. This can be problematic since these are
often simple non-content words, such as a leading
?But,?. While such single words can be important for
tasks such as sentiment analysis, we argue that for
describing visual scenes the DT-RNN captures the
more important effects: The dependency tree struc-
tures push the central content words such as the main
action or verb and its subject and object to be merged
last and hence, by construction, the final sentence
representation is more robust to less important ad-
jectival modifiers, word order changes, etc.
Fourth, we allow some untying of weights de-
pending on either how far away a constituent is from
the current word or what its semantic relationship is.
Now that we can compute compositional vector
representations for sentences, the next section de-
scribes how we represent images.
4 Learning Image Representations with
Neural Networks
The image features that we use in our experiments
are extracted from a deep neural network, replicated
from the one described in (Le et al., 2012). The net-
work was trained using both unlabeled data (random
web images) and labeled data to classify 22,000 cat-
egories in ImageNet (Deng et al., 2009). We then
used the features at the last layer, before the classi-
fier, as the feature representation in our experiments.
The dimension of the feature vector of the last layer
is 4,096. The details of the model and its training
procedures are as follows.
The architecture of the network can be seen in
Figure 4. The network takes 200x200 pixel images
as inputs and has 9 layers. The layers consist of
three sequences of filtering, pooling and local con-
trast normalization (Jarrett et al., 2009). The pooling
function is L2 pooling of the previous layer (taking
the square of the filtering units, summing them up
in a small area in the image, and taking the square-
root). The local contrast normalization takes inputs
in a small area of the lower layer, subtracts the mean
and divides by the standard deviation.
The network was first trained using an unsuper-
vised objective: trying to reconstruct the input while
keeping the neurons sparse. In this phase, the net-
work was trained on 20 million images randomly
sampled from the web. We resized a given image
so that its short dimension has 200 pixels. We then
cropped a fixed size 200x200 pixel image right at the
center of the resized image. This means we may dis-
card a fraction of the long dimension of the image.
After unsupervised training, we used Ima-
geNet (Deng et al., 2009) to adjust the features in the
entire network. The ImageNet dataset has 22,000
categories and 14 million images. The number of
images in each category is equal across categories.
The 22,000 categories are extracted from WordNet.
To speed up the supervised training of this net-
work, we made a simple modification to the algo-
rithm described in Le et al. (2012): adding a ?bottle-
neck? layer in between the last layer and the classi-
fier. to reduce the number of connections. We added
one ?bottleneck? layer which has 4,096 units in be-
tween the last layer of the network and the softmax
layer. This newly-added layer is fully connected to
the previous layer and has a linear activation func-
tion. The total number of connections of this net-
work is approximately 1.36 billion.
212
The network was trained again using the super-
vised objective of classifying the 22,000 classes in
ImageNet. Most features in the networks are local,
which allows model parallelism. Data parallelism
by asynchronous SGD was also employed as in Le
et al. (2012). The entire training, both unsupervised
and supervised, took 8 days on a large cluster of ma-
chines. This network achieves 18.3% precision@1
on the full ImageNet dataset (Release Fall 2011).
We will use the features at the bottleneck layer as
the feature vector z of an image. Each scaled and
cropped image is presented to our network. The net-
work then performs a feedforward computation to
compute the values of the bottleneck layer. This
means that every image is represented by a fixed
length vector of 4,096 dimensions. Note that during
training, no aligned sentence-image data was used
and the ImageNet classes do not fully intersect with
the words used in our dataset.
5 Multimodal Mappings
The previous two sections described how we can
map sentences into a d = 50-dimensional space and
how to extract high quality image feature vectors of
4096 dimensions. We now define our final multi-
modal objective function for learning joint image-
sentence representations with these models. Our
training set consists of N images and their feature
vectors zi and each image has 5 sentence descrip-
tions si1, . . . , si5 for which we use the DT-RNN to
compute vector representations. See Fig. 5 for ex-
amples from the dataset. For training, we use a max-
margin objective function which intuitively trains
pairs of correct image and sentence vectors to have
high inner products and incorrect pairs to have low
inner products. Let vi = WIzi be the mapped image
vector and yij = DTRNN?(sij) the composed sen-
tence vector. We define S to be the set of all sentence
indices and S(i) the set of sentence indices corre-
sponding to image i. Similarly, I is the set of all im-
age indices and I(j) is the image index of sentence
j. The set P is the set of all correct image-sentence
training pairs (i, j). The ranking cost function to
minimize is then: J(WI , ?) =
?
(i,j)?P
?
c?S\S(i)
max(0,?? vTi yj + vTi yc)
+
?
(i,j)?P
?
c?I\I(j)
max(0,?? vTi yj + vTc yj), (5)
where ? are the language composition matrices,
and both second sums are over other sentences com-
ing from different images and vice versa. The hyper-
parameter ? is the margin. The margin is found via
cross validation on the dev set and usually around 1.
The final objective also includes the regulariza-
tion term ?/left(???22 + ?WI?F ). Both the visual
model and the word vector learning require a very
large amount of training data and both have a huge
number of parameters. Hence, to prevent overfitting,
we assume their weights are fixed and only train the
DT-RNN parameters WI . If larger training corpora
become available in the future, training both jointly
becomes feasible and would present a very promis-
ing direction. We use a modified version of Ada-
Grad (Duchi et al., 2011) for optimization of both
WI and the DT-RNN as well as the other baselines
(except kCCA). Adagrad has achieved good perfor-
mance previously in neural networks models (Dean
et al., 2012; Socher et al., 2013a). We modify it
by resetting all squared gradient sums to 1 every 5
epochs. With both images and sentences in the same
multimodal space, we can easily query the model for
similar images or sentences by finding the nearest
neighbors in terms of negative inner products.
An alternative objective function is based on the
squared loss J(WI , ?) = ?(i,j)?P ?vi ? yj?22. This
requires an alternating minimization scheme that
first trains only WI , then fixes WI and trains the
DT-RNN weights ? and then repeats this several
times. We find that the performance with this ob-
jective function (paired with finding similar images
using Euclidean distances) is worse for all models
than the margin loss of Eq. 5. In addition kCCA
also performs much better using inner products in
the multimodal space.
6 Experiments
We use the dataset of Rashtchian et al. (2010) which
consists of 1000 images, each with 5 sentences. See
Fig. 5 for examples.
We evaluate and compare the DT-RNN in three
different experiments. First, we analyze how well
the sentence vectors capture similarity in visual
meaning. Then we analyze Image Search with
Query Sentences: to query each model with a sen-
tence in order to find an image showing that sen-
213
1 . A woman and her dog watch the cameraman in their living with wooden floors .
2 . A woman sitting on the couch while a black faced dog runs across the floor.
3 . A woman wearing a backpack sits on a couch while a small dog runs on the hardwood floor next to her .
4 . A women sitting on a sofa while a small Jack Russell walks towards the camera .
5 . White and black small dog walks toward the camera while woman sits on couch , desk and computer seen 
    in the background as well as a pillow, teddy bear and moggie toy on the wood floor .
1 . A man in a cowboy hat check approaches a small red sports car .
2 . The back and left side of a red Ferrari and two men admiring it .
3 . The sporty car is admired by passer by .
4 . Two men next to a red sports car in a parking lot .
5 . Two men stand beside a red sports car.
Figure 5: Examples from the dataset of images and their sentence descriptions (Rashtchian et al., 2010). Sentence
length varies greatly and different objects can be mentioned first. Hence, models have to be invariant to word ordering.
tence?s visual ?meaning.? The last experiment De-
scribing Images by Finding Suitable Sentences does
the reverse search where we query the model with an
image and try to find the closest textual description
in the embedding space.
In our comparison to other methods we focus on
those models that can also compute fixed, continu-
ous vectors for sentences. In particular, we compare
to the RNN model on constituency trees of Socher
et al. (2011a), a standard recurrent neural network;
a simple bag-of-words baseline which averages the
words. All models use the word vectors provided by
Huang et al. (2012) and do not update them as dis-
cussed above. Models are trained with their corre-
sponding gradients and backpropagation techniques.
A standard recurrent model is used where the hidden
vector at word index t is computed from the hidden
vector at the previous time step and the current word
vector: ht = f(Whht?1 + Wxxt). During training,
we take the last hidden vector of the sentence chain
and propagate the error into that. It is also this vector
that is used to represent the sentence.
Other possible comparisons are to the very differ-
ent models mentioned in the related work section.
These models use a lot more task-specific engineer-
ing, such as running object detectors with bounding
boxes, attribute classifiers, scene classifiers, CRFs
for composing the sentences, etc. Another line of
work uses large sentence-image aligned resources
(Kuznetsova et al., 2012), whereas we focus on eas-
ily obtainable training data of each modality sepa-
rately and a rather small multimodal corpus.
In our experiments we split the data into 800 train-
ing, 100 development and 100 test images. Since
there are 5 sentences describing each image, we
have 4000 training sentences and 500 testing sen-
tences. The dataset has 3020 unique words, half of
which only appear once. Hence, the unsupervised,
pre-trained semantic word vector representations are
crucial. Word vectors are not fine tuned during train-
ing. Hence, the main parameters are the DT-RNN?s
Wl?,Wr? or the semantic matrices of which there are
141 and the image mappingWI . For both DT-RNNs
the weight matrices are initialized to block identity
matrices plus Gaussian noise. Word vectors and hid-
den vectors are set o length 50. Using the develop-
ment split, we found ? = 0.08 and the learning rate
of AdaGrad to 0.0001. The best model uses a mar-
gin of ? = 3.
Inspired by Socher and Fei-Fei (2010) and Ho-
dosh et al. (2013) we also compare to kernelized
Canonical Correlation Analysis (kCCA). We use the
average of word vectors for describing sentences and
the same powerful image vectors as before. We
use the code of Socher and Fei-Fei (2010). Tech-
nically, one could combine the recently introduced
deep CCA Andrew et al. (2013) and train the re-
cursive neural network architectures with the CCA
objective. We leave this to future work. With lin-
ear kernels, kCCA does well for image search but
is worse for sentence self similarity and describing
images with sentences close-by in embedding space.
All other models are trained by replacing the DT-
RNN function in Eq. 5.
6.1 Similarity of Sentences Describing the
Same Image
In this experiment, we first map all 500 sentences
from the test set into the multi-modal space. Then
for each sentence, we find the nearest neighbor sen-
214
Sentences Similarity for Image
Model Mean Rank
Random 101.1
BoW 11.8
CT-RNN 15.8
Recurrent NN 18.5
kCCA 10.7
DT-RNN 11.1
SDT-RNN 10.5
Image Search
Model Mean Rank
Random 52.1
BoW 14.6
CT-RNN 16.1
Recurrent NN 19.2
kCCA 15.9
DT-RNN 13.6
SDT-RNN 12.5
Describing Images
Model Mean Rank
Random 92.1
BoW 21.1
CT-RNN 23.9
Recurrent NN 27.1
kCCA 18.0
DT-RNN 19.2
SDT-RNN 16.9
Table 1: Left: Comparison of methods for sentence similarity judgments. Lower numbers are better since they indicate
that sentences describing the same image rank more highly (are closer). The ranks are out of the 500 sentences in the
test set. Center: Comparison of methods for image search with query sentences. Shown is the average rank of the
single correct image that is being described. Right: Average rank of a correct sentence description for a query image.
tences in terms of inner products. We then sort
these neighbors and record the rank or position of
the nearest sentence that describes the same im-
age. If all the images were very unique and the vi-
sual descriptions close-paraphrases and consistent,
we would expect a very low rank. However, usually
a handful of images are quite similar (for instance,
there are various images of airplanes flying, parking,
taxiing or waiting on the runway) and sentence de-
scriptions can vary greatly in detail and specificity
for the same image.
Table 1 (left) shows the results. We can see that
averaging the high quality word vectors already cap-
tures a lot of similarity. The chain structure of a
standard recurrent neural net performs worst since
its representation is dominated by the last words in
the sequence which may not be as important as ear-
lier words.
6.2 Image Search with Query Sentences
This experiment evaluates how well we can find im-
ages that display the visual meaning of a given sen-
tence. We first map a query sentence into the vector
space and then find images in the same space using
simple inner products. As shown in Table 1 (center),
the new DT-RNN outperforms all other models.
6.3 Describing Images by Finding Suitable
Sentences
Lastly, we repeat the above experiments but with
roles reversed. For an image, we search for suitable
textual descriptions again simply by finding close-
by sentence vectors in the multi-modal embedding
space. Table 1 (right) shows that the DT-RNN again
outperforms related models. Fig. 2assigned to im-
Image Search
Model mRank
BoW 24.7
CT-RNN 22.2
Recurrent NN 28.4
kCCA 13.7
DT-RNN 13.3
SDT-RNN 15.8
Describing Images
Model mRank
BoW 30.7
CT-RNN 29.4
Recurrent NN 31.4
kCCA 38.0
DT-RNN 26.8
SDT-RNN 37.5
Table 2: Results of multimodal ranking when models are
trained with a squared error loss and using Euclidean dis-
tance in the multimodal space. Better performance is
reached for all models when trained in a max-margin loss
and using inner products as in the previous table.
ages. The average ranking of 25.3 for a correct sen-
tence description is out of 500 possible sentences. A
random assignment would give an average ranking
of 100.
6.4 Analysis: Squared Error Loss vs. Margin
Loss
We analyze the influence of the multimodal loss
function on the performance. In addition, we com-
pare using Euclidean distances instead of inner prod-
ucts. Table 2 shows that performance is worse for all
models in this setting.
6.5 Analysis: Recall at n vs Mean Rank
Hodosh et al. (2013) and other related work use re-
call at n as an evaluation measure. Recall at n cap-
tures how often one of the top n closest vectors were
a correct image or sentence and gives a good intu-
ition of how a model would perform in a ranking
task that presents n such results to a user. Below, we
compare three commonly used and high performing
models: bag of words, kCCA and our SDT-RNN on
215
A gray convertible sports car is parked in front of the trees .
A close-up view of the headlights of a blue old -fashioned car.
Black shiny sports car parked on concrete driveway .
Five cows grazing on a patch of grass between two roadways .
A jockey rides a brown and white horse in a dirt corral .
A young woman is riding a Bay hose in a dirt riding -ring.
A white bird pushes a miniature teal shopping cart.
A person rides a brown horse.
A motocross bike with rider flying through the air .
White propeller plane parked in middle of grassy field .
The white jet with its landing gear down flies in the blue sky .
An elderly woman catches a ride on the back of the bicycle .
A green steam train running down the tracks.
Steamy locomotive speeding thou the forest .
A steam engine comes down a train track near trees.
A double decker bus is driving by Big Ben in London .
People in an outrigger canoe sail on emerald green water .
Two people sailing a small white sail boat.
behind a cliff, a boat sails away
Tourist move in on Big Ben on a typical overcast London day .
A group of people sitting around a table on a porch.
A group of four people walking past a giant mushroom.
A man and women smiling for the camera in a kitchen.
A group of men sitting around a table drinking while a man behind 
stands pointing.
Figure 6: Images and their sentence descriptions assigned by the DT-RNN.
Image Search
Model mRank 4 R@1 5 R@5 5 R@10 5
BoW 14.6 15.8 42.2 60.0
kCCA 15.9 16.4 41.4 58.0
SDT-RNN 12.5 16.4 46.6 65.6
Describing Images
BoW 21.1 19.0 38.0 57.0
kCCA 18.0 21.0 47.0 61.0
SDT-RNN 16.9 23.0 45.0 63.0
Table 3: Evaluation comparison between mean rank of
the closest correct image or sentence (lower is better 4)
with recall at different thresholds (higher is better, 5).
With one exception (R@5, bottom table), the SDT-RNN
outperforms the other two models and all other models
we did not include here.
this different metric. Table 3 shows that the mea-
sures do correlate well and the SDT-RNN also per-
forms best on the multimodal ranking tasks when
evaluated with this measure.
6.6 Error Analysis
In order to understand the main problems with the
composed sentence vectors, we analyze the sen-
tences that have the worst nearest neighbor rank be-
tween each other. We find that the main failure mode
of the SDT-RNN occurs when a sentence that should
describe the same image does not use a verb but the
other sentences of that image do include a verb. For
example, the following sentence pair has vectors that
are very far apart from each other even though they
are supposed to describe the same image:
1. A blue and yellow airplane flying straight down
while emitting white smoke
2. Airplane in dive position
Generally, as long as both sentences either have a
verb or do not, the SDT-RNN is more robust to dif-
ferent sentence lengths than bag of words represen-
tations.
6.7 Model Analysis: Semantic Composition
Matrices
The best model uses composition matrices based on
semantic relationships from the dependency parser.
We give some insights into what the model learns
by listing the composition matrices with the largest
Frobenius norms. Intuitively, these matrices have
learned larger weights that are being multiplied with
the child vector in the tree and hence that child will
have more weight in the final composed parent vec-
tor. In decreasing order of Frobenius norm, the re-
lationship matrices are: nominal subject, possession
modifier (e.g. their), passive auxiliary, preposition
at, preposition in front of, passive auxiliary, passive
nominal subject, object of preposition, preposition
in and preposition on.
The model learns that nouns are very important as
well as their spatial prepositions and adjectives.
7 Conclusion
We introduced a new recursive neural network
model that is based on dependency trees. For eval-
uation, we use the challenging task of mapping sen-
tences and images into a common space for finding
one from the other. Our new model outperforms
baselines and other commonly used models that can
compute continuous vector representations for sen-
tences. In comparison to related models, the DT-
RNN is more invariant and robust to surface changes
such as word order.
216
References
G. Andrew, R. Arora, K. Livescu, and J. Bilmes. 2013.
Deep canonical correlation analysis. In ICML, At-
lanta, Georgia.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
F. Costa, P. Frasconi, V. Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language using
recursive neural networks. Applied Intelligence.
M. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin,
Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker,
K. Yang, and A.Y. Ng. 2012. Large scale distributed
deep networks. In NIPS.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic op-
timization. JMLR, 12, July.
P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation. In
ECCV.
A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010.
Every picture tells a story: Generating sentences from
images. In ECCV.
Y. Feng and M. Lapata. 2013. Automatic caption gen-
eration for news images. IEEE Trans. Pattern Anal.
Mach. Intell., 35.
A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean,
M. Ranzato, and T. Mikolov. 2013. Devise: A deep
visual-semantic embedding model. In NIPS.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks.
E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning for
compositional distributional semantics. In IWCS.
A. Gupta and L. S. Davis. 2008. Beyond nouns: Exploit-
ing prepositions and comparative adjectives for learn-
ing visual classifiers. In ECCV.
M. Hodosh, P. Young, and J. Hockenmaier. 2013. Fram-
ing image description as a ranking task: Data, mod-
els and evaluation metrics. J. Artif. Intell. Res. (JAIR),
47:853?899.
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
K. Jarrett, K. Kavukcuoglu, M.A. Ranzato, and Y. Le-
Cun. 2009. What is the best multi-stage architecture
for object recognition? In ICCV.
P. Blunsom. K.M. Hermann. 2013. The role of syntax
in vector space models of compositional semantics. In
ACL.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
Imagenet classification with deep convolutional neural
networks. In NIPS.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.
Berg, and T. L. Berg. 2011. Baby talk: Understanding
and generating image descriptions. In CVPR.
N. Kumar, A. C. Berg, P. N. Belhumeur, , and S. K. Na-
yar. 2009. Attribute and simile classifiers for face ver-
ification. In ICCV.
P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Yejin Choi. 2012. Collective generation of natural
image descriptions. In ACL.
Q. V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen,
G.S. Corrado, J. Dean, and A. Y. Ng. 2012. Build-
ing high-level features using large scale unsupervised
learning. In ICML.
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguistic
regularities in continuous spaceword representations.
In HLT-NAACL.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388?1429.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y.
Ng. 2011. Multimodal deep learning. In ICML.
V. Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. In NIPS.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46, November.
C. Rashtchian, P. Young, M. Hodosh, and J. Hocken-
maier. 2010. Collecting image annotations using
Amazon?s Mechanical Turk. In Workshop on Creat-
ing Speech and Language Data with Amazon?s MTurk.
R. Socher and L. Fei-Fei. 2010. Connecting modalities:
Semi-supervised segmentation and annotation of im-
ages using unaligned text corpora. In CVPR.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
217
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
In NIPS.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
In EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In EMNLP.
R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng. 2013a.
Parsing With Compositional Vector Grammars. In
ACL.
R. Socher, M. Ganjoo, C. D. Manning, and A. Y. Ng.
2013b. Zero-Shot Learning Through Cross-Modal
Transfer. In NIPS.
R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, and
A. Y. Ng. C. D. Manning and. 2013c. Zero-shot learn-
ing through cross-modal transfer. In Proceedings of
the International Conference on Learning Representa-
tions (ICLR, Workshop Track).
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In NIPS.
A. Torralba, K. P. Murphy, and W. T. Freeman. 2010.
Using the forest to see the trees: exploiting context for
visual object detection and localization. Communica-
tions of the ACM.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
B. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. 2010.
I2t:image parsing to text description. IEEE Xplore.
218
