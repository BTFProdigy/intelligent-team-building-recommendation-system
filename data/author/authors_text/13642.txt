Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 581?589,
Beijing, August 2010
Improving the Quality of Text Understanding by Delaying Ambiguity
Resolution
Doo Soon Kim
Dept. of Computer Science
University of Texas
onue5@cs.utexas.edu
Ken Barker
Dept. of Computer Science
University of Texas
kbarker@cs.utexas.edu
Bruce Porter
Dept. of Computer Science
University of Texas
porter@cs.utexas.edu
Abstract
Text Understanding systems often commit
to a single best interpretation of a sen-
tence before analyzing subsequent text.
This interpretation is chosen by resolv-
ing ambiguous alternatives to the one with
the highest confidence, given the context
available at the time of commitment. Sub-
sequent text, however, may contain infor-
mation that changes the confidence of al-
ternatives. This may especially be the
case with multiple redundant texts on the
same topic. Ideally, systems would de-
lay choosing among ambiguous alterna-
tives until more text has been read.
One solution is to maintain multiple can-
didate interpretations of each sentence un-
til the system acquires disambiguating ev-
idence. Unfortunately, the number of al-
ternatives explodes quickly. In this pa-
per, we propose a packed graphical (PG)
representation that can efficiently repre-
sent a large number of alternative interpre-
tations along with dependencies among
them. We also present an algorithm for
combining multiple PG representations to
help resolve ambiguity and prune alterna-
tives when the time comes to commit to a
single interpretation.
Our controlled experiments show that by
delaying ambiguity resolution until multi-
ple texts have been read, our prototype?s
accuracy is higher than when committing
to interpretations sentence-by-sentence.
1 Introduction
A typical text understanding system confronts am-
biguity while parsing, mapping words to concepts
and formal relations, resolving co-references, and
integrating knowledge derived from separate sen-
tences or texts. The system discards many candi-
date interpretations to avoid combinatorial explo-
sion. Commonly, after reading each sentence, a
system will commit to its top ranked interpreta-
tion of the sentence before reading the next.
If a text understanding system could postpone
committing to an interpretation without being
swamped by a combinatorial explosion of alterna-
tives, its accuracy would almost surely improve.
This intuition follows from the observation that
text is redundant in at least two ways. First, within
a single coherent text (about the same entities
and events), each sentence informs the interpre-
tation of its neighbors. Second, within a corpus of
texts on the same topic, the same information is
expressed in different surface forms, ambiguous
in different ways. Related fields, such as Infor-
mation Extraction, exploit textual redundancy to
good effect, and perhaps text understanding can
as well.
One approach is for the text understanding sys-
tem to maintain multiple complete candidate in-
terpretations. After reading each sentence, for ex-
ample, the system would retain a beam of the n-
best interpretations of the sentence. While this
approach avoids a combinatorial explosion (for
reasonable values of n), several problems remain.
First, because the beam width is limited, the sys-
tem may still discard correct interpretations before
benefiting from the extra context from related text.
Second, enumeration of the candidate interpreta-
581
tions does not represent the dependencies among
them. For example, there may be multiple candi-
date word senses and semantic roles for a given
sentence, but sense alternatives might be depen-
dent on role selection (and vice-versa). The set
of reasonable interpretations may be a subset of
all combinations. Finally, maintaining distinct in-
terpretations does not contribute to addressing the
problem of combining evidence to narrow down
alternatives and ultimately select a single best in-
terpretation of a text.
This paper addresses these three problems. We
propose an approach that postpones committing to
an interpretation of a text by representing ambi-
guities and the dependencies among them. There
may still be combinatorial growth in the set of al-
ternative interpretations, but they are represented
only intensionally, using a packed representation,
which maintains alternatives while avoiding enu-
merating them. We also propose an algorithm for
updating and pruning the packed representation as
more sentences and texts are read.
We evaluate our approach by comparing two
reading systems: a baseline system that commits
to its best interpretation after each sentence, and
our prototype system that uses a packed represen-
tation to maintain all interpretations until further
reading enables it to prune. For this initial proof of
concept, we use a small corpus of redundant texts.
The results indicate that our approach improves
the quality of text interpretation by preventing ag-
gressive pruning while avoiding combinatorial ex-
plosion.
In the following sections, we first describe our
target semantic representation of the interpreta-
tion of sentences. We then present the details
of our packed graphical representation (PG rep-
resentation) and our algorithm to resolve ambi-
guities in the PG representations as disambiguat-
ing evidence from subsequent text accrues. We
describe the architecture of a prototype that pro-
duces PG representations for text and implements
the disambiguating algorithm. Finally, we present
the results from controlled experiments designed
to compare the accuracy of the prototype to a
baseline system that prunes more aggressively.
Figure 1: The target semantic graph representa-
tion for S1
2 Target semantic representation
Our target representation is a semantic graph in
which nodes are words and the ontological types
to which they map. Edges are semantic relations
corresponding either to function words or syntac-
tic relations in the sentence?s parse.
Fig. 1 shows the target semantic representation
for the following simple sentence:
S1: An engine ignites gasoline with its spark plug.
3 PG representation
Alternative semantic interpretations for a sentence
can be captured with a single PG representation
with ambiguities represented as local alternatives.
Because candidate representations are often struc-
turally similar, a PG representation can signifi-
cantly compress the representation of alternatives.
Fig. 2 shows the PG representation of alternate
interpretations of S1 (PG1). The different types of
ambiguity captured by the PG representation are
as follows.
3.1 Word-Type ambiguity
In PG1, the node engine-2a corresponds to the
word ?engine? in S1. Its annotation [LIVING-
ENTITY .3 | DEVICE .7] captures the map-
ping to either LIVING-ENTITY (probability 0.3)
or DEVICE (probability 0.7). The PG repre-
sentation does not presume a particular uncer-
Figure 2: The PG representation for S1 (PG1)
582
tainty formalism. Any formalism, (Dempster-
Shafer theory (Pearl, 1988), Markov Logic Net-
works (Richardson and Domingos, 2006), etc.)
could be used.
3.2 Semantic Relation ambiguity
In PG1, the edge label <agent .6 | location .4>
from ignite-3a to engine-2a says that the engine is
either agent or location of the ignition.
3.3 Structural ambiguity
In PG1, edges D and E are alternatives corre-
sponding to the different prepositional phrase at-
tachments for ?with its spark plug? (to ignite-3a
or gasoline-4a). The annotation {D .3 | E .7} says
that the choices are mutually exclusive with prob-
abilities of 0.3 and 0.7.
3.4 Co-reference ambiguity
Co-reference of nodes in a PG representation is
captured using a ?co-reference? edge. In PG1, the
edge labeled <coref .7> represents the probabil-
ity that engine-2a and its-7a are co-referent.
In addition to storing ambiguities explicitly,
the PG representation also captures dependencies
among alternatives.
3.5 Simple dependency
The existence of one element in the graph de-
pends on the existence of another element. If
subsequent evidence suggests that an element is
incorrect, its dependents should be pruned. For
example, the dependency A ? C, means that if
LIVING-ENTITY is ultimately rejected as the type
for engine-2a, the agent relation should be pruned.
3.6 Mutual dependency
Elements of a mutual dependency set are mutually
confirming. Evidence confirming or rejecting an
element also confirms or rejects other elements in
the set. In the example, the box labeled B says that
(engine-2a type DEVICE) and (ignite-3a location
engine-2a) should both be confirmed or pruned
when either of them is confirmed or pruned.
Formally, the PG representation is a structure
consisting of (a) semantic triples ? e.g., (ignite-
3a type BURN), (b) macros ? e.g., the symbol A
refers to (ignite-3a agent engine-2a), and (c) con-
straints ? e.g., A depends on C.
4 Combining PG representations
Maintaining ambiguity within a PG representation
allows us to delay commitment to an interpreta-
tion until disambiguating evidence appears. For
any text fragment that results in a PG represen-
tation (PGa) containing ambiguity, there may ex-
ist other text fragments that are partly redundant,
but result in a less ambiguous (or differently am-
biguous) representation (PGb). PGb can be used
to adjust confidences in PGa. Enough such evi-
dence allows us to prune unlikely interpretations,
ultimately disambiguating the original representa-
tion.
For example, sentence S3 does not have suffi-
cient context to disambiguate between the MO-
TOR sense of ?engine? and the VEHICLE sense (as
in locomotive).
S3: General Electric announced plans this week
for their much anticipated new engine.
The PG3 representation for S3 (PG3) would
maintain the ambiguous representation (with con-
fidences for each sense based on prior probabil-
ities, for example). On subsequently encounter-
ing sentence S4, a Lesk-based word sense disam-
biguation module (as in our prototype) would pro-
duce a PG4 with a strong preference for the loco-
motive sense of ?engine?, given the more specific
context of S4.
S4: The announcement comes to the relief of many
in the railway industry looking to replace the en-
gines in their aging locomotive fleets.
To use PG4 to help disambiguate PG3, we need
to align PG3 and PG4 semantically and merge
their conflict sets. (In the simple example, the
conflict sets for the word ?engine? might be [MO-
TOR .5 | VEHICLE .5] in PG3 and [MOTOR .2 |
VEHICLE .8] in PG4).
Algorithm 1 describes how two PG representa-
tions can be combined to help resolve their ambi-
guities. The algorithm identifies their isomorphic
subgraphs (redundant portions of the interpreta-
tions) and uses the information to disambiguate
their ambiguities. For illustration, we will step
through Algorithm 1, merging PG1 (Fig. 2) with
583
Algorithm 1 Disambiguating PG representations
Input : PG1, PG2
Output: new PG representation
1. Identify semantically aligned parts between
PG1 and PG2. Use graph matching to identify
alignments (redundant portions) between PG1
and PG2: align nodes with the same base word
or with taxonomically related types; from the
node alignments, align identical types as type
alignments; align relations if the relations are
the same and their head and tail nodes have
been aligned.
2. Use alignments to disambiguate PG1 and
PG2. With the available information (the con-
fidence scores and the constraints in PG1 and
PG2 and the alignments between them), use
joint inference to calculate the confidence score
of each candidate interpretation. If the con-
fidence score of one interpretation becomes
much higher than competing ones, the interpre-
tation is chosen while the others are discarded.
3. Combine the disambiguated PG1 and PG2
into one PG representation using the align-
ments identified in the first step.
Figure 3: PG representation for S2, ?The engine?s
spark plug combusts gasoline.?
PG2 (Fig. 3).
1. The graph matcher identifies alignments
between PG1 and PG2. Type alignments include
(engine-2a[DEVICE], Engine-1b[DEVICE]),
(spark-plug-8a[LIVING-ENTITY], spark-plug-
3b[LIVING-ENTITY]). Relation alignments
include ((combust-5b instrument spark-plug-3b),
(ignite-3 instrument spark-plug-8)), ((ignite-3a
instrument spark-plug-8a) (combust-5b instru-
ment spark-plug-3b)).
2. In this example, when two interpreta-
tions are aligned, we simply add their confi-
dence scores. (We are currently incorporating
Alchemy (Richardson and Domingos, 2006) in the
prototype system to do the joint inference). For
example, aligning engine-2a with Engine-1b re-
sults in a score of 1.7 for DEVICE (1 + .7). The
confidence score of LIVING-ENTITY in engine-
2a is unchanged at .3. Since the resulting score
for DEVICE is much higher than 1 the score for
LIVING-ENTITY, LIVING-ENTITY is discarded.
Deleting LIVING-ENTITY causes deletion of the
agent edge between ignite-3a and engine-2a due
to the dependency constraint A ? C.
3. The disambiguated PG1 and PG2 are merged
into a single PG representation (PG1+2) based on
the alignments. Any remaining ambiguity persists
in PG1+2, possibly to be resolved with another
sentence.
5 Prototype system
5.1 Parser
Our prototype system uses the Stanford
Parser (Klein and Manning, 2003). To cap-
ture structural ambiguity for our experiments,
we manually edited the parser output by adding
corrections as alternatives wherever the parse
tree was incorrect. This gave a syntactic PG
representation with both incorrect and correct
alternatives. We gave the original, incorrect
alternatives high confidence scores and the added,
correct alternatives low scores, simulating a
parser pruning correct interpretations in favor
of incorrect ones with higher confidence scores.
The syntactic PG for S1 is shown in Fig. 4. We
have recently designed a modification to the
Stanford Parser to make it produce syntactic PG
representations natively, based on the complete
chart built during parsing.
5.2 Semantic Interpreter
The semantic interpreter assigns types to nodes in
the syntactic PG representation and semantic rela-
tions to the edges.
Type ambiguity. Types and confidence scores
are assigned to words using SenseRelate (Pat-
wardhan et al, 2005), WSD software based on the
1In our prototype, we set the pruning threshold at 13?the
score of the top-scored interpretation.
584
Lesk Algorithm (Lesk, 1986). Assigned senses
are then mapped to our Component Library ontol-
ogy (Barker et al, 2001) using its built-in Word-
Net mappings.
Relational ambiguity. Semantic relations are
assigned to the dependency relations in the syn-
tactic PG representation according to semantic in-
terpretation rules. Most rules consider the head
and tail types as well as the dependency relation,
but do not produce confidence scores. Our proto-
type scores candidates equally. We plan to incor-
porate a more sophisticated scoring method such
as (Punyakanok et al, 2005).
Structural ambiguity. Parse ambiguities (such
as PA vs. PB in Fig. 4) are converted directly to
structural ambiguity representations (D vs. E in
Fig. 2) in the semantic PG representation.
Simple Dependency. A dependency is in-
stalled between a type t for word w and a semantic
relation r when (1) r is produced by a rule based
on t and (2) r is dependent on no other candidate
type for w. In Fig. 2, a dependency relation is in-
stalled from A to C, because (1) LIVING-ENTITY
in engine-2a was used in the rule assigning agent
between ignite-3a and engine-2a and (2) the as-
signment of agent is not dependent on DEVICE,
the other candidate type of engine-2a.
Mutual dependency. If multiple interpreta-
tions depend on one another, a mutual dependency
set is created to include them.
5.3 PG Merger
The PG Merger implements Algorithm 1 to com-
bine PG representations. The PG representation
Figure 4: Syntactic PG representation for S1, cap-
turing the PP-attachment ambiguity of ?with its
spark plug?.
Original Text Hearts pump blood through the body.
Blood carries oxygen to organs throughout the body.
Blood leaves the heart, then goes to the lungs where
it is oxygenated. The oxygen given to the blood by the
lungs is then burned by organs throughout the body.
Eventually the blood returns to the heart, depleted of
oxygen.
Paraphrase The heart begins to pump blood into the
body. The blood first travels to the lungs, where it
picks up oxygen. The blood will then be deposited
into the organs, which burn the oxygen. The blood
will then return to the heart, where it will be lacking
oxygen, and start over again.
Figure 5: The original text and a paraphrase
for each sentence is merged with the cumulative
PG from previous sentences. The global PG repre-
sentation integrates sentence-level PG representa-
tions to the extent that they align semantically. In
the worst case (completely unrelated sentences),
the global PG representation would simply be the
union of individual PG representations. The ex-
tent to which the global PG is more coherent re-
flects redundancy and semantic overlap in the sen-
tences.
6 Experiment 1
We first wanted to evaluate our hypothesis that
Algorithm 1 can improve interpretation accuracy
over multiple redundant texts. We manually
generated ten redundant texts by having volun-
teers rewrite a short, tutorial text, using Amazon
Turk (http://mturk.com) 2 The volunteers had no
knowledge of the purpose of the task, and were
asked to rewrite the text using ?different? lan-
guage. Fig. 5 shows the original text and one vol-
unteer?s rewrite. The total number of sentences
over the ten texts was 37. Average sentence length
was 14.5 words.
6.1 Evaluation Procedure
We ran two systems over the ten texts. The base-
line system commits to the highest scoring consis-
tent interpretation after each sentence. The pro-
totype system produces an ambiguity-preserving
2We ultimately envision a system whose task is to develop
a model of a particular topic by interpreting multiple texts.
Such a system might be given a cluster of documents or use
its own information retrieval to find similar documents given
a tutorial text.
585
0 10 20 30 40
0.65
0.7
0.75
0.8
0.85
0.9
number of sentences
co
rr
e
ct
ne
ss
(%
)
type triples
 
 
0 10 20 30 40
0.65
0.7
0.75
0.8
0.85
0.9
content triples
0 10 20 30 40
0.65
0.7
0.75
0.8
0.85
0.9
all triples
prototype
baseline
Figure 6: Correctness scores for the prototype vs. baseline system on (a) type triples (word sense assignment), (b) content
triples (semantic relations) and (c) all triples (with standard deviation).
PG representation. For each sentence, the proto-
type?s PG Merger merges the PG of the sentence
with the merged PG of the previous sentences. Af-
ter N sentences (varying N from 1..37), the system
is forced to commit to the highest scoring con-
sistent interpretation in the merged PG. For N=1
(commit after the first sentence), both the base-
line and prototype produce the same result. For
N=2, the baseline produces the union of the high-
est scoring interpretations for each of the first two
sentences. The prototype produces a merged PG
for the first two sentences and then prunes to the
highest scoring alternatives.
At each value of N, we measured the cor-
rectness of the interpretations (the percentage
of correct semantic triples) for each system by
comparing the committed triples against human-
generated gold standard triples.
We repeated the experiment ten times with dif-
ferent random orderings of the 37 sentences, aver-
aging the results.
6.2 Evaluation result
Fig. 6 shows that both type assignment and se-
mantic relation assignment by the prototype im-
prove as the system reads more sentences. This
result confirms our hypothesis that delaying com-
mitment to an interpretation resolves ambiguities
better by avoiding overly aggressive pruning.
To determine an upper bound of correctness for
the prototype, we inspected the PG representa-
tions to see how many alternative sets contained
the correct interpretation even if not the highest
scoring alternative. This number is different from
the correctness score in Fig. 6, which is the per-
baseline prototype
nodes w/ the correct type 76 91
edges w/ the correct relation 74 88
Table 1: Percentage of nodes and edges containing the cor-
rect types and semantic relations in the baseline and the pro-
totype for all 37 sentences.
centage of gold standard triples that are the high-
est scoring alternatives in the merged PG.
Table. 1 shows that 91% of the nodes in the PG
contain the correct type (though not necessarily
the highest scoring). 88% of the edges contain the
correct semantic relations among the alternatives.
In contrast, the baseline has pruned away 24% of
the correct types and 26% of the correct semantic
relations.
7 Experiment 2
Our second experiment aims to evaluate the claim
that the prototype can efficiently manage a large
number of alternative interpretations. The top line
in Fig. 7 shows the number of triples in the PG
representations input to the prototype. This is the
total number of triples (including ambiguous al-
ternatives) in the PG for each sentence prior to in-
voking Algorithm 1. The middle line is the num-
ber of triples remaining after merging and pruning
by Algorithm 1. The bottom line is the number of
triples after pruning all but the highest scoring al-
ternatives (the baseline system). The results show
that Algorithm 1 achieves significant compression
over unmerged PG representations. The result-
ing size of the merged PG representations more
closely tracks the size of the aggressively pruned
586
0 5 10 15 20 25 30 35 40
0
200
400
600
800
1000
1200
bar: standard deviation
5 times repeated
sentences
n
u
m
be
r o
f t
rip
le
s
 
 
triples in input PG representations
triples in the PG representation after merging
triples in the baseline system
Figure 7: Total number of triples in individual sentence PG
representations (top); total number of triples in the PG rep-
resentation after merging in the prototype system (middle);
total number of triples after pruning to the highest scoring
alternative (bottom).
representations.
8 Experiment 3
Finally, we wanted to measure the sensitivity of
our approach to the quality of the natural language
interpretation. In this experiment, we artificially
varied the confidence scores for the correct inter-
pretations in the PG representations input to the
prototype and baseline systems by a fixed per-
centage. For example, consider a node heart-1
with multiple candidate types, including the cor-
rect sense for its context: INTERNAL-ORGAN
with confidence 0.8. We reran Experiment 1 vary-
ing the confidence in INTERNAL-ORGAN in in-
crements of +/-10%, while scaling the confidences
in the incorrect types equally. As the confidence
in correct interpretations is increased, all correct
interpretations become the highest scoring, so ag-
gressive pruning is justified and the baseline per-
formance approaches the prototype performance.
As the confidences in correct interpretations are
decreased, they are more likely to be pruned by
both systems.
Fig. 8 shows that Algorithm 1 is able to recover
at least some correct interpretations even when
their original scores (relative to incorrect alterna-
tives) is quite low.
9 Discussion and Future Work
Our controlled experiments suggest that it is both
desirable and feasible to delay ambiguity resolu-
bar: standard deviation
5 times repeated
0.4 0.5 0.6 0.7 0.8 0.9
0.4
0.5
0.6
0.7
0.8
0.9
the quality of the triples in the baseline system(%)
th
e 
qu
al
ity
 o
f t
he
 tr
ip
le
s 
in
 th
e 
pr
ot
ot
yp
e 
sy
st
em
 (%
)
 
 
prototype
baseline
Figure 8: Sensitivity of the prototype and baseline systems
to the quality of the NL system output. The quality of in-
put triples is perturbed affecting performance accuracy of the
two systems. For example, when the quality of input triples
is such that the baseline system performs at 70% accuracy,
the prototype system performs at 80%. The arrow indicates
unperturbed language interpreter performance.
tion beyond sentence and text boundaries. Im-
provements in the correctness of semantic inter-
pretation of sentences is possible without an ex-
plosion in size when maintaining multiple inter-
pretations.
Nevertheless, these experiments are proofs of
concept. The results confirm that it is worthwhile
to subject our prototype to a more real-world,
practical application. To do so, we need to address
several issues.
First, we manually simulated structural (parse)
ambiguities. We will complete modifications to
the Stanford Parser to produce PG representations
natively. This change will result in a significant
increase in the number of alternatives stored in
the PG representation over the current prototype.
Our initial investigations suggest that there is still
enough structural overlap among the candidate
parse trees to allow the PG representation to con-
trol explosion, but this is an empirical question
that will need to be confirmed.
We are modifying our semantic interpreter to
admit induced semantic interpretation rules which
will allow us to train the system in new domains.
The current prototype uses a naive heuristic for
identifying co-reference candidates. We are inves-
tigating the use of off-the-shelf co-reference sys-
tems.
Finally, we are incorporating the
Alchemy (Richardson and Domingos, 2006)
587
probabilistic inference engine to calculate the
probability that a candidate interpretation is
correct given the PG constraints and alignments,
in order to inform confirmation or pruning of
interpretations.
Once these updates are complete, we will per-
form more wide-scale evaluations. We will inves-
tigate the automatic construction of a test corpus
using text clustering to find redundant texts, and
we will conduct experiments in multiple domains.
10 Related Work
Succinctly representing multiple interpretations
has been explored by several researchers. The
packed representation (Maxwell III and Kaplan,
1981; Crouch, 2005) uses logical formulae to de-
note alternative interpretations and treats the dis-
ambiguation task as the propositional satisfiabil-
ity problem. Core Language Engine (Alshawi,
1992) introduces two types of packing mecha-
nism. First, a quasi logical form allows the under-
specification of several types of information, such
as anaphoric references, ellipsis and semantic re-
lations (Alshawi and Crouch, 1992). Second, a
packed quasi logical form (Alshawi, 1992) com-
pactly represents the derivations of alternative
quasi logical forms. In contrast, the PG repre-
sentation is (1) based on a graphical representa-
tion, (2) explicitly represents constraints and (3)
includes confidence scores.
These representations and the PG represen-
tation have one feature in common: they rep-
resent a set of complete alternative interpreta-
tions of a text. Another class of compact repre-
sentations, called ?underspecification?, has been
studied as a formal representation of ambigu-
ous sentences. These representations include
Hole Semantics (Bos, 2004), Underspecified Dis-
course Representation Semantics (Reyle, 1995),
Minimal Recursion Semantics (Copestake et al,
2005) and Dominance Constraints (Egg et al,
2001). These representations, rather than packing
fully-represented candidate interpretations, spec-
ify fragments of interpretations which are un-
ambiguously interpreted, along with constraints
on their combination (corresponding to different
interpretations). They generally focus on spe-
cific ambiguities such as scope ambiguity (Bos,
2004) (Egg et al, 2001) (Copestake et al, 2005)
or discourse relations (Schilder, 1998) (Regneri et
al., 2008).
Disambiguating compact representations has
received relatively less attention. (Riezler et al,
2002; Geman and Johnson, 2002) use a packed
representation to train parsers on a corpus and
uses the learned statistics to disambiguate packed
representations. (Clark and Harrison, 2010) uses
paraphrase databases and a hand-built knowledge
base to resolve underspecified representations.
Different architectures have been proposed to
improve the pipeline architecture. (Sutton and
McCallum, 2005; Wellner et al, 2004) maintain
a beam of n best interpretations in the pipeline
architecture. Their pipeline, however, consists of
only two components. (Finkel et al, 2006) uses
sampling over the distribution of alternative inter-
pretations at each stage of the pipeline and then
passes the sampled data to the next component.
The packed representation (Crouch, 2005) and
CLE (Alshawi, 1992) use packed representation in
the pipeline, though both, at some stages, unpack
them and re-pack the processed result. (Crouch
and King, 2006) later proposes a new method that
does not require unpacking and then repacking.
11 Conclusion
We have begun to address the challenge of effi-
ciently managing multiple alternative interpreta-
tions of text. We have presented (1) a packed
graphical representation that succinctly repre-
sents multiple alternative interpretations as well as
the constraints among them, and (2) an algorithm
for combining multiple PG representations to re-
inforce correct interpretations and discount im-
plausible interpretations. Controlled experiments
show that it is possible to improve the correctness
of semantic interpretations of text by delaying dis-
ambiguation, without incurring the cost of an ex-
ponentially expanding representation.
12 Acknowledgement
Support for this research was provided in part by
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program
588
References
Alshawi, Hiyan and Richard S. Crouch. 1992. Mono-
tonic semantic interpretation. In ACL, pages 32?39.
Alshawi, Hiyan, editor. 1992. The Core Language
Engine. MIT Press, Cambridge, Massachusetts.
Barker, Ken, Bruce Porter, and Peter Clark. 2001. A
library of generic concepts for composing knowl-
edge bases. In Proceedings of the international con-
ference on Knowledge capture, pages 14?21.
Bos, Johan. 2004. Computational semantics in dis-
course: Underspecification, resolution, and infer-
ence. Journal of Logic, Language, and Information,
13(2):139?157.
Clark, Peter and Phil Harrison. 2010. Exploiting para-
phrases and deferred sense commitment to interpret
questions more reliably. In To appear in Proceed-
ings of CoLing 2010.
Copestake, Ann, Dan Flickinger, Carl Pollard, and
Ivan Sag. 2005. Minimal recursion semantics: an
introduction. Research on Language and Computa-
tion, 3:281?332.
Crouch, Richard S. and Tracy Holloway King. 2006.
Semantics via f-struecture rewriting. In Proceed-
ings of LFG06 Conference.
Crouch, Dick. 2005. Packed rewriting for mapping se-
mantics to kr. In In Proceedings Sixth International
Workshop on Computational Semantics.
Egg, Markus, Alexander Koller, and Joachim Niehren.
2001. The constraint language for lambda struc-
tures. Journal of Logic, Language, and Information
Vol 10 (4), 2001, pp.457-485, 10:457?485.
Finkel, Jenny Rose, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cas-
cading errors: approximate bayesian inference for
linguistic annotation pipelines. In EMNLP, pages
618?626, Morristown, NJ, USA.
Geman, Stuart and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In ACL, pages 279?
286.
Klein, Dan and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL, pages 423?430,
Morristown, NJ, USA.
Lesk, Michael. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to
tell a pine cone from an ice cream cone. In SIG-
DOC ?86: Proceedings of the 5th annual interna-
tional conference on Systems documentation, pages
24?26, New York, NY, USA.
Maxwell III, John T. and Ronald M. Kaplan. 1981.
A method for disjunctive constraint satisfaction. In
Tomita, Masaru, editor, Current Issues in Pars-
ing Technology, pages 173?190. Kluwer Academic
Publishers, Dordrecht.
Patwardhan, Siddharth, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate:: Targetword-A gen-
eralized framework for word sense disambiguation.
In ACL.
Pearl, Judea. 1988. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan-Kaufmann.
Punyakanok, Vasin, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role
labeling. In Kaelbling, Leslie Pack and Alessandro
Saffiotti, editors, IJCAI, pages 1117?1123. Profes-
sional Book Center.
Regneri, Michaela, Markus Egg, and Alexander
Koller. 2008. Efficient processing of underspecified
discourse representations. In HLT, pages 245?248,
Morristown, NJ, USA.
Reyle, Uwe. 1995. Underspecified discourse repre-
sentation structures and their logic. Logic Journal
of the IGPL, 3(2-3):473?488.
Richardson, Matthew and Pedro Domingos. 2006.
Markov logic networks. Kluwer Academic Publish-
ers.
Riezler, Stefan, Tracy H. King, Ronald M. Kaplan,
Richard S. Crouch, John T. Maxwell III, and Mark
Johnson. 2002. Parsing the wall street journal us-
ing a lexical-functional grammar and discriminative
estimation techniques. In ACL, pages 271?278.
Schilder, Frank. 1998. An underspecified seg-
mented discourse representation theory (USDRT).
In COLING-ACL, pages 1188?1192.
Sutton, Charles and Andrew McCallum. 2005. Joint
parsing and semantic role labeling. In CONLL,
pages 225?228, Morristown, NJ, USA.
Wellner, Ben, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional
model of information extraction and coreference
with application to citation matching. In UAI, pages
593?601, Arlington, Virginia, United States.
589
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1081?1092, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Building a Lightweight Semantic Model for Unsupervised Information
Extraction on Short Listings
Doo Soon Kim
Accenture Technology Lab
50 W San Fernando St.,
San Jose, CA, 95113
Kunal Verma
Accenture Technology Lab
50 W San Fernando St.,
San Jose, CA, 95113
{doo.soon.kim, k.verma, peter.z.yeh}@accenture.com
Peter Z. Yeh
Accenture Technology Lab
50 W San Fernando St.,
San Jose, CA, 95113
Abstract
Short listings such as classified ads or product
listings abound on the web. If a computer can
reliably extract information from them, it will
greatly benefit a variety of applications. Short
listings are, however, challenging to process
due to their informal styles. In this paper, we
present an unsupervised information extrac-
tion system for short listings. Given a cor-
pus of listings, the system builds a seman-
tic model that represents typical objects and
their attributes in the domain of the corpus,
and then uses the model to extract informa-
tion. Two key features in the system are a se-
mantic parser that extracts objects and their at-
tributes and a listing-focused clustering mod-
ule that helps group together extracted tokens
of same type. Our evaluation shows that the
semantic model learned by these two modules
is effective across multiple domains.
1 Introduction
Short listings such as classified ads or product list-
ings are prevalent on the web. These texts are gen-
erally concise ? around 10 words in length. Fig. 1
shows some example listings. Due to the recent ex-
plosive growth of such listings, extracting informa-
tion from them becomes crucial for tasks such as
faceted search and reasoning. For example, con-
sider an online shopping site on which information
about merchandises for sale is posted. Detecting
brands/styles/features that are frequently mentioned
in the postings would allow a company to design a
better marketing strategy.
Most Information Extraction (IE) techniques de-
veloped for formal texts, however, would be inap-
plicable to listings because of their informal and id-
iosyncratic styles. For example, typos, abbreviations
and synonyms often appear and should be resolved
(e.g., apartment/apt, bike/bicycle). Symbols could
have the special meanings (e.g., x in 2x2 in Fig. 1
indicates number of bedrooms and bathrooms). To-
kenization based only on space is insufficient (e.g.,
RawlingBaseball in Fig. 1). Multiwords such as
granite top should also be detected. Applying off-
the-shelf parsers is infeasible because of unusual
phrasal forms in most listings, such as a long se-
quence of nouns/adjectives (e.g., ?New Paint Wood
Floors New Windows Gated Complex?)
To address these challenges, several approaches
have applied machine learning algorithms (Ghani et
al., 2006) (Putthividhya and Hu, 2011) or an external
knowledge base (Michelson and Knoblock, 2005).
These approaches, however, commonly require hu-
man supervision to produce training data or to build
a knowledge base. This is expensive, requiring re-
peated manual effort whenever a new domain or a
new set of information to be extracted is introduced.
In this paper, we present an unsupervised IE sys-
tem for listings. The system extracts tokens from
a corpus of listings and then clusters tokens of
the same types, where each resulting cluster cor-
responds to an information type (e.g., size, brand,
etc.). For formal texts, contexts (e.g., surrounding
words) have been a major feature for word cluster-
ing (Turney and Pantel, 2010). This feature alone,
however, is insufficient for short listings because of
lack of contextual clues in short listings.
1081
2x2 Charming Condo ? 1515 Martin Av-
enue near Downtown (from Craigslist)
HousingType: Condo, BedroomNum: 2, BathroomNum: 2, Lo-
cation: 1515 Martin Avenue, Neighborhood: Downtown
RawlingsBaseball Gloves Pro Preferred
Black 12? (from EBay)
ProductType: Gloves, Brand: Rawlings, Sport: Baseball, Color:
Black, Size: 12?, SeriesName: Pro Preferred
LG 32? 1080p LCD TV ? $329 @
BestBuy (from FatWallet)
Product Type: TV, Brand: Panasonic, Size: 32?, Resolution :
1080P, DisplayTechnology: LCD, Price 329$, Seller: Best Buy
Figure 1: Example short listings and the information extracted from them.
To address this limitation of context-based clus-
tering, we first identify common types of informa-
tion (main objects and their attributes) represented
in listings and apply customized clustering for these
types. Specifically, we define a semantic model
to explicitly represent these information types, and
based on the semantic model, develop two compo-
nents to improve clustering ? a shallow semantic
parser and listing-focused clustering module. The
semantic parser specifically focuses on extracting
main objects and the listing-focused clustering mod-
ule helps group together extracted tokens of the
same type.
Our evaluation shows that our two main con-
tributions (shallow semantic parser and listing-
focused clustering) significantly improve perfor-
mance across three different domains ? Craigslist,
EBay, and FatWallet. Our system achieves .50?.65
F1-score for the EBay and the FatWallet datasets
based on gold standards constructed by human an-
notators. For the Craigslist dataset, which is more
difficult than the other two, F1-score is .35.
2 Related Work
IE on Listings. (Ghani et al 2006) (Putthividhya
and Hu, 2011) propose semi-supervised approaches
to extract product types and their attributes from
product listings. (Ghani et al 2006) applies the
EM (Expectation-Maximization) algorithm to incor-
porate unlabelled data. (Putthividhya and Hu, 2011)
uses unlabelled data to build dictionaries of values to
be extracted (e.g., brand or model names), which are
then used as a feature for a machine learning system.
(Michelson and Knoblock, 2005) uses a manually-
crafted knowledge base called reference set to de-
fine standard forms of values to be extracted. Using
a string edit function, the system then identifies to-
kens in listings that have a low distance score with
the values defined in the reference set. They also
propose a semi-supervised method to building ref-
erence sets (Michelson and Knoblock, 2009).Unlike
these systems, our approach is unsupervised.
Unsupervised Information Extraction. Most un-
supervised IE systems produce clusters for tokens
of same type extracted from a corpus of unlabelled
texts. (Chambers and Jurafsky, 2011) (Poon and
Domingos, 2010) (Chen et al 2011) focus on ex-
tracting frame-like structures (Baker et al 1998)
by defining two types of clusters, event clusters and
role clusters. Event clusters define an event (a sit-
uation or a frame) such as BOMBING by clustering
verbs/nominalized verbs such as {kill, explosion}.
Role clusters define the semantic roles of the event
(e.g., {terrorist, gunman} for the Perpetrator role in
BOMBING). Similarly, our system defines two types
of clusters ? the main concept clusters (e.g., TV or
book) and the attribute clusters (e.g., size, color).
(Chambers and Jurafsky, 2011) is similar to our ap-
proach in that it learns a semantic model, called tem-
plate, from unlabelled news articles and then uses
the template to extract information.
Our system is different because it focuses on in-
formal listings, which the components (such as a
parser) used by these systems cannot handle.
Field Segmentation (Sequence Modelling). This
task focuses on segmenting a short text, such as
bibliographies or listings. (Grenager et al 2005)
presents an unsupervised HMM based on the obser-
vation that the segmented fields tend to be of mul-
tiple words length. (Haghighi and Klein, 2006)
exploits prototype words (e.g., close, near, shop-
ing for the NEIGHBORHOOD attribute) in an un-
supervised setting. (Chang et al 2007) incorpo-
rates domain specific constraints in semi-supervised
learning. Our task is different than these systems be-
cause we focus on extracting information to enable
a variety of automated applications such as business
1082
intelligence reporting, faceted search or automated
reasoning, rather than segmenting the text. The seg-
mented fields are often a long unstructured text (e.g.,
2 bath 1 bed for size rather than 2 for BathroomNum
and 1 for BedroomNum).
IE on Informal Texts. IE on informal texts is get-
ting much attention because of the recent explosive
growth of these texts. The informal texts that are
attempted for IE include online forums (Gruhl et
al., 2009), SMS (Beaufort et al 2010), twitter mes-
sages (Liu et al 2011).
3 Our Approach
Given a corpus of listings for a domain of interest,
our system constructs a semantic model that repre-
sents the types of information and their values to be
extracted. Our system then uses the resulting model
to extract both the type and value from the corpus 1.
We first describe the semantic model and then the
two key steps for creating this model ? shallow se-
mantic parsing and listing-focused clustering. Fig. 3
illustrates these two steps with example listings.
3.1 Semantic Model
Our semantic model captures two important pieces
of information ? the main concept and its attributes.
This representation is based on the observation that
most listings (e.g. rentals, products) describe the
attributes of a single object (i.e. the main concept
in our model). Our system takes advantage of this
observation by applying customized clustering for
each type of information in the model, which results
in better performance compared to a one-size-fits-all
algorithm. Moreover, this model is general enough
to be applicable across a wide range of domains. We
quantitatively show both benefits in our evaluation.
Fig. 2 illustrates our model along with an instanti-
ation for rental listings. The main concept is a clus-
ter containing tokens referencing the main object in
the listing. For example, in the rental listing, the
main concept cluster includes tokens such as house,
condo, and townhouse.
Each attribute of the main concept (e.g. Address,
BedroomNum, etc.) is also a cluster, and two types
1To handle string variations (e.g., typos) during extraction,
our system uses a string edit distance function, Jaro-Winkler
distance (Winkler, 1990) with a threshold, 0.9.
* Main Concept{house, condo, apt., apartment, townhouse, ?}
* Quantitative Attribute+{bedroom, bdrm, bd, bed,?}
* Qualitative Attribute+{washer, dryer, w/d, washer hookup, d hookup,?}
Figure 2: Semantic model and its instantiation for rental
listings. + indicates multiple clusters can be created.
of attributes are defined in our model ? quantita-
tive attributes and qualitative ones. Quantitative at-
tributes capture numeric values (e.g. 1 bedroom, 150
Hz, and 70 kg), and are generally a number followed
by a token indicating the attribute (e.g., unit of mea-
surement). Hence, clusters for quantitative attributes
include these indicator tokens (see Fig. 2).
Qualitative attributes capture descriptions about
the main concept (e.g., address, shipping informa-
tion, condition). The values of these attributes gen-
erally appear in listings without explicitly mention-
ing the names of these attributes. Hence, the clusters
for qualitative attributes include tokens correspond-
ing to the values themselves (e.g. washer hookup).
3.2 Shallow Semantic Parser
Our Shallow Semantic Parser (SSP) analyzes an in-
put corpus to produce a partial semantic model. SSP
first performs preprocessing and multiword detec-
tion. SSP then identifies which resulting tokens are
the main concepts and which are their attributes.
3.2.1 Preprocessing and Multiword Detection
SSP preprocesses the corpus through three steps.
(1) SSP cleans the corpus by removing duplicate
listings and HTML expressions/tags. (2) SSP tok-
enizes each listing based on spaces along with cus-
tom heuristics ? e.g., handling alpha-numeric to-
kens starting with numbers (e.g., 3bedroom to 3 bed-
room) and mixed case tokens (e.g., NikeShoes to
Nike Shoes). (3) SSP performs POS tagging using an
off-the-shelf tagger (Tsuruoka and Tsujii, 2005). To
improve accuracy, SSP assigns to a token the most
frequent POS across all occurrences of that token.
This heuristic works well because most tokens in fo-
cused domains, like listings, have only one POS.
SSP then detects multiword tokens based on the
following rules:
1083
A corpus of listings Semantically analyzed listings Semantic model
? Brentwood Apt. with 3 bedroom
? 2 BD/ 2 BA +Den ? Open Sun 2/12
? Affordable Rental Apartments-
Come take a Look!
? [brentwood/ATTR] [apt./MC] with 3 
[bedroom/ATTR]
? 2 [BD/ATTR]/ 2 [BA/ATTR] +[den/ATTR] ? [open 
sun 2/12/ATTR]
? [affordable rental/ATTR] [apartments/MC]-
come take a look!
* Main Concept      
{apt., apartment}
* Location 
{brentwood}
* BedroomNum
{bedroom, bd}
Shallow Semantic 
Parser
Listing-Focused 
Clustering
Figure 3: Steps of our system: The parser tokenizes listings (multiword detection) and then identifies main concepts
and attributes (marked as MC and ATTR). The clustering module then clusters tokens of the same type. The tags (such
as Location, BedroomNum) are included to help understand the figure. They are not produced by the system.
1. If a bigram (e.g., top floor) in a listing fre-
quently appears as either a single or dashed token
(e.g., TopFloor or top-floor) in other listings, then
the bigram is regarded as a multiword.
2. For each bigram, w1 w2 (excluding symbols
and numbers), if the conditional probability of the
bigram given either w1 or w2 (i.e., p(w1w2 |
w1(or w2)) is high (over 0.75 in our system)), the
bigram is considered as a candidate multiword. This
rule tests the tendency of two tokens appearing to-
gether when either one appears.
However, this test alone is insufficient, as it of-
ten generates coarse-grained results ? e.g., baseball
glove, softball glove, and Hi-Def TV 2. To prevent
this problem, for each w2, we measure the entropy
over the distribution of the tokens in the w1 position.
Our intuition is that high variability in the w1 posi-
tion (i.e., high entropy) indicates that the multiword
is likely a breakable phrase. Hence, those candidates
with high entropy are removed.
SPP repeatedly applies the above rules to acquire
multiwords of arbitrary length. In our implementa-
tion, we limit multiword detection up to four-gram.
3.2.2 Main Concept Identification
SSP then identifies the main concepts (mc words)
and their attributes (attrs) to produce a partial se-
mantic model. This process is guided by the ob-
servation that main concepts tend to appear as head
nouns in a listing and attributes as the modifiers of
these head nouns (see the examples in Fig. 3).
2Even though these examples are legitimate multiwords,
they overlook useful information such as baseball and softball
are types of gloves and Hi-Def is an attribute of TV.
Algorithm 1 describes the discovery process of
mc words and attrs. First, SSP initializes attrs with
tokens that are likely to be a modifier (line 2), by
choosing tokens that frequently appear as the object
of a preposition within the corpus ? e.g., for rent,
with washer and dryer, for baseball.
SSP then iteratively performs two steps ? PARSE
and EXPANDMODEL (lines 3 ? 6) ? in a boot-
strap manner (see Fig. 4). PARSE tags the noun to-
kens in each listing as either head nouns or modi-
fiers. Specifically, PARSE first assesses if a listing
is ?hard? to parse (line 10) based on two criteria ? (1)
the listing contains a long sequence of nouns (seven
words or longer in our system) without any prepo-
sitions (e.g., worth shutout series 12? womens fast-
pitch softball fielders glove s0120 lefty); and (2) the
majority of these nouns do not appear in mc words
and attrs (e.g., over 70% in our system). The listings
meeting these criteria are generally difficult to rec-
ognize the head noun without any semantic knowl-
edge. PARSE will revisit these listings in the next
round as more mc words and attrs are identified.
If a listing does not meet these criteria, PARSE
tags nouns appearing in mc words and attrs as
head nouns and modifiers respectively (line 11). If
this step fails to recognize a head noun, a heuris-
tic is used to identify the head noun ? it identi-
fies the first noun phrase by finding a sequence of
nouns/adjectives/numbers, and then tags as the head
noun the last noun in the phrase that is not tagged as
a modifier (line 13). For example, in the first listing
of Fig. 3, brentwood apts. is the first noun phrase
that meets the condition above; and hence apt. is
tagged as the head noun. The remaining untagged
nouns in the listing are tagged as modifiers (line 15).
1084
Algorithm 1 Extracting main concepts
1: Input: POS-tagged corpus, corp
2: Initialize attrs
3: repeat
4: (hn,mod) = Parse(corp, mc words, attrs)
5: (mc words,attrs) = ExpandModel(hn,mod)
6: until mc words, attrs not changed
7:
8: function PARSE(mc words, attrs)
9: for all each listing do
10: if parsible then
11: Parse with mc words, attrs
12: if headnoun is not tagged then
13: Tag the last noun in the first noun
phrase that are not a modifier as hn
14: end if
15: Tag the other nouns as mod
16: end if
17: end for
18: end function
19:
20: function EXPANDMODEL(corp)
21: For each token, calculate a ratio of as a head
noun to as a modifier
22: Add tokens with high ratio to mc words
23: Add tokens with low ratio to attrs
24: end function
EXPANDMODEL assigns tokens to either
mc words or attrs based on the tags generated
by PARSE. For each token, EXPANDMODEL
counts the frequency of the token being tagged as a
head noun and as a modifier. If a token is predom-
inately tagged as a head noun (or a modifier), the
token is added to mc words (or attrs) 3.
This bootstrap method is advantageous 4 be-
cause SSP can initially focus on easy cases ?
i.e., mc words and attrs that can be detected with
high confidence, such as condo(mc words) and bed-
room(attrs), which often appear as a head noun and
a modifier in the easy-to-parse listings. These re-
sults can help the system to parse more difficult
3In our system, if the ratio of the frequency of the head noun
to the frequency of the modifier is over .55, the token is added
to mc words. If less than .35, it is added to attrs.
4The bootstrapping cycle generally ends within 3?4 itera-
tions.
? mc_words
? attributes
ExpandModel
? Listings with head noun 
and modifiers detected 
Parse
Figure 4: Bootstrapped PARSE and EXPANDMODEL
listings. For example, identifying condo(mc words)
helps parsing the following more difficult listing,
?2 bedroom 1 bathroom condo large patio washer
dryer available? ? condo would be tagged as a head
noun and the rest of the nouns as modifiers.
The result of this step is a partial semantic model
that contains a cluster for the main concept and a list
of candidate attribute tokens.
3.3 Listing-Focused Clustering
Listing-Focused Clustering (LFC) further expands
the partial semantic model (constructed by SSP) by
grouping the remaining candidate attribute tokens
into attribute clusters ? i.e. one cluster for each at-
tribute of the main concept. LFC may also add a
token to the main concept cluster if appropriate.
For formal texts, distributional similarity is
widely used for clustering words because the con-
textual clues in these texts are sufficiently discrimi-
native (Lin and Pantel, 2001). This feature alone,
however, is insufficient for listings because they
lack discriminative contexts due to the short length.
Hence, our approach augments context-based sim-
ilarity with the following rules (presented in order
of precedence), based on general properties we ob-
served from listing data across various domains.
? Two quantitative attribute tokens cannot be
placed into the same cluster if they frequently
appear together in a listing. For example, bed
and bath should not be clustered because they
frequently appear together (e.g. 2 bed / 2bath).
This rule is based on the observation that a
quantitative attribute is likely to appear only
once in a listing. To enforce this restriction, for
all pairs of tokens, t1 and t2, LFC measures the
conditional probability of the pair appearing to-
gether in a listing given the appearance of either
1085
t1 and t2. If any of these conditional probabili-
ties are high, t1 and t2 are not clustered.
? Attribute types are strongly enforced by never
clustering together a quantitative attribute to-
ken and a qualitative attribute token. The type
of a token is determined by analyzing the im-
mediate preceding tokens throughout the cor-
pus. If the preceding tokens are generally num-
bers, then LFC regards the token as a quantita-
tive attribute. Otherwise, the token is regarded
as a qualitative attribute.
? Two tokens are similar if the characters in one
token appear in the other, preserving the order
(e.g., bdrm and bedroom)
If the above rules fail to determine the similarity
between two tokens, LFC reverts to context-based
similarity. For each token, LFC creates a context
vector containing frequencies of the context words
around the token with a window size of two. For
example, in the first sentence in Fig. 3, the context
words around apts. are l-start (beginning of the sen-
tence), l-brentwood, r-with, and r-3 5. The frequen-
cies in these vectors are also weighted using PMI
scores (pointwise mutual information) between a to-
ken and its context words, as suggested by (Turney
and Pantel, 2010). The intuition is that a high PMI
indicates a context word is strongly associated with a
token and hence has high discriminative power. We
also apply a smoothing function suggested in (Tur-
ney and Pantel, 2010) to mitigate PMI?s bias towards
infrequent events. The similarity score is based on a
cosine similarity between the two weighted vectors.
Based on this similarity function, LFC applies ag-
glomerative clustering (with average linkage) to pro-
duce attribute clusters (or to expand the main con-
cept cluster). However, calculating similarity scores
for all pairs of tokens is expensive. To address
this problem, LFC performs clustering in two steps.
First, LFC performs agglomerative clustering on all
pairs of tokens with high frequency 6. LFC then cal-
culates the similarity between each low-frequency
token and the clusters resulting from the previous
step. If the similarity score is over a user-specified
5l and r indicates the left or right window.
6The threshold for stopping clustering is determined with
the development dataset.
Dataset Dev Test Avg word
Rent Ad 8,950 9,400 9.44
Glove 8,600 9,500 10.56
TV Deal - 900 15.60
Table 1: Dev/Test indicates the number of listings used
for development/testing. Avg word indicates the average
number of words in a listing. The development dataset is
used to tune the parameters.
threshold, then LFC addes the token to the cluster.
If the score is less than the threshold but the token
still appears relatively frequently, then LFC creates
a new cluster for the token.
4 Evaluation
We perform two evaluations to assess the perfor-
mance of our approach. First, we evaluate how
well our approach extracts the correct type (e.g.,
BedroomNum) and value (e.g., 2) across multiple
domains. Next, we evaluate the contribution of
each main component in our approach ? i.e., shal-
low semantic parsing and listing-focused clustering
? through an ablation study. We also provide an
in-depth error analysis along with how our system?s
performance is affected by the corpus size.
4.1 Evaluation Setup
We first assemble three different listing datasets for
our evaluation ? housing rental advertisements from
Craigslist (Rent Ad), auction listings of baseball
gloves from EBay (Glove), and hot deal postings of
TV/Projector from FatWallet (TV Deal) 7. Table 1
shows the size of each dataset, and example listings
are shown in Fig. 1 8. We also include example ex-
tractions for each domain in Table. 3.
We then construct a gold standard by employing
two independent human annotators. To do this, we
first define the information types (i.e. main concept
and attribute) for each dataset (and hence the tar-
gets for extraction). We use attributes from EBay
7The datasets are available at https://sites.
google.com/site/2soonk/
8The parameters were tuned by the development set. Our
system is sensitive to the similarity score threshold in agglom-
erative clustering but less sensitive to the other parameters.
Hence, we tuned the similarity threshold for each domain while
fixing the values for the other parameters across different do-
mains.
1086
Rent Ad housing type (.70/.93), num bedroom (.94/.99), num bathroom (.97/1), location (-.64/.82),
neighborhood (.55/.79), others(14 types)
Glove product type (.71/1.00), brand (.58/.98), sport (.71/1.00), size (.87/.99), series name
(.51/.77), others(10 types)
TV Deal product type (.98/1.00), size (.85/1.00), display technology (.93/1.00), resolution (.89/.99),
seller (.73/1.00), others(14 types)
Table 2: Information types for each domain. See Fig. 1 for the examples of each type. Due to space limitation, we
show only top five types in terms of the number of extractions made by the annotators. The parentheses indicate inter-
annotator agreement based on exact match between two annotators (first number) and partial match (second number).
Exact agreement for qualitative attributes (e.g., location, neighborhood, series name) is found to be difficult.
Rent Ad
housing type studio, townhome, condo, townhouse, cottage,
num bedroom bd, bed, br, bedroom, bdrm, bedrooms
neighborhood downtown, San Francisco, china town
Glove
product type mitt, base mitt, glove, glove mitt
size ?, inch, in
brand rawlings, louisville slugger, mizuno, wilson
TV Deal
product type hdtv, wall mount, monitor
size ?, inch
seller wallmart, amazon, newegg, best buy
Table 3: Examples of positive extractions for the main concept attributes (e.g., housing type, product type), the
qualitative attributes (e.g., num bedroom, size) and the quantitative attributes (e.g., neighborhood, brand, seller)
used in the experiment
as the starting point for Glove and TV Deal; and
attributes from Rent.com9 for Rent Ad. We review
these attributes with two independent Subject Mat-
ter Experts (SMEs) to identify (and include) addi-
tional, missing attributes that are useful for analyt-
ics and reporting. In total, 19 types (Rent Ad), 15
types (Glove) and 19 types (TV Deal) are defined.
For each dataset, we randomly select 100 listings
from the Test listings, and instruct each annotator
to extract values from these listings, based on the
information types for the dataset. Table 2 shows
the defined attributes of each data set and the inter-
annotator agreement across the attributes 10.
Finally, we apply our system to the Test listings
in each dataset; and evaluate the extraction results
9http://www.rent.com
10We use Cohen?s kappa, P (a)?P (e)1?P (e) . P(a), the agree-
ment probability, is calculated by the number of list-
ings in which the two annotators agree divided by 100.
P(e), the chance agreement probability, is calculated by
?
(Ti,vi)
P1((Ti, vi)) ? P2((Ti, vi)) in which Pj((Ti, vi)) de-
notes a probability of extracting vi of the type Ti by the anno-
tator j. P ((Ti, vi)) is calculated by the frequency of (Ti, vi)
extracted divided by the frequency of Ti extracted.
against the gold standards using the metrics of preci-
sion (P), recall (R), and F1-score (a harmonic mean
of precision and recall). Each extraction result is a
tuple (Ti, vi) where Ti is the information type (e.g.,
BedroomNum) and vi is the value (e.g. 2).
P =
# correct extractions by our system
Total # extractions by our system
R =
# correct extractions by our system
Total # extractions by an annotator
We say that an extraction result is correct if vi
matches exactly the value extracted by the annotator
and Ti matches the information type assigned to vi
by the annotator. To enforce this criteria, we match
the attribute clusters produced by our system to the
information types. This matching step is needed be-
cause our approach is unsupervised and hence the
clusters are unlabelled. We use two methods for this
matching step ? many-to-one mapping and one-to-
one mapping. For many-to-one mapping (as used
in (Chen et al 2011)), we match an attribute cluster
to the information type whose values (as extracted
by the annotator) have the highest overlap with the
1087
values of the cluster. Hence, many attribute clusters
can map to the same information type. This method,
however, has one major disadvantage: high perfor-
mance can be easily achieved by creating small-
sized clusters ? e.g., singleton clusters in the extreme
case. To mitigate this problem, we also use a one-
to-one mapping method ? i.e. at most one attribute
cluster (i.e. the one with the best overlap) can be
mapped to one information type.
We report results for both methods. We also re-
port results for partial matches using the same met-
rics above. We say that an extraction result is par-
tially correct if vi partially matches the value ex-
tracted by the annotator (hardwood vs. hardwood
floors) and Ti matches the information type assigned
to vi by the annotator. 11
4.2 Performance Across Multiple Domains
Table 4 shows the system performance result across
the domains. Considering our approach is unsuper-
vised, the result is encouraging. For the baseball
glove and the TV dataset, F-score is .51 and .66 (in
one-to-one mapping). For the rent ad, which is more
difficult, F-score is .39. We hypothesize that the low
F1-score in the rent ad dataset is the result of poor
extraction due to spurious tokens (e.g., our commu-
nity, this weather). To test this hypothesis, we mea-
sure the performance of our system only on the ex-
traction task (i.e., excluding the information type as-
signment task). Table 5 shows that the performance
on extraction for the rent ad dataset is the lowest,
confirming our hypothesis.
We also measure the performance per each infor-
mation type. Fig. 5 shows the result, revealing sev-
eral facts. First, the main concept clusters (hous-
ing type and product type) achieve a high F1-score,
showing the benefit of our semantic parser. Sec-
11We could not compare our system to other systems for sev-
eral reasons. First, to the best of our knowledge, no unsuper-
vised IE system has been built specifically for short listings.
Second, semi-supervised systems such as (Putthividhya and
Hu, 2011) (Michelson and Knoblock, 2005) require domain-
specific dictionaries, which are expensive to build and scale.
Third, even developing a supervised IE system is non-trivial. In
our preliminary evaluation with the (linear chain) conditional
random field (170/30 training/testing listings) using basic fea-
tures (the lexemes and POS of the current word and words in the
two left/right windows), precision/recall/F1-score are .5/.33/.4.
This result is no better than our system. More training data
and/or better features seem to be required.
Full Full(Par)
P R F P R F
Rent Ad 0.3 0.41 0.35 0.43 0.55 0.48
(302/219) 0.34 0.46 0.39 0.65 0.69 0.67
Glove 0.54 0.48 0.51 0.72 0.58 0.64
(563/631) 0.57 0.51 0.54 0.81 0.65 0.72
TV Deal 0.7 0.63 0.66 0.81 0.7 0.75
(765/851) 0.74 0.67 0.7 0.86 0.74 0.79
Table 4: Performance based on one-to-one (first row) and
many-to-one mappings (second row) combined across
both annotators. Full indicates exact match between sys-
tem?s extraction and annotators? extraction. (Par) indi-
cates partial match. The parentheses indicate the total
number of extraction made by our system and the anno-
tators (averaged) respectively.
Full Full(Par) Ext Ext(Par)
Rent Ad 0.35 0.48 0.42 0.58
Glove 0.51 0.64 0.62 0.69
TV Deal 0.66 0.75 0.75 0.77
Table 5: F1-score when considering only extraction task
(Ext and Ext(Par)). Ext(Par) is based on partial match.
ond, the quantitative attributes (e.g., num bedroom,
glove size, screen size) generally have a higher F1
than the qualitative attributes (e.g., location, neigh-
borhood, series name). These qualitative attributes,
in fact, have a low inter-annotator agreement (e.g., -
.64, .55 for location and neighborhood in Rent Ad
and .51 for series name in Glove), indicating the
difficulty of exactly predicting the extractions made
by the annotators. If we consider the partial match or
the extraction only match (Ext) for those qualitative
attributes, their F1-scores are significantly higher
than the exact match in the Full task.
4.3 Ablation Study
To evaluate our semantic parser and listing-focused
clustering module, we ablate these two components
to create four versions of our system for compari-
son ? Baseline, Baseline+LFC, Baseline+SSP and
Full System. Baseline performs a space-based tok-
enization followed by clustering based only on the
context feature. Baseline+LFC and Baseline+SSP
add listing-focused clustering and shallow semantic
parser features respectively. The Full system uses
both features.
1088
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Full Full(Par) Ext Ext(Par)
Figure 5: F1-score across the attribute types shown in Table 2. The parentheses indicate the number of extractions
made by our system and the annotators respectively.
Rent Ad Glove TV Deal
P R F P R F P R F
Full
Baseline 0.10 0.27 0.14 0.26 0.34 0.29 0.42 0.51 0.46
Baseline+LFC 0.11 0.30 0.16 0.30? 0.39? 0.34 0.42 0.51 0.46
Baseline+SSP 0.21? 0.37? 0.26 0.46? 0.47? 0.46 0.65? 0.59? 0.62
Full System 0.30 ? 0.41 ? 0.35 0.54 ? 0.48 ? 0.51 0.70 ? 0.63 ? 0.66
Full(Par)
Baseline 0.15 0.40 0.22 0.37 0.50 0.42 0.59 0.69 0.63
Baseline+SSP 0.15 0.43 0.22 0.44? 0.56? 0.49 0.59 0.69 0.63
Baseline+LFC 0.39? 0.55? 0.46 0.37 0.34 0.35 0.79? 0.69 0.73
Full System 0.43 ? 0.55 ? 0.48 0.72 ? 0.58 ? 0.64 0.81 ? 0.70 0.75
Table 6: Based on one-to-one mapping. * indicates two-tail statistically significant difference (p < 0.05) against
Baseline in Fisher?s test. ? indicates one-tail difference. The Fisher?s test is inapplicable to F1-scores.
Rent Ad Glove TV Deal
NOT IN MAPPING 0.42 NOT IN MAPPING 0.27 NOT IN MAPPING 0.33
WRONG EXT 0.25 WRONG EXT 0.16 WRONG EXT 0.20
TK-neighborhood 0.05 WRONG TYPE 0.15 TK-display technology 0.13
TK-housing type 0.05 TK-series name 0.13 TK-shipping info 0.07
TK-location 0.03 TK-dexterity 0.12 WRONG TYPE 0.07
Table 7: The top five errors based on the one-to-one mapping.
1089
The results shown in Table 6 lead to the fol-
lowing observations. First, the use of SSP (Base-
line+SSP) makes an improvement for all categories
except Full(Par) in the glove dataset. This is because
SSP identifies main concepts accurately. Second,
while LFC by itself (Bseline+LFC) is effective only
in the glove dataset, it has the best F1-score in all
three domains when combined with SSP (Full Sys-
tem). The result also shows that the simple space-
based tokenization and the context-based clustering
(Baseline) is insufficient for handling short listings.
4.4 Error Analysis and Corpus Size Effect
We analyze the error types for the wrong extraction
made by our system. Specifically, for each error, we
assign it to one (or more) of the following causes:
(1) a cluster (and hence attribute type) was excluded
due to the 1-to-1 mapping methodology described
above (NOT IN MAPPING); (2) the value extracted
by the system was not extracted by any of the anno-
tators (WRONG EXT); (3) wrong information type ?
i.e., the token belonged to a wrong cluster (WRONG
TYPE); (4) incorrect tokenization for an information
type (TK-<type name>).
Table 7 shows the result. In all three domains,
NOT IN MAPPING is a major source of error, indi-
cating the system?s clusters are too fine-grained as
compared to the gold standard. WRONG EXT is
another source of error (especially in the housing
rental), indicating the system should extract more
informative tokens. Tokenization on the qualitative
attributes (neighborhood, series name, display tech-
nology in Table 7) should be improved also.
Finally, we measure the effect of the corpus size
on the system performance. Fig. 6 shows how the
F1-score varies with the corpus size 12. It shows that
a small corpus size is sufficient for achieving good
performance. We hypothesize that, for focused do-
mains such as our dataset, only a couple of hundred
listings are sufficient to acquire meaningful statis-
tics.
5 Conclusion and Future Work
We presented an unsupervised IE system on short
listings. The key features in our system are a shal-
12Due to the space limitation, we include only the rent do-
main result. However, all three datasets follow a similar pattern.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 2000 4000 6000 8000 10000
Full
Full(P)
Ext
Ext(P)
number of listings
F1
Figure 6: F1-score of our system over varying corpus size
for the rent domain
low semantic parser and a listing-focused clustering
module. Our evaluation shows the benefits of the
two features across multiple domains. To improve
our system further, we plan the following works.
First, we plan to compare our system with super-
vised systems to identify the gap between the two
systems. Second, as in (Poon and Domingos, 2010),
we plan to explore a joint learning method to com-
bine the tasks of tokenization, forming the main con-
cept cluster and forming the attribute clusters; these
tasks depend on the outputs of one another. Fi-
nally, we plan to explore that external knowledge
resources such as DBPedia (Auer et al 2007) and
FreeBase (Bollacker et al 2008) can be used to fur-
ther improve performance.
6 Acknowledgements
We would like to thank Colin Puri and Rey Vasquez
for their contribution to this work. We also thank
the anonymous reviewers for their helpful comments
and suggestions for improving the paper.
References
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: a nucleus for a web of open data. In Pro-
ceedings of the 6th international The semantic web
and 2nd Asian conference on Asian semantic web con-
ference, ISWC?07/ASWC?07, pages 722?735, Berlin,
Heidelberg. Springer-Verlag.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics - Volume 1,
1090
ACL ?98, pages 86?90, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 770?779, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ?08, pages 1247?1250, New York, NY,
USA. ACM.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 976?
986, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 280?287, Prague, Czech Republic, June.
Association for Computational Linguistics.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 530?540,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Rayid Ghani, Katharina Probst, Yan Liu, Marko Krema,
and Andrew Fano. 2006. Text mining for product at-
tribute extraction. SIGKDD Explor. Newsl., 8(1):41?
48, June.
Trond Grenager, Dan Klein, and Christopher D. Man-
ning. 2005. Unsupervised learning of field segmen-
tation models for information extraction. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ?05, pages 371?378,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel Gruhl, Meena Nagarajan, Jan Pieper, Christine
Robson, and Amit Sheth. 2009. Context and domain
knowledge enhanced entity spotting in informal text.
In Proceedings of the 8th International Semantic Web
Conference, ISWC ?09, pages 260?276, Berlin, Hei-
delberg. Springer-Verlag.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, HLT-NAACL
?06, pages 320?327, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Dirt
@sbt@discovery of inference rules from text. In Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?01, pages 323?328, New York, NY, USA.
ACM.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 359?
367, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Matthew Michelson and Craig A. Knoblock. 2005. Se-
mantic annotation of unstructured and ungrammatical
text. In Proceedings of the 19th international joint
conference on Artificial intelligence, IJCAI?05, pages
1091?1098, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Matthew Michelson and Craig A. Knoblock. 2009. Ex-
ploiting background knowledge to build reference sets
for information extraction. In Proceedings of the
21st international jont conference on Artifical intel-
ligence, IJCAI?09, pages 2076?2082, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings of
the 22nd national conference on Artificial intelligence
- Volume 1, AAAI?07, pages 913?918. AAAI Press.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 296?305,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Duangmanee (Pew) Putthividhya and Junling Hu. 2011.
Bootstrapped named entity recognition for product at-
tribute extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1557?1567, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joseph Reisinger and Marius Pas?ca. 2011. Fine-grained
class label markup of search queries. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1200?1209,
1091
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In In Advances in Neural Information Pro-
cessing Systems 17, pages 1185?1192.
Satoshi Sekine. 2006. On-demand information ex-
traction. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 731?738, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 467?474, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188, January.
William E. Winkler. 1990. String comparator metrics
and enhanced decision rules in the fellegi-sunter model
of record linkage. In Proceedings of the Section on
Survey Research, pages 354?359.
1092
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 10?14,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Building an end-to-end text reading system based on a packed representation
Doo Soon Kim
Dept. of Computer Science
University of Texas
Austin, TX, 78712
onue5@cs.utexas.edu
Ken Barker
Dept. of Computer Science
University of Texas
Austin, TX, 78712
kbarker@cs.utexas.edu
Bruce Porter
Dept. of Computer Science
University of Texas
Austin, TX, 78712
porter@cs.utexas.edu
Abstract
We previously proposed a packed graphical
representation to succinctly represent a huge
number of alternative semantic representa-
tions of a given sentence. We also showed that
this representation could improve text inter-
pretation accuracy considerably because the
system could postpone resolving ambiguity
until more evidence accumulates. This paper
discusses our plan to build an end-to-end text
reading system based on our packed represen-
tation.
1 Introduction
Our goal is to build an end-to-end text understanding
system by assembling together existing components
for parsing, semantic interpretation, co-reference
resolution and so on. Commonly, these components
are combined in a pipeline in which each one passes
forward a single best interpretation (see (a) in fig. 1).
Although this approach is relatively straightforward,
it can suffer from overly aggressive pruning; a com-
ponent might prune those interpretations that down-
stream components might have been able to recog-
nize as correct. Similarly, a component might prune
an interpretation that would be validated by reading
subsequent texts. The system?s accuracy would al-
most certainly improve if it were able to delay prun-
ing until sufficient evidence accumulates to make a
principled commitment.
There is a na??ve way of delaying pruning deci-
sions in a pipelined architecture: each component
passes forward not just a single interpretation, but
multiple alternatives, thereby creating multiple in-
terpretation paths (see (b) in fig 1). Then, the system
might choose the best interpretation at the last step
of the pipeline. However, this approach is intractable
due to the combinatorial explosion in the number of
interpretation paths.
In previous work (Kim et al, 2010), we pro-
posed an alternative approach in which each compo-
nent passes forward multiple interpretations which
are compressed into an intensional representation
that we call a packed graphical (PG) representation
(see (c) in fig. 1). Our experiment showed that the
approach could improve the interpretation accuracy
considerably by delaying ambiguity resolution while
avoiding combinatorial explosion.
In this paper, we discuss our plan to build an end-
to-end text understanding system using the PG rep-
resentation. We first introduce the language inter-
pretation system we are currently building, which
produces a PG representation from the parse of each
sentence. Then, we propose an architecture for an
end-to-end reading system that is based on the PG
representation. The architecture allows the system
to improve as it acquires knowledge from reading
texts.
In the following sections, we briefly describe the
PG representation and its disambiguation algorithm
(see (Kim et al, 2010) for details). Then, we present
the plan and the current status in the development of
an end-to-end text understanding system.
2 Packed graphical representation
The PG representation compresses a huge number
of alternative interpretations by locally represent-
10
Parser WSD SI
(a) single interpretation, single component
Parser WSD SI
WSD
WSD
?
SI
SI
SI
SI
?
(b) single interpretation, multiple components
Parser WSD SI
(c) single PG representation, single component
    
	


	





 	




  













Figure 1: The three different architectures for text understanding system: In (a), each component passes forward a
single interpretation. (b) can improve (a) by considering multiple interpretation paths, but suffers from combinatorial
explosion. (c) is our approach in which the system considers multiple alternative interpretations (in contrast to (a))
while avoiding combinatorial explosion by packing the alternatives (in contrast to (b)).
ing common types of ambiguities and other types
of constraints among the interpretations. Section 2.1
presents these ambiguity and constraint representa-
tions. Section 2.2 introduces an algorithm which
aims to resolve the ambiguities captured in a PG rep-
resentation.
2.1 Representation
Fig. 2 shows a PG representation produced from the
interpretation of the following sentence:
S1 : The engine ignites the gasoline with its spark
plug.
With this example, we will explain the ambigu-
ity representations and the other types of constraints
expressed in the PG representation.
Type ambiguity. Ambiguity in the assignment of
a type for a word. In PG1, for example, the node
engine-2a (corresponding to the word ?engine?) has
type annotation [LIVING-ENTITY .3 | DEVICE .7].
It means that the two types are candidates for the
type of engine-2a and their probabilities are respec-
tively .3 (Living-Entity) and .7 (Device) .
Relational ambiguity. Ambiguity in the assign-
ment of semantic relation between nodes. The edge
from ignite-3 to engine-2a in PG1 has relation anno-
tation <agent .6 | location .4>. It means that engine-
2a is either agent (probability .6) or location (prob-
ability .4) of ignite-3.
Structural ambiguity. It represents structural al-
ternatives in different interpretations. In PG1, for
example, D and E represent an ambiguity of prepo-
sitional phrase attachment for ?with its spark plug?;
Figure 2: The PG representation for S1 (PG1)
the phrase can be attached to ?ignites? (D) or ?spark
plug? (E). The annotation {D .3 | E .7} means either
D or E (not both) is correct and the probability of
each choice is respectively .3 and .7.
Co-reference ambiguity. A ?co-reference? edge
represents a possibility of co-reference between two
nodes. For example, the edge labeled <coref .7>
represents that the probability of engine-2a and its-
7a being co-referent is .7.
Besides ambiguity representations above, the
PG representation can also represent dependencies
among different interpretations.
Simple dependency. It represents that the ex-
istence of one interpretation depends on the exis-
tence of another. For example, A ? C means that
if LIVING-ENTITY is found to be a wrong type for
engine-2a (by subsequent evidence), the agent rela-
tion should be discarded, too.
Mutual dependency. It represents that the in-
terpretations in a mutual dependency set depend
on one another ? if any interpretation in the set is
11
found to be wrong or correct (by subsequent evi-
dence), the others should also be rejected or con-
firmed. For example, the box labeled B means that
if either (engine-2a type DEVICE) or (ignite-3a lo-
cation engine-2a) is confirmed or rejected, the other
interpretation should be confirmed or rejected.
Formally, the PG representation can be repre-
sented as a list of
? semantic triples ? e.g., (ignite-3a type BURN),
(ignite-3a instrument spark-plug-9a)
? macros ? e.g., the symbol A refers to (ignite-3a
agent engine-2a)
? constraints ? e.g., A depends on C,
D (.3) is exclusive to E (.7)
2.2 Disambiguating ambiguities in a PG
representations
In this section, we briefly explain how our disam-
biguating algorithm resolves ambiguities in a PG
representation. For details, please see (Kim et al,
2010).
The PG representation allows the system to de-
lay commitment to an interpretation (by explicitly
representing ambiguities) until enough evidence ac-
crues to disambiguate. One source of such evidence
is the other texts with redundant content. For a sen-
tence which is hard to interpret, there may be other
texts which describe the same content, but in ways
that the system can better interpret. These new reli-
able interpretations can be used to disambiguate the
original unreliable interpretations. Our algorithm is
based on this approach of combining multiple PG
representations to resolve their ambiguities.
The disambiguation algorithm uses graph match-
ing. The algorithm aligns two PG representations to
identify their redundant subgraphs (redundant por-
tions of the interpretations), then increases the con-
fidence scores of these subgraphs because the same
interpretation was derived from two independent
sentences (on the same topic). When the confidence
scores reach a high or low threshold, the associated
interpretations are confirmed or pruned. Confirming
or pruning one interpretation may lead to confirming
or pruning others. For example, the dependents of a
pruned interpretation should also be pruned.
Figure 3: The PG representation for S2 (PG2), ?The en-
gine?s spark plug combusts gasoline?
To illustrate the algorithm, we will show how PG1
(fig. 2) is merged with PG2 (fig. 3) to resolve their
ambiguities.
1. engine-2 in PG1 is aligned with engine-1 in
PG2. This operation chooses Device as the type
of engine-2 (i.e., it discards Living-Entity) be-
cause Device is favored in both nodes
2. Deleting LIVING-ENTITY causes deletion of
the agent edge between ignite-3a and engine-
2a due to the dependency constraint A ? C,
(meaning agent (in A) depends on the existence
of LIVING-ENTITY (in C)).
3. Co-reference between engine-2a and its-7a is
greedily confirmed because merging the two
nodes enables the alignment of (its-7a has-part
spark-plug-8a) with (Engine-1b has-part spark-
plug-3b).
4. The algorithm aligns (ignite-3a instrument
spark-plug-8a) with (combust-5b instru-
ment spark-plug-3b), because ignite-3a and
combust-5b share the same type, [BURN].
This operation increases the score of D (the
structure corresponding to PP attachment of
?with its spark plug? to ?ignite?) over E (the
structure corresponding to attachment of ?with
its spark plug? to ?gasoline?).
3 Taking advantage of the PG
representation in an end-to-end system
Our experiment showed that, for ten texts with re-
dundant content, our approach improved the inter-
pretation accuracy by 10% (Kim et al, 2010). En-
couraged by this result, we present our on-going
work and future plans.
12
3.1 Producing PG representation
We are currently constructing a fully automated lan-
guage interpretation system to produce PG represen-
tations from English sentences. The system will be
able to maintain all possible interpretations gener-
ated at each step (including parsing, word sense dis-
ambiguation (WSD) and semantic relation assign-
ment) and represent them using the PG representa-
tion. This is straightforward for WSD and semantic
relation assignment because most off-the-shelf soft-
ware (e.g., (Patwardhan et al, 2005) (Punyakanok
et al, 2005)) outputs a list of candidate choices and
confidence scores for type and relational ambigui-
ties. (Kim et al, 2010) describes a prototype system
implemented with these WSD and semantic assign-
ment components.
However, ambiguities in parsing are more dif-
ficult because it is hard to efficiently identify
structural differences among various parses. We
are currently developing an algorithm (similar to
(Schiehlen, 1996)) which converts a parse forest (the
ambiguity-preserving chart built during PCFG pars-
ing) (Tomita, 1986) into the syntactic-level PG rep-
resentation (as shown in fig. 4). We plan to imple-
ment this algorithm in the Stanford Parser (Klein and
Manning, 2003) and to evaluate it along the follow-
ing dimensions.
First, we will measure the improvement in parsing
accuracy that results from delaying commitment to
a single best parse.
Second, even though the PG representation
achieves substantial compression, its size is still
bounded. The parser might generate more interpre-
tations than will fit within the bound. We plan to
handle this problem in the following way. When a
PG representation grows to the bound, the system
applies the components downstream of the parser to
the candidate parses. Because these components use
additional sources of knowledge, including knowl-
edge derived from previous reading (Clark and Har-
rison, 2009), they might be able to prune some can-
didate interpretations. In this way, a part of a sen-
tence may be processed early while the other parts
are left unprocessed, in contrast with the traditional
approach of fully processing each sentence before
starting with the next.
Figure 4: Syntactic-level PG representation for S1: the
structural ambiguity represents an ambiguity of attaching
the preposition, ?with its spark plug?.
3.2 System Architecture
The PG representation and its disambiguating algo-
rithm allow an interesting property in a text under-
standing system: the system?s interpretation capa-
bility could increase as it acquires knowledge from
texts. This property can be shown in two ways. First,
the ambiguities of the current text could be resolved
later when the system reads subsequent texts. Sec-
ond, the knowledge acquired from the prior texts
could be used to resolve the ambiguities of the cur-
rent text. Fig. 5 shows an architecture that exhibits
this property.
Given a text, or a set of texts on the same topic,
the language interpreter generates a PG representa-
tion. Then, the knowledge integration component
(KI) adds the PG representation into the knowledge
base. For a first text, the PG representation is simply
put into the knowledge base. For subsequent texts,
KI merges the subsequent PG representations with
the PG representation in the knowledge base. This
step may resolve ambiguities in the PG representa-
tion maintained in the knowledge base.
When the language interpreter confronts an ambi-
guity, it has two choices: it either (a) locally repre-
sents the ambiguity in the PG representation or (b)
asks the RESOLVER to resolve the ambiguity. When
the RESOLVER is called, it searches the knowledge
base for information to resolve the ambiguity. If
this is unsuccessful, it uses the information retrieval
module (TEXT MINER) to find relevant documents
from external sources which might resolve the am-
biguity. The documents are added in the Text Queue
to be read subsequently. In the near future, we plan
to evaluate the ability of the KI and Resolver mod-
ules to resolve ambiguities as the system reads more
texts.
13
Parser SemanticInterpreter
Knowledge 
Base
KI
Language Interpreter
Text 
Queue
Text Miner
Resolver
Figure 5: Architecture
4 Summary
In this paper, we discuss the development of an end-
to-end text understanding system based on a packed
representation. With this representation, the system
can delay ambiguity resolution while avoiding com-
binatorial explosion, thereby effectively improving
the accuracy of text interpretation.
References
Peter Clark and Philip Harrison. 2009. Large-scale ex-
traction and use of knowledge from text. In Yolanda
Gil and Natasha Fridman Noy, editors, K-CAP, pages
153?160. ACM.
Doo Soon Kim, Ken Barker, and Bruce Porter. 2010.
Improving the quality of text understanding by delay-
ing ambiguity resolution. Technical Report TR-10-12,
University of Texas at Austin.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. of ACL.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. Senserelate:: Targetword-A generalized
framework for word sense disambiguation. In ACL.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role la-
beling. In IJCAI.
Michael Schiehlen. 1996. Semantic construction from
parse forests. In COLING, pages 907?912.
Masaru Tomita. 1986. Efficient Parsing for Natural Lan-
guage ? A Fast Algorithm for Practical Systems. Int.
Series in Engineering and Computer Science. Kluwer,
Hingham, MA.
14
