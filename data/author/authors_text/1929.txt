Splitting Input Sentence for Machine Translation Using
Language Model with Sentence Similarity
Takao Doi Eiichiro Sumita
ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan
{takao.doi, eiichiro.sumita}@atr.jp
Abstract
In order to boost the translation quality of
corpus-based MT systems for speech transla-
tion, the technique of splitting an input sen-
tence appears promising. In previous re-
search, many methods used N-gram clues to
split sentences. In this paper, to supplement
N-gram based splitting methods, we introduce
another clue using sentence similarity based
on edit-distance. In our splitting method,
we generate candidates for sentence splitting
based on N-grams, and select the best one by
measuring sentence similarity. We conducted
experiments using two EBMT systems, one of
which uses a phrase and the other of which
uses a sentence as a translation unit. The
translation results on various conditions were
evaluated by objective measures and a subjec-
tive measure. The experimental results show
that the proposed method is valuable for both
systems.
1 Introduction
We are exploring methods to boost the transla-
tion quality of corpus-based Machine Translation
(MT) systems for speech translation. Among
them, the technique of splitting an input sentence
and translating the split sentences appears promis-
ing (Doi and Sumita, 2003).
An MT system sometimes fails to translate an
input correctly. Such a failure occurs particu-
larly when an input is long. In such a case, by
splitting the input, translation may be successfully
performed for each portion. Particularly in a di-
alogue, sentences tend not to have complicated
nested structures, and many long sentences can be
split into mutually independent portions. There-
fore, if the splitting positions and the translations
of the split portions are adequate, the possibility
that the arrangement of the translations can pro-
vide an adequate translation of the complete in-
put is relatively high. For example, the input sen-
tence, ?This is a medium size jacket I think it?s
a good size for you try it on please? 1 can be split
into three portions, ?This is a medium size jacket?,
?I think it?s a good size for you? and ?try it on
please?. In this case, translating the three portions
and arranging the results in the same order give us
the translation of the input sentence.
In previous research on splitting sentences,
many methods have been based on word-sequence
characteristics like N-gram (Lavie et al, 1996;
Berger et al, 1996; Nakajima and Yamamoto,
2001; Gupta et al, 2002). Some research efforts
have achieved high performance in recall and pre-
cision against correct splitting positions. Despite
such a high performance, from the view point of
translation, MT systems are not always able to
translate the split sentences well.
In order to supplement sentence splitting based
on word-sequence characteristics, this paper intro-
duces another measure of sentence similarity. In
our splitting method, we generate candidates for
splitting positions based on N-grams, and select
the best combination of positions by measuring
sentence similarity. This selection is based on the
assumption that a corpus-based MT system can
correctly translate a sentence that is similar to a
sentence in its training corpus.
The following sections describe the proposed
splitting method, present experiments using two
Example-Based Machine Translation (EBMT)
systems, and evaluate the effect of introducing the
similarity measure on translation quality.
2 Splitting Method
We define the term sentence-splitting as the re-
sult of splitting a sentence. A sentence-splitting
is expressed as a list of sub-sentences that are
1Punctuation marks are not used in translation input in
this paper.
Input
Splitter
Generation
Selection
Corpus-based MT
Translation
 
 




Parallel
Corpus
 




Source
Sentences
Language
Model
 




Translation
Knowledge
 




Probability
Similarity
ffff


ff
ffff
Figure 1: Configuration
portions of the original sentence. A sentence-
splitting includes a portion or several portions. We
use an N-gram Language Model (NLM) to gener-
ate sentence-splitting candidates, and we use the
NLM and sentence similarity to select one of the
candidates. The configuration of the method is
shown in Figure 1.
2.1 Probability Based on N-gram Language
Model
The probability of a sentence can be calculated
by an NLM of a corpus. The probability of a
sentence-splitting, Prob, is defined as the product
of the probabilities of the sub-sentences in equa-
tion (1), where P is the probability of a sentence
based on an NLM, S is a sentence-splitting, that
is, a list of sub-sentences that are portions of a sen-
tence, and P is applied to the sub-sentences.
Prob(S) =
?
s?S
P (s) (1)
To judge whether a sentence is split at a
position, we compare the probabilities of the
sentence-splittings before and after splitting.
When calculating the probability of a sentence
including a sub-sentence, we put pseudo words at
the head and tail of the sentence to evaluate the
probabilities of the head word and the tail word.
For example, the probability of the sentence,
?This is a medium size jacket? based on a trigram
language model is calculated as follows. Here,
p(z | x y) indicates the probability that z occurs
after the sequence x y, and SOS and EOS indicate
the pseudo words.
P (this is a medium size jacket) =
p(this | SOS SOS) ?
p(is | SOS this) ?
p(a | this is) ?
...
p(jacket | medium size) ?
p(EOS | size jacket) ?
p(EOS | jacket EOS)
This causes a tendency for the probability of
the sentence-splitting after adding a splitting posi-
tion to be lower than that of the sentence-splitting
before adding the splitting position. Therefore,
when we find a position that makes the probability
higher, it is plausible that the position divides the
sentence into sub-sentences.
2.2 Sentence Similarity
An NLM suggests where we should split a sen-
tence, by using the local clue of several words
among the splitting position. To supplement it
with a wider view, we introduce another clue
based on similarity to sentences, for which trans-
lation knowledge is automatically acquired from a
parallel corpus. It is reasonably expected that MT
systems can correctly translate a sentence that is
similar to a sentence in the training corpus.
Here, the similarity between two sentences is
defined using the edit-distance between word se-
quences. The edit-distance used here is extended
to consider a semantic factor. The edit-distance is
normalized between 0 and 1, and the similarity is 1
minus the edit-distance. The definition of the sim-
ilarity is given in equation (2). In this equation, L
is the word count of the corresponding sentence.
I and D are the counts of insertions and deletions
respectively. Substitutions are permitted only be-
tween content words of the same part of speech.
Substitution is considered as the semantic distance
between two substituted words, described as Sem,
which is defined using a thesaurus and ranges from
0 to 1. Sem is the division of K (the level of the
least common abstraction in the thesaurus of two
words) by N (the height of the thesaurus) accord-
ing to equation (3) (Sumita and Iida, 1991).
Sim
0
(s
1
, s
2
) = 1 ?
I + D + 2
?
Sem
L
s
1
+ L
s
2
(2)
Sem =
K
N
(3)
Using Sim
0
, the similarity of a sentence-
splitting to a corpus is defined as Sim in equa-
tion (4). In this equation, S is a sentence-splitting
and C is a given corpus that is a set of sentences.
Sim is a mean similarity of sub-sentences against
the corpus weighted with the length of each sub-
sentence. The similarity of a sentence including a
sub-sentence to a corpus is the greatest similarity
between the sentence and a sentence in the corpus.
Sim(S) =
?
s?S
L
s
? max{Sim
0
(s, c)|c ? C}
?
s?S
L
s
(4)
2.3 Generating Sentence-Splitting
Candidates
To calculate Sim is similar to retrieving the most
similar sentence from a corpus. The retrieval pro-
cedure can be efficiently implemented by the tech-
niques of clustering (Cranias et al, 1997) or using
A* search algorithm on word graphs (Doi et al,
2004). However, it still takes more cost to cal-
culate Sim than Prob when the corpus is large.
Therefore, in the splitting method, we first gen-
erate sentence-splitting candidates by Prob alone.
In the generating process, for a given sentence, the
sentence itself is a candidate. For each sentence-
splitting of two portions whose Prob does not de-
crease, the generating process is recursively exe-
cuted with one of the two portions and then with
the other. The results of recursive execution are
combined into candidates for the given sentence.
Through this process, sentence-splittings whose
Probs are lower than that of the original sentence,
are filtered out.
2.4 Selecting the Best Sentence-Splitting
Next, among the candidates, we select the one
with the highest score using not only Prob but
also Sim. We use the product of Prob and Sim as
the measure to select a sentence-splitting by. The
measure is defined as Score in equation (5), where
?, ranging from 0 to 1, gives the weight of Sim.
In particular, the method uses only Prob when ?
is 0, and the method generates candidates by Prob
and selects a candidate by only Sim when ? is 1.
Score = Prob1?? ? Sim? (5)
2.5 Example
Here, we show an example of generating sentence-
splitting candidates with Prob and selecting one
by Score. For the input sentence, ?This is a
medium size jacket I think it?s a good size for
you try it on please?, there may be many candi-
dates. Below, five candidates, whose Prob are not
less than that of the original sentence, are gener-
ated. A ?|? indicates a splitting position. The left
numbers indicate the ranking based on Prob. The
5th candidate is the input sentence itself. For each
candidate, Sim, and further, Score are calculated.
Among the candidates, the 2nd is selected because
its Score is the highest.
1. This is a medium size jacket | I think it?s a good
size for you try it on please
2. This is a medium size jacket | I think it?s a good
size for you | try it on please
3. This is a medium size jacket | I think it?s a good
size | for you try it on please
4. This is a medium size jacket | I think it?s a good
size | for you | try it on please
5. This is a medium size jacket I think it?s a good
size for you try it on please
3 Experimental Conditions
We evaluated the splitting method through experi-
ments, whose conditions are as follows.
3.1 MT Systems
We investigated the splitting method using MT
systems in English-to-Japanese translation, to de-
termine what effect the method had on transla-
tion. We used two different EBMT systems as
test beds. One of the systems was Hierarchical
Phrase Alignment-based Translator (HPAT) (Ima-
mura, 2002), whose unit of translation expres-
sion is a phrase. HPAT translates an input sen-
tence by combining phrases. The HPAT system is
equipped with another sentence splitting method
based on parsing trees (Furuse et al, 1998). The
other system was DP-match Driven transDucer
(D3) (Sumita, 2001), whose unit of expression is a
sentence. For both systems, translation knowledge
is automatically acquired from a parallel corpus.
3.2 Linguistic Resources
We used Japanese-and-English parallel corpora,
i.e., a Basic Travel Expression Corpus (BTEC)
and a bilingual travel conversation corpus of Spo-
ken Language (SLDB) for training, and English
sentences in Machine-Translation-Aided bilingual
Dialogues (MAD) for a test set (Takezawa and
Kikui, 2003). BTEC is a collection of Japanese
sentences and their English translations usually
found in phrase-books for foreign tourists. The
contents of SLDB are transcriptions of spoken
dialogues between Japanese and English speak-
ers through human interpreters. The Japanese
and English parts of the corpora correspond to
each other sentence-to-sentence. The dialogues of
MAD took place between Japanese and English
speakers through human typists and an experimen-
tal MT system.
(Kikui et al, 2003) shows that BTEC and SLDB
are both required for handling MAD-type tasks.
Therefore, in order to translate test sentences in
MAD, we merged the parallel corpora, 152,170
sentence pairs of BTEC and 72,365 of SLDB,
into a training corpus for HPAT and D3. The En-
glish part of the training corpus was also used to
make an NLM and to calculate similarities for the
sentence-splitting method. The statistics of the
training corpus are shown in Table 1. The per-
plexity in the table is word trigram perplexity.
The test set of this experiment was 505 English
sentences uttered by human speakers in MAD, in-
cluding no sentences generated by the MT system.
The average length of the sentences in the test set
was 9.52 words per sentence. The word trigram
perplexity of the test set against the training cor-
pus was 63.66.
We also used a thesaurus whose hierarchies are
based on the Kadokawa Ruigo-shin-jiten (Ohno
and Hamanishi, 1984) with 80,250 entries.
English Japanese
# of sentences 224,535
# of words 1,589,983 1,865,298
avg. sentence length 7.08 8.31
vocabulary size 14,548 21,686
perplexity 27.58 27.37
Table 1: Statistics of the training corpus
3.3 Instantiation of the Method
For the splitting method, the NLM was the word
trigram model using Good-Turing discounting.
The number of split portions was limited to 4 per
sentence. The weight of Sim, ? in equation (5)
was assigned one of 5 values: 0, 1/2, 2/3, 3/4 or 1.
3.4 Evaluation
We compared translation quality under the con-
ditions of with or without splitting. To evalu-
ate translation quality, we used objective measures
and a subjective measure as follows.
The objective measures used were the BLEU
score (Papineni et al, 2001), the NIST score (Dod-
dington, 2002) and Multi-reference Word Error
Rate (mWER) (Ueffing et al, 2002). They were
calculated with the test set. Both BLEU and NIST
compare the system output translation with a set
of reference translations of the same source text
by finding sequences of words in the reference
translations that match those in the system output
translation. Therefore, achieving higher scores by
these measures means that the translation results
can be regarded as being more adequate transla-
tions. mWER indicates the error rate based on
the edit-distance between the system output and
the reference translations. Therefore, achieving a
lower score by mWER means that the translation
results can be regarded as more adequate transla-
tions. The number of references was 15 for the
three measures.
In the subjective measure (SM), the transla-
tion results of the test set under different two
conditions were evaluated by paired comparison.
Sentence-by-sentence, a Japanese native speaker
who had acquired a sufficient level of English,
judged which result was better or that they were
of the same quality. SM was calculated compared
to a baseline. As in equation (6), the measure was
the gain per sentence, where the gain was the num-
ber of won translations subtracted by the number
of defeated translations as judged by the human
evaluator.
SM =
# of wins? # of defeats
# of test sentences
(6)
4 Effect of Splitting for MT
4.1 Translation Quality
Table 2 shows evaluations of the translation results
of two MT systems, HPAT and D3, under six con-
ditions. In ?original?, the input sentences of the
systems were the test set itself without any split-
ting. In the other conditions, the test set sentences
were split using Prob into sentence-splitting can-
didates, and a sentence-splitting per input sentence
was selected with Score. The weights of Prob
and Sim in the definition of Score in equation (5)
were varied from only Prob to only Sim. The
baseline of SM was the original.
The number of input sentences, which have
multi-candidates generated with Prob, was 237,
where the average and the maximum number of
candidates were respectively 5.07 and 64. The av-
erage length of the 237 sentences was 12.79 words
original P1S0 P 1/2S1/2 P 1/3S2/3 P 1/4S3/4 P 0S1
# of split sentences 0 237 236 236 235 233
BLEU 0.2979 0.3179 0.3201 0.3192 0.3193 0.3172
NIST 7.1030 7.2616 7.2618 7.2709 7.2748 7.2736
mWER 0.5828 0.5683 0.5665 0.5666 0.5658 0.5703
HPAT SM +6.9% +8.7% +10.1% +10.1% +9.5%
# of wins 89 95 99 99 104
# of defeats 54 51 48 48 56
# of draws 94 90 89 88 73
BLEU 0.2992 0.3702 0.3704 0.3685 0.3695 0.3705
NIST 2.1302 5.7809 5.8524 5.9115 5.9786 6.2545
mWER 0.5844 0.5432 0.5433 0.5434 0.5424 0.5440
D3 SM +20.6% +21.8% +21.8% +22.4% +23.0%
# of wins 141 145 145 146 151
# of defeats 37 35 35 33 35
# of draws 59 56 56 56 47
Table 2: MT Quality: Using splitting vs. not using splitting, on the test set of 505 sentences (P indicates
Prob and S indicates Sim)
per sentence. The word trigram perplexity of the
set of the 237 sentences against the training corpus
was 73.87.
The table shows certain tendencies. The differ-
ences in the evaluation scores between the origi-
nal and the cases with splitting are significant for
both systems and especially for D3. Although the
differences among the cases with splitting are not
so significant, SM steadily increases when using
Sim compared to using only Prob, by 3.2% for
HPAT and by 2.4% for D3. Among objective mea-
sures, the NIST score corresponds well to SM.
4.2 Effect of Selection Using Similarity
Table 3 allows us to focus on the effect of Sim in
the sentence-splitting selection. The table shows
the evaluations on 237 sentences of the test set,
where selection was required. In this table, the
number of changes is the number of cases where a
candidate other than the best candidate using Prob
was selected. The table also shows the average and
maximum Prob ranking of candidates which were
not the best using Prob but were selected as the
best using Score. The condition of ?IDEAL? is to
select such a candidate that makes the mWER of
its translation the best value in any candidate. In
IDEAL, the selections are different between MT
systems. The two values of the number of changes
are for HPAT and for D3. The baseline of SM was
the condition of using only Prob.
From the table, we can extract certain tenden-
cies. The number of changes is very small when
using both Prob and Sim in the experiment. In
these cases, the procedure selects the best candi-
dates or the second candidates in the measure of
Prob. Although the change is small when the
weights of Prob and Sim are equal, SM shows
that most of the changed translations become bet-
ter, some remain even and none become worse.
The heavier the weight of Sim is, the higher the
SM score is. The NIST score also increases espe-
cially for D3 when the weight of Sim increases.
The IDEAL condition overcomes most of the con-
ditions as was expected, except that the SM score
and the NIST score of D3 are worse than those
in the condition using only Sim. For D3, the
sentence-splitting selection with Sim is a match
for the ideal selection.
So far, we have observed that SM and NIST
tend to correspond to each other, although SM and
BLEU or SM and mWER do not. The NIST score
uses information weights when comparing the re-
sult of an MT system and reference translations.
We can infer that the translation of a sentence-
splitting, which was judged as being superior to
another by the human evaluator, is more informa-
tive than the other.
4.3 Effect of Using Thesaurus
Furthermore, we conducted an experiment with-
out using a thesaurus in calculating Sim. In the
definition of Sim, all semantic distances of Sem
P1
S
0
P
1/2
S
1/2
P
1/3
S
2/3
P
1/4
S
3/4
P
0
S
1 IDEAL
# of changes 10 19 25 91 111; 111
changed rank avg. 2.00 2.00 2.00 4.01 3.77; 3.78
(max) (2) (2) (2) (20) (29); (23)
BLEU 0.3004 0.3036 0.3022 0.3025 0.2994 0.3351
NIST 7.1883 7.1911 7.2034 7.2068 7.1993 7.3057
mWER 0.6363 0.6324 0.6328 0.6310 0.6405 0.5820
HPAT SM +3.4% +3.8% +3.8% +5.9% +14.8%
# of wins 8 12 15 40 59
# of defeats 0 3 6 26 24
# of draws 2 4 4 25 28
BLEU 0.3310 0.3316 0.3291 0.3308 0.3340 0.3917
NIST 6.0700 6.1687 6.2450 6.3372 6.6778 5.3250
mWER 0.6181 0.6183 0.6185 0.6164 0.6197 0.5567
D3 SM +3.4% +3.4% +5.5% +6.3% +5.5%
# of wins 8 10 15 37 41
# of defeats 0 2 2 22 28
# of draws 2 7 8 32 42
Table 3: MT Quality: Using similarity vs. not using similarity, on the test set of 237 sentences (P
indicates Prob and S indicates Sim)
P
1
S
0
P
1/2
S
1/2
P
1/3
S
2/3
P
1/4
S
3/4
P
0
S
1 IDEAL
# of changes 10 19 26 93 111; 111
changed rank avg. 2.00 2.00 2.00 4.05 3.77; 3.78
(max) (2) (2) (2) (20) (29); (23)
BLEU 0.3004 0.3027 0.3034 0.3039 0.2973 0.3351
NIST 7.1883 7.1830 7.1921 7.2003 7.1741 7.3057
mWER 0.6363 0.6342 0.6320 0.6321 0.6346 0.5820
HPAT SM +1.7% +3.8% +3.4% +6.3% +14.8%
# of wins 6 13 15 40 59
# of defeats 2 4 7 25 24
# of draws 2 2 4 28 28
BLEU 0.3310 0.3301 0.3310 0.3290 0.3370 0.3917
NIST 6.0700 6.1387 6.2414 6.3341 6.6739 5.3250
mWER 0.6181 0.6196 0.6188 0.6198 0.6175 0.5567
D3 SM +3.0% +4.6% +5.9% +7.6% +5.5%
# of wins 7 12 16 41 41
# of defeats 0 1 2 23 28
# of draws 3 6 8 29 42
Table 4: MT Quality: Using similarity vs. not using similarity, on the test set of 237 sentences, without
a thesaurus (P indicates Prob and S indicates Sim)
were assumed to be equal to 0.5. Table 4 shows
evaluations on the 237 sentences.
Compared to Table 3, the SM score is worse
when the weight of Sim in Score is small, and
better when the weight of Sim is great. However,
the difference between the conditions of using or
not using a thesaurus is not so significant.
5 Concluding Remarks
In order to boost the translation quality of corpus-
based MT systems for speech translation, the
technique of splitting an input sentence appears
promising. In previous research, many methods
used N-gram clues to split sentences. To supple-
ment N-gram based splitting methods, we intro-
duce another clue using sentence similarity based
on edit-distance. In our splitting method, we gen-
erate sentence-splitting candidates based on N-
grams, and select the best one by the measure
of sentence similarity. The experimental results
show that the method is valuable for two kinds of
EBMT systems, one of which uses a phrase and
the other of which uses a sentence as a translation
unit.
Although we used English-to-Japanese transla-
tion in the experiments, the method depends on
no particular language. It can be applied to multi-
lingual translation. Because the semantic distance
used in the similarity definition did not show any
significant effect, we need to find another fac-
tor to enhance the similarity measure. Further-
more, as future work, we?d like to make the split-
ting method cooperate with sentence simplifica-
tion methods like (Siddharthan, 2002) in order to
boost the translation quality much more.
Acknowledgements
The authors? heartfelt thanks go to Kadokawa-Shoten
for providing the Ruigo-Shin-Jiten. The research re-
ported here was supported in part by a contract with
the Telecommunications Advancement Organization of
Japan entitled, ?A study of speech dialogue translation
technology based on a large corpus?.
References
A.L. Berger, S. A. Della Pietra, and V. J.
Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Compu-
tational Linguistics, 22(1):1?36.
L. Cranias, H. Papageorgiou, and S. Piperidis.
1997. Example retrieval from a transla-
tion memory. Natural Language Engineering,
3(4):255?277.
G. Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. Proc. of the HLT 2002
Conference.
T. Doi and E. Sumita. 2003. Input sentence split-
ting and translating. Proc. of Workshop on
Building and Using Parallel Texts, HLT-NAACL
2003, pages 104?110.
T. Doi, E. Sumita, and H. Yamamoto. 2004. Ef-
ficient retrieval method and performance evalu-
ation of example-based machine translation us-
ing edit-distance (in Japanese). Transactions of
IPSJ, 45(6).
O. Furuse, S. Yamada, and K. Yamamoto.
1998. Splitting long or ill-formed input for
robust spoken-language translation. Proc. of
COLING-ACL?98, pages 421?427.
N.K. Gupta, S. Bangalore, and M. Rahim. 2002.
Extracting clauses for spoken language under-
standing in conversational systems. Proc. of IC-
SLP 2002, pages 361?364.
K. Imamura. 2002. Application of transla-
tion knowledge acquired by hierarchical phrase
alignment for pattern-based mt. Proc. of TMI-
2002, pages 74?84.
G. Kikui, E. Sumita, T. Takezawa, and S. Ya-
mamoto. 2003. Creating corpora for speech-
to-speech translation. Proc. of EUROSPEECH,
pages 381?384.
A. Lavie, D. Gates, N. Coccaro, and L. Levin.
1996. Input segmentation of spontaneous
speech in janus: a speech-to-speech translation
system. Proc. of ECAI-96 Workshop on Dia-
logue Processing in Spoken Language Systems,
pages 86?99.
H. Nakajima and H. Yamamoto. 2001. The statis-
tical language model for utterance splitting in
speech recognition (in Japanese). Transactions
of IPSJ, 42(11):2681?2688.
S. Ohno and M. Hamanishi. 1984. Ruigo-Shin-
Jiten (in Japanese). Kadokawa, Tokyo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. Bleu: a method for automatic evaluation
of machine translation. RC22176, September
17, 2001, Computer Science.
A. Siddharthan. 2002. An architecture for a text
simplification system. Proc. of LEC 2002.
E. Sumita and H. Iida. 1991. Experiments and
prospects of example-based machine transla-
tion. Proc. of 29th Annual Meeting of ACL,
pages 185?192.
E. Sumita. 2001. Example-based machine trans-
lation using dp-matching between word se-
quences. Proc. of 39th ACL Workshop on
DDMT, pages 1?8.
T. Takezawa and G. Kikui. 2003. Collecting
machine-translation-aided bilingual dialogues
for corpus-based speech translation. Proc. of
EUROSPEECH, pages 2757?2760.
N. Ueffing, F.J. Och, and H. Ney. 2002. Genera-
tion of word graphs in statistical machine trans-
lation. Proc. of Conf. on Empirical Methods for
Natural Language Processing, pages 156?163.
171
172
173
174
Adaptation Using Out-of-Domain Corpus within EBMT 
Takao Doi,  Eiichiro Sumita, Hirofumi Yamamoto 
ATR Spoken Language Translation Research Laboratories  
2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan 
{takao.doi, eiichiro.sumita, hirofumi.yamamoto}@atr.co.jp 
 
 
Abstract 
In order to boost the translation quality of 
EBMT based on a small-sized bilingual cor-
pus, we use an out-of-domain bilingual corpus 
and, in addition, the language model of an in-
domain monolingual corpus. We conducted 
experiments with an EBMT system. The two 
evaluation measures of the BLEU score and 
the NIST score demonstrated the effect of us-
ing an out-of-domain bilingual corpus and the 
possibility of using the language model. 
1 Introduction 
Example-Based Machine Translation (EBMT) is adapt-
able to new domains. If you simply prepare a bilingual 
corpus of a new domain, you?ll get a translation system 
for the domain. However, if only a small-sized corpus is 
available, low translation quality is obtained. We ex-
plored methods to boost translation quality based on a 
small-sized bilingual corpus in the domain. Among 
these methods, we use an out-of-domain bilingual cor-
pus and, in addition, the language model (LM) of an in-
domain monolingual corpus. For accuracy of the LM, a 
larger training set is better. The training set is a target 
language corpus, which can be more easily prepared 
than a bilingual corpus. 
In prior works, statistical machine translation 
(Brown, 1993) used not only LM but also translation 
models. However, making a translation model requires a 
bilingual corpus. On the other hand, in some studies on 
multiple-translation selection, the LM of the target lan-
guage is used to calculate translation scores (Kaki, 
1999; Callison-Burch, 2001). For adaptation, we use the 
LM of an in-domain target language. 
In the following sections, we describe the methods 
using an out-of-domain bilingual corpus and an in-
domain monolingual corpus. Moreover, we report on 
our experiments. 
2 Adaptation Methods 
EBMT (Nagao, 1984) retrieves the translation ex-
amples that are most similar to an input expression and 
adjusts the examples to obtain the translation. The 
EBMT system in our approach retrieves not only in-
domain examples, but also out-of-domain examples. 
When using out-of-domain examples, suitability to the 
target domain is considered. We tried the following 
three types of adaptation methods. 
 
(1) Merging equally 
 
An in-domain corpus and an out-of-domain corpus are 
simply merged and used without distinction. 
 
(2) Merging with preference for in-domain corpus 
 
An in-domain corpus and an out-of-domain corpus are 
merged. However, when multiple examples with the 
same similarity are retrieved, the in-domain examples 
are used. 
 
(3) Using LM 
 
Beforehand, we make an LM of an in-domain target 
language corpus and, according to the LM, assign a 
probability to the target sentence of each out-of-domain 
example. 
In the example retrieval phase of the EBMT system, 
two types of examples are handled differently.  
 
(3-1) From in-domain examples, the most similar exam-
ples are retrieved. 
(3-2) From out-of-domain examples, not only the most 
similar examples but also other examples that are 
nearly as similar are retrieved. In the retrieved ex-
amples, examples with the highest probabilities of 
their target sentences by the LM are selected. 
(3-3) From the results of both (3-1) and (3-2), the most 
similar examples are selected. Examples of (3-1) are 
used when the similarities are equal to each other. 
3 Translation Experiments 
3.1 Conditions 
In order to evaluate the adaptability of an EBMT with 
out-of-domain examples, we applied the methods de-
scribed in Section 2 to the EBMT and evaluated the 
translation quality in Japanese-to-English translation. 
We used an EBMT, DP-match Driven transDucer (D3, 
Sumita, 2001)  as a test bed. 
We used two Japanese-and-English bilingual cor-
pora. In this experiment on adaptation, as an out-of-
domain corpus, we used Basic Travel Expression Cor-
pus (BTEC, described as BE-corpus in Takezawa, 
2002); as an in-domain corpus,  we used a telephone 
conversation corpus (TEL). The statistics of the corpora 
are shown in Table 1. TEL is split into two parts: a  test 
set of 1,653 sentence pairs and a training set of 9,918. 
Perplexities reveal the large difference between the in-
domain and out-of-domain corpora. 
 
 
 
 
The translation qualities were evaluated by the 
BLEU score (Papineni, 2001) and the NIST score 
(Doddington, 2002). The evaluation methods compare 
the system output translation with a set of reference 
translations of the same source text by  finding se-
quences of words in the reference translations that 
match those in the system output translation. We used  
the English sentence corresponding to each input Japa-
nese sentence in the test set as the reference translation. 
Therefore, achieving a better score by the evaluation 
means that the translation results can be regarded  as  
more adequate translations for the domain. 
In order to simulate incremental expansion of an in-
domain bilingual corpus and to observe the relationship 
between corpus size and translation quality, translations 
were performed with some subsets of the training cor-
pus. The numbers of the sentence pairs are 0, 1000, .. , 
5000 and 9918, adding randomly selected examples 
from the training set. 
The LM of the domain?s target language was the 
word trigram model of the English sentences of the 
training set of TEL. We tried two patterns of training set 
quantities in making the LM: 1) all of the training set, 
and 2) the part of the set used for translation examples 
according to the numbers mentioned above. 
3.2 Results 
Table 2 shows the BLEU scores from the translation 
experiment, which show certain tendencies. Generally, 
by using more in-domain examples, the translation re-
sults steadily achieve better scores. The score when us-
ing 4,000 in-domain examples exceeded that when 
using 152,172 out-of-domain examples. Equal merging 
outperformed using only out-of-domain examples. 
Merging with in-domain preference outperformed equal 
merging, and using LM outperformed merging with in-
domain preference. Comparing the two cases using LM, 
using LM made from all of the training set got a slightly 
better scores than the other, which implies that better 
LM is made from a larger corpus. All of the adaptation 
methods are more effective when a smaller-sized in-
domain corpus is available. When using no in-domain 
examples, the effect of using LM made from the entire 
training set was relatively large. 
Table 3 shows the NIST scores for the same experi-
ment. We can observe the same tendencies as in the 
table of BLEU scores, except that the advantage of us-
ing LM made from all of the training set over that from 
a partial set was not observed. 
4 Conclusion and Future Work 
A corpus-based approach is able to quickly build a ma-
chine translation system for a new domain if a bilingual 
corpus of that domain is available. However, if only a 
small-sized corpus is available, a low translation quality 
is obtained. In order to boost the performance, several 
methods using out-of-domain data were explored in this 
paper. The experimental results showed the effect of 
using an out-of-domain corpus by two evaluation meas-
ures, i.e., the BLEU score and the NIST score. 
We also showed the possibility of increasing the 
translation quality by using the LM of the domain?s 
target language. However, the gains from using the LM 
in the evaluation scores were not significant. We must 
continue experiments with other corpora and under 
various conditions. In addition, though we?ve implicitly 
assumed a high-quality in-domain corpus, next we?d 
like to investigate using a low-quality corpus. 
 
 
 
 
 
 
 
Table 1. Corpus Statistics 
BTEC TEL  
Japanese English Japanese English
# of sentences 152,172 11,571 
# of words 1,045,694 909,270 103,860 92,749
Vocabulary size 19,999 12,268 5,242 4,086
Average sen-
tence length 6.87 5.98 8.98 8.02
24.19 28.85 37.22 40.04
TEL language model BTEC language modelPerplexity (word trigram) 
190.77 142.04 57.27 81.26
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Acknowledgements 
The research reported here was supported in part by a 
contract with the Telecommunications Advancement 
Organization of Japan entitled, ?A study of speech dia-
logue translation technology based on a large corpus?. 
References 
Takezawa, T. et al 2002. Toward a Broad-coverage 
Bilingual Corpus for Speech Translation of Travel 
Conversations in the Real World, Proc. of  LREC-
2002 
Papineni, K. et al 2001. Bleu: a Method for Automatic 
Evaluation of Machine Translation, RC22176, Sep-
tember 17, 2001, Computer Science 
Doddington, G. 2002. Automatic Evaluation of Machine 
Translation Quality Using N-gram Co-Occurrence 
Statistics. Proc. of the HLT 2002 Conference 
Nagao, M. 1984. A Framework of a Mechanical Trans-
lation between Japanese and English by Analogy 
Principle, in Artificial and Human Intelligence, 
Elithorn, A. and Banerji, R. (eds.). North-Holland 
Sumita, E. 2001 Example-based machine translation 
using DP-matching between word sequences, Proc. 
of DDMT Workshop of 39th ACL 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Brown, P. F. et al 1993. The mathematics of statistical 
machine translation: Parameter estimation, Computa-
tional Linguistics, 19(2) 
Kaki, S. et al 1999. Scoring multiple translations using 
character N-gram, Proc. of  NLPRS-99 
Callison-Burch, C. et al 2001. A Program for Auto-
matically Selecting the Best Output from Multiple 
Machine Translation Engines, Proc. of MT Summit 
VIII 
Table 2. Experimental results of translation by BLEU scores 
# of in-domain examples 0 1,000 2,000 3,000 4,000 5,000 9,918 
Using in-domain examples --- 0.0190 0.0602 0.0942 0.1200 0.1436 0.2100 
Using out-of-domain examples 0.1099 
Merging equally 0.1271 0.1430 0.1590 0.1727 0.1868 0.2303 
Merging with preference for in-domain 0.1296 0.1469 0.1632 0.1776 0.1922 0.2333 
Using LM of partial training set 
0.1099
0.1361 0.1538 0.1686 0.1829 0.1976 
Using LM of all training set 0.1225 0.1393 0.1557 0.1716 0.1852 0.1987 0.2387 
Table 3. Experimental results of translation by NIST scores 
# of in-domain examples 0 1,000 2,000 3,000 4,000 5,000 9,918 
Using in-domain examples --- 0.0037 0.1130 0.4168 0.7567 1.1619 2.7400 
Using out-of-domain examples 1.1126 
Merging equally 1.4283 1.7367 2.0690 2.3405 2.6142 3.5772 
Merging with preference for in-domain 1.4580 1.7975 2.1343 2.4045 2.7088 3.6255 
Using LM of partial training set 
1.1126
1.7454 2.0449 2.3639 2.5825 2.9304 
Using LM of all training set 1.4404 1.7007 2.0125 2.3484 2.5992 2.8973 3.7544 
Input Sentence Splitting and Translating 
Takao Doi,  Eiichiro Sumita 
ATR Spoken Language Translation Research Laboratories  
2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan 
{takao.doi, eiichiro.sumita}@atr.co.jp 
 
 
  
Abstract 
We propose a method to split and translate 
input sentences for speech translation in order 
to overcome the long sentence problem. This 
approach is based on three criteria used to 
judge the goodness of translation  results. 
The criteria utilize the output of an MT sys-
tem only and assumes neither a particular lan-
guage nor a particular MT approach. In an 
experiment with an EBMT system, in which 
prior methods cannot work or work badly, the 
proposed split-and-translate method achieves 
much better results in translation  quality. 
1 Introduction 
To achieve translation technology that is adequate for 
speech translation, the possibilities of several corpus-
based approaches are being investigated. Among these 
methods, DP-match Driven transDucer (D3) has been 
proposed as an Example-Based Machine Translation 
(EBMT). When D3 is adapted to Japanese-to-English 
translation in a travel conversation domain, the method 
can achieve a high translation quality (Sumita, 2001 and 
2002). On the other hand, the translation method is sen-
sitive to the long sentence problem, where longer input 
sentences make it more difficult for a machine transla-
tion (MT) system to perform good translation. To over-
come this problem, the technique of splitting an input 
sentence 1  and translating the split sentences appears 
promising. 
The methods of previous studies related to this ap-
proach can be roughly classified into two types: one 
splits sentences before translation and the other splits 
them in the parsing phase of translation. We?ll call the 
former pre-process-splitting, the latter parse-time-
                                                           
1 Strictly speaking, this isn't necessarily a sentence but an 
utterance including sentences. In this paper, we use the term 
sentence without strictly defining it to simplify discussion. 
splitting, and translation with any splitting split-and-
translate. 
In previous research on pre-process-splitting, such as 
Takezawa (1999), many methods have been based 
on word-sequence characteristics. Some research efforts 
have achieved high performance in recall and precision 
against correct splitting positions. Despite such a high 
performance, from the view point of translation, MT 
systems are not always able to translate the split sen-
tences well. 
In some research works  on parse-time-splitting, 
such as Furuse (1998 and 2001), sentences have been 
split based on parsing trees under construction. Partly 
constructed trees are combined and translated. A sen-
tences is split according to the sub-trees. The split sen-
tences can be translated because an internal parsing 
mechanism guarantees their fitness. However, parse-
time-splitting technique cannot be adapted, or can be 
adapted only as pre-process-splitting by using an exter-
nal parsing system, to MT systems that deal with no 
parsing tree, such as D3 and Statistical MT.  
In this paper, we propose another split-and-translate 
technique in which splitting and translation act in har-
mony. This technique depends on no particular MT 
method, therefore can be applied to D3. In order to 
prove the effect for translation quality, our proposed 
split-and-translate method and, for the purpose of com-
parison, a pre-process-splitting technique are evaluated. 
For convenience, we'll call the two split-and-translate 
methods in our experiments as follows. 
  
method-T: Our proposed method based on partial Trans-
lation results, described in section 2. 
method-N: Before translation, splitting an input sen-
tence with the pre-process-splitting method  based 
on N-gram, described in section 3. 
  
The following sections describe the two methods, the 
MT system, D3 that the methods are applied to, and ex-
periments. 
2 Proposed Split-and-Translate: Method-
T 
An MT system sometimes fails to translate an input, for 
example, due to failure in parsing a sentence or retriev-
ing examples. Such a failure occurs particularly when 
an input is longer. In such a case, by splitting the input, 
translation may be successfully performed for each por-
tion. Therefore, one idea is to arrange the translations of 
split portions in the same order as in the source sentence 
and to consider the arrangement as a translation of the 
entire input sentence. Particularly in a dialogue, sen-
tences tend not to have complicated nested structures, 
and many long sentences can be split into mutually in-
dependent portions. Therefore, if splitting positions and 
translations of split portions are adequate, the possibility 
that this simple arrangement of the translations can pro-
vide an adequate translation of the complete input is 
relatively high. 
In the example below, a Japanese sentence (1-j) has 
potentially adequate splitting positions such as (1-j'). 
The arrangement of the English translations of the por-
tions (1-e) is an adequate translation. 
 
(1-j) sou desu ka ee kekkou desu jaa tsuin de o negai shi 
masu2 
(1-j?) sou desu ka | ee | kekkou desu | jaa | tsuin de o 
negai shi masu 
(1-e) i see | yes | that's fine | then | a twin please 
2.1 Criteria 
When you split-and-translate a sentence, some por-
tions can be translated while others cannot. We call the 
count of words in the portions that cannot be translated 
the fault-length. It is natural to consider (X) as a crite-
rion to judge the goodness of split-and-translate results. 
  
(X) The smaller the fault-length is,  the better the result 
is. 
  
Let the term partial-translation be the translation of a 
portion that can be translated. In a split-and-translate 
result, there can be some partial-translations. Partial-
translation-count expresses the number of partial-
translations. (Y) is also a natural criterion to judge the 
goodness of a split-and-translate result. 
  
(Y) The smaller the partial-translation-count is, the 
better the result is. 
  
Many current MT methods produce not only target sen-
tences but also scores. The meaning of a score, depend-
                                                           
2 Its English translation is ?I see. Then, fine, we?ll take a 
twin.? in a corpus 
ing on the translation method, can be parsing cost, dis-
tance between sentences, word correspondence prob-
ability, or other meanings or combinations of the above. 
If there is a correlation between the score and the trans-
lation quality, we can make use of this score as a confi-
dence factor of translation. We can use the confidence 
factor as another criterion for split-and-translate results. 
In order to ensure reliability for the complete result of 
split-and-translate procedures from confidence factors, 
the scores of all partial-translations are combined. We 
call this combined score the combined-reliability. How 
to combine scores depends on the mathematical charac-
teristics of the scores. Therefore the third criterion (Z) is 
added. 
  
(Z) The higher the combined-reliability is, the better the 
result is. 
  
From the above considerations, the proposed method 
utilizes these criteria to judge the goodness of split-and-
translate results with the priority as follows. 
  
1. The smaller the fault-length is, the better the result 
is. 
2. Unless judged with criterion-1, the smaller the 
partial-translation-count is, the better the result is. 
3. Unless judged with criterion-1 or criterion-2, the 
higher the combined-reliability is, the better the 
result is. 
  
The case where translation can be performed without 
splitting meets these criteria. In this case, the fault-
length is 0, the partial-translation-count is 1, and the 
combined-reliability equals the score of the complete 
translation that must be utilized by the MT system; 
therefore, this result is the best. 
Criterion-3 has a low priority. Unless an MT system 
has a confidence factor, only criteria-1-2 are used.  
These three criteria are based on the output of an 
MT system, that is, how well the MT system can trans-
late portions. Split portions are translated, and the par-
tial-translation results are evaluated to select the best 
split positions (the algorithm is discussed in section 6). 
As the proposed split-and-translate method is based on 
these criteria only, this method assumes no parsing 
process and depends on neither a particular language 
nor a particular MT method. 
2.2 Example 
Below, we show an example of selecting a result 
from candidates based on criteria-1-2. 
  
(2-j) hai wakari mashi ta sore to ne choushoku na n desu 
kedomo dou nat teru n deshou ka3 
  
(2-j') hai | wakari mashi ta | sore to ne | choushoku na n 
desu kedomo | dou nat teru n deshou ka 
  
For a Japanese input (2-j), there are many candidates of 
splitting points such as (2-j?). We consider three split-
tings: (2-a), (2-b) and (2-c). 
  
(2-a) hai wakari mashi ta | sore to ne choushoku na n 
desu kedomo dou nat teru n deshou ka 
(2-b) hai wakari mashi ta sore to ne choushoku na n 
desu kedomo | dou nat teru n deshou ka 
(2-c) hai wakari mashi ta | sore to ne choushoku na n 
desu kedomo | dou nat teru n deshou ka 
  
Suppose the partial translations corresponding to these 
candidates are as follows, where fault-lengths and par-
tial-translation-counts are calculated. 
  
(2-a?) 
hai wakari mashi ta => yes i see 
sore to ne cyoushoku na n desu kedomo dou nat teru 
n deshou ka 
=> and what about breakfast 
fault-length = 0 
partial-translation-count = 2 
(2-b?) 
hai wakari mashi ta sore to ne choushoku na n desu 
kedomo => FAIL 
dou nat teru n deshou ka => what happened to it 
fault-length = 12 
partial-translation-count = 1 
(2-c?) 
hai wakari mashi ta => yes i see 
sore to ne choushoku na n desu kedomo => and i 
breakfast 
dou nat teru n deshou ka => what happened to it 
fault-length = 0 
partial-translation-count = 3 
  
(2-a) and (2-c) are better than (2-b) based on criterion-1, 
and (2-a) is better than (2-c) based on criterion-2, so the 
rank is (2-a), (2-c), (2-b). 
3 Pre-Process-Splitting for Method-N 
For splitting input sentences as a pre-process of MT 
systems, we consider a previous study of pre-process-
splitting. Many pre-process-splitting methods are based 
on word-sequence characteristics. Among them, we use 
the method of Takezawa (1999), a pre-process-splitting 
based on the N-gram of part-of-speech subcategories. 
                                                           
3 Its English translation is ?I see. And also how about break-
fast?? in a corpus 
This method is derived from that of Lavie (1996) and 
modified especially for Japanese. 
The function of this method is to infer where split-
ting positions are. Splitting positions are defined as po-
sitions at which we can put periods. For each position, 
to calculate the plausibility that the position is a splitting 
position, we consider the previous two words and the 
following one word, three words in total. Part-of-speech 
and conjugation-type are considered as word character-
istics. When the plausibility is higher than a given 
threshold, the position is regarded as a splitting position. 
The threshold is manually selected to tune the perform-
ance for a training set. Equation [1] shows how to calcu-
late the plausibility 
~
F . 
  
[1]
 
 
 
     
     3221
3221
321
~
wwCwwC
wwCwwCwwwF



, 
  
where 
~
F ([w1w2 w3]) is the plausibility that the posi-
tion after a word sequence w1w2  and before a word w3  
is a splitting position,   [wlwm] is a bigram, [wlwmwn] is 
a trigram,   indicates a boundary of sentences, and 
C(N-gram) means the appearance count of the N-gram 
in a training set. 
  
It has also been reported that, for Japanese, three 
heuristics for Japanese part-of-speech and conjugation-
type improve the performance. The heuristics indicate 
that the positions before and after particular part-of-
speeches with particular conjugation types must or must 
not be splitting positions. 
4 Applying Split-and-Translate to MT 
Systems 
We apply the two split-and-translate methods to an MT 
system, D3. To apply method-N to an MT system is 
straightforward. When applying method-T, we consider 
the confidence factor of the MT system for criterion-3, 
rather as an optional criterion.  
4.1 D3 Overview 
D3 (Sumita, 2001) is an EBMT whose language re-
sources are [i] a bilingual corpus, in which sentences are 
aligned beforehand; [ii] a bilingual dictionary, which is 
used for word alignment and generating target sen-
tences; and [iii] thesauri of both languages, which are 
used for aiding word alignment and incorporating the 
semantic distance between words into the word se-
quence distance. 
  D3 retrieves the most similar source sentence of exam-
ples from a bilingual corpus. For this purpose, DP-
matching is used, which tells us the distance between 
word sequences, dist, while giving us the matched por-
tions between the input and the example. dist is calcu-
lated as equation [2]. The counts of Insertion (I), 
Deletion (D), and substitution operations are summed. 
Then, this total is normalized by the sum of the lengths 
of the source and example sequences. Substitution is 
considered the semantic distance between two substi-
tuted words, or SEMDIST, which is defined using a the-
saurus and ranges from 0 to 1. 
  
[2] 
exampleinput LL
SEMDISTDI
dist




2
 
  
The characteristics of D3, especially in comparison with 
most EBMT proposals, are a) D3 does not assume syn-
tactic parsing and bilingual tree-banks; b) D3 generates 
translation patterns on the fly according to the input and 
the retrieved translation examples as needed; c) D3 uses 
examples sentence-by-sentence and does not combine 
examples. 
Because of c), D3's result is pretty good when a simi-
lar example is retrieved, but very bad otherwise. There-
fore, we usually decide a threshold. If there is no 
example whose dist is within the given threshold, we 
must give up performing translation. 
In an experiment using Basic Travel Expression Corpus 
(BTEC, described as BE-corpus in Takezawa, 2002), 
D3?s translation quality is very high. The experiment 
also shows a clear correlation between dist and the qual-
ity of translation. In other words, the accuracy decreases 
as the dist increases. In particular, the longer input sen-
tences are, the more difficult for D3 to find examples 
with a small dist. 
4.2 Applying Method-T to D3 
As there is a correlation between dist and the translation 
quality, we can make use of dist as a confidence factor. 
To make the combined-reliability, each partial transla-
tion is weighted with its source word's number. That is, 
for each partial translation, its dist is multiplied by its 
source portion's length, and the resulting values are 
summed. 
  
[3] combined reliability

 portionportion Ldist  
  
Adapting to D3, criterion-3 is instantiated by the com-
bined-reliability defined in equation [3]. 
5 Experiment 
5.1 Preliminary 
Target Systems 
We investigated the two split-and-translate methods 
using D3 in Japanese-to-English translation. We used a 
Japanese-and-English bilingual corpus, BTEC as the 
training set for D3 and the Japanese part of BTEC as 
that for pre-process-splitting method for method-N. 
BTEC is a collection of Japanese sentences and their 
English translations usually found in phrase-books for 
foreign tourists. The statistics of the corpus is shown in 
Table 1. 
Regarding D3, the threshold for dist is 1/3.  
For the pre-process-splitting method of method-N, 
the combinations of the parameters were used:  1) 
whether the heuristics for Japanese are used or not; 2) 
the threshold of splitting plausibility. The best results 
were selected from among the combinations in subsec-
tions 5.3 and 5.5. 
  
Table 1. Corpus Statistics 
 Japanese English 
# of sentences 152,172 
# of words 1,039,482 890,466
Vocabulary size 18,098 11,690
Average sen-
tence length 6.83 5.85
  
Evaluation 
The target is Japanese-to-English translation in this ex-
periment. We extracted a test set from Bilingual Travel 
Conversation Corpus of Spoken Language (TC-corpus, 
Takezawa, 2002). All of the contents of TC-corpus are 
transcriptions of spoken dialogues between Japanese 
and English speakers through human interpreters.  The 
test set of this experiment is 330 Japanese sentences 
from TC-corpus including no sentences spoken by the 
interpreters. The average length of the sentences in the 
test set is 11.4 (words). Therefore, the test sentences 
used in this experiment are much longer than the sen-
tences in the training set, BTEC. 
In this experiment, each translation result is graded 
into one of four ranks (described below) by a bilingual 
human translator who is a native speaker of the target 
language, American English: 
(A) Perfect: no problem in either information or 
grammar; 
(B) Fair: easy-to-understand with some unimportant 
information missing or flawed grammar; 
(C) Acceptable: broken but understandable with effort; 
(D)  Nonsense: important information has been trans-
lated incorrectly (Sumita, 1999). 
Adding to the four ranks, we use FAIL, or F, to indicate 
that there is no output sentence. 
5.2 Translation without Splitting 
Translations of the test set by D3 without splitting were 
performed. The coverage of the output is lower. For 127 
sentences, D3 cannot yield results. The average length of 
the 127 sentences is 15.6. Afterward, we used these 127 
sentences as the test set for split-and-translate methods. 
5.3 Pre-Process-Splitting Quality 
Before evaluating translation qualities of split-and-
translate methods, we calculated the quality of the pre-
process-splitting method of method-N on the 127 sen-
tences. The positions where periods were manually in-
serted were regarded as the correct splitting positions. In 
the manual splitting process, they put periods at posi-
tions considered both grammatically and semantically 
adequate. There were 60 splitting positions, and 79 sen-
tences, accounting for 62% of the 127 sentences, had no 
splitting position. Table 2 shows the numbers of sen-
tences corresponding to those of splitting positions in a 
sentence. 
 
 
Table 2. Number of splitting positions in a 
sentence vs. total number of sentences 
# of split positions 0 1 2 3 
# of sentences 79 37 10 1 
 
 
The evaluation measure is based on how closely the 
result of the method corresponds to the correct solution, 
that is, recall and precision. We got a good result. The 
count of inferred positions is 65 in total, in which 55 
positions are correct and 10 are incorrect, that is, recall 
is 91.7% and precision is 84.6%. 
We also conducted an experiment on method-T as a 
method for only splitting sentences, extracting partial-
translation boundaries. The result was bad: The count of 
inferred positions is 277 in total, in which 28 positions 
are correct and 249 are incorrect, that is, recall is 46.7% 
and precision is 10.1%. Although a smaller number of 
splittings is preferred with method-T, when most of the 
translations of long portions fail, method-T results in 
over-splitting. 
The results show that the performance of method-N 
is much better than that of method-T when the target is 
only to split sentences. 
5.4 Translation Quality of Method-T 
Applying method-T to D3, we performed translations of 
the 127 sentences by D3. Table 3 shows the results, the 
number of each evaluation rank and the rate of the total 
number for each rank and better ranks than itself. As 
shown in the table, the rate of output is 100%, and the 
rate of success, which means that the rank is A, B or C, 
is 42.5%. 
  
 
Table 3. Number and percentage of each Rank 
(Method-T) 
A 
 (A) 
B  
(A+B) 
C 
(A+B+C) 
D 
(A+B+C+D) 
F
4 
(3.1%)
16 
(15.7%) 
34 
(42.5%) 
73 
 (100%) 
0
  
 
There are correlations between quality ranks and fault-
length or partial-translation-count. When the ratio of 
the fault-length to the entire input length is greater than 
40% or the partial-translation-count is greater than 4, 
no result is successful. 
 
 
 
 
Figure 1. Success rate and dist-in-splitting 
  
  
Furthermore, we can observe a correlation between suc-
cess rate and dist-in-splitting in Figure 1. dist-in-
splitting is defined by equation [4], an extension of dist, 
and ranges from 0 to 1. These correlations can give us a 
confidence factor on split-and-translate results. 
 
 
[4] dist in splitting
input
portionportion
L
Ldist


  , 
where 0.1portiondist  when the portion cannot be 
translated. 
dist-in-splitting < 
0
20
40
60
80
100
0.
1
0.
15 0.
2
0.
25 0.
3
0.
35 0.
4
0.
45 0.
5
0.
55 0.
6
0.
65 0.
7
0.
75 0.
8
A
+B
+C
 (%
)
5.5 Translation Quality of Method-N 
Applying method-N to D3, we performed translations of 
the 127 sentences by D3. Table 4 shows the results, 
which give the largest rate of success among the combi-
nations of the parameters. 
 
 
Table 4. Number and percentage of each 
Rank (Method-N) 
A  
(A) 
B 
(A+B) 
C 
(A+B+C) 
D 
(A+B+C+D) 
F 
4 
(3.1%) 
7 
(8.7%) 
16 
(21.3%) 
85 
 (88.2%) 
15
 
 
The condition that is good for sentence splitting quality 
is not good for split-and-translate quality. On the condi-
tion of the parameters that gave the recall of 91.7% and 
the precision of 84.6%, the rate of output was 41.7% 
and that of success 6.3%. According to the correct split-
ting solution, among the 127 sentences that D3 fails to 
translate without splitting, 79 sentences have no split-
ting position. Therefore, a good splitting for recall and 
precision has low probabilities for the rate of output and 
that of success. Put simply, when the threshold is 
smaller, although precision is worse, the rate of output 
and that of success are larger. However, the rates are 
much lower than those of method-T?s results. 
5.6 Summary of Experiments 
  
Table 5. Splitting Quality and Split-and-Translate 
Quality 
Splitting Split-and-Translate 
recall precision success 
rate 
output 
rate 
Method-T 46.7% 10.1% 42.5% 100.0%
Method-N 91.7% 84.6% 21.3% 88.2%
 
 
Table 5 shows the summary of experiments. Though 
method-N is better in sentence splitting quality, method-
T is better in split-and-translate quality. 
6 Concluding Remarks 
We have proposed a split-and-translate method and 
shown its effect through experiments. However, much 
more work remains to be accomplished. 
 
To Improve Accuracy 
The proposed method is based on three criteria. Al-
though we have shown one combination of the criteria, 
there may be better combinations. Another possibility 
might be to integrate our method with another pre-
process-splitting method, for example, by giving higher 
priorities to splitting positions as the latter method im-
plies, which can be also used to improve the efficiency 
discussed below. 
For Efficiency 
Let N be the length of an input sentence, a naive imple-
mentation must search the solution in 2N-1 combinations, 
while trying (N+1)N/2 kinds of partial translations. 
However, there are several ways to optimize the algo-
rithm. For example, it can be regarded as a shortest path 
problem, where each portion is an arc and portions 
without translations have high costs. There are effective 
algorisms for a shortest path problem. In addition, when 
the quality of translation has correlations with fault-
length, partial-translation-count, and dist-in-splitting, 
as observed in subsection 5.4, candidates can be pruned 
by placing constraints on these factors. 
 
Acknowledgements 
The research reported here was supported in part by a 
contract with the Telecommunications Advancement 
Organization of Japan entitled, ?A study of speech dia-
logue translation technology based on a large corpus?. 
References 
Takezawa, T. et al 2002. Toward a Broad-coverage 
Bilingual Corpus for Speech Translation of Travel 
Conversations in the Real World, Proc. of  LREC-
2002 
Sumita, E. et al 1999 Solutions to Problems Inherent in 
Spoken-language Translation: The ATR-MATRIX 
Approach, Proc. of MT Summit VII 
Berger, A.L. et al 1996. A Maximum Entropy Ap-
proach to Natural Language Processing, Association 
for Computatial Linguistics 
Lavie, A. et al 1996. Input Segmentation of Spontane-
ous Speech in JANUS: a Speech-to-speech Transla-
tion System, Proc. of ECAI-96 Workshop on 
Dialogue Processing in Spoken Language Systems 
Takezawa, T. et al 1999. Transformation into Meaning-
ful Chunks by Dividing or Connecting Utterance 
Units, Journal of Natural Language Processing, Vol. 6 
No. 2 (in Japanese) 
Nakajima, H. et al 2001. The Statistical Language 
Model for Utterance Splitting in Speech Recognition, 
Transactions of IPSJ, Vol. 42 No. 11 (in Japanese) 
Kim, Y. B. et al 1994. An Automatic Sentence Break-
ing and Subject Supplement Method for J/E Machine 
Translation, Transactions of IPSJ, Vol. 35 No. 6 (in 
Japanese) 
Furuse, O. et al 1998. Splitting Long or Ill-formed In-
put for Robust Spoken-language Translation, Proc. of 
COLING-ACL?98, pp. 421-427 
Furuse, O. et al 2001. Splitting Ill-formed Input for 
Robust Multi-lingual Speech Translation, Transac-
tions of IPSJ, Vol. 42 No. 5 (in Japanese) 
Wakita, Y. et al 1997. Correct parts extraction from 
speech recognition results using semantic distance 
calculation, and its application to speech translation. 
Proc. of ACL/EACL Workshop on Spoken Language 
Translation, pp. 24-31 
Sumita, E. 2001 Example-based machine translation 
using DP-matching between word sequences, Proc. of 
DDMT Workshop of 39th ACL 
Sumita, E. 2002. Corpus-Centered Computation, ACL-
02 Workshop on Speech-to-speech Translation, pp. 1-
8 
  
