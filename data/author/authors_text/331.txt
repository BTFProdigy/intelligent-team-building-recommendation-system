Proceedings of the Third Workshop on Statistical Machine Translation, pages 123?126,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The University of Washington Machine Translation System for
ACL WMT 2008
Amittai Axelrod, Mei Yang, Kevin Duh, Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA 98195
{amittai,yangmei,kevinduh,katrin} @ee.washington.edu
Abstract
This paper present the University of Washing-
ton?s submission to the 2008 ACL SMT shared ma-
chine translation task. Two systems, for English-to-
Spanish and German-to-Spanish translation are de-
scribed. Our main focus was on testing a novel
boosting framework for N-best list reranking and
on handling German morphology in the German-to-
Spanish system. While boosted N-best list reranking
did not yield any improvements for this task, simpli-
fying German morphology as part of the preprocess-
ing step did result in significant gains.
1 Introduction
The University of Washington submitted systems
to two data tracks in the WMT 2008 shared task
competition, English-to-Spanish and German-to-
Spanish. In both cases, we focused on the in-domain
test set only. Our main interest this year was on in-
vestigating an improved weight training scheme for
N-best list reranking that had previously shown im-
provements on a smaller machine translation task.
For German-to-Spanish translation we additionally
investigated simplifications of German morphology,
which is known to be fairly complex due to a large
number of compounds and inflections. In the fol-
lowing sections we first describe the data, baseline
system and postprocessing steps before describing
boosted N-best list reranking and morphology-based
preprocessing for German.
2 Data and Basic Preprocessing
We used the Europarl data as provided (version 3b,
1.25 million sentence pairs) for training the transla-
tion model for use in the shared task. The data was
lowercased and tokenized with the auxiliary scripts
provided, and filtered according to the ratio of the
sentence lengths in order to eliminate mismatched
sentence pairs. This resulted in about 965k paral-
lel sentences for English-Spanish and 950k sentence
pairs for German-Spanish. Additional preprocess-
ing was applied to the German corpus, as described
in Section 5. For language modeling, we addition-
ally used about 82M words of Spanish newswire text
from the Linguistic Data Consortium (LDC), dating
from 1995 to 1998.
3 System Overview
3.1 Translation model
The system developed for this year?s shared task
is a state-of-the-art, two-pass phrase-based statisti-
cal machine translation system based on a log-linear
translation model (Koehn et al 2003). The trans-
lation models and training method follow the stan-
dard Moses (Koehn et al 2007) setup distributed as
part of the shared task. We used the training method
suggested in the Moses documentation, with lexical-
ized reordering (the msd-bidirectional-fe
option) enabled. The system was tuned via Mini-
mum Error Rate Training (MERT) on the first 500
sentences of the devtest2006 dataset.
123
3.2 Decoding
Our system used the Moses decoder to generate
2000 output hypotheses per input sentence during
the first translation pass. For the second pass, the
N-best lists were rescored with the additional lan-
guage models described below. We re-optimized the
model combination weights with a parallelized im-
plementation of MERT over 16 model scores on the
test2007 dataset. Two of these model scores for
each hypothesis were from the two language models
used in our second-pass system, and the rest corre-
spond to the 14 Moses model weights (for reorder-
ing, language model, translation model, and word
penalty).
3.3 Language models
We built all of our language models using the
SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney discounting and interpolating all n-
gram estimates of order > 1. For first-pass de-
coding we used a 4-gram language model trained
on the Spanish side of the Europarl v3b data. The
optimal n-gram order was determined by testing
language models with varying orders (3 to 5) on
devtest2006; BLEU scores obtained using the
various language models are shown in Table 1. The
4-gram model performed best.
Table 1: LM ngram size vs. output BLEU on the dev sets.
order devtest2006 test2007
3-gram 30.54 30.69
4-gram 31.03 30.94
5-gram 30.85 30.84
Two additional language models were used for
second pass rescoring. First, we trained a large out-
of-domain language model on Spanish newswire
text obtained from the LDC, dating from 1995 to
1998.
We used a perplexity-filtering method to filter out
the least relevant half of the out-of-domain text, in
order to significantly reduce the training time of
the large language model and accelerate the rescor-
ing process. This was done by computing the per-
plexity of an in-domain language model on each
newswire sentence, and then discarding all sen-
tences with greater than average perplexity. This
reduced the size of the training set from 5.8M sen-
tences and 166M tokens to 2.8M sentences and 82M
tokens. We then further restricted the vocabulary to
the union of the vocabulary lists of the Spanish sides
of the de-es and en-es parallel training corpora. The
remaining text was used to train the language model.
The second language model used for rescoring
was a 5-gram model over part-of-speech (POS) tags.
This model was built using the Spanish side of the
English-Spanish parallel training corpus. The POS
tags were obtained from the corpus using Freeling
v2.0 (Atserias et al 2006).
We selected the language models for our transla-
tion system were selected based on performance on
the English-to-Spanish task, and reused them for the
German-to-Spanish task.
4 Boosted Reranking
We submitted an alternative system, based on a
different re-ranking method, called BoostedMERT
(Duh and Kirchhoff, 2008), for each task. Boosted-
MERT is a novel boosting algorithm that uses Mini-
mum Error Rate Training (MERT) as a weak learner
to build a re-ranker that is richer than the standard
log-linear models. This is motivated by the obser-
vation that log-linear models, as trained by MERT,
often do not attain the oracle BLEU scores of the N-
best lists in the development set. While this may be
due to a local optimum in MERT, we hypothesize
that log-linear models based on our K re-ranking
features are also not sufficiently expressive.
BoostedMERT is inspired by the idea of Boosting
(for classification), which has been shown to achieve
low training (and generalization) error due to classi-
fier combination. In BoostedMERT, we maintain a
weight for each N-best list in the development set.
In each iteration, MERT is performed to find the best
ranker on weighted data. Then, the weights are up-
dated based on whether the current ranker achieves
oracle BLEU. For N-best lists that achieve BLEU
scores far lower than the oracle, the weights are in-
creased so that they become the emphasis of next
iteration?s MERT. We currently use the factor e?r
to update the N-best list distribution, where r is the
ratio of the oracle hypothesis? BLEU to the BLEU
of the selected hypothesis. The final ranker is a
124
weighted combination of many such rankers.
More precisely, let wi be the weights trained by
MERT at iteration i. Given any wi, we can gener-
ate a ranking yi over an N-best list where yi is an
N-dimensional vector of predicted ranks. The final
ranking vector is a weighted sum: y =
?T
i=1 ?iyi,
where ?i are parameters estimated during the boost-
ing process. These parameters are optimized for
maximum BLEU score on the development set. The
only user-specified parameter is T , the number of
boosting iterations. Here, we choose T by divid-
ing the dev set in half: dev1 and dev2. First, we
train BoostedMERT on dev1 for 50 iterations, then
pick the T with the best BLEU score on dev2. Sec-
ond, we train BoostedMERT on dev2 and choose the
optimal T from dev1. Following the philosophy of
classifier combination, we sum the final rank vectors
y from each of the dev1- and dev2-trained Boosted-
MERT to obtain our final ranking result.
5 German ? Spanish Preprocessing
German is a morphologically complex language,
characterized by a high number of noun compounds
and rich inflectional paradigms. Simplification of
morphology can produce better word alignment, and
thus better phrasal translations, and can also signifi-
cantly reduce the out-of-vocabulary rate. We there-
fore applied two operations: (a) splitting of com-
pound words and (b) stemming.
After basic preprocessing, the German half of the
training corpus was first tagged by the German ver-
sion of TreeTagger (Schmid, 1994), to identify part-
of-speech tags. All nouns were then collected into
a noun list, which was used by a simple compound
splitter, as described in (Yang and Kirchhoff, 2006).
This splitter scans the compound word, hypothesiz-
ing segmentations, and selects the first segmentation
that produces two nouns that occur individually in
the corpus. After splitting the compound nouns in
the filtered corpus, we used the TreeTagger again,
only this time to lemmatize the (filtered) training
corpus.
The stemmed version of the German text was used
to train the translation system?s word alignments
(through the end of step 3 in the Moses training
script). After training the alignments, they were pro-
jected back onto the unstemmed corpus. The parallel
phrases were then extracted using the standard pro-
cedure. Stemming is only used during the training
stage, in order to simplify word alignment. During
the evaluation phase, only the compound-splitter is
applied to the German input.
6 Results
6.1 English ? Spanish
The unofficial results of our 2nd-pass system for the
2008 test set are shown in Table 2, for recased, unto-
kenized output. We note that the basic second-pass
model was better than the first-pass system on the
2008 task, but not on the 2007 task, whereas Boost-
edMERT provided a minor improvement in the 2007
task but not the 2008 task. This is contrary to previ-
ous results in the Arabic-English IWSLT 2007 task,
where boosted MERT gave an appreciable improve-
ment. This result is perhaps due to the difference in
magnitude between the IWSLT and WMT transla-
tion tasks.
Table 2: En?Es system on the test2007 and test2008
sets.
System test2007 test2008
First-Pass 30.95 31.83
Second-Pass 30.94 32.72
BoostedMERT 31.05 32.62
6.2 German ? Spanish
As previously described, we trained two German-
Spanish translation systems: one via the default
method provided in the Moses scripts, and an-
other using word stems to train the word align-
ments and then projecting these alignments onto
the unstemmed corpus and finishing the training
process in the standard manner. Table 3 demon-
strates that the word alignments generated with
word-stems markedly improved first-pass transla-
tion performance on the dev2006 dataset. How-
ever, during the evaluation period, the worse of the
two systems was accidentally used, resulting in a
larger number of out-of-vocabulary words in the
system output and hence a poorer score. Rerun-
ning our German-Spanish translation system cor-
rectly yielded significantly better system results,
also shown in Table 3.
125
Table 3: De?Es first-pass system on the development
and 2008 test set.
System dev2006 test2008
Baseline 23.9 21.2
Stemmed Alignments 26.3 24.4
6.3 Boosted MERT
BoostedMERT is still in an early stage of experi-
mentation, and we were interested to see whether it
improved over traditional MERT in re-ranking. As it
turns out, the BLEU scores on test2008 and test2007
data for the En-Es track are very similar for both re-
rankers. In our post-evaluation analysis, we attempt
to understand the reasons for similar BLEU scores,
since the weights wi for both re-rankers are quali-
tatively different. We found that out of 2000 En-Es
N-best lists, BoostedMERT and MERT differed on
1478 lists in terms of the final hypothesis that was
chosen. However, although the rankers are choosing
different hypotheses, the chosen strings appear very
similar. The PER of BoostedMERT vs. MERT re-
sults is only 0.077, and manual observation indicates
that the differences between the two are often single
phrase differences in a sentence.
We also computed the sentence-level BLEU for
each ranker with respect to the true reference. This
is meant to check whether BoostedMERT improved
over MERT in some sentences but not others: if the
improvements and degradations occur in the same
proportions, a similar corpus-level BLEU may be
observed. However, this is not the case. For a major-
ity of the 2000 sentences, the sentence-level BLEU
for both systems are the same. Only 10% of sen-
tences have absolute BLEU difference greater than
0.1, and the proportion of improvement/degradation
is similar (each 5%). For BLEU differences greater
than 0.2, the percentage drops to 4%.
Thus we conclude that although BoostedMERT
and MERT choose different hypotheses quite of-
ten, the string differences between their hypotheses
are negligible, leading to similar final BLEU scores.
BoostedMERT has found yet another local optimum
during training, but has not improved upon MERT
in this dataset. We hypothesize that dividing up the
original development set into halves may have hurt
BoostedMERT.
7 Conclusion
We have presented the University of Washing-
ton systems for English-to-Spanish and German-to-
Spanish for the 2008 WMT shared translation task.
A novel method for reranking N-best lists based on
boosted MERT training was tested, as was morpho-
logical simplification in the preprocessing compo-
nent for the German-to-Spanish system. Our con-
clusions are that boosted MERT, though successful
on other translation tasks, did not yield any improve-
ment here. Morphological simplification, however,
did result in significant improvements in translation
quality.
Acknowledgements
This work was funded by NSF grants IIS-0308297
and IIS-0326276.
References
Atserias, J. et al 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP library.
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Genoa, Italy.
Duh, K., and Kirchhoff, K. 2008. Beyond Log-Linear
Models: Boosted Minimum Error Rate Training for
MT Re-ranking. To appear, Proceedings of the Associ-
ation for Computational Linguistics (ACL). Columbus,
Ohio.
Koehn, P. and Och, F.J. and Marcu, D. 2003. Statistical
phrase-based translation. Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, (HLT/NAACL). Edmonton, Canada.
Koehn, P. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation Proceedings of MT Summit.
Koehn, P. et al 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session. Prague, Czech Republic.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. International Conference on New
Methods in Language Processing, Manchester, UK.
Stolcke, A. 2002. SRILM - An extensible language mod-
eling toolkit. Proceedings of ICSLP.
Yang, M. and K. Kirchhoff. 2006. Phrase-based backoff
models for machine translation of highly inflected lan-
guages. Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL 2006). Trento, Italy.
126
On building a high performance gazetteer database
Amittai E. Axelrod
MetaCarta, Inc.
875 Massachusetts Ave., 6th Flr.
Cambridge, MA, 02139
amittai@metacarta.com
Abstract
We define a data model for storing geographic
information from multiple sources that en-
ables the efficient production of customizable
gazetteers. The GazDB separates names from
features while storing the relationships be-
tween them. Geographic names are stored in
a variety of resolutions to allow for i18n and
for multiplicity of naming. Geographic fea-
tures are categorized along several axes to fa-
cilitate selection and filtering.
1 Introduction
We are interested in collecting the largest possible set of
geographic entities, so as to be able to produce a variety
of extremely comprehensive gazetteers. These gazetteers
are currently produced to search for both direct and indi-
rect geospatial references in text. The production process
can be tailored to produce custom gazetteers for other ap-
plications, such as historical queries.
The purpose of the MetaCarta GazDB is to provide
both a place and supporting mechanisms for storing,
maintaining, and exporting everything we know about
our collection of geographic entities.
To produce a gazetteer from various data sources, we
make use of a database, the GazDB, as well as two sets
of scripts: conversion scripts, to transfer the data from
its source format into the GazDB, and export scripts to
output data from the GazDB in the form of gazetteers.
The interaction between these elements is illustrated in
Figure 1.
Geographic input data is collected from multiple (not
necessarily disjoint) sources, each with their own pecu-
liar format. As such, the conversion scripts must perform
some amount of normalization and classification of the
input data in order to maintain a single unified repository
Figure 1: The gazetteer production process
of geographic data. However, in order to justify the over-
head of consolidating all the data into a single entity, it
must be possible to output all of it into multiple gazetteers
designed for different goals.
It should also be possible to perform filtering oper-
ations on the gazetteer entries, such as comparing en-
try names against common-language dictionaries. This
can be used determine whether occurrences of gazetteer
names in documents are geographically relevant (Rauch
et al, 2003).
This is the task for the export scripts. However, in this
paper, we shall focus on the heart of the system, namely
the GazDB. Section 2 describes how the GazDB relates
geographic names and features. In Section 3 we describe
how the GazDB handles ambiguities and inconsistencies
in geographic names. Finally, in Section 4 we outline
the classification and storage system used for geographic
features.
2 Gazetteer entries in the GazDB
The most basic form of a gazetteer entry consists of a
mapping between a geographic name and a geographic
Figure 2: Relating features and names in the GazDB
location. The Alexandria Digital Library Project (Hill,
2000), however, defines a gazetteer entry as also requir-
ing a type designation to describe the entity referred to
by the name and location. Because a geographical type
designation classifies the physical entity rather than the
name assigned to it, we think of gazetteer entries pro-
duced by the GazDB as relating geographic names and
geographic features (which have inherent types). We will
separately discuss geographic names and geographic fea-
tures in greater detail later, and focus on the stored rela-
tions between them first.
A naive approach to creating a gazetteer is to main-
tain a flat file with one gazetteer entry per line, as follows:
Boston 42? 21?30?N, 71? 4?23?W
Cambridge 42? 23?30?N, 71? 6?22?W
Somerville 42? 23?15?N, 71? 6?00?W
This schema is overly simplistic because it supposes a
one-to-one mapping between geographic names and fea-
tures, when in reality many geographic features have
more than one name commonly associated with them.
For instance, the tallest mountain in North America is un-
ambiguously referred to as either Mount McKinley or De-
nali. Using this gazetteer, recording both names for the
mountain would result in the creation of two entries. This
is highly impractical on a large scale due to space require-
ments and the complexity of systematically updating or
modifying the gazetteer.
The GazDB uses the well-known relational ap-
proach (Codd, 1970) to store the geographic data for
the gazetteer. To do so, we separate the notion of
a geographic name from the geographic feature that it
represents. We maintain distinct tables for locations
and names? mappings between names and locations are
stored in a third table, keyed by the unique numerical
identifiers of both the name and the location, as shown
Figure 3: Updating a name in the GazDB
in Figure 2. This system enables the GazDB to support
both many-to-one relations between names and features,
as in the case of Denali and McKinley, and one-to-many
relations such as London being the name of both a city in
Britain and a town in Connecticut.
In the GazDB, several other relational tables are used
to store numerical data associated with the known geo-
graphic features. For example, population data is kept in
a separate table that links census figures with the ID?s of
entries in the feature table. This is useful because it facil-
itates queries to be performed only on inhabited places.
Elevation data is stored in a similar manner.
As gazetteers get updated, corrections are often made
to the name or to the feature data. To update a name, we
formally abandon the old ID, create a new name entry,
and update the name?feature mapping table by replac-
ing the old name ID with the new one, as in Figure 3.
We repeat this process for each table in the GazDB that
refers to the old ID? this is simple, because the tables are
indexed by ID. Updating geographic locations or numer-
ical data in the GazDB is done in an identical manner.
The GazDB also includes a table for storing de-
tailed information about the sources of the data in the
GazDB? for instance, ?NIMA GeoNet names datafile for
Afghanistan (AF), published November 8 2002?. Every
element in the GazDB is then associated with the ap-
propriate entry in the source table. This enables the ac-
countability of all entries in the GazDB, preventing the
appearance of ?mystery data?. The source table also al-
lows easy, systematic, source-specific modifications of
the GazDB?s entries to keep pace with frequently up-
dated datasets, thereby maintaining the freshness of the
GazDB?s data.
The GazDB also includes a complete log of all updates
to the database tables and entries. Because data rows are
abandoned but not deleted during updates, it is possible
to recreate the state of the database prior to any particular
set of updates.
The flexibility of the relational design also allows the
inclusion of new kinds of data that were not thought of
or not available in the original schema. For instance, one
could add yearly precipitation data for geographic loca-
tions by creating an additional table mapping locations to
rainfall amounts, without the need to re-ingest the data
already in the GazDB.
The GazDB also maintains a historical geographical
record by capturing temporal extents for mappings ? i.e.
the city at 59? 54?20?N, 30? 16?9?E would be associated
with the names:
? St. Petersburg from 1991-present day
? Leningrad from 1924-1991
? Petrograd from 1914-1924
The GazDB can thus export temporally-sensitive
gazetteers customized for use in historical documents.
3 Geographic names
Geographic names present a number of challenges to a
gazetteer. These include issues inherent to translation and
transliteration of foreign names, mediation between re-
peated entries and multiple sources, and the (in)accuracy
of placename specifications.
3.1 Resolution of names
The first hurdle is internationalization (i18n). Differences
between character encodings and display capabilities re-
sult in some names taking on a variety of forms (e.g.
printing Sa?o Tome? as Sao Tome). Although the printed
forms of the name are not character-identical, the name
itself has not changed from its original representation.
To resolve this, the GazDB defines and stores a geo-
graphic name as a triple: [canonical name, display name,
search name], with each element at a different level of
resolution. The canonical form of the feature?s name is
kept as a 16 bit string (Unicode / UTF-8), the display
form is 8 bits (ISO 8859-1), and the search name is 7-bit
uppercase ASCII. These resolutions are appropriate for
different purposes: wide characters are necessary for Chi-
nese/Japanese/Korean (CJK) content, the display name is
a necessary compromise given the default display capa-
bilities of Internet browsers, and the search name is nec-
essary given the data entry capabilities of the default (US-
ASCII) keyboard. We henceforth use the term name to
implicitly refer to this triple.
We also support Soundex and Metaphone geographic
name searches at a 7 bit resolution, by storing the hash
codes in separate tables within the GazDB.
However, there are cases when variances in a name
arise due to multiple transliteration, rather than character
encodings, as in the case of Macau and Macao. As such,
we further define a spelling of a geographic name to be
a similarly constructed triple of [UTF-8, 8859-1, ASCII]
encodings, with the added restriction that while the au-
thoritative name is directly associated to a geographical
entity, a spelling is only directly associated to a name.
Thus while Macao is a spelling variant of Macau, and
Macau is the name of a city in Southern China, nonethe-
less Macao is not considered to be a GazDB name proper
for the city.
3.2 Authoritativeness
The GazDB also makes a distinction about the authorita-
tiveness of names. We view a placename as an informa-
tion resource in and of itself, independent of the feature
that it names. This is analogous to the Unicode standard,
where the name of a character is treated as an information
resource independent of the glyph it corresponds to.
There are multiple names that refer to the same geo-
graphic feature but are neither spelling variants of another
nor are they seemingly derived from one another, such as
Holland vs. The Netherlands or Nihon vs. Japan. Be-
cause of this, we define and maintain alternate names for
each authoritative name. Each geographic entity is per-
mitted to have only one authoritative name, but that au-
thoritative name can have several more informal alternate
names associated to it. Both alternate names and author-
itative names can have variant spellings.
Conflicts between authoritative names from different
sources are inevitable. However, we cannot indepen-
dently determine the proper solution in an objective way
because we are not a mapping agency? we seek to use
geographic data, not produce it. Without being able to
take our own measurements, resolving these discrepan-
cies must therefore be done on the basis of the perceived
trustworthiness of the sources providing the data. The
GazDB?s source data consists of many sources that can be
trusted to varying degrees. We put the highest trust in the
Geographic Names Information System (USGS, 2003)
data and the GEOnet Names Server (NIMA, 2003) data,
and mediate the incorporation of all the other sources ac-
cordingly.
To enforce the distinction between the authoritative
and the alternate versions of a name,, and to emphasize
the authoritative name, we speak of ?names? referring
only to the authoritative name. For all others, we speak
of ?alternates? and ?spellings?.
3.3 Explicitness
Lastly, the GazDB distinguishes fully specified geo-
graphic names, such as New York City, New York, USA
from their short forms such as New York City or even the
more colloquial yet ambiguous New York.
The GazDB maintains a taxonomy of geographic fea-
tures, consisting of an administrative hierarchy of the
world. The administrative hierarchy serves to locate ge-
ographic entities by country, then state, county, and so
forth. This is based upon both the FIPS 10-4 citeFIPS
and the ISO 3166-2 (ISO, 1998) codes. However, these
standards often disagree and update infrequently, so we
base ours upon the Hierarchical Administrative Subdi-
vision Codes (HASC) system (Law, 1999). Using this
taxonomy, we can specify geographic entities by name
and by their location within the political divisions of the
world. The GazDB is capable of maintaining multiple
taxonomies for geographic entities, such as one based
upon physical features (for instance: ?Mont Blanc is a
mountain in the Alps which are in Europe?, in addition
to ?Mont Blanc is a mountain in France?), however these
have not yet been completed.
We define as an authoritative title the unambiguous
list of hierarchical administrative regions that contain the
geographic entity. Here New York State, United States
would be the authoritative title, such that the sequence
New York City, New York State, USA unambiguously
refers to a single geographic entity. The authoritative ti-
tle is the ordered sequence of the authoritative names for
the list of hierarchical regions that contain the feature, so
it is easy to compute from a hierarchical region tree in the
GazDB. Other titles can be computed by using variants or
spellings of the containing regions? names, or by omitting
some of them (New York City, USA, for example).
We have thus imposed an order on the GazDB geo-
graphic names: each feature can have one primary (most
authoritative) GazDB name and some alternate GazDB
names. Each GazDB name, both primary and alternate,
can have multiple spellings associated with it. All of the
above are available at all three encoding resolutions.
This ordering allows the GazDB to classify geographic
names along three orthogonal scales: general/vernacular
vs. authoritative; raw (original character encoding) vs.
cooked (character-set- and transliteration-normalized);
and implicit (short form) vs. explicit (long form). This
allows us to export, on an as-needed basis, multiple
gazetteers from the GazDB at different name resolutions.
3.4 Language information
The multilingual support in the GazDB goes beyond the
use of Unicode. To map different name entries to geo-
graphic features for different languages, we also maintain
within the GazDB a detailed list of the world?s languages
(Grimes and Grimes, 2000), and associate all names and
descriptions with their language.
The GazDB can keep one authoritative name (but ar-
bitrary numbers of associated spellings, variants, and ti-
tles) per language in the world for any geographic fea-
ture. Therefore, given authoritative sets of raw geo-
graphic data in a foreign language, the GazDB could pro-
duce a gazetteer in that language. By matching gazetteer
entries by feature, the GazDB could potentially issue a
multilingual gazetteer as well. Of course, obtaining the
large, accurate, geographic datasets in foreign languages
required for this purpose is a major ongoing undertaking?
one that we make no claim to have completed!
4 Geographic features
As mentioned in Section 2, a geographic feature includes
both a geographic location and some categorization of
what is situated there. The GazDB classifies geographic
entities along 3 orthogonal scales: spatial representation,
functional class, and administrative type. These classi-
fications allows users to better restrict gazetteer queries,
perhaps via pull-down menus, for more relevant results.
4.1 Spatial representations
Simple point/bounding-box categorization does not ac-
curately depict the topological footprint of most features
(Hill et al, 1999). Points do not represent the geographic
extents of locations, and bounding boxes misrepresent
features by oversimplifying the shape. Of particular in-
terest is the ability to categorize geographic entities with
?fuzzy boundaries?, such as the extent of wetlands, or
disjoint regions, such as an archipelago. The GazDB
classifies features by their footprint into 6 major types
(each with numerous subtypes):
1 point ? 0-dimensional (approximated to a point, e.g.
a factory gate or a well)
2 line ? 1-dimensional (e.g. a road or power line)
3 area ? 2-dimensional without clearly defined bound-
aries (e.g. wetlands)
4 point-area ? a 2-D region with clearly defined
boundaries (e.g. county or lake)
5 cluster of point-areas ? e.g. an archipelago
6 probability density distribution ? a feature that shifts
over time, e.g. ice packs
0 unknown/unclassified
4.2 Functional classes
Many features, particularly structures, can also be de-
scribed by their functional class:
1 building ? a man-made structure
2 campus ? a feature that contains a number of build-
ings on open space, such as a military base.
3 field ? a feature that predominantly open space with-
out structures, such as a cemetery.
4 city
0 unknown/unclassified
4.3 Administrative types
We also distinguish administrative types:
1 international organization ? encompasses multiple
countries
2 nation
3 province ? first-order administrative subdivision
within a nation
4 county ? first-order administrative subdivision
within a province
5 smaller than county ? anything below second-order
subdivision within a nation
0 unknown/unclassified
It is worth reiterating that these categorizations are de-
liberately broad and are used for filtering purposes only.
The GazDB maintains a complete hierarchical tree of all
the administrative subdivisions within a country and the
geographic entities contained therein, without any depth
limitations.
4.4 Using feature categorization
The particular categories and classifications are specified
for a number of reasons:
To facilitate Knowledge Representation within the
GazDB by axiomatizing how we classify data. We cur-
rently have no ontology for the geographic entities, but
we leave open the option to add one to our taxonomies.
To reduce the need for human training, such that an
average user of the gazetteer can have reasonable expec-
tations of what each category includes based on intuition.
User convenience: the categories in the appropriate
pull-down menu should be ones useful to a user.
To make querying more efficient: for example, we can
use axiomatic expectation to assume a polygonal feature
to only match other polygons.
4.5 Storing geographic locations
A major advantage that coordinate systems have over
naming systems is that, given an appropriate method, it is
possible to convert from one coordinate system to another
with reasonable accuracy. As such, the GazDB currently
only stores geocoordinates in decimal degrees (albeit in
two versions: one high-precision, and the other rounded
for display purposes). However, the conversion and ex-
port scripts are already prepared to handle a wide variety
of coordinate systems, such as Degrees-Minutes-Seconds
(DMS), Military Grid Reference System (MGRS), Uni-
versal Transverse Mercator (UTM) coordinates, to name
a few.
The GazDB scripts can also convert between map pro-
jections, but so far it is only done to convert source data
into the GazDB standard format.
5 Conclusions
Maintaining a large-scale gazetteer database is a non-
trivial task. Nonetheless, we have created a gazetteer
database containing tens of millions of entries collected
from several large gazetteers (each with their own for-
mat, encoding, classification, and field conventions), and
providing output in several highly compressed binary for-
mats. We believe that the problems we have encountered
in designing and building the GazDB are not unique to
us, but rather, they are inherent to the task. We therefore
hope that others can use the solutions proposed here to
some advantage.
Acknowledgements
We would like to thank Dr. Andra?s Kornai for invaluable
ideas and support, Dr. Michael Bukatin for technical
assistance and caffeine, the anonymous reviewers for
providing useful comments, and lastly, Kenneth Baker
and Keith Baker for their roles in the development of this
project. Thank you.
References
Edgar Frank Codd. 1970. A relational model of data
for large shared data banks. Communications of the
ACM. 13(6):377?387.
US Department of Commerce, National Institute of Stan-
dards and Technology (NIST). 1995. FIPS PUB 10-4:
Countries, dependencies, areas of special sovereignty,
and their principal administrative divisions.
http://www.nima.mil/gns/html/
fips10-4.html
Barbara F. Grimes and Joseph E. Grimes. 2000. Ethno-
logue. Volume 1: languages of the world. SIL Interna-
tional.
Linda L. Hill, James Frew, and Qi Zheng. 1999. Geo-
graphic Names: The implementation of a gazetteer in
a georeferenced digital library. D-Lib Magazine. 5(1).
Linda L. Hill. 2000. Core elements of digital gazetteers:
placenames, categories, and footprints. In J. Borbinha
& T. Baker (Eds.), Research and Advanced Technol-
ogy for Digital Libraries : Proceedings of the 4th Euro-
pean Conference, ECDL 2000 Lisbon, Portugal. (pp.
280-290)
International Organization for Standardization. 1998.
ISO/IEC 3166 ISO ISO 3166-2:1998 Codes for the
representation of names of countries and their sub-
divisions ? Part 2: Country subdivision code. Pub-
lished by International Organization for Standardiza-
tion, Geneva.
Gwillim Law. 1999. Administrative subdivisions of
countries. McFarland & Company, Inc.
http://www.mindspring.com/?gwil/
statoids.html
National Imagery and Mapping Agency.
2003. GEOnet Names Server (GNS).
http://www.nima.mil/gns/html/index.html
United States Geological Survey.
2003. Geographic Names Information System (GNIS).
http://geonames.usgs.gov/
Erik Rauch, Michael Bukatin, and Kenneth Baker. 2003.
A confidence-based framework for disambiguating ge-
ographic terms. Published in this volume.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355?362,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Domain Adaptation via Pseudo In-Domain Data Selection
Amittai Axelrod
University of Washington
Seattle, WA 98105
amittai@uw.edu
Xiaodong He
Microsoft Research
Redmond, WA 98052
xiaohe@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA 98052
jfgao@microsoft.com
Abstract
We explore efficient domain adaptation for the
task of statistical machine translation based
on extracting sentences from a large general-
domain parallel corpus that are most relevant
to the target domain. These sentences may
be selected with simple cross-entropy based
methods, of which we present three. As
these sentences are not themselves identical
to the in-domain data, we call them pseudo
in-domain subcorpora. These subcorpora ?
1% the size of the original ? can then used
to train small domain-adapted Statistical Ma-
chine Translation (SMT) systems which out-
perform systems trained on the entire corpus.
Performance is further improved when we use
these domain-adapted models in combination
with a true in-domain model. The results
show that more training data is not always
better, and that best results are attained via
proper domain-relevant data selection, as well
as combining in- and general-domain systems
during decoding.
1 Introduction
Statistical Machine Translation (SMT) system per-
formance is dependent on the quantity and quality
of available training data. The conventional wisdom
is that more data is better; the larger the training cor-
pus, the more accurate the model can be.
The trouble is that ? except for the few all-purpose
SMT systems ? there is never enough training data
that is directly relevant to the translation task at
hand. Even if there is no formal genre for the text
to be translated, any coherent translation task will
have its own argot, vocabulary or stylistic prefer-
ences, such that the corpus characteristics will nec-
essarily deviate from any all-encompassing model of
language. For this reason, one would prefer to use
more in-domain data for training. This would em-
pirically provide more accurate lexical probabilities,
and thus better target the task at hand. However, par-
allel in-domain data is usually hard to find1, and so
performance is assumed to be limited by the quan-
tity of domain-specific training data used to build the
model. Additional parallel data can be readily ac-
quired, but at the cost of specificity: either the data
is entirely unrelated to the task at hand, or the data is
from a broad enough pool of topics and styles, such
as the web, that any use this corpus may provide is
due to its size, and not its relevance.
The task of domain adaptation is to translate a text
in a particular (target) domain for which only a small
amount of training data is available, using an MT
system trained on a larger set of data that is not re-
stricted to the target domain. We call this larger set
of data a general-domain corpus, in lieu of the stan-
dard yet slightly misleading out-of-domain corpus,
to allow a large uncurated corpus to include some
text that may be relevant to the target domain.
Many existing domain adaptation methods fall
into two broad categories. Adaptation can be done at
the corpus level, by selecting, joining, or weighting
the datasets upon which the models (and by exten-
sion, systems) are trained. It can be also achieved at
the model level by combining multiple translation or
language models together, often in a weighted man-
ner. We explore both categories in this work.
1Unless one dreams of translating parliamentary speeches.
355
First, we present three methods for ranking the
sentences in a general-domain corpus with respect to
an in-domain corpus. A cutoff can then be applied to
produce a very small?yet useful? subcorpus, which
in turn can be used to train a domain-adapted MT
system. The first two data selection methods are ap-
plications of language-modeling techniques to MT
(one for the first time). The third method is novel
and explicitly takes into account the bilingual na-
ture of the MT training corpus. We show that it is
possible to use our data selection methods to subse-
lect less than 1% (or discard 99%) of a large general
training corpus and still increase translation perfor-
mance by nearly 2 BLEU points.
We then explore how best to use these selected
subcorpora. We test their combination with the in-
domain set, followed by examining the subcorpora
to see whether they are actually in-domain, out-of-
domain, or something in between. Based on this, we
compare translation model combination methods.
Finally, we show that these tiny translation mod-
els for model combination can improve system per-
formance even further over the current standard way
of producing a domain-adapted MT system. The re-
sulting process is lightweight, simple, and effective.
2 Related Work
2.1 Training Data Selection
An underlying assumption in domain adaptation is
that a general-domain corpus, if sufficiently broad,
likely includes some sentences that could fall within
the target domain and thus should be used for train-
ing. Equally, the general-domain corpus likely in-
cludes sentences that are so unlike the domain of the
task that using them to train the model is probably
more harmful than beneficial. One mechanism for
domain adaptation is thus to select only a portion of
the general-domain corpus, and use only that subset
to train a complete system.
The simplest instance of this problem can be
found in the realm of language modeling, using
perplexity-based selection methods. The sentences
in the general-domain corpus are scored by their per-
plexity score according to an in-domain language
model, and then sorted, with only the lowest ones
being retained. This has been done for language
modeling, including by Gao et al(2002), and more
recently by Moore and Lewis (2010). The ranking
of the sentences in a general-domain corpus accord-
ing to in-domain perplexity has also been applied to
machine translation by both Yasuda et al(2008), and
Foster et al(2010). We test this approach, with the
difference that we simply use the source side per-
plexity rather than computing the geometric mean
of the perplexities over both sides of the corpus. We
also reduce the size of the training corpus far more
aggressively than Yasuda et als 50%. Foster et al
(2010) do not mention what percentage of the cor-
pus they select for their IR-baseline, but they con-
catenate the data to their in-domain corpus and re-
port a decrease in performance. We both keep the
models separate and reduce their size.
A more general method is that of (Matsoukas et
al., 2009), who assign a (possibly-zero) weight to
each sentence in the large corpus and modify the em-
pirical phrase counts accordingly. Foster et al(2010)
further perform this on extracted phrase pairs, not
just sentences. While this soft decision is more flex-
ible than the binary decision that comes from includ-
ing or discarding a sentence from the subcorpus, it
does not reduce the size of the model and comes
at the cost of computational complexity as well as
the possibility of overfitting. Additionally, the most
effective features of (Matsoukas et al, 2009) were
found to be meta-information about the source doc-
uments, which may not be available.
Another perplexity-based approach is that taken
by Moore and Lewis (2010), where they use the
cross-entropy difference as a ranking function rather
than just cross-entropy. We apply this criterion for
the first time to the task of selecting training data
for machine translation systems. We furthermore ex-
tend this idea for MT-specific purposes.
2.2 Translation Model Combination
In addition to improving the performance of a sin-
gle general model with respect to a target domain,
there is significant interest in using two translation
models, one trained on a larger general-domain cor-
pus and the other on a smaller in-domain corpus, to
translate in-domain text. After all, if one has ac-
cess to an in-domain corpus with which to select
data from a general-domain corpus, then one might
as well use the in-domain data, too. The expectation
is that the larger general-domain model should dom-
356
inate in regions where the smaller in-domain model
lacks coverage due to sparse (or non-existent) ngram
counts. In practice, most practical systems also per-
form target-side language model adaptation (Eck et
al., 2004); we eschew this in order to isolate the ef-
fects of translation model adaptation alone.
Directly concatenating the phrase tables into one
larger one isn?t strongly motivated; identical phrase
pairs within the resulting table can lead to unpre-
dictable behavior during decoding. Nakov (2008)
handled identical phrase pairs by prioritizing the
source tables, however in our experience identical
entries in phrase tables are not very common when
comparing across domains. Foster and Kuhn (2007)
interpolated the in- and general-domain phrase ta-
bles together, assigning either linear or log-linear
weights to the entries in the tables before combining
overlapping entries; this is now standard practice.
Lastly, Koehn and Schroeder (2007) reported
improvements from using multiple decoding paths
(Birch et al, 2007) to pass both tables to the Moses
SMT decoder (Koehn et al, 2003), instead of di-
rectly combining the phrase tables to perform do-
main adaptation. In this work, we directly com-
pare the approaches of (Foster and Kuhn, 2007) and
(Koehn and Schroeder, 2007) on the systems gener-
ated from the methods mentioned in Section 2.1.
3 Experimental Framework
3.1 Corpora
We conducted our experiments on the Interna-
tional Workshop on Spoken Language Translation
(IWSLT) Chinese-to-English DIALOG task 2, con-
sisting of transcriptions of conversational speech in
a travel setting. Two corpora are needed for the
adaptation task. Our in-domain data consisted of the
IWSLT corpus of approximately 30,000 sentences
in Chinese and English. Our general-domain cor-
pus was 12 million parallel sentences comprising a
variety of publicly available datasets, web data, and
private translation texts. Both the in- and general-
domain corpora were identically segmented (in Chi-
nese) and tokenized (in English), but otherwise un-
processed. We evaluated our work on the 2008
IWSLT spontaneous speech Challenge Task3 test
2http://iwslt2010.fbk.eu/node/33
3Correct-Recognition Result (CRR) condition
set, consisting of 504 Chinese sentences with 7 En-
glish reference translations each. This is the most
recent IWSLT test set for which the reference trans-
lations are available.
3.2 System Description
In order to highlight the data selection work, we
used an out-of-the-box Moses framework using
GIZA++ (Och and Ney, 2003) and MERT (Och,
2003) to train and tune the machine translation sys-
tems. The only exception was the phrase table
for the large out-of-domain system trained on 12m
sentence pairs, which we trained on a cluster us-
ing a word-dependent HMM-based alignment (He,
2007). We used the Moses decoder to produce all
the system outputs, and scored them with the NIST
mt-eval31a 4 tool used in the IWSLT evalutation.
3.3 Language Models
Our work depends on the use of language models to
rank sentences in the training corpus, in addition to
their normal use during machine translation tuning
and decoding. We used the SRI Language Model-
ing Toolkit (Stolcke, 2002) was used for LM train-
ing in all cases: corpus selection, MT tuning, and
decoding. We constructed 4gram language mod-
els with interpolated modified Kneser-Ney discount-
ing (Chen and Goodman, 1998), and set the Good-
Turing threshold to 1 for trigrams.
3.4 Baseline System
The in-domain baseline consisted of a translation
system trained using Moses, as described above, on
the IWSLT corpus. The resulting model had a phrase
table with 515k entries. The general-domain base-
line was substantially larger, having been trained on
12 million sentence pairs, and had a phrase table
containing 1.5 billion entries. The BLEU scores of
the baseline single-corpus systems are in Table 1.
Corpus Phrases Dev Test
IWSLT 515k 45.43 37.17
General 1,478m 42.62 40.51
Table 1: Baseline translation results for in-domain and
general-domain systems.
4http://www.itl.nist.gov/iad/mig/tools/
357
4 Training Data Selection Methods
We present three techniques for ranking and select-
ing subsets of a general-domain corpus, with an eye
towards improving overall translation performance.
4.1 Data Selection using Cross-Entropy
As mentioned in Section 2.1, one established
method is to rank the sentences in the general-
domain corpus by their perplexity score accord-
ing to a language model trained on the small in-
domain corpus. This reduces the perplexity of the
general-domain corpus, with the expectation that
only sentences similar to the in-domain corpus will
remain. We apply the method to machine trans-
lation, even though perplexity reduction has been
shown to not correlate with translation performance
(Axelrod, 2006). For this work we follow the proce-
dure of Moore and Lewis (2010), which applies the
cosmetic change of using the cross-entropy rather
than perplexity.
The perplexity of some string s with empirical n-
gram distribution p given a language model q is:
2?
?
x p(x) log q(x) = 2H(p,q) (1)
where H(p, q) is the cross-entropy between p and
q. We simplify this notation to just HI(s), mean-
ing the cross-entropy of string s according to a lan-
guage model LMI which has distribution q. Se-
lecting the sentences with the lowest perplexity is
therefore equivalent to choosing the sentences with
the lowest cross-entropy according to the in-domain
language model. For this experiment, we used a lan-
guage model trained (using the parameters in Sec-
tion 3.3) on the Chinese side of the IWSLT corpus.
4.2 Data Selection using Cross-Entropy
Difference
Moore and Lewis (2010) also start with a language
model LMI over the in-domain corpus, but then fur-
ther construct a language modelLMO of similar size
over the general-domain corpus. They then rank the
general-domain corpus sentences using:
HI(s)?HO(s) (2)
and again taking the lowest-scoring sentences. This
criterion biases towards sentences that are both like
the in-domain corpus and unlike the average of the
general-domain corpus. For this experiment we re-
used the in-domain LM from the previous method,
and trained a second LM on a random subset of
35k sentences from the Chinese side of the general
corpus, except using the same vocabulary as the in-
domain LM.
4.3 Data Selection using Bilingual
Cross-Entropy Difference
In addition to using these two monolingual criteria
for MT data selection, we propose a new method
that takes in to account the bilingual nature of the
problem. To this end, we sum cross-entropy differ-
ence over each side of the corpus, both source and
target:
[HI?src(s)?HO?src(s)]+[HI?tgt(s)?HO?tgt(s)]
(3)
Again, lower scores are presumed to be better. This
approach reuses the source-side language models
from Section 4.2, but requires similarly-trained ones
over the English side. Again, the vocabulary of the
language model trained on a subset of the general-
domain corpus was restricted to only cover those
tokens found in the in-domain corpus, following
Moore and Lewis (2010).
5 Results of Training Data Selection
The baseline results show that a translation system
trained on the general-domain corpus outperforms a
system trained on the in-domain corpus by over 3
BLEU points. However, this can be improved fur-
ther. We used the three methods from Section 4 to
identify the best-scoring sentences in the general-
domain corpus.
We consider three methods for extracting domain-
targeted parallel data from a general corpus: source-
side cross-entropy (Cross-Ent), source-side cross-
entropy difference (Moore-Lewis) from (Moore and
Lewis, 2010), and bilingual cross-entropy difference
(bML), which is novel.
Regardless of method, the overall procedure is
the same. Using the scoring method, We rank the
individual sentences of the general-domain corpus,
select only the top N . We used the top N =
{35k, 70k, 150k} sentence pairs out of the 12 mil-
358
lion in the general corpus 5. The net effect is that of
domain adaptation via threshhold filtering. New MT
systems were then trained solely on these small sub-
corpora, and compared against the baseline model
trained on the entire 12m-sentence general-domain
corpus. Table 2 contains BLEU scores of the sys-
tems trained on subsets of the general corpus.
Method Sentences Dev Test
General 12m 42.62 40.51
Cross-Entropy 35k 39.77 40.66
Cross-Entropy 70k 40.61 42.19
Cross-Entropy 150k 42.73 41.65
Moore-Lewis 35k 36.86 40.08
Moore-Lewis 70k 40.33 39.07
Moore-Lewis 150k 41.40 40.17
bilingual M-L 35k 39.59 42.31
bilingual M-L 70k 40.84 42.29
bilingual M-L 150k 42.64 42.22
Table 2: Translation results using only a subset of the
general-domain corpus.
All three methods presented for selecting a sub-
set of the general-domain corpus (Cross-Entropy,
Moore-Lewis, bilingual Moore-Lewis) could be
used to train a state-of-the-art machine transla-
tion system. The simplest method, using only the
source-side cross-entropy, was able to outperform
the general-domain model when selecting 150k out
of 12 million sentences. The other monolingual
method, source-side cross-entropy difference, was
able to perform nearly as well as the general-
domain model with only 35k sentences. The bilin-
gual Moore-Lewis method proposed in this paper
works best, consistently boosting performance by
1.8 BLEU while using less than 1% of the available
training data.
5.1 Pseudo In-Domain Data
The results in Table 2 show that all three meth-
ods (Cross-Entropy, Moore-Lewis, bilingual Moore-
Lewis) can extract subsets of the general-domain
corpus that are useful for the purposes of statistical
machine translation. It is tempting to describe these
as methods for finding in-domain data hidden in a
5Roughly 1x, 2x, and 4x the size of the in-domain corpus.
general-domain corpus. Alas, this does not seem to
be the case.
We trained a baseline language model on the in-
domain data and used it to compute the perplexity
of the same (in-domain) held-out dev set used to
tune the translation models. We extracted the top
N sentences using each ranking method, varying N
from 10k to 200k, and then trained language models
on these subcorpora. These were then used to also
compute the perplexity of the same held-out dev set,
shown below in Figure 1.
020406080100120140
'0
20
25
30
35
40
50
70
100
125
150
175
Top-r
anked
 
gener
al-dom
ain se
ntenc
es (in k
)
Devset Perplexity
In-dom
ain ba
seline
Cross
-
Entrop
y
Moore
-
Lewis
bilingu
al M-L
Figure 1: Corpus Selection Results
The perplexity of the dev set according to LMs
trained on the top-ranked sentences varied from 77
to 120, depending on the size of the subset and the
method used. The Cross-Entropy method was con-
sistently worse than the others, with a best perplex-
ity of 99.4 on 20k sentences, and bilingual Moore-
Lewis was consistently the best, with a lowest per-
plexity of 76.8. And yet, none of these scores are
anywhere near the perplexity of 36.96 according to
the LM trained only on in-domain data.
From this it can be deduced that the selection
methods are not finding data that is strictly in-
domain. Rather they are extracting pseudo in-
domain data which is relevant, but with a differing
distribution than the original in-domain corpus.
As further evidence, consider the results of con-
catenating the in-domain corpus with the best ex-
tracted subcorpora (using the bilingual Moore-
Lewis method), shown in Table 3. The change in
359
both the dev and test scores appears to reflect dissim-
ilarity in the underlying data. Were the two datasets
more alike, one would expect the models to rein-
force each other rather than cancel out.
Method Sentences Dev Test
IWSLT 30k 45.43 37.17
bilingual M-L 35k 39.59 42.31
bilingual M-L 70k 40.84 42.29
bilingual M-L 150k 42.64 42.22
IWSLT + bi M-L 35k 47.71 41.78
IWSLT + bi M-L 70k 47.80 42.30
IWSLT + bi M-L 150k 48.44 42.01
Table 3: Translation results concatenating the in-domain
and pseudo in-domain data to train a single model.
6 Translation Model Combination
Because the pseudo in-domain data should be kept
separate from the in-domain data, one must train
multiple translation models in order to advanta-
geously use the general-domain corpus. We now ex-
amine how best to combine these models.
6.1 Linear Interpolation
A common approach to managing multiple transla-
tion models is to interpolate them, as in (Foster and
Kuhn, 2007) and (Lu? et al, 2007). We tested the
linear interpolation of the in- and general-domain
translation models as follows: Given one model
which assigns the probability P1(t|s) to the trans-
lation of source string s into target string t, and a
second model which assigns the probability P2(t|s)
to the same event, then the interpolated translation
probability is:
P (t|s) = ?P1(t|s) + (1? ?)P2(t|s) (4)
Here ? is a tunable weight between 0 and 1, which
we tested in increments of 0.1. Linear interpolation
of phrase tables was shown to improve performance
over the individual models, but this still may not be
the most effective use of the translation models.
6.2 Multiple Models
We next tested the approach in (Koehn and
Schroeder, 2007), passing the two phrase tables di-
rectly to the decoder and tuning a system using both
phrase tables in parallel. Each phrase table receives
a separate set of weights during tuning, thus this
combined translation model has more parameters
than a normal single-table system.
Unlike (Nakov, 2008), we explicitly did not at-
tempt to resolve any overlap between the phrase ta-
bles, as there is no need to do so with the multiple
decoding paths. Any phrase pairs appearing in both
models will be treated separately by the decoder.
However, the exact overlap between the phrase ta-
bles was tiny, minimizing this effect.
6.3 Translation Model Combination Results
Table 4 shows baseline results for the in-domain
translation system and the general-domain system,
evaluated on the in-domain data. The table also
shows that linearly interpolating the translation
models improved the overall BLEU score, as ex-
pected. However, using multiple decoding paths,
and no explicit model merging at all, produced even
better results, by 2 BLEU points over the best indi-
vidual model and 1.3 BLEU over the best interpo-
lated model, which used ? = 0.9.
System Dev Test
IWSLT 45.43 37.17
General 42.62 40.51
Interpolate IWSLT, General 48.46 41.28
Use both IWSLT, General 49.13 42.50
Table 4: Translation model combination results
We conclude that it can be more effective to not
attempt translation model adaptation directly, and
instead let the decoder do the work.
7 Combining Multi-Model and Data
Selection Approaches
We presented in Section 5 several methods to im-
prove the performance of a single general-domain
translation system by restricting its training corpus
on an information-theoretic basis to a very small
number of sentences. However, Section 6.3 shows
that using two translation models over all the avail-
able data (one in-domain, one general-domain) out-
performs any single individual translation model so
far, albeit only slightly.
360
Method Dev Test
IWSLT 45.43 37.17
General 42.62 40.51
both IWSLT, General 49.13 42.50
IWSLT, Moore-Lewis 35k 48.51 40.38
IWSLT, Moore-Lewis 70k 49.65 40.45
IWSLT, Moore-Lewis 150k 49.50 41.40
IWSLT, bi M-L 35k 48.85 39.82
IWSLT, bi M-L 70k 49.10 43.00
IWSLT, bi M-L 150k 49.80 43.23
Table 5: Translation results from using in-domain and
pseudo in-domain translation models together.
It is well and good to use the in-domain data
to select pseudo in-domain data from the general-
domain corpus, but given that this requires access
to an in-domain corpus, one might as well use it.
As such, we used the in-domain translation model
alongside translation models trained on the subcor-
pora selected using the Moore-Lewis and bilingual
Moore-Lewis methods in Section 4. The results are
in Table 5.
A translation system trained on a pseudo in-
domain subset of the general corpus, selected with
the bilingual Moore-Lewis method, can be further
improved by combining with an in-domain model.
Furthermore, this system combination works better
than the conventional multi-model approach by up
to 0.7 BLEU on both the dev and test sets.
Thus a domain-adapted system comprising two
phrase tables trained on a total of 180k sen-
tences outperformed the standard multi-model sys-
tem which was trained on 12 million sentences. This
tiny combined system was also 3+ points better than
the general-domain system by itself, and 6+ points
better than the in-domain system alone.
8 Conclusions
Sentence pairs from a general-domain corpus that
seem similar to an in-domain corpus may not actu-
ally represent the same distribution of language, as
measured by language model perplexity. Nonethe-
less, we have shown that relatively tiny amounts of
this pseudo in-domain data can prove more useful
than the entire general-domain corpus for the pur-
poses of domain-targeted translation tasks.
This paper has also explored three simple yet
effective methods for extracting these pseudo in-
domain sentences from a general-domain corpus. A
translation model trained on any of these subcorpora
can be comparable ? or substantially better ? than a
translation system trained on the entire corpus.
In particular, the new bilingual Moore-Lewis
method, which is specifically tailored to the ma-
chine translation scenario, is shown to be more ef-
ficient and stable for MT domain adaptation. Trans-
lation models trained on data selected in this way
consistently outperformed the general-domain base-
line while using as few as 35k out of 12 million sen-
tences. This fast and simple technique for discarding
over 99% of the general-domain training corpus re-
sulted in an increase of 1.8 BLEU points.
We have also shown in passing that the linear in-
terpolation of translation models may work less well
for translation model adaptation than the multiple
paths decoding technique of (Birch et al, 2007).
These approaches of data selection and model com-
bination can be stacked, resulting in a compact, two
phrase-table, translation system trained on 1% of the
available data that again outperforms a state-of-the-
art translation system trained on all the data.
Besides improving translation performance, this
work also provides a way to mine very large corpora
in a computationally-limited environment, such as
on an ordinary computer or perhaps a mobile device.
The maximum size of a useful general-domain cor-
pus is now limited only by the availability of data,
rather than by how large a translation model can be
fit into memory at once.
References
Amittai Axelrod. 2006. Factored Language Models for
Statistical Machine Translation. M.Sc. Thesis. Univer-
sity of Edinburgh, Scotland.
Alexandra Birch, Miles Osborne and Philipp Koehn.
2007. CCG Supertags in Factored Translation Models.
Workshop on Statistical Machine Translation, Associ-
ation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report 10-98, Computer Science
Group, Harvard University.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
361
Translation based on Information Retrieval. Language
Resources and Evaluation.
George Foster and Roland Kuhn. 2007. Mixture-Model
Adaptation for SMT. Workshop on Statistical Machine
Translation, Association for Computational Linguis-
tics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative Instatnce Weighting for Domain Adap-
tation in Statistical Machine Translation. Empirical
Methods in Natural Language Processing.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu
Lee. 2002. Toward a Unified Approach to Statistical
Language Modeling for Chinese. ACM Transactions
on Asian Language Information Processing.
Xiaodong He. 2007. Using Word-Dependent Transition
Models in HMM-based Word Alignment for Statisti-
cal Machine Translation. Workshop on Statistical Ma-
chine Translation, Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2003. Moses: Open Source
Toolkit for Statistical Machine Translation. Demo Ses-
sion, Association for Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. Workshop on Statistical Machine Translation,
Association for Computational Linguistics.
Yajuan Lu?, Jin Huang and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning.
Spyros Matsoukas, Antti-Veikko Rosti, Bing Zhang.
2009. Discriminative Corpus Weight Estimation for
Machine Translation. Empirical Methods in Natural
Language Processing.
Robert Moore and William Lewis. 2010. Intelligent Se-
lection of Language Model Training Data. Association
for Computational Linguistics.
Preslav Nakov. 2008. Improving English-Spanish Sta-
tistical Machine Translation: Experiments in Domain
Adaptation, Sentence Paraphrasing, Tokenization, and
Recasing. Workshop on Statistical Machine Transla-
tion, Association for Computational Linguistics.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. Association for Compu-
tational Linguistics
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. Spoken Language Process-
ing.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, Ei-
ichiro Sumita. 2008. Method of Selecting Train-
ing Data to Build a Compact and Efficient Transla-
tion Model. International Joint Conference on Natural
Language Processing.
362
