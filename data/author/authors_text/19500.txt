Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 79?88, Dublin, Ireland, August 23-29 2014.
Joint Inference and Disambiguation of Implicit Sentiments via
Implicature Constraints
Lingjia Deng
1
, Janyce Wiebe
1,2
, Yoonjung Choi
2
1
Intelligent Systems Program, University of Pittsburgh
2
Department of Computer Science, University of Pittsburgh
lid29@pitt.edu, wiebe@cs.pitt.edu, yjchoi@cs.pitt.edu
Abstract
This paper addresses implicit opinions expressed via inference over explicit sentiments and
events that positively/negatively affect entities (goodFor/badFor, gfbf events). We incorporate
the inferences developed by implicature rules into an optimization framework, to jointly improve
sentiment detection toward entities and disambiguate components of gfbf events. The framework
simultaneously beats the baselines by more than 10 points in F-measure on sentiment detection
and more than 7 points in accuracy on gfbf polarity disambiguation.
1 Introduction
Previous work in NLP on sentiment analysis has mainly focused on explicit sentiments. However, as
noted in (Deng and Wiebe, 2014), many opinions are expressed implicitly, as shown by this example:
Ex(1) The reform would lower health care costs, which would be a tremendous positive change across the entire
health-care system.
There is an explicit positive sentiment toward the event of ?reform lower costs?. However, in expressing
this sentiment, the writer also implies he is negative toward the ?costs?, since he?s happy to see the costs
being decreased. Moreover, the writer may be positive toward ?reform? since it contributes to the ?lower?
event. Such inferences may be seen as opinion-oriented implicatures (i.e., defeasible inferences)
1
.
We develop a set of rules for inferring and detecting implicit sentiments from explicit sentiments and
events such as ?lower? (Wiebe and Deng, 2014). In (Deng et al., 2013), we investigate such events,
defining a badFor (bf) event to be an event that negatively affects the theme and a goodFor (gf) event to
be an event that positively affects the theme of the event.
2
Here, ?lower? is a bf event. According to their
annotation scheme, goodFor/badFor (gfbf) events have NP agents and themes (though the agent may be
implicit), and the polarity of a gf event may be changed to bf by a reverser (and vice versa).
The ultimate goal of this work is to utilize gfbf information to improve detection of the writer?s senti-
ments toward entities mentioned in the text. However, this requires resolving several ambiguities: (Q1)
Given a document, which spans are gfbf events? (Q2) Given a gfbf text span, what is its polarity, gf
or bf? (Q3) Is the polarity of a gfbf event being reversed? (Q4) Which NP in the sentence is the agent
and which is the theme? (Q5) What are the writer?s sentiments toward the agent and theme, positive
or negative? Fortunately, the implicature rules in (Deng and Wiebe, 2014) define dependencies among
these ambiguities. As in Ex(1), the sentiments toward the agent and theme, the sentiment toward the gfbf
event (positive or negative), and the polarity of the gfbf event (gf or bf) are all interdependent. Thus,
rather than having to take a pipeline approach, we are able to develop an optimization framework which
exploits these interdependencies to jointly resolve the ambiguities.
Specifically, we develop local detectors to analyze the four individual components of gfbf events,
(Q2)-(Q5) above. Then, we propose an Integer Linear Programming (ILP) framework to conduct global
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Specifically, we focus on generalized conversational implicature (Grice, 1967; Grice, 1989).
2
Compared to (Deng et al., 2013), we change the term ?object? to ?theme? as the later is more appropriate for this task.
79
inference, where the gfbf events and their components are variables and the interdependencies defined by
the implicature rules are encoded as constraints over relevant variables in the framework. The reason we
do not address (Q1) is that the gold standard we use for evaluation contains sentiment annotations only
toward the agents and themes of gfbf events. We are only able to evaluate true hits of gfbf events. Thus,
the input to the system is the set of the text spans marked as gfbf events in the corpus. The results show
that, compared to the local detectors, the ILP framework improves sentiment detection by more than 10
points in F-measure and disambiguating gfbf polarity by more than 7 points in the accuracy, without any
loss in accuracy for other two components.
2 Related Work
Most work in sentiment analysis focuses on classifying explicit sentiments and extracting explicit opinion
expressions, holders and targets (Wiebe et al., 2005; Johansson and Moschitti, 2013; Yang and Cardie,
2013). There is some work investigating features that directly indicate implicit sentiments (Zhang and
Liu, 2011; Feng et al., 2013). In contrast, we focus on how we can bridge between explicit and implicit
sentiments via inference. To infer the implicit sentiments related to gfbf events, some work mines various
syntactic patterns (Choi and Cardie, 2008), proposes linguistic templates (Zhang and Liu, 2011; Anand
and Reschke, 2010; Reschke and Anand, 2011), or generates a lexicon of patient polarity verbs (Goyal
et al., 2013). Different from their work, which do not cover all cases relevant to gfbf events, (Deng and
Wiebe, 2014) defines a generalized set of implicature rules and proposes a graph-based model to achieve
sentiment propagation between the agents and themes of gfbf events. However, that system requires
all of the gfbf information (Q1)-(Q4) to be input from the manual annotations; the only ambiguity it
resolves is sentiments toward entities. In contrast, the method in this paper tackles four ambiguities
simultaneously. Further, as we will see below in Section 6, the improvement over the local detectors by
the current method is greater than that by the previous method, even though it operates over the noisy
output of local components automatically.
Different from pipeline architectures, where each step is computed independently, joint inference has
often achieved better results. Roth and Yih (2004) formulate the task of information extraction using
Integer Linear Programming (ILP). Since then, ILP has been widely used in various tasks in NLP, in-
cluding semantic role labeling (Punyakanok et al., 2004; Punyakanok et al., 2008; Das et al., 2012),
joint extraction of opinion entities and relations (Choi et al., 2006; Yang and Cardie, 2013), co-reference
resolution (Denis and Baldridge, 2007), and summarization (Martins and Smith, 2009). The most similar
ILP model to ours is (Somasundaran and Wiebe, 2009), which improves opinion polarity classification
using discourse constraints in an ILP model. However, their work addresses discourse relations among
explicit opinions in different sentences.
3 GoodFor/BadFor Event and Implicature
This work addresses sentiments toward, in general, states and events which positively or negatively
affect entities. Deng et al. (2013) (hereafter DCW) identify a clear case that occurs frequently in opinion
sentences, namely the gfbf events mentioned above. As defined in DCW, a gf event is an event that
positively affects the theme of the event and a bf event is an event that negatively affects the theme.
According to the annotation schema, gfbf events have NP agents and themes (though the agent may be
implicit). In the sentence ?President Obama passed the bill?, the agent of the gf ?passed? is ?President
Obama? and the theme is ?the bill?. In the sentence ?The bill was denied?, the agent of the bf ?was
denied? is implicit. The polarity of a gf event may be changed to bf by a reverser (and vice versa). For
example, in ?The reform will not worsen the economy,? ?not? is a reverser and it reverses the polarity
from bf to gf.
3
The constraints we encode in the ILP framework described below are based on implicature rules in
(Deng and Wiebe, 2014). Table 1 gives two rule schemas, each of which defines four specific rules. In
3
DCW also introduce retainers. We don?t analyze retainers in this work since they do not affect the polarity of gfbfs, and
only 2.5% of gfbfs have retainers in the corpus.
80
s(gfbf) gfbf ? s(agent) s(theme) s(gfbf) gfbf ? s(agent) s(theme)
1 positive gf ? positive positive 3 positive bf ? positive negative
2 negative gf ? negative negative 4 negative bf ? negative positive
Table 1: Rule Schema 1 & Rule Schema 3 (Deng and Wiebe, 2014)
the table, s(?) = ? means that the writer?s sentiment toward ? is ?, where ? is a gfbf event, or the agent
or theme of a gfbf event, and ? is either positive or negative. P? Q means to infer Q from P.
Applying the rules to Ex(1): the writer expresses a positive sentiment (?positive?) toward a bf event
(?lower?), thus matching Case 3 in Table 1. We infer that the writer is positive toward the agent (?re-
form?) and negative toward the theme (?costs?). Two other rule schemas (not shown) make the same
inferences as Rule Schemas 1 and 3 but in the opposite direction. As we can see, if two entities partic-
ipate in a gf event, the writer has the same sentiment toward the agent and theme, while if two entities
participate in a bf event, the writer has opposite sentiments toward them. Later we use this observation
in our experiments.
4 Global Optimization Framework
Optimization is performed over two sets of variables. The first set is GFBF, containing a variable for
each gfbf event in the document. The other set is Entity, containing a variable for each agent or theme
candidate. Each variable k in GFBF has its corresponding agent and theme variables, i and j, in Entity.
The three form a triple unit, ?i, k, j?. The set Triple consists of each ?i, k, j?, recording the correspon-
dence between variables in GFBF and Entity. The goal of the framework is to assign optimal labels to
variables in Entity and GFBF. We first introduce how we recognize candidates for agents and themes,
then introduce the optimization framework, and then define local scores that are input to the framework.
4.1 Local Agents and Theme Candidates Detector
We extract two agent candidates and two theme candidates for each gfbf event (one each will ultimately
be chosen by the ILP model).
4
We use syntax, and the output of the SENNA (Collobert et al., 2011)
semantic role labeling tool. SENNA labels the A0 (subject), A1 (object), and A2 (indirect object) spans
for each predicate, if possible. To extract the semantic agent candidate: If SENNA labels a span as A0
of the gfbf event, we consider it as the semantic agent; if there is no A0 but A1 is labeled, we consider
A1; if there is no A0 or A1 but A2 is labeled, we consider A2. To extract the syntactic agent candidate,
we find the nearest noun in front of the gfbf span, and then extract any other word that depends on the
noun according to the dependency parse. Similarly, to extract the semantic theme candidate, we consider
A1, A2, A0 in order. To extract the syntactic theme candidate, the same procedure is conducted as for
the syntactic agent, but the nearest noun should be after the gfbf. If there is no A0, A1 or A2, then there
is only one agent candidate, implicit and only one theme candidate, null. We treat a null theme as an
incorrect span in the later evaluations. If the two agent (theme) candidate spans are the same, there is
only one candidate.
4.2 Integer Linear Programming Framework
We use Integer Linear Programming (ILP) to assign labels to variables. Variables in Entity will be
assigned positive or negative, representing the writer?s sentiments toward them. We may have two candi-
date agents for a gfbf and that we will choose between them. Thus, only one agent is assigned a positive
or negative label; the other is considered to be an incorrect agent of the gfbf (similarly for the theme can-
didates). Each variable in GFBF will be assigned the label gf or bf. Optionally, it may also be assigned
the label reversed. Label gf or bf is the polarity of the gfbf event; reversed is assigned if the polarity is
reversed (e.g., for ?not harmed?, the labels are bf and reversed).
The objective function of the ILP is:
4
This framework is able to handle any number of candidates. The methods we tried using more candidates did not perform
as well - the gain in recall was offset by larger losses in precision.
81
min
u
1gf
,u
1bf
...
(
? 1 ?
?
i?GFBF?Entity
?
c?L
i
p
ic
u
ic
)
+
?
?i,k,j??Triple
?
ikj
+
?
?i,k,j??Triple
?
ikj
(1)
subject to
u
ic
? {0, 1}, ?i, c ?
ikj
, ?
ikj
? {0, 1},??i, k, j? ? Triple (2)
where L
i
is the set of labels given to ?i ? GFBF ? Entity. If i ? GFBF, L
i
is {gf, bf, reversed} ({gf,
bf, r}, for short). If i ? Entity, L
i
is {positive, negative} ({pos, neg}, for short). u
ic
is a binary in-
dicator representing whether the label c is assigned to the variable i. When an indicator variable is 1,
the corresponding label is selected. p
ic
is the score given by local detectors, introduced in the following
sections. Variables ?
ikj
and ?
ikj
are binary slack variables that correspond to the gfbf implicature con-
straints of ?i, k, j?. When a given slack variable is 1, the corresponding triple violates the implicature
constraints. Minimizing the objective function could achieve two goals at the same time. The first part
(?1 ?
?
i
?
c
p
ic
u
ic
) tries to select a set of labels that maximize the scores given by the local detectors.
The second part (
?
ikj
?
ikj
+
?
ikj
?
ikj
) aims at minimizing the cases where gfbf implicature constraints
are violated. Here we do not force each triple to obey the implicature constraints, but to minimize the
violating cases. For each variable, we have defined constraints:
?
c?L
GFBF
?
u
kc
= 1, ?k ? GFBF (3)
?
i?Entity
?i,k,j??Triple
?
c?L
Entity
u
ic
= 1,?k ? GFBF (4)
?
j?Entity,
?i,k,j??Triple
?
c?L
Entity
u
jc
= 1,?k ? GFBF (5)
where L
GFBF
?
in Equation (3) is a subset of L
GFBF
, consisting of {gf, bf}. Equation (3) means a
gfbf must be either gf or bf. But it is free to choose whether it is being reversed. Recall that we have two
agent candidates (a1,a2) for a gfbf. Thus we have four agent indicators in Equation (4): u
a1,pos
, u
a1,neg
,
u
a2,pos
and u
a2,neg
. Equation (4) ensures that three of them are 0 and one of them is 1. For instance,
u
a1,pos
assigned 1 means that candidate a1 is selected to be the agent span and pos is selected to be its
polarity. In this way, the framework disambiguates the agent span and sentiment polarity simultaneously.
(Similar comments apply for the theme candidates in Equation (5).)
According to the implicature rules in Table 1 in Section 3, the writer has the same sentiment toward
entities in a gf relation. Thus, for each triple unit ?i, k, j?, the gf constraints are applied via the following:
|
?
i,?i,k,j?
u
i,pos
?
?
j,?i,k,j?
u
j,pos
|+ |u
k,gf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (6)
|
?
i,?i,k,j?
u
i,neg
?
?
j,?i,k,j?
u
j,neg
|+ |u
k,gf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (7)
We use |u
k,gf
? u
k,r
| to represent whether this triple is gf. In Equation (6), if this value is 1, then the
triple should follow the gf constraints. In that case, ?
ikj
= 0 means that the triple doesn?t violate the
gf constraints, and |
?
i
u
i,pos
?
?
j
u
j,pos
| must be 0. Further, in this case,
?
i
u
i,pos
and
?
j
u
j,pos
are
constrained to be of the same value (both 1 or 0) ? that is, entities i and j must be both positive or both
not positive. However, if ?
ikj
= 1, Equation (6) does not constrain the values of the variables at all. If
|u
k,gf
? u
k,r
| is 0, representing that the triple is not gf, then Equation (6) does not constrain the values
of the variables. Similar comments apply to Equation (7).
In contrast, the writer has opposite sentiments toward entities in a bf relation.
|
?
i,?i,k,j?
u
i,pos
+
?
j,?i,k,j?
u
j,pos
? 1|+ |u
k,bf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (8)
|
?
i,?i,k,j?
u
i,neg
+
?
j,?i,k,j?
u
j,neg
? 1|+ |u
k,bf
? u
k,r
| <= 1 + ?
ikj
,?k ? GFBF (9)
We use |u
k,bf
? u
k,r
| to represent whether this triple is bf. In Equation (8), if a triple is bf and the
constraints are not violated, then |
?
i
u
i,pos
+
?
j
u
j,pos
? 1| must be 0. Further, in this case,
?
i
u
i,pos
82
ugf
u
bf
u
r
|u
gf
? u
r
| |u
bf
? u
r
| u
gf
u
bf
u
r
|u
gf
? u
r
| |u
bf
? u
r
|
A 1 0 0 1 0 C 0 1 0 0 1
B 0 1 1 1 0 D 1 0 1 0 1
Table 2: Truth table of being reversed or not (k is omitted)
and
?
j
u
j,pos
are constrained to be of the opposite value ? that is, if entity i is positive then entity j must
not be positive. Similar comments apply to Equation (9).
Note that above we use |u
k,gf
?u
k,r
| and |u
k,bf
?u
k,r
| to represent whether a triple is gf or bf. In Table
2, we show that they always take opposite values and that they are consistent with the actual polarities.
In Table 2, Case A means the triple is gf and Case B means the triple is bf but it is reversed. In both
cases, |u
gf
? u
r
| = 1, indicating that the triple should follow the gf constraints. Similarly for Case C
and Case D to follow the bf constraints.
4.3 Local GoodFor/BadFor Score: p
k,gf
, p
k,bf
We utilize a sense-level gfbf lexicon by (Choi et al., 2014). In total there are 6,622 gf senses and 3,290
bf senses. The gf lexicon covers 64% of the gf words in the corpus and the bf lexicon covers 42% of the
bf words. We then look up the gfbf span k in the gfbf lexicon. If k only appears in the gf lexicon, then
p
k,gf
= 1 ?  and p
k,bf
= . Here  = 0.0001, to prevent there being any 0 scores in our computation.
If k only appears in the bf lexicon, then p
k,bf
= 1 ?  and p
i,gf
= . If k appears in both the gf and bf
lexicon, and there are a senses in the gf lexicon and b senses in the bf lexicon, then p
k,gf
= a/(a + b)
and p
k,bf
= b/(a + b). If k is not in either lexicon, then p
k,gf
= p
k,bf
= . If there is more than one
word in the gfbf span, we take the maximum score.
4.4 Local Reversed Score: p
k,r
As introduced in Section 3, a reverser changes the polarity of a gfbf. First, we build reverser lexicons
from Wilson?s shifter lexicon (2008), namely the entries labeled as genshifter, negation, and shiftneg.
We create two lexicons: one with the verbs and the other with the non-verb entries, excluding nouns,
adjectives, and adverbs, since most non-verb reversers are prepositions or subordinating conjunctions.
There are 219 reversers in the entire corpus; 134 (61.19%) are instances of words in one of the two
lexicons. Based on the lexicon, we categorize reversers into three classes. Examples are shown below.
Ex(2) They will not be able to water down your coverage.
Ex(3) ... how a massive new bureaucracy will cut costs without hurting the old and the helpless.
Ex(4) The new law includes new rules to prevent insurance companies from overcharging patients.
Negation: An instance in this category is ?not? in Ex(2). If any word in the gfbf span has a neg
dependency relation according to the Stanford dependency parser, then we consider the gfbf to be negated
(i.e., reversed). In this case the path between the negator and the gfbf is labeled neg and the length of the
path is one.
Other Non-Verb: This category consists of words such as ?without? in Ex(3) (others are ?never? and
?few?, etc). These words lower the extent of the gfbf event. We look in the sentence for instances of
words in the non-verb reverser lexicon, which are not tagged as noun, verb, adj, or adv. For any found,
we examine the path in the dependency parse between the potential reverser and the gfbf span. If the
path has at least one of advmod, pcomp, cc, xcomp, nsubj, neg and the length of the path is less than four
(learnt from development set), the event is considered to be reversed.
Verb: In Ex(4), the verb ?prevent? stops the gfbf event ?overcharging? from happening. We call such
words Verb reverser (others are ?prohibit? and ?ban?, etc). We look in the sentence for instances of words
in the verb reverser lexicon. For any that appear before the gfbf span in the sentence, if the path has at
least one of xcomp, pcomp, obj and the length of the path is less than four, then the event is reversed.
For the triple ?companies, overcharging, patients? in Ex(4), though it is reversed by ?prevent?, the agent
of the reverser, which is ?law?, is different from the agent of the gfbf, which is ?companies?, so the bf
83
within the ?overcharging? event is not reversed.
5
Though we extract the Verb reversers to evaluate the
performance of recognizing a reverser, in the optimization framework, gfbf events with Verb reversers
are not considered to be reversed, since almost all Verb reversers introduce new agents.
Different from other scores, p
k,r
could be negative. According to the heuristics above, the probability
of a gfbf event being reversed decreases as the length of the path increases. We define p
k,r
so it is
inversely proportional to the length of the path. Further, to make sense of a gfbf triple ?agent, gfbf,
theme?, where, e.g., the local detectors label it ?pos, bf, pos?, the framework is choosing the smaller
one from (a) ?1 ? p
k,r
? u
k,r
(it has a reverser) versus (b) 1 ? ?
ikj
(it is an exception to the rules). The
framework assigns u
k,r
= 0 and ?
ikj
= 1 if ?1 ? p
k,r
> 1. It assigns u
k,r
= 1 and ?
ikj
= 0 if
?1 ? p
k,r
<= 1. For gfbf events which have Negation or Other Non-verb reversers, since we use the
length four as a threshold in the heuristics above, we define p
k,r
=
1
d
?
5
4
, so that ?1 ? p
k,r
=
5
4
?
1
d
> 1
if d > 4. For gfbf events for which no reverser word appears in the sentence, or those which only have
Verb reversers, p
k,r
= ?1 ?
5
4
(so ?1 ? p
k,r
> 1), so that the framework chooses case (b) (choosing the
gfbf event to be not reversed).
4.5 Local Sentiment Score: p
i,pos
, p
i,neg
In the corpus of DCW, only the writer?s sentiments toward the agents and the themes of gfbf events are
annotated. Thus, since there are many false negatives of sentiments toward entities, the corpus does
not support training a classifier. Therefore, we adopt the same local sentiment detector from (Deng
and Wiebe, 2014), using available resources to detect writer?s sentiments toward all agent and theme
candidates.
6
The sentiment scores range from 0.5 to 1.
5 Co-reference In the Framework
So far the constraints in the framework are within a gfbf triple. Consider the following example:
Ex(5) The reform will decrease the healthcare costs and improve the medical qualify as expected.
The two gfbfs, ?decrease? and ?improve? have the same agent, ?reform?. Thus, if there is more than
one gfbf in a sentence, and the path between the two gfbfs in dependency parse contains only conj or
xcomp, and there is no other noun between the latter gfbf and the conjunction, we assume the two agents
are the same and the sentiments toward them should be the same. Thus, for any i, j ? Entity, if i, j
co-refer
7
, or they are the same agent as described above, Coref(i, j) = 1 (otherwise 0). We add two
more constraints, similar to the gf constraints in Equations (6) and (7), as shown in Equation (10) and
(11). where ?
ij
is a slack variable, e(i) is the set of agent/theme candidates linked to the same gfbf as i
is. If Coref(i, j) = 0, Equations (10) and (11) do not constrain the variables. The objective function in
Equation (12) is updated to incorporate these new constraints.
|
?
e(i)
u
i,pos
?
?
e(j)
u
j,pos
|+ Coref(i, j) <= 1 + ?
ij
,?i, j ? Entity (10)
|
?
e(i)
u
i,neg
?
?
e(j)
u
j,neg
|+ Coref(i, j) <= 1 + ?
ij
, ?i, j ? Entity (11)
min
u
1gf
,u
1bf
...
(
? 1 ?
?
i?GFBF?Entity
?
c?L
i
p
ic
u
ic
)
+
?
?i,k,j??Triple
?
ikj
+
?
?i,k,j??Triple
?
ikj
+
?
i,j?Entity
?
ij
(12)
6 Experiment and Performance
In this section we introduce the data we use, the baseline methods, the evaluations and the results. In
addition, we give examples illustrating how opinion inference may improve performances.
5
DCW defines here is a triple chain: ?law, prevent ?companies, overcharging, patients??. The reverser is changing the
polarity between ?law? and ?patients?, but it does not change the polarity between ?companies? and ?patients?.
6
We use Opinion Extractor (Johansson and Moschitti, 2013) , opinionFinder (Wilson et al., 2005), MPQA subjectivity
lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966) and a connotation lexicon (Feng et al., 2013), to detect
writer?s sentiments toward all agent and theme candidates, and all gfbf events. We adopt Rule 1 and Rule 3 to infer from the
sentiment toward event to the sentiment toward theme. Then we conduct a majority voting based on the results.
7
We use the co-reference resolution system from (Stoyanov et al., 2010).
84
6.1 Experiment Data
We use the ?Affordable Care Act? corpus of DCW, consisting of 134 online editorials and blogs. In total,
there are 1,762 annotated triples, out of which 692 are gf or retainers and 1,070 are bf or reversers. From
the writer?s perspective, 1,495 noun phrases are annotated positive, 1,114 noun phrases are negative
and the remaining 8 are neutral. This indicates that there are many opinions in the corpus. Out of 134
documents in the corpus, 3 do not have any annotation. 6 are used as a development set to develop the
heuristics in Sections 4 and 5. We use the remaining 125 for the experiments.
6.2 Baseline Methods and Evaluation Metrics
We compare the output of the global optimization framework with the outputs of baseline systems built
from the local detectors in Section 4. For the gfbf polarity and reverser ambiguities, the local detectors
directly provide a disambiguation result. For the agent/theme span and sentiment ambiguities, the local
sentiment detector assigns positive and negative scores to each candidate. The framework chooses among
the combined options. Thus, for comparison, we build a baseline system that combines the outputs of
the local agent/theme candidate detector and the local sentiment detector.
Recall from Section 4, a variable k ? GFBF has two agent candidates, a1 and a2 ? Entity. Together
there are four binary indicator variables: u
a1,pos
, u
a1,neg
, u
a2,pos
and u
a2,neg
. Among these indicator
variables whose corresponding local scores (e.g., p
a1,pos
is the score of u
a1,pos
) are larger than 0.5,
the baseline system (denoted Local) chooses the one with the largest local sentiment score. If there is
a tie, it prefers the variable representing the semantic candidate. If there is still a tie, it chooses the
variable representing the majority polarity (positive). If all the local scores of the four variables are
0.5 (neutral), Local fails to recognize any sentiment for that entity, so it assigns 0 to all the indicator
variables. Local+coref takes the maximum local score of the entities if they co-ref, and assigns each
entity the maximum score before disambiguation.
Another baseline, Majority, always chooses the semantic candidate and the majority polarity.
To evaluate the performance in detecting sentiment, we use precision, recall, and F-measure. We do
not take into account any agent or theme manually annotated as neutral (there are only 8).
P =
#(auto=gold & gold!=neutral)
#auto!=neutral
Accuracy = R =
#(auto=gold & gold!=neutral)
#gold!=neutral
F =
2*P*R
P+R
(13)
In the equations, auto is the system?s output and gold is the gold-standard label from annotations. Since
we don?t take into account any neutral agent or theme, #gold!=neutral equals to all nodes in the exper-
iment set. Thus accuracy is equal to recall. We only report recall here. Here we have two definitions
of auto=gold: (1) Strict evaluation means that, by saying auto=gold, the agent/theme must have the
same polarity and must be the same NP as the gold standard, and (2) Relaxed evaluation means the
agent/theme has the same polarity as the gold standard, regardless whether the span is correct or not.
Note that according to DCW, an implicit agent isn?t annotated with any sentiment. Thus, for an
implicit agent in gold, if auto outputs the span ?implicit?, we treat it as a correct span with correct
polarity, regardless what sentiment auto gives to it. If auto outputs any span other than ?implicit?, we
treat it as a wrong span with wrong polarity, regardless of its sentiment as well. For the theme span, if
auto outputs a ?null? theme candidate, we treat it as a wrong span but we evaluate its sentiment according
to gold.
To evaluate extracting candidate span, we use accuracy. The baseline for this task always chooses the
semantic candidate. To evaluate gfbf polarity and reverser, we also use accuracy.
Note that although we evaluate the performance in different tasks separately, the framework resolves
all the ambiguities at the same time.
6.3 Results
We report the performance results for (A) sentiment detection in Table 3, on two sets. One is the subset
containing the agents and themes where auto has the correct spans with gold. The other is the set of
all agents and themes. As shown in Table 3, ILP significantly improves performance, approximately
10-20 points on F-measure over different baselines. Though Local has a competitive precision with
85
correct span subset whole set, strict eval whole set, relaxed eval
P R F P R F P R F
1 ILP 0.6421 0.6421 0.6421 0.4401 0.4401 0.4401 0.5939 0.5939 0.5939
2 Local 0.6409 0.3332 0.4384 0.4956 0.2891 0.3652 0.5983 0.3490 0.4408
3 ILP+coref 0.6945 0.6945 0.6945 0.4660 0.4660 0.4660 0.6471 0.6471 0.6471
4 Local+coref 0.6575 0.3631 0.4678 0.5025 0.3103 0.3836 0.6210 0.3834 0.4741
5 Majority 0.5792 0.5792 0.5792 0.3862 0.3862 0.3862 0.5462 0.5462 0.5462
Table 3: Performances of sentiment detection
ILP, it has a much lower recall. That means the local sentiment detector cannot recognize implicit
sentiments toward most entities. But ILP is able to recognize more entities correctly. By adding coref,
performance improves for both ILP and Local. In comparison to (Deng and Wiebe, 2014), our current
method improves more in F-measure (2.43 points more) over local sentiment detector than the earlier
work, even though the earlier work takes the manual annotations of all the gfbf information as input.
In terms of the other tasks: For (B) agent/theme span, the baseline achieves 66.67% in accuracy, com-
pared to 68.54% and 67.10% for ILP and ILP+coref, respectively. For (C) gfbf polarity, the baseline
has an accuracy of 70.68%, whereas ILP achieves 77.25% and ILP+coref achieves 77.47%, respectively,
both 7 points higher. This improvement is interesting because it represents cases in which the optimiza-
tion framework is able to infer the correct polarity even though the gfbf span is not recognized by the
local detector (i.e., the span isn?t in the gfbf lexicon). For (D) reverser, the baseline is 88.07% in accu-
racy. ILP and ILP+coref are competitive with the baseline: 89% and 88.07% respectively. Note that both
our local detector and ILP surpass the majority class (not reversed) which has an accuracy of 86.60%.
Following (Akkaya et al., 2009), since ILP is unsupervised without multiple runs, we adopt McNe-
mar?s test to measure statistical significance of our improvements (Dietterich, 1998). In Table 3, the
improvements in recalls of Line 1 over 2, Line 3 over 4, and Lines 1&3 over 5 are statistically significant
at the p < .001 level. The improvements of Line 3 over 1 are statistically significant at the p < .005
level. For accuracy of gfbf polarity, the improvement is significant at the p < .001 level.
6.4 Examples
This sections gives simplified examples to illustrate how the framework can improve over the local
detectors. The explicit sentiment clues referred to in this section are from MPQA lexicon.
Ex(6) The reform would curb skyrocketing costs in the long run.
The local sentiment detector assigns ?costs? negative due to the single sentiment clue, ?skyrocketing?.
Since the agent and theme are in a bf triple, and the writer is negative toward that theme, we can infer
the writer is positive toward the agent. This illustrates how we improve recall on sentiments.
Ex(7) The supposedly costly reform will curb skyrocketing costs in the long run.
In Ex(7), agent ?reform? is labeled negative because ?costly? is a negative clue in the lexicon. (?sup-
posedly? is not in it.) However, in Ex(7), it is actually positive. The agent?s negative score is 0.6, and
its positive score is 0.5 due to the absence of a positive clue. Since the theme is negative too, by the bf
constraints, we expect to see a positive agent. If we were to assign negative to the agent, the objective
function would have -0.6 subjectivity score and +1 in violation penalty, together giving +0.4. If we as-
sign positive, the subjectivity score is -0.5, and there is no violation, resulting in a total score of -0.5.
Thus, the framework correctly chooses the positive label. This shows how we can improve precision on
sentiments.
Ex(8) The great reform will curb skyrocketing costs in the long run.
In this case, the agent is positive and the theme is negative. If the gfbf word ?curb? is not in the lexicon,
we could still infer its polarity. Given that the entities in the triple have different sentiments, to not violate
86
the implicature rules, the framework will assign it bf, or assign it gf along with reversed. However, there
is no reverser word in the sentence, so the reversed score p
r
= ?
5
4
. The framework will assign the
reverser indicator u
r
= 0, in order to avoid a gain in the objective function by ?1 ? p
r
? u
r
. Thus
the framework assigns the label bf to ?curb?. This is how the framework can improve the accuracy of
recognizing gfbf polarity.
7 Conclusion
The ultimate goal of this work is to utilize gfbf information to improve detection of the writer?s
sentiments toward entities mentioned in the text. Using an unsupervised optimization framework that
incorporates gfbf implicature rules as constraints, our method improves over local sentiment recognition
by almost 20 points in F-measure and over all sentiment baselines by over 10 points in F-measure. The
global optimization framework jointly infers the polarity of gfbf events, whether or not they are reversed,
which candidate NPs are the agent and theme, and the writer?s sentiments toward them. In addition
to beating the baselines for sentiment detection, the framework significantly improves the accuracy of
gfbf polarity disambiguation. This work not only automatically utilizes gfbf information to improve
sentiment detection, it also proposes a framework for jointly solving various ambiguities related to gfbf
events.
Acknowledgement This work was supported in part by DARPA-BAA-12-47 DEFT grant. We would
like to thank the anonymous reviewers for their helpful feedback.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009. Subjectivity word sense disambiguation. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP
?09, pages 190?199, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pranav Anand and Kevin Reschke. 2010. Verb classes as evaluativity functor classes. In Interdisciplinary Work-
shop on Verbs. The Identification and Representation of Verb Features.
Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 793?801, Honolulu, Hawaii, October. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,
pages 431?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Yoonjung Choi, Janyce Wiebe, and Lingjia Deng. 2014. Lexical acquisition for opinion inference: A sense-level
lexicon of benefactive and malefactive events. In 5th Workshop on Computational Approaches to Subjectivity,
Sentiment & Social Media Analysis.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493?2537, November.
Dipanjan Das, Andr?e FT Martins, and Noah A Smith. 2012. An exact dual decomposition algorithm for shallow
semantic parsing with constraints. In Proceedings of the First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Evaluation, pages 209?217. Association for Computational
Linguistics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment propagation via implicature constraints. In Meeting of the
European Chapter of the Association for Computational Linguistics (EACL-2014).
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013. Benefactive/malefactive event and writer attitude anno-
tation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers), pages 120?125, Sofia, Bulgaria, August. Association for Computational Linguistics.
87
Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using
integer programming. In Human Language Technologies 2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 236?243,
Rochester, New York, April. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment
beneath the surface meaning. In Proceedings of the 51th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, Angust. Association for Computational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2013. A computational model for plot units. Computational
Intelligence, 29(3):466?488.
Herbert Paul Grice. 1967. Logic and conversation. The William James lectures.
Herbert Paul Grice. 1989. Studies in the Way of Words. Harvard University Press.
Richard Johansson and Alessandro Moschitti. 2013. Relational features in fine-grained opinion analysis. Compu-
tational Linguistics, 39(3).
Andr?e F. T. Martins and Noah a. Smith. 2009. Summarization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing
- ILP ?09, pages 1?9, Morristown, NJ, USA. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear
programming inference. In Proceedings of the 20th international conference on Computational Linguistics,
page 1346. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics, 34(2):257?287.
Kevin Reschke and Pranav Anand. 2011. Extracting contextual evaluativity. In Proceedings of the Ninth Interna-
tional Conference on Computational Semantics, IWCS ?11, pages 370?374, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language
tasks. In CONLL.
Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 226?234, Suntec, Singapore, August. Association for Computational
Linguistics.
P.J. Stone, D.C. Dunphy, M.S. Smith, and D.M. Ogilvie. 1966. The General Inquirer: A Computer Approach to
Content Analysis. MIT Press, Cambridge.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Corefer-
ence resolution with reconcile. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ?10, pages
156?161, Stroudsburg, PA, USA. Association for Computational Linguistics.
Janyce Wiebe and Lingjia Deng. 2014. An account of opinion implicatures. arXiv:1404.6491v1 [cs.CL].
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language ann. Language Resources and Evaluation, 39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLP/EMNLP, pages 347?354.
Theresa Wilson. 2008. Fine-grained subjectivity analysis. Ph.D. thesis, Doctoral Dissertation, University of
Pittsburgh.
Bishan Yang and Claire Cardie. 2013. Joint Inference for Fine-grained Opinion Extraction. In Proceedings of
ACL, pages 1640?1649.
Lei Zhang and Bing Liu. 2011. Identifying noun product features that imply opinions. In Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
575?580, Portland, Oregon, USA, June. Association for Computational Linguistics.
88
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 377?385,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Sentiment Propagation via Implicature Constraints
Lingjia Deng
Intelligent Systems Program
University of Pittsburgh
lid29@pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
Opinions may be expressed implicitly via
inference over explicit sentiments and
events that positively/negatively affect en-
tities (goodFor/badFor events). We in-
vestigate how such inferences may be
exploited to improve sentiment analysis,
given goodFor/badFor event information.
We apply Loopy Belief Propagation to
propagate sentiments among entities. The
graph-based model improves over explicit
sentiment classification by 10 points in
precision and, in an evaluation of the
model itself, we find it has an 89% chance
of propagating sentiments correctly.
1 Introduction
Previous research in sentiment analysis and
opinion extraction has largely focused on the
interpretation of explicitly stated opinions. How-
ever, many opinions are expressed implicitly
via opinion implicature (i.e., opinion-oriented
defeasible inference). Consider the following
sentence:
EX(1) The bill would lower health care costs, which would
be a tremendous positive change across the entire health-care
system.
The writer is clearly positive toward the idea of
lowering health care costs. But how does s/he feel
about the costs? If s/he is positive toward the idea
of lowering them, then, presumably, she is nega-
tive toward the costs themselves (specifically, how
high they are). The only explicit sentiment expres-
sion, tremendous positive change, is positive, yet
we can infer a negative attitude toward the object
of the event itself (i.e., health care costs).
Going further, since the bill is the agent of an
event toward which the writer is positive, we may
(defeasibly) infer that the writer is positive toward
the bill, even though there are no explicit senti-
ment expressions describing it.
Now, consider The bill would curb skyrocketing
health care costs. The writer expresses an explicit
negative sentiment (skyrocketing) toward the ob-
ject (health care costs) of the event. Note that
curbing costs, like lowering them, is bad for them
(the costs are reduced). We can reason that, be-
cause the event is bad for something toward which
the writer is negative, the writer is positive toward
the event. We can reason from there, as above,
that the writer is positive toward the bill, since it
is the agent of the positive event.
These examples illustrate how explicit sen-
timents toward one entity may be propagated
to other entities via opinion implicature rules.
The rules involve events that positively or nega-
tively affect entities. We call such events good-
For/badFor (hereafter gfbf )events.
This work investigates how gfbf event interac-
tions among entities, combined with opinion in-
ferences, may be exploited to improve classifica-
tion of the writer?s sentiments toward entities men-
tioned in the text. We introduce four rule schemas
which reveal sentiment constraints among gfbf
events and their agents and objects. Those con-
straints are incorporated into a graph-based model,
where a node represents an entity (agent/object),
and an edge exists between two nodes if the two
entities participate in one or more gfbf events with
each other. Scores on the nodes represent the ex-
plicit sentiments, if any, expressed by the writer
toward the entities. Scores on the edges are based
on constraints derived from the rules. Loopy Be-
lief Propagation (LBP) (Pearl, 1982) is applied to
377
accomplish sentiment propagation in the graph.
Two evaluations are performed. The first shows
that the graph-based model improves over an ex-
plicit sentiment classification system. The second
evaluates the graph-based model itself (and hence
the implicature rules), assessing its ability to cor-
rectly propagate sentiments to nodes whose polar-
ities are unknown. We find it has an 89% chance
of propagating sentiment values correctly.
This is the first paper to address this type of
sentiment propagation to improve sentiment anal-
ysis. To eliminate interference introduced by other
components, we use manually annotated gfbf in-
formation to build the graph. Thus, the evaluations
in this paper are able to demonstrate the promise
of the overall framework itself.
2 Related Work
Much work in sentiment analysis has been on
document-level classification. Since different sen-
timents may be expressed toward different entities
in a document, fine-grained analysis may be more
informative for applications.
However, fine-grained sentiment analysis re-
mains a challenging task for NLP systems. For
fully-automatic systems evaluated on the MPQA
corpus (Wiebe et al., 2005), for example, a recent
paper (Johansson and Moschitti, 2013) reports re-
sults that improve over previous work, yet the F-
measures are in the 40s and 50s.
Most work in NLP addresses explicit sentiment,
but some address implicit sentiment. For example,
(Zhang and Liu, 2011) identify noun product fea-
tures that imply opinions, and (Feng et al., 2013)
identify objective words that have positive or neg-
ative connotations. However, identifying terms
that imply opinions is a different task than senti-
ment propagation between entities. (Dasigi et al.,
2012) search for implicit attitudes shared between
authors, while we address inferences within a sin-
gle text.
Several papers apply compositional semantics
to determine polarity (e.g., (Moilanen and Pul-
man, 2007; Choi and Cardie, 2008; Moilanen et
al., 2010); see (Liu, 2012) for an overview). The
goal of such work is to determine one overall po-
larity of an expression or sentence. In contrast, our
framework commits to a holder having sentiments
toward various events and entities in the sentence,
possibly of different polarities.
The idea of gfbf events in sentiment analysis is
not entirely new. For example, two papers men-
tioned above (Zhang and Liu, 2011; Choi and
Cardie, 2008) include linguistic patterns for the
tasks that they address that include gfbf events,
but they don?t define general implicature rules re-
lating sentiments and gfbf events, agents, and ob-
jects as we do. Recently, in linguistics, Anand
and Reschke (2010; 2011) identify classes of
gfbf terms, and carry out studies involving artifi-
cially constructed gfbf triples and corpus exam-
ples matching fixed linguistic templates. Our work
focuses on gfbf triples in naturally-occurring data
and uses generalized implicature rules. Goyal et
al. (2012) generate a lexicon of patient polar-
ity verbs, which correspond to gfbf events whose
spans are verbs. Riloff et al. (2013) investigate
sarcasm where the writer holds a positive senti-
ment toward a negative situation. However, nei-
ther of these works performs sentiment inference.
Graph-based models have been used for various
tasks in sentiment analysis. Some work (Wang et
al., 2011; Tan et al., 2011) apply LBP on a graph
capturing the relations between users and tweets in
Twitter data . However, they assume the nodes and
the neighbors of nodes share the same sentiments.
In contrast, we don?t assume that neighbors share
the same sentiment, and the task we address is dif-
ferent.
3 Opinion Implicatures
This section describes the opinion-implicature
framework motivating the design of the graph-
based method for sentiment analysis proposed be-
low. The components of the framework are gfbf
events, explicit sentiments, and rules operating
over gfbf events and sentiments.
The definition of a gfbf event is from (Deng et
al., 2013). A GOODFOR event is an event that
positively affects an entity (similarly, for BADFOR
events). (Deng et al., 2013) point out that gfbf ob-
jects are not equivalent to benefactive/malefactive
semantic roles. An example they give is She baked
a cake for me: a cake is the object of GOOD-
FOR event baked (creating something is good for
it (Anand and Reschke, 2010)), while me is the
filler of its benefactive semantic role (Z?u?niga and
Kittil?a, 2010).
Four implicature rule schemas are relevant for
this paper.
1
Four individual rules are covered by
1
Implicatures ?normally accompany the utterances of a
given sentence unless special factors exclude that possibility
378
each schema. sent(?) = ? means that the writer?s
sentiment toward ? is ?, where ? is a GOODFOR
event, a BADFOR event, or the agent or object of
a gfbf event, and ? is either positive or negative
(pos or neg, for short). P? Q is to infer Q from P.
Rule1: sent(gfbf event)? sent(object)
1.1 sent(GOODFOR) = pos? sent(object) = pos
1.2 sent(GOODFOR) = neg? sent(object) = neg
1.3 sent(BADFOR) = pos? sent(object) = neg
1.4 sent(BADFOR) = neg? sent(object) = pos
Rule2: sent(object)? sent(gfbf event)
2.1 sent(object) = pos? sent(GOODFOR) = pos
2.2 sent(object) = neg? sent(GOODFOR) = neg
2.3 sent(object) = pos? sent(BADFOR) = neg
2.4 sent(object) = neg? sent(BADFOR) = pos
Rule3: sent(gfbf event)? sent(agent)
3.1 sent(GOODFOR) = pos? sent(agent) = pos
3.2 sent(GOODFOR) = neg? sent(agent) = neg
3.3 sent(BADFOR) = pos? sent(agent) = pos
3.4 sent(BADFOR) = neg? sent(agent) = neg
Rule4: sent(agent)? sent(gfbf event)
4.1 sent(agent) = pos? sent(GOODFOR) = pos
4.2 sent(agent) = neg? sent(GOODFOR) = neg
4.3 sent(agent) = pos? sent(BADFOR) = pos
4.4 sent(agent) = neg? sent(BADFOR) = neg
To explain the rules, we step through an example:
EX(2) Why would [President Obama] support [health care
reform]? Because [reform] could lower [skyrocketing health
care costs], and prohibit [private insurance companies] from
overcharging [patients].
Suppose a sentiment analysis system recognizes
only one explicit sentiment expression, skyrock-
eting. According to the annotations, there are
several gfbf events. Each is listed below in the
form ?agent, gfbf, object?.
E
1
: ?reform, lower, costs?
E
2
: ?reform, prohibit, E
3
?
E
3
: ?companies, overcharge, patients?
E
4
: ?Obama, support, reform?
In E
1
, from the negative sentiment expressed
by skyrocketing (the writer is negative toward the
(p. 39).? (Huddleston and Pullum, 2002)
costs because they are too high), and the fact that
costs is the object of a BADFOR event (lower),
Rule2.4 infers a positive attitude toward E
1
.
Now, Rule3.3 applies. We infer the writer is
positive toward the reform, since it is the agent
of E
1
, toward which the writer is positive.
E
2
illustrates the case where the object is an
event. Specifically, the object of E
2
is E
3
, a BAD-
FOR event (overcharging). As we can see, E
2
keeps E
3
from happening. Events such as E
2
are REVERSERs, because they reverse the polar-
ity of a gfbf event (from BADFOR to GOODFOR,
or vice versa). Note that REVERSERs may be seen
as BADFOR events, because they make their ob-
jects irrealis (i.e., not happen). Similarly, a RE-
TAINER such as help in ?help Mary save Bill? can
be viewed as a GOODFOR event. (We call a RE-
VERSER or a RETAINER an INFLUENCER.) In this
paper, RETAINERS are treated as GOODFOR events
and REVERSERS are treated as BADFOR events.
Above, we inferred that the writer is positive to-
ward reform, the agent of E
2
. By Rule 4.3, the
writer is positive towardE
2
; then by Rule 1.3, the
writer is negative toward E
3
, the object of E
2
.
For E
3
, using Rule 1.4 we know the writer is
positive toward patients and using Rule 3.4 we
know the writer is negative toward companies.
Turning to E
4
, support health care reform is
GOODFOR reform. We already inferred the writer
is positive toward reform. Rule 2.1 infers that the
writer is positive toward E
4
. Rule 3.1 then infers
that the writer is positive toward the agent of E
4
,
Obama.
In summary, we infer that the writer is positive
toward E
1
, health care reform, E
2
, patients, E
4
,
and Obama, and negative toward E
3
and private
insurance companies.
4 Data
We use the data described in (Deng et al., 2013),
2
which consists of 134 documents about a contro-
versial topic, ?the Affordable Care Act.? The doc-
uments are editorials and blogs, and are full of
opinions.
In the data, gfbf triples are annotated specifying
the spans of the gfbf event, its agent, and its object,
as well as the polarity of the gfbf event (GOODFOR
or BADFOR), and the writer?s attitude toward the
agent and object (positive, negative, or neutral).
Influencers are also annotated. The agents of gfbf
2
Available at http://mpqa.cs.pitt.edu
379
and influencer events are noun phrases. The ob-
ject of a gfbf event is a noun phrase, but the object
of an influencer is a gfbf event or another influ-
encer. A triple chain is a chain of zero or more
influencers ending in a gfbf event, where the ob-
ject of each element of the chain is the following
element in the chain. (e.g. in EX(2), the two event
prohibit and overcharging is a triple chain.)
In total, there are 1,762 annotated gfbf triples,
out of which 692 are GOODFOR or RETAINER
and 1,070 are BADFOR or REVERSER. From the
writer?s perspective, 1,495 noun phrases are anno-
tated positive, 1,114 are negative and the remain-
ing 8 are neutral. This is not surprising, given that
most of the sentences in the data are opinionated.
5 Graph-based Model
We propose a graph-based model of entities and
the gfbf relations between them to enable senti-
ment propagation between entities. In this section,
we introduce the definition of the graph (in 5.1),
the LBP algorithm (in 5.2), and the definition of
its functions for our task (in 5.3 and 5.4).
5.1 Definition of the Entity Graph
We define a gfbf entity graph EG = {N,E},
in which the node set N consists of nodes, each
representing an annotated noun phrase agent or
object span. The edge set E consists of edges,
each linking two nodes if they co-occur in a triple
chain with each other. Consider the triples of
EX(2) in Section 3 below.
E
1
: ?reform, lower, costs?
E
2
: ?reform, prohibit, E
3
?
E
3
: ?companies, overcharge, patients?
E
4
: ?Obama, support, reform?
The node of reform is linked to nodes of costs via
E
1
and Obama via E
4
.
3
Note that, for E
2
and
E
3
, the two are linked in a chain: ?reform, pro-
hibit, ?companies, overcharge, patients? ?. The
three nodes reform, companies and patients partic-
ipate in this triple chain; thus, pairwise edges ex-
ist among them. The edge linking companies and
patients is BADFOR (because of overcharging).
The edge linking reform and companies is also a
BADFOR since we treat a REVERSER as BADFOR.
3
This assumes that the two instances of ?reform? co-refer.
However, the system does not resolve co-reference ? the
methods that we tried did not improve overall performance.
The edge linking reform and patients encodes two
BADFOR events (prohibit-overcharge); computa-
tionally we say two BADFORs result in a GOOD-
FOR, so the edge linking the two is GOODFOR.
4
Given a text, we get the spans of gfbf events
and their agents and objects plus the polarities of
the events (GOODFOR/BADFOR) from the manual
annotations, and then build the graph upon them.
However, the manual annotations of the writer?s
sentiments toward the agents and objects are used
as the gold standard for evaluation.
5.2 Sentiment Inference via LBP
initialize all m
i?j
(pos) = m
i?j
(neg) = 1
repeat
foreach n
i
? N do
foreach n
j
? Neighbor(n
i
) do
foreach y ? pos, neg do
calculate m
i?j
(y)
normalize m
i?j
(pos) + m
i?j
(neg) = 1
until all m
i?j
stop changing;
for each n
i
? N assign its polarity as
argmax
y?pos,neg
?
i
(y) ?
?
n
k
?Neighbor(n
i
)
m
k?i
(y)
neutral, in case of a tie
Table 1: Loopy Belief Propagation
With graph EG containing cycles and no appar-
ent structure, we utilize an approximate collective
classification algorithm, loopy belief propagation
(LBP) (Pearl, 1982; Yedidia et al., 2005), to clas-
sify nodes through belief message passing. The
algorithm is shown in Table 1.
In LBP, each node has a score, ?
i
(y), and each
edge has a score, ?
ij
(y
i
, y
j
). In our case, ?
i
(y)
represents the writer?s explicit sentiment toward
n
i
. ?
ij
(y
i
, y
j
) is the score on edge e
ij
, represent-
ing the likelihood that node n
i
has polarity y
i
and
n
j
has polarity y
j
. The specific definitions of the
two functions are given in Sections 5.3 and 5.4.
LBP is an iterative message passing algorithm.
A message from n
i
to n
j
over edge e
ij
has
two values: m
i?j
(pos) is how much information
from node n
i
indicates node n
j
is positive, and
m
i?j
(neg) is how much information from node
n
i
indicates node n
j
is negative. In each iteration,
the two are normalized such that m
i?j
(pos) +
m
i?j
(neg) = 1. The message from n
i
to its
4
Also, GOODFOR+BADFOR=BADFOR; GOOD-
FOR+GOODFOR=GOODFOR
380
neighbor n
j
is computed as:
m
i?j
(pos) =
?
ij
(pos, pos)??
i
(pos)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(pos)+
?
ij
(neg, pos)??
i
(neg)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(neg)
(1)
m
i?j
(neg) =
?
ij
(neg, neg)??
i
(neg)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(neg)+
?
ij
(pos, neg)??
i
(pos)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(pos)
(2)
For example, the first part of Equation (1)
means that the positive message n
i
conveys to
n
j
(i.e., m
i?j
(pos)) comes from n
i
being pos-
itive itself (?
i
(pos)), the likelihood of edge e
ij
with its nodes n
i
being positive and n
j
being
positive (?
ij
(pos, pos)), and the positive mes-
sage n
i
?s neighbors (besides n
j
) convey to it
(
?
k?Neighbor(n
i
)/n
j
m
k?i
(pos)).
After convergence, the polarity of each node is
determined by its explicit sentiment and the mes-
sages its neighbors convey to it, as shown at the
end of the algorithm in Table 1.
By this method, we take into account both sen-
timents and the interactions between entities via
gfbf events in order to discover implicit attitudes.
Note that the node and edge scores are deter-
mined initially and do not change. Only m
i?j
changes from iteration to iteration.
5.3 ?
ij
(y
i
, y
j
): GFBF Implicature Relations
The score ?
i,j
encodes constraints based on the
gfbf relationships that nodes n
i
and n
j
participate
in, together with the implicature rules given above.
Rule schemas 1 and 3 infer sentiments to-
ward entities (agent/object) from sentiments to-
ward gfbf events. All cases covered by them are
shown in Table 2 (use s(?) to represent sent(?)).
Rule 3 Rule1
s(gfbf) gfbf type ? s(agent) s(object)
pos GOODFOR ? pos pos
neg GOODFOR ? neg neg
pos BADFOR ? pos neg
neg BADFOR ? neg pos
Table 2: Rule 1 & Rule 3
A table of Rule schemas 2 and 4 would be
exactly the same, except that the inference (?)
would be in the opposite direction (?).
From Table 2, we see that, regardless of the
writer?s sentiment toward the event, if the event
is GOODFOR, then the writer?s sentiment toward
the agent and object are the same, while if the
event is BADFOR, the writer?s sentiment toward
the agent and object are opposite. Thus, the event
type and the writer?s sentiments toward the agents
and objects give us constraints. Therefore, we de-
fine ?
ij
(pos, pos) and ?
ij
(neg, neg) to be 1 if the
two nodes are linked by a GOODFOR edge; oth-
erwise, it is 0; and we define ?
ij
(neg, pos) and
?
ij
(pos, neg) to be 1 if the two nodes are linked
by a BADFOR edge; otherwise, it is 0.
5.4 ?
i
(y): Explicit Sentiment Classifier
The score of a node, ?
i
(y), represents the sen-
timent explicitly expressed by the writer toward
that entity in the document. Since y ranges over
(pos, neg), each node has a positive and a nega-
tive score; the scores sum to 1. If it is a positive
node, then its positive value ranges from 0.5 to 1,
and its negative value ranges from 0 to 0.5 (sim-
ilarly for negative nodes). For any node without
explicit sentiment, both the positive and negative
values are 0.5, indicating a neutral node.
Thus, we build a sentiment classifier that takes a
node as input and outputs a positive and a negative
score. It is built from widely-used, freely available
resources: the OpinionFinder (Wilson et al., 2005)
and General Inquirer (Stone et al., 1966) lexicons
and the OpinionFinder system.
5
We also use a new
Opinion Extraction system (Johansson and Mos-
chitti, 2013) that shows better performance than
previous work on fine-grained sentiment analy-
sis,
6
and a new automatically developed connota-
tion lexicon (Feng et al., 2013).
7
We implement a weighted voting method
among these various sentiment resources. After
that, for nodes that have not yet been assigned po-
lar values (positive or negative), we implement a
simple local discourse heuristic to try to assign
them polar values.
The particular strategies were chosen based
only on a separate development set, which is not
5
http://mpqa.cs.pitt.edu and
http://www.wjh.harvard.edu/ inquirer/
6
As evaluated on the MPQA corpus. Note that the authors
ran their system for us on the data we use.
7
http://www.cs.stonybrook.edu/?ychoi/connotation
381
included in the data used in the experiments.
5.4.1 Explicit Sentiment Tools
Opinion Extraction outputs a polarity expression
with its source, and OpinionFinder outputs a po-
larity word. But neither of the tools extracts the
target. To extract the target, for each word in the
opinion expression, we select other words in the
sentence which are in a mod, obj dependency pars-
ing relation with it.
We match up the extracted expressions and the
gfbf annotations according to their offsets in the
text. For an opinion expression appearing in the
sentence with no gfbf annotation, if the root word
(in the dependency parse) of the expression span
is the same as the root word of a gfbf span, or the
root word of an agent span, or the root word of an
object span, we assume they match up. Then we
assign polarity as follows. If the expression refers
only to the agent or object, then the agent or object
is assigned the polarity of the expression. If the
expression covers the gfbf event and its object, we
assume the sentiment is toward the gfbf event and
then assign sentiment according to Rule schema 1
(sent(gfbf event)? sent(object)).
5.4.2 Lexicons
To classify the sentiment expressed within the
span of an agent or object, we check whether the
words in the span appear in one or more of the
lexicons.
8
If a lexicon finds both positive and neg-
ative words in the span, we resolve the conflict
by choosing the polarity of the root word in the
span. If the root word does not have a polar value,
we choose the majority polarity of the sentiment
words. If there are an equal number of positive
and negative words, the polarity is neutral.
5.4.3 Voting Scheme among Resources
All together we have two sentiment systems and
three lexicons. Before explicit sentiment classi-
fying, each node has a positive value of 0.5 and
a negative value of 0.5. We give the five votes
equal weight (0.1), and add the number of posi-
tive votes multiplied by 0.1 to the positive value,
and the number of negative votes multiplied by 0.1
to the negative value. After this addition, both val-
ues are in the range 0.5 to 1. If the positive value
is larger, we maintain the positive value and assign
8
The comparison is done after lemmatization, using the
wordNet lemmatization in NLTK, and with the same POS,
according to the Stanford POStagger toolkit.
the negative value to be 1-positive value (similarly
if the negative value is larger).
5.4.4 Discourse
For a sentence s, we assume the writer?s senti-
ments toward the gfbf events in the clauses of s,
the previous sentence, and the next sentence, are
the same. Consider EX(3):
EX(3) ... health-insurance regulations that will prohibit (a)
denying coverage for pre-existing conditions, (b) dropping
coverage if the client gets sick, and (c) capping insurance
company reimbursement...
EX(3) has three clauses, (a)-(c). Suppose the ex-
plicit sentiment classifier recognizes that event (a),
denying coverage for pre-existing conditions, is
negative and it does not find any other explicit sen-
timents in the sentence. The system assumes the
writer?s sentiments toward (b) and (c) are negative
as well.
After assigning all possible polarities to events
within a sentence, polarities are propagated to the
other still-neutral gfbf events in the previous and
next sentences.
Finally, event-level polarities are propagated to
still-neutral objects using Rule schema 1.
9
If there
is a conflict, we take the majority sentiment; if
there is a tie, the object remains neutral.
However, the confidence of the discourse voting
is smaller than the explicit sentiment voting, since
discourse structure is complex. If by discourse an
object node is classified as positive, the positive
value is 0.5 + random(0, 0.1) and the negative
value is 1-positive value. Thus, the positive value
of a positive node is larger than its negative value,
but not exceeding too much (similarly for negative
nodes).
6 Experiments and Results
6.1 Experiment Data
Of the 134 documents in the dataset, 6 were used
as a development set, and 3 do not have any anno-
tation. We use the remaining 125 for experiment.
6.2 Evaluation Metrics
To evaluate the performance of classifying the
writer?s sentiments toward agents and objects, we
9
Note that, in the gfbf entity graph, sentiments can be
propagated from objects to agents, conceptually via Rule
schemas 2 and 3. Thus, here we only classify objects.
382
define three metrics to evaluate performance. For
the entire dataset, accuracy evaluates the percent-
age of nodes that are classified correctly. Preci-
sion and recall are defined to evaluate polar (non-
neutral) classification.
Accuracy =
#node auto=gold
#nodes
(3)
Precision =
#node auto=gold & gold != neutral
#node auto != neutral
(4)
Recall =
#node auto=gold & gold != neutral
#node gold != neutral
(5)
In the equations, auto is the system?s output and
gold is the gold-standard label from annotation.
6.3 Overall Performance
In this section, we evaluate the performance of
the overall system. In 6.5, we evaluate the graph
model itself.
Two baselines are defined. One is assigning
the majority class label, which is positive, to all
agents/objects (Majority(+)). The second is as-
suming that agents/objects in a GOODFOR relation
are positive and agents/objects in a BADFOR rela-
tion are negative (GFBF ). In addition, we eval-
uate the explicit sentiment classifier introduced in
Section 5.4 (Explicit). The results are shown in
Table 3.
Accuracy Precision Recall
Majority(+) 0.5438 0.5621 0.5443
GFBF 0.5437 0.5523 0.5444
Explicit 0.3703 0.5698 0.3703
Graph-LBP 0.5412 0.6660 0.5419
Table 3: Performance of baselines and graph.
As can be seen,Majority andGFBF give ap-
proximately 56% precision. Explicit sentiment
classification alone performs hardly better in pre-
cision and much lower in recall. As mentioned
in Section 2, fine-grained sentiment analysis is
still very difficult for NLP systems. However, the
graph model improves greatly over Explicit in
both precision and recall. While recall of the graph
model is comparable to the Majority, precision
is much higher.
During the experiment, if the LBP does not con-
verge until 100 iterations, it is forced to stop. The
average number of iteration is 34.192.
6.4 Error Analysis
Table 4 shows the results of an error analysis to
determine what contributes to the graph model?s
errors.
1 wrong sentiment from voting 0.2132
2 wrong sentiment from discourse 0.0462
3 subgraph with wrong polarity 0.3189
4 subgraph with no polarity 0.4160
5 other 0.0056
Table 4: Errors for graph model.
Rows 1-2 are the error sources for nodes as-
signed a polar value before graph propagation.
Row 1 errors are due to the sentiment-voting sys-
tem, Row 2 are due to discourse processing.
Rows 3-4 are the error sources for nodes that
have not been assigned a polar value by Explicit.
Such a node receives a polar value only via prop-
agation from other nodes in its subgraph (i.e., the
connected component of the graph containing the
node). Row 5 is the percentage of other errors.
As shown in Rows 1-2, 25.94% of the errors
are due to Explicit. These may propagate incor-
rect labels to other nodes in the graph. As shown
in Row 3, 31.89% of the errors are due to nodes
not classified polar by Explicit, but given incor-
rect values because their subgraph has an incorrect
polarity. Row 4 shows that 41.60% of the errors
are due to nodes that are not assigned any polar
value. Given non-ideal input from sentiment anal-
ysis, how does the graph model increase precision
by 10 percentage points?
There are two main ways. For nodes which re-
main neutral after Explicit, they might be clas-
sified correctly via the graph. For nodes which
are given incorrect polar labels by Explicit, they
might be fixed by the graph. Table 5 shows the
best the graph model could do, given the noisy in-
put from Explicit. Over all of the nodes, more
propagated labels are incorrect than correct. How-
ever, if there are no incorrect, or more correct than
incorrect sentiments in the subgraph (connected
component), then many more of the propagated la-
bels are correct than incorrect. In all cases, more
of the changed labels are correct than incorrect.
6.5 Consistency and Isolated Performance of
Graph Model
The implicature rules are defeasible. In this sec-
tion we introduce an experiment to valid the con-
383
propagated propagated changed changed
label correct label incorrect correctly incorrectly
all subgraphs
399 536 424 274
subgraphs having no incorrect sentiment
347 41 260 23
subgraphs having more correct than incorrect sentiment
356 42 288 35
Table 5: Effects of graph model given Explicit
input
sistency of implicature rule. Recall that in Section
5.3, the definition of ?
i,j
is based on implicature
rules and sentiment is propagated based on ?
i,j
.
Thus, this is also an evaluation of the performance
of the graph model itself. We performed an experi-
ment to assess the chance of a node being correctly
classified only via the graph.
In each subgraph (connected component), we
assign one of the nodes in the subgraph with its
gold-standard polarity. Then we run LBP on the
subgraph and record whether the other nodes in
the subgraph are classified correctly or not. The
experiment is run on the subgraph |S| times, where
|S| is the number of nodes in the subgraph, so
that each node is assigned its gold-standard polar-
ity exactly once. Each node is given a propagated
value |S| ? 1 times, as each of the other nodes in
its subgraph receives its gold-standard polarity.
To evaluate the chance of a node given a correct
propagated label, we use Equations (6) and (7).
correct(a|b) =
{
1 a is correct
0 otherwise
(6)
correctness(a) =
?
b?S
a
,b6=a
correct(a|b)
|S
a
| ? 1
(7)
where S
a
is the set of nodes in a?s subgraph. Given
b being assigned its gold-standard polarity, if a is
classified correctly, then correct(a|b) is 1; other-
wise 0. |S
a
| is the number of nodes in a?s sub-
graph. correctness(a) is the percentage of as-
signments to a that are correct. If it is 1, then a
is correctly classified given the correct classifica-
tion of any single node in its subgraph.
For example, suppose there are three nodes in
a subgraph, A, B and C. For A we (1) as-
sign B its gold label and carry out propagation
on the subgraph, (2) assign C its gold label and
carry out propagation again, then (3) calculate
correctness(A). Then the same process is re-
peated for B and C.
Some subgraphs contain only two nodes, the
agent and the object. In this case, graph propa-
gation corresponds to single applications of two
implicature rules. Other subgraphs contain more
nodes. Two results are shown in Table 6. One is
the result on the whole experiment data, the other
is the result for all nodes whose subgraphs have
more than two nodes.
Dataset # subgraph correctness
all subgraphs 983 0.8874
multi-node subgraphs 169 0.9030
Table 6: Performance of graph model itself.
As we can see, a node has an 89% chance of
being correct if there is one correct explicit sub-
jectivity node in its subgraph. If we only consider
subgraphs with more than two nodes, the correct-
ness chance is higher. The results indicate that, if
given correct sentiments, the graph model will as-
sign the unknown nodes with correct labels 90% of
the time. Further, the results indicate that the im-
plicature rules are consistent for most of the times
across the corpus.
7 Conclusions
We developed a graph-based model based on
implicature rules to propagate sentiments among
entities. The model improves over explicit
sentiment classification by 10 points in precision
and, in an evaluation of the model itself, we find
it has an 89% chance of propagating sentiments
correctly. An important question for future work
is under what conditions do the implicatures
not go through in context. Two cases we have
discovered involve Rule schema 3: the inference
toward the agent is defeated if the action was
accidental or if the agent was forced to perform it.
We are investigating lexical clues for recognizing
such cases.
Acknowledgments. This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank
Richard Johansson and Alessandro Moschitti for
running their Opinion Extraction systems on our
data.
384
References
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 793?801, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude us-
ing textual latent semantics. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 65?69, Jeju Island, Korea, July. Association
for Computational Linguistics.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive and malefactive event and writer
attitude annotation. In 51st Annual Meeting of the
Association for Computational Linguistics (ACL-
2013, short paper).
Song Feng, Jun Sak Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Sofia, Bulgaria, Angust. Association for Com-
putational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2012. A
computational model for plot units. Computational
Intelligence, pages 466?488.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Karo Moilanen, Stephen Pulman, and Yue Zhang.
2010. Packed feelings and ordered sentiments:
Sentiment parsing with quasi-compositional polar-
ity sequencing and compression. In Proceedings of
the 1st Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA 2010),
pages 36?43.
J. Pearl. 1982. Reverend bayes on inference engines:
A distributed hierarchical approach. In Proceedings
of the American Association of Artificial Intelligence
National Conference on AI, pages 133?136, Pitts-
burgh, PA.
Kevin Reschke and Pranav Anand. 2011. Extracting
contextual evaluativity. In Proceedings of the Ninth
International Conference on Computational Seman-
tics, IWCS ?11, pages 370?374, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 704?714, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
P.J. Stone, D.C. Dunphy, M.S. Smith, and D.M.
Ogilvie. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press, Cam-
bridge.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment
analysis incorporating social networks. In Proceed-
ings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 1397?1405. ACM.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming zhou,
and Ming Zhang. 2011. Topic sentiment anaylsis in
twitter: A graph-based hashtag sentiment classifica-
tion appraoch. In CIKM, pages 1031?1040.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language ann. Language Resources and
Evaluation, 39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP, pages
347?354.
Jonathan S Yedidia, William T Freeman, and Yair
Weiss. 2005. Constructing free-energy approx-
imations and generalized belief propagation algo-
rithms. Information Theory, IEEE Transactions on,
51(7):2282?2312.
Lei Zhang and Bing Liu. 2011. Identifying noun prod-
uct features that imply opinions. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 575?580, Portland, Oregon, USA, June.
Association for Computational Linguistics.
F. Z?u?niga and S. Kittil?a. 2010. Introduction. In
F. Z?u?niga and S. Kittil?a, editors, Benefactives and
malefactives, Typological studies in language. J.
Benjamins Publishing Company.
385
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 120?125,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Benefactive/Malefactive Event and Writer Attitude Annotation
Lingjia Deng ?, Yoonjung Choi ?, Janyce Wiebe ??
? Intelligent System Program, University of Pittsburgh
? Department of Computer Science, University of Pittsburgh
?lid29@pitt.edu, ?{yjchoi,wiebe}@cs.pitt.edu
Abstract
This paper presents an annotation scheme
for events that negatively or positively
affect entities (benefactive/malefactive
events) and for the attitude of the writer
toward their agents and objects. Work on
opinion and sentiment tends to focus on
explicit expressions of opinions. However,
many attitudes are conveyed implicitly,
and benefactive/malefactive events are
important for inferring implicit attitudes.
We describe an annotation scheme and
give the results of an inter-annotator
agreement study. The annotated corpus is
available online.
1 Introduction
Work in NLP on opinion mining and sentiment
analysis tends to focus on explicit expressions of
opinions. Consider, however, the following sen-
tence from the MPQA corpus (Wiebe et al, 2005)
discussed by (Wilson and Wiebe, 2005):
(1) I think people are happy because
Chavez has fallen.
The explicit sentiment expression, happy, is pos-
itive. Yet (according to the writer), the people
are negative toward Chavez. As noted by (Wil-
son and Wiebe, 2005), the attitude toward Chavez
is inferred from the explicit sentiment toward the
event. An opinion-mining system that recognizes
only explicit sentiments would not be able to per-
ceive the negative attitude toward Chavez con-
veyed in (1). Such inferences must be addressed
for NLP systems to be able to recognize the full
range of opinions conveyed in language.
The inferences arise from interactions be-
tween sentiment expressions and events such as
fallen, which negatively affect entities (malefac-
tive events), and events such as help, which pos-
itively affect entities (benefactive events). While
some corpora have been annotated for explicit
opinion expressions (for example, (Kessler et
al., 2010; Wiebe et al, 2005)), there isn?t a
previously published corpus annotated for bene-
factive/malefactive events. While (Anand and
Reschke, 2010) conducted a related annotation
study, their data are artificially constructed sen-
tences incorporating event predicates from a fixed
list, and their annotations are of the writer?s
attitude toward the events. The scheme pre-
sented here is the first scheme for annotating, in
naturally-occurring text, benefactive/malefactive
events themselves as well as the writer?s attitude
toward the agents and objects of those events.
2 Overview
For ease of communication, we use the terms
goodFor and badFor for benefactive and malefac-
tive events, respectively, and use the abbreviation
gfbf for an event that is one or the other. There are
many varieties of gfbf events, including destruc-
tion (as in kill Bill, which is bad for Bill), cre-
ation (as in bake a cake, which is good for the
cake), gain or loss (as in increasing costs, which
is good for the costs), and benefit or injury (as in
comforted the child, which is good for the child)
(Anand and Reschke, 2010).
The scheme targets clear cases of gfbf events.
The event must be representable as a triple of con-
tiguous text spans, ?agent, gfbf, object?. The
agent must be a noun phrase, or it may be implicit
(as in the constituent will be destroyed). The ob-
ject must be a noun phrase.
120
Another component of the scheme is the influ-
encer, a word whose effect is to either retain or
reverse the polarity of a gfbf event. For example:
(2) Luckily Bill didn?t kill him.
(3) The reform prevented companies
from hurting patients.
(4) John helped Mary to save Bill.
In (2) and (3), didn?t and prevented, respectively,
reverse the polarity from badFor to goodFor (not
killing Bill is good for Bill; preventing companies
from hurting patients is good for the patients). In
(4), helped is an influencer which retains the polar-
ity (i.e., helping Mary to save Bill is good for Bill).
Examples (3) and (4) illustrate the case where an
influencer introduces an additional agent (reform
in (3) and John in (4)).
The agent of an influencer must be a noun
phrase or implicit. The object must be another in-
fluencer or a gfbf event.
Note that, semantically, an influencer can be
seen as good for or bad for its object. A reverser
influencer makes its object irrealis (i.e., not hap-
pen). Thus, it is bad for it. In (3), for example,
prevent is bad for the hurting event. A retainer in-
fluencer maintains its object, and thus is good for
it. In (4), for example, helped maintains the sav-
ing event. For this reason, influencers and gfbf
events are sometimes combined in the evaluations
presented below (see Section 4.2).
Finally, the annotators are asked to mark the
writer?s attitude towards the agents of the influ-
encers and gfbf events and the objects of the gfbf
events. For example:
(5) GOP Attack on Reform Is a Fight
Against Justice.
(6) Jettison any reference to end-of-life
counselling.
In (5), there are two badFor events: ?GOP, Attack
on, Reform? and ?GOP Attack on Reform,Fight
Against, Justice?. The writer?s attitude toward
both agents is negative, and his or her attitude
toward both objects is positive. In (6), the
writer conveys a negative attitude toward end-of-
life counselling. The coding manual instructs the
annotators to consider whether an attitude of the
writer is communicated or revealed in the particu-
lar sentence which contains the gfbf event.
3 Annotation Scheme
There are four types of annotations: gfbf event,
influencer, agent, and object. For gfbf events, the
agent, object, and polarity (goodFor or badFor) are
identified. For influencers, the agent, object and
effect (reverse or retain) are identified. For agents
and objects, the writer?s attitude is marked (posi-
tive, negative, or none). The annotator links agents
and objects to their gfbf and influencer annotations
via explicit IDs. When an agent is not mentioned
explicitly, the annotator should indicate that it is
implicit. For any span the annotator is not certain
about, he or she can set the uncertain option to be
true.
The annotation manual includes guidelines to
help clarify which events should be annotated.
Though it often is, the gfbf span need not be a
verb or verb phrase. We saw an example above,
namely (5). Even though attack on and fight
against are not verbs, we still mark them because
they represent events that are bad for the object.
Note that, Goyal et al (2012) present a method for
automatically generating a lexicon of what they
call patient polarity verbs. Such verbs correspond
to gfbf events, except that gfbf events are, concep-
tually, events, not verbs, and gfbf spans are not
limited to verbs (as just noted).
Recall from Section 2 that annotators should
only mark gfbf events that may be represented as a
triple, ?agent,gfbf,object?. The relationship should
be perceptible by looking only at the spans in the
triple. If, for example, another argument of the
verb is needed to perceive the relationship, the an-
notators should not mark that event.
(7) His uncle left him a massive amount
of debt.
(8) His uncle left him a treasure.
There is no way to break these sentences into
triples that follow our rules. ?His uncle, left, him?
doesn?t work because we cannot perceive the po-
larity looking only at the triple; the polarity de-
pends on what his uncle left him. ?His uncle, left
him, a massive amount of debt? isn?t correct: the
event is not bad for the debt, it is bad for him. Fi-
nally, ?His uncle, left him a massive amount of
debt, Null? isn?t correct, since no object is iden-
tified.
Note that him in (7) and (8) are both consid-
ered benefactive semantic roles (Zu?n?iga and Kit-
tila?, 2010). In general, gfbf objects are not equiva-
121
lent to benefactive/malefactive semantic roles. For
example, in our scheme, (7) is a badFor event and
(8) is a goodFor event, while him fills the benefac-
tive semantic role in both. Further, according to
(Zu?n?iga and Kittila?, 2010), me is the filler of the
benefactive role in She baked a cake for me. Yet,
in our scheme, a cake is the object of the good-
For event; me is not included in the annotations.
The objects of gfbf events are what (Zu?n?iga and
Kittila?, 2010) refer to as the primary targets of the
events, whereas, they state, beneficiary semantic
roles are typically optional arguments. The reason
we annotate only the primary objects (and agents)
is that the clear cases of attitude implicatures mo-
tivating this work (see Section 1) are inferences
toward agents and primary objects of gfbf events.
Turning to influencers, there may be chains of
them, where the ultimate polarity and agent must
be determined compositionally. For example, the
structure of Jack stopped Mary from trying to kill
Bill is a reverser influencer (stopped) whose object
is a retainer influencer (trying) whose object is, in
turn, a badFor event (kill). The ultimate polarity of
this event is goodFor and the ?highest level? agent
is Jack. In our scheme, all such chains of lengthN
are treated as N ? 1 influencers followed by a sin-
gle gfbf event. It will be up to an automatic system
to calculate the ultimate polarity and agent using
rules such as those presented in, e.g., (Moilanen
and Pulman, 2007; Neviarouskaya et al, 2010).
To save some effort, the annotators are not
asked to mark retainer influencers which do not in-
troduce new agents. For example, for Jack stopped
trying to kill Bill, there is no need to mark ?trying.?
Of course, all reverser influencers must be marked.
4 Agreement Study
To validate the reliability of the annotation
scheme, we conducted an agreement study. In this
section we introduce how we designed the agree-
ment study, present the evaluation method and
give the agreement results. Besides, we conduct
a second-step consensus study to further analyze
the disagreement.
4.1 Data and Agreement Study Design
For this study, we want to use data that is rich in
opinions and implicatures. Thus we used the cor-
pus from (Conrad et al, 2012), which consists of
134 documents from blogs and editorials about a
controversial topic, ?the Affordable Care Act?.
To measure agreement on various aspects of
the annotation scheme, two annotators, who are
co-authors, participated in the agreement study;
one of the two wasn?t involved in developing the
scheme. The new annotator first read the anno-
tation manual and discussed it with the first an-
notator. Then, the annotators labelled 6 docu-
ments and discussed their disagreements to recon-
cile their differences. For the formal agreement
study, we randomly selected 15 documents, which
have a total of 725 sentences. These documents do
not contain any examples in the manual, and they
are different from the documents discussed during
training. The annotators then independently anno-
tated the 15 selected documents.
4.2 Agreement Study Evaluation
We annotate four types of items (gfbf event, influ-
encer, agent, and object) and their corresponding
attributes. As noted above in Section 2, influencers
can also be viewed as gfbf events. Also, the two
may be combined together in chains. Thus, we
measure agreement for gfbf and influencer spans
together, treating them as one type. Then we
choose the subset of gfbf and influencer annota-
tions that both annotators identified, and measure
agreement on the corresponding agents and ob-
jects.
Sometimes the annotations differ even though
the annotators recognize the same gfbf event.
Consider the following sentence:
(9) Obama helped reform curb costs.
Suppose the annotations given by the annotators
were:
Ann 1. ?Obama, helped, curb?
?reform, curb, costs?
Ann 2. ?Obama, helped, reform?
The two annotators do agree on the ?Obama,
helped, reform? triple, the first one marking helped
as a retainer and the other marking it as a goodFor
event. To take such cases into consideration in our
evaluation of agreement, if two spans overlap and
one is marked as gfbf and the other as influencer,
we use the following rules to match up their agents
and objects:
? for a gfbf event, consider its agent and object
as annotated;
122
? for an influencer, assign the agent of the in-
fluencer?s object to be the influencer?s object,
and consider its agent as annotated and the
newly-assigned object. In (9), Ann 2?s anno-
tations remain the same and Ann 1?s become
?Obama, helped, reform? and ?reform, curb,
costs?.
We use the same measurement for agreement
for all types of spans. Suppose A is a set of an-
notations of a particular type and B is the set of
annotations of the same type from the other anno-
tator. For any text span a ? A and b ? B, the span
coverage c measures the overlap between a and b.
Two measures of c are adopted here.
Binary: As in (Wilson and Wiebe, 2003), if two
spans a and b overlap, the pair is counted as 1,
otherwise 0.
c1(a, b) = 1 if |a ? b| > 0
Numerical: (Johansson and Moschitti, 2013)
propose, for the pairs that are counted as 1 by c1, a
measure of the percentage of overlapping tokens,
c2(a, b) =
|a ? b|
|b|
where |a| is the number of tokens in span a, and ?
gives the tokens that two spans have in common.
As (Breck et al, 2007) point out, c2 avoids the
problem of c1, namely that c1 does not penalize a
span covering the whole sentence, so it potentially
inflates the results.
Following (Wilson and Wiebe, 2003), treat-
ing each set A and B in turn as the gold-
standard, we calculate the average F-measure, de-
noted agr(A,B). agr(A,B) is calculated twice,
once with c = c1 and once with c = c2.
match(A,B) =
?
a?A,b?B,
|a?b|>0
c(a, b)
agr(A||B) = match(A,B)|B|
agr(A,B) = agr(A||B) + agr(B||A)2
Now that we have the sets of annotations on
which the annotators agree, we use ? (Artstein
and Poesio, 2008) to measure agreement for the
attributes. We report two ? values: one for the
polarities of the gfbf events, together with the ef-
fects of the influencers, and one for the writer?s
gfbf & agent object
influencer
all anno- c1 0.70 0.92 1.00
tations c2 0.69 0.87 0.97
only c1 0.75 0.92 1.00
certain c2 0.72 0.87 0.98
consensus c1 0.85 0.93 0.99
study c2 0.81 0.88 0.98
Table 1: Span overlapping agreement agr(A,B)
in agreement study and consensus study.
polarity & effect attitude
all 0.97 0.89
certain 0.97 0.89
Table 2: ? for attribute agreement.
attitude toward the agents and objects. Note that,
as in Example (9), sometimes one annotator marks
a span as gfbf and the other marks it as an influ-
encer; in such cases we regard retain and goodfor
as the same attribute value and reverse and badfor
as the same value. Table 1 gives the agr values
and Table 2 gives the ? values.
4.3 Agreement Study Results
Recall that the annotator could choose whether
(s)he is certain about the annotation. Thus, we
evaluate two sets: all annotations and only those
annotations that both annotators are certain about.
The results are shown in the top four rows in Table
1.
The results for agents and objects in Table 1 are
all quite good, indicating that, given a gfbf or in-
fluencer, the annotators are able to correctly iden-
tify the agent and object.
Table 1 also shows that results are not signifi-
cantly worse when measured using c2 rather than
c1. This suggests that, in general, the annotators
have good agreement concerning the boundaries
of spans.
Table 2 shows that the ? values are high for both
sets of attributes.
4.4 Consensus Analysis
Following (Medlock and Briscoe, 2007), we ex-
amined what percentage of disagreement is due to
negligence on behalf of one or the other annota-
tor (i.e., cases of clear gfbfs or influencers that
were missed), though we conducted our consensus
123
study in a more independent manner than face-to-
face discussion between the annotators. For anno-
tator Ann1, we highlighted sentences for which
only Ann2 marked a gfbf event, and gave Ann1?s
annotations back to him or her with the highlights
added on top. For Ann2 we did the same thing.
The annotators reconsidered their highlighted sen-
tences, making any changes they felt they should,
without communicating with each other. There
could be more than one annotation in a highlighted
sentence; the annotators were not told the specific
number.
After re-annotating the highlighted sentences,
we calculate the agreement score for all the an-
notations. As shown in the last two rows in Table
1, the agreement for gfbf and influencer annota-
tions increases quite a bit. Similar to the claim
in (Medlock and Briscoe, 2007), it is reasonable
to conclude that the actual agreement is approx-
imately lower bounded by the initial values and
upper bounded by the consensus values, though,
compared to face-to-face consensus, we provide a
tighter upper bound.
5 Corpus and Examples
Recall from in Section 4.1 that we use the corpus
from (Conrad et al, 2012), which consists of 134
documents with a total of 8,069 sentences from
blogs and editorials about ?the Affordable Care
Act?. There are 1,762 gfbf and influencer annota-
tions. On average, more than 20 percent of the sen-
tences contain a gfbf event or an influencer. Out of
all gfbf and influencer annotations, 40 percent are
annotated as goodFor or retain and 60 percent are
annotated as badFor or reverse. For agents and ob-
jects, 52 percent are annotated as positive and 47
percent as negative. Only 1 percent are annotated
as none, showing that almost all the sentences (in
this corpus of editorials and blogs) which con-
tain gfbf annotations are subjective. The annotated
corpus is available online1.
To illustrate various aspects of the annotation
scheme, in this section we give several examples
from the corpus. In the examples below, words
in square brackets are agents or objects, words in
italics are influencers, and words in boldface are
gfbf events.
1. And [it] will enable [Obama and the
Democrats] - who run Washington - to get
1http://mpqa.cs.pitt.edu/
back to creating [jobs].
(a) Creating is goodFor jobs; the agent is
Obama and the Democrats.
(b) The phrase to get back to is a retainer in-
fluencer. But, the agent span is also Obama
and the Democrats, as the same with the
goodFor, so we don?t have to give an anno-
tation for it.
(c) The phrase enable is a retainer influencer.
Since its agent span is different (namely, it),
we do create an annotation for it.
2. [Repealing [the Affordable Care Act]] would
hurt [families, businesses, and our econ-
omy].
(a) Repealing is a badFor event since it de-
prives the object, the Affordable Care Act, of
its existence. In this case the agent is implicit.
(b) The agent of the badFor event hurt is the
whole phrase Repealing the Affordable Care
Act. Note that the agent span is in fact a noun
phrase (even though it refers to an event).
Thus, it doesn?t break the rule that all agent
gfbf spans should be noun phrases.
3. It is a moral obligation to end this indefensi-
ble neglect of [hard-working Americans].
(a) This example illustrates a gfbf that cen-
ters on a noun (neglect) rather than on a verb.
(b) It also illustrates the case when two words
can be seen as gfbf events: both end and ne-
glect of can be seen as badFor events. Fol-
lowing our specification, they are annotated
as a chain ending in a single gfbf event: end
is an influencer that reverses the polarity of
the badFor event neglect of.
6 Conclusion
Attitude inferences arise from interactions
between sentiment expressions and benefac-
tive/malefactive events. Corpora have been
annotated in the past for explicit sentiment ex-
pressions; this paper fills in a gap by presenting
an annotation scheme for benefactive/malefactive
events and the writer?s attitude toward the agents
and objects of those events. We conducted an
agreement study, the results of which are positive.
Acknowledgement This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank the
anonymous reviewers for their helpful feedback.
124
References
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI?07, pages 2683?
2688, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of the Workshop on
Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, ExProM ?12, pages 80?88,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2012. A
computational model for plot units. Computational
Intelligence, pages no?no.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sen-
timent corpus for the automotive domain. In 4th
Int?l AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2010. Recognition of affect, judg-
ment, and appreciation in text. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10, pages 806?814, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3):164?210.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of the
4th ACL SIGdial Workshop on Discourse and Dia-
logue (SIGdial-03), pages 13?22.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky.
F. Zu?n?iga and S. Kittila?. 2010. Introduction. In
F. Zu?n?iga and S. Kittila?, editors, Benefactives and
malefactives, Typological studies in language. J.
Benjamins Publishing Company.
125
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 8?17,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
An Investigation for Implicatures in Chinese :
Implicatures in Chinese and in English are similar !
Lingjia Deng
Intelligent Systems Program
University of Pittsburgh
lid29@pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
Implicit opinions are commonly seen in
opinion-oriented documents, such as po-
litical editorials. Previous work have uti-
lized opinion inference rules to detect
implicit opinions evoked by events that
positively/negatively affect entities (good-
For/badFor) to improve sentiment analy-
sis for English text. Since people in differ-
ent languages may express implicit opin-
ions in different ways, in this work we in-
vestigate implicit opinions expressed via
goodFor/badFor events in Chinese. The
positive results have provided evidences
that such implicit opinions and inference
rules are similar in Chinese and in English.
Moreover, we have observed cases where
the inferences are blocked.
1 Introduction
In the opinion-oriented documents, many opin-
ions are expressed implicitly rather than explicitly.
Consider the following example from (Deng and
Wiebe, 2014):
EX(1.1) The reform would lower
health care costs, which would be a
tremendous positive change across the
entire health-care system.
There is an explicit positive sentiment (positive)
toward the event of reform lower costs. In express-
ing this sentiment, the writer implies he is nega-
tive toward the costs, because he?s happy to see the
costs being decreased. The writer may be positive
toward reform since it conducts the lower event.
Such inferences may be seen as opinion-oriented
implicatures (i.e., defeasible inferences)
1
.
1
Implicatures ?normally accompany the utterances of a
given sentence unless special factors exclude that possibility
(p. 39).? (Huddleston and Pullum, 2002)
We create an annotated corpus (denoted DCW
corpus) (Deng et al., 2013)
2
and generalizes such
events, defining a badFor (bf) event to be an
event that negatively affects the object and a good-
For (gf) event to be an event that positively af-
fects the object of the event. Here, lower is a
bf event. According to the annotation scheme,
goodFor/badFor (hereafter gfbf ) events have NP
agents and objects (though the agent may be im-
plicit), and the polarity of a gf event may be
changed to bf by a reverser (and vice versa).
We have developed a set of rules for inferring
implicit sentiments, from explicit sentiments and
gfbf events (Deng and Wiebe, 2014). We incor-
porate the rules into a graph-based model, which
significantly improves classifying the sentiments
toward agents and objects in the gfbf events.
The contribution of this work is investigating
implicatures in a second language, specifically in
Chinese. People in different languages may ex-
press implicit opinions in different ways, so it is
better to first assess similarity of implicatures in
the two languages, rather than to directly utilize
the English resources. In this work we conduct an
agreement study for gfbf information in Chinese.
The good agreement scores provide evidence for
the existence of similar implicature in Chinese.
During the analysis of disagreement, we have ob-
served interesting gfbf events triggered by Chinese
syntax, which are rare in English but common in
Chinese. We should provide additional guidance
for such events when developing a Chinese gfbf
manual in the future.
We run the graph-based model on the annotated
Chinese corpus. The good evaluation results sup-
port our hypothesis that the inference rules in En-
glish apply for Chinese. Moreover, we have ob-
served gfbf cases where the sentiment inferences
are blocked, which are similar to what we have
found in English (Wiebe and Deng, 2014).
2
Available at: http://mpqa.cs.pitt.edu/
8
Further, we analyze gfbf words and syntax of
agents/objects in Chinese. Our analysis shows that
it is feasible to extract components of Chinese gfbf
events utilizing the existing resources. In the last
section we briefly talk bout the Chinese explicit
sentiment analysis.
2 Related Work
In addition to researches focusing on explicit sen-
timents (Wiebe et al., 2005; Johansson and Mos-
chitti, 2013; Yang and Cardie, 2013), recently
there are work investigating features that directly
indicate implicit sentiments (Zhang and Liu, 2011;
Feng et al., 2013), or working on inferring implicit
opinions (Choi and Cardie, 2008; Zhang and Liu,
2011; Anand and Reschke, 2010; Reschke and
Anand, 2011; Goyal et al., 2013). Different from
their work, which do not cover all the inferences of
implicit opinions over explicit opinions and gfbf
events, we define a generalized set of inference
rules and incorporate the rules into a graph-based
model to achieve sentiment propagation between
the agents and objects of gfbf events (Deng and
Wiebe, 2014). The result shows that the graph-
based model itself is able to assign the unknown
nodes with correct labels 89% of the time.
Many works in Chinese sentiment analysis de-
velop heuristics for adapting methods in English
to methods appropriate for Chinese (Tsou et al.,
2005; Wang et al., 2007; Li and Sun, 2007). In-
stead of projecting English methods and resources
into Chinese versions, there are also works lever-
aging Chinese-English parallel corpus to assist
Chinese sentiment analysis. Wan (2008) trans-
lates Chinese sentiment sentences into English and
ensemble the sentiment classification results from
both English and Chinese sentiment classifiers.
Wan (2009) adopt co-training methods, utilizing
labeled English sentences and unlabelled Chinese
sentences. Lu et al. (2011) assumes parallel sen-
tences in different languages bear the same sen-
timent. They utilize unlabelled Chinese-English
parallel corpus to jointly improve sentiment clas-
sification in both languages. Boyd-Graber and
Resnik (2010) present a generative model, jointly
modeling topics that are consistent across lan-
guages, to improve sentiment rating predictions.
3 Implicature in Chinese
The definition of a gfbf event is from (Deng et
al., 2013). A goodFor (gf) event is an event that
positively affects an entity (similarly, for badFor
(bf) events). A gfbf triple has the structure of ?
agent, gfbf, object?, though the agent can be im-
plicit. For example, in the sentence from (Deng
et al., 2013), ?Repealing the Affordable Care Act
(ACA) would hurt our economy.?, there are two
gfbf triples. One is ?Repealing the ACA, hurt,
families, our economy?, which is a bf. The other
is ?implicit, Repealing, the ACA?, which is bf and
the agent is implicit. The DCW corpus contains
manually annotated gfbf events, the gfbf polari-
ties, the corresponding agents and objects and the
writer?s attitudes toward the agents and objects.
Because people in different languages may ex-
press their opinions in different ways. In this sec-
tion, we conduct an agreement study for Chinese
gfbf information in Section 3.1 and achieve good
agreement scores, reported in Section 3.2, which
provide supporting evidences for detecting Chi-
nese gfbf events. In the disagreement analysis,
we have observed interesting cases which are gfbf
events in semantics but are triggered by Chinese
own syntax. We explain the cases in Section 3.3.
3.1 Agreement Study Design
Data: We collect 100 political editorials from the
Opinion Column in the Chinese version of New
York Times
3
, where each political editorial has an
English version and a Chinese version. The Chi-
nese editorial is a translated and paraphrased ver-
sion of the corresponding English editorial, writ-
ten by professional translators. The English ver-
sion and the Chinese version are paragraph paral-
leled. In the previous agreement study of (Deng et
al., 2013), the annotators are asked to annotate the
whole document. Because not all the sentences
contain gfbf events and the documents are long,
a large proportion of disagreement we find that is
due to negligence. In order to reduce negligence
and provide a more dense data for annotation, first,
we collect a lexicon of English gfbf words in the
DCW corpus. Then we find the English sentences
containing English gfbf words and select the para-
graphs containing those sentences. The parallel
Chinese paragraphs are collected. Though a para-
graph may contain more than one sentence and
some sentences do not have gfbf events, it is much
more dense to annotate than the document as a
whole. When presenting data to the annotators, we
do not provide an isolated paragraph since it may
3
http://cn.nytimes.com/opinion/
9
lose the context information. Instead, we present
the original Chinese editorials and highlight the
selected paragraphs. The annotators are told to
read through the whole document but only need
to annotate the highlighted paragraphs.
Procedure: We adopt our English manual in
(Deng et al., 2013) to train the annotators. The
annotators read through the manual and several
Chinese gfbf examples. Then, the annotators la-
bel several paragraphs and discuss their disagree-
ments to reconcile their differences. For the for-
mal agreement study, we randomly selected 60
paragraphs, which have a total of 253 Chinese sen-
tences. These paragraphs are different from the
paragraphs discussed during training. The annota-
tors then independently annotated the 60 selected
paragraphs.
3.2 Agreement Study Evaluation and Result
We use the same measurement for agreement for
all types of spans. (The type is either gfbf, agent,
or object). Suppose A is a set of annotations of
a particular type and B is the set of annotations
of the same type from the other annotator. For
any text span a ? A and b ? B, the span cov-
erage c counts the percentage of overlapping Chi-
nese characters between a and b,
c(a, b) =
|a ? b|
|b|
(1)
where |a| is the number of characters in span a,
and ? gives the set of characters that two spans
have in common (Johansson and Moschitti, 2013).
Following (Wilson and Wiebe, 2003), we treat
each set A and B in turn as the gold-standard and
calculate the average F-measure (agr(A,B)).
agr(A||B) =
?
a?A,b?B,
|a?b|>0
c(a, b)
|B|
(2)
agr(A,B) =
agr(A||B) + agr(B||A)
2
(3)
Now that we have the sets of annotations on
which the annotators agree, we use ? (Artstein
and Poesio, 2008) to measure agreement for the at-
tributes. We report three ? values: one for the po-
larities of the gfbf events, and the other two for the
writer?s attitudes toward the agents and objects.
Three annotator participate in the agreement
study. All of them are Chinese graduate students
studying in US. One of them is the co-author
of this work (Anno 1), while the other two do
agr(A,B) gfbf agent object
Anno 1& 2 0.7929 0.9091 0.9091
Anno 1 & 3 0.7044 0.9524 1.0
gfbf agent object
? polarity attitude attitude
Anno 1 & 2 0.9385 0.7830 0.7238
Anno 1 & 3 0.8966 0.5913 0.8478
Table 1: Results for Agreement Study Analysis.
not know details of gfbf and implicature before
(Anno2, Anno3). Since Anno1 is familiar with this
work, we compare the other two?s annotations to
Anno1?s. In Table 1, the upper half is the agree-
ment for span overlapping (agr(A,B)), and the
lower half is the agreement for attribute (?).
The result have shown that the annotators have
good agreement scores, though our training period
is not long and our training data cover multiple
topics. In particular, the annotators agree quite
well on recognizing the agents and objects and
judging the polarity of gfbf events.
For recognizing gfbf events, we have found two
interesting gfbf cases caused by the Chinese syn-
tax that is different from English, elaborated in the
next section. Among the spans only one annota-
tor marks, one third is due to the two cases above;
one third are borderlines that could be marked; one
third are incorrect. For the spans two annotator
mark but the third doesn?t, we regard it as negli-
gence.
For judging the writer?s attitudes toward agents
and objects, we can see from Table 1 that Anno 2
and Anno 3 behave differently. This is understand-
able because we are marking the implicit opinions
of the writer. Though trained, different annotators
have different thresholds for judging whether an
opinion is expressed here. Some annotators may
be more sensitive than the others. If we don?t
count the spans that one annotator marks it as none
(i.e. neutral) but the other doesn?t, the ? scores in-
crease a lot, as Row Polar shows in Table 2. This
indicates that the annotators mainly disagree on
whether the sentiment is neutral or not, rather than
the polarity of opinions.
To further investigate whether the disagreement
is caused by Chinese, or is due to the annotators?
inherent different sensitivities of opinions, we ran-
domly select 5 documents from the DCW corpus,
delete the writer?s attitude toward agents and ob-
jects but keep the remaining annotations. The an-
10
Anno 1 & 2 Anno 1& 3
agent object agent object
Table 1 0.783 0.723 0.591 0.848
Polar 0.875 0.915 1 0.88
Eng 0.738 0.652 0.4633 0.8734
Table 2: ? for Agreement Study Analysis.
notators are then told to mark the attitudes. As
Row Eng in Table 2 shows, we have got consis-
tent agreement results within the same annotators
when they annotate in English and in Chinese.
This supports the idea that the differences between
the annotators are differences on the underlying
task, regardless of the language.
3.3 GoodFor/Badfor Triggered by Chinese
Syntax
During the analysis of disagreement, we have
found gfbf cases which are triggered by the Chi-
nese syntax that is different from English. Since
the annotators are trained by the English manual,
some annotators stay consistent with the English
syntax, but the others go beyond syntax and iden-
tify gfbf according to semantics and pragmatics,
which lead to disagreement. In this section we list
two major cases due to the Chinese own syntax.
This suggests that additional guidance to annotate
such cases should be added to the English manual
to develop a Chinese gfbf manual.
The first case is due to unclear expression of
passive voice in Chinese. In English, the noun
phrase that would be the object of an active sen-
tence (Our troops defeated the enemy) appears as
the subject of a sentence with passive voice (The
enemy was defeated by our troops)
4
. It is clear
that enemy is the object and our troops is the agent
in both sentences. However, this is not intuitive
for some Chinese sentences.
A Chinese example is ? ?????????
? ?, whose English translation is: ?The economic
potential ... appeared to be unleashed?. A word-
to-word translation would be ?...appeared to have
got unleashed?. In the two English versions, po-
tential is obviously the object of unleashed event.
However, some annotators analyze this sentence
according to syntax
5
. The dependency syntax be-
tween the object potential (??) and the gfbf un-
leash (??) is nsubj(??-5, ??-2) so it is not
4
http://en.wikipedia.org/wiki/English passive voice.
5
We use Stanford?s dependency parser in this work.
marked. Some annotators view from pragmatics
and read as a passive voice. Since there is no word
transformation of Chinese verbs for passive voice
(e.g. unleash changes to unleashed in English),
this raises disagreement.
The other case is related to one constraint de-
fined in (Deng et al., 2013). According to the
manual, the polarity of a gfbf triple must be de-
termined within the triple. As explained in the
manual, in the sentence ?Tom has left his cousin
a big trouble?, the triple ?Tom, left, his cousin? is
not a gfbf event, since we cannot judge whether
this event is good for or bad for his cousin without
knowing what Tom leaves to his cousin. While
in the sentence ?They decrease the manufacturing
costs?, the event decrease is a bf no matter how
many or by what means the costs are decreased.
However, a Chinese instance is, ??????
?? ?, whose translation is ?put the reform to
die?. Whether the event put (?) is good for or bad
for the object reform (??), depends on whether
the agent puts the reform to die or puts the reform
to revive, for instance. However, in Chinese, ?
is not main verb (Li and Thompson, 1989), the
object (??, reform) of the main verb (???
?, die) is placed after the function word (?), and
the verb is placed after the object, forming a sub-
ject?object?verb (SOV) sentence (Chao, 1968)
6
,
which is defined as ba structure (Chao, 1968; Li
and Thompson, 1989; Sybesma, 1992). Thus, in
Chinese the sentence is read as: ?kill the reform?,
which could be seen as a gfbf event. This structure
is very common in Chinese.
In conclusion, there are very similar implica-
tures in Chinese. However, in order to fully study
the gfbf events in Chinese, the manual should
be revised to provide guidance for annotating the
cases mentioned above.
4 Implicature Inference in Chinese
We propose a set of sentiment inference rules and
incorporate them into a graph-based model to con-
duct sentiment propagation among entities (agents
and objects) of gfbf events (Deng and Wiebe,
2014). In Section 4.1, we run this graph-based
model on the Chinese annotations. The positive
results of sentiment propagation support our hy-
pothesis that the inference rules apply for Chinese
as well. Further, we categorize interesting gfbf
cases where the inferences are blocked in Section
6
http://en.wikipedia.org/wiki/B%C7%8E construction.
11
4.2. From our observation, the blocking infer-
ences are similar to what we have found in English
(Wiebe and Deng, 2014).
4.1 Graph-based Model
In the graph-based model, a node represents an en-
tity (agent, or object), and an edge exists between
two nodes if the two entities participate in one or
more gfbf events with each other. Scores on the
nodes represent the explicit sentiments, if any, ex-
pressed by the writer toward the entities. Scores
on the edges are based on constraints derived from
the rules. Loopy Belief Propagation (Pearl, 1982;
Yedidia et al., 2005) is applied to accomplish sen-
timent propagation in the graph. Given a graph
built from manually annotations, an evaluation is
carried out to assess the ability to propagate sen-
timent of the model. In the study, for each sub-
graph (connected component), we assign one of
the nodes in the subgraph with its gold-standard
polarity. Then we run LBP on each node in the
subgraph. The experiment is run on the subgraph
|S| times, where |S| is the number of nodes in
the subgraph. Therefore, each node is assigned
its gold-standard polarity exactly once, and each
node is given a propagated value |S| ? 1 times, as
propagated by each of the other nodes in its sub-
graph. We use Equations (4) and (5) to evaluate
the chance of a node given a correct propagated
label.
correct(a|b) =
{
1 a is correct
0 otherwise
(4)
correctness(a) =
?
b?S
a
,b 6=a
correct(a|b)
|S
a
| ? 1
(5)
Here we run the graph-based model on the Chi-
nese annotations. The data we use include the
training and testing paragraphs in the agreement
study, in total 85 paragraphs, 341 sentences and
160 gfbf triples. Later we use this corpus of 160
gfbf triples for analysis (denoted Chinese gfbf cor-
pus). Since the edge scores of the model are de-
fined according to the inference rules, if the senti-
ments are propagated correctly, this is a good evi-
dence that the inference rules apply to Chinese.
The performances of the sentiment propagation
are really good, reported in Table 3. The model
has an 70%-83% chance of propagating senti-
ments correctly in Chinese. This gives us confi-
dence that the inference rules apply in Chinese and
Dataset # subgraph correctness
all subgraphs 136 0.7058
multi-node subgraphs 61 0.8251
Table 3: Performance of Graph-Based Model in
Chinese.
further we can utilize these rules to assist Chinese
sentiment analysis. Compared to the scores of
correctness reported in (Deng and Wiebe, 2014),
which are 0.8874 for all subgraphs and 0.9030 for
multi-node subgraphs, our scores are lower. We
analyze the reasons for the gap between our scores
in Chinese and in English in the next section.
4.2 Blocking the Inference
A wrong propagation indicates the inferences re-
lated to that propagation are blocked. During the
error analysis, we have found three interesting cat-
egories of cases where the inferences are blocked.
Interestingly, we have observed these cases in En-
glish as well (Wiebe and Deng, 2014). In other
words, we didn?t find any blocking case specific to
Chinese. The lower scores of correctness in Chi-
nese might be due to the smaller amount of exper-
iment data and more blocking cases in this corpus.
Irrealis: This category contains gfbf events that
haven?t or will not happen. One of the case is
when the agent tried to conduct the gfbf event,
but failed. In Ex(4.1), the agent and objective are
underlined and the gfbf event is boldfaced. By
the rules, the writer has the same sentiment to-
ward the agents and objects in gf events and op-
posite sentiments toward the agents and objects in
bf events (Deng and Wiebe, 2014). In Ex(4.1), the
writer is negative toward both the agent and the
object, though this is a bf event. This is because
the event counter does not exist due to the failure,
which is implied by intended to. The inferences
for gfbf events in this category are blocked be-
cause the writer expresses the sentiments toward
entities based on what they have done so far.
EX(4.1) ...monetary policy activism in-
tended to counter the cyclical bumps
and grinds of the free market.
Forced GFBF: This category contains gfbf events
whose agents don?t intend to do that or be-
ing forced to conduct the event. For exam-
ple, in Ex(4.2), though the triple ?Obama, delay,
mandate? is an event which does not happen, it
12
is different from Ex(4.1). Here, the agent Obama
is forced to conduct the delaying, though he does
not want to and the writer does not blame him
if he does so. For the entities involved in forced
events, (at least the writer believes the entities are
involuntary,) the forced event will not affect the
writer?s sentiments toward the entities so that the
inferences are blocked.
EX(4.2) Some of them even seem to
think that they can bully Mr. Obama into
delaying the individual mandate too.
Quoted GFBF: This category contains gfbf
events in the quotations. Consider the Ex(4.3),
where one of the gfbf triple is ?law, reduce,
amount of labor ?. In the original editorial, the
writer supports the law and the writer has a posi-
tive sentiment toward the number of jobs (because
he/she expects to see more job opportunities). But
merely from the annotated gfbf triple, it is inferred
that the law has negative effect since it reduces the
number of jobs. This is not contradictory with the
writer?s stance because the writer regards the event
as a deliberate misreading he/she doesn?t believe.
The actual agent of the event should be (misread-
ing, Obama). This example shows that inferences
of a triple in the quotation are blocked, or event
flipped, based on the writer?s sentiment toward the
agent saying the quotation. The agent in a quoted
gfbf is similar to the notion of nested source in
sentiment analysis (Wilson and Wiebe, 2003).
EX(4.3) Some of the job-killer scare
stories are based on a deliberate mis-
reading that estimated the law would
?reduce the amount of labor used in the
economy? by about 800,000 jobs.
In conclusion, the good performance in our pilot
study gives supporting evidence for our hypothe-
sis. That is, the inference rules apply for Chinese.
Moreover, there is no evidence showing that the
cases where the inferences are blocked only hap-
pen in Chinese.
5 Chinese GoodFor/BadFor Lexicon
Above all we have assessed the similarity of im-
plicatures and inference rules in Chinese and En-
glish. In the following sections, we will analyze
whether Chinese gfbf components could be cap-
tured by similar techniques in English.
Description Count (Percentage %)
Parallel Span 122 (76.25%)
Chinese Adding GFBF 10 (6.875%)
Chinese Adding Object 6 (3.75%)
English Out Of Triple 5 (3.125%)
English Neutral 6 (3.125%)
Paraphrase 11 (6.875%)
Table 4: Counts of Chinese-English Corresponds
In this section, we compare the gfbf spans in
the Chinese gfbf corpus and the English version,
to investigate the possibility of deriving a bilingual
gfbf lexicon. Though the Chinese and English ed-
itorials are paragraph paralleled, they are not sen-
tence paralleled, because an English sentence may
be translated into multiple Chinese sentences and
several English sentences may be merged into one
Chinese sentence. Therefore, instead of automatic
word-alignment, we manually pick up the English
parallel spans of the Chinese annotated gfbfs. The
correspondences of Chinese and English spans are
categorized in Table 4. We present pairs of ex-
amples from the Chinese gfbf corpus, beginning
with the original English sentence (Eng), followed
by another English sentence which is the word-by-
word translation of the Chinese sentence (Chi).
Parallel Span: This category contains instances
where the Chinese annotated gfbf spans have
the corresponding translations in the English sen-
tences, and the English spans are also gfbf words.
Chinese Adding GFBF: In the original English
sentence below, its own making is a noun phrase
rather than a gfbf verb used as a noun. However, in
the Chinese version, there is a clear triple, ?itself,
makes, a monetary prison?. In such case the Chi-
nese version adds a gfbf event into the sentence.
Eng: ...the Fed is domiciled in a monetary prison
of its own making.
Chi: ...the Fed is domiciled in a monetary prison
which itself makes.
Chinese Adding Object: As stated in the manual,
all gfbf triples should have objects. Thus, in the
original sentence below, we will not mark exclu-
sion because the object is implicit. However, the
Chinese version clearly states the object, patients.
Eng: ...no more exclusion based on pre-existing
conditions...
Chi: ...no more exclusion of the patients based on
pre-existing conditions...
13
English Out Of Triple: Recall from Section 3.3,
the gfbf polarity must be sufficient to perceive the
gfbf polarity within the triple. The ?the Fed, get,
unemployment? below cannot be considered as a
gfbf, since whether it is good for or bad for the
unemployment depending on whether it is below
6.5% or up 6.5%, for instance. On the contrary,
the Chinese version uses the word decrease, which
is a bf word, no matter how many percents are
changed.
Eng: If and when the Fed ? which now promises
to get unemployment below 6.5%...
Chi: If and when the Fed ? which now promises
to decrease the unemployment to 6.5%...
English Neutral: Sometimes the English word
doesn?t have a gfbf meaning but the Chinese word
has one, based on the translator?s interpretation of
the whole editorial, though the triple structures are
the same in English and Chinese versions.
Eng: We?ve had eight decades of increasingly
frenetic monetary policy activism...
Chi: We?ve been insisting increasingly frenetic
monetary policy activism for eight decades...
In the original English sentence, had eight
decades of is hardly regarded as a gfbf word.
However, in the translated version, the word in-
sisting is a gf word. The change of wording intro-
duces a new gfbf event into the sentence.
Paraphrase: There are other cases where the sen-
tences are paraphrased so largely that we cannot
find a corresponding parallel span of the annotated
Chinese span in the original English sentence. A
majority of cases in this category are gfbf events
triggered by the Chinese syntax in Section 3.3.
In conclusion, the percentage of 76.25% in Row
Parallel Span indicates that it is applicable to de-
rive a bilingual gfbf lexicon from a parallel cor-
pus. However, we need to take into consideration
the 23.75% mismatches for higher precision.
5.1 Chinese Reversers
The polarity of a gfbf event could be changed by
a reverser (Deng et al., 2013). A common class
of reversers is negation. For example, in the sen-
tence, ?the bill will not increase the costs?, the gf
increase is changed to be bf via the negation not.
In this section, we analyze the Chinese reversers.
All of the reversers in the Chinese gfbf corpus
happen to be negations. In the English sentences,
the negations are easily extracted by neg depen-
dency relation. About 50% of the Chinese nega-
tions are linked to the gfbf events via neg as well.
Among this half, there are two negations com-
monly seen. One is ? (Not), often labeled as AD
(adverb) in terms of Part-Of-Speech, the other is
?? (do not have), labeled as VV (verb), shown
below. The negation is underlined and the gfbf
event it negates is boldfaced.
EX(5.1)?/AD??/VV???/NN
EX(5.2)??/VV??/VV??/NN
For the other half, the error mostly arises from
segmentations. For the sentence below, though?
? (doesn?t have), often labeled as VB, could be
regarded as a complete token, if we segment the
two characters into two independent tokens, the
parse is more similar to the English one. Below
we only list the most relevant part of the parses.
Eng: He does n?t have ability control war budget
Eng dep: neg(have-4, n?t-3), root(ROOT-0, have-
4), dobj(have-4, ability-6)
Chi: ? ? ? ?? ?? ?? ??
wrong dep: root(ROOT-0, ??-2), nsubj(??-
4,??-3), dep(??-2,??-4)
correct dep: neg(?-3, ?-2), root(ROOT-0, ?-
3), nsubj(??-5,??-4)
In conclusion, it is feasible to recognize re-
versers in Chinese but it calls for a suitable word
segmentation as input.
6 Syntax of Agent/Object in Chinese
According to (Deng et al., 2013), the agent is the
entity conducting the gfbf event and the object is
the entity that the gfbf event affects. This defini-
tion is very similar to subject and (in)direct ob-
ject in semantic role labeling. Xue and Palmer
(2004) investigate the Chinese semantic role la-
beling. They utilize the PropBank and the con-
stituency parser. However, from a preliminary
analysis of constituency parse, we cannot distin-
guish the agent and object merely from the parse
tree, because the sentences in the editorials are
usually complicated and it is difficult to classify
whether a noun phrase (NP) constituency is agent
or object in terms of its position. Kozhevnikov
and Titov (2013) adopt a model transfer between
different languages using dependency parser. In
our case, the dependency parser has labels such
14
as ?nsubj? and ?dobj?, which are strong indica-
tions of agents and objects. Thus, we use the
Stanford dependency parser, which has both En-
glish and Chinese parsers, to analyze the syntax
of agents/objects in the gfbf events. We count the
types of dependencies on the path in a dependency
parse between the tokens of agents/objects and the
tokens of gfbf events in the DCW corpus and the
Chinese gfbf corpus.
Among all the dependency types, 19.57% of the
labels between agents and gfbfs are the ones spe-
cially designed for Chinese and 25.82% between
objects and gfbf are the ones specially designed
for Chinese. This indicates there is a consider-
able number of differences in dependency types.
Chang et al. (2009), who create the Chinese
parser, discuss the differences between Chinese
and English types, which are similar to our obser-
vations.
First, there are more nsubj in Chinese for
agents (21.53%) and more dobj in Chinese for ob-
jects (21.59%), compared to English (17.43% and
14.01%), which are easier for the parser to detect.
Second, the most common types specially de-
signed for Chinese are assm, assmod and cpm (in
total 12.23% for agents and 16.14% for objects).
The relations assm is associative marker, assmod
is associative modifier, and cpm is complemen-
tizer. These are defined because of the frequent us-
age of ? (whose, of) in Chinese. Though there is
not a direct mapping between Chinese and English
dependency types, they are similar to two common
types in English: prep and pobj (together 23.36%
for agents and 31.62% for objects).
Third, there are more rcmod in Chinese than
those in English. There are 7.05% and 6.5% rc-
mod in Chinese agents and objects, respectively.
But there are only 1.7% and 2.16% in English
agents and objects. The type rcomd is a relative
clause modifier. If a verb is used as the modifier
of a noun, it will be labelled rcmod. Instead, En-
glish writers tend to use more adjectives to mod-
ify nouns, which will be labeled amod (4.04% and
4.48%).
Fourth, there are 7.63% and 6.22% punct in
Chinese agents and object, compared to both 0%
in English. In addition, there are 3.36% and 3.31%
conj in English agents and objects. Chang et
al. (2009) explain that English use conjunctions
(conj) to link clauses while Chinese tend to use
punctuation. Another finding in our corpus is that,
translators tend to break down a long English sen-
tence into several Chinese clauses, linked by punc-
tuations.
For the other Chinese types, most of them are
modifiers, which may be grouped with similar En-
glish modifiers.
7 Chinese Explicit Sentiment Analysis
There are various available resources for Chinese
sentiment analysis, such as sentiment lexicon from
HowNet
7
, NTU Sentiment Dictionary (NTUSD)
(Ku and Chen, 2007)
8
and the sentiment lexi-
con from Tsinghua University (Li and Sun, 2007).
The sentiments recognized from lexicon hits are
explicit, meaning that the writers use sentiment
words to express his/her opinions. These explicit
sentiment results are provided to the graph-based
model as input. Note that the model plays a role
of sentiment inference, instead of directly detect-
ing sentiments from the text. The inferred senti-
ments are implicit, meaning that the writers ex-
press his/her opinions even without using a senti-
ment lexical clue.
8 Conclusion
In this work we investigate implicit opinions
expressed via goodFor/badFor events in Chinese.
The positive results have provided evidences
that such implicit opinions and inference rules
are similar in Chinese and English. There are
some gfbf events caused by the Chinese syntax,
guidance for which could be added to the current
English manual to develop a Chinese manual.
Moreover, there is no evidence showing that the
blocked inferences only happen in Chinese. We
also assess the feasibility of acquiring components
of gfbf events from Chinese text using current
available resources. In the future, it is promising
to utilize gfbf information to assist sentiment
analysis in Chinese.
Acknowledgement This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank
Changsheng Liu and Fan Zhang for their anno-
tations in the agreement study, and thank anony-
mous reviewers for their feedback.
7
Available at: http://www.keenage.com/html/e index.html
8
Available at: http://nlg18.csie.ntu.edu.tw:8080/lwku/pub1.html
15
References
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45?55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59. Association for Computational Linguistics.
Yuen Ren Chao. 1968. A grammar of spoken Chinese.
Univ of California Press.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 793?801, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Meeting
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2014).
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer at-
titude annotation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 120?125,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Sofia, Bulgaria, Angust. Association for Com-
putational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum?e III. 2013. A
computational model for plot units. Computational
Intelligence, 29(3):466?488.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190?1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Lun-Wei Ku and Hsin-Hsi Chen. 2007. Mining
opinions from the web: Beyond relevance retrieval.
Journal of the American Society for Information Sci-
ence and Technology, 58(12):1838?1850.
Jun Li and Maosong Sun. 2007. Experimental
study on sentiment classification of chinese review
using machine learning techniques. In Natural
Language Processing and Knowledge Engineering,
2007. NLP-KE 2007. International Conference on,
pages 393?400. IEEE.
Charles N Li and Sandra A Thompson. 1989. Man-
darin Chinese: A functional reference grammar.
Univ of California Press.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K
Tsou. 2011. Joint bilingual sentiment classifica-
tion with unlabeled parallel corpora. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 320?330. Association
for Computational Linguistics.
J. Pearl. 1982. Reverend bayes on inference engines:
A distributed hierarchical approach. In Proceedings
of the American Association of Artificial Intelligence
National Conference on AI, pages 133?136, Pitts-
burgh, PA.
Kevin Reschke and Pranav Anand. 2011. Extracting
contextual evaluativity. In Proceedings of the Ninth
International Conference on Computational Seman-
tics, IWCS ?11, pages 370?374, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rintje Pieter Eelke Sybesma. 1992. Causatives and
accomplishments: The case of Chinese ba, vol-
ume 1. Holland Institute of Generative Linguistics.
Benjamin KY Tsou, Raymond WM Yuen, Oi Yee
Kwong, TBY La, and Wei Lung Wong. 2005. Po-
larity classification of celebrity coverage in the chi-
nese press. In Proceedings of International Confer-
ence on Intelligence Analysis.
Xiaojun Wan. 2008. Using bilingual knowledge and
ensemble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 553?561, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
16
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, pages 235?243. Association for Compu-
tational Linguistics.
Suge Wang, Yingjie Wei, Deyu Li, Wu Zhang, and
Wei Li. 2007. A hybrid method of feature se-
lection for chinese text sentiment classification. In
Fuzzy Systems and Knowledge Discovery, 2007.
FSKD 2007. Fourth International Conference on,
volume 3, pages 435?439. IEEE.
Janyce Wiebe and Lingjia Deng. 2014. An account of
opinion implicatures. arXiv:1404.6491v1 [cs.CL].
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language ann. Language Resources and
Evaluation, 39(2/3):164?210.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of the
4th ACL SIGdial Workshop on Discourse and Dia-
logue (SIGdial-03), pages 13?22.
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing features for semantic role labeling. In EMNLP,
pages 88?94.
Bishan Yang and Claire Cardie. 2013. Joint Inference
for Fine-grained Opinion Extraction. In Proceed-
ings of ACL, pages 1640?1649.
Jonathan S Yedidia, William T Freeman, and Yair
Weiss. 2005. Constructing free-energy approx-
imations and generalized belief propagation algo-
rithms. Information Theory, IEEE Transactions on,
51(7):2282?2312.
Lei Zhang and Bing Liu. 2011. Identifying noun prod-
uct features that imply opinions. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 575?580, Portland, Oregon, USA, June.
Association for Computational Linguistics.
17
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 107?112,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Lexical Acquisition for Opinion Inference:
A Sense-Level Lexicon of Benefactive and Malefactive Events
Yoonjung Choi
1
, Lingjia Deng
2
, and Janyce Wiebe
1,2
1
Department of Computer Science
2
Intelligent Systems Program
University of Pittsburgh
yjchoi@cs.pitt.edu, lid29@pitt.edu, wiebe@cs.pitt.edu
Abstract
Opinion inference arises when opinions
are expressed toward states and events
which positive or negatively affect entities,
i.e., benefactive and malefactive events.
This paper addresses creating a lexicon of
such events, which would be helpful to in-
fer opinions. Verbs may be ambiguous,
in that some meanings may be benefac-
tive and others may be malefactive or nei-
ther. Thus, we use WordNet to create a
sense-level lexicon. We begin with seed
senses culled from FrameNet and expand
the lexicon using WordNet relationships.
The evaluations show that the accuracy of
the approach is well above baseline accu-
racy.
1 Introduction
Opinions are commonly expressed in many kinds
of written and spoken text such as blogs, reviews,
new articles, and conversation. Recently, there
have been a surge in reserach in opinion analy-
sis (sentiment analysis) research (Liu, 2012; Pang
and Lee, 2008).
While most past researches have mainly ad-
dressed explicit opinion expressions, there are a
few researches for implicit opinions expressed via
implicatures. Deng and Wiebe (2014) showed
how sentiments toward one entity may be prop-
agated to other entities via opinion implicature
rules. Consider The bill would curb skyrocketing
health care costs. Note that curb costs is bad for
the object costs since the costs are reduced. We
can reason that the writer is positive toward the
event curb since the event is bad for the object
health care costs which the writer expresses an ex-
plicit negative sentiment (skyrocketing). We can
reason from there that the writer is positive toward
the bill, since it is the agent of the positive event.
These implicature rules involve events that pos-
itively or negatively affect the object. Such events
are called malefactive and benefactive, or, for ease
of writing, goodFor (gf ) and badFor (bf ) (here-
after gfbf). The list of gfbf events and their polari-
ties (gf or bf) are necessary to develop a fully auto-
matic opinion inference system. On first thought,
one might think that we only need lists of gfbf
words. However, it turns out that gfbf terms may
be ambiguous ? a single word may have both gf
and bf meanings.
Thus, in this work, we take a sense-level ap-
proach to acquire gfbf lexicon knowledge, lead-
ing us to employ lexical resources with fine-
grained sense rather than word representations.
For that, we adopt an automatic bootstrapping
method which disambiguates gfbf polarity at the
sense-level utilizing WordNet, a widely-used lex-
ical resource. Starting from the seed set manually
generated from FrameNet, a rich lexicon in which
words are organized by semantic frames, we ex-
plore how gfbf terms are organized in WordNet via
semantic relations and expand the seed set based
on those semantic relations.
The expanded lexicon is evaluated in two ways.
First, the lexicon is evaluated against a corpus that
has been annotated with gfbf information at the
word level. Second, samples from the expanded
lexicon are manually annotated at the sense level,
which gives some idea of the prevalence of gfbf
lexical ambiguity and provides a basis for sense-
level evaluation. Also, we conduct the agreement
study. The results show that the expanded lexi-
con covers more than half of the gfbf instances
in the gfbf corpus, and the system?s accuracy, as
measured against the sense-level gold standard, is
substantially higher than baseline. In addition, in
the agreement study, the annotators achieve good
agreement, providing evidence that the annotation
task is feasible and that the concept of gfbf gives
us a natural coarse-grained grouping of senses.
107
2 The GFBF Corpus
A corpus of blogs and editorials about the Afford-
able Care Act, a controversial topic, was manu-
ally annotated with gfbf information by Deng et
al. (2013)
1
. This corpus provides annotated gfbf
events and the agents and objects of the events. It
consists of 134 blog posts and editorials. Because
the Affordable Health Care Act is a controversial
topic, the data is full of opinions. In this corpus,
1,411 gfbf instances are annotated, each including
a gfbf event, its agent, and its object (615 gf in-
stances and 796 bf instances). 196 different words
appear in gf instances and 286 different words ap-
pear in bf instances; 10 words appear in both.
3 Sense-Level GFBF Ambiguity
A word may have one or more meanings. For
that, we use WordNet
2
, which is a large lexical
database of English (Miller et al., 1990). In Word-
Net, nouns, verbs, adjectives, and adverbs are or-
ganized by semantic relations between meanings
(senses). We assume that a sense is exactly one
of gf, bf, or neither. Since words often have more
than one sense, the polarity of a word may or may
not be consistent, as the following WordNet exam-
ples show.
? A word with only gf senses: encourage
S1: (v) promote, advance, boost, further, en-
courage (contribute to the progress or growth
of)
S2: (v) encourage (inspire with confidence;
give hope or courage to)
S3: (v) encourage (spur on)
? A word with only bf senses: assault
S1: (v) assail, assault, set on, attack (attack
someone physically or emotionally)
S2: (v) rape, ravish, violate, assault, dis-
honor, dishonour, outrage (force (someone)
to have sex against their will)
S3: (v) attack, round, assail, lash out, snipe,
assault (attack in speech or writing)
All senses of encourage are good for the object,
and all senses of assault are bad for the object.
The polarity is always same regardless of sense.
In such cases, for our purposes, which particular
sense is being used does not need to be determined
because any instance of the word will be good for
1
Available at http://mpqa.cs.pitt.edu/corpora/gfbf/
2
WordNet, http://wordnet.princeton.edu/
(bad for); that is, word-level approaches can work
well. However, word-level approaches are not ap-
plicable for all the words. Consider the following:
? A word with gf and neutral senses: inspire
S3: (v) prompt, inspire, instigate (serve as the
inciting cause of)
S4: (v) cheer, root on, inspire, urge, barrack,
urge on, exhort, pep up (spur on or encourage
especially by cheers and shouts)
S6: (v) inhale, inspire, breathe in (draw in
(air))
? A word with bf and neutral senses: neutral-
ize
S2: (v) neutralize, neutralise, nullify, negate
(make ineffective by counterbalancing the ef-
fect of)
S6: (v) neutralize, neutralise (make chemi-
cally neutral)
The words inspire and neutralize both have 6
senses (we list a subset due to space limitations).
For inspire, while S3 and S4 are good for the ob-
ject, S6 doesn?t have any polarity, i.e., it is a neu-
tral (we don?t think of inhaling air as good for the
air). Also, while S2 of neutralize is bad for the
object, S6 is neutral (neutralizing a solution just
changes its pH). Thus, if word-level approaches
are applied using these words, some neutral in-
stances may be incorrectly classified as gf or bf
events.
? A word with gf and bf senses: fight
S2: (v) fight, oppose, fight back, fight down,
defend (fight against or resist strongly)
S4: (v) crusade, fight, press, campaign, push,
agitate (exert oneself continuously, vigor-
ously, or obtrusively to gain an end or engage
in a crusade for a certain cause or person; be
an advocate for)
As mentioned in Section 2, 10 words are ap-
peared in both gf and bf instances. Since only
words and not senses are annotated in the corpus,
such conflicts arise. These 10 words account for
9.07% (128 instances) of all annotated instances.
One example is fight. In the corpus instance fight
for a piece of legislation, fight is good for the ob-
ject, a piece of legislation. This is S4. However,
in the corpus instance we need to fight this repeal,
the meaning of fight here is S2, so fight is bad for
the object, this repeal.
108
Thesefore, approaches for determining the gfbf
polarity of an instance that are sense-level instead
of word-level promise to have higher precision.
4 Lexicon Acquisition
In this section, we develop a sense-level gfbf lex-
icon by exploiting WordNet. The method boot-
straps from a seed lexicon and iteratively follows
WordNet relations. We consider only verbs.
4.1 Seed Lexicon
To preserve the corpus for evaluation, we created
a seed set that is independent from the corpus. An
annotator who didn?t have access to the corpus
manually selected gfbf words from FrameNet
3
in
the light of semantic frames. The annotator found
592 gf words and 523 bf words. Decomposing
each word into its senses in WordNet, there are
1,525 gf senses and 1,154 bf senses. 83 words ex-
tracted from FrameNet overlap with gfbf instances
in the corpus. For independence, those words were
discarded. Among the senses of the remaining
words, we randomly choose 200 gf senses and 200
bf senses.
4.2 Expansion Method
In WordNet, verb senses are arranged into hier-
archies, that is, verb senses towards the bottom
of the trees express increasingly specific manners.
Thus, we can follow hypernym relations to more
general senses and troponym relations to more spe-
cific verb senses. Since the troponym relation
refers to a specific elaboration of a verb sense, we
hypothesized that troponyms of a synset tends to
have its same polarity (i.e., gf or bf). We only con-
sider the direct troponyms in a single iteration. Al-
though the hypernym is a more general term, we
hypothesized that direct hypernyms tend to have
the the same or neutral polarity, but not the oppo-
site polarity. Also, the verb groups are promising;
even though the coverage is incomplete, we expect
the verb groups to be the most helpful.
WordNet Similarity
4
, is a facility that provides a
variety of semantic similarity and relatedness mea-
sures based on information found in the Word-
Net lexical database. We choose Jiang&Conrath
(1997) (jcn) method which has been found to be
effective for such tasks by NLP researchers. When
two concetps aren?t related at all, it returns 0. The
3
FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/
4
WN Similarity, http://wn-similarity.sourceforge.net/
more they are related, the higher the value is re-
tuned. We regarded words with similarity values
greater than 1.0 to be similar words.
Beginning with its seed set, each lexicon (gf and
bf) is expanded iteratively. On each iteration, for
each sense in the current lexicon, all of its direct
troponyms, direct hypernyms, and members of the
same verb group are extracted and added to the
lexicon for the next iteration. Similarity, for each
sense, all words with above-threshold jcn values
are added. For new senses that are extracted for
both the gf and bf lexicons, we ignore such senses,
since there is conflicting evidence (recall that we
assume a sense has only one polarity, even if a
word may have senses of different polarities).
4.3 Corpus Evaluation
In this section, we use the gfbf annotations in the
corpus as a gold standard. The annotations in the
corpus are at the word level. To use the annota-
tions as a sense-level gold standard, all the senses
of a word marked gf (bf) in the corpus are con-
sidered to be gf (bf). While this is not ideal, this
allows us to evaluate the lexicon against the only
corpus evidence available.
The 196 words that appear in gf instances in
the corpus have a total of 897 senses, and the 286
words that appear in bf instances have a total of
1,154 senses. Among them, 125 senses are con-
flicted: a sense of a word marked gf in the corpus
could be a member of the same synset as a sense
of a word marked bf in the corpus. For a more reli-
able gold-standard set, we ignored these conflicted
senses. Thus, the gold-standard set contains 772 gf
senses and 1,029 bf senses.
Table 1 shows the results after five iterations of
lexicon expansion. In total, the gf lexicon contains
4,157 senses and the bf lexicon contains 5,071
senses. The top half gives the results for the gf
lexicon and the bottom half gives the results for
the bf lexicon. In the table, gfOverlap means the
overlap between the senses in the lexicon in that
row and the gold-standard gf set, while bfOverlap
is the overlap between the senses in the lexicon in
that row and the gold-standard bf set. That is, of
the 772 senses in the gf gold standard, 449 (58%)
are in the gf expanded lexicon while 105 (14%)
are in the bf expanded lexicon.
Accuracy (Acc) for gf is calculated as #gfOver-
lap / (#gfOverlap + #bfOverlap) and bf is calcu-
lated as #bfOverlap / (#gfOverlap + #bfOverlap).
109
goodFor
#senses #gfOverlap #bfOverlap Acc
Total 4,157 449 176 0.72
WN Sim 1,073 134 75 0.64
Groups 242 69 24 0.74
Troponym 4,084 226 184 0.55
Hypernym 223 75 33 0.69
badFor
#senses #gfOverlap #bfOverlap Acc
Total 5,071 105 562 0.84
WN Sim 1,008 34 190 0.85
Groups 255 11 86 0.89
Troponym 4,258 66 375 0.85
Hypernym 286 16 77 0.83
Table 1: Results after lexicon expansion
Overall, accuracy is higher for the bf than the
gf lexicon. The results in the table are broken
down by semantic relation. Note that the individ-
ual counts do not sum to the totals because senses
of different words may actually be the same sense
in WordNet. The results for the bf lexicon are con-
sistently high over all semantic relations. The re-
sults for the gf lexicon are more mixed, but all re-
lations are valuable.
The WordNet Similarity is advantageous be-
cause it detects similar senses automatically, so
may provide coverage beyond the semantic rela-
tions coded in WordNet.
Overall, the verb group is the most informative
relation, as we suspected.
Although the gf-lexicon accuracy for the tro-
ponym relation is not high, it has the advantage
is that it yields the most number of senses. Its
lower accuracy doesn?t support our original hy-
pothesis. We first thought that verbs lower down in
the hierarchy would tend to have the same polar-
ity since they express specific manners character-
izing an event. However, this hypothesis is wrong.
Even though most troponyms have the same polar-
ity, there are many exceptions. For example, pro-
tect#v#1, which means the first sense of the verb
protect, has 18 direct troponyms such as cover
for#v#1, overprotect#v#2, and so on. protect#v#1
is a gf event because the meaning is ?shielding
from danger? and most troponyms are also gf
events. However, overprotect#v#2, which is one
of troponyms of protect#v#1, is a bf event.
For the hypernym relation, the number of de-
tected senses is not large because many were al-
ready detected in previous iterations (in general,
there are fewer nodes on each level as hypernym
links are traversed).
4.4 Sense Annotation Evaluation
For a more direct evaluation, two annotators, who
are co-authors, independently annotated a sample
of senses. We randomly selected 60 words among
the following classes: 10 pure gf words (i.e., all
senses of the words are classified by the expan-
sion method, and all senses are put into the gf lex-
icon), 10 pure bf words, 20 mixed words (i.e., all
senses of the words are classified by the expan-
sion method, and some senses are put into the gf
lexicon while others are put into the bf lexicon),
and 20 incomplete words (i.e., some senses of the
words are not classified by the expansion method).
The total number of senses is 151; 64 senses
are classified as gf, 56 senses are classified as bf,
and 31 senses are not classified. We included more
mixed than pure words to make the results of the
study more informative. Further, we wanted to in-
cluded non-classified senses as decoys for the an-
notators. The annotators only saw the sense en-
tries from WordNet. They didn?t know whether
the system classified a sense as gf or bf or whether
it didn?t classify it at all.
Table 2 evaluates the lexicons against the man-
ual annotations, and in comparison to the ma-
jority class baseline. The top half of the table
shows results when treating Anno1?s annotations
as the gold standard, and the bottom half shows
the results when treating Anno2?s as the gold stan-
dard. Among 151 senses, Anno1 annotated 56
senses (37%) as gf, 51 senses (34%) as bf, and
44 senses (29%) as neutral. Anno2 annotated 66
senses (44%) as gf, 55 senses (36%) as bf, and
30 (20%) senses as neutral. The incorrect cases
are divided into two sets: incorrect opposite con-
sists of senses that are classified as the opposite
polarity by the expansion method (e.g., the sense
is classified into gf, but annotator annotates it as
bf), and incorrect neutral consists of senses that
the expansion method classifies as gf or bf, but the
annotator marked it as neutral. We report the accu-
racy and the percentage of cases for each incorrect
case. The accuracies substantially improve over
baseline for both annotators and for both classes.
In Table 3, we break down the results into gfbf
classes. The gf accuracy measures the percentage
of correct gf senses out of all senses annotated as
gf according to the annotations (same as bf accu-
racy). As we can see, accuracy is higher for the
bf than the gf. The conclusion is consistent with
what we have discovered in Section 4.3.
110
By Anno1, 8 words are detected as mixed
words, that is, they contain both gf and bf senses.
By Anno2, 9 words are mixed words (this set in-
cludes the 8 mixed words of Anno1). Among
the randomly selected 60 words, the proportion of
mixed words range from 13.3% to 15%, according
to the two annotators. This shows that gfbf lexical
ambiguity does exist.
To measure agreement between the annotators,
we calculate two measures: percent agreement and
? (Artstein and Poesio, 2008). ? measures the
amount of agreement over what is expected by
chance, so it is a stricter measure. Percent agree-
ment is 0.84 and ? is 0.75.
accuracy % incorrect % incorrect base-
opposite neutral line
Anno1 0.53 0.16 0.32 0.37
Anno2 0.57 0.24 0.19 0.44
Table 2: Results against sense-annotated data
gf accuracy bf accuracy baseline
Anno1 0.74 0.83 0.37
Anno2 0.68 0.74 0.44
Table 3: Accuracy broken down for gfbf
5 Related Work
Lexicons are widely used in sentiment analysis
and opinion extraction. There are several previ-
ous works to acquire or expand sentiment lexi-
cons such as (Kim and Hovy, 2004), (Strapparava
and Valitutti, 2004), (Esuli and Sebastiani, 2006),
(Gyamfi et al., 2009), (Mohammad and Turney,
2010) and (Peng and Park, 2011). Such senti-
ment lexicons are helpful for detecting explicitly
stated opinions, but are not sufficient for recog-
nizing implicit opinions. Inferred opinions often
have opposite polarities from the explicit senti-
ment expressions in the sentence; explicit senti-
ments must be combined with benefactive, male-
factive state and event information to detect im-
plicit sentiments. There are few previous works
closest to ours. (Feng et al., 2011) build con-
notation lexicons that list words with connotative
polarity and connotative predicates. Goyal et al.
(2010) generate a lexicon of patient polarity verbs
that imparts positive or negative states on their pa-
tients. Riloff et al. (2013) learn a lexicon of nega-
tive situation phrases from a corpus of tweets with
hashtag ?sarcasm?.
Our work is complementary to theirs in that
their acquisition methods are corpus-based, while
we acquire knowledge from lexical resources.
Further, all of their lexicons are word level while
ours are sense level. Finally, the types of entries
among the lexicons are related but not the same.
Ours are specifically designed to support the au-
tomatic recognition of implicit sentiments in text
that are expressed via implicature.
6 Conclusion and Future Work
In this paper, we developed a sense-level gfbf
lexicon which was seeded by entries culled from
FrameNet and then expanded by exploiting se-
mantic relations in WordNet. Our evaluations
show that such lexical resources are promising for
expanding such sense-level lexicons. Even though
the seed set is completely independent from the
corpus, the expanded lexicon?s coverage of the
corpus is not small. The accuracy of the expanded
lexicon is substantially higher than baseline accu-
racy. Also, the results of the agreement study are
positive, providing evidence that the annotation
task is feasible and that the concept of gfbf gives
us a natural coarse-grained grouping of senses.
However, there is still room for improvement.
We believe that gf/bf judgements of word senses
could be effectively crowd-sourced; (Akkaya et
al., 2010), for example, effectively used Ama-
zon Mechanical Turk (AMT) for similar coarse-
grained judgements. The idea would be to use au-
tomatic expansion methods to create a sense-level
lexicon, and then have AMT workers judge the
entries in which we have least confidence. This
would be much more time- and cost-effective.
The seed sets we used are small - only 400 total
senses. We believe it will be worth the effort to
create larger seed sets, with the hope to mine many
additional gfbf senses from WordNet.
To exploit the lexicon to recognize sentiments in
a corpus, the word-sense ambiguity we discovered
needs to be addressed. There is evidence that the
performance of word-sense disambiguation sys-
tems using a similar coarse-grained sense inven-
tory is much better than when the full sense inven-
tory is used (Akkaya et al., 2009; Akkaya et al.,
2011). That, coupled with the fact that our study
suggests that many words are unambiguous with
respect to the gfbf distinction, makes us hopeful
that gfbf information may be practically exploited
to improve sentiment analysis in the future.
111
7 Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190?199.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk
for subjectivity word sense disambiguation. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon?s
Mechanical Turk, pages 195?203.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of
subjectivity word sense disambiguation on contex-
tual opinion analysis. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 87?96.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of EACL.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer atti-
tude annotation. In Proceedings of 51st ACL, pages
120?125.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of 5th LREC, pages
417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of EMNLP, pages 1092?
1103.
Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of EMNLP, pages
77?86.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of NAACL HLT
2009, pages 10?18.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of COLING.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of 20th
COLING, pages 1367?1373.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. International
Journal of Lexicography, 13(4):235?312.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Wei Peng and Dae Hoon Park. 2011. Generate adjec-
tive sentiment dictionary for social media sentiment
analysis using constrained nonnegative matrix fac-
torization. In Proceedings of ICWSM.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP, pages 704?714.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: An affective extension of wordnet.
In Proceedings of 4th LREC, pages 1083?1086.
112
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 154?159,
Baltimore, Maryland, USA. June 27, 2014.
c
?2014 Association for Computational Linguistics
A Conceptual Framework for Inferring Implicatures
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Lingjia Deng
Intelligent Systems Program
University of Pittsburgh
lid29@pitt.edu
Abstract
While previous sentiment analysis re-
search has concentrated on the interpreta-
tion of explicitly stated opinions and atti-
tudes, this work addresses a type of opin-
ion implicature (i.e., opinion-oriented de-
fault inference) in real-world text. This
work describes a rule-based conceptual
framework for representing and analyzing
opinion implicatures. In the course of un-
derstanding implicatures, the system rec-
ognizes implicit sentiments (and beliefs)
toward various events and entities in the
sentence, often of mixed polarities; thus,
it produces a richer interpretation than is
typical in opinion analysis.
1 Introduction
This paper is a brief introduction to a framework
we have developed for sentiment inference (Wiebe
and Deng, 2014). Overall, the goal of this work is
to make progress toward a deeper automatic inter-
pretation of opinionated language by developing
computational models for the representation and
interpretation of opinion implicature (i.e., opinion-
oriented default inference) in language. In this
paper, we feature a rule-based implementation of
a conceptual framework for opinion implicatures,
specifically implicatures that arise in the presence
of explicit sentiments, and events that positively or
negatively affect entities (goodFor/badFor events).
To eliminate interference introduced by the noisy
output of automatic NLP components, the sys-
tem takes as input manually annotated explicit-
sentiment and event information, and makes in-
ferences based on that input information. Thus,
the purpose of this work is to provide a conceptual
understanding of (a type of) opinion implicature,
to provide a blueprint for realizing fully automatic
systems in the future.
Below, we give terminology, overview the rule-
based system, and then present the rule schemas.
Finally, via discussion of an example from the
MPQA opinion-annotated corpus (Wiebe et al.,
2005)
1
, we illustrate the potential of the frame-
work for recognizing implicit sentiments and
writer-level sentiments that are not anchored on
clear sentiment words, and for capturing inter-
dependencies among explicit and implicit senti-
ments.
We have developed a graph-based computa-
tional model implementing some rules introduced
below (Deng and Wiebe, 2014). Moreover, in on-
going work, we have proposed an optimization
framework to jointly extract and resolve the input
ambiguities.
2 Terminology
The building blocks of our opinion implicature
framework are subjectivity, inferred private states,
and benefactive/malefactive events and states.
Subjectivity. Following (Wiebe et al., 2005;
Wiebe, 1994), subjectivity is defined as the ex-
pression of private states in language, where pri-
vate states are mental and emotional states such as
speculations, sentiments, and beliefs (Quirk et al.,
1985). Subjective expressions (i.e., opinions) have
sources (or holders): the entity or entities whose
private states are being expressed. Again follow-
ing (Wiebe et al., 2005; Wiebe, 1994), a private
state is an attitude held by a source toward (op-
tionally) a target. Sentiment and belief are types
of attitudes. Subjectivity is the linguistic expres-
sion of private states. Subjectivity is a pragmatic
notion: as the sentence is interpreted in context,
a private state is attributed to a source in that con-
text (Banfield, 1982). By sentiment expression
or explicit sentiment, we mean a subjective ex-
pression where the attitude type of the expressed
1
Available at http://mpqa.cs.pitt.edu
154
private state is sentiment.
There are many types of linguistic clues that
contribute to recognizing subjective expressions
(Wiebe, 1994). In the clearest case, some word
senses give rise to subjectivity whenever they are
used in discourse (Wiebe and Mihalcea, 2006).
Other clues are not as definitive. For example, re-
searchers in NLP have begun to develop lexicons
of connotations (Feng et al., 2011), i.e., words as-
sociated with polarities out of context (e.g., war
has negative connotation and sunshine has positive
connotation (Feng et al., 2013)). However, words
may be used in context with polarities opposite to
their connotations, as in Ghenghis Kan likes war.
Inferred Private States and Opinion Implica-
tures. We address private states inferred from
other private states, where the attitude type of both
is sentiment. Inference is initiated by explicit sen-
timent subjectivity. We borrow the term implica-
ture from linguistics, specifically generalized con-
versational implicature. Grice (1967; 1989) intro-
duced the notion to account for how more can be
pragmatically communicated than what is strictly
said - what is implicated vs. what is said (Doran
et al., 2012). Generalized conversational implica-
tures are cancellable, or defeasible.
Analogously, we can treat subjectivity as part
of what is said,
2
and the private-state inferences
we address to be part of what is implicated.
Opinion implicatures are default inferences that
may not go through in context.
Benefactive/Malefactive Events and States.
This work addresses sentiments toward, in gen-
eral, states and events which positively or nega-
tively affect entities. Various lexical items and
semantic roles evoke such situations. We adopt
one clear case in this work (Deng et al., 2013):
?agent, event, object? triples, where event nega-
tively (badFor) or positively (goodFor) affects the
object. An event that is goodFor or badFor is a
gfbf event. Note that we have annotated a corpus
with gfbf information and the speaker?s sentiment
toward the agents and objects of gfbf events (Deng
et al., 2013).
3
2
While the focus in the literature on what is said is se-
mantics, Grice and people later working on the topic ac-
knowledge that what is said must include pragmatics such as
co-reference and indexical resolution (Doran et al., 2012),
and subjectivity arises from deixis (Bruder and Wiebe, 1995;
Stein and Wright, 1995). However, as long as what is said is
conceived of as only truth evaluable propositions, then it is
not exactly the notion for our setting.
3
Available at http://mpqa.cs.pitt.edu
3 Overview
In this section, we give an overview of the rule-
based system to provide an intuitive big picture of
what it can infer, instead of elaborating specific
rules, which will be introduced in Section 4.
The system includes default inference rules
which apply if there is no evidence to the contrary.
It requires as input explicit sentiment and gfbf in-
formation (plus any evidence that is contrary to the
inferences). The data structure of the input and the
output are described in Section 3.1. The rules are
applied repeatedly until no new conclusions can be
drawn. If a rule matches a sentiment or event that
is the target of a private state, the nesting struc-
ture is preserved when generating the conclusions.
We say that inference is carried out in private state
spaces, introduced in Section 3.2. Finally in Sec-
tion 3.3, an example is provided to illustrate what
the system is able to infer.
3.1 Data Structure
The system builds a graphical representation of
what it knows and infers about the meaning of
a sentence. A detailed knowledge representation
scheme is presented in (Wiebe and Deng, 2014).
Below is an example from the MPQA corpus.
Ex(1) [He] is therefore planning to trig-
ger [wars] here and there to revive [the
flagging arms industry].
There are two gfbf events in this sentence: ?He,
trigger, wars? and ?He, revive, arms industry?. The
system builds these nodes as input (as printed by
the system):
8 writer positive believesTrue
4 He revive flagging arms industry
6 writer positive believesTrue
1 He trigger wars
The system?s printout does not show all the
structure of a node. Consider node 8. It has a
source edge to the node representing the writer,
and a target edge to node 4, which in turn has an
agent edge to the node representing He and a ob-
ject edge to the node representing flagging arms
industry. The nodes also have attributes which
record, e.g., what type of node it is (node 8 is a
privateState and node 4 is a gfbf), polarity (if rele-
vant), etc.
The graph is directed. For example, node 4 is
a child of 8. A specification for the input is that
each root node must be a sentiment or believesTrue
155
node whose source is the writer. Inference pro-
ceeds by matching rules to the graph built so far
and, when a rule successfully fires, adding nodes
to the graph.
3.2 Private State Spaces
The approach adopted here follows work on rea-
soning in belief spaces and belief ascription in nat-
ural language (Martins and Shapiro, 1983; Rapa-
port, 1986; Slator and Wilks, 1987). Other than
private states of the writer, all propositions and
events must be the target of some private state. In
the simplest case, the writer believes the proposi-
tion or event he/she describes in the document, so
the proposition or event is nested under a writer
positive believesTrue node.
We want to carry out inferences within private
state spaces so that, for example, from S positive
believesTrue P, & P =? Q, the system may in-
fer S positive believesTrue Q. However, we are
working with sentiment, not only belief as in ear-
lier work, and we want to allow, as appropriate,
these types of inferences: from S sentiment to-
ward P, & P =?Q, infer S sentiment toward Q.
For example, if I?m upset my computer is infected
with a virus, then I?m also upset with the conse-
quences (e.g., that my files may be corrupted).
A private state space is defined by a path where
the root is a believesTrue or sentiment node whose
source is the writer, and each node on the path is a
believesTrue or sentiment node. Two paths define
the same private state space if, at each correspond-
ing position, they have the same attitude type, po-
larity, and source. P is in a private state space if P
is the target of the rightmost node on a path defin-
ing that space.
3.3 An Example
Now we have introduced the data structure and the
private state spaces, let?s see the potential conclu-
sions which the system can infer before we go into
the detailed rules in the next section.
Ex(2) However, it appears as if [the in-
ternational community (IC)] is tolerat-
ing [the Israeli] campaign of suppres-
sion against [the Palestinians].
The input nodes are the following.
writer negative sentiment
IC positive sentiment
Israeli suppression Palestinians
The gfbf event ?Israeli, suppression,
Palestinians? is a badFor event. According
to the writer, the IC is positive toward the event
in the sense that they tolerate (i.e., protect) it.
However and appears as if are clues that the
writer is negative toward IC?s positive sentiment.
Given these input annotations, the following are
the sentiments inferred by the system just toward
the entities in the sentence; note that many of the
sentiments are nested in private state spaces.
writer positive sentiment
Palestinians
writer negative sentiment
Israel
writer negative sentiment
IC
writer positive believesTrue
Israel negative sentiment
Palestinians
writer positive believesTrue
IC negative sentiment
Palestinians
writer positive believesTrue
IC positive sentiment
Israel
writer positive believesTrue
IC positive believesTrue
Israel negative sentiment
Palestinians
Note that for the sentiments between two enti-
ties other than the writer (e.g., Israel negative to-
ward Palestinians), they are nested under a writer
positive believesTrue node. This shows why we
need private state spaces. The writer expresses
his/her opinion that the sentiment from Israel to-
ward Palestinians is negative, which may not be
true outside the scope of this single document.
4 Rules
Rules include preconditions and conclusions.
They may also include assumptions (Hobbs et al.,
1993). For example, suppose a rule would suc-
cessfully fire if an entity S believes P. If the entity
S is not the writer but we know that the writer be-
lieves P, and there is no evidence to the contrary
(i.e. there is no evidence showing that the entity
S doesn?t believe P), then we?ll assume that S be-
lieves it as well, if a rule ?asks us to?.
Thus, our rules are conceptually of the form:
P1, ..., P j : A1, .., Ak/Q1, ..., Qm
where the P s are preconditions, the As are as-
sumptions, and the Qs are conclusions. For the Qs
to be concluded, the P s must already hold; there
156
must be a basis for assuming each A; and there
must be no evidence against any of the As or Qs.
Assumptions are indicated using the term ?As-
sume?, as in rule 10, which infers sentiment from
connotation:
rule10:
(Assume Writer positive ...
believesTrue) A gfbf T &
T?s anchor is in connotation lexicon =?
Writer sentiment toward T
The first line contains an assumption, the sec-
ond line contains a precondition, and the third con-
tains a conclusion.
rule8:
S positive believesTrue A gfbf T &
S sentiment toward T =?
S sentiment toward A gfbf T
For example, applying rule 8 to ?The bill would
curb skyrocketing health care costs,? from the
writer?s (S?s) negative sentiment toward the costs
(T) expressed by skyrocketing, we can infer the
writer is positive toward the event ?bill, curb,
costs? (A gfbf T) because it would decrease the
costs.
Note that, in rule 8, the inference is (senti-
ment toward object) =? (sentiment toward event).
Rules 1 and 2 infer in the opposite direction.
rule1:
S sentiment toward A gfbf T =?
S sentiment toward idea of A gfbf T
rule2:
S sentiment toward idea of A gfbf T =?
S sentiment toward T
For rule 1, why ?ideaOf A gfbf T?? Because the
purview of this work is making inferences about
attitudes, not about events themselves. Conceptu-
ally, ideaOf coerces an event into an idea, raising
it into the realm of private-state spaces. Reasoning
about the ideas of events avoids the classification
of whether the events are realis (i.e., whether they
did/will happen).
Rule 9 infers sentiment toward the agent in a
gfbf event.
rule9:
S sentiment toward A gfbf T &
A is a thing &
(Assume S positive believesTrue ...
substantial) A gfbf T =?
S sentiment toward A
By default, the system infers the event is in-
tentional and that the agent is positive toward the
event; if there is evidence against either, the infer-
ence should be blocked.
rule6:
A gfbf T, where A is animate =?
A intended A gfbf T
rule7:
S intended S gfbf T =?
S positive sentiment toward
ideaOf S gfbf T
So far, the preconditions have included only one
sentiment. Rule 3 applies when there are nested
sentiments, i.e., sentiments toward sentiments.
rule3.1:
S1 sentiment toward
S2 sentiment toward Z =?
S1 agrees/disagrees with S2 that
isGood/isBad Z &
S1 sentiment toward Z
rule3.2:
S1 sentiment toward
S2 pos/neg believesTrue substantial Z
=?
S1 agrees/disagrees with S2 that
isTrue/isFalse Z &
S1 pos/neg believesTrue substantial Z
rule3.3:
S1 sentiment toward
S2 pos/neg believesShould Z =?
S1 agrees/disagrees with S2 that
should/shouldNot Z &
S1 pos/neg believesShould Z
Among the subcases of rule 3, one shared con-
clusion is S1 agrees/disagrees with S2 *, which de-
pends on the sentiment from S1 toward S2. The
reason there are subcases is because the attitude
types of S2 are various, which determine the in-
ferred attitude type of S1.
By rule 3, given the sentiment between S1 and
S2, we can infer whether S1 and S2 agree. Simi-
larly, we can infer in the opposite direction, as rule
4 shows.
rule4:
S1 agrees/disagrees with S2 that
*
=?
S1 sentiment toward S2
Two other rules are given in (Wiebe and Deng,
2014).
5 Inferences for An Example from
MPQA Corpus
This section returns to the example from the
MPQA corpus in Section 3.1, illustrating some in-
teresting inference chains and conclusions.
Recall that the input for Ex(1) in Section 3.1 is:
8 writer positive believesTrue
4 He revive flagging arms industry
6 writer positive believesTrue
1 He trigger wars
157
The first inference is from connotation to sen-
timent since the word war is in the connotation
lexicon.
rule10:
(Assume Writer positive ...
believesTrue) A gfbf T &
T?s anchor is in connotation lexicon =?
Writer sentiment toward T
Assumptions:
6 writer positive believesTrue
1 He trigger wars
rule10 =? Infer:
17 writer negative sentiment
2 wars
From the writer?s negative sentiment toward
wars, the system infers a negative sentiment to-
ward trigger wars, since triggering wars is good-
For them:
rule8:
S positive believesTrue A gfbf T &
S sentiment toward T =?
S sentiment toward A gfbf T
Preconditions:
6 writer positive believesTrue
1 He trigger wars
17 writer negative sentiment
2 wars
rule8 =? Infer:
28 writer negative sentiment
1 He trigger wars
On the other hand, since the agent, He, is ani-
mate and there is no evidence to the contrary, the
system infers that the triggering event is inten-
tional, and that He is positive toward the idea of
his performing the event:
rule6 =? Infer:
38 writer negative sentiment
20 He positive intends
1 He trigger wars
rule7 =? Infer:
41 writer negative sentiment
25 He positive sentiment
26 ideaOf
1 He trigger wars
Continuing with inference, since the writer has
a negative sentiment toward the agent?s positive
sentiment, the system infers that the writer dis-
agrees with him (rule 3) and thus that the writer
is negative toward him (rule 4):
rule3.1:
S1 sentiment toward
S2 sentiment toward Z =?
S1 agrees/disagrees with S2 that
isGood/isBad Z &
S1 sentiment toward Z
Preconditions:
41 writer negative sentiment
25 He positive sentiment
26 ideaOf
1 He trigger wars
rule3.1 =? Infer:
50 writer disagrees with He that
49 isGood
26 ideaOf
1 He trigger wars
30 writer negative sentiment
26 ideaOf
1 He trigger wars
Then rule 4 works on node 50 and infers:
rule4 =? Infer:
55 writer negative sentiment
3 He
In addition to the sentiment related to the wars,
we have also drawn several conclusions of senti-
ment toward the arms industry. For example, one
of the output nodes related to the arms industry is:
32 writer positive believesTrue
31 He positive sentiment
5 flagging arms industry
The MPQA annotators marked the writer?s neg-
ative sentiment, choosing the long spans therefore
. . . industry and therefore planning . . . here and
there as attitude and expressive subjective element
spans, respectively. They were not able to pinpoint
any clear sentiment phrases. A machine learning
system trained on such examples would have diffi-
culty recognizing the sentiments. The system, re-
lying on the negative connotation of war and the
gfbf information in the sentence, is ultimately able
to infer several sentiments, including the writer?s
negative sentiment toward the trigger event.
6 Conclusions
While previous sentiment analysis research has
concentrated on the interpretation of explicitly
stated opinions and attitudes, this work addresses
opinion implicature (i.e., opinion-oriented default
inference) in real-world text. This paper described
a rule-based framework for representing and
analyzing opinion implicatures which we hope
will contribute to deeper automatic interpretation
of subjective language. In the course of under-
standing implicatures, the system recognizes
implicit sentiments (and beliefs) toward various
events and entities in the sentence, often of mixed
polarities; thus, it produces a richer interpretation
than is typical in opinion analysis.
Acknowledgements. This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank the
anonymous reviewers for their feedback.
158
References
Ann Banfield. 1982. Unspeakable Sentences. Rout-
ledge and Kegan Paul, Boston.
G. Bruder and J. Wiebe. 1995. Recognizing subjec-
tivity and identifying subjective characters in third-
person fictional narrative. In Judy Duchan, Gail
Bruder, and Lynne Hewitt, editors, Deixis in Nar-
rative: A Cognitive Science Perspective. Lawrence
Erlbaum Associates.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Meeting
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2014).
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer at-
titude annotation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 120?125,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ryan Doran, Gregory Ward, Meredith Larson, Yaron
McNabb, and Rachel E. Baker. 2012. A novel
experimental paradigm for distinguishing between
?what is said? and ?what is implicated?. Language,
88(1):124?154.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1092?1103, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash
of sentiment beneath the surface meaning. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1774?1784, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Herbert Paul Grice. 1967. Logic and conversation.
The William James lectures.
H Paul Grice. 1989. Studies in the Way of Words. Har-
vard University Press.
Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt,
and Paul Martin. 1993. Interpretation as abduction.
Artificial Intelligence, 63(1-2):69?142, October.
Jo?ao Martins and Stuart C. Shapiro. 1983. Reasoning
in multiple belief spaces. In IJCAI.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New York.
William J. Rapaport. 1986. Logical foundations for
belief representation. Cognitive Science, 10(4):371?
422.
Brian M. Slator and Yorick Wilks. 1987. Towards se-
mantic structures from dictionary entries. Technical
Report MCCS-87-96, Computing Research Labora-
tory, NMSU.
Dieter Stein and Susan Wright, editors. 1995. Sub-
jectivity and Subjectivisation. Cambridge Univer-
sity Press, Cambridge.
Janyce Wiebe and Lingjia Deng. 2014. An account of
opinion implicatures. arXiv:1404.6491v1 [cs.CL].
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1065?1072, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164?210.
Janyce Wiebe. 1994. Tracking point of view in narra-
tive. Computational Linguistics, 20(2):233?287.
159
