Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214?220,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning to Differentiate Better from Worse Translations
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez
Alessandro Moschitti Preslav Nakov Massimo Nicosia
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa
Abstract
We present a pairwise learning-to-rank
approach to machine translation evalua-
tion that learns to differentiate better from
worse translations in the context of a given
reference. We integrate several layers
of linguistic information encapsulated in
tree-based structures, making use of both
the reference and the system output simul-
taneously, thus bringing our ranking closer
to how humans evaluate translations. Most
importantly, instead of deciding upfront
which types of features are important, we
use the learning framework of preference
re-ranking kernels to learn the features au-
tomatically. The evaluation results show
that learning in the proposed framework
yields better correlation with humans than
computing the direct similarity over the
same type of structures. Also, we show
our structural kernel learning (SKL) can
be a general framework for MT evaluation,
in which syntactic and semantic informa-
tion can be naturally incorporated.
1 Introduction
We have seen in recent years fast improvement
in the overall quality of machine translation (MT)
systems. This was only possible because of the
use of automatic metrics for MT evaluation, such
as BLEU (Papineni et al., 2002), which is the de-
facto standard; and more recently: TER (Snover et
al., 2006) and METEOR (Lavie and Denkowski,
2009), among other emerging MT evaluation met-
rics. These automatic metrics provide fast and in-
expensive means to compare the output of differ-
ent MT systems, without the need to ask for hu-
man judgments each time the MT system has been
changed.
As a result, this has enabled rapid develop-
ment in the field of statistical machine translation
(SMT), by allowing to train and tune systems as
well as to track progress in a way that highly cor-
relates with human judgments.
Today, MT evaluation is an active field of re-
search, and modern metrics perform analysis at
various levels, e.g., lexical (Papineni et al., 2002;
Snover et al., 2006), including synonymy and
paraphrasing (Lavie and Denkowski, 2009); syn-
tactic (Gim?enez and M`arquez, 2007; Popovi?c
and Ney, 2007; Liu and Gildea, 2005); semantic
(Gim?enez and M`arquez, 2007; Lo et al., 2012);
and discourse (Comelles et al., 2010; Wong and
Kit, 2012; Guzm?an et al., 2014; Joty et al., 2014).
Automatic MT evaluation metrics compare the
output of a system to one or more human ref-
erences in order to produce a similarity score.
The quality of such a metric is typically judged
in terms of correlation of the scores it produces
with scores given by human judges. As a result,
some evaluation metrics have been trained to re-
produce the scores assigned by humans as closely
as possible (Albrecht and Hwa, 2008). Unfortu-
nately, humans have a hard time assigning an ab-
solute score to a translation. Hence, direct hu-
man evaluation scores such as adequacy and flu-
ency, which were widely used in the past, are
now discontinued in favor of ranking-based eval-
uations, where judges are asked to rank the out-
put of 2 to 5 systems instead. It has been shown
that using such ranking-based assessments yields
much higher inter-annotator agreement (Callison-
Burch et al., 2007).
While evaluation metrics still produce numeri-
cal scores, in part because MT evaluation shared
tasks at NIST and WMT ask for it, there has also
been work on a ranking formulation of the MT
evaluation task for a given set of outputs. This
was shown to yield higher correlation with human
judgments (Duh, 2008; Song and Cohn, 2011).
214
Learning automatic metrics in a pairwise set-
ting, i.e., learning to distinguish between two al-
ternative translations and to decide which of the
two is better (which is arguably one of the easiest
ways to produce a ranking), emulates closely how
human judges perform evaluation assessments in
reality. Instead of learning a similarity function
between a translation and the reference, they learn
how to differentiate a better from a worse trans-
lation given a corresponding reference. While the
pairwise setting does not provide an absolute qual-
ity scoring metric, it is useful for most evaluation
and MT development scenarios.
In this paper, we propose a pairwise learning
setting similar to that of Duh (2008), but we extend
it to a new level, both in terms of feature represen-
tation and learning framework. First, we integrate
several layers of linguistic information encapsu-
lated in tree-based structures; Duh (2008) only
used lexical and POS matches as features. Second,
we use information about both the reference and
two alternative translations simultaneously, thus
bringing our ranking closer to how humans rank
translations. Finally, instead of deciding upfront
which types of features between hypotheses and
references are important, we use a our structural
kernel learning (SKL) framework to generate and
select them automatically.
The structural kernel learning (SKL) framework
we propose consists in: (i) designing a struc-
tural representation, e.g., using syntactic and dis-
course trees of translation hypotheses and a refer-
ences; and (ii) applying structural kernels (Mos-
chitti, 2006; Moschitti, 2008), to such representa-
tions in order to automatically inject structural fea-
tures in the preference re-ranking algorithm. We
use this method with translation-reference pairs
to directly learn the features themselves, instead
of learning the importance of a predetermined set
of features. A similar learning framework has
been proven to be effective for question answer-
ing (Moschitti et al., 2007), and textual entailment
recognition (Zanzotto and Moschitti, 2006).
Our goals are twofold: (i) in the short term, to
demonstrate that structural kernel learning is suit-
able for this task, and can effectively learn to rank
hypotheses at the segment-level; and (ii) in the
long term, to show that this approach provides a
unified framework that allows to integrate several
layers of linguistic analysis and information and to
improve over the state-of-the-art.
Below we report the results of some initial ex-
periments using syntactic and discourse structures.
We show that learning in the proposed framework
yields better correlation with humans than apply-
ing the traditional translation?reference similarity
metrics using the same type of structures. We
also show that the contributions of syntax and dis-
course information are cumulative. Finally, de-
spite the limited information we use, we achieve
correlation at the segment level that outperforms
BLEU and other metrics at WMT12, e.g., our met-
ric would have been ranked higher in terms of cor-
relation with human judgments compared to TER,
NIST, and BLEU in the WMT12 Metrics shared
task (Callison-Burch et al., 2012).
2 Kernel-based Learning from Linguistic
Structures
In our pairwise setting, each sentence s in
the source language is represented by a tuple
?t
1
, t
2
, r?, where t
1
and t
2
are two alternative
translations and r is a reference translation. Our
goal is to develop a classifier of such tuples that
decides whether t
1
is a better translation than t
2
given the reference r.
Engineering features for deciding whether t
1
is
a better translation than t
2
is a difficult task. Thus,
we rely on the automatic feature extraction en-
abled by the SKL framework, and our task is re-
duced to choosing: (i) a meaningful structural rep-
resentation for ?t
1
, t
2
, r?, and (ii) a feature func-
tion ?
mt
that maps such structures to substruc-
tures, i.e., our feature space. Since the design
of ?
mt
is complex, we use tree kernels applied
to two simpler structural mappings ?
M
(t
1
, r) and
?
M
(t
2
, r). The latter generate the tree representa-
tions for the translation-reference pairs (t
1
, r) and
(t
2
, r). The next section shows such mappings.
2.1 Representations
To represent a translation-reference pair (t, r), we
adopt shallow syntactic trees combined with RST-
style discourse trees. Shallow trees have been
successfully used for question answering (Severyn
and Moschitti, 2012) and semantic textual sim-
ilarity (Severyn et al., 2013b); while discourse
information has proved useful in MT evaluation
(Guzm?an et al., 2014; Joty et al., 2014). Com-
bined shallow syntax and discourse trees worked
well for concept segmentation and labeling (Saleh
et al., 2014a).
215
DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
VP NP-REL NP VP-REL o-REL o-REL
RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
not to give them the time to think . "
VP NP-REL NP VP-REL o-REL o-REL
TO-REL `` VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
to " give them no time to think . "
a) Hypothesis
b) Reference DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
Bag-of-words relations 
rela
tion
 pro
pag
atio
n di
rect
ion
Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.
Figure 1 shows two example trees combining
discourse, shallow syntax and POS: one for a
translation hypothesis (top) and the other one for
the reference (bottom). To build such structures,
we used the Stanford POS tagger (Toutanova et
al., 2003), the Illinois chunker (Punyakanok and
Roth, 2001), and the discourse parser
1
of (Joty et
al., 2012; Joty et al., 2013).
The lexical items constitute the leaves of the
tree. The words are connected to their respec-
tive POS tags, which are in turn grouped into
chunks. Then, the chunks are grouped into el-
ementary discourse units (EDU), to which the
nuclearity status is attached (i.e., NUCLEUS or
SATELLITE). Finally, EDUs and higher-order dis-
course units are connected by discourse relations
(e.g., DIS:ELABORATION).
2.2 Kernels-based modeling
In the SKL framework, the learning objects are
pairs of translations ?t
1
, t
2
?. Our objective is to
automatically learn which pair features are impor-
tant, independently of the source sentence. We
achieve this by using kernel machines (KMs) over
two learning objects ?t
1
, t
2
?, ?t
?
1
, t
?
2
?, along with
an explicit and structural representation of the
pairs (see Fig. 1).
1
The discourse parser can be downloaded from
http://alt.qcri.org/tools/
More specifically, KMs carry out learning using
the scalar product
K
mt
(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) = ?
mt
(t
1
, t
2
) ??
mt
(t
?
1
, t
?
2
),
where ?
mt
maps pairs into the feature space.
Considering that our task is to decide whether
t
1
is better than t
2
, we can conveniently rep-
resent the vector for the pair in terms of the
difference between the two translation vectors,
i.e., ?
mt
(t
1
, t
2
) = ?
K
(t
1
) ? ?
K
(t
2
). We can
approximate K
mt
with a preference kernel PK to
compute this difference in the kernel space K:
PK(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) (1)
= K(t
1
)? ?
K
(t
2
)) ? (?
K
(t
?
1
)? ?
K
(t
?
2
))
= K(t
1
, t
?
1
) +K(t
2
, t
?
2
)?K(t
1
, t
?
2
)?K(t
2
, t
?
1
)
The advantage of this is that now K(t
i
, t
?
j
) =
?
K
(t
i
) ? ?
K
(t
?
j
) is defined between two transla-
tions only, and not between two pairs of transla-
tions. This simplification enables us to map trans-
lations into simple trees, e.g., those in Figure 1,
and then to apply them tree kernels, e.g., the Par-
tial Tree Kernel (Moschitti, 2006), which carry out
a scalar product in the subtree space.
We can further enrich the representation ?
K
, if
we consider all the information available to the
human judges when they are ranking translations.
That is, the two alternative translations along with
their corresponding reference.
216
In particular, let r and r
?
be the references for
the pairs ?t
1
, t
2
? and ?t
?
1
, t
?
2
?, we can redefine all
the members of Eq. 1, e.g., K(t
1
, t
?
1
) becomes
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
, r), ?
M
(t
?
1
, r
?
))
+ PTK(?
M
(r, t
1
), ?
M
(r
?
, t
?
1
)),
where ?
M
maps a pair of texts to a single tree.
There are several options to produce the bitext-
to-tree mapping for ?
M
. A simple approach is
to only use the tree corresponding to the first ar-
gument of ?
M
. This leads to the basic model
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
), ?
M
(t
?
1
)) +
PTK(?
M
(r), ?
M
(r
?
)), i.e., the sum of two tree
kernels applied to the trees constructed by ?
M
(we
previously informally mentioned it).
However, this simple mapping may be ineffec-
tive since the trees within a pair, e.g., (t
1
, r), are
treated independently, and no meaningful features
connecting t
1
and r can be derived from their
tree fragments. Therefore, we model ?
M
(r, t
1
) by
using word-matching relations between t
1
and r,
such that connections between words and con-
stituents of the two trees are established using
position-independent word matching. For exam-
ple, in Figure 1, the thin dashed arrows show the
links connecting the matching words between t
1
and r. The propagation of these relations works
from the bottom up. Thus, if all children in a con-
stituent have a link, their parent is also linked.
The use of such connections is essential as it en-
ables the comparison of the structural properties
and relations between two translation-reference
pairs. For example, the tree fragment [ELABORA-
TION [SATELLITE]] from the translation is con-
nected to [ELABORATION [SATELLITE]] in the
reference, indicating a link between two entire dis-
course units (drawn with a thicker arrow), and pro-
viding some reliability to the translation
2
.
Note that the use of connections yields a graph
representation instead of a tree. This is problem-
atic as effective models for graph kernels, which
would be a natural fit to this problem, are not cur-
rently available for exploiting linguistic informa-
tion. Thus, we simply use K, as defined above,
where the mapping ?
M
(t
1
, r) only produces a tree
for t
1
annotated with the marker REL represent-
ing the connections to r. This marker is placed on
all node labels of the tree generated from t
1
that
match labels from the tree generated from r.
2
Note that a non-pairwise model, i.e., K(t
1
, r), could
also be used to match the structural information above, but
it would not learn to compare it to a second pair (t
2
, r).
In other words, we only consider the trees en-
riched by markers separately, and ignore the edges
connecting both trees.
3 Experiments and Discussion
We experimented with datasets of segment-level
human rankings of system outputs from the
WMT11 and the WMT12 Metrics shared tasks
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012): we used the WMT11 dataset for training
and the WMT12 dataset for testing. We focused
on translating into English only, for which the
datasets can be split by source language: Czech
(cs), German (de), Spanish (es), and French (fr).
There were about 10,000 non-tied human judg-
ments per language pair per dataset. We scored
our pairwise system predictions with respect to
the WMT12 human judgments using the Kendall?s
Tau (? ), which was official at WMT12.
Table 1 presents the ? scores for all metric vari-
ants introduced in this paper: for the individual
language pairs and overall. The left-hand side of
the table shows the results when using as sim-
ilarity the direct kernel calculation between the
corresponding structures of the candidate transla-
tion and the reference
3
, e.g., as in (Guzm?an et al.,
2014; Joty et al., 2014). The right-hand side con-
tains the results for structured kernel learning.
We can make the following observations:
(i) The overall results for all SKL-trained metrics
are higher than the ones when applying direct sim-
ilarity, showing that learning tree structures is bet-
ter than just calculating similarity.
(ii) Regarding the linguistic representation, we see
that, when learning tree structures, syntactic and
discourse-based trees yield similar improvements
with a slight advantage for the former. More in-
terestingly, when both structures are put together
in a combined tree, the improvement is cumula-
tive and yields the best results by a sizable margin.
This provides positive evidence towards our goal
of a unified tree-based representation with multi-
ple layers of linguistic information.
(iii) Comparing to the best evaluation metrics
that participated in the WMT12 Metrics shared
task, we find that our approach is competitive and
would have been ranked among the top 3 partici-
pants.
3
Applying tree kernels between the members of a pair to
generate one feature (for each different kernel function) has
become a standard practice in text similarity tasks (Severyn et
al., 2013b) and in question answering (Severyn et al., 2013a).
217
Similarity Structured Kernel Learning
Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all
1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198
2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184
3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183
4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231
Table 1: Kendall?s (? ) correlation with human judgements on WMT12 for each language pair.
Furthermore, our result (0.237) is ahead of the
correlation obtained by popular metrics such as
TER (0.217), NIST (0.214) and BLEU (0.185) at
WMT12. This is very encouraging and shows the
potential of our new proposal.
In this paper, we have presented only the first
exploratory results. Our approach can be easily
extended with richer linguistic structures and fur-
ther combined with some of the already existing
strong evaluation metrics.
Testing
Train cs-en de-en es-en fr-en all
1 cs-en 0.210 0.204 0.217 0.204 0.209
2 de-en 0.196 0.251 0.203 0.202 0.213
3 es-en 0.218 0.204 0.240 0.223 0.221
4 fr-en 0.203 0.218 0.224 0.223 0.217
5 all 0.231 0.258 0.226 0.232 0.237
Table 2: Kendall?s (? ) on WMT12 for cross-
language training with DIS+SYN.
Note that the results in Table 1 were for train-
ing on WMT11 and testing on WMT12 for each
language pair in isolation. Next, we study the im-
pact of the choice of training language pair. Ta-
ble 2 shows cross-language evaluation results for
DIS+SYN: lines 1-4 show results when training on
WMT11 for one language pair, and then testing for
each language pair of WMT12.
We can see that the overall differences in perfor-
mance (see the last column: all) when training on
different source languages are rather small, rang-
ing from 0.209 to 0.221, which suggests that our
approach is quite independent of the source lan-
guage used for training. Still, looking at individ-
ual test languages, we can see that for de-en and
es-en, it is best to train on the same language; this
also holds for fr-en, but there it is equally good
to train on es-en. Interestingly, training on es-en
improves a bit for cs-en.
These somewhat mixed results have motivated
us to try tuning on the full WMT11 dataset; as line
5 shows, this yielded improvements for all lan-
guage pairs except for es-en. Comparing to line
4 in Table 1, we see that the overall Tau improved
from 0.231 to 0.237.
4 Conclusions and Future Work
We have presented a pairwise learning-to-rank ap-
proach to MT evaluation, which learns to differen-
tiate good from bad translations in the context of
a given reference. We have integrated several lay-
ers of linguistic information (lexical, syntactic and
discourse) in tree-based structures, and we have
used the structured kernel learning to identify rel-
evant features and learn pairwise rankers.
The evaluation results have shown that learning
in the proposed SKL framework is possible, yield-
ing better correlation (Kendall?s ? ) with human
judgments than computing the direct kernel sim-
ilarity between translation and reference, over the
same type of structures. We have also shown that
the contributions of syntax and discourse informa-
tion are cumulative, indicating that this learning
framework can be appropriate for the combination
of different sources of information. Finally, de-
spite the limited information we used, we achieved
better correlation at the segment level than BLEU
and other metrics in the WMT12 Metrics task.
In the future, we plan to work towards our long-
term goal, i.e., including more linguistic informa-
tion in the SKL framework and showing that this
can help. This would also include more semantic
information, e.g., in the form of Brown clusters or
using semantic similarity between the words com-
posing the structure calculated with latent seman-
tic analysis (Saleh et al., 2014b).
We further want to show that the proposed
framework is flexible and can include information
in the form of quality scores predicted by other
evaluation metrics, for which a vector of features
would be combined with the structured kernel.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
218
References
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1?27.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ?07, pages 136?158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64, Edin-
burgh, Scotland, UK.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 10?51, Montr?eal, Canada.
Elisabet Comelles, Jes?us Gim?enez, Llu??s M`arquez,
Irene Castell?on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 333?
338, Uppsala, Sweden.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, WMT
?08, pages 191?194, Columbus, Ohio, USA.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ?07,
pages 256?264, Prague, Czech Republic.
Francisco Guzm?an, Shafiq Joty, Llu??s M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ?14, pages 687?
698, Baltimore, Maryland, USA.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, pages 486?496, Sofia,
Bulgaria.
Shafiq Joty, Francisco Guzm?an, Llu??s M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ?14, pages 402?408, Balti-
more, Maryland, USA.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25?32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ?12, pages 243?252,
Montr?eal, Canada.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ?07, pages 776?783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of 17th European Conference on Ma-
chine Learning and the 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML/PKDD ?06, pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ?08,
pages 253?262, Napa Valley, California, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovi?c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ?07, pages 48?55, Prague, Czech Republic.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Advances in
Neural Information Processing Systems 14, NIPS
?01, pages 995?1001, Vancouver, Canada.
219
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014a. A study of using syntactic and se-
mantic structures for concept segmentation and la-
beling. In Proceedings of the 25th International
Conference on Computational Linguistics, COLING
?14, pages 193?202, Dublin, Ireland.
Iman Saleh, Alessandro Moschitti, Preslav Nakov,
Llu??s M`arquez, and Shafiq Joty. 2014b. Semantic
kernels for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?14, Doha, Qatar.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 741?750, Portland, Oregon, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?13, pages 75?83, Sofia,
Bulgaria.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning semantic textual sim-
ilarity with structural representations. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ?13, pages 714?718, Sofia, Bulgaria.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ?06, Cambridge, Massachusetts, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ?11, pages
123?129, Edinburgh, Scotland, UK.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, HLT-NAACL ?03, pages 173?180, Ed-
monton, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1060?1068, Jeju Island, Korea.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, COLING-ACL
?06, pages 401?408, Sydney, Australia.
220
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687?698,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Using Discourse Structure Improves Machine Translation Evaluation
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,pnakov}@qf.org.qa
Abstract
We present experiments in using dis-
course structure for improving machine
translation evaluation. We first design
two discourse-aware similarity measures,
which use all-subtree kernels to compare
discourse parse trees in accordance with
the Rhetorical Structure Theory. Then,
we show that these measures can help
improve a number of existing machine
translation evaluation metrics both at the
segment- and at the system-level. Rather
than proposing a single new metric, we
show that discourse information is com-
plementary to the state-of-the-art evalu-
ation metrics, and thus should be taken
into account in the development of future
richer evaluation metrics.
1 Introduction
From its foundations, Statistical Machine Transla-
tion (SMT) had two defining characteristics: first,
translation was modeled as a generative process at
the sentence-level. Second, it was purely statisti-
cal over words or word sequences and made lit-
tle to no use of linguistic information. Although
modern SMT systems have switched to a discrim-
inative log-linear framework, which allows for ad-
ditional sources as features, it is generally hard to
incorporate dependencies beyond a small window
of adjacent words, thus making it difficult to use
linguistically-rich models.
Recently, there have been two promising re-
search directions for improving SMT and its eval-
uation: (a) by using more structured linguistic
information, such as syntax (Galley et al, 2004;
Quirk et al, 2005), hierarchical structures (Chi-
ang, 2005), and semantic roles (Wu and Fung,
2009; Lo et al, 2012), and (b) by going beyond
the sentence-level, e.g., translating at the docu-
ment level (Hardmeier et al, 2012).
Going beyond the sentence-level is important
since sentences rarely stand on their own in a
well-written text. Rather, each sentence follows
smoothly from the ones before it, and leads into
the ones that come afterwards. The logical rela-
tionship between sentences carries important in-
formation that allows the text to express a meaning
as a whole beyond the sum of its separate parts.
Note that sentences can be made of several
clauses, which in turn can be interrelated through
the same logical relations. Thus, in a coherent text,
discourse units (sentences or clauses) are logically
connected: the meaning of a unit relates to that of
the previous and the following units.
Discourse analysis seeks to uncover this coher-
ence structure underneath the text. Several formal
theories of discourse have been proposed to de-
scribe the coherence structure (Mann and Thomp-
son, 1988; Asher and Lascarides, 2003; Webber,
2004). For example, the Rhetorical Structure The-
ory (Mann and Thompson, 1988), or RST, repre-
sents text by labeled hierarchical structures called
Discourse Trees (DTs), which can incorporate sev-
eral layers of other linguistic information, e.g.,
syntax, predicate-argument structure, etc.
Modeling discourse brings together the above
research directions (a) and (b), which makes it an
attractive goal for MT. This is demonstrated by the
establishment of a recent workshop dedicated to
Discourse in Machine Translation (Webber et al,
2013), collocated with the 2013 annual meeting of
the Association of Computational Linguistics.
The area of discourse analysis for SMT is still
nascent and, to the best of our knowledge, no
previous research has attempted to use rhetorical
structure for SMT or machine translation evalua-
tion. One possible reason could be the unavailabil-
ity of accurate discourse parsers. However, this
situation is likely to change given the most recent
advances in automatic discourse analysis (Joty et
al., 2012; Joty et al, 2013).
687
We believe that the semantic and pragmatic in-
formation captured in the form of DTs (i) can help
develop discourse-aware SMT systems that pro-
duce coherent translations, and (ii) can yield bet-
ter MT evaluation metrics. While in this work we
focus on the latter, we think that the former is also
within reach, and that SMT systems would bene-
fit from preserving the coherence relations in the
source language when generating target-language
translations.
In this paper, rather than proposing yet another
MT evaluation metric, we show that discourse
information is complementary to many existing
evaluation metrics, and thus should not be ignored.
We first design two discourse-aware similarity
measures, which use DTs generated by a publicly-
available discourse parser (Joty et al, 2012); then,
we show that they can help improve a number of
MT evaluation metrics at the segment- and at the
system-level in the context of the WMT11 and the
WMT12 metrics shared tasks (Callison-Burch et
al., 2011; Callison-Burch et al, 2012).
These metrics tasks are based on sentence-level
evaluation, which arguably can limit the benefits
of using global discourse properties. Fortunately,
several sentences are long and complex enough to
present rich discourse structures connecting their
basic clauses. Thus, although limited, this setting
is able to demonstrate the potential of discourse-
level information for MT evaluation. Furthermore,
sentence-level scoring (i) is compatible with most
translation systems, which work on a sentence-by-
sentence basis, (ii) could be beneficial to mod-
ern MT tuning mechanisms such as PRO (Hop-
kins and May, 2011) and MIRA (Watanabe et al,
2007; Chiang et al, 2008), which also work at
the sentence-level, and (iii) could be used for re-
ranking n-best lists of translation hypotheses.
2 Related Work
Addressing discourse-level phenomena in ma-
chine translation is relatively new as a research di-
rection. Some recent work has looked at anaphora
resolution (Hardmeier and Federico, 2010) and
discourse connectives (Cartoni et al, 2011; Meyer,
2011), to mention two examples.
1
However, so
far the attempts to incorporate discourse-related
knowledge in MT have been only moderately suc-
cessful, at best.
1
We refer the reader to (Hardmeier, 2012) for an in-depth
overview of discourse-related research for MT.
A common argument, is that current automatic
evaluation metrics such as BLEU are inadequate
to capture discourse-related aspects of translation
quality (Hardmeier and Federico, 2010; Meyer et
al., 2012). Thus, there is consensus that discourse-
informed MT evaluation metrics are needed in or-
der to advance research in this direction. Here we
suggest some simple ways to create such metrics,
and we also show that they yield better correlation
with human judgments.
The field of automatic evaluation metrics for
MT is very active, and new metrics are contin-
uously being proposed, especially in the context
of the evaluation campaigns that run as part of
the Workshops on Statistical Machine Transla-
tion (WMT 2008-2012), and NIST Metrics for
Machine Translation Challenge (MetricsMATR),
among others. For example, at WMT12, 12 met-
rics were compared (Callison-Burch et al, 2012),
most of them new.
There have been several attempts to incorpo-
rate syntactic and semantic linguistic knowledge
into MT evaluation. For instance, at the syn-
tactic level, we find metrics that measure the
structural similarity between shallow syntactic se-
quences (Gim?enez and M`arquez, 2007; Popovic
and Ney, 2007) or between constituency trees (Liu
and Gildea, 2005). In the semantic case, there are
metrics that exploit the similarity over named en-
tities and predicate-argument structures (Gim?enez
and M`arquez, 2007; Lo et al, 2012).
In this work, instead of proposing a new metric,
we focus on enriching current MT evaluation met-
rics with discourse information. Our experiments
show that many existing metrics can benefit from
additional knowledge about discourse structure.
In comparison to the syntactic and semantic ex-
tensions of MT metrics, there have been very few
attempts to incorporate discourse information so
far. One example are the semantics-aware metrics
of Gim?enez and M`arquez (2009) and Comelles et
al. (2010), which use the Discourse Representa-
tion Theory (Kamp and Reyle, 1993) and tree-
based discourse representation structures (DRS)
produced by a semantic parser. They calculate the
similarity between the MT output and references
based on DRS subtree matching, as defined in (Liu
and Gildea, 2005), DRS lexical overlap, and DRS
morpho-syntactic overlap. However, they could
not improve correlation with human judgments, as
evaluated on the MetricsMATR dataset.
688
Compared to the previous work, (i) we use a
different discourse representation (RST), (ii) we
compare discourse parses using all-subtree ker-
nels (Collins and Duffy, 2001), (iii) we evaluate
on much larger datasets, for several language pairs
and for multiple metrics, and (iv) we do demon-
strate better correlation with human judgments.
Wong and Kit (2012) recently proposed an
extension of MT metrics with a measure of
document-level lexical cohesion (Halliday and
Hasan, 1976). Lexical cohesion is achieved using
word repetitions and semantically similar words
such as synonyms, hypernyms, and hyponyms.
For BLEU and TER, they observed improved
correlation with human judgments on the MTC4
dataset when linearly interpolating these metrics
with their lexical cohesion score. Unlike their
work, which measures lexical cohesion at the
document-level, here we are concerned with co-
herence (rhetorical) structure, primarily at the
sentence-level.
3 Our Discourse-Based Measures
Our working hypothesis is that the similarity be-
tween the discourse structures of an automatic and
of a reference translation provides additional in-
formation that can be valuable for evaluating MT
systems. In particular, we believe that good trans-
lations should tend to preserve discourse relations.
As an example, consider the three discourse
trees (DTs) shown in Figure 1: (a) for a reference
(human) translation, and (b) and (c) for transla-
tions of two different systems on the WMT12 test
dataset. The leaves of a DT correspond to con-
tiguous atomic text spans, called Elementary Dis-
course Units or EDUs (three in Figure 1a). Ad-
jacent spans are connected by certain coherence
relations (e.g., Elaboration, Attribution), forming
larger discourse units, which in turn are also sub-
ject to this relation linking. Discourse units linked
by a relation are further distinguished based on
their relative importance in the text: nuclei are
the core parts of the relation while satellites are
supportive ones. Note that the nuclearity and re-
lation labels in the reference translation are also
realized in the system translation in (b), but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis. We ar-
gue that existing metrics that only use lexical and
syntactic information cannot distinguish well be-
tween (b) and (c).
In order to develop a discourse-aware evalua-
tion metric, we first generate discourse trees for
the reference and the system-translated sentences
using a discourse parser, and then we measure the
similarity between the two discourse trees. We de-
scribe these two steps below.
3.1 Generating Discourse Trees
In Rhetorical Structure Theory, discourse analysis
involves two subtasks: (i) discourse segmentation,
or breaking the text into a sequence of EDUs, and
(ii) discourse parsing, or the task of linking the
units (EDUs and larger discourse units) into la-
beled discourse trees. Recently, Joty et al (2012)
proposed discriminative models for both discourse
segmentation and discourse parsing at the sen-
tence level. The segmenter uses a maximum en-
tropy model that achieves state-of-the-art accuracy
on this task, having an F
1
-score of 90.5%, while
human agreement is 98.3%.
The discourse parser uses a dynamic Condi-
tional Random Field (Sutton et al, 2007) as a pars-
ing model in order to infer the probability of all
possible discourse tree constituents. The inferred
(posterior) probabilities are then used in a proba-
bilistic CKY-like bottom-up parsing algorithm to
find the most likely DT. Using the standard set
of 18 coarse-grained relations defined in (Carlson
and Marcu, 2001), the parser achieved an F
1
-score
of 79.8%, which is very close to the human agree-
ment of 83%. These high scores allowed us to de-
velop successful discourse similarity metrics.
2
3.2 Measuring Similarity
A number of metrics have been proposed to mea-
sure the similarity between two labeled trees, e.g.,
Tree Edit Distance (Tai, 1979) and Tree Kernels
(Collins and Duffy, 2001; Moschitti and Basili,
2006). Tree kernels (TKs) provide an effective
way to integrate arbitrary tree structures in kernel-
based machine learning algorithms like SVMs.
In the present work, we use the convolution TK
defined in (Collins and Duffy, 2001), which effi-
ciently calculates the number of common subtrees
in two trees. Note that this kernel was originally
designed for syntactic parsing, where the subtrees
are subject to the constraint that their nodes are
taken with either all or none of the children. This
constraint of the TK imposes some limitations on
the type of substructures that can be compared.
2
The discourse parser is freely available from
http://alt.qcri.org/tools/
689
ElaborationROOT
SPAN NucleusAttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human) translation.
	

 	
		Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402?408,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
DiscoTK: Using Discourse Structure for Machine Translation Evaluation
Shafiq Joty Francisco Guzm
?
an Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{sjoty,fguzman,lmarquez,pnakov}@qf.org.qa
Abstract
We present novel automatic metrics for
machine translation evaluation that use
discourse structure and convolution ker-
nels to compare the discourse tree of an
automatic translation with that of the hu-
man reference. We experiment with five
transformations and augmentations of a
base discourse tree representation based
on the rhetorical structure theory, and we
combine the kernel scores for each of them
into a single score. Finally, we add other
metrics from the ASIYA MT evaluation
toolkit, and we tune the weights of the
combination on actual human judgments.
Experiments on the WMT12 and WMT13
metrics shared task datasets show corre-
lation with human judgments that outper-
forms what the best systems that partici-
pated in these years achieved, both at the
segment and at the system level.
1 Introduction
The rapid development of statistical machine
translation (SMT) that we have seen in recent
years would not have been possible without au-
tomatic metrics for measuring SMT quality. In
particular, the development of BLEU (Papineni
et al., 2002) revolutionized the SMT field, al-
lowing not only to compare two systems in a
way that strongly correlates with human judg-
ments, but it also enabled the rise of discrimina-
tive log-linear models, which use optimizers such
as MERT (Och, 2003), and later MIRA (Watanabe
et al., 2007; Chiang et al., 2008) and PRO (Hop-
kins and May, 2011), to optimize BLEU, or an ap-
proximation thereof, directly. While over the years
other strong metrics such as TER (Snover et al.,
2006) and Meteor (Lavie and Denkowski, 2009)
have emerged, BLEU remains the de-facto stan-
dard, despite its simplicity.
Recently, there has been steady increase in
BLEU scores for well-resourced language pairs
such as Spanish-English and Arabic-English.
However, it was also observed that BLEU-like n-
gram matching metrics are unreliable for high-
quality translation output (Doddington, 2002;
Lavie and Agarwal, 2007). In fact, researchers al-
ready worry that BLEU will soon be unable to dis-
tinguish automatic from human translations.
1
This
is a problem for most present-day metrics, which
cannot tell apart raw machine translation output
from a fully fluent professionally post-edited ver-
sion thereof (Denkowski and Lavie, 2012).
Another concern is that BLEU-like n-gram
matching metrics tend to favor phrase-based SMT
systems over rule-based systems and other SMT
paradigms. In particular, they are unable to cap-
ture the syntactic and semantic structure of sen-
tences, and are thus insensitive to improvement
in these aspects. Furthermore, it has been shown
that lexical similarity is both insufficient and not
strictly necessary for two sentences to convey
the same meaning (Culy and Riehemann, 2003;
Coughlin, 2003; Callison-Burch et al., 2006).
The above issues have motivated a large amount
of work dedicated to design better evaluation met-
rics. The Metrics task at the Workshop on Ma-
chine Translation (WMT) has been instrumental in
this quest. Below we present QCRI?s submission
to the Metrics task of WMT14, which consists of
the DiscoTK family of discourse-based metrics.
In particular, we experiment with five different
transformations and augmentations of a discourse
tree representation, and we combine the kernel
scores for each of them into a single score which
we call DISCOTK
light
. Next, we add to the com-
bination other metrics from the ASIYA MT eval-
uation toolkit (Gim?enez and M`arquez, 2010), to
produce the DISCOTK
party
metric.
1
This would not mean that computers have achieved hu-
man proficiency; it would rather show BLEU?s inadequacy.
402
Finally, we tune the relative weights of the met-
rics in the combination using human judgments
in a learning-to-rank framework. This proved
to be quite beneficial: the tuned version of the
DISCOTK
party
metric was the best performing
metric in the WMT14 Metrics shared task.
The rest of the paper is organized as follows:
Section 2 introduces our basic discourse metrics
and the tree representations they are based on.
Section 3 describes our metric combinations. Sec-
tion 4 presents our experiments and results on
datasets from previous years. Finally, Section 5
concludes and suggests directions for future work.
2 Discourse-Based Metrics
In our recent work (Guzm?an et al., 2014), we used
the information embedded in the discourse-trees
(DTs) to compare the output of an MT system to
a human reference. More specifically, we used
a state-of-the-art sentence-level discourse parser
(Joty et al., 2012) to generate discourse trees for
the sentences in accordance with the Rhetorical
Structure Theory (RST) of discourse (Mann and
Thompson, 1988). Then, we computed the simi-
larity between DTs of the human references and
the system translations using a convolution tree
kernel (Collins and Duffy, 2001), which efficiently
computes the number of common subtrees. Note
that this kernel was originally designed for syntac-
tic parsing, and the subtrees are subject to the con-
straint that their nodes are taken with all or none
of their children, i.e., if we take a direct descen-
dant of a given node, we must also take all siblings
of that descendant. This imposes some limitations
on the type of substructures that can be compared,
and motivates the enriched tree representations ex-
plained in subsections 2.1?2.4.
The motivation to compare discourse trees, is
that translations should preserve the coherence re-
lations. For example, consider the three discourse
trees (DTs) shown in Figure 1. Notice that the
Attribution relation in the reference translation is
also realized in the system translation in (b) but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis.
In (Guzm?an et al., 2014), we have shown that
discourse structure provides additional informa-
tion for MT evaluation, which is not captured by
existing metrics that use lexical, syntactic and se-
mantic information; thus, discourse should be con-
sidered when developing new rich metrics.
Here, we extend our previous work by devel-
oping metrics that are based on new representa-
tions of the DTs. In the remainder of this section,
we will focus on the individual DT representations
that we will experiment with; then, the following
section will describe the metric combinations and
tuning used to produce the DiscoTK metrics.
2.1 DR-LEX
1
Figure 2a shows our first representation of the DT.
The lexical items, i.e., words, constitute the leaves
of the tree. The words in an Elementary Discourse
Unit (EDU) are grouped under a predefined tag
EDU, to which the nuclearity status of the EDU
is attached: nucleus vs. satellite. Coherence re-
lations, such as Attribution, Elaboration, and En-
ablement, between adjacent text spans constitute
the internal nodes of the tree. Like the EDUs, the
nuclearity statuses of the larger discourse units are
attached to the relation labels. Notice that with
this representation the tree kernel can easily be ex-
tended to find subtree matches at the word level,
i.e., by including an additional layer of dummy
leaves as was done in (Moschitti et al., 2007). We
applied the same solution in our representations.
2.2 DR-NOLEX
Our second representation DR-NOLEX (Figure 2b)
is a simple variation of DR-LEX
1
, where we ex-
clude the lexical items. This allows us to measure
the similarity between two translations in terms of
their discourse structures alone.
2.3 DR-LEX
2
One limitation of DR-LEX
1
and DR-NOLEX is that
they do not separate the structure, i.e., the skele-
ton, of the tree from its labels. Therefore, when
measuring the similarity between two DTs, they
do not allow the tree kernel to give partial credit
to subtrees that differ in labels but match in their
structures. DR-LEX
2
, a variation of DR-LEX
1
, ad-
dresses this limitation as shown in Figure 2c. It
uses predefined tags SPAN and EDU to build the
skeleton of the tree, and considers the nuclearity
and/or relation labels as properties (added as chil-
dren) of these tags. For example, a SPAN has two
properties, namely its nuclearity and its relation,
and an EDU has one property, namely its nucle-
arity. The words of an EDU are placed under the
predefined tag NGRAM.
403
Elaboration ROOT
SPANNucleus AttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human-written) translation.
AttributionROOT
SPANSatellite SPANNucleus
In Germany voices , the ECB should be the lender of last resort .
(b) A higher quality (system-generated) translation.
SPANROOT
In Germany the ECB should be for the creditors of last resort .
(c) A lower quality (system-generated) translation.
Figure 1: Three discourse trees for the translations of a source sentence: (a) the reference, (b) a higher
quality automatic translation, and (c) a lower quality automatic translation.
2.4 DR-LEX
1.1
and DR-LEX
2.1
Although both DR-LEX
1
and DR-LEX
2
allow the
tree kernel to find matches at the word level, the
words are compared in a bag-of-words fashion,
i.e., if the trees share a common word, the ker-
nel will find a match regardless of its position in
the tree. Therefore, a word that has occurred in
an EDU with status Nucleus in one tree could be
matched with the same word under a Satellite in
the other tree. In other words, the kernel based
on these representations is insensitive to the nu-
clearity status and the relation labels under which
the words are matched. DR-LEX
1.1
, an exten-
sion of DR-LEX
1
, and DR-LEX
2.1
, an extension
of DR-LEX
2
, are sensitive to these variations at
the lexical level. DR-LEX
1.1
(Figure 2d) and DR-
LEX
2.1
(Figure 2e) propagate the nuclearity sta-
tuses and/or the relation labels to the lexical items
by including three more subtrees at the EDU level.
3 Metric Combination and Tuning
In this section, we describe our Discourse Tree
Kernel (DiscoTK) metrics. We have two main
versions: DISCOTK
light
, which combines the five
DR-based metrics, and DISCOTK
party
, which fur-
ther adds the Asiya metrics.
3.1 DISCOTK
light
In the previous section, we have presented several
discourse tree representations that can be used to
compare the output of a machine translation sys-
tem to a human reference. Each representation
stresses a different aspect of the discourse tree.
In order to make our estimations more robust,
we propose DISCOTK
light
, a metric that takes ad-
vantage of all the previous discourse representa-
tions by linearly interpolating their scores. Here
are the processing steps needed to compute this
metric:
(i) Parsing: We parsed each sentence in order to
produce discourse trees for the human references
and for the outputs of the systems.
(ii) Tree enrichment/simplification: For each
sentence-level discourse tree, we generated the
five different tree representations: DR-NOLEX,
DR-LEX
1
, DR-LEX
1.1
, DR-LEX
2
, DR-LEX
2.1
.
(iii) Estimation: We calculated the per-sentence
similarity scores between tree representations of
the system hypothesis and the human reference
using the extended convolution tree kernel as de-
scribed in the previous section. To compute the
system-level similarity scores, we calculated the
average sentence-level similarity; note that this en-
sures that our metric is ?the same? at the system
and at the segment level.
(iv) Normalization: In order to make the scores of
the different representations comparable, we per-
formed a min?max normalization
2
for each met-
ric and for each language pair.
(v) Combination: Finally, for each sentence, we
computed DISCOTK
light
as the average of the
normalized similarity scores of the different repre-
sentations. For system-level experiments, we per-
formed linear interpolation of system-level scores.
2
Where x
?
= (x?min)/(max?min).
404
	


 

