Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724?1734,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Phrase Representations using RNN Encoder?Decoder
for Statistical Machine Translation
Kyunghyun Cho
Bart van Merri
?
enboer Caglar Gulcehre
Universit?e de Montr?eal
firstname.lastname@umontreal.ca
Dzmitry Bahdanau
Jacobs University, Germany
d.bahdanau@jacobs-university.de
Fethi Bougares Holger Schwenk
Universit?e du Maine, France
firstname.lastname@lium.univ-lemans.fr
Yoshua Bengio
Universit?e de Montr?eal, CIFAR Senior Fellow
find.me@on.the.web
Abstract
In this paper, we propose a novel neu-
ral network model called RNN Encoder?
Decoder that consists of two recurrent
neural networks (RNN). One RNN en-
codes a sequence of symbols into a fixed-
length vector representation, and the other
decodes the representation into another se-
quence of symbols. The encoder and de-
coder of the proposed model are jointly
trained to maximize the conditional prob-
ability of a target sequence given a source
sequence. The performance of a statisti-
cal machine translation system is empiri-
cally found to improve by using the con-
ditional probabilities of phrase pairs com-
puted by the RNN Encoder?Decoder as an
additional feature in the existing log-linear
model. Qualitatively, we show that the
proposed model learns a semantically and
syntactically meaningful representation of
linguistic phrases.
1 Introduction
Deep neural networks have shown great success in
various applications such as objection recognition
(see, e.g., (Krizhevsky et al., 2012)) and speech
recognition (see, e.g., (Dahl et al., 2012)). Fur-
thermore, many recent works showed that neu-
ral networks can be successfully used in a num-
ber of tasks in natural language processing (NLP).
These include, but are not limited to, language
modeling (Bengio et al., 2003), paraphrase detec-
tion (Socher et al., 2011) and word embedding ex-
traction (Mikolov et al., 2013). In the field of sta-
tistical machine translation (SMT), deep neural
networks have begun to show promising results.
(Schwenk, 2012) summarizes a successful usage
of feedforward neural networks in the framework
of phrase-based SMT system.
Along this line of research on using neural net-
works for SMT, this paper focuses on a novel neu-
ral network architecture that can be used as a part
of the conventional phrase-based SMT system.
The proposed neural network architecture, which
we will refer to as an RNN Encoder?Decoder, con-
sists of two recurrent neural networks (RNN) that
act as an encoder and a decoder pair. The en-
coder maps a variable-length source sequence to a
fixed-length vector, and the decoder maps the vec-
tor representation back to a variable-length target
sequence. The two networks are trained jointly to
maximize the conditional probability of the target
sequence given a source sequence. Additionally,
we propose to use a rather sophisticated hidden
unit in order to improve both the memory capacity
and the ease of training.
The proposed RNN Encoder?Decoder with a
novel hidden unit is empirically evaluated on the
task of translating from English to French. We
train the model to learn the translation probabil-
ity of an English phrase to a corresponding French
phrase. The model is then used as a part of a stan-
dard phrase-based SMT system by scoring each
phrase pair in the phrase table. The empirical eval-
uation reveals that this approach of scoring phrase
pairs with an RNN Encoder?Decoder improves
the translation performance.
We qualitatively analyze the trained RNN
Encoder?Decoder by comparing its phrase scores
with those given by the existing translation model.
The qualitative analysis shows that the RNN
Encoder?Decoder is better at capturing the lin-
guistic regularities in the phrase table, indirectly
explaining the quantitative improvements in the
overall translation performance. The further anal-
ysis of the model reveals that the RNN Encoder?
Decoder learns a continuous space representation
of a phrase that preserves both the semantic and
syntactic structure of the phrase.
1724
2 RNN Encoder?Decoder
2.1 Preliminary: Recurrent Neural Networks
A recurrent neural network (RNN) is a neural net-
work that consists of a hidden state h and an
optional output y which operates on a variable-
length sequence x = (x
1
, . . . , x
T
). At each time
step t, the hidden state h
?t?
of the RNN is updated
by
h
?t?
= f
(
h
?t?1?
, x
t
)
, (1)
where f is a non-linear activation func-
tion. f may be as simple as an element-
wise logistic sigmoid function and as com-
plex as a long short-term memory (LSTM)
unit (Hochreiter and Schmidhuber, 1997).
An RNN can learn a probability distribution
over a sequence by being trained to predict the
next symbol in a sequence. In that case, the output
at each timestep t is the conditional distribution
p(x
t
| x
t?1
, . . . , x
1
). For example, a multinomial
distribution (1-of-K coding) can be output using a
softmax activation function
p(x
t,j
= 1 | x
t?1
, . . . , x
1
) =
exp
(
w
j
h
?t?
)
?
K
j
?
=1
exp
(
w
j
?
h
?t?
)
,
(2)
for all possible symbols j = 1, . . . ,K, where w
j
are the rows of a weight matrix W. By combining
these probabilities, we can compute the probabil-
ity of the sequence x using
p(x) =
T
?
t=1
p(x
t
| x
t?1
, . . . , x
1
). (3)
From this learned distribution, it is straightfor-
ward to sample a new sequence by iteratively sam-
pling a symbol at each time step.
2.2 RNN Encoder?Decoder
In this paper, we propose a novel neural network
architecture that learns to encode a variable-length
sequence into a fixed-length vector representation
and to decode a given fixed-length vector rep-
resentation back into a variable-length sequence.
From a probabilistic perspective, this new model
is a general method to learn the conditional dis-
tribution over a variable-length sequence condi-
tioned on yet another variable-length sequence,
e.g. p(y
1
, . . . , y
T
?
| x
1
, . . . , x
T
), where one
x1 x2 xT
yT' y2 y1
c
Decoder
Encoder
Figure 1: An illustration of the proposed RNN
Encoder?Decoder.
should note that the input and output sequence
lengths T and T
?
may differ.
The encoder is an RNN that reads each symbol
of an input sequence x sequentially. As it reads
each symbol, the hidden state of the RNN changes
according to Eq. (1). After reading the end of
the sequence (marked by an end-of-sequence sym-
bol), the hidden state of the RNN is a summary c
of the whole input sequence.
The decoder of the proposed model is another
RNN which is trained to generate the output se-
quence by predicting the next symbol y
t
given the
hidden state h
?t?
. However, unlike the RNN de-
scribed in Sec. 2.1, both y
t
and h
?t?
are also con-
ditioned on y
t?1
and on the summary c of the input
sequence. Hence, the hidden state of the decoder
at time t is computed by,
h
?t?
= f
(
h
?t?1?
, y
t?1
, c
)
,
and similarly, the conditional distribution of the
next symbol is
P (y
t
|y
t?1
, y
t?2
, . . . , y
1
, c) = g
(
h
?t?
, y
t?1
, c
)
.
for given activation functions f and g (the latter
must produce valid probabilities, e.g. with a soft-
max).
See Fig. 1 for a graphical depiction of the pro-
posed model architecture.
The two components of the proposed RNN
Encoder?Decoder are jointly trained to maximize
the conditional log-likelihood
max
?
1
N
N
?
n=1
log p
?
(y
n
| x
n
), (4)
1725
where ? is the set of the model parameters and
each (x
n
,y
n
) is an (input sequence, output se-
quence) pair from the training set. In our case,
as the output of the decoder, starting from the in-
put, is differentiable, we can use a gradient-based
algorithm to estimate the model parameters.
Once the RNN Encoder?Decoder is trained, the
model can be used in two ways. One way is to use
the model to generate a target sequence given an
input sequence. On the other hand, the model can
be used to score a given pair of input and output
sequences, where the score is simply a probability
p
?
(y | x) from Eqs. (3) and (4).
2.3 Hidden Unit that Adaptively Remembers
and Forgets
In addition to a novel model architecture, we also
propose a new type of hidden unit (f in Eq. (1))
that has been motivated by the LSTM unit but is
much simpler to compute and implement.
1
Fig. 2
shows the graphical depiction of the proposed hid-
den unit.
Let us describe how the activation of the j-th
hidden unit is computed. First, the reset gate r
j
is
computed by
r
j
= ?
(
[W
r
x]
j
+
[
U
r
h
?t?1?
]
j
)
, (5)
where ? is the logistic sigmoid function, and [.]
j
denotes the j-th element of a vector. x and h
t?1
are the input and the previous hidden state, respec-
tively. W
r
and U
r
are weight matrices which are
learned.
Similarly, the update gate z
j
is computed by
z
j
= ?
(
[W
z
x]
j
+
[
U
z
h
?t?1?
]
j
)
. (6)
The actual activation of the proposed unit h
j
is
then computed by
h
?t?
j
= z
j
h
?t?1?
j
+ (1? z
j
)
?
h
?t?
j
, (7)
where
?
h
?t?
j
= ?
(
[Wx]
j
+
[
U
(
r h
?t?1?
)]
j
)
. (8)
In this formulation, when the reset gate is close
to 0, the hidden state is forced to ignore the pre-
vious hidden state and reset with the current input
1
The LSTM unit, which has shown impressive results in
several applications such as speech recognition, has a mem-
ory cell and four gating units that adaptively control the in-
formation flow inside the unit, compared to only two gating
units in the proposed hidden unit. For details on LSTM net-
works, see, e.g., (Graves, 2012).
z
r
h h
~ x
Figure 2: An illustration of the proposed hidden
activation function. The update gate z selects
whether the hidden state is to be updated with
a new hidden state
?
h. The reset gate r decides
whether the previous hidden state is ignored. See
Eqs. (5)?(8) for the detailed equations of r, z, h
and
?
h.
only. This effectively allows the hidden state to
drop any information that is found to be irrelevant
later in the future, thus, allowing a more compact
representation.
On the other hand, the update gate controls how
much information from the previous hidden state
will carry over to the current hidden state. This
acts similarly to the memory cell in the LSTM
network and helps the RNN to remember long-
term information. Furthermore, this may be con-
sidered an adaptive variant of a leaky-integration
unit (Bengio et al., 2013).
As each hidden unit has separate reset and up-
date gates, each hidden unit will learn to capture
dependencies over different time scales. Those
units that learn to capture short-term dependencies
will tend to have reset gates that are frequently ac-
tive, but those that capture longer-term dependen-
cies will have update gates that are mostly active.
In our preliminary experiments, we found that
it is crucial to use this new unit with gating units.
We were not able to get meaningful result with an
oft-used tanh unit without any gating.
3 Statistical Machine Translation
In a commonly used statistical machine translation
system (SMT), the goal of the system (decoder,
specifically) is to find a translation f given a source
sentence e, which maximizes
p(f | e) ? p(e | f)p(f),
where the first term at the right hand side is called
translation model and the latter language model
(see, e.g., (Koehn, 2005)). In practice, however,
most SMT systems model log p(f | e) as a log-
linear model with additional features and corre-
1726
sponding weights:
log p(f | e) =
N
?
n=1
w
n
f
n
(f , e) + logZ(e), (9)
where f
n
and w
n
are the n-th feature and weight,
respectively. Z(e) is a normalization constant that
does not depend on the weights. The weights are
often optimized to maximize the BLEU score on a
development set.
In the phrase-based SMT framework
introduced in (Koehn et al., 2003) and
(Marcu and Wong, 2002), the translation model
log p(e | f) is factorized into the translation
probabilities of matching phrases in the source
and target sentences.
2
These probabilities are
once again considered additional features in the
log-linear model (see Eq. (9)) and are weighted
accordingly to maximize the BLEU score.
Since the neural net language model was pro-
posed in (Bengio et al., 2003), neural networks
have been used widely in SMT systems. In
many cases, neural networks have been used to
rescore translation hypotheses (n-best lists) (see,
e.g., (Schwenk et al., 2006)). Recently, however,
there has been interest in training neural networks
to score the translated sentence (or phrase pairs)
using a representation of the source sentence as
an additional input. See, e.g., (Schwenk, 2012),
(Son et al., 2012) and (Zou et al., 2013).
3.1 Scoring Phrase Pairs with RNN
Encoder?Decoder
Here we propose to train the RNN Encoder?
Decoder (see Sec. 2.2) on a table of phrase pairs
and use its scores as additional features in the log-
linear model in Eq. (9) when tuning the SMT de-
coder.
When we train the RNN Encoder?Decoder, we
ignore the (normalized) frequencies of each phrase
pair in the original corpora. This measure was
taken in order (1) to reduce the computational ex-
pense of randomly selecting phrase pairs from a
large phrase table according to the normalized fre-
quencies and (2) to ensure that the RNN Encoder?
Decoder does not simply learn to rank the phrase
pairs according to their numbers of occurrences.
One underlying reason for this choice was that the
existing translation probability in the phrase ta-
ble already reflects the frequencies of the phrase
2
Without loss of generality, from here on, we refer to
p(e | f) for each phrase pair as a translation model as well
pairs in the original corpus. With a fixed capacity
of the RNN Encoder?Decoder, we try to ensure
that most of the capacity of the model is focused
toward learning linguistic regularities, i.e., distin-
guishing between plausible and implausible trans-
lations, or learning the ?manifold? (region of prob-
ability concentration) of plausible translations.
Once the RNN Encoder?Decoder is trained, we
add a new score for each phrase pair to the exist-
ing phrase table. This allows the new scores to en-
ter into the existing tuning algorithm with minimal
additional overhead in computation.
As Schwenk pointed out in (Schwenk, 2012),
it is possible to completely replace the existing
phrase table with the proposed RNN Encoder?
Decoder. In that case, for a given source phrase,
the RNN Encoder?Decoder will need to generate
a list of (good) target phrases. This requires, how-
ever, an expensive sampling procedure to be per-
formed repeatedly. In this paper, thus, we only
consider rescoring the phrase pairs in the phrase
table.
3.2 Related Approaches: Neural Networks in
Machine Translation
Before presenting the empirical results, we discuss
a number of recent works that have proposed to
use neural networks in the context of SMT.
Schwenk in (Schwenk, 2012) proposed a simi-
lar approach of scoring phrase pairs. Instead of the
RNN-based neural network, he used a feedforward
neural network that has fixed-size inputs (7 words
in his case, with zero-padding for shorter phrases)
and fixed-size outputs (7 words in the target lan-
guage). When it is used specifically for scoring
phrases for the SMT system, the maximum phrase
length is often chosen to be small. However, as the
length of phrases increases or as we apply neural
networks to other variable-length sequence data,
it is important that the neural network can han-
dle variable-length input and output. The pro-
posed RNN Encoder?Decoder is well-suited for
these applications.
Similar to (Schwenk, 2012), Devlin et al.
(Devlin et al., 2014) proposed to use a feedfor-
ward neural network to model a translation model,
however, by predicting one word in a target phrase
at a time. They reported an impressive improve-
ment, but their approach still requires the maxi-
mum length of the input phrase (or context words)
to be fixed a priori.
1727
Although it is not exactly a neural network they
train, the authors of (Zou et al., 2013) proposed
to learn a bilingual embedding of words/phrases.
They use the learned embedding to compute the
distance between a pair of phrases which is used
as an additional score of the phrase pair in an SMT
system.
In (Chandar et al., 2014), a feedforward neural
network was trained to learn a mapping from a
bag-of-words representation of an input phrase to
an output phrase. This is closely related to both the
proposed RNN Encoder?Decoder and the model
proposed in (Schwenk, 2012), except that their in-
put representation of a phrase is a bag-of-words.
A similar approach of using bag-of-words repre-
sentations was proposed in (Gao et al., 2013) as
well. Earlier, a similar encoder?decoder model us-
ing two recursive neural networks was proposed
in (Socher et al., 2011), but their model was re-
stricted to a monolingual setting, i.e. the model
reconstructs an input sentence. More recently, an-
other encoder?decoder model using an RNN was
proposed in (Auli et al., 2013), where the de-
coder is conditioned on a representation of either
a source sentence or a source context.
One important difference between the pro-
posed RNN Encoder?Decoder and the approaches
in (Zou et al., 2013) and (Chandar et al., 2014) is
that the order of the words in source and tar-
get phrases is taken into account. The RNN
Encoder?Decoder naturally distinguishes between
sequences that have the same words but in a differ-
ent order, whereas the aforementioned approaches
effectively ignore order information.
The closest approach related to the proposed
RNN Encoder?Decoder is the Recurrent Contin-
uous Translation Model (Model 2) proposed in
(Kalchbrenner and Blunsom, 2013). In their pa-
per, they proposed a similar model that consists
of an encoder and decoder. The difference with
our model is that they used a convolutional n-gram
model (CGM) for the encoder and the hybrid of
an inverse CGM and a recurrent neural network
for the decoder. They, however, evaluated their
model on rescoring the n-best list proposed by the
conventional SMT system and computing the per-
plexity of the gold standard translations.
4 Experiments
We evaluate our approach on the English/French
translation task of the WMT?14 workshop.
4.1 Data and Baseline System
Large amounts of resources are available to build
an English/French SMT system in the framework
of the WMT?14 translation task. The bilingual
corpora include Europarl (61M words), news com-
mentary (5.5M), UN (421M), and two crawled
corpora of 90M and 780M words respectively.
The last two corpora are quite noisy. To train
the French language model, about 712M words of
crawled newspaper material is available in addi-
tion to the target side of the bitexts. All the word
counts refer to French words after tokenization.
It is commonly acknowledged that training sta-
tistical models on the concatenation of all this
data does not necessarily lead to optimal per-
formance, and results in extremely large mod-
els which are difficult to handle. Instead, one
should focus on the most relevant subset of the
data for a given task. We have done so by
applying the data selection method proposed in
(Moore and Lewis, 2010), and its extension to bi-
texts (Axelrod et al., 2011). By these means we
selected a subset of 418M words out of more
than 2G words for language modeling and a
subset of 348M out of 850M words for train-
ing the RNN Encoder?Decoder. We used the
test set newstest2012 and 2013 for data
selection and weight tuning with MERT, and
newstest2014 as our test set. Each set has
more than 70 thousand words and a single refer-
ence translation.
For training the neural networks, including the
proposed RNN Encoder?Decoder, we limited the
source and target vocabulary to the most frequent
15,000 words for both English and French. This
covers approximately 93% of the dataset. All the
out-of-vocabulary words were mapped to a special
token ([UNK]).
The baseline phrase-based SMT system was
built using Moses with default settings. This sys-
tem achieves a BLEU score of 30.64 and 33.3 on
the development and test sets, respectively (see Ta-
ble 1).
4.1.1 RNN Encoder?Decoder
The RNN Encoder?Decoder used in the experi-
ment had 1000 hidden units with the proposed
gates at the encoder and at the decoder. The in-
put matrix between each input symbol x
?t?
and the
hidden unit is approximated with two lower-rank
matrices, and the output matrix is approximated
1728
Models
BLEU
dev test
Baseline 30.64 33.30
RNN 31.20 33.87
CSLM + RNN 31.48 34.64
CSLM + RNN + WP 31.50 34.54
Table 1: BLEU scores computed on the develop-
ment and test sets using different combinations of
approaches. WP denotes a word penalty, where
we penalizes the number of unknown words to
neural networks.
similarly. We used rank-100 matrices, equivalent
to learning an embedding of dimension 100 for
each word. The activation function used for
?
h in
Eq. (8) is a hyperbolic tangent function. The com-
putation from the hidden state in the decoder to
the output is implemented as a deep neural net-
work (Pascanu et al., 2014) with a single interme-
diate layer having 500 maxout units each pooling
2 inputs (Goodfellow et al., 2013).
All the weight parameters in the RNN Encoder?
Decoder were initialized by sampling from an
isotropic zero-mean (white) Gaussian distribution
with its standard deviation fixed to 0.01, except
for the recurrent weight parameters. For the re-
current weight matrices, we first sampled from a
white Gaussian distribution and used its left singu-
lar vectors matrix, following (Saxe et al., 2014).
We used Adadelta and stochastic gradient
descent to train the RNN Encoder?Decoder
with hyperparameters  = 10
?6
and ? =
0.95 (Zeiler, 2012). At each update, we used 64
randomly selected phrase pairs from a phrase ta-
ble (which was created from 348M words). The
model was trained for approximately three days.
Details of the architecture used in the experi-
ments are explained in more depth in the supple-
mentary material.
4.1.2 Neural Language Model
In order to assess the effectiveness of scoring
phrase pairs with the proposed RNN Encoder?
Decoder, we also tried a more traditional approach
of using a neural network for learning a target
language model (CSLM) (Schwenk, 2007). Espe-
cially, the comparison between the SMT system
using CSLM and that using the proposed approach
of phrase scoring by RNN Encoder?Decoder will
clarify whether the contributions from multiple
neural networks in different parts of the SMT sys-
tem add up or are redundant.
We trained the CSLM model on 7-grams
from the target corpus. Each input word
was projected into the embedding space R
512
,
and they were concatenated to form a 3072-
dimensional vector. The concatenated vector was
fed through two rectified layers (of size 1536 and
1024) (Glorot et al., 2011). The output layer was
a simple softmax layer (see Eq. (2)). All the
weight parameters were initialized uniformly be-
tween ?0.01 and 0.01, and the model was trained
until the validation perplexity did not improve for
10 epochs. After training, the language model
achieved a perplexity of 45.80. The validation set
was a random selection of 0.1% of the corpus. The
model was used to score partial translations dur-
ing the decoding process, which generally leads to
higher gains in BLEU score than n-best list rescor-
ing (Vaswani et al., 2013).
To address the computational complexity of
using a CSLM in the decoder a buffer was
used to aggregate n-grams during the stack-
search performed by the decoder. Only when
the buffer is full, or a stack is about to
be pruned, the n-grams are scored by the
CSLM. This allows us to perform fast matrix-
matrix multiplication on GPU using Theano
(Bergstra et al., 2010; Bastien et al., 2012).
?60 ?50 ?40 ?30 ?20 ?10 0?14
?12
?10
?8
?6
?4
?2
0
RNN Scores (log)
TM 
Scor
es (lo
g)
Figure 3: The visualization of phrase pairs ac-
cording to their scores (log-probabilities) by the
RNN Encoder?Decoder and the translation model.
4.2 Quantitative Analysis
We tried the following combinations:
1. Baseline configuration
2. Baseline + RNN
3. Baseline + CSLM + RNN
4. Baseline + CSLM + RNN + Word penalty
1729
Source Translation Model RNN Encoder?Decoder
at the end of the [a la fin de la] [?r la fin des ann?ees] [?etre sup-
prim?es `a la fin de la]
[`a la fin du] [`a la fin des] [`a la fin de la]
for the first time [r
c
? pour la premir?ere fois] [?et?e donn?es pour
la premi`ere fois] [?et?e comm?emor?ee pour la
premi`ere fois]
[pour la premi`ere fois] [pour la premi`ere fois ,]
[pour la premi`ere fois que]
in the United States
and
[? aux ?tats-Unis et] [?et?e ouvertes aux
?
Etats-
Unis et] [?et?e constat?ees aux
?
Etats-Unis et]
[aux Etats-Unis et] [des Etats-Unis et] [des
?
Etats-Unis et]
, as well as [?s , qu?] [?s , ainsi que] [?re aussi bien que] [, ainsi qu?] [, ainsi que] [, ainsi que les]
one of the most [?t ?l? un des plus] [?l? un des plus] [?etre retenue
comme un de ses plus]
[l? un des] [le] [un des]
(a) Long, frequent source phrases
Source Translation Model RNN Encoder?Decoder
, Minister of Commu-
nications and Trans-
port
[Secr?etaire aux communications et aux trans-
ports :] [Secr?etaire aux communications et aux
transports]
[Secr?etaire aux communications et aux trans-
ports] [Secr?etaire aux communications et aux
transports :]
did not comply with
the
[vestimentaire , ne correspondaient pas `a des]
[susmentionn?ee n? ?etait pas conforme aux]
[pr?esent?ees n? ?etaient pas conformes `a la]
[n? ont pas respect?e les] [n? ?etait pas conforme
aux] [n? ont pas respect?e la]
parts of the world . [
c
? gions du monde .] [r?egions du monde con-
sid?er?ees .] [r?egion du monde consid?er?ee .]
[parties du monde .] [les parties du monde .]
[des parties du monde .]
the past few days . [le petit texte .] [cours des tout derniers jours .]
[les tout derniers jours .]
[ces derniers jours .] [les derniers jours .] [cours
des derniers jours .]
on Friday and Satur-
day
[vendredi et samedi `a la] [vendredi et samedi `a]
[se d?eroulera vendredi et samedi ,]
[le vendredi et le samedi] [le vendredi et samedi]
[vendredi et samedi]
(b) Long, rare source phrases
Table 2: The top scoring target phrases for a small set of source phrases according to the translation
model (direct translation probability) and by the RNN Encoder?Decoder. Source phrases were randomly
selected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a Cyrillic
letter ghe.
The results are presented in Table 1. As ex-
pected, adding features computed by neural net-
works consistently improves the performance over
the baseline performance.
The best performance was achieved when we
used both CSLM and the phrase scores from the
RNN Encoder?Decoder. This suggests that the
contributions of the CSLM and the RNN Encoder?
Decoder are not too correlated and that one can
expect better results by improving each method in-
dependently. Furthermore, we tried penalizing the
number of words that are unknown to the neural
networks (i.e. words which are not in the short-
list). We do so by simply adding the number of
unknown words as an additional feature the log-
linear model in Eq. (9).
3
However, in this case we
3
To understand the effect of the penalty, consider the set
of all words in the 15,000 large shortlist, SL. All words x
i
/?
SL are replaced by a special token [UNK] before being scored
by the neural networks. Hence, the conditional probability of
any x
i
t
/? SL is actually given by the model as
p (x
t
= [UNK] | x
<t
) = p (x
t
/? SL | x
<t
)
=
X
x
j
t
/?SL
p
?
x
j
t
| x
<t
?
? p
?
x
i
t
| x
<t
?
,
where x
<t
is a shorthand notation for x
t?1
, . . . , x
1
.
were not able to achieve better performance on the
test set, but only on the development set.
4.3 Qualitative Analysis
In order to understand where the performance im-
provement comes from, we analyze the phrase pair
scores computed by the RNN Encoder?Decoder
against the corresponding p(f | e) from the trans-
lation model. Since the existing translation model
relies solely on the statistics of the phrase pairs in
the corpus, we expect its scores to be better esti-
mated for the frequent phrases but badly estimated
for rare phrases. Also, as we mentioned earlier
in Sec. 3.1, we further expect the RNN Encoder?
Decoder which was trained without any frequency
information to score the phrase pairs based rather
on the linguistic regularities than on the statistics
of their occurrences in the corpus.
We focus on those pairs whose source phrase is
long (more than 3 words per source phrase) and
As a result, the probability of words not in the shortlist is
always overestimated. It is possible to address this issue by
backing off to an existing model that contain non-shortlisted
words (see (Schwenk, 2007)) In this paper, however, we opt
for introducing a word penalty instead, which counteracts the
word probability overestimation.
1730
Source Samples from RNN Encoder?Decoder
at the end of the [`a la fin de la] (?11)
for the first time [pour la premi`ere fois] (?24) [pour la premi`ere fois que] (?2)
in the United States and [aux
?
Etats-Unis et] (?6) [dans les
?
Etats-Unis et] (?4)
, as well as [, ainsi que] [,] [ainsi que] [, ainsi qu?] [et UNK]
one of the most [l? un des plus] (?9) [l? un des] (?5) [l? une des plus] (?2)
(a) Long, frequent source phrases
Source Samples from RNN Encoder?Decoder
, Minister of Communica-
tions and Transport
[ , ministre des communications et le transport] (?13)
did not comply with the [n? tait pas conforme aux] [n? a pas respect l?] (?2) [n? a pas respect la] (?3)
parts of the world . [arts du monde .] (?11) [des arts du monde .] (?7)
the past few days . [quelques jours .] (?5) [les derniers jours .] (?5) [ces derniers jours .] (?2)
on Friday and Saturday [vendredi et samedi] (?5) [le vendredi et samedi] (?7) [le vendredi et le samedi] (?4)
(b) Long, rare source phrases
Table 3: Samples generated from the RNN Encoder?Decoder for each source phrase used in Table 2. We
show the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder?Decoder scores.
Figure 4: 2?D embedding of the learned word representation. The left one shows the full embedding
space, while the right one shows a zoomed-in view of one region (color?coded). For more plots, see the
supplementary material.
frequent. For each such source phrase, we look
at the target phrases that have been scored high
either by the translation probability p(f | e) or
by the RNN Encoder?Decoder. Similarly, we per-
form the same procedure with those pairs whose
source phrase is long but rare in the corpus.
Table 2 lists the top-3 target phrases per source
phrase favored either by the translation model
or by the RNN Encoder?Decoder. The source
phrases were randomly chosen among long ones
having more than 4 or 5 words.
In most cases, the choices of the target phrases
by the RNN Encoder?Decoder are closer to ac-
tual or literal translations. We can observe that the
RNN Encoder?Decoder prefers shorter phrases in
general.
Interestingly, many phrase pairs were scored
similarly by both the translation model and the
RNN Encoder?Decoder, but there were as many
other phrase pairs that were scored radically dif-
ferent (see Fig. 3). This could arise from the
proposed approach of training the RNN Encoder?
Decoder on a set of unique phrase pairs, discour-
aging the RNN Encoder?Decoder from learning
simply the frequencies of the phrase pairs from the
corpus, as explained earlier.
Furthermore, in Table 3, we show for each of
the source phrases in Table 2, the generated sam-
ples from the RNN Encoder?Decoder. For each
source phrase, we generated 50 samples and show
the top-five phrases accordingly to their scores.
We can see that the RNN Encoder?Decoder is
able to propose well-formed target phrases with-
out looking at the actual phrase table. Importantly,
the generated phrases do not overlap completely
with the target phrases from the phrase table. This
encourages us to further investigate the possibility
of replacing the whole or a part of the phrase table
1731
Figure 5: 2?D embedding of the learned phrase representation. The top left one shows the full represen-
tation space (5000 randomly selected points), while the other three figures show the zoomed-in view of
specific regions (color?coded).
with the proposed RNN Encoder?Decoder in the
future.
4.4 Word and Phrase Representations
Since the proposed RNN Encoder?Decoder is not
specifically designed only for the task of machine
translation, here we briefly look at the properties
of the trained model.
It has been known for some time that
continuous space language models using
neural networks are able to learn seman-
tically meaningful embeddings (See, e.g.,
(Bengio et al., 2003; Mikolov et al., 2013)). Since
the proposed RNN Encoder?Decoder also projects
to and maps back from a sequence of words into
a continuous space vector, we expect to see a
similar property with the proposed model as well.
The left plot in Fig. 4 shows the 2?D embedding
of the words using the word embedding matrix
learned by the RNN Encoder?Decoder. The pro-
jection was done by the recently proposed Barnes-
Hut-SNE (van der Maaten, 2013). We can clearly
see that semantically similar words are clustered
with each other (see the zoomed-in plots in Fig. 4).
The proposed RNN Encoder?Decoder naturally
generates a continuous-space representation of a
phrase. The representation (c in Fig. 1) in this
case is a 1000-dimensional vector. Similarly to the
word representations, we visualize the representa-
tions of the phrases that consists of four or more
words using the Barnes-Hut-SNE in Fig. 5.
From the visualization, it is clear that the RNN
Encoder?Decoder captures both semantic and syn-
tactic structures of the phrases. For instance, in
the bottom-left plot, most of the phrases are about
the duration of time, while those phrases that are
syntactically similar are clustered together. The
bottom-right plot shows the cluster of phrases that
are semantically similar (countries or regions). On
the other hand, the top-right plot shows the phrases
that are syntactically similar.
5 Conclusion
In this paper, we proposed a new neural network
architecture, called an RNN Encoder?Decoder
that is able to learn the mapping from a sequence
1732
of an arbitrary length to another sequence, possi-
bly from a different set, of an arbitrary length. The
proposed RNN Encoder?Decoder is able to either
score a pair of sequences (in terms of a conditional
probability) or generate a target sequence given a
source sequence. Along with the new architecture,
we proposed a novel hidden unit that includes a re-
set gate and an update gate that adaptively control
how much each hidden unit remembers or forgets
while reading/generating a sequence.
We evaluated the proposed model with the task
of statistical machine translation, where we used
the RNN Encoder?Decoder to score each phrase
pair in the phrase table. Qualitatively, we were
able to show that the new model is able to cap-
ture linguistic regularities in the phrase pairs well
and also that the RNN Encoder?Decoder is able to
propose well-formed target phrases.
The scores by the RNN Encoder?Decoder were
found to improve the overall translation perfor-
mance in terms of BLEU scores. Also, we
found that the contribution by the RNN Encoder?
Decoder is rather orthogonal to the existing ap-
proach of using neural networks in the SMT sys-
tem, so that we can improve further the perfor-
mance by using, for instance, the RNN Encoder?
Decoder and the neural net language model to-
gether.
Our qualitative analysis of the trained model
shows that it indeed captures the linguistic regu-
larities in multiple levels i.e. at the word level as
well as phrase level. This suggests that there may
be more natural language related applications that
may benefit from the proposed RNN Encoder?
Decoder.
The proposed architecture has large potential
for further improvement and analysis. One ap-
proach that was not investigated here is to re-
place the whole, or a part of the phrase table by
letting the RNN Encoder?Decoder propose target
phrases. Also, noting that the proposed model is
not limited to being used with written language,
it will be an important future research to apply the
proposed architecture to other applications such as
speech transcription.
Acknowledgments
KC, BM, CG, DB and YB would like to thank
NSERC, Calcul Qu?ebec, Compute Canada, the
Canada Research Chairs and CIFAR. FB and HS
were partially funded by the European Commis-
sion under the project MateCat, and by DARPA
under the BOLT project.
References
[Auli et al.2013] Michael Auli, Michel Galley, Chris
Quirk, and Geoffrey Zweig. 2013. Joint language
and translation modeling with recurrent neural net-
works. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1044?1054.
[Axelrod et al.2011] Amittai Axelrod, Xiaodong He,
and Jianfeng Gao. 2011. Domain adaptation via
pseudo in-domain data selection. In Proceedings of
the ACL Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 355?362.
[Bastien et al.2012] Fr?ed?eric Bastien, Pascal Lamblin,
Razvan Pascanu, James Bergstra, Ian J. Goodfellow,
Arnaud Bergeron, Nicolas Bouchard, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. Deep Learning and Unsupervised Fea-
ture Learning NIPS 2012 Workshop.
[Bengio et al.2003] Yoshua Bengio, R?ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neu-
ral probabilistic language model. J. Mach. Learn.
Res., 3:1137?1155, March.
[Bengio et al.2013] Y. Bengio, N. Boulanger-
Lewandowski, and R. Pascanu. 2013. Advances
in optimizing recurrent networks. In Proceedings
of the 38th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2013),
May.
[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: a CPU
and GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
[Chandar et al.2014] Sarath Chandar, Stanislas Lauly,
Hugo Larochelle, Mitesh Khapra, Balaraman Ravin-
dran, Vikas Raykar, and Amrita Saha. 2014. An au-
toencoder approach to learning bilingual word repre-
sentations. arXiv:1402.1454 [cs.CL], Febru-
ary.
[Dahl et al.2012] George E. Dahl, Dong Yu, Li Deng,
and Alex Acero. 2012. Context-dependent pre-
trained deep neural networks for large vocabulary
speech recognition. IEEE Transactions on Audio,
Speech, and Language Processing, 20(1):33?42.
[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, , and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the ACL
2014 Conference, ACL ?14, pages 1370?1380.
1733
[Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau
Yih, and Li Deng. 2013. Learning semantic repre-
sentations for the phrase translation model. Techni-
cal report, Microsoft Research.
[Glorot et al.2011] X. Glorot, A. Bordes, and Y. Ben-
gio. 2011. Deep sparse rectifier neural networks. In
AISTATS?2011.
[Goodfellow et al.2013] Ian J. Goodfellow, David
Warde-Farley, Mehdi Mirza, Aaron Courville, and
Yoshua Bengio. 2013. Maxout networks. In
ICML?2013.
[Graves2012] Alex Graves. 2012. Supervised Se-
quence Labelling with Recurrent Neural Networks.
Studies in Computational Intelligence. Springer.
[Hochreiter and Schmidhuber1997] S. Hochreiter and
J. Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735?1780.
[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Two recurrent continuous
translation models. In Proceedings of the ACL Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1700?1709.
[Koehn et al.2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54.
[Koehn2005] P. Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, pages 79?86, Phuket, Thai-
land.
[Krizhevsky et al.2012] Alex Krizhevsky, Ilya
Sutskever, and Geoffrey Hinton. 2012. Ima-
geNet classification with deep convolutional neural
networks. In Advances in Neural Information
Processing Systems 25 (NIPS?2012).
[Marcu and Wong2002] Daniel Marcu and William
Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Pro-
ceedings of the ACL-02 Conference on Empirical
Methods in Natural Language Processing - Volume
10, EMNLP ?02, pages 133?139.
[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and
their compositionality. In Advances in Neural Infor-
mation Processing Systems 26, pages 3111?3119.
[Moore and Lewis2010] Robert C. Moore and William
Lewis. 2010. Intelligent selection of language
model training data. In Proceedings of the ACL
2010 Conference Short Papers, ACLShort ?10,
pages 220?224, Stroudsburg, PA, USA.
[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho,
and Y. Bengio. 2014. How to construct deep recur-
rent neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.
[Saxe et al.2014] Andrew M. Saxe, James L. McClel-
land, and Surya Ganguli. 2014. Exact solutions
to the nonlinear dynamics of learning in deep lin-
ear neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.
[Schwenk et al.2006] Holger Schwenk, Marta R. Costa-
Juss`a, and Jos?e A. R. Fonollosa. 2006. Continuous
space language models for the iwslt 2006 task. In
IWSLT, pages 166?173.
[Schwenk2007] Holger Schwenk. 2007. Continuous
space language models. Comput. Speech Lang.,
21(3):492?518, July.
[Schwenk2012] Holger Schwenk. 2012. Continuous
space translation models for phrase-based statisti-
cal machine translation. In Martin Kay and Chris-
tian Boitet, editors, Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLIN), pages 1071?1080.
[Socher et al.2011] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Andrew Y. Ng, and Christopher D.
Manning. 2011. Dynamic pooling and unfolding
recursive autoencoders for paraphrase detection. In
Advances in Neural Information Processing Systems
24.
[Son et al.2012] Le Hai Son, Alexandre Allauzen, and
Franc?ois Yvon. 2012. Continuous space transla-
tion models with neural networks. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ?12,
pages 39?48, Stroudsburg, PA, USA.
[van der Maaten2013] Laurens van der Maaten. 2013.
Barnes-hut-sne. In Proceedings of the First Inter-
national Conference on Learning Representations
(ICLR 2013), May.
[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. De-
coding with large-scale neural language models im-
proves translation. Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1387?1392.
[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
an adaptive learning rate method. Technical report,
arXiv 1212.5701.
[Zou et al.2013] Will Y. Zou, Richard Socher,
Daniel M. Cer, and Christopher D. Manning.
2013. Bilingual word embeddings for phrase-based
machine translation. In Proceedings of the ACL
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1393?1398.
1734
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 78?85,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Overcoming the Curse of Sentence Length for Neural Machine
Translation using Automatic Segmentation
Jean Pouget-Abadie
?
Ecole Polytechnique, France
Dzmitry Bahdanau
?
Jacobs University Bremen, Germany
Bart van Merri
?
enboer Kyunghyun Cho
Universit?e de Montr?eal, Canada
Yoshua Bengio
Universit?e de Montr?eal, Canada
CIFAR Senior Fellow
Abstract
The authors of (Cho et al., 2014a) have
shown that the recently introduced neural
network translation systems suffer from
a significant drop in translation quality
when translating long sentences, unlike
existing phrase-based translation systems.
In this paper, we propose a way to ad-
dress this issue by automatically segment-
ing an input sentence into phrases that can
be easily translated by the neural network
translation model. Once each segment has
been independently translated by the neu-
ral machine translation model, the trans-
lated clauses are concatenated to form a
final translation. Empirical results show
a significant improvement in translation
quality for long sentences.
1 Introduction
Up to now, most research efforts in statistical ma-
chine translation (SMT) research have relied on
the use of a phrase-based system as suggested
in (Koehn et al., 2003). Recently, however, an
entirely new, neural network based approach has
been proposed by several research groups (Kalch-
brenner and Blunsom, 2013; Sutskever et al.,
2014; Cho et al., 2014b), showing promising re-
sults, both as a standalone system or as an addi-
tional component in the existing phrase-based sys-
tem. In this neural network based approach, an en-
coder ?encodes? a variable-length input sentence
into a fixed-length vector and a decoder ?decodes?
a variable-length target sentence from the fixed-
length encoded vector.
It has been observed in (Sutskever et al., 2014),
(Kalchbrenner and Blunsom, 2013) and (Cho et
al., 2014a) that this neural network approach
?
Research done while these authors were visiting Uni-
versit?e de Montr?eal
works well with short sentences (e.g., / 20
words), but has difficulty with long sentences (e.g.,
' 20 words), and particularly with sentences that
are longer than those used for training. Training
on long sentences is difficult because few available
training corpora include sufficiently many long
sentences, and because the computational over-
head of each update iteration in training is linearly
correlated with the length of training sentences.
Additionally, by the nature of encoding a variable-
length sentence into a fixed-size vector representa-
tion, the neural network may fail to encode all the
important details.
In this paper, hence, we propose to translate sen-
tences piece-wise. We segment an input sentence
into a number of short clauses that can be confi-
dently translated by the model. We show empiri-
cally that this approach improves translation qual-
ity of long sentences, compared to using a neural
network to translate a whole sentence without seg-
mentation.
2 Background: RNN Encoder?Decoder
for Translation
The RNN Encoder?Decoder (RNNenc) model is
a recent implementation of the encoder?decoder
approach, proposed independently in (Cho et al.,
2014b) and in (Sutskever et al., 2014). It consists
of two RNNs, acting respectively as encoder and
decoder.
The encoder of the RNNenc reads each word in
a source sentence one by one while maintaining a
hidden state. The hidden state computed at the end
of the source sentence then summarizes the whole
input sentence. Formally, given an input sentence
x = (x
1
, ? ? ? , x
T
x
), the encoder computes
h
t
= f (x
t
, h
t?1
) ,
where f is a nonlinear function computing the next
hidden state given the previous one and the current
input word.
78
x1 x2 xT
yT' y2 y1
c
Decoder
Encoder
Figure 1: An illustration of the RNN Encoder?
Decoder. Reprinted from (Cho et al., 2014b).
From the last hidden state of the encoder, we
compute a context vector c on which the decoder
will be conditioned:
c = g(h
T
x
),
where g may simply be a linear affine transforma-
tion of h
T
x
.
The decoder, on the other hand, generates each
target word at a time, until the end-of-sentence
symbol is generated. It generates a word at a time
given the context vector (from the encoder), a pre-
vious hidden state (of the decoder) and the word
generated at the last step. More formally, the de-
coder computes at each time its hidden state by
s
t
= f (y
t?1
, s
t?1
, c) .
With the newly computed hidden state, the de-
coder outputs the probability distribution over all
possible target words by:
p(f
t,j
= 1 | f
t?1
, . . . , f
1
, c) =
exp
(
w
j
h
?t?
)
?
K
j
?
=1
exp
(
w
j
?
h
?t?
)
, (1)
where f
t,j
is the indicator variable for the j-th
word in the target vocabulary at time t and only
a single indicator variable is on (= 1) each time.
See Fig. 1 for the graphical illustration of the
RNNenc.
The RNNenc in (Cho et al., 2014b) uses a spe-
cial hidden unit that adaptively forgets or remem-
bers the previous hidden state such that the acti-
vation of a hidden unit h
?t?
j
at time t is computed
by
h
?t?
j
= z
j
h
?t?1?
j
+ (1? z
j
)
?
h
?t?
j
,
where
?
h
?t?
j
=f
(
[Wx]
j
+
[
U
(
r h
?t?1?
)]
)
,
z
j
=?
(
[W
z
x]
j
+
[
U
z
h
?t?1?
]
j
)
,
r
j
=?
(
[W
r
x]
j
+
[
U
r
h
?t?1?
]
j
)
.
z
j
and r
j
are respectively the update and reset
gates.  is an element-wise multiplication. In
the remaining of this paper, we always assume that
this hidden unit is used in the RNNenc.
Although the model in (Cho et al., 2014b) was
originally trained on phrase pairs, it is straight-
forward to train the same model with a bilin-
gual, parallel corpus consisting of sentence pairs
as has been done in (Sutskever et al., 2014). In
the remainder of this paper, we use the RNNenc
trained on English?French sentence pairs (Cho et
al., 2014a).
3 Automatic Segmentation and
Translation
One hypothesis explaining the difficulty encoun-
tered by the RNNenc model when translating long
sentences is that a plain, fixed-length vector lacks
the capacity to encode a long sentence. When en-
coding a long input sentence, the encoder may lose
track of all the subtleties in the sentence. Con-
sequently, the decoder has difficulties recovering
the correct translation from the encoded represen-
tation. One solution would be to build a larger
model with a larger representation vector to in-
crease the capacity of the model at the price of
higher computational cost.
In this section, however, we propose to segment
an input sentence such that each segmented clause
can be easily translated by the RNN Encoder?
Decoder. In other words, we wish to find a
segmentation that maximizes the total confidence
score which is a sum of the confidence scores of
the phrases in the segmentation. Once the confi-
dence score is defined, the problem of finding the
best segmentation can be formulated as an integer
programming problem.
Let e = (e
1
, ? ? ? , e
n
) be a source sentence com-
posed of words e
k
. We denote a phrase, which is a
subsequence of e, with e
ij
= (e
i
, ? ? ? , e
j
).
79
We use the RNN Encoder?Decoder to measure
how confidently we can translate a subsequence
e
ij
by considering the log-probability log p(f
k
|
e
ij
) of a candidate translation f
k
generated by the
model. In addition to the log-probability, we also
use the log-probability log p(e
ij
| f
k
) from a re-
verse RNN Encoder?Decoder (translating from a
target language to source language). With these
two probabilities, we define the confidence score
of a phrase pair (e
ij
, f
k
) as:
c(e
ij
, f
k
) =
log p(f
k
| e
ij
) + log q(e
ij
| f
k
)
2 |log(j ? i+ 1)|
,
(2)
where the denominator penalizes a short segment
whose probability is known to be overestimated by
an RNN (Graves, 2013).
The confidence score of a source phrase only is
then defined as
c
ij
= max
k
c(e
ij
, f
k
). (3)
We use an approximate beam search to search for
the candidate translations f
k
of e
ij
, that maximize
log-likelihood log p(f
k
|e
ij
) (Graves et al., 2013;
Boulanger-Lewandowski et al., 2013).
Let x
ij
be an indicator variable equal to 1 if we
include a phrase e
ij
in the segmentation, and oth-
erwise, 0. We can rewrite the segmentation prob-
lem as the optimization of the following objective
function:
max
x
?
i?j
c
ij
x
ij
= x ? c (4)
subject to ?k, n
k
= 1
n
k
=
?
i,j
x
ij
1
i?k?j
is the number of source
phrases chosen in the segmentation containing
word e
k
.
The constraint in Eq. (4) states that for each
word e
k
in the sentence one and only one of the
source phrases contains this word, (e
ij
)
i?k?j
, is
included in the segmentation. The constraint ma-
trix is totally unimodular making this integer pro-
gramming problem solvable in polynomial time.
Let S
k
j
be the first index of the k-th segment
counting from the last phrase of the optimal seg-
mentation of subsequence e
1j
(S
j
:= S
1
j
), and s
j
be the corresponding score of this segmentation
(s
0
:= 0). Then, the following relations hold:
s
j
= max
1?i?j
(c
ij
+ s
i?1
), ?j ? 1 (5)
S
j
=argmax
1?i?j
(c
ij
+ s
i?1
), ?j ? 1 (6)
With Eq. (5) we can evaluate s
j
incrementally.
With the evaluated s
j
?s, we can compute S
j
as
well (Eq. (6)). By the definition of S
k
j
we find the
optimal segmentation by decomposing e
1n
into
e
S
k
n
,S
k?1
n
?1
, ? ? ? , e
S
2
n
,S
1
n
?1
, e
S
1
n
,n
, where k is the
index of the first one in the sequence S
k
n
. This
approach described above requires quadratic time
with respect to sentence length.
3.1 Issues and Discussion
The proposed segmentation approach does not
avoid the problem of reordering clauses. Unless
the source and target languages follow roughly the
same order, such as in English to French transla-
tions, a simple concatenation of translated clauses
will not necessarily be grammatically correct.
Despite the lack of long-distance reordering
1
in
the current approach, we find nonetheless signifi-
cant gains in the translation performance of neural
machine translation. A mechanism to reorder the
obtained clause translations is, however, an impor-
tant future research question.
Another issue at the heart of any purely neu-
ral machine translation is the limited model vo-
cabulary size for both source and target languages.
As shown in (Cho et al., 2014a), translation qual-
ity drops considerably with just a few unknown
words present in the input sentence. Interestingly
enough, the proposed segmentation approach ap-
pears to be more robust to the presence of un-
known words (see Sec. 5). One intuition is that the
segmentation leads to multiple short clauses with
less unknown words, which leads to more stable
translation of each clause by the neural translation
model.
Finally, the proposed approach is computation-
ally expensive as it requires scoring all the sub-
phrases of an input sentence. However, the scoring
process can be easily sped up by scoring phrases
in parallel, since each phrase can be scored inde-
pendently. Another way to speed up the segmen-
tation, other than parallelization, would be to use
1
Note that, inside each clause, the words are reordered
automatically when the clause is translated by the RNN
Encoder?Decoder.
80
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
25
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
25
30
35
40
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(a) RNNenc without
segmentation
(b) RNNenc with segmentation
(c) Moses
Figure 2: The BLEU scores achieved by (a) the RNNenc without segmentation, (b) the RNNenc
with the penalized reverse confidence score, and (c) the phrase-based translation system Moses on a
newstest12-14.
an existing parser to segment a sentence into a set
of clauses.
4 Experiment Settings
4.1 Dataset
We evaluate the proposed approach on the task
of English-to-French translation. We use a bilin-
gual, parallel corpus of 348M words selected
by the method of (Axelrod et al., 2011) from
a combination of Europarl (61M), news com-
mentary (5.5M), UN (421M) and two crawled
corpora of 90M and 780M words respectively.
2
The performance of our models was tested
on news-test2012, news-test2013, and
news-test2014. When comparing with the
phrase-based SMT system Moses (Koehn et al.,
2007), the first two were used as a development set
for tuning Moses while news-test2014 was
used as our test set.
To train the neural network models, we use only
the sentence pairs in the parallel corpus, where
both English and French sentences are at most 30
words long. Furthermore, we limit our vocabu-
lary size to the 30,000 most frequent words for
both English and French. All other words are con-
sidered unknown and mapped to a special token
([UNK]).
In both neural network training and automatic
segmentation, we do not incorporate any domain-
specific knowledge, except when tokenizing the
original text data.
2
The datasets and trained Moses models can be down-
loaded from http://www-lium.univ-lemans.fr/
?
schwenk/cslm_joint_paper/ and the website of
ACL 2014 Ninth Workshop on Statistical Machine Transla-
tion (WMT 14).
4.2 Models and Approaches
We compare the proposed segmentation-based
translation scheme against the same neural net-
work model translations without segmentation.
The neural machine translation is done by an RNN
Encoder?Decoder (RNNenc) (Cho et al., 2014b)
trained to maximize the conditional probability
of a French translation given an English sen-
tence. Once the RNNenc is trained, an approxi-
mate beam-search is used to find possible transla-
tions with high likelihood.
3
This RNNenc is used for the proposed
segmentation-based approach together with an-
other RNNenc trained to translate from French to
English. The two RNNenc?s are used in the pro-
posed segmentation algorithm to compute the con-
fidence score of each phrase (See Eqs. (2)?(3)).
We also compare with the translations of a con-
ventional phrase-based machine translation sys-
tem, which we expect to be more robust when
translating long sentences.
5 Results and Analysis
5.1 Validity of the Automatic Segmentation
We validate the proposed segmentation algorithm
described in Sec. 3 by comparing against two
baseline segmentation approaches. The first one
randomly segments an input sentence such that the
distribution of the lengths of random segments has
its mean and variance identical to those of the seg-
ments produced by our algorithm. The second ap-
proach follows the proposed algorithm, however,
using a uniform random confidence score.
From Table 1 we can clearly see that the pro-
3
In all experiments, the beam width is 10.
81
Model Test set
No segmentation 13.15
Random segmentation 16.60
Random confidence score 16.76
Proposed segmentation 20.86
Table 1: BLEU score computed on
news-test2014 for two control experi-
ments. Random segmentation refers to randomly
segmenting a sentence so that the mean and
variance of the segment lengths corresponded to
the ones our best segmentation method. Random
confidence score refers to segmenting a sentence
with randomly generated confidence score for
each segment.
posed segmentation algorithm results in signifi-
cantly better performance. One interesting phe-
nomenon is that any random segmentation was
better than the direct translation without any seg-
mentation. This indirectly agrees well with the
previous finding in (Cho et al., 2014a) that the
neural machine translation suffers from long sen-
tences.
5.2 Importance of Using an Inverse Model
0 2 4 6 8 10
Max. number of unknown words
?9
?8
?7
?6
?5
?4
?3
?2
?1
0
B
L
E
U
s
c
o
r
e
d
e
c
r
e
a
s
e
With segm.
Without segm.
Figure 3: BLEU score loss vs. maximum number
of unknown words in source and target sentence
when translating with the RNNenc model with and
without segmentation.
The proposed confidence score averages the
scores of a translation model p(f | e) and an in-
verse translation model p(e | f) and penalizes for
short phrases. However, it is possible to use alter-
nate definitions of confidence score. For instance,
one may use only the ?direct? translation model or
varying penalties for phrase lengths.
In this section, we test three different confidence
score:
p(f | e) Using a single translation model
p(f | e) + p(e | f) Using both direct and reverse
translation models without the short phrase
penalty
p(f | e) + p(e | f) (p) Using both direct and re-
verse translation models together with the
short phrase penalty
The results in Table 2 clearly show the impor-
tance of using both translation and inverse trans-
lation models. Furthermore, we were able to get
the best performance by incorporating the short
phrase penalty (the denominator in Eq. (2)). From
here on, thus, we only use the original formula-
tion of the confidence score which uses the both
models and the penalty.
5.3 Quantitative and Qualitative Analysis
Model Dev Test
A
l
l
RNNenc 13.15 13.92
p(f | e) 12.49 13.57
p(f | e) + p(e | f) 18.82 20.10
p(f | e) + p(e | f) (p) 19.39 20.86
Moses 30.64 33.30
N
o
U
N
K
RNNenc 21.01 23.45
p(f | e) 20.94 22.62
p(f | e) + p(e | f) 23.05 24.63
p(f | e) + p(e | f) (p) 23.93 26.46
Moses 32.77 35.63
Table 2: BLEU scores computed on the develop-
ment and test sets. See the text for the description
of each approach. Moses refers to the scores by
the conventional phrase-based translation system.
The top five rows consider all sentences of each
data set, whilst the bottom five rows includes only
sentences with no unknown words
As expected, translation with the proposed ap-
proach helps significantly with translating long
sentences (see Fig. 2). We observe that trans-
lation performance does not drop for sentences
of lengths greater than those used to train the
RNNenc (? 30 words).
Similarly, in Fig. 3 we observe that translation
quality of the proposed approach is more robust
82
Source Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel , and
the turn of the century , the weight of the average American 40- to 49-year-old male increased
by 10 per cent , according to U.S. Health Department Data .
Segmentation [[ Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel ,]
[ and the turn of the century , the weight of the average American 40- to 49-year-old male] [
increased by 10 per cent , according to U.S. Health Department Data .]]
Reference Entre le d?ebut des ann?ees 1970 , lorsque le jumbo 747 de Boeing a d?efini le voyage long-courrier
moderne , et le tournant du si`ecle , le poids de l? Am?ericain moyen de 40 `a 49 ans a augment?e
de 10 % , selon les donn?ees du d?epartement am?ericain de la Sant?e .
With
segmentation
Entre les ann?ees 70 , lorsque le Boeing Boeing a d?efini le transport de voyageurs modernes ; et
la fin du si`ecle , le poids de la moyenne am?ericaine moyenne `a l? ?egard des hommes a augment?e
de 10 % , conform?ement aux donn?ees fournies par le U.S. Department of Health Affairs .
Without
segmentation
Entre les ann?ees 1970 , lorsque les avions de service Boeing ont d?epass?e le prix du travail , le
taux moyen ?etait de 40 % .
Source During his arrest Ditta picked up his wallet and tried to remove several credit cards but they
were all seized and a hair sample was taken fom him.
Segmentation [[During his arrest Ditta] [picked up his wallet and tried to remove several credit cards but they
were all seized and] [a hair sample was taken from him.]]
Reference Au cours de son arrestation , Ditta a ramass?e son portefeuille et a tent?e de retirer plusieurs cartes
de cr?edit , mais elles ont toutes ?et?e saisies et on lui a pr?elev?e un ?echantillon de cheveux .
With
segmentation
Pendant son arrestation J? ai utilis?e son portefeuille et a essay?e de retirer plusieurs cartes de
cr?edit mais toutes les pi`eces ont ?et?e saisies et un ?echantillon de cheveux a ?et?e enlev?e.
Without
segmentation
Lors de son arrestation il a tent?e de r?ecup?erer plusieurs cartes de cr?edit mais il a ?et?e saisi de tous
les coups et des blessures.
Source ?We can now move forwards and focus on the future and on the 90 % of assets that make up a
really good bank, and on building a great bank for our clients and the United Kingdom,? new
director general, Ross McEwan, said to the press .
Segmentation [[?We can now move forwards and focus on the future] [and] [on the 90 % of assets that make
up a really good bank, and on building] [a great bank for our clients and the United Kingdom,?]
[new director general, Ross McEwan, said to the press.]]
Reference ?Nous pouvons maintenant aller de l?avant , nous pr?eoccuper de l?avenir et des 90 % des actifs
qui constituent une banque vraiment bonne et construire une grande banque pour la client`ele et
pour le Royaume Uni?, a dit le nouveau directeur g?en?eral Ross McEwan `a la presse .
With
segmentation
?Nous pouvons maintenant passer `a l?avenir et se concentrer sur l avenir ou sur les 90 % d actifs
qui constituent une bonne banque et sur la construction une grande banque de nos clients et du
Royaume-Uni? Le nouveau directeur g?en?eral Ross Ross a dit que la presse.
Without
segmentation
?Nous pouvons maintenant passer et ?etudier les 90 % et mettre en place une banque importante
pour la nouvelle banque et le directeur g?en?eral? a soulign?e le journaliste .
Source There are several beautiful flashes - the creation of images has always been one of Chouinard?s
strong points - like the hair that is ruffled or the black fabric that extends the lines.
Segmentation [[There are several beautiful flashes - the creation of images has always been one of Chouinard?s
strong points -] [like the hair that is ruffled or the black fabric that extends the lines.]]
Reference Il y a quelques beaux flashs - la cr?eation d?images a toujours ?et?e une force chez Chouinard -
comme ces ch eveux qui s??ebouriffent ou ces tissus noirs qui allongent les lignes .
With
segmentation
Il existe plusieurs belles images - la cr?eation d images a toujours ?et?e l un de ses points forts .
comme les cheveux comme le vernis ou le tissu noir qui ?etend les lignes.
Without
segmentation
Il existe plusieurs points forts : la cr?eation d images est toujours l un des points forts .
Source Without specifying the illness she was suffering from, the star performer of ?Respect? confirmed
to the media on 16 October that the side effects of a treatment she was receiving were ?difficult?
to deal with.
Segmentation [[Without specifying the illness she was suffering from, the star performer of ?Respect?] [con-
firmed to the media on 16 October that the side effects of a treatment she was receiving were]
[?difficult? to deal with.]]
Reference Sans pr?eciser la maladie dont elle souffrait , la c?el`ebre interpr`ete de Respect avait affirm?e aux
m?edias le 16 octobre que les effets secondaires d?un traitement qu?elle recevait ?etaient ?diffi-
ciles?.
With
segmentation
Sans pr?eciser la maladie qu?elle souffrait la star de l? ??uvre? de ?respect?. Il a ?et?e confirm?e
aux m?edias le 16 octobre que les effets secondaires d?un traitement ont ?et?e rec?us. ?difficile? de
traiter .
Without
segmentation
Sans la pr?ecision de la maladie elle a eu l?impression de ?marquer le 16 avril? les effets d?un tel
?traitement?.
Table 3: Sample translations with the RNNenc model taken from the test set along with the source
sentences and the reference translations.
83
Source He nevertheless praised the Government for responding to his request for urgent assis-
tance which he first raised with the Prime Minister at the beginning of May .
Segmentation [He nevertheless praised the Government for responding to his request for urgent assis-
tance which he first raised ] [with the Prime Minister at the beginning of May . ]
Reference Il a n?eanmoins f?elicit?e le gouvernement pour avoir r?epondu `a la demande d? aide urgente
qu?il a pr?esent?ee au Premier ministre d?ebut mai .
With
segmentation
Il a n?eanmoins f?elicit?e le Gouvernement de r?epondre `a sa demande d? aide urgente qu?il
a soulev
?
ee . avec le Premier ministre d?ebut mai .
Without
segmentation
Il a n?eanmoins f?elicit?e le gouvernement de r?epondre `a sa demande d? aide urgente qu?il
a adress
?
ee au Premier Ministre d?ebut mai .
Table 4: An example where an incorrect segmentation negatively impacts fluency and punctuation.
to the presence of unknown words. We suspect
that the existence of many unknown words make
it harder for the RNNenc to extract the meaning of
the sentence clearly, while this is avoided with the
proposed segmentation approach as it effectively
allows the RNNenc to deal with a less number of
unknown words.
In Table 3, we show the translations of ran-
domly selected long sentences (40 or more words).
Segmentation improves overall translation quality,
agreeing well with our quantitative result. How-
ever, we can also observe a decrease in transla-
tion quality when an input sentence is not seg-
mented into well-formed sentential clauses. Addi-
tionally, the concatenation of independently trans-
lated segments sometimes negatively impacts flu-
ency, punctuation, and capitalization by the RN-
Nenc model. Table 4 shows one such example.
6 Discussion and Conclusion
In this paper we propose an automatic segmen-
tation solution to the ?curse of sentence length?
in neural machine translation. By choosing an
appropriate confidence score based on bidirec-
tional translation models, we observed significant
improvement in translation quality for long sen-
tences.
Our investigation shows that the proposed
segmentation-based translation is more robust to
the presence of unknown words. However, since
each segment is translated in isolation, a segmen-
tation of an input sentence may negatively impact
translation quality, especially the fluency of the
translated sentence, the placement of punctuation
marks and the capitalization of words.
An important research direction in the future is
to investigate how to improve the quality of the
translation obtained by concatenating translated
segments.
Acknowledgments
The authors would like to acknowledge the sup-
port of the following agencies for research funding
and computing support: NSERC, Calcul Qu?ebec,
Compute Canada, the Canada Research Chairs
and CIFAR.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 355?362. Association for Compu-
tational Linguistics.
Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. 2013. Audio chord recognition with
recurrent neural networks. In ISMIR.
Kyunghyun Cho, Bart van Merri?enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014a. On the
properties of neural machine translation: Encoder?
Decoder approaches. In Eighth Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion, October.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014b. Learning phrase representa-
tions using rnn encoder-decoder for statistical ma-
chine translation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), October. to appear.
A. Graves, A. Mohamed, and G. Hinton. 2013. Speech
recognition with deep recurrent neural networks.
ICASSP.
A. Graves. 2013. Generating sequences with recurrent
neural networks. arXiv:1308.0850 [cs.NE],
August.
Nal Kalchbrenner and Phil Blunsom. 2013. Two re-
current continuous translation models. In Proceed-
ings of the ACL Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1700?1709. Association for Computational Linguis-
tics.
84
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Annual meet-
ing of the association for computational linguistics
(acl). Prague, Czech Republic. demonstration ses-
sion.
Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014.
Anonymized. In Anonymized. under review.
85
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103?111,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
On the Properties of Neural Machine Translation: Encoder?Decoder
Approaches
Kyunghyun Cho Bart van Merri
?
enboer
Universit?e de Montr?eal
Dzmitry Bahdanau
?
Jacobs University Bremen, Germany
Yoshua Bengio
Universit?e de Montr?eal, CIFAR Senior Fellow
Abstract
Neural machine translation is a relatively
new approach to statistical machine trans-
lation based purely on neural networks.
The neural machine translation models of-
ten consist of an encoder and a decoder.
The encoder extracts a fixed-length repre-
sentation from a variable-length input sen-
tence, and the decoder generates a correct
translation from this representation. In this
paper, we focus on analyzing the proper-
ties of the neural machine translation us-
ing two models; RNN Encoder?Decoder
and a newly proposed gated recursive con-
volutional neural network. We show that
the neural machine translation performs
relatively well on short sentences without
unknown words, but its performance de-
grades rapidly as the length of the sentence
and the number of unknown words in-
crease. Furthermore, we find that the pro-
posed gated recursive convolutional net-
work learns a grammatical structure of a
sentence automatically.
1 Introduction
A new approach for statistical machine transla-
tion based purely on neural networks has recently
been proposed (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014). This new approach, which
we refer to as neural machine translation, is in-
spired by the recent trend of deep representational
learning. All the neural network models used in
(Sutskever et al., 2014; Cho et al., 2014) consist of
an encoder and a decoder. The encoder extracts a
fixed-length vector representation from a variable-
length input sentence, and from this representation
the decoder generates a correct, variable-length
target translation.
?
Research done while visiting Universit?e de Montr?eal
The emergence of the neural machine transla-
tion is highly significant, both practically and the-
oretically. Neural machine translation models re-
quire only a fraction of the memory needed by
traditional statistical machine translation (SMT)
models. The models we trained for this paper
require only 500MB of memory in total. This
stands in stark contrast with existing SMT sys-
tems, which often require tens of gigabytes of
memory. This makes the neural machine trans-
lation appealing in practice. Furthermore, un-
like conventional translation systems, each and ev-
ery component of the neural translation model is
trained jointly to maximize the translation perfor-
mance.
As this approach is relatively new, there has not
been much work on analyzing the properties and
behavior of these models. For instance: What
are the properties of sentences on which this ap-
proach performs better? How does the choice of
source/target vocabulary affect the performance?
In which cases does the neural machine translation
fail?
It is crucial to understand the properties and be-
havior of this new neural machine translation ap-
proach in order to determine future research di-
rections. Also, understanding the weaknesses and
strengths of neural machine translation might lead
to better ways of integrating SMT and neural ma-
chine translation systems.
In this paper, we analyze two neural machine
translation models. One of them is the RNN
Encoder?Decoder that was proposed recently in
(Cho et al., 2014). The other model replaces the
encoder in the RNN Encoder?Decoder model with
a novel neural network, which we call a gated
recursive convolutional neural network (grConv).
We evaluate these two models on the task of trans-
lation from French to English.
Our analysis shows that the performance of
the neural machine translation model degrades
103
quickly as the length of a source sentence in-
creases. Furthermore, we find that the vocabulary
size has a high impact on the translation perfor-
mance. Nonetheless, qualitatively we find that the
both models are able to generate correct transla-
tions most of the time. Furthermore, the newly
proposed grConv model is able to learn, without
supervision, a kind of syntactic structure over the
source language.
2 Neural Networks for Variable-Length
Sequences
In this section, we describe two types of neural
networks that are able to process variable-length
sequences. These are the recurrent neural net-
work and the proposed gated recursive convolu-
tional neural network.
2.1 Recurrent Neural Network with Gated
Hidden Neurons
z
rh h~ x
(a) (b)
Figure 1: The graphical illustration of (a) the re-
current neural network and (b) the hidden unit that
adaptively forgets and remembers.
A recurrent neural network (RNN, Fig. 1 (a))
works on a variable-length sequence x =
(x
1
,x
2
, ? ? ? ,x
T
) by maintaining a hidden state h
over time. At each timestep t, the hidden state h
(t)
is updated by
h
(t)
= f
(
h
(t?1)
,x
t
)
,
where f is an activation function. Often f is as
simple as performing a linear transformation on
the input vectors, summing them, and applying an
element-wise logistic sigmoid function.
An RNN can be used effectively to learn a dis-
tribution over a variable-length sequence by learn-
ing the distribution over the next input p(x
t+1
|
x
t
, ? ? ? ,x
1
). For instance, in the case of a se-
quence of 1-of-K vectors, the distribution can be
learned by an RNN which has as an output
p(x
t,j
= 1 | x
t?1
, . . . ,x
1
) =
exp
(
w
j
h
?t?
)
?
K
j
?
=1
exp
(
w
j
?
h
?t?
)
,
for all possible symbols j = 1, . . . ,K, where w
j
are the rows of a weight matrix W. This results in
the joint distribution
p(x) =
T
?
t=1
p(x
t
| x
t?1
, . . . , x
1
).
Recently, in (Cho et al., 2014) a new activation
function for RNNs was proposed. The new activa-
tion function augments the usual logistic sigmoid
activation function with two gating units called re-
set, r, and update, z, gates. Each gate depends on
the previous hidden state h
(t?1)
, and the current
input x
t
controls the flow of information. This is
reminiscent of long short-term memory (LSTM)
units (Hochreiter and Schmidhuber, 1997). For
details about this unit, we refer the reader to (Cho
et al., 2014) and Fig. 1 (b). For the remainder of
this paper, we always use this new activation func-
tion.
2.2 Gated Recursive Convolutional Neural
Network
Besides RNNs, another natural approach to deal-
ing with variable-length sequences is to use a re-
cursive convolutional neural network where the
parameters at each level are shared through the
whole network (see Fig. 2 (a)). In this section, we
introduce a binary convolutional neural network
whose weights are recursively applied to the input
sequence until it outputs a single fixed-length vec-
tor. In addition to a usual convolutional architec-
ture, we propose to use the previously mentioned
gating mechanism, which allows the recursive net-
work to learn the structure of the source sentences
on the fly.
Let x = (x
1
,x
2
, ? ? ? ,x
T
) be an input sequence,
where x
t
? R
d
. The proposed gated recursive
convolutional neural network (grConv) consists of
four weight matrices W
l
, W
r
, G
l
and G
r
. At
each recursion level t ? [1, T ? 1], the activation
of the j-th hidden unit h
(t)
j
is computed by
h
(t)
j
= ?
c
?
h
(t)
j
+ ?
l
h
(t?1)
j?1
+ ?
r
h
(t?1)
j
, (1)
where ?
c
, ?
l
and ?
r
are the values of a gater that
sum to 1. The hidden unit is initialized as
h
(0)
j
= Ux
j
,
where U projects the input into a hidden space.
104
?h
~
(a) (b) (c) (d)
Figure 2: The graphical illustration of (a) the recursive convolutional neural network and (b) the proposed
gated unit for the recursive convolutional neural network. (c?d) The example structures that may be
learned with the proposed gated unit.
The new activation
?
h
(t)
j
is computed as usual:
?
h
(t)
j
= ?
(
W
l
h
(t)
j?1
+W
r
h
(t)
j
)
,
where ? is an element-wise nonlinearity.
The gating coefficients ??s are computed by
?
?
?
c
?
l
?
r
?
?
=
1
Z
exp
(
G
l
h
(t)
j?1
+G
r
h
(t)
j
)
,
where G
l
,G
r
? R
3?d
and
Z =
3
?
k=1
[
exp
(
G
l
h
(t)
j?1
+G
r
h
(t)
j
)]
k
.
According to this activation, one can think of
the activation of a single node at recursion level t
as a choice between either a new activation com-
puted from both left and right children, the acti-
vation from the left child, or the activation from
the right child. This choice allows the overall
structure of the recursive convolution to change
adaptively with respect to an input sample. See
Fig. 2 (b) for an illustration.
In this respect, we may even consider the pro-
posed grConv as doing a kind of unsupervised
parsing. If we consider the case where the gat-
ing unit makes a hard decision, i.e., ? follows an
1-of-K coding, it is easy to see that the network
adapts to the input and forms a tree-like structure
(See Fig. 2 (c?d)). However, we leave the further
investigation of the structure learned by this model
for future research.
3 Purely Neural Machine Translation
3.1 Encoder?Decoder Approach
The task of translation can be understood from the
perspective of machine learning as learning the
Economic growth has slowed down in recent years .
La croissance ?conomique a ralenti ces derni?res ann?es .
[z  ,z  , ... ,z  ]1 2 d
Encode
Decode
Figure 3: The encoder?decoder architecture
conditional distribution p(f | e) of a target sen-
tence (translation) f given a source sentence e.
Once the conditional distribution is learned by a
model, one can use the model to directly sample
a target sentence given a source sentence, either
by actual sampling or by using a (approximate)
search algorithm to find the maximum of the dis-
tribution.
A number of recent papers have proposed to
use neural networks to directly learn the condi-
tional distribution from a bilingual, parallel cor-
pus (Kalchbrenner and Blunsom, 2013; Cho et al.,
2014; Sutskever et al., 2014). For instance, the au-
thors of (Kalchbrenner and Blunsom, 2013) pro-
posed an approach involving a convolutional n-
gram model to extract a vector of a source sen-
tence which is decoded with an inverse convolu-
tional n-gram model augmented with an RNN. In
(Sutskever et al., 2014), an RNN with LSTM units
was used to encode a source sentence and starting
from the last hidden state, to decode a target sen-
tence. Similarly, the authors of (Cho et al., 2014)
proposed to use an RNN to encode and decode a
pair of source and target phrases.
At the core of all these recent works lies an
encoder?decoder architecture (see Fig. 3). The
encoder processes a variable-length input (source
sentence) and builds a fixed-length vector repre-
sentation (denoted as z in Fig. 3). Conditioned on
the encoded representation, the decoder generates
105
a variable-length sequence (target sentence).
Before (Sutskever et al., 2014) this encoder?
decoder approach was used mainly as a part of the
existing statistical machine translation (SMT) sys-
tem. This approach was used to re-rank the n-best
list generated by the SMT system in (Kalchbren-
ner and Blunsom, 2013), and the authors of (Cho
et al., 2014) used this approach to provide an ad-
ditional score for the existing phrase table.
In this paper, we concentrate on analyzing the
direct translation performance, as in (Sutskever et
al., 2014), with two model configurations. In both
models, we use an RNN with the gated hidden
unit (Cho et al., 2014), as this is one of the only
options that does not require a non-trivial way to
determine the target length. The first model will
use the same RNN with the gated hidden unit as
an encoder, as in (Cho et al., 2014), and the second
one will use the proposed gated recursive convo-
lutional neural network (grConv). We aim to un-
derstand the inductive bias of the encoder?decoder
approach on the translation performance measured
by BLEU.
4 Experiment Settings
4.1 Dataset
We evaluate the encoder?decoder models on the
task of English-to-French translation. We use the
bilingual, parallel corpus which is a set of 348M
selected by the method in (Axelrod et al., 2011)
from a combination of Europarl (61M words),
news commentary (5.5M), UN (421M) and two
crawled corpora of 90M and 780M words respec-
tively.
1
We did not use separate monolingual data.
The performance of the neural machien transla-
tion models was measured on the news-test2012,
news-test2013 and news-test2014 sets ( 3000 lines
each). When comparing to the SMT system, we
use news-test2012 and news-test2013 as our de-
velopment set for tuning the SMT system, and
news-test2014 as our test set.
Among all the sentence pairs in the prepared
parallel corpus, for reasons of computational ef-
ficiency we only use the pairs where both English
and French sentences are at most 30 words long to
train neural networks. Furthermore, we use only
the 30,000 most frequent words for both English
and French. All the other rare words are consid-
1
All the data can be downloaded from http:
//www-lium.univ-lemans.fr/
?
schwenk/cslm_
joint_paper/.
ered unknown and are mapped to a special token
([UNK]).
4.2 Models
We train two models: The RNN Encoder?
Decoder (RNNenc)(Cho et al., 2014) and the
newly proposed gated recursive convolutional
neural network (grConv). Note that both models
use an RNN with gated hidden units as a decoder
(see Sec. 2.1).
We use minibatch stochastic gradient descent
with AdaDelta (Zeiler, 2012) to train our two mod-
els. We initialize the square weight matrix (transi-
tion matrix) as an orthogonal matrix with its spec-
tral radius set to 1 in the case of the RNNenc and
0.4 in the case of the grConv. tanh and a rectifier
(max(0, x)) are used as the element-wise nonlin-
ear functions for the RNNenc and grConv respec-
tively.
The grConv has 2000 hidden neurons, whereas
the RNNenc has 1000 hidden neurons. The word
embeddings are 620-dimensional in both cases.
2
Both models were trained for approximately 110
hours, which is equivalent to 296,144 updates and
846,322 updates for the grConv and RNNenc, re-
spectively.
4.2.1 Translation using Beam-Search
We use a basic form of beam-search to find a trans-
lation that maximizes the conditional probability
given by a specific model (in this case, either the
RNNenc or the grConv). At each time step of
the decoder, we keep the s translation candidates
with the highest log-probability, where s = 10
is the beam-width. During the beam-search, we
exclude any hypothesis that includes an unknown
word. For each end-of-sequence symbol that is se-
lected among the highest scoring candidates the
beam-width is reduced by one, until the beam-
width reaches zero.
The beam-search to (approximately) find a se-
quence of maximum log-probability under RNN
was proposed and used successfully in (Graves,
2012) and (Boulanger-Lewandowski et al., 2013).
Recently, the authors of (Sutskever et al., 2014)
found this approach to be effective in purely neu-
ral machine translation based on LSTM units.
2
In all cases, we train the whole network including the
word embedding matrix. The embedding dimensionality was
chosen to be quite large, as the preliminary experiments
with 155-dimensional embeddings showed rather poor per-
formance.
106
Model Development Test
A
l
l
RNNenc 13.15 13.92
grConv 9.97 9.97
Moses 30.64 33.30
Moses+RNNenc
?
31.48 34.64
Moses+LSTM
?
32 35.65
N
o
U
N
K
RNNenc 21.01 23.45
grConv 17.19 18.22
Moses 32.77 35.63
Model Development Test
A
l
l
RNNenc 19.12 20.99
grConv 16.60 17.50
Moses 28.92 32.00
N
o
U
N
K
RNNenc 24.73 27.03
grConv 21.74 22.94
Moses 32.20 35.40
(a) All Lengths
(b) 10?20 Words
Table 1: BLEU scores computed on the development and test sets. The top three rows show the scores on
all the sentences, and the bottom three rows on the sentences having no unknown words. (?) The result
reported in (Cho et al., 2014) where the RNNenc was used to score phrase pairs in the phrase table. (?)
The result reported in (Sutskever et al., 2014) where an encoder?decoder with LSTM units was used to
re-rank the n-best list generated by Moses.
When we use the beam-search to find the k best
translations, we do not use a usual log-probability
but one normalized with respect to the length of
the translation. This prevents the RNN decoder
from favoring shorter translations, behavior which
was observed earlier in, e.g., (Graves, 2013).
5 Results and Analysis
5.1 Quantitative Analysis
In this paper, we are interested in the properties
of the neural machine translation models. Specif-
ically, the translation quality with respect to the
length of source and/or target sentences and with
respect to the number of words unknown to the
model in each source/target sentence.
First, we look at how the BLEU score, reflect-
ing the translation performance, changes with re-
spect to the length of the sentences (see Fig. 4 (a)?
(b)). Clearly, both models perform relatively well
on short sentences, but suffer significantly as the
length of the sentences increases.
We observe a similar trend with the number of
unknown words, in Fig. 4 (c). As expected, the
performance degrades rapidly as the number of
unknown words increases. This suggests that it
will be an important challenge to increase the size
of vocabularies used by the neural machine trans-
lation system in the future. Although we only
present the result with the RNNenc, we observed
similar behavior for the grConv as well.
In Table 1 (a), we present the translation perfor-
mances obtained using the two models along with
the baseline phrase-based SMT system.
3
Clearly
the phrase-based SMT system still shows the su-
perior performance over the proposed purely neu-
ral machine translation system, but we can see that
under certain conditions (no unknown words in
both source and reference sentences), the differ-
ence diminishes quite significantly. Furthermore,
if we consider only short sentences (10?20 words
per sentence), the difference further decreases (see
Table 1 (b).
Furthermore, it is possible to use the neural ma-
chine translation models together with the existing
phrase-based system, which was found recently in
(Cho et al., 2014; Sutskever et al., 2014) to im-
prove the overall translation performance (see Ta-
ble 1 (a)).
This analysis suggests that that the current neu-
ral translation approach has its weakness in han-
dling long sentences. The most obvious explana-
tory hypothesis is that the fixed-length vector rep-
resentation does not have enough capacity to en-
code a long sentence with complicated structure
and meaning. In order to encode a variable-length
sequence, a neural network may ?sacrifice? some
of the important topics in the input sentence in or-
der to remember others.
This is in stark contrast to the conventional
phrase-based machine translation system (Koehn
et al., 2003). As we can see from Fig. 5, the
conventional system trained on the same dataset
(with additional monolingual data for the language
model) tends to get a higher BLEU score on longer
3
We used Moses as a baseline, trained with additional
monolingual data for a 4-gram language model.
107
Source She explained her new position of foreign affairs and security policy representative as a reply to a
question: ?Who is the European Union? Which phone number should I call??; i.e. as an important step
to unification and better clarity of Union?s policy towards countries such as China or India.
Reference Elle a expliqu?e le nouveau poste de la Haute repr?esentante pour les affaires ?etrang`eres et la politique de
d?efense dans le cadre d?une r?eponse `a la question: ?Qui est qui `a l?Union europ?eenne?? ?A quel num?ero
de t?el?ephone dois-je appeler??, donc comme un pas important vers l?unicit?e et une plus grande lisibilit?e
de la politique de l?Union face aux ?etats, comme est la Chine ou bien l?Inde.
RNNEnc Elle a d?ecrit sa position en mati`ere de politique ?etrang`ere et de s?ecurit?e ainsi que la politique de l?Union
europ?eenne en mati`ere de gouvernance et de d?emocratie .
grConv Elle a expliqu?e sa nouvelle politique ?etrang`ere et de s?ecurit?e en r?eponse `a un certain nombre de questions
: ?Qu?est-ce que l?Union europ?eenne ? ? .
Moses Elle a expliqu?e son nouveau poste des affaires ?etrang`eres et la politique de s?ecurit?e repr?esentant en
r?eponse `a une question: ?Qui est l?Union europ?eenne? Quel num?ero de t?el?ephone dois-je appeler??;
c?est comme une ?etape importante de l?unification et une meilleure lisibilit?e de la politique de l?Union
`a des pays comme la Chine ou l?Inde .
Source The investigation should be complete by the end of the year when the findings will be presented to
Deutsche Bank?s board of managing directors - with recommendations for action.
Reference L?examen doit ?etre termin?e d?ici la fin de l?ann?ee, ensuite les r?esultats du conseil d?administration de la
Deutsche Bank doivent ?etre pr?esent?es - avec recommandation, d? habitude.
RNNEnc L??etude devrait ?etre termin?ee `a la fin de l? ann?ee, lorsque les conclusions seront pr?esent?ees au conseil
d?administration de la Deutsche Bank, conseil d?association avec des mesures.
grConv L?enqu?ete devrait ?etre termin?ee `a la fin de l?ann?ee o`u les conclusions seront pr?esent?ees par le conseil
d?administration de la BCE `a la direction des recommandations.
Moses L?enqu?ete devrait ?etre termin?e d?ici la fin de l?ann?ee lorsque les r?esultats seront pr?esent?es `a la Deutsche
Bank conseil des directeurs g?en?eraux - avec des recommandations .
Source And there are thorny mechanical questions that must be resolved during that time, like how to bal-
ance the state?s mandate of ?adequate access? to licensed marijuana with its prohibitions on cannabis
businesses within 1,000 feet of a school, park, playground or child care center.
Reference Pendant ce temps, des questions pratiques restent en suspens: comment ?equilibrer le mandat de l??etat
qui garantit un acc`es appropri?e `a la marijuana agr?e?ee et interdit l?installation de commerces de vente de
cannabis dans un rayon de 30 km autour d?une ?ecole, d?un parc, d?un terrain de jeu ou d?une cr`eche.
RNNEnc Il y a des questions pr?eventives qui se posent quant `a l??equilibre des droits de l?enfant dans les limites
d?une entreprise de collecte de sang.
grConv De fac?on g?en?erale, il y a des raisons de s?ecurit?e pour que les entreprises aient acc`es `a des milliers de
centres de p?eche, d?eau ou de recherche.
Moses Et il y a des probl`emes m?ecaniques complexes qui doivent ?etre r?esolues au cours de cette p?eriode,
comme la mani`ere d??equilibrer le mandat de ?l?acc`es ad?equat? permis de marijuana avec l?interdiction
du cannabis aux entreprises de 1000 pieds d?une ?ecole de jeu ou de parc, le service de garde.
(a) Long Sentences
Source There is still no agreement as to which election rules to follow.
Reference Toutefois il n?existe toujours pas d?accord selon quel r`eglement de vote il faut proc?eder.
RNNEnc Il n?y a pas encore d?accord sur les r`egles ?electorales.
grConv Il n?y a pas encore d?accord sur la question des ?elections `a suivre.
Moses Il y a toujours pas d?accord sur l??election des r`egles `a suivre.
Source Many of these ideas may have been creative, but they didn?t necessarily work.
Reference Beaucoup de ces id?ees ?etaient cr?eatives mais elles n?ont pas forc?ement fonctionn?e.
RNNEnc Bon nombre de ces id?ees ont peut-?etre ?et?e cr?eatrices, mais elles ne s?appliquaient pas n?ecessairement.
grConv Beaucoup de ces id?ees peuvent ?etre cr?eatives, mais elles n?ont pas fonctionn?e.
Moses Beaucoup de ces id?ees ont pu ?etre cr?eatif, mais ils n?ont pas n?ecessairement.
Source There is a lot of consensus between the Left and the Right on this subject.
Reference C?est qu?il y a sur ce sujet un assez large consensus entre gauche et droite.
RNNEnc Il existe beaucoup de consensus entre la gauche et le droit `a la question.
grConv Il y a un consensus entre la gauche et le droit sur cette question.
Moses Il y a beaucoup de consensus entre la gauche et la droite sur ce sujet.
Source According to them, one can find any weapon at a low price right now.
Reference Selon eux, on peut trouver aujourd?hui `a Moscou n?importe quelle arme pour un prix raisonnable.
RNNEnc Selon eux, on peut se trouver de l?arme `a un prix trop bas.
grConv En tout cas, ils peuvent trouver une arme `a un prix tr`es bas `a la fois.
Moses Selon eux, on trouve une arme `a bas prix pour l?instant.
(b) Short Sentences
Table 2: The sample translations along with the source sentences and the reference translations.
108
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(a) RNNenc
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(b) grConv
0 2 4 6 8 10
Max. number of unknown words
10
12
14
16
18
20
22
24
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(c) RNNenc
Figure 4: The BLEU scores achieved by (a) the RNNenc and (b) the grConv for sentences of a given
length. The plot is smoothed by taking a window of size 10. (c) The BLEU scores achieved by the RNN
model for sentences with less than a given number of unknown words.
sentences.
In fact, if we limit the lengths of both the source
sentence and the reference translation to be be-
tween 10 and 20 words and use only the sentences
with no unknown words, the BLEU scores on the
test set are 27.81 and 33.08 for the RNNenc and
Moses, respectively.
Note that we observed a similar trend even
when we used sentences of up to 50 words to train
these models.
5.2 Qualitative Analysis
Although BLEU score is used as a de-facto stan-
dard metric for evaluating the performance of a
machine translation system, it is not the perfect
metric (see, e.g., (Song et al., 2013; Liu et al.,
2011)). Hence, here we present some of the ac-
tual translations generated from the two models,
RNNenc and grConv.
In Table. 2 (a)?(b), we show the translations of
some randomly selected sentences from the de-
velopment and test sets. We chose the ones that
have no unknown words. (a) lists long sentences
(longer than 30 words), and (b) short sentences
(shorter than 10 words). We can see that, despite
the difference in the BLEU scores, all three mod-
els (RNNenc, grConv and Moses) do a decent job
at translating, especially, short sentences. When
the source sentences are long, however, we no-
tice the performance degradation of the neural ma-
chine translation models.
Additionally, we present here what type of
structure the proposed gated recursive convolu-
tional network learns to represent. With a sample
sentence ?Obama is the President of the United
States?, we present the parsing structure learned
by the grConv encoder and the generated transla-
tions, in Fig. 6. The figure suggests that the gr-
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
25
30
35
40
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
Figure 5: The BLEU scores achieved by an SMT
system for sentences of a given length. The plot
is smoothed by taking a window of size 10. We
use the solid, dotted and dashed lines to show the
effect of different lengths of source, reference or
both of them, respectively.
Conv extracts the vector representation of the sen-
tence by first merging ?of the United States? to-
gether with ?is the President of? and finally com-
bining this with ?Obama is? and ?.?, which is
well correlated with our intuition. Note, however,
that the structure learned by the grConv is differ-
ent from existing parsing approaches in the sense
that it returns soft parsing.
Despite the lower performance the grConv
showed compared to the RNN Encoder?Decoder,
4
we find this property of the grConv learning a
grammar structure automatically interesting and
believe further investigation is needed.
4
However, it should be noted that the number of gradient
updates used to train the grConv was a third of that used to
train the RNNenc. Longer training may change the result,
but for a fair comparison we chose to compare models which
were trained for an equal amount of time. Neither model was
trained to convergence.
109
Obama is the President of the United States .
++++++++
+++++++
++++++
+++++
++++
+++
++
+ Translations
Obama est le Pr?esident des
?
Etats-Unis . (2.06)
Obama est le pr?esident des
?
Etats-Unis . (2.09)
Obama est le pr?esident des Etats-Unis . (2.61)
Obama est le Pr?esident des Etats-Unis . (3.33)
Barack Obama est le pr?esident des
?
Etats-Unis . (4.41)
Barack Obama est le Pr?esident des
?
Etats-Unis . (4.48)
Barack Obama est le pr?esident des Etats-Unis . (4.54)
L?Obama est le Pr?esident des
?
Etats-Unis . (4.59)
L?Obama est le pr?esident des
?
Etats-Unis . (4.67)
Obama est pr?esident du Congr`es des
?
Etats-Unis .(5.09)
(a) (b)
Figure 6: (a) The visualization of the grConv structure when the input is ?Obama is the President of
the United States.?. Only edges with gating coefficient ? higher than 0.1 are shown. (b) The top-10
translations generated by the grConv. The numbers in parentheses are the negative log-probability.
6 Conclusion and Discussion
In this paper, we have investigated the property
of a recently introduced family of machine trans-
lation system based purely on neural networks.
We focused on evaluating an encoder?decoder ap-
proach, proposed recently in (Kalchbrenner and
Blunsom, 2013; Cho et al., 2014; Sutskever et al.,
2014), on the task of sentence-to-sentence trans-
lation. Among many possible encoder?decoder
models we specifically chose two models that dif-
fer in the choice of the encoder; (1) RNN with
gated hidden units and (2) the newly proposed
gated recursive convolutional neural network.
After training those two models on pairs of
English and French sentences, we analyzed their
performance using BLEU scores with respect to
the lengths of sentences and the existence of un-
known/rare words in sentences. Our analysis re-
vealed that the performance of the neural machine
translation suffers significantly from the length of
sentences. However, qualitatively, we found that
the both models are able to generate correct trans-
lations very well.
These analyses suggest a number of future re-
search directions in machine translation purely
based on neural networks.
Firstly, it is important to find a way to scale up
training a neural network both in terms of com-
putation and memory so that much larger vocabu-
laries for both source and target languages can be
used. Especially, when it comes to languages with
rich morphology, we may be required to come up
with a radically different approach in dealing with
words.
Secondly, more research is needed to prevent
the neural machine translation system from under-
performing with long sentences. Lastly, we need
to explore different neural architectures, especially
for the decoder. Despite the radical difference in
the architecture between RNN and grConv which
were used as an encoder, both models suffer from
the curse of sentence length. This suggests that it
may be due to the lack of representational power
in the decoder. Further investigation and research
are required.
In addition to the property of a general neural
machine translation system, we observed one in-
teresting property of the proposed gated recursive
convolutional neural network (grConv). The gr-
Conv was found to mimic the grammatical struc-
ture of an input sentence without any supervision
on syntactic structure of language. We believe this
property makes it appropriate for natural language
processing applications other than machine trans-
lation.
Acknowledgments
The authors would like to acknowledge the sup-
port of the following agencies for research funding
and computing support: NSERC, Calcul Qu?ebec,
Compute Canada, the Canada Research Chairs
and CIFAR.
110
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 355?362. Association for Compu-
tational Linguistics.
Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. 2013. Audio chord recognition with
recurrent neural networks. In ISMIR.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), October. to appear.
Alex Graves. 2012. Sequence transduction with re-
current neural networks. In Proceedings of the
29th International Conference on Machine Learning
(ICML 2012).
A. Graves. 2013. Generating sequences with recurrent
neural networks. arXiv:1308.0850 [cs.NE],
August.
S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735?
1780.
Nal Kalchbrenner and Phil Blunsom. 2013. Two re-
current continuous translation models. In Proceed-
ings of the ACL Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1700?1709. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011. Better evaluation metrics lead to better ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 375?384. Association for Computa-
tional Linguistics.
Xingyi Song, Trevor Cohn, and Lucia Specia. 2013.
BLEU deconstructed: Designing a better MT eval-
uation metric. In Proceedings of the 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLING), March.
Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014.
Anonymized. In Anonymized.
Matthew D. Zeiler. 2012. ADADELTA: an adap-
tive learning rate method. Technical report, arXiv
1212.5701.
111
