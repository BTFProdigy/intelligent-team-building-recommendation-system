Proceedings of NAACL HLT 2009: Short Papers, pages 33?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Text Categorization from Category Name via Lexical Reference
Libby Barak
Department of Computer Science
University of Toronto
Toronto, Canada M5S 1A4
libbyb@cs.toronto.edu
Ido Dagan and Eyal Shnarch
Department of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
{dagan, shey}@cs.biu.ac.il
Abstract
Requiring only category names as user input
is a highly attractive, yet hardly explored, set-
ting for text categorization. Earlier bootstrap-
ping results relied on similarity in LSA space,
which captures rather coarse contextual sim-
ilarity. We suggest improving this scheme by
identifying concrete references to the category
name?s meaning, obtaining a special variant of
lexical expansion.
1 Introduction
Topical Text Categorization (TC), the task of clas-
sifying documents by pre-defined topics, is most
commonly addressed as a supervised learning task.
However, the supervised setting requires a substan-
tial amount of manually labeled documents, which
is often impractical in real-life settings.
Keyword-based TC methods (see Section 2) aim
at a more practical setting. Each category is rep-
resented by a list of characteristic keywords, which
should capture the category meaning. Classifica-
tion is then based on measuring similarity between
the category keywords and the classified documents,
typically followed by a bootstrapping step. The
manual effort is thus reduced to providing a key-
word list per category, which was partly automated
in some works through clustering.
The keyword-based approach still requires non-
negligible manual work in creating a representative
keyword list per category. (Gliozzo et al, 2005)
succeeded eliminating this requirement by using the
category name alone as the initial keyword, yet ob-
taining superior performance within the keyword-
based approach. This was achieved by measur-
ing similarity between category names and docu-
ments in Latent Semantic space (LSA), which im-
plicitly captures contextual similarities for the cate-
gory name through unsupervised dimensionality re-
duction. Requiring only category names as user in-
put seems very attractive, particularly when labeled
training data is too costly while modest performance
(relative to supervised methods) is still useful.
The goal of our research is to further improve the
scheme of text categorization from category name,
which was hardly explored in prior work. When an-
alyzing the behavior of the LSA representation of
(Gliozzo et al, 2005) we noticed that it captures
two types of similarities between the category name
and document terms. One type regards words which
refer specifically to the category name?s meaning,
such as pitcher for the category Baseball. How-
ever, typical context words for the category which do
not necessarily imply its specific meaning, like sta-
dium, also come up as similar to baseball in LSA
space. This limits the method?s precision, due to
false-positive classifications of contextually-related
documents that do not discuss the specific category
topic (such as other sports documents wrongly clas-
sified to Baseball). This behavior is quite typical
for query expansion methods, which expand a query
with contextually correlated terms.
We propose a novel scheme that models sepa-
rately these two types of similarity. For one, it
identifies words that are likely to refer specifically
to the category name?s meaning (Glickman et al,
2006), based on certain relations in WordNet and
33
Wikipedia. In tandem, we assess the general contex-
tual fit of the category topic using an LSA model,
to overcome lexical ambiguity and passing refer-
ences. The evaluations show that tracing lexical
references indeed increases classification precision,
which in turn improves the eventual classifier ob-
tained through bootstrapping.
2 Background: Keyword-based Text
Categorization
The majority of keyword-based TC methods fit the
general bootstrapping scheme outlined in Figure 1,
which is cast in terms of a vector-space model. The
simplest version for step 1 is manual generation of
the keyword lists (McCallum and Nigam, 1999).
(Ko and Seo, 2004; Liu et al, 2004) partly auto-
mated this step, using clustering to generate candi-
date keywords. These methods employed a standard
term-space representation in step 2.
As described in Section 1, the keyword list in
(Gliozzo et al, 2005) consisted of the category name
alone. This was accompanied by representing the
category names and documents (step 2) in LSA
space, obtained through cooccurrence-based dimen-
sionality reduction. In this space, words that tend
to cooccur together, or occur in similar contexts, are
represented by similar vectors. Thus, vector similar-
ity in LSA space (in step 3) captures implicitly the
similarity between the category name and contextu-
ally related words within the classified documents.
Step 3 yields an initial similarity-based classifi-
cation that assigns a single (most similar) category
to each document, with Sim(c, d) typically being
the cosine between the corresponding vectors. This
classification is used, in the subsequent bootstrap-
ping step, to train a standard supervised classifier
(either single- or multi-class), yielding the eventual
classifier for the category set.
3 Integrating Reference and Context
Our goal is to augment the coarse contextual simi-
larity measurement in earlier work with the identifi-
cation of concrete references to the category name?s
meaning. We were mostly inspired by (Glickman et
al., 2006), which coined the term lexical reference
to denote concrete references in text to the specific
meaning of a given term. They further showed that
Input: set of categories and unlabeled documents
Output: a classifier
1. Acquire a keyword list per category
2. Represent each category c and document d
as vectors in a common space
3. For each document d
CatSim(d) = argmaxc(Sim(c, d))
4. Train a supervised classifier on step (3) output
Figure 1: Keyword-based categorization scheme
Category name WordNet Wikipedia
Cryptography decipher digital signature
Medicine cardiology biofeedback, homeopathy
Macintosh Apple Mac, Mac
Motorcycle bike, cycle Honda XR600
Table 1: Referring terms from WordNet and Wikipedia
an entailing text (in the textual entailment setting)
typically includes a concrete reference to each term
in the entailed statement. Analogously, we assume
that a relevant document for a category typically in-
cludes concrete terms that refer specifically to the
category name?s meaning.
We thus extend the scheme in Figure 1 by cre-
ating two vectors per category (in steps 1 and 2): a
reference vector ~cref in term space, consisting of re-
ferring terms for the category name; and a context
vector ~ccon, representing the category name in LSA
space, as in (Gliozzo et al, 2005). Step 3 then com-
putes a combined similarity score for categories and
documents based on the two vectors.
3.1 References to category names
Referring terms are collected from WordNet and
Wikipedia, by utilizing relations that are likely to
correspond to lexical reference. Table 1 illustrates
that WordNet provides mostly referring terms of
general terminology while Wikipedia provides more
specific terms. While these resources were used pre-
viously for text categorization, it was mostly for en-
hancing document representation in supervised set-
tings, e.g. (Rodr??guez et al, 2000).
WordNet. Referring terms were found in Word-
Net starting from relevant senses of the category
name and transitively following relation types that
correspond to lexical reference. To that end, we
34
specified for each category name those senses which
fit the category?s meaning, such as the outer space
sense for the category Space.1
A category name sense is first expanded by its
synonyms and derivations, all of which are then ex-
panded by their hyponyms. When a term has no
hyponyms it is expanded by its meronyms instead,
since we observed that in such cases they often spec-
ify unique components that imply the holonym?s
meaning, such as Egypt for Middle East. However,
when a term is not a leaf in the hyponymy hierarchy
then its meronyms often refer to generic sub-parts,
such as door for car. Finally, the hyponyms and
meronyms are expanded by their derivations. As
a common heuristic, we considered only the most
frequent senses (top 4) of referring terms, avoiding
low-ranked (rare) senses which are likely to intro-
duce noise.
Wikipedia. We utilized a subset of a lexical ref-
erence resource extracted from Wikipedia (anony-
mous reference). For each category name we ex-
tracted referring terms of two types, capturing hy-
ponyms and synonyms. Terms of the first type are
Wikipedia page titles for which the first definition
sentence includes a syntactic ?is-a? pattern whose
complement is the category name, such as Chevrolet
for the category Autos. Terms of the second type
are extracted from Wikipedia?s redirect links, which
capture synonyms such as x11 for Windows-X.
The reference vector ~cref for a category consists
of the category name and all its referring terms,
equally weighted. The corresponding similarity
function is Simref (c, d) = cos(~cref , ~dterm)), where
~dterm is the document vector in term space.
3.2 Incorporating context similarity
Our key motivation is to utilize Simref as the ba-
sis for classification in step 3 (Figure 1). However,
this may yield false positive classifications in two
cases: (a) inappropriate sense of an ambiguous re-
ferring term, e.g., the narcotic sense of drug should
not yield classification to Medicine; (b) a passing
reference, e.g., an analogy to cars in a software doc-
ument, should not yield classification to Autos.
1We assume that it is reasonable to specify relevant senses
as part of the typically manual process of defining the set of
categories and their names. Otherwise, when expanding names
through all their senses F1-score dropped by about 2%.
In both these cases the overall context in the docu-
ment is expected to be atypical for the triggered cat-
egory. We therefore measure the contextual similar-
ity between a category c and a document d utilizing
LSA space, replicating the method in (Gliozzo et
al., 2005): ~ccon and ~dLSA are taken as the LSA vec-
tors of the category name and the document, respec-
tively, yielding Simcon(c, d) = cos(~ccon, ~dLSA)).2
The overall similarity score of step 3 is de-
fined as Sim(c, d) = Simref (c, d) ? Simcon(c, d).
This formula fulfils the requirement of finding at
least one referring term in the document; otherwise
Simref (c, d) would be zero. Simcon(c, d) is com-
puted in the reduced LSA space and is thus prac-
tically non-zero, and would downgrade Sim(c, d)
when there is low contextual similarity between the
category name and the document. Documents for
which Sim(c, d) = 0 for all categories are omitted.
4 Results and Conclusions
We tested our method on the two corpora used in
(Gliozzo et al, 2005): 20-NewsGroups, classified
by a single-class scheme (single category per doc-
ument), and Reuters-10 3, of a multi-class scheme.
As in their work, non-standard category names were
adjusted, such as Foreign exchange for Money-fx.
4.1 Initial classification
Table 2 presents the results of the initial classifica-
tion (step 3). The first 4 lines refer to classification
based on Simref alone. As a baseline, including
only the category name in the reference vector (Cat-
Name) yields particularly low recall. Expansion by
WordNet is notably more powerful than by the auto-
matically extracted Wikipedia resource; still, the lat-
ter consistently provides a small marginal improve-
ment when using both resources (Reference), indi-
cating their complementary nature.
As we hypothesized, the Reference model
achieves much better precision than the Context
model from (Gliozzo et al, 2005) alone (Simcon).
For 20-NewsGroups the recall of Reference is lim-
ited, due to partial coverage of our current expansion
2The original method includes a Gaussian Mixture re-
scaling step for Simcon, which wasn?t found helpful when
combined with Simref (as specified next).
310 most frequent categories in Reuters-21578
35
Reuters-10 20 Newsgroups
Method R P F1 R P F1
CatName 0.22 0.67 0.33 0.19 0.55 0.28
WordNet 0.67 0.78 0.72 0.29 0.56 0.38
Wikipedia 0.24 0.68 0.35 0.22 0.57 0.31
Reference 0.69 0.80 0.74 0.31 0.57 0.40
Context 0.59 0.64 0.61 0.46 0.46 0.46
Combined 0.71 0.82 0.76 0.32 0.58 0.41
Table 2: Initial categorization results (step 3)
Method Feature Reuters-10 20 NGSet R P F1 F1
Reference TF-IDF 0.91 0.50 0.65 0.51LSA 0.89 0.67 0.76 0.56
Context TF-IDF 0.84 0.48 0.61 0.48LSA 0.73 0.56 0.63 0.44
Combined TF-IDF 0.92 0.50 0.65 0.52LSA 0.89 0.71 0.79 0.56
Table 3: Final bootstrapping results (step 4)
resources, yielding a lower F1. Yet, its higher pre-
cision pays off for the bootstrapping step (Section
4.2). Finally, when the two models are Combined a
small precision improvement is observed.
4.2 Final bootstrapping results
The output of step 3 was fed as standard training
for a binary SVM classifier for each category (step
4). We used the default setting for SVM-light, apart
from the j parameter which was set to the number of
categories in each data set, as suggested by (Morik
et al, 1999). For Reuters-10, classification was
determined independently by the classifier of each
category, allowing multiple classes per document.
For 20-NewsGroups, the category which yielded the
highest classification score was chosen (one-versus-
all), fitting the single-class setting. We experimented
with two document representations for the super-
vised step: either as vectors in tf-idf weighted term
space or as vectors in LSA space.
Table 3 shows the final classification results.4
First, we observe that for the noisy bootstrapping
training data LSA document representation is usu-
ally preferred. Most importantly, our Reference and
Combined models clearly improve over the earlier
4Notice that P=R=F1 when all documents are classified to
a single class, as in step 4 for 20-NewsGroups, while in step 3
some documents are not classified, yielding distinct P/R/F1.
Context. Combining reference and context yields
some improvement for Reuters-10, but not for 20-
NewsGroups. We noticed though that the actual ac-
curacy of our method on 20-NewsGroups is notably
higher than measured relative to the gold standard,
due to its single-class scheme: in many cases, a doc-
ument should truly belong to more than one cate-
gory while that chosen by our algorithm was counted
as false positive. Future research is proposed to in-
crease the method?s recall via broader coverage lexi-
cal reference resources, and to improve its precision
through better context models than LSA, which was
found rather noisy for quite a few categories.
To conclude, the results support our main contri-
bution ? the benefit of identifying referring terms for
the category name over using noisier context mod-
els alone. Overall, our work highlights the potential
of text categorization from category names when la-
beled training sets are not available, and indicates
important directions for further research.
Acknowledgments
The authors would like to thank Carlo Strapparava
and Alfio Gliozzo for valuable discussions. This
work was partially supported by the NEGEV project
(www.negev-initiative.org).
References
O. Glickman, E. Shnarch, and I. Dagan. 2006. Lexical
reference: a semantic matching subtask. In EMNLP.
A. Gliozzo, C. Strapparava, and I. Dagan. 2005. Inves-
tigating unsupervised learning for text categorization
bootstrapping. In Proc. of HLT/EMNLP.
Y. Ko and J. Seo. 2004. Learning with unlabeled data
for text categorization using bootstrapping and feature
projection techniques. In Proc. of ACL.
B. Liu, X. Li, W. S. Lee, and P. S. Yu. 2004. Text classi-
fication by labeling words. In Proc. of AAAI.
A. McCallum and K. Nigam. 1999. Text classification
by bootstrapping with keywords, EM and shrinkage.
In ACL Workshop for Unsupervised Learning in NLP.
K. Morik, P. Brockhausen, and T. Joachims. 1999. Com-
bining statistical learning with a knowledge-based ap-
proach - a case study in intensive care monitoring. In
Proc. of the 16th Int?l Conf. on Machine Learning.
M. d. B. Rodr??guez, J. M. Go?mez-Hidalgo, and B. D??az-
Agudo, 2000. Using WordNet to complement training
information in text categorization, volume 189 of Cur-
rent Issues in Linguistic Theory, pages 353?364.
36
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 450?458,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Extracting Lexical Reference Rules from Wikipedia
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
shey@cs.biu.ac.il
Libby Barak
Dept. of Computer Science
University of Toronto
Toronto, Canada M5S 1A4
libbyb@cs.toronto.edu
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
dagan@cs.biu.ac.il
Abstract
This paper describes the extraction from
Wikipedia of lexical reference rules, iden-
tifying references to term meanings trig-
gered by other terms. We present extrac-
tion methods geared to cover the broad
range of the lexical reference relation and
analyze them extensively. Most extrac-
tion methods yield high precision levels,
and our rule-base is shown to perform bet-
ter than other automatically constructed
baselines in a couple of lexical expan-
sion and matching tasks. Our rule-base
yields comparable performance to Word-
Net while providing largely complemen-
tary information.
1 Introduction
A most common need in applied semantic infer-
ence is to infer the meaning of a target term from
other terms in a text. For example, a Question An-
swering system may infer the answer to a ques-
tion regarding luxury cars from a text mentioning
Bentley, which provides a concrete reference to the
sought meaning.
Aiming to capture such lexical inferences we
followed (Glickman et al, 2006), which coined
the term lexical reference (LR) to denote refer-
ences in text to the specific meaning of a target
term. They further analyzed the dataset of the First
Recognizing Textual Entailment Challenge (Da-
gan et al, 2006), which includes examples drawn
from seven different application scenarios. It was
found that an entailing text indeed includes a con-
crete reference to practically every term in the en-
tailed (inferred) sentence.
The lexical reference relation between two
terms may be viewed as a lexical inference rule,
denoted LHS? RHS. Such rule indicates that the
left-hand-side term would generate a reference, in
some texts, to a possible meaning of the right hand
side term, as the Bentley? luxury car example.
In the above example the LHS is a hyponym of
the RHS. Indeed, the commonly used hyponymy,
synonymy and some cases of the meronymy rela-
tions are special cases of lexical reference. How-
ever, lexical reference is a broader relation. For
instance, the LR rule physician ? medicine may
be useful to infer the topic medicine in a text cate-
gorization setting, while an information extraction
system may utilize the rule Margaret Thatcher
? United Kingdom to infer a UK announcement
from the text ?Margaret Thatcher announced?.
To perform such inferences, systems need large
scale knowledge bases of LR rules. A prominent
available resource is WordNet (Fellbaum, 1998),
from which classical relations such as synonyms,
hyponyms and some cases of meronyms may be
used as LR rules. An extension to WordNet was
presented by (Snow et al, 2006). Yet, available
resources do not cover the full scope of lexical ref-
erence.
This paper presents the extraction of a large-
scale rule base from Wikipedia designed to cover
a wide scope of the lexical reference relation. As
a starting point we examine the potential of defi-
nition sentences as a source for LR rules (Ide and
Jean, 1993; Chodorow et al, 1985; Moldovan and
Rus, 2001). When writing a concept definition,
one aims to formulate a concise text that includes
the most characteristic aspects of the defined con-
cept. Therefore, a definition is a promising source
for LR relations between the defined concept and
the definition terms.
In addition, we extract LR rules from Wikipedia
redirect and hyperlink relations. As a guide-
line, we focused on developing simple extrac-
tion methods that may be applicable for other
Web knowledge resources, rather than focusing
on Wikipedia-specific attributes. Overall, our rule
base contains about 8 million candidate lexical ref-
450
erence rules. 1
Extensive analysis estimated that 66% of our
rules are correct, while different portions of the
rule base provide varying recall-precision trade-
offs. Following further error analysis we intro-
duce rule filtering which improves inference per-
formance. The rule base utility was evaluated
within two lexical expansion applications, yield-
ing better results than other automatically con-
structed baselines and comparable results to Word-
Net. A combination with WordNet achieved the
best performance, indicating the significant mar-
ginal contribution of our rule base.
2 Background
Many works on machine readable dictionaries uti-
lized definitions to identify semantic relations be-
tween words (Ide and Jean, 1993). Chodorow et
al. (1985) observed that the head of the defining
phrase is a genus term that describes the defined
concept and suggested simple heuristics to find it.
Other methods use a specialized parser or a set of
regular expressions tuned to a particular dictionary
(Wilks et al, 1996).
Some works utilized Wikipedia to build an on-
tology. Ponzetto and Strube (2007) identified
the subsumption (IS-A) relation from Wikipedia?s
category tags, while in Yago (Suchanek et al,
2007) these tags, redirect links and WordNet were
used to identify instances of 14 predefined spe-
cific semantic relations. These methods depend
on Wikipedia?s category system. The lexical refer-
ence relation we address subsumes most relations
found in these works, while our extractions are not
limited to a fixed set of predefined relations.
Several works examined Wikipedia texts, rather
than just its structured features. Kazama and Tori-
sawa (2007) explores the first sentence of an ar-
ticle and identifies the first noun phrase following
the verb be as a label for the article title. We repro-
duce this part of their work as one of our baselines.
Toral and Mun?oz (2007) uses all nouns in the first
sentence. Gabrilovich and Markovitch (2007) uti-
lized Wikipedia-based concepts as the basis for a
high-dimensional meaning representation space.
Hearst (1992) utilized a list of patterns indica-
tive for the hyponym relation in general texts.
Snow et al (2006) use syntactic path patterns as
features for supervised hyponymy and synonymy
1For download see Textual Entailment Resource Pool at
the ACL-wiki (http://aclweb.org/aclwiki)
classifiers, whose training examples are derived
automatically from WordNet. They use these clas-
sifiers to suggest extensions to the WordNet hierar-
chy, the largest one consisting of 400K new links.
Their automatically created resource is regarded in
our paper as a primary baseline for comparison.
Many works addressed the more general notion
of lexical associations, or association rules (e.g.
(Ruge, 1992; Rapp, 2002)). For example, The
Beatles, Abbey Road and Sgt. Pepper would all
be considered lexically associated. However this
is a rather loose notion, which only indicates that
terms are semantically ?related? and are likely to
co-occur with each other. On the other hand, lex-
ical reference is a special case of lexical associa-
tion, which specifies concretely that a reference to
the meaning of one term may be inferred from the
other. For example, Abbey Road provides a con-
crete reference to The Beatles, enabling to infer a
sentence like ?I listened to The Beatles? from ?I
listened to Abbey Road?, while it does not refer
specifically to Sgt. Pepper.
3 Extracting Rules from Wikipedia
Our goal is to utilize the broad knowledge of
Wikipedia to extract a knowledge base of lexical
reference rules. Each Wikipedia article provides
a definition for the concept denoted by the title
of the article. As the most concise definition we
take the first sentence of each article, following
(Kazama and Torisawa, 2007). Our preliminary
evaluations showed that taking the entire first para-
graph as the definition rarely introduces new valid
rules while harming extraction precision signifi-
cantly.
Since a concept definition usually employs
more general terms than the defined concept (Ide
and Jean, 1993), the concept title is more likely
to refer to terms in its definition rather than vice
versa. Therefore the title is taken as the LHS of
the constructed rule while the extracted definition
term is taken as its RHS. As Wikipedia?s titles are
mostly noun phrases, the terms we extract as RHSs
are the nouns and noun phrases in the definition.
The remainder of this section describes our meth-
ods for extracting rules from the definition sen-
tence and from additional Wikipedia information.
Be-Comp Following the general idea in
(Kazama and Torisawa, 2007), we identify the IS-
A pattern in the definition sentence by extract-
ing nominal complements of the verb ?be?, taking
451
No. Extraction Rule
James Eugene ?Jim? Carrey is a Canadian-American actor
and comedian
1 Be-Comp Jim Carrey? Canadian-American actor
2 Be-Comp Jim Carrey? actor
3 Be-Comp Jim Carrey? comedian
Abbey Road is an album released by The Beatles
4 All-N Abbey Road? The Beatles
5 Parenthesis Graph? mathematics
6 Parenthesis Graph? data structure
7 Redirect CPU? Central processing unit
8 Redirect Receptors IgG? Antibody
9 Redirect Hypertension? Elevated blood-pressure
10 Link pet? Domesticated Animal
11 Link Gestaltist? Gestalt psychology
Table 1: Examples of rule extraction methods
them as the RHS of a rule whose LHS is the article
title. While Kazama and Torisawa used a chun-
ker, we parsed the definition sentence using Mini-
par (Lin, 1998b). Our initial experiments showed
that parse-based extraction is more accurate than
chunk-based extraction. It also enables us extract-
ing additional rules by splitting conjoined noun
phrases and by taking both the head noun and the
complete base noun phrase as the RHS for sepa-
rate rules (examples 1?3 in Table 1).
All-N The Be-Comp extraction method yields
mostly hypernym relations, which do not exploit
the full range of lexical references within the con-
cept definition. Therefore, we further create rules
for all head nouns and base noun phrases within
the definition (example 4). An unsupervised reli-
ability score for rules extracted by this method is
investigated in Section 4.3.
Title Parenthesis A common convention in
Wikipedia to disambiguate ambiguous titles is
adding a descriptive term in parenthesis at the end
of the title, as in The Siren (Musical), The Siren
(sculpture) and Siren (amphibian). From such ti-
tles we extract rules in which the descriptive term
inside the parenthesis is the RHS and the rest of
the title is the LHS (examples 5?6).
Redirect As any dictionary and encyclopedia,
Wikipedia contains Redirect links that direct dif-
ferent search queries to the same article, which has
a canonical title. For instance, there are 86 differ-
ent queries that redirect the user to United States
(e.g. U.S.A., America, Yankee land). Redirect
links are hand coded, specifying that both terms
refer to the same concept. We therefore generate a
bidirectional entailment rule for each redirect link
(examples 7?9).
Link Wikipedia texts contain hyper links to ar-
ticles. For each link we generate a rule whose LHS
is the linking text and RHS is the title of the linked
article (examples 10?11). In this case we gener-
ate a directional rule since links do not necessarily
connect semantically equivalent entities.
We note that the last three extraction methods
should not be considered as Wikipedia specific,
since many Web-like knowledge bases contain
redirects, hyper-links and disambiguation means.
Wikipedia has additional structural features such
as category tags, structured summary tablets for
specific semantic classes, and articles containing
lists which were exploited in prior work as re-
viewed in Section 2.
As shown next, the different extraction meth-
ods yield different precision levels. This may al-
low an application to utilize only a portion of the
rule base whose precision is above a desired level,
and thus choose between several possible recall-
precision tradeoffs.
4 Extraction Methods Analysis
We applied our rule extraction methods over a
version of Wikipedia available in a database con-
structed by (Zesch et al, 2007)2. The extraction
yielded about 8 million rules altogether, with over
2.4 million distinct RHSs and 2.8 million distinct
LHSs. As expected, the extracted rules involve
mostly named entities and specific concepts, typi-
cally covered in encyclopedias.
4.1 Judging Rule Correctness
Following the spirit of the fine-grained human
evaluation in (Snow et al, 2006), we randomly
sampled 800 rules from our rule-base and pre-
sented them to an annotator who judged them for
correctness, according to the lexical reference no-
tion specified above. In cases which were too dif-
ficult to judge the annotator was allowed to ab-
stain, which happened for 20 rules. 66% of the re-
maining rules were annotated as correct. 200 rules
from the sample were judged by another annotator
for agreement measurement. The resulting Kappa
score was 0.7 (substantial agreement (Landis and
2English version from February 2007, containing 1.6 mil-
lion articles. www.ukp.tu-darmstadt.de/software/JWPL
452
Extraction Per Method Accumulated
Method P Est. #Rules P %obtained
Redirect 0.87 1,851,384 0.87 31
Be-Comp 0.78 1,618,913 0.82 60
Parenthesis 0.71 94,155 0.82 60
Link 0.7 485,528 0.80 68
All-N 0.49 1,580,574 0.66 100
Table 2: Manual analysis: precision and estimated number
of correct rules per extraction method, and precision and %
of correct rules obtained of rule-sets accumulated by method.
Koch, 1997)), either when considering all the ab-
stained rules as correct or as incorrect.
The middle columns of Table 2 present, for each
extraction method, the obtained percentage of cor-
rect rules (precision) and their estimated absolute
number. This number is estimated by multiplying
the number of annotated correct rules for the ex-
traction method by the sampling proportion. In to-
tal, we estimate that our resource contains 5.6 mil-
lion correct rules. For comparison, Snow?s pub-
lished extension to WordNet3, which covers simi-
lar types of terms but is restricted to synonyms and
hyponyms, includes 400,000 relations.
The right part of Table 2 shows the perfor-
mance figures for accumulated rule bases, created
by adding the extraction methods one at a time in
order of their precision. % obtained is the per-
centage of correct rules in each rule base out of
the total number of correct rules extracted jointly
by all methods (the union set).
We can see that excluding the All-N method
all extraction methods reach quite high precision
levels of 0.7-0.87, with accumulated precision of
0.84. By selecting only a subset of the extrac-
tion methods, according to their precision, one can
choose different recall-precision tradeoff points
that suit application preferences.
The less accurate All-N method may be used
when high recall is important, accounting for 32%
of the correct rules. An examination of the paths
in All-N reveals, beyond standard hyponymy and
synonymy, various semantic relations that satisfy
lexical reference, such as Location, Occupation
and Creation, as illustrated in Table 3. Typical re-
lations covered by Redirect and Link rules include
3http://ai.stanford.edu/?rion/swn/
4As a non-comparable reference, Snow?s fine-grained
evaluation showed a precision of 0.84 on 10K rules and 0.68
on 20K rules; however, they were interested only in the hy-
ponym relation while we evaluate our rules according to the
broader LR relation.
synonyms (NY State Trooper ? New York State
Police), morphological derivations (irritate ? ir-
ritation), different spellings or naming (Pytagoras
? Pythagoras) and acronyms (AIS? Alarm Indi-
cation Signal).
4.2 Error Analysis
We sampled 100 rules which were annotated as in-
correct and examined the causes of errors. Figure
1 shows the distribution of error types.
Wrong NP part - The most common error
(35% of the errors) is taking an inappropriate part
of a noun phrase (NP) as the rule right hand side
(RHS). As described in Section 3, we create two
rules from each extracted NP, by taking both the
head noun and the complete base NP as RHSs.
While both rules are usually correct, there are
cases in which the left hand side (LHS) refers to
the NP as a whole but not to part of it. For ex-
ample, Margaret Thatcher refers to United King-
dom but not to Kingdom. In Section 5 we suggest
a filtering method which addresses some of these
errors. Future research may exploit methods for
detecting multi-words expressions.
All-N pa
ttern er
rors
13%Tra
nsparen
t head 11%
Wrong N
P part 35%
Technic
al error
s
10%
Dates a
nd Plac
es
5% Link err
ors 5% Redirec
t errors 5%
Related
 
but not Referrin
g 16%
Figure 1: Error analysis: type of incorrect rules
Related but not Referring - Although all terms
in a definition are highly related to the defined con-
cept, not all are referred by it. For example the
origin of a person (*The Beatles? Liverpool5) or
family ties such as ?daughter of? or ?sire of?.
All-N errors - Some of the articles start with a
long sentence which may include information that
is not directly referred by the title of the article.
For instance, consider *Interstate 80 ? Califor-
nia from ?Interstate 80 runs from California to
New Jersey?. In Section 4.3 we further analyze
this type of error and point at a possible direction
for addressing it.
Transparent head - This is the phenomenon in
which the syntactic head of a noun phrase does
5The asterisk denotes an incorrect rule
453
Relation Rule Path Pattern
Location Lovek? Cambodia Lovek city in Cambodia
Occupation Thomas H. Cormen? computer science Thomas H. Cormen professor of computer science
Creation Genocidal Healer? James White Genocidal Healer novel by James White
Origin Willem van Aelst? Dutch Willem van Aelst Dutch artist
Alias Dean Moriarty? Benjamin Linus Dean Moriarty is an alias of Benjamin Linus on Lost.
Spelling Egushawa? Agushaway Egushawa, also spelled Agushaway...
Table 3: All-N rules exemplifying various types of LR relations
not bear its primary meaning, while it has a mod-
ifier which serves as the semantic head (Fillmore
et al, 2002; Grishman et al, 1986). Since parsers
identify the syntactic head, we extract an incorrect
rule in such cases. For instance, deriving *Prince
William ? member instead of Prince William ?
British Royal Family from ?Prince William is a
member of the British Royal Family?. Even though
we implemented the common solution of using a
list of typical transparent heads, this solution is
partial since there is no closed set of such phrases.
Technical errors - Technical extraction errors
were mainly due to erroneous identification of the
title in the definition sentence or mishandling non-
English texts.
Dates and Places - Dates and places where a
certain person was born at, lived in or worked at
often appear in definitions but do not comply to
the lexical reference notion (*Galileo Galilei ?
15 February 1564).
Link errors - These are usually the result of
wrong assignment of the reference direction. Such
errors mostly occur when a general term, e.g. rev-
olution, links to a more specific albeit typical con-
cept, e.g. French Revolution.
Redirect errors - These may occur in some
cases in which the extracted rule is not bidirec-
tional. E.g. *Anti-globalization ? Movement of
Movements is wrong but the opposite entailment
direction is correct, as Movement of Movements is
a popular term in Italy for Anti-globalization.
4.3 Scoring All-N Rules
We observed that the likelihood of nouns men-
tioned in a definition to be referred by the con-
cept title depends greatly on the syntactic path
connecting them (which was exploited also in
(Snow et al, 2006)). For instance, the path pro-
duced by Minipar for example 4 in Table 1 is title
subj
??album vrel??released by?subj?? bypcomp?n?? noun.
In order to estimate the likelihood that a syn-
tactic path indicates lexical reference we collected
from Wikipedia all paths connecting a title to a
noun phrase in the definition sentence. We note
that since there is no available resource which cov-
ers the full breadth of lexical reference we could
not obtain sufficiently broad supervised training
data for learning which paths correspond to cor-
rect references. This is in contrast to (Snow et al,
2005) which focused only on hyponymy and syn-
onymy relations and could therefore extract posi-
tive and negative examples from WordNet.
We therefore propose the following unsuper-
vised reference likelihood score for a syntactic
path p within a definition, based on two counts:
the number of times p connects an article title with
a noun in its definition, denoted by Ct(p), and the
total number of p?s occurrences in Wikipedia de-
finitions, C(p). The score of a path is then de-
fined as Ct(p)C(p) . The rational for this score is that
C(p)? Ct(p) corresponds to the number of times
in which the path connects two nouns within the
definition, none of which is the title. These in-
stances are likely to be non-referring, since a con-
cise definition typically does not contain terms that
can be inferred from each other. Thus our score
may be seen as an approximation for the probabil-
ity that the two nouns connected by an arbitrary
occurrence of the path would satisfy the reference
relation. For instance, the path of example 4 ob-
tained a score of 0.98.
We used this score to sort the set of rules ex-
tracted by the All-N method and split the sorted list
into 3 thirds: top, middle and bottom. As shown in
Table 4, this obtained reasonably high precision
for the top third of these rules, relative to the other
two thirds. This precision difference indicates that
our unsupervised path score provides useful infor-
mation about rule reliability.
It is worth noting that in our sample 57% of All-
N errors, 62% of Related but not Referring incor-
rect rules and all incorrect rules of type Dates and
454
Extraction Per Method Accumulated
Method P Est. #Rules P %obtained
All-Ntop 0.60 684,238 0.76 83
All-Nmiddle 0.46 380,572 0.72 90
All-Nbottom 0.41 515,764 0.66 100
Table 4: Splitting All-N extraction method into 3 sub-types.
These three rows replace the last row of Table 2
Places were extracted by the All-Nbottom method
and thus may be identified as less reliable. How-
ever, this split was not observed to improve per-
formance in the application oriented evaluations
of Section 6. Further research is thus needed to
fully exploit the potential of the syntactic path as
an indicator for rule correctness.
5 Filtering Rules
Following our error analysis, future research is
needed for addressing each specific type of error.
However, during the analysis we observed that all
types of erroneous rules tend to relate terms that
are rather unlikely to co-occur together. We there-
fore suggest, as an optional filter, to recognize
such rules by their co-occurrence statistics using
the common Dice coefficient:
2 ? C(LHS,RHS)
C(LHS) + C(RHS)
where C(x) is the number of articles in Wikipedia
in which all words of x appear.
In order to partially overcome the Wrong NP
part error, identified in Section 4.2 to be the most
common error, we adjust the Dice equation for
rules whose RHS is also part of a larger noun
phrase (NP):
2 ? (C(LHS,RHS)? C(LHS,NPRHS))
C(LHS) + C(RHS)
where NPRHS is the complete NP whose part
is the RHS. This adjustment counts only co-
occurrences in which the LHS appears with the
RHS alone and not with the larger NP. This sub-
stantially reduces the Dice score for those cases in
which the LHS co-occurs mainly with the full NP.
Given the Dice score rules whose score does not
exceed a threshold may be filtered. For example,
the incorrect rule *aerial tramway? car was fil-
tered, where the correct RHS for this LHS is the
complete NP cable car. Another filtered rule is
magic? cryptography which is correct only for a
very idiosyncratic meaning.6
We also examined another filtering score, the
cosine similarity between the vectors representing
the two rule sides in LSA (Latent Semantic Analy-
sis) space (Deerwester et al, 1990). However, as
the results with this filter resemble those for Dice
we present results only for the simpler Dice filter.
6 Application Oriented Evaluations
Our primary application oriented evaluation is
within an unsupervised lexical expansion scenario
applied to a text categorization data set (Section
6.1). Additionally, we evaluate the utility of our
rule base as a lexical resource for recognizing tex-
tual entailment (Section 6.2).
6.1 Unsupervised Text Categorization
Our categorization setting resembles typical query
expansion in information retrieval (IR), where the
category name is considered as the query. The ad-
vantage of using a text categorization test set is
that it includes exhaustive annotation for all doc-
uments. Typical IR datasets, on the other hand,
are partially annotated through a pooling proce-
dure. Thus, some of our valid lexical expansions
might retrieve non-annotated documents that were
missed by the previously pooled systems.
6.1.1 Experimental Setting
Our categorization experiment follows a typical
keywords-based text categorization scheme (Mc-
Callum and Nigam, 1999; Liu et al, 2004). Tak-
ing a lexical reference perspective, we assume that
the characteristic expansion terms for a category
should refer to the term (or terms) denoting the
category name. Accordingly, we construct the cat-
egory?s feature vector by taking first the category
name itself, and then expanding it with all left-
hand sides of lexical reference rules whose right-
hand side is the category name. For example, the
category ?Cars? is expanded by rules such as Fer-
rari F50? car. During classification cosine sim-
ilarity is measured between the feature vector of
the classified document and the expanded vectors
of all categories. The document is assigned to
the category which yields the highest similarity
score, following a single-class classification ap-
proach (Liu et al, 2004).
6Magic was the United States codename for intelligence
derived from cryptanalysis during World War II.
455
Rule Base R P F1
Baselines:
No Expansion 0.19 0.54 0.28
WikiBL 0.19 0.53 0.28
Snow400K 0.19 0.54 0.28
Lin 0.25 0.39 0.30
WordNet 0.30 0.47 0.37
Extraction Methods from Wikipedia:
Redirect + Be-Comp 0.22 0.55 0.31
All rules 0.31 0.38 0.34
All rules + Dice filter 0.31 0.49 0.38
Union:
WordNet + WikiAll rules+Dice 0.35 0.47 0.40
Table 5: Results of different rule bases for 20 newsgroups
category name expansion
It should be noted that keyword-based text
categorization systems employ various additional
steps, such as bootstrapping, which generalize to
multi-class settings and further improve perfor-
mance. Our basic implementation suffices to eval-
uate comparatively the direct impact of different
expansion resources on the initial classification.
For evaluation we used the test set of the
?bydate? version of the 20-News Groups collec-
tion,7 which contains 18,846 documents parti-
tioned (nearly) evenly over the 20 categories8.
6.1.2 Baselines Results
We compare the quality of our rule base expan-
sions to 5 baselines (Table 5). The first avoids any
expansion, classifying documents based on cosine
similarity with category names only. As expected,
it yields relatively high precision but low recall,
indicating the need for lexical expansion.
The second baseline is our implementation of
the relevant part of the Wikipedia extraction in
(Kazama and Torisawa, 2007), taking the first
noun after a be verb in the definition sentence, de-
noted as WikiBL. This baseline does not improve
performance at all over no expansion.
The next two baselines employ state-of-the-art
lexical resources. One uses Snow?s extension to
WordNet which was mentioned earlier. This re-
source did not yield a noticeable improvement, ei-
7www.ai.mit.edu/people/jrennie/20Newsgroups.
8The keywords used as category names are: athe-
ism; graphic; microsoft windows; ibm,pc,hardware;
mac,hardware; x11,x-windows; sale; car; motorcycle;
baseball; hockey; cryptography; electronics; medicine; outer
space; christian(noun & adj); gun; mideast,middle east;
politics; religion
ther over the No Expansion baseline or over Word-
Net when joined with its expansions. The sec-
ond uses Lin dependency similarity, a syntactic-
dependency based distributional word similarity
resource described in (Lin, 1998a)9. We used var-
ious thresholds on the length of the expansion list
derived from this resource. The best result, re-
ported here, provides only a minor F1 improve-
ment over No Expansion, with modest recall in-
crease and significant precision drop, as can be ex-
pected from such distributional method.
The last baseline uses WordNet for expansion.
First we expand all the senses of each category
name by their derivations and synonyms. Each ob-
tained term is then expanded by its hyponyms, or
by its meronyms if it has no hyponyms. Finally,
the results are further expanded by their deriva-
tions and synonyms.10 WordNet expansions im-
prove substantially both Recall and F1 relative to
No Expansion, while decreasing precision.
6.1.3 Wikipedia Results
We then used for expansion different subsets
of our rule base, producing alternative recall-
precision tradeoffs. Table 5 presents the most in-
teresting results. Using any subset of the rules
yields better performance than any of the other
automatically constructed baselines (Lin, Snow
and WikiBL). Utilizing the most precise extrac-
tion methods of Redirect and Be-Comp yields the
highest precision, comparable to No Expansion,
but just a small recall increase. Using the entire
rule base yields the highest recall, while filtering
rules by the Dice coefficient (with 0.1 threshold)
substantially increases precision without harming
recall. With this configuration our automatically-
constructed resource achieves comparable perfor-
mance to the manually built WordNet.
Finally, since a dictionary and an encyclopedia
are complementary in nature, we applied the union
of WordNet and the filtered Wikipedia expansions.
This configuration yields the best results: it main-
tains WordNet?s precision and adds nearly 50% to
the recall increase of WordNet over No Expansion,
indicating the substantial marginal contribution of
Wikipedia. Furthermore, with the fast growth of
Wikipedia the recall of our resource is expected to
increase while maintaining its precision.
9Downloaded from www.cs.ualberta.ca/lindek/demos.htm
10We also tried expanding by the entire hyponym hierarchy
and considering only the first sense of each synset, but the
method described above achieved the best performance.
456
Category Name Expanding Terms
Politics opposition, coalition, whip(a)
Cryptography adversary, cryptosystem, key
Mac PowerBook, Radius(b), Grab(c)
Religion heaven, creation, belief, missionary
Medicine doctor, physician, treatment, clinical
Computer Graphics radiosity(d), rendering, siggraph(e)
Table 6: Some Wikipedia rules not in WordNet, which con-
tributed to text categorization. (a) a legislator who enforce
leadership desire (b) a hardware firm specializing in Macin-
tosh equipment (c) a Macintosh screen capture software (d)
an illumination algorithm (e) a computer graphics conference
Configuration Accuracy Accuracy Drop
WordNet + Wikipedia 60.0 % -
Without WordNet 57.7 % 2.3 %
Without Wikipedia 58.9 % 1.1 %
Table 7: RTE accuracy results for ablation tests.
Table 6 illustrates few examples of useful rules
that were found in Wikipedia but not in WordNet.
We conjecture that in other application settings
the rules extracted from Wikipedia might show
even greater marginal contribution, particularly in
specialized domains not covered well by Word-
Net. Another advantage of a resource based on
Wikipedia is that it is available in many more lan-
guages than WordNet.
6.2 Recognizing Textual Entailment (RTE)
As a second application-oriented evaluation we
measured the contributions of our (filtered)
Wikipedia resource and WordNet to RTE infer-
ence (Giampiccolo et al, 2007). To that end, we
incorporated both resources within a typical basic
RTE system architecture (Bar-Haim et al, 2008).
This system determines whether a text entails an-
other sentence based on various matching crite-
ria that detect syntactic, logical and lexical cor-
respondences (or mismatches). Most relevant for
our evaluation, lexical matches are detected when
a Wikipedia rule?s LHS appears in the text and
its RHS in the hypothesis, or similarly when pairs
of WordNet synonyms, hyponyms-hypernyms and
derivations appear across the text and hypothesis.
The system?s weights were trained on the devel-
opment set of RTE-3 and tested on RTE-4 (which
included this year only a test set).
To measure the marginal contribution of the two
resources we performed ablation tests, comparing
the accuracy of the full system to that achieved
when removing either resource. Table 7 presents
the results, which are similar in nature to those ob-
tained for text categorization. Wikipedia obtained
a marginal contribution of 1.1%, about half of the
analogous contribution of WordNet?s manually-
constructed information. We note that for current
RTE technology it is very typical to gain just a
few percents in accuracy thanks to external knowl-
edge resources, while individual resources usually
contribute around 0.5?2% (Iftene and Balahur-
Dobrescu, 2007; Dinu and Wang, 2009). Some
Wikipedia rules not in WordNet which contributed
to RTE inference are Jurassic Park ? Michael
Crichton, GCC? Gulf Cooperation Council.
7 Conclusions and Future Work
We presented construction of a large-scale re-
source of lexical reference rules, as useful in ap-
plied lexical inference. Extensive rule-level analy-
sis showed that different recall-precision tradeoffs
can be obtained by utilizing different extraction
methods. It also identified major reasons for er-
rors, pointing at potential future improvements.
We further suggested a filtering method which sig-
nificantly improved performance.
Even though the resource was constructed by
quite simple extraction methods, it was proven to
be beneficial within two different application set-
ting. While being an automatically built resource,
extracted from a knowledge-base created for hu-
man consumption, it showed comparable perfor-
mance to WordNet, which was manually created
for computational purposes. Most importantly, it
also provides complementary knowledge to Word-
Net, with unique lexical reference rules.
Future research is needed to improve resource?s
precision, especially for the All-N method. As
a first step, we investigated a novel unsupervised
score for rules extracted from definition sentences.
We also intend to consider the rule base as a di-
rected graph and exploit the graph structure for
further rule extraction and validation.
Acknowledgments
The authors would like to thank Idan Szpektor
for valuable advices. This work was partially
supported by the NEGEV project (www.negev-
initiative.org), the PASCAL-2 Network of Excel-
lence of the European Community FP7-ICT-2007-
1-216886 and by the Israel Science Foundation
grant 1112/08.
457
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
Szpektor. 2008. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proceedings of TAC.
Martin S. Chodorow, Roy J. Byrd, and George E. Hei-
dorn. 1985. Extracting semantic hierarchies from a
large on-line dictionary. In Proceedings of ACL.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Georgiana Dinu and Rui Wang. 2009. Inference rules
for recognizing textual entailment. In Proceedings
of the IWCS.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of LREC.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
IJCAI.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3):205?215.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Nancy Ide and Ve?ronis Jean. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings of
KB & KS Workshop.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
J. Richard Landis and Gary G. Koch. 1997. The
measurements of observer agreement for categorical
data. In Biometrics, pages 33:159?174.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on Eval-
uation of Parsing Systems at LREC.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.
2004. Text classification by labeling words. In Pro-
ceedings of AAAI.
Andrew McCallum and Kamal Nigam. 1999. Text
classification by bootstrapping with keywords, EM
and shrinkage. In Proceedings of ACL Workshop for
unsupervised Learning in NLP.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Simone P. Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from wikipedia. In
Proceedings of AAAI.
Reinhard Rapp. 2002. The computation of word asso-
ciations: comparing syntagmatic and paradigmatic
approaches. In Proceedings of COLING.
Gerda Ruge. 1992. Experiment on linguistically-based
term associations. Information Processing & Man-
agement, 28(3):317?332.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Antonio Toral and Rafael Mun?oz. 2007. A proposal
to automatically build and maintain gazetteers for
named entity recognition by using wikipedia. In
Proceedings of NAACL/HLT.
Yorick A. Wilks, Brian M. Slator, and Louise M.
Guthrie. 1996. Electric words: dictionaries, com-
puters, and meanings. MIT Press, Cambridge, MA,
USA.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Analyzing and accessing wikipedia as a lex-
ical semantic resource. In Data Structures for Lin-
guistic Resources and Applications, pages 197?205.
458
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 1?10,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Modeling the Acquisition of Mental State Verbs
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
Children acquire mental state verbs (MSVs)
much later than other, lower-frequency, words.
One factor proposed to contribute to this de-
lay is that children must learn various seman-
tic and syntactic cues that draw attention to the
difficult-to-observe mental content of a scene.
We develop a novel computational approach
that enables us to explore the role of such cues,
and show that our model can replicate aspects
of the developmental trajectory of MSV acqui-
sition.
1 Introduction
Mental State Verbs (MSVs), such as think, know,
and want, are very frequent in child-directed lan-
guage, yet children use them productively much
later than lower-frequency action verbs, such as fall
and throw (Johnson and Wellman, 1980; Shatz et al,
1983). Psycholinguistic theories have suggested that
there is a delay in the acquisition of MSVs because
they require certain cognitive and/or linguistic skills
that are not available during the early stages of lan-
guage development. For example, MSVs typically
occur with a sentential complement (SC) that refers
to the propositional content of the mental state, as in
He thinks Mom went home. Children have to reach a
stage of syntactic development that includes some
facility with SCs in order to fully acquire MSVs.
However, even at 3?5 years old, children are able to
process SCs only imperfectly (e.g., Asplin, 2002).
Even when children are able to produce SCs with
other verbs (such as verbs of communication, as in
He said Mom went home), there is a lag before they
productively use MSVs referring to actual mental
content (Diessel and Tomasello, 2001).1 Psycholin-
guists have suggested that young children lack the
conceptual ability to conceive that others have men-
tal states separate from their own (Bartsch and Well-
man, 1995; Gopnik and Meltzoff, 1997), further de-
laying the acquisition of MSVs.
Another factor suggested to contribute to the dif-
ficulty of acquiring MSVs is their informational re-
quirements (Gleitman et al, 2005; Papafragou et al,
2007). Children learn word meanings by figuring
out which aspects of an observed scene are referred
to by a particular word (Quine, 1960). MSVs of-
ten refer to aspects of the world that are not directly
observable (i.e., the beliefs and desires of another
entity). Thus, in addition to the above-mentioned
challenges posed by children?s developing linguis-
tic/conceptual abilities, children may simply have
difficulty in identifying the relevant mental content
necessary to learning MSVs.
In particular, Papafragou et al (2007) [PCG] have
shown that even given adequate conceptual and lin-
guistic abilities (as in adults) the mental events in a
scene (the actors? internal states) are not attended
to as much as the actions, unless there are cues
that heighten the salience of the mental content.
PCG further demonstrate that children?s sensitivity
to such cues lags behind that of adults, suggesting an
additional factor in the acquisition of MSVs which
1Researchers have noted that children use MSVs in fixed
phrases, in a performative use or as a pragmatic marker, well be-
fore they use them to refer to actual mental content (e.g., Diessel
and Tomasello, 2001; Shatz et al, 1983). Here by ?acquisition
of MSVs?, we are specifically referring to children learning us-
ages that genuinely refer to mental content.
1
is the developmental change in how strongly such
cues are associated with the relevant mental content.
We develop a computational model of MSV ac-
quisition (the first, to our knowledge) to further il-
luminate these issues. We extend an existing model
of verb argument structure acquisition (Alishahi and
Stevenson, 2008) to enable the representation and
processing of mental state semantics and syntax.
We simulate the developmental change proposed by
PCG through a gradually increasing ability in the
model to appropriately attend to the mental content
of a scene. In addition, we suggest that even when
the learner?s semantic representation is biased to-
wards the action content, the learner attends to the
observed SC syntax in an MSV utterance. This is
especially important to account for the pattern of er-
rors in child data. Our model thus extends the ac-
count of PCG to show that a probabilistic interplay
of the semantic and syntactic features of a partial and
somewhat erroneous perception of the input, com-
bined with a growing ability to attend to cues indica-
tive of mental content, can help to account for chil-
dren?s developmental trajectory in learning MSVs.
2 Background and Our Approach
To investigate the linguistic and contextual cues that
could help in learning MSVs, PCG use a procedure
called the Human Simulation Paradigm (originally
proposed by Gillette et al, 1999). In this paradigm,
subjects are put in situations intended to simulate
various word learning conditions of young children.
E.g., in one condition, adults watch silent videos of
caregivers interacting with children, and are asked
to predict the verb uttered by the caregiver. In an-
other condition, subjects hear a sentence containing
a nonce verb (e.g., gorp) after watching the video,
and are asked what gorp might mean.
We focus on two factors investigated by PCG in
the performance of adults and children in identifying
MSVs. The first factor they investigated involved
the syntactic frame used when subjects were given a
sentence with a nonce verb. PCG hypothesized that
an SC frame would be a cue to mental content (and
an MSV), since the SC refers to propositional con-
tent. The second factor PCG examined was whether
the video described a ?true belief? or a ?false be-
lief? scene: A true belief scene shows an ordinary
situation which unfolds as the character in the scene
expects ? e.g., a little boy takes food to his grand-
mother, and she is there in the house as expected.
The corresponding false belief scene has an unex-
pected outcome for the character ? in this case, an-
other character has replaced the grandmother in her
bed. Here the hypothesis was that such false belief
scenes would heighten the salience of mental activ-
ity in the scene and lead to greater belief verb re-
sponses in describing them.
PCG?s results showed that both adults and chil-
dren were sensitive to both the scene and syntax
cues, but children?s ability to draw on such cues was
inferior to that of adults. They thus propose that the
difference between children and adults is that chil-
dren have not yet formed as strong an association
as adults between the cues and the mental content
of a scene as required to match the performance of
adults. Nonetheless, their results suggest that the
participating children had the conceptual and lin-
guistic abilities required for MSVs, since they were
able to produce them under conditions with suffi-
ciently strong cues.
We simulate PCG?s experiments using a novel
computational approach. Following PCG, we as-
sume that even when a learner is able to perceive
the general semantic and syntactic properties of a
belief scene and associated utterance, they may not
attend to the mental content in every situation, and
that this ability improves over time. We model a de-
velopmental change in a learner?s attention to mental
content: At early stages, corresponding to the state
of young children, the learner largely focuses on the
action aspects of a belief scene, even in the presence
of an utterance using an MSV. Over time, the learner
gradually increases in the ability to attend appropri-
ately to the mental aspects of such a scene and ut-
terance, until adult-like competence is achieved in
associating the available cues with mental content.
Importantly, our work extends the proposal of
PCG by bringing in evidence from other relevant
studies on children?s ability to process SCs. More
specifically, we suggest that when children hear a
sentence like I think Mom went home, they recog-
nize (and record) the existence of an SC, while at
the same time they focus on the action semantics
as the main (most salient) event. In other words,
we assume that children?s imperfect syntactic abil-
2
ities are at least sufficient to recognize the SC us-
age (Nelson et al, 1989; Asplin, 2002). However,
their attention is mostly directed towards the action
expressed in the embedded complement, either be-
cause mental content is less easily observable than
action (Papafragou et al, 2007), or due to the lin-
guistic saliency of the embedded clause (Diessel and
Tomasello, 2001; Dehe and Wichmann, 2010). As
mentioned above, we model this misrepresentation
by considering the possibility of not attending to
mental content in a belief scene. Specifically, we
assume that (i) the model is very likely to overlook
the mental content at earlier stages (corresponding to
children?s observed behaviour); and (ii) as the model
?ages? (i.e., receives more input), its attentional abil-
ities improve and thus the model is more likely to
focus on the mental content as the main proposition.
Our results suggest that these changes to the model
lead to a match between our model?s behaviour and
PCG?s differential results for children and adults.
3 The Computational Model
A number of computational models have examined
the role of interacting syntactic and semantic cues
in the acquisition of verb argument structure (e.g.,
Niyogi, 2002; Buttery, 2006; Alishahi and Steven-
son, 2008; Perfors et al, 2010; Parisien and Steven-
son, 2011). However, to our knowledge no com-
putational model has addressed the developmental
trajectory in the acquisition of MSVs. Here we ex-
tend the verb argument structure acquisition model
of Alishahi and Stevenson (2008) to enable it to ac-
count for MSV acquisition. Specifically, we use
their core Bayesian learning algorithm, but modify
the input processing component to reflect a develop-
mental change in attention to the mental state con-
tent of an MSV usage and its consequent represen-
tation, as noted above.
We use this model for the following reasons: (i) it
focuses on argument structure learning, and the in-
terplay between syntax and semantics, which are key
to MSV acquisition; (ii) it is probabilistic and hence
can naturally capture gradient responses to different
cues; and (iii) it is incremental, which allows us to
investigate changes in behaviour over time. We first
give an overview of the original model, and then ex-
plain our extensions.
3.1 Model Overview
The input to the model is a sequence of utterances
(what the child hears), each paired with a scene
(what the child perceives); see Table 1 for an ex-
ample. First, the frame extraction component of
the model extracts from the input pair a frame?
a collection of features. We use features that in-
clude both semantic properties (?event primitives?
and ?event participants?) and syntactic properties
(?syntactic pattern? and ?verb count?). See Table 2
for examples of two possible frames extracted from
the pair in Table 1. Second, the learning component
of the model incrementally clusters the extracted
frames one by one. These clusters correspond to
constructions that reflect probabilistic associations
of semantic and syntactic features across similar us-
ages, such as an agentive intransitive or causative
transitive. The model can use these associations to
simulate various language tasks as the prediction of
a missing feature given others. For example, to sim-
ulate the human simulation paradigm setting, we can
use the model to predict a missing verb on the basis
of the available semantic and syntactic information
(as in Alishahi and Pyykkon?en, 2011).
3.2 Algorithm for Learning Constructions
The model clusters the input frames into construc-
tions on the basis of their overall similarity in the
values of their features. Importantly, the model
learns these constructions incrementally, consider-
ing the possibility of creating a new construction for
a given frame if the frame is not sufficiently similar
to any of the existing constructions. Formally, the
model finds the best construction (including a new
one) for a given frame F as in:
BestConstruction(F ) = argmax
k?Constructions
P (k|F )
(1)
where k ranges over all existing constructions and a
new one. Using Bayes rule:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of each construction P (k) is
estimated as the proportion of observed frames that
are in k, assigning a higher prior to constructions
3
Think[state,consider,cogitate](I[experiencer,preceiver,considerer ],Go[physical,act,move](MOM[agent,actor,change],HOME[location,destination]))
I think Mom went home.
Table 1: A sample Scene?Utterance input pair.
(a) Interpretation#1 (mental event is attended to) (b) Interpretation#2 (mental event not attended to)
main predicate think main predicate go
other predicate go other predicate think
event primitives { state, consider , cogitate } event primitives { physical , act ,move}
event participants { experiencer , perceiver , considerer} event participants { agent , actor , change}
{ preposition, action, perceivable} { location, destination}
syntactic pattern arg1 verb arg-S syntactic pattern arg1 verb arg-S
verb count 2 verb count 2
Table 2: Two frames extracted from the scene?utterance pair in Table 1. The bottom left and right panels of the table
describe the two possible interpretations given the input pair. (a) Interpretation#1 assumes that the mental event is the
focus of attention. Here think is interpreted as the main predicate, which the event primitives and participants refer
to. (b) Interpretation#2 assumes that attention is mostly directed to the physical action in the scene, and thus go is
taken to be the main predicate, which also determines the extracted event primitives and participants. Note that for
both interpretations, the learner is assumed to perceive the utterance in full, thus both verbs are heard in the context
of the sentential complement syntax (i.e., syntactic pattern with SC and 2 verbs), without fully extracting the syntactic
relations between the clauses.
that are more entrenched (i.e., observed more fre-
quently). The likelihood P (F |k) is estimated based
on the values of features in F and the frames in k:
P (F |k) =
?
i?frameFeatures
Pi(j|k) (3)
where i refers to the ith feature of F and j refers
to its value. The conditional probability of a feature
i to have the value j in construction k, Pi(j|k), is
calculated with a smoothed version of:
Pi(j|k) =
counti(j, k)
nk
(4)
where counti(j, k) reflects the number of times fea-
ture i has the value j in construction k, and nk is the
number of frames in k. We have two types of fea-
tures: single-valued and set-valued. The result of the
counti operator for a single-valued feature is based
on exact match to the value j, while the result for a
set-valued feature is based on the degree of overlap
between the compared sets, as in the original model.
3.3 Modeling Developmental Changes in
Attending to Mental Content
We extend the model above to account for the in-
crease in the ability to attend to cues associated with
MSVs, as observed by PCG. In addition, we pro-
pose that children?s representation of this situation
includes the observed syntax of the MSV. That is,
children do not simply ignore the MSV usage, focus-
ing only on the action expressed in its complement
? they must also note that this action semantics oc-
curs in the context of an SC usage.
To adapt the model in these ways, we change
the frame extraction component to allow two pos-
sible interpretations for a mental event input. First,
to reflect PCG?s proposal, we incorporate a mecha-
nism into the model?s frame-extraction process that
takes into account the probability of attending to
mental content. Specifically, we assume that when
presented with an input pair containing an MSV,
as in Table 1, a learner attends to the perceptu-
ally salient action/state expressed in the comple-
ment (here Go) with probability p, and to the non-
perceptually salient mental event expressed in the
main verb (here Think) with probability 1? p. This
probability p is a function over time, correspond-
ing to the observed developmental progression. At
very early stages, p will be high (close to 1), sim-
ulating the much greater saliency of physical ac-
tions compared to mental events for younger chil-
dren. With subsequent input, p will decrease, giv-
ing more and more attention to the mental content
of a scene with a mental event, gradually approach-
ing adult-like abilities.
4
We adopt the following function for p:
p =
1
? ? t+ 1
, 0 < ?  1 (5)
where t is the current time, expressed as the total
number of scene?utterance pairs observed thus far
by the model, and the parameter ? is set to a small
value to assign a high probability to the physical ac-
tion interpretation of the scene in the initial stages of
learning (when t is small).
We must specify the precise make-up of the
frames that correspond to the two possible inter-
pretations considered with probability p and 1 ? p.
PCG state only that children and adults differen-
tially attend to the action vs. mental content of the
scene. We operationalize this by forming two pos-
sible frames in response to an MSV usage. We pro-
pose that one of the frames (with probability 1?p) is
the interpretation of the mental content usage, as in
Table 2(a). However, we extend the account of PCG
by proposing that the other frame considered is not
simply a standard representation of an action scene?
utterance pair. Rather, we suggest that the interpre-
tation of an MSV scene?utterance pair that focuses
on the action semantics does so within the context of
the SC syntax, given the assumed stage of linguistic
abilities of the learner. This leads to the frame (with
probability p) as in Table 2(b), which represents the
action semantics within a two-verb construction as-
sociated with the SC syntax.
4 Experimental Setup
4.1 Input Data
We generate artificial corpora for our simulations,
since we do not have access to sufficient data of ac-
tual utterances paired with scene representations. In
order to create naturalistic data that resembles what
children are exposed to, we follow the approach of
Alishahi and Stevenson (2008) to build an input-
generation lexicon that has the distributional prop-
erties of actual child-directed speech (CDS). Their
original lexicon contains only high-frequency phys-
ical action verbs that appear in limited syntactic pat-
terns. Our expanded lexicon also includes mental
state, perception, and communication verbs, all of
which can appear with SCs.
We extracted our verbs and their distributional
properties from the child-directed speech of 8
children in the CHILDES database (MacWhinney,
2000).2 We selected 28 verbs from different se-
mantic classes and different frequency ranges: 12
physical action verbs taken from the original model
(come, go, fall, eat, play, get, give, take, make, look,
put, sit), 6 perception and communication verbs
(see, hear, watch, say, tell, ask), 5 belief verbs (think,
know, guess, bet, believe), and 5 desire verbs (want,
wish, like, mind, need). For each verb, we manually
analyzed a random sample of 100 CDS usages (or
all usages if fewer than 100) to extract distributional
information about its argument structures.
We construct the input-generation lexicon by list-
ing each of the 28 verbs (i.e. the ?main predicate?),
along with its overall frequency, as well as the fre-
quency with which it appears with each argument
structure. Each entry contains values of the syn-
tactic and semantic features (see Table 2 for ex-
amples), including ?event primitives?, ?event partic-
ipants?, ?syntactic pattern?, and ?verb count?. By
including these features, we assume that a learner
is capable of understanding basic syntactic proper-
ties of an utterance, including word syntactic cat-
egories (e.g., noun and verb), word order, and the
appearance of SCs (e.g., Nelson et al, 1989). We
also assume that a learner has the ability to perceive
and conceptualize the general semantic properties
of events ? including mental, perceptual, commu-
nicative, and physical actions ? as well as those
of the event participants. Values for the semantic
features (the event primitives and event participants)
are taken from Alishahi and Stevenson (2008) for
the action verbs, and from several sources including
VerbNet (Kipper et al, 2008) and Dowty (1991) for
the additional verbs.
For each simulation in our experiments (explained
below), we use the input-generation lexicon to
automatically generate an input corpus of scene?
utterance pairs that reflects the observed frequency
distribution in CDS.3 For an input utterance that
contains an MSV, we randomly pick one of the ac-
tion verbs as the verb appearing within the sentential
complement (the ?other predicate?).
2Corpora of Brown (1973); Suppes (1974); Kuczaj (1977);
Bloom et al (1974); Sachs (1983); Lieven et al (2009).
3The model does not use the input-generation lexicon in
learning.
5
4.2 Setup of Simulations
We perform simulations by training the model on
a randomly generated input corpus, and examin-
ing changes in its performance over time with pe-
riodic tests. Specifically, we perform simulations of
the verb identification task in the human simulation
paradigm as follows: At each test point, we present
the model with a partial test frame with missing
predicate (verb) values, and different amounts of in-
formation for the other features. The tests corre-
spond to the scenarios in the original experiments of
PCG, where each scenario is represented by a partial
frame as follows:
1. scene-only scenario: Corresponds to subjects
watching a silent video depicting either an Ac-
tion or a Belief scene. Our test frame includes
values for the semantic features (event primi-
tives and event participants) corresponding to
the scene type, but no syntactic features.
2. syntax-only scenario: Corresponds to subjects
hearing either an SC or a non-SC utterance.
The test frame includes the corresponding syn-
tactic pattern and verb count of the utterance
type heard, but no semantic features.
3. syntax & scene scenario: Corresponds to sub-
jects watching a silent video (with Action or
Belief content), and hearing an associated (non-
SC or SC) utterance. The test frame includes all
the relevant syntactic and semantic features.
We perform 100 simulations, each on 15000
randomly-generated training frames, and examine
the type of verbs that the model predicts in response
to test frames for the three scenarios. For each
scenario and each simulation, we generate a test
frame by including the relevant feature values of a
randomly-selected physical action or belief verb us-
age from the input-generation lexicon.
PCG code the individual verb responses of their
human subjects into various verb classes. To analo-
gously code our model?s response to each test frame,
we estimate the likelihood of each of two verb
groups, Belief and Action,4 by summing over the
4The Action verbs include action, communication, and per-
ception verbs, as in PCG. Verbs from the desire group are not
considered here, also as in PCG.
Figure 1: Likelihood of Belief verb prediction given Ac-
tion or Belief input.
likelihood of all the verbs in that group. In the re-
sults below, these likelihood scores are averaged for
each test point over the 100 simulations.
When our model is presented with a test frame
containing a Belief scene, we assume that the model
(like a language learner) may not attend to the men-
tal content, resulting in one of the two interpreta-
tions described in Section 3.3 (see Table 2). We thus
calculate the verb class likelihoods using a weighted
average of the verbs predicted under the two inter-
pretations. Following PCG, we test our model with
two types of Belief scenes: True Belief and False
Belief, with the latter having a higher level of be-
lief saliency. We model the difference between these
two scene types as a difference in the probabilities
of perceiving the two interpretations, with a higher
probability for the belief interpretation given a False
Belief test frame. In the experiments presented here,
we set this probability to 80% for False Belief, and
to 60% (just above chance) for True Belief. (Un-
like in training, where we assume a change over time
in the probability of a belief interpretation, for each
presentation of the test frame we use the same prob-
abilities of the two interpretations.)
5 Experimental Results
We present two sets of results: In Section 5.1, we
examine the role of syntactic and semantic cues in
MSV identification, by comparing the likelihoods
of the model?s Belief verb predictions across the
three scenarios. Here we test the model after pro-
cessing 15000 input frames, simulating an adult-like
behaviour (as in PCG). At this stage, we present
the model with an Action test frame (Action scene
and/or Transitive syntax), or a Belief test frame
6
(False Belief scene and/or SC syntax). In Sec-
tion 5.2, we look into the role of semantic cues
that enhance belief saliency, by comparing the like-
lihoods of Belief vs. Action verb predictions in the
syntax & scene scenario. The test frames depict ei-
ther a True Belief or a False Belief scene, paired with
an SC utterance. Here, we test our model periodi-
cally to examine the developmental pattern of MSV
identification, comparing our results with the differ-
ence in the behaviour of children and adults in PCG.
5.1 Linguistic Cues for Belief Verb Prediction
The left side of Figure 1 presents the results of PCG
(for adult subjects); the right side shows the likeli-
hood of Belief verb prediction by our model. Simi-
lar to the results of PCG, our model?s likelihood of
Belief verb prediction is extremely low when given
an Action test frame (Action scene and/or Transi-
tive syntax), whereas it is much higher when the
model is presented with a Belief test frame (False
Belief scene and/or SC syntax). Moreover, as in
PCG, when the model is tested with Belief content,
the lowest likelihood is for the scene-only scenario
and the highest is for the syntax & scene scenario.
PCG found, somewhat surprisingly, that the
syntax-only scenario was more informative for MSV
prediction than the scene-only scenario. Our results
replicate this finding, which we believe is due to the
way our Bayesian clustering groups verb usages to-
gether. Non-SC usages of MSVs are often grouped
with action verbs that frequently appear with non-
SC syntax, and this results in constructions with
mixed (action and belief) semantics. When using
MSV semantic features to make the verb predic-
tion, the action verbs get a higher likelihood based
on such mixed constructions. However, the frequent
usage of MSVs with SC results in entrenched con-
structions of mostly MSVs. Although other verbs,
such as see and say, may also be used with SC syn-
tax, they are grouped with verbs such as watch and
tell into constructions with mixed (SC and non-SC)
syntax. When given SC syntax in verb prediction,
the more coherent MSV constructions result in a
high likelihood of predicting Belief verbs.
5.2 Belief Saliency in Verb Prediction
Figure 2(a) shows the PCG results, for children
and adults, and for True Belief and False Belief.
(a)
(b)
(c)
Figure 2: Verb class likelihood: (a) PCG results for
adults and children (aged 3;7?5;9); (b) Model?s results
given True Belief; (c) Model?s results given False Belief.
Figures 2(b) and (c) present the likelihoods of the
model?s Belief vs. Action verb prediction, over time,
for True and False Belief situations (True/False Be-
lief scene and SC syntax), respectively. We first
compare the responses of our model at the final stage
of training to those of adults in PCG. At this stage,
the model?s verb predictions (for both True and False
Belief) follow a similar trend to that of adult sub-
jects in PCG. The likelihood of Belief verbs is much
higher than the likelihood of Action verbs given a
False Belief situation. Moreover, the likelihood of
Belief verbs is higher given a False Belief situation,
compared to a True Belief situation.
Next, we compare the developmental pattern of
Belief/Action verb predictions in the model with the
difference in behaviour of children and adults in
PCG. We focus on the model?s responses after pro-
7
cessing about 3000 input pairs, as it corresponds to
the trends observed for the children in PCG. At this
stage, the likelihood of Belief verbs is lower than
that of Action verbs for the True Belief situation,
but the pattern is reversed for False Belief; a pattern
similar to children?s behaviour in PCG (see Figure
2(a)). As in PCG, the likelihood of Belief verb pre-
dictions in our model is higher than that of Action
verbs for the False Belief situation, in both ?child?
and ?adult? stages, with a larger difference as the
model ?ages? (i.e., processes more input). For the
True Belief situation also the pattern is similar to
that of PCG: Belief verbs are less likely than Action
verbs to be predicted at early stages, but as the model
receives more input, the likelihood of Belief verbs
becomes slightly higher than that of Action verbs.
PCG?s hypothesis of greater attention to the action
content of a scene implicitly implies that children
focus on the action semantics and syntax of the em-
bedded SC of a Belief verb. We have suggested in-
stead that the focus is on the action semantics within
the context of the SC syntax of the MSV. To directly
evaluate the necessity of our latter assumption, we
performed a simulation using both action syntax and
semantics to represent the physical interpretation of
the belief scene. Specifically, the syntactic features
in this representation were non-SC structure with
only one verb. Based on these settings, the model
predicted high likelihood for the Belief verbs from a
very early stage, not showing the same delayed ac-
quisition pattern exhibited by PCG?s results. This
result suggests that the SC syntax plays an impor-
tant role in MSV acquisition.
6 Discussion
Various studies have considered why mental state
verbs (MSVs) appear relatively late in children?s
productions (e.g., Shatz et al, 1983; Bartsch and
Wellman, 1995). The Human Simulation Paradigm
has revealed that adult participants tend to focus on
the physical action cues of a scene (Gleitman et al,
2005). PCG?s results further show that cues empha-
sizing mental content lead to a significant increase
in MSV responses in such tasks. Moreover, they
show that a sentential complement (SC) structure is
a stronger cue to an MSV than the semantic cues
emphasizing mental content.
In this paper we adapt a computational Bayesian
model to analyze such semantic and syntactic cues
in the ability of children to identify them. We sim-
ulate an attentional mechanism of the growing sen-
sitivity to mental content in a scene into the model.
We show that both the ability to observe the obscure
mental content and the ability to recognize the use of
an SC structure are essential to replicate PCG?s ob-
servations. Moreover, our results predict the strong
association of MSVs to the SC syntax, for the first
time (to our knowledge) in a computational model.
Children often use verbs other than MSVs in ex-
perimental settings in which MSVs would be the ap-
propriate or correct verb choice (Asplin, 2002; Kidd
et al, 2006; Papafragou et al, 2007). Our model
presents similar variability in verb choice. One un-
derlying cause of this behaviour in the model is its
association of action semantics to SC syntax, due to
the tendency to observe the physical cues in a scene
associated with an utterance using an MSV with an
SC. Preliminary results (not reported here) imply
that the association of perception and communica-
tion verbs that frequently appear with SC contribute
to this pattern of verb choice (see de Villiers, 2005,
for theoretical support). Our results require further
work to fully understand this behaviour.
Finally, our model will facilitate future work in re-
gards to the performative usage of MSVs, in which
MSVs do not indicate mental content, but rather di-
rect the conversation. Several studies (e.g., Diessel
and Tomasello, 2001; Howard et al, 2008), have re-
ferred to the role performative use likely plays in
MSV acquisition, since the first MSV usages by
children are performative. The semantic properties
MSVs take in performative usages is not currently
represented in our lexicon. However, the physical
interpretation of the mental scene that we have used
in our experiments here is similar to the performa-
tive usage: i.e., the main perceived action and the
observed syntactic structure are the same. At the
moment, our results imply that the association of
MSVs with their genuine mental meaning is delayed
by interpretations of the mental scene which over-
look the mental content. In the future, we aim to in-
corporate the semantic representation of performa-
tive usages to better analyze their effect on MSV ac-
quisition.
8
References
Afra Alishahi and Pirita Pyykkon?en. 2011. The on-
set of syntactic bootstrapping in word learning:
Evidence from a computational study. In Pro-
ceedings of the 33st Annual Conference of the
Cognitive Science Society.
Afra Alishahi and Suzanne Stevenson. 2008. A com-
putational model of early argument structure ac-
quisition. Cognitive Science, 32(5):789?834.
Kristen N. Asplin. 2002. Can complement frames
help children learn the meaning of abstract
verbs? Ph.D. thesis, UMass Amherst.
Karen Bartsch and Henry M. Wellman. 1995. Chil-
dren talk about the mind.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development: If,
when, and why. Cognitive Psychology, 6(3):380?
420.
Roger Brown. 1973. A first language: The early
stages. Harvard U. Press.
Paula J. Buttery. 2006. Computational models
for first language acquisition. Technical Report
UCAM-CL-TR-675, University of Cambridge,
Computer Laboratory.
Jill G. de Villiers. 2005. Can language acquisition
give children a point of view. In Why Language
Matters for Theory of Mind, pages 199?232. Ox-
ford University Press.
Nicole Dehe and Anne Wichmann. 2010. Sentence-
initial I think (that) and i believe (that): Prosodic
evidence for use as main clause, comment clause
and dicourse marker. Stuides in Language,
34(1):36?74.
Holger Diessel and Michael Tomasello. 2001. The
acquisition of finite complement clauses in en-
glish: A corpus-based analysis. Cognitive Lin-
guistics, 12(2):97?142.
David Dowty. 1991. Thematic Proto-Roles and Ar-
gument Selection. Language, 67(3):547?619.
Jane Gillette, Lila Gleitman, Henry Gleitman, and
Anne Lederer. 1999. Human simulations of lexi-
cal acquisition. Cognition, 73(2):135?176.
Lila R. Gleitman, Kimberly Cassidy, Rebecca
Nappa, Anna Papafragou, and John C. Trueswell.
2005. Hard words. Language Learning and De-
velopment, 1(1):23?64.
Alison Gopnik and Andrew N. Meltzoff. 1997.
Words, thoughts, and theories.
Alice A. Howard, Lara Mayeux, and Letitia R.
Naigles. 2008. Conversational correlates of chil-
dren?s acquisition of mental verbs and a theory of
mind. First Language, 28(4):375.
Carl Nils Johnson and Henry M. Wellman. 1980.
Children?s developing understanding of mental
verbs: Remember, know, and guess. Child De-
velopment, 51(4):1095?1102.
Evan Kidd, Elena Lieven, and Michael Tomasello.
2006. Examining the role of lexical frequency in
the acquisition and processing of sentential com-
plements. Cognitive Development, 21(2):93?107.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Eval-
uation, 42(1):21?40?40.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
Elena Lieven, Dorothe? Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
Deborah G. Kemler Nelson, Kathy Hirsh-Pasek, Pe-
ter W. Jusczyk, and Kimberly Wright Cassidy.
1989. How the prosodic cues in motherese might
assist language learning. Journal of child Lan-
guage, 16(1):55?68.
Sourabh Niyogi. 2002. Bayesian learning at the
syntax-semantics interface. In Proceedings of the
24th Annual Conference of the Cognitive Science
Society.
Anna Papafragou, Kimberly Cassidy, and Lila Gleit-
man. 2007. When we think about thinking:
The acquisition of belief verbs. Cognition,
105(1):125?165.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
9
learned verb classes. In Proceedings of the 33rd
Annual Meeting of the Cognitive Science Society.
Amy Perfors, Joshua B. Tenenbaum, and Elizabeth
Wonnacott. 2010. Variability, negative evidence,
and the acquisition of verb argument construc-
tions. Journal of Child Language, 37(03):607?
642.
Willard .V.O. Quine. 1960. Word and object, vol-
ume 4. The MIT Press.
Jacqueline Sachs. 1983. Talking about the there and
then: The emergence of displaced reference in
parent-child discourse. Children?s Language, 4.
Marilyn Shatz, Henry M. Wellman, and Sharon Sil-
ber. 1983. The acquisition of mental verbs: A
systematic investigation of the first reference to
mental state. Cognition, 14(3):301?321.
Patrick Suppes. 1974. The semantics of children?s
language. American Psychologist, 29(2):103.
10
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 231?240,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Acquisition of Desires before Beliefs: A Computational Investigation
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
The acquisition of Belief verbs lags be-
hind the acquisition of Desire verbs in
children. Some psycholinguistic theo-
ries attribute this lag to conceptual differ-
ences between the two classes, while oth-
ers suggest that syntactic differences are
responsible. Through computational ex-
periments, we show that a probabilistic
verb learning model exhibits the pattern of
acquisition, even though there is no dif-
ference in the model in the difficulty of
the semantic or syntactic properties of Be-
lief vs. Desire verbs. Our results point
to the distributional properties of various
verb classes as a potentially important, and
heretofore unexplored, factor in the ob-
served developmental lag of Belief verbs.
1 Introduction
Psycholinguistic studies have shown great inter-
est in the learning of Mental State Verbs (MSVs),
such as think and want, given the various cogni-
tive and linguistic challenges in their acquisition.
MSVs refer to an entity?s inner states, such as
thoughts and wishes, which the language learner
must be able to perceive and conceptualize appro-
priately. Moreover, such verbs often appear in a
Sentential Complement (SC) construction, which
is complex for children because of the embedded
clause.
Despite some shared properties, MSVs are
a heterogeneous group, with different types of
verbs exhibiting different developmental patterns.
Specifically, a wealth of research shows that chil-
dren produce Desire verbs, such as want and
wish, earlier than Belief verbs, such as think and
know (Shatz et al, 1983; Bartsch and Wellman,
1995; Asplin, 2002; Perner et al, 2003; de Vil-
liers, 2005; Papafragou et al, 2007; Pascual et al,
2008). Some explanations for this pattern posit
that differences in the syntactic usages of Desire
and Belief verbs underlie the observed develop-
mental lag of the latter (de Villiers, 2005; Pas-
cual et al, 2008). In particular, Desire verbs oc-
cur mostly with an infinitival SC (as in I want
(her) to leave), while Belief verbs occur mostly
with a finite SC (a full tensed embedded clause,
as in I think (that) she left). Notably, infiniti-
vals appear earlier than finite SCs in the speech
of young children (Bloom et al, 1984, 1989).
Others suggest that Desire verbs are conceptu-
ally simpler (Bartsch and Wellman, 1995) or prag-
matically/communicatively more salient (Perner,
1988; Fodor, 1992; Perner et al, 2003). Propo-
nents of the conceptual and pragmatic accounts ar-
gue that syntax alone cannot explain the delay in
the acquisition of Belief verbs, because children
use finite SCs with verbs of Communication (e.g.,
say) and Perception (e.g., see) long before they
use them with Belief verbs (Bartsch and Wellman,
1995).
We use a computational model of verb argu-
ment structure acquisition to shed light on the fac-
tors that might be responsible for the developmen-
tal gap between Desire and Belief verbs. Impor-
tantly, our model exhibits the observed pattern of
learning Desire before Belief verbs, without hav-
ing to encode any differences in difficulty between
the two classes in terms of their syntactic or con-
ceptual/pragmatic requirements. The behaviour of
the model can thus be attributed to its probabilistic
learning mechanisms in conjunction with the dis-
tributional properties of the input. In particular, we
investigate how the model?s learning mechanism
interacts with the distributions of several classes
of verbs ? including Belief, Desire, Perception,
Communication, and Action ? in the finite and
infinitival SC syntax to produce the observed pat-
tern of acquisition of Desire and Belief verbs. Us-
ing a computational model can reveal the poten-
231
tial effects of interactions of verb classes in hu-
man language acquisition which would be difficult
to investigate experimentally. Our results suggest
that the distributional properties of relevant verb
classes are a potentially important, and heretofore
unexplored, factor in experimental studies of the
developmental lag of Belief verbs.
2 The Computational Model
We require an incremental model in which we
can examine developmental patterns as it gradu-
ally learns relevant aspects of argument structures.
This task calls for an ability to represent the se-
mantic and syntactic properties of verb usages, in-
cluding those containing MSVs and other kinds of
verbs taking sentential complements (SCs). Most
computational models of verb argument structure
acquisition have largely focused on physical ac-
tion verbs (Alishahi and Stevenson, 2008; Chang,
2009; Perfors et al, 2010; Parisien and Steven-
son, 2011). Recently, Barak et al (2012) ex-
tended the incremental Bayesian model of Al-
ishahi and Stevenson (2008) to include the syntac-
tic and semantic features required for the process-
ing of MSVs and other verbs that take SCs. While
Barak et al (2012) modeled some developmental
patterns of MSVs overall, their work did not ac-
count for the difference between Desire and Be-
lief verbs. In this section, we present their model,
which we adopt for our experiments. In Section 3,
we describe how we modify the representation of
the input in Barak et al (2012) to enable our inves-
tigation of the differences among the MSV classes.
2.1 Overview of the Model
The input to the Barak et al (2012) model is a
sequence of frames, where each frame is a col-
lection of syntactic and semantic features repre-
senting what the learner might extract from an ut-
terance s/he has heard paired with a scene s/he
has perceived. In particular, we consider syntactic
properties, including syntactic pattern, argument
count, and complement type, as well as seman-
tic properties, including event primitives and event
participants. Table 1 presents a sample frame il-
lustrating possible values for these features.
The model incrementally groups the input
frames into clusters that reflect probabilistic as-
sociations of the syntactic and semantic features
across similar verb usages. Each learned cluster
is a probabilistic (and possibly noisy) representa-
head predicate think
other predicate make
Syntactic Features:
syntactic pattern arg1 verb arg2 verb arg3
argument count 3
complement type SC-fin
Semantic Features:
event primitives { state, consider , cogitate, action }
event participants { experiencer , perceiver , considerer}
{ agent , animate}
{ theme, changed}
Table 1: An example input frame. The Syntactic features
reflect an utterance such as He thinks Mom made pancakes:
i.e., syntactic pattern ?arg1 verb arg2 verb arg3?, 3 arguments,
and finite SC. The Semantic features reflect a corresponding
conceptualized belief event with a physical action described
in the SC ({state, consider , cogitate, action}) whose
?arg1? participant ({experiencer , perceiver , considerer})
perceives the ?arg2? ({agent , animate}) acting on the ?arg3?
({theme, changed}).
tion of an argument structure construction: e.g.,
a cluster containing frames corresponding to us-
ages such as I eat apples, She took the ball, and
He got a book, etc., represents a Transitive Action
construction.1 Note that a cluster operates as more
than simply a set of similar frames: The model
can use the probabilistic associations among the
various features of the frames in a cluster to gen-
eralize over the individual verb usages that it has
seen. For example, if the model is presented with a
frame corresponding to a transitive utterance using
a verb it has not observed before, such as She gor-
ped the ball, the example cluster above would lead
the model to predict that gorp has semantic event
primitives in common with other Action verbs like
eat, take, and get. Such probabilistic reasoning is
especially powerful because clusters involve com-
plex interactions of features, and the model rea-
sons across all such clusters to make suitable gen-
eralizations over its learned knowledge.
2.2 Algorithm for Learning Clusters
The model groups input frames into clusters on
the basis of the overall similarity in the values of
their syntactic and semantic features. Importantly,
the model learns these clusters incrementally; the
number and type of clusters is not predetermined.
The model considers the creation of a new cluster
for a given frame if the frame is not sufficiently
similar to any of the existing clusters. Formally,
the model finds the best cluster for a given input
1Note that, because the associations are probabilistic, a
construction may be represented by more than one cluster.
232
frame F as in:
BestCluster(F ) = argmax
k?Clusters
P (k|F ) (1)
where k ranges over all existing clusters and a new
one. Using Bayes rule:
P (k|F ) = P (k)P (F |k)P (F ) ? P (k)P (F |k) (2)
The prior probability of a cluster P (k) is estimated
as the proportion of frames that are in k out of
all observed input frames, thus assigning a higher
prior to larger clusters, representing more frequent
constructions. The likelihood P (F |k) is estimated
based on the match of feature values in F and in
the frames of k (assuming independence of the
features):
P (F |k) =
?
i?Features
Pi(j|k) (3)
where i refers to the ith feature of F and j refers
to its value, and Pi(j|k) is calculated using a
smoothed version of:
Pi(j|k) =
counti(j, k)
nk
(4)
where counti(j, k) is the number of times feature
i has the value j in cluster k, and nk is the number
of frames in k.
2.3 Attention to Mental Content
One factor proposed to play an important role in
the acquisition of MSVs is the difficulty children
have in being aware of (or perceiving the salience
of) the mental content of a scene that an utterance
may be describing (Papafragou et al, 2007). This
difficulty arises because the aspects of a scene as-
sociated with an MSV ? the ?believing? or the
?wanting? ? are not directly observable, as they
involve the inner states of an event participant. In-
stead, younger children tend to focus on the phys-
ical (observable) parts of the scene, which gener-
ally correspond to the event described in the em-
bedded clause of an MSV utterance. For instance,
young children may focus on the ?making? action
in He thinks Mom made pancakes, rather than on
the ?thinking?.
A key component of the model of Barak
et al (2012) is a mechanism that simulates the
gradually-developing ability in children to attend
to the mental content rather than solely to the (em-
bedded) physical action. This mechanism basi-
cally entails that the model may ?misinterpret? an
input frame containing an MSV as focusing on the
semantics of the action in the sentential comple-
ment. Specifically, when receiving an input frame
with an MSV, as in Table 1, there is a probability p
that the frame is perceived with attention to the se-
mantics corresponding to the physical action verb
(here, make). In this case, the model correctly in-
cludes the syntactic features as in Table 1, on the
assumption that the child can accurately note the
number and pattern of arguments. However, the
model replaces the semantic features with those
that correspond to the physical action event and its
participants. At very early stages, p is very high
(close to 1), simulating the much greater saliency
of physical actions compared to mental events for
younger children. As the model ?ages? (i.e., re-
ceives more input), p decreases, giving more and
more attention to the mental content, gradually ap-
proaching adult-like abilities.
3 Experimental Setup
3.1 Generation of the Input Corpora
Because there are no readily available large cor-
pora of actual child-directed speech (CDS) associ-
ated with appropriate semantic representations, we
generate artificial corpora for our simulations that
mimic the relevant syntactic properties of CDS
along with automatically-produced semantic prop-
erties. Importantly, these artificial corpora have
the distributional properties of the argument struc-
tures for the verbs under investigation based on
an analysis of verb usages in CDS. To accomplish
this, we adopt and extend the input-generation lex-
icon of Barak et al (2012), which is used to au-
tomatically generate the syntactic and semantic
features of the frames that serve as input to the
model. Using this lexicon, each simulation cor-
pus is created through a probabilistic generation of
argument structure frames according to their rela-
tive frequencies of occurrence in CDS. Since the
corpora are probabilistically generated, all exper-
imental results are averaged over simulations on
100 different input corpora, to ensure the results
are not dependent on idiosyncratic properties of a
single generated corpus.
Our input-generation lexicon contains 31 verbs
from various semantic classes and different fre-
quency ranges; these verbs appear in a variety
233
Semantic Verb Frequency % Relative
class frequency with
SC-fin SC-inf
Belief think 13829 100 -
bet 391 100 -
guess 278 76 -
know 7189 61 -
believe 78 21 -
Desire wish 132 94 -
hope 290 86 -
want 8425 - 76
like 6944 - 51
need 1690 - 60
Communication tell 2953 64 -
say 8622 60 -
ask 818 29 10
speak 62 - -
talk 1322 - -
Perception hear 1370 21 25
see 9717 14 -
look 5856 9 -
watch 1045 - 27
listen 413 33 2
Action go 20364 - 5
get 16493 - 14
make 4165 - 10
put 8794 - -
come 6083 - -
eat 3894 - -
take 3239 - -
play 2565 - -
sit 2462 - -
give 2341 - -
fall 1555 - -
Table 2: The list of our 31 verbs from the five semantic
classes, along with their overall frequency, and their rela-
tive frequency with the finite SC (SC-fin) or the infinitival
SC (SC-inf).
of syntactic patterns including the sentential com-
plement (SC) construction. Our focus here is on
learning the Belief and Desire classes; however,
we include verbs from other classes to have a re-
alistic context of MSV acquisition in the presence
of other types of verbs. In particular, we include
(physical) Action verbs because of their frequent
usage in CDS, and we include Communication
and Perception groups because of their suggested
role in the acquisition of MSVs (Bloom et al,
1989; de Villiers, 2005). Table 2 lists the verbs of
each semantic class, along with their overall fre-
quency and their relative frequency with the finite
(SC-fin) and infinitival SC (SC-inf) in our data.
For each of these 31 verbs, the distributional in-
formation about its argument structure was manu-
ally extracted from a random sample of 100 CDS
usages (or all usages if fewer than 100) from eight
corpora from CHILDES (MacWhinney, 2000).2
The input-generation lexicon then contains the
overall frequency of each verb, as well as the rela-
tive frequency with which it appears with each of
its argument structures. Each argument structure
entry for a verb also contains the values for all the
syntactic and semantic features in a frame (see Ta-
ble 1 for an example), which are determined from
the manual inspection of the usages.
The values for syntactic features are based on
simple observation of the order and number of
verbs and arguments in the usage, and, if an ar-
gument is an SC, whether it is finite or infiniti-
val. We add this latter feature (the type of the
SC) to the syntactic representation used by Barak
et al (2012) to allow distinguishing the syntac-
tic properties associated with Desire and Belief
verbs. Note that this feature does not incorporate
any potential level of difficulty in processing an
infinitival vs. finite SC; the feature simply records
that there are three different types of embedded ar-
guments: SC-inf, SC-fin, or none. Thus, while
Desire and Belief verbs that typically occur with
an SC-inf or SC-fin have a distinguishing feature,
there is nothing in this representation that makes
Desire verbs inherently easier to process. This
syntactic representation reflects our assumptions
that a learner: (i) understands basic syntactic prop-
erties of an utterance, such as syntactic categories
(e.g., noun and verb) and word order; and (ii) dis-
tinguishes between a finite complement, as in He
thinks that Mom left, and an infinitival, as in He
wants Mom to leave.
The values for the semantic features of a verb
and its arguments are based on a simple taxonomy
of event and participant role properties adapted
from several resources, including Alishahi and
Stevenson (2008), Kipper et al (2008), and Dowty
(1991). In particular, we assume that the learner is
able to perceive and conceptualize the general se-
mantic properties of different kinds of events (e.g.,
state and action), as well as those of the event par-
ticipants (e.g., agent, experiencer, and theme). In
an adaptation of the lexicon of Barak et al, we
make minimal assumptions about shared seman-
tics across verb classes. Specifically, to encode
suitable semantic distinctions among MSVs, and
between MSVs and other verbs, we aimed for a
representation that would capture reasonable as-
2Brown (1973); Suppes (1974); Kuczaj (1977); Bloom
et al (1974); Sachs (1983); Lieven et al (2009).
234
sumptions about high-level similarities and differ-
ences among the verb classes. As with the syn-
tactic features, we ensured that we did not simply
encode the result we are investigating (that chil-
dren have facility with Desire verbs before Be-
lief verbs) by making the representation for Desire
verbs easier to learn.
In the results presented in Section 4, ?our
model? refers to the computational model of Barak
et al (2012) together with our modifications to the
input representation.
3.2 Simulations and Verb Prediction
Psycholinguistic studies have used variations of
a novel verb prediction task to examine how
strongly children (or adults) have learned to asso-
ciate the various syntactic and semantic properties
of a typical MSV usage. In particular, the typical
Desire verb usage combines desire semantics with
an infinitival SC syntax, while the typical Belief
verb usage combines belief semantics with a finite
SC syntax. In investigating the salience of these
associations in human experiments, participants
are presented with an utterance containing a nonce
verb with an SC (e.g., He gorped that his grand-
mother was in the bed), sometimes paired with a
corresponding scene representing a mental event
(e.g., a picture or a silent video depicting a think-
ing event with heightened saliency). An experi-
menter then asks each participant what the nonce
verb (gorp) ?means? ? i.e., what existing English
verb does it correspond to (see, e.g., Asplin, 2002;
Papafragou et al, 2007). The expectation is that,
e.g., if a participant has a well-entrenched Belief
construction, then they should have a strong as-
sociation between the finite-SC syntax and belief
semantics, and hence should produce more Belief
verbs as the meaning of a novel verb in an finite-
SC utterance (and analogously for infinitival SCs
and Desire verbs).
We perform simulations that are based on such
psycholinguistic experiments. After training the
model on some number of input frames, we then
present it with a test frame in which the main verb
(head predicate) is replaced by a nonce verb like
gorp (a verb that doesn?t occur in our lexicon).
Analogously to the human experiments, in order
to study the differences in the strength of associ-
ation between the syntax and semantics of Desire
and Belief verbs, we present the model with two
types of test frames: (i) a typical desire test frame,
with syntactic features corresponding to the infini-
tival SC syntax, optionally paired (depending on
the experiment) with semantic features associated
with a Desire verb in our lexicon; and (ii) a typi-
cal belief test frame, with syntactic features corre-
sponding to the finite SC syntax, optionally paired
with semantic features from a Belief verb.3
Given a test frame Ftest, we use the clusters
learned by the model to calculate the likelihood of
each of the 31 verbs v as the response of the model
indicating the meaning of the novel verb, as in:
P (v|Ftest) (5)
=
?
k?Clusters
Phead(v|k)P (k|Ftest)
?
?
k?Clusters
Phead(v|k)P (Ftest|k)P (k)
where Phead(v|k) is the probability of the head
feature having the value v in cluster k, calculated
as in Eqn. (4); P (Ftest|k) is the probability of the
test frame Ftest given cluster k, calculated as in
Eqn. (3); and P (k) is the prior probability of clus-
ter k, calculated as explained in Section 2.2.
What we really want to know is the likelihood
of the model producing a verb from each of the
semantic classes, rather than the likelihood of any
particular verb. For each test frame, we calculate
the likelihood of each semantic class by summing
the likelihoods of the verbs in that class:
P (Class|Ftest) =
?
vc?Class
P (vc|Ftest)
where vc is one of the verbs in Class, and Class
ranges over the 5 classes in Table 2. We average
the verb class likelihoods across the 100 simula-
tions.
4 Experimental Results
The novel verb prediction experiments described
above have found differences in the performance
of children across the two MSV classes (e.g., As-
plin, 2002; Papafragou et al, 2007). For exam-
ple, children performed better at predicting that a
novel verb is a Desire verb in a typical desire con-
text (infinitival-SC utterance paired with a desire
scene), compared to their performance at identify-
ing a novel verb as a Belief verb in a typical belief
3Table 2 shows that, in our data, Belief verbs occur ex-
clusively with finite clauses in an SC usage. Although Desire
verbs occur in both SC-inf and SC-fin usages, the former out-
number the latter by almost 30 to 1 over all Desire verbs.
235
context (finite-SC utterance accompanied by a be-
lief scene). In Section 4.1, we examine whether
the model exhibits this behaviour in our verb class
prediction task, thereby mimicking children?s lag
in facility with Belief verbs compared to Desire
verbs.
Recall that some researchers attribute the
above-mentioned developmental gap to the con-
ceptual and pragmatic differences between the two
MSV classes, whereas others suggest it is due to a
difference in the syntactic requirements of the two
classes. As noted in Section 3.1, we have tailored
our representation of Desire and Belief verbs to
not build in any differences in the ease or difficulty
of acquiring their syntactic or semantic properties.
Moreover, the possibility in the model for ?misin-
terpretation? of mental content as action semantics
(see Section 2.3) also applies equally to both types
of verbs. Thus, any observed performance gap in
the model reflects an interaction between its pro-
cessing approach and the distributional properties
of CDS. To better understand the role of the in-
put, in Section 4.2 we examine how the distribu-
tional pattern of appearances of various semantic
classes of verbs (including Belief, Desire, Com-
munication, Perception and Action verbs) with the
finite and infinitival SC constructions affects the
learning of the two types of MSVs.
4.1 Verb Prediction Simulations
Here we compare the verb prediction responses of
the participants in the experiments of Papafragou
et al (2007) (PCG), with those of the model when
presented with a novel verb in a typical desire or
belief test frame. (See Section 3.2 for how we con-
struct these frames.) PCG report verb responses
for the novel verb meaning as desire, belief, or ac-
tion, where the latter category contains all other
verb responses. Looking closely at the latter cat-
egory in PCG, we find that most verbs are what
we have termed (physical) Action verbs. We thus
report the verb class likelihoods of the model for
the Belief, Desire, and Action verbs in our lexi-
con. To compare the model?s responses with those
of the children and adults in PCG, we report the
responses of the model to the test frames at two
test points: after training the model with 500 in-
put frames, resembling the ?Child stage?, and after
presenting the model with 10, 000 input frames,
representing the ?Adult stage?.
Figure 1(a) gives the percent verb types from
(a) Human participants in Papafragou et al (2007)
(b) The model
Figure 1: (a) Percent verb types produced by adult and
child participants given a desire or belief utterance and scene.
(b) The model?s verb class likelihoods given a desire or be-
lief test frame. Child stage is represented by 500 input frames
compared to the 10, 000 input frames for Adult stage.
PCG;4 Figure 1(b) presents the results of the
model. Similarly to the children in PCG, the
model at earlier stages of learning (?Child stage?)
is better at predicting Desire verbs for a desire test
frame (.56) than it is at predicting Belief verbs for
a belief test frame (.42) ? cf. 59% Desire vs.
41% Belief prediction for PCG. In addition, as for
both the children and adult participants of PCG,
the model produces more Action verbs in a desire
context than in a belief context at both stages.
We note that although the adult participants of
PCG perform well at identifying both Desire and
Belief verbs, the model does not identify Belief
verbs with the same accuracy as it does Desire
verbs, even after processing 10, 000 input frames
(i.e., the ?Adult stage?). In Section 4.2, we will see
that this is due to the model forming strong asso-
ciations between the Communication and Percep-
tion verbs and the SC-fin usage (the typical syn-
tax of Belief verbs). These associations might be
4Based on results presented in Table 4, Page 149 in Pa-
pafragou et al (2007), for the utterance and scene condition.
236
overly strong in our model because of the limited
number of verbs and verb classes ? an issue we
will need to address in the future. We also note
that, unlike the results of PCG, the model only
rarely produces Desire verbs in a Belief context.
This also may be due to our choice of Desire verbs,
which have extremely few SC-fin usages overall.
To summarize, similarly to children (Asplin,
2002; Papafragou et al, 2007), the model per-
forms better at identifying Desire verbs compared
to Belief verbs. Moreover, we replicate the ex-
perimental results of PCG without encoding any
conceptual or syntactic differences in difficulty be-
tween the two types of verbs. Specifically, because
the representation of Desire and Belief classes in
our experiments does not build in a bias due to the
ease of processing Desire verbs, the differential
results in the model must be due to the interac-
tion of the different distributional patterns in CDS
(see Table 2) and the processing approach of the
model. Although this finding does not rule out the
role of conceptual or syntactic differences between
Desire and Belief verbs in delayed acquisition of
the latter, it points to the importance of the dis-
tributional patterns as a potentially important and
relevant factor worth further study in human ex-
periments. We further investigate this hypothesis
in the following section.
4.2 A Closer Look at the Role of Syntax
The goal of the experiments presented here is to
understand how an interaction among the 5 dif-
ferent semantic classes of verbs, in terms of their
distribution of appearance with the two types of
SC constructions, coupled with the probabilistic
?misinterpretation? of MSVs in the model, might
play a role in the acquisition of Desire before Be-
lief verbs. Because our focus is on the syntactic
properties of the verbs, we present the model with
partial test frames containing a novel verb and syn-
tactic features that correspond to either a finite SC
usage (the typical use of a Belief verb) or an infini-
tival SC usage (the typical use of a Desire verb).5
We refer to the partial test frames as SC-fin or SC-
inf test frames. We test the model periodically,
over the course of 10, 000 input frames, in order
to examine the progression of the verb class like-
5Verb prediction given an isolated utterance has been per-
formed with adult participants (e.g., Gleitman et al, 2005;
Papafragou et al, 2007). Here we simulate the settings of
such experiments, but do not compare our results with the
experimental data, since they have not included children.
(a) Model?s likelihoods given SC-inf test frame
(b) Model?s likelihoods given SC-fin test frame
Figure 2: The model?s verb class likelihoods for the indi-
vidual semantic classes.
lihoods over time.
First, we examine the verb class prediction like-
lihoods, given an SC-inf test frame; see Fig-
ure 2(a). We can see that all through training,
the likelihoods are mainly divided between Desire
and Action verbs, with the Desire likelihood im-
proving over time. Looking at Table 2, we note
that the Desire and Action verbs have the highest
frequency of occurrence with SC-inf (taking into
account both the overall frequency of verbs, and
their relative frequency with SC-inf), contributing
to their strength of association with the infinitival-
SC syntax. Note that the very high likelihood of
Action verbs given an SC-inf test frame, especially
at the earlier stages of training, cannot be solely
due to their occurrence with SC-inf, since these
verbs mostly occur with other syntactic patterns.
Recall that the model incorporates a mechanism
that simulates a higher probability of erroneously
attending to the physical action (as opposed to the
mental event) at earlier stages, simulating what has
been observed in young children (see Section 2.3
for details). We believe that this mechanism is re-
237
sponsible for some of the Action verb responses of
the model for an SC-inf test frame.
Next, we look at the pattern of verb class likeli-
hoods given an SC-fin test frame; see Figure 2(b).
We can see that the likelihoods here are divided
across a larger number of classes ? namely, Ac-
tion, Communication, and Perception ? com-
pared with Figure 2(a) for the SC-inf test frame.
Since Action verbs do not occur in our data with
SC-fin (see Table 2), their likelihood here comes
from the misinterpretation of mental events (ac-
companied with SC-fin) as action. The initially
high likelihoods of Communication and Percep-
tion verbs results from their high frequency of oc-
currence with SC-fin. Because at this stage Belief
verbs are not always correctly associated with SC-
fin due to the high probability of misinterpreting
them as action, we see a lower likelihood of pre-
dicting Belief verbs. Eventually, the model pro-
duces more Belief responses than any other verb
class, since Beliefs have the highest frequency of
occurrence with the finite-SC syntax.
To summarize, our results here confirm our hy-
pothesis that the distributional properties of the
verb classes with the finite and infinitival SC pat-
terns, coupled with the learning mechanisms of
the model, account for the observed developmen-
tal pattern of MSV acquisition in our model.
5 Discussion
We use a computational model of verb argument
structure learning to shed light on the factors that
might underlie the earlier acquisition of Desire
verbs (e.g., wish and want) than Belief verbs (e.g.,
think and know). Although this developmental gap
has been noted by many researchers, there are at
least two competing theories as to what might be
the important factors: differences in the concep-
tual/pragmatic requirements (e.g., Fodor, 1992;
Bartsch and Wellman, 1995; Perner et al, 2003),
or differences in the syntactic properties (e.g., de
Villiers, 2005; Pascual et al, 2008). Using a com-
putational model, we suggest other factors that
may play a role in an explanation of the observed
gap, and should be taken into account in experi-
mental studies on human subjects.
First, we show that the model exhibits a simi-
lar pattern to children, in that it performs better at
predicting Desire verbs compared to Belief verbs,
given a novel verb paired with typical Desire or
Belief syntax and semantics, respectively. This
difference in performance suggests that the model
forms a strong association between the desire se-
mantics and the infinitival-SC syntax ? one that
is formed earlier and is stronger than the associa-
tion it forms between the belief semantics and the
finite-SC syntax. Importantly, the replication of
this behaviour in the model does not require an
explicit encoding of conceptual/pragmatic differ-
ences between Desire and Belief verbs, nor of a
difference between the two types of SC syntax (fi-
nite and infinitival) with respect to their ease of
acquisition. Instead, we find that what is responsi-
ble for the model?s behaviour is the distribution of
the semantic verb classes (Desire, Belief, Percep-
tion, Communication, and Action) with the finite
and infinitival SC syntactic patterns in the input.
Children are also found to produce
semantically-concrete verbs, such as Com-
munication (e.g., say) and Perception verbs (e.g.,
see), with the finite SC before they produce
(more abstract) Belief verbs with the same syntax.
Psycholinguistic theories have different views
on what this observation tells us about the delay
in the acquisition of Belief verbs. For example,
Bartsch and Wellman (1995) suggest that the
earlier production of Communication verbs shows
that even when children have learned the finite-SC
syntax (and use it with more concrete verbs),
they lack the required conceptual development
to talk about the beliefs of others. Our results
suggest a different take on these same findings:
because Communication (and Perception) verbs
also frequently appear with the finite-SC syntax in
the input, the model learns a relatively strong as-
sociation between each of these semantic classes
and the finite SC. This in turn causes a delay in
the formation of a sufficiently-strong association
between the Belief verbs and that same syntax,
compared with the association between the Desire
verbs and the infinitival SC.
de Villiers (2005) suggests that associating
Communication verbs with the finite-SC syntax
has a facilitating effect on the acquisition of Be-
lief verbs. In our model, we observe a competi-
tion between Communication and Belief verbs, in
terms of their association with the finite-SC syn-
tax. To further explore the hypothesis of de Vil-
liers (2005) will require expanding our model with
enriched semantic representations that enable us to
investigate the bootstrapping role of Communica-
tion verbs in the acquisition of Beliefs.
238
References
Afra Alishahi and Suzanne Stevenson. 2008. A
computational model of early argument struc-
ture acquisition. Cognitive Science, 32(5):789?
834.
Kristen N. Asplin. 2002. Can complement frames
help children learn the meaning of abstract
verbs? Ph.D. thesis, UMass Amherst.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2012. Modeling the acquisition of mental
state verbs. NAACL-HLT 2012.
Karen Bartsch and Henry M. Wellman. 1995.
Children talk about the mind. New York: Ox-
ford Univ. Press.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development:
If, when, and why. Cognitive Psychology,
6(3):380?420.
Lois Bloom, Matthew Rispoli, Barbara Gartner,
and Jeremie Hafitz. 1989. Acquisition of com-
plementation. Journal of Child Language,
16(01):101?120.
Lois Bloom, Jo Tackeff, and Margaret Lahey.
1984. Learning to in complement constructions.
Journal of Child Language, 11(02):391?406.
Roger Brown. 1973. A first language: The early
stages. Harvard Univ. Press.
Nancy Chih-Lin Chang. 2009. Constructing
grammar: A computational model of the emer-
gence of early constructions. Ph.D. thesis, Uni-
versity of California, Berkeley.
Jill G. de Villiers. 2005. Can language acquisi-
tion give children a point of view. In Why Lan-
guage Matters for Theory of Mind, pages 199?
232. Oxford Univ. Press.
David Dowty. 1991. Thematic Proto-Roles and
Argument Selection. Language, 67(3):547?
619.
Jerry A Fodor. 1992. A theory of the child?s theory
of mind. Cognition, 44(3):283?296.
Lila R. Gleitman, Kimberly Cassidy, Rebecca
Nappa, Anna Papafragou, and John C.
Trueswell. 2005. Hard words. Language
Learning and Development, 1(1):23?64.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classifica-
tion of English verbs. Language Resources and
Evaluation, 42(1):21?40.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
Elena Lieven, Dorothe? Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
B. MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
Anna Papafragou, Kimberly Cassidy, and Lila
Gleitman. 2007. When we think about think-
ing: The acquisition of belief verbs. Cognition,
105(1):125?165.
Christopher Parisien and Suzanne Stevenson.
2011. Generalizing between form and meaning
using learned verb classes. In Proceedings of
the 33rd Annual Meeting of the Cognitive Sci-
ence Society.
Bele?n Pascual, Gerardo Aguado, Mar??a Sotillo,
and Jose C Masdeu. 2008. Acquisition of men-
tal state language in Spanish children: a longitu-
dinal study of the relationship between the pro-
duction of mental verbs and linguistic develop-
ment. Developmental Science, 11(4):454?466.
Amy Perfors, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, negative
evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
37(03):607?642.
Josef Perner. 1988. Developing semantics for the-
ories of mind: From propositional attitudes to
mental representation. Developing theories of
mind, pages 141?172.
Josef Perner, Manuel Sprung, Petra Zauner, and
Hubert Haider. 2003. Want That is understood
well before Say That, Think That, and False Be-
lief: A test of de Villiers?s linguistic determin-
ism on German?speaking children. Child devel-
opment, 74(1):179?188.
Jacqueline Sachs. 1983. Talking about the There
and Then: The emergence of displaced refer-
ence in parent?child discourse. Children?s lan-
guage, 4.
Marilyn Shatz, Henry M. Wellman, and Sharon
Silber. 1983. The acquisition of mental verbs:
A systematic investigation of the first reference
to mental state. Cognition, 14(3):301?321.
239
Patrick Suppes. 1974. The semantics of children?s
language. American psychologist, 29(2):103.
240
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 37?45,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Learning Verb Classes in an Incremental Model
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
The ability of children to generalize over
the linguistic input they receive is key to
acquiring productive knowledge of verbs.
Such generalizations help children extend
their learned knowledge of constructions
to a novel verb, and use it appropriately in
syntactic patterns previously unobserved
for that verb?a key factor in language
productivity. Computational models can
help shed light on the gradual development
of more abstract knowledge during verb
acquisition. We present an incremental
Bayesian model that simultaneously and
incrementally learns argument structure
constructions and verb classes given nat-
uralistic language input. We show how the
distributional properties in the input lan-
guage influence the formation of general-
izations over the constructions and classes.
1 Introduction
Usage-based accounts of language learning note
that young children rely on verb-specific knowl-
edge to produce their early utterances (e.g.,
Tomasello, 2003). However, evidence suggests
that even young children can generalize their
verb knowledge to novel verbs and syntactic
frames (e.g., Fisher, 2002), and that the abstract
knowledge gradually strengthens over time (e.g.,
Tomasello and Abbot-Smith, 2002). One area of
verb usage where more sophisticated abstraction
appears necessary for fully adult productivity in
language is the knowledge of verb alternations.
A verb alternation is a pairing of constructions
shared by a number of verbs, in which the two
constructions express related argument structures
(Levin, 1993): e.g., the dative alternation involves
the related forms of the prepositional dative (PD;
X gave Y to Z) and the double-object dative (DO; X
gave Z Y). Such alternations enable language users
to readily adapt new and low frequency verbs to
appropriate constructions of the language by gen-
eralizing the observed use of one such form to the
other.
1
For example, Conwell and Demuth (2007) show
that 3-year-old children understand that a novel
verb observed only in the DO dative (John gor-
ped Heather the book) can also be used in the PD
form (John gorped the book to Heather), though
the children can only generalize such knowledge
under certain experimental conditions. Wonnacott
et al. (2008) demonstrate the proficiency of adults
in making such generalizations within an artificial
language learning scenario, which enables the re-
searchers to explore the distributional properties
of the linguistic input that facilitate the acquisition
of such generalizations. The results suggest that
the overall frequency of the syntactic patterns as
well as the distribution of verbs across the patterns
play a facilitatory role in the formation of abstract
verb knowledge (in the form of verb alternations)
in adult language learners.
In this work, we propose a computational
model that extends an existing Bayesian model of
verb argument structure acquisition (Alishahi and
Stevenson, 2008)[AS08] to support the learning of
verb classes over the acquired constructions. Our
model is novel in its approach to verb class forma-
tion, because it clusters tokens of a verb that reflect
the distribution of the verb over the learned con-
structions each time the verb is used in an input.
That is, the model forms verb classes by cluster-
ing verb tokens that reflect the evolving usages of
the verbs in various constructions.
We use this new model to analyze the role of
the classes and the distributional properties of the
input in learning abstract verb knowledge, given
1
The generalization of an alternation refers to a speaker
using one variant of an alternation for a verb (e.g., PD) having
only observed the verb in the other variant (e.g., DO).
37
naturalistic input that contains many verbs and
many constructions. The model can form higher-
level generalizations such as learning verb alterna-
tions, which is not possible with the AS08 model
(cf. the findings of Parisien and Stevenson, 2010).
Moreover, because our model gradually forms its
representations of constructions and classes over
time (in contrast to other Bayesian models, such
as Parisien and Stevenson, 2010; Perfors et al.,
2010), it is possible to analyze the monotonically-
growing representations and show their compati-
bility with the developmental patterns seen in chil-
dren (Conwell and Demuth, 2007). We also repli-
cate some of the observations of Wonnacott et al.
(2008) on the role of distributional properties of
the language in influencing the degree of general-
ization over an alternation.
2 Related Work
To explore the properties of learning mechanisms
that are capable of mimicking child and adult psy-
cholinguistic observations, a number of cognitive
modeling studies have focused on learning ab-
stract verb knowledge from individual verb usages
(e.g., Alishahi and Stevenson, 2008; Perfors et al.,
2010; Parisien and Stevenson, 2010). Here we fo-
cus on such computational models that enable the
sort of higher-level generalization that people do
across verb alternations, unlike the AS08 model.
The hierarchical Bayesian models of Perfors
et al. (2010) and Parisien and Stevenson (2010)
focus on learning this kind of higher-level general-
ization. The model of Perfors et al. (2010) learns
verb alternations, i.e., pairs of syntactic patterns
shared by certain groups of verbs. By incorpo-
rating this sort of abstract knowledge into their
model, Perfors et al. are able to simulate the abil-
ity of adults to generalize across verb alternations
(as in Wonnacott et al., 2008). That is, Perfors
et al. predict the ability of a novel verb to occur
in a syntactic structure after exposure to it in the
alternative pattern of that alternation. However,
this model is trained on data that contains only a
limited number of verbs and syntactic patterns un-
like naturalistic Child-directed Speech (CDS) and
moreover incorporates built-in information about
verb constructions.
The hierarchical Dirichlet model of Parisien
and Stevenson (2010) addresses these limitations
by working with natural child-directed speech
(CDS) data. Moreover, the model of Parisien and
Stevenson simultaneously learns constructions as
in AS08 and verb classes based on verb alterna-
tion behaviour, showing that the latter level of ab-
straction is necessary to support effective learn-
ing of verb alternations. Still, the models of both
Parisien and Stevenson and Perfors et al. can only
be utilized as a batch process and hence are lim-
ited in the analysis of developmental trajectories.
Although it is possible to simulate development by
training such models on increasing portions of in-
put, such an approach does not ensure that the rep-
resentations given n + i inputs can be developed
from the representation given n inputs.
In this paper, we propose a significant extension
to the model of AS08, by adding an extra level of
abstraction that incrementally learns verb classes
by drawing on the distribution of verbs over the
learned constructions. The new model combines
the advantages of having a monotonic clustering
model that enables the analysis of developing clus-
ters, with the simultaneous learning of construc-
tions and verb classes.
3 The Computational Model
As mentioned above, our model is an extension
of the model of AS08 in which we add a level of
learned abstract knowledge about verbs. Specif-
ically, our model uses a Bayesian clustering pro-
cess to learn clusters of verb usages that occur in
similar argument structure constructions, as in the
original model of AS08. To this, we add another
level of abstraction that learns clusters of verbs
that exhibit similar distributional patterns of oc-
currence across the learned constructions?that is,
classes of verbs that occur in similar sets of con-
structions, and in similar proportions. To distin-
guish between the clusters of the two levels of ab-
straction in our new model, we refer to the clusters
of verb usages as constructions, and to the group-
ings of verbs given their distribution over those
constructions as verb classes.
3.1 Overview of the Model
The model learns from a sequence of frames,
where each frame is a collection of features rep-
resenting what the learner might extract from an
utterance s/he has heard. Similarly to previous
computational studies (e.g., Parisien and Steven-
son, 2010), here we focus on syntactic features
since our goal is to understand the acquisition of
acceptable syntactic structures of verbs indepen-
38
Figure 1: A visual representation of the two levels of ab-
straction in the model, with sample verb usages input (and
extracted input frames), constructions, and classes.
dently of their meaning, as in some relevant psy-
cholinguistic (Wonnacott et al., 2008) and com-
putational studies (Parisien and Stevenson, 2010).
We focus particularly on properties such as syn-
tactic slots and argument count. (These features,
as in Parisien and Stevenson (2010), provide a
more flexible and generalizable representation of a
syntactic structure than the syntactic pattern string
used by AS08.) See the bottom rows of boxes in
Figure 1 for sample input verb usages with their
extracted frames.
The model incrementally clusters the extracted
input frames into constructions that reflect prob-
abilistic associations of the features across simi-
lar verb usages; see the middle level of Figure 1.
Each learned cluster is a probabilistic (and possi-
bly noisy) representation of an argument structure
construction: e.g., a cluster containing frames cor-
responding to usages such as I eat apples, She took
the ball, and He got a book, etc., represents a Tran-
sitive Action construction.
2
Such constructions al-
low for some degree of generalization over the ob-
served input; e.g., when seeing a novel verb in a
Transitive utterance, the model predicts the simi-
larity of this verb to other Action verbs appearing
in that pattern (Alishahi and Stevenson, 2008).
Grouping of verb usages into constructions may
not be sufficient for making higher-level general-
izations across verb alternations. Knowledge of al-
ternations is only captured indirectly in construc-
tions (because usages of the same verb can oc-
cur in multiple clusters). Following Parisien and
Stevenson (2010), we hypothesize that true gen-
eralization behaviour requires explicit knowledge
that verbs have commonalities in their patterns of
occurrence across constructions; this is the basis
2
Because the associations are probabilistic, a linguistic
construction may be represented by more than one cluster.
for verb classes (Levin, 1993; Merlo and Steven-
son, 2000; Schulte im Walde and Brew, 2002).
To capture this, our model learns groupings of
verbs that have similar distributions across the
learned constructions. These groupings form verb
classes that provide a higher-level of abstraction
over the input; see the top level in Figure 1. Con-
sider the dative alternation: the classes capture the
fact that some verbs may occur only in preposi-
tional dative (PD) forms, such as sing, while oth-
ers occur only in double object (DO) forms (call),
while still others alternate ? i.e., they occur in both
(bring).
Our model simultaneously learns both of these
types of knowledge: constructions are clusters of
verb usages, and classes are clusters of verb dis-
tributions over those constructions. Importantly, it
does so incrementally, which allows us to exam-
ine the developmental trajectory of acquiring al-
ternations such as the dative as the learned clus-
ters grow over time. Moreover, both types of clus-
tering are monotonic, i.e., we do not re-structure
the groupings that our model learns. However, the
model in both levels is clustering verb tokens ? i.e.,
the features corresponding to the verb at that time
in the input, its usage or its current distribution ?
so that the same verb type may be added to various
clusters at different stages in the training.
3.2 Learning Constructions of Verb Usages
The model of AS08 groups input frames into clus-
ters on the basis of the overall similarity in the
values of their features. Importantly, the model
learns these clusters incrementally in response to
the input; the number and type of clusters is not
predetermined. The model considers the creation
of a new cluster for a given frame if the frame is
not sufficiently similar to any of the existing clus-
ters. Formally, the model finds the best cluster for
a given input frame F as in:
BestCluster(F ) = argmax
k?Clusters
P (k|F ) (1)
where k ranges over all existing clusters and a new
one. Using Bayes rule:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of a cluster P (k) is estimated
as the proportion of frames that are in k out of
all observed input frames, thus assigning a higher
39
prior more frequent constructions. The likelihood
P (F |k) is estimated based on the match of fea-
ture values in F and in the frames of k (assuming
independence of the features):
P (F |k) =
?
i?Features
P
i
(j|k) (3)
where j is the value of the i
th
feature of F , and
P
i
(j|k) is calculated using a smoothed version of:
P
i
(j|k) =
count
i
(j, k)
n
k
(4)
where count
i
(j, k) is the number of times feature i
has the value j in cluster k, and n
k
is the number of
frames in k. We compare the slot features as sets to
capture similarities in overlapping syntactic slots
rather than enforcing an exact match. The model
uses the Jaccard similarity score to measure the
degree of overlap between two feature sets, instead
of the direct count of occurrence in Eqn. (4):
sim score(S
1
, S
2
) =
|S
1
? S
2
|
|S
1
? S
2
|
(5)
where S
1
and S
2
in our experiments here are the
sets of syntactic slot features.
3.3 Learning Verb Classes
Our new model extends the construction-
formation model of AS08 by grouping verbs into
classes on the basis of their distribution across
the learned constructions. That is, verbs that have
statistically-similar patterns of occurrence across
the learned constructions will be considered as
forming a verb class. For example, in Figure 1 we
see that bring and read may be put into the same
class because they both occur in a similar relative
frequency across the DO and PD constructions
(the leftmost and rightmost constructions in the
figure).
We use the same incremental Bayesian cluster-
ing algorithm for learning the verb classes as for
learning constructions. At the class level, the fea-
ture used for determining similarity of items in
clustering is the distribution of each verb across
the learned constructions. As for constructions,
the model learns the verb classes incrementally;
the number and type is not predetermined. More-
over, just as constructions are gradually formed
from successively processing a particular verb us-
age at each input step, the model forms verb
classes from a sequence of snapshots of the input
verb?s distribution over the constructions at each
input step. This means that our model is forming
classes of verb tokens rather than types; if a verb?s
behaviour changes over the duration of the input,
subsequent tokens (the distributions over construc-
tions at later points in time) may be clustered into
a different class (or classes) than earlier tokens,
even though prior decisions cannot be undone.
Formally, after clustering the input frame at
time t into a construction, as explained above, the
model extracts the current distribution d
v
t
of its
head verb v over the learned constructions; this is
estimated as a smoothed version of v?s relative fre-
quency in each construction:
P (k|v) =
count(v, k)
n
v
(6)
where count(v, k) is the number of times that in-
puts with verb v have been clustered into construc-
tion k, and n
v
is the number of times v has oc-
curred in the input thus far.
To cluster this snapshot of the verb?s distribu-
tion, d
v
t
, it is compared to the distributions en-
coded by the model?s classes. The distribution d
c
of an existing class c is the weighted average of
the distributions of its member verb tokens:
d
c
=
1
|c|
?
v?c
count(v, c)? d
v
(7)
where |c| is the size of class c, count(v, c) is the
number of occurrences of v that have been as-
signed to c, and d
v
is the distribution of the verb v
given by the tokens of v (the ?snapshots? of distri-
butions of v assigned to class c). That is, d
v
in c is
an average of the distributions of all d
v
t
for verb v
that have been clustered into c.
The model finds the best class for a given verb
distribution d
v
t
based on its similarity to the dis-
tributions of all existing classes and a new one:
BestClass(d
v
t
) = argmax
c?Classes
(1?D
JS
(d
c
?d
v
t
))
(8)
where c ranges over all existing classes as well as
a new class that is represented as a uniform dis-
tribution over the existing constructions. Jensen?
Shannon divergence, D
JS
, is a popular method for
measuring the distance between two distributions:
It is based on the KL?divergence, but it is symmet-
ric and has a finite value between 0 and 1:
D
JS
(p?q) =
1
2
D
KL
(p?
1
2
(p+ q)) +
1
2
D
KL
(q?
1
2
(p+ q)) (9)
40
non-ALT ALT
DO-only PD-only DO PD
Number of verbs 12 5 6
Relative frequency 14% 2% 2% 1%
Table 1: Number of non-alternating (non-ALT) and alter-
nating (ALT) verbs in our lexicon, as well as the relative fre-
quency of each construction in our generated input corpora.
4 Experimental Setup
4.1 Generation of the Input Corpora
We follow the input generation method of AS08
to create naturalistic corpora that are based on the
distributional properties of verbs over various con-
structions, as observed in child-directed speech
(CDS). Our input-generation lexicon contains 71
verbs drawn from AS08 (11 action verbs) and
Barak et al. (2013) (31 verbs of varying syntac-
tic patterns), plus an additional 40 of the most fre-
quent verbs in CDS, in order to have a range of
verbs that occur with the PD and DO construc-
tions. Table 4.1 shows the number of verbs that
appear in the DO or PD construction only (non-
alternating), as well as those that alternate across
the two. (The table also gives the relative fre-
quency of each dative construction in our gener-
ated input corpora.) Each verb lexical entry in-
cludes its overall frequency, and its relative fre-
quency with each of a number of observed syn-
tactic constructions. The frequencies are extracted
from a manual annotation of a sample of 100
child-directed utterances per verb from a collec-
tion of eight corpora from CHILDES (MacWhin-
ney, 2000).
3
An input corpus is generated by it-
eratively selecting a random verb and a syntactic
construction based on their frequencies according
to the lexicon, so that all input corpora used in our
simulations have the distributional properties ob-
served in CDS, but show some variation in precise
make-up and ordering of verb usages. The gener-
ated input consists of frames (a set of features) that
correspond to verb usages in CDS.
4.2 Simulations
Because the generation of the input data is prob-
abilistic, we conduct 100 simulations for each
experiment (each using a different input cor-
pus) to avoid any dependency on specific id-
iosyncratic properties of a single generated cor-
pus. For each simulation, we train our model
3
Brown (1973); Suppes (1974); Kuczaj (1977); Bloom
et al. (1974); Sachs (1983); Lieven et al. (2009).
on an automatically-generated corpus of 15, 000
frames, from which the model learns construc-
tions and verb classes. At specified points in
the input, we present the model with usages of
a novel verb in a DO and/or PD frame, and
then test the model?s generalization ability by
predicting DO and PD frames given that verb.
Since we are interested in the relative likeli-
hoods of the two frames, we report the differ-
ence between the log-likelihood of the DO frame
and the log-likelihood of the PD frame, i.e.,
log-likelihood(DO)? log-likelihood(PD).
Specifically, we form a partial frame F
test
(con-
taining all usage features except for the verb) that
reflects either the PD or the DO syntax, and assess
the probability P (F
test
|v) for each of these, as in:
P (F
test
|v) =
?
k?Constructions
P (F
test
|k)P (k|v)
(10)
where P (F
test
|k) is calculated as in Eqn. (3).
We can calculate P (k|v) in two different ways:
using only the knowledge in the constructions of
the model, and using the knowledge that takes into
account the verb classes over the constructions.
For model predictions based on the construction
level only, we calculate P (k|v) as in Eqn. (6),
which is the smoothed relative frequency of the
verb v over construction k.
Predictions using knowledge of the verb classes
will instead determine P (k|v) drawing on the fit
of verb v to the various classes (specifically, the
similarity of v?s distribution over constructions to
the distribution encoded in each class), and the
likelihood of each construction k for each class c
(specifically, the likelihood of k given the distribu-
tion over constructions encoded in c), as in:
P (k|v) ?
?
c?Classes
P (k|c)P (c|v) (11)
where P (k|c) is the probability of construction
k given class c?s distribution over constructions
(d
c
); and P (c|v) is the probability of c given verb
v?s distribution d
v
over the constructions (using
Jensen-Shannon divergence as in Eqn. (9)).
Due to the different number of clusters in each
of the construction and class layers of the model,
the likelihoods computed for each will differ in
the range of values. For this reason, specific val-
ues cannot be directly compared across the layers
of the model, rather we must analyze the general
trends of the construction-only and class-based re-
sults.
41
5 Evaluation
In this section we examine whether and how our
model generalizes across the two variants of the
dative alternation, the double-object dative (DO)
and the prepositional dative (PD). To do so, we
measure the tendency of the model to produce a
novel verb observed in one dative frame in that
same frame, or in the other dative frame (unob-
served for that verb). Our goal is to understand the
impact of the learned constructions and classes on
this generalization behaviour. Following Parisien
and Stevenson (2010), we examine three input
conditions in which the novel verb occurs: (i)
twice with the DO syntax (non-alternating); (ii)
twice with the PD syntax (non-alternating); or (iii)
once each with DO and PD syntax (alternating).
4
We then ask the model to predict the likelihood of
producing each dative frame with that verb. Our
focus here is on comparing the generalization abil-
ities of the two levels of abstract knowledge in our
model: the constructions versus the verb classes.
As a reminder, we use the dative alternation as
one example for considering this kind of higher-
level generalization behaviour observed in adults
and to a lesser extent in children. Moreover, we
perform the analysis in the context of naturalistic
input that contains many verbs (those that appear
in the dative and those that do not), and a variety of
constructions , to provide a realistic setting for the
task. Our settings differ from the psycholinguis-
tic studies in the variability of constructions com-
pared with the artificial language used by Won-
nacott et al., and in focusing only on the syntac-
tic properties unlike Conwell and Demuth. How-
ever, we follow the settings of these studies in an-
alyzing the syntactic properties of a generated ut-
terance given minimal exposure to a novel verb.
Therefore, we aim to replicate their general ob-
servations by showing that (i) children are limited
in their ability to generalize across verb alterna-
tions compared with adults, and (ii) the frequency
of a construction has a positive correlation with the
generalization rate of the construction.
5.1 Generalization of Learned Knowledge
We examine the generalization patterns of our
model when presented with a novel verb in DO/PD
forms after being trained on 15, 000 inputs, which
we compare to the performance of adults in such
4
For the alternating condition, half the simulations have
DO first, and half have PD first.
Figure 2: The difference between the log-likelihood values
of the DO and PD frames, given each of the three input con-
ditions: DO only, PD only, and Alternating. Values above
zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
language tasks. We first consider the case where
the model predictions are based solely on the
knowledge of constructions. Here we expect the
predictions to correspond to the syntactic proper-
ties of the two inputs observed for the novel verb,
with limited generalization. That is, we expect a
non-alternating verb to be much more likely in the
observed dative frame, and an alternating verb to
be equally likely in both frames. The left hand
side of Figure 2 presents the differences in log-
likelihoods of the predicted DO and PD frames for
the novel verb using the construction-based prob-
abilities. The results confirm our expectation that
the knowledge of constructions can support only
limited generalization across the variants of an al-
ternation. For the non-alternating conditions, the
observed frame is highly favoured, and for the
Alternating test scenario, the DO and PD frames
have nearly equal likelihoods.
We next turn to using the knowledge of verb
classes, which we expect to enable generaliza-
tions that correspond to verb alternation behaviour
? that is, we expect the model predictions here
to reflect the knowledge that verbs that occur in
one form of the alternation also often occur in
the other form of the alternation. This is possible
because the classes in the model encode the dis-
tributional patterns of verbs across constructions.
In the absence of other factors, we would expect
the Alternating condition to again show near equal
likelihoods for the two frames, and the two non-
alternating conditions to show a slight preference
for the observed frame (rather than the strong pref-
erence seen in the construction-based predictions),
because the unobserved frame is also likely due to
the knowledge here of the alternation.
The right hand side of Figure 2 presents the
42
difference in the log-likelihoods of the DO and
PD frames when using the knowledge encoded
in the verb classes. The results are not directly
in line with the simple prediction above: The
non-alternating (DO-only and PD-only) condi-
tions show a weak preference (as expected) for one
frame over another, but both favour the DO frame,
as does the Alternating condition. That is, the PD-
only and Alternating conditions show a preference
for the DO frame that does not follow simply from
the knowledge of alternations.
The DO preference in the PD-only and Alter-
nating conditions arises due to distributional fac-
tors in the input, related to the frequencies of the
constructions reported in Table 1. First, the DO
frame is overall much more likely than the PD
frame, causing generalization in the PD-only and
Alternating conditions to lean more to that frame.
Second, fully 1/3 of the uses of the PD frame in
the corpus are with verbs that alternate (i.e., 1%
of the corpus are PD frames of alternating PD-
DO verbs, out of a total of 3% of the corpus be-
ing PD frames), while only 1/8 of the uses of the
DO frame are with alternating rather than non-
alternating verbs. Recall that our classes encode
the distribution (roughly relative frequency) of the
verbs in the class occurring across the different
constructions. This means that in our class-based
predictions, greater weight will be given to con-
structions with DO when observing a PD frame
than to constructions with PD when observing a
DO frame. These results underline the importance
of using naturalistic input and considering the im-
pact of various distributional factors on general-
ization of verb knowledge.
In contrast to the construction-based results, our
class-based results conform with the experimental
findings of Wonnacott et al. (2008), who show that
adult (artificial) language learners robustly gener-
alize a newly-learned verb observed in a single
syntactic form by producing it in the alternating
syntactic form under certain language conditions.
Moreover, we show similar distributional effects
to theirs ? the overall frequency of the syntactic
patterns, as well as the distribution of verbs across
those patterns ? in the level of preference for one
form over another, within the context of our nat-
uralistic data with multiple verbs, constructions,
and alternations. These results show that the verb
classes in the model are able to capture useful ab-
stract knowledge that is key to understanding the
human ability to make high-level generalizations
across verb alternations.
5.2 Development of Generalizations
Next, we present the results of our model evalu-
ated throughout the course of training in order to
understand the developmental pattern of general-
ization. We perform the same construction-based
or class-based prediction tasks (the likelihoods of
a DO and PD frame), following the same input
conditions (a novel verb with two DO frames, two
PD frames, or one of each) at given points during
the 15, 000 inputs. As above, we present the dif-
ference in the log-likelihood values of the DO and
the PD frames in order to focus on the relative like-
lihoods of the two frames within each condition of
construction-based or class-based predictions.
Figure 3(a) presents the results for the DO-
only test scenario. As in Section 5.1, for
both construction-based and class-based predic-
tions there is a higher likelihood for the DO frame
throughout the course of training. In contrast, the
incremental results for the PD-only test scenario,
in Figure 3(b), display a developing level of gen-
eralization throughout the training stage for the
class-based predictions. While the construction-
based predictions reflect a much higher likelihood
for the PD frame, the results from the verb classes
are in favor of the PD frame only initially; after
training on 5000 input frames, the likelihood of
the DO frame becomes higher for this test sce-
nario. These results indicate that using construc-
tion knowledge alone does not enable generaliza-
tion from the PD frame to the DO frame; in con-
trast, the verb class knowledge enables the grad-
ual acquisition of generalization ability over the
course of training.
Finally, Figure 3(c) presents the results for the
Alternating test scenario for the two types of pre-
dictions. As in Section 5.1, both construction-
based and class-based predictions have a small
preference for the DO frame. In the construction-
based predictions, this preference lessens over
time to where the likelihoods for DO and PD are
almost equal, while the class-based predictions
stay relatively constant in their preference for the
DO frame. In some ways the construction-based
predictions are more expected in response to an
apparently alternating verb; however, the class-
based predictions show a higher degree of general-
ization, responding to the higher frequency of the
43
(a) DO only (b) PD only (c) Alternating
Figure 3: Difference of log-likelihood values of the DO and PD frames over the course of training for the constructions and
the verb classes for each of the 3 test scenarios. Values above zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
DO frame and the higher association of PD frames
with DO alternates. These results again empha-
size the importance of further exploring the role
of distributional factors on generalization of verb
knowledge in children.
The developmental results presented here are in
line with the suggestions of Tomasello (2003) that
the productions of younger children follow ob-
served patterns in the input, and only later reflect
robust generalizations of their knowledge across
verbs. Conwell and Demuth (2007) for example,
found evidence of generalization across verb al-
ternations in 3-year-old children, but their produc-
tion of unobserved forms for a novel verb was
very sensitive to the precise context of the ex-
periment and the distributional patterns across the
novel verbs. In accord with these observations, the
developmental trajectories in our model show that
our class-based predictions increase in their degree
of generalization over time, and are sensitive to
various distributional factors in the input, such as
the overall expectation for a frame and the expec-
tation that a verb will alternate.
6 Discussion
We present a novel computational model that
probabilistically learns two levels of abstractions
over individual verb usages: constructions that
are clusters of similar verb usages, and classes of
verbs with similar distributional behaviour across
the constructions. Specifically, we extend the
model of AS08 by incrementally learning token-
based verb classes that generalize over the con-
struction knowledge level. In contrast to the mod-
els of Parisien and Stevenson and Perfors et al.,
our model is incremental, and hence enables the
analysis of the monotonically developing classes
to show the relation to the development of gener-
alization ability in human learners.
We analyze how generalization is supported by
each level of learning in our model: constructions
and verb classes. Our results confirm (cf. Parisien
and Stevenson, 2010) that a higher-level knowl-
edge of the verb classes is required to replicate the
observed patterns of generalization, such as pro-
ducing a novel verb gorp in the in the prepositional
dative pattern after hearing it in the double object
dative pattern. In addition, our analysis of the in-
crementally developing verb classes shows that the
generalization knowledge gradually emerges over
time, similar to what is observed in children.
The flexibility of input representation of our
model enables us to further explore the properties
of the input in learning abstract knowledge, fol-
lowing psycholinguistic studies. Our results repli-
cate the findings of Wonnacott et al. on the role
of the distributional properties over the alternat-
ing syntactic forms, but in naturalistic settings of
many constructions. In future, we plan to extend
this analysis by manipulating the distributions of
our input data to replicate the exact settings of the
artificial language used by Wonnacott et al.. More-
over, in this study, we followed the settings of pre-
vious computational and psycholinguistic studies
that focused on the syntactic properties of the in-
put (Perfors et al., 2010; Parisien and Stevenson,
2010; Wonnacott et al., 2008; Conwell and De-
muth, 2007). However, we can further our anal-
ysis by incorporating semantic features in the in-
put to study syntactic bootstrapping effects (Scott
and Fisher, 2009) as well as the role of seman-
tic properties in constraining the generalizations
across the alternating forms.
Acknowledgments
The authors would like to thank Afra Alishahi for
providing us with the code and data for her model,
and to Chris Parisien for sharing his data with us.
44
References
Afra Alishahi and Suzanne Stevenson. 2008. A
computational model of early argument struc-
ture acquisition. Cognitive Science, 32(5):789?
834.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2013. Acquisition of desires before beliefs:
A computational investigation. In Proceedings
of CoNLL-2013.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development:
If, when, and why. Cognitive Psychology,
6(3):380?420.
Roger Brown. 1973. A first language: The early
stages. Harvard Univ. Press.
Erin Conwell and Katherine Demuth. 2007. Early
syntactic productivity: Evidence from dative
shift. Cognition, 103(2):163?179.
Cynthia Fisher. 2002. The role of abstract syntac-
tic knowledge in language acquisition: A reply
to Tomasello. Cognition, 82(3):259?278.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
B. Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation, volume 348.
University of Chicago press Chicago, IL.
Elena Lieven, Doroth?e Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
B. MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
P. Merlo and S. Stevenson. 2000. Automatic verb
classification based on statistical distributions
of argument structure. Computational Linguis-
tics, 27(3):373?408.
Christopher Parisien and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the
32nd annual meeting of the Cognitive Science
Society.
Amy Perfors, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, negative
evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
37(03):607?642.
Jacqueline Sachs. 1983. Talking about the There
and Then: The emergence of displaced refer-
ence in parent?child discourse. Children?s lan-
guage, 4.
Sabine Schulte im Walde and Chris Brew. 2002.
Inducing German semantic verb classes from
purely syntactic subcategorisation information.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 223?230, Philadelphia, PA.
Rose M Scott and Cynthia Fisher. 2009. Two-
year-olds use distributional cues to interpret
transitivity-alternating verbs. Language and
cognitive processes, 24(6):777?803.
Patrick Suppes. 1974. The semantics of children?s
language. American psychologist, 29(2):103.
Michael Tomasello. 2003. Constructing a lan-
guage: A usage-based theory of language ac-
quisition.
Michael Tomasello and Kirsten Abbot-Smith.
2002. A tale of two theories: Response to
Fisher. Cognition, 83(2):207?214.
Elizabeth Wonnacott, Elissa L Newport, and
Michael K Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cog-
nitive psychology, 56(3):165?209.
45
