Proceedings of the ACL 2007 Demo and Poster Sessions, pages 225?228,
Prague, June 2007. c?2007 Association for Computational Linguistics
Japanese Dependency Parsing Using Sequential Labeling
for Semi-spoken Language
Kenji Imamura and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
{imamura.kenji, kikui.genichiro}@lab.ntt.co.jp
Norihito Yasuda
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
n-yasuda@cslab.kecl.ntt.co.jp
Abstract
The amount of documents directly published
by end users is increasing along with the
growth of Web 2.0. Such documents of-
ten contain spoken-style expressions, which
are difficult to analyze using conventional
parsers. This paper presents dependency
parsing whose goal is to analyze Japanese
semi-spoken expressions. One characteris-
tic of our method is that it can parse self-
dependent (independent) segments using se-
quential labeling.
1 Introduction
Dependency parsing is a way of structurally ana-
lyzing a sentence from the viewpoint of modifica-
tion. In Japanese, relationships of modification be-
tween phrasal units called bunsetsu segments are an-
alyzed. A number of studies have focused on parsing
of Japanese as well as of other languages. Popular
parsers are CaboCha (Kudo and Matsumoto, 2002)
and KNP (Kurohashi and Nagao, 1994), which were
developed to analyze formal written language ex-
pressions such as that in newspaper articles.
Generally, the syntactic structure of a sentence
is represented as a tree, and parsing is carried out
by maximizing the likelihood of the tree (Charniak,
2000; Uchimoto et al, 1999). Units that do not
modify any other units, such as fillers, are difficult
to place in the tree structure. Conventional parsers
have forced such independent units to modify other
units.
Documents published by end users (e.g., blogs)
are increasing on the Internet alng with the growth
of Web 2.0. Such documents do not use controlled
written language and contain fillers and emoticons.
This implies that analyzing such documents is diffi-
cult for conventional parsers.
This paper presents a new method of Japanese
dependency parsing that utilizes sequential labeling
based on conditional random fields (CRFs) in or-
der to analyze semi-spoken language. Concretely,
sequential labeling assigns each segment a depen-
dency label that indicates its relative position of de-
pendency. If the label set includes self-dependency,
the fillers and emoticons would be analyzed as seg-
ments depending on themselves. Therefore, since it
is not necessary for the parsing result to be a tree,
our method is suitable for semi-spoken language.
2 Methods
Japanese dependency parsing for written language
is based on the following principles. Our method re-
laxes the first principle to allow self-dependent seg-
ments (c.f. Section 2.3).
1. Dependency moves from left to right.
2. Dependencies do not cross each other.
3. Each segment, except for the top of the parsed
tree, modifies at most one other segment.
2.1 Dependency Parsing Using Cascaded
Chunking (CaboCha)
Our method is based on the cascaded chunking
method (Kudo and Matsumoto, 2002) proposed as
the CaboCha parser 1. CaboCha is a sort of shift-
reduce parser and determines whether or not a seg-
ment depends on the next segment by using an
1http://www.chasen.org/?taku/software/cabocha/
225
SVM-based classifier. To analyze long-distance de-
pendencies, CaboCha shortens the sentence by re-
moving segments for which dependencies are al-
ready determined and which no other segments de-
pend on. CaboCha constructs a tree structure by re-
peating the above process.
2.2 Sequential Labeling
Sequential labeling is a process that assigns each
unit of an input sequence an appropriate label (or
tag). In natural language processing, it is applied
to, for example, English part-of-speech tagging and
named entity recognition. Hidden Markov models
or conditional random fields (Lafferty et al, 2001)
are used for labeling. In this paper, we use linear-
chain CRFs.
In sequential labeling, training data developers
can design labels with no restrictions.
2.3 Cascaded Chunking Using Sequential
Labeling
The method proposed in this paper is a generaliza-
tion of CaboCha. Our method considers not only
the next segment, but also the followingN segments
to determine dependencies. This area, including the
considered segment, is called the window, and N is
called the window size. The parser assigns each seg-
ment a dependency label that indicates where the
segment depends on the segments in the window.
The flow is summarized as follows:
1. Extract features from segments such as the
part-of-speech of the headword in a segment
(c.f. Section 3.1).
2. Carry out sequential labeling using the above
features.
3. Determine the actual dependency by interpret-
ing the labels.
4. Shorten the sentence by deleting segments for
which the dependency is already determined
and that other segments have never depended
on.
5. If only one segment remains, then finish the
process. If not, return to Step 1.
An example of dependency parsing for written
language is shown in Figure 1 (a).
In Steps 1 and 2, dependency labels are supplied
to each segment in a way similar to that used by
Label Description
? Segment depends on a segment outside of win-
dow.
0Q Self-dependency
1D Segment depends on next segment.
2D Segment depends on segment after next.
-1O Segment is top of parsed tree.
Table 1: Label List Used by Sequential Labeling
(Window Size: 2)
other sequential labeling methods. However, our
sequential labeling has the following characteristics
since this task is dependency parsing.
? The labels indicate relative positions of the de-
pendent segment from the current segment (Ta-
ble 1). Therefore, the number of labels changes
according to the window size. Long-distance de-
pendencies can be parsed by one labeling process
if we set a large window size. However, growth
of label variety causes data sparseness problems.
? One possible label is that of self-dependency
(noted as ?0Q? in this paper). This is assigned
to independent segments in a tree.
? Also possible are two special labels. Label ?-1O?
denotes a segment that is the top of the parsed
tree. Label ??? denotes a segment that depends
on a segment outside of the window. When the
window size is two, the segment depends on a
segment that is over two segments ahead.
? The label for the current segment is determined
based on all features in the window and on the
label of the previous segment.
In Step 4, segments, which no other segments de-
pend on, are removed in a way similar to that used
by CaboCha. The principle that dependencies do
not cross each other is applied in this step. For ex-
ample, if a segment depends on a segment after the
next, the next segment cannot be modified by other
segments. Therefore, it can be removed. Similarly,
since the ??? label indicates that the segment de-
pends on a segment after N segments, all interme-
diate segments can be removed if they do not have
??? labels.
The sentence is shortened by iteration of the
above steps. The parsing finishes when only one
segment remains in the sentence (this is the segment
226
(a) Written Language
--- 2D 1D 1D -1O
2D 1D -1O
Output
Input
Label
Label
kare wa
(he)
kanojo no
(her)
atatakai
(warm)
magokoro ni
(heart)
kando-shita.
(be moved)
(He was moved by her warm heart.)
Seg. No. 1 2 3 4 5
kare wa
(he)
kanojo no
(her)
atatakai
(warm)
magokoro ni
(heart)
kando-shita.
(be moved)
(b) Semi-spoken Language
Input Uuuum, kyo wa
(today)
...... choshi
(condition)
yokatta desu.
(be good)
0Q --- 0Q 1D -1O
1D -1O
(Uuuum, my condition .... was good today.)
Seg. No. 1 2 3 4 5
Label
Label
Uuuum, kyo wa
(today)
...... choshi
(condition)
yokatta desu.
(be good)Output
1st
Labeling
2nd
Labeling
Figure 1: Examples of Dependency Parsing (Window Size: 2)
Corpus Type # of Sentences # of Segments
Kyoto Training 24,283 234,685
Test 9,284 89,874
Blog Training 18,163 106,177
Test 8,950 53,228
Table 2: Corpus Size
at the top of the parsed tree). In the example in Fig-
ure 1 (a), the process finishes in two iterations.
In a sentence containing fillers, the self-
dependency labels are assigned by sequential label-
ing, as shown in Figure 1 (b), and are parsed as in-
dependent segments. Therefore, our method is suit-
able for parsing semi-spoken language that contains
independent segments.
3 Experiments
3.1 Experimental Settings
Corpora In our experiments, we used two cor-
pora. One is the Kyoto Text Corpus 4.0 2, which is
a collection of newspaper articles with segment and
dependency annotations. The other is a blog cor-
pus, which is a collection of blog articles taken as
semi-spoken language. The blog corpus is manually
annotated in a way similar to that used for the Kyoto
text corpus. The sizes of the corpora are shown in
Table 2.
Training We used CRF++ 3, a linear-chain CRF
training tool, with eleven features per segment. All
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html
3http://www.chasen.org/?taku/software/CRF++/
of these are static features (proper to each segment)
such as surface forms, parts-of-speech, inflections
of a content headword and a functional headword
in a segment. These are parts of a feature set that
many papers have referenced (Uchimoto et al, 1999;
Kudo and Matsumoto, 2002).
Evaluation Metrics Dependency accuracy and
sentence accuracy were used as evaluation metrics.
Sentence accuracy is the proportion of total sen-
tences in which all dependencies in the sentence
are accurately labeled. In Japanese, the last seg-
ment of most sentences is the top of the parsed trees,
and many papers exclude this last segment from the
accuracy calculation. We, in contrast, include the
last one because some of the last segments are self-
dependent.
3.2 Accuracy of Dependency Parsing
Dependency parsing was carried out by combining
training and test corpora. We used a window size
of three. We also used CaboCha as a reference for
the set of sentences trained only with the Kyoto cor-
pus because it is designed for written language. The
results are shown in Table 3.
CaboCha had better accuracies for the Kyoto test
corpus. One reason might be that our method man-
ually combined features and used parts of com-
binations, while CaboCha automatically finds the
best combinations by using second-order polyno-
mial kernels.
For the blog test corpus, the proposed method
using the Kyoto+Blog model had the best depen-
227
Test Corpus Method Training Corpus Dependency Accuracy Sentence Accuracy
(Model)
Kyoto Proposed Method Kyoto 89.87% (80766 / 89874) 48.12% (4467 / 9284)
(Written Language) (Window Size: 3) Kyoto + Blog 89.76% (80670 / 89874) 47.63% (4422 / 9284)
CaboCha Kyoto 92.03% (82714 / 89874) 55.36% (5140 / 9284)
Blog Proposed Method Kyoto 77.19% (41083 / 53226) 41.41% (3706 / 8950)
(Semi-spoken Language) (Window Size: 3) Kyoto + Blog 84.59% (45022 / 53226) 52.72% (4718 / 8950)
CaboCha Kyoto 77.44% (41220 / 53226) 43.45% (3889 / 8950)
Table 3: Dependency and Sentence Accuracies among Methods/Corpora
 88
 88.5
 89
 89.5
 90
 90.5
 91
 1  2  3  4  5
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
D
ep
en
de
nc
y 
Ac
cu
ra
cy
 (%
)
# 
of
 F
ea
tu
re
s
Window Size
Dependency Accuracy
# of Features
Figure 2: Dependency Accuracy and Number of
Features According to Window Size (The Kyoto
Text Corpus was used for training and testing.)
dency accuracy result at 84.59%. This result was
influenced not only by the training corpus that con-
tains the blog corpus but also by the effect of self-
dependent segments. The blog test corpus contains
3,089 self-dependent segments, and 2,326 of them
(75.30%) were accurately parsed. This represents
a dependency accuracy improvement of over 60%
compared with the Kyoto model.
Our method is effective in parsing blogs be-
cause fillers and emoticons can be parsed as self-
dependent segments.
3.3 Accuracy According to Window Size
Another characteristic of our method is that all de-
pendencies, including long-distance ones, can be
parsed by one labeling process if the window cov-
ers the entire sentence. To analyze this characteris-
tic, we evaluated dependency accuracies in various
window sizes. The results are shown in Figure 2.
The number of features used for labeling in-
creases exponentially as window size increases.
However, dependency accuracy was saturated after a
window size of two, and the best accuracy was when
the window size was four. This phenomenon implies
a data sparseness problem.
4 Conclusion
We presented a new dependency parsing method us-
ing sequential labeling for the semi-spoken language
that frequently appears in Web documents. Sequen-
tial labeling can supply segments with flexible la-
bels, so our method can parse independent words
as self-dependent segments. This characteristic af-
fects robust parsing when sentences contain fillers
and emoticons.
The other characteristics of our method are us-
ing CRFs and that long dependencies are parsed in
one labeling process. SVM-based parsers that have
the same characteristics can be constructed if we in-
troduce multi-class classifiers. Further comparisons
with SVM-based parsers are future work.
References
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL-2000, pages 132?139.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analyisis using cascaded chunking. In Proc. of
CoNLL-2002, Taipei.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 282?289.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese dependency structure analysis based
on maximum entropy models. In Proc. of EACL?99,
pages 196?203, Bergen, Norway.
228
Spoken Dialogue Control Based on a Turn-minimization
Criterion Depending on the Speech Recognition Accuracy
YASUDA Norihito and DOHSAKA Kohji and AIKAWA Kiyoaki
NTT Communication Science Laboratories
3-1 Morinosato-Wakamiya, Atsugi, Kanagawa, 243-0198 Japan
{yasuda, dohsaka}@atom.brl.ntt.co.jp, aik@idea.brl.ntt.co.jp
Abstract
This paper proposes a new dialogue
control method for spoken dialogue
systems. The method configures a
dialogue plan so as to minimize the
estimated number of turns to com-
plete the dialogue. The number of
turns is estimated depending on the
current speech recognition accuracy
and probability distribution of the
true user?s request. The proposed
method reduces the number of turns
to complete the task at almost any
recognition accuracy.
1 Introduction
A spoken dialogue system determines user
requests from user utterances. Spoken di-
alogue systems, however, can?t determine a
user?s request only from an initial utterance,
because there is a limitation to automatic
speech recognition and recognition errors are
unavoidable. Thus, most spoken dialogue sys-
tems confirm a user?s utterance or demand the
information that is lacking in order to deter-
mine user?s request. Such dialogues for con-
firmation or demand between the system and
the user are called ?confirmation dialogues?.
Long confirmation dialogues are annoying, so
more efficient confirmation is desirable. To
measure the efficiency of the dialogue, we use
the number of turns (exchanges), where of
course, the fewer number of turns is better.
In practical applications, the system can
accepts multiple types of user requests like
?making a new appointment?, ?changing a
schedule?, and ?inquiring about a schedule?.
If the user request type is different, the re-
quired information for determining the user
request is also different. Sometimes the user
request type is ambiguous due to recognition
errors, and various types of user requests are
possible. In such a case, it is important for
the system to choose the type of user request
it will confirm at first, since it will be useless
to confirm items that are required for unlikely
type of request.
The recognition accuracy affects the effi-
ciency in other cases. For example, if there
are multiple items to be confirmed, intu-
itively, it seems efficient to confirm all of them
at once. However, the system must include
candidates for all attributes in recognition vo-
cabulary, which cause more recognition er-
rors. Moreover, even though there is only one
misrecognized item in confirmed items, the
user might just say coldly ?No?, and the sys-
tem cannot know that what are correct items.
Several efficient dialogue control methods
have been proposed (Niimi and Kobayashi,
1996; Litman et al, 2000). But there is no
previous works that take into account mul-
tiple types of user requests and recognition
accuracy during confirmation, which changes
what to be confirmed without domain-specific
rules or training.
To prevent needlessly long confirmation di-
alogues even if the system can accepts mul-
tiple types of user request, our method esti-
mates the expected number of turns to a cer-
tain use request type and the approximated
probability distribution of user request types.
The expected number of turns can be derived
from the required vocabulary for confirmation
and base recognition accuracy under certain
vocabulary size.
2 Method
Overview First, we describe about a sys-
tem to which we assume this method will be
applied. The system has belief state which
is represented by the set of attributes, their
values, and the certainty of the values. The
certainty is in [0 .. 1], and the certainty for
the determined value is 1. That is, if the
user replies ?Yes? to the confirmation, the
system changes the certainty for that value to
1. In practice, we can use the score from the
recognition engine as this certainty. The sys-
tem changes the recognition vocabulary ac-
cording to the attributes to be confirmed at
each confirmation. At any given time, the
system either confirms or demands some at-
tribute(s); it doesn?t confirm and demand at
the same time. Any values required in order
to determine the user request are explicitly-
confirmed without exception. Words that are
irrelevant to the present confirmation are ex-
cluded from the recognition vocabulary. The
system knows the base recognition accuracy
under a certain vocabulary size, which is used
to estimate the recognition accuracy.
Our method can be divided roughly into
five parts; the first three parts are used to
obtain the expected number of turns, granting
that the user request type are already known,
the fourth part is used to approximate the
probability distribution of the user request,
and the last part is used to decide the next
action to be taken by the system.
The system needs to know only three sorts
of information: 1) the vocabulary for each
attribute; 2) the meaning constraints among
words like ?If the family name of the person
is Yasuda, then his department must be ac-
counting?; and 3) the required information
for each type of user request like ?To can-
cel an appointment; the day and the time are
required?. No other domain-specific rules or
training are necessary.
Guessing the Recognition Accuracy
Here we consider how to estimate the recogni-
tion accuracy during confirmation from con-
firmation target. Once attributes for confir-
mation are decided, the recognition vocabu-
lary will consist of the words accepted by the
attributes and general words for moving the
dialogue along that are at least necessary to
progress the dialogue such as ?Yes?, ?No?,
etc. We call the recognition accuracy at this
time the ?attribute recognition accuracy?.
We adopt the rule of thumb that the recog-
nition error rate is in proportion to the square
root of vocabulary size (Rosenfeld, 1996; Nak-
agawa and Ida, 1998). Thus, the approxi-
mated attribute recognition accuracy can be
derived from the number of words accepted
by the attributes.
Note that the attribute recognition accu-
racy can?t be estimated beforehand, because
the candidates for some attributes are dynam-
ically change, as a result of the meaning con-
straints among words; if the value of one at-
tribute is fixed, then candidates for other at-
tributes will be limited to values that satisfy
the constraints. Besides, the degree of lim-
itation varies with the values. The relation
between the user?s family name and depart-
ment is such an example.
Turn Estimation to Determine Some
Attributes Next we consider how to esti-
mate the expected number of turns for de-
termining some attributes using the approxi-
mated attribute recognition accuracy.
We assume that the user?s reply to the con-
firmation must contain the intention that cor-
responds to ?Yes? or ?No?, and the inten-
tion must be transmitted to the system with-
out fail. Then, the expected number of turns
to complete confirming for some attributes is
equal to the expected number of turns in the
case that the confirmation is incorrect (i.e.
misrecognized). Therefore, we can derive the
number of expected turns to complete con-
firming Tc and demanding Td for some at-
tributes by the following expression:
Tc =
?
?
t=1
tr(1? r)t?1 = 1
r
Td = Tc + 1 = 1 +
1
r
where r denotes the attribute recognition ac-
curacy for attributes that are to be confirmed.
Turn Estimation to a Certain User Re-
quest Type Here we estimate the expected
number of turns, granting that the type of
user request is already known.
If the user request type is fixed, the re-
quired attributes for that type are also fixed.
By comparing the belief state with these at-
tributes, we can represent the required actions
to determine the user request by a set of pairs
made up of attributes and actions for the at-
tribute (confirmation or demand). Once this
set of pairs is given, we can choose the optimal
plan, because we can estimate the expected
turns of any permutations of any partitions
of this set. The expected number of turns
for this optiomal plan is used as the expected
number of turns for a given user request type.
Probability Distribution of User Re-
quest Types Here, we consider how to es-
timate the relevance between the belief state
and each user request types.
As it is hard to obtain the actual probabil-
ity distribution, we define the degree of rele-
vance between the belief state and each user
request type as an approximation.
Let ai, vi, ci be the i-th attribute, the value
of ai, and the certainty of vi respectively. We
define the relevance Rel(S, Rj) between the
belief state S and the user request type Rj as
for any vi which can be accepted
by Rj:
Rel(S, Rj) =
1
NG
j
? ci
Mv
i
where NR
j
denotes the number of required at-
tributes in user request type Rj, and Mv
i
de-
notes the number of user requests that accept
the value vi.
Choosing the Next Action Even if there
is a highly possible user request type, choos-
ing confirmation plan for it is not always best,
if the expected number of turns for that re-
quest is very large. In such case, confirm-
ing another type of request that is easily con-
firmed and medium possibility may better.
We assume that when the user request type
guessed by the system is not the real user re-
quest type, the number of turns required to
know that the guess is incorrect is equal to
the number of turns when the guess is correct
and finish confirming the contents.
Let pR
i
be the probability of user request
type Ri, and tR
i
be the expected number of
turns to user request type Ri.
From permutations of request types,
our method chooses the optimal order
a(1), a(2), . . . , a(n) such that the expression
pR
a(1)
tR
a(1)
+ pR
a(2)
(tR
a(1)
+ tR
a(2)
) + . . . +
pR
a(n)
(tR
a(1)
+ . . .+ tR
a(n)
) is minimal. Then
our method chooses the action that appears
first in the optimal plan for request type Ra(1)
as the next action.
3 Experiments
We evaluated the proposed method by simula-
tion. In the simulation, the system conversed
with a simulated user program. Simulation
with a simulated user enables rapid prototyp-
ing and evaluation (Eckert et al, 1998). The
conversation was not done by exchanging spo-
ken language, but by exchanging attribute-
value pairs.
Simulated User Program The simulated
user program works in the following steps:
1. Select a request. The request never
changes throughout the dialogue
2. Tell the system the request or a subset of
the request
3. Respond Yes or No if the system confirms
4. Give corrections at random if confirma-
tion contains errors
5. Respond to the demand from the system
6. Tell the system that there is no infor-
mation if the system refers to attributes
with which the user is not concerned
Specification of Test Task We prepared
a fictitious task for simulation. This task ac-
cepts six types of user demand. There are
six attributes, and two of them have meaning
dependence like the family name and depart-
ment. The numbers of persons, family names,
and departments are 3000, 1000, 300 respec-
tively.
24
6
8
10
12
14
16
0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
m
e
a
n
 n
u
m
be
r o
f t
ur
ns
recognition rate under 500 words vocaburaly
Our method
Naive method
Figure 1: Average number of turns to com-
plete a dialogue
0
20
40
60
80
100
0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
va
ria
nc
e 
of
 th
e 
nu
m
be
r o
f t
ur
ns
recognition rate under 500 words vocaburaly
Our method
Naive method
Figure 2: Variance of the number of turns to
complete a dialogue
Comparison with a Naive Method For
comparison, we prepared a naive confirmation
dialogue control method, with the following
specifications:
1. If the user request can be fixed uniquely
and there are unbound attributes re-
quired for that request, demand those at-
tributes one by one.
2. If there are values that are not confirmed,
confirm them one by one.
3. If the user request type can?t be fixed yet,
demand a value for an attribute in the
order of the number of user request types
that require that attribute.
Experimental Results Figures 1 and 2
show the average number of turns and its vari-
ance out of 1000 diaglogue. We can see from
these figures that our method can complete
dialogues in shorter turns than other methods
under various levels of recognition accuracy.
In addition, the variance is small in almost
every range, which illustrates the stability of
our method.
4 Conclusion
A new dialogue control method is proposed.
The method takes into consideration the ex-
pected number of turns based on the guessed
recognition accuracy and the approximated
probability distribution of user requests.
We don?t have to write domain-specific
rules manually by using this method. We can
thus easily transfer domain of the system.
We evaluated our method by simulation.
The result shows that it can complete di-
alogues in shorter turns than conventional
methods under various recognition accuracy.
Acknowledgements
We thank Ken?ichiro Ishii, Norihiro Hagita,
and all our colleagues in the Dialogue Un-
derstanding Research Group for useful discus-
sions.
References
Wieland Eckert, Esther Levin, and Roberto Pier-
accini. 1998. Automatic evaluation of spoken
dialogue systems. In TWLT13: Formal seman-
tics and pragmatics of dialogue.
Diane J. Litman, Michael S. Kearns, and Mari-
lyn A. Walker. 2000. Automatic optimization
of dialogue management. In COLING.
Seiichi Nakagawa and Masaki Ida. 1998. A
new measure of task complexity for continuous
speech recognition. IEICE, J81-D-II(7):1491?
1500(in Japanese).
Yasuhisa Niimi and Yutaka Kobayashi. 1996. Di-
alog control stragey based on the reliability of
speech recognition. In International Confer-
ence on Spoken Language Processing, pages 25?
30.
R. Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language model-
ing. Computer, Speech and Language, 10:187?
228.
WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogue 
Systems 
Mikio Nakano* Noboru Miyazaki, Norihito Yasuda, Akira Sugiyama, 
Jun-ichi Hirasawa, Kohji Dohsaka, Kiyoaki Aikawa 
NTT Corporation 
3-1 Morinosato-Wakamiya 
Atsugi, Kanagawa 243-0198, Japan 
E-mail: nakano@atom.brl.ntt.co.jp 
Abstract 
This paper describes WI'I; a toolkit 
for building spoken dialogue systems. 
WIT features an incremental under- 
standing mechanism that enables ro- 
bust utterance understanding and real- 
time responses. WIT's ability to com- 
pile domain-dependent system specifi- 
cations into internal knowledge sources 
makes building spoken dialogue sys- 
tems much easier than :it is from 
scratch. 
1 Introduction 
The recent great advances in speech and language 
technologies have made it possible to build fully 
implemented spoken dialogue systems (Aust et 
al., 1995; Allen et al, 1996; Zue et al, 2000; 
Walker et al, 2000). One of the next research 
goals is to make these systems task-portable, that 
is, to simplify the process of porting to another 
task domain. 
To this end, several toolkits for building spo- 
ken dialogue systems have been developed (Bar- 
nett and Singh, 1997; Sasajima et al, 1999). 
One is the CSLU Toolkit (Sutton et al, 1998), 
which enables rapid prototyping of a spoken di- 
alogue system that incorporates a finite-state dia- 
logue model. It decreases the amount of the ef- 
fort required in building a spoken dialogue sys- 
tem in a user-defined task domain. However, it 
limits system functions; it is not easy to employ 
the advanced language processing techniques de- 
veloped in the realm of computational linguis- 
tics. Another is GALAXY-II (Seneffet al, 1998), 
*Mikio Nakano is currently a visiting scientist at MIT 
Laboratory for Computer Science. 
which enables modules in a dialogue system to 
communicate with each other. It consists of the 
hub and several servers, such as the speech recog- 
nition server and the natural language server, and 
the hub communicates with these servers. Al- 
though it requires more specifications than finite- 
state-model-based toolkits, it places less limita- 
tions on system functions. 
Our objective is to build robust and real-time 
spoken dialogue systems in different ask do- 
mains. By robust we mean utterance understand- 
ing is robust enough to capture not only utter- 
ances including rammatical errors or self-repairs 
but also utterances that are not clearly segmented 
into sentences by pauses. Real time means the 
system can respond to the user in real time. The 
reason we focus on these features i  that they are 
crucial to the usability of spoken dialogue sys- 
tems as well as to the accuracy of understand- 
ing and appropriateness of the content of the sys- 
tem utterance. Robust understanding allows the 
user to speak to the system in an unrestricted 
way. Responding in real time is important be- 
cause if a system response is delayed, the user 
might think that his/her utterance was not recog- 
nized by the system and make another utterance, 
making the dialogue disorderly. Systems having 
these features hould have several modules that 
work in parallel, and each module needs some 
domain-dependent k owledge sources. Creat- 
ing and maintaining these knowledge sources re- 
quire much effort, thus a toolkit would be help- 
ful. Previous toolkits, however, do not allow us to 
achieve these features, or do not provide mecha- 
nisms that achieve these features without requir- 
ing excessive fforts by the developers. 
This paper presents WIT 1, which is a toolkit 
IWIT is an acronym of Workable spoken dialogue lnter- 
150 
for building spoken dialogue systems that inte- 
grate speech recognition, language understanding 
and generation, and speech output. WIT features 
an incremental understanding method (Nakano et 
al., 1999b) that makes it possible to build a robust 
and real-time system. In addition, WIT compiles 
domain-dependent system specifications into in- 
ternal knowledge sources o that building systems 
is easier. Although WIT requires more domain- 
dependent specifications than finite-state-model- 
based toolkits, WIT-based systems are capable 
of taking full advantage of language processing 
technology. WIT has been implemented and used 
to build several spoken dialogue systems. 
In what follows, we overview WIT, explain its 
architecture, domain-dependent system specifica- 
tions, and implementation, and then discuss its 
advantages and problems. 
2 Overview 
A WIT-based spoken dialogue system has four 
main modules: the speech recognition module, 
the language understanding module, the lan- 
guage generation module, and the speech out- 
put module. These modules exploit domain- 
dependent knowledge sources, which are auto- 
matically generated from the domain-dependent 
system specifications. The relationship among 
the modules, knowledge sources, and specifica- 
tions are depicted in Figure 1. 
WIT can also display and move a human-face- 
like animated agent, which is controlled by the 
speech output module, although this paper does 
not go into details because it focuses only on spo- 
ken dialogue. We also omit the GUI facilities pro- 
vided by WIT. 
3 Architecture of  WIT-Based Spoken 
Dialogue Systems 
Here we explain how the modules in WIT work 
by exploiting domain-dependent k owledge and 
how they interact with each other. 
3.1 Speech Recognition 
The speech recognition module is a phoneme- 
HMM-based speaker-independent continuous 
speech recognizer that incrementally outputs 
face Toolldt. 
word hypotheses. As the recogn/fion engine, 
either VoiceRex, developed by NTI" (Noda et 
al., 1998), or HTK from Entropic Research can 
be used. Acoustic models for HTK is trained 
with the continuous peech database of the 
Acoustical Society of Japan (Kobayashi et al, 
1992). This recognizer incrementally outputs 
word hypotheses a soon as they are found in the 
best-scored path in the forward search (Hirasawa 
et al, 1998) using the ISTAR (Incremental 
Structure Transmitter And Receiver) protocol, 
which conveys word graph information as well as 
word hypotheses. This incremental output allows 
the language understanding module to process 
recognition results before the speech interval 
ends, and thus real-time responses are possible. 
This module continuously runs and outputs 
recognition results when it detects a speech 
interval. This enables the language generation 
module to react immediately touser interruptions 
while the system is speaking. 
The language model for speech recognition 
is a network (regular) grammar, and it allows 
each speech interval to be an arbitrary number 
of phrases. A phrase is a sequence of words, 
which is to be defined in a domain-dependent 
way. Sentences can be decomposed into a cou- 
ple of phrases. The reason we use a repeti- 
tion of phrases instead of a sentence grammar 
for the language model is that the speech recog- 
nition module of a robust spoken dialogue sys- 
tem sometimes has to recognize spontaneously 
spoken utterances, which include self-repairs and 
repetition. In Japanese, bunsetsu is appropriate 
for defining phrases. A bunsetsu consists of one 
content word and a number (possibly zero) of 
function words. In the meeting room reservation 
system we have developed, examples of defined 
phrases are bunsetsu to specify the room to be re- 
served and the time of the reservation and bun- 
setsu to express affirmation and negation. 
When the speech recognition module finds a 
phrase boundary, it sends the category of the 
phrase to the language understanding module, 
and this information is used in the parsing pro- 
cess. 
It is possible to hold multiple language mod- 
els and use any one of them when recogniz- 
ing a speech interval. The language models are 
151 
Semantic \[ 
I ~e  I 
/ specifications \[ r 1 
I R~ae I /L..___..--  - ' - ' -~ ' -  / Ph~e l 
/de~;,i~ions I /" "-. I de~i*~._l 
I Feature I L._.___..~.. -~-'-'-'~ , ( "~ "'. 
L____i----',.,'-... ~ ~ "-.. 
? Surface- I de .~ons  \[:- ,, l ;~ : : - - ?Y \ l  Language II Language I ~Genera f io~n_ /  . I , ~ ,  "., "~"----~__Z_--.--" M . -. i i . ) i . . . . . . .  l,,_J generaraon I ~ \  ,, ~ ~ unaerstanding I I generation IO  t procedures I TM / . . . .  ~ . . . .  I 
\ I I I . . . .  J 
I definitions I '\ \ I word I strings I hypothesea + 
I . - -  I _ __~ '~seto f - " -L__~I  Speech i I~ ,~ L iT -s t  o f - - / L / i . , i s to f  
I ~"  t I ~angu.~ge I- -I ~t io .  I I ou~u, r ' - - ' \ ]  pre-r~o.dedr'--\] pre-r~o~ed I 
I d~f~i~23~_l t.models .....~1 I module I I ~oam~ I~Peech m~._J , I L_  j 
user utterance system utterance 
domain-dependent: 
specification knowledge source module 
Figure 1: Architecture of WIT 
switched according to the requests from the lan- 
guage understanding module. In this way, the 
speech recognition success rate is increased by 
using the context of the dialogue. 
Although the current version of WIT does not 
exploit probabilistic language models, such mod- 
els can be incorporated without changing the ba- 
sic WIT architecture. 
3.2 Language Understanding 
The language understanding :module receives 
word hypotheses from the speech recognition 
module and incrementally understands the se- 
quence of the word hypotheses to update the di- 
alogue state, in which the resnlt of understand- 
ing and discourse information are represented 
by a frame (i.e., attribute-value pairs). The un- 
derstanding module utilizes ISSS (Incremental 
Significant-utterance Sequence Search) (Nakano 
et al, 1999b), which is an integrated parsing and 
discourse processing method. ISSS enables the 
incremental understanding of user utterances that 
are not segmented into sentences prior to pars- 
ing by incrementally finding the most plausible 
sequence of sentences (or significant utterances 
in the ISSS terms) out of the possible sentence 
sequences for the input word sequence. ISSS 
also makes it possible for the language generation 
module to respond in real time because it can out- 
put a partial result of understanding at any point 
in time. 
The domain-dependent knowledge used in this 
module consists of a unification-based lexicon 
and phrase structure rules. Disjunctive feature 
descriptions are also possible; WIT incorporates 
an efficient method for handling disjunctions 
(Nakano, 1991). When a phrase boundary is de- 
tected, the feature structure for a phrase is com- 
puted using some built-in rules from the feature 
structure rules for the words in the phrase. The 
phrase structure rules specify what kind of phrase 
sequences can be considered as sentences, and 
they also enable computing the semantic repre- 
sentation for found sentences. Two kinds of sen- 
tenees can be considered; domain-related ones 
that express the user's intention about he reser- 
152 
vafion and dialogue-related ones that express the 
user's attitude with respect to the progress of the 
dialogue, such as confirmation and denial. Con- 
sidering the meeting room reservation system, ex- 
amples of domain-related sentences are "I need to 
book Room 2 on Wednesday", I need to book 
Room 2", and "Room 2" and dialogue-related 
ones are "yes", "no", and "Okay". 
The semantic representation for a sentence is
a command for updatingthe dialogue state. The 
dialogue state is represented bya list of attribute- 
value pairs. For example, attributes used in the 
meeting room reservation system include task- 
related attributes, such as the date and time of 
the reservation, as well as attributes that represent 
discourse-related information, such as confirma- 
tion and grounding. 
3.3 Language Generation 
How the language generation module works 
varies depending on whether the user or system 
has the initiative of turn taking in the dialogue 2.
Precisely speaking, the participant having the ini- 
tiative is the one the system assumes has it in the 
dialogue. 
The domain-dependent k owledge used by the 
language generation module is generation proce- 
dures, which consist of a set of dialogue-phase 
definitions. For each dialogue phase, an initial 
function, an action function, a time-out function, 
and a language model are assigned. In addition, 
phase definitions designate whether the user or 
the system has the initiative. In the phases in 
which the system has the initiative, only the ini- 
tial function and the language model are assigned. 
The meeting room reservation system, for exam- 
ple, has three phases: the phase in which the 
user tells the system his/her equest, he phase in 
which the system confirms it, and the phase in 
which the system tells the user the result of the 
database access. In the first two phases, the user 
holds the initiative, and in the last phase, the sys- 
tern holds the initiative. 
Functions defined here decide what string 
should be spoken and send that string to the 
speech output module based on the current di- 
alogue state. They can also shift the dialogue 
2The notion of the initiative inthis paper isdifferent from 
that of the dialogue initiative of Chu-Carroll (2000). 
phase and change the holder of the initiative as 
well as change the dialogue state. When the dia- 
logue phase shifts, the language model foi" speech 
recognition is changed to get better speech recog- 
nition performance. Typically, the language gen- 
eration module is responsible for database access. 
The language generation module works as fol- 
lows. It first checks which dialogue participant 
has the initiative. If the initiative is held by the 
user, it waits until the user's speech interval ends 
or a duration of silence after the end of a system 
utterance is detected. The action function in the 
dialogue phase at that point in time is executed in
the former case; the time-out function is executed 
in the latter case. Then it goes back to the initial 
stage. If the system holds the initiative, the mod- 
ule executes the initial function of the phase. In 
typical question-answer systems, the user has the 
initiative when asking questions and the system 
has it when answering. 
Since the language generation module works in 
parallel with the language understanding module, 
utterance generation is possible even while the 
system is listening to user utterances and that ut- 
terance understanding is possible even while it is 
speaking (Nakano et al, 1999a). Thus the system 
can respond immediately after user pauses when 
the user has the initiative. When the system holds 
the initiative, it can immediately react to an in- 
terruption by the user because user utterances are 
understood in an incremental way (Dohsaka nd 
Shimazu, 1997). 
The time-out function is effective in moving 
the dialogue forward when the dialogue gets 
stuck for some reason. For example, the system 
may be able to repeat the same question with an- 
other expression and may also be able to ask the 
user a more specific question. 
3.4 Speech Output 
The speech output module produces peech ac- 
cording to the requests from the language gener- 
ation module by using the correspondence table 
between strings and pre-recorded speech data. It 
also notifies the language generation module that 
speech output has finished so that the language 
generation module can take into account the tim- 
ing of the end of system utterance. The meeting 
room reservation system uses speech files of short 
153 
phrases. 
4 Building Spoken Dialo~te Systems 
with WIT  
4.1 Domain-Dependent System 
Specifications 
Spoken dialogue systems can be built with WIT 
by preparing several domain-dependent specifica- 
tions. Below we explain the specifications. 
Feature Definitions: Feature definitions pec- 
ify the set of features used in the grammar for lan- 
guage understanding. They also specify whether 
each feature is a head feature or a foot feature 
(Pollard and Sag, 1994). This information isused 
when constructing feature structures for phrases 
in a built-in process. 
The following is an example of a feature defini- 
tion. Here we use examples from the specification 
of the meeting room reservation system. 
(case head) 
It means that the case feature is used and it is a 
head feature 3.
Lexieal Descriptions: Lexical descriptions 
specify both pronunciations and grammatical 
features for words. Below is an example lexical 
item for the word 1-gatsu (January). 
(l-gatsu ichigatsu month nil i) 
The first three elements are the identifier, the pro- 
nunciation, and the grammatical category of the 
word. The remaining two elements are the case 
and semantic feature values. 
Phrase Definitions: Phrase definitions pecify 
what kind of word sequence can be recognized 
as a phrase. Each definition is a pair compris- 
ing a phrase category name and a network of 
word categories. In the example below, month-  
phrase is the phrase category name and the re- 
maining part is the network of word categories. 
opt  means an option and or  means a disjunc- 
tion. For instance, a word sequence that con- 
sists of a word in the month  category, such as 1- 
gatsu (January), and a word in the adraon ina l -  
par t i c le  category, such as no (of), forms a 
phrase in the month-phrase  category. 
3In this section, we use examples of different description 
from the actual ones for simplicity. Actual specifications are 
written in part in Japanese. 
(month-phrase 
(month 
(opt 
(or 
expression-following-subject 
(admoninal-particle 
(opt 
sentence-final-particle)))))) 
Network Definitions: Network definitions 
specify what kind of phrases can be included in 
each language model. Each definition is a pair 
comprising a network name and a set of phrase 
category names. 
Semantic-Frame Specifications: The result of 
understanding and dialogue history can be stored 
in the dialogue state, which is represented by a 
flat frame structure, i.e., a set of attribute-value 
pairs. Semantic-frame specifications define the 
attributes used in the frame. The meeting room 
reservation system uses task-related attributes. 
Two are s tar t  and end, which represent the 
user's intention about the start and end times of 
the reservation for some meeting room. It also 
has attributes that represent discourse informa- 
tion. One is conf i rmed,  whose value indicates 
whether if the system has already made an utter- 
ance to confirm the content of the task-related at- 
tributes. 
Rule Definitions: Each rule has one of the fol- 
lowing two forms. 
((rule name) 
(child feature structure) 
? . .  (child feature structure) 
=> (mother feature structu_e) 
(priority increase) ) 
((role name) 
(child feature structure) 
? . .  (child feature structure) 
=> (flame operation command) 
(priority increase) ) 
These roles are similar to DCG (Pereira nd War- 
ren, 1980) rules; they can include logical vari- 
ables and these variables can be bound when 
these rules are applied. It is possible to add to the 
rules constraints that stipulate relationships that 
must hold among variables (Nakano, 199 I), but 
we do not explain these constraints indetail in this 
154 
paper. The priorities are used for disambiguat- 
ing interpretation i  the incremental understand- 
ing method (Nakano et al, 1999b). 
When the command on the right-hand side of 
the arrow is a frame operation command, phrases 
to which this rule can be applied can be consid- 
ered a sentence, and the sentence's semantic rep- 
resentation is the command for updating the dia- 
logue state. The command is one of the follow- 
ing: 
? A command to set the value of an attribute 
of the frame, 
? A command to increase the priority, 
Conditional commands (If-then-else type 
command, the condition being whether the 
value of an attribute of the flame is or is not 
equal to a specified value, or a conjunction 
or disjunction of the above condition), or 
? A list of commands to be sequentially exe- 
cuted. 
Thanks to conditional commands, it is possible 
to represent the semantics of sentences context- 
dependently. 
The following rule is an example. 
( s ta r t -end- t imes-command 
( t ime-phrase  : f rom *start) 
( t ime-phrase  (:or :to nil) *end) 
=> (command (set :start *start) 
(set :end *end))) 
The name of this rule is s ta r t -end- t imes-  
command. The second and third elements 
are child feature structures. In these elements, 
t ime-phrase  is a phrase category, : f rom and 
( : or : to n i l  ) are case feature values, and 
*s tar t  and *end are semantic feature val- 
ues. Here :or means a disjunction, and sym- 
bols starting with an asterisk are variables. The 
right-hand side of the arrow is a command to up- 
date the frame. The second element of the com- 
mand, (set :start  *start), changes the 
: s ta r t  atttribute value of the frame to the in- 
stance of *s tar t ,  which should be bound when 
applying this rule to the child feature structures. 
Phase Definitions: Each phase definition con- 
sists of a phase name, a network name, an ini- 
tiative holder specification, an initial function, an 
action function, a maximum silence duration, and 
a time-out function. The network name is the 
identifier of the language model for the speech 
recognition. The maximum silence duration spec- 
ifies how long the generation module should wait 
until the time-out function is invoked. 
Below is an example of a phase definition. 
The first element request  is the name of this 
phase, " f ra r_ request"  is the name of the 
network, and move- to - reques  t -phase  and 
request -phase-act ion  are the names of 
the initial and action functions. In this phase, 
the maximum silence duration is ten seconds and 
the name of the time-out function is request -  
phas e- t imeou t. 
(request " fmr_request"  
move-  to - reques  t -phase  
request -phase-act ion  
10.0 
request -phase-  t imeout  ) 
For the definitions of these functions, WIT pro- 
vides functions for accessing the dialogue state, 
sending a request o speak to the speech out- 
put module, generating strings to be spoken us- 
ing surface generation templates, hifting the di- 
alogue phase, taking and releasing the initiative, 
and so on. Functions are defined in terms of the 
Common Lisp program. 
Surface-generation Templates: Surface- 
generation templates are used by the surface 
generation library function, which converts 
a list-structured semantic representation to a 
sequence of strings. Each string can be spoken, 
i.e., it is in the list of pre-recorded speech files. 
For example, let us consider the conversion 
of the semantic representation (date  (date -  
express ion  3 15) ) to strings using the fol- 
lowing template. 
( (date 
(date -express ion  *month  *day)) 
( (*month gatsu) (*day nichi)  ) ) 
The surface generation library function matches 
the input semantic representation with the first el- 
ement of the template and checks if a sequences 
155 
of strings appear in the speech file list. It re- 
turns ( '  '3gagsu l5n ich i ' ' )  (March 15th) 
if the string "3gatsul5nichi" s in the list of 
pre-recorded speech files, and otherwise, returns 
( ' ' 3gatsu  . . . .  15n ich i '  ' ) when these 
strings are in the list. 
List of Pre-recorded Speech Files: The list of 
pre-recorded speech files should show the corre- 
spondence between strings and speech files to be 
played by the speech output module. 
4.2 Compiling System Specifications 
From the specifications explained above, domain- 
dependent knowledge sources are created as indi- 
cated by the dashed arrows in Figure 1. When cre- 
ating the knowledge sources, WIT checks for sev- 
eral kinds of consistency. For example, the set of 
word categories appearing in the lexicon and the 
set of word categories appearing in phrase deft- 
nifions are compared. This makes it easy to find 
errors in the domain specifications. 
5 Implementation 
WIT has been implemented in Common Lisp and 
C on UNIX, and we have built several experi- 
mental and demonstration dialogue systems using 
it, including a meeting room reservation system 
(Nakano et al, 1999b), a video-recording pro- 
gramming system, a schedule management sys- 
tem (Nakano et al, 1999a), and a weather in- 
formation system (Dohsaka et al, 2000). The 
meeting room reservation system has vocabulary 
of about 140 words, around 40 phrase structure 
rules, nine attributes in the semantic frame, and 
around 100 speech files. A sample dialogue be- 
tween this system and a naive user is shown 
in Figure 2. This system employs HTK as the 
speech recognition engine. The weather informa- 
tion system can answer the user's questions about 
weather forecasts in Japan. The vocabulary size 
is around 500, and the number of phrase structure 
rules is 31. The number of attributes in the se- 
mantic flame is 11, and the number of the files of 
the pre-recorded speech is about 13,000. 
6 Discussion 
As explained above, the architecture of WIT al- 
lows us to develop a system that can use utter- 
ances that are not clearly segmented into sen- 
tences by pauses and respond in real time. Below 
we discuss other advantages and remaining prob- 
lems. 
6.1 Descriptive Power 
Whereas previous finite-state-model-based tool- 
kits place many severe restrictions on domain de- 
scriptions, WIT has enough descriptive power to 
build a variety of dialogue systems. Although the 
dialogue state is represented bya simple attribute- 
value matrix, since there is no limitation on the 
number of attributes, it can hold more compli- 
cated information. For example, it is possible to 
represent a discourse stack whose depth is lim- 
ited. Recording some dialogue history is also 
possible. Since the language understanding mod- 
ule utilizes unification, a wide variety of lin- 
guistic phenomena can be covered. For exam- 
ple, speech repairs, particle omission, and fillers 
can be dealt with in the framework of unifica- 
tion grammar (Nakano et al, 1994; Nakano and 
Shimazu, 1999). The language generation mod- 
ule features Common Lisp functions, so there is 
no limitation on the description. Some of the 
systems we have developed feature a generation 
method based on hierarchical planning (Dohsaka 
and Shirnazu, 1997). It is also possible to build a 
simple finite-state-model-based dialogue system 
using WIT. States can be represented bydialogue 
phases in WIT. 
6.2 Consistency 
In an agglutinative language such as Japanese, 
there is no established definition of words, so dia- 
logue system developers must define words. This 
sometimes causes a problem in that the defini- 
tion of word, that is, the word boundaries, in the 
speech recognition module are different from that 
in the language understanding module. In WIT, 
however, since the common lexicon is used in 
both the speech recognition module and language 
understanding module, the consistency between 
them is maintained. 
6-3 Avoiding Information Loss 
In ordinary spoken language systems, the speech 
recognition module sends just a word hypoth- 
esis to the language processing module, which 
156 
speaker start end utterance 
time (s) time (s) 
system: 614.53 615.93 
user: 616.38 618.29 
system: 619.97 620.13 
user: 622.65 624.08 
system: 625.68 625.91 
user: 626.65 627.78 
system: 629.25 629.55 
user: 629.91 631.67 
system: 633.29 633.57 
user: 634.95 636.00 
system: 637.50 645.43 
user: 645.74 646.04 
system: 647.05 648.20 
Figure 2: 
donoy6na goy6ken desh6 ka (how may I help you?) 
kaigishitsu oyoyaku shitai ndesu ga (I'd like to make a reserva- 
tion for a meeting room) 
hai (uh-huh) 
san-gatsujfini-nichi (on March 12th) 
hal (uh-huh) 
jayo-ji kara (from 14:00) 
hai (uh-huh) 
jashichi-ji sanjup-pun made (to 17:30) 
hai (uh-huh) 
dai-kaigishitsu (the large meeting room) 
san-gatsu jani-nichi, j~yo-ji kara, jashichi-ji sanjup-pun made, 
dai-kaigishitsu toyfi koto de yoroshf deshrka (on March 12th, 
from 14:00 to 17:30, the large meeting room, is that right?) " 
hai (yes) 
kashikomarimashitd (allright) 
An example dialogue of an example system 
must disambiguate word meaning and find phrase 
boundaries by parsing. In contrast, the speech 
recognition module in WIT sends not only words 
but also word categories, phrase boundaries, and 
phrase categories. This leads to less expensive 
and better language understanding. 
6.4 Problems and Limitations 
Several problems remain with WIT. One of the 
most significant is that he system developer must 
write language generation functions. If the gen- 
eration functions employ sophisticated dialogue 
strategies, the system can perform complicated 
dialogues that are not just question answering. 
WIT, however, does not provide task-independent 
facilities that make it easier to employ such dia- 
logue strategies. 
There have been several efforts aimed at de- 
veloping a domain-independent me hod for gen- 
erating responses from a frame representation f 
user requests (Bobrow et al, 1977; Chu-CarroU, 
1999). Incorporating such techniques would deo 
crease the system developer workload. However, 
there has been no work on domain-independent 
response generation for robust spoken dialogue 
systems that can deal with utterances that might 
include pauses in the middle of a sentence, which 
WIT handles well. Therefore incorporating those 
techniques remains as a future work. 
Another limitation is that WIT cannot deal with 
multiple speech recognition candidates such as 
those in an N-best list. Extending WIT to deal 
with multiple recognition results would improve 
the performance of the whole system. The ISSS 
preference mechanism is expected to play a role 
in choosing the best recognition result. 
7 Conclusion 
This paper described WIT, a toolkit for build- 
ing spoken dialogue systems. Although it re- 
quires more system specifications than previous 
finite-state-model-based toolkits, it enables one 
to easily construct real-time, robust spoken dia- 
logue systems that incorporates advanced compu- 
tational linguistics technologies. 
Acknowledgements 
The authors thank Drs. Ken'ichiro Ishii, Nori- 
hiro Hagita, and Takeshi Kawabata for their sup- 
port of this research. Thanks also go to Tetsuya 
Kubota, Ryoko Kima, and the members of the 
Dialogue Understanding Research Group. We 
used the speech recognition engine VoiceRex de- 
veloped by NTT Cyber Space Laboratories and 
thank those who helped us use it. Comments by 
157 
the anonymous reviewers were of' great help. 
References 
James F. Allen, Bradford W. Miller, Eric K. Ringger, 
and Teresa Sikorski. 1996. A robust system for nat- 
ural spoken dialogue. In Proceedings of the 34th 
Annual Meeting of the Association for Computa- 
tional Linguistics (A CL-96), pages 62-70. 
Harald Aust, Martin Oerder, Frank Seide, and Volker 
Steinbiss. 1995. The Philips automatic train 
timetable information system. Speech Communi- 
cation, 17:249--262. 
James Barnett and Mona Singh. 1997. Designing 
a portable spoken language system. In Elisabeth 
Maier, Marion Mast, and Susann LuperFoy, editors, 
Dialogue Processing inSpoken Language Systems, 
pages 156--170. Springer-Vedag. 
Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, 
Dona!d A. Norman, Henry Thompson, and Terry 
Winograd. 1977. GUS, a frame driven dialog sys- 
tem. Arnficial Intelligence, 8:155-173. 
Jennifer Chu-Carroll. 1999. Fo:rrn-based reason- 
ing for mixed-initiative dialogue management in 
information-query systems. In Proceedings of the 
Sixth European Conference on Speech Communica- 
tion and Technology (Eurospeech-99) , pages 1519- 
1522. 
Junnifer Chu-Carroll. 2000. MIMIC: An adaptive 
mixed initiative spoken dialogue system for infor- 
mation queries. In Proceedings of the 6th Con- 
f~rence on Applied Natural Language Processing 
(ANLP-O0), pages 97-104. 
Kohji Dohsaka nd Akira Shimazu. 1997. System ar- 
chitecture for spoken utterance production in col- 
laborative dialogue. In Working Notes of IJCAI 
1997 Workshop on Collaboration, Cooperation and 
Conflict in Dialogue Systems. 
Kohji Dohsaka, Norihito Yasuda, Noboru Miyazaki, 
Mikio Nakano, and Kiyoaki AJkawa. 2000. An ef- 
ficient dialogue control method under system's lim- 
ited knowledge. In Proceedings of the Sixth Inter- 
national Conference on Spoken Language Process- 
ing (ICSLP-O0). 
Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano, 
and Takeshi Kawabata. 1998. Implementation 
of coordinative nodding behavior on spoken dia- 
logue systems. In Proceedings of the Fgth Interna- 
tional Conference on Spoken Language Processing 
(1CSLP-98), pages 2347-2350. 
Tetsunod Kobayashi, Shuichi Itahashi, Satoru 
Hayamizu, and Toshiyuki Takezawa. 1992. Asj 
continuous speech corpus for research. The journal 
of th e Acoustical Society of Japan, 48(12): 888-893. 
Mikio Nakano and Akira Shimazu. 1999. Pars- 
ing utterances including self-repairs. In Yorick 
Wilks, editor, Machine Conversations, pages 99- 
112. Kluwer Academic Publishers. 
Mikio Nakano, Aldra Shimazu, and Kiyoshi Kogure. 
1994. A grammar and a parser for spontaneous 
speech. In Proceedings of the 15th Interna- 
tional Conference on Computational Linguistics 
(COLING-94), pages 1014-1020. 
Mildo Nakano, Kohji Dohsaka, Noboru Miyazald, 
Inn ichi Hirasawa, Masafiami Tamoto, Masahito 
Kawarnon, Akira Sugiyama, and Takeshi Kawa- 
bata. 1999a. Handling rich turn-taking in spoken 
dialogue systems. In Proceedings of the Sixth Eu- 
ropean Conference on Speech Communication a d 
Technology (Eurospeech-99), pages 1167-1170. 
Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa, 
Kohji Dohsaka, and Takeshi Kawabata. 1999b. 
Understanding unsegmented user utterances in real- 
time spoken dialogue systems. In Proceedings of 
the 37th Annual Meeting of the Association for 
Computational Linguistics (ACL-99), pages 200-- 
207. 
Mikio Nakano. 1991. Constraint projection: An ef- 
ficient treatment of disjunctive f ature descriptions. 
In Proceedings of the 29th Annual Meeting of the 
Association for Computational Linguistics (ACL- 
90, pages 307-314. 
Yoshiaki Noda, Yoshikazu Yamaguchi, Tomokazu 
Yamada, Akihiro Imamura, Satoshi Takahashi, 
Tomoko Matsui, and Kiyoaki Aikawa. 1998. The 
development of speech recognition engine REX. In 
Proceedings of the 1998 1EICE General Confer- 
ence D-14-9, page 220. (in Japanese). 
Fernando C. N. Pereira and David H. D. Warren. 
1980. Definite clause grammars for language 
analysis--a survey of the formalism and a compar- 
ison with augmented transition etworks. Artificial 
Intelligence, 13:231-278. 
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar. CSLI, Stanford. 
Munehiko Sasajima, Yakehide Yano, and Yasuyuki 
Kono. 1999. EUROPA: A genetic framework for 
developing spoken dialogue systems. In Proceed- 
ings of the Sixth European Conference on Speech 
Communication a d Technology (Eurospeech-99), 
pages 1163--1166. 
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris- 
fine Pao, Philipp Sehmid, and Victor Zue. 1998. 
GALAXY-H: A reference architecture for conver- 
sational system development. In Proceedings of 
158 
the Fifth International Con l~rence on Spoken Lan- 
guage Processing (ICSLP-98). 
Stephen Sutton, Ronaid A. Cole, Jacques de Villiers, 
Johan SchMkwyk, Pieter Vermeulen, Michael W. 
Macon, Yonghong Yah, Edward Kaiser, Brian Run- 
die, K.haldoun Shobaki, Paul Hosom, Alex Kain, 
Johan Wouters, Dominic W. Massaro, and Michael 
Cohen. 1998. Universal speech tools: The 
CSLU toolkit. In Proceedings of the Fifth Interna- 
tional Conference on Spoken Language Processing 
(1CSLP-98), pages 3221-3224. 
Marilyn Walker, Irene Langkilde, Jerry Wright, Allen 
Gorin, and Diane Litman. 2000. Learning to pre- 
dict problematic situations in a spoken dialogue 
system: Experiments with how may I help you? In 
Proceedings of the First Meeting of the North Amer- 
ican Chapter of the Association for Computational 
Linguistics (NAA CL-O0), pages 210--217. 
Victor Zue, Stephanie Seneff, James Glass, Joseph Po- 
lifroni, Christine Pao, Timothy J. Hazen, and Lee 
He~erington. 2000. Jupiter: A telephone-based 
conversational interface for weather information. 
1EEE Transactions on Speech and Audio Process- 
ing, 8(1):85-96. 
159 
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1515?1520,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Single-Document Summarization as a Tree Knapsack Problem
Tsutomu Hirao? Yasuhisa Yoshida? Masaaki Nishino? Norihito Yasuda? Masaaki Nagata?
?NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,yoshida.y,nishino.masaaki,
nagata.masaaki}@lab.ntt.co.jp
? Japan Science and Technology Agency
North 14 West 9, Sapporo, Hokkaido, 060-0814, Japan
yasuda@erato.ist.hokudai.ac.jp
Abstract
Recent studies on extractive text summariza-
tion formulate it as a combinatorial optimiza-
tion problem such as a Knapsack Problem, a
Maximum Coverage Problem or a Budgeted
Median Problem. These methods successfully
improved summarization quality, but they did
not consider the rhetorical relations between
the textual units of a source document. Thus,
summaries generated by these methods may
lack logical coherence. This paper proposes a
single document summarization method based
on the trimming of a discourse tree. This is
a two-fold process. First, we propose rules
for transforming a rhetorical structure theory-
based discourse tree into a dependency-based
discourse tree, which allows us to take a tree-
trimming approach to summarization. Sec-
ond, we formulate the problem of trimming
a dependency-based discourse tree as a Tree
Knapsack Problem, then solve it with integer
linear programming (ILP). Evaluation results
showed that our method improved ROUGE
scores.
1 Introduction
State-of-the-art extractive text summarization meth-
ods regard a document (or a document set) as a set
of textual units (e.g. sentences, clauses, phrases)
and formulate summarization as a combinatorial op-
timization problem, i.e. selecting a subset of the set
of textual units that maximizes an objective with-
out violating a length constraint. For example, Mc-
Donald (2007) formulated text summarization as a
Knapsack Problem, where he selects a set of textual
units that maximize the sum of significance scores
of each unit. Filatova et al (2004) proposed a
summarization method based on a Maximum Cov-
erage Problem, in which they select a set of textual
units that maximizes the weighted sum of the con-
ceptual units (e.g. unigrams) contained in the set.
Although, their greedy solution is only an approxi-
mation, Takamura et al (2009a) extended it to ob-
tain the exact solution. More recently, Takamura et
al. (2009b) regarded summarization as a Budgeted
Median Problem and obtain exact solutions with in-
teger linear programming.
These methods successfully improved ROUGE
(Lin, 2004) scores, but they still have one critical
shortcoming. Since these methods are based on sub-
set selection, the summaries they generate cannot
preserve the rhetorical structure of the textual units
of a source document. Thus, the resulting summary
may lack coherence and may not include significant
textual units from a source document.
One powerful and potential way to overcome the
problem is to include discourse tree constraints in
the summarization procedure. Marcu (1998) re-
garded a document as a Rhetorical Structure The-
ory (RST) (William Charles, Mann and Sandra An-
near, Thompson, 1988)-based discourse tree (RST-
DT) and selected textual units according to a prefer-
ence ranking derived from the tree structure to make
a summary. Daume? et al (2002) proposed a docu-
ment compression method that directly models the
probability of a summary given an RST-DT by us-
ing a noisy-channel model. These methods generate
well-organized summaries, however, since they do
not formulate summarizations as combinatorial op-
1515
Root?
N? S?
N? S? N? S?
S? N? N? S? S? N? S? N?
N? N?
N? S?
Elaboration?
Elaboration?
Background?
Elaboration?
Elaboration?
Contrast? Contrast?
Evidence?
Example?
Concession? Antithesis?
Figure 1: Example RST-DT from (Marcu, 1998).
timization problems, the optimality of the generated
summaries is not guaranteed.
In this paper, we propose a single document sum-
marization method based on the trimming of a dis-
course tree based on the Tree Knapsack Problem. If
a discourse tree explicitly represents parent-child re-
lationships between textual units, we can apply the
well-known tree-trimming approach to a discourse
tree and reap the benefit of combinatorial optimiza-
tion methods. In other words, to apply the tree-
trimming approach, we need a tree whose all nodes
represent textual units. Unfortunately, the RST-DT
does not allow it, because textual units in the RST-
DT are located only on leaf nodes and parent-child
relationship between textual units are represented
implicitly at higher positions in a tree. Therefore, we
first propose rules that transform an RST-DT into a
dependency-based discourse tree (DEP-DT) that ex-
plicitly defines the parent-child relationships. Sec-
ond, we treat it as a rooted subtree selection, in other
words, a Tree Knapsack Problem and formulate the
problem as an ILP.
2 From RST-DT to DEP-DT
2.1 RST-DT
According to RST, a document is represented as an
RST-DT whose terminal nodes correspond to ele-
mentary discourse units (EDU)s1 and whose non-
terminal nodes indicate the role of the contiguous
1EDUs roughly correspond to clauses.
Root?
N? S?
N? S? N? S?
S? N? N? S? S? N? S? N?
N? N?
N? S?
0	
1	
2	
4	 6	 7	
5	
3	
8	
10	
9	
11	
12	
13	
14	 15	 18	
16	
17	
Figure 2: Heads of non-terminal nodes.
EDUs namely, ?nucleus (N)? or ?satellite (S)?. A nu-
cleus is more important than a satellite in terms of
the writer?s purpose. That is, a satellite is a child of
a nucleus in the RST-DT. Some discourse relations
such as ?Elaboration?, ?Contrast? and ?Evidence? be-
tween a nucleus and a satellite or two nuclei are de-
fined. Figure 1 shows an example of an RST-DT.
2.2 DEP-DT
An RST-DT is not suitable for tree trimming because
it does not always explicitly define parent-child re-
lationships between textual units. For example, if
we consider how to trim the RST-DT in Figure 1,
when we drop e8, we have to drop e7 because of the
parent-child relationship defined between e7 and e8,
i.e. e7 is a satellite (child) of the nucleus (parent)
e8. On the other hand, we cannot judge whether we
have to drop e9 or e10 because the parent-child rela-
tionships are not explicitly defined between e8 and
e9, e8 and e10. This view motivates us to produce a
discourse tree that explicitly defines parent-child re-
lationships and whose root node represents the most
important EDU in a source document. If we can ob-
tain such a tree, it is easy to formulate summariza-
tion as a Tree Knapsack Problem.
To construct a discourse tree that represents
the parent-child relationships between EDUs, we
propose rules for transforming an RST-DT to a
dependency-based discourse tree (DEP-DT). The
procedure is defined as follows:
1. For each non-terminal node excluding the par-
1516
Depth=1
Depth=2
Depth=3
Depth=4
Figure 3: The DEP-DT obtained from the RST-DT in Fig-
ure 1.
ent of an EDU in the RST-DT, we define a
?head?. Here, a ?head? of a non-terminal node
is the leftmost descendant EDUwhose parent is
N. In Figure 2, ?H? indicates the ?head? of each
node.
2. For each EDU whose parent is N, we pick the
nearest S with a ?head? from the EDU?s ances-
tors and we add the EDU to the DEP-DT as a
child of the head of the S?s parent. If there is no
nearest S, the EDU is the root of the DEP-DT.
For example, in Figure 2, the nearest S to e3
that has a head is node 5 and the head of node
5?s parent is e2. Thus, e3 is a child of e2.
3. For each EDU whose parent is S, we pick the
nearest non-terminal with a ?head? from the an-
cestors and we add the EDU to the DEP-DT as
a child of the head of the non-terminal node.
For example, the nearest non-terminal node of
e9 that has a head is node 16 and the head of
node 16 is e10. Thus, e9 is a child of e10.
Figure 3 shows the DEP-DT obtained from the
RST-DT in Figure 1. The DEP-DT expresses the
parent-child relationship between the EDUs. There-
fore, we have to drop e7, e9 and e10 when we drop
e8. Note that, by applying the rules, discourse rela-
tions defined between non-terminals of an RST-DT
are eliminated. However, we believe that these re-
lations are no needed for the summarization that we
are attempting to realize.
3 Tree Knapsack Model for
Single-Document Summarization
3.1 Formalization
We denote T as a set of all possible rooted subtrees
obtained from a DEP-DT. F (t) is the significance
score for a rooted subtree t ? T and L is the maxi-
mum number of words allowed in a summary. The
optimal subtree t? is defined as follows:
t? = argmax
t?T
F (t) (1)
s.t. Length(t) ? L. (2)
Here, we define F (t) as
F (t) =
?
e?E(t)
W(e)
Depth(e)
. (3)
E(t) is the set of EDUs contained in t, Depth(e)
is the depth of an EDU e within the DEP-DT. For
example, Depth(e2) = 1, Depth(e6) = 4 for the
DEP-DT of Figure 3. W(e) is defined as follows:
W(e) =
?
w?W (e)
tf(w,D). (4)
W (e) is the set of words contained in e and
tf(w,D) is the term frequency of word w in a docu-
ment D.
3.2 ILP Formulation
We formulate the optimization problem in the pre-
vious section as a Tree Knapsack Problem, which is
a kind of Precedence-Constrained Knapsack Prob-
lem (Samphaiboon and Yamada, 2000) and we can
obtain the optimal rooted subtree by solving the fol-
lowing ILP problem2:
maximize
x
N
?
i=1
W(ei)
Depth(ei)
xi (5)
s.t.
N
?
i=1
?ixi ? L (6)
?i : xparent(i) ? xi (7)
?i : xi ? {0, 1}, (8)
2A similar approach has been applied to sentence compres-
sion (Filippova and Strube, 2008).
1517
ROUGE-1 ROUGE-2
F R F R
TKP(G) .310H,K,L .321G,H,K,L .108 .112H
TKP(H) .281H .284H .092 .093
Marcu(G) .291H .272H .101 .093
Marcu(H) .236 .219 .073 .068
MCP .279 .295H .073 .077
KP .251 .266H .071 .075
LEAD .255 .240 .092 .086
Table 1: ROUGE scores of the RST discourse treebank
dataset. In the table, G,H,K,L indicate a method sta-
tistically significant against Marcu(G), Marcu(H), KP,
LEAD, respectively.
where x is an N -dimensional binary vector that
represents the summary, i.e. xi=1 denotes that the i-
th EDU is included in the summary. N is the number
of EDUs in a document, ?i is the length (the number
of words) of the i-th EDU, and parent(i) indicates
the ID of the parent of the i-th EDU in the DEP-DT.
Constraint (6) ensures that the length of a summary
is less than limit L. Constraint (7) ensures that a
summary is a rooted subtree of the DEP-DT. Thus,
xparent(i) is always 1 when the i-th EDU is included
in the summary.
In general, the Tree Knapsack Problem is NP-
hard, but fortunately we can obtain the optimal solu-
tion in a feasible time by using ILP solvers for doc-
uments of practical tree size. In addition, bottom-
up DP (Lukes, 1974) and depth-first DP algorithms
(Cho and Shaw, 1997) are known to find the optimal
solution efficiently.
4 Experimental Evaluation
4.1 Settings
We conducted an experimental evaluation on the test
collection for single document summarization eval-
uation contained in the RST Discourse Treebank
(RST-DTB)(Carlson et al, 2001) distributed by the
Linguistic Data Consortium (LDC)3. The RST-DTB
Corpus includes 385 Wall Street Journal articles
with RST annotation, and 30 of these documents
also have one human-made reference summary. The
average length of the reference summaries corre-
sponds to about 10 % of the words in the source
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2002T07
document.
We compared our method (TKP) with Marcu?s
method (Marcu) (Marcu, 1998), a simple knapsack
model (KP), a maximum coverage model (MCP)
and a lead method (LEAD). MCP is known to be a
state-of-the-art method for multiple document sum-
marization and we believe that MCP also performs
well in terms of single document summarization.
LEAD is also a widely used summarizer that simply
takes the first K textual units of the document. Al-
though this is a simple heuristic rule, it is known as
a state-of-the-art summarizer (Nenkova and McKe-
own, 2011).
For our method, we examined two types of
DEP-DT. One was obtained from the gold RST-
DT. The other was obtained from the RST-DT pro-
duced by a state-of-the-art RST parser, HILDA (du-
Verle and Prendinger, 2009; Hernault et al, 2010).
For Marcu?s method, we examined both the gold
RST-DT and HILDA?s RST-DT. We re-implemented
HILDA and re-trained it on the RST-DT Corpus ex-
cluding the 30 documents used in the evaluation.
The F-score of the parser was around 0.5. For KP,
we exclude constraint (7) from the ILP formulation
of TKP and set the depth of all EDUs in equations
(3) and (5) at 1. For MCP, we use tf (equation (4))
as the word weight.
We evaluated the summarization systems with
ROUGE version 1.5.5 4. Performance metrics were
the recall (R) and F-score (F) of ROUGE-1,2.
4.2 Results and Discussion
Table 1 shows the evaluation results. In the ta-
ble, TKP(G) and TKP(H) denote methods with the
DEP-DT obtained from the gold RST-DT and from
HILDA, respectively. Marcu(G) and Marcu(H) de-
note Marcu?s method described in (Marcu, 1998)
with gold RST-DT and with HILDA, respectively.
We performed a multiple comparison test for the dif-
ferences among ROUGE scores, we calculated the p-
values between systems with the Wilcoxon signed-
rank test (Wilcoxon, 1945) and used the False Dis-
covery Rate (FDR) (Benjamini and Hochberg, 1995)
to calculate adjusted p-values, in order to limit false
positive rate to 5%.
From the table, TKP(G) and Marcu(G) achieved
4Options used: -n 2 -s -m -x
1518
Reference:
The Fuji apple may one day replace the Red Delicious as the number one U.S. apple. Since the Red Delicious has been
over-planted and prices have dropped to new lows, the apple industry seems ready for change. Along with growers, supermarkets
are also trying different varieties of apples. Although the Fuji is smaller and not as perfectly shaped as the Red Delicious, it is
much sweeter, less mealy and has a longer shelf life.
TKP(G):
We?ll still have mom and apple pie. A Japanese apple called the Fuji. Some fruit visionaries say the Fuji could someday tumble
the Red Delicious from the top of America?s apple heap. It has a long shelf life. Now, even more radical changes seem afoot. The
Delicious hegemony won?t end anytime soon. New apple trees grow slowly. But the apple industry is ripe for change. There?s a
Fuji apple cult.
Marcu(G):
We?ll still have mom and apple pie. On second thought, make that just mom. The Fuji could someday tumble the Red Delicious
from the top of America?s apple heap. Now, even more radical changes seem afoot. The Delicious hegemony won?t end anytime
soon. More than twice as many Red Delicious apples are grown as the Golden variety, America?s No. 2 apple. But the apple
industry is ripe for change.
MCP:
Called the Fuji. It has a long shelf life. New apple trees grow slowly. Its roots are patriotic. I?m going to have to get another job
this year. Scowls. They still buy apples mainly for big, red good looks. Japanese researchers have bred dozens of strains of Fujis.
Mr. Auvil, the Washington grower, says. Stores sell in summer. The ? big boss ? at a supermarket chain even rejected his Red
Delicious recently. Many growers employ.
LEAD:
Soichiro Honda?s picture now hangs with Henry Ford?s in the U.S. Automotive Hall of Fame, and the game-show ? Jeopardy ? is
soon to be Sony-owned. But no matter how much Japan gets under our skin, we?ll still have mom and apple pie. On second
thought, make that just mom. A Japanese apple called the Fuji is cropping up in orchards the way Hondas did on U.S. roads.
Figure 4: Summaries obtained from wsj 1128.
better results than MCP, KP and LEAD, although
some of the comparisons are not significant. In par-
ticular, TKP(G) achieved the highest ROUGE scores
on all measures. On ROUGE-1 Recall, TKP(G) sig-
nificantly outperformed Marcu(G), Marcu(H), KP
and LEAD. These results support the effectiveness
of our method that utilizes the discourse structure.
Comparing TKP(H) with Marcu(H), the former
achieved higher scores with statistical significance
on ROUGE-1. In addition, Marcu(H) was outper-
formed by MCP, KP and LEAD. The results confirm
the effectiveness of our summarization model and
trimming proposal for DEP-DT. Moreover, the dif-
ference between TKP(G) and TKP(H) was smaller
than that between Marcu(G) and Marcu(H). This
implies that our method is more robust against dis-
course parser error than Marcu?s method.
Figure 4 shows the example summaries gener-
ated by TKP(G), Marcu(G), MCP and LEAD, re-
spectively for an article, wsj 1128. Since TKP(G)
and Marcu(G) utilize a discourse tree, the summary
generated by TKP(G) is similar to that generated by
Marcu(G) but it is different from those generated by
MCP and LEAD.
5 Conclusion
This paper proposed rules for transforming an RST-
DT to a DEP-DT to obtain the parent-child relation-
ships between EDUs. We treated a single document
summarization method as a Tree Knapsack Problem,
i.e. the summarizer selects the best rooted subtree
from a DEP-DT. To demonstrate the effectiveness of
our method, we conducted an experimental evalua-
tion using 30 documents selected from the RST Dis-
course Treebank Corpus. The results showed that
our method achieved the highest ROUGE-1,2 scores.
References
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: A practical and powerful
approach to multiple testing. Journal of the Royal Sta-
tistical Society, Series B (Methodological), 57(1):289?
300.
Lynn Carlson, Daniel Marcu, andMary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proc. of
the SIGDIAL01, pages 1?10.
Geon Cho and Dong X Shaw. 1997. A depth-first
dynamic programming algorithm for the tree knap-
1519
sack problem. INFORMS Journal on Computing,
9(4):431?438.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proc. of the 40th
ACL, pages 449?456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine clas-
sification. In Proc. of the Joint Conference of the 47th
ACL and 4th IJCNLP, pages 665?673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence extraction. In Proc. of the 20th COLING,
pages 397?403.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proc. of the 5th
International Natural Language Generation Confer-
ence (INLG), pages 25?32.
Hugo Hernault, Helmut Prendinger, David A duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification. Di-
alogue and Discourse, 1(3):1?33.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Workshop on
Text Summarization Branches Out, pages 74?81.
J. A. Lukes. 1974. Efficient algorithm for the partition-
ing of trees. IBM Journal of Research and Develop-
ment, 18(3):217?224.
Daniel Marcu. 1998. Improving summarization through
rhetorical parsing tuning. In Proc. of the 6th Workshop
on Very Large Corpora, pages 206?215.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Proc.
of the 29th ECIR, pages 557?564.
Ani Nenkova and Kathaleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in In-
formation Retrieval, 5(2-3):103?233.
Natthawut Samphaiboon and Takeo Yamada. 2000.
Heuristic and exact algorithms for the precedence-
constrained knapsack problem. Journal of Optimiza-
tion Theory and Applications, 105(3):659?676.
Hiroya Takamura and Manabu Okumura. 2009a. Text
summarization model based on maximum coverage
problem and its variant. In Proc. of the 12th EACL,
pages 781?789.
Hiroya Takamura and Manabu Okumura. 2009b. Text
summarization model based on the budgeted median
problem. In Proceedings of the 18th CIKM.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80?83.
William Charles, Mann and Sandra Annear, Thompson.
1988. Rhetorical Structure Theory: Toward a func-
tional theory of text organization. Text, 8(3):243?281.
1520
