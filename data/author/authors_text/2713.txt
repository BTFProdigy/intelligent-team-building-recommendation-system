Representing discourse coherence: A corpus-based analysis 
Florian WOLF 
MIT NE20-448 
Cambridge, MA 02139, USA 
fwolf@mit.edu 
Edward GIBSON 
MIT NE20-459 
Cambridge, MA, 02139, USA 
egibson@mit.edu 
 
Abstract 
We present a set of discourse structure relations 
that are easy to code, and develop criteria for an 
appropriate data structure for representing these 
relations.  Discourse structure here refers to 
informational relations that hold between sentences 
in a discourse (cf. Hobbs, 1985).  We evaluated 
whether trees are a descriptively adequate data 
structure for representing coherence.  Trees are 
widely assumed as a data structure for representing 
coherence but we found that more powerful data 
structures are needed: In coherence structures of 
naturally occurring texts, we found many different 
kinds of crossed dependencies, as well as many 
nodes with multiple parents.  The claims are 
supported by statistical results from a database of 
135 texts from the Wall Street Journal and the AP 
Newswire that were hand-annotated with 
coherence relations, based on the annotation 
schema presented in this paper. 
1 Introduction 
An important component of natural language 
discourse understanding and production is having a 
representation of discourse structure.  A coherently 
structured discourse here is assumed to be a 
collection of sentences that are in some relation to 
each other.  This paper aims to present a set of 
discourse structure relations that are easy to code, 
and to develop criteria for an appropriate data 
structure for representing these relations. 
Discourse structure relations here refer to 
informational relations that hold between sentences 
or other non-overlapping segments in a discourse 
monologue.  That is, discourse structure relations 
reflect how the meaning conveyed by one 
discourse segment relates to the meaning conveyed 
by another discourse segment (cf. Hobbs, 1985; 
Marcu, 2000; Webber et al, 1999). 
Accounts of discourse structure vary greatly with 
respect to how many discourse relations they 
assume, ranging from two (Grosz & Sidner, 1986) 
to over 400 different coherence relations, reported 
in Hovy and Maier (1995).  However, Hovy and 
Maier (1995) argue that taxonomies with more 
relations represent subtypes of taxonomies with 
fewer relations.  This means that different 
taxonomies can be compatible with each other. 
We describe an account with a small number of 
relations in order to achieve more generalizable 
representations of discourse structures; however, 
the number is not so small that informational 
structures that we are interested in are obscured.  
The next section will describe in detail the set of 
coherence relations we use, which are mostly 
based on Hobbs (1985).  Additionally, we try to 
make as few a priori theoretical assumptions about 
representational data structures as possible.  These 
assumptions will be outlined in the next section.  
Importantly, however, we do not assume a tree 
data structure to represent discourse coherence 
structures.  In fact, a major goal of this paper is to 
show that trees do not seem adequate to represent 
discourse structures. 
2 Collecting a database of texts annotated 
with coherence relations 
This section describes (1) how we define 
discourse segments, (2) which coherence relations 
we used to connect the discourse segments, and (3) 
how the annotation procedure worked. 
2.1 Discourse segments 
Discourse segments can be defined as non-
overlapping spans of prosodic units (Hirschberg & 
Nakatani, 1996), intentional units (Grosz & Sidner, 
1986), phrasal units (Lascarides & Asher, 1993), or 
sentences (Hobbs, 1985).  We adopted a sentence 
unit-based definition of discourse segments.  
However, we also assume that contentful 
coordinating and subordinating conjunctions (cf. 
Table 1) can delimit discourse segments. 
2.2 Coherence relations 
We assume a set of coherence relations that is 
similar to that of Hobbs (1985) and Kehler (2002).  
Table 1 shows the coherence relations we assume, 
along with contentful conjunctions that can signal 
the coherence relation. 
 
 
 
 
 
 
cause-effect because 
violated expectation although; but 
condition if?then; as long as 
similarity (and) similarly 
contrast but; however 
elaboration also, furthermore 
attribution ?said, according to? 
temporal sequence before; afterwards 
Table 1.  Coherence relations with contentful 
conjunctions for determining coherence relations. 
 
Below are examples of each coherence relation. 
(1) Cause-Effect 
[There was bad weather at the airport]a [and so 
our flight got delayed.]b 
(2) Violated Expectation 
[The weather was nice]a [but our flight got 
delayed.]b 
(3) Condition 
[If the new software works,]a [everyone will be 
happy.]b 
(4) Similarity 
[There is a train on Platform A.]a [There is 
another train on Platform B.]b 
(5) Contrast 
[John supported Bush]a [but Susan opposed 
him.]b 
(6) Elaboration 
[A probe to Mars was launched this week.]a [The 
European-built ?Mars Express? is scheduled to 
reach Mars by late December.]b 
(7) Attribution 
[John said that]a [the weather would be nice 
tomorrow.]b 
(8) Temporal Sequence 
[Before he went to bed,]a [John took a shower.]b 
The same relation, illustrated by (9), is an 
epiphenomenon of assuming contiguous distinct 
elements of text.  (a) is the first segment and (c) is 
the second segment of what is actually one single 
discourse segment, separated by the intervening 
discourse segment (b), which is in an attribution 
relation with (a) (and therefore also with (c), since 
(a) and (c) are actually one single discourse 
segment). 
(9) Same 
[The economy,]a [according to some analysts,]b 
[is expected to improve by early next year.]c 
Cause-effect, violated expectation, condition, 
elaboration, temporal sequence, and attribution 
are asymmetrical or directed relations, whereas 
similarity, contrast, temporal sequence, and same 
are symmetrical or undirected relations (Mann & 
Thompson, 1988; Marcu, 2000).  The directions of 
asymmetrical or directed relations are as follows: 
cause ? effect for cause-effect; cause ? absent 
effect for violated expectation; condition ? 
consequence for condition; elaborating ? 
elaborated for elaboration, and source ? attributed 
for attribution. 
2.3 Coding procedure 
In order to code the coherence relations of a text, 
annotators used a procedure consisting of three 
steps.  In Step One, a text is segmented into 
discourse segments as described above.  In Step 
Two, adjacent discourse segments that are 
topically related are grouped together.  For 
example, if a text discusses inventions in 
information technology, there could be groups of a 
few discourse segments each talking about 
inventions by specific companies.  There might 
also be subgroups of several discourse segments 
each talking about specific inventions at specific 
companies.  Thus, marking groups determines a 
partially hierarchical structure for the text.  In Step 
Three, coherence relations are determined between 
discourse segments and groups of discourse 
segments.  Each previously unconnected (group of) 
discourse segment(s) is tested to see if it connects 
to any of the (groups of) discourse segments in the 
already existing representation of discourse 
structure. 
In order to help determine the coherence relation 
between (groups of) discourse segments, the 
(groups of) discourse segments under 
consideration are connected with a contentful 
conjunction like the ones shown in Table 1.  If 
using a contentful conjunction to connect (groups 
of) discourse segments results in an acceptable 
passage, this is used as evidence that the coherence 
relation corresponding to the contentful 
conjunction holds between the (groups of) 
discourse segments under consideration. 
2.4 Statistics on annotated database 
In order to evaluate hypotheses about 
appropriate data structures for representing 
coherence structures, we annotated 135 texts, from 
the Wall Street Journal 1987-1989 and the AP 
Newswire 1989 (Harman & Liberman, 1993), with 
the coherence relations described above.  For the 
135 texts, the mean number of words was 545 
(min.: 161; max.: 1409; median: 529), the mean 
number of discourse segments was 61 (min.: 6; 
max.: 143; median: 60). 
Each text was independently annotated by two 
annotators.  In order to determine inter-annotator 
agreement for the database of annotated texts, we 
computed kappa statistics (Carletta, 1996).  For all 
annotations of the 135 texts, the agreement was 
88.45%, per chance agreement was 24.86%, and 
kappa was 84.63%.  Annotator agreement did not 
differ by text length (?2 = 1.27; p < 0.75), arc 
length (?2 < 1), or kind of coherence relation (?2 < 
1). 
3 Data structures for representing coherence 
relations 
Most accounts of discourse coherence assume 
tree structures to represent coherence relations 
between discourse segments in a text (Carlson et 
al., 2002; Corston-Oliver, 1998; Lascarides & 
Asher, 1993; Longacre, 1983; Grosz & Sidner, 
1986; Mann & Thompson, 1988; Marcu, 2000; 
Polanyi, 1988; van Dijk & Kintsch, 1983; Walker, 
1998; Webber et al, 1999).  Other accounts 
assume less constrained graphs (Hobbs, 1985).  
The proponents of tree structures argue that trees 
are easier to formalize and derive than less 
constrained graphs (Marcu, 2000).  We tested 
whether coherence structures of naturally 
occurring texts can be represented by trees, i.e. if 
these structures are free of crossed dependencies or 
nodes with multiple parents.  However, we found a 
large number of both crossed dependencies as well 
as nodes with multiple parents in the coherence 
structures of naturally occurring texts.  Therefore 
we argue for less constrained graphs as an 
appropriate data structure for representing 
coherence, where an ordered array of nodes 
represents discourse segments and labeled directed 
arcs represent the coherence relations that hold 
between these discourse segments.1  The following 
two sections will give examples of coherence 
structures with crossed dependencies and nodes 
with multiple parents.  The section after that will 
present statistical results from our database of 135 
coherence-annotated texts. 
3.1 Crossed dependencies 
Crossed dependencies are rampant and occur in 
many different forms in the coherence structures of 
naturally occurring texts.  Here we will give some 
examples.  Consider the text passage in (10).  
                                                     
1 Other accounts also acknowledge examples that 
cannot be represented in tree structures (Webber et al, 
1999).  In order to maintain trees, these accounts 
distinguish non-anaphoric coherence structures, 
represented in a tree, and anaphoric coherence 
structures, which are not subject to tree constraints.  
However, e.g., Haliday & Hasan (1976) stress the 
importance of anaphoric links as a cue for coherence 
structures.  Therefore, by Occam?s Razor, we assume a 
single level of representation for coherence rather than 
multiple levels. 
Figure 1 represents the coherence relations in (10).  
The arrowheads of the arcs represent directionality 
for asymmetrical relations (elaboration) and 
bidirectionality for symmetrical relations 
(contrast). 
(10) Example text (from SAT practicing materials) 
0. Schools tried to teach students history of 
science. 
1. At the same time they tried to teach them how 
to think logically and inductively. 
2. Some success has been reached in the first of 
these aims. 
3. However, none at all has been reached in the 
second. 
 
Figure 1.  Coherence graph for (10). 
 
The coherence structure for (10) can be derived 
as follows:  there is a contrast relation between 0 
and 1; 0 and 1 describe teaching different things to 
students.  There is another contrast relation 
between 2 and 3; 2 and 3 describe varying degrees 
of success (some vs. none).  2 provides more 
details (the degree of success) about the teaching 
described in 0, so there is an elaboration relation 
between 2 and 0.  Furthermore, in another 
elaboration relation, 3 provides more details (the 
degree of success) about the teaching described in 
1.  In the resultant coherence structure for (10), 
there is a crossed dependency between {2, 0} and 
{3, 1}. 
In order to be able to represent the crossed 
dependency in the coherence structure of (10) in a 
tree without violating validity assumptions about 
tree structures, one might consider augmenting a 
tree with feature propagation (Shieber, 1986) or 
with a coindexation mechanism (Chomsky, 1973). 
But the problem is that both the tree structure itself 
as well as the features and coindexations represent 
the same kind of information (coherence relations).  
It is unclear how one could decide which part of a 
text coherence structure should be represented by 
the tree structure and which by the augmentation. 
As pointed out above, coherence structures of 
naturally occurring texts contain many different 
kinds of crossed dependencies.  This is important 
because it means that one cannot simply make 
special provisions to account for list-like structures 
like the structure of (10) and otherwise assume tree 
structures.  As an example of a non-list-like 
structure with a crossed dependency (between {3, 
1} and {2, 0-1}), consider (11). 
 
contr 
elab elab
contr
0 1 2 3
(11) Example text 
0. Susan wanted to buy some tomatoes 
1. and she also tried to find some basil 
2. because her recipe asked for these 
ingredients. 
3. The basil would probably be quite expensive 
at this time of the year. 
 
Figure 2.  Coherence graph for (11). 
 
The coherence structure for (11) can be derived 
as follows:  there is a parallel relation between 0 
and 1; 0 and 1 both describe shopping for grocery 
items.  There is a cause-effect relation between 2 
and 0-1; 2 describes the cause for the shopping 
described by 0 and 1.  Furthermore, there is an 
elaboration relation between 3 and 1; 3 provides 
details about the basil in 1. 
(12) from the AP Newswire1989 corpus is an 
example with a similar structure: 
(12) Example text (from text ap890109-0012) 
0. The flight Sunday took off from Heathrow 
Airport at 7:52pm 
1. and its engine caught fire 10 minutes later, 
2. the Department of Transport said. 
3. The pilot told the control tower he had the 
engine fire under control. 
 
Figure 3.  Coherence graph for (12). 
 
The coherence structure for (12) can be derived 
as follows: 1 and 0 are in a temporal sequence 
relation; 0 describes the takeoff that happens 
before the engine fire described by 1 occurs.  2 and 
0-1 are in an attribution relation; 2 mentions the 
source of what is said in 0-1.  3 and 1 are in an 
elaboration relation; 3 provides more detail about 
the engine fire in 1.  The resulting coherence 
structure, shown in Figure 3, contains a crossed 
dependency between {3, 1} and {2, 0-1}. 
3.2 Nodes with multiple parents 
In addition to crossed dependencies, many 
coherence structures of natural texts include nodes 
with multiple parents.  Such nodes cannot be 
represented in tree structures.  For instance, in the 
coherence structure of (10), nodes 0 and 2 have 
two parents.  Similarly, in the coherence structure 
of (13) from the AP Newswire 1989, node 1 has 
one attribution and one condition ingoing arc (cf. 
Figure 4). 
(13) Example text (from text ap890103-0014) 
0. ?Sure I?ll be polite,? 
1. promised one BMW driver 
2. who gave his name only as Rudolf. 
3. ?As long as the trucks and the timid stay out 
of the left lane.? 
 
Figure 4.  Coherence graph for (13). 
 
The coherence structure for (13) can be derived 
as follows:  1 states the source of what is stated in 
0 and in 3, so there are attribution relations 
between 1 and 0 and 1 and 3 respectively.  2 and 1 
are in an elaboration relation; 2 provides 
additional detail about the BMW driver in 1. 3 and 
0 are in a condition relation; 3 states the BMW 
driver?s condition for being polite, stated in 0; the 
condition relation is also indicated by the phrase 
?as long as?. 
4 Statistics 
4.1 Crossed dependencies 
An important question is how frequent the 
phenomena discussed in the previous sections are.  
The more frequent they are, the more urgent the 
need for a data structure that can adequately 
represent them. 
This section reports counts on crossed 
dependencies in the annotated database of 135 
texts.  In order to track the frequency of crossed 
dependencies for the coherence structure graph of 
each text, we counted the minimum number of arcs 
that would have to be deleted in order to make the 
coherence structure graph free of crossed 
dependencies (i.e. the minimum number of arcs 
that participate in crossed dependencies).  The 
example graph in Figure 10 illustrates this process.  
This graph contains the following crossed 
dependencies: (1, 3} crosses with {0, 2} and {2, 
4}.  By deleting {1, 3}, both crossed dependencies 
can be eliminated.  The crossed dependency count 
for the graph in Figure 5 is thus ?one?. 
 
Figure 5.  Example graph with crossed 
dependencies. 
 
On average for the 135 annotated texts, 12.5% of 
arcs in a coherence graph have to be deleted in 
order to make the graph free of crossed 
dependencies (min.: 0%; max.: 44.4%; median: 
10.9%).  Seven texts out of 135 had no crossed 
0 1 2 3 4
cond
elabattr
0 1 2 3
attr 
ce elab 
par 
0 1 2 3
0-1
elab 
ts 
0 1 2 3
0-1
attr 
dependencies.  The mean number of arcs for the 
coherence graphs of these texts was 36.9 (min.: 8; 
max.: 69; median: 35).  The mean number of arcs 
for the other 128 coherence graphs (those with 
crossed dependencies) was 125.7 (min.: 20; max.: 
293; median: 115.5).  Thus, the graphs with no 
crossed dependencies have significantly fewer arcs 
than those graphs that have crossed dependencies 
(?2=15330.35; p < 10-4).  Text length is hence a 
likely explanation for why these seven texts had no 
crossed dependencies. 
Linear regressions show that the more arcs a 
graph has, the higher the number of crossed 
dependencies (R2 = 0.39; p < 10-4).  Also, the 
longer a text, the more crossed dependencies are in 
its coherence structure graph (for text length in 
discourse segments: R2 = .29, p < 10-4; for text 
length in words: R2 = .24, p < 10-4). 
Another important question is whether certain 
types of coherence relations participate more or 
less frequently in crossed dependencies than other 
types of coherence relations.  In other words, the 
question is whether the frequency distribution over 
types of coherence relations is different for arcs 
participating in crossed dependencies compared to 
the overall frequency distribution over types of 
coherence relations in the whole database. 
Results from our database indicate that the 
overall distribution over types of coherence 
relations participating in crossed dependencies is 
not different from the distribution over types of 
coherence relations overall.  This is confirmed by a 
linear regression, which shows a significant 
correlation between the two distributions of 
percentages (R2 = 0.84; p < .0001).  Notice that the 
overall distribution includes only arcs with length 
greater than one, since arcs of length one could not 
participate in crossed dependencies. 
However, some types of coherence relations 
occur considerably less frequently in crossed 
dependencies than overall in the database.  The 
proportion of same relations is 15.21 times greater, 
and the percentage of condition relations is 5.93 
times greater overall than in crossed dependencies.  
We do not yet understand the reason for these 
differences, and plan to address this question in 
future research. 
Another question is how great the distance or arc 
length typically is between sentences that 
participate in crossed dependencies.  It is possible, 
for instance, that crossed dependencies primarily 
involve long-distance arcs and that more local 
crossed dependencies are disfavored.  However, 
the distribution over arc lengths is practically 
identical for the overall database and for coherence 
relations participating in crossed dependencies (R2 
= 0.937; p < 10-4), with short-distance relations 
being more frequent than long-distance relations 
for coherence relations overall as well as for those 
participating in crossed dependencies.  The arc 
lengths are normalized in order to take into account 
the length of a text; the absolute length of an arc is 
divided by the maximum length that that arc could 
have, given its position in a text.  Furthermore, we 
exclude arcs of (absolute) length 1 from the overall 
distribution, since such arcs could not participate in 
crossed dependencies. 
Taken together, statistical results on crossed 
dependencies suggest that crossed dependencies 
are too frequent to be ignored by accounts of 
coherence.  Furthermore, the results suggest that 
any type of coherence relation can participate in a 
crossed dependency.  However, there are some 
cases where knowing the type of coherence 
relation that an arc represents can be informative as 
to how likely that arc is to participate in a crossed 
dependency.  The statistical results reported here 
also suggest that crossed dependencies occur 
primarily locally, as evidenced by the distribution 
over lengths of arcs participating in crossed 
dependencies. 
4.2 Nodes with multiple parents 
Above we provided examples of coherence 
structure graphs that contain nodes with multiple 
parents.  Nodes with multiple parents are another 
reason why trees are inadequate for representing 
natural language coherence structures. The mean 
in-degree (=mean number of parents) of all nodes 
in the investigated database of 135 texts is 1.6 
(min.: 1; max.: 12; median: 1).  41% of all nodes in 
the database have an in-degree greater than 1.  This 
suggests that even if a mechanism could be derived 
for representing crossed dependencies in 
(augmented) tree graphs, nodes with multiple 
parents present another significant problem for 
trees representing coherence structures.  Results 
from our database indicate that the overall 
distribution over types of coherence relations 
ingoing to nodes with multiple parents is 
significantly correlated with the distribution over 
types of coherence relations overall (R2 = 0.967; p 
< 10-4). 
As for crossed dependencies, we also compared 
arc lengths.  Here, we compared the length of arcs 
that are ingoing to nodes with multiple parents to 
the overall distribution of arc length.  Again, we 
compared normalized arc lengths.  By contrast to 
the comparison for crossed dependencies, we 
included arcs of (absolute) length 1 because such 
arcs can be ingoing to nodes with either single or 
multiple parents.  The distribution over arc lengths 
is practically identical for the overall database and 
for arcs ingoing to nodes with multiple parents (R2 
= 0.993; p < 10-4), suggesting a strong locality bias 
for coherence relations overall as well as for those 
participating in crossed dependencies. 
In sum, statistical results on nodes with multiple 
parents suggest that they are a frequent 
phenomenon, and that they are not limited to 
certain kinds of coherence relations.  Additionally, 
the statistical results reported here suggest that 
ingoing arcs to nodes with multiple parents are 
primarily local. 
5 Conclusion 
The goals of this paper have been to present a set 
of coherence relations that are easy to code, and to 
illustrate the inadequacy of trees as a data structure 
for representing discourse coherence structures.  
We have developed a coding scheme with high 
inter-annotator reliability and used that scheme to 
annotate 135 texts with coherence relations.  An 
investigation of these annotations has shown that 
discourse structures of naturally occurring texts 
contain various kinds of crossed dependencies as 
well as nodes with multiple parents.  Both 
phenomena cannot be represented using trees, 
which implies that existing databases of coherence 
structures that use trees are not descriptively 
adequate. 
Our statistical results suggest that crossed 
dependencies and nodes with multiple parents are 
not restricted phenomena that could be ignored or 
accommodated with a few exception rules.  
Furthermore, even if one could find a way of 
augmenting tree structures to account for crossed 
dependencies and nodes with multiple parents, 
there would have to be a mechanism for unifying 
the tree structure with the augmentation features.  
Thus, in terms of derivational complexity, trees 
would just shift the burden from having to derive a 
less constrained data structure to having to derive a 
unification of trees and features or coindexation. 
Because trees are neither a descriptively 
adequate data structure for representing coherence 
structures nor easier to derive, we argue for less 
constrained graphs as a data structure for 
representing coherence structures.  Such less 
constrained graphs would have the advantage of 
being able to adequately represent coherence 
structures in one single data structure (cf. Skut et 
al., 1997).  Furthermore, they are at least not 
harder to derive than (augmented) tree structures.  
The greater descriptive adequacy might in fact 
make them easier to derive.  However, this is still 
an open issue and will have to be addressed in 
future research. 
References  
Jean Carletta. 1996.  Assessing agreement on 
classifi-cation tasks: the kappa statistic.  
Computational Linguistics, 22(2): 249-254. 
Lynn Carlson, Daniel Marcu, and Mary E. 
Okurowski. 2002.  RST Discourse Treebank.  
Philadelphia, PA: LDC. 
Noam Chomsky. 1973.  Conditions on 
transformations.  In: Anderson, S. & Kiparsky, 
P., eds., A Festschrift for Morris Halle, 232-286. 
New York: Holt, Rinehart and Winston. 
Simon Corston-Oliver. 1998.  Computing 
representations of the structure of written 
discourse.  Microsoft Research Technical 
Report MSR-TR-98-15.  Redmont, WA, USA. 
Barbara J. Grosz and Candace L. Sidner. 1986.  
Attention, intentions, and the structure of 
discourse.  Computational Linguistics, 12(3): 
175-204. 
Michael A.K. Haliday and Ruqaiya Hasan. 1976.  
Cohesion in English.  Longman, London. 
Donna Harman and Mark Liberman. 1993.  
TIPSTER complete.  Philadelphia, PA: LDC. 
Marti Hearst. 1997.  TextTiling: Segmenting text 
into multi-paragraph subtopic passages.  
Computational Linguistics, 23(1): 33-64. 
Julia Hirschberg and Christine H. Nakatani. 1996.  
A prosodic analysis of discourse segments in 
direction-giving monologues.  In: Proceedings 
of the 34th Annual Meeting of the ACL, 286-
293.  Santa Cruz, CA, USA. 
Jerry R. Hobbs. 1985.  On the coherence and 
structure of discourse.  CSLI Technical Report 
85-37.  Stanford, CA, USA. 
Eduard Hovy and Elisabeth Maier. 1995.  
Parsimonious or profligate: How many and 
which discourse relations?  Unpublished 
manuscript. 
Andrew Kehler. 2002.  Coherence, reference, and 
the theory of grammar.  Stanford, CA: CSLI 
Publications. 
Alex Lascarides and Nicholas Asher. 1993.  
Temporal interpretation, discourse relations, and 
common sense entailment.  Linguistics and 
Philosophy, 16(5): 437-493. 
Robert E. Longacre. 1983.  The grammar of 
discourse.  New York: Plenum Press. 
William C. Mann and Sandra A. Thompson. 1988.  
Rhetorical structure theory: Toward a functional 
theory of text organization.  Text, 8(3): 243-281. 
Daniel Marcu. 2000.  The theory and practice of 
discourse parsing and summarization.  
Cambridge, MA: MIT Press. 
Mitchell Marcus, Grace Kim, Mary A. 
Marcinkiewicz, Robert MacIntyre, Ann Bies, 
Mark Ferguson, Karen Katz and Britta 
Schasberger. 1994.  The Penn Treebank: 
Annotating predicate argument structure.  In: 
Proceedings of the ARPA Human Language 
Technology Workshop.  San Francisco, CA: 
Morgan Kaufman. 
Livia Polanyi. 1988.  A formal model of the 
structure of discourse.  Journal of Pragmatics, 
12: 601-638. 
Stuart M. Shieber. 1985.  Evidence against the 
context-freeness of natural language.  
Linguistics and Philosophy, 8: 333-343. 
Stuart M. Shieber. 1986.  An introduction to 
unification-based approaches to grammar.  
Stanford University: CSLI Lecture Notes 4. 
Wojciech Skut, Brigitte Krenn, Thorsten Brants 
and Hans Uszkoreit. 1997.  An annotation 
scheme for free word order languages.  In: 
Proceedings of the 5th ANLP Conference.  
Washington, DC, USA. 
Teun A. van Dijk and Walter Kintsch. 1983.  
Strategies of discourse comprehension.  New 
York: Academic. 
Marilyn A. Walker. 1998.  Centering, anaphora 
resolution, and discourse structure.  In: Prince, 
E., Joshi, A.K. & Walker, M.A., eds., Centering 
Theory in discourse.  Oxford: Oxford University 
Press. 
Bonnie L. Webber, Alastair Knott, Stone, M. & 
Joshi, A.K. 1999.  Discourse relations: A 
structural and presuppositional account using 
lexicalized TAG.  In: Proceedings of the 37th 
Annual Meeting of the ACL, 41-48.  College 
Park, MD, USA. 
 
Representing Discourse Coherence:
A Corpus-Based Study
Florian Wolf?
University of Cambridge
Edward Gibson??
Massachusetts Institute of Technology
This article aims to present a set of discourse structure relations that are easy to code and to
develop criteria for an appropriate data structure for representing these relations. Discourse
structure here refers to informational relations that hold between sentences in a discourse. The
set of discourse relations introduced here is based on Hobbs (1985).
We present a method for annotating discourse coherence structures that we used to manually
annotate a database of 135 texts from the Wall Street Journal and the AP Newswire. All texts
were independently annotated by two annotators. Kappa values of greater than 0.8 indicated
good interannotator agreement.
We furthermore present evidence that trees are not a descriptively adequate data structure for
representing discourse structure: In coherence structures of naturally occurring texts, we found
many different kinds of crossed dependencies, as well as many nodes with multiple parents. The
claims are supported by statistical results from our hand-annotated database of 135 texts.
1. Introduction
An important component of natural language discourse understanding and production
is having a representation of discourse structure. A coherently structured discourse here
is assumed to be a collection of sentences that are in some relation to each other. This
article aims to present a set of discourse structure relations that are easy to code and to
develop criteria for an appropriate data structure for representing these relations.
There have been two kinds of approaches to defining and representing discourse
structure and coherence relations. These approaches differ with respect to what kinds
of discourse structure they are intended to represent. Some accounts aim to represent
the intentional-level structure of a discourse; in these accounts, coherence relations
reflect how the role played by one discourse segment with respect to the interlocu-
tors? intentions relates to the role played by another segment (e.g., Grosz and Sidner
1986). Other accounts aim to represent the informational structure of a discourse; in
these accounts, coherence relations reflect how the meaning conveyed by one discourse
segment relates to the meaning conveyed by another discourse segment (e.g., Hobbs
1985; Marcu 2000; Webber et al 1999). Furthermore, accounts of discourse structure
vary greatly with respect to how many discourse relations they assume, ranging from 2
(Grosz and Sidner 1986) to over 400 different coherence relations (reported in Hovy and
? Computer Laboratory and Genetics Department, Cambridge, CB3 0FD, U.K.
E-mail: Florian.Wolf@cl.cam.ac.uk
?? Department of Brain and Cognitive Sciences, Cambridge, MA 02139. E-mail: egibson@mit.edu.
Submission received: 15th June 2004; Revised submission received: 5th September 2004; Accepted for
publication: 23rd October 2004
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 2
Maier [1995]). However, Hovy and Maier (1995) argue that, at least for informational-
level accounts, taxonomies with more relations represent subtypes of taxonomies with
fewer relations. This means that different informational-level-based taxonomies can be
compatible with each other; they differ with respect to how detailed or fine-grained a
manner they represent informational structures of texts. Going beyond the question of
how different informational-level accounts can be compatible with each other, Moser
and Moore (1996) discuss the compatibility of rhetorical structure theory (RST) (Mann
and Thompson 1988) with the theory of Grosz and Sidner (1986). However, note that
Moser and Moore (1996) focus on the question of how compatible the claims are that
Mann and Thompson (1988) and Grosz and Sidner (1986) make about intentional-level
discourse structure.
In this article, we aim to develop an easy-to-code representation of informational
relations that hold between sentences or other nonoverlapping segments in a dis-
course monologue. We describe an account with a small number of relations in order
to achieve more generalizable representations of discourse structures; however, the
number is not so small that informational structures that we are interested in are
obscured. The goal of the research presented is not to encode intentional relations in
texts. We consider annotating intentional relations too difficult to implement in practice
at this time. Note that we do not claim that intentional-level structure of discourse is
not relevant to a full account of discourse coherence; it just is not the focus of this
article.
The next section describes in detail the set of coherence relations we use, which are
mostly based on Hobbs (1985). We try to make as few a priori theoretical assumptions
about representational data structures as possible. These assumptions are outlined in
the next section. Importantly, however, we do not assume a tree data structure to
represent discourse coherence structures. In fact, a major result of this article is that
trees do not seem adequate to represent discourse structures.
This article is organized as follows. Section 2 describes the procedure we used to
collect a database of 135 texts annotated with coherence relations. Section 3 describes
in detail the descriptional inadequacy of tree structures for representing discourse
coherence, and Section 4 provides statistical evidence from our database that supports
this claim. Section 5 offers some concluding remarks.
2. Collecting a Database of Texts Annotated with Coherence Relations
This section describes (1) how we defined discourse segments, (2) which coherence
relations we used to connect discourse segments, and (3) how the annotation procedure
worked.
2.1 Discourse Segments
There is agreement that discourse segments should be nonoverlapping spans of text.
However, there is disagreement in the literature about how to define discourse segments
(cf. the discussion in Marcu [2000]). Whereas some argue that discourse segments
should be prosodic units (Hirschberg and Nakatani 1996), others argue for intentional
units (Grosz and Sidner 1986), phrasal units (Lascarides and Asher 1993; Longacre 1983;
Webber et al 1999), or sentences (Hobbs 1985).
For our database, we mostly adopted a clause-unit-based definition of discourse
segments. We chose this method of segmenting discourse because it was easy to use.
250
Wolf and Gibson Representing Discourse Coherence
Table 1
Contentful conjunctions used to illustrate coherence relations.
Cause?effect because; and so
Violated expectation although; but; while
Condition if . . . (then); as long as; while
Similarity and; (and) similarly
Contrast by contrast; but
Temporal sequence (and) then; first, second, . . . ; before; after; while
Attribution according to . . . ; . . . said; claim that . . . ; maintain that . . . ; stated that . . .
Example for example; for instance
Elaboration also; furthermore; in addition; note (furthermore) that; (for, in, on, against,
with, . . . ) which; who; (for, in, on, against, with, . . . ) whom
Generalization in general
However, we also assumed that contentful coordinating and subordinating conjunc-
tions (cf. Table 1) can delimit discourse segments.
Note that we did not classify and as delimiting discourse segments if it was used
to conjoin nouns in a conjoined noun phrase, like dairy plants and dealers in example (1)
(from wsj 0306; Wall Street Journal 1989 corpus [Harman and Liberman 1993]) or if it
was used to conjoin verbs in a conjoined verb phrase, like snowed and rained in example
(2) (constructed):
(1) Milk sold to the nation?s dairy plants and dealers averaged $14.50 for each
hundred pounds.
(2) It snowed and rained all day long.
We classified periods, semicolons, and commas as delimiting discourse segments. How-
ever, in cases like example (3) (constructed), in which they conjoin a complex noun
phrase, commas were not classified as delimiting discourse segments.
(3) John bought bananas, apples, and strawberries.
We furthermore treated attributions (John said that . . .) as discourse segments. This was
empirically motivated. The texts used here were taken from news corpora, and there,
attributions can be important carriers of coherence structures. For instance, consider a
case in which some source A and some source B both comment on some event X. It
should be possible to distinguish between a situation in which source A and source B
make basically the same statement about event X and a situation in which source A and
source B make contrasting comments about event X. Note, however, that we treated
cases like example (4) (constructed) as one discourse segment and not as two separate
ones ( . . . cited and transaction costs . . .). We separated attributions only if the attributed
material was a complementizer phrase, a sentence, or a group of sentences. This is not
the case in example (4): The attributed material is a complex NP (transaction costs from
its 1988 recapitalization).
(4) The restaurant operator cited transaction costs from its 1988 recapitalization.
251
Computational Linguistics Volume 31, Number 2
2.2 Discourse Segment Groupings
Adjacent discourse segments could, in our approach, be grouped together. For example,
discourse segments were grouped if they all stated something that could be attributed
to the same source (cf. section 2.3 for a definition of attribution coherence relations).
Furthermore, discourse segments were grouped if they were topically related. For
example, if a text discussed inventions in information technology, there could be groups
of a few discourse segments each talking about inventions by specific companies. There
might also be subgroups, consisting of several discourse segments each, talking about
specific inventions at specific companies. Thus, marking groups could determine a
partially hierarchical structure for the text.
Other examples of discourse segment groupings included cases in which several
discourse segments described an event or a group of events that all occurred before
another event or another group of events described by another (group of) discourse
segments. In those cases, what was described by a group of discourse segments was in
a temporal sequence relation with what was described by another (group of) discourse
segments (cf. section 2.3 for a definition of temporal-sequence coherence relations). Note
furthermore that in cases in which one topic required one grouping and a following
topic required a grouping that was different from the first grouping, both groupings
were annotated.
Unlike approaches such as the TextTiling algorithm (Hearst 1997), ours allowed
partially overlapping groups of discourse segments. The idea behind this option was
to allow groupings of discourse segments in which a transition discourse segment
belonged to the previous as well as the following group. However, the option was not
used by the annotators (i.e., in our database of 135 hand-annotated texts, there were no
instances of partially overlapping discourse segment groups).
2.3 Coherence Relations
As pointed out in section 1, we aim to develop a representation of informational
relations between discourse segments. Note one difference between schema-based
approaches (McKeown 1985) and coherence relations as we used them: Whereas
schemas are instantiated from information contained in a knowledge base, coherence
relations as we used them do not make (direct) reference to a knowledge base.
There are a number of different informational coherence relations, dating back, in
their basic definitions, to Hume, Plato, and Aristotle (cf. Hobbs 1985; Hobbs et al 1993;
Kehler 2002). The coherence relations we used are mostly based on Hobbs (1985); below
we describe each coherence relation we used and note any differences between ours and
Hobbs?s (1985) set of coherence relations (cf. Table 2 for an overview of how our set of
coherence relations relates to the set of coherence relations in Hobbs [1985]).
The kinds of coherence relations we used include cause?effect relations, as in
example (5) (constructed), in which discourse segment 1 states the cause for the effect
that is stated in discourse segment 2:
(5) Cause?effect
1. There was bad weather at the airport
2. and so our flight got delayed.
Our cause?effect relation subsumed the cause as well as the explanation relation in
Hobbs (1985). A cause relation holds if a discourse segment stating a cause occurs
252
Wolf and Gibson Representing Discourse Coherence
before a discourse segment stating an effect; an explanation relation holds if a discourse
segment stating an effect occurs before a discourse segment stating a cause. We encoded
this difference by adding a direction to the cause?effect relation. In a graph, this can be
represented by a directed arc going from cause to effect.
Another kind of causal relation is condition. Hobbs (1985) does not distinguish con-
dition relations from either cause or explanation relations. However, we felt that it might
be important to distinguish between a causal relation describing an actual causal event
(cause?effect, cf. above), on the one hand, and a causal relation describing a possible
causal event (condition, cf. below), on the other hand. In example (6) (constructed),
discourse segment 2 states an event that will take place if the event described by
discourse segment 1 also takes place:
(6) Condition
1. If the new software works,
2. everyone should be happy.
In a third type of causal relation, the violated expectation relation (also violated
expectation in Hobbs [1985]), a causal relation between two discourse segments that
normally would be present is absent. In example (7) (constructed), discourse segment 1
normally would be a cause for everyone?s being happy; this expectation is violated by
what is stated by discourse segment 2:
(7) Violated expectation
1. The new software worked great,
2. but nobody was happy.
Other possible coherence relations include similarity (parallel in Hobbs [1985]) or
contrast (also contrast in Hobbs [1985]) relations, in which similarities or contrasts are
determined between corresponding sets of entities or events, such as between discourse
segments 1 and 2 in example (8) (constructed) and discourse segments 1 and 2 in
example (9) (constructed), respectively:
(8) Similarity
1. The first flight to Frankfurt this morning was delayed.
2. The second flight arrived late as well.
(9) Contrast
1. The first flight to Frankfurt this morning was delayed.
2. The second flight arrived on time.
Discourse segments might also elaborate (also elaboration in Hobbs [1985]) on other
sentences, as in example (10) (constructed), in which discourse segment 2 elaborates
on discourse segment 1:
(10) Elaboration
1. A probe to Mars was launched from the Ukraine this week.
2. The European-built ?Mars Express? is scheduled to reach Mars by
late December.
Discourse segments can provide examples for what is stated by another discourse
segment. In example (11) (constructed), discourse segment 2 states an example
253
Computational Linguistics Volume 31, Number 2
(exemplification in Hobbs [1985]) for what is stated in discourse segment 1:
(11) Example
1. There have been many previous missions to Mars.
2. A famous example is the Pathfinder mission.
Hobbs (1985) also includes an evaluation relation, as in example (12) (from Hobbs
[1985]), in which discourse segment 2 states an evaluation of what is stated in discourse
segment 1. We decided to call such relations elaborations, since we found it too difficult
in practice to reliably distinguish elaborations from evaluations (according to our annota-
tion scheme, in example (12), what is stated in discourse segment 2 elaborates on what
is stated in discourse segment 1):
(12) Elaboration (labeled as evaluation in Hobbs [1985])
1. (A story.)
2. It was funny at the time.
Unlike Hobbs (1985), we did not have a separate background relation as in exam-
ple (13) (modified from Hobbs [1985]), in which what is stated in discourse segment
1 provides the background for what is stated in discourse segment 2. As with the
evaluation relation, we found the background relation too difficult to reliably distinguish
from elaboration relations (according to our annotation scheme, in example (13), what is
stated in discourse segment 1 elaborates on what is stated in discourse segment 2):
(13) Elaboration (labeled as background in Hobbs [1985])
1. T is the pointer to the root of a binary tree.
2. Initialize T.
In a generalization relation, as in example (14) (constructed), one discourse seg-
ment (here discourse segment 2) states a generalization for what is stated by another
discourse segment (here discourse segment 1):
(14) Generalization
1. Two missions to Mars in 1999 failed.
2. There are many missions to Mars that have failed.
Furthermore, discourse segments can be in an attribution relation, as in example
(15) (constructed), in which discourse segment 1 states the source of what is stated
in discourse segment 2 (cf. [Bergler 1991] for a more detailed semantic analysis of
attribution relations):
(15) Attribution
1. John said that
2. the weather would be nice tomorrow.
Hobbs (1985) does not include an attribution relation. However, we decided to include
attribution as a relation because, as pointed out in section 2.1, the texts we annotated
are taken from news corpora. There, attributions can be important carriers of coherence
structures.
254
Wolf and Gibson Representing Discourse Coherence
In a temporal sequence relation, as in example (16) (constructed), one discourse
segment (here discourse segment 1) states an event that takes place before another event
stated by the other discourse segment (here discourse segment 2):
(16) Temporal Sequence
1. First, John went grocery shopping.
2. Then he disappeared in a liquor store.
In contrast to cause?effect relations, there is no causal relation between the events
described by the two discourse segments. The temporal sequence relation is equivalent to
the occasion relation in Hobbs (1985).
The same relation, illustrated by example (17) (constructed), is an epiphenomenon
of assuming contiguous distinct elements of text (Hobbs [1985] does not include a same
relation). A same relation holds if a subject NP is separated from its predicate by an
intervening discourse segment. For instance, in example (17), discourse segment 1 is the
subject NP of a predicate in discourse segment 3, and so there is a same relation between
discourse segments 1 and 3; discourse segment 1 is the first and discourse segment 3
is the second segment of what is actually one single discourse segment, separated by
the intervening discourse segment 2, which is in an attribution relation with discourse
segment 1 (and therefore also with discourse segment 3, since discourse segments 1 and
3 are actually one single discourse segment):
(17) Same
1. The economy,
2. according to some analysts,
3. is expected to improve by early next year.
Table 2 provides an overview of how our set of coherence relations relates to the set
of coherence relations in Hobbs (1985).
We distinguish between asymmetrical or directed relations, on the one hand, and
symmetrical or undirected relations, on the other hand (Mann and Thompson 1988;
Marcu 2000). Cause?effect, condition, violated expectation, elaboration, example, generaliza-
tion, attribution, and temporal sequence are asymmetrical or directed relations, whereas
similarity, contrast, and same are symmetrical or undirected relations. In asymmetrical or
directed relations, the directions of relations are as follows:
 Cause?effect: from the discourse segment stating the cause to the discourse
segment stating the effect
 Condition: from the discourse segment stating the condition to the
discourse segment stating the consequence
 Violated expectation: from the discourse segment stating the cause to the
discourse segment describing the absent effect
 Elaboration: from the elaborating discourse segment to the elaborated
discourse segment
 Example: from the discourse segment stating the example to the discourse
segment stating the exemplified
 Generalization: from the discourse segment stating the special case to the
discourse segment stating the general case
255
Computational Linguistics Volume 31, Number 2
Table 2
Correspondence between the set of coherence relations in Hobbs (1985) and our set of coherence
relations.
Hobbs (1985) Our annotation scheme
Occasion Temporal sequence
Cause Cause?effect: cause stated first, then effect;
directionality indicated by directed arcs in a
coherence graph
Explanation Cause?effect: effect stated first, then cause;
directionality indicated by directed arcs in a
coherence graph
? Condition
Evaluation Elaboration
Background Elaboration
Exemplification: example stated first, then Example
general case; directionality indicated by
directed arcs in a coherence graph
Exemplification: general case stated first, then Generalization
example; directionality indicated by
directed arcs in a coherence graph
Elaboration Elaboration
Parallel Similarity
Contrast Contrast
Violated expectation Violated expectation
? Attribution
? Same
 Attribution: from the discourse segment stating the source to the attributed
statement
 Temporal sequence: from the discourse segment stating the event that
happened first to the discourse segment stating the event that happened
second
This definition of directionality is related to Mann and Thompson?s (1988) notion
of nucleus and satellite nodes (where the nodes can represent [groups of] discourse
segments): For asymmetrical or directed relations, the directionality is from satellite
to nucleus node; by contrast, symmetrical or undirected relations hold between two
nucleus nodes.
Note also that in our annotation project, we decided to annotate a coherence relation
either if there was a coherence relation between the complete content of two discourse
segments, or if there was a relation between parts of the content of two discourse
segments. Consider the following example (from ap890104-0003; AP Newswire corpus;
[Harman and Liberman 1993]):
(18) 1. a[ Difficulties have arisen ] b[ in enacting the accord for the
independence of Namibia ]
2. for which SWAPO has fought many years,
For this example we would annotate an elaboration relation from discourse segment 2 to
discourse segment 1 (discourse segment 2 provides additional details about the accord
256
Wolf and Gibson Representing Discourse Coherence
mentioned in discourse segment 1), although the relation actually only holds between
discourse segment 2 and the second part of discourse segment 1, indicated by brackets.
Although it is beyond the scope of the current project, future research should
investigate annotations with discourse segmentations that allow annotating rela-
tions only between parts of discourse segments that are responsible for a coherence
relation. For example, consider example (19) (from ap890104-0003; AP Newswire
corpus [Harman and Liberman 1993]), in which brackets indicate how more-fine-
grained discourse segments might be marked:
(19) 1. a[ for which ] b[ SWAPO ] c[ has fought many years, ]
2. referring to the acronym of the South-West African Peoples
Organization nationalist movement.
In our current project, we annotated an elaboration relation from discourse segment 2 to
discourse segment 1 (discourse segment 2 provides additional details, the full name,
for SWAPO, which is mentioned in discourse segment 1). A future, more detailed,
annotation of coherence relations could then annotate this elaboration relation to hold
only between discourse segment 2 and the word SWAPO in discourse segment 1.
2.4 Coding Procedure
To code the coherence relations of a text, we used a procedure consisting of three steps.
In the first step, a text was segmented into discourse segments (cf. section 2.1).
In the second step, adjacent discourse segments that were topically related were
grouped together. The criteria for this step are described in section 2.2.
In the third step, coherence relations (cf. section 2.3) were determined between
discourse segments and groups of discourse segments. Each previously unconnected
(group of) discourse segment(s) was tested to see whether it connected to any of the
(groups of) discourse segments that had already been connected to the already existing
representation of discourse structure.
In order to help determine the coherence relation between (groups of) discourse
segments, the annotators judged which, if any, of the contentful coordinating conjunc-
tions in Table 1 resulted, when used, in the most acceptable passage (cf. Hobbs 1985;
Kehler 2002). If using a contentful conjunction to connect (groups of) discourse seg-
ments resulted in an acceptable passage, this was used as evidence that the coherence
relation corresponding to the mentally inserted contentful conjunction held between
the (groups of) discourse segments under consideration. This mental exercise was done
only if there was not already a contentful coordinating conjunction that disambiguated
the coherence relation.
The following list (which was also used by the annotators to guide them in their
task) shows in more detail how the annotations were carried out:
1. Segment the text into discourse segments:
(a) Insert segment boundaries at every period that marks a sentence
boundary (i.e., not at periods such as those in Mrs. or Dr.).
(b) Insert segment boundaries at every semicolon and colon that marks
a sentence or clause boundary.
(c) Insert segment boundaries at every comma that marks a sentence
or clause boundary; do not insert segment boundaries at commas
that conjoin complex noun or verb phrases.
257
Computational Linguistics Volume 31, Number 2
(d) Insert segment boundaries at every quotation mark, if there is not
already a segment boundary based on steps (a)?(c).
(e) Insert segment boundaries at the contentful coordinating
conjunctions listed in Table 1, if there is not already a segment
boundary based on steps (a)?(d). For and, do not insert a segment
boundary if it is used to conjoin verbs or nouns in a conjoined verb
or noun phrase.
2. Generate groupings of related discourse segments:
(a) Group contiguous discourse segments that are enclosed by pairs of
quotation marks.
(b) Group contiguous discourse segments that are attributed to the
same source.
(c) Group contiguous discourse segments that belong to the same
sentence (marked by periods, commas, semicolons, or
colons).
(d) Group contiguous discourse segments that are topically centered
on the same entities or events.
3. Determine coherence relations between discourse segments and groups of
discourse segments. For each previously unconnected (group of) discourse
segment(s), test whether it connects to any of the (groups of) discourse
segments that have already been connected to the already existing
representation of discourse structure. Use the following steps for each
decision:
(a) Use pairs of quotation marks as a signal for attribution.
(b) For pairs of (groups of) discourse segments that are already
connected with one of the contentful coordinating conjunctions
from Table 1, choose the coherence relation that corresponds to the
coordinating conjunction.
(c) For pairs of (groups of) discourse segments that are not connected
with one of the contentful coordinating conjunctions from
Table 1:
i. Mentally connect the (groups of) discourse segments with
one of the coordinating conjunctions from Table 1
and judge whether the resultant passage sounds
acceptable.
ii. If the passage sounds acceptable, choose the coherence
relation that corresponds to the coordinating conjunction
selected in step (c.i).
iii. If the passage does not sound acceptable, repeat step (c.i)
until an acceptable coordinating conjunction is found.
iv. If the passage does not sound acceptable with any of the
coordinating conjunctions from Table 1, assume that the
(groups of) discourse segments under consideration are not
related by a coherence relation.
(d) Iterative procedure for steps (a) and (b):
i. Start with any of the unambiguous coordinating
conjunctions from Table 1 (because, although, if . . . then, . . .
said, for example).
258
Wolf and Gibson Representing Discourse Coherence
Table 3
Statistics for texts in our database.
Number of words Number of discourse segments
Mean 545 61
Minimum 161 6
Maximum 1,409 143
Median 529 60
ii. If none of the unambiguous coordinating conjunctions results
in an acceptable passage, use the more ambiguous
coordinating conjunctions (and, but, while, also, etc.).
(e) Important distinctions for steps (2) and (3) (this is based on
practical issues that came up during the annotation project):
i. Example versus elaboration: An example relation sets up an
additional entity or event (the example), whereas an
elaboration relation provides more details about an already
introduced entity or event (the one on which one elaborates).
ii. Cause?effect versus temporal sequence: Both cause?effect and
temporal sequence describe a temporal order of events (in
cause?effect, the cause has to precede the effect). However,
only cause?effect relations have a causal relation between
what is stated by the (groups of) discourse segments under
consideration. Thus, if there is a causal relation between the
(groups of) discourse segments under consideration, assume
cause?effect rather than temporal sequence (cf. Lascarides and
Asher 1993).
2.5 Annotators
The annotators for the database were MIT undergraduate students who worked in our
lab as research students. For training, the annotators received a manual that described
the background of the project, discourse segmentation, coherence relations and how to
recognize them, and how to use the annotation tools that we developed in our lab (Wolf
et al 2003). The first author of this article provided training for the annotators. Training
consisted of explaining the background of the project and the annotation method and
of annotating example texts (these texts are not included in our database). Training took
8?10 hours in total, distributed over five days of a week. After completing the training,
annotators worked independently.
2.6 Statistics on Annotated Database
In order to evaluate hypotheses about appropriate data structures for representing
coherence structures, we have collected a database of 135 texts from the Wall Street
Journal 1987?1989 (30 texts) and the AP Newswire 1989 (105 texts) (both from Harman
and Liberman [1993]) in which the relations between discourse segments have been
labeled with the coherence relations described above. Table 3 shows statistics for this
database.
259
Computational Linguistics Volume 31, Number 2
Steps 2 (discourse segment grouping) and 3 (coherence relation annotation) of
the coding procedure described in section 2.4 were performed independently by two
annotators. For step 1 (discourse segmentation), a pilot study on 10 texts showed
that agreement on this step, as determined by number of common segments/(number of
common segments + number of differing segments), was never below 90%. Therefore, all 135
texts were segmented by two annotators together, resulting in segmentations that both
annotators could agree on.
In order to determine interannotator agreement for step 2 of the coding procedure
for the database of annotated texts, we calculated kappa statistics (Carletta 1996). We
used the following procedure to construct a confusion matrix: First, all groups marked
by either annotator were extracted. Annotator 1 had marked 2,616 groups, and an-
notator 2 had marked 3,021 groups in the whole database. The groups marked by
the annotators consisted of 536 different discourse segment group types (for example,
groups that included the first two discourse segments of each text were marked 31 times
by both annotators; groups that included the first three discourse segments of each text
were marked 6 times by both annotators). Therefore, the confusion matrix had 536 rows
and columns. For all annotations of the 135 texts, the agreement was 0.8449, per chance
agreement was 0.0161, and kappa was 0.8424. Annotator agreement did not differ as a
function of text length, arc length, or kind of coherence relation (all ?2 values < 1).
We also calculated kappa statistics to determine interannotator agreement for step 3
of the coding procedure.1 For all annotations of the 135 texts, the agreement was 0.8761,
per chance agreement was 0.2466, and kappa was 0.8355. Annotator agreement did not
differ as a function of text length (?2 = 1.27, p < 0.75), arc length (?2 < 1), or kind of
coherence relation (?2 < 1). Table 4 shows the confusion matrix for the database of
135 annotated texts that was used to compute the kappa statistics. The table shows,
for example, that much of the interannotator disagreement seems to have been driven
by disagreement over how to annotate elaboration relations (in the whole database,
annotator 1 marked 260 elaboration relations where annotator 2 marked no relation;
annotator 2 marked 467 elaboration relations where annotator 1 marked no relation).
The only other comparable discourse annotation project that we are currently aware
of is that of Carlson, Marcu, and Okurowski (2002).2 Since they use trees and split the
annotation process into different substeps than those in our procedure, their annotator
agreement figures are not directly comparable to ours. Furthermore, note that Carlson
and her colleagues do not report annotator agreement figures for their database as a
whole, but for different subsets of four to seven documents that were each annotated
by different pairs of annotators. For discourse segmentation, they report kappa values
ranging from 0.951 to 1.00; for annotation of discourse tree spans, their kappa values
ranged from 0.778 to 0.929; for annotation of coherence relation nuclearity (whether a
node in a discourse tree is a nucleus or a satellite, cf. section 2.3 for the definition of
these terms), kappa values ranged from 0.695 to 0.882; for assigning types of coherence
relations, they reported kappa values ranging from 0.624 to 0.823.
1 Note that interannotator agreement for step 3 was influenced by interannotator agreement for step 2. For
example, one annotator might mark a group of discourse segments 2 and 3, whereas the second
annotator might not mark that group of discourse segments. If the first annotator then marks, for
example, a cause?effect coherence relation between discourse segment 4 and the group of discourse
segments 2 and 3, whereas the second annotator marks a cause?effect coherence relation between
discourse segment 4 and discourse segment 3, this would count as a disagreement. Thus, our measure of
interannotator agreement for step 3 is conservative.
2 Note that Miltsakaki et al (2004) report results on annotating connectives but not on annotating whole
discourse structures.
260
Wolf and Gibson Representing Discourse Coherence
Ta
b
le
4
C
on
fu
si
on
m
at
ri
x
of
an
no
ta
ti
on
s
fo
r
th
e
d
at
ab
as
e
of
13
5
an
no
ta
te
d
te
xt
s.
co
nt
r
=
co
nt
ra
st
;e
xp
v
=
vi
ol
at
ed
ex
pe
ct
at
io
n;
ce
=
ca
us
e?
ef
fe
ct
;n
on
e
=
no
co
he
re
nc
e
re
la
ti
on
;g
en
=
ge
ne
ra
liz
at
io
n;
co
nd
=
co
nd
it
io
n;
ex
am
p
=
ex
am
pl
e;
ts
=
te
m
po
ra
ls
eq
ue
nc
e;
at
tr
=
at
tr
ib
ut
io
n;
el
ab
=
el
ab
or
at
io
n;
si
m
=
si
m
ila
ri
ty
.
A
n
n
ot
at
or
2
A
n
n
ot
at
or
1
co
nt
r
ex
pv
ce
no
ne
ge
n
co
nd
ex
am
p
ts
at
tr
el
ab
sa
m
e
si
m
Su
m
P
er
ce
nt
ag
e
co
nt
r
38
3
11
0
34
0
0
0
2
0
0
0
0
43
0
4.
47
ex
pv
4
11
3
0
7
0
0
0
0
0
0
0
0
12
4
1.
29
ce
0
0
44
6
14
0
0
0
0
0
5
0
0
46
5
4.
83
no
ne
66
24
42
0
0
2
27
16
6
46
7
1
64
71
5
7.
43
ge
n
0
0
0
1
21
0
0
0
0
1
0
0
23
0.
24
co
nd
0
0
0
2
0
12
7
0
1
0
1
0
0
13
1
1.
36
ex
am
p
0
0
1
18
0
0
21
9
0
0
3
0
0
24
1
2.
51
ts
1
1
2
7
0
0
0
21
4
0
1
0
0
22
6
2.
35
at
tr
0
0
0
5
0
0
0
0
1,
38
7
0
0
0
1,
39
2
14
.4
7
el
ab
0
0
17
26
0
0
3
0
3
0
3,
91
3
1
0
4,
19
7
43
.6
3
sa
m
e
0
0
2
5
0
0
0
1
0
0
53
0
1
53
9
5.
60
si
m
7
0
3
43
0
0
0
6
0
0
3
1,
07
4
1,
13
6
11
.8
1
Su
m
46
1
14
9
51
3
39
6
21
13
2
24
6
24
3
1,
39
3
4,
39
1
53
5
1,
13
9
P
er
ce
nt
ag
e
4.
79
1.
55
5.
30
4.
12
0.
20
1.
37
2.
56
2.
53
14
.5
0
45
.6
0
5.
56
11
.8
0
261
Computational Linguistics Volume 31, Number 2
3. Data Structures for Representing Coherence Relations
In order to represent the coherence relations between discourse segments in a text, most
accounts of discourse coherence assume tree structures (Britton 1994; Carlson, Marcu,
and Okurowski 2002; Corston-Oliver 1998; Longacre 1983; Grosz and Sidner 1986; Mann
and Thompson 1988; Marcu 2000; Polanyi and Scha 1984; Polanyi 1996; Polanyi et al
2004; van Dijk and Kintsch 1983; Walker 1998); some accounts do not allow crossed
dependencies but appear to allow nodes with multiple parents (Lascarides and Asher
1991).3 Other accounts assume less constrained graphs that allow crossed dependencies
as well as nodes with multiple parents (e.g., Bergler 1991; Birnbaum 1982; Danlos 2004;
Hobbs 1985; McKeown 1985; Reichman 1985; Zukerman and McConachy 1995; for
dialogue structure, Penstein Rose et al 1995).
Some proponents of tree structures assume that trees are easier to formalize and to
derive than less constrained graphs (Marcu 2000; Webber et al 2003). We demonstrate
that in fact many coherence structures in naturally occurring texts cannot be adequately
represented by trees. Therefore we argue for less constrained graphs in which nodes
represent discourse segments and labeled directed arcs represent the coherence rela-
tions that hold between these discourse segments as an appropriate data structure for
representing coherence.
Some proponents of more general graphs argue that trees cannot account for a full
discourse structure that represents informational, intentional, and attentional discourse
relations. For example, Moore and Pollack (1992) point out that rhetorical structure
theory (Mann and Thompson 1988) has both informational and intentional coherence
relations but then forces annotators to decide on only one coherence relation between
any two discourse segments. Moore and Pollack argue that often there is an informa-
tional as well as an intentional coherence relation between two discourse segments,
which then presents a problem for RST, since only one of the relations can be annotated.
Instead, Moore and Pollack propose allowing more than one coherence relation between
two discourse segments, which violates the tree constraint of not having nodes with
multiple parents.
Reichman (1985) argues that tree-based story grammars are not sufficient to account
for discourse structure. Instead, she argues that in order to account for the intentional
structure of discourse, more general data structures are needed. We argue that the same
is true for the informational structure of discourse.
Moore and Pollack (1992), Moser and Moore (1996), and Reichman (1985) argue
that trees are insufficient for representing informational, intentional, and attentional
discourse structure. Note, however, that the focus of our work is on informational
coherence relations, not on intentional relations. That does not mean that we think that
attentional or intentional structure should not be part of a full account of discourse
structure. Rather, we would like to argue that whereas the above accounts argue against
trees for representing informational, intentional, and attentional discourse structure
together, we argue that trees are not even descriptively adequate to describe just in-
formational discourse structure by itself.
3 Although Lascarides and Asher (1991) do not explicitly disallow crossed dependencies, they argue that
when a discourse structure is being constructed, the right frontier of an already existing discourse
structure is the only possible attachment point for a new incoming discourse segment (cf. also Polanyi
1996; Polanyi and Scha 1984; Webber et al 1999). This constraint on building discourse structures
effectively disallows crossed dependencies.
262
Wolf and Gibson Representing Discourse Coherence
Some accounts of informational discourse structure do not assume tree structures
(e.g., Bergler [1991] and Hobbs [1985] for monologue and Penstein Rose et al [1995]
for dialogue structure). However, none of these accounts provides systematic empir-
ical support for using more general graphs rather than trees. Providing a systematic
empirical study of whether trees are descriptively adequate for representing discourse
coherence is the goal of this article.
There are also accounts of informational discourse structure that argue for trees
as a ?backbone? for discourse structure but allow certain violations of tree constraints
(crossed dependencies or nodes with multiple parents). Examples of such accounts
include Webber et al (1999) and Knott (1996). Similarly to our approach, Webber
et al (1999) investigated informational coherence relations. The kinds of coherence
relations they used are basically the same as those that we used (cf. also Hobbs 1985).
However, they argue for a tree structure as a backbone for discourse structure but
have also addressed violations of tree structure constraints. In order to accommodate
violations of tree structure constraints (in particular, crossed dependencies), Webber
et al (1999) argue for a distinction between ?structural? discourse relations, on the
one hand, and ?nonstructural? or ?anaphoric? discourse relations on the other hand.
Structural discourse relations are represented within a lexicalized tree-adjoining gram-
mar framework, and the resultant structural discourse structure is represented by a
tree. However, more recently, Webber et al (2003) have argued that structural discourse
structure should allow nodes with multiple parents, but no crossed dependencies. It is
unclear, however, why Webber et al (2003) allow one kind of tree constraint violation
(nodes with multiple parents) but not another (crossed dependencies).
Note that there seems to be a problem with the definition of ?structural? versus
?nonstructural? discourse structure in Webber et al (1999): According to Webber et al
(1999), nonstructural discourse relations are licensed by anaphoric relations and can
be involved in crossed dependencies. However, Webber et al (1999) also argue that
one criterion for nonstructural coherence relations is that they can cross (non)structural
coherence relations. Since this definition of ?nonstructural? appears to be circular, it
is necessary to find an independent way to validate the difference between structural
and nonstructural coherence relations. Knott (1996) might provide a way to empirically
formalize the claims in Webber et al (1999), or at least claims that seem to be very
similar to those in Webber et al (1999): Based on the observation that he cannot identify
characteristic cue phrases for elaboration relations (e.g., because would be a characteristic
cue phrase for cause?effect), Knott argues that elaboration relations are more permissive
than other types of coherence relations (e.g., cause?effect, similarity, contrast). As a conse-
quence, Knott argues, elaboration relations would be better described in terms of focus
structures (cf. Grosz and Sidner 1986), which Knott argues are less constrained, than in
terms of rhetorical relations (cf. Hobbs 1985; Mann and Thompson 1988), which Knott
argues are more constrained. This hypothesis makes testable empirical claims: Elabora-
tion relations should in some way pattern differently from other coherence relations. We
come back to this issue in sections 4.1 and 4.2.
In this article we present evidence against trees as a data structure for representing
discourse coherence. Note, though, that the evidence does not support the claim that
discourse structures are completely arbitrary. The goal of our research program is to first
determine which constraints on discourse structure are empirically viable. To us, the
work we present here seems to be the crucial first step in avoiding arbitrary constraints
on inferences for building discourse structures. In other words, the point we wish
to make here is that although there might be other constraints on possible discourse
annotations that will have to be identified in future research, tree structure constraints
263
Computational Linguistics Volume 31, Number 2
do not seem to be the right kinds of constraints. This appears to be a crucial difference
between approaches like Knott?s (1996), Marcu?s (2000), or Webber et al?s (2003), on
the one hand, and our approach, on the other hand. The goal of the former approaches
seems to be to first specify a set of constraints on possible discourse annotations and
then to annotate texts with these constraints in mind.
The following two sections illustrate problems with trees as a representation of
discourse coherence structures. Section 3.1 shows that the discourse structures of nat-
urally occurring texts contain crossed dependencies, which cannot be represented in
trees. Another problem for trees, in addition to crossed dependencies, is that many
nodes in coherence graphs of naturally occurring texts have multiple parents. This is
shown in section 3.2. Because of these problems for trees, we argue for a representation
such as chain graphs (cf. Frydenberg 1989; Lauritzen and Wermuth 1989), in which
directed arcs represent asymmetrical or directed coherence relations and undirected arcs
represent symmetrical or undirected coherence relations (this is equivalent to arguing
for directed graphs with cycles). For all the examples in sections 3.1 and 3.2, chain-
graph-based analyses are given. RST analyses are given only for those examples that
are also annotated by Carlson, Marcu, and Okurowski (2002) (in those cases, the RST
analyses are those provided by Carlson, Marcu, and Okurowski).
3.1 Crossed Dependencies
Consider the text passage in example (20) (modified from SAT practice materials):
(20) 1. Schools tried to teach students history of science.
2. At the same time they tried to teach them how to think logically and
inductively.
3. Some success has been reached in the first of these aims.
4. However, none at all has been reached in the second.
Figure 1 shows the coherence graph for example (20). Note that the arrowheads of the
arcs represent directionality for asymmetrical relations (elaboration) and bidirectionality
for symmetrical relations (similarity, contrast).
The coherence structure for example (20) can be derived as follows:
 Contrast relation between discourse segments 1 and 2: Discourse segments
1 and 2 describe teaching different things to students.
 Contrast relation between discourse segments 3 and 4: Discourse segments
3 and 4 describe varying degrees of success (some vs. none).
 Elaboration relation between discourse segments 3 and 1: Discourse
segment 3 provides more details (the degree of success) about the teaching
described in discourse segment 1.
Figure 1
Coherence graph for example (20). contr = contrast; elab = elaboration.
264
Wolf and Gibson Representing Discourse Coherence
 Elaboration relation between discourse segments 4 and 2: Discourse
segment 4 provides more details (the degree of success) about the teaching
described in discourse segment 2.
In the resultant coherence structure for (20), there is a crossed dependency between
{3, 1} and {4, 2}.
In order to be able to represent a structure like the one for (20) in a tree without
violating validity assumptions about tree structures (Diestel 2000), one might consider
augmenting a tree either with feature propagation (Shieber 1986) or with a coindex-
ation mechanism (Chomsky 1973). There is a problem, however, with both feature
propagation and coindexation mechanisms: Both the tree structure itself and the fea-
tures and coindexations as well represent the same kind of information (coherence
relations). It is unclear how a dividing line could be drawn between tree structures
and their augmentation. That is, it is unclear how one could decide which part of a
text coherence structure should be represented by the tree structure and which part
should be represented by the augmentation. Other areas of linguistics have faced this
issue as well. Researchers investigating data structures for representing intrasentential
structure, for instance, generally fall into two groups. One group tries to formulate
principles that allow representation of some aspects of structure in the tree itself
and other aspects in some augmentation formalism (e.g., Chomsky 1973; Marcus
et al 1994). Another group argues that it is more parsimonious to assume a unified
dependency-based representation that drops the tree constraints of allowing no crossed
dependencies (e.g., Brants et al 2002; Skut et al 1997; Ko?nig and Lezius 2000). Our
approach falls into the latter group. As we point out, there does not seem to be a well-
defined set of constraints on crossed dependencies in discourse structures. Without such
constraints, it does not seem viable to represent discourse structures as augmented tree
structures.
An important question is how many different kinds of crossed dependencies occur
in naturally occurring discourse. If there are only a very limited number of different
structures with crossed dependencies in natural texts, one could make special
provisions to account for these structures and otherwise assume tree structures.
Example (20), for instance, has a listlike structure. It is possible that listlike examples
are exceptional in natural texts. However, there are many other naturally occurring
nonlistlike structures that contain crossed dependencies. As an example of a nonlistlike
structure with a crossed dependency (between {4, 2} and {3, 1?2}), consider example
(21) (constructed):
(21) 1. Susan wanted to buy some tomatoes
2. and she also tried to find some basil
3. because her recipe asked for these ingredients.
4. The basil would probably be quite expensive at this time of the year.
The coherence structure for (21), shown in Figure 2, can be derived as follows:
 Similarity relation between 1 and 2: 1 and 2 both describe shopping for
grocery items.
 Cause?effect relation between 3 and 1?2: 3 describes the cause for the
shopping described by 1 and 2.
265
Computational Linguistics Volume 31, Number 2
Figure 2
Coherence graph for example (21). sim = similarity; ce = cause?effect; elab = elaboration.
 Elaboration relation between 4 and 2: 4 provides details about the basil
in 2.
Example (22), (from ap890109-0012; AP Newswire 1989 corpus [Harman and
Liberman 1993]) has a similar structure:
(22) 1. The flight Sunday took off from Heathrow Airport at 7:52pm
2. and its engine caught fire 10 minutes later,
3. the Department of Transport said.
4. The pilot told the control tower he had the engine fire under
control.
The coherence structure for example (22) can be derived as follows:
 Temporal sequence relation between 1 and 2: 1 describes the takeoff that
happens before the engine fire described by 2 occurs.
 Attribution relation between 3 and 1?2: 3 mentions the source of what is
said in 1?2.
 Elaboration relation between 4 and 2: 4 provides more detail about the
engine fire in 2.
The resulting coherence structure, shown in Figure 3, contains a crossed dependency
between {4, 2} and {3, 1?2}.
Consider example (23) (from wsj 0655; Wall Street Journal 1989 corpus [Harman
and Liberman 1993]):
(23) 1. 1a[ Mr. Baker?s assistant for inter-American affairs, ] 1b[ Bernard
Aronson, ]
2. while maintaining
3. that the Sandinistas had also broken the cease-fire,
4. acknowledged:
5. ?It?s never very clear who starts what.?
Figure 3
Coherence graph for example (22). ts = temporal sequence; attr = attribution; elab = elaboration.
266
Wolf and Gibson Representing Discourse Coherence
Figure 4
Coherence graph for example (23). expv = violated expectation; elab = elaboration; attr = attribution.
Figure 5
Coherence graph for example (23) with discourse segment 1 split into two segments. expv =
violated expectation; elab = elaboration; attr = attribution.
Figure 6
Tree-based RST annotation for example (23) from Carlson, Marcu, and Okurowski (2002). Broken
lines represent the start of asymmetric coherence relations; continuous lines represent the end of
asymmetric coherence relations; symmetric coherence relations have two continuous lines
(cf. section 2.3). attr = attribution; elab = elaboration.
The annotations based on our annotation scheme with the discourse segmentation
based on the segmentation guidelines in Carlson, Marcu, and Okurowski (2002) are
presented in Figure 4, and those with the discourse segmentation based on our
segmentation guidelines from section 2.1 are presented in Figure 5. Figure 6 shows
a tree-based RST annotation for example (23) from Carlson, Marcu, and Okurowski
(2002). The only difference between our approach and that of Carlson, Marcu, and
Okurowski with respect to how example (23) is segmented is that Carlson and her
colleagues assume discourse segment 1 to be one single segment. By contrast, based
on our segmentation guidelines, discourse segment 1 would be segmented into two
segments (because of the comma that does not separate a complex NP or VP), 1a and
1b, as indicated by the brackets in example (24):4
4 Based on our segmentation guidelines, the complementizer that in discourse segment 3 would be part of
discourse segment 2 instead (cf. (15)). However, since this would not make a difference in terms of the
resulting discourse structure, we do not provide alternative analyses with that as part of discourse
segment 2 instead of discourse segment 3.
267
Computational Linguistics Volume 31, Number 2
(24) 1a[ Mr. Baker?s assistant for inter-American affairs, ] 1b[ Bernard Aronson, ]
The coherence structure for example (23) can be derived as follows:
 If discourse segment 1 is segmented into 1a and 1b (following our
discourse segmentation guidelines), elaboration relation between 1a and
1b: 1b provides additional detail (a name) about what is stated in 1a
(Mr. Baker?s assistant).
 Same relation between 1 (or 1a) and 4: The subject NP in 1 (Mr. Baker?s
assistant) is separated from its predicate in 4 (acknowledged) by
intervening discoure segments 2 and 3 (and 1b in our discourse
segmentation).
 Attribution relation between 2 and 3: 2 states the source (the elided
Mr. Baker) of what is stated in 3.
 Elaboration relation between the group of discourse segments 2 and 3 and
discourse segment 1 (or the group of discourse segments 1a and 1b in our
discourse segmentation): 2 and 3 state additional detail (a statement about
a political process) about what is stated in 1 (or 1a and 1b) (Mr. Baker?s
assistant).
 Attribution relation between 4 (and by virtue of the same relation, also
1 or 1a) and 5: 4 states the source (Mr. Baker?s assistant) of what is stated
in 5.
 Violated expectation relation between the group of discourse segments 2
and 3 and the group of discourse segments 4 and 5: Although Mr. Baker?s
assistant acknowledges cease-fire violations by one side (discourse
segments 2 and 3), he acknowledges that it is in fact difficult to clearly
blame one side for cease-fire violations (discourse segments
4 and 5).
The resulting coherence structure, shown in Figure 5 (discourse segmentation from
Carlson, Marcu, and Okurowski [2002]) and Figure 6 (our discourse segmentation),
contains a crossed dependency: The same relation between discourse segment 1 and dis-
course segment 4 crosses the violated expectation relation between the group of discourse
segments 2 and 3 and the group of discourse segments 4 and 5.
Figure 6 represents a tree-based RST annotation for example (23) from Carlson,
Marcu, and Okurowski (2002); in Figure 6, dashed lines represent the start of asym-
metric coherence relations and continuous lines mark the end of asymmetric coherence
relations; symmetric coherence relations have two continuous lines (cf. section 2.3 for
the distinction between symmetric and asymmetric coherence relations and for the
directions of asymmetric coherence relations). Carlson, Marcu, and Okurowski (2002)
do not provide descriptions of how they derived tree-based RST structures for their
examples that are used in this article. Therefore, instead of discussing how the tree-
based RST structures were derived, we show comparisons of the RST structure and
our chain-graph-based structure; the comparison for (23) is provided in Table 5. Note
in particular that the RST structure for example (23) does not represent the violated
expectation relation between 2?3 and 4?5; that relation could not be annotated without
violating the tree constraint of not allowing crossed dependencies.
268
Wolf and Gibson Representing Discourse Coherence
Table 5
Comparison for example (23) of tree-based RST structure from Carlson, Marcu, and Okurowski
(2002) and our chain-graph-based structure.
Tree-based RST structure Our chain-graph-based structure
(1a and 1b are one discourse segment) Elaboration between 1a and 1b
Same between 1?2 and 4 Same between 1 (or 1a) and 4
Attribution between 1 and 2 Attribution between 1 and 2
Elaboration between 2?3 and 1 Elaboration between 2?3 and 1 (or 1a and 1b)
Attribution between 1?4 and 5 Attribution between 4 and 5
(no relation) Violated expectation between 2?3 and 4?5
Figure 7
Coherence graph for example (25). cond = condition; attr = attribution; elab = elaboration.
3.2 Nodes with Multiple Parents
In addition to including crossed dependencies, many coherence structures of natural
texts include nodes with multiple parents. Such nodes cannot be represented in tree
structures. Consider example (25) (from ap890103 = 0014; AP Newswire 1989 corpus
[Harman and Liberman 1993]).
(25) 1. ?Sure I?ll be polite,?
2. promised one BMW driver
3. who gave his name only as Rudolf.
4. ?As long as the trucks and the timid stay out of the left lane.?
The coherence structure for example (25) can be derived as follows:
 Attribution relation between 2 and 1 and 2 and 4: 2 states the source of
what is stated in 1 and 4, respectively.
 Elaboration relation between 3 and 2: 3 provides additional detail (the
name) about the BMW driver in 2.
 Condition relation between 4 and 1: 4 states the BMW driver?s condition for
being polite, stated in 1.5 This condition relation is also indicated by the
phrase ?as long as.?
In the resultant coherence structure for example (25), node 1 has two parents?one
attribution and one condition ingoing arc (cf. Figure 7).
5 A cultural reference: In Germany, when driving on a highway, it is only lawful to pass on the left side.
Thus, Rudolf is essentially saying that he will be polite as long as the trucks and the timid do not keep
him from passing other cars.
269
Computational Linguistics Volume 31, Number 2
Figure 8
Coherence graph for example (26). Additional coherence relation used (from Carlson, Marcu,
and Okurowski [2002]): evaluation-s = the situation presented in the satellite assesses the
situation presented in the nucleus (evaluation-s would be elaboration in our annotation scheme).
attr = attribution; cond = condition.
Figure 9
Coherence graph for example (26) with discourse segments 1 and 2 merged into one single
discourse segment. Additional coherence relation used (from Carlson, Marcu, and Okurowski
[2002]): evaluation-s = the situation presented in the satellite assesses the situation presented in
the nucleus (evaluation-s would be elaboration in our annotation scheme). attr = attribution;
cond = condition.
As another example of a discourse structure that contains nodes with multiple
parents, consider the structure of example (26) (from wsj 0655; Wall Street Journal 1989
corpus [Harman and Liberman 1993]):
(26) (they in 4 and 6 = Contra supporters; this is clear from the whole text
wsj 0655)
1. ?The administration should now state
2. that
3. if the February election is voided by the Sandinistas
4. they should call for military aid,?
5. said former Assistant Secretary of State Elliott Abrams.
6. ?In these circumstances, I think they?d win.?
Our annotations are shown in Figures 8 (discourse segmentation from Carlson, Marcu,
and Okurowski [2002]) and 9 (our discourse segmentation); Carlson et al?s (2002) tree-
based RST annotation is shown in Figure 10. The only difference between our annotation
and that of Carlson, Marcu, and Okurowski is that we do not assume two separate
discourse segments for 1 and 2; 1 and 2 are one discourse segment in our annotation
(represented by the node 1+2 in Figure 9). Note also that in discourse segment 3 of
example (23) ?that? is not in a separate discourse segment; it is unclear why in example
(26), ?that? is in a separate discourse segment (discourse segment 2) and not part of
discourse segment 3. The discourse structure for example (26) can be derived as follows:
1. According to our discourse segmentation guidelines (cf. section 2.1), 1 and
2 should be one single discourse segment: Therefore either same relation
between 1 and 2 (cf. Figure 8), or merge 1 and 2 into one single discourse
segment, 1+2 (cf. Figure 9).
270
Wolf and Gibson Representing Discourse Coherence
Figure 10
Tree-based RST annotation for example (26) from Carlson, Marcu, and Okurowski (2002). Broken
lines represent the start of asymmetric coherence relations; continuous lines represent the end of
asymmetric coherence relations; symmetric coherence relations have two continuous lines (cf.
section 2.3). Additional coherence relation used (from Carlson, Marcu, and Okurowski [2002]):
evaluation-s = the situation presented in the satellite assesses the situation presented in the
nucleus (evaluation-s would be elaboration in our annotation scheme). attr = attribution;
cond = condition.
2. Attribution relation between 1 or 1+2 and 3?4: 1 or 1+2 state the source (the
administration) of what is stated in 3?4.
3. Condition relation between 3 and 4: 3 states the condition for what is stated
in 4 (the condition relation is also signaled by the cue phrase if in 3).
4. Attribution relation between 5 and 1?4: 5 states the source of what is stated
in 1?4.
5. Attribution relation between 5 and 6: 5 states the source of what is stated in 6.
6. Evaluation-s6 relation between 6 and 3?4: 3?4 state what is evaluated by
6?the Contra supporters should call for military aid, and if the February
election is voided (group of discourse segments 3?4), the Contra
supporters might win (discourse segment 6). Note that in our annotation
scheme, the evaluation-s relation would be an elaboration relation (6
provides additional detail about 3?4: Elliott Abrams?s opinion on the
Contras? chances of winning).
In the resultant coherence structure for example (26), node 3?4 has multiple parents or
ingoing arcs: one attribution ingoing arc and one evaluation-s ingoing arc (cf. Figures 8
and 9).
Table 6 presents a comparison of the RST annotation and our chain-graph-based
annotation for (26). Note in particular that the attribution relation between 5 and 6 cannot
be represented in the RST tree structure. Note furthermore that the RST tree contains an
evaluation-s relation between 6 and 1?5. However, this evaluation-s relation seems to hold
rather between 6 and 3?4: What is being evaluated is a chance for the Contras to win
6 The relation evaluation-s is part of the annotation scheme in Carlson, Marcu, and Okurowski (2002) but
not part of our annotation scheme. In an evaluation-s relation, the situation presented in the satellite
assesses the situation presented in the nucleus (Carlson, Marcu, and Okurowski 2002). An evaluation-s
relation would be an elaboration relation in our annotation scheme.
271
Computational Linguistics Volume 31, Number 2
Table 6
Comparison for (26) of tree-based RST structure (from Carlson, Marcu, and Okurowski (2002)
and our chain-graph-based structure.
Tree-based RST structure Our chain-graph-based structure
Same between 2 and 3?4 Same between 1 and 2, or merging of 1 and 2 to 1+2
Attribution between 1 and 2?4 Attribution between 1 or 1+2 and 3?4
Condition between 3 and 4 Condition between 3 and 4
Attribution between 5 and 1?4 Attribution between 5 and 1?4
(no relation) Attribution between 5 and 6
Evaluation-s between 6 and 1?5 Evaluation-s between 6 and 3?4
a military conflict under certain circumstances. But a coherence relation between 6 and
3?4 could not have been annotated in a tree structure.
4. Statistics
We performed a number of statistical analyses on our annotated database to test our
hypotheses. Each set of statistics was calculated for both annotators separately. How-
ever, since the statistics for both annotators were never different from each other (as
confirmed by significant R2s > 0.9 or by ?2s > 1), we report only the statistics for one
annotator in the following sections.
An important question is how frequent the phenomena discussed in the previous
sections are. The more frequent they are, the more urgent the need for a data structure
that can adequately represent them. The following sections report statistical results on
crossed dependencies (section 4.1) and nodes with multiple parents (section 4.2).
4.1 Crossed Dependencies
The following sections report counts on crossed dependencies in the annotated database
of 135 texts (cf. section 1). Section 4.1.1 reports results on the frequency of crossed
dependencies, section 4.1.2 reports results concerning the question of what types of
coherence relations tend to be involved in crossed dependencies, and section 4.1.3
reports results on the arc lengths of coherence relations involved in crossed depen-
dencies. Section 4.1.4 provides a short summary of the statistical results on crossed
dependencies.
4.1.1 Frequency of Crossed Dependencies. In order to track the frequency of crossed
dependencies for the coherence structure graph of each text, we counted the minimum
number of arcs that would have to be deleted in order to eliminate crossed dependen-
cies in the coherence structure. Figure 11 illustrates this process. The example graph
depicted in the figure contains the following crossed dependencies: {1, 3} crosses with
{2, 4}, {3, 5} with {2, 4}, and {5, 7} with {6, 8}. By deleting {2, 4}, two crossed
dependencies can be eliminated: the crossing of {1, 3} with {2, 4} and the crossing of
{3, 5} with {2, 4}. By deleting either {5, 7} or {6, 8} the remaining crossed dependency
between {5, 7} and {6, 8} can be eliminated. Therefore two edges would have to be
deleted from the graph in Figure 11 in order to make it free of crossed dependencies.
272
Wolf and Gibson Representing Discourse Coherence
Figure 11
Example graph with crossed dependencies.
Figure 12
Correlation between number of arcs and number of crossed dependencies.
Table 7
Percentages of arcs to be deleted in order to eliminate crossed dependencies in the database texts.
Mean 12.5
Minimum 0
Maximum 44.4
Median 10.9
Table 7 shows the results of the counts. On average for the 135 annotated texts,
12.5% of arcs in a coherence graph have to be deleted in order to make the graph free
of crossed dependencies. Seven texts out of the 135 had no crossed dependencies. The
mean number of arcs for the coherence graphs of these texts was 36.9 (minimum: 8,
maximum: 69, median: 35). The mean number of arcs for the other 128 coherence graphs
(those with crossed dependencies) was 125.7 (minimum: 20, maximum: 293, median:
115.5). Thus, the graphs with no crossed dependencies had significantly fewer arcs
than the graphs that had crossed dependencies (?2(1) = 15,330.35 (Yates?s correction
for continuity applied), p < 10?6). This is a likely explanation for why these seven texts
had no crossed dependencies.
More generally, linear regressions show a correlation between the number of arcs in
a coherence graph and the number of crossed dependencies. The more arcs a graph has,
the higher the number of crossed dependencies (R2 = 0.39, p < 10?4; cf. Figure 12). The
same linear correlation holds between text length and number of crossed dependencies:
The longer a text, the more crossed dependencies are in its coherence structure graph
(for text length in discourse segments: R2 = .29, p < 10?4; for text length in words:
R2 = .24, p < 10?4).
4.1.2 Types of Coherence Relations Involved in Crossed Dependencies. In addition
to the question of how frequent crossed dependencies are, another question is whether
273
Computational Linguistics Volume 31, Number 2
Table 8
Percentages of arcs to be deleted in order to eliminate crossed dependencies.
Coherence relation Percentage of coherence Percentage of overall Factor
relations participating in coherence relations (= overall/crossed
crossed dependencies dependencies)
Same 1.13 17.21 15.23
Condition 0.05 0.28 5.59
Attribution 1.93 6.31 3.27
Temporal sequence 0.94 1.56 1.66
Generalization 0.24 0.34 1.40
Contrast 5.84 7.93 1.36
Cause?effect 1.13 1.53 1.35
Violated expectation 0.61 0.82 1.40
Elaboration 50.52 37.97 0.71
Example 4.43 3.15 1.34
Similarity 33.18 22.91 0.69
there are certain types of coherence relations that participate more or less frequently in
crossed dependencies than other types of coherence relations. For an arc to participate
in a crossed dependency, it must be in the set of arcs that would have to be deleted from
a coherence graph in order to make that graph free of crossed dependencies (cf. the
procedure outlined in section 4.1.1). In other words, the question is whether the fre-
quency distribution over types of coherence relations is different for arcs participating
in crossed dependencies compared to the overall frequency distribution over types of
coherence relations in the whole database.
Figure 13 shows that the overall distribution over types of coherence relations
participating in crossed dependencies is not different from the distribution over types of
coherence relations overall. This is confirmed by the results of a linear regression, which
show a significant correlation between the two distributions of percentages (R2 = 0.84,
p < .0001). Note that the overall distribution includes only arcs with length greater than
one, since arcs of length one cannot participate in crossed dependencies.
However, there are some differences for individual coherence relations. Some types
of coherence relations occur considerably less frequently in crossed dependencies than
overall in the database. Table 8 shows the data from Figure 13 ranked by the factor
of ?percentage of overall coherence relations? by ?percentage of coherence relations
participating in crossed dependencies.? The proportion of same relations, for instance, is
15.23 times greater, and the percentage of condition relations is 5.59 times greater, overall
in the database than in crossed dependencies. We do not yet understand the reason for
these differences and plan to address this question in future research.
Another way of testing whether certain coherence relations contribute more than
others to crossed dependencies is to remove coherence relations of a certain type from
the database and then count the remaining number of crossed dependencies. For exam-
ple, it is possible that the number of crossed dependencies is reduced once all elaboration
relations are removed from the database. Table 9 shows that by removing all elaboration
relations from the database of 135 annotated texts, the percentage of coherence relations
involved in crossed dependencies is reduced from 12.5% to 4.96% of the remaining
coherence relations. That percentage is reduced even further, to 0.84%, by removing all
elaboration and similarity relations from the database. These numbers seem to be partial
support for Knott?s (1996) hypothesis: Knott argued that elaboration relations are less
274
Wolf and Gibson Representing Discourse Coherence
Fi
gu
re
13
D
is
tr
ib
u
ti
on
s
ov
er
ty
p
es
of
co
he
re
nc
e
re
la
ti
on
s.
Fo
r
ea
ch
co
nd
it
io
n
(?
ov
er
al
ls
ta
ti
st
ic
s?
an
d
?c
ro
ss
ed
-d
ep
en
d
en
ci
es
st
at
is
ti
cs
?)
,t
he
su
m
ov
er
al
l
co
he
re
nc
e
re
la
ti
on
s
is
10
0;
ea
ch
ba
r
in
ea
ch
co
nd
it
io
n
re
p
re
se
nt
s
a
fr
ac
ti
on
of
th
e
to
ta
lo
f1
00
in
th
at
co
nd
it
io
n.
T
he
y-
ax
is
u
se
s
a
lo
g 1
0
sc
al
e.
at
tr
=
at
tr
ib
ut
io
n;
ce
=
ca
us
e?
ef
fe
ct
;c
on
d
=
co
nd
it
io
n;
co
nt
r
=
co
nt
ra
st
;e
la
b
=
el
ab
or
at
io
n;
ex
am
p
=
ex
am
pl
e;
ex
pv
=
V
io
la
te
d
ex
pe
ct
at
io
n;
ge
n
=
ge
ne
ra
liz
at
io
n;
si
m
=
si
m
ila
ri
ty
;t
s
=
te
m
po
ra
ls
eq
ue
nc
e.
275
Computational Linguistics Volume 31, Number 2
Table 9
Effect of removing different types of coherence relations on the percentage of coherence relations
involved in crossed dependencies.
Remaining percentage of coherence relations
involved in crossed dependencies
Coherence relation removed Mean Min Max Median
Same 13.08 0 44.44 11.39
Condition 12.63 0 45.28 10.89
Attribution 13.44 0 44.86 11.36
Temporal sequence 12.53 0 44.44 10.87
Generalization 12.53 0 44.44 10.84
Contrast 11.88 0 46.15 9.86
Cause?effect 12.67 0 49.47 11.03
Violated expectation 12.51 0 44.44 10.87
Elaboration 4.96 0 47.47 1.23
Example 12.08 0 44.44 9.89
Similarity 7.32 0 24.56 7.04
Elaboration and similarity 0.84 0 10.68 0.00
constrained than other types of coherence relations (cf. the discussion of Knott [1996] in
section 3).
However, there is a possible alternative hypothesis to Knott?s (1996). In particular,
elaboration relations are very frequent (37.97% of all coherence relations; cf. Table 8). It
is possible that removing elaboration relations from the database reduces the number of
crossed dependencies only because a large number of coherence relations are removed
when elaborations are removed. In other words, an alternative hypothesis to that of
Knott (1996) is that the lower number of crossed dependencies is just due to less-
dense coherence graphs (i.e., the less dense coherence graphs are, the lower the chance
for crossed dependencies). We tested this hypothesis by correlating the percentage of
coherence relations removed with the percentage of crossed dependencies that remain
after removing a certain type of coherence relation.7 Figure 14 shows that the higher
the percentage of removed coherence relations, the lower the percentage of coherence
relations becomes that are involved in crossed dependencies. This correlation is con-
firmed by a linear regression (R2 = 0.7697, p < .0005; after removing the elaboration data
point: R2 = 0.4504, p < .05; these linear regressions do not include the data point elabora-
tion + similarity). Thus, although removing certain types of coherence relations reduces
the number of crossed dependencies, it results in a very impoverished representation of
coherence structure (i.e., after removing all elaboration and all similarity relations, only
39.12% of all coherence relations would still be represented [cf. Table 8]; the figure is
52.13% based on the distribution over coherence relations including those with absolute
arc length one [cf. Table 11]).
With respect to Knott?s (1996) hypothesis, note that leaving out elaboration relations
still leaves the proportion of remaining crossed dependencies at 4.96% (cf. Table 9).
7 Note that the percentages of removed coherence relations do not include coherence relations of absolute
arc length one, since removing those coherence relations cannot have any influence on the number of
crossed dependencies (coherence relations of absolute arc length one cannot be involved in crossed
dependencies). Thus, the percentages of coherence relations removed in Figure 14 are from the third
column of Table 8.
276
Wolf and Gibson Representing Discourse Coherence
Figure 14
Correlation between removed percentage of overall coherence relations and remaining
percentage of crossed dependencies. Note that the data point for elaboration + similarity is not
included in the figure. R2 = 0.7699, p < .0005.
In order to further reduce the proportion of remaining crossed dependencies, it is
necessary to remove similarity relations in addition to removing elaboration relations
(cf. Table 9). This is a pattern of results that is not predicted by any literature that we
are aware of (including Knott [1996], among others, although he predicts these results
partially). We believe this issue should be addressed in future research.
4.1.3 Arc Lengths of Coherence Relations Involved in Crossed Dependencies. An-
other question is how great the distance typically is between discourse segments that
participate in crossed dependencies, or how great the arc length is for coherence
relations that participate in crossed dependencies.8 It is possible, for instance, that
crossed dependencies primarily involve long-distance arcs and that more local crossed
dependencies are disfavored. However, Figure 15 shows that the distribution over arc
lengths is practically identical for the overall database and for coherence relations par-
ticipating in crossed dependencies (linear regression: R2 = 0.937, p < 10?4), suggesting
a strong locality bias for coherence relations overall as well as for those participating
in crossed dependencies.9 The arc lengths are normalized in order to take into account
the varying length of texts. Normalized arc length is calculated by dividing the absolute
length of an arc by the maximum length that that arc could have, given its position in
its text. For example, if there is a coherence relation between discourse segment 1 and
discourse segment 4 in a text, the raw distance between them would be three. If these
discourse segments are part of a text that has five discourse segments total (i.e., 1 to 5),
8 The distance between two discourse segments is not measured in terms of how many coherence links one
has to follow from any discourse segment x to any discourse segment y to which discourse segment x is
related via a coherence relation. Instead, distance is measured in terms of the number of intervening
discourse segments. Thus, distance between nodes reflects linear distance between two discourse
segments in a text. For example, the distance between a discourse segment 1 and a discourse segment 4
would be three.
9 The arc length distribution for the database overall does not include arcs of (absolute) length one, since
such arcs cannot participate in crossed dependencies.
277
Computational Linguistics Volume 31, Number 2
Figure 15
Comparison of normalized arc length distributions. For each condition (?overall statistics? and
?crossed-dependencies statistics?), the sum over all coherence relations is 100; each bar in each
condition represents a fraction of the total of 100 in that condition.
the normalized distance would be 3/4 = 0.75 (because four would be the maximum
possible length of an arc that originates in discourse segment 1 or 4, given that the text
has five discourse segments in total).
4.1.4 Summary of Crossed-Dependencies Statistics. Taken together, the statistical re-
sults on crossed dependencies suggest that crossed dependencies are too frequent to
be ignored by accounts of coherence. Furthermore, the results suggest that any type of
coherence relation can participate in a crossed dependency. However, there are some
cases in which knowing the type of coherence relation that an arc represents can be
informative as to how likely that arc is to participate in a crossed dependency. The
statistical results reported here also suggest that crossed dependencies occur primarily
locally, as evidenced by the distribution over lengths of arcs participating in crossed
dependencies.
4.2 Nodes with Multiple Parents
Section 3.2 provided examples of coherence structure graphs that contain nodes with
multiple parents. In addition to crossed dependencies, nodes with multiple parents are
another reason why trees are inadequate for representing natural language coherence
structures. The following sections report statistical results from our database on nodes
with multiple parents. As in the previous section on crossed dependencies, we report
results on the frequency of nodes with multiple parents (section 4.2.1), the types of
coherence relations ingoing to nodes with multiple parents (section 4.2.2), and the arc
length of coherence relations ingoing to nodes with multiple parents (section 4.2.3).
Table 10
In-degree of nodes in the overall database.
Mean 1.60
Minimum 1
Maximum 12
Median 1
278
Wolf and Gibson Representing Discourse Coherence
Figure 16
Correlation between number of arcs and number of nodes with multiple parents.
Section 4.2.4 provides a short summary of the statistical results on nodes with multiple
parents.
4.2.1 Frequency of Nodes with Multiple Parents. We determined the frequency of
nodes with multiple parents by counting the number of nodes with in-degree greater
than one. We assume nodes with in-degree greater than one in a graph to be the equiv-
alent of nodes with multiple parents in a tree. The results of our count indicated that
41.22% of all nodes in the database have an in-degree greater than one. In addition to
counting the number of nodes with in-degree greater than one, we determined the mean
in-degree of the nodes in our database. Table 10 shows that the mean in-degree (= mean
number of parents) of all nodes in the investigated database of 135 texts is 1.6. As for co-
herence relations involved in crossed dependencies (cf. section 4.1.1), a linear regression
showed a significant correlation between the number of arcs in a coherence graph and
the number of nodes with multiple parents (cf. Figure 16; R2 = 0.7258, p < 10?4; for text
length in discourse segments: R2 = .6999, p < 10?4; for text length in words: R2 = .6022,
p < 10?4). The proportion of nodes with in-degree greater than one and the mean in-
degree of the nodes in our database suggest that even if a mechanism could be derived
for representing crossed dependencies in (augmented) tree graphs, nodes with multiple
parents present another significant problem for trees representing coherence structures.
4.2.2 Types of Coherence Relations Ingoing to Nodes with Multiple Parents. As
with crossed dependencies, an important question is whether there are certain types
of coherence relations that are more or less frequently ingoing to nodes with mul-
tiple parents than other types of coherence relations. In other words, the question
is whether the frequency distribution over types of coherence relations is different
for arcs ingoing to nodes with multiple parents compared to the overall frequency
distribution over types of coherence relations in the whole database. Figure 17 shows
that the overall distribution over types of coherence relations ingoing to nodes with
multiple parents is not different from the distribution over types of coherence rela-
tions overall.10 This is confirmed by the results of a linear regression, which show
10 Note that, unlike in section 4.1.2, the distribution over coherence relations for all coherence relations
includes arcs with length one, since there was in this case no reason to exclude them.
279
Computational Linguistics Volume 31, Number 2
Table 11
Proportion of coherence relations.
Coherence relation Percentage of Percentage of Factor (= overall/
coherence relations overall coherence ingoing to nodes with
ingoing to nodes with relations multiple parents)
multiple parents
Attribution 7.38 12.68 1.72
Cause?effect 2.63 4.19 1.59
Temporal sequence 1.38 2.11 1.53
Condition 0.83 1.21 1.46
Violated expectation 0.90 1.13 1.26
Generalization 0.17 0.21 1.22
Contrast 6.72 7.62 1.13
Same 10.72 9.74 0.91
Similarity 20.22 20.79 1.03
Elaboration 45.83 38.13 0.83
Example 3.20 2.19 0.68
a significant correlation between the two distributions of percentages (R2 = 0.967,
p < 10?4).
Unlike for crossed dependencies (cf. Table 8), there are no big differences for indi-
vidual coherence relations. Table 11 shows the data from Figure 17, ranked by the factor
of ?percentage of overall coherence relations? by ?percentage of coherence relations
ingoing to nodes with multiple parents.?
As for crossed dependencies, we also tested whether removing certain kinds of
coherence relations reduced the mean in-degree (number of parents) and/or the per-
centage of nodes with in-degree greater than one (more than one parent). Table 12 shows
that removing all elaboration relations from the database reduces the mean in-degree
of nodes from 1.60 to 1.238 and the percentage of nodes with in-degree greater than
one from 41.22% to 20.29%. Removing all elaboration as well as all similarity relations
reduces these numbers further to 1.142 and 11.24%, respectively. As Table 12 also shows,
removing other types of coherence relations does not lead to as great a reduction in the
mean in-degree and the percentage of nodes with in-degree greater than one.
However, as with crossed dependencies (cf. section 4.1.2), we also tested whether
the reduction in nodes with multiple parents could simply be due to removing more
and more coherence relations (i.e., the less dense a graph is, the smaller the chance
that there are nodes with multiple parents). We correlated the percentage of coherence
relations removed with the mean in-degree of the nodes after removing different types
of coherence relations.11 Figure 18 shows that the higher the percentage of removed
coherence relations, the lower the mean in-degree of the nodes in the database becomes.
This correlation is confirmed by the results of a linear regression (R2 = 0.9455, p < 10?4;
after removing the elaboration data point: R2 = 0.8310, p < .0005; note that these linear
regressions do not include the data point elaboration + similarity). We also correlated
11 Note that in the correlations in this section, the proportions of removed coherence relations include
coherence relations of absolute arc length one, because removing these coherence relations also has an
effect on the mean in-degree of nodes and the proportion of nodes with in-degree greater than one. Thus,
the proportions of coherence relations removed in Figure 18 and in Figure 19 are from the third column of
Table 11.
280
Wolf and Gibson Representing Discourse Coherence
Fi
gu
re
17
D
is
tr
ib
u
ti
on
s
ov
er
ty
p
es
of
co
he
re
nc
e
re
la
ti
on
s.
Fo
r
ea
ch
co
nd
it
io
n
(?
ov
er
al
ls
ta
ti
st
ic
s?
an
d
?i
ng
oi
ng
to
no
d
es
w
it
h
m
u
lt
ip
le
p
ar
en
ts
?)
,t
he
su
m
ov
er
al
l
co
he
re
nc
e
re
la
ti
on
s
is
10
0;
ea
ch
ba
r
in
ea
ch
co
nd
it
io
n
re
p
re
se
nt
s
a
fr
ac
ti
on
of
th
e
to
ta
lo
f1
00
in
th
at
co
nd
it
io
n.
T
he
y-
ax
is
u
se
s
a
lo
g 1
0
sc
al
e.
at
tr
=
at
tr
ib
ut
io
n;
ce
=
ca
us
e?
ef
fe
ct
;c
on
d
=
co
nd
it
io
n;
co
nt
r
=
co
nt
ra
st
;e
la
b
=
el
ab
or
at
io
n;
ex
am
p
=
ex
am
pl
e;
ex
pv
=
V
io
la
te
d
ex
pe
ct
at
io
n;
ge
n
=
ge
ne
ra
liz
at
io
n;
si
m
=
si
m
ila
ri
ty
;t
s
=
te
m
po
ra
ls
eq
ue
nc
e.
281
Computational Linguistics Volume 31, Number 2
Table 12
Effect of removing different types of coherence relations on the mean in-degree of nodes and on
the percentage of nodes with in-degree greater than 1.
Coherence relation removed In-degree of nodes Percentage of nodes with
in-degree > 1
Mean Min Max Median
Same 1.519 1 12 1 35.85
Condition 1.599 1 12 1 41.01
Attribution 1.604 1 12 1 41.18
Temporal sequence 1.599 1 12 1 41.12
Generalization 1.600 1 12 1 41.16
Contrast 1.569 1 12 1 39.45
Cause?effect 1.599 1 12 1 41.14
Violated expectation 1.598 1 12 1 40.96
Elaboration 1.238 1 11 1 20.29
Example 1.574 1 11 1 40.37
Similarity 1.544 1 12 1 36.25
Elaboration and similarity 1.142 1 11 1 11.24
Figure 18
Correlation between percentage of removed coherence relations and mean in-degree of
remaining nodes. Note that the data point for elaboration + similarity is not included in the figure.
R2 = 0.9455, p < 10?4.
the percentage of coherence relations removed with the percentage of nodes with in-
degree greater than one after removing different types of coherence relations. Figure 19
shows that the higher the percentage of removed coherence relations, the lower the
percentage of nodes with in-degree greater than one. This correlation is also confirmed
by the results of a linear regression (R2 = 0.9574, p < 10?4; after removing the elaboration
data point: R2 = 0.8146, p < .0005; note that these correlations do not include the data
point elaboration + similarity).
Thus, although removing certain types of coherence relations (the same ones as
for crossed dependencies, i.e., elaboration and similarity; cf. section 4.1.2) can reduce the
mean in-degree of nodes and the proportion of nodes with in-degree greater than one,
the result is a very impoverished coherence structure. For example, after removing both
282
Wolf and Gibson Representing Discourse Coherence
Figure 19
Correlation between percentage of removed coherence relations and percentage of nodes with
in-degree > 1. Note that the data point for elaboration + similarity is not included in the figure.
R2 = 0.9574, p < 10?4.
elaboration and similarity relations, only 52.13% of all coherence relations would still be
represented (cf. Table 11). Furthermore, note that this pattern of results is not predicted
by any literature we are aware of, including Knott (1996), although he predicts the
results partially (he predicts that removing elaboration relations but not that removing
elaboration as well as similarity relations is necessary in order to remove basically all
nodes with multiple parents; cf. the discussion in the last paragraph of section 4.1.2).
This issue will have to be investigated in future research.
4.2.3 Arc Lengths of Coherence Relations Ingoing to Nodes with Multiple Parents.
As for crossed dependencies, we also compared arc lengths. Here, we compared the
length of arcs that are ingoing to nodes with multiple parents to the overall distribution
of arc lengths. Again, we compared normalized arc lengths (see section 4.1.3 for the
normalization procedure). By contrast to the comparison for crossed dependencies,
we included in this comparison arcs of (absolute) length one, because such arcs can
be ingoing to nodes with either single or multiple parents. Figure 20 shows that the
distribution over arc lengths is practically identical for the overall database and for
arcs ingoing to nodes with multiple parents (linear regression: R2 = 0.993, p < 10?4),
suggesting a strong locality bias for coherence relations overall as well as for those
participating in crossed dependencies.
4.2.4 Summary of Statistical Results on Nodes with Multiple Parents. In sum, the
statistical results on nodes with multiple parents suggest that they are a frequent phe-
nomenon and that they are not limited to certain kinds of coherence relations. However,
as with crossed dependencies, removing certain kinds of coherence relations (elaboration
and similarity) can reduce the mean in-degree of nodes and the proportion of nodes
with in-degree greater than one. But also as with crossed dependencies, our data at
present do not distinguish whether this reduction in nodes with multiple parents is
due to a property of the coherence relations removed (elaboration and similarity) or
whether it is just that removing more and more coherence relations simply reduces
the chance for nodes to have multiple parents. We plan to address this question in
future research. In addition to the results on frequency of nodes with multiple parents
283
Computational Linguistics Volume 31, Number 2
Figure 20
Comparison of normalized arc length distributions. For each condition (?overall statistics? and
?arcs ingoing to nodes with multiple parents?), the sum over all coherence relations is 100; each
bar in each condition represents a fraction of the total of 100 in that condition.
and types of coherence relations ingoing to nodes with multiple parents, the statistical
results reported here suggest that ingoing arcs to nodes with multiple parents are
primarily local.
5. Conclusion
The goals of this article have been to present a set of coherence relations that are easy
to code and to illustrate the inadequacy of trees as a data structure for representing
discourse coherence structures. We have developed a coding scheme with high interan-
notator reliability and used that scheme to annotate 135 texts with coherence relations.
An investigation of these annotations has shown that discourse structures of naturally
occurring texts contain various kinds of crossed dependencies as well as nodes with
multiple parents. Neither phenomenon can be represented using trees. This implies that
existing databases of coherence structures that use trees are not descriptively adequate.
Our statistical results suggest that crossed dependencies and nodes with multiple
parents are not restricted phenomena that could be ignored or accommodated with a
few exception rules. Furthermore, even if one could find a way of augmenting tree
structures to account for crossed dependencies and nodes with multiple parents, there
would have to be a mechanism for unifying the tree structure with the augmentation
features. Thus, in terms of derivational complexity, trees would just shift the burden
from having to derive a less constrained data structure to having to derive a unification
of trees and features or coindexation.
Because trees are neither a descriptively adequate data structure for representing
coherence structures nor easier to derive, we argue for less constrained graphs as a data
structure for representing coherence structures. In particular, we argue for a representa-
tion such as chain graphs (cf. final paragraph of section 3). Such less constrained graphs
would have the advantage of being able to adequately represent coherence structures in
one single data structure (cf. Brants et al 2002; Skut et al 1997; Ko?nig and Lezius 2000).
284
Wolf and Gibson Representing Discourse Coherence
Furthermore, they are at least not harder to derive than (augmented) tree structures.
The greater descriptive adequacy might in fact make them easier to derive. However,
this is still an open issue and will have to be addressed in future research.
In section 2.3 we briefly illustrated the possibility of more-fine-grained discourse
segmentation than in the current project. Although such a detailed annotation of co-
herence relations was beyond the scope of the current project, future research should
address this issue. More-fine-grained discourse segmentation could then also facilitate
integration of discourse-level with sentence-level structural descriptions.
Another issue that should be addressed in future research is empirically viable
constraints on inferences for building discourse structures. As pointed out in section 3,
even though we have argued against trees as a data structure for representing discourse
structures, that does not necessarily mean that discourse structures can be completely
arbitrary. Future research should investigate questions such as whether there are struc-
tural constraints on coherence graphs (e.g., as proposed by Danlos [2004]) or whether
there are systematic structural differences between the coherence graphs of texts that
belong to different genres (e.g., as proposed by Bergler [1991]).
References
Bergler, Sabine. 1991. The semantics of
collocational patterns for reporting verbs.
In Proceedings of the Fifth Conference of the
European Chapter of the Association for
Computational Linguistics, Berlin,
Germany.
Birnbaum, Lawrence. 1982. Argument
molecules: A functional representation of
argument structures. In Proceedings of the
Third National Conference on Artificial
Intelligence (AAAI-82), Pittsburgh, PA,
pages 63?65.
Brants, Sabine, Sabine Dipper, Silvia Hansen,
Wolfgang Lezius, and George Smith. 2002.
The tiger treebank. In Proceedings of the
Workshop on Treebanks and Linguistic
Theories, Sozopol, Bulgaria.
Britton, Bruce K. 1994. Understanding
expository text. In Morton Ann
Gernsbacher, editor, Handbook of
Psycholinguistics. Academic Press,
Madison, WI, pages 641?674.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Carlson, Lynn, Daniel Marcu, and Mary E.
Okurowski. 2002. RST discourse treebank.
Corpus number LDC 2002T07, Linguistic
Data Consortium, Philadelphia.
Chomsky, Noam, 1973. Conditions on
transformations. In S. Anderson and
P. Kiparsky, editors, A Festschrift for Morris
Halle. Holt, Rinehart and Winston, New
York, pages 232?286.
Corston-Oliver, Simon. 1998. Computing
representations of the structure of written
discourse. Technical Report MSR-TR-98-15,
Microsoft Research, Redmond, WA.
Danlos, Laurence. 2004. Discourse
dependency structures as dags.
In SigDIAL2004, Cambridge, MA.
Diestel, Reinhard. 2000. Graph Theory.
Springer Verlag, New York.
Frydenberg, Morten. 1989. The chain graph
Markov property. Scandinavian Journal of
Statistics, 17:333?353.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intentions, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Harman, Donna and Mark Liberman. 1993.
Tipster complete. Corpus number
LDC93T3A, Linguistic Data Consortium,
Philadelphia.
Hearst, Marti. 1997. Texttiling: Segmenting
text into multi-paragraph subtopic
passages. Computational Linguistics,
23(1):33?64.
Hirschberg, Julia and Christine H. Nakatani.
1996. A prosodic analysis of discourse
segments in direction-giving monologues.
In Proceedings of the 34th Annual Meeting
of the Association for Computational
Linguistics, pages 286?293, Santa Cruz, CA.
Hobbs, Jerry R. 1985. On the coherence
and structure of discourse. Technical
Report 85-37, Center for the Study of
Language and Information (CSLI),
Stanford, CA.
Hobbs, Jerry R., Martin E. Stickel, Douglas E.
Appelt, and Paul Martin. 1993.
Interpretation as abduction. Artificial
Intelligence, 63:69?142.
Hovy, Eduard and Elisabeth Maier. 1995.
Parsimonious or profligate: How many
and which discourse relations? Technical
report, University of Southern California.
285
Computational Linguistics Volume 31, Number 2
Kehler, Andrew. 2002. Coherence, Reference,
and the Theory of Grammar. Stanford
University Press, Stanford, CA.
Knott, Alistair. 1996. A Data-Driven
Methodology for Motivating a Set of Coherence
Relations. Ph.D. thesis, University of
Edinburgh.
Ko?nig, Esther and Wolfgang Lezius. 2000.
A description language for syntactically
annotated corpora. In Proceedings of the
Computational Linguistics Conference
(COLING), pages 1056?1060, Saarbru?cken,
Germany.
Lascarides, Alex and Nicholas Asher. 1991.
Discourse relations and defeasible
knowledge. In Proceedings of the 29th
Annual Meeting of the Association for
Computational Linguistics, pages 55?63,
Berkeley, CA.
Lascarides, Alex and Nicholas Asher. 1993.
Temporal interpretation, discourse
relations and common sense entailment.
Linguistics and Philosophy, 16(5):
437?493.
Lauritzen, Steffen and Nanny Wermuth.
1989. Graphical models for associations
between variables, some of which are
qualitative and some quantitative. Annals
of Statistics, 17:31?57.
Longacre, Robert E. 1983. The Grammar of
Discourse. Plenum, New York.
Mann, William C. and Sandra A. Thompson.
1988. Rhetorical structure theory: Toward a
functional theory of text organization. Text,
8(3):243?281.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
MIT Press, Cambridge, MA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and
Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In Proceedings of the ARPA
Human Language Technology Workshop,
Plainsboro, NJ. San Francisco, CA. Morgan
Kaufmann.
McKeown, Kathleen R. 1985. Text Generation:
Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Cambridge University Press,
Cambridge.
Miltsakaki, Eleni, Rashmi Prasad, Aravind K.
Joshi, and Bonnie L. Webber. 2004. The
Penn discourse treebank. In Proceedings of
the Language and Resources and Evaluation
Conference, Lisbon.
Moore, Johanna D. and Martha E. Pollack.
1992. A problem for rst: The need for
multi-level discourse analysis.
Computational Linguistics, 18(4):537?544.
Moser, Megan and Johanna D. Moore. 1996.
Toward a synthesis of two accounts of
discourse structure. Computational
Linguistics, 22(3):409?419.
Penstein Rose, Carolyn, Barbara Di Eugenio,
Lori S. Levin, and Carol Van Ess-Dykema.
1995. Discourse processing of dialogues
with multiple threads. In Proceedings of
the 33rd Annual Meeting of the Association
for Computational Linguistics, Cambridge,
MA.
Polanyi, Livia. 1996. The linguistic structure
of discourse. Technical Report 96-118,
Center for the Study of Language and
Information (CSLI), Stanford, CA.
Polanyi, Livia, Chris Culy, Martin van den
Berg, Gian Lorenzo Thione, and David
Ahn. 2004. A rule based approach to
discourse parsing. In SigDIAL 2004,
Cambridge, MA.
Polanyi, Livia and Remko Scha. 1984. A
syntactic approach to discourse semantics.
In Proceedings of the 10th International
Conference on Computational Linguistics,
Stanford, CA.
Reichman, Rachel. 1985. Getting Computers to
Talk Like You and Me. MIT Press,
Cambridge, MA.
Shieber, Stuart M. 1986. An introduction to
unification-based approaches to grammar.
Lecture Notes 4, Center for the Study of
Language and Information (CSLI),
Stanford, CA.
Skut, Wojciech, Brigitte Krenn, Thorsten
Brants, and Hans Uszkoreit. 1997. An
annotation scheme for free word order
languages. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing (ANLP-97), Washington,
DC.
van Dijk, Teun A. and Walter Kintsch. 1983.
Strategies of Discourse Comprehension.
Academic Press, New York.
Walker, Marilyn A. 1998. Centering,
anaphora resolution, and discourse
structure. In E. Prince, A. K. Joshi, and
M. A. Walker, editors, Centering Theory in
Discourse. Oxford University Press,
Oxford, pages 401?435.
Webber, Bonnie L., Alistair Knott, Matthew
Stone, and Aravind K. Joshi. 1999.
Discourse relations: A structural and
presuppositional account using lexicalized
tag. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99), College Park, MD,
pages 41?48.
286
Wolf and Gibson Representing Discourse Coherence
Webber, Bonnie L., Matthew Stone,
Aravind K. Joshi, and Alistair Knott.
2003. Anaphora and discourse
structure. Computational Linguistics,
29(4):545?587.
Wolf, Florian, Edward Gibson, Amy Fisher,
and Meredith Knight. 2003. A procedure
for collecting a database of texts annotated
with coherence relations. Technical report,
Massachusetts Institute of Technology,
Cambridge, MA.
Zukerman, Ingrid and Richard McConachy.
1995. Generating discourse across several
user modules: Maximizing belief while
avoiding boredom and overload. In
Proceedings of the International Joint
Conference on Artificial Intelligence
(IJCAI-95), pages 1251?1257, Montreal.
287

Proceedings of NAACL HLT 2007, Companion Volume, pages 77?80,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ILR-Based MT Comprehension Test with Multi-Level Questions 
 
Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind Jairam, Wade Shen,  
Edward Gibson and Michael Emonts 
 MIT Lincoln Laboratory 
Lexington, MA 02420 
{DAJ,Arvind,SWade}@LL.MIT.EDU 
MHerzog2005@comcast.net 
DLI Foreign Language Center
Monterey, CA 93944 
{Hussny.Ibrahim,Michael.Emonts}
@monterey.army.mil  
MIT Brain and Cognitive 
Sciences Department 
Cambridge MA, 02139 
EGibson@MIT.EDU 
Abstract 
We present results from a new Interagency 
Language Roundtable (ILR) based compre-
hension test. This new test design presents 
questions at multiple ILR difficulty levels 
within each document. We incorporated 
Arabic machine translation (MT) output 
from three independent research sites, arbi-
trarily merging these materials into one MT 
condition.  We contrast the MT condition, 
for both text and audio data types, with high 
quality human reference Gold Standard 
(GS) translations.  Overall, subjects 
achieved 95% comprehension for GS and 
74% for MT, across 4 genres and 3 diffi-
culty levels. Surprisingly, comprehension 
rates do not correlate highly with translation 
error rates, suggesting that we are measur-
ing an additional dimension of MT quality.   
We observed that it takes 15% more time 
overall to read MT than GS.  
1 Introduction 
The official Defense Language Proficiency Test 
(DLPT) is constructed according to rigorous and 
well-established principles that have been devel-
oped to measure the foreign language proficiency 
of human language learners in U.S. Department of 
Defense settings.  In 2004, a variant of that test 
type was constructed, following the general DLPT 
design principles, but modified to measure the 
quality of machine translation.  This test, known as 
the DLPTstar (Jones et al 2005),  was based on 
authentic Arabic materials at ILR  text difficulty 
levels 1, 2, and 3, accompanied by constructed-
response questions at matching levels.  The ILR 
level descriptors, used throughout the U.S. gov-
ernment, can be found at the website cited in the 
list of references. The text documents were pre-
sented in two conditions in English translation: (1) 
professionally translated into English, and (2) ma-
chine translated with state-of-the art MT systems, 
often quite garbled.  Results showed that native 
readers of English could generally pass the Levels 
1 and 2 questions on the test, but not those at Level 
3.  Also, Level 1 comprehension was less than ex-
pected, given the low level of the original material.  
It was not known whether the weak Level 1 per-
formance was due to systematic deficits in MT 
performance at Level 1, or whether the materials 
were simply mismatched to the MT capabilities. 
In this paper, we present a new variant of the 
test, using materials specifically created to test the 
capabilities of the MT systems.  To guarantee that 
the MT systems were up to the task of processing 
the documents, we used the DARPA GALE 2006 
evaluation data sets, against which several research 
sites were testing MT algorithms.  We arbitrarily 
merged the MT output from three sites. The ILR 
difficulty of the documents ranged from Level 2 to 
Level 3, but the test did not contain any true Level 
1 documents.  To compensate for this lack, we 
constructed questions about Level 1 elements (e.g., 
personal and place names) in Level 2 and 3 docu-
ments.  A standard DLPT would have more varia-
tion at Level 1.  
2 Related and Previous Work 
Earlier work in MT evaluation incorporated an in-
formativeness measure, based on comprehension 
test answers, in addition to fluency, a measure of 
output readability without reference to a gold stan-
dard, and adequacy, a measure of accuracy with 
reference to a gold standard translation (White and 
O'Connell, 1994).  Later MT evaluation found flu-
ency and adequacy to correlate well enough with 
automatic measures (BLEU), and since compre-
hension tests are relatively more expensive to cre-
ate, the informativeness test was not used in later 
77
MT evaluations, such as the ones performed by 
NIST from 2001-2006.  In other work, task-based 
evaluation has been used for MT evaluation (Voss 
and Tate, 2006), which measures human perform-
ance on exhaustively extracting ?who?, ?when?, and 
?where? type elements in MT output. The DLPT-
star also uses this type of factual question, particu-
larly for Level 2 documents, but not exhaustively.  
Instead, the test focuses on text elements most 
characteristic of the levels as defined in the ILR 
scale.  At Level 3, for example, questions may 
concern abstract concepts or hypotheses found in 
the documents.  Applying the ILR construct pro-
vides Defense Department decision makers with 
test scores that are readily interpretable. 
3 Test Construction and Administration 
In this paper, we present a new test, based entirely 
on the DARPA GALE 2006 evaluation data, se-
lecting approximately half of the material for our 
test. We selected twenty-four test documents, with 
balanced coverage across four genres: newswire, 
newsgroups, broadcast news and talk radio.  Our 
target was to have at least 2500 words for each 
genre, which we exceeded slightly with approxi-
mately 12,200 words in total for the test.  We be-
gan with a random selection of documents and 
adjusted it for better topic coverage.  We con-
structed an exhaustive set of questions for each 
document, approximately 200 questions in total.  
The questions ranged in ILR difficulty, from "0+, 
1,1+, 2, 2+ and 3, with Levels 0+, 1 and 1+ com-
bined to a pseudo-level we called L1~, providing 
four levels of difficulty to be measured.  We di-
vided the questions into two sets, and each indi-
vidual subject answered questions for one of the 
sets. The test itself was constructed by a DLPT 
testing expert and a senior native-speaking Arabic 
language instructor, using only the original Arabic 
documents and the Gold Standard translations.  
They had no access to any machine translation 
output during the test construction or scoring. 
In August 2006, we administered the test at MIT 
to 49 test subjects who responded to announce-
ments for paid experimental subjects.  The subjects 
read the documents in a Latin square design, mean-
ing that each subject saw each document, but only 
in one of the two conditions, randomly assigned.  
Subjects were allowed 5 hours to complete the test.  
Since the questions were divided into two sets for 
each document, the actual set of 49 subjects 
yielded approximately 25 ?virtual subjects? read-
ing the full list of 228 questions.  The mean time 
spent on testing, not counting breaks or subject 
orientation, was 2.5 hours; fastest was 1.1 hours, 
slowest was 3.4 hours. 
The subject responses were hand-graded by the 
two testing experts, following the pre-established 
answers in the test protocol.  There was no pre-
assessment of whether information was preserved 
or garbled in the MT when designing questions or 
responses in the test protocol.  The testing experts 
were provided the reference translations and the 
original Arabic documents, but not the MT during 
scoring.  Moreover, test conditions were masked in 
order to provide a blind assessment.  The two test-
ing experts provided both preliminary and final 
scores; multiple passes provided an opportunity to 
clarify the correct answers and to normalize scor-
ing.  The scoring agreement rate was 96% for the 
final scores. 
4 Overall Results 
The overall result for comprehension accuracy was 
95% for subjects reading the Gold Standard trans-
lation and 74% for reading Machine Translation, 
across each of the genres and difficulty levels. The 
comprehension accuracy for each genre is shown 
in Figure 1. The two text genres score better than 
the audio genres, which is to be expected because 
the audio MT condition has more opportunities for 
error.  Within each modality, the more standard, 
more structured genre fares better: newswire re-
sults are better than newsgroup results, and the 
more structured genre of broadcast news scores 
better than the less constrained, less structured 
conversations present in the talk radio shows. 
 
 
Figure 1. Comprehension Accuracy per Genre  
97% 93% 94% 94%
80% 77% 
72% 66%
0%
20%
40%
60%
80%
100%
Newswire Broadcast News Talk Radio 
GS
MT
Newsgroups 
Overall Comprehension Accuracy 
78
The break-down by ILR level of difficulty for each 
question is shown in Figure 2.  The general trend is 
consistent with what has been observed previously 
(Jones et al 2005).  The best results are at Level 2; 
Level 1 does well but not as well as expected.  
Thus the test has provided a key finding, which is 
that MT systems perform more poorly on Level 1, 
even when the data is matched to their capabilities. 
Level 3 is very challenging for the MT condition, 
and also more difficult in the GS condition.  Using 
a standard 70 percent passing threshold, responses 
to questions on all MT documents, except for 
Level 3, received a passing grade. 
 
Figure 2. Comprehension Accuracy per Level. 
To provide a snapshot of the ILR levels: L1 in-
dicates sentence-level comprehensibility, and may 
include factual local announcements, etc.; L2 indi-
cates paragraph-level comprehensibility; factual/ 
concrete, covering a wide spectrum of topics (poli-
tics, economy, society, culture, security, science); 
L3 involves extended discourse comprehensibility; 
the ability to understand hypotheses, supported 
opinion, implications, and abstract linguistic for-
mulations, etc. 
It was not possible to balance Level 3 documents 
across genres within the GALE evaluation data; 
except for those taken from Talk Radio, most 
documents did not reach that level of complexity.  
Hence, genre and difficulty level were not com-
pletely independent in this test. 
5 Comprehension and Translation Error 
We expect to see a relationship between compre-
hension rates and translation error.  In an idealized 
case, we may expect a precise inverse correlation.  
We then compared comprehension rates with Hu-
man Translation Error Rate (HTER), an error 
measure for machine translation that counts the 
number of human edits required to change system 
MT output so that it contains all and only the in-
formation present in a Gold Standard reference 
(NIST, 2006).  The linear regression line in Figure 
3 shows the kind of inverse correlation we might 
expect.  Subjects lose about 12% in comprehension 
for every 10% of translation error. The R2 value is 
33%.  The low correlation suggests that the com-
prehension results are measuring a somewhat inde-
pendent aspect of MT quality, which we feel is 
important.  HTER does not directly address the 
facts that not all MT errors are equally important 
and that the texts contain inherent redundancy that 
the readers use to answer the questions.  For ex-
ploratory purposes, we divide the graph of Figure 3 
into four quadrants.  Quadrant I and IV contain 
expected behavior: 122 data points of good transla-
tions and good comprehension results versus 43 
points of bad translations and poor comprehension.  
Q-II has 24 robust points: the translations have 
high error, but somehow managed to contain 
enough well-translated words that people can an-
swer the questions.  Q-III has 28 fragile points: the 
few translation errors impaired comprehension. 
 
Figure 3. Comprehension vs. Translation Error. 
We point out that there is a 1-to-1 mapping be-
tween comprehension questions and individual 
sub-passages of the documents in the data.  Each 
point in Figure 3 plots the HTER of a single seg-
ment versus the average comprehension score on 
the corresponding question. The good and bad 
items are essentially a sanity-check on the experi-
mental design.  We expect to see good comprehen-
sion when translations are good, and we expect to 
see poor comprehension when translations are bad.  
Next we will examine the two other types: fragile 
and robust translations. 
Overall Comprehension Accuracy 
97% 96% 91% 88%
77% 82% 76% 
51%
0% 
20% 
40% 
60% 
80% 
100% 
L1~ L2 L2+ L3
GS
MT
  Q-I (Good)                        Q-II (Robust) 
122 points (57%)               24 points (10%)                          
(All Levels and Genres)
0%
20%
40%
60%
80%
100%
0% 20% 40% 60% 80% 100%
x = Translation Error (HTER) 
y = Comprehension (DLPT*)
Q-III (Fragile)                      Q-IV (Bad) 
28 points (13%)                 43 points (20%)                          
79
A fragile translation is one that has a good 
HTER score but a bad comprehension score.  A 
sample fragile translation is one from a broadcast 
news which asks for a particular name:  the HTER 
was a respectable 24%, but the MT comprehension 
accuracy was a flat 0%, since the name was miss-
ing.  Everyone reading GS answered correctly. 
A robust translation is one that has a bad HTER 
score but still manages to get a good comprehen-
sion score.  A sample robust translation is one 
drawn from a posting providing instructions for 
foot massage.  The text was quite garbled, with an 
HTER score of 48%, but the MT comprehension 
accuracy was a perfect 100%. Everyone reading 
the GS condition also answered the question cor-
rectly, which was that one should start a foot mas-
sage with oil. We note in passing that the highest 
error rate for a question with 100% comprehension 
is about 50%, shown with the up-arrow in Figure 
3.  We should be surprised to see any items with 
100% comprehension for HTER rates above 50%, 
considering Shannon?s estimate that written Eng-
lish is about 50% redundant. We expect that MT 
readers are making use of their general world 
knowledge to interpret the garbled MT output.  A 
challenge is to identify robust translations, which 
are useful despite their high translation error rate. 
6 Detailed Discussion 
In this section we will discuss several aspects of 
the test in more detail: the scoring methodology, 
including a discussion of partial credit and inter-
rater agreement; timing information; questions 
about personal names. 
Each correct answer was assigned a score of 1, 
and each incorrect answer was assigned a score of 
0.  Partial credit was assigned on an ad-hoc basis, 
but normalized for scoring by assigning all non-
integer scores to 0.5.  This method yielded scores 
that were generally at the midpoint between binary 
scoring, in which non-integer scored were uni-
formly mapped either harshly to 0 or leniently to 1, 
the average difference between harsh and lenient 
scoring being approximately 11%.  Inter-rater 
agreement was 96%. 
The testing infrastructure we used recorded the 
amount of time spent on each document.  The gen-
eral trend is that people spend longer on MT than 
on GS.  The mean percentage of time spent on MT 
compared with GS is 115% per item, meaning that 
it takes 15% more time to read MT than GS. The 
standard error was 4%.  The median is 111%; 
minimum is 89% and maximum is 159%.  In future 
analysis and experimentation we will conduct more 
fine-grained temporal estimates.    
As we have seen in previous experiments, the 
performance for personal names is lower than for 
non-names.  We observed that the name questions 
have 71% comprehension accuracy, compared with 
the 83% for questions about things other than per-
sonal names.  
7 Conclusions and Future Work 
We have long felt that Level 2 is the natural and 
successful level for machine translation.  The abil-
ity to present concrete factual information that can 
be retrieved by the reader, without requirements 
for understanding the style, tone, or organizational 
pattern used by the writer seemed to be present in 
the previous work. It is worth pointing out that 
though we have many Level 1 questions, we are 
still not really testing Level 1 because the test does 
not contain true Level 1 documents. In future tests 
we wish to include Level 1 documents and ques-
tions.  
Continuing along these lines, we are currently 
creating two new tests. We are constructing a new 
Arabic DLPT-star test, tailoring the document se-
lection more specifically for comprehension testing 
and ensuring texts and tasks are at the intended 
ILR levels. We are also constructing a Mandarin 
Chinese test with similar design specifications.  
We intend for both of these tests to be available for 
a public machine translation evaluation to be con-
ducted in 2007. 
References 
Doddington, G. 2002. Automatic Evaluation of Machine 
Translation Quality Using N-gram Co-Occurrence 
Statistics. Proceedings of HLT 2002. 
NIST 2006. GALE Go/No-Go Eval Plan; www.nist.gov/ 
speech/tests/gale/2006/doc/GALE06_evalplan.v2.pdf 
Jones, D. A., W. Shen, et al 2005a. Measuring Transla-
tion Quality by Testing English Speakers with a New 
DLPT for Arabic. Int?l Conf. on Intel. Analysis. 
Interagency Language Roundtable Website. 2005. ILR 
Skill Level Descriptions: http://www.govtilr.org 
Voss, Clare and Calandra Tate. 2006. Task-based 
Evaluation of MT Engines. European Association for 
Machine Translation conference. 
White, JS and TA O'Connell. 1994. Evaluation in the 
ARPA machine translation program: 1993 method-
ology. Proceedings of the HLT workshop. 
80
Paragraph-, word-, and coherence-based approaches to sentence ranking:       
A comparison of algorithm and human performance 
Florian WOLF  
Massachusetts Institute of Technology 
MIT NE20-448, 3 Cambridge Center 
Cambridge, MA 02139, USA 
fwolf@mit.edu 
Edward GIBSON 
Massachusetts Institute of Technology 
MIT NE20-459, 3 Cambridge Center 
Cambridge, MA 02139, USA 
egibson@mit.edu 
 
Abstract 
Sentence ranking is a crucial part of 
generating text summaries.  We compared 
human sentence rankings obtained in a 
psycholinguistic experiment to three different 
approaches to sentence ranking: A simple 
paragraph-based approach intended as a 
baseline, two word-based approaches, and two 
coherence-based approaches.  In the 
paragraph-based approach, sentences in the 
beginning of paragraphs received higher 
importance ratings than other sentences.  The 
word-based approaches determined sentence 
rankings based on relative word frequencies 
(Luhn (1958); Salton & Buckley (1988)).  
Coherence-based approaches determined 
sentence rankings based on some property of 
the coherence structure of a text (Marcu 
(2000); Page et al (1998)).  Our results 
suggest poor performance for the simple 
paragraph-based approach, whereas word-
based approaches perform remarkably well.  
The best performance was achieved by a 
coherence-based approach where coherence 
structures are represented in a non-tree 
structure.  Most approaches also outperformed 
the commercially available MSWord 
summarizer. 
1 Introduction 
Automatic generation of text summaries is a 
natural language engineering application that has 
received considerable interest, particularly due to 
the ever-increasing volume of text information 
available through the internet.  The task of a 
human generating a summary generally involves 
three subtasks (Brandow et al (1995); Mitra et al 
(1997)): (1) understanding a text; (2) ranking text 
pieces (sentences, paragraphs, phrases, etc.) for 
importance; (3) generating a new text (the 
summary).  Like most approaches to 
summarization, we are concerned with the second 
subtask (e.g. Carlson et al (2001); Goldstein et al 
(1999); Gong & Liu (2001); Jing et al (1998); 
Luhn (1958); Mitra et al (1997); Sparck-Jones & 
Sakai (2001); Zechner (1996)).  Furthermore, we 
are concerned with obtaining generic rather than 
query-relevant importance rankings (cf. Goldstein 
et al (1999), Radev et al (2002) for that 
distinction). 
We evaluated different approaches to sentence 
ranking against human sentence rankings.  To 
obtain human sentence rankings, we asked people 
to read 15 texts from the Wall Street Journal on a 
wide variety of topics (e.g. economics, foreign and 
domestic affairs, political commentaries).  For each 
of the sentences in the text, they provided a 
ranking of how important that sentence is with 
respect to the content of the text, on an integer 
scale from 1 (not important) to 7 (very important). 
The approaches we evaluated are a simple 
paragraph-based approach that serves as a baseline, 
two word-based algorithms, and two coherence-
based approaches1.  We furthermore evaluated the 
MSWord summarizer. 
2 Approaches to sentence ranking 
2.1 Paragraph-based approach 
Sentences at the beginning of a paragraph are 
usually more important than sentences that are 
further down in a paragraph, due in part to the way 
people are instructed to write.  Therefore, probably 
the simplest approach conceivable to sentence 
ranking is to choose the first sentences of each 
                                                     
1 We did not use any machine learning techniques to 
boost performance of the algorithms we tested.  
Therefore performance of the algorithms tested here 
will almost certainly be below the level of performance 
that could be reached if we had augmented the 
algorithms with such techniques (e.g. Carlson et al 
(2001)).  However, we think that a comparison between 
?bare-bones? algorithms is viable because it allows to 
see how performance differs due to different basic 
approaches to sentence ranking, and not due to 
potentially different effects of different machine 
learning algorithms on different basic approaches to 
sentence ranking.  In future research we plan to address 
the impact of machine learning on the algorithms tested 
here. 
paragraph as important, and the other sentences as 
not important.  We included this approach merely 
as a simple baseline. 
2.2 Word-based approaches 
Word-based approaches to summarization are 
based on the idea that discourse segments are 
important if they contain ?important? words.  
Different approaches have different definitions of 
what an important word is.  For example, Luhn 
(1958), in a classic approach to summarization, 
argues that sentences are more important if they 
contain many significant words.  Significant words 
are words that are not in some predefined stoplist 
of words with high overall corpus frequency2.  
Once significant words are marked in a text, 
clusters of significant words are formed.  A cluster 
has to start and end with a significant word, and 
fewer than n insignificant words must separate any 
two significant words (we chose n = 3, cf. Luhn 
(1958)).  Then, the weight of each cluster is 
calculated by dividing the square of the number of 
significant words in the cluster by the total number 
of words in the cluster.  Sentences can contain 
multiple clusters.  In order to compute the weight 
of a sentence, the weights of all clusters in that 
sentence are added.  The higher the weight of a 
sentence, the higher is its ranking. 
A more recent and frequently used word-based 
method used for text piece ranking is tf.idf (e.g. 
Manning & Schuetze (2000); Salton & Buckley 
(1988); Sparck-Jones & Sakai (2001); Zechner 
(1996)).  The tf.idf measure relates the frequency 
of words in a text piece, in the text, and in a 
collection of texts respectively.  The intuition 
behind tf.idf is to give more weight to sentences 
that contain terms with high frequency in a 
document but low frequency in a reference corpus.  
Figure 1 shows a formula for calculating tf.idf, 
where dsij is the tf.idf weight of sentence i in 
document j, nsi is the number of words in sentence 
i, k is the kth word in sentence i, tfjk is the 
frequency of word k in document j, nd is the 
number of documents in the reference corpus, and 
dfk is the number of documents in the reference 
corpus in which word k appears. 
 
???
?
???
?
?=?
= df
ntfds
k
d
k
jkij
nsi
log
1
 
Figure 1.  Formula for calculating tf.idf (Salton & 
Buckley (1988)). 
 
                                                     
2 Instead of stoplists, tf.idf values have also been used 
to determine significant words (e.g. Buyukkokten et al 
(2001)). 
We compared both Luhn (1958)?s measure and 
tf.idf scores to human rankings of sentence 
importance.  We will show that both methods 
performed remarkably well, although one 
coherence-based method performed better. 
2.3 Coherence-based approaches 
The sentence ranking methods introduced in the 
two previous sections are solely based on layout or 
on properties of word distributions in sentences, 
texts, and document collections.  Other approaches 
to sentence ranking are based on the informational 
structure of texts.  With informational structure, we 
mean the set of informational relations that hold 
between sentences in a text.  This set can be 
represented in a graph, where the nodes represent 
sentences, and labeled directed arcs represent 
informational relations that hold between the 
sentences (cf. Hobbs (1985)).  Often, informational 
structures of texts have been represented as trees 
(e.g. Carlson et al (2001), Corston-Oliver (1998), 
Mann & Thompson (1988), Ono et al (1994)).  We 
will present one coherence-based approach that 
assumes trees as a data structure for representing 
discourse structure, and one approach that assumes 
less constrained graphs.  As we will show, the 
approach based on less constrained graphs 
performs better than the tree-based approach when 
compared to human sentence rankings. 
3 Coherence-based summarization revisited 
This section will discuss in more detail the data 
structures we used to represent discourse structure, 
as well as the algorithms used to calculate sentence 
importance, based on discourse structures. 
3.1 Representing coherence structures 
3.1.1 Discourse segments 
Discourse segments can be defined as non-
overlapping spans of prosodic units (Hirschberg & 
Nakatani (1996)), intentional units (Grosz & 
Sidner (1986)), phrasal units (Lascarides & Asher 
(1993)), or sentences (Hobbs (1985)).  We adopted 
a sentence unit-based definition of discourse 
segments for the coherence-based approach that 
assumes non-tree graphs.  For the coherence-based 
approach that assumes trees, we used Marcu 
(2000)?s more fine-grained definition of discourse 
segments because we used the discourse trees from 
Carlson et al (2002)?s database of coherence-
annotated texts. 
3.1.2 Kinds of coherence relations 
We assume a set of coherence relations that is 
similar to that of Hobbs (1985).  Below are 
examples of each coherence relation. 
(1) Cause-Effect 
[There was bad weather at the airport]a [and so our 
flight got delayed.]b 
(2) Violated Expectation 
[The weather was nice]a [but our flight got 
delayed.]b 
(3) Condition 
[If the new software works,]a [everyone will be 
happy.]b 
(4) Similarity 
[There is a train on Platform A.]a [There is another 
train on Platform B.]b 
(5) Contrast 
[John supported Bush]a [but Susan opposed him.]b 
(6) Elaboration 
[A probe to Mars was launched this week.]a [The 
European-built ?Mars Express? is scheduled to 
reach Mars by late December.]b 
(7) Attribution 
[John said that]a [the weather would be nice 
tomorrow.]b 
(8) Temporal Sequence 
[Before he went to bed,]a [John took a shower.]b 
 
Cause-effect, violated expectation, condition, 
elaboration, temporal sequence, and attribution 
are asymmetrical or directed relations, whereas 
similarity, contrast, and temporal sequence are 
symmetrical or undirected relations (Mann & 
Thompson, 1988; Marcu, 2000).  In the non-tree-
based approach, the directions of asymmetrical or 
directed relations are as follows: cause ? effect 
for cause-effect; cause ? absent effect for violated 
expectation; condition ? consequence for 
condition; elaborating ? elaborated for 
elaboration, and source ? attributed for 
attribution.  In the tree-based approach, the 
asymmetrical or directed relations are between a 
more important discourse segment, or a Nucleus, 
and a less important discourse segment, or a 
Satellite (Marcu (2000)).  The Nucleus is the 
equivalent of the arc destination, and the Satellite 
is the equivalent of the arc origin in the non-tree-
based approach.  The symmetrical or undirected 
relations are between two discourse elements of 
equal importance, or two Nuclei.  Below we will 
explain how the difference between Satellites and 
Nuclei is considered in tree-based sentence 
rankings. 
3.1.3 Data structures for representing discourse 
coherence 
As mentioned above, we used two alternative 
representations for discourse structure, tree- and 
non-tree based.  In order to illustrate both data 
structures, consider (9) as an example: 
(9) Example text 
0. Susan wanted to buy some tomatoes. 
1. She also tried to find some basil. 
2. The basil would probably be quite expensive 
at this time of the year. 
Figure 2 shows one possible tree representation 
of the coherence structure of (9)3.  Sim represents a 
similarity relation, and elab an elaboration 
relation.  Furthermore, nodes with a ?Nuc? 
subscript are Nuclei, and nodes with a ?Sat? 
subscript are Satellites. 
 
 
Figure 2.  Coherence tree for (9). 
 
Figure 3 shows a non-tree representation of the 
coherence structure of (9).  Here, the heads of the 
arrows represent the directionality of a relation. 
 
 
Figure 3.  Non-tree coherence graph for (9). 
 
3.2 Coherence-based sentence ranking 
This section explains the algorithms for the tree- 
and the non-tree-based sentence ranking approach. 
3.2.1 Tree-based approach 
We used Marcu (2000)?s algorithm to determine 
sentence rankings based on tree discourse 
structures.  In this algorithm, sentence salience is 
determined based on the tree level of a discourse 
segment in the coherence tree.  Figure 4 shows 
Marcu (2000)?s algorithm, where r(s,D,d) is the 
rank of a sentence s in a discourse tree D with 
depth d.  Every node in a discourse tree D has a 
promotion set promotion(D), which is the union of 
all Nucleus children of that node.  Associated with 
every node in a discourse tree D is also a set of 
parenthetical nodes parentheticals(D) (for 
example, in ?Mars ? half the size of Earth ? is 
red?, ?half the size of earth? would be a 
parenthetical node in a discourse tree).  Both 
promotion(D) and parentheticals(D) can be empty 
sets.  Furthermore, each node has a left subtree, 
                                                     
3 Another possible tree structure might be 
( elab ( par ( 0 1 ) 2 ) ). 
0Nuc 1Nuc 2Sat 
elabNuc
sim
elab
sim
0 1 2
lc(D), and a right subtree, rc(D).  Both lc(D) and 
rc(D) can also be empty. 
 
??
?
?
??
?
?
?
?
?
??
?
=
otherwisedDrcsr
dDlcsr
Dcalsparenthetisifd
Dpromotionsifd
NILisDif
dDsr
))1),(,(
),1),(,(max(
),(1
),(
,0
),,(  
Figure 4.  Formula for calculating coherence-tree-
based sentence rank (Marcu (2000)). 
 
The discourse segments in Carlson et al 
(2002)?s database are often sub-sentential.  
Therefore, we had to calculate sentence rankings 
from the rankings of the discourse segments that 
form the sentence under consideration.  We did 
this by calculating the average ranking, the 
minimal ranking, and the maximal ranking of all 
discourse segments in a sentence.  Our results 
showed that choosing the minimal ranking 
performed best, followed by the average ranking, 
followed by the maximal ranking (cf. Section 4.4). 
3.2.2 Non-tree-based approach 
We used two different methods to determine 
sentence rankings for the non-tree coherence 
graphs4.  Both methods implement the intuition 
that sentences are more important if other 
sentences relate to them (Sparck-Jones (1993)). 
The first method consists of simply determining 
the in-degree of each node in the graph.  A node 
represents a sentence, and the in-degree of a node 
represents the number of sentences that relate to 
that sentence. 
The second method uses Page et al (1998)?s 
PageRank algorithm, which is used, for example, 
in the Google? search engine.  Unlike just 
determining the in-degree of a node, PageRank 
takes into account the importance of sentences that 
relate to a sentence.  PageRank thus is a recursive 
algorithm that implements the idea that the more 
important sentences relate to a sentence, the more 
important that sentence becomes.  Figure 5 shows 
how PageRank is calculated.  PRn is the PageRank 
of the current sentence, PRn-1 is the PageRank of 
the sentence that relates to sentence n, on-1 is the 
out-degree of sentence n-1, and ? is a damping 
parameter that is set to a value between 0 and 1.  
We report results for ? set to 0.85 because this is a 
value often used in applications of PageRank (e.g. 
Ding et al (2002); Page et al (1998)).  We also 
                                                     
4 Neither of these methods could be implemented for 
coherence trees since Marcu (2000)?s tree-based 
algorithm assumes binary branching trees.  Thus, the in-
degree for all non-terminal nodes is always 2. 
calculated PageRanks for ? set to values between 
0.05 and 0.95, in increments of 0.05; changing ? 
did not affect performance. 
  
o
PRPR
n
n
n
1
11
?
?+?= ??  
Figure 5.  Formula for calculating PageRank (Page 
et al (1998)). 
 
4 Experiments 
In order to test algorithm performance, we 
compared algorithm sentence rankings to human 
sentence rankings.  This section describes the 
experiments we conducted.  In Experiment 1, the 
texts were presented with paragraph breaks; in 
Experiment 2, the texts were presented without 
paragraph breaks.  This was done to control for the 
effect of paragraph information on human sentence 
rankings. 
4.1 Materials for the coherence-based 
approaches 
In order to test the tree-based approach, we took 
coherence trees for 15 texts from a database of 385 
texts from the Wall Street Journal that were 
annotated for coherence (Carlson et al (2002)).  
The database was independently annotated by six 
annotators.  Inter-annotator agreement was 
determined for six pairs of two annotators each, 
resulting in kappa values (Carletta (1996)) ranging 
from 0.62 to 0.82 for the whole database (Carlson 
et al (2003)).  No kappa values for just the 15 texts 
we used were available. 
For the non-tree based approach, we used 
coherence graphs from a database of 135 texts 
from the Wall Street Journal and the AP 
Newswire, annotated for coherence.  Each text was 
independently annotated by two annotators.  For 
the 15 texts we used, kappa was 0.78, for the 
whole database, kappa was 0.84. 
4.2 Experiment 1: With paragraph 
information 
15 participants from the MIT community were 
paid for their participation.  All were native 
speakers of English and were na?ve as to the 
purpose of the study (i.e. none of the subjects was 
familiar with theories of coherence in natural 
language, for example). 
Participants were asked to read 15 texts from the 
Wall Street Journal, and, for each sentence in each 
text, to provide a ranking of how important that 
sentence is with respect to the content of the text, 
on an integer scale from 1 to 7 (1 = not important; 
7 = very important).   The   texts  were  selected  so  
 1
2
3
4
5
6
7
8
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
sentence number
im
po
rta
nc
e r
an
kin
g
NoParagraph
WithParagraph
 
Figure 6.  Human ranking results for one text (wsj_1306). 
 
that there was a coherence tree annotation 
available in Carlson et al (2002)?s database.  Text 
lengths for the 15 texts we selected ranged from 
130 to 901 words (5 to 47 sentences); average text 
length was 442 words (20 sentences), median was 
368 words (16 sentences).  Additionally, texts were 
selected so that they were about as diverse topics 
as possible. 
The experiment was conducted in front of 
personal computers.  Texts were presented in a 
web browser as one webpage per text; for some 
texts, participants had to scroll to see the whole 
text.  Each sentence was presented on a new line.  
Paragraph breaks were indicated by empty lines; 
this was pointed out to the participants during the 
instructions for the experiment. 
4.3 Experiment 2: Without paragraph 
information 
The method was the same as in Experiment 1, 
except that texts in Experiment 2 did not include 
paragraph information.  Each sentence was 
presented on a new line.  None of the 15 
participants who participated in Experiment 2 had 
participated in Experiment 1. 
4.4 Results of the experiments 
Human sentence rankings did not differ 
significantly between Experiment 1 and 
Experiment 2 for any of the 15 texts (all Fs < 1).  
This suggests that paragraph information does not 
have a big effect on human sentence rankings, at 
least not for the 15 texts that we examined. Figure 
6 shows the results from both experiments for one 
text. 
We compared human sentence rankings to 
different algorithmic approaches.  The paragraph-
based rankings do not provide scaled importance 
rankings but only ?important? vs. ?not important?.  
Therefore, in order to compare human rankings to 
the paragraph-based baseline approach, we 
calculated point biserial correlations (cf. Bortz 
(1999)).  We obtained significant correlations 
between paragraph-based rankings and human 
rankings only for one of the 15 texts. 
All other algorithms provided scaled importance 
rankings.  Many evaluations of scalable sentence 
ranking algorithms are based on precision/recall/F-
scores (e.g. Carlson et al (2001); Ono et al 
(1994)).  However, Jing et al (1998) argue that 
such measures are inadequate because they only 
distinguish between hits and misses or false 
alarms, but do not account for a degree of 
agreement.  For example, imagine a situation 
where the human ranking for a given sentence is 
?7? (?very important?) on an integer scale ranging 
from 1 to 7, and Algorithm A gives the same 
sentence a ranking of ?7? on the same scale, 
Algorithm B gives a ranking of ?6?, and Algorithm 
C gives a ranking of ?2?.  Intuitively, Algorithm B, 
although it does not reach perfect performance, 
still performs better than Algorithm C.  
Precision/recall/F-scores do not account for that 
difference and would rate Algorithm A as ?hit? but 
Algorithm B as well as Algorithm C as ?miss?.  In 
order to collect performance measures that are 
more adequate to the evaluation of scaled 
importance rankings, we computed Spearman?s 
rank correlation coefficients.  The rank correlation 
coefficients were corrected for tied ranks because 
in our rankings it was possible for more than one 
sentence to have the same importance rank, i.e. to 
have tied ranks (Horn (1942); Bortz (1999)). 
In addition to evaluating word-based and 
coherence-based algorithms, we evaluated one 
commercially available summarizer, the MSWord 
summarizer, against human sentence rankings.  
Our reason for including an evaluation of the 
MSWord summarizer was to have a more useful 
baseline for scalable sentence rankings than the 
paragraph-based approach provides. 
 
00.1
0.2
0.3
0.4
0.5
0.6
MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank
me
an
 ra
nk
 co
rre
lat
ion
 co
eff
ici
en
t
NoParagraph
WithParagraph
 
Figure 7.  Average rank correlations of algorithm and human sentence rankings. 
 
Figure 7 shows average rank correlations (?avg) 
of each algorithm and human sentence ranking for 
the 15 texts.  MarcuAvg refers to the version of 
Marcu (2000)?s algorithm where we calculated 
sentence rankings as the average of the rankings of 
all discourse segments that constitute that sentence; 
for MarcuMin, sentence rankings were the 
minimum of the rankings of all discourse segments 
in that sentence; for MarcuMax we selected the 
maximum of the rankings of all discourse 
segments in that sentence. 
Figure 7 shows that the MSWord summarizer 
performed numerically worse than most other 
algorithms, except MarcuMin.  Figure 7 also 
shows that PageRank performed numerically better 
than all other algorithms.  Performance was 
significantly better than most other algorithms 
(MSWord, NoParagraph: F(1,28) = 21.405, p = 
0.0001; MSWord, WithParagraph: F(1,28) = 
26.071, p = 0.0001; Luhn, WithParagraph: F(1,28) 
= 5.495, p = 0.026; MarcuAvg, NoParagraph: 
F(1,28) = 9.186, p = 0.005; MarcuAvg, 
WithParagraph: F(1,28) = 9.097, p = 0.005; 
MarcuMin, NoParagraph: F(1,28) = 4.753, p = 
0.038; MarcuMax, NoParagraph F(1,28) = 24.633, 
p = 0.0001; MarcuMax, WithParagraph: F(1,28) = 
31.430, p =0.0001).  Exceptions are Luhn, 
NoParagraph (F(1,28) = 1.859, p = 0.184); tf.idf, 
NoParagraph (F(1,28) = 2.307, p = 0.14); 
MarcuMin, WithParagraph (F(1,28) = 2.555, p = 
0.121).  The difference between PageRank and 
tf.idf, WithParagraph was marginally significant 
(F(1,28) = 3.113, p = 0.089). 
As mentioned above, human sentence rankings 
did not differ significantly between Experiment 1 
and Experiment 2 for any of the 15 texts (all Fs < 
1).  Therefore, in order to lend more power to our 
statistical tests, we collapsed the data for each text 
for the WithParagraph and the NoParagraph 
condition, and treated them as one experiment.  
Figure 8 shows that when the data from 
Experiments 1 and 2 are collapsed, PageRank 
performed significantly better than all other 
algorithms except in-degree (two-tailed t-test 
results: MSWord: F(1, 58) = 48.717, p = 0.0001; 
Luhn: F(1,58) = 6.368, p = 0.014; tf.idf: F(1,58) = 
5.522, p = 0.022; MarcuAvg: F(1,58) = 18.922, p = 
0.0001; MarcuMin: F(1,58) = 7.362, p = 0.009; 
MarcuMax: F(1,58) = 56.989, p = 0.0001; in-
degree: F(1,58) < 1). 
 
0
0.1
0.2
0.3
0.4
0.5
MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank
m
ea
n 
ra
nk
 co
rre
lat
io
n 
co
eff
ici
en
t
 
Figure 8.  Average rank correlations of algorithm 
and human sentence rankings with collapsed data. 
 
5 Conclusion 
The goal of this paper was to evaluate the results 
of three different kinds of sentence ranking 
algorithms and one commercially available 
summarizer.  In order to evaluate the algorithms, 
we compared their sentence rankings to human 
sentence rankings of fifteen texts of varying length 
from the Wall Street Journal. 
Our results indicated that a simple paragraph-
based algorithm that was intended as a baseline 
performed very poorly, and that word-based and 
some coherence-based algorithms showed the best 
performance.  The only commercially available 
summarizer that we tested, the MSWord 
summarizer, showed worse performance than most 
other algorithms.  Furthermore, we found that a 
coherence-based algorithm that uses PageRank and 
takes non-tree coherence graphs as input 
performed better than most versions of a 
coherence-based algorithm that operates on 
coherence trees.  When data from Experiments 1 
and 2 were collapsed, the PageRank algorithm 
performed significantly better than all other 
algorithms, except the coherence-based algorithm 
that uses in-degrees of nodes in non-tree coherence 
graphs. 
References 
J?rgen Bortz. 1999. Statistik f?r Sozialwissen-
schaftler. Berlin: Springer Verlag. 
Ronald Brandow, Karl Mitze, & Lisa F Rau. 1995. 
Automatic condensation of electronic 
publications by sentence selection. 
Information Processing and Management, 
31(5), 675-685. 
Orkut Buyukkokten, Hector Garcia-Molina, & 
Andreas Paepcke. 2001. Seeing the whole 
in parts: Text summarization for web 
browsing on handheld devices. Paper 
presented at the 10th International WWW 
Conference, Hong Kong, China. 
Jean Carletta. 1996. Assessing agreement on 
classification tasks: The kappa statistic. 
Computational Linguistics, 22(2), 249-
254. 
Lynn Carlson, John M Conroy, Daniel Marcu, 
Dianne P O'Leary, Mary E Okurowski, 
Anthony Taylor, et al 2001. An empirical 
study on the relation between abstracts, 
extracts, and the discourse structure of 
texts. Paper presented at the DUC-2001, 
New Orleans, LA, USA. 
Lynn Carlson, Daniel Marcu, & Mary E 
Okurowski. 2002. RST Discourse 
Treebank. Philadelphia, PA: Linguistic 
Data Consortium. 
Lynn Carlson, Daniel Marcu, & Mary E 
Okurowski. 2003. Building a discourse-
tagged corpus in the framework of 
rhetorical structure theory. In J. van 
Kuppevelt & R. Smith (Eds.), Current 
directions in discourse and dialogue. New 
York: Kluwer Academic Publishers. 
Simon Corston-Oliver. 1998. Computing 
representations of the structure of written 
discourse. Redmont, WA. 
Chris Ding, Xiaofeng He, Perry Husbands, 
Hongyuan Zha, & Horst Simon. 2002. 
PageRank, HITS, and a unified framework 
for link analysis. (No. 49372). Berkeley, 
CA, USA. 
Jade Goldstein, Mark Kantrowitz, Vibhu O Mittal, 
& Jamie O Carbonell. 1999. Summarizing 
text documents: Sentence selection and 
evaluation metrics. Paper presented at the 
SIGIR-99, Melbourne, Australia. 
Yihong Gong, & Xin Liu. 2001. Generic text 
summarization using relevance measure 
and latent semantic analysis. Paper 
presented at the Annual ACM Conference 
on Research and Development in 
Information Retrieval, New Orleans, LA, 
USA. 
Barbara J Grosz, & Candace L Sidner. 1986. 
Attention, intentions, and the structure of 
discourse. Computational Linguistics, 
12(3), 175-204. 
Julia Hirschberg, & Christine H Nakatani. 1996. A 
prosodic analysis of discourse segments in 
direction-giving monologues. Paper 
presented at the 34th Annual Meeting of 
the Association for Computational 
Linguistics, Santa Cruz, CA. 
Jerry R Hobbs. 1985. On the coherence and 
structure of discourse. Stanford, CA. 
D Horn. 1942. A correction for the effect of tied 
ranks on the value of the rank difference 
correlation coefficient. Journal of 
Educational Psychology, 33, 686-690. 
Hongyan Jing, Kathleen R McKeown, Regina 
Barzilay, & Michael Elhadad. 1998. 
Summarization evaluation methods: 
Experiments and analysis. Paper presented 
at the AAAI-98 Spring Symposium on 
Intelligent Text Summarization, Stanford, 
CA, USA. 
Alex Lascarides, & Nicholas Asher. 1993. 
Temporal interpretation, discourse 
relations and common sense entailment. 
Linguistics and Philosophy, 16(5), 437-
493. 
Hans Peter Luhn. 1958. The automatic creation of 
literature abstracts. IBM Journal of 
Research and Development, 2(2), 159-165. 
William C Mann, & Sandra A Thompson. 1988. 
Rhetorical structure theory: Toward a 
functional theory of text organization. 
Text, 8(3), 243-281. 
Christopher D Manning, & Hinrich Schuetze. 
2000. Foundations of statistical natural 
language processing. Cambridge, MA, 
USA: MIT Press. 
Daniel Marcu. 2000. The theory and practice of 
discourse parsing and summarization. 
Cambridge, MA: MIT Press. 
Mandar Mitra, Amit Singhal, & Chris Buckley. 
1997. Automatic text summarization by 
paragraph extraction. Paper presented at 
the ACL/EACL-97 Workshop on 
Intelligent Scalable Text Summarization, 
Madrid, Spain. 
Kenji Ono, Kazuo Sumita, & Seiji Miike. 1994. 
Abstract generation based on rhetorical 
structure extraction. Paper presented at the 
COLING-94, Kyoto, Japan. 
Lawrence Page, Sergey Brin, Rajeev Motwani, & 
Terry Winograd. 1998. The PageRank 
citation ranking: Bringing order to the 
web. Stanford, CA. 
Dragomir R Radev, Eduard Hovy, & Kathleen R 
McKeown. 2002. Introduction to the 
special issue on summarization. 
Computational Linguistics, 28(4), 399-
408. 
Gerard Salton, & Christopher Buckley. 1988. 
Term-weighting approaches in automatic 
text retrieval. Information Processing and 
Management, 24(5), 513-523. 
Karen Sparck-Jones. 1993. What might be in a 
summary? In G. Knorz, J. Krause & C. 
Womser-Hacker (Eds.), Information 
retrieval 93: Von der Modellierung zur 
Anwendung (pp. 9-26). Konstanz: 
Universitaetsverlag. 
Karen Sparck-Jones, & Tetsuya Sakai. 2001, 
September 2001. Generic summaries for 
indexing in IR. Paper presented at the 
ACM SIGIR-2001, New Orleans, LA, 
USA. 
Klaus Zechner. 1996. Fast generation of abstracts 
from general domain text corpora by 
extracting relevant sentences. Paper 
presented at the COLING-96, 
Copenhagen, Denmark. 
 
Paragraph-, word-, and coherence-based approaches to sentence ranking:
A comparison of algorithm and human performance
Florian Wolf, Edward Gibson
Massachusetts Institute of Technology, Department of Brain and Cognitive Sciences
fwolf@mit.edu, egibson@mit.edu
Abstract
Sentence ranking is a crucial part of generating text
summaries. We compared human sentence rankings
obtained in a psycholinguistic experiment to three
different approaches to sentence ranking: A simple
paragraph-based approach intended as a baseline,
two word-based approaches, and two coherence-
based approaches. In the paragraph-based ap-
proach, sentences in the beginning of paragraphs
received higher importance ratings than other sen-
tences. The word-based approaches determined
sentence rankings based on relative word frequen-
cies (Luhn (1958); Salton & Buckley (1988)).
Coherence-based approaches determined sentence
rankings based on some property of the coher-
ence structure of a text (Marcu (2000); Page et
al. (1998)). Our results suggest poor perfor-
mance for the simple paragraph-based approach,
whereas word-based approaches perform remark-
ably well. The best performance was achieved
by a coherence-based approach where coherence
structures are represented in a non-tree structure.
Most approaches also outperformed the commer-
cially available MSWord summarizer.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 781?782, Dublin, Ireland, August 23-29 2014.
Language for Communication: Language as Rational Inference
Edward Gibson
Massachusetts Institute of Technology
Brain and Cognitive Sciences Department
Cambridge, MA, USA
egibson@mit.edu
Invited Speaker Abstract
Perhaps the most obvious hypothesis for the evolutionary function of human language is for use in com-
munication. Chomsky has famously argued that this is a flawed hypothesis, because of the existence of
such phenomena as ambiguity. Furthermore, he argues that the kinds of things that people tend to say
are not short and simple, as would be predicted by communication theory. Contrary to Chomsky, my
group applies information theory and communication theory from Shannon (1948) in order to attempt
to explain the typical usage of language in comprehension and production, together with the structure
of languages themselves. First, we show that ambiguity out of context is not only not a problem for an
information-theoretic approach to language, it is a feature. Second, we show that language comprehen-
sion appears to function as a noisy channel process, in line with communication theory. Given si, the
intended sentence, and sp, the perceived sentence we propose that people maximize P (s
i
|s
p
), which is
equivalent to maximizing the product of the prior P (s
i
) and the likely noise processes P (s
i
? s
p
).
We show that several predictions of this way of thinking of language are true:
1. the more noise that is needed to edit from one alternative to another leads to lower likelihood that
the alternative will be considered;
2. in the noise process, deletions are more likely than insertions;
3. increasing the noise increases the reliance on the prior (semantics); and
4. increasing the likelihood of implausible events decreases the reliance on the prior.
Third, we show that this way of thinking about language leads to a simple re-thinking of the P600 from
the ERP literature. The P600 wave was originally proposed to be due to people?s sensitivity to syntactic
violations, but there have been many instances of problematic data in the literature for this interpretation.
We show that the P600 can best be interpreted as sensitivity to an edit in the signal, in order to make it
more easily interpretable.
Finally, we discuss how thinking of language as communication can explain aspects of the origin of
word order. Some recent evidence suggests that subject-object-verb (SOV) may be the default word order
for human language. For example, SOV is the preferred word order in a task where participants gesture
event meanings (Goldin-Meadow et al. 2008). Critically, SOV gesture production occurs not only for
speakers of SOV languages, but also for speakers of SVO languages, such as English, Chinese, Spanish
(Goldin-Meadow et al. 2008) and Italian (Langus and Nespor, 2010). The gesture-production task
therefore plausibly reflects default word order independent of native language. However, this leaves open
the question of why there are so many SVO languages (41.2% of languages; Dryer, 2005). We propose
that the high percentage of SVO languages cross-linguistically is due to communication pressures over a
noisy channel. We provide several gesture experiments consistent with this hypothesis, and we speculate
how a noisy channel approach might explain several typical word order patterns that occur in the world?s
languages.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
781
References
Matthew S. Dryer. 2005. The order of subject, object and verb. In The World Atlas of Language Structures, pages
330?333. Oxford University Press, Oxford, UK.
Susan Goldin-Meadow, Wing Chee So, Asl?
?
Ozy?urek, and Carolyn Mylander. 2008. The natural order of events:
How speakers of different languages represent events nonverbally. Proceedings of the National Academy of
Sciences of the United States of America, 105(27):9163?9168.
Alan Langusa and Marina Nespor. 2010. Cognitive systems struggling for word order. Cognitive Psychology,
60(4):291?318.
C.E. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:623?656.
782
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 115?119,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Arguments and Modifiers from the Learner?s Perspective
Leon Bergen
MIT
Brain and Cognitive Science
bergen@mit.edu
Edward Gibson
MIT
Brain and Cognitive Science
egibson@mit.edu
Timothy J. O?Donnell
MIT
Brain and Cognitive Science
timod@mit.edu
Abstract
We present a model for inducing sen-
tential argument structure, which distin-
guishes arguments from optional modi-
fiers. We use this model to study whether
representing an argument/modifier distinc-
tion helps in learning argument structure,
and whether a linguistically-natural argu-
ment/modifier distinction can be induced
from distributional data alone. Our results
provide evidence for both hypotheses.
1 Introduction
A fundamental challenge facing the language
learner is to determine the content and structure
of the stored units in the lexicon. This problem is
made more difficult by the fact that many lexical
units have argument structure. Consider the verb
put. The sentence, John put the socks is incom-
plete; when hearing such an utterance, a speaker
of English will expect a location to also be speci-
fied: John put the socks in the drawer. Facts such
as these can be captured if the lexical entry for put
also specifies that the verb has three required ar-
guments: (i) who is doing the putting (ii) what is
being put (iii) and the destination of the putting.
The problem of acquiring argument structure is
further complicated by the fact that not all phrases
in a sentence fill an argument role. Instead, many
are modifiers. Consider the sentence John put the
socks in the drawer at 5 o?clock. The phrase at
5 o?clock occurs here with the verb put, but it is
not an argument. Removing this phrase does not
change the core structure of the PUTTING event,
nor is the sentence incomplete without this phrase.
The distinction between arguments and mod-
ifiers has a long history in traditional grammar
and is leveraged in many modern theories of syn-
tax (Haegeman, 1994; Steedman, 2001; Sag et
al., 2003). Despite the ubiquity of the distinc-
S
NP
John
VP
V
put
NP
the socks
PP
in the drawer
PP
at 5 o?clock
S
NP
John
VP
V
put
NP
the socks
PP
in the drawer
Figure 1: The VP?s in these sentences only share
structure if we separate arguments from modifiers.
tion in syntax, however, there is a lack of consen-
sus on the necessary and sufficient conditions for
argumenthood (Schu?tze, 1995; Schu?tze and Gib-
son, 1999). It remains unclear whether the argu-
ment/modifier distinction is purely semantic or is
also represented in syntax, whether it is binary or
graded, and what effects argument/modifierhood
have on the distribution of linguistic forms.
In this work, we take a new approach to these
problems. We propose that the argument/modifier
distinction is inferred on a phrase?by?phrase basis
using probabilistic inference. Crucially, allowing
the learner to separate the core argument structure
of phrases from peripheral modifier content in-
creases the generalizability of argument construc-
tions. For example, the two sentences in Figure 1
intuitively share the same argument structures, but
this overlap can only be identified if the preposi-
tional phrase, ?at 5 o?clock,? is treated as a modi-
fier. Thus representing the argument/modifier dis-
tinction can help the learner find useful argument
structures which generalize robustly.
Although, like the majority of theorists, we
agree that the argument/adjunct distinction is fun-
damentally semantic, in this work we focus on its
distributional correlates. Does the optionality of
modifier phrases help the learner acquire lexical
items with the right argument structure?
2 Approach
We adopt an approach where the lexicon consists
of an inventory of stored tree fragments. These
115
tree fragments encode the necessary phrase types
(i.e., arguments) that must be present in a struc-
ture before it is complete. In this system, sen-
tences are generated by recursive substitution of
tree fragments at the frontier argument nodes of
other tree fragments. This approach extends work
on learning probabilistic Tree?Substitution Gram-
mars (TSGs) (Post and Gildea, 2009; Cohn et al,
2010; O?Donnell, 2011; O?Donnell et al, 2011).1
To model modification, we introduce a second
structure?building operation, adjunction. While
substitution must be licensed by the existence
of an argument node, adjunction can insert con-
stituents into well?formed trees. Many syntactic
theories have made use of an adjunction operation
to model modification. Here, we adopt the variant
known as sister?adjunction (Rambow et al, 1995;
Chiang and Bikel, 2002) which can insert a con-
stituent as the sister to any node in an existing tree.
In order to derive the complete tree for a sen-
tence, starting from an S root node, we recursively
sample arguments and modifiers as follows.2 For
every nonterminal node on the frontier of our
derivation, we sample an elementary tree from our
lexicon to substitute into this node. As already
noted, these elementary trees represent the argu-
ment structure of our tree. Then, for each argu-
ment nonterminal on the tree?s interior, we sister?
adjoin one or more modifier nodes, which them-
selves are built by the same recursive process.
Figure 2 illustrates two derivations of the
same tree, one in standard TSG without sister?
adjunction, and one in our model. In the TSG
derivation, at top, an elementary tree with four ar-
guments ? including the intuitively optional tem-
poral PP ? is used as the backbone for the deriva-
tion. The four phrases filling these arguments
are then substituted into the elementary tree, as
indicated by arrows. In the bottom derivation,
which uses sister?adjunction, an elementary tree
with only three arguments is used as the back-
bone. While the right-most temporal PP needed
to be an argument of the elementary tree in the
TSG derivation, the bottom derivation uses sister?
adjunction to insert this PP as a child of the VP.
Sister?adjunction therefore allows us to use an ar-
1Note that we depart from many discussions of argument
structure in that we do not require that every stored fragment
has a head word. In effect, we allow completely abstract
phrasal constructions to also have argument structures.
2Our generative model is related to the generative model
for Tree?Adjoining Grammars proposed in (Chiang, 2000)
S
NP
John
VP
V
put
NP
the socks
PP
in the drawer
PP
at 5 o?clock
NP
John
NP
the socks
PP
in the drawer
PP
at 5 o?clock
S
NP
John
VP
V
put
NP
the socks
PP
in the drawer
PP
at 5 o?clock
NP
John
NP
the socks
PP
at 5 o?clock
PP
in the drawer
Figure 2: The first part of the figure shows how
to derive the tree in TSG, while the second part
shows how to use sister-adjunction to derive the
same tree in our model.
gument structure that matches the true argument
structure of the verb ?put.?
This figure illustrates how derivations in our
model can have a greater degree of generalizabil-
ity than those in a standard TSG. Sister?adjunction
will be used to derive children which are not part
of the core argument structure, meaning that a
greater variety of structures can be derived by a
combination of common argument structures and
sister-adjoined modifiers. Importantly, this makes
the learning problem for our model less sparse
than for TSGs; our model can derive the trees in a
corpus using fewer types of elementary trees than
a TSG. As a result, the distribution over these ele-
mentary trees is easier to estimate.
To understand what role modifiers play during
learning, we will develop a learning model that
can induce the lexicon and modifier contexts used
by our generative model.
3 Model
Our model extends earlier work on induction
of Bayesian TSGs (Post and Gildea, 2009;
O?Donnell, 2011; Cohn et al, 2010). The model
uses a Bayesian non?parametric distribution?the
Pitman-Yor Process, to place a prior over the lex-
icon of elementary trees. This distribution allows
the complexity of the lexicon to grow to arbitrary
size with the input, while still enforcing a bias for
more compact lexicons.
116
For each nonterminal c, we define:
Gc|ac, bc, PE ? PYP(ac, bc, PE(?|c)) (1)
e|c,Gc ? Gc, (2)
where PE(?|c) is a context free distribution over
elementary trees rooted at c, and e is an elementary
tree.
The context-free distribution over elementary
trees PE(e|c) is defined by:
PE(e|c) =
?
i?I(e)
(1?sci)
?
f?F (e)
scf
?
c????e
Pc?(?|c?),
(3)
where I(e) is the set of internal nodes in e, F (e) is
the set of frontier nodes, ci is the nonterminal cat-
egory associated with node i, and sc is the proba-
bility that we stop expanding at a node c. For this
paper, the parameters sc are set to 0.5.
In addition to defining a distribution over ele-
mentary trees, we also define a distribution which
governs modification via sister?adjunction. To
sample a modifier, we first decide whether or not
to sister?adjoin into location l in a tree. Following
this step, we sample a modifier category (e.g., a
PP) conditioned on the location l?s context: its par-
ent and left siblings. Because contexts are sparse,
we use a backoff scheme based on hierarchical
Dirichlet processes similar to the ngram backoff
schemes defined in (Teh, 2006; Goldwater et al,
2006). Let c be a nonterminal node in a tree de-
rived by substitution into argument positions. The
node c will have n ? 1 children derived by ar-
gument substitution: d0, ..., dn. In order to sister?
adjoin between two of these children di, di+1, we
recursively sample nonterminals si,1, ..., si,k until
we hit a STOP symbol:
Pa(si,1, ..., si,k, STOP |C0) (4)
=
k?
j=1
Pa(si,j |Cj) ? (1? PCj (STOP ))
? PCk+1(STOP )
where Cj = d1, s1,1, ..., di, si,1, ..., si,j?1, c is the
context for the j?th modifier between these chil-
dren. The distribution over sister?adjoined non-
terminals is defined using a hierarchical Dirichlet
process to implement backoff in a prefix tree over
contexts. We define the distribution G(ql, ..., q1)
over sister?adjoined nonterminals si,j given the
context ql, ..., q1 by:
G(ql, ..., q1) ? DP(?,G(ql?1, ..., q1)). (5)
The distribution G at the root of the hierarchy is
not conditioned on any prior context. We define G
by:
G ? DP(?,Multinomial(m)) (6)
where m is a vector with entries for each nonter-
minal, and where we samplem ? Dir(1,...,1).
To perform inference, we developed a local
Gibbs sampler which generalizes the one proposed
by (Cohn et al, 2010).
4 Results
We evaluate our model in two ways. First,
we examine whether representing the argu-
ment/modifier distinction increases the ability of
the model to learn highly generalizable elemen-
tary trees that can be used as argument structures
across a variety of sentences. Second, we ask
whether our model is able to induce the correct
argument/modifier distinction according to a lin-
guistic gold?standard. We trained our model on
sections 2?21 of the WSJ part of the Penn Tree-
bank (Marcus et al, 1999). The model was trained
on the trees in this corpus, without any further an-
notations for substitution or modification.
To address the first question, we compared the
structure of the grammar learned by our model to
a grammar learned by a version of our model with-
out sister?adjunction (i.e., a TSG similar to the
one used in Cohn et al). Our model should find
more common structure among the trees in the in-
put corpus, and therefore it should learn a set of el-
ementary trees which are more complex and more
widely shared across sentences. We evaluated this
hypothesis by analyzing the average complexity
of the most probable elementary trees learned by
these models. As Table 1 shows, our model dis-
covers elementary trees that have greater depth
and more nodes than those found by the TSG. In
addition, our model accounts for a larger portion
of the corpus with fewer rules: the top 50, 100, and
200 most common elementary trees in our model?s
lexicon account for a greater portion of the corpus
than the corresponding sets in the TSG.
Figure 3 illustrates a representative example
from the corpus. By using sister-adjuntion to sepa-
rate the ADVP node from the rest of the sentence?s
derivation, our model was able to use a common
depth-3 elementary tree to derive the backbone of
the sentence. In contrast, the TSG cannot give the
same derivation, as it needs to include the ADVP
117
SNP
Most of those who left stock funds
ADVP
simply
VP
VP
VBD
switched
PP
into money market fundsPP
into money market fundsVBD
switched
NP
Most of those who left stock funds
Figure 3: Part of a derivation found by our model.
Model Rank Avg tree
depth
Avg tree
size
#Tokens
Modifier 50 1.59 3.42 97282
TSG 50 1.38 2.98 88023
Modifier 100 1.84 3.98 134205
TSG 100 1.58 3.38 116404
Modifier 200 1.97 4.27 170524
TSG 200 1.77 3.84 146040
Table 1: This table shows the average depth and
node count for elementary trees in our model and
the TSG. The results are shown for the 50, 100,
and 200 most frequent types of elementary trees.
node in the elementary tree; this wider elementary
tree is much less common in the corpus.
We next examined whether our model learned
to correctly identify modifiers in the corpus. Un-
fortunately, marking for argument/modifiers in the
Penn Treebank is incomplete, and is limited to
certain adverbials, e.g. locative and temporal
PP?s. To supplement this markup, we made use of
the corpus of (Kaeshammer and Demberg, 2012).
This corpus adds annotations indicating, for each
node in the Penn Treebank, whether that node is
a modifier. This corpus was compiled by com-
bining information from Propbank (Palmer et al,
2005) with a set of heuristics, as well as the NP-
branching structures proposed in (Vadas and Cur-
ran, 2007). It is important to note that this corpus
can only serve as a rough benchmark for evalua-
tion of our model, as the heuristics used in its de-
velopment did not always follow the correct lin-
guistic analysis; the corpus was originally con-
structed for an alternative application in compu-
tational linguistics, for which non?linguistically?
natural analyses were sometimes convenient. Our
model was trained on this corpus, after it had been
stripped of argument/modifier annotations.
We compare our model?s performance to a ran-
dom baseline. Our model constrains every non-
terminal to have at least one argument child, and
our Gibbs sampler initializes argument/modifier
choices randomly subject to this constraint. We
Model Precision Recall #Guessed #Correct
Random 0.27 0.19 298394 82702
Modifier 0.62 0.15 108382 67516
Table 2: This table shows precision and recall in
identifying modifier nodes in the corpus.
therefore calculated the probability that a node
that was randomly initialized as a modifier was in
fact a modifier, i.e. the precision of random ini-
tialization. Next, we looked at the precision of
our model following training. Table 2 shows that
among nodes that were labeled as modifiers, 0.27
were labeled correctly before training and 0.62
were labeled correctly after. This table also shows
the recall performance for our model decreased by
0.04. Some of this decrease is due to limitations of
the gold standard; for example, our model learns
to classify infinitives and auxiliary verbs as argu-
ments ? consistent with standard linguistic anal-
yses ? whereas the gold standard classifies these
as modifiers. Future work will investigate how the
metric used for evaluation can be improved.
5 Summary
We have investigated the role of the argu-
ment/modifier distinction in learning. We first
looked at whether introducing this distinction
helps in generalizing from an input corpus.
Our model, which represents modification using
sister?adjunction, learns a richer lexicon than a
model without modification, and its lexicon pro-
vides a more compact representation of the in-
put corpus. We next looked at whether the tra-
ditional linguistic classification of arguments and
modifiers can be induced from distributional in-
formation. Without supervision from the correct
labelings of modifiers, our model learned to iden-
tify modifiers more accurately than chance. This
suggests that although the argument/modifier dis-
tinction is traditionally drawn without reference to
distributional properties, the distributional corre-
lates of this distinction are sufficient to partially
reconstruct it from a corpus. Taken together, these
results suggest that representing the difference be-
tween arguments and modifiers may make it easier
to acquire a language?s argument structure.
Acknowledgments
We thank Vera Demberg for providing the gold
standard, and Tom Wasow for helpful comments.
118
References
David Chiang and Daniel Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING 2002.
David Chiang. 2000. Staistical parsing with an
automatically?extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree?substitution grammars. Jour-
nal of Machine Learning Research, 11:3053?3096.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Interpolating between types and to-
kens by estimating power?law generators. In Ad-
vances in Neural Information Processing Systems
18, Cambridge, Ma. MIT Press.
Liliane Haegeman. 1994. Government & Binding The-
ory. Blackwell.
Mirian Kaeshammer and Vera Demberg. 2012. Ger-
man and English treebanks and lexica for tree?
adjoining grammars. In Proceedings of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2012).
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank?
3. Technical report, Linguistic Data Consortium,
Philadelphia.
Timothy J. O?Donnell, Jesse Snedeker, Joshua B.
Tenenbaum, and Noah D. Goodman. 2011. Pro-
ductivity and reuse in language. In Proceedings of
the 33rd Annual Conference of the Cognitive Science
Society.
Timothy J. O?Donnell. 2011. Productivity and Reuse
in Language. Ph.D. thesis, Harvard University.
Martha Palmer, P. Kingsbury, and Daniel Gildea. 2005.
The proposition bank. Computational Linguistics,
31(1):71?106.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D?tree grammars. In Proceedings of the
33rd annual meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
CSLI, Stanford, CA, 2 edition.
Carson T Schu?tze and Edward Gibson. 1999. Ar-
gumenthood and english prepositional phrase at-
tachment. Journal of Memory and Language,
40(3):409?431.
Carson T. Schu?tze. 1995. PP attachment and argu-
menthood. Technical report, Papers on language
processing and acquisition, MIT working papers in
linguistics, Cambridge, Ma.
Mark Steedman. 2001. The syntactic process. The
MIT press.
Yee Whye Teh. 2006. A Bayesian interpretation of in-
terpolated Kneser-Ney. Technical Report TRA2/06,
National University of Singapore, School of Com-
puting.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th annual meeting of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
119
