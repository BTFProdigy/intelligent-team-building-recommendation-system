Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929?937,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Hypernym Discovery Based on Distributional Similarity                     
and Hierarchical Structures 
Ichiro Yamada?, Kentaro Torisawa?, Jun?ichi Kazama?, Kow Kuroda?,  
Masaki Murata?, Stijn De Saeger?, Francis Bond? and Asuka Sumida? 
 
?National Institute of Information and Communications Technology 
3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN 
{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp
?Japan Advanced Institute of Science and Technology 
1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPAN 
a-sumida@jaist.ac.jp 
 
Abstract 
This paper presents a new method of devel-
oping a large-scale hyponymy relation data-
base by combining Wikipedia and other Web 
documents. We attach new words to the hy-
ponymy database extracted from Wikipedia 
by using distributional similarity calculated 
from documents on the Web. For a given tar-
get word, our algorithm first finds k similar 
words from the Wikipedia database. Then, 
the hypernyms of these k similar words are 
assigned scores by considering the distribu-
tional similarities and hierarchical distances 
in the Wikipedia database. Finally, new hy-
ponymy relations are output according to the 
scores. In this paper, we tested two distribu-
tional similarities. One is based on raw verb-
noun dependencies (which we call ?RVD?), 
and the other is based on a large-scale clus-
tering of verb-noun dependencies (called 
?CVD?). Our method achieved an attachment 
accuracy of 91.0% for the top 10,000 rela-
tions, and an attachment accuracy of 74.5% 
for the top 100,000 relations when using 
CVD. This was a far better outcome com-
pared to the other baseline approaches. Ex-
cluding the region that had very high scores, 
CVD was found to be more effective than 
RVD. We also confirmed that most relations 
extracted by our method cannot be extracted 
merely by applying the well-known lexico-
syntactic patterns to Web documents. 
1 Introduction 
Large-scale taxonomies such as WordNet (Fell-
baum 1998) play an important role in informa-
tion extraction and question answering. However, 
extremely high costs are borne to manually en-
large and maintain such taxonomies. Thus, appli-
cations using these taxonomies tend to face the 
drawback of data sparseness. This paper presents 
a new method for discovering a large set of hy-
ponymy relations. Here, a word1 X is regarded as 
a hypernym of a word Y if Y is a kind of X or Y 
is an instance of X. We are able to generate 
large-scale hyponymy relations by attaching new 
words to the hyponymy database extracted from 
Wikipedia (referred to as ?Wikipedia relation 
database?) by using distributional similarity cal-
culated from Web documents. Relations ex-
tracted from Wikipedia are relatively clean. On 
the other hand, reliable distributional similarity 
can be calculated using a large number of docu-
ments on the Web. In this paper, we combine the 
advantages of these two resources.  
Using distributional similarity, our algorithm 
first computes k similar words for a target word. 
Then, each k similar word assigns a score to its 
ancestors in the hierarchical structures of the 
Wikipedia relation database. The hypernym that 
has the highest score for the target word is se-
lected as the hypernym of the target word. Figure 
1 is an overview of the proposed approach. 
In the experiment, we extracted hypernyms for 
approximately 670,000 target words that are not 
included in the Wikipedia relation database but 
are found on the Web. We tested two distribu-
tional similarities: one based on raw verb-noun 
dependencies (RVD) and the other based on a 
large-scale clustering of verb-noun dependencies 
(CVD). The experimental results showed that the 
proposed methods were more effective than the 
other baseline approaches. In addition, we con-
firmed that most of the relations extracted by our 
method could not be extracted using the lexico-
syntactic pattern-based method.  
In the remainder of this paper, we first intro-
                                                 
1 In this paper, we use the term ?word? for both ?a 
single-word word? and ?a multi-word word.? 
929
duce some related works in Section 2. Section 3 
describes the Wikipedia relation database. Sec-
tion 4 describes the distributional similarity cal-
culated by the two methods. In Section 5, we 
describe a method to discover an appropriate 
hypernym for each target word. The experimen-
tal results are presented in Section 6 before con-
cluding the paper in Section 7. 
2 Related Works 
Most previous researchers have relied on lex-
ico-syntactic patterns for hyponymy acquisition. 
Lexico-syntactic patterns were first used by 
Hearst (1992). The patterns used by her included 
?NP0 such as NP1,? in which NP0 is a hypernym 
of NP1. Using these patterns as seeds, Hearst dis-
covered new patterns by which to semi-
automatically extract hyponymy relations. Pantel 
et al (2004a) proposed a method to automatical-
ly discover the patterns using a minimal edit dis-
tance. Ando et al (2003) applied predefined lex-
ico-syntactic patterns to Japanese news articles. 
Snow et al (2005) generalized these lexico-
syntactic pattern-based methods by using depen-
dency path features for machine learning. Then, 
they extended the framework such that this me-
thod was capable of making use of heterogenous 
evidence (Snow et al 2006). These pattern-based 
methods require the co-occurrences of a target 
word and the hypernym in a document. It should 
be noted that the requirement of such co-
occurrences actually poses a problem when we 
extract a large set of hyponymy relations since 
they are not frequently observed (Shinzato et al 
2004, Pantel et al 2004b). 
Clustering-based methods have been proposed 
as another approach. Caraballo (1999), Pantel et 
al. (2004b), and Shinzato et al (2004) proposed a 
method to find a common hypernym for word 
classes, which are automatically constructed us-
ing some measures of word similarities or hierar-
chical structures in HTML documents. Etzioni et 
al. (2005) used both a pattern-based approach 
and a clustering-based approach. The required 
amount of co-occurrences is significantly re-
duced due to class-based generalization 
processes. Note that these clustering-based me-
thods obtain the same hypernym for all the words 
in a particular class. This causes a problem for 
selecting an appropriate hypernym for each word 
in the case when the granularity or the construc-
tion of the classes is incorrect. Figure 2 shows 
the drawbacks of the existing approaches. 
Ponzetto et al (2007) and Sumida et al (2008) 
proposed a method for acquiring hyponymy rela-
tions from Wikipedia. This Wikipedia-based ap-
proach can extract a large volume of hyponymy 
relations with high accuracy. However, it is also 
true that this approach does not account for many 
words that usually appear in Web documents; 
this could be because of the unbalanced topics in 
Wikipedia or merely because of the incomplete 
coverage of articles on Wikipedia. Our method 
can target words that frequently appear on the 
Web but are not included in the Wikipedia rela-
tion database, thus making the results of the Wi-
kipedia-based approach richer and more ba-
lanced. Our approach uses distributional similari-
Figure 1: Overview of the proposed approach. 
hypernym : 
Target word:  Selected from the Web 
: word
k similar words
No direct co-occurrences of 
hypernym and hyponym in 
corpora are needed.
Selected from hypernyms in the 
Wikipedia relation database.
A hypernym is selected for 
each word independently.
Wikipedia relation database
Wikipedia-based approach
(Ponzetto et al 2007 and 
Sumida et al 2008)
Hyponymy relations are 
extracted using the layout 
information of Wikipedia.
Wikipedia
Figure 2: Drawbacks in existing approaches for hypo-
nymy acquisition. 
Pattern-based method
(Hearst 1992, Pantel et al 
2004a, Ando et al 2003, 
Snow et al 2005, Snow et al 
2006, and Etzioni et al 2005)
Clustering-based method
(Caraballo 1999, Pantel et al 
2004b, Shinzato et al 2004, 
and Etzioni et al 2005)
DocumentsCorpus/documents
Co-occurrences 
in a pattern are 
needed 
hypernym such as word hypernym ..?   word
word
word
wordword
Word Class
word
The same hypernym 
is selected for all 
words in a class.
930
ty, which is computed based on the noun-verb 
dependency profiles on the Web. The use of dis-
tributional similarity resembles the clustering-
based approach; however, our method can select 
a hypernym for each word independently, and it 
does not suffer from class granularity mismatch 
or the low quality of classes. In addition, our ap-
proach exploits the hierarchical structures of the 
Wikipedia hypernym relations.  
3 Wikipedia Relation Database 
Our Wikipedia relation database is based on the 
extraction method of Sumida et al (2008). They 
proposed a method of automatically acquiring 
hyponymy relations by focusing on the hierar-
chical layout of articles on Wikipedia. By way of 
an example, Figure 3 shows part of the source 
code clipped from the article titled ?Penguin.? 
An article has hierarchical structures composed 
of titles, sections, itemizations, etc. The entire 
article is divided into sections titled ?Anatomy,? 
?Mating habits,? ?Systematics and evolution,? 
?Penguins in popular culture,? and so on. The 
section ?Systematics and evolution? has a sub-
section ?Systematics,? which is further divided 
into ?Aptenodytes,? ?Eudyptes,? and so on. 
Some of these section-subsection relations can be 
regarded as valid hyponymy relations. In this 
article, relations such as the one between ?Apte-
nodytes? and ?Emperor Penguin? and that be-
tween ?Book? and ?Penguins of the World? are 
valid hyponymy relations.  
First, Sumida et al (2008) extracted hypony-
my relation candidates from hierarchical struc-
tures on Wikipedia. Then, they selected proper 
hyponymy relations using a support vector ma-
chine classifier. They used several kinds of fea-
tures for the hyponymy relation candidate, such 
as a POS tag for each word, the appearance of 
morphemes of each word, the distance between 
two words in the hierarchical structures of Wiki-
pedia, and the last character of each word. As a 
result of their experiments, approximately 2.4 
million hyponymy relations in Japanese were 
extracted, with a precision rate of 90.1%.  
Compared to the traditional taxonomies, these 
extracted hyponymy relations have the following 
characteristics (Fellbaum 1998, Bond et al 2008). 
(a) The database includes a more extensive 
vocabulary. 
(b)  The database includes a large number of 
named entities. 
Popular Japanese taxonomies GoiTaikei (Ike-
hara et al 1997) and Bunrui-Goi-Hyo (1996) 
contain approximately 300,000 words and 
96,000 words, respectively. In contrast, the ex-
tracted hyponymy relations contain approximate-
ly 1.2 million hyponyms and are undoubtedly 
much larger than the existing taxonomies. 
Another difference is that since Wikipedia covers 
a large number of named entities, the extracted 
hyponymy relations also contain a large number 
of named entities.  
Note that the extracted relations have a hierar-
chical structure because one hypernym of a cer-
tain word may also be the hyponym of another 
hypernym. However, we observed that the depth 
of the hierarchy, on an average, is extremely 
shallow. To make the hierarchy appropriate for 
our method, we extended these into a deeper hie-
rarchical structure. The extracted relations in-
clude many compound nouns as hypernyms, and 
we decomposed a compound noun into a se-
quence of nouns using a morphological analyzer. 
Since Japanese is a head-final language, the suf-
fix of a noun sequence becomes the hypernym of 
the original compound noun if the suffix forms 
another valid compound noun. We extracted suf-
fixes of compound nouns and manually checked 
whether they were valid compound nouns; then, 
we constructed a hierarchy of compound nouns. 
The hierarchy can be extended such that it in-
cludes the hyponyms of the original hypernym 
and the resulting hierarchy constitutes a hierar-
chical taxonomy. We use this hierarchical tax-
onomy as a target for expansion.2  
                                                 
2  Note that this modification was performed as part of 
another project of ours aimed at constructing a large-scale 
and clean hypernym knowledge base by human annotation. 
We do not think this cost is directly relevant to the method 
proposed here. 
Figure 3: A part of source code clipped from the 
article ?Penguin? in Wikipedia. 
'''Penguins''' are a group of 
[[Aquatic animal|aquatic]], 
[[flightless bird]]s. 
== Anatomy == 
== Mating habits == 
==Systematics and evolution== 
===Systematics=== 
* Aptenodytes 
**[[Emperor Penguin]] 
** [[King Penguin]] 
* Eudyptes 
== Penguins in popular culture == 
== Book == 
* Penguins 
* Penguins of the World 
== Notes == 
* Penguinone 
* the [[Penguin missile]] 
[[Category:Penguins]] 
[[Category:Birds]]
931
4 Distributional Similarity 
The distributional hypothesis states that words 
that occur in similar contexts tend to be semanti-
cally similar (Harris 1985). In this section, we 
first introduce distributional similarity based on 
raw verb-noun dependencies (RVD). To avoid 
the sparseness problem of the co-occurrence of 
verb-noun dependencies, we also use distribu-
tional similarity based on a large-scale clustering 
of verb-noun dependencies (CVD). 
In the experiment mentioned in the following 
section, we used the TSUBAKI corpus (Shinzato 
et al 2008) to calculate distributional similarity. 
This corpus provides a collection of 100 million 
Japanese Web pages containing 6 ? 109
 
sentences. 
4.1 Distributional Similarity Based on RVD 
When calculating the distributional similarity 
based on RVD, we use the triple <v, rel, n>, 
where v is a verb, n is a noun phrase, and rel 
stands for the relation between v and n. In Japa-
nese, a relation rel is represented by postposi-
tions attached to n and the phrase composed of n 
and rel modifies v. Each triple is divided into two 
parts. The first is <v, rel> and the second is n. 
Then, we consider the conditional probability of 
occurrence of the pair <v, rel>: P(<v, rel>|n).  
P(<v, rel>|n) can be regarded as the distribution 
of the grammatical contexts of the noun phrase n. 
The distributional similarity can be defined as 
the distance between these distributions. There 
are several kinds of functions for evaluating the 
distance between two distributions (Lee 1999). 
Our method uses the Jensen-Shannon divergence. 
The Jensen-Shannon divergence between two 
probability distributions, )|( 1nP ?  and )|( 2nP ? , 
can be calculated as follows: 
 
)),
2
)|()|(
||)|((
)
2
)|()|(
||)|(((
2
1
))|(||)|((
21
2
21
1
21
nPnP
nPD
nPnP
nPD
nPnPD
KL
KL
JS
?+??+
?+??=
??
 
 
where DKL indicates the Kullback-Leibler diver-
gence and is defined as follows: 
 
.
)|(
)|(
log)|())|(||)|((
2
1
121 ? ???=?? nP nPnPnPnPDKL  
 
Finally, the distributional similarity between 
two words, n1 and n2, is defined as follows: 
 
)).|(||)|((1),( 2121 nPnPDnnsim JS ???=  
 
This similarity assumes a value from 0 to 1. If 
two words are similar, the value will be close to 
1; if two words have entirely different meanings, 
the value will be 0.
 
In the experiment, we used 1,000,000 noun 
phrases and 100,000 pairs of verbs and postposi-
tions to calculate the probability P(<v, rel>|n) 
from the dependency relations extracted from the 
above-mentioned Web corpus (Shinzato et al 
2008). The probabilities are computed using the 
following equation by modifying for the fre-
quency using the log function: 
 
?
>?<
+><
+><=><
Drelv
nrelvf
nrelvf
nrelvP
,
1),,(log(
1)),,(log(
)|,(
,0),,(if >>< nrelvf
  
where f(<v, rel, n>) is the frequency of a triple 
<v, rel, n> and D is the set defined as { <v, rel > | 
f(<v, rel, n>) > 0 }. In the case of f(<v, rel, n>) = 
0, P(<v, rel>|n) is set to 0.  
Instead of using the observed frequency di-
rectly as in the usual maximum likelihood esti-
mation, we modified it as above. Although this 
might seems strange, this kind of modification is 
common in information retrieval as a term 
weighing method (Manning et al 1999) and  it is 
also applied in some studies to yield better word 
similarities (Terada et al 2006, Kazama et al 
2009). We also adopted this idea in this study. 
4.2 Distributional Similarity Based on CVD 
Rooth et al (1999) and Torisawa (2001) showed 
that EM-based clustering using verb-noun de-
pendencies can produce semantically clean noun 
clusters. We exploit these EM-based clustering 
results as the smoothed contexts for noun n. In 
Torisawa?s model (2001), the probability of oc-
currence of the triple <v, rel, n> is defined as 
follows: 
 
,)()|()|,(
),,(
? ? ><=
><
Aadef aPanParelvP
nrelvP
 
 
where a denotes a hidden class of <v,rel> and n. 
In this equation, the probabilities P(<v,rel>|a), 
P(n|a), and P(a) cannot be calculated directly 
because class a is not observed in a given corpus. 
The EM-based clustering method estimates these 
probabilities using a given corpus. In the E-step, 
932
the probability P(a|<v,rel>) is calculated. In the 
M-step, the probabilities P(<v,rel>|a), P(n|a), 
and P(a) are updated to arrive at the maximum 
likelihood using the results of the E-step. From 
the results of estimation of this EM-based clus-
tering method, we can obtain the probabilities 
P(<v,rel>|a), P(n|a), and P(a) for each <v, rel>, n, 
and a. Then, P(a|n) is calculated by the following 
equation: 
 
.
)()|(
)()|(
)|( ? ?= Aa aPanP
aPanP
naP  
 
P(a|n) can be used to find the class of n. For 
example, the class that has the maximum P(a|n) 
can be regarded as the class to which n belongs. 
Noun phrases that occur with similar pairs 
<v,rel> tend to be classified in the same class. 
Kazama et al (2008) proposed the paralleliza-
tion of this EM-based clustering with the aim of 
enabling large-scale clustering and using the re-
sulting clusters in named entity recognition. Ka-
zama et al (2009) reported the calculation of 
distributional similarity using the clustering re-
sults. The distributional similarity was calculated 
by the Jensen-Shannon divergence, which was 
used in this paper. Similar to the case in Kazama 
et al, we performed word clustering using 
1,000,000 noun phrases and 2,000 classes. Note 
that the frequencies of dependencies were mod-
ified with the log function, as in RVD, described 
in the previous section. 
5 Discovering an Appropriate Hyper-
nym for a Target word 
In the Wikipedia relation database, there are 
about 95,000 hypernyms and about 1.2 million 
hyponyms. In both RVD and CVD, the words 
used were selected according to the number (the 
number of kinds, not the frequency) of <v, rel >s 
that n has dependencies in the data. As a result, 1 
million words were selected. The number of 
common words that are also included in the Wi-
kipedia relation database are as follows: 
 
Hypernyms     28,015 (common hypernyms) 
Hyponyms   175,022 (common hyponyms) 
 
These common hypernyms become candidates 
for hypernyms for a target word. On the other 
hand, the common hyponyms are used as clues 
for identifying appropriate hypernyms. 
In our task, the potential target words are 
about 810,000 in number and are not included in 
the Wikipedia relation database. These include 
some strange words or word phrases that are ex-
tracted due to the failure of morphological analy-
sis. We exclude these words using simple rules. 
Consequently, the number of target words for our 
process is reduced to about 670,000.  
In the following section, we outline the scor-
ing method that uses k similar words to discover 
an appropriate hypernym for a target word. We 
also explain several baseline approaches that use 
distributional similarity. 
5.1 Scoring with k similar Words 
In this approach, we first calculate the similari-
ties between the common hyponyms and a target 
word and select the k most similar common hy-
ponyms. Here, we use a similarity threshold val-
ue Smin to avoid the effect of words having lower 
similarities. If the similarity is less than the thre-
shold value, the word is excluded from the set of 
k similar words. Next, each k similar word votes 
a score to its ancestors in the hierarchical struc-
tures of the Wikipedia relation database. The 
score used to vote for a hypernym nhyper is as fol-
lows: 
 
,),(
)(
)()(
1),(?
??
? ?=
trghyperhypo
hypohyper
nksimilarnDescn
hypotrg
nnr
hyper
nnsimd
nscore
 
 
where ntrg is the target word, Desc(nhyper) is the 
descendant of the hypernym nhyper, ksimilar(ntrg) 
is the k similar word of ntrg, 
1),( ?hypohyper nnrd is a 
penalty that depends on the differences in the 
depth of hierarchy, d is a parameter for the penal-
ty value and has a value between 0 and 1, and 
r(ntrg, nhypo) is the difference in the depth of hie-
rarchy between ntrg and nhypo. sim(ntrg,nhypo) is a 
distributional similarity between ntrg and nhypo.  
As a result of scoring, each hypernym has a 
score for the target word. The hypernym that has 
the highest score for the target word is selected 
as its hypernym. The hyponymy relations thus 
produced are ranked according to the scores. 
Figure 4 shows an example of the scoring 
process. In this example, we use CitroenAX as the 
target word whose hypernym will be identified. 
First, the k similar words are extracted from the 
common hyponyms in the Wikipedia relation: 
Opel Astra, TVR Tuscan, Mitsubishi Minica, and 
Renault Lutecia are extracted. Next, each k simi-
lar word votes a score to its ancestors. The words 
Opel Astra, TVR Tuscan, and Renault Lutecia 
vote to their parent car and the word Mitsubishi 
933
Minica votes to its parent mini-vehicle and its 
grandparent car with a small penalty. Finally, the 
hypernym car, which has the highest score, is 
selected as the hypernym of the target word Ci-
troenAX. 
5.2 Baseline Approaches 
Using distributional similarity, we can also de-
velop the following baseline approaches to dis-
cover hyponymy relations. 
 
Selecting the hypernym of the most similar hy-
ponym (baseline approach 1) 
We use the heuristics that similar words tend to 
have the same hypernym. In this approach, we 
first calculate the similarities between the com-
mon hyponyms and the target word. The com-
mon hyponym most similar to the target word is 
extracted. Then, the parent of the extracted 
common hyponym is regarded as the hypernym 
of the target word. This approach outputs several 
hypernyms when the most similar hyponym has 
several hypernyms. This approach can be consi-
dered to be the same as the scoring method using 
k similar words when k = 1. We use the distribu-
tional similarity between the target word and the 
most similar hyponym in the Wikipedia relation 
database as the score for the appropriateness of 
the resulting hyponymy. 
 
Selecting the most similar hypernym (baseline 
approach 2) 
The distributional similarity between the com-
mon hypernym and the target word is calculated. 
Then, the hypernym that has the highest distribu-
tional similarity is regarded as the hypernym of 
the target word. The similarity is used as the 
score of the appropriateness of the produced hy-
ponymy. 
 
Scoring based on the average similarity of the 
hypernym?s children (baseline approach 3) 
This approach uses the probabilistic distributions 
of the hypernym?s children. We define the prob-
ability )|( hyperchild nP ? characterized by the children 
of the hypernym nhyper, as follows: 
 
,
)(
)()|(
)|(
)(
)(
?
?
?
?
?
=?
hyperhypo
hyperhypo
nChn
hypo
nChn
hypohypo
hyperchild nP
nPnP
nP  
 
where Ch(nhyper) is a set of all children of nhyper. 
Then, distributional similarities between a com-
mon hypernym nhyper and the target word nhypo are 
calculated. The hypernym that has the highest 
distributional similarity is selected as the hyper-
nym of the word. This distributional similarity is 
used as the score of the appropriateness of the 
produced hyponymy. 
If a hypernym has only a few children, the re-
liability of the probabilistic distribution of 
hypernym defined here will be low because the 
Wikipedia relation database includes some incor-
rect relations. For this reason, we use the hyper-
nym only if the number of children it has is more 
than a threshold value.  
6 Experiments 
We evaluated our proposed methods by using it 
in experiments to discover hypernyms from the 
Wikipedia relation database for the target words 
extracted from about 670,000 noun phrases.  
6.1 Parameter Estimation by Preliminary 
Experiments 
In the proposed methods, there are several para-
meters. We performed parameter optimization by 
randomly selecting 694 words as development 
data in our preliminary experiments. The hyper-
nyms of these words were determined manually. 
We adjusted the parameters so that each method 
achieved the best performance for this develop-
ment data. 
The parameters in the scoring method with k 
similar words were adjusted as follows3:  
 (RVD) 
Number of similar words:         k = 100. 
Similarity threshold:           Smin = 0.05. 
Penalty value for ancestors:    d = 0.6. 
                                                 
3 We tested the parameter values k = {100, 200, 300, 400, 
500, 600, 700, 800, 900, 1000}, Smin={0, 0.05, 0.1, 0.15, 0.2, 
0.25, 0.3, 0.35, 0.4} and d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 
0.8, 0.85, 0.9, 0.95, 1.0}. 
Figure 4: Overview of the scoring process.
car
CitroenAX
mini
vehicle
hybrid 
vehicle
Opel 
Astra
Renault 
Lutecia
Mitsubishi
Minica
k similar words
Each k-similar word
votes the score to its 
ancestors in the Wikipedia 
relation database.
Target word selected 
from the Web text (ntrg).
TVR 
Tuscan
: common hypernym(nhyper)
: k similar word &  
common hyponym(nhypo)
x d1
x d0
x d0
934
(CVD) 
Number of similar words:         k = 200. 
Similarity threshold:                Smin = 0.3. 
Penalty value for ancestors:    d = 0.6. 
 
The parameter in baseline approach 3 was ad-
justed as follows: 
Threshold for the number of children: 20. 
6.2 Evaluation of the Experimental Results 
on the Basis of Score Ranking 
Using the adjusted parameters, we conducted 
experiments to extract the hypernym of each tar-
get word with the help of the scoring method 
based on k similar words. In these experiments, 
two kinds of distributional similarity mentioned 
in Section 4 were exploited individually. The 
words that were used in the development data 
were excluded.  
We also conducted a comparative experiment 
in which the parameter value for the penalty of 
the hierarchal difference, d, was set to 0 to clari-
fy the ability of using hierarchal structures in the 
k similar words method. This means each k simi-
lar word votes only to their parent. 
We then judged the quality of each acquired 
hypernym. The evaluation data sets were sam-
pled from the top 1,000, 10,000, 100,000, and 
670,000 results that were ranked according to the 
score of each method. Then, against 200 samples 
that were randomly sampled from each set, one 
of the authors judged whether the hypernym ex-
tracted by each method for the target word was 
correct or not. In this evaluation, if the sentence 
?The target word is a kind of the hypernym? or 
?The target word is an instance of the hypernym? 
was consistent, the extracted hyponymy was 
judged as correct. It should be noted that the out-
puts of the compared methods are combined and 
shuffled to enable fair comparison. In addition, 
baseline approach 1 extracted several hypernyms 
for the target word. In this case, we judged the 
hypernym as correct when the case where one of 
the hypernyms was correct.  
The precision of each result is shown in Table 
1. The results of the k similar words method are 
far better than those of the other baseline me-
thods. In particular, the k similar words method 
with CVD outperformed the methods of the k 
similar words where the parameter value d was 
set to 0 and the method using RVD except for the 
top 1,000 results. This means that the use of hie-
rarchal structures and the clustering process for 
calculating distributional similarity are effective 
for this task. We confirmed the significant differ-
ences of the proposed method (CVD) as com-
pared with all the baseline approaches at the 1% 
significant level by the Fisher?s exact test (Hays 
1988). 
The precision of baseline approach 2 that se-
lected the most similar hypernym was the worst 
among all the methods. There were words that 
were similar to the target word among the hyper-
nyms extracted incorrectly. For example, the 
word semento-kojo (cement factory) was ex-
tracted for the hypernym of the word kuriningu-
kojo (dry cleaning plant). It is difficult to judge 
whether the word is a hypernym or just a similar 
word by using only the similarity measure. 
As for the results of baseline approach 1 using 
the most similar hyponym and baseline approach 
3 using the similarity of the set of hypernym?s 
children, the noise on the Wikipedia relation da-
tabase decreased the precision. Moreover, over-
specified hypernyms were extracted incorrectly 
by these methods. In contrast, the method of 
scoring based on the use of k similar words was 
robust against noise because it uses the voting 
approach for the similarities. Further, this me-
thod can extract hypernyms that are not over-
specific because it uses all descendants for scor-
ing.  
Table 2 shows some examples of relations ex-
tracted by the k similar words method using 
CVD. 
 
Table 1:  Precision of each approach based on the score ranking. CVD represents the method that uses the dis-
tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the 
one based on raw verb-noun dependencies. 
 k-similar words
(CVD) 
k-similar words
(RVD) 
k-similar words
(CVD, d = 0)
Baseline  
approach 1 
(CVD) 
Baseline  
approach 2 
(CVD) 
Baseline  
approach 3 
(CVD) 
1,000 0.940 1.000 0.850 0.730 0.290 0.630 
10,000 0.910 0.875 0.875 0.555 0.300 0.445 
100,000 0.745 0.710 0.730 0.500 0.280 0.435 
670,000 0.520 0.500 0.470 0.345 0.115 0.170 
935
6.3 Investigation of the Extracted Relation 
Overlap with a Conventional Method 
We randomly sampled 300 hyponymy rela-
tions that were extracted correctly using the k 
similar words method exploiting CVD and inves-
tigated whether or not these relations can be ex-
tracted by the conventional method based on the 
lexico-syntactic pattern. The possible hyponymy 
relations were extracted using the pattern-based 
method (Ando et al 2003) from the TSUBAKI 
corpus (Shinzato et al 2008). From a comparison 
of these relations, we found only 57 common 
hyponymy relations. That is, the remaining 243 
hyponymy relations were not included in the 
possible hyponymy relations. This result indi-
cates that our method can acquire the hyponymy 
relations that cannot be extracted by the conven-
tional pattern-based method. 
6.4 Discussions 
We investigated the reason for the errors gener-
ated by the method of scoring using k similar 
words exploiting CVD. We conducted experi-
ments on hypernym extraction targeting 694 
words in the development data mentioned in Sec-
tion 6.1. Among these, 286 relations were ex-
tracted incorrectly. In these relations, there were 
some frequent hypernyms. For example, the 
word sakuhin (work) appeared 28 times and hon 
(book) appeared 20 times. As shown in Table 2, 
hon (book) was also extracted for the target word 
meru-seminah (mail seminar). It is really diffi-
cult even for a human to identify whether the 
title is that of the book or the event. If we can 
identify these difficult hypernyms in advance, we 
can improve precision by excluding them from 
the target hypernyms. This will be one of the top-
ics for future study. 
7 Conclusion 
In this paper, we proposed a method for disco-
vering hyponymy relations between nouns by 
fusing the Wikipedia relation database and words 
from the Web. We demonstrated that the method 
using k similar words has high accuracy. The 
experimental results showed the effectiveness of 
using hierarchal structures and the clustering 
process for calculating distributional similarity 
for this task. The experimental results showed 
that our method could achieve 91.0% attachment 
accuracy for the top 10,000 hyponymy relations 
and 74.5% attachment accuracy for the top 
100,000 relations when using the clustering-
based similarity. We confirmed that most rela-
tions extracted by the proposed method could not 
be handled by the lexico-syntactic pattern-based 
method. Future work will be to filter out difficult 
hypernyms for hyponymy extraction process to 
achieve higher precision. 
References 
M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic 
Extraction of Hyponyms from Newspaper Using 
Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-
NL-157, pp. 77?82 (in Japanese). 
F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto. 
2008. Boot-strapping a WordNet Using Multiple 
Existing WordNets. In the 6th International Confe-
rence on Language Resources and Evaluation 
(LREC), Marrakech.  
Bunruigoihyo. 1996. The National Language Re-
search Institute (in Japanese). 
S. A. Caraballo. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text. In 
Proceedings of the Conference of the Association 
for Computational Linguistics (ACL). 
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld and A. Yates. 2005. 
Unsupervised Named-Entity Extraction from the 
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Table2:  Hypernym discovery results by the k-similar 
words based approach (CVD). The underline indi-
cates the hypernyms which are extracted incorrectly.
Score Target word Extracted hypernym 
58.6 INDIVI burando 
(fashion label)
54.3 kureome (Cleome) hana (flower)
34.4 UOKR  gemu (game)
21.7 Okido (Okido) machi (town)
20.5 Sumatofotsu 
(Smart fortwo) 
kuruma  
(car) 
15.6 Fukagawameshi 
(Fukagawa rice)
ryori (dish) 
8.9 John Barry sakkyokuka 
 (composer)
8.5 JVM sofuto-wea 
(software) 
6.6 metangasu 
(methane gas) 
genso 
(chemical element)
5.4 me-ru semina 
(mail seminar) 
Hon (book) 
3.9 gurometto 
(grommet) 
shohin 
(merchandise)
3.1 supuringubakku  
(spring back) 
gensho 
(phenomenon)
936
Database. Cambridge, MA: MIT Press. 
Z. Harris. 1985. Distributional Structure. In Katz, J. J. 
(ed.) The Philosophy of Linguistics, Oxford Uni-
versity Press, pp. 26?47. 
W. L. Hays. 1988. Statistics: Analyzing Qualitative 
Data, Rinehart and Winston, Inc., Ch. 18, pp. 769?
783. 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of 
the 14th Conference on Computational Linguistics 
(COLING), pp. 539?545.  
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-
kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997. 
Goi-Taikei A Japanese Lexicon, Iwanami Shoten. 
J. Kazama and K. Torisawa. 2008. Inducing Gazet-
teers for Named Entity Recognition by Large-scale 
Clustering of Dependency Relations. In Proceed-
ings of ACL-08: HLT, pp. 407?415. 
J. Kazama, Stijn De Saeger, K. Torisawa and M. Mu-
rata. 2009. Generating a Large-scale Analogy List 
Using a Probabilistic Clustering Based on Noun-
Verb Dependency Profiles. In 15th Annual Meeting 
of the Association for Natural Language 
Processing, C1?3 (in Japanese). 
L. Lee. 1999. Measures of Distributional Similarity. 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, pp. 25?
32. 
C. D. Manning and H. Schutze. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press. 
P. Pantel, D. Ravichandran and E. Hovy. 2004a. To-
wards Terascale Knowledge Acquisition. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics. 
P. Pantel and D. Ravichandran. 2004b. Automatically 
Labeling Semantic Classes. In Proceedings of the 
Human Language Technology and North American 
Chapter of the Association for Computational Lin-
guistics Conference. 
S. P. Ponzetto, and M. Strube. 2007. Deriving a Large 
Scale Taxonomy from Wikipedia. In Proceedings 
of the 22nd National Conference on Artificial Intel-
ligence, pp. 1440?1445. 
M. Rooth, S. Riezler, D. Presher, G. Carroll and F. 
Beil. 1999. Inducing a Semantically Annotated 
Lexicon via EM-based Clustering. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics, pp. 104?111. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypo-
nymy Relations from Web Documents. In Proceed-
ings of HLT-NAACL, pp. 73?80. 
K. Shinzato, D. Kawahara, C. Hashimoto and S. Ku-
rohashi. 2008. A Large-Scale Web Data Collection 
as A Natural Language Processing Infrastructure. 
In the 6th International Conference on Language 
Resources and Evaluation (LREC). 
R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Dis-
covery. NIPS 2005. 
R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogenous Evidence. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th annual 
meeting of the Association for Computational Lin-
guistics, pp. 801?808. 
A. Sumida, N. Yoshinaga and K. Torisawa. 2008. 
Boosting Precision and Recall of Hyponymy Rela-
tion Acquisition from Hierarchical Layouts in Wi-
kipedia. In the 6th International Conference on 
Language Resources and Evaluation (LREC). 
A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool 
for Constructing a Synonym Dictionary using con-
text Information. In proceedings of IPSJ SIG Tech-
nical Reports, vol.2006 No.124, pp. 87-94. (In Jap-
anese). 
K. Torisawa. 2001. An Unsupervised Method for Ca-
nonicalization of Japanese Postpositions. In Pro-
ceedings of the 6th Natural Language Processing 
Pacific Rim Symposium (NLPRS), pp. 211?218. 
K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-
ma, M. Murata, D. Noguchi and A. Sumida. 2008. 
TORISHIKI-KAI, An Autogenerated Web Search 
Directory. In Proceedings of the second interna-
tional symposium on universal communication, pp. 
179?186, 2008. 
937
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1172?1181,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Large-Scale Verb Entailment Acquisition from the Web
Chikara Hashimoto? Kentaro Torisawa? Kow Kuroda?
Stijn De Saeger? Masaki Murata? Jun?ichi Kazama?
National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, JAPAN
{
? ch,? torisawa,? kuroda,? stijn,?murata,? kazama}@nict.go.jp
Abstract
Textual entailment recognition plays a
fundamental role in tasks that require in-
depth natural language understanding. In
order to use entailment recognition tech-
nologies for real-world applications, a
large-scale entailment knowledge base is
indispensable. This paper proposes a con-
ditional probability based directional sim-
ilarity measure to acquire verb entailment
pairs on a large scale. We targeted 52,562
verb types that were derived from 108
Japanese Web documents, without regard
for whether they were used in daily life
or only in specific fields. In an evaluation
of the top 20,000 verb entailment pairs ac-
quired by previous methods and ours, we
found that our similarity measure outper-
formed the previous ones. Our method
also worked well for the top 100,000 re-
sults.
1 Introduction
We all know that if you snored, you must have
been sleeping, that if you are divorced, you must
have been married, and that if you won a lawsuit,
you must have sued somebody. These relation-
ships between events where one is the logical con-
sequence of the other are called entailment. Such
knowledge plays a fundamental role in tasks that
require in-depth natural language understanding,
e.g., answering questions and using natural lan-
guage interfaces.
This paper proposes a novel method for verb
entailment acquisition. Using a Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents, we automat-
ically acquired such verb pairs as snore ? sleep
and divorce ? marry, where entailment holds be-
tween the verbs in the pair.1 Our definition of ?en-
tailment? is the same as that in WordNet3.0; v
1
entails v
2
if v
1
cannot be done unless v
2
is, or has
been, done.2
Our method follows the distributional similar-
ity hypothesis, i.e., words that occur in the same
context tend to have similar meanings. Just as in
the methods of Lin and Pantel (2001) and Szpek-
tor and Dagan (2008), we regard the arguments
of verbs as the context in the hypothesis. How-
ever, unlike the previous methods, ours is based
on conditional probability and is augmented with
a simple trick that improves the accuracy of verb
entailment acquisition. In an evaluation of the top
20,000 verb entailment pairs acquired by the pre-
vious methods and ours, we found that our similar-
ity measure outperformed the previous ones. Our
method also worked well for the top 100,000 re-
sults,
Since the scope of Natural Language Process-
ing (NLP) has advanced from a formal writing
style to a colloquial style and from restricted to
open domains, it is necessary for the language re-
sources for NLP, including verb entailment knowl-
edge bases, to cover a broad range of expressions,
regardless of whether they are used in daily life
or only in specific fields that are highly techni-
cal. As we will discuss later, our method can ac-
quire, with reasonable accuracy, verb entailment
pairs that deal not only with common and familiar
verbs but also with technical and unfamiliar ones
like podcast ? download and jibe ? sail.
Note that previous researches on entailment ac-
quisition focused on templates with variables or
word-lattices (Lin and Pantel, 2001; Szpektor and
Dagan, 2008; Barzilay and Lee, 2003; Shinyama
1Verb entailment pairs are described as v
1
? v
2
(v
1
is
the entailing verb and v
2
is the entailed one) henceforth.
2WordNet3.0 provides entailment relationships between
synsets like divorce, split up?marry, get married, wed, con-
join, hook up with, get hitched with, espouse.
1172
et al, 2002). Certainly these templates or word
lattices are more useful in such NLP applications
as Q&A than simple entailment relations between
verbs. However, our contention is that entailment
certainly holds for some verb pairs (like snore ?
sleep) by themselves, and that such pairs consti-
tute the core of a future entailment rule database.
Although we focused on verb entailment, our
method can also acquire template-level entailment
pairs with a reasonable accuracy.
The rest of this paper is organized as follows.
In ?2, related works are described. ?3 presents our
proposed method. After this, an evaluation of our
method and the existing methods is presented in
Section 4. Finally, we conclude the paper in ?5.
2 Related Work
Previous studies on entailment, inference rules,
and paraphrase acquisition are roughly classi-
fied into those that require comparable corpora
(Shinyama et al, 2002; Barzilay and Lee, 2003;
Ibrahim et al, 2003) and those that do not (Lin
and Pantel, 2001; Weeds and Weir, 2003; Geffet
and Dagan, 2005; Pekar, 2006; Bhagat et al, 2007;
Szpektor and Dagan, 2008).
Shinyama et al (2002) regarded newspaper arti-
cles that describe the same event as a pool of para-
phrases, and acquired them by exploiting named
entity recognition. They assumed that named en-
tities are preserved across paraphrases, and that
text fragments in the articles that share several
comparable named entities should be paraphrases.
Barzilay and Lee (2003) also used newspaper ar-
ticles on the same event as comparable corpora
to acquire paraphrases. They induced paraphras-
ing patterns by sentence clustering. Ibrahim et al
(2003) relied on multiple English translations of
foreign novels and sentence alignment to acquire
paraphrases. We decided not to take this approach
since using comparable corpora limits the scale
of the acquired paraphrases or entailment knowl-
edge bases. Although obtaining comparable cor-
pora has been simplified by the recent explosion
of the Web, the availability of plain texts is incom-
parably better.
Entailment acquisition methods that do not re-
quire comparable corpora are mostly based on the
distributional similarity hypothesis and use plain
texts with a syntactic parser. Basically, they parse
texts to obtain pairs of predicate phrases and their
arguments, which are regarded as features of the
predicates with appropriately assigned weights.
Lin and Pantel (2001) proposed a paraphrase ac-
quisition method (non-directional similarity mea-
sure) called DIRT which acquires pairs of binary-
templates (predicate phrases with two argument
slots) that are paraphrases of each other. DIRT em-
ploys the following similarity measure proposed
by Lin (1998):
Lin(l, r) =
?
f?F
l
?F
r
[w
l
(f) + w
r
(f)]
?
f?F
l
w
l
(f) +
?
f?F
r
w
r
(f)
where l and r are the corresponding slots of two
binary templates, F
s
is s?s feature vector (argu-
ment nouns), and w
s
(f) is the weight of f ? F
s
(PMI between s and f ). The intuition behind this
is that the more nouns two templates share, the
more semantically similar they are. Since we ac-
quire verb entailment pairs based on unary tem-
plates (Szpektor and Dagan, 2008) we used the
Lin formula to acquire unary templates directly
rather than using the DIRT formula, which is the
arithmetic-geometric mean of Lin?s similarities for
two slots in a binary template.
Bhagat et al (2007) developed an algorithm
called LEDIR for learning the directionality of
non-directional inference rules like those pro-
duced by DIRT. LEDIR implements a Direction-
ality Hypothesis: when two binary semantic re-
lations tend to occur in similar contexts and the
first one occurs in significantly more contexts than
the second, then the second most likely implies the
first and not vice versa.
Weeds and Weir (2003) proposed a general
framework for distributional similarity that mainly
consists of the notions of what they call Precision
(defined below) and Recall:
Precision(l, r) =
?
f?F
l
?F
r
w
l
(f)
?
f?F
l
w
l
(f)
where l and r are the targets of a similarity mea-
surement, F
s
is s?s feature vector, and w
s
(f) is the
weight of f ? F
s
. The best performing weight is
PMI. Precision is a directional similarity measure
that examines the coverage of l?s features by those
of r?s, with more coverage indicating more simi-
larity.
Szpektor and Dagan (2008) proposed a direc-
tional similarity measure called BInc (Balanced-
Inclusion) that consists of Lin and Precision, as
BInc(l, r) =
?
Lin(l, r) ? Precision(l, r)
1173
where l and r are the target templates. For weight-
ing features, they used PMI. Szpektor and Dagan
(2008) also proposed a unary template, which is
defined as a template consisting of one argument
slot and one predicate phrase. For example, X take
a nap ? X sleep is an entailment pair consisting
of two unary templates. Note that the slot X must
be shared between templates. Though most of the
previous entailment acquisition studies focused on
binary templates, unary templates have an obvi-
ous advantage over binary ones; they can handle
intransitive predicate phrases and those that have
omitted arguments. The Japanese language, which
we deal with here, often omits arguments, and thus
the advantage of unary templates is obvious.
As shown in ?4, our method outperforms Lin,
Precision, and BInc in accuracy.
Szpector et al (2004) addressed broad coverage
entailment acquisition. But their method requires
an existing lexicon to start, while ours does not.
Apart from the dichotomy of the compara-
ble corpora and the distributional similarity ap-
proaches, Torisawa (2006) exploited the structure
of Japanese coordinated sentences to acquire verb
entailment pairs. Pekar (2006) used the local
structure of coherent text by identifying related
clauses within a local discourse. Zanzotto et al
(2006) exploited agentive nouns. For example,
they acquired win ? play from ?the player wins.?
Geffet and Dagan (2005) proposed the Distribu-
tional Inclusion Hypotheses, which claimed that if
a word v entails another word w, then all the char-
acteristic features of v are expected to appear with
w, and vice versa. They applied this to noun en-
tailment pair acquisition, rather than verb pairs.
3 Proposed Method
This section presents our method of verb entail-
ment acquisition. First, the basics of Japanese are
described. Then, we present the directional sim-
ilarity measure that we developed in ?3.2. ?3.3
describes the structure and acquisition of the web-
based data from which entailment pairs are de-
rived. Finally, we show how we acquire verb en-
tailment pairs using our proposed similarity mea-
sure and the web-based data in ?3.4.
3.1 Basics of Japanese
Japanese explicitly marks arguments including the
subject and object by postpositions, and is a head-
final language. Thus, a verb phrase consisting of
an object hon (book) and a verb yomu (read), for
example, is expressed as hon-wo yomu (book-ACC
read) ?read a book? with the accusative postpo-
sition wo marking the object.3 Accordingly, we
refer to a unary template as ?p, v? hereafter, with
p and v referring to the postposition and a verb.
Also, we abbreviate a template-level entailment
?p
l
, v
l
? ? ?p
r
, v
r
? as l ? r for simplicity. We
define a unary template as a template consisting
of one argument slot and one predicate, following
Szpektor and Dagan (2008).
3.2 Directional Similarity Measure based on
Conditional Probability
The directional similarity measure that we devel-
oped and called Score is defined as follows:
Score(l, r) = Score
base
(l, r) ? Score
trick
(l, r)
where l and r are unary templates, and Score in-
dicates the probability of l ? r. Score
base
, which
is the base of Score, is defined as follows:
Score
base
(l, r) =
?
f?F
l
?F
r
P (r|f)P (f |l)
where F
s
is s?s feature vector (nouns including
compounds). The intention behind the definition
of Score
base
is to emulate the conditional proba-
bility P (v
r
|v
l
)
4 in a distributional similarity style
function. Note that P (v
r
|v
l
) should be 1 when en-
tailment v
l
? v
r
holds (i.e., v
r
is observed when-
ever v
l
is observed) and we have reliable proba-
bility values. Then, if we can directly estimate
P (v
r
|v
l
), it is reasonable to assume v
l
? v
r
if
P (v
r
|v
l
) is large enough. However, we cannot es-
timate P (v
r
|v
l
) directly since it is unlikely that we
will observe the verbs v
r
and v
l
at the same time.
(People do not usually repeat v
r
and v
l
in the same
document to avoid redundancy.) Thus, instead of
a direct estimation, we substitute Score
base
(l, r)
as defined above. In other words, we assume
P (v
r
|v
l
) ? P (r|l) ? ?
f?F
l
?F
r
P (f |l)P (r|f).
Actually, Score
base
originally had another mo-
tivation, inspired by Torisawa (2005), for which no
postposition but the instrumental postposition de
was relevant. In this discussion, all of the nouns
(fs) that are marked by the instrumental postposi-
tion are seen as ?tools,? and P (f |l) is interpreted
3ACC represents an accusative postposition in Japanese.
Likewise, NOM, DAT, INS, and TOP are the symbols for the
nominative, dative, instrumental, and topic postpositions.
4Remember that v
l
and v
r
are the verbs of unary tem-
plates l and r.
1174
as a measure of how typically the tool f is used
to perform the action denoted by (the v
l
of) l; if
P (f |l) is large enough, f is a typical tool used in
l. On the other hand, P (r|f) indicates the proba-
bility of (the v
r
of) r being the purpose for using
the tool f . See (1) for an example.
(1) konro-de chouri-suru
cooking.stove-INS cook
?cook (something) using a cooking stove.?
The purpose of using a cooking stove is to cook.
Torisawa (2005) has pointed out that when r ex-
presses the purpose of using a tool f , P (r|f) tends
to be large. This predicts that P (r|cooking stove)
is large, where r is ?de, cook?.
According to this observation, if f is a single
purpose tool and P (f |l), the probability of f be-
ing the tool by which l is performed, and P (r|f),
the probability of r being the purpose of using the
tool f , are large enough, then the typical perfor-
mance of the action v
l
should contain some ac-
tions that can be described by v
r
, i.e., the pur-
pose of using f . Moreover, if all the typical tools
(fs) used in v
l
are also used for v
r
, most perfor-
mances of the action v
l
should contain a part de-
scribed by the action v
r
. In summary, this means
that when ?
f?F
l
?F
r
P (r|f)P (f |l), Score
base
, has
a large value, we can expect v
l
? v
r
.
For example, let v
l
be deep-fry and v
r
be cook.
Note that v
l
? v
r
holds for this example. There
are many tools that are used for deep-frying,
such as cooking stove, pot, or pan. This means
that P (cooking stove|l), P (pot|l), or P (pan|l) are
large. On the other hand, the purpose of using all
of these tools is cooking, based on common sense.
Thus, probabilities such as P (r|cooking stove)
and P (r|pan) should have large values. Accord-
ingly, ?
f?F
l
?F
r
P (f |l)P (r|f), Score
base
, should
be relatively large for deep-fry ? cook,
Actually, we defined Score
base
based on the
above assumption However, through a series of
preliminary experiments, we found that the same
score could be applied without losing the preci-
sion to the other postpositions. Thus, we gener-
alized the framework so that it could deal with
most postpositions, namely ga (NOM), wo (ACC),
ni (DAT), de (INS), and wa (TOP). Note that this
is a variation of the distributional inclusion hy-
pothesis (Geffet and Dagan, 2005), but that we do
not use mutual information as in previous works,
based on the hypothesis discussed above. Actu-
ally, as shown in ?4, our conditional probability
based method outperformed the mutual informa-
tion based metrics in our experiments.
On the other hand, Score
trick
implements an-
other assumption that if only one feature con-
tributes to Score
base
and the contribution of the
other nouns is negligible, if any, the similarity is
unreliable. Accordingly, for Score
trick
, we uni-
formly ignore the contribution of the most domi-
nant feature from the similarity measurement.
Score
trick
(l, r)
= Score
base
(l, r) ? max
f?F
l
?F
r
P (r|f)P (f |l)
As shown in ?4, this trick actually improved the
entailment acquisition accuracy.
We used maximum likelihood estimation to ob-
tain P (r|f) and P (f |l) in the above discussion.
Bannard and Callison-Burch (2005) and Fujita
and Sato (2008) also proposed directional simi-
larity measures based on conditional probability,
which are very similar to Score
base
, although ei-
ther their method?s prerequisites or the targets of
the similarity measurements were different from
ours. The method of Bannard and Callison-Burch
(2005) requires bilingual parallel corpora, and
uses the translations of expressions as its feature.
Fujita and Sato (2008) dealt with productive pred-
icate phrases, while our target is non-productive
lexical units, i.e., verbs. Thus, this is the first
attempt to apply a conditional probability based
similarity measure to verb entailment acquisition.
In addition, the trick implemented in Score
trick
is
novel.
3.3 Preparing Template-Feature Tuples
Our method starts from a dataset called template-
feature tuples, which was derived from the Web
in the following way: 1) Parse the Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents with Japanese
dependency parser KNP (Kawahara and Kuro-
hashi, 2006b). 2) Extract triples ?n, p, v? consist-
ing of nouns (n), postpositions (p), and verbs (v),
where an n marked by a p depends on a v from
the parsed Web text. 3) From the triple database,
construct template-feature tuples ?n, ?p, v?? by re-
garding ?p, v? as a unary template and n as one of
its features. 4) Convert the verbs into their canon-
ical forms as defined by KNP. 5) Filter out tuples
that fall into one of the following categories: 5-
1) Freq(?p, v?) < 20. 5-2) Its verb is passivized,
1175
causativized, or negated. 5-3) Its verb is semanti-
cally vague like be, do, or become. 5-4) Its post-
position is something other than ga (NOM), wo
(ACC), ni (DAT), de (INS), or wa (TOP).
The resulting unary template-feature tuples in-
cluded 127,808 kinds of templates that consisted
of 52,562 verb types and five kinds of postpo-
sitions. The verbs included compound words
like bosi-kansen-suru (mother.to.child-infection-
do) ?infect from mothers to infants.?
3.4 Acquiring Entailment Pairs
We acquired verb entailment pairs using the fol-
lowing procedure: i) From the template-feature
tuples mentioned in ?3.3, acquire unary template
pairs that exhibit an entailment relation between
them using the directional similarity measure in
?3.2. ii) Convert the acquired unary templates
?p, v? into naked verbs v by stripping the postpo-
sitions p. iii) Remove the duplicated verb pairs
resulting from stripping ps. To be precise, when
we removed the duplicated pairs, we left the high-
est ranked one. iv) Retrieve N-best verb pairs as
the final output from the result of iii). That is, we
first acquired unary template pairs and then trans-
formed them into verb pairs.
Although this paper focuses on verb entailment
acquisition, we also evaluated the accuracy of
template-level entailment acquisition, in order to
show that our similarity measure works well, not
only for verb entailment acquisition, but also for
template entailment acquisition (See ?4.4). we
created two kinds of unary templates: the ?Scoring
Slots? template and the ?Nom(inative) Slots? tem-
plate. The first is simply the result of the procedure
i); all of the templates have slots that are used for
similarity scoring. The second one was obtained
in the following way: 1) Only templates whose p
is not a nominative are sampled from the result of
the procedure i). 2) Their ps are all changed to a
nominative. Templates of the second kind are used
to show that the corresponding slots between tem-
plates (nominative, in this case) that are not used
for similarity scoring can be incorporated to re-
sulting template-level entailment pairs if the scor-
ing function really captures the semantic similarity
between templates.
Note that, for unary template entailment pairs
like (2) to be well-formed, the two unary slots (X-
wo) between templates must share the same noun
as the index i indicates. This is relevant in ?4.4.
(2) X
i
-wo musaborikuu ? X
i
-wo taberu
X
i
-ACC gobble X
i
-ACC eat
4 Evaluation
We compare the accuracy of our method with that
of the alternative methods in ?4.1. ?4.2 shows
the effectiveness of the trick. We examine the en-
tailment acquisition accuracy for frequent verbs in
?4.3, and evaluate the performance of our method
when applied to template-level entailment acquisi-
tion in ?4.4. Finally, by showing the accuracy for
verb pairs obtained from the top 100,000 results,
we claim that our method provides a good start-
ing point from which a large-scale verb entailment
resource can be constructed in ?4.5.
For the evaluation, three human annotators (not
the authors) checked whether each acquired entail-
ment pair was correct. The average of the three
Kappa values for each annotator pair was 0.579
for verb entailment pairs and 0.568 for template
entailment pairs, both of which indicate the mid-
dling stability of this evaluation annotation.
4.1 Experiment 1: Verb Pairs
We applied Score, BInc, Lin, and Precision to the
template-feature tuples (?3.3), obtained template
entailment pairs, and finally obtained verb entail-
ment pairs by removing the postpositions from the
templates as described in ?3. As a baseline, we
created pairs from randomly chosen verbs.
Since we targeted all of the verbs that ap-
peared on the Web (under the condition of
Freq(?p, v?) ? 20), the annotators were con-
fronted with technical terms and slang that they
did not know. In such cases, they consulted dic-
tionaries (either printed or machine readable ones)
and the Web. If they still could not find the mean-
ing of a verb, they labeled the pair containing the
unknown verb as incorrect.
We used the accuracy = # of correct pairs# of acquired pairs as
an evaluation measure. We regarded a pair as cor-
rect if it was judged correct by one (Accuracy-1),
two (Accuracy-2), or three (Accuracy-3) annota-
tors.
We evaluated 200 entailment pairs sampled
from the top 20,000 for each method (# of ac-
quired pairs = 200). For fairness, the evaluation
samples for each method were shuffled and placed
in one file from which the annotators worked. In
this way, they were unable to know which entail-
ment pair came from which method.
1176
Note that the verb entailment pairs produced
by Lin do not provide the directionality of en-
tailment. Thus, the annotators decided the direc-
tionality of these entailment pairs as follows: i)
Copy 200 original samples and reverse the order
of v
1
and v
2
. ii) Shuffle the 400 Lin samples
(the original and reversed samples) with the other
ones. iii) Evaluate all of the shuffled pairs. Each
Lin pair was regarded as correct if either direction
was judged correct. In other words, we evaluated
the upper bound performance of the LEDIR algo-
rithm.
Table 1 shows the accuracy of the acquired
verb entailment pairs for each method. Figure 1
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
BInc 0.450 0.255 0.125
Precision 0.725 0.545 0.385
Lin 0.590 0.370 0.160
Random 0.050 0.010 0.005
Table 1: Accuracy of verb entailment pairs.
shows the accuracy figures for the N-best entail-
ment pairs for each method, with N being 1,000,
2,000, . . ., or 20,000. We observed the following
points from the results. First, Score outperformed
all the other methods. Second, Score and Pre-
cision, which are directional similarity measures,
worked well, while Lin, which is a symmetric one,
performed poorly even though the directionality of
its output was determined manually.
Looking at the evaluated samples, Score suc-
cessfully acquired pairs in which the entailed
verbs generalized entailing verbs that were techni-
cal terms. (3) shows examples of Score?s outputs.
(3) a. RSS-haisin-suru ? todokeru
RSS-feed-do deliver
?feed the RSS data?
b. middosippu-maunto-suru ? tumu
midship-mounting-do mount
?have (engine) midship-mounted?
The errors made by DIRT (4) and BInc (5) in-
cluded pairs consisting of technical terms.
(4) kurakkingu-suru
software.cracking-do
?crack a (security) system?
? koutiku-hosyu-suru
building-maintenance-do
?build and maintain a system?
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Figure 1: Accuracy of verb entailment pairs.
(5) suisou-siiku-suru
tank-raising-do
?raise (fish) in a tank?
? siken-houryuu-suru
test-discharge-do
?stock (with fish) experimentally?
These terms are related in some sense, but they
are not entailment pairs.
4.2 Experiment 2: Effectiveness of the Trick
Next, we investigated the effectiveness of the trick
described in ?3. We evaluated Score, Score
trick
,
and Score
base
. Table 2 shows the accuracy figures
for each method. Figure 2 shows the accuracy fig-
ures for the N-best outputs for each method. The
1177
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
Score
trick
0.725 0.610 0.395
Score
base
0.590 0.465 0.315
Table 2: Effectiveness of the trick.
results illustrate that introducing the trick signif-
icantly improved the performance of Score
base
,
and so did multiplying Score
trick
and Score
base
,
which is our proposal Score.
(6) shows an example of Score
base
?s errors.
(6) gazou-sakusei-suru ? henkou-suru
image-making-do change-do
?make an image? ?change?
This pair has only two shared nouns (f ? F
l
?F
r
),
and more than 99.99% of the pair?s similarity re-
flects only one of the two. Clearly, the trick would
have prevented the pair from being highly ranked.
4.3 Experiment 3: Pairs of Frequent Verbs
We found that the errors made by Lin and BInc
in Experiment 1 were mostly pairs of infrequent
verbs such as technical terms. Thus, we con-
ducted the acquisition of entailment pairs targeting
more frequent verbs to see how their performance
changed. The experimental conditions were the
same as in Experiment 1, except that the templates
(?p, v?) used were all Freq(?p, v?) ? 200.
Table 3 shows the accuracy figures for each
method with the changes in accuracy from those
of the original methods in parentheses. The re-
Method Acc-1 Acc-2 Acc-3
Score
0.690 0.520 0.335
(?0.080) (?0.140) (?0.125)
BInc 0.455 0.295 0.160(+0.005) (+0.040) (+0.035)
Precision 0.450 0.355 0.205(?0.275) (?0.190) (?0.180)
Lin 0.635 0.385 0.205(+0.045) (+0.015) (+0.045)
Table 3: Accuracy of frequent verb pairs.
sults show that the accuracies of Score and Pre-
cision (the two best methods in Experiment 1) de-
graded, while the other two improved a little. We
suspect that the performance difference between
these methods would get smaller if we further re-
stricted the target verbs to more frequent ones.
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Figure 2: Accuracy of verb entailment pairs ac-
quired by Score, Score
trick
, and Score
base
.
However, we believe that dealing with verbs com-
prehensively, including infrequent ones, is impor-
tant, since, in the era of information explosion, the
impact on applications is determined not only by
frequent verbs but also infrequent ones that consti-
tute the long tail of a verb-frequency graph. Thus,
this tendency does not matter for our purpose.
4.4 Experiment 4: Template Pairs
This section presents the entailment acquisition
accuracy for template pairs to show that our
method can also perform the entailment acqui-
sition of unary templates. We presented pairs
of unary templates, obtained by the procedure in
1178
?3.4, to the annotators. In doing so, we restricted
the correct entailment pairs to those for which en-
tailment always held regardless of what argument
filled the two unary slots, and the two slots had to
be filled with the same argument, as exemplified
in (2). We evaluated Score and Precision.
Table 4 shows the accuracy of the acquired pairs
of unary templates. Compared to verb entailment
Method Acc-1 Acc-2 Acc-3
Score
0.655 0.510 0.300
Scoring (?0.115) (?0.150) (?0.160)
Slots Precision 0.565 0.430 0.265(?0.160) (?0.115) (?0.120)
Score
0.665 0.515 0.315
Nom (?0.105) (?0.145) (?0.145)
Slots Precision 0.490 0.325 0.215(?0.235) (?0.220) (?0.170)
Table 4: Accuracy of entailment pairs of templates
whose slots were used for scoring.
acquisition, the accuracy of both methods dropped
by about 10%. This was mainly due to the evalua-
tion restriction exemplified in (2) which was not
introduced in the previous experiments; the an-
notators ignored the argument correspondence be-
tween the verb pairs in Experiment 1. Also note
that Score outperformed Precision in this experi-
ment, too.
(7) and (8) are examples of the Scoring Slots
template entailment pairs and (9) is that of the
Nom Slots acquired by our method.
(7) X-wo tatigui-suru ? X-wo taberu
X-ACC standing.up.eating-do X-ACC eat
?eat X standing up? ?eat X?
(8) X-de marineedo-suru ? X-wo ireru
X-INS marinade-do X-ACC pour
?marinate with X? ?pour X?
(9) X-ga NBA-iri-suru ? ? ? (was X-de (INS))
X-NOM NBA-entering-do
?X joins an NBA team?
? X-ga nyuudan-suru ? ? ? (was X-de)
X-NOM enrollment-do
?X joins a team?
4.5 Experiment 5: Verb Pairs form the Top
100,000
Finally, we examined the accuracy of the top
100,000 verb pairs acquired by Score and Preci-
sion. As Table 5 shows, Score outperformed Pre-
Method Acc-1 Acc-2 Acc-3
Score 0.610 0.480 0.300
Precision 0.470 0.295 0.190
Table 5: Accuracy of the top 100,000 verb pairs.
cision. Note also that Score kept a reasonable ac-
curacy for the top 100,000 results (Acc-2: 48%).
The accuracy is encouraging enough to consider
human annotation for the top 100,000 results to
produce a language resource for verb entailment,
which we actually plan to do.
Below are correct verb entailment examples
from the top 100,000 results of our method.
(10) The 121th pair
kaado-kessai-suru ? siharau
card-payment-do pay
?pay by card? ?pay?
(11) The 6,081th pair
saitei-suru ? sadameru
adjudicate-do settle
?adjudicate? ?settle?
(12) The 15,464th pair
eraa-syuuryou-suru ? jikkou-suru
error-termination-do perform-do
?abend? ?execute?
(13) The 30,044th pair
ribuuto-suru ? kidou-suru
reboot-do start-do
?reboot? ?boot?
(14) The 57,653th pair
rinin-suru ? syuunin-suru
resignation-do accession-do
?resign? ?accede?
(15) The 70,103th pair
sijou-tounyuu-suru ? happyou-suru
market-input-do publication-do
?bring to the market? ?publicize?
Below are examples of erroneous pairs from our
results. (16) is a causal relation but not an entail-
ment. (17) is a contradictory pair.
(16) The 5,475th pair
juken-suru ? goukaku-suru
take.an.exam-do acceptance-do
?take an exam? ?gain admission?
1179
(17) The 40,504th pair
ketujou-suru ? syutujou-suru
not.take.part-do take.part-do
?not take part? ?take part?
5 Conclusion
This paper addressed verb entailment acquisition
from the Web, and proposed a novel directional
similarity measure Score. Through a series of ex-
periments, we showed i) that Score outperforms
the previously proposed measures, Lin, Precision,
and BInc in large scale verb entailment acquisi-
tion, ii) that our proposed trick implemented in
Score
trick
significantly improves the accuracy of
verb entailment acquisition despite its simplicity,
iii) that Score worked better than the others even
when we restricted the target verbs to more fre-
quent ones, iv) that our method is also moder-
ately successful at producing template-level en-
tailment pairs, and v) that our method maintained
reasonable accuracy (in terms of human annota-
tion) for the top 100,000 results. As examples of
the acquired verb entailment pairs illustrated, our
method can acquire from an ocean of information,
namely the Web, a variety of verb entailment pairs
ranging from those that are used in daily life to
those that are used in very specific fields.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL2005),
pages 597?604.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL 2003, pages 16?23.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP2007), pages 161?170.
Atsushi Fujita and Satoshi Sato. 2008. A probabilis-
tic model for measuring grammaticality and similar-
ity of automatically generated paraphrases of pred-
icate phrases. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING2008), pages 225?232.
Maayan Geffet and Ido Dagan. 2005. The dis-
tributional inclusion hypotheses and lexical entail-
ment. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL2005), pages 107?114.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing (IWP2003), pages
57?64.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case Frame Compilation from the Web using High-
Performance Computing. In Proceedings of The 5th
International Conference on Language Resources
and Evaluation (LREC-06), pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
Fully-Lexicalized Probabilistic Model for Japanese
Syntactic and Case Structure Analysis. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL2006), pages 176?183.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (COLING-ACL1998), pages
768?774.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics (HLT-NAACL2006),
pages 49?56.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the 2nd international
Conference on Human Language Technology Re-
search (HLT2002), pages 313?318.
Idan Szpector, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2004), pages 41?48.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary template. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING2008), pages 849?856.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556?560.
1180
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese cood-
inated sentences and noun-verb co-occurences. In
Proceedings of the Human Language Technology
Conference of the Norh American Chapter of the
ACL (HLT-NAACL2006), pages 57?64.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP2003), pages 81?88.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics and 21th InternationalConference
on Computational Linguistics (COLING-ACL2006),
pages 849?856.
1181
Obtaining Japanese Lexical Units for Semantic Frames
from Berkeley FrameNet Using a Bilingual Corpus
Toshiyuki Kanamaru
Kyoto University
Yoshida Nihonmatsu-cho, Sakyo-ku
Kyoto, 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Masaki Murata Kow Kuroda Hitoshi Isahara
National Institute of Information and
Communications Technology (NICT)
3-5 Hikaridai, Seikacho, Sorakugun
Kyoto, 619-0289, Japan
{murata,kuroda,isahara}@nict.go.jp
Abstract
An attempt was made to semi-automatically ob-
tain ?lexical units? (LUs) for Japanese from
the English LUs defined in the semantic frame
database provided by Berkeley FrameNet (BFN)
using an English-Japanese bilingual corpus.
This task was a prerequisite to building a com-
plete database of semantic frames for Japanese.
In the task, a Japanese word is first translated
into an English word or phrase, E. E is one
of the lexical units that evoked a particular se-
mantic frame, F , in the BFN database. When
other lexical units of F are translated back into
Japanese, this defines a candidate set of F for
the lexical units of F in Japanese. The via-
bility of the proposed method was tested on a
Japanese verb (X-ga Y -wo) osou (roughly mean-
ing ?X attack(s) Y ,? ?X hit(s) Y ,? ?X surprise(s)
Y ? in English, showing that it is a relatively pol-
ysemous word). The resulting translation was
compared to semantic descriptions provided by
IPAL and Nihongo Goi-Taikei (A Japanese Lex-
icon), two well-known language resources for
Japanese, and also by the Frame Oriented Con-
cept Analysis of Language (FOCAL). The com-
parison revealed that FOCAL, BFN, Goi Taikei,
and IPAL provided finer-grained descriptions in
this specific order.
1 Introduction
Making use of deep semantics in information pro-
cessing is one of the major problems confronting
today?s NLP community. More and more NLP
researchers are realizing that they need seman-
tic/lexical resources that go beyond such ones as
WordNet (Fellbaum, 1998) that only specify hier-
archical semantic relationships. One of the cru-
cial reasons for this is that raw linguistic data
embodies semantic associations that are difficult
to capture in terms of such hierarchical relation-
ships, one of which is the so-called ?semantic
field? effect, a class of associative relationships
among words (or concepts). To deal with these
issues, deeper semantics are needed with descrip-
tions that incorporate ontological inferences. Let
us assume that X attacked Y is to be interpreted.1
This is a complex situation. In interpreting The
man attacked a bank, it may be necessary to spec-
ify (by inference) that the subject used a weapon
(e.g., a gun) and his purpose was to obtain money
(illegally), whereas in interpreting The wolf at-
tacked a flock of sheep, it may be necessary to
specify that the subject never used a weapon and
its purpose was to eat one or two individual sheep
(rather than the entire flock) after killing them.
Relevant inferences are clearly situation-based, or
?case-based? in the sense of Case-based Reason-
ing (Kolodner, 1993), and difficult to specify in
terms of the lexical semantic descriptions avail-
able in resources such as WordNet (Fellbaum,
1998) which don?t specify associative relation-
ships among concepts, including the relationships
between ROBBER (e.g., a man) and WAREHOUSE
OF VALUABLES (e.g., a bank, museum, jewelry
shop), and the one between a PREDATOR (e.g., a
wolf) and its PREY (e.g., sheep, rabbit). Thus, the
NLP community has a critical need for resources
that encode this kind of information.
Along with PropBank (Kingsbury and Palmer,
2002; Ellsworth et al, 2004), Berkeley FrameNet
1One of the anonymous reviewers told us that it was un-
clear how ontological inferences of this sort are related to
BFN?s frame definitions. The question boils down to the
question of definition, i.e., what kind of information we need
to define semantic frames to encode, and as we will see later,
this is exactly the question addressed by FOCAL claiming
that BFN frames are too coarse-grained to be used as an ef-
fective knowledge-base for ontological inferences.
11
(BFN) (Baker et al, 1998) is an ongoing research
project that is attempting to meet the demand for
resources that encode deeper lexical semantics by
providing a semantic frame lexicon (sometimes
called the ?FrameNet?) and a corpus annotated
for semantic information encoded in terms of se-
mantic frames.
Thus far, BFN has produced ?a lexical database
that currently contains more than 8,900 lexical
units, more than 6,100 of which are fully anno-
tated, in more than 625 semantic frames, exem-
plified in more than 135,000 annotated sentences?
(cited from the FrameNet web page). Other
ongoing projects, i.e., the German FrameNet
or ?SALSA? (Erk et al, 2003), the Spanish
FrameNet (Subirats and Petruck, 2003), and the
Japanese FrameNet (Ohara et al, 2003), are try-
ing to build lexical resources that are compatible
with the BFN, but for Japanese at least, no data
has been released in a usable form, except for a
few annotation examples for verbs of motion.
In sum, no useful resource exists for frame-
based description/analysis of Japanese. This is
one of the reasons that we attempted the task in
this paper, along with our efforts to assess the use-
fulness of the database provided by BFN.
The anonymous reviewers of our paper pointed
out that there have been some similar projects
and other methodologies that have tried to trans-
late BFN into other languages automatically, such
as BiFrameNet (Chen and Fung, 2004) and Ro-
mance FrameNet2, and that it would have been
better to include the comparison against them.
BiFrameNet presented an automatic approach
to constructing a bilingual semantic network us-
ing the Chinese HowNet, which is a Chinese
ontology. While it is an interesting approach,
we have not compared their results with ours,
mainly because they seem to have used differ-
ent resources and had somewhat different goals,
along with the space consideration.
No papers are released, let alne being avail-
able to us, related to the Romance FrameNet
project for the time being. We couldn?t help
putting a comparison with it on hold.3
2http://ic2.epfl.ch/?pallotta/rfn/
3One of the anonymous reviewers criticized us for failing
to mention Romance FrameNet project in our paper; it is just
unreasonable. The project was announced on June 1 on the
2 Proposed Procedure
We used a bilingual corpus (Utiyama and Isahara,
2003) to examine which semantic frames of BFN
contained LUs relevant to the Japanese verb osou.
JFN, for example, used a mono-lingual corpus to
construct the semantic frames. In cases like this,
the construction might be inefficient because they
have to construct all semantic frames by them-
selves. But this affects on the reliability of the
frames identified and described. This risk of arbi-
trary description can be reduced by using a bilin-
gual corpus, if it is of high-quality.
2.1 Identifying English equivalents of ?osou?
We chose Japanese-English alignments from the
bilingual corpus in which the Japanese text con-
tained osou, i.e., the target verb. We obtained 135
alignments from the corpus.
The bilingual corpus is consists of two subcor-
pra. One subcorpus is made of one-to-one align-
ments. Another is of one-to-many alignments. In
the latter, one Japanese sentence is aligned with
several English sentences.
In the first case, it was straightforward to spec-
ify an English word or phrase that translated the
target verb, osou. In the second case, however, it
is not. So, we singled out an English sentence that
corresponds to a Japanese sentence that contained
osou. In this process, the identification of osou?s
English translations was done manually.
After this procedure, the following five verbs
were identified as English translations of osou:
assault, attack, hit, pound, and strike4.
2.2 Identifying relevant semantic frames
Based on these five verbs, we extracted seman-
tic frames using FrameSQL (Sato, 2003). Seman-
tic frames with LUs that included any of the five
verbs were chosen from the BFN semantic frame
database (referred to here as BFN).
Corpora Mailing List, just one week before the submission
deadline. This means that we had little chance to know about
the project unless we were ?insiders.?
4There were a few other verbs or constructions that
served as English translations of osou in the alignments: for
example, besiege, engulf, feel pain, occur, hurt, kill, rob,
shoot, stab, suffer, wreak on were used as its translations.
But we filtered out those less frequent items (whose fre-
quency is less than 3) for purposes of simplicity.
12
Based on Frame Semantics (Fillmore, 1982),
BFN posits that a semantic frame is an organi-
zation of ?semantic roles,? which BFN terms as
?Frame Elements? (FEs). Usually, LUs are in-
stantiations or lexical realizations of FEs. Thus,
an LU in a frame, F , is a word, or phrase, that, ac-
cording to the assumptions of Frame Semantics,
?evokes? frame F . The definition of the ?Attack?
frame in the BFN database is used in Figure 1to
illustrate the procedure. As indicated, assault, at-
tack and strike are listed as LUs of the ?Attack?
frame.
After manually examining all the semantic
frames thus obtained, the five BFN frames were
recognized as relevant to the various senses of the
target word osou: 1. ?Attack?; 2. ?Cause harm?
3. ?Experience bodily harm? 4. ?Cause impact?
5. ?Impact?
Semantic frames in the BFN database are sup-
posedly related to one another. There are vari-
ous relationships, some of which are sometimes
encoded by establishing explicit ?frame-to-frame
relations? (such as ?is used? relation) between
two frames. Using this information, we obtained
the following relationships between the five
frames: 1. ?Attack?; 2. ?Cause harm?, is used:
?Experience bodily harm?; 3. ?Cause impact?,
uses: ?Impact?
2.3 Identifying relevant frame-evoking LUs
in English
Each semantic frame has a number of FEs, each of
which has lexical realizations, which called LUs.
In the work reported here, only verbal LUs were
selected as relevant from the English LUs made
available in the BFN database.5 Admittedly, there
5 On this point, we recognize a certain kind of discrep-
ancy between the theory and the practice in the BFN frame-
work. If a LU is, according to its defintion, a lexical realiza-
tion of a certain FE of a certain frame, more nominals should
be identified and listed as LUs. For example, in Jack or-
dered a hamburger at McDonald?s, hamburger is a noun that
evokes the ?Cooking creation? frame. While the ?Selling?
frame is evoked by order.v, this means that, according the
definition of LU, hamburger.n needs to be identified as an
LU of the ?Cooking creation? frame; more specifically, it is
an LU that instantiates the ?Food? FE of the frame. It is ob-
vious that the QUALIA STRUCTURE (Pustejovsky, 1995) of
hamburger.n contains information of this sort. We suspect
that this aspect of ?frame-evocation by nominals? does not
seem to be properly recognized and coded, and that BFN?s
current practice of mostly identifying predicates as LUs is
somewhat misleading, if we could say so, because it con-
are a few nominal LUs in certain frames in the
BFN, but we ignored them because they found
them to be less relevant to our specific task.
After identifying all the relevant LUs for the
three frames above, we obtained all the English
verbs that translated the senses of the target word
osou identified in terms of Frame Semantics.
For example, the relevant LUs for the ?Attack?
frame are the following verbs: ambush, assault,
attack, charge, invade, jump, lay, set, storm, and
strike
As was the case with the ?Attack? frame, we
extracted the relevant LUs for the ?Cause harm?
and ?Cause impact? frames. We manually
merged the extracted LUs, and obtained 93 ver-
bal LUs relevant to the Japanese verb osou.
2.4 Obtaining LU candidates for Japanese
FEs
Table 1: 15 most frequently occurring nouns
Noun Freq.
jiken (incident) 39
boukou (criminal assault) 32
josei (woman) 28
taiho (arrest) 23
hikoku (accused, defendant) 21
yougi (charge, suspicion) 20
kougeki (attack) 20
shounen (boy) 14
tero (terrorism) 14
shougai (injury) 13
higai (damage, harm) 12
kenkei (prefectural police department) 12
manshon (apartment) 12
butai (military unit) 10
fujo (girl and woman) 10
Using the bilingual corpus again, we gathered
alignments that had English texts containing the
English LUs specified in the way previously de-
scribed. We obtained 262 alignments. This proce-
dure defined a set of Japanese sentences contain-
ing Japanese words or phrases that were natural
translations of the LUs in the BFN.
ceals the fact that there can be, and actually are, many kinds
of frame-evoking effects. BFN has been concentrating on
identifying LUs for ?governors,? not LUs for the entire set of
FEs, for whatever reason. In this respect, it is crucial to note
that not all frame-evokers are frame-governors: hamburger.n
clearly evokes the ?Cooking creation? frame, but there the
noun does not govern the ?Cooking creation? frame. Ar-
guably, it is unreasonable and even gratuitous to posit the
?Hamburger? frame to make hamburger.n a governor.
13
Attack
Definition:
An Assailant physically attacks a Victim (which is usually but not always sentient), causing or intending to cause the Victim
physical injury. The Weapon used by the Assailant may also be mentioned, in addition to the usual Place, Time, Purpose, and
Reason. Sometimes a location is used metonymically to stand for the Assailant or the Victim, and in such cases the Place FE
will be annotated on a second FE layer.
As soon as he stepped out of the bar he was SET upon by four men in ski-masks.
Is he INVADING Iraq just to cover other shortcomings?
Then Jon-O?s forces AMBUSHED them on the left flank from a line of low hills.
FEs:
Core:
Assailant [Asl] The person (or other self-directed entity) that is attempting physical harm to the Victim.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Victim [Vic] This FE is the being or entity that is injured by the Assailant?s attack.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Lexical Units
ambush.n, ambush.v, assail.v, assault.n, assault.v, attack.n, attack.v, charge.n, charge.v, fall.v, incursion.n, invade.v, inva-
sion.n, jump.v, lay ((into)).v, offensive.n, onset.n, onslaught.n, raid.v, set.v, storm. v, strike.n, strike.v
Created by infinity on Fri Nov 22 14:05:22 PST 2002
Figure 1: BFN definition of ?Attack? frame (partial)
It should be noted, however, that there is no es-
tablished method of recognizing these units au-
tomatically; they are part of a text without being
marked as such. To solve this problem, we hy-
pothesized that their statistical properties in the
texts could be used to pick them up; i.e., we as-
sumed that these LUs were relatively specific to
these types of texts and would appear at higher
frequencies than usual in the collected text.
We collected nouns with higher frequencies un-
der this assumption using a KH Coder 6.
The results were sorted according to the parts
of speech. The high-frequency nouns thus ob-
tained are listed in Table 1.
This provided little information about the se-
mantic classification of the nouns because there
was no indication of the LUs that they instan-
tiated. Semantic groupings are latent, how-
ever. This meant that we were able to ?clus-
ter? the nouns based on certain generic proper-
ties to obtain an initial approximation of these
groupings. We used a tool called msort (stand-
ing for ?meaning sort?) (Murata et al, 2001) to
establish generic, domain-independent semantic
6The KH Coder is a free analyzer that uses a combination
of ChaSen (Matsumoto et al, 1999) and MySQL. This is
freely available at http://khc.sourceforge.net/.
groupings.78
Nouns occurring more than three times were
obtained, as shown below:9
human dansei (man), danshi (boy), josei (woman), fujo
(woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), . . .
organization kokka (country), gaikoku (foreign country),
kokusai (international), sekai (world), . . .
product yakubutsu (drug), manshon (apartment), heya
(room), keesu (case), naifu (knife), shoujuu (rifle), . . .
7msort sorts a given set of nouns based on their encod-
ings in a Japanese thesaurus Bunrui Goi-hyou (National Lan-
guage Research Institute, 1964).
8One of the anonymous reviewers commented on this
?domain-independence? with a critical tone, questioning the
validity of the proposed method. This evaluation is clearly
based on a misunderstanding: the semantic association, or
conceptual dependence, between the ?Assailant? and the
?Victim? FEs is already encoded when we collected only
sentences whose main verbs are osou (in Japanese texts) or
its translations (in English texts). What we have done with
msort is to get subgroupings given a larger semantic group-
ing of ?harm-causing? at a more generic level. Based on
our coding experience, we are sure that subclassfication of a
given semantic class is based on ?semantic types? rather than
semantic roles. To give proper subgroupings of the events
that the ?Attack? frame is relevant, it is necessary to know
whether an ?Assailant? is a human ([+human, +animate,
. . . ]) or an animal ([?human, +animate, . . . ]), or whether
a ?Victim? is a human ([+human, +animate, . . . ]) or an
animal ([?human, +animate, . . . ]). If we insist that such
subclassifications in terms of semantic types into messy de-
tails are irrelevant, we are committing what we meant by
?mere generalizations for generalizations,? failing to recog-
nized what is really needed in NLP tasks.
9The listings ending with ?. . . ? are partial.
14
body part itai (body), soshiki (organization)
plant dansei (man), josei (woman), soshiki (tissue)
space genba (field), chiiki (region), mokuteki (purpose),
hokubu (northern area), shinai (city center)
amount gruupu (group)
relation jijou (circumstances), keesu (case), jitai (matter),
jiken (incident), ryakushiki (informality), kankei (rela-
tionship), mokuteki (purpose), genkou (current), . . .
activity jisatsu (suicide), satsugai (slaying), shougai (in-
jury), juushou (serious injuries), ishiki (conscious-
ness), utagai (doubt), yougi (suspicion), sousa (inves-
tigation), sousaku (search), shirabe (investigation), . . .
2.5 Identifying LUs for Japanese FEs
Based on the generic semantic groupings pro-
duced by msort, we classified nouns into sub-
classes by intution, so that they corresponded to
the FEs of the BFN frames in the following way:
Recall that a semantic frame is a collection of
semantic roles, or FEs. In the case of ?Attack?,
the frame has two ?core? FEs, i.e., ?Assailant?
and ?Victim?, and some other ?peripheral? or
?noncore? FEs such as ?Place?, ?Time?, and
?Weapon?. Thus, ?Attack? denotes a situation
in which an agent recognizable as an ?Assailant?
causes (or tries to cause) some ?Harm? or ?Injury?
to someone or a group of people recognizable as a
?Victim? at some ?Place? and ?Time?, sometimes
using an item recognizable as a ?Weapon?.
This means that all we need to do is to clas-
sify the nouns in Table 1 into semantic classes
such as ?Assailant?, ?Victim?, ?Place?, ?Time?, or
?Weapon?, with appropriate subclasses where hu-
man assailants are distinguished from nonhuman
assailants.10 The groupings provided by msort
turned out to be useful for this purpose.11
Using this procedure, the nouns obtained on a
frequency-basis for ?Attack? were classified into
the two core FEs, as follows:
10It is important to note that the target data selection pro-
cedure of BFN is biased. For example, they put aside a num-
ber of problematic cases like metaphorical expressions, and
this is clearly reflected in the current frame definitions. We
repeated noticed that metaphorically extended senses of a
word were systematically dropped in the current release of
BFN. For illustration, the sense of attack.n in heart attack
is not described in BFN. Descriptive ?gaps? of this sort are
clearly undesirable; some specific kinds of mapping prob-
lems between English LUs provided in BFN and Japanese
LUs arise from this.
11We were sometimes unable to identify an FE for a noun
class based solely on the output of msort. In these cases, we
looked at its usage in the corpus to determine its FE.
? ?Assailant?: dansei (man), goutou (burglary/burglar,
robbery/robber), heishi (soldier), hikoku (accused per-
son), butai (military unit), kyoudan (religious group)
? ?Victim?: danshi (boy), josei (woman), fujo (girl and
woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), shoujo (girl),
aite (opponent), nihonjin (Japanese), . . .
2.6 Advantages of proposed method
Using msort turned out to be more beneficial
than anticipated when it came to selecting non-
core FEs. msort helped to determine noncore
FEs correctly to a certain extent. The ?Attack?
frame, for example, includes noncore FEs such as
?Place?, ?Time?, ?Purpose?, and ?Reason? in ad-
dition to its core FEs, ?Assailant? and ?Victim?.
msort automatically groups naifu (knife), raifuru
(rifle), and pisutoru (pistol) into the ?product?
category, which corresponds to the ?Weapon? FE.
Similarly, it automatically groups chiiki (Regional
site), hokubu (northern area), and shinai (Inner
city) into the ?location? category, which corre-
sponds to ?Place?. Thus, part of the FE assign-
ment task can be done automatically using msort.
The procedure also produced some interesting
results. For example, the proposed method auto-
matically specifies a set of lexical items (or lex-
ical units) that clearly have the frame-evocation
effect but that are not properly identified as frame
elements of a semantic frame in BFN, either in
terms of core FEs or peripheral FEs (= noncore
FEs). The semantic groupings that were thus au-
tomatically identified are enumerated below:
1. Names denoting an act(ion) of N (N suru (or sareru))
(?(make) do N?): ranbou (violence), boukou (crimi-
nal assault), bouryoku (violence), jikkou (execution),
shuugeki (assault), kougeki (attack)
2. Names denoting a state of affairs N (V shita + N) (N
that S V ): satsugai (slaying), shougai (injury), goutou
(burglary/burglar, robbery/robber), satsujin (murder),
sasshou (killing and wounding)
3. Result ((Y ni) V shite, N wo owaseta) (?did V , and in-
flicted N to Y ): juushou (serious injuries)
4. Parts of the compound words: kyoushuu (assault
force) (a part of ?assault? force)
5. LUs of crime-related frames resulting from ?Attack?:
utagai (doubt), yougi (charge, suspicion), sousa (in-
vestigation), sousaku (search), shirabe (investigation),
kentou (investigation), hanketsu (judgement), . . .
A second look at the lexical items in 1 above
confirmed that most of these words or phrases can
15
be seen as LUs that realize, in Japanese, some of
the FEs of BFN?s ?Attack? frame.12 As sets of
lexical items were not classified automatically, we
had to determine all classifications manually.
2.7 Overall results
When the procedure was applied to ?Attack?,
?Cause harm? and ?Cause impact?, the following
Japanese LUs for their major FEs were specified:
1. Core FEs of ?Attack?:
?Assailant?: dansei (man), goutou (burglary/burglar, rob-
bery/robber), heishi (soldier), hikoku (accused
person), . . .
?Victim?: danshi (boy), josei (woman), fujo (girls and
women), joshi (girl), danji (young boy), . . .
2. Noncore FEs of ?Attack?:
?Place?: genba (field), chiiki (region), hokubu (northern
part), shinai (city center)
?Weapon?: naifu (knife), shoujuu (rifle), tanjuu (pistol)
3. Core FEs of ?Cause harm?:
?Body part? : senaka (back)
4. Core FEs of ?Cause impact?:
?Impactee?: doru (dollar), shijou (market), ginkou (bank),
shokoku (some countries)
?Impactor?: saigai (disaster), jishin (earthquake), fukyou
(depression), dageki (damage)
3 Comparison with other resources
To evaluate our results, we compared them with
other Japanese resources and methods for anal-
ysis, i.e., IPAL (IPA, 1987) and Nihongo Goi
Taikei (a Japanese lexicon) (hereafter called Goi
Taikei) (Ikehara et al, 1997), which are widely
used lexical resources, and semantic frame anal-
ysis by FOCAL (Nakamoto et al, to appear;
Kuroda et al, 2004), which is a recent frame-
work being developed with the aim of provid-
ing BFN-style semantic annotation and analy-
sis for Japanese independent of the Japanese
FrameNet (Ohara et al, 2003).
3.1 Comparison with Goi Taikei descriptions
Goi Taikei contains detailed information on the
predicate-argument structure classified according
to usage. Its semantic description of osou is given
below:
12For the reason of this argument, see note 5 above.
(1) 20 zokusei henka (property change) (motion)
N1 ga N2 wo osou
N1 strike N2
N1 (1270 shimpai (concern) 1262 kanashimi (sorrow)
2056 sainann (disaster) 2359 kishou (atmospheric
phenomena) 1000 tyuushou (abstract)) N2 (2 gutai
(object))
(2) 23 shintai dousa (physical motion) (motion)
N1 ga N2 wo osou
N1 attack N2
N1 (3 shutai (subject) 535 doubutsu (animal) 2416 by-
ouki (disease)) N2 (2 gutai (object))
(3) 23 shintai dousa (physical motion)
31 kanjou dousa (affective motion) (motion)
N1 ga N2 no fui wo osou
N1 surprise N2
N1 (4 hito (man) 1001 tyuushoubutsu (abstruc-
tion/abstraction?) 1235 koto (event)) N2 (4 hito
(man))
The word meanings were classified from the
properties of osou for nouns related to surface
cases of the verb. When we compared the frames
in BFN and the description provided by Goi
Taikei, and examined how the BFN frames corre-
sponded to the Goi Taikei definitions, we obtained
the following relationships:
Table 2: BFN/Goi-Taikei correspondences
Attack (2) 23 shintai dousa (physical motion)
Cause harm (1) 20 zokusei henka (property change)
Cause impact (1) 20 zokusei henka (property change)
First, we did not obtain the meaning ?An unex-
pected event occurred? like (3) in the Goi Taikei.
It was difficult to extract words whose meanings
described a manner of action, such as fui wo (by
surprise) using this method. It was also insuffi-
cient to extract only co-occurring nouns from sen-
tences related to verbs. As might be expected,
there was a close relationship between (2) and the
?Attack? frame. However, we were unable to find
?Assailant?s such as sickness in the BFN FEs. Fi-
nally, the ?Cause impact? frame and (1) were very
similar, except that assailant in (1) includes feel-
ings such as worry or sadness.
There was a good correlation between the se-
mantic frame constructed from BFN and the one
from Goi Taikei. With this method, however, we
met difficulties in extracting frames that did not
appear on the surface, such as ?manner of action?.
16
3.2 Comparison with IPAL descriptions
We compared the frames we obtained with the
definitions from the IPA Lexicon (IPA, 1987). Be-
low is an excerpt from the description of osou
from IPAL:
? Caption: osou001001 Semantic definition: An unde-
sirable thing unexpectedly occurs to someone.
Sentence valence pattern: N1 -ga N2 -wo
Noun phrase 1: bouto (rioter), goutou (burglary),
kuma (bear), sentouki (fighter plane), boufuu (wind
storm), jishinn (earthquake), ekibyou (plague), keizai
kiki (economic crisis)
Noun phrase 2: tabibito (traveler), fune (ship), nin-
gen (human)/kokudo (national land), kuni (country),
kouban (police box)
Example 1: Boufuu ga fune wo osotta. (A stormy wind
struck a ship.)
? Caption: osou001002
Semantic definition: Undesirable feelings and physio-
logical phenomena happening suddenly.
Sentence pattern: N1 -ga N2 -wo
Noun phrase 1: takamaru fuann (increased anxiety),
shi no kyoufu (fear of death), iyana kimochi (unpleas-
ant feelings)/ hageshii hiroukan (acute tiredness), ne-
muke (drowsiness)
Noun phrase 2: kare (he)
Example 1: Nemuke ga totsuzen kare wo osotta.
(Drowsiness fell upon him suddenly.)
Example 2: Kanojo ha fuann ni osowareta. (She be-
came uneasy suddenly.)
The IPAL description of osou identifies its two
senses13 We compared the BFN frames and the
IPAL descriptions (in terms of predicate frames)
and obtained the following correspondences:
Table 3: BFN/IPAL correspondences
Attack osou001001
Cause harm osou001001
Cause impact osou001001
All of the frames obtained from BFN seemed to
be classified into the first meaning in IPAL, e.g.,
there were no BFN frames in which ?Assailant?
recognized ?sickness.? With IPAL definitions,
it was difficult to distinguish the difference be-
tween The bear attacked the traveler and *An eco-
nomic crisis attacked the traveler, the latter of
which sounds unnatural and quite odd, whereas
we can do it with BFN definitions: the former
13A term, ?predicate frame,? is used in the IPAL to char-
acterize semantic properties of a predicate. While the idea
of predicate frames is somewhat related to semantic frames,
predicate frames are not defined as semantic frames in the
sense of Frame Semantics/BFN.
can be classified as an expression in the ?Attack?
frame, whereas the latter can not. The reason
for this is probably that BFN frames successfully
specify the semantic interdependence between the
?Assailant? and ?Victim? roles, whereas such in-
terdependece is not encoded in the IPAL descrip-
tions. We believe this is one of the strengths of
frame-based semantic description.
BFN definitions are not detailed enough, how-
ever. They face problems when we try to ac-
count for the constrast between The shark at-
tacked the swimmer and ?*The shark attacked the
bank, for example. The latter sentences doesn?t
makes sense unless it is reinterpreted some way,
while it is straightforward to interpret the first sen-
tence against a predatory situation.
In interpreting the second, there is a clear con-
flict or ?competition? between two strong read-
ings: one interpretation (reading 1) is against the
situation of ?Predation?, where the shark is inter-
preted as a ?Predator? and the bank as a ?Prey?.
Another (reading 2) is against the situation of
?Bank Robbery?, where the shark is interpreted
as a ?Bank Robber? and the bank as a ?Warehouse
of Valuables? (or simply as a ?Bank?). If reading
2 wins out, an implicit ?type coercion? (Puste-
jovsky, 1995) takes place to the shark so that the
referent of the shark is switched to a human who
acts as a ?Robber? with a nickname ?shark.? If
reading 1 wins out, by contrast, another kind of
implicit type coercion takes place to the bank so
that the referent of the bank is switched to an ani-
mal (an instance of fish, dolphin, or whale) which
acts as a ?Prey?, being called ?the bank? for some
unclear reasons. The preference of the reinter-
pretation for reading 2 over the other can be ac-
counted for if we are allowed to say that to find
someone being called ?shark? is more likely than
to find some animal being called ?bank.?
What this suggests is this: pieces of semantic
information that would account for ?selectional
restrictions? of this sort are not specified in the
BFN definitions (yet). Therefore, it can be said
that the frames constructed from BFN do not
classify all meanings of osou in the same way
IPAL does not, but these frames specify some
finer-grained, selectional aspects of osou?s lexical
meaning than the IPAL description. As we will
see in the next section, this is one of the strong
17
motivations that a framework called FOCAL has
tried to extend the BFN.
3.3 Comparison with FOCAL descriptions
FOCAL is a theoretical framework for semantic
analysis and annotation. Its development has been
strongly influenced by BFN, but it also tries to
extend BFN?s scope of semantic analysis to the
next stage.
In the case of X-ga Y-wo osou, FOCAL recog-
nizes 15 frames in total, listed in Table 4, specify-
ing their hierarchical organization.14
These frames are identified and classified based
on the semantic co-variations between ?Harm
Cause(r))? X , a special case of ?Cause(r)?,
and ?Harm Experiencer? Y , a special case of
?Experiencer?. This is important to note that FO-
CAL puts more emphasis on the specification of
the semantic co-variation between X and Y in
terms of semantic features because they are cru-
cial characteristics of a semantic frame, which are
not captured in the Goi Taikei and IPAL descrip-
tions, and are not clearly encoded even in the BFN
description.
In FOCAL, frames are defined as idealized
models of situations such as Robbery, Predation,
assuming that human understanding is situation-
based. The descriptive task of FOCAL, then, is
to recognize situations and give adequately de-
tailed descriptions to them. Given R is a set of
situation-specific roles {r1, . . . , rn}, which are
called semantic roles in BFN. Semantic frames
are useful only if they serves as specifications of
the co-variations among such Rs.
For example, F06, as a subclass of the ?Attack?
class event is defined as follows:
Definition of F06: Attack(R) = Attack(Predator(X),
Prey(Y ))
= Hunt(Hunter(X), Target(Y ), Purpose(Z))
where Z = Eat(Eater(X), Food(Y ), Purpose(Z?));
where Z? = Satisfy (r1(Z), Hunger)
There seems to be no English noun that names r1.
These are the frames that account for more or
less all possible readings of X-ga Y -wo osou. The
14 Space limitation disallowed us to show that the 15
frames thus recognized are nearly optimal to exhaustively
specify all the situations against which the senses of osou are
determined. This was confirmed by multivariate analyses on
psychological experiments (Nakamoto et al, to appear). We
regret this because the result would surely have answered the
question from one of the anonymous reviewers.
Table 4: 15 FOCAL frames with groups G1?G5
G1 F01 harm to Y caused by conflict between
groups X and Y
G1 F02 harm to Y caused by X?s invasion
G1 F03 harm to Y caused by X?s robbery
G1 F04 harm to Y caused by X?s violence
G1 F05 harm to Y caused by X?s raping
G2 F06 harm to Y caused by X?s preying attack
G2 F07 harm to Y caused by X?s nonpreying attack
(e.g., X?s defense)
G3 F08 harm to Y due to an unexpected accident X
G3 F09 harm to Y caused by a natural phenomenon
X (on a smaller scale, e.g., gust)
G3 F10 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., earthquake, flood)
G3 F11 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., spread of an
epidemic)
G4 F12 harm to Y caused by a social phenomenon X
G5 F13 harm to Y caused by a disease X
(nontemporary, e.g., cancer)
G5 F14 harm to Y caused by a disease symptom X
(temporary, e.g., heart attack)
G5 F15 harm to Y caused by a bad feeling X
(temporary, e.g., drowsiness)
validity of this claim was confirmed through psy-
chological experiments, and reported in (Kuroda
et al, 2004; Nakamoto et al, to appear). The
BFN identifies 3 frames relevant to the semantics
of osou, while FOCAL uses a total of 15 frames
to determine the range of situations against which
people understand the sentences whose main verb
is osou.
The 3 BFN frames have been compared with
the 15 frames below to assess how well they cor-
respond to one another:
Table 5: BFN/FOCAL correspondences
Attack Part of G1 F01?F05
Cause harm [UNCLEAR] [UNCLEAR]
Cause impact [UNCLEAR] [UNCLEAR]
[UNCLEAR] G5 F13?F15
This comparison revealed several differences.
First, FOCAL specifies situations that the
?Attack? frame applies to in much greater de-
tail, although its descriptions are based on se-
mantic frames like BFN?s descriptions are. This
is mainly because FOCAL identifies frames in
terms of conceivable differences in the ?pur-
poses,? or ?intended effects? of the ?Harm
18
Cause(r)?15, of which BFN?s ?Assailant? is a spe-
cial case. This suggests that BFN frames can be
further elaborated according to the subclassifica-
tion of ?Assailant? in terms of its purpose.16
The same is conversely true of ?Cause harm?
and ?Cause impact? frames. These BFN frames
need to be generalized so that they include nonhu-
man, nonintentional agents, which is not done in
the current BFN. Better matches would be found
if the ?Cause harm? and ?Cause impact? frames
were further classified according to the properties
of the ?Harm causer? and ?Impactor? just as in the
?Attack? frame.
While FOCAL explicitly groups the F01?F05
frames into G1 and combines it with another
group, G2, to yield a more general semantic class
{G1, G2}, it is not clear whether BFN captures
this hybrid class, since the hierarchical relation-
ships among frames are not sufficiently specified.
In fact, the comparison with FOCAL revealed
that BFN does not classify the ?Assailant? types in
as much detail as FOCAL does. According to FO-
CAL?s assumptions, it is ?Assailant??s ?Purpose?
(including the ?null? value) that defines the differ-
ences in otherwise similar situations. To identify
such subtle differences is exactly what humans
are very good at and computers are not. Speci-
fication of information of this kind is one of the
serious demands arising from many of the NLP
tasks.
To conclude, we noted that the granularity
of the semantic descriptions provided by BFN,
IPAL, Goi Taikei, and FOCAL had the following
hierarchy: FOCAL > BFN ? Goi Taikei > IPAL
This suggests that, while BFN is clearly useful
for a variety of purposes, its semantic descrip-
tions are not detailed enough, particularly when
dealing with the polysemy of relatively frequent
words like osou in Japanese or hit in English.
While our result is only suggestive at best, let
15This is not the same as BFN?s ?Harm causer? role,
which is much more specific than ?Harm Cause(r)? in FO-
CAL?s sense.
16The question of ?where to stop,? addressed by one of
the anonymous reviewers, would have been answered if we
had enough space to show that those 15 frames/situations are
nearly optimal to account for all the semantic classifications
reflected in selectional restrictions, as explained in note 14.
Clearly, we do not need to identify all semantically possible
subclassifications; we just need to identify psychologically
real subclassifications.
us make a brief comment on some methodologi-
cal aspects of the BFN framework.
Overall, BFN definitions for semantic frames
are much more oriented or even ?biased? for de-
scriptions of activities intended and caused by
human, volitional agents. In fact, BFN took a
methodological decision not to include metaphor-
ical uses and other ?problematic? uses of words
for ease of lexicon-building, thereby sacrificing
its descriptive range, causing a problem with bi-
ased data coverage, as far as we could see. In
the case of osou, for example, there were clearly
many examples in which harm is not caused by
a human, i.e., cases described by FOCAL frame
clusters G2: F06?F07, G3: F08?F11, G4: F12,
and G5: F13?F15. Therefore, as far as we are
concerned with the viability of the frame-based
description of situations that can be expressed us-
ing osou in Japanese, the current status of the
BFN database is only partially successful in that it
successfully captures the class of situations spec-
ified by G1.
4 Conclusion
We proposed a new translation-like method using
BFN to find Japanese LUs that corresponded to
English LUs in BFN semantic frames. We eval-
uated a technique of identifying Japanese LUs
based on English LUs using a bilingual corpus.
We evaluated the results by comparing them with
other Japanese language resources and analyses,
IPAL, Goi Taikei, and FOCAL. The comparison
revealed that FOCAL, BFN, Goi Taikei, and IPAL
provided finer-grained descriptions in this specific
order.
Our method allowed us to easily find Japanese
LUs that corresponded to LUs in BFN seman-
tics and at the same level of granularity as BFN.
Even if all the relevant sentenceswere not manu-
ally examined when the semantic frame was con-
structed, we were able to collect several members
of FEs. Our method also automatically specified
a set of lexical titems that clearly had the frame-
evocation effect but that were not properly iden-
tified as Frame Elements of a semantic frame in
BFN.
There are several problems still remaining that
need to be addressed. Because the bilingual cor-
pus used was a newspaper corpus, the target se-
19
mantic domains were limited. There is therefore
a possibility that we failed to identify certain se-
mantic frames. We plan to do further experiments
using a greater number of bilingual corpora with
a wider domain coverage.
In the comparison of the analyses by BFN and
by FOCAL, only one target verb osou is used in
this work. Clearly, this is insufficient and our re-
sult is only suggestive at best. To draw a realis-
tic conclusion, we will definitely need to examine
more target words and make the comparison more
reliable.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the COLING-ACL ?98, Montreal;
Canada.
Benfung Chen and Pascale Fung. 2004. Biframenet:
Bilingual frame semantics resource construction by
cross-lingual induction. In Proceedings of the 20th
International Conference on Computational Lin-
guistics (COLING 2004).
Michael Ellsworth, Katrin Erk, Paul Kingsbury, and
Sebastian Pado?. 2004. PropBank, SALSA, and
FrameNet: How design determines product. In
Proceedings of the LREC 2004 Workshop on Build-
ing Lexical Resources from Semantically Annotated
Corpora, Lisbon.
Katrin Erk, Andrea Kowalski, Sebastian Pado?, and
Manfred Pinkal. 2003. Towards a resource for
lexical semantics: A large German corpus with ex-
tensive semantic annotation. In Proceedings of the
ACL-03.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Charles J. Fillmore. 1982. Frame semantics. In Lin-
guistic Society of Korea, editor, Linguistics in the
Morning Calm, pages 111?137, Seoul. Hanshin.
Satoru Ikehara, Mahahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei: A Japanese Lexicon. Iwanami Shoten,
Tokyo. (in Japanese, 5 volumes/CDROM).
IPA, 1987. IPA Lexicon of the Japanese Language for
Computers: Basic Verbs. Information-Technology
Promotion Agency. (in Japanese).
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd In-
ternational Conference on Language Resources and
Evaluation (LREC-2002).
Kolodner, Janet. L. 2004. Case-Based Reasoning.
Morgan Kauffman.
Kow Kuroda, Keiko Nakamoto, Toshiyuki Kanamaru,
Masahiro Tatsuoka, and Hajime Nozawa. 2004.
A scope of concept analysis based on ?seman-
tic frames?: Berkeley FrameNet and Beyond. In
Conference Handbook of the 5th Meeting of The
Japanese Cognitive Linguistics Association, pages
133?153. (in Japanese).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara, 1999. Japanese
Morphological Analysis System ChaSen version
2.2.1. NAIST Technical Report NAIST-IS-TR. (in
Japanese).
Masaki Murata, Kyoko Kanzaki, Kiyotaka Uchimoto,
Qing Ma, and Hitoshi Isahara. 2001. Meaning sort
? three examples: dictionary construction, tagged
corpus construction, and information presentation
system ?. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
Second International Conference, CICLing 2001,
Mexico City, February 2001 Proceedings, pages
305?318. Springer Publisher.
Keiko Nakamoto, Kow Kuroda, and Hajime Nozawa.
to appear. Defining the feature rating task as
a(nother) powerful method to explore sentence
meanings: With a special interest with how they are
mentally represented. In Japanese Journal of Cog-
nitive Psychology. (in Japanese).
National Language Research Institute. 1964. Bunrui
Goihyo (Word List by Semantic Principles). Syuei
Shuppan. (in Japanese).
Pustejovsky, James. 1995. The Generative Lexicon.
MIT Press.
Kyoko Hirose Ohara, Seiko Fujii, Hiroaki Saito, Shun
Ishizaki, Toshio Ohori, and Ryoko Suzuki. 2003.
The Japanese FrameNet project: A preliminary re-
port. In Proceedings of Pacific Association for
Computational Linguistics, pages 249?254.
Hiroaki Sato. 2003. FrameSQL: A software tool for
FrameNet. In ASIALEX ?03 Tokyo Proceedings,
pages 251?258. Asian Association of Lexicogra-
phy.
Carlos Subirats and Miriam R. L. Petruck. 2003. Sur-
prise: Spanish FrameNet. Presentation at Work-
shop on Frame Semantics, International Congress
of Linguists. July 29, 2003, Prague, Czech Repub-
lic.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the Annual
Meeting of the ACL-03, pages 72?79. ACL-2003.
20
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247?256,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Bayesian Method for Robust Estimation of Distributional Similarities
Jun?ichi Kazama Stijn De Saeger Kow Kuroda
Masaki Murata? Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan
{kazama, stijn, kuroda, torisawa}@nict.go.jp
?Department of Information and Knowledge Engineering
Faculty/Graduate School of Engineering, Tottori University
4-101 Koyama-Minami, Tottori, 680-8550 Japan?
murata@ike.tottori-u.ac.jp
Abstract
Existing word similarity measures are not
robust to data sparseness since they rely
only on the point estimation of words?
context profiles obtained from a limited
amount of data. This paper proposes a
Bayesian method for robust distributional
word similarities. The method uses a dis-
tribution of context profiles obtained by
Bayesian estimation and takes the expec-
tation of a base similarity measure under
that distribution. When the context pro-
files are multinomial distributions, the pri-
ors are Dirichlet, and the base measure is
the Bhattacharyya coefficient, we can de-
rive an analytical form that allows efficient
calculation. For the task of word similar-
ity estimation using a large amount of Web
data in Japanese, we show that the pro-
posed measure gives better accuracies than
other well-known similarity measures.
1 Introduction
The semantic similarity of words is a long-
standing topic in computational linguistics be-
cause it is theoretically intriguing and has many
applications in the field. Many researchers have
conducted studies based on the distributional hy-
pothesis (Harris, 1954), which states that words
that occur in the same contexts tend to have similar
meanings. A number of semantic similarity mea-
sures have been proposed based on this hypothesis
(Hindle, 1990; Grefenstette, 1994; Dagan et al,
1994; Dagan et al, 1995; Lin, 1998; Dagan et al,
1999).
?The work was done while the author was at NICT.
In general, most semantic similarity measures
have the following form:
sim(w1, w2) = g(v(w1), v(w2)), (1)
where v(wi) is a vector that represents the con-
texts in which wi appears, which we call a context
profile of wi. The function g is a function on these
context profiles that is expected to produce good
similarities. Each dimension of the vector corre-
sponds to a context, fk, which is typically a neigh-
boring word or a word having dependency rela-
tions with wi in a corpus. Its value, vk(wi), is typ-
ically a co-occurrence frequency c(wi, fk), a con-
ditional probability p(fk|wi), or point-wise mu-
tual information (PMI) between wi and fk, which
are all calculated from a corpus. For g, various
works have used the cosine, the Jaccard coeffi-
cient, or the Jensen-Shannon divergence is uti-
lized, to name only a few measures.
Previous studies have focused on how to de-
vise good contexts and a good function g for se-
mantic similarities. On the other hand, our ap-
proach in this paper is to estimate context profiles
(v(wi)) robustly and thus to estimate the similarity
robustly. The problem here is that v(wi) is com-
puted from a corpus of limited size, and thus in-
evitably contains uncertainty and sparseness. The
guiding intuition behind our method is as follows.
All other things being equal, the similarity with
a more frequent word should be larger, since it
would be more reliable. For example, if p(fk|w1)
and p(fk|w2) for two given words w1 and w2 are
equal, but w1 is more frequent, we would expect
that sim(w0, w1) > sim(w0, w2).
In the NLP field, data sparseness has been rec-
ognized as a serious problem and tackled in the
context of language modeling and supervised ma-
chine learning. However, to our knowledge, there
247
has been no study that seriously dealt with data
sparseness in the context of semantic similarity
calculation. The data sparseness problem is usu-
ally solved by smoothing, regularization, margin
maximization and so on (Chen and Goodman,
1998; Chen and Rosenfeld, 2000; Cortes and Vap-
nik, 1995). Recently, the Bayesian approach has
emerged and achieved promising results with a
clearer formulation (Teh, 2006; Mochihashi et al,
2009).
In this paper, we apply the Bayesian framework
to the calculation of distributional similarity. The
method is straightforward: Instead of using the
point estimation of v(wi), we first estimate the
distribution of the context profile, p(v(wi)), by
Bayesian estimation and then take the expectation
of the original similarity under this distribution as
follows:
simb(w1, w2) (2)
= E[sim(w1, w2)]{p(v(w1)),p(v(w2))}
= E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}.
The uncertainty due to data sparseness is repre-
sented by p(v(wi)), and taking the expectation en-
ables us to take this into account. The Bayesian
estimation usually gives diverging distributions for
infrequent observations and thus decreases the ex-
pectation value as expected.
The Bayesian estimation and the expectation
calculation in Eq. 2 are generally difficult and
usually require computationally expensive proce-
dures. Since our motivation for this research is to
calculate good semantic similarities for a large set
of words (e.g., one million nouns) and apply them
to a wide range of NLP tasks, such costs must be
minimized.
Our technical contribution in this paper is to
show that in the case where the context profiles are
multinomial distributions, the priors are Dirich-
let, and the base similarity measure is the Bhat-
tacharyya coefficient (Bhattacharyya, 1943), we
can derive an analytical form for Eq. 2, that en-
ables efficient calculation (with some implemen-
tation tricks).
In experiments, we estimate semantic similari-
ties using a large amount of Web data in Japanese
and show that the proposed measure gives bet-
ter word similarities than a non-Bayesian Bhat-
tacharyya coefficient or other well-known similar-
ity measures such as Jensen-Shannon divergence
and the cosine with PMI weights.
The rest of the paper is organized as follows. In
Section 2, we briefly introduce the Bayesian esti-
mation and the Bhattacharyya coefficient. Section
3 proposes our new Bayesian Bhattacharyya coef-
ficient for robust similarity calculation. Section 4
mentions some implementation issues and the so-
lutions. Then, Section 5 reports the experimental
results.
2 Background
2.1 Bayesian estimation with Dirichlet prior
Assume that we estimate a probabilistic model for
the observed data D, p(D|?), which is parame-
terized with parameters ?. In the maximum like-
lihood estimation (MLE), we find the point esti-
mation ?? = argmax?p(D|?). For example, we
estimate p(fk|wi) as follows with MLE:
p(fk|wi) = c(wi, fk)/
X
k
c(wi, fk). (3)
On the other hand, the objective of the Bayesian
estimation is to find the distribution of ? given
the observed data D, i.e., p(?|D), and use it in
later processes. Using Bayes? rule, this can also
be viewed as:
p(?|D) = p(D|?)pprior(?)p(D) . (4)
pprior(?) is a prior distribution that represents the
plausibility of each ? based on the prior knowl-
edge. In this paper, we consider the case where
? is a multinomial distribution, i.e.,
?
k ?k = 1,
that models the process of choosing one out of K
choices. Estimating a conditional probability dis-
tribution ?k = p(fk|wi) as a context profile for
each wi falls into this case. In this paper, we also
assume that the prior is the Dirichlet distribution,
Dir(?). The Dirichlet distribution is defined as
follows.
Dir(?|?) =
?(
PK
k=1 ?k)
QK
k=1 ?(?k)
K
Y
k=1
??k?1k . (5)
?(.) is the Gamma function. The Dirichlet distri-
bution is parametrized by hyperparameters ?k(>
0).
It is known that p(?|D) is also a Dirichlet dis-
tribution for this simplest case, and it can be ana-
lytically calculated as follows.
p(?|D) = Dir(?|{?k + c(k)}), (6)
where c(k) is the frequency of choice k in data D.
For example, c(k) = c(wi, fk) in the estimation
of p(fk|wi). This is very simple: we just need to
add the observed counts to the hyperparameters.
248
2.2 Bhattacharyya coefficient
When the context profiles are probability distribu-
tions, we usually utilize the measures on probabil-
ity distributions such as the Jensen-Shannon (JS)
divergence to calculate similarities (Dagan et al,
1994; Dagan et al, 1997). The JS divergence is
defined as follows.
JS(p1||p2) =
1
2(KL(p1||pavg) + KL(p2||pavg)),
where pavg = p1+p22 is a point-wise average of p1
and p2 and KL(.) is the Kullback-Leibler diver-
gence. Although we found that the JS divergence
is a good measure, it is difficult to derive an ef-
ficient calculation of Eq. 2, even in the Dirichlet
prior case.1
In this study, we employ the Bhattacharyya co-
efficient (Bhattacharyya, 1943) (BC for short),
which is defined as follows:
BC(p1, p2) =
K
X
k=1
?
p1k ? p2k.
The BC is also a similarity measure on probabil-
ity distributions and is suitable for our purposes as
we describe in the next section. Although BC has
not been explored well in the literature on distribu-
tional word similarities, it is also a good similarity
measure as the experiments show.
3 Method
In this section, we show that if our base similarity
measure is BC and the distributions under which
we take the expectation are Dirichlet distributions,
then Eq. 2 also has an analytical form, allowing
efficient calculation.
Here, we calculate the following value given
two Dirichlet distributions:
BCb(p1, p2) = E[BC(p1, p2)]{Dir(p1|?? ),Dir(p2|?? )}
=
ZZ
???
Dir(p1|?
?
)Dir(p2|?
?
)BC(p1, p2)dp1dp2.
After several derivation steps (see Appendix A),
we obtain the following analytical solution for the
above:
1A naive but general way might be to draw samples of
v(wi) from p(v(wi)) and approximate the expectation using
these samples. However, such a method will be slow.
= ?(?
?
0)?(?
?
0)
?(??0 + 12 )?(?
?
0 + 12 )
K
X
k=1
?(?
?
k + 12 )?(?
?
k + 12 )
?(??k)?(?
?
k)
, (7)
where ??0 =
?
k ?
?
k and ?
?
0 =
?
k ?
?
k. Note that
with the Dirichlet prior, ??k = ?k + c(w1, fk) and
??k = ?k + c(w2, fk), where ?k and ?k are the
hyperparameters of the priors of w1 and w2, re-
spectively.
To put it all together, we can obtain a new
Bayesian similarity measure on words, which can
be calculated only from the hyperparameters for
the Dirichlet prior, ? and ?, and the observed
counts c(wi, fk). It is written as follows.
BCb(w1, w2) = (8)
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + 12 )?(?0 + b0 +
1
2 )
?
K
X
k=1
?(?k + c(w1, fk) + 12 )?(?k + c(w2, fk) +
1
2 )
?(?k + c(w1, fk))?(?k + c(w2, fk))
,
where a0 =
?
k c(w1, fk) and b0 =
?
k c(w2, fk). We call this new measure the
Bayesian Bhattacharyya coefficient (BCb for
short). For simplicity, we assume ?k = ?k = ? in
this paper.
We can see that BCb actually encodes our guid-
ing intuition. Consider four words, w0, w1, w2,
and w4, for which we have c(w0, f1) = 10,
c(w1, f1) = 2, c(w2, f1) = 10, and c(w3, f1) =
20. They have counts only for the first dimen-
sion, i.e., they have the same context profile:
p(f1|wi) = 1.0, when we employ MLE. When
K = 10, 000 and ?k = 1.0, the Bayesian similar-
ity between these words is calculated as
BCb(w0, w1) = 0.785368
BCb(w0, w2) = 0.785421
BCb(w0, w3) = 0.785463
We can see that similarities are different ac-
cording to the number of observations, as ex-
pected. Note that the non-Bayesian BC will re-
turn the same value, 1.0, for all cases. Note
also that BCb(w0, w0) = 0.78542 if we use Eq.
8, meaning that the self-similarity might not be
the maximum. This is conceptually strange, al-
though not a serious problem since we hardly use
sim(wi, wi) in practice. If we want to fix this,
we can use the special definition: BCb(wi, wi) ?
1. This is equivalent to using simb(wi, wi) =
E[sim(wi, wi)]{p(v(wi))} = 1 only for this case.
249
4 Implementation Issues
Although we have derived the analytical form
(Eq. 8), there are several problems in implement-
ing robust and efficient calculations.
First, the Gamma function in Eq. 8 overflows
when the argument is larger than 170. In such
cases, a commonly used way is to work in the log-
arithmic space. In this study, we utilize the ?log
Gamma? function: ln?(x), which returns the log-
arithm of the Gamma function directly without the
overflow problem.2
Second, the calculation of the log Gamma func-
tion is heavier than operations such as simple mul-
tiplication, which is used in existing measures.
In fact, the log Gamma function is implemented
using an iterative algorithm such as the Lanczos
method. In addition, according to Eq. 8, it seems
that we have to sum up the values for all k, be-
cause even if c(wi, fk) is zero the value inside the
summation will not be zero. In the existing mea-
sures, it is often the case that we only need to sum
up for k where c(wi, fk) > 0. Because c(wi, fk)
is usually sparse, that technique speeds up the cal-
culation of the existing measures drastically and
makes it practical.
In this study, the above problem is solved by
pre-computing the required log Gamma values, as-
suming that we calculate similarities for a large
set of words, and pre-computing default values for
cases where c(wi, fk) = 0. The following values
are pre-computed once at the start-up time.
For each word:
(A) ln?(?0 + a0) ? ln?(?0 + a0 + 12)
(B) ln?(?k+c(wi, fk))?ln?(?k+c(wi, fk)+ 12)
for all k where c(wi, fk) > 0
(C) ? exp(2(ln?(?k + 12) ? ln?(?k)))) +
exp(ln?(?k + c(wi, fk)) ? ln?(?k +
c(wi, fk) + 12) + ln?(?k +
1
2) ? ln?(?k))
for all k where c(wi, fk) > 0;
For each k:
(D): exp(2(ln?(?k + 12)).
In the calculation of BCb(w1, w2), we first as-
sume that all c(wi, fk) = 0 and set the output
variable to the default value. Then, we iterate
over the sparse vectors c(w1, fk) and c(w2, fk). If
2We used the GNU Scientific Library (GSL)
(www.gnu.org/software/gsl/), which implements this
function.
c(w1, fk) > 0 and c(w2, fk) = 0 (and vice versa),
we update the output variable just by adding (C).
If c(w1, fk) > 0 and c(w2, fk) > 0, we update
the output value using (B), (D) and one additional
exp(.) operation. With this implementation, we
can make the computation of BCb practically as
fast as using other measures.
5 Experiments
5.1 Evaluation setting
We evaluated our method in the calculation of sim-
ilarities between nouns in Japanese.
Because human evaluation of word similari-
ties is very difficult and costly, we conducted au-
tomatic evaluation in the set expansion setting,
following previous studies such as Pantel et al
(2009).
Given a word set, which is expected to con-
tain similar words, we assume that a good simi-
larity measure should output, for each word in the
set, the other words in the set as similar words.
For given word sets, we can construct input-and-
answers pairs, where the answers for each word
are the other words in the set the word appears in.
We output a ranked list of 500 similar words
for each word using a given similarity measure
and checked whether they are included in the an-
swers. This setting could be seen as document re-
trieval, and we can use an evaluation measure such
as the mean of the precision at top T (MP@T ) or
the mean average precision (MAP). For each input
word, P@T (precision at top T ) and AP (average
precision) are defined as follows.
P@T = 1T
T
X
i=1
?(wi ? ans),
AP = 1R
N
X
i=1
?(wi ? ans)P@i.
?(wi ? ans) returns 1 if the output word wi is
in the answers, and 0 otherwise. N is the number
of outputs and R is the number of the answers.
MP@T and MAP are the averages of these values
over all input words.
5.2 Collecting context profiles
Dependency relations are used as context profiles
as in Kazama and Torisawa (2008) and Kazama et
al. (2009). From a large corpus of Japanese Web
documents (Shinzato et al, 2008) (100 million
250
documents), where each sentence has a depen-
dency parse, we extracted noun-verb and noun-
noun dependencies with relation types and then
calculated their frequencies in the corpus. If a
noun, n, depends on a word, w, with a relation,
r, we collect a dependency pair, (n, ?w, r?). That
is, a context fk, is ?w, r? here.
For noun-verb dependencies, postpositions
in Japanese represent relation types. For
example, we extract a dependency relation
(???, ? ??,? ?) from the sentence below,
where a postposition ?? (wo)? is used to mark
the verb object.
??? (wine)? (wo)?? (buy) (? buy a wine)
Note that we leave various auxiliary verb suf-
fixes, such as ??? (reru),? which is for passiviza-
tion, as a part of w, since these greatly change the
type of n in the dependent position.
As for noun-noun dependencies, we considered
expressions of type ?n1 ? n2? (? ?n2 of n1?) as
dependencies (n1, ?n2,? ?).
We extracted about 470 million unique depen-
dencies from the corpus, containing 31 million
unique nouns (including compound nouns as de-
termined by our filters) and 22 million unique con-
texts, fk. We sorted the nouns according to the
number of unique co-occurring contexts and the
contexts according to the number of unique co-
occurring nouns, and then we selected the top one
million nouns and 100,000 contexts. We used only
260 million dependency pairs that contained both
the selected nouns and the selected contexts.
5.3 Test sets
We prepared three test sets as follows.
Set ?A? and ?B?: Thesaurus siblings We
considered that words having a common
hypernym (i.e., siblings) in a manually
constructed thesaurus could constitute a
similar word set. We extracted such sets
from a Japanese dictionary, EDR (V3.0)
(CRL, 2002), which contains concept hier-
archies and the mapping between words and
concepts. The dictionary contains 304,884
nouns. In all, 6,703 noun sibling sets were
extracted with the average set size of 45.96.
We randomly chose 200 sets each for sets
?A? and ?B.? Set ?A? is a development set to
tune the value of the hyperparameters and
?B? is for the validation of the parameter
tuning.
Set ?C?: Closed sets Murata et al (2004) con-
structed a dataset that contains several closed
word sets such as the names of countries,
rivers, sumo wrestlers, etc. We used all of
the 45 sets that are marked as ?complete? in
the data, containing 12,827 unique words in
total.
Note that we do not deal with ambiguities in the
construction of these sets as well as in the calcu-
lation of similarities. That is, a word can be con-
tained in several sets, and the answers for such a
word is the union of the words in the sets it belongs
to (excluding the word itself).
In addition, note that the words in these test sets
are different from those of our one-million-word
vocabulary. We filtered out the words that are not
included in our vocabulary and removed the sets
with size less than 2 after the filtering.
Set ?A? contained 3,740 words that are actually
evaluated, with about 115 answers on average, and
?B? contained 3,657 words with about 65 answers
on average. Set ?C? contained 8,853 words with
about 1,700 answers on average.
5.4 Compared similarity measures
We compared our Bayesian Bhattacharyya simi-
larity measure, BCb, with the following similarity
measures.
JS Jensen-Shannon divergence between p(fk|w1)
and p(fk|w2) (Dagan et al, 1994; Dagan et
al., 1999).
PMI-cos The cosine of the context profile vec-
tors, where the k-th dimension is the point-
wise mutual information (PMI) between
wi and fk defined as: PMI(wi, fk) =
log p(wi,fk)p(wi)p(fk) (Pantel and Lin, 2002; Pantel
et al, 2009).3
Cls-JS Kazama et al (2009) proposed using
the Jensen-Shannon divergence between hid-
den class distributions, p(c|w1) and p(c|w2),
which are obtained by using an EM-based
clustering of dependency relations with a
model p(wi, fk) =
?
c p(wi|c)p(fk|c)p(c)
(Kazama and Torisawa, 2008). In order to
3We did not use the discounting of the PMI values de-
scribed in Pantel and Lin (2002).
251
alleviate the effect of local minima of the EM
clustering, they proposed averaging the simi-
larities by several different clustering results,
which can be obtained by using different ini-
tial parameters. In this study, we combined
two clustering results (denoted as ?s1+s2? in
the results), each of which (?s1? and ?s2?)
has 2,000 hidden classes.4 We included this
method since clustering can be regarded as
another way of treating data sparseness.
BC The Bhattacharyya coefficient (Bhat-
tacharyya, 1943) between p(fk|w1) and
p(fk|w2). This is the baseline for BCb.
BCa The Bhattacharyya coefficient with absolute
discounting. In calculating p(fk|wi), we sub-
tract the discounting value, ?, from c(wi, fk)
and equally distribute the residual probabil-
ity mass to the contexts whose frequency is
zero. This is included as an example of naive
smoothing methods.
Since it is very costly to calculate the sim-
ilarities with all of the other words (one mil-
lion in our case), we used the following approx-
imation method that exploits the sparseness of
c(wi, fk). Similar methods were used in Pantel
and Lin (2002), Kazama et al (2009), and Pan-
tel et al (2009) as well. For a given word, wi,
we sort the contexts in descending order accord-
ing to c(wi, fk) and retrieve the top-L contexts.5
For each selected context, we sort the words in de-
scending order according to c(wi, fk) and retrieve
the top-M words (L = M = 1600).6 We merge
all of the words above as candidate words and cal-
culate the similarity only for the candidate words.
Finally, the top 500 similar words are output.
Note also that we used modified counts,
log(c(wi, fk)) + 1, instead of raw counts,
c(wi, fk), with the intention of alleviating the ef-
fect of strangely frequent dependencies, which can
be found in the Web data. In preliminary ex-
periments, we observed that this modification im-
proves the quality of the top 500 similar words as
reported in Terada et al (2004) and Kazama et al
(2009).
4In the case of EM clustering, the number of unique con-
texts, fk, was also set to one million instead of 100,000, fol-
lowing Kazama et al (2009).
5It is possible that the number of contexts with non-zero
counts is less than L. In that case, all of the contexts with
non-zero counts are used.
6Sorting is performed only once in the initialization step.
Table 1: Performance on siblings (Set A).
Measure MAP MP
@1 @5 @10 @20
JS 0.0299 0.197 0.122 0.0990 0.0792
PMI-cos 0.0332 0.195 0.124 0.0993 0.0798
Cls-JS (s1) 0.0319 0.195 0.122 0.0988 0.0796
Cls-JS (s2) 0.0295 0.198 0.122 0.0981 0.0786
Cls-JS (s1+s2) 0.0333 0.206 0.129 0.103 0.0841
BC 0.0334 0.211 0.131 0.106 0.0854
BCb (0.0002) 0.0345 0.223 0.138 0.109 0.0873
BCb (0.0016) 0.0356 0.242 0.148 0.119 0.0955
BCb (0.0032) 0.0325 0.223 0.137 0.111 0.0895
BCa (0.0016) 0.0337 0.212 0.133 0.107 0.0863
BCa (0.0362) 0.0345 0.221 0.136 0.110 0.0890
BCa (0.1) 0.0324 0.214 0.128 0.101 0.0825
without log(c(wi, fk)) + 1 modification
JS 0.0294 0.197 0.116 0.0912 0.0712
PMI-cos 0.0342 0.197 0.125 0.0987 0.0793
BC 0.0296 0.201 0.118 0.0915 0.0721
As for BCb, we assumed that all of the hyper-
parameters had the same value, i.e., ?k = ?. It
is apparent that an excessively large ? is not ap-
propriate because it means ignoring observations.
Therefore, ?must be tuned. The discounting value
of BCa is also tuned.
5.5 Results
Table 1 shows the results for Set A. The MAP and
the MPs at the top 1, 5, 10, and 20 are shown for
each similarity measure. As for BCb and BCa, the
results for the tuned and several other values for ?
are shown. Figure 1 shows the parameter tuning
for BCb with MAP as the y-axis (results for BCa
are shown as well). Figure 2 shows the same re-
sults with MPs as the y-axis. The MAP and MPs
showed a correlation here. From these results, we
can see that BCb surely improves upon BC, with
6.6% improvement in MAP and 14.7% improve-
ment in MP@1 when ? = 0.0016. BCb achieved
the best performance among the compared mea-
sures with this setting. The absolute discounting,
BCa, improved upon BC as well, but the improve-
ment was smaller than with BCb. Table 1 also
shows the results for the case where we did not
use the log-modified counts. We can see that this
modification gives improvements (though slight or
unclear for PMI-cos).
Because tuning hyperparameters involves the
possibility of overfitting, its robustness should be
assessed. We checked whether the tuned ? with
Set A works well for Set B. The results are shown
in Table 2. We can see that the best ? (= 0.0016)
found for Set A works well for Set B as well. That
is, the tuning of ? as above is not unrealistic in
252
 0.02
 0.022
 0.024
 0.026
 0.028
 0.03
 0.032
 0.034
 0.036
 1e-06  1e-05  0.0001  0.001  0.01  0.1  1
MA
P
? (log-scale)
BayesAbsolute Discounting
Figure 1: Tuning of ? (MAP). The dashed hori-
zontal line indicates the score of BC.
 0.04 0.06
 0.08 0.1
 0.12 0.14
 0.16 0.18
 0.2 0.22
 0.24 0.26
 1e-06  1e-05  0.0001  0.001  0.01
MP
? (log-scale)
 MP@1
 MP@5
 MP@10
 MP@20
 MP@30
 MP@40
Figure 2: Tuning of ? (MP).
practice because it seems that we can tune it ro-
bustly using a small subset of the vocabulary as
shown by this experiment.
Next, we evaluated the measures on Set C, i.e.,
the closed set data. The results are shown in Ta-
ble 3. For this set, we observed a tendency that
is different from Sets A and B. Cls-JS showed a
particularly good performance. BCb surely im-
proves upon BC. For example, the improvement
was 7.5% for MP@1. However, the improvement
in MAP was slight, and MAP did not correlate
well with MPs, unlike in the case of Sets A and
B.
We thought one possible reason is that the num-
ber of outputs, 500, for each word was not large
enough to assess MAP values correctly because
the average number of answers is 1,700 for this
dataset. In fact, we could output more than 500
words if we ignored the cost of storage. Therefore,
we also included the results for the case where
L = M = 3600 and N = 2, 000. Even with
this setting, however, MAP did not correlate well
with MPs.
Although Cls-JS showed very good perfor-
mance for Set C, note that the EM clustering
is very time-consuming (Kazama and Torisawa,
2008), and it took about one week with 24 CPU
cores to get one clustering result in our computing
environment. On the other hand, the preparation
Table 2: Performance on siblings (Set B).
Measure MAP MP
@1 @5 @10 @20
JS 0.0265 0.208 0.116 0.0855 0.0627
PMI-cos 0.0283 0.203 0.116 0.0871 0.0660
Cls-JS (s1+s2) 0.0274 0.194 0.115 0.0859 0.0643
BC 0.0295 0.223 0.124 0.0922 0.0693
BCb (0.0002) 0.0301 0.225 0.128 0.0958 0.0718
BCb (0.0016) 0.0313 0.246 0.135 0.103 0.0758
BCb (0.0032) 0.0279 0.228 0.127 0.0938 0.0698
BCa (0.0016) 0.0297 0.223 0.125 0.0934 0.0700
BCa (0.0362) 0.0298 0.223 0.125 0.0934 0.0705
BCa (0.01) 0.0300 0.224 0.126 0.0949 0.0710
Table 3: Performance on closed-sets (Set C).
Measure MAP MP
@1 @5 @10 @20
JS 0.127 0.607 0.582 0.566 0.544
PMI-cos 0.124 0.531 0.519 0.508 0.493
Cls-JS (s1) 0.125 0.589 0.566 0.548 0.525
Cls-JS (s2) 0.137 0.608 0.592 0.576 0.554
Cls-JS (s1+s2) 0.152 0.638 0.617 0.603 0.583
BC 0.131 0.602 0.579 0.565 0.545
BCb (0.0004) 0.133 0.636 0.605 0.587 0.563
BCb (0.0008) 0.131 0.647 0.615 0.594 0.568
BCb (0.0016) 0.126 0.644 0.615 0.593 0.564
BCb (0.0032) 0.107 0.573 0.556 0.529 0.496
L = M = 3200 and N = 2000
JS 0.165 0.605 0.580 0.564 0.543
PMI-cos 0.165 0.530 0.517 0.507 0.492
Cls-JS (s1+s2) 0.209 0.639 0.618 0.603 0.584
BC 0.168 0.600 0.577 0.562 0.542
BCb (0.0004) 0.170 0.635 0.604 0.586 0.562
BCb (0.0008) 0.168 0.647 0.615 0.594 0.568
BCb (0.0016) 0.161 0.644 0.615 0.593 0.564
BCb (0.0032) 0.140 0.573 0.556 0.529 0.496
for our method requires just an hour with a single
core.
6 Discussion
We should note that the improvement by using our
method is just ?on average,? as in many other NLP
tasks, and observing clear qualitative change is rel-
atively difficult, for example, by just showing ex-
amples of similar word lists here. Comparing the
results of BCb and BC, Table 4 lists the numbers
of improved, unchanged, and degraded words in
terms of MP@20 for each evaluation set. As can
be seen, there are a number of degraded words, al-
though they are fewer than the improved words.
Next, Figure 3 shows the averaged differences of
MP@20 in each 40,000 word-ID range.7 We can
observe that the advantage of BCb is lessened es-
7Word IDs are assigned in ascending order when we chose
the top one million words as described in Section 5.2, and
they roughly correlate with frequencies. So, frequent words
tend to have low-IDs.
253
Table 4: The numbers of improved, unchanged,
and degraded words in terms of MP@20 for each
evaluation set.
# improved # unchanged # degraded
Set A 755 2,585 400
Set B 643 2,610 404
Set C 3,153 3,962 1,738
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0 0.01
 0.02 0.03 0.04
 0.05 0.06 0.07
 0.08
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
Figure 3: Averaged Differences of MP@20 be-
tween BCb (0.0016) and BC within each 40,000
ID range (Left: Set A. Right: Set B. Bottom: Set
C).
pecially for low-ID words (as expected) with on-
average degradation.8 The improvement is ?on av-
erage? in this sense as well.
One might suspect that the answer words tended
to be low-ID words, and the proposed method is
simply biased towards low-ID words because of
its nature. Then, the observed improvement is a
trivial consequence. Table 5 lists some interest-
ing statistics about the IDs. We can see that BCb
surely outputs more low-ID words than BC, and
BC more than Cls-JS and JS.9 However, the av-
erage ID of the outputs of BC is already lower
than the average ID of the answer words. There-
fore, even if BCb preferred lower-ID words than
BC, it should not have the effect of improving
the accuracy. That is, the improvement by BCb
is not superficial. From BC/BCb, we can also see
that the IDs of the correct outputs did not become
smaller compared to the IDs of the system outputs.
Clearly, we need more analysis on what caused
the improvement by the proposed method and how
that affects the efficacy in real applications of sim-
ilarity measures.
The proposed Bayesian similarity measure out-
performed the baseline Bhattacharyya coefficient
8This suggests the use of different ?s depending on ID
ranges (e.g., smaller ? for low-ID words) in practice.
9The outputs of Cls-JS are well-balanced in the ID space.
Table 5: Statistics on IDs. (A): Avg. ID of an-
swers. (B): Avg. ID of system outputs. (C): Avg.
ID of correct system outputs.
Set A Set C
(A) 238,483 255,248
(B) (C) (B) (C)
Cls-JS (s1+s2) 282,098 176,706 273,768 232,796
JS 183,054 11,3442 211,671 201,214
BC 162,758 98,433 193,508 189,345
BCb(0.0016) 55,915 54,786 90,472 127,877
BC/BCb 2.91 1.80 2.14 1.48
and other well-known similarity measures. As
a smoothing method, it also outperformed a
naive absolute discounting. Of course, we can-
not say that the proposed method is better than
any other sophisticated smoothing method at this
point. However, as noted above, there has
been no serious attempt to assess the effect of
smoothing in the context of word similarity cal-
culation. Recent studies have pointed out that
the Bayesian framework derives state-of-the-art
smoothing methods such as Kneser-Ney smooth-
ing as a special case (Teh, 2006; Mochihashi et
al., 2009). Consequently, it is reasonable to re-
sort to the Bayesian framework. Conceptually,
our method is equivalent to modifying p(fk|wi)
as p(fk|wi) =
{
?(?0+a0)?(?k+c(wi,fk)+ 12 )
?(?0+a0+ 12 )?(?k+c(wi,fk))
}2
and
taking the Bhattacharyya coefficient. However,
the implication of this form has not yet been in-
vestigated, and so we leave it for future research.
Our method is the simplest one as a Bayesian
method. We did not employ any numerical opti-
mization or sampling iterations, as in a more com-
plete use of the Bayesian framework (Teh, 2006;
Mochihashi et al, 2009). Instead, we used the ob-
tained analytical form directly with the assump-
tion that ?k = ? and ? can be tuned directly by
using a simple grid search with a small subset of
the vocabulary as the development set. If substan-
tial additional costs are allowed, we can fine-tune
each ?k using more complete Bayesian methods.
We also leave this for future research.
In terms of calculation procedure, BCb has the
same form as other similarity measures, which is
basically the same as the inner product of sparse
vectors. Thus, it can be as fast as other similar-
ity measures with some effort as we described in
Section 4 when our aim is to calculate similarities
between words in a fixed large vocabulary. For ex-
ample, BCb took about 100 hours to calculate the
254
top 500 similar nouns for all of the one million
nouns (using 16 CPU cores), while JS took about
57 hours. We think this is an acceptable additional
cost.
The limitation of our method is that it can-
not be used efficiently with similarity measures
other than the Bhattacharyya coefficient, although
that choice seems good as shown in the experi-
ments. For example, it seems difficult to use the
Jensen-Shannon divergence as the base similar-
ity because the analytical form cannot be derived.
One way we are considering to give more flexi-
bility to our method is to adjust ?k depending on
external knowledge such as the importance of a
context (e.g., PMIs). In another direction, we will
be able to use a ?weighted? Bhattacharyya coeffi-
cient:
?
k ?(w1, fk)?(w2, fk)
?
p1k ? p2k, where
the weights, ?(wi, fk), do not depend on pik, as
the base similarity measure. The analytical form
for it will be a weighted version of BCb.
BCb can also be generalized to the case where
the base similarity is BCd(p1, p2) =
?K
k=1 pd1k ?
pd2k, where d > 0. The Bayesian analytical form
becomes as follows.
BCdb (w1, w2) =
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + d)?(?0 + b0 + d)
?
K
X
k=1
?(?k + c(w1, fk) + d)?(?k + c(w2, fk) + d)
?(?k + c(w1, fk))?(?k + c(w2, fk))
.
See Appendix A for the derivation. However, we
restricted ourselves to the case of d = 12 in this
study.
Finally, note that our BCb is different from
the Bhattacharyya distance measure on Dirichlet
distributions of the following form described in
Rauber et al (2008) in its motivation and analyti-
cal form:
p
?(??0)?(?
?
0)
q
Q
k ?(?
?
k)
q
Q
k ?(?
?
k)
?
Q
k ?((?
?
k + ?
?
k)/2)
?( 12
PK
k (?
?
k + ?
?
k))
. (9)
Empirical and theoretical comparisons with this
measure also form one of the future directions.10
7 Conclusion
We proposed a Bayesian method for robust distri-
butional word similarities. Our method uses a dis-
tribution of context profiles obtained by Bayesian
10Our preliminary experiments show that calculating sim-
ilarity using Eq. 9 for the Dirichlet distributions obtained by
Eq. 6 does not produce meaningful similarity (i.e., the accu-
racy is very low).
estimation and takes the expectation of a base sim-
ilarity measure under that distribution. We showed
that, in the case where the context profiles are
multinomial distributions, the priors are Dirichlet,
and the base measure is the Bhattacharyya coeffi-
cient, we can derive an analytical form, permitting
efficient calculation. Experimental results show
that the proposed measure gives better word simi-
larities than a non-Bayesian Bhattacharyya coeffi-
cient, other well-known similarity measures such
as Jensen-Shannon divergence and the cosine with
PMI weights, and the Bhattacharyya coefficient
with absolute discounting.
Appendix A
Here, we give the analytical form for the general-
ized case (BCdb ) in Section 6. Recall the following
relation, which is used to derive the normalization
factor of the Dirichlet distribution:
Z
?
Y
k
??
?
k?1
k d? =
Q
k ?(?
?
k)
?(??0)
= Z(?
?
)?1. (10)
Then, BCdb (w1, w2)
=
ZZ
???
Dir(?1|?
?
)Dir(?2|?
?
)
X
k
?d1k?d2k d?1 d?2
= Z(?
?
)Z(?
?
)?
ZZ
???
Y
l
??
?
l?1
1l
Y
m
??
?
m?1
2m
X
k
?d1k?d2k d?1 d?2
| {z }
A
.
Using Eq. 10, A in the above can be calculated as
follows:
=
Z
?
Y
m
??
?
m?1
2m
2
4
X
k
?d2k
Z
?
??
?
k+d?1
1k
Y
l?=k
??
?
l?1
1l d?1
3
5 d?2
=
Z
?
Y
m
??
?
m?1
2m
"
X
k
?d2k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
#
d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
Z
?
??
?
k+d?1
2k
Y
m ?=k
??
?
m?1
2m d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
?(?
?
k + d)
Q
m?=k ?(?
?
m)
?(??0 + d)
=
Q
?(?
?
l)
Q
?(?
?
m)
?(??0 + d)?(?
?
0 + d)
X
k
?(?
?
k + d)
?(??k)
?(?
?
k + d)
?(??k)
.
This will give:
BCdb (w1, w2) =
?(?
?
0)?(?
?
0)
?(??0 + d)?(?
?
0 + d)
K
X
k=1
?(?
?
k + d)?(?
?
k + d)
?(??k)?(?
?
k)
.
255
References
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bull. Calcutta Math. Soc.,
49:214?224.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. TR-10-98, Computer Science Group,
Harvard University.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20:273?297.
CRL. 2002. EDR electronic dictionary version 2.0
technical guide. Communications Research Labo-
ratory (CRL).
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of ACL 94.
Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1995. Contextual word similarity and estimation
from sparse data. Computer, Speech and Language,
9:123?152.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of ACL 97.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Gregory Grefenstette. 1994. Explorations In Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers.
Zellig Harris. 1954. Distributional structure. Word,
pages 146?142.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of
ACL-90, pages 268?275.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
and Masaki Murata. 2009. Generating a large-scale
analogy list using a probabilistic clustering based on
noun-verb dependency profiles. In Proceedings of
15th Annual Meeting of The Association for Natural
Language Processing (in Japanese).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768?774.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of ACL-IJCNLP 2009, pages 100?
108.
Masaki Murata, Qing Ma, Tamotsu Shirado, and Hi-
toshi Isahara. 2004. Database for evaluating ex-
tracted terms and tool for visualizing the terms. In
Proceedings of LREC 2004 Workshop: Computa-
tional and Computer-Assisted Terminology, pages
6?9.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP 2009, pages 938?947.
T. W. Rauber, T. Braun, and K. Berns. 2008. Proba-
bilistic distance measures of the Dirichlet and Beta
distributions. Pattern Recognition, 41:637?645.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
Tsubaki: An open search engine infrastructure for
developing new information access. In Proceedings
of IJCNLP 2008.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL 2006, pages 985?992.
Akira Terada, Minoru Yoshida, and Hiroshi Nakagawa.
2004. A tool for constructing a synonym dictionary
using context information. In IPSJ SIG Technical
Report (in Japanese), pages 87?94.
256
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 40?49,
Beijing, August 2010
A Look inside the Distributionally Similar Terms
Kow Kuroda
kuroda@nict.go.jp
Jun?ichi Kazama
kazama@nict.go.jp
National Institute of Information and Communications Technology (NICT)
Kentaro Torisawa
torisawa@nict.go.jp
Abstract
We analyzed the details of aWeb-derived
distributional data of Japanese nominal
terms with two aims. One aim is to
examine if distributionally similar terms
can be in fact equated with ?semanti-
cally similar? terms, and if so to what
extent. The other is to investigate into
what kind of semantic relations con-
stitute (strongly) distributionally similar
terms. Our results show that over 85%
of the pairs of the terms derived from
the highly similar terms turned out to
be semantically similar in some way.
The ratio of ?classmate,? synonymous,
hypernym-hyponym, and meronymic re-
lations are about 62%, 17%, 8% and 1%
of the classified data, respectively.
1 Introduction
The explosion of online text allows us to enjoy
a broad variety of large-scale lexical resources
constructed from the texts in the Web in an un-
supervised fashion. This line of approach was
pioneered by researchers such as Hindle (1990),
Grefenstette (1993), Lee (1997) and Lin (1998).
At the heart of the approach is a crucial working
assumption called ?distributional hypothesis,? as
with Harris (1954). We now see an impressive
number of applications in natural language pro-
cessing (NLP) that benefit from lexical resources
directly or indirectly derived from this assump-
tion. It seems that most researchers are reason-
ably satisfied with the results obtained thus far.
Does this mean, however, that the distribu-
tional hypothesis was proved to be valid? Not
necessarily: while we have a great deal of con-
firmative results reported in a variety of research
areas, but we would rather say that the hypothe-
sis has never been fully ?validated? for two rea-
sons. First, it has yet to be tested under the pre-
cise definition of ?semantic similarity.? Second,
it has yet to be tested against results obtained at
a truly large scale.
One of serious problems is that we have seen
no agreement on what ?similar terms? mean and
should mean. This paper intends to cast light
on this unsolved problem through an investiga-
tion into the precise nature of lexical resources
constructed under the distributional hypothesis.
The crucial question to be asked is, Can distri-
butionally similar terms really be equated with
semantically similar terms or not? In our investi-
gation, we sought to recognize what types of se-
mantic relations can be found for pairs of terms
with high distributional similarity, and see where
the equation of distributional similarity with se-
mantic similarity fails. With this concern, this
paper tried to factor out as many components of
semantic similarity as possible. The effort of fac-
torization resulted in the 18 classes of semantic
(un)relatedness to be explained in ?2.3.1. Such
factorization is a necessary step for a full valida-
tion of the hypothesis. To meet the criterion of
testing the hypothesis at a very large scale, we
analyzed 300,000 pairs of distributionally simi-
lar terms. Details of the data we used are given
in ?2.2.
This paper is organized as follows. In ?2, we
present our method and data we used. In ?3, we
present the results and subsequent analysis. In
?4, we address a few remaining problems. In ?5,
we state tentative conclusions.
40
2 Method and Data
2.1 Method
The question we need to address is how many
subtypes of semantic relation we can identify in
the highly similar terms. We examined the ques-
tion in the following procedure:
(1) a. Select a set of ?base? terms B.
b. Use a similarity measure M to con-
struct a list of n terms T = [ti,1, ti,2,. . . ,
ti, j, . . . , ti,n] where ti, j denotes the j-
th most similarity term in T against
bi ? B. P(k) are pairs of bi and ti,k, i.e.,
the k-th most similar term to bi.
c. Human raters classify a portion Q of
the pairs in P(k) with reference to
a classification guideline prepared for
the task.
Note that the selection of base set B can be
independent of the selection of T . Note also that
T is indexed by terms in B. To encode this, we
write: T [bi] = [ti,1, ti,2,. . . , ti, j, . . . , ti,n].
2.2 Data
For T , we used Kazama?s nominal term cluster-
ing (Kazama and Torisawa, 2008; Kazama et al,
2009). In this data, base set B for T is one mil-
lion terms defined by the type counts of depen-
dency relations, which is roughly equated with
the ?frequencies? of the terms. Each base term
in B is associated with up to 500 of the most dis-
tributionally similar terms. This defines T .
For M, we used the Jensen-Shannon diver-
gence (JS-divergence) base on the probability
distributions derived by an EM-based soft clus-
tering (Kazama and Torisawa, 2008). For con-
venience, some relevant details of the data con-
struction are described in Appendix A, but in a
nutshell, we used dependency relations as dis-
tributional information. This makes our method
comparable to that used in Hindle (1990). The
statistics of the distributional data used were as
follows: roughly 920 million types of depen-
dency relations1) were automatically acquired
1)The 920 million types come in two kinds of context
triples: 590 million types of (t, p,v) and 320 million types
from a large-scale Japanese Web-corpus called
the Tsubaki corpus (Shinzato et al, 2008) which
consists of roughly 100 million Japanese pages
with six billion sentences. After excluding hapax
nouns, we had about 33 million types of nouns
(in terms of string) and 27 million types of verbs.
These nouns were ranked by type count of the
two context triples, i.e., (t, p,v) and (n?, p?, t). B
was determined by selecting the top one million
terms with the most variations of context triples.
2.2.1 Sample of T [b]
For illustration, we present examples of the
Web-derived distributional similar terms. (2)
shows the 10 most distributionally similar terms
(i.e., [t1070,1, t1070,2, . . . , t1070,10] in T (b1070))
where b1070 = ????? (piano) is the 1070-th
term in B. Likewise, (3) shows the 10 most dis-
tributionally similar terms [t38555,1, t38555,2, . . . ,
t38555,10] in T (b38555)) where b38555 = ??????
???? (Tchaikovsky) is the 38555-th term in B.
(2) 10 most similar to ?????
1. ?????? (Electone; electronic or-
gan) [-0.322]
2. ????? (violin) [-0.357]
3. ?????? (violin) [-0.358]
4. ??? (cello) [-0.358]
5. ?????? (trumpet) [-0.377]
6. ??? (shamisen) [-0.383]
7. ???? (saxophone) [-0.39]
8. ???? (organ) [-0.392]
9. ?????? (clarinet) [-0.394]
10. ?? (erh hu) (-0.396)
(3) 10 most similar to ??????????
1. ????? (Brahms) [-0.152]
2. ????? (Schumann) [-0.163]
3. ???????? (Mendelssohn) [-
0.166]
4. ????????? (Shostakovich) [-
0.178]
5. ????? (Sibelius) [-0.18]
of (t, p?,n?), where t denotes the target nominal term, p a
postposition, v a verb, and n? a nominal term that follows t
and p?, i.e., ?t-no? analogue to the English ?of t.?
41
6. ???? (Haydn) [-0.181]
7. ???? (Ha?ndel) [-0.181]
8. ???? (Ravel) [-0.182]
9. ?????? (Schubert) [-0.187]
10. ??????? (Beethoven) [-0.19]
For construction of P(k), we had the follow-
ing settings: i) k = 1,2; and ii) for each k, we
selected the 150,000 most frequent terms (out of
one million terms) with some filtering specified
below. Thus, Q was 300,000 pairs whose base
terms are roughly the most frequent 150,000
terms in B with filtering and targets are terms
k = 1 or k = 2.
2.2.2 Filtering of terms in B
For filtering, we excluded the terms of B with
one of the following properties: a) they are in an
invalid form that could have resulted from parse
errors; b) they have regular ending (e.g., -??
, -? [event], -? [time or when], -?? [thing or
person], -? [thing], -? [person]). The reason
for the second is two-fold. First, it was desir-
able to reduce the ratio of the class of ?class-
mates with common morpheme,? which is ex-
plained in ?2.3.2, whose dominance turned out to
be evident in the preliminary analysis. Second,
the semantic property of the terms in this class
is relatively predictable from their morphology.
That notwithstanding, this filtering might have
had an undesirable impact on our results, at least
in terms of representativeness. Despite of this,
we decided to place priority on collecting more
varieties of classes.
The crucial question is, again, whether dis-
tributionally similar terms can really be equated
with semantically similar terms. Put differently,
what kinds of terms can we find in the sets con-
structed using distributionally similarity? We
can confirm the hypothesis if the most of the
term pairs are proved to be semantically simi-
lar for most sets of terms constructed based on
the distributional hypothesis. To do this, how-
ever, we need to clarify what constitutes seman-
tic similarity. We will deal with this prerequisite.
2.3 Classification
2.3.1 Factoring out ?semantic similarity?
Building on lexicographic works like Fell-
baum (1998) and Murphy (2003), we assume
that the following are the four major classes
of semantic relation that contribute to semantic
similarity between two terms:
(4) a. ?synonymic? relation (one can substi-
tute for another on an identity basis).
Examples are (Microsoft, MS).
b. ?hypernym-hyponym? relation be-
tween two terms (one can substitute
for another on un underspecifica-
tion/abstraction basis). Examples are
(guys, players)
c. ?meronymic? (part-whole) relation be-
tween two terms (one term can be a
substitute for another on metonymic
basis). Examples are (bodies, players)
[cf. All the players have strong bodies]
d. ?classmate? relation between two
terms, t1 and t2, if and only if (i) they
are not synonymous and (ii) there is a
concrete enough class such that both t1
and t2 are instances (or subclasses).2)
For example, (China, South Korea)
[cf. (Both) China and South Korea
are countries in East Asia], (Ford, Toy-
ota) [cf. (Both) Ford and Toyota are
top-selling automotive companies] and
(tuna, cod) [cf. (Both) tuna and cod
are types of fish that are eaten in the
Europe] are classmates.
For the substitution, the classmate class behaves
somewhat differently. In this case, one term can-
not substitute for another for a pair of terms. It
is hard to find out the context in which pairs like
(China, South Korea), (Ford, Toyota) and (tuna,
cod) can substitute one another. On the other
hand, substitution is more or less possible in the
other three types. For example, a synonymic pair
of (MS, Microsoft) can substitute for one another
in contexts like Many people regularly complain
2)The proper definition of classmates is extremely hard
to form. The authors are aware of the incompleteness of
their definition, but decided not to be overly meticulous.
42
pair of forms
pair of 
meaningful 
terms
x: pair with a 
meaningless 
form
u: pair of terms 
in no conceivable 
semantic relation
r: pair of terms in 
a conceivable 
semantic relation
s:* synonymous 
pair in the 
broadest sense
a: acronymic 
pair
v: allographic 
pair
n: alias pair
e: erroneous 
pair
f: quasi-
erroneous pair
v*: notational 
variation of the 
same term
m: misuse pair
o: pair in other, 
unindentified 
relation
h: hypernym-
hyponym pair
k**: classmate 
in the broadest 
sense
k*: classmate 
without obvious 
contrastiveness
c*: contrastive 
pairs d: antonymic 
pair
c: contrastive 
pair without 
antonymity
p: meronymic 
pair
t: pair of terms 
with inherent 
temporal order
y: undecidable
k: classmate 
without shared 
morpheme
w: classmate 
with shared 
morpheme
s: synonymous 
pair of different 
terms
Figure 1: Classification tree for semantic relations used
about products { i. MS; ii. Microsoft }. A
hypernym-hyponym pair of (guys, players) can
substitute in contexts like We have dozens of ex-
cellent { i. guys; ii. players } on our team. A
meronymic pair of (bodies, players) can substi-
tute for each other in contexts like They had a few
of excellent { i. bodies; ii. players} last year.
2.3.2 Classification guidelines
The classification guidelines were specified
based on a preliminary analysis of 5,000 ran-
domly selected examples. We asked four annota-
tors to perform the task. The guidelines were fi-
nalized after several revisions. This revision pro-
cess resulted in a hierarchy of binary semantic
relations as illustrated in Figure 1, which sub-
sumes 18 types as specified in (5). The essen-
tial division is made at the fourth level where
we have s* (pairs of synonyms in the broadest
sense) with two subtypes, p (pairs of terms in
the ?part-whole? relation), h (pairs of terms in
the ?hypernym-hyponym? relation), k** (pairs
of terms in the ?classmate? relation), and o (pairs
of terms in any other relation). Note that this
system includes the four major types described
in (4). The following are some example pairs of
Japanese terms with or without English transla-
tions:
(5) s: synonymic pairs (subtype of s*) in
which the pair designates the same en-
tity, property, or relation. Examples
are: (??, ??) [both mean root], (?
?????,????) [(supporting mem-
ber, cooperating member)], (????
?, ?????) [(invoker of the pro-
cess, parent process)], (????????
?, ?????) [(venture business, ven-
ture)], (????, ???????) [(op-
posing hurler, opposing pitcher)], (?
?, ???) [(medical history, anamne-
ses)],
n: alias pairs (subtype of s*) in which
one term of the pair is the ?alias? of
the other term. Examples are (Steve
Jobs, founder of Apple, Inc.), (Barak
Obama, US President), (???,????
???), (???, ????)
43
a: acronymic pair (subtype of s*) in
which one term of the pair is the
acronym of of the other term. Ex-
amples are: (DEC, Digital Equip-
ment), (IBM, International Business
Machine) (Microsoft ?, MS ?), (??
?, ????), (????, ??),
v: allographic pairs (subtype of s*) in
which the pair is the pair of two forms
of the same term. Examples are:
(convention centre, convention cen-
ter), (colour terms, color terms), (??
???, ????), (????, ????),
(??????????, ????????
???), (??, ??), (????, ????
), (???, ??), (????, ?????),
(??, ??), (????, ????)
h: hypernym-hyponym pair in which one
term of the pair designates the ?class?
of the other term. Examples (or-
der is irrelevant) are: (thesaurus, Ro-
get?s), (?????, ?????) [(search
tool, search software)], (????, ??
??) [(unemployment measures, em-
ployment measures)], (??, ????
) [(business conditions, employment
conditions)], (???????, ???)
[(festival, music festival)], (???, ?
????) [(test agent, pregnancy test)],
(??????, ???) [(cymbidium, or-
chid)], (????,?????) [(company
logo, logo)], (????,????) [(mys-
tical experiences, near-death experi-
ences)]
p: meronymic pair in which one term of
the pair designates the ?part? of the
other term. Examples (order is ir-
relevant) are: (????, ??) [(earth,
sea)], (??, ??) [(affirmation, ad-
mission)], (??, ????) [(findings,
research progress)], (????????
?, ?????) [(solar circuit system,
exterior thermal insulation method)],
(?????, ??) [(Provence, South
France)],
k: classmates not obviously contrastive
without common morpheme (subtype
of k*). Examples are: (????, ???
?) [(self-culture, training)], (????
, ??) [(sub-organs, services)], (???
??,??????) [(Dongba alphabets,
hieroglyphs)], (Tom, Jerry)
w: classmates not obviously contrastive
with common morpheme (subtype of
k*). Examples are: (????, ????)
[(gas facilities, electric facilities)], (?
???,???) [(products of other com-
pany, aforementioned products)], (??
?, ???) [(affiliate station, local sta-
tion)], (???,????) [(Niigata City,
Wakayama City)], (?????, ???
??) [(Sinai Peninsula, Malay Penin-
sula)],
c: contrastive pairs without antonymity
(subtype of c*). Examples are: (???
??, ????) [(romanticism, natural-
ism)], (????????, ???????
????) [(mobile user, internet user)],
(???, PS2?), [(bootleg edition, PS2
edition)]
d: antonymic pairs = contrastive pairs
with antonymity (subtype of c*). Ex-
amples are: (??, ??) [(bond-
ing, disintegration)], (???, ???)
[(gravel road, pavement)], (??, ??
) [(west walls, east walls)], (???,
????) [(daughter and son-in-law,
son and daughter-in-law)], (??, ??
) [(tax-exclusive prices, tax-inclusive
prices)], (??????, ????????
) [(front brake, rear brake)], (????
??, ???????) [(tag-team match,
solo match)], (???, ???) [(wip-
ing with dry materials, wiping with
wet materials)], (??????, ??)
[(sleeveless, long-sleeved)]
t: pairs with inherent temporal order
(subtype of c*). Examples are: (??
?, ???) [(harvesting of rice, plant-
ing of rice)], (????, ????) [(day
of departure, day of arrival)], (???
?, ????) [(career decision, career
selection)], (???, ????) [(catnap,
stay up)], (??, ??) [(poaching, con-
44
traband trade)], (??, ??) [(surren-
der, dispatch of troops)], (???, ?
??) [(2nd-year student, 3rd-year stu-
dent)]
e: erroneous pairs are pairs in which
one term of the pair seems to suffer
from character-level input errors, i.e.
?mistypes.? Examples are: (???, ?
??), (???????, ???????),
(???, ???)
f: quasi-erroneous pair is a pair of terms
with status somewhat between v and e.
Examples (order is irrelevant) are: (?
???, ????) [(supoito, supoido)],
(??????, ??????) [(goru-
fubaggu, gorufugakku)], (?????,
?????) [(biggu ban, bikku ban)],
m: misuse pairs in which one term of the
pair seems to suffer from ?mistake? or
?bad memory? of a word (e is caused
by mistypes but m is not). Examples
(order is irrelevant) are: (???, ???
), (?????, ?????), (??, ??),
(???, ???), (??, ??)
o: pairs in other unidentified relation in
which the pair is in some semantic re-
lation other than s*, k**, p, h, and
u. Examples are: (??, ???) [(ul-
terior motives, possessive feeling)], (?
????,?????) [(theoretical back-
ground, basic concepts)], (?????
???, ????) [(Alexandria, Sira-
cusa)],
u: unrelated pairs in which the pair is in
no promptly conceivable semantic re-
lation. Examples are: (???, ????
) [(noncontact, high resolution)], (??
, ????) [(imitation, overinterpreta-
tion)],
x: nonsensical pairs in which either of the
pair is not a proper term of Japanese.
(but it can be a proper name with very
low familiarity). Examples are: (???
?, ???), (????, ??), (??, ??
?), (???, ??), (ma, ?????)
y: unclassifiable under the allowed time
limit.3) Examples are: (???, ???
???), (fj, ???), (??, ??),
Note that some relation types are symmetric
and others are asymmetric: a, n, h, p, and t (and
e, f, and m, too) are asymmetric types. This
means that the order of the pair is relevant, but it
was not taken into account during classification.
Annotators were asked to ignore the direction of
pairs in the classification task. In the finaliza-
tion, we need to reclassify these to get them in
the right order.
2.3.3 Notes on implicational relations
The overall implicational relation in the hier-
archy in Figure 1 is the following:
(6) a. s, k**, p, h, and o are supposed to be
mutually exclusive, but the distinction
is sometimes obscure.4)
b. k** has two subtypes: k* and c*.
c. k and w are two subtypes k*.
d. c, d and t three subtypes of c*.
To resolve the issue of ambiguity, priority was
set among the labels so that e, f < v < a < n <
p < h < s < t < d < c < w < k < m < o < u <
x < y, where the left label is more preferred over
the right. This guarantees preservation of the im-
plicational relationship among labels.
2.3.4 Notes on quality of classification
We would like to add a remark on the quality.
After a quick overview, we reclassified o and w,
because the first run of the final task ultimately
produced a resource of unsatisfactory quality.
Another note on inter-annotator agreement:
originally, the classification task was designed
and run as a part of a large-scale language re-
source development. Due to its overwhelming
size, we tried to make our development as effi-
cient as possible. In the final phase, we asked
3)We did not ask annotators to check for unknown terms.
4)To see this, consider pairs like (large bowel, bowel),
(small bowel, bowel). Are they instances of p or h? The
difficulty in the distinction between h and p becomes harder
in Japanese due to the lack of plurality marking: cases
like (Mars, heavenly body) (a case of h) and (Mars, heav-
enly bodies) (a p case) cannot be explicitly distinguished.
In fact, the Japanese term ?? can mean both ?heavenly
body? (singular) and ?heavenly bodies? (plural).
45
Table 1: Distribution of relation types
rank count ratio (%) cum. (%) class label
1 108,149 36.04 36.04 classmates without common morpheme k
2 67,089 22.35 58.39 classmates with common morpheme w
3 26,113 8.70 67.09 synonymic pairs s
4 24,599 8.20 75.29 hypernym-hyponym pairs h
5 20,766 6.92 82.21 allographic pairs v
6 18,950 6.31 88.52 pairs in other ?unidentified? relation o
7 12,383 4.13 92.65 unrelated pairs u
8 8,092 2.70 95.34 contrastive pairs without antonymity c
9 3,793 1.26 96.61 pairs with inherent temporal order t
10 3,038 1.01 97.62 antonymic pairs d
11 2,995 1.00 98.62 meronymic pairs p
12 1,855 0.62 99.23 acronymic pairs a
13 725 0.24 99.48 alias pairs n
14 715 0.24 99.71 erroneous pairs e
15 397 0.13 99.85 misuse pairs m
16 250 0.08 99.93 nonsensical pairs x
17 180 0.06 99.99 quasi-erroneous pairs f
18 33 0.01 100.00 unclassified y
17 annotators to classify the data with no over-
lap. Ultimately we obtained results that deserve
a detailed report. This history, however, brought
us to an undesirable situation: no inter-annotator
agreement is calculable because there was no
overlap in the task. This is why no inter-rater
agreement data is now available.
3 Results
Table 1 summarizes the distribution of relation
types with their respective ranks and proportions.
The statistics suggests that classes of e, f, m, x,
and y can be ignored without risk.
3.1 Observations
We noticed the following. Firstly, the largest
class is the class of classmates, narrowly defined
or broadly defined. The narrow definition of the
classmates is the conjunction of k and w, which
makes 58.39%. The broader definition of class-
mates, k**, is the union of k, w, c, d and t, which
makes 62.10%. This confirms the distributional
hypothesis.
The second largest class is the narrowly de-
fined synonymous pairs s. This is 8.7% of the
total, but the general class of synonymic pairs,
s* as the union of s, a, n, v, e, f, and m, makes
16.91%. This comes next to h and w. Notice
also that the union of k** and s* makes 79.01%.
The third largest is the class of terms in
hypernym-hyponym relations. This is 8.20% of
the total. We are not sure if this is large or small.
These results look reasonable and can be
seen as validation of the distributional hypothe-
sis. But there is something uncomfortable about
the the fourth and fifth largest classes, pairs in
?other? relation and ?unrelated? pairs, which
make 6.31% and 4.13% of the total, respectively.
Admittedly, 6.31% are 4.13% are not very large
numbers, but it does not guarantee that we can
ignore them safely. We need a closer examina-
tion of these classes and return to this in ?4.
3.2 Note on allography in Japanese
There are some additional notes: the rate of al-
lographic pairs [v] (6.92%) is rather high.5) We
suspect that this ratio is considerably higher than
the similar results that are to be expected in other
5)Admittedly, 6.92% is not a large number in an absolute
value, but it is quite large for the rate of allographic pairs.
46
languages. In fact, the range of notational varia-
tions in Japanese texts is notoriously large. Many
researchers in Japanese NLP became to be aware
of this, by experience, and claim that this is one
of the causes of Japanese NLP being less effi-
cient than NLP in other (typically ?segmented?)
languages. Our result revealed only the allogra-
phy ratio in nominal terms. It is not clear to what
extent this result is applied to the notional varia-
tions on predicates, but it is unlikely that predi-
cates have a lesser degree of notational variation
than nominals. At the least, informal analysis
suggests that the ratio of allography is more fre-
quent and has more severe impacts in predicates
than in nominals. So, it is very unlikely that we
had a unreasonably high rate of allography in our
data.
3.3 Summary of the results
Overall, we can say that the distributional hy-
pothesis was to a great extent positively con-
firmed to a large extent. Classes of classmates
and synonymous pairs are dominant. If the side
effects of filtering described in ?2.2.2 are ig-
nored, nearly 88% (all but o, u, m, x, and y)
of the pairs in the data turned out to be ?se-
mantically similar? in the sense they are clas-
sified into one of the regular semantic relations
defined in (5). While the status of the inclusion
of hypernym-hyponym pairs in classes of seman-
tically similar terms could be controversial, this
result cannot be seen as negative.
One aspect somewhat unclear in the results we
obtained, however, is that highly similar terms
in our data contain such a number of pairs in
unidentifiable relation. We will discuss this in
more detail in the following section.
4 Discussion
4.1 Limits induced by parameters
Our results have certain limits. We specify those
here.
First, our results are based on the case of
k = 1, 2 for P(k). This may be too small and
it is rather likely that we did not acquire results
with enough representativeness. For more com-
plete results, we need to compare the present re-
sults under larger k, say k = 4, 8, 16, . . .. We did
not do this, but we have a comparable result in
one of the preliminary studies. In the prepara-
tion stage, we classified samples of pairs whose
base term is at frequency ranks 13?172, 798?
1,422 and 12,673?15,172 where k = 1, 2, 3, . . . ,
9, 10.6) Table 2 shows the ratios of relation types
for this sample (k = 1, 2, 4, 8, 10).
Table 2: Similarity rank = 1, 2, 4, 8, 10
rank 1 2 4 8 10
v 18.13 10.48 3.92 2.51 1.04
o 17.08 21.24 26.93 28.24 29.56
w 13.65 13.33 14.30 12.19 12.75
s 11.74 9.14 7.05 4.64 4.06
u 11.07 16.48 17.63 20.79 20.87
h 10.50 10.29 11.17 12.96 10.20
k 7.82 8.38 7.84 7.74 8.22
d 2.58 2.00 1.57 1.16 0.85
p 2.00 1.14 1.08 1.35 1.79
c 1.43 1.05 1.27 1.35 1.89
a 1.05 1.33 0.88 0.39 0.57
x 1.05 1.14 1.27 1.64 2.08
t 0.29 0.19 0.20 0.39 0.47
f 0.10 0.10 0.00 0.10 0.09
m 0.00 0.10 0.20 0.00 0.19
#item 1,048 1,050 1,021 1,034 1,059
From Table 2, we notice that: as similarity
rank decreases, (i) the ratios of v, s, a, and d
decrease monotonically, and the ratios of v and s
decrease drastically; (ii) the ratios of o, u, and x
increases monotonically, and the ratio of o and u
increases considerably; and while (iii) the ratios
of h, k, p, w, m, and f seem to be constant. But
it is likely that the ratios of h, k, p, w, m, and f
change at larger k, say 128, 256.
Overall, however, this suggests that the differ-
ence in similarity rank has the greatest impact
on s* (recall that s and v are subtypes of s*),
o, and u, but not so much on others. Two ten-
dencies can be stated: first, terms at lower sim-
ilarity ranks become less synonymous. Second,
6)The frequency/rank in B was measured in terms of the
count of types of dependency relation.
47
the relationships among terms at lower similar-
ity ranks become more obscure. Both are quite
understandable.
There are, however, two caveats concerning
the data in Table 2, however. First, the 15 la-
bels used in this preliminary task are a subset of
the 18 labels used in the final task. Second, the
definitions of some labels are not completely the
same even if the same labels are used (this is why
we have this great of a ratio of o in Table 2. We
must admit, therefore, that no direct comparison
is possible between the data in Tables 1 and 2.
Second, it is not clear if we made the best
choices for clustering algorithm and distribu-
tional data. For the issue of algorithm, there
are too many clustering algorithms and it is hard
to reasonably select candidates for comparison.
We do, however, plan to extend our evaluation
method to other clustering algorithms. Cur-
rently, one of such options is Bayesian cluster-
ing. We are planning to perform some compar-
isons.
For the issue of what kind of distributional in-
formation to use, many kinds of distributional
data other than dependency relation are avail-
able. For example, simple co-occurrences within
a ?window? are a viable option. With a lack
of comparison, however, we cannot tell at the
present what will come about if another kind of
distributional data was used in the same cluster-
ing algorithm.
4.2 Possible overestimation of hypernyms
A closer look suggests that the ratio of
hypernym-hyponym pairs was somewhat overes-
timated. This is due to the algorithm used in our
data construction. It was often the case that head
nouns were extracted as bare nouns from com-
plex, much longer noun phrases, sometimes due
to the extraction algorithms or parse errors. This
resulted in accidental removal of modifiers be-
ing attached to head nouns in their original uses.
We have not yet checked how often this was the
case. We are aware that this could have resulted
in the overestimation of the ratio of hypernymic
relations in our data.
4.3 Remaining issues
As stated, the fourth largest class, roughly 6.31%
of the total, is that of the pairs in the ?other?
unidentified relation [o]. In our setting, ?other?
means that it is in none among the synonymous,
classmate, part-whole or hypernym-hyponym re-
lation. A closer look into some examples of
o suggest that they are pairs of terms with ex-
tremely vague association or contrast.
Admittedly, 6.31% is not a large number, but
its ratio is comparable with that of the allo-
graphic pairs [v], 6.92%. We have no explana-
tion why we have this much of an unindenfiable
kind of semantic relation distinguished from un-
related pairs [u]. All we can say now is that we
need further investigation into it.
u is not as large as o, but it has a status similar
to o. We need to know why this much amount of
this kind of pairs. A possible answer would be
that they are caused by parse errors, directly or
indirectly.
5 Conclusion
We analyzed the details of the Japanese nominal
terms automatically constructed under the ?dis-
tributional hypothesis,? as in Harris (1954). We
had two aims. One aim was to examine to see
if what we acquire under the hypothesis is ex-
actly what we expect, i.e., if distributional sim-
ilarity can be equated with semantic similarity.
The other aim was to see what kind of seman-
tic relations comprise a class of distributionally
similar terms.
For the first aim, we obtained a positive result:
nearly 88% of the pairs in the data turned out to
be semantically similar under the 18 criteria de-
fined in (5), which include hypernym-hyponym,
meronymic, contrastive, and synonymic rela-
tions. Though some term pairs we evaluated
were among none of these relations, the ratio of
o and u in sum is about 14% and within the ac-
ceptable range.
For the second aim, our result revealed that
the ratio of the classmates, synonymous, rela-
tion, hypernym-hyponym, and meronymic rela-
tions are respectively about 62%, 17%, 8% and
1% of the classified data.
48
Overall, these results suggest that automatic
acquisition of terms under the distributional hy-
pothesis give us reasonable results.
A Clustering of one million nominals
This appendix provides some details on how the
clustering of one million nominal terms was per-
formed.
To determine the similarity metric of a pair of
nominal terms (t1, t2), Kazama et al (2009) used
the Jensen-Shannon divergence (JS-divergence)
DJS(p||q) = 12D(p||M) + 12D(q||M), where pand q are probability distributions, and D =
?i p(i)log p(i)q(i) (Kullback-Leibler divergence, or
KL-divergence) of p and q, and M = 12(p+ q).We obtained p and q in the following way.
Instead of using raw distribution, Kazama et
al. (2009) applied smoothing using EM algo-
rithm (Rooth et al, 1999; Torisawa, 2001). In
Torisawa?s model (2001), the probability of the
occurrence of the dependency relation ?v,r,n? is
defined as:
P(?v,r, t?) =def ?
a?A
P(?v,r?|a)P(t|a)P(a),
where a denotes a hidden class of ?v,r? and term
t. In this equation, the probabilities P(?v,r?|a),
P(t|a), and P(a) cannot be calculated directly
because class a is not observed in a given depen-
dency data. The EM-based clustering method
estimates these probabilities using a given cor-
pus. In the E-step, the probability P(a|?v,r?)
is calculated. In the M-step, the probabilities
P(?v,r?|a), P(t|a), and P(a) are updated until
the likelihood is improved using the results of
the E-step. From the results of this EM-based
clustering method, we can obtain the probabili-
ties P(?v,r?|a), P(t|a), and P(a) for each ?v,r?, t,
and a. Then, P(a|t) is calculated by the follow-
ing equation:
P(a|t) = P(t|a)P(a)?a?AP(t|a)P(a) .
The distributional similarity between t1 and t2
was calculated by the JS divergence between
P(a|t1) and P(a|t2).
References
Fellbaum, C., ed. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Grefenstette, G. 1993. Automatic thesaurus gener-
ation from raw text using knowledge-poor tech-
niques. In In Making Sense of Words: The 9th
Annual Conference of the UW Centre for the New
OED and Text Research.
Harris, Z. S. 1954. Distributional structure. Word,
10(2-3):146?162. Reprinted in Fodor, J. A and
Katz, J. J. (eds.), Readings in the Philosophy
of Language, pp. 33?49. Englewood Cliffs, NJ:
Prentice-Hall.
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90,
pp. 268?275, Pittsburgh, PA.
Kazama, J. and K. Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of ACL-2008: HLT, pp. 407?415.
Kazama, J., S. De Saeger, K. Torisawa, and M. Mu-
rata. 2009. Generating a large-scale analogy list
using a probabilistic clustering based on noun-
verb dependency profiles. In Proceedings of the
15th Annual Meeting of the Association for Natu-
ral Language Processing. [in Japanese].
Lee, L. 1997. Similarity-Based Approaches to Natu-
ral Language Processing. Unpublished Ph.D. the-
sis, Harvard University.
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-
98, Montreal, Canda, pages 768?774.
Murphy, M. L. 2003. Semantic Relations and the
Lexicon. Cambridge University Press, Cambridge,
UK.
Rooth, M., S. Riezler, D. Presher, G. Carroll, and
F. Beil. 1999. Inducing a semantically annotated
lexicon via em-based clustering. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics, pp. 104?111.
Shinzato, K., T. Shibata, D. Kawahara, C. Hashimoto,
and S. Kurohashi. 2008. TSUBAKI: An open
search engine infrastructure for developing new
information access. In Proceedings of IJCNLP
2008.
Torisawa, K. 2001. An unsupervised method for
canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS), pp. 211?
218.
49
