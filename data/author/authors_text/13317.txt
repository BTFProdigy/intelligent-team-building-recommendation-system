Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 186?191,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Multimodal Annotation of Conversational Data
P. Blache1, R. Bertrand1, B. Bigi1, E. Bruno3, E. Cela6, R. Espesser1, G. Ferr?4, M. Guardiola1, D. Hirst1,
E.-P. Magro6, J.-C. Martin2, C. Meunier1, M.-A. Morel6, E. Murisasco3, I Nesterenko1, P. Nocera5,
B. Pallaud1, L. Pr?vot1, B. Priego-Valverde1, J. Seinturier3, N. Tan2, M. Tellier1, S. Rauzy1
(1) LPL-CNRS-Universit? de Provence (2) LIMSI-CNRS-Universit? Paris Sud
(3) LSIS-CNRS-Universit? de Toulon (4) LLING-Universit? de Nantes
(5) LIA-Universit? d?Avignon (6) RFC-Universit? Paris 3
blache@lpl-aix.fr
Abstract
We propose in this paper a broad-coverage
approach for multimodal annotation of
conversational data. Large annotation pro-
jects addressing the question of multimo-
dal annotation bring together many dif-
ferent kinds of information from different
domains, with different levels of granula-
rity. We present in this paper the first re-
sults of the OTIM project aiming at deve-
loping conventions and tools for multimo-
dal annotation.
1 Introduction
We present in this paper the first results of the
OTIM1 project aiming at developing conventions
and tools for multimodal annotation. We show
here how such an approach can be applied in the
annotation of a large conversational speech cor-
pus.
Before entering into more details, let us men-
tion that our data, tools and conventions are des-
cribed and freely downlodable from our website
(http ://www.lpl-aix.fr/ otim/).
The annotation process relies on several tools
and conventions, most of them elaborated within
the framework of the project. In particular, we pro-
pose a generic transcription convention, called En-
riched Orthographic Trancription, making it pos-
sible to annotate all specific pronunciation and
speech event, facilitating signal alignment. Dif-
ferent tools have been used in order to prepare
or directly annotate the transcription : grapheme-
phoneme converter, signal alignment, syllabifica-
tion, prosodic analysis, morpho-syntactic analysis,
chunking, etc. Our ambition is to propose a large
corpus, providing rich annotations in all the dif-
1OTIM stands for Outils pour le Traitement de l?Informa-
tion Multimodale (Tools for Multimodal Annotation). This
project in funded by the French ANR agency.
ferent linguistic domains, from prosody to gesture.
We describe in the following our first results.
2 Annotations
We present in this section some of the annota-
tions of a large conversational corpus, called CID
(Corpus of Interactional Data, see (Bertrand08)),
consisting in 8 dialogues, with audio and video si-
gnal, each lasting 1 hour.
Transcription : The transcription process is
done following specific conventions derived from
that of the GARS (Blanche-Benveniste87). The
result is what we call an enriched orthographic
construction, from which two derived transcrip-
tions are generated automatically : the standard or-
thographic transcription (the list of orthographic
tokens) and a specific transcription from which
the phonetic tokens are obtained to be used by the
grapheme-phoneme converter.
From the phoneme sequence and the audio si-
gnal, the aligner outputs for each phoneme its
time localization. This aligner (Brun04) is HMM-
based, it uses a set of 10 macro-classes of vowel
(7 oral and 3 nasal), 2 semi-vowels and 15 conso-
nants. Finally, from the time aligned phoneme se-
quence plus the EOT, the orthographic tokens is
time-aligned.
Syllables : The corpus was automatically seg-
mented in syllables. Sub-syllabic constituents (on-
set, nucleus and coda) are then identified as well
as the syllable structure (V, CV, CCV, etc.). Sylla-
bic position is specified in the case of polysyllabic
words.
Prosodic phrasing : Prosodic phrasing refers
to the structuring of speech material in terms of
boundaries and groupings. Our annotation scheme
supposes the distinction between two levels of
phrasing : the level of accentual phrases (AP, (Jun,
2002)) and the higher level of intonational phrases
186
(IP). Mean annotation time for IPs and APs was
30 minutes per minute.
Prominence : The prominence status of a syl-
lable distinguishes between accentuability (the
possibility for syllable to be prominent) and pro-
minence (at the perception level). In French the
first and last full syllables (not containing a
schwa) of a polysyllabic word can be prominent,
though this actual realization depends on spea-
kers choices. Accentuability annotation is auto-
matic while prominence annotation is manual and
perceptually based.
Tonal layer : Given a lack of consensus on the
inventory of tonal accents in French, we choose to
integrate in our annotation scheme three types of
tonal events : a/ underlying tones (for an eventual
FrenchToBI annotation) ; b/ surface tones (anno-
tated in terms of MOMel-Intsint protocol Hirst et
al 2000) ; c/ melodic contours (perceptually anno-
tated pitch movements in terms of their form and
function). The interest to have both manual and
automatic INTSINT annotations is that it allows
the study of their links.
Hand gestures : The formal model we use for
the annotation of hand gestures is adapted from
the specification files created by Kipp (2004) and
from the MUMIN coding scheme (Allwood et al,
2005). Among the main gesture types, we anno-
tate iconics, metaphoric, deictics, beats, emblems,
butterworths or adaptors.
We used the Anvil tool (Kipp, 2004) for the ma-
nual annotations. We created a specification files
taking into account the different information types
and the addition of new values adapted to the
CID corpus description (e.g. we added a separate
track Symmetry). For each hand, the scheme has 10
tracks. We allowed the possibility of a gesture per-
taining to several semiotic types using a boolean
notation. A gesture phrase (i.e. the whole gesture)
can be decomposed into several gesture phases i.e.
the different parts of a gesture such as the prepara-
tion, the stroke (the climax of the gesture), the hold
and the retraction (when the hands return to their
rest position) (McNeill, 1992). The scheme also
enables to annotate gesture lemmas (Kipp, 2004),
the shape and orientation of the hand during the
stroke, the gesture space, and contact. We added
the three tracks to code the hand trajectory, ges-
ture velocity and gesture amplitude.
Discourse and Interaction : Our discourse an-
notation scheme relies on multidimensional fra-
meworks such as DIT++ (Bunt, 2009) and is com-
patible with the guidelines defined by the Semantic
Annotation Framework (Dialogue Act) working
group of ISO TC37/4.
Discourse units include information about their
producer, have a form (clause, fragment, dis-
fluency, non-verbal), a content and a communi-
cative function. The same span of raw data may
be covered by several discourse units playing dif-
ferent communicative functions. Two discourse
units may even have exactly the same temporal ex-
tension, due to the multifonctionality that cannot
be avoided (Bunt, 2009).
Compared to standard dialogue act annotation
frameworks, three main additions are proposed :
rhetorical function, reported speech and humor.
Our rhetorical layer is an adaptation of an exis-
ting schema developed for monologic written data
in the context of the ANNODIS project.
Disfluencies : Disfluencies are organized
around an interruption point, which can occur al-
most anywhere in the production. Disfluencies can
be prosodic (lenghtenings, silent and filled pauses,
etc.), or lexicalized. In this case, they appear as a
word or a phrase truncation, that can be comple-
ted. We distinguish three parts in a disfluency (see
(Shriberg, 1994), (Blanche-Benveniste87)) :
? Reparandum : what precedes the interruption
point. This part is mandatory in all disfluen-
cies. We indicate there the nature of the inter-
rupted unit (word or phrase), and the type of
the truncated word (lexical or grammatical) ;
? Break interval. It is optional, some disfluen-
cies do not bear any specific event there.
? Reparans : the part following the break, repai-
ring the reparandum. We indicate there type
of the repair (no restart, word restart, determi-
ner restart, phrase restart, etc.), and its func-
tion (continuation, repair without change, re-
pair with change, etc.).
3 Quantitative information
We give in this section some indication about
the state of development of the CID annotation.
Hand gestures : 75 minutes involving 6 spea-
kers have been annotated, yielding a total number
of 1477 gestures. The onset and offset of gestures
correspond to the video frames, starting from and
187
going back to a rest position.
Face and gaze : At the present time, head move-
ments, gaze directions and facial expressions have
been coded in 15 minutes of speech yielding a to-
tal number of 1144 movements, directions and ex-
pressions, to the exclusion of gesture phases. The
onset and offset of each tag are determined in the
way as for hand gestures.
Body Posture : Our annotation scheme consi-
ders, on top of chest movements at trunk level,
attributes relevant to sitting positions (due to the
specificity of our corpus). It is based on the Pos-
ture Scoring System (Bull, 1987) and the Annota-
tion Scheme for Conversational Gestures (Kipp et
al., 2007). Our scheme covers four body parts :
arms, shoulders, trunk and legs. Seven dimensions
at arm level and six dimensions at leg level, as well
as their related reference points we take in fixing
the spatial location, are encoded.
Moreover, we added two dimensions to describe
respectively the arm posture in the sagittal plane
and the palm orientation of the forearm and the
hand. Finally, we added three dimensions for leg
posture : height, orientation and the way in which
the legs are crossed in sitting position.
We annotated postures on 15 minutes of the cor-
pus involving one pair of speakers, leading to 855
tags with respect to 15 different spatial location
dimensions of arms, shoulder, trunk and legs.
Annotation Time (min.) Units
Transcript 480 -
Hands 75 1477
Face 15 634
Gaze 15 510
Posture 15 855
R. Speech 180
Com. Function 6 229
Disfluencies At the moment, this annotation is
fully manual (we just developed a tool helping the
process in identifying disfluencies, but it has not
yet been evaluated). Annotating this phenomenon
requires 15mns for 1 minute of the corpus. The
following table illustrates the fact that disfluen-
cies are speaker-dependent in terms of quantity
and type. These figures also shows that disfluen-
cies affect lexicalized words as well as grammati-
cal ones.
Speaker_1 Speaker_1
Total number of words 1,434 1,304
Disfluent grammatical words 17 54
Disfluent lexicalized words 18 92
Truncated words 7 12
Truncated phrases 26 134
Transcription and phonemes The following
table recaps the main figures about the different
specific phenomena annotated in the EOT. To the
best of our knowledge, these data are the first of
this type obtained on a large corpus. This informa-
tion is still to be analyzed.
Phenomenon Number
Elision 11,058
Word truncation 1,732
Standard liaison missing 160
Unusual liaison 49
Non-standard phonetic realization 2,812
Laugh seq. 2,111
Laughing speech seq. 367
Single laugh IPU 844
Overlaps > 150 ms 4,150
Syntax We used the stochastic parser developed
at the LPL (Blache&Rauzy, 2008) to automaticaly
generate morppho-syntactic and syntactic annota-
tions. The parser has been adapted it in order to ac-
count for the specificities of speech analysis. First,
the system implements a segmentation technique,
identifying large syntactic units that can be consi-
dered as the equivalent of sentences in written
texts. This technique distinguishes between strong
and weak or soft punctuation marks. A second mo-
dification concerns the lexical frequencies used by
the parser model in order to capture phenomena
proper to conversational data.
The categories and chunks counts for the whole
corpus are summarized in the following figure :
Category Count Group Count
adverb 15123 AP 3634
adjective 4585 NP 13107
auxiliary 3057 PP 7041
determiner 9427 AdvP 15040
conjunction 9390 VPn 22925
interjection 5068 VP 1323
preposition 8693 Total 63070
pronoun 25199
noun 13419 Soft Pct 9689
verb 20436 Strong Pct 14459
Total 114397 Total 24148
4 Evaluations
Prosodic annotation : Prosodic annotation of
1 dialogue has been done by 2 experts. The
annotators worked separately using Praat. Inter-
transcriber agreement studies were done for the
annotation of higher prosodic units. First anno-
tator marked 3,159 and second annotator 2,855
188
Intonational Phrases. Mean percentage of inter-
transcriber agreement was 91.4% and mean
kappa-statistics 0.79, which stands for a quite sub-
stantial agreement.
Gesture : We performed a measure of inter-
reliability for three independent coders for Gesture
Space. The measure is based on Cohen?s correc-
ted kappa coefficient for the validation of coding
schemes (Carletta96).
Three coders have annotated three minutes for
GestureSpace including GestureRegion and Ges-
tureCoordinates. The kappa values indicated that
the agreement is high for GestureRegion of right
hand (kappa = 0.649) and left hand (kappa =
0.674). However it is low for GestureCoordinates
of right hand (k= 0.257) and left hand (k= 0.592).
Such low agreement of GestureCoordinates might
be due to several factors. First, the number of ca-
tegorical values is important.
Second, three minutes might be limited in terms
of data to run a kappa measure. Third, GestureRe-
gion affects GestureCoordinates : if the coders di-
sagree about GestureRegion, they are likely to also
annotate GestureCoordinates in a different way.
For instance, it was decided that no coordinate
would be selected for a gesture in the center-center
region, whereas there is a coordinate value for ges-
tures occurring in other parts of the GestureRe-
gion. This means that whenever coders disagree
between the center-center or center region, the an-
notation of the coordinates cannot be congruent.
5 Information representation
5.1 XML encoding
Our approach consists in first precisely define
the organization of annotations in terms of typed-
feature structures. We obtain an abstract descrip-
tion from which we automatically generate a for-
mal schema in XML. All the annotations are then
encoded following this schema.
Our XML schema, besides a basic encoding of
data following AIF, encode all information concer-
ning the organization as well as the constraints on
the structures. In the same way as TFS are used
as a tree description language in theories such as
HPSG, the XML schema generated from our TFS
representation also plays the same role with res-
pect to the XML annotation data file. On the one
hand, basic data are encoded with AIF, on the
other hand, the XML schema encode all higher
level information. Both components (basic data +
structural constraints) guarantee against informa-
tion loss that otherwise occurs when translating
from one coding format to another (for example
from Anvil to Praat).
5.2 Querying
To ease the multimodal exploitation of the data,
our objective is to provide a set of operators dedi-
cated to concurrent querying on hierarchical an-
notation. Concurrent querying consists in que-
rying annotations belonging to two or more mo-
dalities or even in querying the relationships bet-
ween modalities. For instance, we want to be able
to express queries over gestures and intonation
contours (what kind of intonational contour does
the speaker use when he looks at the listener ?).
We also want to be able to query temporal relation-
ships (in terms of anticipation, synchronization or
delay) between both gesture strokes and lexical af-
filiates.
Our proposal is to define these operators as an
extension of XQuery. From the XML encoding
and the temporal alignment of annotated data, it
will possible to express queries to find patterns and
to navigate in the structure. We also want to en-
able a user to check predicates on parts of the cor-
pus using classical criteria on values, annotations
and existing relationships (temporal or structural
ones corresponding to inclusions or overlaps bet-
ween annotations). First, we shall rely on one of
our previous proposal called MSXD (MultiStruc-
tured XML Document). It is a XML-compatible
model designed to describe and query concurrent
hierarchical structures defined over the same tex-
tual data which supports Allen?s relations.
6 Conclusion
Multimodal annotation is often reduced to
the encoding of gesture, eventually accompa-
nied with another level of linguistic information
(e.g. morpho-syntax). We reported in this paper a
broad-coverage approach, aiming at encoding all
the linguistic domains into a unique framework.
We developed for this a set of conventions and
tools making it possible to bring together and align
all these different pieces of information. The result
is the CID (Corpus of Interactional Data), the first
large corpus of conversational data bearing rich
annotations on all the linguistic domains.
189
References
Allen J. (1999) Time and time again : The many way to re-
present time. International Journal of Intelligent Systems,
6(4)
Allwood, J., Cerrato, L., Dybkjaer, L., Jokinen, K., Navar-
retta, C., Paggio, P. (2005) The MUMIN Multimodal Co-
ding Scheme, NorFA yearbook 2005.
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P.
F. Patel-Schneider (2003) The Description Logic Hand-
book : Theory, Implementation, Applications. Cambridge
University Press.
Bertrand, R., Blache, P., Espesser, R., Ferr?, G., Meunier, C.,
Priego-Valverde, B., Rauzy, S. (2008) ?Le CID - Corpus
of Interactional Data - Annotation et Exploitation Multi-
modale de Parole Conversationnelle?, in revue Traitement
Automatique des Langues, 49 :3.
Bigi, C. Meunier, I. Nesterenko, R. Bertrand 2010. ?Syllable
Boundaries Automatic Detection in Spontaneous Speech?,
in proceedings of LREC 2010.
Blache P. and Rauzy S. 2008. ?Influence de la qualit? de
l??tiquetage sur le chunking : une corr?lation d?pendant de
la taille des chunks?. in proceedings of TALN 2008 (Avi-
gnon, France), pp. 290-299.
Blache P., R. Bertrand, and G. Ferr? 2009. ?Creating and
Exploiting Multimodal Annotated Corpora : The ToMA
Project?. In Multimodal Corpora : From Models of Natu-
ral Interaction to Systems and Applications, Springer.
Blanche-Benveniste C. & C. Jeanjean (1987) Le fran?ais
parl?. Transcription et ?dition, Didier Erudition.
Blanche-Benveniste C. 1987. ?Syntaxe, choix du lexique et
lieux de bafouillage?, in DRLAV 36-37
Browman C. P. and L. Goldstein. 1989. ?Articulatory ges-
tures as phonological units?. In Phonology 6, 201-252
Brun A., Cerisara C., Fohr D., Illina I., Langlois D., Mella O.
& Smaili K. (2004- ?Ants : Le syst?l?me de transcription
automatique du Loria?, Actes des XXV Journ?es d?Etudes
sur la Parole, F?s.
E. Bruno, E. Murisasco (2006) Describing and Querying hie-
rarchical structures defined over the same textual data, in
Proceedings of the ACM Symposium on Document Engi-
neering (DocEng 2006).
Bull, P. (1987) Posture and Gesture, Pergamon Press.
Bunt H. 2009. ?Multifunctionality and multidimensional
dialogue semantics.? In Proceedings of DiaHolmia?09,
SEMDIAL.
B?rki A., C. Gendrot, G. Gravier & al.(2008) ?Alignement
automatique et analyse phon?tique : comparaison de dif-
f?rents syst?mes pour l?analyse du schwa?, in revue TAL
,49 :3
Carletta, J. (1996) ?Assessing agreement on classification
tasks : The kappa statistic?, in Computational Linguistics
22.
Corlett, E. N., Wilson,John R. Manenica. I. (1986) ?Influence
Parameters and Assessment Methods for Evaluating Body
Postures?, in Ergonomics of Working Postures : Models,
Methods and Cases , Proceedings of the First International
Occupational Ergonomics Symposium.
Di Cristo & Hirst D. (1996) ?Vers une typologie des unites in-
tonatives du fran?ais?, XXI?me JEP, 219-222, 1996, Avi-
gnon, France
Di Cristo A. & Di Cristo P. (2001) ?Syntaix, une approche
m?trique-autosegmentale de la prosodie?, in revue Traite-
ment Automatique des Langues, 42 :1.
Dipper S., M. Goetze and S. Skopeteas (eds.) 2007. Informa-
tion Structure in Cross-Linguistic Corpora : Annotation
Guidelines, Working Papers of the SFB 632, 7 :07
FGNet Second Foresight Report (2004) Face
and Gesture Recognition Working Group.
http ://www.mmk.ei.tum.de/ waf/fgnet-intern/3rd-
fgnet-foresight-workshop.pdf
Gendner V. et al 2003. ?PEAS, the first instantiation of a
comparative framework for evaluating parsers of French?.
in Research Notes of EACL 2003 (Budapest, Hungaria).
Hawkins S. and N. Nguyen 2003. ?Effects on word re-
cognition of syllable-onset cues to syllable-coda voicing?,
in Papers in Laboratory Phonology VI. Cambridge Univ.
Press.
Hirst, D., Di Cristo, A., Espesser, R. 2000. ?Levels of des-
cription and levels of representation in the analysis of in-
tonation?, in Prosody : Theory and Experiment, Kluwer.
Hirst, D.J. (2005) ?Form and function in the representation
of speech prosody?, in K.Hirose, D.J.Hirst & Y.Sagisaka
(eds) Quantitative prosody modeling for natural speech
description and generation (Speech Communication 46 :3-
4.
Hirst, D.J. (2007) ?A Praat plugin for Momel and INTSINT
with improved algorithms for modelling and coding into-
nation?, in Proceedings of the XVIth International Confe-
rence of Phonetic Sciences.
Hirst, D. (2007), Plugin Momel-Intsint. Inter-
net : http ://uk.groups.yahoo.com/group/praat-
users/files/Daniel_Hirst/plugin_momel-intsint.zip,
Boersma, Weenink, 2007.
Jun, S.-A., Fougeron, C. 2002. ?Realizations of accentual
phrase in French intonation?, in Probus 14.
Kendon, A. (1980) ?Gesticulation and Speech : Two Aspects
of the Porcess of Utterance?, in M.R. Key (ed.), The Re-
lationship of Verbal and Nonverbal Communication, The
Hague : Mouton.
Kita, S., Ozyurek, A. (2003) ?What does cross-linguistic va-
riation in semantic coordination of speech and gesture re-
veal ? Evidence for an interface representation of spatial
thinking and speaking?, in Journal of Memory and Lan-
guage, 48.
Kipp, M. (2004). Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation. Boca
Raton, Florida, Dissertation.com.
Kipp, M., Neff, M., Albrecht, I. (2007). An annotation
scheme for conversational gestures : how to economically
capture timing and form. Language Resources and Eva-
luation, 41(3).
Koiso H., Horiuchi Y., Ichikawa A. & Den Y.(1998) ?An ana-
lysis of turn-taking and backchannels based on prosodic
and syntactic features in Japanese map task dialogs?, in
Language and Speech, 41.
McNeill, D. (1992). Hand and Mind. What Gestures Re-
veal about Thought, Chicago : The University of Chicago
Press.
McNeill, D. (2005). Gesture and Thought, Chicago, London :
The University of Chicago Press.
Milborrow S., F. Nicolls. (2008). Locating Facial Features
with an Extended Active Shape Model. ECCV (4).
Nesterenko I. (2006) ?Corpus du parler russe spontan? : an-
notations et observations sur la distribution des fronti?res
prosodiques?, in revue TIPA, 25.
190
Paroubek P. et al 2006. ?Data Annotations and Measures in
EASY the Evaluation Campaign for Parsers in French?. in
proceedings of the 5th international Conference on Lan-
guage Resources and Evaluation 2006 (Genoa, Italy), pp.
314-320.
Pierrehumbert & Beckman (1988) Japanese Tone Structure.
Coll. Linguistic Inquiry Monographs, 15. Cambridge,
MA, USA : The MIT Press.
Platzer, W., Kahle W. (2004) Color Atlas and Textbook of
Human Anatomy, Thieme. Project MuDis. Technische
Universitat Munchen. http ://www9.cs.tum.edu/research
Scherer, K.R., Ekman, P. (1982) Handbook of methods in
nonverbal behavior research. Cambridge University Press.
Shriberg E. 1994. Preliminaries to a theory of speech dis-
fluencies. PhD Thesis, University of California, Berkeley
Wallhoff F., M. Ablassmeier, and G. Rigoll. (2006) ?Mul-
timodal Face Detection, Head Orientation and Eye Gaze
Tracking?, in proceedings of International Conference on
Multisensor Fusion and Integration (MFI).
White, T. D., Folkens, P. A. (1991) Human Osteology. San
Diego : Academic Press, Inc.
191
JEP-TALN-RECITAL 2012, Atelier DEGELS 2012: D?fi GEste Langue des Signes, pages 9?21,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Crit?res de segmentation de la gestualit? co-verbale 
Ga?lle Ferr? 
LLING, Chemin de la Censive du Tertre, BP 81227, 44312 Nantes, cedex 3 
Gaelle.Ferre@univ-nantes.fr 
R?SUM? ____________________________________________________________________________________________________________   
La gestualit? suscite un int?r?t croissant chez les linguistes qui s?int?ressent au caract?re 
multimodal de la structuration de l?information : modalit? verbale, vocale et visuelle. 
Cependant, la prise en compte des informations visuelles passe n?cessairement par une 
r?flexion sur les unit?s gestuelles. Qu?est-ce qu?un geste ? Comment subdiviser le flux 
gestuel en unit?s discr?tes ? Quel degr? de finesse est n?cessaire dans la segmentation 
des unit?s gestuelles pour permettre leur mise en relation avec les informations relevant 
d?autres modalit?s ? A partir du corpus DEGELS fourni par les organisatrices de cet 
atelier de r?flexion, nous d?crivons les crit?res adopt?s pour l?annotation de la 
gestualit? co-verbale ainsi que de la direction du regard des locuteurs. 
ABSTRACT _________________________________________________________________________________________________________  
Segmentation criteria for the annotation of co-speech gestures 
Gesture has been arousing a growing interest in linguists who are attracted by the 
multimodality of information structuring, organized into the verbal, vocal and visual 
modes. Yet, in order for visual information to be taken into account, one has to 
consider gesture units. What is a gesture? How can the constant flood of movement be 
subdivided into discrete units? What degree of fineness is necessary in the segmentation 
of gesture units to put them into relationship with information in other modes? 
Drawing on the DEGELS corpus provided by the organizers of the workshop, we 
describe the criteria adopted in our practice for the annotation of co-speech gesture as 
well as gaze direction. 
MOTS-CL?S : Annotation, gestualit?, segmentation, unit?s. 
KEYWORDS : Annotation, gesture, segmentation, units. 
1 Introduction 
Si la multimodalit? prend une part croissante dans les ?tudes linguistiques, les corpus 
annot?s d?enregistrements vid?os, prenant en compte la gestualit? co-verbale, restent 
encore assez peu nombreux ? l?heure actuelle. Cela ne signifie pas cependant qu?une 
r?flexion n?a pas ?t? conduite sur l?annotation de ces ph?nom?nes linguistiques, bien au 
contraire. L?annotation de la gestualit? a fait l?objet d?une r?flexion concert?e ces dix 
derni?res ann?es, au cours de divers projets de recherche, et c?est le travail de r?flexion 
men? au cours d?un de ces projets que je souhaiterais pr?senter ici, lui-m?me aliment? 
par une longue pratique de ce type d?annotation dans les recherches personnelles des 
divers participants au projet. Il s?agit du projet ANR OTIM (ANR-08-BLAN-0239), dont 
le but est la constitution d?un corpus de fran?ais spontan?, annot? dans diverses 
modalit?s (verbale, avec des annotations discursives et syntaxiques ; vocale, avec des 
annotations prosodiques ; et visuelle, avec des annotations des gestes et des postures). 
9
Nous nous int?resserons ici ? l?annotation des gestes, et plus pr?cis?ment ? la 
segmentation des unit?s gestuelles qui constituent la base d?une annotation 
multimodale, et pr?senterons une annotation du corpus DEGELS construite ? partir 
d?une adaptation du sch?ma d?encodage propos? dans OTIM pour le corpus CID 
(Bertrand et al, 2008). Cette adaptation a permis d?une part de r?pondre aux consignes 
de l?atelier dans le but de faciliter le partage d?informations, mais aussi de corriger 
certaines imperfections du sch?ma d?encodage initial. 
Sur le corpus CID, comme sur le corpus DEGELS, les annotations ont ?t? enti?rement 
r?alis?es de mani?re manuelle sous ANVIL (Kipp, 2001), ce qui a un impact certain sur 
le type d?annotation r?alis? : il est impossible de noter tous les micro-mouvements des 
locuteurs car cela prendrait un temps consid?rable, ni d?avoir la pr?cision d?une 
annotation automatique. Le gain par rapport ? l?annotation automatique est cependant 
de pouvoir faire des inf?rences et des mises en relation entre signifiant et signifi? sur le 
plan linguistique, ce qu?aucune machine n?est en mesure de r?aliser ? pr?sent. La 
moindre pr?cision par rapport ? l?annotation automatique ne signifie pas cependant une 
absence totale de crit?res formels pour la segmentation. Ce sont ces crit?res que je vais 
tenter de pr?ciser ici, en abordant dans un premier temps les gestes manuels, puis les 
mouvements de t?te, de la face et du buste, qui pr?sentent leurs propres particularit?s, 
pour finir par la direction du regard. 
2 Les gestes manuels 
La premi?re question qui se pose lors de l?annotation des gestes manuels concerne le 
type d?unit? que l?on va annoter. Prenons l?exemple du mouvement r?alis? par 
l?exp?rimentatrice ? et interlocutrice ? dans DEGELS (entre 0.15 et 0.16 s dans le 
corpus) et reproduit en s?quence dans la figure 1 ci-dessous. Il appara?t clairement que 
la main droite de l?interlocutrice est soulev?e de son appui sur la cuisse, puis repos?e 
dans une position l?g?rement modifi?e, qu?elle va conserver jusqu?? la fin de l?extrait. 
    
Figure 1 ? Mouvement r?alis? par l?exp?rimentatrice dans DEGELS. 
M?me si l?annotation s?est concentr?e sur les gestes du locuteur et non sur ceux de 
l?interlocutrice, ce mouvement n?aurait de toute mani?re pas ?t? annot? car il ne serait 
pas consid?r? comme un geste co-verbal, ni m?me comme un adaptateur (geste d?auto-
contact1), mais plut?t comme un changement de posture. Ainsi, pour ?tre consid?r? 
comme geste co-verbal, un mouvement doit-il r?pondre aux crit?res suivants : 
 ?tre perceptible par l?annotateur, 
                                                   
1 Les adaptateurs ne sont pas des gestes co-verbaux, mais ont ?t? n?anmoins annot?s dans le projet OTIM 
dans la mesure o? ils fournissent des indications sur la r?gulation des tours de parole. Aucun adaptateur n?a 
?t? annot? dans DEGELS, car le locuteur principal n?en produit pas sur cet extrait. 
10
 op?rer un contraste, 
 participer d?une intention de communication. 
Cela signifie que des mouvements de tr?s petite amplitude, op?r?s qui plus est de 
mani?re tr?s progressive ne seront pas per?us comme gestes. De m?me, certains gestes 
peuvent avoir une tr?s petite amplitude (comme le fait de pianoter avec les doigts) et 
l?on peut imaginer que pour un locuteur atteint de tremblement, le geste devra se 
d?marquer du mouvement g?n?r? par le tremblement afin d??tre per?u comme geste, et 
donc op?rer un contraste. De plus, pour que deux gestes encha?n?s puissent ?tre 
d?marqu?s, il doit y avoir un contraste dans au moins une des caract?ristiques entre les 
deux gestes (configuration de la main, direction du mouvement, type de mouvement, 
etc). Pour certains types de geste comme les mouvements de la t?te vers la gauche ou la 
droite par exemple, le contraste op?r? doit ?galement ?tre limit? dans le temps. En 
effet, si un locuteur tourne la t?te pendant une certaine dur?e, le mouvement sera 
interpr?t? plut?t comme un changement de posture que comme un geste. A ma 
connaissance, aucune dur?e n?a ?t? d?termin?e pour ce type de geste et l?annotateur 
h?site parfois entre geste et changement de posture, bien que le cas ne ce soit pas 
pr?sent? dans DEGELS. Enfin, le mouvement doit participer d?une intention de 
communication pour ?tre class? comme geste et c?est ce qui manque dans le celui qui 
illustr? dans la figure 1. On pourrait imaginer que le fait de soulever la main (non 
n?cessaire pour d?placer la main sur la cuisse) rel?ve d?une intention de 
communication (un geste ?bauch? mais inachev?), mais cela n?est qu?une supposition et 
en l?absence de certitude, le mouvement ne sera pas consid?r? comme geste. 
2.1 Segmentation des gestes manuels 
Lorsque les gestes manuels sont produits en isolation, la/les mains est/sont d?abord en 
position de repos (muscles d?tendus, mains pos?es sur les cuisses par exemple), comme 
c?est le cas dans la figure 2 ci-dessous. Dans cette figure, les mains sont au repos, sans 
aucun mouvement dans les deux premi?res images. Les doigts commencent ? s??carter 
dans la troisi?me image pour ?baucher le geste. Le d?but du geste est donc not? entre la 
deuxi?me et la troisi?me image, car ?tant donn? la granularit? vid?o (25 images par 
seconde), le geste a commenc? l?g?rement avant la troisi?me image. Ceci a ?t? d?crit 
?galement dans Ferr? (2011). Pour la fin du geste, la fronti?re se situe juste avant 
l?image de retour ? la position de repos (ainsi, dans DEGELS, les phases de r?traction et 
de rebond d?crites dans la section suivante ne comptent pas dans la ?phrase gestuelle?, 
contrairement ? la segmentation qui a ?t? adopt?e pour OTIM). 
 
 
 
 d?but du geste 
Figure 2 ? Segmentation d?un geste depuis la position de repos. 
La segmentation est diff?rente lorsque deux gestes sont produits ? la suite l?un de 
l?autre. Deux cas de figure se pr?sentent alors : (a) le premier geste se termine par une 
11
tenue des mains et dans ce cas, la fronti?re entre les deux gestes est pos?e apr?s la fin 
de la tenue du premier geste (voir la section suivante pour une d?finition de la tenue du 
geste), et (b) la fin du premier geste est une phase dynamique et la fronti?re est pos?e 
imm?diatement avant l?image qui figure soit un changement de direction du 
mouvement, soit un changement de la configuration de la main. 
Dans le fichier d?annotation que je propose pour DEGELS, cette segmentation 
correspond aux ?phrases gestuelles? propos?es par Kendon (2004 : 111) en termes de 
segmentation, mais ? la typologie de McNeill (1992, 2005) pour ce qui concerne le lien 
entre le type de geste et la parole pour chaque ?tiquette. La typologie distingue entre 
les gestes iconiques, m?taphoriques, d?ictiques, les battements purs (voir la section 
suivante pour d?autres types de battement), les embl?mes, ? laquelle nous avons ajout? 
les adaptateurs dans le projet OTIM. Cette relation du geste avec le verbal poss?de une 
certaine influence sur la segmentation pour certains types de gestes bi-manuels 
asym?triques. Nous avions d?j? not? dans Ferr? (2011 : 39) que deux gestes, r?alis?s 
avec les deux mains, peuvent ?tre produits en chevauchement, et alrs que l?une des 
deux mains place un r?f?rent du discours dans l?espace, la seconde main, qui figure un 
deuxi?me r?f?rent, effectue un d?placement par rapport ? la premi?re main. On 
comptera donc ici deux unit?s distinctes. 
2.2 Les phases gestuelles 
Toujours en suivant la segmentation de Kendon (2004 : 112), chaque geste est ensuite 
segment? en diff?rentes ?phases gestuelles?. Dans le fichier d?annotation, cela 
correspond n?cessairement aux pistes primaires exig?es dans les consignes car les pistes 
primaires doivent constituer les plus petites unit?s dans Anvil. Les phases qui ont ?t? 
retenues sont : la pr?paration (?preparation?, mise en place des articulateurs), la 
r?alisation (?stroke? : partie dynamique du geste), la tenue (?hold?, les mains restent en 
tension mais ne bougent pas), la r?traction totale ou partielle (r?traction totale : retour 
des mains ? une position de repos ; r?traction partielle : les mains n?atteignent pas la 
position de repos), le rebond (?recoil?, les mains peuvent avoir un l?ger rebond 
lorsqu?elles sont pos?es sur la cuisse par exemple). A ces phases d?termin?es par 
Kendon, nous avons ajout? dans OTIM le battement lorsqu?un battement est r?alis? au 
cours d?un autre geste (le plus souvent pendant la tenue). 
2.2.1 Pr?paration 
Dans le cas o? le geste est r?alis? de mani?re isol?e (o? les articulateurs partent d?une 
position de repos), il peut compter une phase de pr?paration (mais elle n?est pas 
obligatoire) : parfois, pour un mouvement de la main vers le bas par exemple, il est 
n?cessaire dans un premier temps de lever la main. C?est ce qui se produit dans DEGELS 
? plusieurs reprises, ainsi que l?illustre la figure 3. 
 
 
 
Figure 3 ? Phases de pr?paration [image 1034] et de r?alisation [image 1045] du geste 
R?alisation Pr?paration 
12
(la fl?che indique la direction du mouvement). 
Ici, l?on voit que le locuteur commence par lever la main droite pour ensuite la 
descendre pour indiquer une direction situ?e devant lui, l?g?rement ? gauche. La 
fronti?re entre les deux phases se situe juste apr?s l?image montrant l?extension 
maximale de la pr?paration. La distinction entre la phase de r?alisation et la 
pr?paration suppose donc un changement de direction du mouvement pour ce type de 
geste mais cela n?est pas n?cessairement le cas. Ainsi, par exemple, dans le geste 
produit entre les images 1962 et 2008 sur ? la route qui monte vers Sormiou ?, la phase 
de pr?paration consiste ? changer la configuration manuelle par rapport ? celle du geste 
pr?c?dent et ce n?est que lorsque cette nouvelle configuration est adopt?e que le 
locuteur commence ? lever la main en un mouvement qui ?voque une route sinueuse, 
ainsi que le montre la figure 4. 
(a) (b) (c) 
Figure 4 ? (a) Fin du geste pr?c?dent [image 1962] ; (b) fin de la phase de pr?paration 
[image 1968] ; (c) fin de la phase de r?alisation [image 1987]. 
2.2.2 Tenue et battement 
La phase de tenue correspond ? une s?quence comprenant au moins deux images sans 
mouvement de la part du locuteur, mais o? les mains sont toujours dans la 
configuration adopt?e pour le geste. Cette phase peut intervenir avant et/ou apr?s la 
phase de r?alisation. Contrairement ? Kendon, qui cite plusieurs auteurs, nous ne 
distinguons pas la tenue produite avant la r?alisation (?prestroke hold?) de celle 
produite apr?s (?poststroke hold?), dans la mesure o? cette phase peut se substituer ? la 
r?alisation dans les gestes ?statiques?. DEGELS n?offre aucun exemple de ce type de 
geste, mais au cours des annotations produites sur le CID, nous avons remarqu? que 
certains gestes impliquent un mouvement des articulateurs dans leur phase pertinente 
de r?alisation, alors que d?autres sont statiques. Ainsi, juste avant l?exemple donn? en 
figure 5, les mains de la locutrice ?taient pos?es sur ses cuisses, puis en disant ? j?ai lu 
le r?sum? ?, la locutrice les soul?ve pour les placer dans la configuration illustr?e 
(phase de pr?paration) et les laisse sans faire de mouvement avant de les reposer. On a 
donc dans ce geste une phase de pr?paration et une tenue mais pas de phase de 
r?alisation. 
 Figure 5 ? Geste statique (tenu) sans phase de 
r?alisation. 
Enfin, l?on remarque dans les corpus de conversation spontan?e, que la tenue du geste, 
13
lorsqu?elle est pr?sente, n?est pas toujours tout ? fait parfaite. Le locuteur peut avoir ? 
certains moments de la tenue un tr?s l?ger mouvement de l?ordre d?un centim?tre. Il 
importe que ce mouvement ne soit pas rapide pour ne pas op?rer un contraste et ne pas 
?tre compris comme un battement. Et pr?cis?ment, lorsqu?un mouvement rapide vers 
l?avant, les c?t?s ou vers le bas est r?alis? alors que les mains sont plac?es dans la 
configuration pour la phase de r?alisation et que le type de geste n?implique pas ce type 
de mouvement dans son s?mantisme2, alors, le locuteur r?alise un battement. Ce 
battement compte comme une phase du geste dans la mesure o? il n?est pas r?alis? pour 
lui-m?me mais au c?ur d?un autre geste. En ce qui concerne la segmentation, la tenue 
commence juste avant l?arr?t du mouvement et s?ach?ve juste apr?s. C?est l?inverse pour 
le battement. 
2.2.3 R?alisation 
La phase de r?alisation du geste est une phase dynamique qui apporte son s?mantisme 
au mouvement. Elle n?est pas n?cessairement pr?c?d?e d?une phase de pr?paration, 
comme le montre la figure 6. 
  
(a) (b) 
Figure 6 ? Deux gestes produits en s?quence. 
La figure 6 illustre deux gestes produits en s?quence par le locuteur. 6(a) correspond au 
geste iconique qui accompagne ? le rond-point du pouce ? [image 1101, fin de la phase 
de r?alisation] et 6(b) est la fin de la r?alisation du geste m?taphorique qui 
accompagne ? avec le mus?e d?art euh ? [image 1115]. Sit?t la configuration atteinte en 
6(a), le locuteur effectue une rotation du poignet et une ouverture de la main pour 
atteindre la configuration en 6(b). Plusieurs interpr?tations sont possibles pour ce 
deuxi?me geste. On peut penser que l?aspect signifiant du geste est la tenue de la main 
paume ouverte orient?e vers le haut (c?est le point de vue adopt? par Kendon qui 
nomme ce type de geste ?Open Hand Supine? ou ?Palm up?, op. cit. : 264, bien qu?il ne 
soit pas exactement certain qu?il s?agisse du m?me type de geste). Dans ce cas, le geste 
comporte une pr?paration et une tenue. Mais on peut aussi penser ? l?instar de Kipp 
(2004 : 267, ?hand flip?) que l?aspect signifiant du geste est pr?cis?ment cette rotation 
du poignet et dans ce cas, la rotation constitue la phase de r?alisation du geste. C?est le 
point de vue qui a ?t? adopt? ici et dans OTIM. On voit alors que le deuxi?me geste 
encha?ne directement sur le premier avec une phase de r?alisation et la fronti?re se 
situe donc ici ? l?image 1101. 
2.2.4 R?traction totale et partielle 
La r?traction totale consiste ? un retour ? une position de repos (main pendante, pos?e 
                                                   
2 On pense par exemple ? un geste accompagnant un verbe de type ? frapper ? qui serait alors consid?r? 
comme un geste iconique, mimant l?action de frapper. 
14
sur un accoudoir, sur les cuisses, etc.) c?est-?-dire une position qui n?implique aucune 
tension musculaire comme c?est le cas par exemple de la main gauche dans la figure 
6(a), par rapport ? 6(b) qui montre une l?g?re ouverture de la main. Si la phase de 
r?alisation consiste en un mouvement vers le bas et qu?il n?y a aucun changement de 
configuration manuelle entre la r?alisation et la position de repos, alors il n?y a pas de 
phase de r?traction, celle-ci ?tant englob?e dans la r?alisation. Parfois, le locuteur initie 
un mouvement vers la position de repos, ou bien referme la main par exemple apr?s 
l?avoir ouverte lors de la r?alisation mais sans atteindre la position de repos et encha?ne 
ensuite imm?diatement sur un autre geste, on compte alors une r?traction partielle. 
2.2.5 Rebond 
Le cas ne se pr?sente pas dans DEGELS, mais il arrive que la main du locuteur ait un 
l?ger rebond lorsque la r?traction est rapide sur les cuisses du locuteur ou sur un 
accoudoir, par exemple. C?est une phase purement physiologique dans laquelle la main 
se soul?ve l?g?rement avant de retomber. La segmentation suit le principe adopt? pour 
les autres phases : la fronti?re de d?but de l?unit? est plac?e juste avant que la main se 
soul?ve et la fronti?re de fin est plac?e juste apr?s que la main soit repos?e. 
2.3 Les unit?s gestuelles 
A l?instar de Kendon (2004 : 111-124), nous distinguons une unit? sup?rieure dans la 
hi?rarchie des unit?s manuelles : les unit?s gestuelles (?gesture units?). Ceci constitue 
une adaptation par rapport au sch?ma d?encodage utilis? dans OTIM. Selon Kendon, 
(op. cit., p. 111), les unit?s gestuelles constituent des excursions de la / des main(s) du 
locuteur, bas?es en partie sur la segmentation des phrases gestuelles. Les ?tiquettes 
permettent de noter si les unit?s gestuelles sont r?alis?es avec une seule main, avec les 
deux mains, de mani?re sym?trique ou asym?trique. 
 
Figure 7 ? Deux unit?s gestuelles distinctes mais de m?me nature. 
Ainsi que le montre la figure 7, ? deux reprises, le locuteur se touche le pouce en 
?voquant la ? statue du pouce ?. Les deux gestes impliquent les deux mains et 
constituent deux unit?s gestuelles distinctes dans la mesure o? ils sont s?par?s par la 
r?traction partielle dont nous avons vu plus haut qu?elle ne faisait pas partie du geste. 
Dans la figure 8 ci-dessous apparaissent ?galement deux unit?s gestuelles distinctes, 
mais celles-ci sont cette fois de nature diff?rente. Dans un premier temps, le locuteur 
encha?ne deux gestes bimanuels (d?ictique et m?taphorique). Ces deux gestes 
constituent une seule unit? dans la mesure o? la main ne retourne pas ? une position de 
repos entre les deux. Mais ensuite, le locuteur r?tracte la main gauche (bien que la 
phase de r?traction ne fasse pas partie du geste) et continue la s?quence avec un geste 
r?alis? de la main droite. J?ai d?cid? que dans la mesure o? le locuteur change de mode 
de gestualisation, alors il entame une nouvelle unit? gestuelle. 
15
 Figure 8 ? Deux unit?s gestuelles distinctes de nature diff?rente. 
Un cas l?g?rement diff?rent se pr?sente dans la figure 9 ci-dessous. Le locuteur produit 
un geste iconique avec la main droite, qui compte comme une unit? gestuelle sur la 
premi?re tire. Puis il encha?ne directement avec un geste m?taphorique r?alis? 
?galement avec la main droite (derni?re tire sur la figure). On pourrait donc penser que 
ce m?taphorique fait partie de la m?me unit? gestuelle que le geste iconique. 
Cependant, pendant la production de ce second geste, un l?ger m?taphorique est 
produit avec la main gauche du locuteur (piste 5), en chevauchement avec le 
m?taphorique produit par la main droite. Ces l?gers ?cartements de la main gauche ne 
me semblent pas faire partie des gestes produits avec la main droite et passent d?ailleurs 
relativement inaper?us puisqu?ils n?avaient pas ?t? not?s lors de la premi?re ?dition de 
l?atelier. Ici, dans la mesure o? les deux mains r?alisent chacune des gestes en 
chevauchement, on peut compter une unit? gestuelle dans laquelle les deux mains 
fonctionnent de mani?re asym?trique. 
 
Figure 9 ? Deux mains asym?triques. 
En r?sum?, ce qui compte pour la segmentation des unit?s gestuelles est : (a) la fin 
d?une phrase gestuelle qui n?encha?ne pas directement sur une autre phrase gestuelle, et 
(b) un changement de mode de gestualisation (passage d?un geste ? une seule main ? un 
geste bimanuel et vice versa, ou passage d?un geste bimanuel sym?trique ? un geste 
bimanuel asym?trique et vice versa). 
3 Les gestes non manuels 
Les gestes non manuels sont beaucoup plus simples ? annoter dans le sch?ma 
d?encodage propos? ici, dans la mesure o? il ne poss?de qu?un seul niveau hi?rarchique. 
En effet, alors que les gestes manuels sont annot?s en termes d?unit?s gestuelles, elles-
m?mes d?compos?es en phrases gestuelles, ? leur tour d?compos?es en phases 
gestuelles, le tout formant trois pistes d?annotation (multipli?es pour les phrases et les 
phases par le fait que le geste est bimanuel, r?alis? avec la main gauche ou r?alis? avec 
la main droite), les gestes non manuels ne sont d?crits que sur une seule piste. 
3.1 La t?te 
En ce qui concerne la t?te, il faut distinguer les gestes des changements de posture. 
L?annotation repose sur le principe que la posture de repos par d?faut est lorsque la t?te 
16
est orient?e dans l?alignement du corps du locuteur, sans tension au niveau du cou. 
Tout mouvement par rapport ? cette position de repos peut ?tre interpr?t? comme un 
changement de posture ou comme un geste. Un changement de posture implique 
qu?une fois la nouvelle posture adopt?e, la t?te ne bouge plus. En revanche, aucune 
dur?e n?a ?t? d?termin?e dans OTIM pour distinguer entre geste et changement de 
posture. Parmi les gestes, on compte les ?nods? (acquiescements) et les ?shakes? (gestes 
de n?gation). Mais aussi, les ?tilts? (inclinaisons de la t?te sur le c?t?), les ?pointings? 
(pointages du menton), les ?jerks? (rejets de la t?te vers l?arri?re) et les ?beats? 
(mouvements du menton vers le bas sans valeur d?acquiescement). Le principe de 
segmentation adopt? est le m?me que pour les gestes manuels : le geste commence juste 
avant l?image o? la t?te quitte la position de repos, et s?arr?te juste apr?s le retour ? la 
position de repos. La pr?cision est cependant moins grande que pour les gestes manuels 
dans la mesure o? (a) certains gestes de la t?te ont une tr?s petite amplitude et, tout en 
?tant perceptibles lorsque la vid?o d?file ? vitesse r?elle, deviennent difficiles ? 
distinguer de la position de repos dans l?annotation image par image, et (b) la position 
de repos n?est pas elle-m?me d?finie au pixel pr?s. 
3.2 Les sourcils 
Ekman et al (2002) ont r?pertori? dans leur guide d?annotation des mouvements de la 
face un grand nombre de mouvements des sourcils. Dans OTIM et DEGELS, cependant, 
seules deux positions ont ?t? retenues ?raising? (sourcils hauss?s) et ?frowning? (sourcils 
fronc?s). Ceci s?explique par le type de corpus utilis?s : il s?agit de corpus de type 
conversationnel ? faible charge ?motionnelle, dans lesquels le visage des locuteurs n?est 
pas film? en gros plan. Cette absence de gros plan rend difficile l?annotation fine de 
mouvements de faible amplitude, mais les diff?rents mouvements relev?s par Ekman et 
al. ne sont pas non plus fr?quents sur ce type de corpus o? la charge ?motionnelle des 
locuteurs est faible. Une ?tude des ?motions exigerait de travailler sur un tout autre 
type de corpus. En d?autres termes, pour un travail sur le lien entre le verbal et le non-
verbal hors ?motions, les deux valeurs sont amplement suffisantes. La segmentation des 
mouvements des sourcils repose sur les principes adopt?s pour la t?te sans la distinction 
posture / geste. La position par d?faut (non annot?e) correspond ? celle de la figure 
10(a). En 10(b), le locuteur hausse les sourcils et en 10(c), c?est l?interlocutrice qui 
fronce les sourcils. 
   
(a) (b) (c) 
Figure 10 ? Mouvements des sourcils. (a) Position de repos ; (b) sourcils hauss?s ; (c) 
sourcils fronc?s. 
La segmentation dans Anvil se fait comme pour la t?te juste avant le d?but du 
mouvement pour l?onset et juste apr?s la fin du mouvement pour l?offset. Les 
mouvements des sourcils pr?sentent cependant une particularit? que ne pr?sentent pas 
les mouvements de t?te : ? l?instar des sourires (annot?s en partie dans OTIM, mais pas 
dans DEGELS), le d?but des mouvements des sourcils est facile ? identifier car les 
sourcils passent tr?s rapidement de la position de repos ? la position haute ou basse. En 
17
revanche, le retour ? la position de repos se fait le plus souvent de mani?re tr?s 
progressive et la fin de la segmentation est donc plus difficile ? rep?rer et donc moins 
pr?cise. 
3.3 Le buste 
Beaucoup plus complexe dans OTIM, l?annotation des mouvements du buste a ?t? 
limit?e dans DEGELS ? deux valeurs ?forwards? (vers l?avant) et ?backwards? (vers 
l?arri?re), le locuteur changeant relativement peu de posture dans ce court 
enregistrement. Le principe de segmentation est une fois encore identique ? celui des 
autres niveaux : le d?but de chaque ?tiquette est fix? juste avant le d?but du 
mouvement et la fin des ?tiquettes juste apr?s le retour ? la position de repos. La 
position de repos est lorsque le locuteur se tient droit, le dos appuy? sur le dossier du 
si?ge, sans pression perceptible. Il se penche vers l?avant lorsque son dos d?colle du 
si?ge et vers l?arri?re lorsqu?une pression sur le dossier et un recul des ?paules est 
perceptible. DEGELS pr?sente un avantage pour l?annotation des mouvements du buste 
par rapport au CID, le corpus utilis? dans OTIM : la vue de profil permet de mieux 
appr?hender les mouvements vers l?avant ou vers l?arri?re du buste. 
4 Le regard 
L?annotation de la direction du regard comporte deux types d?information. Certaines 
informations sont d?ordre interactionnel : regard vers l?interlocuteur, regard vers une 
partie du corps du locuteur (pointage du regard : nous avons vu l?an dernier dans 
l?atelier DEGELS 2011 que le regard peut instancier un geste manuel et constituer un 
pointage). D?autres informations ne sont pas d?ordre interactionnel : sur le c?t? (? 
droite, ? gauche), en haut / en bas (? droite, ? gauche). Les changements d?orientation 
du regard sont le plus souvent accompagn?s d?une fermeture des paupi?res. La 
segmentation se fait donc juste avant la r?ouverture des paupi?res pour le d?but de 
chaque ?tiquette et juste apr?s la premi?re image de fermeture des paupi?res pour la fin 
de chaque ?tiquette. Les fermetures des paupi?res n?ont pas ?t? annot?es dans DEGELS 
mais est pr?vue dans le sch?ma d?encodage d?OTIM. Dans DEGELS, lorsque le regard est 
orient? vers le bas, il n?est pas toujours possible de savoir si les paupi?res sont ouvertes 
ou ferm?es (c?est le cas notamment sur tout le d?but de l?enregistrement). Dans ce cas, 
l?annotation de la direction du regard n?a pas ?t? r?alis?e. Il en a ?t? de m?me lorsque 
les paupi?res du locuteur sont presque closes et que la direction du regard est alors 
incertaine. Lorsque le regard change de direction sans fermeture des paupi?res, nous 
avons segment? le d?but du changement avant la premi?re image sur laquelle il 
s?affiche. Il est certain que lors d?un changement de direction du regard, celui-ci 
effectue un changement progressif avant d?atteindre la nouvelle direction. Cependant, 
nous avons choisi de ne pas annoter cette mise en place du regard en raison de la 
p?nibilit? que pr?sente cette annotation par rapport ? l?annotation des autres types de 
mouvement. Pour finir, il est important de noter que ce type d?annotation ne permet de 
noter que la direction du regard et non ce que per?oit effectivement le participant ou ce 
sur quoi il se concentre.  
L?ensemble des crit?res de segmentation des gestes est repris dans la Table 1 ci-dessous. 
18
Crit?res 
formels 
Perception Segmentation 
Toute 
unit? 
gestuelle 
Mouvement perceptible par 
l?annotateur 
Contraste avec ce qui pr?c?de et ce 
qui suit 
Intention de communication 
D?but : image pr?c?dant le d?but du 
mouvement 
Fin : image pr?c?dant le retour au 
repos ou le changement de direction 
ou de configuration de la main 
Gestes 
manuels Segmentation 
Phases Pr?paration Mise en place des articulateurs 
 Tenue Absence de mouvement de la/des main(s) dans la 
configuration utilis?e lors de la r?alisation 
 Battement Mouvement rapide vers l?avant, les c?t?s ou vers le bas est 
r?alis? alors que les mains sont plac?es dans la 
configuration pour la phase de r?alisation 
 R?alisation Phase dynamique qui apporte son s?mantisme au 
mouvement 
 R?traction Totale Retour ? la position de repos 
  Partielle Mouvement vers la position de repos 
sans que celle-ci soit atteinte 
 Rebond Apr?s le retour au repos, la main se soul?ve l?g?rement en 
rebondissant sur un objet ou une partie du corps 
Phrases Unit?s s?mantiques qui commence ? la pr?paration et se termine apr?s la 
r?alisation ou la tenue en excluant la r?traction 
Unit?s Excursions de la / des main(s) du locuteur sans retour ? la position de 
repos ou sans changement de mode (1 main vs. 2 mains par exemple) 
19
Gestes 
non 
manuels 
T?te Sourcils Buste Regard 
 Une seule unit? (pas de subdivision) r?pondant aux crit?res formels 
TABLE 1 ?Crit?res de segmentation des gestes manuels et non manuels du corpus 
DEGELS. 
5 Conclusion 
Les crit?res de segmentation de la gestualit? co-verbale que nous avons pr?sent?s ici ? 
et qui ont pour objet l?annotation du corpus DEGELS ? s?inscrivent dans ce que Boutet 
(2008 : 82) nomme un ?rep?rage axial? (haut/bas, gauche/droite) et pr?sentent donc 
une pr?cision moindre par rapport au ?rep?rage polaire? d?crit dans le m?me article. Ce 
type de segmentation correspond cependant mieux aux besoins de l?analyse de 
l?interaction conversationnelle d?finis dans le projet OTIM dont il s?inspire. Une 
segmentation de type morphologique ou m?me des mesures tr?s pr?cises r?alis?es avec 
des syst?mes de capture de mouvement apportent une information si dense qu?il peut 
?tre difficile parfois de la mettre en relation avec des actes discursifs. 
L?annotation pr?sente une segmentation hi?rarchique des gestes manuels, organis?e 
autour de trois types d??tiquette, bas?s sur les travaux de Kendon (2004) et McNeill 
(1992, 2005) : les ?unit?s gestuelles? sont form?es par les d?placements des mains ou 
des doigts sans retour ? la position de repos. Nous distinguons ?galement des gestes 
produits avec une seule main de ceux produits avec deux mains, de mani?re sym?trique 
(les deux mains r?alisent une seule unit? gestuelle) ou asym?trique (chaque main 
r?alise une unit? gestuelle diff?rente de mani?re plus ou moins simultan?e). Ces unit?s 
gestuelles se d?composent en une ou plusieurs ?phrases gestuelles? dont le s?mantisme 
se distingue des autres, ainsi que leurs caract?ristiques formelles telles que la direction 
du mouvement, la configuration de la main, etc. Enfin, les phrases gestuelles sont elles-
m?mes d?compos?es en ?phases gestuelles? o? l?on distingue la partie pertinente du 
geste (qui lui donne son s?mantisme) des phases de mise en place et de r?traction des 
articulateurs ou encore de leur tenue. 
Les gestes non manuels tels que les mouvements de t?te, des sourcils et du buste ont 
?galement ?t? pr?sent?s ici. Leur segmentation est moins complexe ? en termes de 
hi?rarchie ? que celle des gestes manuels puisque seul l??quivalent des ?phrases 
gestuelles? est annot?. Il est important cependant de savoir que le principe de 
segmentation (? quelle image d?bute et se termine l?unit?) adopt? pour les gestes non 
manuels est strictement identique ? celui qui a ?t? adopt? pour les gestes manuels. 
Enfin, la segmentation de la direction du regard des locuteurs a ?galement ?t? 
pr?sent?e, avec ses difficult?s particuli?res et notamment le fait qu?elle est d?pendante 
de l?ouverture des paupi?res contrairement aux autres annotations sur ce type de 
corpus. 
Pour finir cet article, il me semble que la d?marche de mise en commun des pratiques 
20
d?annotation et de segmentation propos?e par l?atelier DEGELS est tr?s important pour 
la communaut? des gestualistes en France car elle n?a pas ?t? effectu?e jusque-l? et il 
est extr?mement difficile d??tablir un dialogue entre les chercheurs d?une communaut? 
sans un minimum de partage des principes de base de l?annotation qui pourront plus 
tard ?tre transmis aux jeunes chercheurs. 
 
BERTRAND, R., et al (2008). Le CID - Corpus of Interactional Data - Annotation et 
Exploitation Multimodale de Parole Conversationnelle. TAL 49, pages 105-133. 
BOUTET, D. (2008). Une Morphologie De La Gestualit? : Structuration Articulaire, 
Cahiers De Linguistique Analogique 5, pages 80-115. 
EKMAN, P., FRIESEN, W. V et HAGER, J. C. (2002). The Facial Action Coding System (2nd 
ed.). http://www.face-and-emotion.com/dataface/facs/manual/TitlePage.html. 
FERR?, G. (2011). Annotation multimodale du fran?ais parl?. Le cas des pointages. In 
Proceedings of TALN - Atelier Degels, Montpellier, 1er juillet 2011, pages 29-43. 
KENDON, A. (2004). Gesture. Visible Action as Utterance. CUP, Cambridge. 
KIPP, M. (2001). Anvil - A Generic Annotation Tool for Multimodal Dialogue. In 
Proceedings of 7th European Conference on Speech Communication and Technology 
(Eurospeech), Aalborg, Denmark, pages 1367-1370. 
KIPP, M. (2004) Gesture Generation by Imitation - From Human Behavior to Computer 
Character Animation. Boca Raton, Florida. 
MCNEILL, D. (1992). Hand and Mind : What Gestures Reveal about Thought. The 
University of Chicago Press, Chicago and London. 
MCNEILL, D. (2005). Gesture & Thought. The University of Chicago Press, Chicago and 
London. 
 
21

