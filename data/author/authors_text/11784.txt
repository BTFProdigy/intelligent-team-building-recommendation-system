Proceedings of the 12th Conference of the European Chapter of the ACL, pages 51?59,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Clique-Based Clustering for improving Named Entity Recognition systems
Julien Ah-Pine
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan, France
julien.ah-pine@xrce.xerox.com
Guillaume Jacquet
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan, France
guillaume.jacquet@xrce.xerox.com
Abstract
We propose a system which builds, in a
semi-supervised manner, a resource that
aims at helping a NER system to anno-
tate corpus-specific named entities. This
system is based on a distributional ap-
proach which uses syntactic dependen-
cies for measuring similarities between
named entities. The specificity of the
presented method however, is to combine
a clique-based approach and a clustering
technique that amounts to a soft clustering
method. Our experiments show that the
resource constructed by using this clique-
based clustering system allows to improve
different NER systems.
1 Introduction
In Information Extraction domain, named entities
(NEs) are one of the most important textual units
as they express an important part of the meaning
of a document. Named entity recognition (NER)
is not a new domain (see MUC1 and ACE2 confer-
ences) but some new needs appeared concerning
NEs processing. For instance the NE Oxford illus-
trates the different ambiguity types that are inter-
esting to address:
? intra-annotation ambiguity: Wikipedia lists
more than 25 cities named Oxford in the world
? systematic inter-annotation ambiguity: the
name of cities could be used to refer to the uni-
versity of this city or the football club of this
city. This is the case for Oxford or Newcastle
? non-systematic inter-annotation ambiguity:
Oxford is also a company unlike Newcastle.
The main goal of our system is to act in a com-
plementary way with an existing NER system, in
order to enhance its results. We address two kinds
1http://www-nlpir.nist.gov/related projects/muc/
2http://www.nist.gov/speech/tests/ace
of issues: first, we want to detect and correctly
annotate corpus-specific NEs3 that the NER sys-
tem could have missed; second, we want to correct
some wrong annotations provided by the existing
NER system due to ambiguity. In section 3, we
give some examples of such corrections.
The paper is organized as follows. We present,
in section 2, the global architecture of our system
and from ?2.1 to ?2.6, we give details about each
of its steps. In section 3, we present the evalu-
ation of our approach when it is combined with
other classic NER systems. We show that the re-
sulting hybrid systems perform better with respect
to F-measure. In the best case, the latter increased
by 4.84 points. Furthermore, we give examples of
successful correction of NEs annotation thanks to
our approach. Then, in section 4, we discuss about
related works. Finally we sum up the main points
of this paper in section 5.
2 Description of the system
Given a corpus, the main objectives of our system
are: to detect potential NEs; to compute the possi-
ble annotations for each NE and then; to annotate
each occurrence of these NEs with the right anno-
tation by analyzing its local context.
We assume that this corpus dependent approach
allows an easier NE annotation. Indeed, even if
a NE such as Oxford can have many annotation
types, it will certainly have less annotation possi-
bilities in a specific corpus.
Figure 1 presents the global architecture of our
system. The most important part concerns steps
3 (?2.3) and 4 (?2.4). The aim of these sub-
processes is to group NEs which have the same
annotation with respect to a given context. On
the one hand, clique-based methods (see ?2.3 for
3In our definition a corpus-specific NE is the one which
does not appear in a classic NEs lexicon. Recent news articles
for instance, are often constituted of NEs that are not in a
classic NEs lexicon.
51
Figure 1: General description of our system
details on cliques) are interesting as they allow
the same NE to be in different cliques. In other
words, cliques allow to represent the different pos-
sible annotations of a NE. The clique-based ap-
proach drawback however, is the over production
of cliques which corresponds to an artificial over
production of possible annotations for a NE. On
the other hand, clustering methods aim at struc-
turing a data set and such techniques can be seen
as data compression processes. However, a sim-
ple NEs hard clustering doesn?t allow a NE to be
in several clusters and thus to express its differ-
ent annotations. Then, our proposal is to combine
both methods in a clique-based clustering frame-
work. This combination leads to a soft-clustering
approach that we denote CBC system. The fol-
lowing paragraphs, from 2.1 to 2.6, describe the
respective steps mentioned in Figure 1.
2.1 Detection of potential Named Entities
Different methods exist for detecting potential
NEs. In our system, we used some lexico-
syntactic constraints to extract expressions from a
corpus because it allows to detect some corpus-
specific NEs. In our approach, a potential NE is a
noun starting with an upper-case letter or a noun
phrase which is (see (Ehrmann and Jacquet, 2007)
for similar use):
? a governor argument of an attribute syntactic
relation with a noun as governee argument (e.g.
president
attribute
????? George Bush)
? a governee argument of a modifier syntactic re-
lation with a noun as a governor argument (e.g.
company
modifier
????? Coca-Cola).
The list of potential NEs extracted from the cor-
pus will be denoted NE and the number of NEs
|NE|.
2.2 Distributional space of NEs
The distributional approach aims at evaluating a
distance between words based on their syntac-
tic distribution. This method assumes that words
which appear in the same contexts are semanti-
cally similar (Harris, 1951).
To construct the distributional space associated
to a corpus, we use a robust parser (in our ex-
periments, we used XIP parser (A??t et al, 2002))
to extract chunks (i.e. nouns, noun phrases, . . . )
and syntactic dependencies between these chunks.
Given this parser?s output, we identify triple in-
stances. Each triple has the form w1.R.w2 where
w1 and w2 are chunks and R is a syntactic relation
(Lin, 1998), (Kilgarriff et al, 2004).
One triple gives two contexts (1.w1.R and
2.w2.R) and two chunks (w1 and w2). Then, we
only select chunks w which belong to NE. Each
point in the distributional space is a NE and each
dimension is a syntactic context. CT denotes the
set of all syntactic contexts and |CT| represents its
cardinal.
We illustrate this construction on the sentence
?provide Albania with food aid?. We obtain the
three following triples (note that aid and food aid
are considered as two different chunks):
provide VERB?I-OBJ?Albania NOUN
provide VERB?PREP WITH?aid NOUN
provide VERB?PREP WITH?food aid NP
From these triples, we have the following
chunks and contexts4:
Chunks: Contexts:
provide VERB 1.provide VERB.I-OBJ
Albania NOUN 1.provide VERB.PREP WITH
aid NOUN 2.Albania NOUN.I-OBJ
food aid NP 2.aid NOUN.PREP WITH
2.food aid NP.PREP WITH
According to the NEs detection method de-
scribed previously, we only keep the chunks and
contexts which are in bold in the above table.
4In the context 1.VERB:provide.I-OBJ, the figure 1
means that the verb provide is the governor argument of the
Indirect OBJect relation.
52
We also use an heuristic in order to reduce the
over production of chunks and contexts: in our ex-
periments for example, each NE and each context
should appear more than 10 times in the corpus for
being considered.
D is the resulting (|NE| ? |CT|) NE-Context
matrix where ei : i = 1, . . . , |NE| is a NE and
cj : j = 1, . . . , |CT| is a syntactic context. Then
we have:
D(ei, cj) = Nb. of occ. of cj associated to ei (1)
2.3 Cliques of NEs computation
A clique in a graph is a set of pairwise adja-
cent nodes which is equivalent to a complete sub-
graph. A maximal clique is a clique that is not a
subset of any other clique. Maximal cliques com-
putation was already employed for semantic space
representation (Ploux and Victorri, 1998). In this
work, cliques of lexical units are used to represent
a precise meaning. Similarly, we compute cliques
of NEs in order to represent a precise annotation.
For example, Oxford is an ambiguous NE
but a clique such as <Cambridge, Oxford, Ed-
inburgh University, Edinburgh, Oxford Univer-
sity> allows to focus on the specific annota-
tion <organization> (see (Ehrmann and Jacquet,
2007) for similar use).
Given the distributional space described in the
previous paragraph, we use a probabilistic frame-
work for computing similarities between NEs.
The approach that we propose is inspired from
the language modeling framework introduced in
the information retrieval field (see for example
(Lavrenko and Croft, 2003)). Then, we construct
cliques of NEs based on these similarities.
2.3.1 Similarity measures between NEs
We first compute the maximum likelihood esti-
mation for a NE ei to be associated with a con-
text cj : Pml(cj |ei) =
D(ei,cj)
|ei|
, where |ei| =
?|CT|
j=1 D(ei, cj) is the total occurrences of the NE
ei in the corpus.
This leads to sparse data which is not suitable
for measuring similarities. In order to counter
this problem, we use the Jelinek-Mercer smooth-
ing method: D?(ei, cj) = ?Pml(cj |ei) + (1 ?
?)Pml(cj |CORP) where CORP is the corpus and
Pml(cj |CORP) =
P
i D(ei,cj)P
i,j D(ei,cj)
. In our experi-
ments we took ? = 0.5.
Given D?, we then use the cross-entropy as a
similarity measure between NEs. Let us denote by
s this similarity matrix, we have:
s(ei, e
?
i) = ?
?
cj?CT
D?(ei, cj) log(D
?(ei? , cj)) (2)
2.3.2 From similarity matrix to adjacency
matrix
Next, we convert s into an adjacency matrix de-
noted s?. In a first step, we binarize s as fol-
lows. Let us denote {ei1, . . . , e
i
|NE|}, the list of NEs
ranked according to the descending order of their
similarity with ei. Then, L(ei) is the list of NEs
which are considered as the nearest neighbors of
ei according to the following definition:
L(ei) = (3)
{ei1, ..., e
i
p :
?p
i?=1 s(ei, e
i
i?)
?|NE|
i?=1 s(ei, ei?)
? a; p ? b}
where a ? [0, 1] and b ? {1, . . . , |NE|}. L(ei)
gathers the most significant nearest neighbors of ei
by choosing the ones which bring the a most rele-
vant similarities providing that the neighborhood?s
size doesn?t exceed b. This approach can be seen
as a flexible k-nearest neighbor method. In our
experiments we chose a = 20% and b = 10.
Finally, we symmetrize the similarity matrix as
follows and we obtain s?:
s?(ei, ei?) =
{
1 if ei? ? L(ei) or ei ? L(ei?)
0 otherwise
(4)
2.3.3 Cliques computation
Given s?, the adjacency matrix between NEs, we
compute the set of maximal cliques of NEs de-
noted CLI. Then, we construct the matrix T of
general term:
T (clik, ei) =
{
1 if ei ? clik
0 otherwise
(5)
where clik is an element of CLI. T will be the
input matrix for the clustering method.
In the following, we also use clik
for denoting the vector represented by
(T (clik, e1), . . . , T (clik, e|NE|)).
Figure 2 shows some cliques which contain Ox-
ford that we can obtain with this method. This fig-
ure also illustrates the over production of cliques
since at least cli8, cli10 and cli12 can be annotated
as <organization>.
53
Figure 2: Examples of cliques containing Oxford
2.4 Cliques clustering
We use a clustering technique in order to group
cliques of NEs which are mutually highly simi-
lar. The clusters of cliques which contain a NE
allow to find the different possible annotations of
this NE.
This clustering technique must be able to con-
struct ?pure? clusters in order to have precise an-
notations. In that case, it is desirable to avoid
fixing the number of clusters. That?s the reason
why we propose to use the Relational Analysis ap-
proach described below.
2.4.1 The Relational Analysis approach
We propose to apply the Relational Analysis ap-
proach (RA) which is a clustering model that
doesn?t require to fix the number of clusters
(Michaud and Marcotorchino, 1980), (Be?de?carrax
and Warnesson, 1989). This approach takes as in-
put a similarity matrix. In our context, since we
want to cluster cliques of NEs, the correspond-
ing similarity matrix S between cliques is given
by the dot products matrix taken from T : S =
T ? T ?. The general term of this similarity matrix
is: S(clik, clik?) = Skk? = ?clik, clik??. Then, we
want to maximize the following clustering func-
tion:
?(S,X) = (6)
|CLI|?
k,k?=1
(
Skk? ?
?
(k??,k???)?S+ Sk??k???
|S+|
)
? ?? ?
contkk?
Xkk?
where S+ = {(clik, clik?) : Skk? > 0}.
In other words, clik and clik? have more chances
to be in the same cluster providing that their sim-
ilarity measure, Skk? , is greater or equal to the
mean average of positive similarities.
X is the solution we are looking for. It is a bi-
nary relational matrix with general term: Xkk? =
1, if clik is in the same cluster as clik? ; andXkk? =
0, otherwise. X represents an equivalence rela-
tion. Thus, it must respect the following proper-
ties:
? binarity: Xkk? ? {0, 1};?k, k?,
? reflexivity: Xkk = 1;?k,
? symmetry: Xkk? ?Xk?k = 0;?k, k?,
? transitivity: Xkk? + Xk?k?? ? Xkk?? ?
1;?k, k?, k??.
As the objective function is linear with respect
toX and as the constraints thatX must respect are
linear equations, we can solve the clustering prob-
lem using an integer linear programming solver.
However, this problem is NP-hard. As a result, in
practice, we use heuristics for dealing with large
data sets.
2.4.2 The Relational Analysis heuristic
The presented heuristic is quite similar to another
algorithm described in (Hartigan, 1975) known as
the ?leader? algorithm. But unlike this last ap-
proach which is based upon euclidean distances
and inertial criteria, the RA heuristic aims at max-
imizing the criterion given in (6). A sketch of this
heuristic is given in Algorithm 1, (see (Marco-
torchino and Michaud, 1981) for further details).
Algorithm 1 RA heuristic
Require: nbitr = number of iterations; ?max = maximal
number of clusters; S the similarity matrix
m?
P
(k,k?)?S+ Skk?
|S+|
Take the first clique clik as the first element of the first
cluster
? = 1 where ? is the current number of cluster
for q = 1 to nbitr do
for k = 1 to |CLI| do
for l = 1 to ? do
Compute the contribution of clique clik with clus-
ter clul: contl =
P
clik??clul
(Skk? ?m)
end for
clul? is the cluster id which has the highest contribu-
tion with clique clik and contl? is the corresponding
contribution value
if (contl? < (Skk ?m)) ? (? < ?max) then
Create a new cluster where clique clik is the first
element and ?? ?+ 1
else
Assign clique clik to cluster clul?
if the cluster where was taken clik before its new
assignment, is empty then
?? ?? 1
end if
end if
end for
end for
We have to provide a number of iterations
54
or/and a delta threshold in order to have an approx-
imate solution in a reasonable processing time.
Besides, it is also required a maximum number of
clusters but since we don?t want to fix this param-
eter, we put by default ?max = |CLI|.
Basically, this heuristic has a O(nbitr??max?
|CLI|) computation cost. In general terms, we can
assume that nbitr << |CLI|, but not ?max <<
|CLI|. Thus, in the worst case, the algorithm has
a O(?max ? |CLI|) computation cost.
Figure 3 gives some examples of clusters of
cliques5 obtained using the RA approach.
Figure 3: Examples of clusters of cliques (only the
NEs are represented) and their associated contexts
2.5 NE resource construction using the CBC
system?s outputs
Now, we want to exploit the clusters of cliques in
order to annotate NE occurrences. Then, we need
to construct a NE resource where for each pair (NE
x syntactic context) we have an annotation. To this
end, we need first, to assign a cluster to each pair
(NE x syntactic context) (?2.5.1) and second, to
assign each cluster an annotation (?2.5.2).
2.5.1 Cluster assignment to each pair (NE x
syntactic context)
For each cluster clul we provide a score
Fc(cj , clul) for each context cj and a score
5We only represent the NEs and their frequency in the
cluster which corresponds to the number of cliques which
contain the NEs. Furthermore, we represent the most relevant
contexts for this cluster according to equation (7) introduced
in the following.
Fe(ei, clul) for each NE ei. These scores6 are
given by:
Fc(cj , clul) = (7)
?
ei?clul
D(ei, cj)
?|NE|
i=1 D(ei, cj)
?
ei?clul
1{D(ei,cj) 6=0}
where 1{P} equals 1 if P is true and 0 otherwise.
Fe(ei, clul) = #(clul, ei) (8)
Given a NE ei and a syntactic context
cj , we now introduce the contextual clus-
ter assignment matrix Actxt(ei, cj) as fol-
lows: Actxt(ei, cj) = clu? where: clu? =
Argmax{clul:clul3ei;Fe(ei,clul)>1}Fc(cj , clul).
In other words, clu? is the cluster for which we
find more than one occurrence of ei and the high-
est score related to the context cj .
Furthermore, we compute a default cluster as-
signment matrix Adef , which does not depend on
the local context: Adef (ei) = clu? where: clu? =
Argmax{clul:clul3{clik:clik3ei}}|clik|.
In other words, clu? is the cluster containing the
biggest clique clik containing ei.
2.5.2 Clusters annotation
So far, the different steps that we have introduced
were unsupervised. In this paragraph, our aim is to
give a correct annotation to each cluster (hence, to
all NEs in this cluster). To this end, we need some
annotation seeds and we propose two different
semi-supervised approaches (regarding the classi-
fication given in (Nadeau and Sekine, 2007)). The
first one is the manual annotation of some clusters.
The second one proposes an automatic cluster an-
notation and assumes that we have some NEs that
are already annotated.
Manual annotation of clusters This method is
fastidious but it is the best way to match the cor-
pus data with a specific guidelines for annotating
NEs. It also allows to identify new types of an-
notation. We used the ACE2007 guidelines for
manually annotating each cluster. However, our
CBC system leads to a high number of clusters of
cliques and we can?t annotate each of them. For-
tunately, it also leads to a distribution of the clus-
ters? size (number of cliques by cluster) which is
6For data fusion tasks in information retrieval field, the
scoring method in equation (7) is denoted CombMNZ (Fox
and Shaw, 1994). Other scoring approaches can be used see
for example (Cucchiarelli and Velardi, 2001).
55
similar to a Zipf distribution. Consequently, in our
experiments, if we annotate the 100 biggest clus-
ters, we annotate around eighty percent of the de-
tected NEs (see ?3).
Automatic annotation of clusters We suppose
in this context that many NEs in NE are already
annotated. Thus, under this assumption, we have
in each cluster provided by the CBC system, both
annotated and non-annotated NEs. Our goal is to
exploit the available annotations for refining the
annotation of a cluster by implicitly taking into
account the syntactic contexts and for propagating
the available annotations to NEs which have no
annotation.
Given a cluster clul of cliques, #(clul, ei) is the
weight of the NE ei in this cluster: it is the number
of cliques in clul that contain ei. For all annota-
tions ap in the set of all possible annotations AN,
we compute its associated score in cluster clul: it
is the sum of the weights of NEs in clul that is
annotated ap.
Then, if the maximal annotation score is greater
than a simple majority (half) of the total votes7, we
assign the corresponding annotation to the clus-
ter. We precise that the annotation <none>8 is
processed in the same way as any other annota-
tions. Thus, a cluster can be globally annotated
<none>. The limit of this automatic approach is
that it doesn?t allow to annotate new NE types than
the ones already available.
In the following, we will denote by Aclu(clul)
the annotation of the cluster clul.
The cluster annotation matrix Aclu associated
to the contextual cluster assignment matrix Actxt
and the default cluster assignment matrix Adef in-
troduced previously will be called the CBC sys-
tem?s NE resource (or shortly the NE resource).
2.6 NEs annotation processes using the NE
resource
In this paragraph, we describe how, given the CBC
system?s NE resource, we annotate occurrences of
NEs in the studied corpus with respect to its local
context. We precise that for an occurrence of a NE
ei its associated local context is the set of syntac-
tical dependencies cj in which ei is involved.
7The total votes number is given byP
ei?clul
#(clul, ei).
8The NEs which don?t have any annotation.
2.6.1 NEs annotation process for the CBC
system
Given a NE occurrence and its local context we
can use Actxt(ei, cj) and Adef (ei) in order to get
the default annotation Aclu(Adef (ei)) and the list
of contextual annotations {Aclu(Actxt(ei, cj))}j .
Then for annotating this NE occurrence using
our NE resource, we apply the following rules:
? if the list of contextual annotations
{Aclu(Actxt(ei, cj))}j is conflictual, we
annotate the NE occurrence as <none>,
? if the list of contextual annotations is non-
conflictual, then we use the corresponding an-
notation to annotate the NE occurrence
? if the list of contextual annotations is empty,
we use the default annotation Aclu(Adef (ei)).
The NE resource plus the annotation process de-
scribed in this paragraph lead to a NER system
based on the CBC system. This NER system will
be called CBC-NER system and it will be tested in
our experiments both alone and as a complemen-
tary resource.
2.6.2 NEs annotation process for an hybrid
system
We place ourselves into an hybrid situation where
we have two NER systems (NER 1 + NER 2)
which provide two different lists of annotated
NEs. We want to combine these two systems when
annotating NEs occurrences.
Therefore, we resolve any conflicts by applying
the following rules:
? If the same NE occurrence has two different an-
notations from the two systems then there are
two cases. If one of the two system is CBC-
NER system then we take its annotation; oth-
erwise we take the annotation provided by the
NER system which gave the best precision.
? If a NE occurrence is included in another one
we only keep the biggest one and its annota-
tion. For example, if Jacques Chirac is anno-
tated <person> by one system and Chirac by
<person> by the other system, then we only
keep the first annotation.
? If two NE occurrences are contiguous and have
the same annotation, we merge the two NEs in
one NE occurrence.
3 Experiments
The system described in this paper rather target
corpus-specific NE annotation. Therefore, our ex-
56
periments will deal with a corpus of recent news
articles (see (Shinyama and Sekine, 2004) for
motivations regarding our corpus choice) rather
than well-known annotated corpora. Our corpus
is constituted of news in English published on
the web during two weeks in June 2008. This
corpus is constituted of around 300,000 words
(10Mb) which doesn?t represent a very large cor-
pus. These texts were taken from various press
sources and they involve different themes (sports,
technology, . . . ). We extracted randomly a sub-
set of articles and manually annotated 916 NEs (in
our experiments, we deal with three types of an-
notation namely <person>, <organization> and
<location>). This subset constitutes our test set.
In our experiments, first, we applied the XIP
parser (A??t et al, 2002) to the whole corpus in or-
der to construct the frequency matrix D given by
(1). Next, we computed the similarity matrix be-
tween NEs according to (2) in order to obtain s? de-
fined by (4). Using the latter, we computed cliques
of NEs that allow us to obtain the assignment ma-
trix T given by (5). Then we applied the clustering
heuristic described in Algorithm 1. At this stage,
we want to build the NE resource using the clus-
ters of cliques. Therefore, as described in ?2.5,
we applied two kinds of clusters annotations: the
manual and the automatic processes. For the first
one, we manually annotated the 100 biggest clus-
ters of cliques. For the second one, we exploited
the annotations provided by XIP NER (Brun and
Hage`ge, 2004) and we propagated these annota-
tions to the different clusters (see ?2.5.2).
The different materials that we obtained consti-
tute the CBC system?s NE resource. Our aim now
is to exploit this resource and to show that it allows
to improve the performances of different classic
NER systems.
The different NER systems that we tested are
the following ones:
? CBC-NER system M (in short CBC M) based
on the CBC system?s NE resource using the
manual cluster annotation (line 1 in Table 1),
? CBC-NER system A (in short CBC A) based
on the CBC system?s NE resource using the au-
tomatic cluster annotation (line 1 in Table 1),
? XIP NER or in short XIP (Brun and Hage`ge,
2004) (line 2 in Table 1),
? Stanford NER (or in short Stanford) associ-
ated to the following model provided by the
tool and which was trained on different news
Systems Prec. Rec. F-me.
1
CBC-NER system M 71.67 23.47 35.36
CBC-NER system A 70.66 32.86 44.86
2
XIP NER 77.77 56.55 65.48
XIP + CBC M 78.41 60.26 68.15
XIP + CBC A 76.31 60.48 67.48
3
Stanford NER 67.94 68.01 67.97
Stanford + CBC M 69.40 71.07 70.23
Stanford + CBC A 70.09 72.93 71.48
4
GATE NER 63.30 56.88 59.92
GATE + CBC M 66.43 61.79 64.03
GATE + CBC A 66.51 63.10 64.76
5
Stanford + XIP 72.85 75.87 74.33
Stanford + XIP + CBC M 72.94 77.70 75.24
Stanford + XIP + CBC A 73.55 78.93 76.15
6
GATE + XIP 69.38 66.04 67.67
GATE + XIP + CBC M 69.62 67.79 68.69
GATE + XIP + CBC A 69.87 69.10 69.48
7
GATE + Stanford 63.12 69.32 66.07
GATE + Stanford + CBC M 65.09 72.05 68.39
GATE + Stanford + CBC A 65.66 73.25 69.25
Table 1: Results given by different hybrid NER
systems and coupled with the CBC-NER system
corpora (CoNLL, MUC6, MUC7 and ACE):
ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel
et al, 2005) (line 3 in Table 1),
? GATE NER or in short GATE (Cunningham et
al., 2002) (line 4 in Table 1),
? and several hybrid systems which are given by
the combination of pairs taken among the set
of the three last-mentioned NER systems (lines
5 to 7 in Table 1). Notice that these baseline
hybrid systems use the annotation combination
process described in ?2.6.1.
In Table 1 we first reported in each line, the re-
sults given by each system when they are applied
alone (figures in italics). These performances rep-
resent our baselines. Second, we tested for each
baseline system, an extended hybrid system that
integrates the CBC-NER systems (with respect to
the combination process detailed in ?2.6.2).
The first two lines of Table 1 show that the
two CBC-NER systems alone lead to rather poor
results. However, our aim is to show that the
CBC-NER system is, despite its low performances
alone, complementary to other basic NER sys-
tems. In other words, we want to show that the
exploitation of the CBC system?s NE resource is
beneficial and non-redundant compared to other
baseline NER systems.
This is actually what we obtained in Table 1 as
for each line from 2 to 7, the extended hybrid sys-
tems that integrate the CBC-NER systems (M or
57
A) always perform better than the baseline either
in terms of precision9 or recall. For each line, we
put in bold the best performance according to the
F-measure.
These results allow us to show that the NE re-
source built using the CBC system is complemen-
tary to any baseline NER systems and that it al-
lows to improve the results of the latter.
In order to illustrate why the CBC-NER systems
are beneficial, we give below some examples taken
from the test corpus for which the CBC system A
had allowed to improve the performances by re-
spectively disambiguating or correcting a wrong
annotation or detecting corpus-specific NEs.
First, in the sentence ?From the start, his par-
ents, Lourdes and Hemery, were with him.?, the
baseline hybrid system Stanford + XIP anno-
tated the ambiguous NE ?Lourdes? as <location>
whereas Stanford + XIP + CBC A gave the correct
annotation <person>.
Second, in the sentence ?Got 3 percent chance
of survival, what ya gonna do?? The back read,
?A) Fight Through, b) Stay Strong, c) Overcome
Because I Am a Warrior.?, the baseline hybrid
system Stanford + XIP annotated ?Warrior? as
<organization> whereas Stanford + XIP + CBC
A corrected this annotation with <none>.
Finally, in the sentence ?Matthew, also a fa-
vorite to win in his fifth and final appearance,
was stunningly eliminated during the semifinal
round Friday when he misspelled ?secernent?.?,
the baseline hybrid system Stanford + XIP didn?t
give any annotation to ?Matthew? whereas Stan-
ford + XIP + CBC A allowed to give the annota-
tion <person>.
4 Related works
Many previous works exist in NEs recognition and
classification. However, most of them do not build
a NEs resource but exploit external gazetteers
(Bunescu and Pasca, 2006), (Cucerzan, 2007).
A recent overview of the field is given in
(Nadeau and Sekine, 2007). According to this pa-
per, we can classify our method in the category
of semi-supervised approaches. Our proposal is
close to (Cucchiarelli and Velardi, 2001) as it uses
syntactic relations (?2.2) and as it relies on exist-
ing NER systems (?2.6.2). However, the partic-
ularity of our method concerns the clustering of
9Except for XIP+CBC A in line 2 where the precision is
slightly lower than XIP?s one.
cliques of NEs that allows both to represent the
different annotations of the NEs and to group the
latter with respect to one precise annotation ac-
cording to a local context.
Regarding this aspect, (Lin and Pantel, 2001)
and (Ngomo, 2008) also use a clique computa-
tion step and a clique merging method. However,
they do not deal with ambiguity of lexical units
nor with NEs. This means that, in their system, a
lexical unit can be in only one merged clique.
From a methodological point of view, our pro-
posal is also close to (Ehrmann and Jacquet, 2007)
as the latter proposes a system for NEs fine-
grained annotation, which is also corpus depen-
dent. However, in the present paper we use all
syntactic relations for measuring the similarity be-
tween NEs whereas in the previous mentioned
work, only specific syntactic relations were ex-
ploited. Moreover, we use clustering techniques
for dealing with the issue related to over produc-
tion of cliques.
In this paper, we construct a NE resource from
the corpus that we want to analyze. In that con-
text, (Pasca, 2004) presents a lightly supervised
method for acquiring NEs in arbitrary categories
from unstructured text of Web documents. How-
ever, Pasca wants to improve web search whereas
we aim at annotating specific NEs of an ana-
lyzed corpus. Besides, as we want to focus on
corpus-specific NEs, our work is also related to
(Shinyama and Sekine, 2004). In this work, the
authors found a significant correlation between the
similarity of the time series distribution of a word
and the likelihood of being a NE. This result mo-
tivated our choice to test our approach on recent
news articles rather than on well-known annotated
corpora.
5 Conclusion
We propose a system that allows to improve NE
recognition. The core of this system is a clique-
based clustering method based upon a distribu-
tional approach. It allows to extract, analyze and
discover highly relevant information for corpus-
specific NEs annotation. As we have shown in our
experiments, this system combined with another
one can lead to strong improvements. Other appli-
cations are currently addressed in our team using
this approach. For example, we intend to use the
concept of clique-based clustering as a soft clus-
tering method for other issues.
58
References
S. A??t, J.P. Chanod, and C. Roux. 2002. Robustness
beyond shallowness: incremental dependency pars-
ing. NLE Journal.
C. Be?de?carrax and I. Warnesson. 1989. Relational
analysis and dictionnaries. In Proceedings of AS-
MDA 1988, pages 131?151. Wiley, London, New-
York.
C. Brun and C. Hage`ge. 2004. Intertwining deep
syntactic processing and named entity detection. In
Proceedings of ESTAL 2004, Alicante, Spain.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL 2006.
A. Cucchiarelli and P. Velardi. 2001. Unsupervised
Named Entity Recognition using syntactic and se-
mantic contextual evidence. Computational Lin-
guistics, 27(1).
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP/CoNLL 2007, Prague, Czech Republic.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of ACL 2002, Philadel-
phia.
M. Ehrmann and G. Jacquet. 2007. Vers une dou-
ble annotation des entite?s nomme?es. Traitement Au-
tomatique des Langues, 47(3).
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL 2005.
E.A. Fox and J.A. Shaw. 1994. Combination of multi-
ple searches. In Proceedings of the 3rd NIST TREC
Conference, pages 105?109.
Z. Harris. 1951. Structural Linguistics. University of
Chicago Press.
J.A. Hartigan. 1975. Clustering Algorithms. John Wi-
ley and Sons.
A. Kilgarriff, P. Rychly, P. Smr, and D. Tugwell. 2004.
The sketch engine. In In Proceedings of EURALEX
2004.
V. Lavrenko and W.B. Croft. 2003. Relevance models
in information retrieval. In W.B. Croft and J. Laf-
ferty (Eds), editors, Language modeling in informa-
tion retrieval. Springer.
D. Lin and P. Pantel. 2001. Induction of semantic
classes from natural language text. In Proceedings
of ACM SIGKDD.
D. Lin. 1998. Using collocation statistics in informa-
tion extraction. In Proceedings of MUC-7.
J.F. Marcotorchino and P. Michaud. 1981. Heuris-
tic approach of the similarity aggregation problem.
Methods of operation research, 43:395?404.
P. Michaud and J.F. Marcotorchino. 1980. Optimisa-
tion en analyse de donne?es relationnelles. In Data
Analysis and informatics. North Holland Amster-
dam.
D. Nadeau and S. Sekine. 2007. A survey of Named
Entity Recognition and Classification. Lingvisticae
Investigationes, 30(1).
A. C. Ngonga Ngomo. 2008. Signum a graph algo-
rithm for terminology extraction. In Proceedings of
CICLING 2008, Haifa, Israel.
M. Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of CIKM
2004, New York, NY, USA.
S. Ploux and B. Victorri. 1998. Construction d?espaces
se?mantiques a` l?aide de dictionnaires de synonymes.
TAL, 39(1).
Y. Shinyama and S. Sekine. 2004. Named Entity Dis-
covery using comparable news articles. In Proceed-
ings of COLING 2004, Geneva.
59
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 488?491,
Prague, June 2007. c?2007 Association for Computational Linguistics
XRCE-M: A Hybrid System for Named Entity Metonymy Resolution 
*Caroline Brun 
 
*Maud Ehrmann 
 
*Guillaume Jacquet 
 
 
* Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240 Meylan France 
*{Caroline.Brun, Maud.Ehrmann, Guillaume.Jacquet}@xrce.xerox.com 
 
Abstract 
This paper describes our participation to the 
Metonymy resolution at SemEval 2007 (task 
#8). In order to perform named entity me-
tonymy resolution, we developed a hybrid 
system based on a robust parser that extracts 
deep syntactic relations combined with a 
non-supervised distributional approach, also 
relying on the relations extracted by the 
parser.  
1 Description of our System 
SemEval 2007 introduces a task aiming at resolving 
metonymy for named entities, for location and or-
ganization names (Markert and Nissim 2007). Our 
system addresses this task by combining a symbolic 
approach based on robust deep parsing and lexical 
semantic information, with a distributional method 
using syntactic context similarities calculated on 
large corpora. Our system is completely unsuper-
vised, as opposed to state-of-the-art systems (see  
(Market and Nissim, 2005)).  
1.1 Robust and Deep Parsing Using XIP 
We use the Xerox Incremental Parser (XIP, (A?t et 
al., 2002)) to perform robust and deep syntactic 
analysis. Deep syntactic analysis consists here in the 
construction of a set of syntactic relations1 from an 
input text.  These relations, labeled with deep syn-
tactic functions, link lexical units of the input text 
and/or more complex syntactic domains that are 
constructed during the processing (mainly chunks, 
see (Abney, 1991)).  
                                                 
                                                
1 inspired from dependency grammars, see (Mel??uk, 
1998), and (Tesni?re, 1959). 
Moreover, together with surface syntactic relations, 
the parser calculates more sophisticated relations 
using derivational morphologic properties, deep 
syntactic properties2, and some limited lexical se-
mantic coding (Levin's verb class alternations, see 
(Levin, 1993)), and some elements of the Framenet3 
classification, (Ruppenhofer et al, 2006)). These 
deep syntactic relations correspond roughly to the 
agent-experiencer roles that is subsumed by the 
SUBJ-N relation and to the patient-theme role sub-
sumed by the OBJ-N relation, see (Brun and  Ha-
g?ge, 2003). Not only verbs bear these relations but 
also deverbal nouns with their corresponding argu-
ments.  
Here is an example of an output (chunks and 
deep syntactic relations): 
Lebanon still wanted to see the implementation of a UN 
resolution 
 
TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the 
implementation} PP{of NP{a UN resolution}} .} 
MOD_PRE(wanted,still) 
MOD_PRE(resolution,UN) 
MOD_POST(implementation,resolution) 
COUNTRY(Lebanon) 
ORGANISATION(UN) 
EXPERIENCER_PRE(wanted,Lebanon) 
EXPERIENCER(see,Lebanon) 
CONTENT(see,implementation) 
EMBED_INFINIT(see,wanted) 
OBJ-N(implement,resolution) 
1.2 Adaptation to the Task 
Our parser includes a module for ?standard? 
named entity recognition, but needs to be adapted to 
handle named entity metonymy. Following the 
guidelines of the SemEval task #8, we performed a 
 
2 Subject and object of infinitives in the context of con-
trol verbs. 
3 http://framenet.icsi.berkeley.edu/ 
488
corpus study on the trial data in order to detect lexi-
cal and syntactic regularities triggering a metonymy, 
for both location names and organization names. 
For example, we examined the subject relation be-
tween organizations or locations and verbs and we 
then classify the verbs accordingly: we draw hy-
pothesis like ?if a location name is the subject of a 
verb referring to an economic action, like import, 
provide, refund, repay, etc., then it is a place-for-
people?. We adapted our parser by adding dedicated 
lexicons that encode the information collected from 
the corpus and develop rules modifying the interpre-
tation of the entity, for example:  
 
 If (LOCATION(#1) & SUBJ-N(#2[v_econ],#1))4
 ? PLACE-FOR-PEOPLE(#1) 
 
We focus our study on relations like subject, object, 
experiencer, content, modifiers (nominal and prepo-
sitional) and attributes.  We also capitalize on the 
already-encoded lexical information attached to 
verbs by the parser, like communication verbs like 
say, deny, comment, or categories of the FrameNet 
Experiencer subject frame, i.e. verbs like feel, sense, 
see. This information was very useful since experi-
encers denote persons, therefore all organizations or 
locations having an experiencer role can be consid-
ered as organization-for-members or place-for-
people. Here is an example of output5, when apply-
ing the modified parser on the following sentence: 
?It was the largest Fiat everyone had ever seen?. 
ORG-FOR-PRODUCT(Fiat) 
MOD_PRE(seen,ever) 
SUBJ-N_PRE(was,It) 
EXPERIENCER_PRE(seen,everyone) 
SUBJATTR(It,Fiat) 
    QUALIF(Fiat,largest)  
 
Here, the relation QUALIF(Fiat, largest) triggers 
the metonymical interpretation of ?Fiat? as org-for-
product. 
This first development step is the starting point of 
our methodology, which is completed by a non-
supervised distributional approach described in the 
next section.  
                                                 
4 Which read as ?if the parser has detected a location 
name (#1), which is the subject of a verb (#2) bearing the 
feature ?v-econ?, then create a PLACE-FOR-PEOPLE 
unary predicate on #1.  
5 Only dependencies are shown. 
1.3 Hybridizing with a Distributional Approach 
The distributional approach proposes to establish a 
distance between words depending on there syntac-
tic distribution. 
The distributional hypothesis is that words that ap-
pear in similar contexts are semantically similar 
(Harris, 1951): the more two words have the same 
distribution, i.e. are found in the same syntactic con-
texts, the more they are semantically close. 
We propose to apply this principle for metonymy 
resolution. Traditionally, the distributional approach 
groups words like USA, Britain, France, Germany 
because there are in the same syntactical contexts:  
 
 (1) Someone live in Germany. 
(2) Someone works in Germany. 
(3) Germany declares something. 
(4) Germany signs something. 
 
The metonymy resolution task implies to distin-
guish the literal cases, (1) & (2), from the meto-
nymic ones, (3) & (4). Our method establishes these 
distinctions using the syntactic context distribution. 
We group contexts occurring with the same words: 
the syntactic contexts live in and work in are occur-
ring with Germany, France, country, city, place, 
when syntactic contexts subject-of-declare and sub-
ject-of-sign are occurring with Germany, France, 
someone, government, president. 
For each Named Entity annotation, the hybrid 
method consists in using symbolic annotation if 
there is (?1.2), else using distributional annotation 
(?1.3) as presented below. 
Method: We constructed a distributional space with 
the 100M-word BNC. We prepared the corpus by 
lemmatizing and then parsing with the same robust 
parser than for the symbolic approach (XIP, see sec-
tion 3.1). It allows us to identify triple instances. 
Each triple have the form w1.R.w2 where w1 and 
w2 are lexical units and R is a syntactic relation 
(Lin, 1998; Kilgarriff & al. 2004).  
Our approach can be distinguished from classical 
distributional approach by different points. 
First, we use triple occurrences to build a distribu-
tional space (one triple implies two contexts and 
two lexical units), but we use the transpose of the 
classical space: each point xi of this space is a syn-
tactical context (with the form R.w.), each dimen-
sion j is a lexical units, and each value xi(j) is the 
frequency of corresponding triple occurrences. Sec-
489
ond, our lexical units are words but also complex 
nominal groups or verbal groups. Third, contexts 
can be simple contexts or composed contexts6. 
We illustrate these three points on the phrase pro-
vide Albania with food aid. The XIP parser gives 
the following triples where for example, food aid is 
considered as a lexical unit: 
OBJ-N('VERB:provide','NOUN: Albania'). 
PREP_WITH('VERB: provide ','NOUN:aid'). 
PREP_WITH('VERB: provide ','NP:food aid'). 
From these triples, we create the following lexical 
units and contexts (in the context 1.VERB: provide. 
OBJ-N, ?1? mean that the verb provide is the gov-
ernor of the relation OBJ-N): 
Words: Contexts: 
VERB:provide 1.VERB: provide. OBJ-N 
NOUN:Albania 1.VERB: provide.PREP_WITH 
NOUN:aid 2.NOUN: Albania.OBJ-N 
NP:food aid 2.NOUN: aid. PREP_WITH 
 2.NP: food aid. PREP_WITH 
 1.VERB:provide.OBJ-N+2.NOUN:aid. PREP_WITH 
 1.VERB:provide.OBJ-N+2.NP:food aid. PREP_WITH 
 1.VERB:provide.PREP_WITH +2.NO:Albania.OBJ-N 
 
We use a heuristic to control the high productivity 
of these lexical units and contexts. Each lexical unit 
and each context should appear more than 100 times 
in the corpus. From the 100M-word BNC we ob-
tained 60,849 lexical units and 140,634 contexts. 
Then, our distributional space has 140,634 units and 
60,849 dimensions. 
Using the global space to compute distances be-
tween each context is too consuming and would 
induce artificial ambiguity (Jacquet, Venant, 2005). 
If any named entity can be used in a metonymic 
reading, in a given corpus each named entity has not 
the same distribution of metonymic readings. The 
country Vietnam is more frequently used as an event 
than France or Germany, so, knowing that a context 
is employed with Vietnam allow to reduce the meto-
nymic ambiguity. 
For this, we construct a singular sub-space de-
pending to the context and to the lexical unit (the 
ambiguous named entity): 
For a given couple context i + lexical unit j we 
construct a subspace as follows:  
Sub_contexts = list of contexts which are occur-
ring with the word i. If there are more than k con-
texts, we take only the k more frequents. 
Sub_dimension = list of lexical units which are 
occurring with at least one of the contexts from the 
                                                 
6 For our application, one context can be composed by 
two simple contexts. 
Sub_contexts list. If there are more than n words, 
we take only the n more frequents (relative fre-
quency) with the Sub_contexts list (for this applica-
tion, k = 100 and n = 1,000). 
We reduce dimensions of this sub-space to 10 
dimensions with a PCA (Principal Components 
Analysis). 
In this new reduced space (k*10), we compute 
the closest context of the context j with the Euclid-
ian distance. 
At this point, we use the results of the symbolic 
approach described before as starting point. We at-
tribute to each context of the Sub_contexts list, the 
annotation, if there is, attributed by symbolic rules. 
Each kind of annotation (literal, place-for-people, 
place-for-event, etc) is attributed a score corre-
sponding to the sum of the scores obtained by each 
context annotated with this category. The score of a 
context i  decreases in inverse proportion to its dis-
tance from the context j: score(context i) = 
1/d(context i, context j) where d(i,j) is the Euclidian 
distance between i and j. 
We illustrate this process with the sentence pro-
vide Albania with food aid. The unit Albania is 
found in 384 different contexts (|Sub_contexts| = 
384) and 54,183 lexical units are occurring with at 
least one of the contexts from the Sub_contexts list 
(|Sub_dimension| = 54,183). 
After reducing dimension with PCA, we obtain 
the context list below ordered by closeness with the 
given context (1.VERB:provide.OBJ-N):  
Contexts   d symb. annot. 
1.VERB:provide.OBJ-N  0.00  
1.VERB:allow.OBJ-N  0.76         place-for-people 
1.VERB:include.OBJ-N  0.96  
2.ADJ:new.MOD_PRE  1.02  
1.VERB:be.SUBJ-N  1.43  
1.VERB:supply.SUBJ-N_PRE 1.47 literal 
1.VERB:become.SUBJ-N_PRE 1.64  
1.VERB:come.SUBJ-N_PRE  1.69  
1.VERB:support.SUBJ-N_PRE 1.70          place-for-people 
etc. 
 
Score for each metonymic annotation of Albania: 
? place-for-people 3.11 
 literal  1.23 
place-for-event  0.00 
?  0.00 
The score obtained by each annotation type al-
lows annotating this occurrence of Albania as a 
place-for-people metonymic reading. If we can?t 
choose only one annotation (all score = 0 or equal-
ity between two annotations) we do not annotate.  
490
2 Evaluation and Results 
The following tables show the results on the test 
corpus: 
type Nb. 
samp 
accuracy coverage Baseline 
accuracy 
Baseline 
coverage 
Loc/coarse 908 0.851 1 0.794 1 
Loc/medium 908 0.848 1 0.794 1 
Loc /fine 908 0.841 1 0.794 1 
Org/coarse 842 0.732 1 0.618 1 
Org/medium 842 0.711 1 0.618 1 
Org/fine 842 0.700 1 0.618 1 
Table 1: Global Results 
 
 Nb 
occ. 
Prec. Recall F-score
Literal 721 0.867 0.960 0.911 
Place-for-people 141 0.651 0.490 0.559 
Place-for-event 10 0.5 0.1 0.166 
Place-for-product 1 _ 0 0 
Object-for-name 4 1 0.5 0.666 
Object-for-representation 0 _ _ _ 
Othermet 11 _ 0 0 
mixed 20 _ 0 0 
Table 2: Detailed Results for Locations 
 
 Nb 
occ. 
Prec. Recall F-score
Literal 520 0.730 0.906 0.808 
Organization-for-members 161 0.622 0.522 0.568 
Organization-for-event 1 _ 0 0 
Organization-for-product 67 0.550 0.418 0.475 
Organization-for-facility 16 0.5 0.125 0.2 
Organization-for-index 3 _ 0 0 
Object-for-name 6 1 0.666 0.8 
Othermet 8 _ 0 0 
Mixed  60 _ 0 0 
Table 3: Detailed Results for Organizations 
 
The results obtained on the test corpora are above 
the baseline for both location and organization 
names and therefore are very encouraging for the 
method we developed. However, our results on the 
test corpora are below the ones we get on the train 
corpora, which indicates that there is room for im-
provement for our methodology.  
Identified errors are of different nature: 
Parsing errors: For example in the sentence ?Many 
galleries in the States, England and France de-
clined the invitation.?, because the analysis of the 
coordination is not correct, France is calculated as 
subject of declined, a context triggering a place-for-
people interpretation, which is wrong here.  
Mixed cases: These phenomena, while relatively 
frequent in the corpora, are not properly treated. 
Uncovered contexts: some of the syntactico-
semantic contexts triggering a metonymy are not 
covered by the system at the moment.  
3 Conclusion 
This paper describes a system combining a sym-
bolic and a non-supervised distributional approach, 
developed for resolving location and organization 
names metonymy. We plan to pursue this work in 
order to improve the system on the already-covered 
phenomenon as well as on different names entities.  
References 
Abney S. 1991. Parsing by Chunks.  In Robert Berwick, Steven 
Abney and Carol Teny (eds.). Principle-based Parsing, Klu-
wer Academics Publishers.  
A?t-Mokhtar S., Chanod, J.P., Roux, C. 2002. Robustness be-
yond Shallowness: Incremental Dependency Parsing. Spe-
cial issue of NLE journal.  
Brun, C., Hag?ge C., 2003. Normalization and Paraphrasing 
Using Symbolic Methods, Proceeding of the Second Interna-
tional Workshop on Paraphrasing. ACL 2003, Vol. 16, Sap-
poro, Japan.  
Harris Z. 1951. Structural Linguistics, University of Chicago 
Press. 
Jacquet G.,Venant F. 2003. Construction automatique de clas-
ses de s?lection distributionnelle, In Proc. TALN 2003, 
Dourdan. 
Kilgarriff A., Rychly P., Smrz P., Tugwell D.  2004. The sketch 
engine. In Proc. EURALEX, pages 105-116. 
Levin, B. 1993. English Verb Classes and Alternations ? A 
preliminary Investigation. The University of Chicago Press.  
Nissim, M. and Markert, K. 2005. Learning to buy a Renault 
and to talk to a BMW: A supervised approach to conven-
tional metonymy. Proceedings of the 6th International Work-
shop on Computational Semantics, Tilburg. 
Nissim, M. and Markert, K. 2007. SemEval-2007 Task 08: Me-
tonymy Resolution at SemEval-2007. In Proceedings of Se-
mEval-2007.  
Lin D. 1998. Automatic retrieval and clustering of similar 
words. In COLING-ACL, pages 768-774. 
Mel??uk I. 1988. Dependency Syntax. State University of New 
York, Albany.  
Ruppenhofer, J. Michael Ellsworth, Miriam R. L. Petruck, 
Christopher R Johnson and Jan Scheffczyk. 2006. Framenet 
II: Extended Theory and Practice.  
Tesni?re L. 1959. El?ments de Syntaxe Structurale. Klincksiek 
Eds. (Corrected edition Paris 1969).  
491
