Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 37?45,
Beijing, August 2010
Plagiarism Detection across Distant Language Pairs
Alberto Barro?n-Ceden?o Paolo Rosso
Natural Language Engineering Lab. - ELiRF
Universidad Polite?cnica de Valencia
{lbarron, prosso}@dsic.upv.es
Eneko Agirre Gorka Labaka
IXA NLP Group
Basque Country University
{e.agirre, gorka.labaka}@ehu.es
Abstract
Plagiarism, the unacknowledged reuse of
text, does not end at language boundaries.
Cross-language plagiarism occurs if a text
is translated from a fragment written in a
different language and no proper citation
is provided. Regardless of the change of
language, the contents and, in particular,
the ideas remain the same. Whereas dif-
ferent methods for the detection of mono-
lingual plagiarism have been developed,
less attention has been paid to the cross-
language case.
In this paper we compare two recently
proposed cross-language plagiarism de-
tection methods (CL-CNG, based on char-
acter n-grams and CL-ASA, based on sta-
tistical translation), to a novel approach
to this problem, based on machine trans-
lation and monolingual similarity analy-
sis (T+MA). We explore the effectiveness
of the three approaches for less related
languages. CL-CNG shows not be ap-
propriate for this kind of language pairs,
whereas T+MA performs better than the
previously proposed models.
1 Introduction
Plagiarism is a problem in many scientific and cul-
tural fields. Text plagiarism may imply differ-
ent operations: from a simple cut-and-paste, to
the insertion, deletion and substitution of words,
up to an entire process of paraphrasing. Differ-
ent models approach the detection of monolin-
gual plagiarism (Shivakumar and Garc??a-Molina,
1995; Hoad and Zobel, 2003; Maurer et al, 2006).
Each of these models is appropriate only in those
cases where all the implied documents are written
in the same language.
Nevertheless, the problem does not end at lan-
guage boundaries. Plagiarism is also committed if
the reused text is translated from a fragment writ-
ten in a different language and no citation is pro-
vided. When plagiarism is generated by a transla-
tion process, it is known as cross-language plagia-
rism (CLP).
Less attention has been paid to the detection of
this kind of plagiarism due to its enhanced diffi-
culty (Ceska et al, 2008; Barro?n-Ceden?o et al,
2008; Potthast et al, 2010). In fact, in the recently
held 1st International Competition on Plagiarism
Detection (Potthast et al, 2009), no participants
tried to approach it.
In order to describe the prototypical process of
automatic plagiarism detection, we establish the
following notation. Let dq be a plagiarism suspect
document. Let D be a representative collection
of reference documents. D presumably includes
the source of the potentially plagiarised fragments
in dq . Stein et al, (2007) divide the process into
three stages1:
1. heuristic retrieval of potential source doc-
uments: given dq, retrieving an appropri-
ate number of its potential source documents
D? ? D such that |D?|? |D|;
2. exhaustive comparison of texts: comparing
the text from dq and d ? D? in order to
identify reused fragments and their potential
1This schema was formerly proposed for monolingual
plagiarism detection. Nevertheless, it can be applied with-
out further modifications to the cross-language case.
37
sources; and
3. knowledge-based post-processing: those de-
tected fragments with proper citation are dis-
carded as they are not plagiarised.
The result is offered to the human expert to take
the final decision. In the case of cross-language
plagiarism detection (CLPD), the texts are written
in different languages: dq ? L and d? ? L?.
In this research we focus on step 2: cross-
language exhaustive comparison of texts, ap-
proaching it as an Information Retrieval problem
of cross-language text similarity. Step 1, heuristic
retrieval, may be approached by different CLIR
techniques, such as those proposed by Dumais et
al. (1997) and Pouliquen et al (2003).
Cross-language similarity between texts,
?(dq, d?), has been previously estimated on
the basis of different models: multilingual
thesauri (Steinberger et al, 2002; Ceska et
al., 2008), comparable corpora ?CL-Explicit
Semantic Analysis CL-ESA? (Potthast et
al., 2008), machine translation techniques
?CL-Alignment-based Similarity Analysis CL-
ASA? (Barro?n-Ceden?o et al, 2008; Pinto et al,
2009) and n-grams comparison ?CL-Character
n-Grams CL-CNG? (Mcnamee and Mayfield,
2004).
A comparison of CL-ASA, CL-ESA, and CL-
CNG was carried out recently by Potthast et
al. (2010). The authors report that in general,
despite its simplicity, CL-CNG outperformed the
other two models. Additionally, CL-ESA showed
good results in the cross-language retrieval of
topic-related texts, whereas CL-ASA obtained
better results in exact (human) translations.
However, most of the language pairs used in the
reported experiments (English-{German, Span-
ish, French, Dutch, Polish}) are related, whether
because they have common predecessors or be-
cause a large proportion of their vocabularies
share common roots. In fact, the lower syntactical
relation between the English-Polish pair caused
a performance degradation for CL-CNG, and for
CL-ASA to a lesser extent. In order to confirm
whether the closeness among languages is an im-
portant factor, this paper works with more dis-
tant language pairs: English-Basque and Spanish-
Basque.
The rest of the paper is structured as follows.
Section 2 describes the motivation for working
on this research topic, stressing the situation of
cross-language plagiarism among writers in less
resourced languages. A brief overview of the few
works on CLPD is included. The three similar-
ity estimation models compared in this research
work are presented in Section 3. The experimental
framework and the obtained results are included
in Section 4. Finally, Section 5 draws conclusions
and discusses further work.
2 Motivation
Cases of CLP are common nowadays because in-
formation in multiple languages is available on the
Web, but people still write in their own language.
This special kind of plagiarism occurs more often
when the target language is a less resourced one2,
as is the case of Basque.
Basque is a pre-indoeuropean language with
less than a million speakers in the world and
no known relatives in the language families
(Wikipedia, 2010a). Still, Basque shares a portion
of its vocabulary with its contact languages (Span-
ish and French). Therefore, we decided to work
with two language pairs: Basque with Spanish,
its contact language, and with English, perhaps
the language with major influence over the rest of
languages in the world. Although the considered
pairs share most of their alphabet, the vocabulary
and language typologies are very different. For
instance Basque is an agglutinative language.
In order to illustrate the relations among these
languages, Fig. 1 includes extracts from the En-
glish (en), Spanish (es) and Basque (eu) versions
of the same Wikipedia article. The fragments are
a sample of the lexical and syntactic distance be-
tween Basque and the other two languages. In
fact, these sentences are completely co-derived
and the corresponding entire articles are a sample
of the typical imbalance in text available in the dif-
ferent languages (around 2, 000, 1, 300, and only
2Less resourced language is that with a low degree of rep-
resentation on the Web (Alegria et al, 2009). Whereas the
available text for German, French or Spanish is less than for
English, the difference is more dramatic with other languages
such as Basque.
38
The Party of European Socialists (PES) is
a European political party comprising thirty-two
socialist, social democratic and labour parties
from each European Union member state and
Norway.
El Partido Socialista Europeo (PSE) es un
partido pol??tico pan-europeo cuyos miembros
son de partidos socialdemo?cratas, socialistas y
laboristas de estados miembros de la Unio?n Eu-
ropea, as?? como de Noruega.
Europako Alderdi Sozialista Europar Bata-
suneko herrialdeetako eta Norvegiako hogeita
hamahiru alderdi sozialista, sozialdemokrata eta
laborista biltzen dituen alderdia da.
Figure 1: First sentences from the Wikipedia arti-
cles ?Party of European Socialists? (en),?Partido
Socialista Europeo? (es), and ?Europako Alderdi
Sozialista? (eu) (Wikipedia, 2010b).
100 words are contained in the en, es and eu arti-
cles, respectively).
Of high relevance is that the two corpora used
in this work were manually constructed by trans-
lating English and Spanish text into Basque. In the
experiments carried out by Potthast et al (2010),
which inspired our work, texts from the JCR-
Acquis corpus (Steinberger et al, 2006) and
Wikipedia were used. The first one is a multilin-
gual corpus with no clear definition of source and
target languages, whereas in Wikipedia no spe-
cific relationship exists between the different lan-
guages in which a topic may be broached. In some
cases (cf. Fig. 1) they are clearly co-derived, but
in others they are completely independent.
CLPD has been investigated just recently,
mainly by adapting models formerly proposed
for cross-language information retrieval. This
is the case of cross-language explicit seman-
tic analysis (CL-ESA), proposed by Potthast et
al. (2008). In this case the comparison be-
tween texts is not carried out directly. Instead,
a comparable corpus CL,L? is required, contain-
ing documents on multiple topics in the two im-
plied languages. One of the biggest corpora
of this nature is Wikipedia. The similarity be-
tween dq ? L and every document c ? CL
is computed based on the cosine measure. The
same process is made for L?. This step gener-
ates two vectors [cos(dq, c1), . . . , cos(dq, c|CL|)]
and [cos(d?, c?1), . . . , cos(d?, c?|CL? |)], where each
dimension is comparable between the two vectors.
Therefore, the cosine between such vectors can be
estimated in order to ?indirectly? estimate how
similar dq and d? are. The authors suggest that this
model can be used for CLPD.
Another recent model is MLPlag, proposed by
Ceska et al (2008). It exploits the EuroWord-
Net Thesaurus3, that includes sets of synonyms in
multiple European languages, with common iden-
tifiers across languages. The authors report ex-
periments over a subset of documents of the En-
glish and Czech sections of the JRC-Acquis cor-
pus as well as a corpus of simplified vocabulary4 .
The main difficulty they faced was the amount of
words in the documents not included in the the-
saurus (approximately 50% of the vocabulary).
This is a very similar approach to that pro-
posed by Pouliquen et al (2003) for the identi-
fication of document translations. In fact, both
approaches have something in common: transla-
tions are searched at document level. It is assumed
that an entire document has been reused (trans-
lated). Nevertheless, a writer is free to plagiarise
text fragments from different sources, and com-
pose a mixture of original and reused text.
A third model is the cross-language alignment-
based similarity analysis (CL-ASA), proposed by
Barro?n-Ceden?o et al (2008), which is based on
statistical machine translation technology. This
model was proposed to detect plagiarised text
fragments (similar models have been proposed for
extraction of parallel sentences from comparable
corpora (Munteanu et al, 2004)). The authors
report experiments over a short set of texts from
which simulated plagiarism was created from En-
glish to Spanish. Human as well as automatic ma-
chine translations were included in the collection.
Further descriptions of this model are included in
Section 3, as it is one of those being assessed in
this research work.
To the best of our knowledge, no work (in-
cluding the three previously mentioned) has been
done considering less resourced languages. In this
research work we approach the not uncommon
problem of CLPD in Basque, with source texts
written in Spanish (the co-official language of the
3http://www.illc.uva.nl/EuroWordNet/
4The authors do not mention the origin of the documents.
39
low tok pd bd sd lem
T+MA   
CL-ASA   
CL-CNG    
Table 1: Text preprocessing operations re-
quired for the different models. low=lowercasing,
tok=tokenization, pd=punctuation marks deletion, bd=blank
space deletion, sd=symbols deletion, lem=lematization.
Basque Country) and English (the language with
most available texts in the world).
We compare three cross-language similarity
analysis methods: T+MA (translation followed
by monolingual analysis), a novel method based
on machine translation followed by a monolin-
gual similarity estimation; CL-CNG, a character
n-gram based comparison model; and CL-ASA
a model that combines translation and similarity
estimation in a single step. Neither MLPlag nor
CL-ESA are included in the comparison. On the
one hand, we are interested in plagiarism at sen-
tence level, and MLPlag is designed to compare
entire documents. On the other hand, in previous
experiments over exact translations, CL-ASA has
shown to outperform it on language pairs whose
alphabet or syntax are unrelated (Potthast et al,
2010). This is precisely the case of en-eu and
es-eu language pairs. Additionally, the amount
of Wikipedia articles in Basque available for the
construction of the required comparable corpus is
insufficient for the CL-ESA data requirements.
3 Definition of Models
In this section, we describe the three cross-
language similarity models we compare. For ex-
perimental purposes (cf. Section 4) we consider
dq to be a suspicious sentence written in L and
D? to be a collection of potential source sentences
written in L? (L 6= L?). The text pre-processing
required by the different models is summarised
in Table 1. Examples illustrating how the models
work are included in Section 4.3.
3.1 Translation + Monolingual Analysis
dq ? L is translated into L? on the basis of
the Giza++ (Och and Ney, 2003), Moses (Koehn
et al, 2007) and SRILM (Stolcke, 2002) tools,
generating d?q . The translation system uses a
log-linear combination of state-of-the-art features,
such as translation probabilities and lexical trans-
lation models on both directions and a target lan-
guage model. After translation, d?q and d? are
lexically related, making possible a monolingual
comparison.
Multiple translations from dq into d?q are pos-
sible. Therefore, performing a monolingual sim-
ilarity analysis based on ?traditional? techniques,
such as those based on word n-grams compari-
son (Broder, 1997) or hash collisions (Schleimer
et al, 2003), is not an option. Instead, we take the
approach of the bag-of-words, which has shown
good results in the estimation of monolingual text
similarity (Barro?n-Ceden?o et al, 2009). Words in
d?q and d? are weighted by the standard tf -idf , and
the similarity between them is estimated by the
cosine similarity measure.
3.2 CL-Alignment-based Similarity Analysis
In this model an estimation of how likely is that d?
is a translation of dq is performed. It is based on
the adaptation of the Bayes rule for MT:
p(d? | dq) = p(d
?) p(dq | d?)
p(dq)
. (1)
As p(dq) does not depend on d?, it is neglected.
From an MT point of view, the conditional prob-
ability p(dq | d?) is known as translation model
probability and is computed on the basis of a sta-
tistical bilingual dictionary. p(d?) is known as lan-
guage model probability; it describes the target
language L? in order to obtain grammatically ac-
ceptable translations (Brown et al, 1993).
Translating dq into L? is not the concern of
this method, rather it focuses on retrieving texts
written in L? which are potential translations of
dq . Therefore, Barro?n-Ceden?o et al (2008) pro-
posed replacing the language model (the one used
in T+MA) by that known as length model. This
model depends on text?s character lengths instead
of language structures.
Multiple translations from d into L? are possi-
ble, and it is uncommon to find a pair of translated
texts d and d? such that |d| = |d?|. Nevertheless,
the length of such translations is closely related
to a translation length factor. In accordance with
Pouliquen et al (2003), the length model is de-
fined as:
40
?(d?) = e
?0.5
0
B
@
|d?|
|dq| ??
?
1
C
A
2
, (2)
where ? and ? are the mean and the standard devi-
ation of the character lengths between translations
of texts from L into L?. If the length of d? is not the
expected given dq, it receives a low qualification.
The translation model probability is defined as:
p(d | d?) =
Y
x?d
X
y?d?
p(x, y), (3)
where p(x, y), a statistical bilingual dictionary,
represents the likelihood that x is a valid transla-
tion of y. After estimating p(x, y) from a parallel
corpus, on the basis of the IBM statistical trans-
lation models (Brown et al, 1993), we consider,
for each word x, only the k best translations y
(those with the highest probabilities) up to a min-
imum probability mass of 0.4. This threshold was
empirically selected as it eliminated noisy entries
without discarding an important amount of rele-
vant pairs.
The similarity estimation based on CL-ASA is
finally computed as:
?(dq, d?) = ?(d?) p(dq | d?). (4)
3.3 CL-Character n-Gram Analysis
This model, the simplest of those compared in this
research, has been used in (monolingual) Author-
ship Attribution (Keselj et al, 2003) as well as
cross-language Information Retrieval (Mcnamee
and Mayfield, 2004). The simplified alphabet con-
sidered is ? = {a, . . . , z, 0, . . . , 9}; any other
symbol is discarded (cf. Table 1). The resulting
text strings are codified into character 3-grams,
which are weighted by the standard tf -idf (con-
sidering this n has previously shown to produce
the best results). The similarity between such rep-
resentations of dq and d? is estimated by the cosine
similarity measure.
4 Experiments
The objective of our experiments is to compare
the performance of the three similarity estimation
models. Section 4.1 introduces the corpora we
have exploited. The experimental framework is
described in Section 4.2. Section 4.3 illustrates
how the models work, and the obtained results are
presented and discussed in Section 4.4.
4.1 Corpora
In other Information Retrieval tasks a plethora of
corpora is available for experimental and compar-
ison purposes. However, plagiarism implies an
ethical infringement and, to the best of our knowl-
edge, there is no corpora of actual cases available,
other than some seminal efforts on creating cor-
pora of text reuse (Clough et al, 2002), artificial
plagiarism (Potthast et al, 2009), and simulated
plagiarism (Clough and Stevenson, 2010). The
problem is worse for cross-language plagiarism.
Therefore, in our experiments we use two
parallel corpora: Software, an en-eu translation
memory of software manuals generously supplied
by Elhuyar Fundazioa5; and Consumer, a cor-
pus extracted from a consumer oriented mag-
azine that includes articles written in Spanish
along with their Basque, Catalan, and Galician
translations6 (Alca?zar, 2006). Software includes
288, 000 parallel sentences; 8.66 (6.83) words per
sentence in the English (Basque) section. Con-
sumer contains 58, 202 sentences; 19.77 (15.20)
words per sentence in Spanish (Basque). These
corpora also reflect the imbalance of text available
in the different languages.
4.2 Experimental Framework
We consider Dq and D? to be two entire docu-
ments from which plagiarised sentences and their
source are to be detected. We work at this level
of granularity, and not entire documents, for two
main reasons: (i) we are focused on the exhaus-
tive comparison stage of the plagiarism detection
process (cf. Section 1); and (ii) even a single sen-
tence could be considered a case of plagiarism,
as it transmits a complete idea. However, a pla-
giarised sentence is usually not enough to auto-
matically negate the validity of an entire docu-
ment. This decision is left to the human expert,
which can examine the documents where several
plagiarised sentences occur. Note that the task be-
comes computationally more expensive as, for ev-
ery sentence, we are looking through thousands
5http://www.elhuyar.org
6http://revista.consumer.es
41
es-eu en-eu
? ? ? ?
f1 1.1567 0.2346 1.0561 0.5497
f2 1.1569 0.2349 1.0568 0.5510
f3 1.1571 0.2349 1.0566 0.5433
f4 1.1565 0.2363 1.0553 0.5352
f5 1.1571 0.2348 1.0553 0.5467
avg. 1.1569 0.2351 1.0560 0.5452
Table 2: Length models estimated for each train-
ing partition f1,...,5. The values describe a normal distri-
bution centred in ? ? ?, representing the expected length of
the source text given the suspicious one.
0
0.2
0.4
0.6
0.8
1
0 50 100 150 200 250
Pr
o
ba
bi
lit
y
di
st
rib
u
tio
n
Length of the sentences
eu
es
en
Figure 2: Example length factor for a sentence
written in Basque (eu) dq , such that |dq| = 90.
The normal distributions represent the expected lengths for
the translation d?, either in Spanish (es) or English (en).
of topically-related sentences that are potential
sources of dq, and not only those of a specific doc-
ument.
CLPD is considered a ranking problem. Let
dq ? Dq be a plagiarism suspicious sentence and
d? ? D? be its source sentence. We consider that
the result of the process is correct if, given dq, d?
is properly retrieved. A 5-fold cross validation for
both en-eu and es-eu was performed. Bilingual
dictionaries, language and length models were es-
timated with the corresponding training partitions.
The computed values for ? and ? are those in-
cluded in Table 2. The values for the different
partitions are very similar, showing the low vari-
ability in the translation lengths. On the basis of
these estimated parameters, an example of length
factor for a specific sentence is plotted in Fig. 2.
In the test partitions, for each suspicious sen-
tence dq , 11, 640 source candidate sentences exist
for es-eu and 57, 290 for en-eu. This results in
more than 135 million and 3 billion comparisons
carried out for es-eu and en-eu respectively.
xeu yen p(x, y) xeu yen p(x, y)
beste another 0.288 beste other 0.348
dokumentu document 0.681 batzu some 0.422
makro macro 0.558 ezin not 0.179
ezin cannot 0.279 izan is 0.241
izan the 0.162 atzi access 0.591
. . 0.981
Table 3: Entries in the bilingual dictionary for the
words in dq. Relevant entries for the example are in bold.
4.3 Illustration of Models
In order to clarify how the different models work,
consider the following sentence pair, a suspicious
sentence dq written in Basque and its source d?
written in English (sentences are short for illustra-
tive purposes):
dq beste dokumentu batzuetako makroak ezin dira atzitu.
d? macros from other documents are not accessible.
CL-CNG Example
In this case, symbols and spaces are discarded.
Sentences become:
dq bestedokumentubatzuetakomakroakezindiraatzitu
d? macrosfromotherdocumentsarenotaccessible
Only three 3-grams appear in both sentences
(ume, men, ent). In order to keep the example sim-
ple, the 3-grams are weighted by tf only (in the
actual experiments, tf -idf is used), resulting in a
dot product of 3. The corresponding vectors mag-
nitudes are |dq| = 6.70 and |d?| = 5.65. There-
fore, the estimated similarity is ?(dq, d?) = 0.079.
CL-ASA Example
In this case, the text must be tokenised and lem-
matised, resulting in the following string:
dq beste dokumentu batzu makro ezin izan atzi .
d? macro from other document be not accessible .
The sentences? lengths are |dq| = 38 and |d?| =
39. Therefore, on the basis of Eq. 2, the length
factor between them is ?(dq, d?) = 0.998.
The relevant entries of the previously estimated
dictionary are included in Table 3. Such entries
are substituted in Eq. 3, and the overall process
results in a similarity ?(dq , d?) = 2.74. Whereas
not a stochastic value, this is a weight used when
ranking all the potential source sentences in D?.
T+MA Example
In this case, the same pre-processing than
in CL-ASA is performed. In T+MA dq is
translated into L?, resulting in the new pair:
d?q other document macro cannot be access .
d? macro from other document be not accessible .
42
Note that d?q is a valid translation of dq . Never-
theless, it has few syntactic relation to d?. There-
fore, applying more sophisticated codifications
than the cosine measure over bag-of-words is not
an option. The example is again simplified by
weighting the words based on tf . Five words ap-
pear in both sentences, resulting in a dot product
of 5. The vectors magnitudes are |d?q| = |d?| =?
7. The estimation by T+MA is ?(dq, d?) =
0.71, a high similarity level.
4.4 Results and Discussion
For evaluation we consider a standard measure:
Recall. More specifically Recall after n texts have
been retrieved (n = [1 . . . , 50]). Figure 3 plots the
average Recall value obtained in the 5-folds with
respect to the rank position (n).
In both language pairs, CL-CNG obtained
worse results than those reported for English-
Polish by Potthast et al (2010): R@50 = 0.68
vs. R@50 = 0.53 for es-eu and 0.28 for en-eu.
This is due to the fact that neither the vocabulary
nor its corresponding roots keep important rela-
tions. Therefore, when language pairs have a low
syntactical relationship, CL-CNG is not an op-
tion. Still, CL-CNG performs better with es-eu
than with en-eu because the first pair is composed
of contact languages (cf. Section 1).
About CL-ASA, the results obtained with es-
eu and en-eu are quite different: R@50 = 0.68
for en-eu and R@50 = 0.53 for es-eu. Whereas
in the first case they are comparable to those of
CL-CNG, in the second one CL-ASA completely
outperforms it. The improvement of CL-ASA ob-
tained for en-eu is due to the size of the training
corpus available in this case (approximately five
times the number of sentences available for es-
eu). This shows the sensitivity of the model with
respect to the size of the available resources.
Lastly, although T+MA is a simple approach
that reduces the cross-language similarity estima-
tion to a translation followed by a monolingual
process, it obtained a good performance (R@50=
0.77 for en-eu and R@50=0.89 for es-eu). More-
over, this method proved to be less sensitive than
CL-ASA to the lack of resources. This could
be due to the fact that it considers both direc-
tions of the translation model (e[n|s]-eu and eu-
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 10 20 50
R
ec
al
l
rank
CL-ASA
CL-CNG
T+MA
(a) es-eu
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 10 20 50
R
ec
al
l
rank
(b) en-eu
Figure 3: Evaluation of the cross-language rank-
ing. Results plotted as rank versus Recall for the three eval-
uated models and the two language pairs (R@[1, . . . , 50]).
e[n|s]). Additionally, the language model, applied
in order to compose syntactically correct transla-
tions, reduces the amount of wrong translations
and, indirectly, includes more syntactic informa-
tion in the process. On the contrary, CL-ASA
only considers one direction translation model eu-
e[n|s] and completely disregards syntactical rela-
tions between the texts.
Note that the better results come at the cost
of higher computational demand. CL-CNG only
requires easy to compute string comparisons.
CL-ASA requires translation probabilities from
aligned corpora, but once the probabilities are es-
timated, cross-language similarity can be com-
puted very fast. T+MA requires the previous
translation of all the texts, which can be very
costly for large collections.
5 Conclusions and Further Work
In a society where information in multiple lan-
guages is available on the Web, cross-language
43
plagiarism is occurring every day with increasing
frequency. Still, cross-language plagiarism de-
tection has not been approached sufficiently due
to its intrinsic complexity. Though few attempts
have been made, even less work has been made to
tackle this problem for less resourced languages,
and to explore distant language pairs.
We investigated the case of Basque, a lan-
guage where, due to the lack of resources, cross-
language plagiarism is often committed from texts
in Spanish and English. Basque has no known rel-
atives in the language family. However, it shares
some of its vocabulary with Spanish.
Two state-of-the-art methods based on trans-
lation probabilities and n-gram overlapping, and
a novel technique based on statistical machine
translation were evaluated. The novel technique
obtains the best results in both language pairs,
with the n-gram overlap technique performing
worst. In this sense, our results complement those
of Potthast et al (2010), which includes closely
related language pairs as well.
Our results also show that better results come at
the cost of more expensive processing time. For
the future, we would like to investigate such per-
formance trade-offs in more demanding datasets.
For future work we consider that exploring se-
mantic text features across languages could im-
prove the results. It could be interesting to fur-
ther analyse how the reordering of words through
translations might be relevant for this task. Addi-
tionally, working with languages even more dis-
tant from each other, such as Arabic or Hindi,
seems to be a challenging and interesting task.
Acknowledgements
The research work of the first two authors is partially funded
by CONACYT-Mexico and the MICINN project TEXT-
ENTERPRISE 2.0 TIN2009-13391-C04-03 (Plan I+D+i).
The research work of the last two authors is partially funded
by the MICINN projects OPENMT-2 TIN2009-14675-C03-
01 and KNOW2 TIN2009-14715-C04-01.
References
Alca?zar, Asier. 2006. Towards Linguistically Search-
able Text. In Proceedings of the BIDE 2005, Bilbao,
Basque Country.
Alegria, In?aki, Mikel L. Forcada, and Kepa Sara-
sola, editors. 2009. Proceedings of the SEPLN
2009 Workshop on Information Retrieval and Infor-
mation Extraction for Less Resourced Languages,
Donostia, Basque Country. University of the Basque
Country.
Barro?n-Ceden?o, Alberto, Paolo Rosso, David Pinto,
and Alfons Juan. 2008. On Cross-lingual Plagia-
rism Analysis Using a Statistical Model. In Stein,
Stamatatos, and Koppel, editors, ECAI 2008 Work-
shop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2008), pages 9?13,
Patras, Greece. CEUR-WS.org.
Barro?n-Ceden?o, Alberto, Andreas Eiselt, and Paolo
Rosso. 2009. Monolingual Text Similarity Mea-
sures: A Comparison of Models over Wikipedia Ar-
ticles Revisions. In Sharma, Verma, and Sangal, ed-
itors, ICON 2009, pages 29?38, Hyderabad, India.
Macmillan Publishers.
Broder, Andrei Z. 1997. On the Resemblance and
Containment of Documents. In Compression and
Complexity of Sequences (SEQUENCES?97), pages
21?29. IEEE Computer Society.
Brown, Peter F., Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Ceska, Zdenek, Michal Toman, and Karel Jezek. 2008.
Multilingual Plagiarism Detection. In Proceedings
of the 13th International Conference on Artificial
Intelligence, pages 83?92. Springer Verlag Berlin
Heidelberg.
Clough, Paul and Mark Stevenson. 2010. Developing
a Corpus of Plagiarised Short Answers. Language
Resources and Evaluation: Special Issue on Plagia-
rism and Authorship Analysis.
Clough, Paul, Robert Gaizauskas, and Scott Piao.
2002. Building and Annotating a Corpus for the
Study of Journalistic Text Reuse. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation (LREC 2002), volume V,
pages 1678?1691, Las Palmas, Spain.
Dumais, Susan T., Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic Cross-Language Retrieval Using Latent Se-
mantic Indexing. In AAAI-97 Spring Symposium
Series: Cross-Language Text and Speech Retrieval,
pages 24?26. Stanford University.
Hoad, Timothy C. and Justin Zobel. 2003. Meth-
ods for Identifying Versioned and Plagiarized Doc-
uments. Journal of the American Society for Infor-
mation Science and Technology, 54(3):203?215.
44
Keselj, Vlado, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based Author Profiles
for Authorship Attribution. In Proceedings of the
Conference Pacific Association for Computational
Linguistics, PACLING?03, pages 255?264, Halifax,
Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), demonstra-
tion session, Prague, Czech Republic.
Maurer, Hermann, Frank Kappe, and Bilal Zaka. 2006.
Plagiarism - A Survey. Journal of Universal Com-
puter Science, 12(8):1050?1084.
Mcnamee, Paul and James Mayfield. 2004. Character
N-Gram Tokenization for European Language Text
Retrieval. Information Retrieval, 7(1-2):73?97.
Munteanu, Dragos S., Alexander Fraser, and Daniel
Marcu. 2004. Improved Machine Translation
Performace via Parallel Sentence Extraction from
Comparable Corpora. In Proceedings of the Hu-
man Language Technology and North American As-
sociation for Computational Linguistics Conference
(HLT/NAACL 2004), Boston, MA.
Och, Frank Josef and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
See also http://www.fjoch.com/GIZA++.html.
Pinto, David, Jorge Civera, Alberto Barro?n-Ceden?o,
Alfons Juan, and Paolo Rosso. 2009. A Statistical
Approach to Crosslingual Natural Language Tasks.
Journal of Algorithms, 64(1):51?60.
Potthast, Martin, Benno Stein, and Maik Anderka.
2008. A Wikipedia-Based Multilingual Retrieval
Model. In Macdonald, Ounis, Plachouras, Ruthven,
and White, editors, 30th European Conference on
IR Research, ECIR 2008, Glasgow, volume 4956
LNCS of Lecture Notes in Computer Science, pages
522?530, Berlin Heidelberg New York. Springer.
Potthast, Martin, Benno Stein, Andreas Eiselt, Alberto
Barro?n-Ceden?o, and Paolo Rosso. 2009. Overview
of the 1st International Competition on Plagiarism
Detection. In Stein, Rosso, Stamatatos, Koppel, and
Agirre, editors, SEPLN 2009 Workshop on Uncov-
ering Plagiarism, Authorship, and Social Software
Misuse (PAN 09), pages 1?9, San Sebastian, Spain.
CEUS-WS.org.
Potthast, Martin, Alberto Barro?n-Ceden?o, Benno
Stein, and Paolo Rosso. 2010. Cross-Language Pla-
giarism Detection. Language Resources and Eval-
uation, Special Issue on Plagiarism and Authorship
Analysis.
Pouliquen, Bruno, Ralf Steinberger, and Camelia Ig-
nat. 2003. Automatic Identification of Docu-
ment Translations in Large Multilingual Document
Collections. In Proceedings of the International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP-2003), pages 401?408,
Borovets, Bulgaria.
Schleimer, Saul, Daniel S. Wilkerson, and Alex Aiken.
2003. Winnowing: Local Algorithms for Document
Fingerprinting. In Proceedings of the 2003 ACM
SIGMOD International Conference on Management
of Data, New York, NY. ACM.
Shivakumar, Narayanan and Hector Garc??a-Molina.
1995. SCAM: A Copy Detection Mechanism for
Digital Documents. In Proceedings of the 2nd An-
nual Conference on the Theory and Practice of Dig-
ital Libraries.
Stein, Benno, Sven Meyer zu Eissen, and Martin Pot-
thast. 2007. Strategies for Retrieving Plagiarized
Documents. In Clarke, Fuhr, Kando, Kraaij, and de
Vries, editors, Proceedings of the 30th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 825?
826, Amsterdam, The Netherlands. ACM.
Steinberger, Ralf, Bruno Pouliquen, and Johan Hag-
man. 2002. Cross-lingual Document Similarity
Calculation Using the Multilingual Thesaurus EU-
ROVOC. Computational Linguistics and Intelligent
Text Processing. Proceedings of the CICLing 2002,
2276:415?424.
Steinberger, Ralf, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Da?niel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006),
volume 9, Genoa, Italy.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling toolkit. In Intl. Conference on Spo-
ken Language Processing, Denver, Colorado.
Wikipedia. 2010a. Basque language. [Online; ac-
cessed 5-February-2010].
Wikipedia. 2010b. Party of European Socialists | Par-
tido Socialista Europeo | Europako Alderdi Sozial-
ista . [Online; accessed 10-February-2010].
45
Coling 2010: Poster Volume, pages 997?1005,
Beijing, August 2010
An Evaluation Framework for Plagiarism Detection
Martin Potthast Benno Stein
Web Technology & Information Systems
Bauhaus-Universit?t Weimar
{martin.potthast, benno.stein}@uni-weimar.de
Alberto Barr?n-Cede?o Paolo Rosso
Natural Language Engineering Lab?ELiRF
Universidad Polit?cnica de Valencia
{lbarron, prosso}@dsic.upv.es
Abstract
We present an evaluation framework for
plagiarism detection.1 The framework
provides performance measures that ad-
dress the specifics of plagiarism detec-
tion, and the PAN-PC-10 corpus, which
contains 64 558 artificial and 4 000 sim-
ulated plagiarism cases, the latter gener-
ated via Amazon?s Mechanical Turk. We
discuss the construction principles behind
the measures and the corpus, and we com-
pare the quality of our corpus to exist-
ing corpora. Our analysis gives empirical
evidence that the construction of tailored
training corpora for plagiarism detection
can be automated, and hence be done on a
large scale.
1 Introduction
The lack of an evaluation framework is a seri-
ous problem for every empirical research field.
In the case of plagiarism detection this short-
coming has recently been addressed for the first
time in the context of our benchmarking work-
shop PAN [15, 16]. This paper presents the eval-
uation framework developed in the course of the
workshop. But before going into details, we sur-
vey the state of the art in evaluating plagiarism de-
tection, which has not been studied systematically
until now.
1.1 A Survey of Evaluation Methods
We have queried academic databases and search
engines to get an overview of all kinds of con-
tributions to automatic plagiarism detection. Al-
together 275 papers were retrieved, from which
139 deal with plagiarism detection in text,
1The framework is available free of charge at
http://www.webis.de/research/corpora.
Table 1: Summary of the plagiarism detection
evaluations in 205 papers, from which 104 deal
with text and 101 deal with code.
Evaluation Aspect Text Code
Experiment Task
local collection 80% 95%
Web retrieval 15% 0%
other 5% 5%
Performance Measure
precision, recall 43% 18%
manual, similarity 35% 69%
runtime only 15% 1%
other 7% 12%
Comparison
none 46% 51%
parameter settings 19% 9%
other algorithms 35% 40%
Evaluation Aspect Text Code
Corpus Acquisition
existing corpus 20% 18%
homemade corpus 80% 82%
Corpus Size [# documents]
[1, 10) 11% 10%
[10, 102) 19% 30%
[102, 103) 38% 33%
[103, 104) 8% 11%
[104, 105) 16% 4%
[105, 106) 8% 0%
123 deal with plagiarism detection in code, and
13 deal with other media types. From the pa-
pers related to text and code we analyzed the
205 which present evaluations. Our analysis
covers the following aspects: experiment tasks,
performance measures, underlying corpora, and,
whether comparisons to other plagiarism detec-
tion approaches were conducted. Table 1 summa-
rizes our findings.
With respect to the experiment tasks the ma-
jority of the approaches perform overlap detec-
tion by exhaustive comparison against some lo-
cally stored document collection?albeit a Web
retrieval scenario is more realistic. We explain
this shortcoming by the facts that the Web can-
not be utilized easily as a corpus, and, that in the
case of code plagiarism the focus is on collusion
detection in student courseworks. With respect to
performance measures the picture is less clear: a
manual result evaluation based on similarity mea-
sures is used about the same number of times for
text (35%), and even more often for code (69%),
as an automatic computation of precision and re-
call. 21% and 13% of the evaluations on text and
code use custom measures or examine only the de-
997
tection runtime. This indicates that precision and
recall may not be well-defined in the context of
plagiarism detection. Moreover, comparisons to
existing research are conducted in less than half
of the papers, a fact that underlines the lack of an
evaluation framework.
The right-hand side of Table 1 overviews two
corpus-related aspects: the use of existing cor-
pora versus the use of handmade corpora, and the
size distribution of the used corpora. In particu-
lar, we found that researchers follow two strate-
gies to compile a corpus. Small corpora (<1 000
documents) are built from student courseworks or
from arbitrary documents into which plagiarism-
alike overlap is manually inserted. Large corpora
(>1 000 documents) are collected from sources
where overlap occurs more frequently, such as
rewritten versions of news wire articles, or from
consecutive versions of open source software. Al-
together, we see a need for an open, commonly
used plagiarism detection corpus.
1.2 Related Work
There are a few surveys about automatic plagia-
rism detection in text [7, 8, 14] and in code [12,
17, 19, 20]. These papers, as well as nearly all
papers of our survey, omit a discussion of evalua-
tion methodologies; the following 4 papers are an
exception.
In [21] the authors introduce graph-based per-
formance measures for code plagiarism detection
that are intended for unsupervised evaluations.
We argue that evaluations in this field should be
done in a supervised manner. An aside: the pro-
posed measures have not been adopted since their
first publication. In [15] we introduce preliminary
parts of our framework. However, the focus of
that paper is less on methodology but on the com-
parison of the detection approaches that were sub-
mitted to the first PAN benchmarking workshop.
In [9, 10] the authors report on an unnamed cor-
pus that comprises 57 cases of simulated plagia-
rism. We refer to this corpus as the Clough09 cor-
pus; a comparison to our approach is given later
on. Finally, a kind of related corpus is the ME-
TER corpus, which has been the only alternative
for the text domain up to now [11]. It comprises
445 cases of text reuse among 1 716 news articles.
Although the corpus can be used to evaluate pla-
giarism detection its design does not support this
task. This is maybe the reason why it has not been
used more often. Furthermore, it is an open ques-
tion whether or not cases of news reuse differ from
plagiarism cases where the plagiarists strive to re-
main undetected.
1.3 Contributions
Besides the above survey, the contributions of our
paper are threefold: Section 2 presents formal
foundations for the evaluation of plagiarism detec-
tion and introduces three performance measures.
Section 3 introduces methods to create artificial
and simulated plagiarism cases on a large scale,
and the PAN-PC-10 corpus in which these meth-
ods have been operationalized. Section 4 then
compares our corpus with the Clough09 corpus
and the METER corpus. The comparison reveals
important insights for the different kinds of text
reuse in these corpora.
2 Plagiarism Detection Performance
This section introduces measures to quantify the
precision and recall performance of a plagiarism
detection algorithm; we present a micro-averaged
and a macro-averaged variant. Moreover, the so-
called detection granularity is introduced, which
quantifies whether the contiguity between plagia-
rized text passages is properly recognized. This
concept is important: a low granularity simpli-
fies both the human inspection of algorithmically
detected passages as well as an algorithmic style
analysis within a potential post-process. The three
measures can be applied in isolation but also
be combined into a single, overall performance
score. A reference implementation of the perfor-
mance measures is distributed with our corpus.
2.1 Precision, Recall, and Granularity
Let dplg denote a document that contains pla-
giarism. A plagiarism case in dplg is a 4-tuple
s = ?splg, dplg, ssrc, dsrc?, where splg is a plagia-
rized passage in dplg, and ssrc is its original coun-
terpart in some source document dsrc. Likewise,
a plagiarism detection for document dplg is de-
noted as r = ?rplg, dplg, rsrc, d?src?; r associates
an allegedly plagiarized passage rplg in dplg with
998
a passage rsrc in d?src. We say that r detects s iff
rplg ? splg = ?, rsrc ? ssrc = ?, and d?src = dsrc.
With regard to a plagiarized document dplg it is as-
sumed that different plagiarized passages of dplg
do not intersect; with regard to detections for dplg
no such restriction applies. Finally, S and R de-
note sets of plagiarism cases and detections.
While the above 4-tuples resemble an intu-
itive view of plagiarism detection we resort to
an equivalent, more concise view to simplify the
subsequent notations: a document d is repre-
sented as a set of references to its characters d =
{(1, d), . . . , (|d|, d)}, where (i, d) refers to the
i-th character in d. A plagiarism case s can then be
represented as s = splg ? ssrc, where splg ? dplg
and ssrc ? dsrc. The characters referred to in splg
and ssrc form the passages splg and ssrc. Likewise,
a detection r can be represented as r = rplg?rsrc.
It follows that r detects s iff rplg ? splg = ? and
rsrc?ssrc = ?. Based on these representations, the
micro-averaged precision and recall of R under S
are defined as follows:
precmicro(S,R) =
|?(s,r)?(S?R)(s 	 r)|
|?r?R r|
, (1)
recmicro(S,R) =
|?(s,r)?(S?R)(s 	 r)|
|?s?S s|
, (2)
where s 	 r =
{
s ? r if r detects s,
? otherwise.
The macro-averaged precision and recall are
unaffected by the length of a plagiarism case; they
are defined as follows:
precmacro(S,R) =
1
|R|
?
r?R
|?s?S(s 	 r)|
|r| , (3)
recmacro(S,R) =
1
|S|
?
s?S
|?r?R(s 	 r)|
|s| , (4)
Besides precision and recall there is another
concept that characterizes the power of a detec-
tion algorithm, namely, whether a plagiarism case
s ? S is detected as a whole or in several pieces.
The latter can be observed in today?s commercial
plagiarism detectors, and the user is left to com-
bine these pieces to a consistent approximation
of s. Ideally, an algorithm should report detec-
tions R in a one-to-one manner to the true cases S.
To capture this characteristic we define the detec-
tion granularity of R under S:
gran(S,R) = 1|SR|
?
s?SR
|Rs|, (5)
where SR ? S are cases detected by detections
in R, and Rs ? R are the detections of a given s:
SR = {s | s ? S ? ?r ? R : r detects s},
Rs = {r | r ? R ? r detects s}.
The domain of gran(S,R) is [1, |R|], with 1
indicating the desired one-to-one correspondence
and |R| indicating the worst case, where a single
s ? S is detected over and over again.
Precision, recall, and granularity allow for a
partial ordering among plagiarism detection algo-
rithms. To obtain an absolute order they must be
combined to an overall score:
plagdet(S,R) = F?log2(1 + gran(S,R))
, (6)
where F? denotes the F?-Measure, i.e., the
weighted harmonic mean of precision and recall.
We suggest using ? = 1 (precision and recall
equally weighted) since there is currently no indi-
cation that either of the two is more important. We
take the logarithm of the granularity to decrease
its impact on the overall score.
2.2 Discussion
Plagiarism detection is both a retrieval task and
an extraction task. In light of this fact not only
retrieval performance but also extraction accuracy
becomes important, the latter of which being ne-
glected in the literature. Our measures incorpo-
rate both. Another design objective of our mea-
sures is the minimization of restrictions imposed
on plagiarism detectors. The overlap restriction
for plagiarism cases within a document assumes
that a certain plagiarized passage is unlikely to
have more than one source. Imprecision or lack
of evidence, however, may cause humans or algo-
rithms to report overlapping detections, e.g., when
being unsure about the true source of a plagia-
rized passage. The measures (1)-(4) provide for a
sensible treatment of this fact since the set-based
999
passage representations eliminate duplicate detec-
tions of characters. The macro-averaged vari-
ants allot equal weight to each plagiarism case,
regardless of its length. Conversely, the micro-
averaged variants favor the detection of long pla-
giarism passages, which are generally easier to be
detected. Which of both is to be preferred, how-
ever, is still an open question.
3 Plagiarism Corpus Construction
This section organizes and analyzes the practices
that are employed?most of the time implicitly?
for the construction of plagiarism corpora. We
introduce three levels of plagiarism authentic-
ity, namely, real plagiarism, simulated plagiarism,
and artificial plagiarism. It turns out that simu-
lated plagiarism and artificial plagiarism are the
only viable alternatives for corpus construction.
We propose a new approach to scale up the gen-
eration of simulated plagiarism based on crowd-
sourcing, and heuristics to generate artificial pla-
giarism. Moreover, based on these methods, we
compile the PAN plagiarism corpus 2010 (PAN-
PC-10) which is the first corpus of its kind that
contains both a large number and a high diversity
of artificial and simulated plagiarism cases.
3.1 Real, Simulated, and Artificial Plagiarism
Syntactically, a plagiarism case is the result of
copying a passage ssrc from a source document
into another document dplg. Since verbatim
copies can be detected easily, plagiarists often
rewrite ssrc to obfuscate their illegitimate act.
This behavior must be modeled when constructing
a training corpus for plagiarism detection, which
can be done at three levels of authenticity. Ide-
ally, one would secretly observe a large number
of plagiarists and use their real plagiarism cases;
at least, one could resort to plagiarism cases which
have been detected in the past. The following as-
pects object against this approach:
? The distribution of detected real plagiarism
is skewed towards ease of detectability.
? The acquisition of real plagiarism is expen-
sive since it is often concealed.
? Publishing real cases requires the consents
from the plagiarist and the original author.
? A public corpus with real cases is question-
able from an ethical and legal viewpoint.
? The anonymization of real plagiarism is dif-
ficult due to Web search engines and author-
ship attribution technology.
It is hence more practical to let people create
plagiarism cases by ?purposeful? modifications,
or to tap resources that contain similar kinds of
text reuse. We subsume these strategies under the
term simulated plagiarism. The first strategy has
often been applied in the past, though on a small
scale and without a public release of the corpora;
the second strategy comes in the form of the ME-
TER corpus [11]. Note that, from a psycholog-
ical viewpoint, people who simulate plagiarism
act under a different mental attitude than plagia-
rists. From a linguistic viewpoint, however, it is
unclear whether real plagiarism differs from sim-
ulated plagiarism.
A third possibility is to generate plagiarism al-
gorithmically [6, 15, 18], which we call artificial
plagiarism. Generating artificial plagiarism cases
is a non-trivial task if one requires semantic equiv-
alence between a source passage ssrc and the pas-
sage splg that is obtained by an automatic obfus-
cation of ssrc. Such semantics-preserving algo-
rithms are still in their infancy; however, the sim-
ilarity computation between texts is usually done
on the basis of document models like the bag of
words model and not on the basis of the original
text, which makes obfuscation amenable to sim-
pler approaches.
3.2 Creating Simulated Plagiarism
Our approach to scale up the creation of simu-
lated plagiarism is based on Amazon?s Mechani-
cal Turk, AMT, a commercial crowdsourcing ser-
vice [3]. This service has gathered considerable
interest, among others to recreate TREC assess-
ments [1], but also to write and translate texts [2].
We offered the following task on the Mechani-
cal Turk platform: Rewrite the original text found
below [on the task Web page] so that the rewritten
version has the same meaning as the original, but
with a different wording and phrasing. Imagine a
scholar copying a friend?s homework just before
class, or imagine a plagiarist willing to use the
1000
Table 2: Summary of 4 000 Mechanical Turk tasks
completed by 907 workers.
Worker Demographics
Age Education
18, 19 10% HS 11%
20?29 37% College 30%
30?39 16% BSc. 17%
40?49 7% MSc. 11%
50?59 4% Dr. 2%
60?69 1%
n/a 25% n/a 29%
Native Speaker Gender
yes 62% male 37%
no 14% female 39%
n/a 23% n/a 24%
Prof. Writer Plagiarized
yes 10% yes 16%
no 66% no 60%
n/a 24% n/a 25%
Task Statistics
Tasks per Worker
average 15
std. deviation 20
minimum 1
maximum 103
Work Time (minutes)
average 14
std. deviation 21
minimum 1
maximum 180
Compensation
pay per task 0.5 US$
rejected results 25%
original text without proper citation.
Workers were required to be fluent in English
reading and writing, and they were informed that
every result was to be reviewed. A questionnaire
displayed alongside the task description asked
about the worker?s age, education, gender, and na-
tive speaking ability. Further we asked whether
the worker is a professional writer, and whether
he or she has ever plagiarized. Completing the
questionnaire was optional in order to minimize
false answers, but still, these numbers have to
be taken with a grain of salt: the Mechanical
Turk is not the best environment for such sur-
veys. Table 2 overviews the worker demographics
and task statistics. The average worker appears
to be a well-educated male or female in the twen-
ties, whose mother tongue is English. 16% of the
workers claim to have plagiarized at least once,
and if at least the order of magnitude of the lat-
ter number can be taken seriously this shows that
plagiarism is a prevalent problem.
A number of pilot experiments were conducted
to determine the pay per task, depending on the
text length and the task completion time: for
50 US-cents about 500 words get rewritten in
about half an hour. We observed that decreasing
or increasing the pay per task has proportional ef-
fect on the task completion time, but not on the
result quality. This observation is in concordance
with earlier research [13]. Table 3 contrasts a
source passage ssrc and its rewritten, plagiarized
passage splg obtained via the Mechanical Turk.
3.3 Creating Artificial Plagiarism
To create artificial plagiarism, we propose three
obfuscation strategies. Given a source passage
ssrc a plagiarized passage splg can be created as
follows (see Table 4):
? Random Text Operations. splg is created
from ssrc by shuffling, removing, inserting,
or replacing words or short phrases at ran-
dom. Insertions and replacements are taken
from the document dplg where splg is to be
inserted.
? Semantic Word Variation. splg is created
from ssrc by replacing words by one of their
synonyms, antonyms, hyponyms, or hyper-
nyms, chosen at random. A word is kept if
none of them is available.
Table 3: Example of a simulated plagiarism case s, generated with Mechanical Turk.
Source Passage ssrc Plagiarized Passage splg
The emigrants who sailed with Gilbert were better fitted for a
crusade than a colony, and, disappointed at not at once find-
ing mines of gold and silver, many deserted; and soon there
were not enough sailors to man all the four ships. Accord-
ingly, the Swallow was sent back to England with the sick;
and with the remainder of the fleet, well supplied at St. John?s
with fish and other necessaries, Gilbert (August 20) sailed
south as far as forty-four degrees north latitude. Off Sable
Island a storm assailed them, and the largest of the ves-
sels, called the Delight, carrying most of the provisions, was
driven on a rock and went to pieces.
[Excerpt from ?Abraham Lincoln: A History? by John Nicolay and John Hay.]
The people who left their countries and sailed with Gilbert
were more suited for fighting the crusades than for leading a
settled life in the colonies. They were bitterly disappointed as
it was not the America that they had expected. Since they did
not immediately find gold and silver mines, many deserted.
At one stage, there were not even enough man to help sail
the four ships. So the Swallow was sent back to England
carrying the sick. The other fleet was supplied with fish and
the other necessities from St. John. On August 20, Gilbert
had sailed as far as forty-four degrees to the north latitude.
His ship known as the Delight, which bore all the required
supplies, was attacked by a violent storm near Sable Island.
The storm had driven it into a rock shattering it into pieces.
1001
Table 4: Examples of the obfuscation strategies.
Obfuscation Examples
Original Text
The quick brown fox jumps over the lazy dog.
Manual Obfuscation (by a human)
Over the dog which is lazy jumps quickly the fox which is brown.
Dogs are lazy which is why brown foxes quickly jump over them.
A fast auburn vulpine hops over an idle canine.
Random Text Operations
over The. the quick lazy dog <context word> jumps brown fox
over jumps quick brown fox The lazy. the
brown jumps the. quick dog The lazy fox over
Semantic Word Variation
The quick brown dodger leaps over the lazy canine.
The quick brown canine jumps over the lazy canine.
The quick brown vixen leaps over the lazy puppy.
POS-preserving Word Shuffling
The brown lazy fox jumps over the quick dog.
The lazy quick dog jumps over the brown fox.
The brown lazy dog jumps over the quick fox.
? POS-preserving Word Shuffling. The se-
quence of parts of speech in ssrc is deter-
mined and splg is created by shuffling words
at random while retaining the original POS
sequence.
To generate different degrees of obfuscation the
strategies can be adjusted by varying the number
of operations made on ssrc, and by limiting the
range of affected phrases within ssrc. For our cor-
pus, the strategies were combined and adjusted to
match an intuitive understanding of a ?low? and
a ?high? obfuscation. Of course other obfusca-
tion strategies are conceivable, e.g., based on au-
tomatic paraphrasing methods [4], but for perfor-
mance reasons simple strategies are preferred at
the expense of readability of the obfuscated text.
3.4 Overview of the PAN-PC-10
To compile the PAN plagiarism corpus 2010, sev-
eral other parameters besides the above plagiarism
obfuscation methods have been varied. Table 5
gives an overview.
The documents used in the corpus are derived
from books from the Project Gutenberg.2 Every
document in the corpus serves one of two pur-
poses: it is either used as a source for plagiarism
or as a document suspicious of plagiarism. The
latter documents divide into documents that actu-
ally contain plagiarism and documents that don?t.
2http://www.gutenberg.org
Table 5: Corpus statistics of the PAN-PC-10 for
its 27 073 documents and 68 558 plagiarism cases.
Document Statistics
Document Purpose
source documents 50%
suspicious documents
? with plagiarism 25%
? w/o plagiarism 25%
Intended Algorithms
external detection 70%
intrinsic detection 30%
Plagiarism per Document
hardly (5%-20%) 45%
medium (20%-50%) 15%
much (50%-80%) 25%
entirely (>80%) 15%
Document Length
short (1-10 pp.) 50%
medium (10-100 pp.) 35%
long (100-1000 pp.) 15%
Plagiarism Case Statistics
Topic Match
intra-topic cases 50%
inter-topic cases 50%
Obfuscation
none 40%
artificial
? low obfuscation 20%
? high obfuscation 20%
simulated (AMT) 6%
translated ({de,es} to en) 14%
Case Length
short (50-150 words) 34%
medium (300-500 words) 33%
long (3000-5000 words) 33%
The documents without plagiarism allow to deter-
mine whether or not a detector can distinguish pla-
giarism cases from overlaps that occur naturally
between random documents.
The corpus is split into two parts, correspond-
ing to the two paradigms of plagiarism detection,
namely external plagiarism detection and intrinsic
plagiarism detection. Note that in the case of in-
trinsic plagiarism detection the source documents
used to generate the plagiarism cases are omitted:
intrinsic detection algorithms are expected to de-
tect plagiarism in a suspicious document by an-
alyzing the document in isolation. Moreover, the
intrinsic plagiarism cases are not obfuscated in or-
der to preserve the writing style of the original au-
thor; the 40% of unobfuscated plagiarism cases in
the corpus include the 30% of the cases belonging
to the intrinsic part.
The fraction of plagiarism per document, the
lengths of the documents and plagiarism cases,
and the degree of obfuscation per case deter-
mine the difficulty of the cases: the corpus con-
tains short documents with a short, unobfuscated
plagiarism case, resulting in a 5% fraction of
plagiarism, but it also contains large documents
with several obfuscated plagiarism cases of vary-
ing lengths, drawn from different source docu-
ments and resulting in fractions of plagiarism up
to 100%. Since the true distributions of these pa-
rameters in real plagiarism are unknown, sensible
1002
estimations were made for the corpus. E.g., there
are more simple plagiarism cases than complex
ones, where ?simple? refers to short cases, hardly
plagiarism per document, and less obfuscation.
Finally, plagiarism cases were generated be-
tween topically related documents and between
unrelated documents. To this end, the source doc-
uments and the suspicious documents were clus-
tered into k = 30 clusters using bisecting k-
means [22]. Then an equal share of plagiarism
cases were generated for pairs of source docu-
ments and suspicious documents within as well
as between clusters. Presuming the clusters cor-
respond to (broad) topics, we thus obtained intra-
topic plagiarism and inter-topic plagiarism.
4 Corpus Validation
This section reports on validation results about
the ?quality? of the plagiarism cases created for
our corpus. We compare both artificial plagia-
rism cases and simulated plagiarism cases to cases
of the two corpora Clough09 and METER. Pre-
suming that the authors of these corpora put their
best efforts into case construction and annotation,
the comparison gives insights whether our scale-
up strategies are reasonable in terms of case qual-
ity. To foreclose the results, we observe that sim-
ulated plagiarism and, in particular, artificial pla-
giarism behave similar to the two handmade cor-
pora. In the light of the employed strategies to
construct plagiarism this result may or may not
be surprising?however, we argue that it is neces-
sary to run such a comparison in order to provide
a broadly accepted evaluation framework in this
sensitive area.
The experimental setup is as follows: given a
plagiarism case s = ?splg, dplg, ssrc, dsrc?, the pla-
giarized passage splg is compared to the source
passage ssrc using 10 different retrieval models.
Each model is an n-gram vector space model
(VSM) where n ranges from 1 to 10 words,
employing stemming, stop word removal, tf -
weighting, and the cosine similarity. Similarity
values are computed for all cases found in each
corpus, but since the corpora are of different sizes,
100 similarities are sampled from each corpus to
ensure comparability.
The rationale of this setup is as follows: a well-
known fact from near-duplicate detection is that
if two documents share only a few 8-grams?so-
called shingles?it is highly probable that they are
duplicates [5]. Another well-known fact is that
two documents which are longer than a few sen-
tences and which are exactly about the same topic
will, with a high probability, share a considerable
portion of their vocabulary. I.e., they have a high
1
0.8
0.6
0.4
0.2
0
n = 1 2 3 4 5 6 7 8 9 10
Si
m
ila
rit
y
n-gram VSM
Clough09
Artificial
Median
25% Quartile
75% Quartile
Simulated (AMT)
METER
Left to right:
Figure 1: Comparison of four corpora of text reuse and plagiarism: each box plot shows the middle
range of the measured similarities when comparing source passages to their rewritten versions. Basis is
an n-gram VSM, where n ? {1, 2, . . . , 10} words.
1003
similarity under a 1-gram VSM. It follows for pla-
giarism detection that a common shingle between
splg and ssrc pinpoints very accurately an unob-
fuscated portion of splg, while it is inevitable that
even a highly obfuscated splg will share a portion
of its vocabulary with ssrc. The same holds for all
other kinds of text reuse.
Figure 1 shows the obtained similarities, con-
trasting each n-gram VSM and each corpus. The
box plots show the middle 50% of the respective
similarity distributions as well as median similar-
ities. The corpora divide into groups with compa-
rable behavior: in terms of the similarity ranges
covered, the artificial plagiarism compares to the
METER corpus, except for n ? {2, 3}, while the
simulated plagiarism from the Clough09 corpus
behaves like that from our corpus, but with a dif-
ferent amplitude. In terms of median similarity,
METER, Clough09, and our simulated plagiarism
behave almost identical, while the artificial plagia-
rism differs. Also note that our simulated plagia-
rism as well as the Clough09 corpus contain some
cases which are hardly obfuscated.
We interpret these results as follows: (1) Dif-
ferent kinds of plagiarism and text reuse do not
differ very much under n-gram models. (2) Ar-
tificial plagiarism, if carefully generated, is a vi-
able alternative to simulated plagiarism cases and
real text reuse cases. (3) Our strategies to scale-up
the construction of plagiarism corpora works well
compared to existing, handmade corpora.
5 Summary
Current evaluation methodologies in the field
of plagiarism detection research have conceptual
shortcomings and allow only for a limited compa-
rability. Our research contributes right here: we
present tailored performance measures for plagia-
rism detection and the large-scale corpus PAN-
PC-10 for the controlled evaluation of detection
algorithms. The corpus features various kinds
of plagiarism cases, including obfuscated cases
that have been generated automatically and man-
ually. An evaluation of the corpus in relation to
previous corpora reveals a high degree of matu-
rity. Until now, 31 plagiarism detectors have been
compared using our evaluation framework. This
high number of systems has been achieved based
on two benchmarking workshops in which the
framework was employed and developed, namely
PAN?09 [15] and PAN?10 [16]. We hope that our
framework will be beneficial as a challenging and
yet realistic test bed for researchers in order to pin-
point the room for the development of better pla-
giarism detection systems.
Acknowledgements
We thank Andreas Eiselt for his devoted work
on the corpus over the past two years. This
work is partially funded by CONACYT-Mexico
and the MICINN project TEXT-ENTERPRISE
2.0 TIN2009-13391-C04-03 (Plan I+D+i).
Bibliography
[1] Omar Alonso and Stefano Mizzaro. Can
We Get Rid of TREC Assessors? Using
Mechanical Turk for Relevance
Assessment. In SIGIR?09: Proceedings of
the Workshop on The Future of IR
Evaluation, 2009.
[2] Vamshi Ambati, Stephan Vogel, and Jaime
Carbonell. Active learning and
crowd-sourcing for machine translation. In
Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike
Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh conference on
International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may
2010. European Language Resources
Association (ELRA). ISBN 2-9517408-6-7.
[3] Jeff Barr and Luis Felipe Cabrera. AI Gets
a Brain. Queue, 4(4):24?29, 2006. ISSN
1542-7730. doi:
10.1145/1142055.1142067.
[4] Regina Barzilay and Lillian Lee. Learning
to Paraphrase: An Unsupervised Approach
Using Multiple-Sequence Alignment. In
NAACL?03: Proceedings of the 2003
Conference of the North American Chapter
of the Association for Computational
Linguistics on Human Language
Technology, pages 16?23, Morristown, NJ,
USA, 2003. Association for Computational
Linguistics. doi:
10.3115/1073445.1073448.
[5] Andrei Z. Broder. Identifying and Filtering
Near-Duplicate Documents. In COM?00:
Proceedings of the 11th Annual Symposium
on Combinatorial Pattern Matching, pages
1004
1?10, London, UK, 2000. Springer-Verlag.
ISBN 3-540-67633-3.
[6] Manuel Cebrian, Manuel Alfonseca, and
Alfonso Ortega. Towards the Validation of
Plagiarism Detection Tools by Means of
Grammar Evolution. IEEE Transactions on
Evolutionary Computation, 13(3):477?485,
June 2009. ISSN 1089-778X.
[7] Paul Clough. Plagiarism in Natural and
Programming Languages: An Overview of
Current Tools and Technologies. Internal
Report CS-00-05, University of Sheffield,
2000.
[8] Paul Clough. Old and New Challenges in
Automatic Plagiarism Detection. National
UK Plagiarism Advisory Service,
http://ir.shef.ac.uk/cloughie/papers/pas_plagiarism.pdf,
2003.
[9] Paul Clough and Mark Stevenson. Creating
a Corpus of Plagiarised Academic Texts. In
Proceedings of Corpus Linguistics
Conference, CL?09 (to appear), 2009.
[10] Paul Clough and Mark Stevenson.
Developing A Corpus of Plagiarised Short
Answers. Language Resources and
Evaluation: Special Issue on Plagiarism
and Authorship Analysis (in press), 2010.
[11] Paul Clough, Robert Gaizauskas, and S. L.
Piao. Building and Annotating a Corpus for
the Study of Journalistic Text Reuse. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC-02), pages 1678?1691,
2002.
[12] Wiebe Hordijk, Mar?a L. Ponisio, and Roel
Wieringa. Structured Review of Code
Clone Literature. Technical Report
TR-CTIT-08-33, Centre for Telematics and
Information Technology, University of
Twente, Enschede, 2008.
[13] Winter Mason and Duncan J. Watts.
Financial Incentives and the "Performance
of Crowds". In HCOMP?09: Proceedings
of the ACM SIGKDD Workshop on Human
Computation, pages 77?85, New York, NY,
USA, 2009. ACM. ISBN
978-1-60558-672-4. doi:
10.1145/1600150.1600175.
[14] Hermann Maurer, Frank Kappe, and Bilal
Zaka. Plagiarism - A Survey. Journal of
Universal Computer Science, 12(8):
1050?1084, 2006.
[15] Martin Potthast, Benno Stein, Andreas
Eiselt, Alberto Barr?n-Cede?o, and Paolo
Rosso. Overview of the 1st International
Competition on Plagiarism Detection. In
Benno Stein, Paolo Rosso, Efstathios
Stamatatos, Moshe Koppel, and Eneko
Agirre, editors, SEPLN 2009 Workshop on
Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 09), pages
1?9. CEUR-WS.org, September 2009. URL
http://ceur-ws.org/Vol-502.
[16] Martin Potthast, Benno Stein, Andreas
Eiselt, Alberto Barr?n-Cede?o, and Paolo
Rosso. Overview of the 2nd International
Benchmarking Workshop on Plagiarism
Detection. In Benno Stein, Paolo Rosso,
Efstathios Stamatatos, and Moshe Koppel,
editors, Proceedings of PAN at CLEF 2010:
Uncovering Plagiarism, Authorship, and
Social Software Misuse, September 2010.
[17] Chanchal K. Roy and James R. Cordy.
Scenario-Based Comparison of Clone
Detection Techniques. In ICPC ?08:
Proceedings of the 2008 The 16th IEEE
International Conference on Program
Comprehension, pages 153?162,
Washington, DC, USA, 2008. IEEE
Computer Society. ISBN
978-0-7695-3176-2.
[18] Chanchal K. Roy and James R. Cordy.
Towards a Mutation-based Automatic
Framework for Evaluating Code Clone
Detection Tools. In C3S2E ?08:
Proceedings of the 2008 C3S2E conference,
pages 137?140, New York, NY, USA, 2008.
ACM. ISBN 978-1-60558-101-9.
[19] Chanchal K. Roy, James R. Cordy, and
Rainer Koschke. Comparison and
Evaluation of Code Clone Detection
Techniques and Tools: A Qualitative
Approach. Sci. Comput. Program., 74(7):
470?495, 2009. ISSN 0167-6423.
[20] Chanchal K. Roy and James R. Cordy. A
survey on software clone detection
research. Technical Report 2007-541,
School of Computing, Queen?s University
at Kingston, Ontario, Canada, 2007.
[21] Geoffrey R. Whale. Identification of
Program Similarity in Large Populations.
The Computer Journal, 33(2):140?146,
1990. doi: 10.1093/comjnl/33.2.140.
[22] Ying Zhao, George Karypis, and Usama
Fayyad. Hierarchical Clustering Algorithms
for Document Datasets. Data Min. Knowl.
Discov., 10(2):141?168, 2005. ISSN
1384-5810. doi:
10.1007/s10618-005-0361-3.
1005
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 1?4,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
DeSoCoRe: Detecting Source Code Re-Use across Programming Languages?
Enrique Flores, Alberto Barro?n-Ceden?o, Paolo Rosso, and Lidia Moreno
ELiRF, Departament of Information Systems and Computation
Universidad Polite?cnica de Valencia, Spain
{eflores,lbarron,prosso,lmoreno}@dsic.upv.es
Abstract
Source code re-use has become an important
problem in academia. The amount of code
available makes necessary to develop systems
supporting education that could address the
problem of detection of source code re-use.
We present the DeSoCoRe tool based on tech-
niques of Natural Language Processing (NLP)
applied to detect source code re-use. DeSo-
CoRe compares two source codes at the level
of methods or functions even when written in
different programming languages. The system
provides an understandable output to the hu-
man reviewer in order to help a teacher to de-
cide whether a source code is re-used.
1 Introduction
Identifying whether a work has been re-used has re-
ceived increasing interest in recent years. As for
documents in natural language, the amount of source
code on Internet is increasing; facilitating the re-use
of all or part of previously implemented programs.1
If no reference to the original work is included, pla-
giarism would be committed. The interest for detect-
ing software re-use is great discouraging academic
cheating.
Many online tools exist for detecting re-use in
text, such as Churnalism2. To the best of our knowl-
edge the unique online service to detecting re-use in
?Screencast available at: http://vimeo.com/33148670. The
tool is available at: http://memex2.dsic.upv.es:8080/DeSoCoRe/
1Source code re-use is often allowed, thanks to licenses as
those of Creative Commons (http://creativecommons.org/ )
2http://churnalism.com/
source code is JPlag3. This tool can process different
programming languages, but at monolingual level.
This paper presents the DeSoCoRe tool for de-
tection source code re-use across programming lan-
guages. We estimate the similarity between two
source codes independently of the programming lan-
guage using NLP techniques. In fact, programming
languages are similar to natural languages; both can
be represented as strings of symbols (characters,
words, phrases, etc.).
DeSoCoRe aims at supporting a reviewer in the
process of detecting highly similar source code func-
tions. It allows to visualize the matches detected be-
tween two source codes d and dq. The programs are
represented as a graph. An edge exists between a
function in dq and a function in d if re-use between
them is suspected. The code chunks are displayed
to the user for further review. With the information
provided, the reviewer can decide whether a frag-
ment is re-used or not.
2 Related Work
In the previous section we mention only one online
tool but many research works for source code re-use
detection exist. Two main approaches have been ex-
plored: content-based and structure-based.
Content-based approaches are based on analysis
of strings within the source codes. The pioneering
work of (Halstead, 1972) is based on units count-
ings. He counts the total number of operands, total
number of different operands and number of opera-
tors, among others.
3https://www.ipd.uni-karlsruhe.de/jplag/
1
Figure 1: Architecture of DeSoCoRe tool. The source code d has N functions, and dq has M functions. Each function
of d is compared against all the functions of dq .
No
changes
Comments
Identifiers
Variable position
Procedure combination
Program statements
Control logic
Level 0
Level 1
Level 2
Level 3
Level 4
Level 5
Level 6
Figure 2: Levels of program modifications in a plagiarism
spectrum proposed by Faidhi and Robinson.
Structure-based approaches, the most used up to
date, focus the analysis on the code structure (ex-
ecution tree) in order to estimate the level of simi-
larity between two source codes. A seminal model
is the proposed by (Whale, 1990b). This approach
codifies branches, repeats, and statements in order
to estimate the similarity between two programs.
This model has inspired several other tools, such as
Plague (Whale, 1990a) and its further developments
YAP[1,2,3] (Wise, 1992).
JPlag (Prechelt et al, 2002) combines both ap-
proaches. In the first stage, it exploits syntax in order
to normalize variables and function names. In the
second stage, it looks for common strings between
programs. This work attempts to detect several lev-
els of obfuscation4. It achieves better results than
JPlag for highly obfuscated cases but worst results
with low degree of obfuscation.
JPlag is able to detect source code re-use in dif-
ferent programming languages although at monolin-
gual level; that is, one programming language at
a time. None of the reviewed approaches is able
4Obfuscation in re-use can be considered as reformulation,
which inserts noise.
to perform cross-language analysis. To the best of
our knowledge the only approach to analyze cross-
language source code re-use is the one of (Arwin and
Tahaghoghi, 2006). Instead of processing source
code, this approach compares intermediate code pro-
duced by a compiler which includes noise in the de-
tection process. The comparison is in fact mono-
lingual and compiler dependent. The resulting tool,
Xplag, allows to compute similarity between codes
in Java and C.
3 Architecture
As shown in Figure 1, DeSoCoRe consists of three
general modules. As input user gives a pair of source
codes (d, dq). The source code splitter is responsi-
ble for dividing the codes in functions. To split each
code into functions we have developed syntactic an-
alyzers for Python and for C syntax family language:
C, C++, Java, C#, etc.
The next module compares the functions of dq
against the functions of d. To make this comparison
we have divided the module into three sub-modules:
(a) Pre-processing: line breaks, tabs and spaces re-
moval as well as case folding; (b) Features extrac-
tion: character n-grams extraction, weighting based
on normalized term frequency (tf ); and (c) Compar-
ison: a cosine similarity estimation. As output, we
obtain a similarity value in the range [0-1] for all the
pairs of functions between the source codes.
We carried out several experiments in order to
find the best way to detect re-use in source codes.
These experiments were inspired by what proposed
in (Faidhi and Robinson, 1987). They describes the
modifications that a programmer makes to hide the
re-use of source code as Figure 2 shows. These lev-
els are: (i) changes in comments and indentation;
2
(ii) changes in identifiers; (iii) changes in declara-
tions; (iv) changes in program modules; (v) changes
in the program statements; (vi) changes in the de-
cision logic. As result of these experiments we ob-
tained best configuration of our system to use the
entire source code and to apply 3-grams (Flores et
al., 2011).
Once the similarity value has been calculated for
all the possible pairs, the pair selector decides what
pairs are good source re-used candidates. This mod-
ule has to discard the pairs which have obtained a
similarity value lower than a threshold established
by the user. As output DeSoCoRe returns the suspi-
cious pairs that have been re-used.
4 Demonstration
In order to interact with our developed system, we
provide a Java applet interface. It is divided in two
interfaces: (i) input screen: which allows the user
for inserting two source codes, select their program-
ming language and additionally to select a value for
the similarity threshold;5 (ii) output screen: which
shows the results divided in two sections: (a) a
graphical visualization of the codes; and (b) a plain
text representation of the codes. In the first section
we have used the Prefuse Library6 in order to draw a
graph representing the similarity between the func-
tions of the source codes. The painted graph consists
of two red nodes which represent each source code.
Their functions are represented by purple nodes and
connected to the source code node with edges. If any
of these functions has been selected by the system as
re-used, its nodes will be connected to a node from
the other source code.
Finally, a node is marked in red if it composes a
potential case of reuse. When a function is pointed,
the plain text section displays the source code. Also,
if this function has any potential case of re-use, the
function and the potential re-used function will be
shown to perform a manual review of the codes. In
order to be introduced to DeSoCoRe an example is
provided and can be accessed clicking on the Ex-
ample button. Figure 3 shows an example of two
supicious source codes: one in C++ and one in Java.
5In agreement with (Flores et al, 2011), the default thresh-
old for C-like languages (C, C++, Java...) is 0.8.
6Software tools for creating rich interactive data visualiza-
tions (http://prefuse.org/ )
The user is able to start the estimation of similarity
clicking on Estimate! button.
After similarity estimation, the result is displayed
as in Figure 3(a). For exploratory purpouses, ex-
ample source codes are available through the Ex-
ample button. The user is able to start the estima-
tion of similarity clicking on Estimate! button. Fig-
ure 3(b) shows an example of potential cases of re-
use. The function crackHTTPAuth is selected in the
right source code node, and the selected as possi-
ble case of re-use is marked on orange. The plain
text representation of these two parts of source code
shows that they are practically identical.
5 Conclusions and Future Work
The main goal of this research work is to provide
a helpful tool for source code reviewers in order to
help them to decide wheter or not a source code
has been re-used. DeSoCoRe is the first online tool
which it can detect source code re-use across lan-
guages as far of our knowledge.
We have developed a methodology for detect-
ing source code re-use across languages, and have
shown their functionality by presenting DeSoCoRe
tool, which works between and within programming
languages. This makes our tool a valuable cross-
lingual source code detector. DeSoCoRe allows
comparing source codes written in Python, Java and
C syntax family languages: C, C++ or C#. We plan
in the next future to extend its functionality to other
common programming languages. As future work
we aim at allowing for the comparison at fragment
level, where a fragment is considered a part of a
function, a group of functions.
Acknowledgments
This work was done in the framework of the VLC/ CAMPUS
Microcluster on Multimodal Interaction in Intelligent Systems
and it has been partially funded by the European Commission
as part of the WiQ-Ei IRSES project (grant no. 269180) within
the FP 7 Marie Curie People Framework, and by MICINN as
part of the Text-Enterprise 2.0 project (TIN2009-13391-C04-
03) within the Plan I+D+i. The research work of the second
author is funded by the CONACyT-Mexico 192021 grant.
3
(a) Input screen: user have to select each language manually.
(b) Output screen: the re-used functions are connected using an edge and their codes
are shown in the text areas below.
Figure 3: Screenshot of the interface of DeSoCoRe.
References
C. Arwin and S. Tahaghoghi. 2006. Plagiarism de-
tection across programming languages. Proceedings
of the 29th Australian Computer Science Conference,
48:277?286.
J. Faidhi and S. Robinson. 1987. An empirical approach
for detecting program similarity and plagiarism within
a university programming enviroment. Computers and
Education, 11:11?19.
E. Flores, A. Barro?n-Ceden?o, P. Rosso and L. Moreno.
2011. Towards the Detection of Cross-Language
Source Code Reuse. Proceedings 16th International
Conference on Applications of Natural Language to
Information Systems, NLDB-2011, Springer-Verlag,
LNCS(6716), pp. 250?253.
M. Halstead. 1972. Naturals laws controlling algorithm
structure?. SIGPLAN Noticies, 7(2).
L. Prechelt, G. Malpohl and M. Philippsen. 2002. Find-
ing plagiarisms among a set of programs with JPlag.
Journal of Universal Computer Science, 8(11):1016?
1038.
G. Whale. 1990. Identification of program similarity in
large populations. The Computer Journal, 33(2).
G. Whale. 1990. Software metrics and plagiarism detec-
tion. Journal of Systems and Software, 13:131?138.
M. Wise. 1992. Detection of similarities in student pro-
grams: Yaping may be preferable to Plagueing. ?Pro-
ceedings of the 23th SIGCSE Technical Symposium.
4
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 143?147, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UPC-CORE: What Can Machine Translation Evaluation Metrics and
Wikipedia Do for Estimating Semantic Textual Similarity?
Alberto Barro?n-Ceden?o1,2 Llu??s Ma`rquez1 Maria Fuentes1 Horacio Rodr??guez1 Jordi Turmo1
1 TALP Research Center, Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, 08034, Barcelona, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid
Boadilla del Monte, 28660 Madrid, Spain
albarron, lluism, mfuentes, horacio, turmo @lsi.upc.edu
Abstract
In this paper we discuss our participation to
the 2013 Semeval Semantic Textual Similarity
task. Our core features include (i) a set of met-
rics borrowed from automatic machine trans-
lation, originally intended to evaluate auto-
matic against reference translations and (ii) an
instance of explicit semantic analysis, built
upon opening paragraphs of Wikipedia 2010
articles. Our similarity estimator relies on a
support vector regressor with RBF kernel. Our
best approach required 13 machine transla-
tion metrics + explicit semantic analysis and
ranked 65 in the competition. Our post-
competition analysis shows that the features
have a good expression level, but overfitting
and ?mainly? normalization issues caused
our correlation values to decrease.
1 Introduction
Our participation to the 2013 Semantic Textual Sim-
ilarity task (STS) (Agirre et al, 2013)1 was focused
on the CORE problem: GIVEN TWO SENTENCES,
s1 AND s2, QUANTIFIABLY INFORM ON HOW SIMI-
LAR s1 AND s2 ARE. We considered real-valued fea-
tures from four different sources: (i) a set of linguis-
tic measures computed with the Asiya Toolkit for
Automatic MT Evaluation (Gime?nez and Ma`rquez,
2010b), (ii) an instance of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), built on top
of Wikipedia articles, (iii) a dataset predictor, and
(iv) a subset of the features available in Takelab?s
Semantic Text Similarity system (?Saric? et al, 2012).
1http://ixa2.si.ehu.es/sts/
Our approaches obtained an overall modest result
compared to other participants (best position: 65 out
of 89). Nevertheless, our post-competition analysis
shows that the low correlation was caused mainly by
a deficient data normalization strategy.
The paper distribution is as follows. Section 2 of-
fers a brief overview of the task. Section 3 describes
our approach. Section 4 discuss our experiments and
obtained results. Section 5 provides conclusions.
2 Task Overview
Detecting two similar text fragments is a difficult
task in cases where the similarity occurs at seman-
tic level, independently of the implied lexicon (e.g
in cases of dense paraphrasing). As a result, simi-
larity estimation models must involve features other
than surface aspects. The STS task is proposed as
a challenge focused in short English texts of dif-
ferent nature: from automatic machine translation
alternatives to human descriptions of short videos.
The test partition also included texts extracted from
news headlines and FrameNet?Wordnet pairs.
The range of similarity was defined between 0
(no relation) up to 5 (semantic equivalence). The
gold standard values were averaged from different
human-made annotations. The expected system?s
output was composed of a real similarity value, to-
gether with an optional confidence level (our confi-
dence level was set constant).
Table 1 gives an overview of the development
(2012 training and test) and test datasets. Note
that both collections extracted from SMT data are
highly biased towards the maximum similarity val-
ues (more than 75% of the instances have a similar-
143
Table 1: Overview of sub-collections in the development and test datasets, including number of instances and distri-
bution of similarity values (in percentage) as well as mean, minimum, and maximum lengths.
similarity distribution length
dataset instances [0, 1) [1, 2) [2, 3) [3, 4) [4, 5] mean min max
dev-[train + test]
MSRpar 1,500 1.20 8.13 17.13 48.73 24.80 17.84 5 30
MSRvid 1,500 31.00 14.13 15.47 20.87 18.53 6.66 2 24
SMTEuroparl 1,193 0.67 0.42 1.17 12.32 85.4 21.13 1 72
OnWN 750 2.13 2.67 10.40 25.47 59.33 7.57 1 34
SMTnews 399 1.00 0.75 5.51 13.03 79.70 11.72 2 28
test
headlines 750 15.47 22.00 16.27 24.67 21.60 7.21 3 22
OnWN 561 36.54 9.80 7.49 17.11 29.05 7.17 5 22
FNWN 189 34.39 29.63 28.57 6.88 0.53 19.90 3 71
SMT 750 0.00 0.27 3.47 20.40 75.87 26.40 1 96
ity higher than 4) and include the longest instances.
On the other hand, the FNWN instances are shifted
towards low similarity levels (more than 60% have a
similarity lower than 2).
3 Approach
Our similarity assessment model relies upon
SVMlight?s support vector regressor, with RBF ker-
nel (Joachims, 1999).2 Our model estimation pro-
cedure consisted of two steps: parameter defini-
tion and backward elimination-based feature selec-
tion. The considered features belong to four fami-
lies, briefly described in the following subsections.
3.1 Machine Translation Evaluation Metrics
We consider a set of linguistic measures originally
intended to evaluate the quality of automatic trans-
lation systems. These measures compute the quality
of a translation by comparing it against one or sev-
eral reference translations, considered as gold stan-
dard. A straightforward application of these mea-
sures to the problem at hand is to consider s1 as the
reference and s2 as the automatic translation, or vice
versa. Some of the metrics are not symmetric so we
compute similarity between s1 and s2 in both direc-
tions and average the resulting scores.
The measures are computed with the Asiya
Toolkit for Automatic MT Evaluation (Gime?nez and
Ma`rquez, 2010b). The only pre-processing carried
out was tokenization (Asiya performs additional in-
box pre-processing operations, though). We consid-
2We also tried with linear kernels, but RBF always obtained
better results.
ered a sample from three similarity families, which
was proposed in (Gime?nez and Ma`rquez, 2010a) as
a varied and robust metric set, showing good corre-
lation with human assessments.3
Lexical Similarity Two metrics of Translation
Error Rate (Snover et al, 2006) (i.e. the esti-
mated human effort to convert s1 into s2): -TER
and -TERpA. Two measures of lexical precision:
BLEU (Papineni et al, 2002) and NIST (Dod-
dington, 2002). One measure of lexical recall:
ROUGEW (Lin and Och, 2004). Finally, four vari-
ants of METEOR (Banerjee and Lavie, 2005) (exact,
stemming, synonyms, and paraphrasing), a lexical
metric accounting for F -Measure.
Syntactic Similarity Three metrics that estimate
the similarity of the sentences over dependency
parse trees (Liu and Gildea, 2005): DP-HWCMic-4
for grammatical categories chains, DP-HWCMir-4
over grammatical relations, and DP-Or(?) over
words ruled by non-terminal nodes. Also, one mea-
sure that estimates the similarity over constituent
parse trees: CP-STM4 (Liu and Gildea, 2005).
Semantic Similarity Three measures that esti-
mate the similarities over semantic roles (i.e. ar-
guments and adjuncts): SR-Or, SR-Mr(?), and
SR-Or(?). Additionally, two metrics that es-
timate similarities over discourse representations:
DR-Or(?) and DR-Orp(?).
3Asiya is available at http://asiya.lsi.upc.edu.
Full descriptions of the metrics are available in the Asiya Tech-
nical Manual v2.0, pp. 15?21.
144
3.2 Explicit Semantic Analysis
We built an instance of Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) with
the first paragraph of 100k Wikipedia articles (dump
from 2010).Pre-processing consisted of tokenization
and lemmatization.
3.3 Dataset Prediction
Given the similarity shifts in the different datasets
(cf. Table 1), we tried to predict what dataset an in-
stance belonged to on the basis of its vocabulary. We
built binary maxent classifiers for each dataset in the
development set, resulting in five dataset likelihood
features: dMSRpar, dSMTeuroparl, dMSRvid,
dOnWN, and dSMTnews.4 Pre-processing consisted
of tokenization and lemmatization.
3.4 Baseline
We considered the features included in the Takelab
Semantic Text Similarity system (?Saric? et al, 2012),
one of the top-systems in last year competition. This
system is used as a black box. The resulting features
are named tklab n, where n = [1, 21].
Our runs departed from three increasing subsets
of features: AE machine translation evaluation met-
rics and explicit semantic analysis, AED the pre-
vious set plus dataset prediction, and AED T the
previous set plus Takelab?s baseline features (cf. Ta-
ble 3). We performed a feature normalization, which
relied on the different feature?s distribution over the
entire dataset. Firstly, features were bounded in the
range ??3??2 in order to reduce the potentially neg-
ative impact of outliers. Secondly, we normalized
according to the z-score (Nardo et al, 2008, pp. 28,
84); i.e. x = (x ? ?)/?. As a result, each real-
valued feature distribution in the dataset has ? = 0
and ? = 1. During the model tuning stage we tried
with other numerous normalization options: normal-
izing each dataset independently, together with the
training set, and without normalization at all. Nor-
malizing according to the entire dev-test dataset led
to the best results
4We used the Stanford classifier; http://nlp.
stanford.edu/software/classifier.shtml
Table 2: Tuning process: parameter definition and feature
selection. Number of features at the beginning and end
of the feature selection step included.
run parameter def. feature sel.
c ? ? corr b e corr
AE 3.7 0.06 0.3 0.8257 19 14 0.8299
AED 3.8 0.03 0.2 0.8413 24 19 0.8425
AED T 2.9 0.02 0.3 0.8761 45 33 0.8803
4 Experiments and Results
Section 4.1 describes our model tuning strategy.
Sections 4.2 and 4.3 discuss the official and post-
competition results.
4.1 Model Tuning
We used only the dev-train partition (2012 training)
for tuning. By means of a 10-fold cross validation
process, we defined the trade-off (c), gamma (?),
and tube width (?) parameters for the regressor and
performed a backward-elimination feature selection
process (Witten and Frank, 2005, p. 294), indepen-
dently for the three experiments.
The results for the cross-validation process are
summarized in Table 2. The three runs allow for cor-
relations higher than 0.8. On the one hand, the best
regressor parameters obtain better results as more
features are considered, still with very small differ-
ences. On the other hand, the low correlation in-
crease after the feature selection step shows that a
few features are indeed irrelevant.
A summary of the features considered in each ex-
periment (also after feature selection) is displayed in
Table 3. The correlation obtained over the dev-test
partition are corrAE = 0.7269, corrAED = 0.7638,
and corrAEDT = 0.8044 ?it would have appeared
in the top-10 ranking of the 2012 competition.
4.2 Official Results
We trained three new regressors with the features
considered relevant by the tuning process, but using
the entire development dataset. The test 2013 parti-
tion was normalized again by means of z-score, con-
sidering the means and standard deviations of the en-
tire test dataset. Table 4 displays the official results.
Our best approach ?AE?, was positioned in rank
65. The worst results of run AED can be explained
by the difference in the nature of the test respect to
145
Table 3: Features considered at the beginning of each run, represented as empty squares (). Filled squares ()
represent features considered relevant after feature selection.
Feature AE AED AED T Feature AE AED AED T Feature AED T
DP-HWCM c-4    METEOR-pa    tklab 7 
DP-HWCM r-4    METEOR-st    tklab 8 
DP-Or(*)    METEOR-sy    tklab 9 
CP-STM-4    ESA    tklab 10 
SR-Or(*)    dMSRpar   tklab 11 
SR-Mr(*)    dSMTeuroparl   tklab 12 
SR-Or    dMSRvid   tklab 13 
DR-Or(*)    dOnWN   tklab 14 
DR-Orp(*)    dSMTnews   tklab 15 
BLEU    tklab 1  tklab 16 
NIST    tklab 2  tklab 17 
-TER    tklab 3  tklab 18 
-TERp-A    tklab 4  tklab 19 
ROUGE-W    tklab 5  tklab 20 
METEOR-ex    tklab 6  tklab 21 
Table 4: Official results for the three runs (rank included).
run headlines OnWN FNWN SMT mean
AE (65) 0.6092 0.5679 -0.1268 0.2090 0.4037
AED (83) 0.4136 0.4770 -0.0852 0.1662 0.3050
AED T (72) 0.5119 0.6386 -0.0464 0.1235 0.3671
the development dataset. AED T obtains worst re-
sults than AE on the headlines and SMT datasets.
The reason behind this behavior can be in the dif-
ference of vocabularies respect to that stored in the
Takelab system (it includes only the vocabulary of
the development partition). This could be the same
reason behind the drop in performance with respect
to the results previously obtained on the dev-test par-
tition (cf. Section 4.1).
4.3 Post-Competition Results
Our analysis of the official results showed the main
issue was normalization. Thus, we performed a
manifold of new experiments, using the same con-
figuration as in run AE, but applying other normal-
ization strategies: (a) z-score normalization, but ig-
noring the FNWN dataset (given its shift through
low values); (b) z-score normalization, but consid-
ering independent means and standard deviations for
each test dataset; and (c) without normalizing any of
dataset (including the regressor one).
Table 5 includes the results. (a) makes evident
that the instances in FNWN represent ?anomalies?
that harm the normalized values of the rest of sub-
sets. Run (b) shows that normalizing the test sets
Table 5: Post-competition experiments results
run headlines OnWN FNWN SMT mean
AE (a) 0.6210 0.5905 -0.0987 0.2990 0.4456
AE (b) 0.6072 0.4767 -0.0113 0.3236 0.4282
AE (c) 0.6590 0.6973 0.1547 0.3429 0.5208
independently is not a good option, as the regressor
is trained considering overall normalizations, which
explains the correlation decrease. Run (c) is com-
pletely different: not normalizing any dataset ?
both in development and test? reduces the influ-
ence of the datasets to each other and allows for the
best results. Indeed, this configuration would have
advanced practically forty positions at competition
time, locating us in rank 27.
Estimating the adequate similarities over FNWN
seems particularly difficult for our systems. We ob-
serve two main factors. (i) FNWN presents an im-
portant similarity shift respect to the other datasets:
nearly 90% of the instances similarity is lower than
2.5 and (ii) the average lengths of s1 and s2 are very
different: 30 vs 9 words. These characteristics made
it difficult for our MT evaluation metrics to estimate
proper similarity values (be normalized or not).
We performed two more experiments over
FNWN: training regressors with ESA as the only
feature, before and after normalization. The correla-
tion was 0.16017 and 0.3113, respectively. That is,
the normalization mainly affects the MT features.
146
5 Conclusions
In this paper we discussed on our participation to the
2013 Semeval Semantic Textual Similarity task. Our
approach relied mainly upon a combination of au-
tomatic machine translation evaluation metrics and
explicit semantic analysis. Building an RBF support
vector regressor with these features allowed us for a
modest result in the competition (our best run was
ranked 65 out of 89).
Acknowledgments
We would like to thank the organizers of this chal-
lenging task for their efforts.
This research work was partially carried out dur-
ing the tenure of an ERCIM ?Alain Bensoussan?
Fellowship. The research leading to these results re-
ceived funding from the EU FP7 Programme 2007-
2013 (grants 246016 and 247762). Our research
work is partially supported by the Spanish research
projects OpenMT-2 and SKATER (TIN2009-14675-
C03, TIN2012-38584-C06-01).
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pilot on
Typed-Similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Gold-
stein et al (Goldstein et al, 2005), pages 65?72.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-Gram Co-
occurrence Statistics. In Proceedings of the Second
International Conference on Human Language Tech-
nology Research, pages 138?145, San Francisco, CA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial Intel-
ligence, pages 1606?1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94).
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4):209?240.
Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare
Voss, editors. 2005. Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization. Asso-
ciation for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods ?
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical. MIT Press.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Stroudsburg, PA. Association for Com-
putational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Goldstein
et al (Goldstein et al, 2005), pages 25?32.
Michela Nardo, Michaela Saisana, Andrea Saltelli, Ste-
fano Tarantola, Anders Hoffmann, and Enrico Giovan-
nini. 2008. Handbook on Constructing Composite In-
dicators: Methodology and User Guide. OECD Pub-
lishing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 311?318,
Philadelphia, PA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,
and Bojana Dalbelo Bas?ic?. 2012. TakeLab: Sys-
tems for Measuring Semantic Text. In First Joint
Conference on Lexical and Computational Semantics
(*SEM), pages 441?448, Montre?al, Canada. Associa-
tion for Computational Linguistics.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, CA, 2 edition.
147
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134?140,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Phrase-based Translation Systems for WMT13:
System Combination with Morphology Generation,
Domain Adaptation and Corpus Filtering
Llu??s Formiga?, Marta R. Costa-jussa`?, Jose? B. Marin?o?
Jose? A. R. Fonollosa?, Alberto Barro?n-Ceden?o??, Llu??s Ma`rquez?
?TALP Research Centre ?Facultad de Informa?tica
Universitat Polite`cnica de Catalunya Universidad Polite?cnica de Madrid
Barcelona, Spain Madrid, Spain
{lluis.formiga,marta.ruiz,jose.marino,jose.fonollosa}@upc.edu
{albarron, lluism}@lsi.upc.edu
Abstract
This paper describes the TALP participa-
tion in the WMT13 evaluation campaign.
Our participation is based on the combi-
nation of several statistical machine trans-
lation systems: based on standard phrase-
based Moses systems. Variations include
techniques such as morphology genera-
tion, training sentence filtering, and do-
main adaptation through unit derivation.
The results show a coherent improvement
on TER, METEOR, NIST, and BLEU
scores when compared to our baseline sys-
tem.
1 Introduction
The TALP-UPC center (Center for Language and
Speech Technologies and Applications at Univer-
sitat Polite`cnica de Catalunya) focused on the En-
glish to Spanish translation of the WMT13 shared
task.
Our primary (contrastive) run is an internal
system selection comprised of different train-
ing approaches (without CommonCrawl, unless
stated): (a) Moses Baseline (Koehn et al,
2007b), (b) Moses Baseline + Morphology Gener-
ation (Formiga et al, 2012b), (c) Moses Baseline
+ News Adaptation (Henr??quez Q. et al, 2011),
(d) Moses Baseline + News Adaptation + Mor-
phology Generation , and (e) Moses Baseline +
News Adaptation + Filtered CommonCrawl Adap-
tation (Barro?n-Ceden?o et al, 2013). Our sec-
ondary run includes is the full training strategy
marked as (e) in the previous description.
The main differences with respect to our last
year?s participation (Formiga et al, 2012a) are: i)
the inclusion of the CommonCrawl corpus, using
a sentence filtering technique and the system com-
bination itself, and ii) a system selection scheme
to select the best translation among the different
configurations.
The paper is organized as follows. Section 2
presents the phrase-based system and the main
pipeline of our baseline system. Section 3 de-
scribes the our approaches to improve the baseline
system on the English-to-Spanish task (special at-
tention is given to the approaches that differ from
last year). Section 4 presents the system combi-
nation approach once the best candidate phrase of
the different subsystems are selected. Section 5
discusses the obtained results considering both in-
ternal and official test sets. Section 6 includes con-
clusions and further work.
2 Baseline system: Phrase-Based SMT
Our contribution is a follow up of our last year par-
ticipation (Formiga et al, 2012a), based on a fac-
tored Moses from English to Spanish words plus
their Part-of-Speech (POS). Factored corpora aug-
ments words with additional information, such as
POS tags or lemmas. In that case, factors other
than surface (e.g. POS) are usually less sparse, al-
lowing the construction of factor-specific language
models with higher-order n-grams. Such language
models can help to obtain syntactically more cor-
rect outputs.
We used the standard models available in Moses
as feature functions: relative frequencies, lexi-
cal weights, word and phrase penalties, wbe-msd-
bidirectional-fe reordering models, and two lan-
guage models (one for surface and one for POS
tags). Phrase scoring was computed using Good-
Turing discounting (Foster et al, 2006).
As aforementioned, we developed five factored
Moses-based independent systems with different
134
approaches. We explain them in Section 3. As
a final decision, we applied a system selection
scheme (Formiga et al, 2013; Specia et al, 2010)
to consider the best candidate for each sentence,
according to human trained quality estimation
(QE) models. We set monotone reordering of
the punctuation signs for the decoding using the
Moses wall feature.
We tuned the systems using the Moses
MERT (Och, 2003) implementation. Our focus
was on minimizing the BLEU score (Papineni et
al., 2002) of the development set. Still, for ex-
ploratory purposes, we tuned configuration (c) us-
ing PRO (Hopkins and May, 2011) to set the ini-
tial weights at every iteration of the MERT algo-
rithm. However, it showed no significant differ-
ences compared to the original MERT implemen-
tation.
We trained the baseline system using all
the available parallel corpora, except for
common-crawl. That is, European Parlia-
ment (EPPS) (Koehn, 2005), News Commentary,
and United Nations. Regarding the monolingual
data, there were more News corpora organized
by years for Spanish. The data is available at
the Translation Task?s website1. We used all
the News corpora to busld the language model
(LM). Firstly, a LM was built for every corpus
independently. Afterwards, they were combined
to produce de final LM.
For internal testing we used the News 2011 and
News 2012 data and concatenated the remaining
three years of News data as a single parallel corpus
for development.
We processed the corpora as in our participa-
tion to WMT12 (Formiga et al, 2012a). Tok-
enization and POS-tagging in both Spanish and
English was obtained with FreeLing (Padro? et al,
2010). Stemming was carried out with Snow-
ball (Porter, 2001). Words were conditionally case
folded based on their POS: proper nouns and ad-
jectives were separated from other categories to
determine whether a string should be fully folded
(no special property), partially folded (noun or ad-
jective) or not folded at all in (acronym).
Bilingual corpora was filtered with the clean-
corpus-n script of Moses (Koehn et al, 2007a), re-
moving those pairs in which a sentence was longer
than 70. For the CommonCrawl corpus we used a
more complex filtering step (cf. Section 3.3).
1http://www.statmt.org/wmt13/translation-task.html
Postprocessing included two special scripts to
recover contractions and clitics. Detruecasing was
done forcing the capitals after the punctuation
signs. Furthermore we used an additional script in
order to check the casing of output names with re-
spect to the source. We reused our language mod-
els and alignments (with stems) from WMT12.
3 Improvement strategies
We tried three different strategies to improve the
baseline system. Section 3.1 shows a strategy
based on morphology simplification plus genera-
tion. Its aim is dealing with the problems raised
by morphology-rich languages, such as Spanish.
Section 3.2 presents a domain?adaptation strategy
that consists of deriving new units. Section 3.3
presents an advanced strategy to filter the good bi-
sentences from the CommonCrawl corpus, which
might be useful to perform the domain adaptation.
3.1 Morphology generation
Following the success of our WMT12 participa-
tion (Formiga et al, 2012a), our first improve-
ment is based on the morphology generalization
and generation approach (Formiga et al, 2012b).
We focus our strategy on simplifying verb forms
only.
The approach first translates into Spanish sim-
plified forms (de Gispert and Marin?o, 2008). The
final inflected forms are predicted through a mor-
phology generation step, based on the shallow
and deep-projected linguistic information avail-
able from both source and target language sen-
tences.
Lexical sparseness is a crucial aspect to deal
with for an open-domain robust SMT when trans-
lating to morphology-rich languages (e.g. Span-
ish) . We knew beforehand (Formiga et al, 2012b)
that morphology generalization is a good method
to deal with generic translations and it provides
stability to translations of the training domain.
Our morphology prediction (generation) sys-
tems are trained with the WMT13 corpora (Eu-
roparl, News, and UN) together with noisy data
(OpenSubtitles). This combination helps to obtain
better translations without compromising the qual-
ity of the translation models. These kind of mor-
phology generation systems are trained with a rel-
atively short amount of parallel data compared to
standard SMT training corpora.
Our main enhancement to this strategy is the
135
addition of source-projected deep features to the
target sentence in order to perform the morphol-
ogy prediction. These features are Dependency
Features and Semantic Role Labelling, obtained
from the source sentence through Lund Depen-
dency Parser2. These features are then projected
to the target sentence as explained in (Formiga et
al., 2012b).
Projected deep features are important to pre-
dict the correct verb morphology from clean and
fluent text. However, the projection of deep fea-
tures is sentence-fluency sensitive, making it un-
reliable when the baseline MT output is poor. In
other words, the morphology generation strategy
becomes more relevant with high-quality MT de-
coders, as their output is more fluent, making the
shallow and deep features more reliable classifier
guides.
3.2 Domain Adaptation through pivot
derived units
Usually the WMT Translation Task focuses on
adapting a system to a news domain, offering an
in-domain parallel corpus to work with. How-
ever this corpus is relatively small compared to
the other corpora. In our previous participation
we demonstrated the need of performing a more
aggressive domain adaptation strategy. Our strat-
egy was based on using in-domain parallel data to
adapt the translation model, but focusing on the
decoding errors that the out-of-domain baseline
system makes when translating the in-domain cor-
pus.
The idea is to identify the system mistakes and
use the in-domain data to learn how to correct
them. To that effect, we interpolate the transla-
tion models (phrase and lexical reordering tables)
with a new adapted translation model with derived
units. We obtained the units identifying the mis-
matching parts between the non-adapted transla-
tion and the actual reference (Henr??quez Q. et al,
2011). This derivation approach uses the origi-
nal translation as a pivot to find a word-to-word
alignment between the source side and the target
correction (word-to-word alignment provided by
Moses during decoding).
The word-to-word monolingual alignment be-
tween output translation target correction was ob-
tained combining different probabilities such as
i)lexical identity, ii) TER-based alignment links,
2http://nlp.cs.lth.se/software/
Corpus Sent. Words Vocab. avg.len.
Original EN 1.48M 29.44M 465.1k 19.90ES 31.6M 459.9k 21.45
Filtered EN 0.78M 15.3M 278.0k 19.72ES 16.6M 306.8k 21.37
Table 1: Commoncrawl corpora statistics for
WMT13 before and after filtering.
iii) lexical model probabilities, iv) char-based Lev-
enshtein distance between tokens and v) filtering
out those alignments from NULL to a stop word
(p = ??).
We empirically set the linear interpolation
weight as w = 0.60 for the baseline translation
models and w = 0.40 for the derived units trans-
lations models. We applied the pivot derived units
strategy to the News domain and to the filtered
Commoncrawl corpus (cf. Section 5). The proce-
dure to filter out the Commoncrawl corpus is ex-
plained next.
3.3 CommonCrawl Filtering
We used the CommonCrawl corpus, provided for
the first time by the organization, as an impor-
tant source of information for performing aggres-
sive domain adaptation. To decrease the impact
of the noise in the corpus, we performed an auto-
matic pre-selection of the supposedly more correct
(hence useful) sentence pairs: we applied the au-
tomatic quality estimation filters developed in the
context of the FAUST project3. The filters? pur-
pose is to identify cases in which the post-editions
provided by casual users really improve over auto-
matic translations.
The adaptation to the current framework is as
follows. Example selection is modelled as a bi-
nary classification problem. We consider triples
(src, ref , trans), where src and ref stand for the
source-reference sentences in the CommonCrawl
corpus and trans is an automatic translation of the
source, generated by our baseline SMT system. A
triple is assigned a positive label iff ref is a bet-
ter translation from src than trans. That is, if the
translation example provided by CommonCrawl is
better than the output of our baseline SMT system.
We used four feature sets to characterize the
three sentences and their relationships: sur-
face, back-translation, noise-based and similarity-
based. These features try to capture (a) the simi-
larity between the different texts on the basis of
3http://www.faust-fp7.eu
136
diverse measures, (b) the length of the different
sentences (including ratios), and (c) the likelihood
of a source or target text to include noisy text.4
Most of them are simple, fast-calculation and
language-independent features. However, back-
translation features require that trans and ref are
back-translated into the source language. We did
it by using the TALP es-en system from WMT12.
Considering these features, we trained lin-
ear Support Vector Machines using SVMlight
(Joachims, 1999). Our training collection was the
FFF+ corpus, with +500 hundred manually anno-
tated instances (Barro?n-Ceden?o et al, 2013). No
adaptation to CommonCrawl was performed. To
give an idea, classification accuracy over the test
partition of the FFF+ corpus was only moderately
good (?70%). However, ranking by classification
score a fresh set of over 6,000 new examples, and
selecting the top ranked 50% examples to enrich a
state-of-the-art SMT system, allowed us to signifi-
cantly improve translation quality (Barro?n-Ceden?o
et al, 2013).
For WMT13, we applied these classifiers to
rank the CommonCrawl translation pairs and then
selected the top 53% instances to be processed by
the domain adaptation strategy. Table 1 displays
the corpus statistics before and after filtering.
4 System Combination
We approached system combination as a system
selection task. More concretely, we applied Qual-
ity Estimation (QE) models (Specia et al, 2010;
Formiga et al, 2013) to select the highest qual-
ity translation at sentence level among the trans-
lation candidates obtained by our different strate-
gies. The QE models are trained with human
supervision, making use of no system-dependent
features.
In a previous study (Formiga et al, 2013),
we showed the plausibility of building reliable
system-independent QE models from human an-
notations. This type of task should be addressed
with a pairwise ranking strategy, as it yields bet-
ter results than an absolute quality estimation ap-
proach (i.e., regression) for system selection. We
also found that training the quality estimation
models from human assessments, instead of au-
tomatic reference scores, helped to obtain better
4We refer the interested reader to (Barro?n-Ceden?o et al,
2013) for a detailed description of features, process, and eval-
uation.
models for system selection for both i) mimicking
the behavior of automatic metrics and ii) learning
the human behavior when ranking different trans-
lation candidates.
For training the QE models we used the data
from the WMT13 shared task on quality estima-
tion (System Selection Quality Estimation at Sen-
tence Level task5), which contains the test sets
from other WMT campaigns with human assess-
ments. We used five groups of features, namely:
i) QuestQE: 17 QE features provided by the Quest
toolkit6; ii) AsiyaQE: 26 QE features provided by
the Asiya toolkit for MT evaluation (Gime?nez and
Ma`rquez, 2010a); iii) LM (and LM-PoS) perplex-
ities trained with monolingual data; iv) PR: Clas-
sical lexical-based measures -BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Denkowski and Lavie, 2011)- computed
with a pseudo-reference approach, that is, using
the other system candidates as references (Sori-
cut and Echihabi, 2010); and v) PROTHER: Ref-
erence based metrics provided by Asiya, including
GTM, ROUGE, PER, TER (Snover et al, 2008),
and syntax-based evaluation measures also with a
pseudo-reference approach.
We trained a Support Vector Machine ranker by
means of pairwise comparison using the SVMlight
toolkit (Joachims, 1999), but with the ?-z p? pa-
rameter, which can provide system rankings for
all the members of different groups. The learner
algorithm was run according to the following pa-
rameters: linear kernel, expanding the working set
by 9 variables at each iteration, for a maximum of
50,000 iterations and with a cache size of 100 for
kernel evaluations. The trade-off parameter was
empirically set to 0.001.
Table 2 shows the contribution of different fea-
ture groups when training the QE models. For
evaluating performance, we used the Asiya nor-
malized linear combination metric ULC (Gime?nez
and Ma`rquez, 2010b), which combines BLEU,
NIST, and METEOR (with exact, paraphrases and
synonym variants). Within this scenario, it can
be observed that the quality estimation features
(QuestQE and AsiyaQE) did not obtain good re-
sults, perhaps because of the high similarity be-
tween the test candidates (Moses with different
configurations) in contrast to the strong differ-
ence between the candidates in training (Moses,
5http://www.quest.dcs.shef.ac.uk/wmt13 qe.html
6http://www.quest.dcs.shef.ac.uk
137
Features Asiya ULCWMT?11 WMT?12 AVG WMT?13
QuestQE 60.46 60.64 60.55 60.06
AsiyaQE 61.04 60.89 60.97 60.29
QuestQE+AsiyaQE 60.86 61.07 60.96 60.42
LM 60.84 60.63 60.74 60.37
QuestQE+AsiyaQE+LM 60.80 60.55 60.67 60.21
QuestQE+AsiyaQE+PR 60.97 61.12 61.05 60.54
QuestQE+AsiyaQE+PR+PROTHER 61.05 61.19 61.12 60.69
PR 61.24 61.08 61.16 61.04
PR+PROTHER 61.19 61.16 61.18 60.98
PR+PROTHER+LM 61.11 61.29 61.20 61.03
QuestQE+AsiyaQE+PR+PROTHER+LM 60.70 60.88 60.79 60.14
Table 2: System selection scores (ULC) obtained using QE models trained with different groups of
features. Results displayed for WMT11, WMT12 internal tests, their average, and the WMT13 test
EN?ES BLEU TER
wmt13 Primary 29.5 0.586
wmt13 Secondary 29.4 0.586
Table 4: Official automatic scores for the WMT13
English?Spanish translations.
RBMT, Jane, etc.). On the contrary, the pseudo-
reference-based features play a crucial role in the
proper performance of the QE model, confirming
the hypothesis that PR features need a clear dom-
inant system to be used as reference. The PR-
based configurations (with and without LM) had
no big differences between them. We choose the
best AVG result for the final system combination:
PR+PROTHER+LM, which it is consistent with
the actual WMT13 evaluated afterwards.
5 Results
Evaluations were performed considering different
quality measures: BLEU, NIST, TER, and ME-
TEOR in addition to an informal manual analy-
sis. This manifold of metrics evaluates distinct as-
pects of the translation. We evaluated both over
the WMT11 and WMT12 test sets as internal in-
dicators of our systems. We also give our perfor-
mance on the WMT13 test dataset.
Table 3 presents the obtained results for the
different strategies: (a) Moses Baseline (w/o
commoncrawl) (b) Moses Baseline+Morphology
Generation (w/o commoncrawl) (c) Moses Base-
line+News Adaptation through pivot based align-
ment (w/o commoncrawl) (d) Moses Baseline +
News Adaptation (b) + Morphology Generation
(c) (e) Moses Baseline + News Adaptation (b) +
Filtered CommonCrawl Adaptation.
The official results are in Table 4. Our primary
(contrastive) run is the system combination strat-
egy whereas our secondary run is the full training
strategy marked as (e) on the system combination.
Our primary system was ranked in the second clus-
ter out of ten constrained systems in the official
manual evaluation.
Independent analyzes of the improvement
strategies show that the highest improvement
comes from the CommonCrawl Filtering + Adap-
tation strategy (system e). The second best strat-
egy is the combination of the morphology pre-
diction system plus the news adaptation system.
However, for the WMT12 test the News Adap-
tation strategy contributes to main improvement
whereas for the WMT13 this major improvement
is achieved with the morphology strategy. Analyz-
ing the distance betweem each test set with respect
to the News and CommonCrawl domain to further
understand the behavior of each strategy seems an
interesting future work. Specifically, for further
contrasting the difference in the morphology ap-
proach, it would be nice to analyze the variation in
the verb inflection forms. Hypothetically, the per-
son or the number of the verb forms used may have
a higher tendency to be different in the WMT13
test set, implying that our morphology approach is
further exploited.
Regarding the system selection step (internal
WMT12 test), the only automatic metric that has
an improvement is TER. However, TER is one of
138
EN?ES BLEU NIST TER METEOR
wmt12 Baseline 32.97 8.27 49.27 49.91
wmt12 + Morphology Generation 33.03 8.29 49.02 50.01
wmt12 + News Adaptation 33.22 8.31 49.00 50.16
wmt12 + News Adaptation + Morphology Generation 33.29 8.32 48.83 50.29
wmt12 + News Adaptation + Filtered CommonCrawl Adaptation 33.61 8.35 48.82 50.52
wmt12 System Combination 33.43 8.34 48.78 50.44
wmt13 Baseline 29.02 7.72 51.92 46.96
wmt13 Morphology Generation 29.35 7.73 52.04 47.04
wmt13 News Adaptation 29.19 7.74 51.91 47.07
wmt13 News Adaptation + Morphology Generation 29.40 7.74 51.96 47.12
wmt13 News Adaptation + Filtered CommonCrawl Adaptation 29.47 7.77 51.82 47.22
wmt13 System Combination 29.54 7.77 51.76 47.34
Table 3: Automatic scores for English?Spanish translations.
the most reliable metrics according to human eval-
uation. Regarding the actual WMT13 test, the sys-
tem selection step is able to overcome all the auto-
matic metrics.
6 Conclusions and further work
This paper described the TALP-UPC participa-
tion for the English-to-Spanish WMT13 transla-
tion task. We applied the same systems as in last
year, but enhanced with new techniques: sentence
filtering and system combination.
Results showed that both approaches performed
better than the baseline system, being the sentence
filtering technique the one that most improvement
reached in terms of all the automatic quality indi-
cators: BLEU, NIST, TER, and METEOR. The
system combination was able to outperform the
independent systems which used morphological
knowledge and/or domain adaptation techniques.
As further work would like to focus on further
advancing on the morphology-based techniques.
Acknowledgments
This work has been supported in part by
Spanish Ministerio de Econom??a y Competitivi-
dad, contract TEC2012-38939-C03-02 as well
as from the European Regional Development
Fund (ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the fol-
lowing grants: 247762 (FAUST, FP7-ICT-2009-
4-247762), 29951 (the International Outgoing
Fellowship Marie Curie Action ? IMTraP-2011-
29951) and 246016 (ERCIM ?Alain Bensoussan?
Fellowship).
References
Alberto Barro?n-Ceden?o, Llu??s Ma`rquez, Carlos A.
Henr??quez Q, Llu??s Formiga, Enrique Romero, and
Jonathan May. 2013. Identifying Useful Hu-
man Correction Feedback from an On-line Machine
Translation Service. In Proceedings of the Twenty-
Third International Joint Conference on Artificial
Intelligence. AAAI Press.
Adria` de de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication, 50(11-12):1034?
1046.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012a. The TALP-UPC phrase-
based translation systems for WMT12: Morphol-
ogy simplification and domain adaptation. In Pro-
ceedings of the Seventh Workshop on Statistical
Machine Translation, pages 275?282, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Llu??s Formiga, Adolfo Herna?ndez, Jose? B. Marin?, and
Enrique Monte. 2012b. Improving english to
spanish out-of-domain translations by morphology
generalization and generation. In Proceedings of
139
the AMTA Monolingual Machine Translation-2012
Workshop.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 53?61, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
measures for automatic machine translation evalu-
ation. Machine Translation, 24(3-4):209?240, De-
cember.
Carlos A. Henr??quez Q., Jose? B. Marin?o, and Rafael E.
Banchs. 2011. Deriving translation units using
small additional corpora. In Proceedings of the 15th
Conference of the European Association for Ma-
chine Translation.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007a. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007b. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010), La Val-
letta, MALTA, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
140
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 359?364,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Approach to System Selection: ASIYA Features
and Pairwise Classification using Random Forests
Llu??s Formiga1, Meritxell Gonza`lez1, Alberto Barro?n-Ceden?o1,2
Jose? A. R. Fonollosa1 and Llu??s Ma`rquez1
1 TALP Research Center, Universitat Polite`cnica de Catalunya, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid, Spain
{lluis.formiga,jose.fonollosa}@upc.edu, {mgonzalez,albarron,lluism}@lsi.upc.edu
Abstract
This paper describes the TALP-UPC par-
ticipation in the WMT?13 Shared Task
on Quality Estimation (QE). Our partic-
ipation is reduced to task 1.2 on System
Selection. We used a broad set of fea-
tures (86 for German-to-English and 97
for English-to-Spanish) ranging from stan-
dard QE features to features based on
pseudo-references and semantic similarity.
We approached system selection by means
of pairwise ranking decisions. For that,
we learned Random Forest classifiers es-
pecially tailored for the problem. Evalua-
tion at development time showed consider-
ably good results in a cross-validation ex-
periment, with Kendall?s ? values around
0.30. The results on the test set dropped
significantly, raising different discussions
to be taken into account.
1 Introduction
In this paper we discuss the TALP-UPC1 partici-
pation in the WMT?13 Shared Task on Quality Es-
timation (QE). Our participation is circumscribed
to task 1.2, which deals with System Selection.
Concretely, we were required to rank up to five al-
ternative translations for the same source sentence
produced by multiple MT systems, in the absence
of any reference translation.
We used a broad set of features; mainly avail-
able through the last version of the ASIYA toolkit
for MT evaluation2 (Gime?nez and Ma`rquez,
2010). Concretely, we derived 86 features for
the German-to-English subtask and 97 features for
English-to-Spanish. These features cover different
approaches and include standard Quality Estima-
tion features, as provided by the above mentioned
1Center for Language and Speech Technologies and Ap-
plications (TALP), Technical University of Catalonia (UPC).
2http://asiya.lsi.upc.edu
ASIYA toolkit and Quest (Specia et al, 2010),
but also a variety of features based on pseudo-
references (Soricut and Echihabi, 2010), explicit
semantic analysis (Gabrilovich and Markovitch,
2007) and specialized language models. See sec-
tion 3 for details.
In order to model the ranking problem associ-
ated to the system selection task, we adapted it
to a classification task of pairwise decisions. We
trained Random Forest classifiers (and compared
them to SVM classifiers), expanding the work of
Formiga et al (2013), from which a full ranking
can be derived and the best system per sentence
identified.
Evaluation at development time, using cross-
validation, showed considerably good and stable
results for both language pairs, with correlation
values around 0.30 (Kendall ? coefficient) classi-
fication accuracies around 52% (pairwise classifi-
cation) and 41% (best translation identification).
Unfortunately, the results on the test set were sig-
nificantly lower. Current research is devoted to ex-
plain the behavior of the system at testing time. On
the one hand, it seems clear that more research re-
garding the assignment of ties is needed in order
to have a robust model. On the other hand, the re-
lease of the gold standard annotations for the test
set will facilitate a deeper analysis and understand-
ing of the current results.
The rest of the paper is organized as follows.
Section 2 describes the ranking models studied for
the system selection problem. Section 3 describes
the features used for learning. Section 4 presents
the setting for parameter optimization and feature
selection and the results obtained. Finally, Sec-
tion 5 summarizes the lessons learned so far and
outlines some lines for further research.
2 Ranking Model
We considered two learning strategies to obtain the
best translation ranking model: SVM and Random
359
Forests. Both strategies were based on predicting
pairwise quality ranking decisions by means of su-
pervised learning. These decision was motivated
from our previous work (Formiga et al, 2013)
were we learned that they were more consistent to
select the best system (according to human and au-
tomatic metrics) compared to absolute regression
approaches. In that work we used only the subset
of features 1, 2, 3 and 8 described in Section 3.
For this shared task we have introduced additional
similarity measures (subsets 4 to 7) that feature se-
mantic analysis and automatic alignments between
the source and the translations.
The rationale for transforming a ranking prob-
lem to a pairwise classification problem has been
described previously in several work (Joachims,
2002; Burges et al, 2005). The main idea is to en-
semble the features of both individuals and assign
a class {-1,1} which tries to predict the pairwise
relation among them. For linear based approach
this adaptation is as simple to compute the differ-
ence between features between all the pairs of the
training data.
We used two different learners to perform that
task. First, we trained a Support Vector Machine
ranker by means of pairwise comparison using
the SVMlight toolkit (Joachims, 1999), but with
the ?-z p? parameter, which can provide system
rankings for all the members of different groups.
The learner algorithm was run according to the
following parameters: RBF-kernel, expanding the
working set by 9 variables at each iteration, for a
maximum of 50,000 iterations and with a cache
size of 100 for kernel evaluations. The trade-off
parameter was empirically set to 0.001. This im-
plementation ignores the ties for the training step
as it only focuses in better than/ worse than rela-
tions.
Secondly, we used Random Forests (Breiman,
2001), the rationale was the same as ranking-to-
pairwise implementation from SVMlight. How-
ever, SVMlight considers two different data pre-
processing methods depending on the kernel of
the classifier: LINEAR and RBF-Kernel. We
used the same data-preprocessing algorithm from
SVMlight in order to train a Random Forest clas-
sifier with ties (three classes: {0,-1,1}) based
upon the pairwise relations. We used the Random
Forests implementation of scikit-learn toolkit (Pe-
dregosa et al, 2011) with 50 estimators.
Once the classes are given by the Random For-
est, we build a graph by means of the adjacency
matrix of the pairwise decision. Once the adja-
cency matrix has been built, we assign the final
ranking through a dominance scheme similar to
Pighin et al (2012). In that case, however, there
are not topological problems as the pairwise rela-
tions are complete across all the edges.
3 Features Sets
We considered a broad set of features: 97 and
86 features for English-to-Spanish (en-es) and
German-to-English (de-en), respectively. We
grouped them into the following categories: base-
line QE metrics, comparison against pseudo-
references, source-translation, and adapted lan-
guage models. We describe them below. Unless
noted otherwise, the features apply to both lan-
guage pairs.
3.1 Baseline Features
The baseline features are composed of well-known
quality estimation metrics:
1. Quest Baseline (QQE)
Seventeen baseline features from Specia et
al. (2010). This set includes token counts
(and their ratio), LM probabilities for source
and target sentences, percentage of n-grams
in different quartiles of a reference corpus,
number of punctuation marks, and fertility
ratios. We used these features in the en-es
partition only.
2. ASIYA?s QE-based features (AQE)
Twenty-six QE features provided by
ASIYA (Gonza`lez et al, 2012), comprising
bilingual dictionary ambiguity and overlap;
ratios concerning chunks, named-entities and
PoS; source and candidate LM perplexities
and inverse perplexities over lexical forms,
chunks and PoS; and out-of-vocabulary word
indicators.
3.2 Pseudo-Reference-based Features
Soricut and Echihabi (2010) introduced the con-
cept of pseudo-reference-based features (PR) for
translation ranking estimation. The principle is
that, in the lack of human-produced references,
automatic ones are still good for differentiating
good from bad translations. One or more sec-
ondary MT systems are required to generate trans-
lations starting from the same input, which are
360
taken as pseudo-references. The similarity to-
wards the pseudo-references can be calculated
with any evaluation measure or text similarity
function, which gives us all feature variants in this
group. We consider the following PR-based fea-
tures:
3. Derived from ASIYA?s metrics (APR)
Twenty-three PR features, including GTM-l
(l?{1,2,3}) to reward different length match-
ing (Melamed et al, 2003), four variants of
ROUGE (-L, -S*, -SU* and -W) (Lin and
Och, 2004), WER (Nie?en et al, 2000),
PER (Tillmann et al, 1997), TER, and
TERbase (i.e., without stemming, synonymy
look-up, nor paraphrase support) (Snover et
al., 2009), and all the shallow and full pars-
ing measures (i.e., constituency and depen-
dency parsing, PoS, chunking and lemmas)
that ASIYA provides either for Spanish or En-
glish as target languages.
4. Lexical similarity (NGM)
Cosine and Jaccard coefficient similarity
measures for both token and character
n-grams considering n ? [2, 5] (i.e., sixteen
features). Additionally, one Jaccard-based
similarity measure for ?pseudo-prefixes?
(considering only up to four initial characters
for every token).
5. Based on semantic information (SEM)
Twelve features calculated with named
entity- and semantic role-based evaluation
measures (again, provided by ASIYA). Sen-
tences are automatically annotated using
SwiRL (Surdeanu and Turmo, 2005) and
BIOS (Surdeanu et al, 2005). We used these
features in the de-en subtask only.
6. Explicit semantic analysis (ESA)
Two versions of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), a
semantic similarity measure, built on top of
Wikipedia (we used the opening paragraphs
of 100k Wikipedia articles as in 2010).
3.3 Source-Translation Extra Features
Source-translation features include explicit com-
parisons between the source sentence and its trans-
lation. They are meant to measure how adequate
the translation is, that is, to what extent the trans-
lation expresses the same meaning as the source.
Note that a considerable amount of the features
described in the baseline group (QQE and AQE)
fall in this category. In this subsection we include
some extra features we devised to capture source?
translation dependencies.
7. Alignment-based features (ALG / ALGPR)
One measure calculated over the aligned
words between a candidate translation and
the source (ALG); and two measures based on
the comparison between these alignments for
two different translations (e.g., candidate and
pseudo-reference) and the source (ALGPR).3
8. Length model (LeM)
A measure to estimate the quality likeli-
hood of a candidate sentence by considering
the ?expected length? of a proper translation
from the source. The measure was introduced
by (Pouliquen et al, 2003) to identify docu-
ment translations. We estimated its param-
eters over standard MT corpora, including
Europarl, Newswire, Newscommentary and
UN.
3.4 Adapted Language-Model Features
We interpolated different language models com-
prising the WMT?12 Monolingual corpora (EPPS,
News, UN and Gigafrench for English). The in-
terpolation weights were computed as to minimize
the perplexity according to the WMT Translation
Task test data (2008-2010)4. The features are as
follow:
9. Language Model Features (LM)
Two log-probabilities of the translation can-
didate with respect to the above described in-
terpolated language models over word forms
and PoS labels.
4 Experiments and Results
In this section we describe the experiments car-
ried out to select the best feature set, learner, and
learner configuration. Additionally, we present
the final performance within the task. The set-
up experiments were addressed doing two separate
10-fold cross validations on the training data and
averaging the final results. We evaluated the re-
sults through three indicators: Kendall?s ? with no
3Alignments were computed with the Berkeley aligner
https://code.google.com/p/berkeleyaligner/
4http://www.statmt.org/wmt13/translation-task.html
361
penalization for the ties, accuracy in determining
the pairwise relationship between candidate trans-
lations, and global accuracy in selecting the best
candidate for each source sentence.
First, we compared our SVM learner against
Random Forests with the two variants of data
preprocessing (LINEAR and RBF). In terms of
Kendall?s ? , we found that the Random Forests
(RF) were clearly better compared to SVM imple-
mentation. Concretely, depending on the final fea-
ture set, we found that RF achieved a ? between
0.23 and 0.29 while SVM achieved a ? between
0.23 and 0.25. With respect to the accuracy mea-
sures we did not find noticeable differences be-
tween methods as their results moved from 49% to
52%. However, considering the accuracy in terms
of selecting only the best system there was a dif-
ference of two points (42.2% vs. 40.0%) between
methods, being RF again the best system. Regard-
ing the pairwise preprocessing the results between
RBF and LINEAR based preprocessing were com-
parable, being RBF slightly better than LINEAR.
Hence, we selected Random Forests with RBF
pairwise preprocessing as our final learner.
de-en ? with ties AccuracyIgnored Penalized All Best
AQE+LeM+ALGPR+LM 33.70 15.72 52.56 41.57
AQE+SEM+LM 32.49 14.61 52.72 40.92
AQE+LeM+ALGPR+ESA+LM 32.08 13.81 52.71 41.37
AQE+ALG+ESA+SEM+LM 32.06 13.96 52.20 40.64
AQE+ALG+LM 31.97 14.29 52.00 40.83
AQE+LeM+ALGPR+SEM+LM 31.93 13.57 52.52 40.98
AQE+ESA+SEM+LM 31.79 13.68 52.50 40.76
AQE+LeM+ALGPR+ESA+SEM+LM 31.72 14.01 52.65 40.83
AQE+ALG+SEM+LM 31.17 12.86 52.18 40.51
AQE+ALG+SEM 30.72 12.58 51.75 39.66
AQE+LeM+ALGPR+ESA+SEM 30.47 11.79 51.85 39.58
AQE+ESA+LM 30.31 12.23 52.60 40.69
AQE+ALG+ESA+LM 30.26 12.40 52.03 40.99
AQE+LeM+ALGPR 30.24 11.83 51.96 40.42
AQE+LeM+ALGPR+SEM 30.23 11.84 52.10 40.32
AQE+LeM+ALGPR+ESA 29.89 11.87 51.83 40.07
AQE+ALG+ESA 29.81 11.30 51.37 39.47
AQE+SEM 29.80 12.06 51.75 39.52
AQE+NGM+APR+ESA+SEM+LM 29.34 10.58 51.33 38.55
AQE+ESA+SEM 29.31 11.46 51.66 39.24
AQE+ESA 29.13 11.12 51.82 39.90
AQE+ALG+ESA+SEM 28.35 10.32 51.37 38.98
AQE+NGM+APR+ESA+SEM 27.55 9.22 51.01 38.12
Table 1: Set-up results for de-en
For the feature selection process, we considered
the most relevant combinations of feature groups.
Table 1 shows the set-up results for the de-en sub-
task and Table 2 shows the results for the en-es
subtask.
In terms of ? we observed similar results be-
tween the two language pairs. However accura-
cies for the de-en subtask were one point above
the ones for en-es. Regarding the features used, we
found that the best feature combination to use was
composed of: i) a baseline QE feature set (Asiya
or Quest) but not both of them, ii) Length Model,
iii) Pseudo-reference aligned based features and
the use of iv) adapted language models. However,
within the de-en subtask, we found that substitut-
ing Length Model and Aligned Pseudo-references
by the features based on Semantic Roles (SEM)
could bring marginally better accuracy. We also
noticed that the learner was sensitive to the fea-
tures used so selecting the appropriate set of fea-
tures was crucial to achieve a good performance.
en-es ? with ties AccuracyIgnored Penalized All Best
QQE+LeM+ALGPR+LM 33.81 15.87 51.66 41.01
AQE+LeM+ALGPR+LM 33.75 16.44 51.56 41.52
QQE+AQE+LM 32.71 14.59 51.18 41.02
QQE+AQE+LM+ESA 32.69 15.30 51.48 41.30
QQE+AQE+LeM+ALGPR+LM+ESA 32.63 13.64 51.39 40.48
QQE+AQE+LeM+ALGPR+LM 32.41 14.06 51.43 40.49
QQE+LeM+ALGPR+LM+ESA 31.66 13.39 51.37 41.05
QQE+AQE+ALG+LM 31.46 13.62 51.28 41.29
AQE+LeM+ALGPR+LM+ESA 31.29 14.10 51.55 41.43
QQE+AQE+ALG+LM+ESA 31.25 13.58 51.64 41.66
QQE+AQE+NGM+APR+LM+ESA 30.58 12.48 50.93 40.66
QQE+AQE+NGM+APR+LM 29.94 12.54 50.95 40.25
QQE+AQE 28.98 10.92 49.97 39.65
QQE+AQE+LeM+ALGPR 28.94 10.48 49.99 39.71
QQE+AQE+NGM+ESA+LM 28.85 11.88 50.90 40.22
AQE+LeM+ALGPR 28.81 10.11 50.06 40.01
QQE+AQE+ESA 28.68 10.31 49.96 39.27
AQE+ESA 28.67 10.81 50.35 39.18
AQE 28.65 10.68 49.76 38.90
QQE+AQE+ALG 28.47 9.63 49.67 39.66
QQE+AQE+NGM+APR+ESA 28.43 9.75 49.67 38.74
QQE+AQE+NGM 27.23 9.10 49.44 38.98
QQE+AQE+ALG+ESA 27.08 7.93 50.26 39.71
QQE+AQE+LeM+ALGPR+ESA 27.03 8.65 50.35 40.49
AQE+LeM+ALGPR+ESA 26.96 8.26 50.30 39.47
QQE+AQE+NGM+ESA 26.59 7.56 49.52 38.62
QQE+AQE+NGM+APR 25.39 6.97 49.90 39.53
Table 2: Setup results for en-es
de-en ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.31
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
Baseline Random-ranks-with-ties -0.12
Worst -0.49
Table 3: Official results for the de-en subtask (ties
penalized)
en-es ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.15
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
Baseline Random-ranks-with-ties -0.23
Worst -0.63
Table 4: Official results for the en-es subtask (ties
penalized)
In Tables 3, 4, 5 and 6 we present the official re-
sults for the WMT?13 Quality Estimation Task, in
all evaluation variants. In each table we compare
to the best/worst performing systems and also to
the official baseline.
We can observe that in general the results on
the test sets drop significantly, compared to our
362
de-en ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.31 882
UPC AQE+SEM+LM 0.27 768
UPC AQE+LeM+ALGPR+LM 0.24 788
Baseline Random-ranks-with-ties 0.08 718
Worst -0.03 558
Table 5: Official results for the de-en subtask (ties
ignored)
en-es ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.23 192
UPC QQE+LeM+ALGPR+LM 0.11 554
UPC AQE+LeM+ALGPR+LM 0.08 554
Baseline Random-ranks-with-ties 0.03 507
Worst -0.11 633
Table 6: Official results for the en-es subtask (ties
ignored)
set-up experiments. Restricting to the evaluation
setting in which ties are not penalized (i.e., cor-
responding to our setting during system and pa-
rameter tuning), we can see that the results corre-
sponding to de-en (Table 5) are comparable to our
set-up results and close to the best performing sys-
tem. However, in the en-es language pair the final
results are comparatively much lower (Table 6).
We find this behavior strange. In this respect, we
analyzed the inter-annotator agreement within the
gold standard. Concretely we computed the Co-
hen?s ? for all overlapping annotations concerning
at least 4 systems for both language pairs. The re-
sults of our analysis are presented in Table 7 and
therefore it confirms our hypothesis that en-es an-
notations had more noise providing an explanation
for the accuracy decrease of our QE models and
setting the subtask into a more challenging sce-
nario. However, further research will be needed to
analyze other factors such as oracles and improve-
ment on automatic metrics prediction and reliabil-
ity compared to linguistic expert annotators.
Another remaining issue for our research con-
cerns investigating better ways to deal with ties,
as their penalization lowered our results dramati-
cally. In this direction we plan to work further on
# of Lang Cohen?s # ofsystems ? elements
4 en-es 0.210 560de-en 0.369 640
5 en-es 0.211 130de-en 0.375 145
Table 7: Golden standard test set agreement coef-
ficients measured by Cohen?s ?
the adjacency matrix reconstruction heuristics and
presenting the features to the learner in a struc-
tured form.
5 Conclusions
This paper described the TALP-UPC participation
in the WMT?13 Shared Task. We approached the
Quality Estimation task based on system selection,
where different systems have to be ranked accord-
ing to their quality. We derive a full ranking and
identify the best system per sentence on the basis
of Random Forest classifiers.
After the model set-up, we observed consid-
erably good and robust results for both transla-
tion directions, German-to-English and English-
to-Spanish: Kendall?s ? around 0.30 as well as
accuracies around 52% on pairwise classification
and 41% on best translation identification. How-
ever, the results over the official test set were
significantly lower. We have found that the low
inter-annotator agreement between users on that
set might provide an explanation to the poor per-
formance of our QE models.
Our current efforts are centered on explaining
the behavior of our QE models when facing the of-
ficial test sets. We are following two directions: i)
studying the ties? impact to come out with a more
robust model and ii) revise the English-to-Spanish
gold standard annotations in terms of correlation
with automatic metrics to facilitate a deeper un-
derstanding of the results.
Acknowledgments
Acknowledgements
This work has been partially funded by the
Spanish Ministerio de Econom??a y Competitivi-
dad, under contracts TEC2012-38939-C03-02
and TIN2009-14675-C03, as well as from
the European Regional Development Fund
(ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the
following grants: 247762 (FAUST, FP7-ICT-
2009-4-247762) and 246016 (ERCIM ?Alain
Bensoussan? Fellowship).
References
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
363
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning, pages 89?96. ACM.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of the 20th International Joint Conference on Artifi-
cial Intelligence, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??s
Ma`rquez. 2012. A graphical interface for mt evalu-
ation and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In ACM, editor, Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD).
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and recall of machine translation.
In HLT-NAACL.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd Language Resources and
Evaluation Conference (LREC 2000).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
Daniele Pighin, Llu??s Formiga, and Llu??s Ma`rquez.
2012. A graph-based strategy to streamline trans-
lation quality assessments. In Proceedings of the
Tenth Conference of the Association for Machine
Translation in the Americas (AMTA?2012), San
Diego, USA, October. AMTA.
Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.
2003. Automatic Identification of Document Trans-
lations in Large Multilingual Document Collections.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), pages 401?408, Borovets, Bulgaria.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase,
Semantic, and Alignment Enhancements to Trans-
lation Edit Rate. Machine Translation, 23(2):117?
127.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic
Role Labeling Using Complete Syntactic Analysis.
In Proceedings of CoNLL Shared Task.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech).
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H Sawaf. 1997. Accelerated dp based search for
statistical translation. In Proceedings of European
Conference on Speech Communication and Technol-
ogy.
364
