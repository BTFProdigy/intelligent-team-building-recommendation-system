Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 43?48,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Excitement Open Platform for Textual Inferences
Bernardo Magnini
?
, Roberto Zanoli
?
, Ido Dagan
?
, Kathrin Eichler
?
, G?unter Neumann
?
,
Tae-Gil Noh
?
, Sebastian Pado
?
, Asher Stern
?
, Omer Levy
?
?
FBK (magnini|zanoli@fbk.eu)
?
Heidelberg, Stuttgart Univ. (pado|noh@cl.uni-heidelberg.de)
?
DFKI (neumann|eichler@dfki.de)
?
Bar Ilan University (dagan|sterna3|omerlevy@cs.biu.ac.il)
Abstract
This paper presents the Excitement Open
Platform (EOP), a generic architecture and
a comprehensive implementation for tex-
tual inference in multiple languages. The
platform includes state-of-art algorithms,
a large number of knowledge resources,
and facilities for experimenting and test-
ing innovative approaches. The EOP is
distributed as an open source software.
1 Introduction
In the last decade textual entailment (Dagan et al.,
2009) has been a very active topic in Computa-
tional Linguistics, providing a unifying framework
for textual inference. Several evaluation exercises
have been organized around Recognizing Textual
Entailment (RTE) challenges and many method-
ologies, algorithms and knowledge resources have
been proposed to address the task. However, re-
search in textual entailment is still fragmented and
there is no unifying algorithmic framework nor
software architecture.
In this paper, we present the Excitement Open
Platform (EOP), a generic architecture and a com-
prehensive implementation for multilingual textual
inference which we make available to the scien-
tific and technological communities. To a large
extent, the idea is to follow the successful experi-
ence of the Moses open source platform (Koehn et
al., 2007) in Machine Translation, which has made
a substantial impact on research in that field. The
EOP is the result of a two-year coordinated work
under the international project EXCITEMENT.
1
A
consortium of four academic partners has defined
the EOP architectural specifications, implemented
the functional interfaces of the EOP components,
imported existing entailment engines into the EOP
1
http://www.excitement-project.eu
and finally designed and implemented a rich envi-
ronment to support open source distribution.
The goal of the platform is to provide function-
ality for the automatic identification of entailment
relations among texts. The EOP is based on a modu-
lar architecture with a particular focus on language-
independent algorithms. It allows developers and
users to combine linguistic pipelines, entailment al-
gorithms and linguistic resources within and across
languages with as little effort as possible. For ex-
ample, different entailment decision approaches
can share the same resources and the same sub-
components in the platform. A classification-based
algorithm can use the distance component of an
edit-distance based entailment decision approach,
and two different approaches can use the same set
of knowledge resources. Moreover, the platform
has various multilingual components for languages
like English, German and Italian. The result is an
ideal software environment for experimenting and
testing innovative approaches for textual inferences.
The EOP is distributed as an open source software
2
and its use is open both to users interested in using
inference in applications and to developers willing
to extend the current functionalities.
The paper is structured as follows. Section 2
presents the platform architecture, highlighting
how the EOP component-based approach favors
interoperability. Section 3 provides a picture of
the current population of the EOP in terms of both
entailment algorithms and knowledge resources.
Section 4 introduces expected use cases of the plat-
form. Finally, Section 5 presents the main features
of the open source package.
2 Architecture
The EOP platform takes as input two text portions,
the first called the Text (abbreviated with T), the
second called the Hypothesis (abbreviated with H).
2
http://hltfbk.github.io/
Excitement-Open-Platform/
43
Linguis'c)Analysis)Pipeline)(LAP))
Entailment)Core)(EC))
Entailment)Decision))Algorithm)(EDA))
Dynamic)and)Sta'c)Components)(Algorithms)and)Knowledge))
Linguis'c)Analysis)Components)
Decision)
1)
Raw)Data)
Figure 1: EOP architecture
The output is an entailment judgement, either ?En-
tailment? if T entails H, or ?NonEntailment? if the
relation does not hold. A confidence score for the
decision is also returned in both cases.
The EOP architecture (Pad?o et al., 2014) is based
on the concept of modularization with pluggable
and replaceable components to enable extension
and customization. The overall structure is shown
in Figure 1 and consists of two main parts. The
Linguistic Analysis Pipeline (LAP) is a series of
linguistic annotation components. The Entailment
Core (EC) performs the actual entailment recog-
nition. This separation ensures that (a) the com-
ponents in the EC only rely on linguistic analysis
in well-defined ways and (b) the LAP and EC can
be run independently of each other. Configuration
files are the principal means of configuring the EOP.
In the rest of this section we first provide an intro-
duction to the LAP, then we move to the EC and
finally describe the configuration files.
2.1 Linguistic Analysis Pipeline (LAP)
The Linguistic Analysis Pipeline is a collection of
annotation components for Natural Language Pro-
cessing (NLP) based on the Apache UIMA frame-
work.
3
Annotations range from tokenization to
part of speech tagging, chunking, Named Entity
Recognition and parsing. The adoption of UIMA
enables interoperability among components (e.g.,
substitution of one parser by another one) while
ensuring language independence. Input and output
of the components are represented in an extended
version of the DKPro type system based on UIMA
3
http://uima.apache.org/
Common Analysis Structure (CAS) (Gurevych et
al., 2007; Noh and Pad?o, 2013).
2.2 Entailment Core (EC)
The Entailment Core performs the actual entail-
ment recognition based on the preprocessed text
made by the Linguistic Analysis Pipeline. It con-
sists of one or more Entailment Decision Algo-
rithms (EDAs) and zero or more subordinate com-
ponents. An EDA takes an entailment decision
(i.e., ?entailment? or ?no entailment?) while com-
ponents provide static and dynamic information for
the EDA.
Entailment Decision Algorithms are at the top
level in the EC. They compute an entailment deci-
sion for a given Text/Hypothesis (T/H) pair, and
can use components that provide standardized al-
gorithms or knowledge resources. The EOP ships
with several EDAs (cf. Section 3).
Scoring Components accept a Text/Hypothesis
pair as an input, and return a vector of scores.
Their output can be used directly to build minimal
classifier-based EDAs forming complete RTE sys-
tems. An extended version of these components are
the Distance Components that can produce normal-
ized and unnormalized distance/similarity values
in addition to the score vector.
Annotation Components can be used to add dif-
ferent annotations to the Text/Hypothesis pairs. An
example of such a type of component is one that
produces word or phrase alignments between the
Text and the Hypothesis.
Lexical Knowledge Components describe se-
mantic relationships between words. In the
EOP, this knowledge is represented as directed
rules made up of two word?POS pairs, where
the LHS (left-hand side) entails the RHS (right-
hand side), e.g., (shooting star,Noun) =?
(meteorite,Noun). Lexical Knowledge Compo-
nents provide an interface that allows for (a) listing
all RHS for a given LHS; (b) listing all LHS for
a given RHS; and (c) checking for an entailment
relation for a given LHS?RHS pair. The interface
also wraps all major lexical knowledge sources cur-
rently used in RTE research, including manually
constructed ontologies like WordNet, and encyclo-
pedic resources like Wikipedia.
Syntactic Knowledge Components capture en-
tailment relationships between syntactic and
44
lexical-syntactic expressions. We represent such
relationships by entailment rules that link (option-
ally lexicalized) dependency tree fragments that
can contain variables as nodes. For example, the
rule fall of X =? X falls, or X sells Y to Z =?
Z buys Y from X express general paraphrasing pat-
terns at the predicate-argument level that cannot be
captured by purely lexical rules. Formally, each
syntactic rule consists of two dependency tree frag-
ments plus a mapping from the variables of the
LHS tree to the variables of the RHS tree.
4
2.3 Configuration Files
The EC components can be combined into actual
inference engines through configuration files which
contain information to build a complete inference
engine. A configuration file completely describes
an experiment. For example, it specifies the re-
sources that the selected EDA has to use and the
data set to be analysed. The LAP needed for data
set preprocessing is another parameter that can be
configured too. The platform ships with a set of
predefined configuration files accompanied by sup-
porting documentation.
3 Entailment Algorithms and Resources
This section provides a description of the Entail-
ment Algorithms and Knowledge Resources that
are distributed with the EOP.
3.1 Entailment Algorithms
The current version of the EOP platform ships with
three EDAs corresponding to three different ap-
proaches to RTE: an EDA based on transformations
between T and H, an EDA based on edit distance
algorithms, and a classification based EDA using
features extracted from T and H.
Transformation-based EDA applies a sequence
of transformations on T with the goal of making
it identical to H. If each transformation preserves
(fully or partially) the meaning of the original text,
then it can be concluded that the modified text
(which is actually the Hypothesis) can be inferred
from the original one. Consider the following sim-
ple example where the text is ?The boy was located
by the police? and the Hypothesis is ?The child
was found by the police?. Two transformations for
?boy? ? ?child? and ?located? ? ?found? do the
job.
4
Variables of the LHS may also map to null, when material
of the LHS must be present but is deleted in the inference step.
In the EOP we include a transformation based
inference system that adopts the knowledge based
transformations of Bar-Haim et al. (2007), while in-
corporating a probabilistic model to estimate trans-
formation confidences. In addition, it includes a
search algorithm which finds an optimal sequence
of transformations for any given T/H pair (Stern et
al., 2012).
Edit distance EDA involves using algorithms
casting textual entailment as the problem of map-
ping the whole content of T into the content of H.
Mappings are performed as sequences of editing
operations (i.e., insertion, deletion and substitu-
tion) on text portions needed to transform T into H,
where each edit operation has a cost associated with
it. The underlying intuition is that the probability
of an entailment relation between T and H is related
to the distance between them; see Kouylekov and
Magnini (2005) for a comprehensive experimental
study.
Classification based EDA uses a Maximum En-
tropy classifier to combine the outcomes of sev-
eral scoring functions and to learn a classification
model for recognizing entailment. The scoring
functions extract a number of features at various
linguistic levels (bag-of-words, syntactic dependen-
cies, semantic dependencies, named entities). The
approach was thoroughly described in Wang and
Neumann (2007).
3.2 Knowledge Resources
As described in Section 2.2, knowledge resources
are crucial to recognize cases where T and H use
different textual expressions (words, phrases) while
preserving entailment. The EOP platform includes
a wide range of knowledge resources, including lex-
ical and syntactic resources, where some of them
are grabbed from manual resources, like dictionar-
ies, while others are learned automatically. Many
EOP resources are inherited from pre-existing RTE
systems migrated into the EOP platform, but now
use the same interfaces, which makes them acces-
sible in a uniform fashion.
There are about two dozen lexical (e.g. word-
nets) and syntactic resources for three languages
(i.e. English, Italian and German). However,
since there is still a clear predominance of En-
glish resources, the platform includes lexical and
syntactic knowledge mining tools to bootstrap re-
sources from corpora, both for other languages and
45
EDA Accuracy / F1
Transformation-based English RTE-3 67.13%
Transformation-based English RTE-6 49.55%
Edit-Distance English RTE-3 64.38%
Edit-Distance German RTE-3 59.88%
Edit-Distance Italian RTE-3 63.50%
Classification-based English RTE-3 65.25%
Classification-based German RTE-3 63.75%
Median of RTE-3 (English) submissions 61.75%
Median of RTE-6 (English) submissions 33.72%
Table 1: EDAs results
for specific domains. Particularly, the EOP plat-
form includes a language independent tool to build
Wikipedia resources (Shnarch et al., 2009), as well
as a language-independent framework for building
distributional similarity resources like DIRT (Lin
and Pantel, 2002) and Lin similarity(Lin, 1998).
3.3 EOP Evaluation
Results for the three EDAs included in the EOP
platform are reported in Table 1. Each line rep-
resents an EDA, the language and the dataset
on which the EDA was evaluated. For brevity,
we omit here the knowledge resources used for
each EDA, even though knowledge configuration
clearly affects performance. The evaluations were
performed on RTE-3 dataset (Giampiccolo et al.,
2007), where the goal is to maximize accuracy. We
(manually) translated it to German and Italian for
evaluations: in both cases the results fix a refer-
ence for the two languages. The two new datasets
for German and English are available both as part
of the EOP distribution and independently
5
. The
transformation-based EDA was also evaluated on
RTE-6 dataset (Bentivogli et al., 2010), in which
the goal is to maximize the F1 measure.
The results of the included EDAs are higher than
median values of participated systems in RTE-3,
and they are competing with state-of-the-arts in
RTE-6 results. To the best of our knowledge, the
results of the EDAs as provided in the platform are
the highest among those available as open source
systems for the community.
4 Use Cases
We see four primary use cases for the EOP. Their
requirements were reflected in our design choices.
Use Case 1: Applied Textual Entailment. This
category covers users who are not interested in the
5
http://www.excitement-project.eu/
index.php/results
details of RTE but who are interested in an NLP
task in which textual entailment can take over part
of or all of the semantic processing, such as Ques-
tion Answering or Intelligent Tutoring. Such users
require a system that is as easy to deploy as possi-
ble, which motivates our offer of the EOP platform
as a library. They also require a system that pro-
vides good quality at a reasonable efficiency as
well as guidance as to the best choice of parame-
ters. The latter point is realized through our results
archive in the official EOP Wiki on the EOP site.
Use Case 2: Textual Entailment Development.
This category covers researchers who are interested
in Recognizing Textual Entailment itself, for exam-
ple with the goal of developing novel algorithms
for detecting entailment. In contrast to the first
category, this group need to look ?under the hood?
of the EOP platform and access the source code of
the EOP. For this reason, we have spent substantial
effort to provide the code in a well-structured and
well-documented form.
A subclass of this group is formed by researchers
who want to set up a RTE infrastructure for lan-
guages in which it does not yet exist (that is, al-
most all languages). The requirements of this class
of users comprises clearly specified procedures to
replace the Linguistic Analysis Pipeline, which are
covered in our documentation, and simple methods
to acquire knowledge resources for these languages
(assuming that the EDAs themselves are largely
language-independent). These are provided by the
language-independent knowledge acquisition tools
which we offer alongside the platform (cf. Section
3.2).
Use Case 3: Lexical Semantics Evaluation. A
third category consists of researchers whose pri-
mary interest is in (lexical) semantics.
As long as their scientific results can be phrased
in terms of semantic similarities or inference rules,
the EOP platform can be used as a simple and stan-
dardized workbench for these results that indicates
the impact that the semantic knowledge under con-
sideration has on deciding textual entailment. The
main requirement for this user group is the simple
integration of new knowledge resources into the
EOP platform. This is catered for through the defi-
nition of the generic knowledge component inter-
faces (cf. Section 2.2) and detailed documentation
on how to implement these interfaces.
46
Use Case 4: Educational Use. The fourth and
final use case is as an educational tool to support
academic courses and projects on Recognizing Tex-
tual Entailment and inference more generally. This
use case calls, in common with the others, for easy
usability and flexibility. Specifically for this use
case, we have also developed a series of tutorials
aimed at acquainting new users with the EOP plat-
form through a series of increasingly complexity
exercises that cover all areas of the EOP. We are
also posting proposals for projects to extend the
EOP on the EOP Wiki.
5 EOP Distribution
The EOP infrastructure follows state-of-the-art soft-
ware engineering standards to support both users
and developers with a flexible, scalable and easy to
use software environment. In addition to communi-
cation channels, like the mailing list and the issue
tracking system, the EOP infrastructure comprises
the following set of facilities.
Version Control System: We use GitHub,
6
a
web-based hosting service for code and documen-
tation storage, development, and issue tracking.
Web Site: The GitHub Automatic Page Genera-
tor was used to build the EOP web site and Wiki,
containing a general introduction to the software
platform, the terms of its license, mailing lists to
contact the EOP members and links to the code
releases.
Documentation: Both user and developer docu-
mentation is available from Wiki pages; the pages
are written with the GitHub Wiki Editor and hosted
on the GitHub repository. The documentation in-
cludes a Quick Start guide to start using the EOP
platform right away, and a detailed step by step
tutorial.
Results Archive: As a new feature for commu-
nity building, EOP users can, and are encouraged
to, share their results: the platform configuration
files used to produce results as well as contact infor-
mation can be saved and archived into a dedicated
page on the EOP GitHub repository. That allows
other EOP users to replicate experiments under
the same condition and/or avoid doing experiments
that have already been done.
6
https://github.com/
Build Automation Tool: The EOP has been de-
veloped as a Maven
7
multi-modules project, with
all modules sharing the same Maven standard struc-
ture, making it easier to find files in the project once
one is used to Maven.
Maven Artifacts Repository: Using a Maven
repository has a twofold goal: (i) to serve as an
internal private repository of all software libraries
used within the project (libraries are binary files
and should not be stored under version control sys-
tems, which are intended to be used with text files);
(ii) to make the produced EOP Maven artifacts
available (i.e., for users who want to use the EOP
as a library in their own code). We use Artifactory
8
repository manager to store produced artifacts.
Continuous Integration: The EOP uses Jenk-
ins
9
for Continuous Integration, a software develop-
ment practice where developers of a team integrate
their work frequently (e.g., daily).
Code Quality Tool: Ensuring the quality of the
produced software is one of the most important
aspects of software engineering. The EOP uses
tools like PMD
10
that can automatically be run
during development to help the developers check
the quality of their software.
5.1 Project Repository
The EOP Java source code is hosted on the EOP
Github repository and managed using Git. The
repository consists of three main branches: the
release branch contains the code that is supposed to
be in a production-ready state, whereas the master
branch contains the code to be incorporated into the
next release. When the source code in the master
branch reaches a stable point and is ready to be
released, all of the changes are merged back into
release. Finally, the gh-pages branch contains the
web site pages.
5.2 Licensing
The software of the platform is released under the
terms of General Public License (GPL) version
3.
11
The platform contains both components and
resources designed by the EOP developers, as well
as others that are well known and freely available
7
http://maven.apache.org/
8
http://www.jfrog.com/
9
http://jenkins-ci.org/
10
http://pmd.sourceforge.net
11
http://www.gnu.org/licenses/gpl.html
47
in the NLP research community. Additional com-
ponents and resources whose license is not compat-
ible with the EOP license have to be downloaded
and installed separately by the user.
6 Conclusion
This paper has presented the main characteristics
of Excitement Open Platform platform, a rich envi-
ronment for experimenting and evaluating textual
entailment systems. On the software side, the EOP
is a complex endeavor to integrate tools and re-
sources in Computational Linguistics, including
pipelines for three languages, three pre-existing
entailment engines, and about two dozens of lex-
ical and syntactic resources. The EOP assumes a
clear and modular separation between linguistic
annotations, entailment algorithms and knowledge
resources which are used by the algorithms. A
relevant benefit of the architectural design is that
a high level of interoperability is reached, provid-
ing a stimulating environment for new research in
textual inferences.
The EOP platform has been already tested in sev-
eral pilot research projects and educational courses,
and it is currently distributed as open source soft-
ware under the GPL-3 license. To the best of our
knowledge, the entailment systems and their con-
figurations provided in the platform are the best
systems available as open source for the commu-
nity. As for the future, we are planning several
initiatives for the promotion of the platform in the
research community, as well as its active experi-
mentation in real application scenarios.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages 871?
876, Vancouver, BC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Chal-
lenge. In Proceedings of TAC, Gaithersburg, MD.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Journal of Natural
Language Engineering, 15(4):i?xvii.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, Prague, Czech Repub-
lic.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture (UIMA@GSCL 2007), T?ubingen, Ger-
many.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL demo session, pages 177?
180, Prague, Czech Republic.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the First PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 17?20, Southampton, UK.
Dekang Lin and Patrick Pantel. 2002. Discovery of
Inference Rules for Question Answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL/COLING,
pages 768?774, Montr?eal, Canada.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using
UIMA to structure an open platform for textual en-
tailment. In Proceedings of the 3rd Workshop on
Unstructured Information Management Architecture
(UIMA@GSCL 2013).
Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realiza-
tion of a modular architecture for textual entailment.
Journal of Natural Language Engineering. doi:
10.1017/S1351324913000351.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP, pages 450?458, Sin-
gapore.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL, pages 283?291,
Jeju Island, South Korea.
Rui Wang and G?unter Neumann. 2007. Recogniz-
ing textual entailment using a subsequence kernel
method. In Proceedings of AAAI, pages 937?945,
Vancouver, BC.
48
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 150?153,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
DFKI KeyWE: Ranking keyphrases extracted from scientific articles
Kathrin Eichler
DFKI - Language Technology
Berlin, Germany
kathrin.eichler@dfki.de
G?unter Neumann
DFKI - Language Technology
Saarbr?ucken, Germany
neumann@dfki.de
Abstract
A central issue for making the content
of a scientific document quickly acces-
sible to a potential reader is the extrac-
tion of keyphrases, which capture the main
topic of the document. Keyphrases can
be extracted automatically by generating a
list of keyphrase candidates, ranking these
candidates, and selecting the top-ranked
candidates as keyphrases. We present the
KeyWE system, which uses an adapted
nominal group chunker for candidate ex-
traction and a supervised ranking algo-
rithm based on support vector machines
for ranking the extracted candidates. The
system was evaluated on data provided
for the SemEval 2010 Shared Task on
Keyphrase Extraction.
1 Introduction
Keyphrases capture the main topic of the docu-
ment in which they appear and can be useful for
making the content of a document quickly ac-
cessible to a potential reader. They can be pre-
sented to the reader directly, in order to provide
a short overview of the document, but can also
be processed further, e.g. for text summarization,
document clustering, question-answering or rela-
tion extraction. The task of extracting keyphrases
automatically can be performed by generating a
list of keyphrase candidates, ranking these can-
didates, and selecting the top-ranked candidates
as keyphrases. In the KeyWE system, candidates
are generated based on an adapted nominal group
chunker described in section 3 and ranked using
the SVM
rank
algorithm (Joachims, 2006), as de-
scribed in section 4. The used features are spec-
ified in section 5. In section 6, we present the
results achieved on the test data provided for the
SemEval 2010 Shared Task on Keyphrase Extrac-
tion
1
by selecting as keyphrases the top 5, 10, and
15 top-ranked candidates, respectively.
2 Related work
The task of keyphrase extraction came up in the
1990s and was first treated as a supervised learn-
ing problem in the GenEx system (Turney, 1999).
Since then, the task has evolved and various new
approaches have been proposed. The task is usu-
ally performed in two steps: 1. candidate ex-
traction (or generation) and 2. keyphrase selec-
tion. The most common approach towards can-
didate extraction is to generate all n-grams up to
a particular length and filter them using stopword
lists. Lately, more sophisticated candidate extrac-
tion methods, usually based on additional linguis-
tic information (e.g. POS tags), have been pro-
posed and shown to produce better results (e.g.
Hulth (2004)). Liu et al (2009) restrict their can-
didate list to verb, noun and adjective words. Kim
and Kan (2009) generate regular expression rules
to extract simplex nouns and nominal phrases. As
the majority of technical terms is in nominal group
positions
2
, we assume that the same holds true for
keyphrases and apply an adapted nominal group
chunker to extract keyphrase candidates.
The selection process is usually based on some
supervised learning algorithm, e.g. Naive Bayes
(Frank et al, 1999), genetic algorithms (Turney,
1999), neural networks (Wang et al, 2005) or de-
cision trees (Medelyan et al, 2009). Unsuper-
vised approaches have also been proposed, e.g. by
Mihalcea and Tarau (2004) and Liu et al (2009).
However, as for the shared task, annotated train-
ing data was available, we opted for an approach
based on supervised learning.
1
http://semeval2.fbk.eu/semeval2.php?location=tasks#T6
2
Experiments on 100 manually annotated scientific ab-
stracts from the biology domain showed that 94% of technical
terms are in nominal group position (Eichler et al, 2009).
150
3 Candidate extraction
Rather than extracting candidates from the full text
of the article, we restrict our search for candidates
to the first 2000 characters starting with the ab-
stract
3
. We also extract title and general terms
for use in the feature construction process. From
the reduced input text, we extract keyphrase candi-
dates based on the output of a nominal group chun-
ker.
This approach is inspired by findings from cog-
nitive linguistics. Talmy (2000) divides the con-
cepts expressed in language into two subsystems:
the grammatical subsystem and the lexical sub-
system. Concepts associated with the grammati-
cal subsystem provide a structuring function and
are expressed using so-called closed-class forms
(function words, such as conjunctions, determin-
ers, pronouns, and prepositions, but also suf-
fixes such as plural markers and tense markers).
Closed-class elements (CCEs) provide a scaffold-
ing, across which concepts associated with the lex-
ical subsystem (i.e. nouns, verbs, adjectives and
adverbs) can be draped (Evans and Pourcel, 2009).
Spurk (2006) developed a nominal group (NG)
chunker that makes use of this grammatical sub-
system. Using a finite list of CCEs and learned
word class models for identifying verbs and ad-
verbs, a small set of linguistically motivated ex-
traction patterns is stated to extract NGs. The rules
are based on the following four types of occur-
rences of NGs in English: 1. at the sentence be-
ginning, 2. within a determiner phrase, 3. follow-
ing a preposition and 4. following a verb. Not
being trained on a particular corpus, the chunker
works in a domain-independent way. In addition,
it scales well to large amounts of textual data.
In order to use the chunker for keyphrase extrac-
tion, we manually analysed annotated keyphrases
in scientific texts, and, based on the outcome of the
evaluation, made some adaptations to the chun-
ker, which take care of the fact that the boundaries
of a keyphrase do not always coincide with the
boundaries of a NG. In particular, we remove de-
terminers, split NGs on conjunctions, and process
text within parentheses separately from the main
text. An evaluation on the provided training data
showed that the adapted chunker extracts 80% of
the reader-annotated keyphrases found in the text.
3
This usually covers the introductory part of the article
and is assumed to contain most of the keyphrases. Partial
sentences at the end of this input are cut off.
4 Candidate ranking
The problem of ranking keyphrase candidates can
be formalized as follows: For a document d and
a collection of n keyword candidates C = c
1
...c
n
,
the goal is to compute a ranking r that orders
the candidates in C according to their degree of
keyphraseness in d.
The problem can be transformed into an ordinal
regression problem. In ordinal regression, the la-
bel assigned to an example indicates a rank (rather
than a nominal class, as in classification prob-
lems). The ranking algorithm we use is SVM
rank
,
developed by Joachims (2006). This algorithm
learns a linear ranking function and has shown to
outperform classification algorithms in keyphrase
extraction (Jiang et al, 2009).
The target (i.e. rank) value defines the order of
the examples (i.e. keyphrase candidates). Dur-
ing training, the target values are used to gener-
ate pairwise preference constraints. A preference
constraint is included for all pairs of examples in
the training file, for which the target value differs.
Two examples are considered for a pairwise pref-
erence constraint only if they appear within the
same document.
The model that is learned from the training data
is then used to make predictions on the test ex-
amples. For each line in the test data, the model
predicts a ranking score, from which the ranking
of the test examples can be recovered via sorting.
For ranking the candidates, they are transformed
into vectors based on the features described in sec-
tion 5.
During training, the set of candidates is made up
of the annotated reader and author keywords as
well as all NG chunks extracted from the text.
These candidates are mapped to three different
ranking values: All annotated keywords are given
a ranking value of 2; all extracted NG chunks
that were annotated somewhere else in the train-
ing data are given a ranking value of 1; all other
NG chunks are assigned a ranking value of 0.
Giving a special ranking value to chunks an-
notated somewhere else in the corpus is a way
of exploiting domain-specific information about
keyphrases. Even though not annotated in this par-
ticular document, a candidate that has been anno-
tated in some other document of the domain, is
more likely to be a keyphrase than a candidate that
has never been annotated before (cf. Frank et al
(1999)).
151
5 Features
We used two types of features: term-specific
features and document-specific features. Term-
specific features cover properties of the candidate
term itself (e.g. term length). Document-specific
features relate properties of the candidate to the
text, in which it appears (e.g. frequency of the
term in the document). Our term-specific features
concern the following properties:
? Term length refers to the length of a can-
didate in number of tokens. We express
this property in terms of five boolean fea-
tures: has1token, has2tokens, has3tokens,
has4tokens, has5orMoreTokens. The advan-
tage over expressing term length as a nu-
meric value is that using binary features, we
allow the algorithm to learn that candidates
of medium lengths are more likely to be
keyphrases than very short or very long can-
didates.
? The MSN score of a candidate refers to the
number of results retrieved when querying
the candidate string using the MSN search
engine
4
. The usefulness of MSN scores for
technical term extraction has been shown by
Eichler et al (2009). We normalize the MSN
scores based on the number of digits of the
score and store the normalized value in the
feature normalizedMsn. We also use a binary
feature isZeroMsn expressing whether query-
ing the candidate returns no results at all.
? Special characters can indicate whether a
candidate is (un)likely to be a keyphrase. We
use two features concerning special charac-
ters: containsDigit and containsHyphen.
? Wikipedia has shown to be a valuable source
for extracting keywords (Medelyan et al,
2009). We use a feature isWikipediaTerm,
expressing whether the term candidate corre-
sponds to an entry in Wikipedia.
In addition, we use the following document-
specific features:
? TFIDF, a commonly used feature introduced
by Salton and McGill (1983), relates the fre-
quency of a candidate in a document to its
frequency in other documents of the corpus.
4
http://de.msn.com/
? Term position relates the position of the first
appearance of the candidate in the document
to the length of the document. In addition,
our feature appearsInTitle covers the fact that
candidates appearing in the document title
are very likely to be keyphrases.
? Average token count measures the average
occurrence of the individual (lemmatized) to-
kens of the term in the document. Our
assumption is that candidates with a high
average token count are more likely to be
keyphrases.
? Point-wise mutual information (PMI,
Church and Hanks (1989)) is used to capture
the semantic relatedness of the candidate to
the topic of the document. A similar feature
is introduced by Turney (2003), who, in
a first pass, ranks the candidates based on
a base feature set, and then reranks them
by calculating the statistical association
between the given candidate and the top K
candidates from the first pass. To avoid the
two-pass method, rather than calculating
inter-candidate association, we calculate the
association of each candidate to the terms
specified in the General Terms section of
the paper. Like Turney, we calculate PMI
based on web search results (in our case,
using MSN). The feature maxPmi captures
the maximum PMI score achieved with the
lemmatized candidate and any of the general
terms.
6 Results and critical evaluation
Table 1 presents the results achieved by applying
the KeyWE system on the data set of scientific
articles provided by the organizers of the shared
task along with two sets of manually assigned
keyphrases for each article (reader-assigned and
author-assigned keyphrases). Our model was
trained on the trial and training data (144 articles)
and evaluated on the test data set (100 articles).
The evaluation is based on stemmed keyphrases,
where stemming is performed using the Porter
stemmer (Porter, 1980).
Since SVM
rank
learns a linear function, one can
analyze the individual features by studying the
learned weights. Roughly speaking, a high pos-
itive (negative) weight indicates that candidates
with this feature should be higher (lower) in the
152
Top Set P R F
5
reader 24.40% 10.13% 14.32%
combined 29.20% 9.96% 14.85%
10
reader 19.80% 16.45% 17.97%
combined 23.30% 15.89% 18.89%
15
reader 17.40% 21.68% 19.31%
combined 20.27% 20.74% 20.50%
Table 1: Results on the two keyword sets:
reader (reader-assigned keyphrases) and combined
(reader- and author-assigned keyphrases)
ranking. In our learned model, the four most im-
portant features (i.e. those with the highest ab-
solute weight) were containsDigit (-1.17), isZe-
roMsn (-1.12), normalizedMsn (-1.00), and avgTo-
kenCount (+0.97). This result confirms that web
frequencies can be used as a valuable source for
ranking keyphrases. It also validates our assump-
tion that a high average token count indicates a
good keyphrase candidate. The maxPMI feature
turned out to be of minor importance (-0.16). This
may be due to the fact that we used the terms from
the General Terms section of the paper to calculate
the association scores, which may be too general
for this purpose.
Acknowledgments
We thank Angela Schneider for her adaptations to
the chunker and helpful evaluations. The research
project DiLiA is co-funded by the European Re-
gional Development Fund (ERDF) in the context
of Investitionsbank Berlins ProFIT program under
grant number 10140159. We gratefully acknowl-
edge this support.
References
K. W. Church and P. Hanks. 1989. Word associa-
tion norms, mutual information and lexicography. In
Proceedings of the 27th Annual Conference of the
Association of Computational Linguistics.
K. Eichler, H. Hemsen, and G. Neumann. 2009. Un-
supervised and domain-independent extraction of
technical terms from scientifc articles in digital li-
braries. In Proceedings of the LWA Information Re-
trieval Workshop, TU Darmstadt, Germany.
V. Evans and S. Pourcel. 2009. New Directions in Cog-
nitive Linguistics. John Benjamins Publishing Com-
pany.
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin,
and C. G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In Proceedings of the 16th
International Joint Conference on Artificial Intelli-
gence.
A. Hulth. 2004. Combining Machine Learning and
Natural Language Processing for Automatic Key-
word Extraction. Ph.D. thesis, Department of Com-
puter and Systems Sciences, Stockholm University.
X. Jiang, Y. Hu, and H. Li. 2009. A ranking ap-
proach to keyphrase extraction. In Proceedings of
the 32nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowl-
edge Discovery and Data Mining.
S. N. Kim and M. Y. Kan. 2009. Re-examining auto-
matic keyphrase extraction approaches in scientific
articles. In Proceedings of the ACL/IJCNLP Multi-
word Expressions Workshop.
F. Liu, D. Pennell, F. Liu, and Y. Liu. 2009. Unsu-
pervised approaches for automatic keyword extrac-
tion using meeting transcripts. In Proceedings of the
Conference of the NAACL, HLT.
O. Medelyan, E. Frank, and I.H. Witten. 2009.
Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the Interna-
tional Conference of Empirical Methods in Natural
Language Processing (EMNLP).
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the EMNLP.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
G. Salton and M. J. McGill. 1983. Introduction to
modern information retrieval. McGraw-Hill.
C. Spurk. 2006. Ein minimal ?uberwachtes Verfahren
zur Erkennung generischer Eigennamen in freien
Texten. Diplomarbeit, Saarland University, Ger-
many.
L. Talmy. 2000. Towards a cognitive semantics. MIT
Press, Cambridge, MA.
P. D. Turney. 1999. Learning to extract keyphrases
from text. Technical report, National Research
Council, Institute for Information Technology.
P. D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence.
J.-B. Wang, H. Peng, and J.-S. Hu. 2005. Automatic
keyphrases extraction from document using back-
propagation. In Proceedings of 2005 international
conference on Machine Learning and Cybernetics.
153
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 69?74,
Dublin, Ireland, August 23-24 2014.
An analysis of textual inference in German customer emails
Kathrin Eichler
?
, Aleksandra Gabryszak
?
, G?unter Neumann
?
?
German Research Center for Artificial Intelligence (DFKI), Berlin
(kathrin.eichler|aleksandra.gabryszak@dfki.de)
?
German Research Center for Artificial Intelligence (DFKI), Saarbr?ucken
(neumann@dfki.de)
Abstract
Human language allows us to express the
same meaning in various ways. Recogniz-
ing that the meaning of one text can be in-
ferred from the meaning of another can be
of help in many natural language process-
ing applications. One such application is
the categorization of emails. In this paper,
we describe the analysis of a real-world
dataset of manually categorized customer
emails written in the German language.
We investigate the nature of textual infer-
ence in this data, laying the ground for de-
veloping an inference-based email catego-
rization system. This is the first analysis
of this kind on German data. We compare
our results to previous analyses on English
data and present major differences.
1 Introduction
A typical situation in customer support is that
many customers send requests describing the same
issue. Recognizing that two different customer
emails refer to the same problem can help save
resources, but can turn out to be a difficult task.
Customer requests are usually written in the form
of unstructured natural language text, i.e., when
automatically processing them, we are faced with
the issue of variability: Different speakers of a lan-
guage express the same meanings using different
linguistic forms. There are, in fact, cases where
two sentences expressing the same meaning do not
share a single word:
1. ?Bild und Ton sind asynchron.? [Picture and
sound are asynchronous.]
2. ?Die Tonspur stimmt nicht mit dem Film
?uberein.? [The audio track does not match the
video.]
Detecting the semantic equivalence of sentences
1 and 2 requires several textual inference steps: At
the lexical level, it requires mapping the word pic-
ture to video and sound to audio track. At the
level of compositional semantics, it requires de-
tecting the equivalence of the expressions A and B
are asynchronous and A does not match B.
In this paper, we describe our analysis of a large
set of manually categorized customer emails, lay-
ing the ground for developing an email catego-
rization system based on textual inference. In
our analysis, we compared each email text to the
description of its associated category in order to
investigate the nature of the inference steps in-
volved. In particular, our analysis aims to give an-
swers to the following questions: What text repre-
sentation is appropriate for the email categoriza-
tion task? What kind of inference steps are in-
volved and how are they distributed in real-world
data? Answering these questions will not only
help us decide, which existing tools and resources
to integrate in an inference-based email catego-
rization system, but also, which non-existing tools
may be needed in addition.
2 Related Work
The task of email categorization has been ad-
dressed by numerous people in the last decade.
In the customer support domain, work to be men-
tioned includes Eichler (2005), Wicke (2010), and
Eichler et al. (2012).
Approaching the task using textual inference re-
lates to two tasks, for which active research is go-
ing on: Semantic Textual Similarity, which mea-
sures the degree of semantic equivalence (Agirre
et al., 2012) of two texts, and Recognizing Textual
Entailment (RTE), which is defined as recogniz-
ing, given a hypothesis H and a text T, whether the
meaning of H can be inferred from (is entailed in)
T (Dagan et al., 2005). The task of email catego-
rization can be viewed as an RTE task, where T
69
refers to the email text and H refers to the cate-
gory description. The goal then is to find out if the
email text entails the category description, and if
so, assign it to the respective category.
In connection with RTE, several groups have
analyzed existing datasets in order to investigate
the nature of textual inference. Bar-Haim (2010)
introduces two levels of entailment, lexical and
lexical-syntactic, and analyzes the contribution of
each level and of individual inference mechanisms
within each level over a sample from the first RTE
Challenge test set (Dagan et al., 2005). He con-
cludes that the main contributors are paraphrases
and syntactic transformations.
Volokh and Neumann (2011) analyzed a subset
of the RTE-7 (Bentivogli et al., 2011) development
data to measure the complexity of the task. They
divide the T/H pairs into three different classes,
depending on the type of knowledge required to
solve the problem: In class A, the relevant infor-
mation is expressed with the same words in both
T and H. In class B, the words used in T are
synonyms to those used in H. In class C, recog-
nizing entailment between H and T requires the
use of logical inference and/or world knowledge.
They conclude that for two thirds of the data a
good word-level analysis is enough, whereas the
remainder of the data contains diverse phenomena
calling for a more sophisticated approach.
A detailed analysis of the linguistic phenomena
involved in semantic inferences in the T-H pairs of
the RTE-5 dataset was presented by (Cabrio and
Magnini, 2013).
As the approaches described above, our anal-
ysis aims at measuring the contribution of infer-
ence mechanisms at different representation lev-
els. However, we focus on a different type of text
(customer request as compared to news) and a dif-
ferent language (German as compared to English).
We thus expect our results to differ from the ones
obtained in previous work.
3 Setup
3.1 Dataset
We analyzed a dataset consisting of a set of emails
and a set of categories associated to these emails.
The emails contain customer requests sent to the
support center of a multimedia software company,
and mainly concern the products offered by this
company. Each email was manually assigned to
one or more matching categories by a customer
support agent (a domain expert). These categories,
predefined by the data provider, represent previ-
ously identified problems reported by customers.
All emails and category descriptions are written in
German. As is common for this type of data, many
emails contain spelling mistakes, grammatical er-
rors or abbreviations, which make automatic text
processing difficult. An anonymized
1
version of
the dataset is available online
2
. Our data analysis
was done on the original dataset. The data exam-
ples we use in the following, however, are taken
from the anonymized dataset.
In our analysis, we manually compared the
email texts to the descriptions of their associated
categories in order to investigate the nature of the
inference steps involved. In order to reduce the
complexity of the task, we based our analysis on
the subset of categories, for which the category
text described a single problem (a single H, speak-
ing in RTE terms). We also removed emails for
which we were not able to relate the category de-
scription to the email text. However, we kept
emails associated to several categories and ana-
lyzed all of the assignments. The reduced dataset
we used for our analysis consists of 369 emails as-
sociated to 25 categories. The email lengths vary
between 2 and 1246 tokens. Category descriptions
usually consist of a single sentence or a phrase.
3.2 Task definition
The task of automatically assigning emails to
matching categories can be viewed as an RTE task,
where T refers to the email text and H refers to the
category description. The goal then is to find out if
the email text entails the category description, and
if so, assign it to the respective category.
For the analysis of inference steps involved, we
distinguish between two levels of inference: lexi-
cal semantics and compositional semantics. At the
lexical level, we distinguish two different types of
text representation: First, the bag-of-tokens repre-
sentation, where both the email text and the cate-
gory description are represented as the set of con-
tent word tokens contained in the respective text.
1
The anonymization step was performed to eliminate ref-
erences to the data provider and anonymize personal data
about the customers. During this step, the data was trans-
ferred into a different product domain (online auction sales).
However, the anonymized version is very similar to the orig-
inal one in terms of language style (including spelling errors,
anglicisms, abbreviations, and special characters).
2
http://www.excitement-project.eu/attachments/
article/97/omq_public_email_data.zip
70
Second, the bag-of-terms representation, where a
?term? can consist of one or more content tokens
occurring consecutively. At this level, following
Bar Haim (2010), we assume that entailment holds
between T (the email) and H (the category descrip-
tion) if every token (term) in H can be matched by
a corresponding entailing token (term) in T.
At the level of compositional semantics, we rep-
resent each text as the set of complex expressions
(combinations of terms linked syntactically and
semantically) contained in it. At this level, we
assume that entailment holds between T and H if
every term in H is part of at least one complex ex-
pression that can be matched by a corresponding
entailing expression in T.
The data analysis was carried out by two people
separately (one of them an author of this paper),
who analyzed each assignment of an email E to
a category C based on predefined analysis guide-
lines. For each of the text representation types de-
scribed above, the task of the annotators was to
find, for each expression in the description of C, a
semantically equivalent or entailing expression in
E.
3
If such an expression was found, all involved
inference steps were to be noted down in an anno-
tation table. The predefined list of possible infer-
ence steps is explained in detail in the following.
4 Inference steps
4.1 Lexical semantics level
For each of the three different types of represen-
tation (token, term, complex expression), we dis-
tinguish various inference steps. At the lexical
level, we distinguish among spelling, inflection,
derivation, composition, lexical semantics at the
token level and lexical semantics at the term level.
This distinction was made based on the assump-
tion that for each of these steps a different NLP
tool or resource is required (e.g., a lemmatizer for
inflection, a compound splitter for composition,
a lexical-semantic net for lexical semantics). We
also distinguish between token and term level lexi-
cal semantics, as, for term-level lexical semantics,
we assume that a tool for detecting multi-token
terms would be required.
3
A preanalysis of the data revealed that in some cases,
the entailment direction seemed to be flipped: Expressions
in the category description entailed expressions in the email
text, e.g. ?Video? (video) ? ?Film? (film). In our analysis,
we counted these as positive cases if the context suggested
that both expressions were used to express the same idea. We
consider this an interesting issue to be further investigated.
4.2 Compositional semantics level
At the level of compositional semantics, we con-
sider inference steps involving complex expres-
sions.
4
These steps go beyond the lexical level
and would require the usage of at least a syntac-
tic parser for detecting word dependencies and a
tool for recognizing entailment between two com-
plex expressions. At this level, we also record the
frequency of three particular phenomena: parti-
cle verbs, negation, and light verb constructions,
which we considered worth addressing separately.
Particle verbs are important when processing
German because, unlike in English, they can oc-
cur both as one token or two, dependending on the
syntactic construction, in which they are embed-
ded (e.g., ?aufnehmen? and ?nehme [...] auf? [(to)
record]. Recognizing the scope of negation can be
required in cases where negation is expressed im-
plicitly in one of the sentences, e.g., ?A und B sind
nicht synchron? [A and B are not synchronous]
vs. ?Es kommt zu Versetzung zwischen A und
B? [There is a misaligment between A and B]. By
light verbs we refer to verbs with little semantic
content of their own, forming a linguistic unit with
a noun or prepositional phrase, for which a single
verb with a similar meaning exists, e.g., ?Meldung
kommt? [message appears] vs. ?melden? [notify].
For example, for the text pair ?Das Brennen
bricht ab mit der Meldung X? [Burning breaks
with message X] and ?Beim Brennen kommt die
Fehlermeldung X? [When burning, error message
X appears], the word ?Meldung? [message] was
recorded as inference at the token level because
it can be derived from ?Fehlermeldung? [error
message] using decomposition. The verb ?bricht
ab? [break] was considered inference at the level
of compositional semantics because there is no
lexical-semantic relation to the verb ?kommt? [ap-
pears]. The verb can thus only be matched by con-
sidering the complete expression.
4.3 Possible effects on precision
The focus of the analysis described so far was
on ways to improve recall in an email catego-
rization system: We count the inference steps re-
quired to increase the amount of mappable infor-
mation (similar to query expansion in informa-
tion retrieval). However, the figures do not show
the impact of these mappings on precision, i.e.,
4
Additional lexical inference steps required at this level
are not recorded.
71
whether an inference step we take would nega-
tively affect the precision of the system. Taking
a more precision-oriented view at the problem, we
also counted the number of cases for which a more
complex representation could be ?helpful? (albeit
not necessary). For example, inferring the negated
expression ?Programm kann die DVD nicht ab-
spielen? [Program cannot play the DVD] from
?Programm kann die DVD nicht laden?? [Program
does not load the DVD] is possible at the lexical
level, assuming that ?abspielen? [(to) play] entails
?laden? [(to) load]. However, knowing that both
verbal expressions are negated is expected to be
beneficial to precision, in order to avoid wrongly
inferring a negated from a non-negated expression.
5 Results
5.1 Interannotator agreement
Our analysis was done by two people separately,
which allowed us to measure the reliability of the
annotation for the different inference steps. The
kappa coefficient (Cohen, 1960) for spelling, in-
flection, derivation and composition ranged be-
tween 0.46 and 0.67, i.e., moderate to substan-
tial agreement according to the scale proposed by
Landis and Koch (1977). For lexical semantics,
the value is only fair (0.38). An analysis showed
that the identification of a lexical semantic rela-
tion is often not straightforward, and may require
a good knowledge of the domain. For example,
the verbs ?aufrufen? [call] and ?importieren? [im-
port], which would usually not be considered to
be semantically related, may in fact be used to de-
scribe the same action in the computer domain, re-
ferring to files. Also for the more complex infer-
ence steps, we measured only fair agreement, due
to the number of positive and negative cases being
very skewed. For the ?helpful? cases, the values
ranged between 0.73 and 0.79 (substantial agree-
ment).
5.2 Distribution of inference steps
Table 1 summarizes the distribution of inference
steps identified in our data for each text represen-
tation type, ordered by their frequency of occur-
rence.
5
For multi-token terms, particle verbs, and
negation, the number of ?helpful? cases is given in
brackets.
Our results show that the most important infer-
ence step at the lexical level is lexical semantics.
5
Based on the steps agreed on after a consolidation phase.
At the lexical level, we found 157 different word
mappings. Only 26 of them correspond to a re-
lation in GermaNet (Hamp and Feldweg, 1997),
version 7.0. 48 of the involved words had no Ger-
maNet entry at all, due to the word being an an-
glicism (e.g., ?Error? instead of ?Fehler?), a non-
lexicalized compound (e.g., ?Bildschirmbereich?
[screen area]) or a highly domain- or application-
specific word (for only 37.5% of the words miss-
ing in GermaNet, we found an entry in Wikipedia).
In 72 cases, both words had a GermaNet entry,
but no relation existed, usually because the rela-
tion was too domain-specific.
For more than 30% of the words (as compared
to 10.1% in Bar-Haim?s (2010) analysis on En-
glish), a morphological transformation is required,
which can be explained by the high complexity of
German morphology as compared to the morphol-
ogy of English. Spelling mistakes or differences,
which are not considered in other analyses, are
also found in a considerable number of words, the
reason being that customer emails are less well-
formed than, for example, news texts.
The significance of multi-token terms was sur-
prisingly high for German, where word combina-
tions are usually expressed in the form of com-
pounds (i.e., a single token). In our data, multi-
token terms were usually compounds consisting
of at least one anglicism (e.g., ?USB Anschluss?
[USB port]). This suggests that texts written in
a domain language with a high proportion of En-
glish loan words may be more difficult to process
than general language texts, as multi-token terms
have to be recognized.
At the level of compositional semantics, it
should be noted that, in many cases, recogniz-
ing the entailment relation between two expres-
sions requires world or domain knowledge. Sev-
eral of the mappings involved particle verbs or
light verbs. Detecting negation scope is expected
to be important in a precision-oriented system.
5.3 Comparing text representations
We also had a look at the amount of information
left unmapped at each level. For the lexical level,
we determined for how many of the content tokens
(terms) occurring in the category descriptions, no
matching expression was found in the associated
emails. For the level of compositional semantics,
we looked at each term left unmapped at the lexi-
cal level and tried to map a complex expression in
72
Type of inference Data example Total (Share)
Lexical semantics
(Token)
?Anfang? [start]? ?Beginn? [beginning] 310 (20.2%)
Inflection ?startet? [starts]? ?starten? [start] 206 (13.4%)
Derivation ?Import? [import]? ?importieren? [(to) import] 164 (10.7%)
Composition ?Fehlermeldung? [error message]? ?Meldung? [message] 158 (10.3%)
Spelling ?Dateine?? ?Dateien? [files] 47 (3.1%)
Lexical semantics
(Term)
?MPEG Datei? [MPEG file]? ?Video? [video]
60 (4.1%)
[+124 (8.6%)]
Particle verbs ?spielt [...] ab? [play]? ?abspielen? [play]
26 (1.8%)
[+34 (2.4%)]
Light verbs ?Meldung kommt? [message appears]? ?melden? [notify] 17 (1.2%)
Negation
?Brennerger?at kann nicht gefunden werden? [Burning device cannot be found]
? ?Es wird kein Brenner gefunden? [No burner is found ]
8 (0.6%)
[+121 (8.4%)]
Other complex
expressions
?Das Brennen bricht ab mit der Meldung X? [Burning breaks with message X]
? ?Beim Brennen kommt die Fehlermeldung X? [Burning yields error message X ]
83 (5.7%)
Table 1: Distribution of inference steps in the dataset.
which the term occurred. If for none of these ex-
pressions a matching expression was found in the
email, the term was counted as non-mappable at
this level.
Representation Non-mappable Share
Tokens 428 / 1538 27.8%
Terms 365 /1446 25.2%
Complex expressions 229 / 1446 15.8%
The above table shows that the majority of
the required inference relates to the lexical level.
Choosing a representation that allows us to map
more complex expressions, increases the amount
of mappable terms by almost 10%. However, even
with this more complex representation, a consider-
able amount of terms (15.8%) cannot be mapped
at all because the email text does not contain all
information specified in the category description.
6 Conclusions
In our analysis, we examined the inference steps
required to determine that the text of a category de-
scription can be inferred from the text of a particu-
lar email associated to this category. We identified
major inference phenomena and determined their
distribution in a German real-world dataset. Our
analysis supports previous results for English data
in that a large portion of the required inference re-
lates to the lexical level. Choosing a representa-
tion that allows us to map more complex expres-
sions significantly increases the amount of map-
pable expressions, but some expressions simply
cannot be mapped because the categorization was
done relying on partial information in the email.
Our results extend previous results by investi-
gating inference steps specific to the German lan-
guage (such as morphology, composition, and par-
ticle verbs). Some outcomes are unexpected for
the German language, such as the high share of
multi-token terms. Our analysis also stresses the
importance of inference steps relying on domain-
specific resources, i.e., for this type of data, the
development of tools and resources to support in-
ference in highly specialized domains is crucial.
We are currently using the results of our anal-
ysis to build an email categorization system that
integrates linguistic resources and tools to expand
the linguistic expressions in an incoming email
with entailed expressions. This will allow us to
measure the performance of such a system, in par-
ticular with respect to the effect on precision.
Acknowledgements
This work was partially supported by the EX-
CITEMENT project (EU grant FP7 ICT-287923)
and the German Federal Ministry of Education and
Research (Software Campus grant 01?S12050 ).
We would like to thank OMQ GmbH for provid-
ing the dataset, Britta Zeller and Jonas Placzek for
the data anonymization, and Stefania Racioppa for
her help in the annotation phase.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
73
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6:
A Pilot on Semantic Textual Similarity. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics (SemEval 2012), pages
385?393, Montr?eal, Canada, 7-8 June. Association
for Computational Linguistics.
Roy Bar-Haim. 2010. Semantic Inference at the
Lexical-Syntactic Level. Ph.D. thesis, Department
of Computer Science, Bar Ilan University, Ramat
Gan, Israel.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T. Dang,
and Danilo Giampiccolo. 2011. The Seventh PAS-
CAL Recognizing Textual Entailment Challenge. In
Proceedings of TAC.
Elena Cabrio and Bernardo Magnini. 2013. Decom-
posing Semantic Inferences. Linguistics Issues in
Language Technology - LiLT. Special Issues on the
Semantics of Entailment, 9(1), August.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entail-
ment Challenge. In Proceedings of the PASCAL
Challenges Workshop on Recognising Textual En-
tailment.
Kathrin Eichler, Matthias Meisdrock, and Sven
Schmeier. 2012. Search and Topic Detection in
Customer Requests - Optimizing a Customer Sup-
port System. KI, 26(4):419?422.
Kathrin Eichler. 2005. Automatic classification of
Swedish email messages. Bachelor thesis, Eberhard-
Karls-Universit?at, T?ubingen, Germany.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In In Proceed-
ings of ACL workshop Automatic Information Ex-
traction and Building of Lexical Semantic Resources
for NLP Applications, pages 9?15.
J. R. Landis and G. G. Koch. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics, 33(1):159?174, March.
Alexander Volokh and G?unter Neumann. 2011. Using
MT-Based Metrics for RTE. In Proceedings of the
4th Text Analysis Conference (TAC 2011), Gaithers-
burg, Maryland, USA, November. National Institute
of Standards and Technology.
Janine Wicke. 2010. Automated Email Classification
using Semantic Relationships. Master thesis, KTH
Royal Institute of Technology, Stockholm, Sweden.
74
