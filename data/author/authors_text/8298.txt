Design of Chinese Morphological Analyzer 
 
Huihsin Tseng 
Institute of Information Science 
Academia Sinica, Taipei 
kaori@hp.iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information Science 
Academia Sinica, Taipei 
kchen@iis.sinica.edu.tw 
 
Abstract 
This is a pilot study which aims at the design of a 
Chinese morphological analyzer which is in state 
to predict the syntactic and semantic properties of 
nominal, verbal and adjectival compounds. 
Morphological structures of compound words 
contain the essential information of knowing their 
syntactic and semantic characteristics. In 
particular, morphological analysis is a primary 
step for predicting the syntactic and semantic 
categories of out-of-vocabulary (unknown) words. 
The designed Chinese morphological analyzer 
contains three major functions, 1) to segment a 
word into a sequence of morphemes, 2) to tag the 
part-of-speech of those morphemes, and 3) to 
identify the morpho-syntactic relation between 
morphemes. We propose a method of using 
associative strength among morphemes, 
morpho-syntactic patterns, and syntactic 
categories to solve the ambiguities of 
segmentation and part-of-speech. In our 
evaluation report, it is found that the accuracy of 
our analyzer is 81%. 5% errors are caused by the 
segmentation and 14% errors are due to 
part-of-speech. Once the internal information of a 
compound is known, it would be beneficial for the 
further researches of the prediction of a word 
meaning and its function. 
 
1. Introduction 
This is the first attempt to design a morphological 
analyzer to automatically analyze the 
morphological structures of Chinese compound 
words1. Morphological structures of compound 
words contain the essential information of 
knowing their syntactic and semantic 
characteristics. In particular, morphological 
analysis is a primary step for predicting the 
syntactic and semantic categories of 
out-of-vocabulary (unknown) words. The 
existence of unknown words is a major obstacle in 
Chinese natural language processing. Due to the 
                                                 
1 Compound words here include compounds in 
traditional Chinese linguistics and morphological 
complex words. 
fact that new words are easily coined by 
morphemes in Chinese text, the number of 
unknown words is increasingly large. As a result, 
we cannot collect all the unknown words and 
manually mark their syntactic categories and 
meanings. Our hypothesis to predict the category 
and the meaning of a word is basically based on 
Frege?s principle: ?The meaning of the whole is a 
function of the meanings of the parts?. The 
meanings of morphemes are supposed to make up 
the meanings of the words. However, some words 
like idioms and proper nouns cannot be included 
in the principle. In general, unknown words could 
be divided into two different types: the type that 
has the property of semantic transparency, i.e. the 
words whose meanings can be derived from their 
morphemes and the type without meaning 
transparency, such as proper nouns. In this paper 
we are dealing with the compound words with 
semantic transparency only. For the type of 
compounds without semantic transparency, such 
as proper nouns, their morphemes and 
morphological structures do not provide useful 
information for predicting their syntactic and 
semantic categories. Therefore they are processed 
differently and independently. In addition, some 
regular types of compounds, such as numbers, 
dates, and determinant-measure compounds, are 
easily analyzed by matching their morphological 
structures with their regular expression grammars 
and the result can be used to predict their syntactic 
and semantic properties, so they will be handled 
by matching regular expressions at the stage of 
word segmentation. According to our observation, 
most Chinese compounds have semantic 
transparency except proper nouns, which means 
the meaning of an unknown word can be 
interpreted by their own morpheme components. 
The design of our morphological analyzer will 
focus on processing these compounds, but words 
without semantic transparency are excluded. It 
takes a compound word as input and produces the 
morphological structure of the word. The major 
functions are 1) to segment a word into a sequence 
of morphemes, 2) to tag the part-of-speech of 
those morphemes, and 3) to identify the 
 morpho-syntactic relation between morphemes. 
Once the morpho-syntactic structure of a 
compound is known, the head morpheme provides 
the major clue for determining its syntactic and 
semantic category. 
It seems that a Chinese morphological analyzer is 
similar to a tagging program. Indeed both systems 
have to resolve the segmentation and tagging 
ambiguities. However the major difference is that 
the morphological analyzer does not have 
contextual information of each target word. In 
other words, morphological structures of 
compounds are context independent. We cannot 
apply the same methods, such as n-gram language 
models, to resolve the ambiguities. We proposed a 
method of using the associative strength among 
morphemes, morpho-syntactic patterns, and 
syntactic categories to solve the ambiguities. 
Detail algorithms for morpheme segmentation, 
part-of-speech, and morpho-syntactic relation 
assignment are discussed in Section 2. In the final 
section, we will evaluate the morphological 
analyzer by comparing its results with those 
obtained from the analyses of 5 linguists and 
discussing the categorization of errors found. 
 
2. The Morphological Analyzer 
The morphological analyzer contains three 
functions: to segment a word, to tag the 
part-of-speech (POS) of morphemes, and to 
identify the relation between them.  
2.1 Segmentation  
The goal of this process is to segment a compound 
word into a sequence of morphemes. Since there 
are ambiguous segmentations, simple dictionary 
look-up methods may not work well. For instance, 
the compound of meiguoren (???) could be 
ambiguously segmented into either mei-guoren 
([?[??]] beautiful countryman) or meiguo-ren 
([[ ? ? ] ? ] American people), but only 
meiguo-ren ([[??]?] American-people) is the 
proper segmentation. The left-to-right longest 
matching method is commonly applied to segment 
either words or text. It works well, but there are 
still some small percent of compound words that 
cannot be properly segmented by such a simple 
algorithm. For instance, the word xin-shenghuo 
([?[??]] new life) will be segmented wrongly 
into xinsheng-huo ([[??]?] the life of a new 
student) without considering the priority of 
segmenting the affix xin (?  new) first. In 
particular, words with multi-syllabic suffixes and 
words with reduplication constructions commonly 
cause segmentation errors. Those special types of 
words should be analyzed with other methods. 
 
2.1.1 Affixes and reduplication  
In order to remedy the segmentation error caused 
by the left-to-right longest matching, we observe 
the results of the algorithm and find that there are 
two useful clues to avoid segmentation errors, i.e. 
the information of affix and reduplication.  
A word of a reduplication pattern cannot and need 
not be segmented by the longest left-to-right 
method, since it has special morphological 
structures and the reduplication patterns bring 
enough clues of knowing the syntactic functions 
of the word. Therefore we try to identify words 
that belong to reduplication patterns first. In 
general they fall into the following two types of 
patterns: reduplications and parallel words. Words, 
which do not conform to these patterns, will be 
segmented later.  
Table 1 Special types of patterns and their examples 
Patterns Pattern Maker Note and examples 
Reduplication 
word 
AA,  
AAA, ABB, AAB, 
AxA, ABAB 
AABB, AxAy, 
xByB, 
liang-liang (??), dui-dui-dui 
(???), song-kua-kua (???), 
chi-chi-kan (???),  
xiang-yi-xiang (???). 
yan-jiu-yan-jiu(????),  
chi-chi-he-he(????), 
pao-shang-pao-xia(????),  
yi-nian-zai-nian(????) 
Parallel word A-BC (AC, BC)  zhong-xiao-xue (???) 
 
Reduplication means to duplicate the one or two 
character words into multi-character words. All 
reduplication patterns we used are listed in Table 1. 
If a word belongs to a reduplication pattern, the 
meaning of the word doesn?t change too much. 
The reduplication word?s category can be 
predicted by their patterns. For example, when B 
is not a noun, a word which belongs to the pattern 
AAB is intransitive verb. The category of a word 
that belongs to the pattern of ?parallel word? is 
always a noun. The characteristic of parallel 
words is that both AC and BC are words with 
shared head word C.  
At the next step of the morpheme segmentation, 
we will consider the compounds with affixes. The 
most productive compound construction is the 
structure of a morpheme plus an affix. Hence after 
the affix is identified, it would be easier to 
segment a word into two parts. The segmentation 
algorithm works as follows. A word is segmented 
immediately only if a prefix, infix or suffix 
morpheme is found. The affix table contains 186 
 prefixes, 2 infixes and 648 suffixes. Some affixes 
of the table are multi-syllabic. To segment an affix 
with higher priority will resolve most of the errors 
caused by the left-to-right longest matching 
algorithm. For instance, if tiaoshangqu (??? 
to jump up) is segmented by the left-to-right 
longest matching method, and the result of the 
segmentation is tiaoshang-qu. The left-to-right 
longest matching method might cause error 
segmentation here. However, shangqu is one of 
the suffixes in the affix table, so in our 
morphological analyzer it would be segmented as 
tiao-shangqu. A word containing an infix is also 
not suitable for the general segmentation and it 
would be segmented into single character. There 
are some affixes examples in Table 2: 
Table2 Types of affixes and their examples 
Types of affix Morpheme Examples 
Prefix xin(?) xinsheng-huo (???) 
Infix de(?) suan-de-shang(???) 
Suffix ju(?) 
shangqu (??) 
feizao-ju(???) 
tiao-shangqu (???) 
 
2.1.2 Left-to-right longest matching 
If a word is neither reduplication nor a compound 
with an affix, it should be segmented from left to 
right with longest matching. This general method 
can segment words into morphemes and also 
provide a possible part-of-speech of each 
morpheme by looking it up in the morpheme 
dictionary. 
 
2.2 Tagging  
The work here is to provide the part-of-speech for 
each morpheme and identify a morpho-syntactic 
relation between two morphemes based on the 
information of segmentation and their pos. This is 
the most difficult part of morphological analysis. 
In achieving the goal, we face two obstacles: the 
information insufficiency of morpheme categories 
and morphemes with the multiple categories. 
Since morpheme categories are not the same as 
word categories, it is necessary to assign each 
morpheme with appropriate categories and to 
compile a morpheme dictionary. Once the 
morpheme dictionary is built, the remaining job is 
to resolve the part-of-speech ambiguities of each 
morpheme. Since the part-of-speech of the 
morpheme is independent of its word level context, 
we cannot apply n-gram like language models to 
resolve part-of-speech ambiguity of morphemes. 
Even worse, there is no structure tagging training 
data available either. An EM-like unsupervised 
training on part-of-speech morphological 
structures is also not a sensible solution, since 
morpho-syntactic structure is more sensitive to the 
semantic combination than the syntactic 
combination of morphemes. Therefore we propose 
a method of using morphemes to predict the 
possible syntactic categories of the target 
compound word and selecting the most probable 
consistent result among the candidates of 
part-of-speech structures and the predicted 
categories.  
2.2.1 Preparation of the morpheme 
dictionary 
Before we start to tag morphemes, two steps are 
carried out to resolve the obstacles. That is the 
lack for a morpheme dictionary and morpheme 
ambiguity. First, in order to resolve the lack for 
morpheme categories, it is necessary to edit an 
affix table, as mentioned in Section 2.1, which 
contains prefixes, infixes, suffixes and their 
categories. Most frequently encountered 186 
prefixes and 648 suffixes are listed in this table. 
Basically, if its morpheme has more than 2 
characters, we adopt its categories in the CKIP 
Dictionary. Conversely, if it has fewer than 2 
characters and it functions as a prefix or a suffix, 
we use the categories in the affix table.  
Below we illustrate two examples to explain the 
need of morpheme categories. Both words yu (? 
to speak) and wu (?  to dance) are verbs. 
However they could also function as morphemes. 
When they function as morphemes, they are listed 
as nouns in their category. It is worth noticing that 
the categories of a morpheme are not the same as 
those of a word, even if they are in the same form. 
Therefore, it is important to assign morphemes 
categories properly. 
Table 3 The categories of ?yu and ?wu as a suffix and as a word and 
their examples 
Suffix Category 2  as a 
suffix 
Category as a 
word 
Example 
-yu(?) Na VE ying-yu(??),  
de-yu(??) 
-wu(?) Na VC, Na jueshi-wu(???) 
                                                 
2 The category symbols here are based on CKIP(1993). 
The meaning of each category we adopt here is as 
following: A(non-predicative adjective), Na(common 
noun), Nb(proper noun), Nc(location noun), Nd(time 
noun), VA(active intransitive verb), VB(semi-transitive 
verb), VC(active transitive verb), VCL(active transitive 
verb with locative object), VD(ditransitive verb), 
VE(active transitive verb with sentential object), 
VG(classificatory verb), VH(stative intransitive verb), 
VHC(stative causative verb), VJ(stative transitive verb) 
and so on. 
 Second, in order to resolve the problem of 
morpheme ambiguity, we need a list of 
probabilities which contains all the possible 
combinations of categorical rules and their 
probabilities. For instance, in the list the 
probability P(Na+Na|Na) = 0.4692 means that the 
categorical rule of combining two common nouns 
(Na+Na) to form a common noun (Na) has the 
probability 0.4692. The probability values of each 
categorical rule were estimated from the set of 
11,322 unknown words extracted from Sinica 
Corpus 3.0. The syntactic category of each 
extracted word is known but its structure 
annotation is unknown. Therefore the probability 
of each categorical rule is roughly estimated by 
assuming that every ambiguous categorical 
combination of a word have equal probability. 
The process of computing the possibility of a 
combination is as follows: 
1) We assign morphemes in a word with all their 
possible categories found in the dictionary and the 
affix table; for example, sheyingzhan ([[??]?] 
photography exhibition), which belongs to Na 
category means ?photography exhibition?. 
Sheyingzhan could be segmented as sheying (?? 
photography) and zhan (?  exhibition). After 
segmentation, we found sheying with the 
categories Na and VA, and zhan with the category 
Na. The possible combinations of sheying-zhan 
are ?Na+Na->Na? and ?VA+Na->Na?. However, 
we don?t know which one is correct, so we 
presumably assign a frequency of 0.5 to each 
combination. 
2) After we assign morphemes their categories and 
frequencies, we add up the frequencies of 
identical combinations. A list containing possible 
categorical rules and their probabilities is then 
established. Table 4 shows a part of the 
categorical rules of VHC. 
Table 4 A partial list of categorical rules and their probabilities 
Rule Category Probabilities 
Na+VHC VHC 0.4494 
VH+VHC VHC 0.2303 
Nc+VHC VHC 0.0674 
VHC+VHC VHC 0.0449 
VA+VH VHC 0.0280 
VC+VHC VHC 0.0224 
VJ+VH VHC 0.0224 
Nd+VHC VHC 0.0112 
VC+Na VHC 0.0112 
VC+VC VHC 0.0112 
VC+VHC VHC 0.0112 
 
2.2.2 Part-of-speech 
Once the affix table and the list of categorical 
rules are prepared, we can tackle the problems of 
the obstacles we mentioned in the beginning. 
After morpheme segmentation, each morpheme is 
assigned with their proper categories according to 
the morpheme dictionary and the affix table. 
However, morphemes might be ambiguous, so if 
the category of the target word is known, the most 
probable part-of-speech combination is chosen 
based on the list of categorical rules. However in 
the real implementation, it is assumed that the 
syntactic category of a target word is not known. 
The method mentioned above would not work, 
unless its syntactic category can be predicted. In 
our implementation, we adopted the method 
proposed by Chen, Bai and Chen (1997), by using 
the association strength between morphemes and 
categories to predict the syntactic categories of 
target words. By using the mutual information 
between affixes and categories, the top one 
prediction has the accuracy of 67.00% and the top 
three accuracy of the prediction can reach about 
94.02%. We will then check the consistency 
between predicted the categories and their 
morpho-syntactic structures to make the final 
judgments on both the word category prediction 
and the morpheme category disambiguation.  
The final prediction is based on the maximal value 
of the combined probabilities of the category 
prediction and the categorical rule prediction. 
Since P(Rule|compound) = P(Cat|compound) * 
P(Rule|Cat, compound) ? P(Cat|compound) * 
P(Rule|Cat), we try to find Cat and Rule which 
maximizes P(Cati|compound) * P(Rulej|Cati), for 
all Cati and Rulej. The following is an example 
of .she-ying-zhan. 
 
==================================== 
sheying-zhan (??? photography exhibition) 
 
P(Na|sheying-zhan) *P(Na+Na|Na)= 0.6324*0.4692=0.2967  
---max 
P(Na|sheying-zhan) * P(VA+Na|Na)=0.6324 *0.0865=0.0547 
P(VC|sheying-zhan)* P(Na+Na|VC)=0.3675* 0.0069=0.0025 
P(VC|sheying-zhan)*P(VA+Na|VC)= 0.3675* 0.001=0.0003 
 
sheying-zhan=(Na+Na)->Na 
==================================== 
 
The top1 accuracy of the original category 
prediction for unknown words is 67% by mutual 
information, but after the combination of the 
morphological analyzer, the accuracy of the word 
category prediction is raised to 71%. This is 
because the morphological analyzer will check if 
the categorical combination in a word is in its 
proper category. Therefore, when the original 
unknown word prediction system predicts a word 
in a category which the morphological analyzer 
 finds the probability of its categorical combination 
in the category low, the morphological analyzer 
might reject the category and suggest the 
unknown word prediction system to choose the 
next highest-scoring category in which the 
categorical combination has higher probability. 
In the case that the syntactic category of the 
compound word is known, we will let 
P(Cat|compound) = 1 and the most probable 
part-of-speech combination will simply be the 
categorical rule Rulej such that P(Rulej|Cat) is 
maximized. 
 
2.2.3 Morpho-syntactic relation between 
morphemes 
Once the information of segmentation and 
part-of-speech is ready, the morpho-syntactic 
relation between morphemes can be identified. 
According to Chao (1968) and Li&Thompson 
(1981), there are relations between morphemes in 
compounds such as ?modifier-hear?, ?predicate 
object? and so on. The purpose of knowing 
morpho-syntactic relation between morphemes is 
to help decide the meaning of the target word. The 
morpho-syntactic relation between morphemes is 
grouped into the types listed in Table 5. Generally, 
the relation between morphemes is highly related 
to the category of an unknown word. So the 
relation we assign to morphemes must be based 
on the category of the word. When the unknown 
word is a noun, the relation between its 
morphemes is ?modifier-head?. If it?s a verb, it 
will be more complicated. There are five relation 
types in verbs. The first one is ?verb-object?, such 
as chifan (?? to eat rice).  The first morpheme 
must be a transitive one and the second one should 
be a noun. The second type of the relation is 
?modifier-head?, and it means the second 
morpheme is the semantic head of the word. The 
third type is ?resultative verb?. The second 
morpheme in this type?s word always expresses 
the result of the action. The forth type is 
?head-suffix?. The appearance of the suffix 
changes the augment structure of the head verb, 
but the representing event remains the same. 
These suffixes are ru (? to be similar to), yu (? 
by), wei (? to become), gei (? to give), chu (?
to exit) and cheng (? to become). The fifth type 
of the relation is ?modifier-head?, and there is 
only a morpheme hua (? to transform) which 
belongs to this type. Hua is the head of a word. If 
a non-predicative adjective is an adjective, there 
are two kinds of structure. First, a 
non-predicative adjective has the same structure 
as a noun. The relation between its morphemes is 
also called ?modifier-head?. Second, the relation 
between morphemes for a non-predicative 
adjective which cannot be in the predicate 
position but has verbs structures can be 
?predicate-object? or ?modifier head?. This 
information will be helpful for predicting the core 
meaning of a new word. 
Table 5 The morpho-syntactic relation between morphemes 
 The morpho-syntactic relation between 
morphemes 
Noun Modifier-head 
Verb Verb-object 
Modifier-head 
Resultative Verb (RVC) 
Head-suffix 
Modifier-head(suffix) 
Adjective An: Modifier-head 
Av: verb-object, and modifier-head 
Other directional RVC and reduplication 
 
Once the morpho-syntactic structure of a 
compound is identified, the head morpheme 
provides the major clue for determining its 
syntactic and semantic category. The compound 
word will inherit from the semantic and syntactic 
property of its head and the information will be 
beneficial for the semantic and syntactic 
categorization of new compound words in the 
future.  
 
3. Evaluation and Discussion 
The major functions of the morphological 
analyzer are to segment a word into a sequence of 
morphemes, to tag the part-of-speech of the 
morphemes, and to identify the morpho-syntactic 
relation between morphemes. The work in this 
section is to evaluate the quality of the word 
information which is processed by each function 
of the morphological analyzer. However, it is hard 
to evaluate the accuracy of the morphological 
analyzer automatically, so we compare the results 
generated by the morphological analyzer with 
results generated by human experts, which are 
made out of their language intuition. The answers 
agreed by the majority of the human experts are 
assumed to be the right answers. The closer the 
results of the morphological analyzer are to the 
human experts, the more accurate the 
morphological analyzer is.  
The testing data is the set of unknown words 
extracted from the recently collected text by the 
system of Ma, Hsieh, Yang and Chen (2001). 
There is total 4,566 unknown words in our testing 
data. However, the validity of the morphological 
information is still uncertain; therefore five 
 linguistic specialists have to manually verify the 
morphological structure of unknown words by 
filling out the survey. First, we randomly select 
100 words as a testing set and the following three 
questions are answered by these five specialists. 
 
1) What's the category of the unknown word? 
2) What are the morpheme segmentations of 
the testing words? 
3) What is the syntactic tag of each morpheme? 
 
The definition of our "standard answer" is the 
answer the majority of the subjects give. For 
example, if three out of the five subjects consider 
the category of an unknown word X as VG, the 
standard answer of X would be VG. If five 
subjects think unknown word X belongs to five 
different categories, we would ask one more 
language specialist for opinions to determine the 
category of this unknown word. The standard 
answer we obtained from this survey will be the 
standard answer of the morphological analyzer.  
The morphological analyzer contains three 
functions: to segment a word into morphemes, to 
tag pos, and to identify the relation between 
morphemes. The accuracy we mention here is the 
result from comparing the morphological result 
with the majority answer. 
================================================= 
T=the total number of test set 
R=the total number of being the same with the ?Standard answer? of 
X 
X=Subject1, Subject2, Subject3, Subject4, Subject5, Morphological 
Analyzer 
Accuracy(X)= R(X)/T 
================================================= 
Table 6 The accuracy of five subjects and morphological analyzer 
(MA) 
 S 1 S 2 S 3 S 4 S 5 Average of 5 Ss MA 
Accuracy 89% 94% 94% 86% 83% 89.2% 81% 
 
 
After comparing the result of the morphological 
analyzer with the standard answers obtained from 
the five linguists, we come to the conclusion that 
the accuracy of the morphological analyzer is 81%. 
Out of all the errors, 5% is caused by 
segmentation on proper nouns and loanwords, 
such as bilinshan (??? a name of a mountain), 
dingwan (?? a name of a place), yanyou (?? 
a name of a dynasty), maniuda (??? a name of 
a place), and hongburang (??? home run). 
These words cannot be segmented because they 
only make sense when they are treated as a unit. 
The remaining 14% is caused in the tagging 
process produced by a morpheme table which 
lacks in accuracy. For example, in some cases the 
suffix zhou (? week) is supposed to be listed as 
Nd but is instead listed as Nc. Next, there are no 
proper categories for certain morphemes, such as 
the morpheme lie (? to list as a verb, a row as a 
noun) in the word qinglie (??). In the suffix 
table, the category of lie is only Na, but the 
morpheme lie should have a VC category when 
the meaning of qinglie "to list completely" is 
adopted. Another possible error-causing factor 
would be the choice made by following the 
combination rule. When there is more than one 
possible combination, errors might appear. For 
example, there are two possible combination for 
waizhan (?? to stretch out), ?Ng+Na? and 
?Ng+VC?. Comparing the score of the two 
combinations, the combination of ?Ng+Na? is 
chosen. However, it is not the correct category of 
zhan (? to stretch).  
The best way to resolve these problems mentioned 
above is to revise the morpheme table more often. 
Since the category of the suffix and prefix is fixed, 
it might cause a reduction in morpheme ambiguity. 
We are also interested in the similarity (or the 
range of agreement with language intuition of 
each individual) between those subjects. Since the 
standard answers are the answer of the majority, 
we can compare the standard answer with each 
individual. The average rate of the similarity rate 
is 89.2%. The ten-percentage puzzle might be due 
to the ambiguity of the word and can be 
interpreted that there are indeed some words that 
are not only difficult for a machine to analyze but 
also difficult for human beings to categorize.  
 
Table 7 The error rate and examples 
 Percent-age Examples 
hongburang(???), maniuda(??
?) 
Segmentation 
Error 
5%  
yanyou(??), dingwan(??), 
bilinshan (???) 
Tagging Error 14% mihou(??) tao(?)(Nc), 
zixun(??) zhou(?)(Nc), 
wai(?) zhan(?)(Na,VC) 
 
The evaluation of the identification of the 
morpho-syntactic relation is separated from the 
evaluation of segmentation and tagging, because 
the relation between morphemes is identified 
based on previous information, such as the 
category of a word, segmentation, and the pos. 
Once the essential information is clear, the 
morpho-syntactic relation is known. Nine out of a 
hundred examples are marked by linguists as 
errors of the morpho-syntactic identifier. 
 Furthermore, the reasons causing the error of 
relation identification are 1) the category 
predication?s error, 2) the part-of-speech error, and 
3) the lack of the relation type. Firstly, since the 
relation identifier is based on the result of the 
segmentation and pos, it is understandable that the 
error here is caused by previous functions. The 
category of qipai-jia ([[??]?] initial bidding 
price) is Na, but the system predicts it as an 
intransitive verb VA. So the identifier guesses the 
relation between qipai and jia as ?verb-object? 
based on the previous information. The error of 
the category prediction system might result in 
errors of the relation identifier. Secondly, the 
relation of qing-lie (?? to clearly list) should be 
?modifier-head?, but the identifier marks it as 
?verb-object? relation because lie(?) is tagged as 
Na. When the suffix is a Na, the prefix is a verb, 
and since the category of qing-lie is predicted as a 
verb, the identifier can only predict the relation of 
qing-lie as ?Verb-object?. Therefore, the error of 
part-of-speech might cause the identifier errors. 
Thirdly, the linguists suggest the relation between 
morphemes in nian-song (??  to read) is 
?conjunction relation?. That means that both the 
semantic meaning and syntactic function of nian 
(? to read) and song (? to read) are the same. 
However, we don?t have the ?conjunction 
relation?, because we think the number of words 
which belong to the kind of the relation is very 
limited, and since both morphemes the bring same 
information, there is no difference that enables us 
to mark both of them as heads or only one of them 
as a head for the application of predicating the 
semantic and syntactic property of a word. 
Therefore, in the morphological analyzer the 
words which belong to the ?conjunction? relation 
are identified as ?head-final? relations. 
 
4. Conclusion and future work 
This is a pilot study to design a morphological 
analyzer to analyze the morphological structures 
of Chinese compound words automatically. The 
major functions are 1) to segment a word into a 
sequence of morphemes, 2) to tag the 
part-of-speech of those morphemes, and 3) to 
identify the morpho-syntactic relation between 
morphemes. We evaluate the morphological 
analyzer by comparing 5 linguists? research results 
and discuss the type of errors we find. The more 
similar the results of the morphological analyzer 
compared with the human results, the better the 
morphological analyzer is. It is found that the 
accuracy of our analyzer is 81%. In comparison 
with the performance of human experts resulting 
in an accuracy of 89%, the performance of the 
current morphological analyzer is not bad, but still 
has room for improvement. More, the types and 
the identification of relations of morphemes still 
have much room to be improved. It is also worth 
noticing that the syntactic category prediction for 
general compounds can also be improved by the 
morphological analyzer. Once the internal 
information of a compound is known, it can 
provide clues for prediction of a word meaning 
and its function. The prediction of a word?s 
meaning is very hard and will be one of the main 
themes in our future researches. 
 
5 Reference 
Bosch, Antal van den, Walter Daelemans and Ton 
Weijters. (1996) Morphological Analysis 
Classification: an Inductive-Learning Approach. 
NeMLaP. 
Chao, Yuen Ren. (1968) A grammar of spoken Chinese. 
Berkeley:University of California Press. 
Chen, Chao-jan, Ming-hung Bai and Keh-jiann Chen. 
(1997) Category Guessing for Chinese Unknown 
Words. Proceedings of the Natural Language 
Processing Pacific Rim Symposium 1997, 35-40. 
Chen Yun-chai. (2001) Corpus Analysis of 
Reduplication in Mandarin Chinese. National 
Kaohsiung Normal University: English Department.  
CKIP. (1993) Technical Report no. 93-05: The analysis 
of Chinese category. [??????] CKIP:Nankang 
Creutz, Mathias and Krista Lagus. (2002) 
Unsupervised Discovery of Morphemes. Proceedings 
of Morphological and Phonological Learning 
Workshop of ACL'02. 
Beaney, Michael.(editor) (1997) The Frege Reader. 
Oxfort: Blackwell. 
Li, Charles and Sandra A. Thompson. (1981) Mandarin 
Chinese. Berkeley: University of California Press. 
Ma, Weiyun, Youming Hsieh, Changhua Yang, and 
Keh-jiann Chen. (2001) ?Chinese Corpus 
Development and Management System ? [????
??????????]. Proceedings of Research 
on Computational Linguistics Conference XIV, 
175-191. 
 
Morphological features help POS tagging of unknown words across 
language varieties 
Huihsin Tseng 
Dept. of Linguistics 
University of Colorado 
Boulder, CO 80302 
tseng@colorado.edu
Daniel Jurafsky 
Dept. of Linguistics 
Stanford University 
Stanford, CA 94305 
jurafsky@stanford.edu
Christopher Manning 
Dept. of Computer Science 
Stanford University 
Stanford, CA 94305 
manning@stanford.edu
Abstract
Part-of-speech tagging, like any supervised statistical 
NLP task, is more difficult when test sets are very 
different from training sets, for example when tag-
ging across genres or language varieties. We exam-
ined the problem of POS tagging of different 
varieties of Mandarin Chinese (PRC-Mainland, PRC-
Hong Kong, and Taiwan). An analytic study first 
showed that unknown words were a major source of 
difficulty in cross-variety tagging. Unknown words 
in English tend to be proper nouns. By contrast, we 
found that Mandarin unknown words were mostly 
common nouns and verbs. We showed these results 
are caused by the high frequency of morphological 
compounding in Mandarin; in this sense Mandarin is 
more like German than English. Based on this analy-
sis, we propose a variety of new morphological un-
known-word features for POS tagging, extending 
earlier work by others on unknown-word tagging in 
English and German. Our features were implemented 
in a maximum entropy Markov model. Our system 
achieves state-of-the-art performance in Mandarin 
tagging, including improving unknown-word tagging 
performance on unseen varieties in Chinese Treebank 
5.0 from 61% to 80% correct. 
1 Introduction 
Part-of-speech tagging is an important enabling task 
for natural language processing, and state-of-the-art 
taggers perform quite well, when training and test 
data are drawn from the same corpus. Part-of-speech 
tagging is more difficult, however, when a test set is 
drawn from a corpus that includes significantly dif-
ferent varieties of the language. One factor that may 
play a role in this cross-variety difficulty is the pres-
ence of test-set words that were unseen in cross-
variety training sets. 
We chose Mandarin Chinese to study this question of 
cross-variety and unknown-word POS tagging. Man-
darin is both a spoken and a written language; as a 
written language, it is the official written language of 
the PRC (Mainland and Hong Kong), and Taiwan. 
Thus regardless of which dialect people speak at 
home, they write in Mandarin. But the varieties of 
Mandarin written in the PRC (Mainland and Hong 
Kong) and Taiwan differ in orthography, lexicon, 
and even grammar about as much as the British, 
American, and Australian varieties of English (or 
more in some cases). The corpus we use, Chinese 
Treebank 5.0 (Palmer et al, 2005), contains data 
from the three language varieties as well as different 
genres within the varieties. It thus provides a good 
data set for the impact of language variation on tag-
ging performance. 
Previous work on POS tagging of unknown words 
has proposed a number of features based on prefixes 
and suffixes and spelling cues like capitalization 
(Toutanova et al 2003, Brants 2000, Ratnaparkhi 
1996). For example, these systems followed 
Samuelsson (1993) in using n-grams of letter se-
quences ending and starting each word as unknown 
word features. But these features have mainly been 
tested on inflectional languages like English and 
German, whose derivational and inflectional affixes 
tend to be a strong indicator of word classes; Brants 
(2000), for example, showed that an English word 
ending in the suffix -able was very likely to be an 
adjective. Chinese, by contrast, has more than 4000 
frequent affix characters. The amount of training data 
for each affix is thus quite sparse and (as we will 
show later) Chinese affixes are quite ambiguous in 
their part-of-speech identity. Furthermore, it is possi-
ble that n-gram features may not be suited to Chinese 
at all, since Chinese words are much shorter than 
English (averaging 2.4 characters per word compared 
with 7.7 for English, for unknown words in CTB 5.0 
and Wall Street Journal (Marcus et el., 1993)). 
In order to deal with these difficulties, we first per-
formed an analytic study with the goal of understand-
ing the morphological properties of unknown words 
in Chinese. Based on this analysis, we then propose 
new morphological features for addressing the un-
known word problem. We also showed how these 
features could make use of a non-CTB corpus that 
had been labeled with very different POS tags, by 
converting those tags into features. 
32
The remainder of the paper is organized as follows. 
The next section is concerned with a corpus analysis 
of cross language variety differences and introduces 
Chinese morphology. In Section 3, we evaluate a 
number of lexical, sequence, and linguistic features. 
Section 4 reviews related work and summarizes our 
contribution.  
2 Data
Chinese Treebank 5.0 (CTB) contains 500K words of 
newspaper and magazine articles annotated with seg-
mentation, part-of-speech, and syntactic constituency 
information. It includes data from three major media 
sources, XH1 from PRC, HKSAR2 from Hong Kong, 
and SM3 from Taiwan. In terms of genre, both XH 
and HKSAR focus on politics and economic issues, 
and SM more on topics such as culture, health, edu-
cation and travel. All of the files in CTB are encoded 
using Guo Biao (GB) and use simplified characters.  
We did some cleanup of character encoding errors in 
CTB before running our experiments. Taiwan and 
Hong Kong still use the traditional forms of charac-
ters, while PRC-Mainland has adopted simplified 
forms of many characters, which also collapse some 
distinctions between characters. Additionally a dif-
ferent character set encoding is standardly used. The 
articles in HKSAR and SM originally used tradi-
tional characters and Big 5 encoding, but prior to 
inclusion in the CTB corpus they had been converted 
into simplified characters and GB. Some errors seem 
to have crept into this conversion process, acciden-
tally leaving traditional characters such as ?  instead 
of simplified ? (after),  ?  for ? (for),  ?? and 
?? and ??  (what), all of which we fixed. We 
also normalized half width numbers, alphabets, and 
punctuation to full width. Finally we removed the -
NONE- traces left over from CTB parse trees.  
3 Corpus analysis 
We begin with an analytic study of potential prob-
lems for POS tagging on cross language variety data.  
3.1 More unknown words across varieties? 
We first test our hypothesis that a test set from a dif-
ferent language variety will contain more unknown 
words. Table 1 has the number of words in our 
devset that were unseen in the XH-only training set 
(we describe our training/dev/test split more fully in 
the next section). The devset contains equal amounts 
of data from all three varieties (XH, HKSAR, and 
SM). As table 1 shows, in data taken from the same 
                                                          
1 Xinhua Agency 
2 Information Services Department of Hong Kong Special Admin-
istrative Region 
3 Sinorama magazine 
source as the training data (XH), 4.63% of the words 
were unseen in training, compared to the much larger 
numbers of unknown words in the cross-variety data-
sets (14.3% and 16.7%). Some of this difference is 
probably due to genre as well, especially for the out-
lier-genre SM set. 
Table 1 Percent of words in devset that were unseen in an 
XH-only  training set. See Table 4 for more details. 
Data Set Lang Variety Source Genre % unk
XH  Mainland 
Mandarin
Xinhua News 4.6
HKSAR Hong Kong 
Mandarin
HKSAR News 14.2
SM Taiwan 
Mandarin
Sino-
rama
Magazine 16.7
Devset Mix Mix Mix 12.0
3.2 What are the unknown words? 
In this section, we analyze the part-of-speech charac-
teristics of the unknown words in our devset. 
Table 2 Word class distribution of unknown words in 
devset, XH, HKSAR, SM. Devset represents the conjunc-
tion of the three varieties. CC, DT, LC, P, PN, PU, and SP 
are considered as closed classes by CTB. 
Word class Devset XH HKSAR SM
AD (adverb) 74 2 23 49
CC (coordinating conj.) 7 - - 7
CD (cardinal number) 151 108 23 20
DT (determiner) 10 - 6 4
FW (foreign words) 2 2 - -
JJ (other noun modifier) 79 14 38 27
LC (localizer/postposit) 1 - 1 -
M (measure word) 12 2 4 6
NN (common noun) 1128 131 520 477
NR (proper noun) 400 92 156 152
NT (temporal noun) 53 3 38 12
OD (ordinal number) 4 - 4 -
P (preposition) 16 1 8 7
PN (pronoun) 10 - 3 7
PU (punctuation) 361 - 110 251
SP(sentence final particle) 1 - - 1
VA(predicative adjective) 43 1 19 23
VV (other verbs) 497 25 215 257
Total 2849 381 1168 1300
Table 2 shows that the majority of Chinese unknown 
words are common nouns (NN) and verbs (VV). This 
holds both within and across different varieties. Be-
yond the content words, we find that 10.96% and 
21.31% of unknown words are function words in 
HKSAR and SM data. Such unknown function words 
include the determiner gewei (?everybody?), the con-
junction huoshi (?or?), the preposition liantong
(?with?), the pronoun nali (?where?), and symbols 
used as quotes  ??? and ??? (punctuation). XH 
does contain words with similar function (huozhe
33
?or?, yu ?with?, dajia ?everybody?, quotation marks 
???and ???). Our result thus suggests that each 
Mandarin variety may have characteristic function 
words. 
3.3 Cross language comparison 
A key goal of our work is to understand the way that 
unknown words differ across languages. We thus 
compare Chinese, German, and English. Following 
Brants (2000), we extracted 10% of the data from the 
Penn Treebank Wall Street Journal (WSJ 4 ) and 
NEGRA5 (Brants et al, 1999) as observation samples 
to compare to the rest of the corpora. 
In these observation samples, we found that Chinese 
words are more ambiguous in POS than English and 
German; 29.9% of tokens in CTB have more than 
one POS tag, while only 19.8% and 22.9% of tokens 
are ambiguous in English and German, respectively. 
Table 3 shows that 40.6% of unknown words are 
proper nouns6  in English, while both Chinese and 
German have less than 15% of unknown words as 
proper nouns. Unlike English, 60% of the unknown 
words in Chinese and German are verbs and common 
nouns. In the next section we investigate the cause of 
this similarity between Chinese and German un-
known word distribution. 
Table 3 Comparison of unknown words in English, Ger-
man and Mandarin. The English and German data are ex-
tracted from WSJ and NEGRA. Chinese data is our CTB 
devset.
Language English% German% Chinese%
Proper nouns 40.6 12.2 14.0 
Other nouns 24.0 53.0 41.5 
Verbs 6.8 11.4 19.0 
ALL 100.0 100.0 100.0 
4 Morphological analysis 
In order to understand the causes of the similarity of 
Chinese and German, and to help suggest possible 
features, we turn here to an introduction to Chinese 
morphology and its implications for part-of-speech 
tagging. 
                                                          
4 WSJ unknown words are those in WSJ 19-21 but unseen in WSJ 
0-18; these are the devset and training set from Toutanova et al 
(2003). 
5 The unknown words of NEGRA are words in a 10% randomly 
extracted set that were unseen in the rest of the corpus. 
6 We treat NNP (proper noun) and NNPS(proper noun plural) as 
proper nouns, NN(noun) and NNS(noun plural) as other nouns, 
and V* as verbs in WSJ. We treat NE (Eigennamen) as proper 
nouns, NN (Normales Nomen) as other nouns, and V* as verbs in 
NEGRA. We treat NR as proper nouns, NN and NT as other nouns, 
and V* as verbs in CTB.  
4.1 Chinese morphology 
Chinese words are typically formed by four morpho-
logical processes: affixation, compounding, idiomi-
zation, and reduplication, as shown in Table 4. 
In affixation, a bound morpheme is added to other 
morphemes, forming a larger unit. Chinese has a 
small number of prefixes and infixes7 and numerous 
suffixes (Chao 1968, Li and Thompson 1981). Chi-
nese prefixes include items such as gui (?noble?) in 
guixing (?your name?), bu (?not?) in budaode (?im-
moral?), and  lao (?senior?) in laohu (?tiger?) and 
laoshu (?mouse?). There are a number of Chinese 
suffixes, including zhe (?marks a person who is an 
agent of an action?) in zuozhe (?author?), shi (?mas-
ter?) in laoshi (?teacher?), ran (-ly) in huran (?sud-
denly?), and xin (-ity or ?ness) in kenengxin
(?possibility?). 
Compound words are composed of multiple stem 
morphemes. Chao (1968) describes a few of the dif-
ferent compounding rules in Mandarin, such as coor-
dinate compound, subject predicate compound, noun 
noun compound, adj noun compound and so on. Two 
examples of coordinate compounds are anpai
ARRANGE-ARRANGE (?to arrange, arrangement?) 
and xuexi STUDY-STUDY (?to study?). 
Table 4 Chinese morphological rules and examples 
 Examples 
Prefix lao (?senior?) in laohu ( ?tiger?)  
Suffix shi (?master?) in laoshi (?teacher?) 
Compounding xuexi  (?to study?, ?study?) 
Idiomization wanshiruyi (?everything is fine?) 
Reduplication changchang (?taste a bit?) 
Compounding is extremely common in both Chinese 
and German. The phrase ?income tax? is treated as 
an NP in English, but it is a word in German, Ein-
kommensteuer, and in Chinese, suodesui. We suggest 
that it is this rich use of compounding that causes the 
wide variety of unknown common nouns and verbs 
in Chinese and German. However, there are still dif-
ferences in their compound rules. German com-
pounds can compose with a large number of elements, 
but Chinese compounds normally consist of two 
bases. Most German compounds are nouns, but Chi-
nese has both noun and verb compounds.  
Two final types of Chinese morphological processes 
that we will not focus on are idiomization (in which a 
whole phrase such as wanshiruyi (?everything is 
fine?) functions as a word, and reduplication, in 
which a morpheme or word is repeated to form a new 
word such as the formation of changchang (?taste a 
                                                          
7 Chinese only has two infixes, which are de and bu (not). We do 
not discuss infixes in the paper, because they are handled phrasally 
rather than lexically in CTB. 
34
bit?), from chang ?taste?. (Chao 1968, Li and 
Thompson 1981).  
4.2 Difficulty
The morphological characteristics of Chinese create 
various problems for part-of-speech tagging. First, 
Chinese suffixes are short and sparse. Because of the 
prevalence of compounding and the fact that the mor-
phemes are short (1 character long), there are more 
than 4000 affixes. This means that the identity of an 
affix is often a sparsely-seen feature for predicting 
POS. Second, Chinese affixes are poor cues to POS 
because they are ambiguous; for example 63% of 
Chinese suffix tokens in CTB have more than one 
possible tag, while only 31% of English suffix tokens 
in WSJ have more than one tag. Most English suf-
fixes are derivational and inflectional suffixes like   -
able, -s and -ed. Such functional suffixes are used to 
indicate word classes or syntactic function. Chinese, 
however, has no inflectional suffixes and only a few 
derivational suffixes and so suffixes may not be as 
good a cue for word classes. Finally, since Chinese 
has no derivational morpheme for nominalization, it 
is difficult to distinguish a nominalization and a verb.  
These points suggest that morpheme identity, which 
is the major feature used in previous research on un-
known words in English and German, will be insuffi-
cient in Chinese. This suggests the need for more 
sophisticated features, which we will introduce be-
low.  
5 Experiments
We evaluate our tagger under several experimental 
conditions: after showing the effects of data cleanup 
we show basic results based on features found to be 
useful by previous research. Next, we introduce addi-
tional morphology-based unknown word features, 
and finally, we experiment with training data of vari-
able sizes and different language varieties. 
5.1 Data sets 
To study the significance of training on different 
varieties of data, we created three training sets: train-
ing set I contains data only from one variety, training 
set II contains data from 3 varieties, and is similar in 
total size to training set I. Training set III also con-
tains data from 3 varieties and has twice much data 
as training set I. To facilitate comparison of perform-
ance both between and within Mandarin varieties, 
both the devset and the test set we created are com-
posed of three varieties of data. The XH test data we 
selected was identical to the test set used in previous 
parsing research by Bikel and Chiang (2000). For the 
remaining data, we included HKSAR and SM data 
that is similar in size to the XH test set. Table 5 de-
tails characteristics of the data sets. 
Table 5 Data set splits used. The unknown word tokens are 
with respect to Training I. 
Data set Sect'ns Token Un-
known
Training I 26-270, 600-931 213986 - 
Training II 600-931, 500-527,  
1001-1039
204701 - 
Training III 001-270, 301-527,  
590-593, 600-1039,  
1043-1151
485321 - 
Devset  23839 2849 
XH 001-025 7844 381 
HKSAR 500-527 8202 1168 
SM 590-593, 1001-1002 7793 1300 
Test set  23522 2957 
XH 271-300 8008 358 
HKSAR 528-554 7153 1020 
SM 594-596, 1040-1042 8361 1579 
5.2 The model 
Our model builds on research into loglinear models 
by Ng and Low (2004), Toutanova et al, (2003) and 
Ratnaparkhi (1996). The first research uses inde-
pendent maximum entropy classifiers, with a se-
quence model imposing categorical valid tag 
sequence constraints. The latter two use maximum 
entropy Markov models (MEMM) (McCallum et al, 
2000), that use log-linear  models to obtain the prob-
abilities of a state transition given an observation and 
the previous state, as illustrated in Figure 1 (a).  
Figure 1 Graphical representation of transition probability 
calculation used in maximum entropy Markov models. (a) 
The previous state and the current word are used to calcu-
late the transition probabilities for the next state transition. 
(b) Same as (a), but when model is run right to left. 
Using left-to-right transition probabilities, as in Fig-
ure 1 (a), the equation for the MEMM can be for-
mally stated as the following, where by di represents 
the set of features the transition probabilities are con-
ditioned on: 
( ) ( )iii d|tPwt,P ?=
Maximum entropy is used to calculate the probability 
P(ti| di) using the equation below. Here, fj(ti,di) repre-
sents a feature derived from the available contextual 
information (e.g. current word, previous tag, next 
word, etc.) 
TiTi-1 
Wi
Ti
Wi
(a) (b) 
Ti+1 
35
( )
( )
( )? ?
?
?
=
Tt'
i
j
ii
j
i
)d,t'exp(
)d,t(exp
d|tP i f
f
jj
jj
?
?
We also used Gaussian prior to prevent overfitting. 
This technique allows us to utilize a large number of 
lexical and MEMM state sequence based features 
and also provides an intuitive framework for the use 
of morphological features generated from unknown 
word models. 
5.3 Data cleanup 
Before investigating the effect of our new features, 
we show the effects of data cleanup. Table 6 illus-
trates the .46 (absolute) performance gain obtained 
by cleaning character encoding errors and normaliz-
ing half width to full width.  
We also clustered punctuation symbols, since train-
ing set I has too many (36) variety of punctuations, 
compared to 9 in WSJ. We clustered punctuations, 
for example grouping ??? and ??? together. This 
mapping renders an overall improvement of .08%. 
All models in the following sections are then trained 
on font-normalized and punctuation clustered data. 
Table 6 Improvement of tagging accuracy after data 
cleanup. The features used by all of the models are the 
identity of the two previous words, the current word and 
the two following word. No features based on the sequence 
of tags were used. 
Models Token A8 % ? Token A% Unk A % 
2Rw+2Lw 87.11 - 47.03 
+Cleanup 87.57 0.46 48.54 
+PU 87.65 0.08 49.26 
5.4 Sequence features 
We examined several tag sequence features from 
both left and right side of the current word. We use 
the term lexical features to refer to features derived 
from the identity of a word, and tag sequence fea-
tures refer to features derived from the tags of sur-
rounding words.  
These features have been shown to be useful in pre-
vious research on English (Toutanova et al 2003, 
Brants 2000, Thede and Harper 1999) 
The models9 in Table 7 list the different tag sequence 
features used; they also use the same lexical features 
from the model 2Rw+2Lw shown in Table 6. The ta-
ble shows that Model Lt+LLt conditioning on the 
previous tag and the conjunction of the two previous 
                                                          
8 We abbreviate accuracy as ?A?. 
9 Except where otherwise stated, during training, a count cutoff of 
3 is applied to all features found in the training set. If a feature 
occurs fewer than 3 times, it is simply removed from the training 
data. All models are trained on training set I and evaluated on the 
devset.  
tags yields 88.27%. As such, using the sequence fea-
tures<ti-1, ti-1ti-2> achieves the current best result.  
So far, there are no features specifically tailored to-
ward unknown words in the model. 
Table 7 Tagging accuracy of different sequence feature sets.  
Models Feature sets Token A 
%
Unk A %
Rt+RRt
+2Rw+2Lw
<ti,ti+1>,<ti,ti+1,ti+2>
+ lexical features 
88.10 50.11 
Lt+LLt
+2Rw+2Lw
<ti,ti-1>,<ti,ti-1,ti-2>
+lexical features 
88.27 51.16 
5.5 Unknown word model 
Starting with Model Lt+LLt from the last section, we 
introduce 8 features to improve the performance of 
the tagger on unknown words. In the sections that 
follow, the model using affixation in conjunction 
with the basic lexical features described above is 
considered to be our baseline. 
We considered words that occur less than 7 times in 
the training set I as rare; if Wi is rare, an unknown 
word feature is used in place of a feature based on 
the actual word?s identity. During evaluation, un-
known word features are used for all words that oc-
curred zero to 7 times in the training data. In addition, 
when tagging such rare and unknown words, we re-
strict the set of possible tags to just those tags that 
were associated with one or more rare words in the 
training data. 
5.5.1 Affixation
Our affixation feature is motivated by similar fea-
tures seen in inflectional language models. (Ng and 
Low 2004, Toutanova et al 2003, Brants 2000, Rat-
naparkhi 1996, Samuelsson 1993). Since Chinese 
also has affixation, it is reasonable to incorporate this 
feature into our model. For this feature, we use char-
acter n-gram prefixes and suffixes for n up to 4.10 An 
example is:  
??? INFORMATION-BAG "folder"  
Wi=??? ?a folder? 
FAFFIX={(prefix1,?), (prefix2,??), (prefix3,??
?), (suffix1,?), (suffix2,??), (suffix3,???)} 
5.5.2 CTBMorph (CTBM) 
While affix information can be very informative, we 
showed earlier that affixes in Chinese are sparse, 
short, and ambiguous.  Thus as our first new feature 
we used a POS-vector of the set of tags a given affix 
could have. We used the training set to build a mor-
pheme/POS dictionary with the possible tags for each 
                                                          
10 Despite the short average word length, we found that affixes up 
to size 4 worked better than affixes only up to size 2, perhaps 
mainly because they help with long proper nouns and temporal 
expressions. 
36
morpheme. Thus for each prefix and suffix that oc-
curs with each CTB tag in the training set I, we asso-
ciate a set of binary features corresponding to each 
CTB tag. In the example below the prefix ? oc-
curred in both NN and VV words, but not AD or AS. 
Prefix1=?, suffix1=?
FCTBM-pre= {(AD,0),(AS,0),?(NN,1),?(VV,1)} 
FCTBM-suf= {(AD,0),(AS,0),?(NN,1),?(VV,0)} 
This model smoothes affix identity and the quantity 
of active CTBMorph features for a given affix ex-
presses the degree of ambiguity associated with that 
affix.  
Figure 2 Pseudo-code for CTBMorph
GenCTBMorphFeatureSet (Word W) 
  FeatureSet f; 
  for each t in CTB tag set: 
     for each single-character prefix or suffix k of W 
       if t.affixList contain k f.appendPair(t, 1) 
        else f.appendPair(t, 0)  
5.5.3 ASBC
One way to deal with robustness is to add more var-
ied training data.  For example the Academic Sinica 
Balanced Corpus11 contains POS-tagged data from a 
different variety (Taiwanese Mandarin). But the tags 
in this corpus are not easily converted to the CTB 
tags. This problem of labeled data from very differ-
ent tagsets can happen more generally. We introduce 
two alternative methods for making use of such a 
corpus.
5.5.3.1 ASBCMorph (ASBCM) 
The ASBCMorph feature set is generated in an iden-
tical manner to the CTBMorph feature set, except 
that rather than generating the morpheme table using 
CTB, another corpus is used. The morpheme table is 
generated from the Academic Sinica Balanced Cor-
pus, ASBC (Huang and Chen 1995), a 5 M word 
balanced corpus written in Taiwanese Mandarin. As 
the CTB annotation guide12 states, the mapping be-
tween the tag sets used in the two corpora is non-
trivial. As such, the ASBC data can not be directly 
used to augment the training set. However, using our 
ASBCMorph feature, we are still able to derive some 
benefit out of such an alternative corpus.  
5.5.3.2 ASBCWord (ASBCW) 
The ASBCWord feature is identical to the 
ASBCMorph feature, except that instead of using a 
table of tags that occur with each affix, we use a table 
of tags that a word occurs with in the ASBC data. 
                                                          
11 The ASBC was originally encoded in traditional Big5 character, 
and we converted it to simplified GB. 
12 http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf 
Thus, a rare word in the CTB training/test set is 
augmented with features that correspond to all of the 
tags that the given word occurred with in the ASBC 
corpus, i.e. in this case, the pos tag of the identical 
word in ASBC, ???.
Wi=???
FASBCWord={(A,0),(Caa,0),(Cab,0)?(V_2,0)}  
5.5.4 Verb affix 
This feature set contains only two feature values, 
based on whether a list of verb affixes contains the 
prefix or suffix of an unknown word. We use the 
verb affix list created by the Chinese Knowledge 
Information Processing Group13 at Academia Sinica. 
It contains 735 frequent verb prefixes and 282 fre-
quent verb suffixes. For  example, 
Prefix1=?,  suffix1=?
Fverb={(verb prefix, 1), (verb suffix, 0)} 
5.5.5 Radicals
Radicals are the basic building blocks of Chinese 
characters. There are over 214 radicals, and all Chi-
nese characters contain one or more of them. Some-
times radicals reflect the meaning of a character. For 
example, the characters ?  (monkey), ?  (pig) ?
(kitty cat) all contain the radical ?  that roughly 
means ?something that is an animal?. For our radical 
based feature, we use the radical map from the Uni-
han database.14 The radicals associated with the char-
acters in the prefix and suffix of unknown words 
were incorporated into the model as features, for ex-
ample: 
Prefix1=?, suffix1=?
FRADICAL={(radical prefix, ?), (radical suffix,?)} 
5.5.6 Named Entity Morpheme (NEM) 
There is a convention that the suffix of a named en-
tity points out the essential meaning of the named 
entity. For example, the suffix bao (newspaper) ap-
pears in Chinese translation of ?WSJ?, huaerjierebao.
The suffix he (river) is used to identify rivers, for 
example in huanghe (yellow river).  
To take advantage of this fact, we made 3 tables of 
named entity characters from the Chinese English 
Named Entity Lists (CENEL) (Huang 2002). These 
lists consist of a table of Chinese first name charac-
ters, a table of Chinese last name characters, and a
                                                          
13 http://turing.iis.sinica.edu.tw/affix/ 
14 Unihan database is downloadable from their website: 
http://www.unicode.org/charts/unihan.html. 
37
Table 8 Devset performance of the cumulatively rare word models, starting with the baseline. The second and third columns show the 
change in token accuracies and unknown word accuracies from the baseline for each feature introduced cumulatively. The fourth column 
shows the improvement from each feature set. The six columns on the right side of the table shows the error rate for the 5 most frequent 
tagsets of unknown words and the rest of unknown words.  
 Error analysis: error rate % of unknown words in each POS 
Feature (add one in) Token Unk A% ? Unk A% NN VV NR PU CD Others 
Lt+LLt 88.27 51.16 - 16.67 57.14 68.25 100.00 16.56 60.86 
+Suffix 89.70 60.74 9.58 12.50 41.65 44.75 100.00 5.30 37.25 
  +Prefix ? baseline 90.03 63.66 2.92 10.55 36.62 40.00 100.00 3.97 34.76 
    +CTBM 91.48 76.13 12.47 13.74 31.99 36.00 1.99 0.00 20.58 
       +ASBCM 91.69 77.36 1.23 14.01 28.37 33.75 1.99 0.66 19.57 
         +ASBCW 91.85 78.84 1.48 13.30 23.54 33.50 1.42 0.00 17.93 
           +Verb affix 91.82 79.05 0.21 12.59 24.14 32.75 0.85 0.00 17.76 
              +Radical 91.85 79.09 0.04 11.88 24.75 33.50 0.85 0.00 18.78 
                +NEM 91.91 79.61 0.53 12.23 23.54 31.00 0.85 0.00 18.39 
                   +Length?best 91.97 79.86 0.25 12.15 22.94 30.25 0.85 0.00 18.21 
table of named entity suffixes such as organization, 
place, and company names in CENEL. Our named 
entity feature set contains 3 features, each corre-
sponding to one of the three tables just described. To 
generate these features, first, we check if the prefix 
of an unknown is in the Chinese last name table. Sec-
ond, we check if the suffix is in the Chinese first 
name table. Third, we check if the suffix of an un-
known word is in the table of named entity suffixes. 
In Chinese last names are written in front of a first 
name, and the whole name is considered as a word, 
for example: 
Prefix1=?,  suffix1=?
FNEM={(last name, 0), (first name, 0), (NE suffix, 
1)}
5.5.7 Length of a word 
The length of a word can be a useful feature, because 
the majority of words in CTB have less than 3 char-
acters. Words that have more than 3 characters are 
normally proper nouns, numbers, and idioms. There-
fore, we incorporate this feature into the system. For 
example:  
Wi=???, Flength={(length , 3)} 
5.5.8 Evaluation
Table 8 shows our results using the standard maxi-
mum entropy forward feature selection algorithm; at 
each iteration we add the feature family that most 
significantly improves the log likelihood of the train-
ing data given the model. We seed the feature space 
search with the features in Model Lt+LLt. From this 
model, adding suffix information gives a 9.58% (ab-
solute) gain on unknown word tagging. Subsequently 
adding in prefix makes unknown word accuracy go 
up to 63.66%. Our first result is that Chinese affixes 
are indeed informative for unknown words.  On the 
right side of Table 8, we can see that this perform-
ance gain is derived from better tagging of common 
nouns, verbs, proper nouns, numbers and others. Be-
cause earlier work in many languages including Chi-
nese uses these simple prefix and suffix features 
(Brants 2000, Ng and Low 2004) we consider this 
performance (63.66% on unknown words) as our 
baseline. 
Adding in the feature set CTBM gives another 
12.47% (absolute) improvement on unknown words. 
With this feature, punctuation shows the largest tag-
ging improvement. The CTBM feature helps to iden-
tify punctuation since all other characters have been 
seen in different morpheme table made from the 
training set. That is, for a given word the lack of 
CTBM features cues that the word is a punctuation 
mark. Also, while this feature set generally helps all 
tagsets, it hurts a bit on nouns. 
Adding in the ASBC feature sets yields another 
1.23% and 1.48% (absolute) gains on unknown 
words. These two feature sets generally improve per-
formance on all tagsets. Including the verb affix fea-
ture helps with common nouns and proper nouns, but 
hurts the performance on verbs. Overall, it yields 
0.21% gain on unknown words. Finally, adding the 
radical feature helps the most on nouns, while subse-
quently adding in the name entity morphemes help to 
reduce the error on proper nouns by 2.50%. Finally, 
adding in feature length renders a 0.25% gain on 
unknown words.  Commutatively, applying the fea-
ture sets results in an overall accuracy of 91.97% and 
an unknown word accuracy of 79.86%. 
5.6   Experiments with the training sets of 
variable sizes and varieties 
In this section, we compare our best model with the 
baseline model using different corpora size and lan-
guage varieties in the training set. All the evaluations 
are reported on the test set, which has roughly equal 
amounts of data from XH, HKSAR, and SM. 
The left column of Table 9 shows that when we train 
a model only on a single language variety and test on 
a mixed variety data, our unknown word accuracy is 
79.50%, which is 18.48% (absolute) better than the 
baseline. The middle column shows when the train-
ing set is composed of different varieties and hence 
looks like the test set, performance of both the base-
line and our best model improves. 
38
Table 9 Comparison of the baseline and our best model. 
Using different training sets to evaluate on the test set. 
(McNemar?s Test  p <.001)
 Training  I Training  II Training III 
Token Unk Token Unk  Token Unk 
Base-
line
89.17 61.02 92.54 74.78 93.51 81.11
Best 91.34 79.50 93.00 81.62 93.74 86.33
The right column shows the effect of doubling the 
training set size, using mixed varieties. As expected, 
using more data benefits both models.  
These results show that having training data from 
different varieties is better than having data from one 
source. But crucially, our morphological-based fea-
tures improve the tagging performance on unknown 
words even when the training set includes some data 
that resembles the test set. 
How good are our best numbers, i.e. 93.7% on POS 
tagging in CTB 5.0? Unfortunately, there are no 
clean direct comparisons in the literature. The closest 
result in the literature is Xue et al (2002), who re-
train the Ratnaparkhi (1996) tagger and reach accu-
racies of 93% using CTB-I. However CTB-I contains 
only XH data and furthermore the data split is no 
longer known for this experiment (Xue p.c.) so a 
comparison is not informative. However, our per-
formance on tagging when trained on Training I and 
tested on just the XH part of the test set is 94.44%, 
which might be a more relevant comparison to Xue 
et al (2002).   
6 Conclusion
Previous research in part-of-speech tagging has re-
sulted in taggers that perform well when the training 
set and test set are both drawn from the same corpus. 
Unfortunately, for many potential real world applica-
tions, such an arrangement is just not possible.  
Our results show that using sophisticated morpho-
logical features can help solve this robustness prob-
lem. These features would presumably also be 
applicable to other languages and NLP tasks that 
could benefit from the use of morphological informa-
tion 
Besides these tagging results, our research provides 
valuable analytic results on understanding the nature 
of unknown words cross-linguistically. Our results 
that unknown words in Chinese are not proper nouns 
like in English, but rather common nouns and verbs, 
suggest a similarity to German. We suggest this is 
because both German and Chinese, despite their huge 
differences in genetic, area, and other typological 
characteristics, tend to form unknown words through 
a similar word formation rule, compounding.  
7 Acknowledgement 
Thanks to Kristina Toutanova and Galen Andrew for 
their generous help and to the anonymous reviewers. 
This work was partially funded by ARDA 
AQUAINT and by NSF award IIS-0325646. 
8 References 
Bikel, Daniel and David Chiang. 2000. Two statisti-
cal parsing models applied to the Chinese Tree-
bank. In CLP 2.
Brants, Thorsten. 2000. TnT: a statistical part-of-
speech tagger. In ANLP 6. 
Brants, Thorsten Wojciech Skut, Hans Uszkoreit. 
1999. Syntactic Annotation of a German Newspa-
per Corpus In: Anne Abeill?: ATALA sur le Corpus 
Annot?s pour la Syntaxe Treebanks.
Chao, Yuen Ren. 1968. A Grammar of Spoken Chi-
nese. Berkeley: University of California Press. 
Huang, Chu-ren. and Keh-Jiann Chen. 1995. Aca-
demic Sinica Balanced Corpus. Technical Report 
95-02/98-04. Academic Sinica. 
Huang, Shudong. 2002. Chinese <-> English Name 
Entity Lists Version 1.0 beta. Catalog number: 
LDC2003E01. 
Li, Charles and Sandra A Thompson. 1981. Manda-
rin Chinese: A Functional Reference Grammar.
Berkeley: University of California Press. 
McCallum, Andrew, Dayne Freitag, Fernando 
Pereira. 2000. Maximum Entropy Markov Models 
for Information Extraction and Segmentation. In
ICML 17. 
Marcus, Mitchel, Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: The Penn Treebank. In Compu-
tational Linguistics, 19. 
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based? In EMNLP 9.  
Martha Palmer, Fu-Dong Chiou, Nianwen Xue, 
Tsan-Kuang Lee. 2005. Chinese Treebank 5.0. 
Catalog number: LDC2005T01. 
Ratnaparkhi, Adwait. 1996. A maximum entropy 
model forpart-of-speech tagging. In EMNLP 1.
Thede, Scott and Mary P. Harper. 1999. Second-
order hidden Markov model for part-of-speech 
tagging. In ACL 37.
Toutanova, Kristina, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In HLT-NAACL 2003. 
Samuelsson, Christer. 1993. Morphological tagging 
based entirely on bayesian inference. In NCCL 9.
Xue, Nianwen, Fu-dong Chiou and Martha Palmer. 
2002. Building a large-scale annotated Chinese 
corpus. In COLING.
39
 A Conditional Random Field Word Segmenter  
for Sighan Bakeoff 2005 
Huihsin Tseng 
Dept. of Linguistics 
University of Colorado 
Boulder, CO 80302 
tseng@colorado.edu
Pichuan Chang, Galen Andrew,  
Daniel Jurafsky, Christopher Manning 
Stanford Natural Language Processing Group 
Stanford University 
Stanford, CA 94309 
{pichuan, pupochik, jurafsky, manning}@stanford.edu 
Abstract
We present a Chinese word seg-
mentation system submitted to the 
closed track of Sighan bakeoff 2005. 
Our segmenter was built using a condi-
tional random field sequence model 
that provides a framework to use a 
large number of linguistic features such 
as character identity, morphological 
and character reduplication features. 
Because our morphological features 
were extracted from the training cor-
pora automatically, our system was not 
biased toward any particular variety of 
Mandarin. Thus, our system does not 
overfit the variety of Mandarin most 
familiar to the system's designers. Our 
final system achieved a F-score of 
0.947 (AS), 0.943 (HK), 0.950 (PK) 
and 0.964 (MSR). 
1 Introduction 
The 2005 Sighan Bakeoff included four dif-
ferent corpora, Academia Sinica (AS), City 
University of Hong Kong (HK), Peking Univer-
sity (PK), and Microsoft Research Asia (MSR), 
each of which has its own definition of a word. 
In the 2003 Sighan Bakeoff (Sproat & Emer-
son 2003), no single model performed well on 
all corpora included in the task. Rather, systems 
tended to do well on corpora largely drawn from 
a set of similar Mandarin varieties to the one 
they were originally developed for. Across cor-
pora, variation is seen in both the lexicons and 
also in the word segmentation standards. We 
concluded that, for future systems, generaliza-
tion across such different Mandarin varieties is 
crucial. To this end, we proposed a new model 
using character identity, morphological and 
character reduplication features in a conditional 
random field modeling framework. 
2 Algorithm
Our system builds on research into condi-
tional random field (CRF), a statistical sequence 
modeling framework first introduced by Lafferty 
et al (2001). Work by Peng et al (2004) first 
used this framework for Chinese word segmen-
tation by treating it as a binary decision task, 
such that each character is labeled either as the 
beginning of a word or the continuation of one. 
Gaussian priors were used to prevent overfitting 
and a quasi-Newton method was used for pa-
rameter optimization.  
The probability assigned to a label sequence 
for a particular sequence of characters by a CRF 
is given by the equation below: 
( ) ( )??
?
??
?
= ??
?Cc k
c cXYkkXZ
XYP f ,,exp)(
1| ??
Y is the label sequence for the sentence, X is 
the sequence of unsegmented characters, Z(X) is 
a normalization term, fk is a feature function, and 
c indexes into characters in the sequence being 
labeled.
A CRF allows us to utilize a large number of 
n-gram features and different state sequence 
168
based features and also provides an intuitive 
framework for the use of morphological features.  
3 Feature engineering 
3.1 Features
The linguistic features used in our model fall 
into three categories: character identity n-grams,
morphological and character reduplication fea-
tures.
For each state, the character identity features 
(Ng & Low 2004, Xue & Shen 2003, Goh et al 
2003) are represented using feature functions 
that key off of the identity of the character in the 
current, proceeding and subsequent positions. 
Specifically, we used four types of unigram fea-
ture functions, designated as C0 (current charac-
ter), C1 (next character), C-1 (previous character), 
C-2 (the character two characters back). Fur-
thermore, four types of bi-gram features were 
used, and are notationally designated here as 
conjunctions of the previously specified unigram 
features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0.
Given that unknown words are normally 
more than one character long, when representing 
the morphological features as feature functions, 
such feature functions keyed off the morpho-
logical information extracted from both the pro-
ceeding state and the current state. Our morpho-
logical features are based upon the intuition re-
garding unknown word features given in Gao et 
al. (2004). Specifically, their idea was to use 
productive affixes and characters that only oc-
curred independently to predict boundaries of 
unknown words. To construct a table containing 
affixes of unknown words, rather than using 
threshold-filtered affix tables in a separate un-
known word model as was done in Gao et al 
(2004), we first extracted rare words from a cor-
pus and then collected the first and last charac-
ters to construct the prefix and suffix tables. For 
the table of individual character words, we col-
lected an individual character word table for 
each corpus of the characters that always oc-
curred alone as a separate word in the given cor-
pus. We also collected a list of bi-grams from 
each training corpus to distinguish known 
strings from unknown. Adopting all the features 
together in a model and using the automatically 
generated morphological tables prevented our 
system from manually overfitting the Mandarin 
varieties we are most familiar with.  
The tables are used in the following ways: 
1) C-1+C0 unknown word feature functions 
were created for each specific pair of characters 
in the bi-gram tables. Such feature functions are 
active if the characters in the respective states 
match the corresponding feature function?s 
characters. These feature functions are designed 
to distinguish known strings from unknown.  
2) C-1, C0, and C1 individual character feature 
functions were created for each character in the 
individual character word table, and are likewise 
active if the respective character matches the 
feature function?s character. 
3) C-1 prefix feature functions are defined 
over characters in the prefix table, and fire if the 
character in the proceeding state matches the 
feature function?s character. 
4) C0 suffix feature functions are defined 
over suffix table characters, and fire if the char-
acter in the current state matches the feature 
function?s character. 
Additionally, we also use reduplication fea-
ture functions that are active based on the repeti-
tion of a given character. We used two such fea-
ture functions, one that fires if the previous and 
the current character, C-1 and C0, are identical 
and one that does so if the subsequent and the 
previous characters, C-1 and C1, are identical.  
Most features appeared in the first-order tem-
plates with a few of character identity features in 
the both zero-order and first-order templates. 
We also did normalization of punctuations due 
to the fact that Mandarin has a huge variety of 
punctuations.  
Table 1 shows the number of data features 
and lambda weights in each corpus.  
Table 1 The number of features in each corpus 
# of data features # of lambda weights 
AS 2,558,840 8,076,916
HK 2,308,067 7,481,164
PK 1,659,654 5,377,146
MSR 3,634,585 12,468,890
3.2 Experiments 
3.2.1 Results on Sighan bakeoff 2003 
Experiments done while developing this sys-
tem showed that its performance was signifi-
cantly better than that of Peng et al (2004).  
As seen in Table 2, our system?s F-score was 
0.863 on CTB (Chinese Treebank from Univer-
169
sity of Pennsylvania) versus 0.849 F on Peng et 
al. (2004). We do not at present have a good 
understanding of which aspects of our system 
give it superior performance. 
Table 2 Comparisons of Peng et al (2004) and our F-
score on the closed track in Sighan bakeoff 2003 
Sighan  
Bakeoff 2003 
Our F-score F-score 
Peng et al (2004) 
CTB 0.863 0.849 
AS 0.970 0.956 
HK 0.947 0.928 
PK 0.953 0.941 
3.2.2 Results on Sighan bakeoff 2005 
Our final system achieved a F-score of 0.947 
(AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). 
This shows that our system successfully general-
ized and achieved state of the art performance 
on all four corpora. 
Table 3 Performance of the features cumulatively, 
starting with the n-gram.  
F-score AS HK PK MSR
n-gram 0.943 0.946 0.950 0.961
n-gram (PU fixed)  0.953   
+Unk&redupl 0.947 0.943 0.950 0.964
+Unk&redupl 
(PU fixed) 
 0.952   
Table 3 lists our results on the four corpora. 
We give our results using just character identity 
based features; character identity features plus 
unknown words and reduplication features. Our 
unknown word features only helped on AS and 
MSR. Both of these corpora have words that 
have more characters than HK and PK. This in-
dicates that our unknown word features were 
more useful for corpora with segmentation stan-
dards that tend to result in longer words. 
In the HK corpus, when we added in un-
known word features, our performance dropped. 
However, we found that the testing data uses 
different punctuation than the training set. Our 
system could not distinguish new word charac-
ters from new punctuation, since having a com-
plete punctuation list is considered external 
knowledge for closed track systems. If the new 
punctuation were not unknown to us, our per-
formance on HK data would have gone up to 
0.952 F and the unknown word features would 
have not hurt the system too much. 
Table 4 present recalls (R), precisions (P), f-
scores (F) and recalls on both unknown (Roov)
and known words (Riv).
Table 4 Detailed performances of each corpus 
R P F Roov Riv
AS 0.950 0.943 0.947? 0.718? 0.960
HK 0.941 0.946 0.943? 0.698? 0.961
HK
(PU-fix)
0.952 0.952 0.952 0.791 0.965
PK 0.946 0.954 0.950? 0.787? 0.956
MSR 0.962 0.966 0.964? 0.717? 0.968
3.3 Error analysis 
Our system performed reasonably well on 
morphologically complex new words, such as 
??? (CABLE in AS) and ??? (MUR-
DER CASE in PK), where ? (LINE) and ?
(CASE) are suffixes. However, it over-
generalized to words with frequent suffixes such 
as ?? (it should be ? ? ?to burn some-
one? in PK) and ?? (it should be? ? ?
?to look backward? in PK). For the corpora that 
considered 4 character idioms as a word, our 
system combined most of new idioms together. 
This differs greatly from the results that one 
would likely obtain with a more traditional 
MaxMatch based technique, as such an algo-
rithm would segment novel idioms. 
One short coming of our system is that it is 
not robust enough to distinguish the difference 
between ordinal numbers and numbers with 
measure nouns. For example, ?? (3rd year) 
and ?? (three years) are not distinguishable 
to our system. In order to avoid this problem, it 
might require having more syntactic knowledge 
than was implicitly given in the training data.  
Finally, some errors are due to inconsisten-
cies in the gold segmentation of non-hanzi char-
acter. For example, ?Pentium4? is a word, but 
?PC133? is two words. Sometimes, ?8? is a 
word, but sometimes it is segmented into two 
words.
170
4 Conclusion
Our system used a conditional random field 
sequence model in conjunction with character 
identity features, morphological features and 
character reduplication features. We extracted 
our morphological information automatically to 
prevent overfitting Mandarin from particular 
Mandarin-speaking area. Our final system 
achieved a F-score of 0.947 (AS), 0.943 (HK), 
0.950 (PK) and 0.964 (MSR).  
5 Acknowledgment 
Thanks to Kristina Toutanova for her gener-
ous help and to Jenny Rose Finkel who devel-
oped such a great conditional random field 
package. This work was funded by the Ad-
vanced Research and Development Activity's 
Advanced Question Answering for Intelligence 
Program, National Science Foundation award 
IIS-0325646 and a Stanford Graduate Fellow-
ship.
References
Lafferty, John, A. McCallum, and F. Pereira. 2001. 
Conditional Random Field: Probabilistic Models 
for Segmenting and Labeling Sequence Data. In 
ICML 18. 
Gao, Jianfeng Andi Wu, Mu Li, Chang-Ning Huang, 
Hongqiao Li, Xinsong Xia and Haowei Qin. 2004. 
Adaptive Chinese word segmentation. In ACL-
2004.
Goh, Chooi-Ling, Masayuki Asahara, Yuji Matsu-
moto. 2003. Chinese unknown word identification 
using character-based tagging and chunking. In 
ACL 2003 Interactive Poster/Demo Sessions. 
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based? In EMNLP 9.
Peng, Fuchun, Fangfang Feng and Andrew 
McCallum. 2004. Chinese segmentation and new 
word detection using conditional random fields. In 
COLING 2004.
Sproat, Richard and Tom Emerson. 2003. The first 
international Chinese word segmentation bakeoff. 
In SIGHAN 2. 
Xue, Nianwen and Libin Shen. 2003. Chinese Word 
Segmentation as LMR Tagging. In SIGHAN 2.
171
Semantic classification of Chinese unknown words 
Huihsin Tseng 
 Linguistics 
University of Colorado  
at Boulder 
tseng@colorado.edu 
 
 
Abstract 
This paper describes a classifier that assigns se-
mantic thesaurus categories to unknown Chinese 
words (words not already in the CiLin thesaurus 
and the Chinese Electronic Dictionary, but in the 
Sinica Corpus). The focus of the paper differs in 
two ways from previous research in this particular 
area.  
Prior research in Chinese unknown words mostly 
focused on proper nouns (Lee 1993, Lee, Lee and 
Chen 1994, Huang, Hong and Chen 1994, Chen 
and Chen 2000).  This paper does not address 
proper nouns, focusing rather on common nouns, 
adjectives, and verbs. My analysis of the Sinica 
Corpus shows that contrary to expectation, most of 
unknown words in Chinese are common nouns, 
adjectives, and verbs rather than proper nouns. 
Other previous research has focused on features 
related to unknown word contexts (Caraballo 1999; 
Roark and Charniak 1998). While context is 
clearly an important feature, this paper focuses on 
non-contextual features, which may play a key role 
for unknown words that occur only once and hence 
have limited context. The feature I focus on, fol-
lowing Ciaramita (2002), is morphological similar-
ity to words whose semantic category is known. 
My nearest neighbor approach to lexical acquisi-
tion computes the distance between an unknown 
word and examples from the CiLin thesaurus based 
upon its morphological structure. The classifier 
improves on baseline semantic categorization per-
formance for adjectives and verbs, but not for 
nouns. 
1 Introduction 
The biggest problem for assigning semantic cate-
gories to words lies in the incompleteness of dic-
tionaries. It is impractical to construct a dictionary 
that will contain all words that may occur in some 
previously unseen corpora. This issue is particu-
larly problematic for natural language processing 
applications that work with Chinese texts. Specifi-
cally, for the Sinica Corpus1, Bai, Chen and Chen 
(1998) found that articles contain on average 
3.51% words that were not listed in the Chinese 
Electronic Dictionary2 of 80,000 words. Because 
novel words are created daily, it is impossible to 
collect them all. Furthermore, across most of the 
corpora, many of these newly coined words seem 
to be used only once, and thus they may not even 
be worth collecting. However, the occurrence of 
unknown words makes a number of NLP (Natural 
Language Processing) tasks such as segmentation 
and word sense disambiguation more difficult. 
Consequently, it would be valuable to have some 
means of automatically assigning meaning to un-
known words. This paper describes a classifier that 
assigns semantic thesaurus categories to unknown 
Chinese words.  
The Caraballo (1999)?s system adopted the contex-
tual information to assign nouns to their hyponyms. 
Roark and Charniak (1998) used the co-occurrence 
of words as features to classify nouns. While con-
text is clearly an important feature, this paper fo-
cuses on non-contextual features, which may play 
a key role for unknown words that occur only once 
                                                          
1 The Sinica Corpus is a balanced corpus contained five 
million part-of-speech words in Mandarin Chinese.  
2The Chinese Electronic Dictionary is from the 
Computational Linguistics Society of R.O.C. 
and hence have limited context. The feature I focus 
on, following Ciaramita (2002), is morphological 
similarity to words whose semantic category is 
known. Ciaramita (2002) boosted the lexical ac-
quisition system by simple morphological rules 
and found a significant improvement. Such a find-
ing suggests that a reliable source of semantic in-
formation lies in the morphology used to construct 
the unknown words.  
In Chinese morphology, the two ways to generate 
new words are compounding and affixation. 
Orthographically, such compounding and affixa-
tion is represented by combinations of characters, 
and as a result, the character combinations and the 
morpho-syntactic relationship used to link them 
together can be clues for classification. Further-
more, my analysis of the Sinica Corpus indicates 
that only 49.68% monosyllabic3 words have one 
word class, but 91.67% multisyallabic words have 
one word class in Table 1. Once characters merge 
together, only 8.33% words remain ambiguous. It 
implies that as characters are combined together, 
the degree of ambiguity tends to decrease. 
 
 
Word Class4 Monosyllabic Multisyllabic
1 49.68% 91.67% 
2 21.94% 7.30% 
3 10.94% 0.82% 
4 6.55% 0.15% 
more than 4 10.89% 0.06% 
Table 1 The ambiguity distribution of monosyllabic and 
multisyllabic words 
 
The remainder of this paper is organized in the fol-
lowing manner: section 2 introduces the CiLin the-
saurus, section 3 provides an analysis of unknown 
words in the Sinica Corpus, and section 4 details 
the algorithm used for the semantic classification 
and explains the results. 
 
                                                          
3 ?Monosyllabic word? means a word with only a char-
acter, and ?multisynllabic word? means a word with 
more than one character. 
4 ?Word Class? means the number of each word?s word 
class. 
2 The CiLin thesaurus 
The CiLin (Mei et al1986) is a thesaurus that con-
tains 12 main categories: A-human, B-object, C-
time and space, D-abstract, E-attribute, F-action, 
G-mental action, H-activity, I-state, J-association, 
K-auxiliary, and L-respect. The majority of words 
in the A-D categories are nouns, while the majority 
in the F-J categories are verbs. As shown in Figure 
1, the main categories are further subdivided into 
more specific subcategories in a three-tier hierar-
chy.  
B Object
0.1636 
Bn Building
0.0174 
Bm Material
0.0128 
Bl Excrement
0.0036 
Bk The whole 
body
0.0135 
Bj 
Microorganism
0.0013 
Bi Animal
0.0179 
Bh Plant
0.0064 
Bh07 Fruit
0.0003 
Bh06 
Vegetable
0.0003 
Bh01 Tree
0.0005 
Fanqie 
(tomato)
Hamigua 
(hami melon)
Word level
Concept level1
Concept level 2
Concept level 3
 
Figure 1 The taxonomy of the CiLin with the probabil-
ity (partial) 
 
 
 
3 Corpus analysis of Chinese unknown 
words 
3.1 Definition of unknown words 
Unknown words are the Sinica Corpus lexicons 
that are not listed in the Chinese Electronic Dic-
tionary of 80,000 lexicons and the CiLin. The 5 
million word Sinica Corpus contains 77,866 un-
known words consisting of 1.59% adjectives, 
33.73% common nouns, 25.18% proper nouns, 
12.48% location nouns, 2.98% time nouns, and 
24.04% verbs as shown in Table 2.  
The focus of most other Chinese unknown word 
research is on identification of proper nouns such 
as proper names (Lee 1993), personal names (Lee, 
Lee and Chen 1994), abbreviation (Huang, Hong 
and Chen 1994), and organization names (Chen & 
Chen 2000). Unknown words in categories outside 
the class of proper nouns are seldom mentioned. 
One of the few examples of multiple class word 
prediction is Chen, Bai and Chen?s 1997 work em-
ploying statistical methods based on the prefix-
category and suffix-category associations to pre-
dict the syntactic function of unknown words. Al-
though proper nouns may contain lots of useful and 
valuable information in a sentence, the majority of 
unknown words in Chinese are lexical words, and 
consequently, it is also important to classify lexical 
words. If not, the remaining 70% of unknown 
words5 will be an obstacle to Chinese NLP, where 
24.04% of verbs are unknown can be a major prob-
lem for parsers. 
 
Class Unknown words Corpus lexicons6
Adjective 1.59% 1.49% 
Common noun 33.73% 37.12% 
Proper noun7 25.18% 16.53% 
Location noun8 12.48% 10.38% 
Time noun9 2.98% 2.36% 
Verb 24.04% 32.11% 
Table 2 The distribution of unknown words and all lexi-
cons of the Sinica Corpus in 6 classes 
 
3.2 Types of unknown words 
In Chinese morphology, the two ways to generate 
new words are compounding and affixation.  
Compounds 
A compound is a word made up of other words. In 
general, Chinese compounds are made up of words 
                                                          
5 Part of location noun still contains some proper nouns 
like country names. 
6 It contains both known and unknown words. 
7 Proper noun contains two classes: 1) formal name, 
such as personal names, races, titles of magazines and 
so on. 2) Family name, such as Chen and Lee.  
8 Location noun contains 4 subclasses: 1) country names, 
such as China. 2) common location noun, such as??
/youju ?post office? and??/xuexiao ?school?. 3) noun 
+ position, such as??/haiwei ?oversea?. 4) direction 
noun, such as?/shang ?up? and?/xia ?down?.  
9 Time noun contains 3 classes: 1) historical event and 
recursive time noun, such as?/Qing dynasty and??
/yiyue ?January?. 2) noun + position, such as??
/wanjian ?in the evening?, 3) adverbial time noun, such 
as??/jianglai ?in the future?. 
that are linked together by morpho-syntactic rela-
tions such as modifier-head, verb-object, and so on 
(Chao 1968, Li and Thompson 1981). For example, 
???/guanghuanjue LIGHT-ILLUSION ?optical 
illusion?, consists of ?/guang ?light? and  ??
/huanjue ?illusion?, and the relation is modifier-
head. ???/ guangguomin LIGHT-ALLERGY 
?photosensitization? is made up of?/ guang ?light? 
and ?? / guomin ?allergy?, and the relation is 
modifier-head. 
Affixation 
A word is formed by affixation when a stem is 
combined with a prefix or a suffix morpheme. For 
example English suffixes such as -ian and -ist are 
used to create words referring to a person with a 
specialty, such as `musician' and `scientist'. Such 
suffixes can give very specific evidence for the 
semantic class of the word.  Chinese has suffixes 
with similar meanings to -ian or -ist, such as the 
Chinese suffix -jia. But the Chinese affix is a much 
weaker cue to the semantic category of the word 
than English -ist or -ian, because it is more am-
biguous. The suffix ?jia contains three major con-
cepts: 1) expert, such as ? ? ? /kexuejia 
SCIENCE-EXPERT ?scientist? and ? ? ? / 
yinyuejia MUSIC-EXPERT ?musician?, 2) family 
and home, such as ? ? /quanjia WHOLE-
FAMILY ?whole family? and ??? /fuguijia 
RICH-FAMILY ?rich family?, 3) house, such as ?
? /banjia MOVE-HOUSE ?to move house?. In 
English, the meaning of an unknown word with the 
suffix ?ian or ?ist is clear, but in Chinese an un-
known word with the suffix ?jia could have multi-
ple interpretations. Another example of ambiguous 
suffix, ?xing, has three main concepts: 1) gender, 
such as ??/nuxing FEMALE-SEX ?female?, 2) 
property, such as ? ? /yaoxing MEDICINE-
PROPERTY ?property of a medicine?, 3) a charac-
teristic, ???? /shishachengxing LIKE-KILL-
AS-HABIT ?a characteristic of being bloodthirsty?. 
Even though Chinese also has morphological suf-
fixes to generate unknown words, they do not de-
termine meaning and syntactic category as clearly 
as they do in English.  
 
 
 
4 Semantic classification 
For the task of classifying unknown words, two 
algorithms are evaluated. The first algorithm uses a 
simple heuristic where the semantic category of an 
unknown word is determined by the head of the 
unknown word. The second algorithm adopts a 
more sophisticated nearest neighbor approach such 
that the distance between an unknown word and 
examples from the CiLin thesaurus computed 
based upon its morphological structure. The first 
algorithm serves to provide a baseline against 
which the performance of the second can be evalu-
ated.  
 
4.1 Baseline 
The baseline method is to assign the semantic 
category of the morphological head to each word. 
4.2 An example-base semantic classification  
The algorithm for the nearest neighbor classifier is 
as follows: 
1) An unknown word is parsed by a morphological 
analyzer (Tseng and Chen 2002). The analyzer a) 
segments a word into a sequence of morphemes, b) 
tags the syntactic categories of morphemes, and c) 
predicts morpho-syntactic relationships between 
morphemes, such as modifier-head, verb-object 
and resultative verbs as shown as in Table 3. For 
example, if ??? /wudaojia DANCE-EXPERT 
?dancer? is an unknown word, the morphological 
segmentation is ??/wudao DANCE ?dance? and 
?/jia EXPERT ?expert?, and the relation is modi-
fier-head. 
2) The CiLin thesaurus is then searched for entries 
(examples) that are similar to the unknown word. 
A list of words sharing at least one morpheme with 
the unknown word, in the same position, is con-
structed. In the case of ???/wudaojia, such a 
list would include ? ? ? /gechangjia SING-
EXPERT ?singer?, ?? /huijia GO-HOME ?go 
home?, ???/fuguijia RICH-FAMILY ?rich fam-
ily? and so on.  
 
Word 
Class 
The morpho-syntactic relations 
Noun Modifier-head10 
??/lanqie  
BASKET-BALL `baseketball? 
Verb 1) Verb-object :  
??/chifan  
EAT-RICE ?to eat` 
2) Modifier-head:  
??/qinglie CLEAR-LIST ?clearly list? 
3) Resultative Verb 
??/chibao EAT-FULL ?to have eaten? 
4) Head-suffix: 
??/biancheng CHANG-TO ?become? 
5) Modifier-head (suffix): 
???/zidonghua  
AUTOMATIC-BECOME ?automatize? 
6) Directional resultative compound and 
reduplication 
???/paoshanglai  
RUN-UP-TO ?run up to? 
Adjective An: modifier-head 
???/zhongguoshi 
CHINESE-STYLE ?Chinese stylish? 
Av: verb-object and modifier-head 
??/yumin 
FOOL-PEOPLE ?keeping the people unin-
formed? 
Table 3 The morpho-syntactic relations 
 
3) The examples that do not have the same mor-
pho-syntactic relationships but shared morpheme 
belongs to the unknown word?s modifier are 
pruned away. If no examples are found, the system 
falls back to the baseline classification method. 
4) The semantic similarity metric used to compute 
the distance between the unknown word and the 
selected examples from the CiLin thesaurus is 
based upon a method first proposed by Chen and 
Chen (1997).  
They assume that similarity of two semantic cate-
gories is the information content of their parent?s 
                                                          
10There are still a very small number of coordinate rela-
tion compounds that is both of the morphemes in a 
compound are heads. Since either one of the morphemes 
can be the meaning of the whole compound, in order to 
simplify the system, words that have coordinate rela-
tions are categorized as modifier head relation. 
node.  For instance, the similarity of ???
/hamigua ?hami melon? (Bh07) and ?? /fanqie 
?tomato? (Bh06) is based on the information con-
tent of the node of their least common ancestor Bh.  
The CiLin thesaurus can be used as an information 
system, and the information content of each se-
mantic category is defined as  
category) manticEntropy(Sestem)Entropy(Sy ?  
The similarity of two words is the least common 
ancestor information content(IC), and hence, the 
higher the information content is, the more similar 
two the words are. The information content is 
normalized by Entropy(system) in order to keep 
the similarity between 0 and 1. To simplify the 
computation, the probabilities of all leaf nodes are 
assumed equal. For example, the probability of Bh 
is .0064 and the information content of Bh is ?
log(.0064). Hence, the similarity between???/ 
hamigua and ??/ fanqie is .61. 
( ) ( )( )
( )( )
( ) )1(             SystemEntropy 
Plog
SystemEntropy 
 IC Sim 2122121
WWWWWW III ?==  
 
fanqie) ofcategory  (the Bh06
  hamihua), ofcategory  (the Bh07
 CiLin,SystemLet  
2
1
=
=
=
W
W
 
( ) ( )( )
( )( )
( )
0.61
11.94
7.29
0.0026log-
0.0064log-
CiLinEntropy 
BhPlog
CiLinEntropy 
Bh06Bh07 ICBh06Bh07 Sim
2
2
2
===
?== II  
Resnik (1995, 1998 and 2000) and Lin (1998) also 
proposed information content algorithms for simi-
larity measurement. The Chen and Chen (1997) 
algorithm is a simplification of the Resnik algo-
rithm, which makes the simplifying assumption 
that the occurrence probability of each leaf node is 
equal. 
One problem for this algorithm is the insufficient 
coverage of the CiLin (CiLin may not cover all 
morphemes). The backup method is to run the clas-
sifier recursively to predict the possible categories 
of the unlisted morphemes. If a morpheme of an 
unknown word or of an unknown word?s example 
is not listed in the CiLin, the similarity measure-
ment will suspend measuring the similarity be-
tween the unknown word and the examples and run 
the classifier to predict he semantic category of the 
morpheme first. After the category of the mor-
pheme is known, the classifier will continue to 
measure the similarity between the unknown word 
and its examples. The probability of adopting this 
backup method in my experiment is on the average 
of 3%.  
Here is an example of the recursive semantic 
measurement. ??? /paomatou RUN-WHARF 
?wharf-worker? is an example of an unknown word
???/paohanchuan RUN-DRY BOAT ?folk ac-
tivities?. The morphological analyzer breaks the 
two words into ? ??/pao matou and ? ??
/pao hanchuan. The measurement function will 
compute the similarity between??/matou and?
?/hanchuan, but in this case, ??/hanchuan is 
not listed in the CiLin. The next approach is then 
to run the semantic classifier to guess the possible 
category of??/hanchuan. Based on the predicted 
category, it then goes on to compute the similarity 
for ??/matuo and ??/hanchuan. By applying 
this method, there will not be any words without a 
similarity measurement. 
5) After the distances from the unknown word to 
each of the selected examples from the CiLin the-
saurus are determined, the average distance to the 
K nearest neighbors from each semantic category 
is computed. The category with the lowest distance 
is assigned to the unknown word.  
The similarity of ??/wudao and ??/gechang 
is .87, of ??/wudao and ?/hui is .26, and of ?
? /wudao and ?? /fugui is 0. Thus, ???
/wudaojia is more similar to ??? /gechangjia 
than??/huijia or???/fuguijia. The category of 
???/wudaojia is thus most likely to be ???
/gechangjia.  
The semantic category is predicted as the category 
that gets the highest score in formula (2). The lexi-
cal similarity and frequency of examples of each 
category are considered as the most important fea-
tures to decide a category.  
In formula (2), RankScore(Ci) includes SS(Ci) and 
FS(Ci). The score of SS(Ci) is a lexical similarity 
score, which is from the maximum score of Simi-
larity (W1,W2) in the category of W2. FS(Ci) is a 
frequency score to show how many examples there 
are in a category. ? and (1-?) are respectively 
weights for the lexical similarity score and the fre-
quency score. 
 
)Taxonomy  nA...L(CiLi
CiLin   thein  definedcategory  semantic  whoseword
 wordunknownLet  1
=
=
=
i
W
W
i
 
 ( ) ( ) ( ) ( )
( )
( )
( )
( ) ( )
( )
(4)                                                       
Freq
FreqFS
(3)                                           ,SimmaxargSS
2)(                                FS?1SS?Rankscore
L
Ai
1
A...Li
C
?
=
=
?
=
=
??+?=
i
i
i
i
CW
i
iii
C
CC
WWC
CCC
ii
 
 
5 Experiment 
5.1 Data 
There are 56,830 words in the CiLin. For experi-
ments, CiLin lexicons are divided into 2 sets: a 
training set of 80% CiLin words, a development 
set of 10% of CiLin words, and a test set of 10% 
CiLin words. All words in the test set are assumed 
to be unknown, which means the semantic catego-
ries in both sets are unknown. Nevertheless, the 
morphological structures of proper nouns are dif-
ferent from lexical words. Their identification 
methods are also different and will be out of the 
scope of this paper. The correct category of the 
unknown word is the semantic category in the 
CiLin, and if an unknown word is ambiguous, 
which means it contains more than one category, 
the system then chooses only one possible category. 
In evaluation, any one of the categories of an am-
biguous word is considered correct. 
5.2 Result 
On the test set, the baseline predicts 53.50% of 
adjectives, 70.84% of nouns and 47.19% of verbs 
correctly. The classifier reaches 64.20% in adjec-
tives, 71.77% in nouns and 53.47% in verbs, when 
? is 0.5 and K is five. 
Word class
Baseline
accuracy
Semantic classification 
accuracy 
Adjective 53.50% 64.20% 
Noun 70.84% 71.77% 
Verb 47.19% 53.47% 
Table 4 The accuracy of the baseline and semantic clas-
sification in the development set 
 
Word class
Baseline
accuracy
Semantic classification 
accuracy 
Adjective 52.92% 65.76% 
Noun 70.89% 71.39% 
Verb 44.10% 52.84% 
Table 5 The accuracy of the baseline and semantic clas-
sification in the test set 
 
Table 4 and table 5 show a comparison of the base-
line and the classifier. Generally, nouns are easier 
to predict than the other categories, because their 
morpho-syntactic relation is not as complex as 
verbs and adjectives. The classifier improves on 
baseline semantic categorization performance for 
adjectives and verbs, but not for nouns. The lack of 
a performance increase for nouns is most likely 
because nouns only have one kind of morpho-
syntactic relation. The advantage of the classifier is 
to filter out examples in different relations and to 
find out the most similar example in morphemes 
and morpho-syntactic relation. The classifier pre-
dicts better than the baseline in word classes with 
multiple relations, such as adjectives and verbs. 
For example, ??? /kaikuaiche OPEN-FAST 
CAR ?drive fast? is a verb-object verb. The base-
line wrongly predicted it due to the verb, ?/kai 
OPEN ?open?. However, the semantic classifier 
grouped it to the category of its similar example, 
???/kaiyeche OPEN-NIGHT CAR ?drive dur-
ing the night?. 
 
5.3 Error analysis 
Error sources can be grouped into two types: data 
errors and the classifier errors. The testing data is 
from the CiLin. Some of testing data are not se-
mantically transparent such as idioms, metaphors, 
and slang. The meaning of such words is different 
from the literal meaning. For instance, the literal 
meaning of ???/kanmengou WATCH-DOOR-
DOG is a door-watching dog, and in fact it refers 
to a person with the belittling meaning. ???
/mulaohu FEMALE-TIGER is a female tiger liter-
ally, and it refers to a mean woman. These words 
do not carry the meaning of their head anymore. 
An unknown word will be created such as ???
/kanmenmao WATCH-DOOR-CAT ?a door-
watching cat?, but it is impossible for unknown 
words to carry similar meaning of words as???
/kanmengou. 
The classifier errors are due primarily to three fac-
tors: a lack of examples, the preciseness of the 
similarity measurement, and the taxonomy of the 
CiLin.  
First, some errors occur when there are not enough 
examples in training data. For example, ???
/tielangan IRON-POLE ?iron pole` does not have 
any similar examples after the classifier filters out 
examples whose relations are different and whose 
shared morphemes are not head. ???/tielangan 
is segmented as ? /tie IRON ?iron? and ??
/langan POLE ?pole?. There are examples of the 
first morpheme, ?/tie, but no similar examples of 
the second,?? /langan. Since ??? /tielangan 
has modifier-head relation and ??/langan is the 
head of the compound, then the classifier filters out 
the examples of?/tie. There are hence not enough 
examples. Filtering examples in different structures 
is performed to make the remaining examples 
more similar since the similarity measurement may 
not be able to distinguish slight differences. How-
ever, the cost of this filtering of different structure 
examples is that sometimes this leaves no exam-
ples. 
Second, the similarity measurement is sometimes 
not powerful enough. ? ? ? /yundongchang 
SPORT-SPACE ?a sports ground` has a sufficient 
number of examples, but has problems with the 
similarity measurement. The head ?/chang is am-
biguous. ?/chang has two senses and both mean 
space. One of them means abstract space and the 
other means physical space. Hence, in the CiLin 
thesaurus ?/chang can be found in C (time and 
space) and D (abstract). Words in C such as ??
/shangchang BUSINESS-SPACE ?a market?, ??
? /tuzaichang BUTCHER-SPACE ?a slaughter 
house? , ?? /huichang MEETING-SPACE ?the 
place of a meeting?, and in D are ??/ qiuchang 
BALL-SPACE ?a court?, ? ? ? /tiyuchang 
PHYSICAL TRAINING-SPACE ?a stadium?. ??
?/yundongchang should be more similar to ??
?/tiyuchang than other space nouns, but the simi-
larity score does not show that they are related and 
C group has more examples. Thus, the system 
chooses C incorrectly. 
Third, the taxonomy of the thesaurus is ambiguous. 
For instance, ???/tichaofang GYMNASTICS?
ROOM ?gymnastics room? has similar examples in 
both B (object) and D (abstract). These two groups 
are very similar. Words in B group include ??
/xingfan PUNISHMENT-ROOM ?punishment 
room?, ??/shufan BOOK-ROOM ?study room?, 
??/anfan DARK-ROOM ?dark room?, and ??
/chufan KITCHEN-ROOM ?kitchen?. Words in D 
are such as ??/laofan PRISON-ROOM ?a jail? 
and ??? /danzifan BILLIARD-ROOM ?a bil-
liard room?. There are no obvious features to dis-
tinguish between these examples. According to the 
CiLin, ??? /tichaofang belongs to D, but the 
classifier predicts it as B class which does not ac-
tually differ much with D. Such problems may oc-
cur with any semantic taxonomy. 
 
6 Conclusion 
The paper presents an algorithm for classifying the 
unknown words semantically. The classifier adopts 
a nearest neighbor approach such that the distance 
between an unknown word and examples from the 
CiLin thesaurus is computed based upon its mor-
phological structure. The main contributions of the 
system are: first, it is the first attempt in adding 
semantic knowledge to Chinese unknown words. 
Since over 70% of unknown words are lexical 
words, the inability to resolve their meaning is a 
major obstacle to Chinese NLP such as semantic 
parsers. Second, without contextual information, 
the system can still successfully classify 65.76% of 
adjectives, 71.39% of nouns and 52.84% of verbs. 
Future work will explore the use of the contextual 
information of the unknown words and the contex-
tual information of the lexicons in the predicted 
category of the unknown words to boost predictive 
power.  
Acknowledgment  
Thanks to S. Bethard, D. Cer, K. J. Chen, D. Juraf-
sky and to the anonymous reviewers for many 
helpful suggestions. This research was partially 
supported by the NSF via a KDD extension to NSF 
IIS-9978025 (Dan Jurafsky, PI) and by the CKIP 
group, Institute of Information Science, Academia 
Sinica.  
References 
Bai, M. H., C.J. Chen, and K. J. Chen. 1998. ?????
???????? ? <????????????
???>??????????????????
? 47-60? 
Caraballo, S. 1999. Automatic acquisition of a 
hypemymlabeled noun hierarchy from text, in 
Proceedings of the 37th ACL. 
Ciaramita. M. 2002. Boosting automatic lexical acquisi-
tion with morphological information", in Proceedings 
of the Workshop on Unsupervised Lexical Acquisi-
tion, ACL-02. 
Chao, Y. R. 1968. A grammar of spoken Chinese. 
Berkeley:University of California Press. 
Chen, C. J., M. H. Bai and K. J. Chen. 1997. Category 
Guessing for Chinese Unknown Words, in Proceed-
ings of the Natural Language Processing Pacific Rim 
Symposium, 35-40. 
Lee, J. C. 1993. ????? ????????????
?????????????????????
?? 
Lee, J. C., Y. H. Lee and H. H. Chen. 1994. ?????
??????????????????????
???<??????????????>??
203-222? 
Chen. K. J. and C. J Chen. 1997. ??????????
<??????????????????>??
??????????????????????
????????? 283-305???????
?? 
Chen, K. J. and M. H. Bai. 1998. Unknown Word 
Detection for Chinese by a Corpus-based Learning 
Method, in Computational Linguistics and Chinese 
Language Processing vol3 no. 1, 27-44. 
Chen, C. J. and K. J. Chen. 2002. Knowledge Extraction 
for Identification of Chinese Organization Names, in 
Proceedings of the second Chinese Language Proc-
essing Workshop, 15-21. 
Huang, C. R., W. M. Hong and K. J. Chen.  1994.  An 
Introduction Based Lexical of Abbreviation, in Pro-
ceedings of the 2th Pacific Asia Conference on For-
mal and Computational Linguistics, 49-52. 
Huang, C. R. and K. J. Chen. 1995. ????  ??
?? ????????????????????
?? 
Li, C. and S. A. Thompson. 1981. Mandarin Chinese. 
Berkeley: University of California Press. 
Lin, D.. 1998. An information-theoretic definition of 
similarity, in Proceedings 15th International Conf. on 
Machine Learning, p 296?304. 
Lin, D. and P. Pantel.. 2001. Induction of Semantic 
Classes from Natural Language Text, In Proceedings 
of ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining 2001, 317-322.  
Mei, J., Y. Zhu., Y. Gao, and H. Ying. 1986. ?????
?????????????1986???????
???????????? 
Resnik, P.. 1995. Using Information Content to Evalu-
ate Semantic Similarity in a Taxonomy. Proceedings 
of the 14th International Joint Conference on Artifi-
cial Intelligence, pp. 448-453. 
---. 1998. Semantic Similarity in a Taxonomy: An In-
formation-Based Measure and its Application to 
Problems of Ambiguity in Natural Language, in 
Journal of Artificial Intelligence Research (11), 95-
130. 
Resnik, P. and M. Diab. 2000. Measuring Verbal Simi-
larity. Technical Report: LAMP-TR-047//UMIACS-
TR-2000-40/CS-TR-4149/MDA-9049-6C-1250. 
University of Maryland, College Park. 
Roark, B. and E. Charniak. 1998. Noun-phrase co-
occurrence statistics from semi-automatic semantic 
lexicon construction, in Proceedins of the 36th ACL. 
Tseng, H and K. J. Chen. 2002. Design of Chinese 
Morphological Analyzer. SigHan Workshop on Chi-
nese Language Processing, Taipei. 
 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 524?533,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Mining Search Engine Clickthrough Log for  
Matching N-gram Features  
 
 
Huihsin Tseng, Longbin Chen, Fan Li, Ziming Zhuang,  
Lei Duan, Belle Tseng 
Yahoo! Inc., Santa Clara, CA 95054 
{huihui,longbin,fanli,ziming,leiduan,belle}@yahoo-inc.com 
 
Abstract 
User clicks on a URL in response to a query are 
extremely useful predictors of the URL?s rele-
vance to that query. Exact match click features 
tend to suffer from severe data sparsity issues in 
web ranking. Such sparsity is particularly pro-
nounced for new URLs or long queries where 
each distinct query-url pair will rarely occur. To 
remedy this, we present a set of straightforward 
yet informative query-url n-gram features that al-
lows for generalization of limited user click data 
to large amounts of unseen query-url pairs. The 
method is motivated by techniques leveraged in 
the NLP community for dealing with unseen 
words. We find that there are interesting regulari-
ties across queries and their preferred destination 
URLs; for example, queries containing ?form? 
tend to lead to clicks on URLs containing ?pdf?. 
We evaluate our set of new query-url features on 
a web search ranking task and obtain improve-
ments that are statistically significant at a p-value 
< 0.0001 level over a strong baseline with exact 
match clickthrough features.   
1 Introduction 
Clickthrough logs record user click behaviors, 
which are a critical source for improving search 
relevance (Bilenko and White, 2008; Radlinski et 
al., 2007; Agichtein and Zheng, 2006; Lu et al 
2006). Previous work (Agichtein et al, 2006) 
demonstrated that clickthrough features (e.g., 
IsNextClicked and IsPreviousClicked) can lead 
to substantial improvements in relevance. Such 
features summarize query-specific user interac-
tions on a search engine. One commonly used 
clickthrough feature is generated based on the 
following observation: if a URL receives a large 
number of first and last clicks across many user 
sessions, then it indicates that this URL might be 
a strongly preferred destination of a query. For 
example, when a user searches for ?yahoo?, they 
tend to only click on the URL www.yahoo.com 
rather than other alternatives. This results in 
www.yahoo.com being the first and last clicked 
URL for the query. We refer to such behavior as 
being navigational clicks (NavClicks). Features 
that use exact query and URL string matches 
(e.g., NavClick, IsNextClicked and IsPrevious-
Clicked)  are referred to as exact match features 
(ExactM) for the remainder of this paper.  
 
The coverage of ExactM features is sparse, espe-
cially for long queries and new URLs. Many 
long queries are either unique or very low fre-
quency. Hence, the improvements from ExactM 
features are limited to the more popular queries. 
In addition, ExactM features tend to be weighted 
heavily in the ranking of results when they are 
available. This introduces a bias where the rank-
ing models tend to strongly favor older URLs 
over new URLs even when the latter otherwise 
appear to be more relevant.  
 
By inspecting the clickthrough logs, we observed 
that unseen query-url pairs are often composed of 
informative previously observed subsequences. 
Specifically, we saw that query n-grams can be 
correlated with sequences of URL n-grams.  For 
example, we find that there are interesting regu-
larities across queries and URLs, such as queries 
containing ?form? tending to lead to clicks on 
URLs containing ?pdf?. This strongly motivates 
the adoption of an approach similar to the Natu-
ral Language Processing (NLP) technique of us-
ing n-grams to deal with unseen words. For ex-
ample, part-of-speech tagging (Brants, 2000) and 
parsing (Klein and Manning, 2003) both require 
dealing with unknown words. By using n-gram 
substrings, novel items can be dealt with using 
any informative substrings they contain that were 
actually observed in the training data.  
 
524
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce our overall me-
thodology. Section 2.1 presents a data mining 
method for building a query-url n-gram diction-
ary, Section 2.2 describes the new ranking fea-
tures in detail. In section 3, we present our ex-
perimental results. Section 4 discusses related 
work, and Section 5 summarizes the contribution 
of this work.  
2 Methodology 
This section describes the detailed methodology 
used in generating the query-url n-gram features. 
Our features require association scores to be pre-
viously calculated, and, hence, we first introduce 
a data mining approach that is used to build an 
association dictionary in Section 2.1. Then, we 
present the procedure used to generate the query-
url n-gram features that use the dictionary in Sec-
tion 2.2. 
 
 
Figure 1: Steps to build a query-url n-gram dic-
tionary 
 
2.1 Data Mining on a Query-URL 
 N-gram Dictionary 
 
The steps involved in building the dictionary are 
shown in Figure 1. We first collect seed query-
url pairs from clickthrough data based on Nav-
Clicks. The queries and URLs from the collected 
pairs are tokenized and converted into a collec-
tion of paired query-url n-grams. For each pair, 
we calculate the mutual information of the query 
n-gram and its corresponding URL n-gram. For 
our experiment, we collect a total of more than 
15M seed pairs and 0.5B query-url n-gram pairs 
using six months of query log data. The details 
are described in the following sections.  
2.1.1 Seed List 
We identify the seed list based on characteristic 
user click behavior. Given a query, we select the 
URL with the most NavClicks as compared to 
other URLs returned. During data collection, the 
rank positions of the top 5 URLs were shuffled to 
avoid the position bias. We aggregate NavClicks 
for a URL occurring in these positions in order to 
both obtain more click data and to avoid the posi-
tion bias issue discussed in Dupret and Piwowar-
ski (2008) and Craswell et al (2008).  
 
For example, in Figure 1, the numbers of Nav-
Clicks for the top three URLs are shown. The 
URL www.irs.gov/pub/irs-pdf/f1040.pdf receives 
the largest number of NavClicks, and, therefore, 
it is used to create the query-url pair: 
 
[irs 1040 form, www.irs.gov/pub/irs-pdf/f1040.pdf] 
 
2.1.2 Query and URL Segmentation  
We segment the seed pairs to n-gram pairs in 
order to increase the coverage beyond that of 
ExactM click features. Within NLP, n-grams are 
typically extracted such that words that are adja-
cent in the original sequence are also adjacent in 
the extracted n-grams. Furthermore, we attempt 
to achieve additional generalization by using skip 
n-grams (Lin and Och, 2004). This means we not 
only extract n-grams for adjacent terms but also 
for sequences that leave out intermediate terms. 
This is motivated by the observation that the se-
mantics of user queries is often preserved even 
when some intermediate terms are removed.  The 
details of the segmentation methods are de-
scribed below. 
2.1.2.1 Query Segmentation  
Prior to query segmentation, we normalize raw 
queries by replacing punctuations with spaces. 
Queries are then segmented into a sequence of 
525
space delimited tokens. From these, we extract 
all possible query n-grams and skip n-grams for 
n smaller than or equal to three (i.e., all unigrams, 
bigrams, and trigrams). For example, given the 
sequence ?irs 1040 form? the adjacent bigrams 
would be ?irs 1040? and ?1040 form?. With skip 
n-grams we also extract ?irs form? as shown in 
Table 1. We do not use n-grams longer than 3 in 
order to avoid problems with overfitting. We will 
refer to this segmentation method as Affix Seg-
mentation.  
 
Table 1: An Example of Affix Segmentation 
N-gram Affix Segmentation 
Unigram irs, 1040, form 
Bigram irs 1040, 1040 form, irs form 
Trigram irs 1040 form 
2.1.2.2 URL Segmentation  
As shown in Table 2, after the queries are seg-
mented, URLs are categorized into four groups: 
domain, URL language, URL region and URL 
path. In general, a URL is delimited by punctua-
tion characters such as ???, ?.?,? ?/?, and ?=?.  
 
Table 2: An Example of URL Segmentation 
URL Groups Example 
Domain irs.gov 
URL language en 
URL region us 
URL path pub, irs, pdf, f1040, pdf 
 
The domain group includes one domain token, 
for example, irs.gov. Although domains could be 
divided into multiple n-grams, we treat them as a 
single unit, with the exception of encoded lan-
guage and region information.  
 
The language and region groups are based on the 
language or region part of the URL n-grams such 
as the suffixes ?.en? and ?.de?. The language and 
region of a URL n-gram are identified by a table 
look-up method. The table is created based on 
the information available at en.wikipedia.org/ 
wiki/List_of_ISO_639-1_codes and en.wikipedia. 
org/wiki/ISO_3166. When there is no clear lan-
guage or region URL n-gram, we use English (en) 
as the default language and United States (us) as 
the default region. 
2.1.3 Calculation of Mutual Information 
After query and URL n-grams are extracted, we 
calculate mutual information (Gale and Church, 
1991) to determine the degree of association be-
tween the n-grams. The definition of query-url n-
gram mutual information (MI) is given in Equa-
tion 1. 
 
)Freq( )Freq(
),Freq(
log2),MI( uq
uq
uq =   (1) 
 
Here q corresponds to a query n-gram and u cor-
responds to a URL n-gram. Freq (q) is the count 
of q in the seed list normalized by the total num-
ber of q. Freq (u) is the count of u normalized by 
the total number of u.  Freq (q, u) is the count of 
q and u that co-occurred in a full query-url pair 
normalized by the total number of q and u. A pair 
will be assigned to a MI score of zero if the items 
occur together no more than expected by chance, 
under the assumption that the two items are sta-
tistically independent. When a pair occurs more 
than is expected by chance, the MI score is posi-
tive. On the other hand, if a pair occurs together 
less than is expected by chance, the mutual in-
formation score is negative. In order to increase 
the confidence of the MI scores, we remove all 
n-grams with less than 3 occurrences in the seed 
list, and assign a zero MI score for any pairs in-
volving these n-grams. No smoothing is applied. 
 
This scoring scheme fits well with the associa-
tion properties we would like to have for our 
query-url n-gram click features. If a query n-
gram cues for a certain URL through one of its n-
grams, the feature will take on a positive value. 
Similarly, if a query n-gram cues against a cer-
tain URL, the feature will take on a negative val-
ue.  
2.1.4 Analysis of Query-URL N-gram 
Association 
By examining our dictionary, we observed a 
number of pairs that are interesting from a rele-
vance ranking perspective. To illustrate, we pre-
sent four examples of n-gram pairs and intui-
tively explore the nature of the n-gram associa-
tions in the dictionary.  
 
Table 3: Examples of MI Scores 
Query n-gram URL n-gram MI score 
?iphone? apple.com 8.7713 
?iphone? amazon.com -0.1555 
?iphone plan? att.com 11.5388 
?iphone plan? apple.com 8.9676 
 
First, let?s examine the association between 
query n-grams and URL n-grams for the queries 
526
?iphone? and ?iphone plan?. Notice that the 
query unigram ?iphone? is strongly associated 
with apple.com, but negatively associated with 
amazon.com. This can be explained by the fact 
that ?iphone? as a product is not only developed 
by Apple but also strongly associated with the 
Apple brand. In contrast, while Amazon.com 
sells iphones, it also sells a large variety of other 
products, thus is not regarded as a very authorita-
tive source of information about the ?iphone?. 
However, by adding additional context, the most 
preferred URL according to MI can change. The 
two examples in the bottom of Table 3 illustrate 
the URL preferences for the query bigram 
?iphone plan?. While apple.com is still a strongly 
preferred destination, there is a much stronger 
preference for att.com. This preference follows 
since apple.com has more product information on 
the ?iphone? while the information provided by 
att.com will be more targeted at visitors who 
want to explore what rate plans are available. 
 
Second, Table 4 shows the association between 
?kimo?, ?.tw? and ?.us?. ?Kimo? was a Taiwan-
ese start-up acquired by Yahoo!. The mutual in-
formation scores accurately reflect the associa-
tion between the query n-gram and region ids.  
 
Table 4: Example of MI Scores 
Query n-gram URL n-gram MI score 
?kimo? tw (taiwan) 12.8303 
?kimo? us (united states) 0.7209 
 
Third, Table 5 shows the association between 
?kanji?, and URLs with Language identification 
of ?Japanese?, ?Chinese? and ?English?. ?Kanji? 
means ?Chinese? in Japanese. Since queries con-
taining ?Kanji? are typically from users inter-
ested in Japanese sites, the mutual information 
shows higher correlation with Japanese than with 
English or Chinese.  
 
Table 5: Example of MI Scores 
Query n-gram URL n-gram MI score 
?kanji? ja (japanese) 11.3862 
?kanji? zh (chinese) 6.2567 
?kanji? en (english) 4.2110 
 
Table 6: Example of MI Score 
Query n-gram URL n-gram MI score 
?form? pdf 4.9067 
?form? htm 1.0916 
?video? watch 5.7192 
?video? htm -1.9079 
 
Fourth, Table 6 shows the association between 
two query n-grams, ?form? and ?video?, that at 
first glance may not actually look very informa-
tive for URL path selection. However, notice that 
the unigram ?form? has a strong preference for 
pdf documents over more standard web pages 
with an html extension. Similarly, queries that 
include ?video? convey a preference for URLs 
containing ?watch?, a characteristic URL n-gram 
for many video sharing websites. 
 
It is reasonable to anticipate that incorporating 
such associations into a search engine?s ranking 
function should help improve both search quality 
and user experience. Take the example where, 
there are two high ranking competing URLs for 
the query ?irs 1040 form?. Let?s also assume 
both documents contain the same query relevant 
keywords, but one is an introduction of the ?irs 
1040 form? as an htm webpage and the other one 
is the real filing form given as a pdf document. 
Since in our dictionary, ?form? is more associ-
ated with pdf than htm, we predict that most us-
ers would prefer the real pdf form directly, so it 
should be placed first in the list of query results. 
While click data for the exact query-url pairs 
confirms this preference, it is reassuring that we 
could identify it without needing to rely on see-
ing the specific query string before. As described 
in detail below, and motivated by this analysis, 
we designed our query-url click features based 
on the contents of the n-gram MI dictionary. 
2.2 Query-URL N-gram Features  
For our feature set, we explored the use of differ-
ent query segmentation approaches (concept and 
affix segmentation) in order to increase the di-
versity of n-grams. In the following section, we 
use an unseen query ?irs 1040 forms? and con-
trast it with the known query ?irs 1040 form? 
from the last section.  
2.2.1 Concept Segmentation Features 
Query concept segmentation is a weighted query 
segmentation approach. Each query is analyti-
cally interpreted as being a main concept and a 
sub concept. We search for the unique segmenta-
tion of the query that maximizes its cumulative 
mutual information score with the URL n-grams. 
Main concepts and sub concepts are n-grams 
from the query that have the strongest association 
with URL n-grams and thus assist in identifying 
relevant landing URL n-grams when the whole 
query or the whole URL has not been seen.  
527
 Algorithm 1: Concept Segmentation 
for U = domain, URL language, URL region, 
URL path do 
    for j = 0... n-1 do 
          M  ?  W0...j 
          S   ?  Wj+1...n 
          for k = 0... m do          
               curr_mi_M ? arg maxk=1...m  MI (M, Uk) 
               curr_mi_S ? arg maxk=1...m  MI (S, Uk) 
                if curr_mi_M + curr_mi_S > curr_best                    
                then 
                    curr_best = curr_mi_M + curr_mi_S 
                    mi_M ? curr_mi_M 
                    mi_S ? curr_mi_S 
                end if 
           end for 
           adding mi_M as a feature 
           adding mi_S as a feature 
      end for 
end for 
 
Pseudo-code for generating query-url n-gram 
features based on the concept segmentation is 
given in Algorithm 1. Each query (Q) is com-
posed of a number of words, w1, w2, w3?,wn. 
Each URL is segmented and categorized to four 
groups: domain, URL language, URL region and 
URL path. Each URL group has m number of 
URL n-grams.  M is the main concept of Q and S 
is the sub concept of Q.  
 
One potential drawback of such concept segmen-
tation is data sparsity. When we look for the 
maximum of cumulative mutual information, we 
may obtain main concepts with very high mutual 
information and sub concepts which do not exist 
in the dictionary. In order to address this problem, 
we implement a second query segmentation me-
thod, affix segmentation, that is discussed in sec-
tion 2.2.2.  
 
Table 7 shows eight concept segmented features. 
?Coverage? is the percentage of query-url pairs 
that have valid feature values. Some of the sam-
ples do not have values because no clicks for the 
pairs were seen in the sample of data used to 
build the dictionary. When a pair does not have a 
value, the default value of zero is assigned. This 
default value is based on the assumption that 
unless we have evidence otherwise, we assume 
all query-url n-grams are statistically independ-
ent and thus provide no preference signal. 
 
Table 7: Eight Features Generated based on 
Concept Segmentation.  
Feature Query N-
gram 
URL N-
gram 
Coverage 
(%) 
MainDS M domain 54.09 
SubDS S domain 30.46 
MainLang M lang. 94.41 
SubLang S lang. 72.40 
MainReg M reg. 90.34 
SubReg S reg. 68.19 
MainPath M path 64.96 
SubPath S path 58.76 
 
Query-URL Domain Features are defined as 
the mutual information of a query n-gram and the 
domain level URL. There are two features in this 
category, one for the query main concept and one 
for the sub concept. They help to identify the 
user preferred host given a query.  
 
Table 8: Example of Selecting Query Segmenta-
tion 
MI(q,u) irs.gov 
?irs? 11.2174 
?1040? 
  
11.6175 
?forms? 7.5049 
11.5550 
Cumulative MI 19.1224 22.7724 
  Seg. 1 Seg.2 
 
To illustrate the concept segmentation features, 
let?s examine the query, ?irs 1040 forms? in the 
context of the domain irs.gov.  The query ?irs 
1040 forms? can be segmented either as ?irs 
1040? and ?forms? or as ?irs? and ?1040 forms?. 
As shown in Table 8, taking the cumulative max-
imum, the second segmentation scores higher 
than the first one. Therefore, the ?irs? and ?1040 
forms? segmentation is preferred. The feature 
value for the main concept is 11.5550, and the 
sub concept is then assigned to be 11.2174. 
 
Query-URL Language and Region Features 
are the mutual information of a query n-gram and 
URL language/region. They are used for provid-
ing language and region information.  
 
Query-URL Path Features are the mutual 
information of a query n-gram and a URL path n-
gram. While there are typically many URL path 
n-grams, only one URL path n-gram is selected 
to be paired with each query n-gram. The se-
lected n-gram is the one that achieves the highest 
528
cumulative maximum MI score. They are used 
for providing association between query n-grams 
and url n-grams such as ?forms? and ?pdf?. 
2.2.2 Affix Segmentation Features 
As previously mentioned, affix segmentation 
addresses sparsity issues associated with concept 
segmentation. Here, we introduce the features 
generated based on affix segmentation. Pseudo-
code for generating the features is given in Algo-
rithm 2. Two query unigrams (w0 and wn) and 
one bigram (w0wn) is used. Each URL is seg-
mented and categorized to four groups: domain, 
URL language, URL region and URL path. Each 
URL group has m number of URL n-grams.   
 
This approach is complementary to the concept 
segmentation for long queries. The affix n-grams 
are in smaller unit, and therefore, are less sparse.  
In addition, the skip bigrams allow for generali-
zations using non-adjacent terms. Table 9 shows 
the coverage of the twelve affix features.  
 
Algorithm 2:  Affix Segmentation  
for U = domain, URL language, URL region, 
URL path do 
     for q = w0, wn, w0wn do 
          for k = 0... m do          
               curr_mi_q ? arg maxk=1...m  MI (q, Uk) 
                if curr_mi_q > curr_best then 
                    curr_best = curr_mi_q  
                end if 
           end for 
           adding curr_mi_q as a feature 
      end for 
end for 
 
Table 9: Twelve Features Generated based on 
Affix Segmentation  
Feature Query N-
gram 
URL N-
gram 
Coverage 
(%) 
PreDS w0 domain 48.09 
SufDS wn domain 47.72 
PresufDS w0wn domain 23.57 
PreLang w0 lang. 55.58 
SufLang wn lang. 58.22 
PresufLang w0wn lang. 24.91 
PreReg w0 reg. 93.82 
SufReg wn reg. 93.59 
PresufReg w0wn reg. 69.29 
PrePath w0 path 98.15 
SufPath wn path 97.80 
PresufPath w0wn path 75.81 
 
Query-url domain affix features has three fea-
tures: MI(w0, domain), MI(wn, domain), and 
MI(w0wn, domain). In the example of ?irs 1040 
forms? and ?irs.gov?, the features are MI(irs, 
irs.gov), MI(forms, irs.gov), and MI(irs forms, 
irs.gov).  
 
Query-url language and region affix features 
has three features respectively: MI(w0, language), 
MI(wn, language), MI(w0wn, language) MI(w0, 
region), MI(wn, region), and MI(w0wn, region). 
In the example of ?irs 1040 forms?, ?en? and 
?us?, the features are MI (irs, en), MI (forms, en), 
MI (irs forms, en), MI (irs, us), MI (forms, us), 
and MI (irs forms,us).  
 
Query-url path affix features has three fea-
tures: MI(w0, path), MI(wn, path), and MI(w0wn, 
path). In the example of ?irs 1040 forms? and 
?www.irs.gov/pub/irs-pdf/f1040.pdf?, there are 
four URL path n-grams, ?pub?, ?irs?, ?pdf?, and 
?f1040?. The URL path n-gram, irs, gets maxi-
mum MI score. Therefore, the query-url path af-
fix features are MI (irs, irs), MI (forms, irs), and 
MI (irs forms, irs).  
 
We demonstrated the procedure to generate 20 
query-url n-gram features, and in Section 3, we 
will present their effectiveness in relevance rank-
ing.   
3 Experiment 
We evaluate the performance of query-url n-
grams features (8 concept and 12 affix features) 
on a ranking application and analyze the results 
from several different perspectives.  
3.1 Datasets 
For all experiments, our training and test data are 
query-url pairs annotated with human judgments.  
In our data, we use five grades to evaluate rele-
vance of a query and URL pair.  
 
The data includes 94K queries for training and 
3.4K queries for evaluation, and each query is 
associated with the top ranked URLs returned 
from a search engine. Totally, there are 916K 
query-url pairs for training and 42K pairs for 
testing. The queries are general and uniformly 
and randomly sampled with replacement, result-
ing in more frequent queries also appearing more 
frequently in our training and test sets. 
529
3.2 Ranking Algorithm 
GBRank is a supervised learning algorithm that 
uses boosted decision trees and incorporates the 
pair-wise information from the training data 
(Zheng et al 2007). It is able to deal with a large 
amount of training data with hundreds of features. 
We use an internal C++ implementation of 
GBRank. 
3.3 Evaluation Metric 
We use Discounted Cumulative Gain (J?rvelin 
and Kek?l?inen, 2002) to evaluate our ranking 
accuracy. Discounted Cumulative Gain (DCG) 
has been widely used in evaluating the quality of 
search engine rankings and is defined as: 
 
?
=
+
=
k
i
i
k i
G
DCG
1 2 )1(log
  (2) 
     
Gi represents the editorial judgment of the i-th 
document. In this paper, we only report normal-
ized DCG5, which is an absolute DCG5 normal-
ized by a baseline, and relative DCG5 im-
provement, which is an improvement normal-
ized by the baseline. Note normalized DCG5 is 
different than NDCG (Normalized Discounted 
Cumulative Gain defined in J?rvelin and 
Kek?l?inen, 2002). We use Wilcoxon signed test 
(Wilcoxon, 1945) to evaluate the significance for 
model comparison. 
3.4 Feature Sets 
Five feature sets are used in our experiments. 
Details are listed in Table 10.  
 
Table 10: Five Feature Sets 
Tag Description  
Base Feature 
Set  
Core Feature Set and ExactM 
click features 
Q-U N-gram 
Feature Set (I) 
Base Feature Set and Q-U N-
gram features 
Core Feature 
Set  
query-based, document-based, 
query-document based fea-
tures 
NavClick Fea-
ture Set 
Core Feature Set and Nav-
Click 
Q-U N-gram 
Feature Set (II) 
Core Feature Set and Q-U N-
gram features 
 
Base Feature Set is a strong baseline feature set 
from a state-of-the-art commercial search engine. 
This set includes NavClick features, and other 
internal ExactM click features. It is used for 
evaluating Query-URL N-gram Feature Set (I) in 
order to know whether query-url n-gram features 
can achieve gains when stacked on top of Ex-
actM features.  
 
Core Feature Set is a weaker variant of the 
baseline system that excludes ExactM click fea-
tures. This system is used for evaluating 
NavClick Feature Set and Query-URL N-gram 
Feature Set (II) independently in order to study 
and contrast the effected queries.  
3.5 Experimental Results 
We compare the query-URL N-gram feature set 
(I) with the base feature set in Section 3.5.1, and 
contrast the NavClick features and the query-
URL N-gram features (II) using the Core Feature 
Set in Section 3.5.2. 
3.5.1 Query-URL N-gram Feature Set (I) 
versus Base Feature Set 
As shown in Figure 2, Query-URL N-gram Fea-
ture Set (I) outperforms Base Feature Set. The 
additional 20 query-url n-gram features achieve 
statistically significant gains at a p-value < 
0.0001 level, suggesting that they are compli-
mentary to ExactM click features. Even though 
the query-url n-gram features are generated from 
the same data as the ExactM features, the gain is 
additive and stackable. The DCG5 impact is 
0.53% relative improvement when running 
GBRank using 2500 trees. Every data point is 
normalized by the DCG5 of the baseline feature 
set using 2500 trees. This is represented in the 
graph as the rightmost point of Base Feature Set 
curve. 
 
0.96
0.97
0.98
0.99
1
1.01
500 1000 1500 2000
Q-U N-gram Features (I)
Base Features
NavClick
Q-U N-gram Features (II)
Core Features
 
Figure 2: Comparison of the five feature sets on 
the normalized DCG5 (Y-axis) against number of 
trees (X-axis).  
 
530
3.5.2 NavClick and Query-URL N-gram 
Feature Set (II) versus Core Fea-
ture Set 
We compare NavClick Feature Set and Query-
URL N-gram Feature Set (II) in the context of 
Core Feature Set, in order to evaluate the two 
independently. As shown in Figure 2, both 
NavClick and Query-URL N-gram Feature Set 
(II) outperform Core Feature Set. It is not sur-
prising that NavClick also outperforms Query-
URL N-gram Feature Set (II) since the n-gram 
features are backoff of NavClick. However, their 
gains are competitive suggesting the query-url n-
gram features are very good relevance indicators. 
The impact of NavClick and Query-URL N-gram 
Feature Set (II) is 0.72% and 0.62% relative 
DCG5 improvement at Tree 2500 respectively. 
3.5.3 Feature Importance 
Using the GBRank model, features are evaluated 
and sequentially selected to build the boosted 
decision trees. The split of each node increases 
the DCG during training. We evaluate a feature?s 
importance by aggregating the DCG impact of 
the feature over all trees (Zheng et al, 2007). 
Here, the feature importance is rescaled so that 
the feature with largest DCG impact is assigned a 
normalized score of 1. Figure 3 illustrates the 
relative influence of each of query-url n-gram 
feature. Of these, n-gram features associated with 
a domain name (i.e., MainDS) rank highest. 
 
 
Figure 3: Feature importance of query-url n-
gram features. The importance (Y axis) is nor-
malized so that the most important feature 
(MainDS)?s importance is 1. 
3.6 Analysis 
We access system performance with respect to 
both query length and frequency using the two 
click features sets in combination with the Core 
Feature Set in order to gain insight into the ef-
fected queries.  
3.6.1 Query Length 
As shown in Table 11, NavClick (NavClick Fea-
ture Set) best improves relevance for two word 
queries. In contrast, Query-url n-gram features in 
isolation (Query-URL N-gram Feature II) are 
able to show sizable improvements on longer 
queries, while slightly degrading performance on 
short 1-word queries. Using both feature sets to-
gether (Query-URL N-gram Feature I) results in 
improvement for queries of all lengths. 
 
These results suggest that the strong signal being 
provided by NavClick for short queries helps to 
compensate for any additional noisy introduced 
by the n-gram features, while allowing the n-
gram features to handle  longer queries that are 
less well covered by NavClick. These longer 
queries are exactly the type of queries our query-
url n-gram features were designed to help with. 
 
Table 11: Relative DCG5 Improvement of 
NavClick, Query-URL N-gram (II), and Query-
URL N-gram Features  (I) vs Core Feature Set 
Length NavClick 
vs Core 
(%) 
QU N-
gram (II)  
vs Core 
(%) 
QU N-
gram (I) 
vs Core 
(%) 
1 word 0.03 -0.04 0.62 
2 words 1.04 1.06 1.58 
3 words 1.00 1.44 2.12 
4+ words 0.4 0.68 1.01 
3.6.2 Query Frequency 
We found that query-url n-gram features improve 
tail queries. Head queries are considered as top 
two million frequent queries in our traffic and 
tail queries include anything outside of that range.  
 
Table 12: Relative DCG5 Improvement of 
NavClick, Query-URL N-gram Features (II) and 
Query-URL N-gram Features (I) vs Core Feature 
Set 
 NavClick vs 
Core (%) 
QU N-gram 
(II) vs Core 
(%) 
QU N-
gram (I) vs 
Core (%) 
Head 0.91 -0.15 1.11 
Tail 0.59 1.11 1.40 
 
As shown in Table 12, query-url n-gram features 
(Query-URL Feature Set II) differ from 
NavClick (NavClick Feature Set) in that they get 
531
more gain from tail queries. Together, they 
(Query-URL Feature Set I) improve both head 
and tail queries. 
3.7 Case Study 
Below we examine queries from the test set and 
analyze the effects of Query-URL N-gram Fea-
ture Set (II) versus Core Feature Set. 
3.7.1 Positive Cases 
1) Animal shelter in va: this query targets a spe-
cific geographic location. Using the baseline fea-
ture set, the root url wvanimalshelter.org is incor-
rectly ranked higher than www.netpets.com/ 
cats/catresc/virginia.htm. Without any addition-
ally ranking information, general URLs (root) 
tend to be ranked more highly than more specific 
URLs (path), as the root pages tend to be more 
popular. However, our new features express a 
preference between ?va? and ?virginia?, and this 
correctly flips the ranking order.  
2) Myspace profile generator: www. myspacgens. 
com/handler.php?gen=profile was incorrectly 
ranked higher than www.profilemods.com/ 
myspace-generators. Our new features convey a 
high user preference association between ?profile 
generator? and the domain profilemods.com, 
which helps to correctly swap the order.   
3.7.2 Negative Cases 
We determined that negative cases where the 
baseline feature set outperforms the new features 
are typically one word navigational queries such 
as ?craigslist?. However, after we combine the 
query-url n-gram features with NavClick, one 
word navigational queries are ranked correctly. 
4 Related Work 
Our work is mainly related to Gao et al (2009) 
and Bilenko and White (2008). Gao et al (2009) 
addressed the sparsity issue by propagating click 
information among similar queries in the same 
cluster. Their idea is based on an observation that 
similar queries go to similar pages. When two 
queries have similar clicked URLs, it is likely 
that they share clicked URLs. In contrast, our 
idea is to utilize NLP techniques to break down 
long, infrequent queries into shorter, frequent 
queries. The two approaches can be mutually 
beneficial. Bilenko and White (2008) expanded 
click data with a search engine by using post-
search user experience collected from toolbars. 
Toolbars keep track of users? click behavior both 
when they are using the search engine directly 
and beyond. Their relevance features are built 
based on whole session clicks extracted from the 
toolbar. In contrast, our n-gram features are built 
on search engine clicks directly. We should be 
able to expand our method to integrate the post-
search clicks with toolbar data.    
 
Other related work can be found in the domain of 
query rewriting. Our n-gram dictionary was orig-
inally designed for query rewriting. Query re-
writing (Xu and Croft, 1996; Salton and Voor-
hees, 1984) reformulates a query to its synonyms 
or related terms automatically. However, the 
coverage of query rewriting is normally small, 
because an inappropriate rewrite can cause sig-
nificant decrease in precision. In contrast, our 
approach can cover a larger number of queries 
without decreasing precision, because it does not 
need to make a binary decision whether a query 
should be reformulated. The association scores 
between queries and rewrites are used as ranking 
features which are trained discriminatively to-
ward search quality.   
5 Conclusion 
In this paper, we presented a set of straightfor-
ward yet informative query-url n-gram features. 
They allow for generalization of limited user 
click data to large amounts of unseen query-url 
pairs. Our experiments showed such features 
gave significant improvement over models with-
out using the features. In addition, we mined an 
interesting dictionary which contains informa-
tive, but not necessarily obvious, query-url syno-
nym pairs such as ?form? and ?pdf?. We are cur-
rently extending our work to a variety of exact 
match features and different sources of click-
through logs.  
Acknowledgement 
Thanks to the anonymous reviewers for detailed 
suggestion and our colleagues: Jon Degenhardt 
and Narayanan Sadagopan for assistance on gen-
erating clickthrough data, Jiang Chen for devel-
oping the decision tree package, Xiangyu Jin for 
a discussion on map/reduce, Beno?t Dumoulin, 
Fuchun Peng, Yumao Lu, and Xing Wei for pro-
ductizing the work, and Rosie Jones, Su-lin Wu, 
Bo Long, Xin Li and Ruiqiang Zhang for com-
ments on an earlier draft.  
 
 
 
532
References  
Agichtein, E., E. Brill, and S. Dumais. 2006. Im-
proving web search ranking by incorporating 
user behavior information. In Proceedings of 
the ACM SIGIR 29. 
Agichtein, Eugene, Zijian Zheng. 2006. Identify-
ing "best bet" web search results by mining 
past user behavior.  In Proceedings of KDD. 
Bilenko, Mikhail and Ryen W. White. 2008.  
Mining the search trails of surfing crowds: 
identifying relevant websites from user activ-
ity. In Proceedings of WWW.  
Brants, T. 2000. Tnt: a statistical part-ofspeech 
tagger. In Proceedings of ANLP 6. 
Craswell, Nick and Martin Szummer. 2007. Ran-
dom walks on the click graph. In Proceedings 
of SIGIR. 
Craswell, Nick, Onno Zoeter, Michael Taylor, 
Bill Ramsey. 2008. An experimental compari-
son of click position-bias models in WSDM. 
Dupret, Georges, Benjamin Piwowarski. 2008. A 
user browsing model to predict search engine 
click data from past observations. In Proceed-
ings of SIGIR 31. 
Gale, William A. and Kenneth W. Church. 1991. 
Identifying word correspondence in parallel 
texts. In Proceedings of HLT 91. 
Gao, Jianfeng, Wei Yuan, Xiao Li, Kefeng Deng, 
and Jian-Yun Nie. 2009. Smoothing Click-
through Data for Web Search Ranking. In Pro-
ceedings of SIGIR 32. 
J?rvelin, K. and J. Kek?l?inen. 2002. Cumulated 
gain-based evaluation of IR techniques, Jour-
nal ACM Transactions on Information Sys-
tems, 20: 422-446. 
Klein, D. and C. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of ACL 41. 
Lin, Chin-Yew and Franz Josef Och. 2004. Au-
tomatic evaluation of machine translation 
quality using longest common subsequence 
and skip-bigram. In In Proceedings of ACL 42.  
Lu, Yumao, Fuchun Peng, Xin Li and Nawaaz 
Ahmed, 2006, Coupling Feature Selection and 
Machine Learning Methods for Navigational 
Query Identification, In Proceeding of CIKM. 
Radlinski, F., Kurup, M. and Joachims, T. 2007. 
Active exploration for learning rankings from 
clickthrough data. In SIGKDD. 
Salton G. and E. Voorhees. 1984. Comparison of 
two methods for Boolean query relevancy 
feedback. Information Processing & Manage-
ment, 20(5).   
Wilcoxon, F. 1945. Individual Comparisons by 
Ranking Methods. Biometrics, 1:80?83. 
Xu Q. and W. Croft. 1996. Query expansion us-
ing local and global document analysis. In 
Proceed of the 19th annual international ACM 
SIGIR. 
Zheng, Z., H. Zha, K. Chen, and G. Sun. 2007. A 
regression framework for learning ranking 
functions using relative relevance judgments. 
In Proceedings of SIGIR 30.  
533
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 51?59,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discriminative Reordering with Chinese Grammatical Relations Features
Pi-Chuan Changa, Huihsin Tsengb, Dan Jurafskya, and Christopher D. Manninga
aComputer Science Department, Stanford University, Stanford, CA 94305
bYahoo! Inc., Santa Clara, CA 95054
{pichuan,jurafsky,manning}@stanford.edu, huihui@yahoo-inc.com
Abstract
The prevalence in Chinese of grammatical
structures that translate into English in dif-
ferent word orders is an important cause of
translation difficulty. While previous work has
used phrase-structure parses to deal with such
ordering problems, we introduce a richer set of
Chinese grammatical relations that describes
more semantically abstract relations between
words. Using these Chinese grammatical re-
lations, we improve a phrase orientation clas-
sifier (introduced by Zens and Ney (2006))
that decides the ordering of two phrases when
translated into English by adding path fea-
tures designed over the Chinese typed depen-
dencies. We then apply the log probabil-
ity of the phrase orientation classifier as an
extra feature in a phrase-based MT system,
and get significant BLEU point gains on three
test sets: MT02 (+0.59), MT03 (+1.00) and
MT05 (+0.77). Our Chinese grammatical re-
lations are also likely to be useful for other
NLP tasks.
1 Introduction
Structural differences between Chinese and English
are a major factor in the difficulty of machine trans-
lation from Chinese to English. The wide variety
of such Chinese-English differences include the or-
dering of head nouns and relative clauses, and the
ordering of prepositional phrases and the heads they
modify. Previous studies have shown that using syn-
tactic structures from the source side can help MT
performance on these constructions. Most of the
previous syntactic MT work has used phrase struc-
ture parses in various ways, either by doing syntax-
directed translation to directly translate parse trees
into strings in the target language (Huang et al,
2006), or by using source-side parses to preprocess
the source sentences (Wang et al, 2007).
One intuition for using syntax is to capture dif-
ferent Chinese structures that might have the same
(a) (ROOT  (IP     (LCP       (QP (CD ?)
        (CLP (M ?)))
      (LC ?))
    (PU ?)    (NP       (DP (DT ??))
      (NP (NN ??)))    (VP       (ADVP (AD ??))
      (VP (VV ??)        (NP           (NP             (ADJP (JJ ??))
            (NP (NN ??)))
          (NP (NN ??)))
        (QP (CD ?????)
          (CLP (M ?)))))
    (PU ?)))
(b) (ROOT  (IP     (NP       (DP (DT ??))
      (NP (NN ??)))    (VP       (LCP         (QP (CD ?)
          (CLP (M ?)))
        (LC ?))
      (ADVP (AD ??))
      (VP (VV ??)        (NP           (NP             (ADJP (JJ ??))
            (NP (NN ??)))
          (NP (NN ??)))
        (QP (CD ?????)
          (CLP (M ?)))))
    (PU ?)))
?
?
? ??
??
?? ?? ?
?? ??
??
?????
? (three) 
? (year) 
? (over; in) ?? (city)
??(complete)
??(collectively) ??(invest) ?(yuan)
 (these) ??(asset)
??(fixed)
?????(12 billion)
loc nsubj advmod dobj range
lobj det nn
nummod amod
nummod
Figure 1: Sentences (a) and (b) have the same mean-
ing, but different phrase structure parses. Both sentences,
however, have the same typed dependencies shown at the
bottom of the figure.
meaning and hence the same translation in English.
But it turns out that phrase structure (and linear or-
der) are not sufficient to capture this meaning rela-
tion. Two sentences with the same meaning can have
different phrase structures and linear orders. In the
example in Figure 1, sentences (a) and (b) have the
same meaning, but different linear orders and dif-
ferent phrase structure parses. The translation of
sentence (a) is: ?In the past three years these mu-
nicipalities have collectively put together investment
in fixed assets in the amount of 12 billion yuan.? In
sentence (b), ?in the past three years? has moved its
51
position. The temporal adverbial ??#u? (in the
past three years) has different linear positions in the
sentences. The phrase structures are different too: in
(a) the LCP is immediately under IP while in (b) it
is under VP.
We propose to use typed dependency parses in-
stead of phrase structure parses. Typed dependency
parses give information about grammatical relations
between words, instead of constituency informa-
tion. They capture syntactic relations, such as nsubj
(nominal subject) and dobj (direct object) , but also
encode semantic information such as in the loc (lo-
calizer) relation. For the example in Figure 1, if we
look at the sentence structure from the typed depen-
dency parse (bottom of Figure 1), ??#u? is con-
nected to the main verb q? (finish) by a loc (lo-
calizer) relation, and the structure is the same for
sentences (a) and (b). This suggests that this kind
of semantic and syntactic representation could have
more benefit than phrase structure parses.
Our Chinese typed dependencies are automati-
cally extracted from phrase structure parses. In En-
glish, this kind of typed dependencies has been in-
troduced by de Marneffe and Manning (2008) and
de Marneffe et al (2006). Using typed dependen-
cies, it is easier to read out relations between words,
and thus the typed dependencies have been used in
meaning extraction tasks.
We design features over the Chinese typed depen-
dencies and use them in a phrase-based MT sys-
tem when deciding whether one chunk of Chinese
words (MT system statistical phrase) should appear
before or after another. To achieve this, we train a
discriminative phrase orientation classifier follow-
ing the work by Zens and Ney (2006), and we use
the grammatical relations between words as extra
features to build the classifier. We then apply the
phrase orientation classifier as a feature in a phrase-
based MT system to help reordering.
2 Discriminative Reordering Model
Basic reordering models in phrase-based systems
use linear distance as the cost for phrase move-
ments (Koehn et al, 2003). The disadvantage of
these models is their insensitivity to the content of
the words or phrases. More recent work (Tillman,
2004; Och et al, 2004; Koehn et al, 2007) has in-
troduced lexicalized reordering models which esti-
mate reordering probabilities conditioned on the ac-
tual phrases. Lexicalized reordering models have
brought significant gains over the baseline reorder-
ing models, but one concern is that data sparseness
can make estimation less reliable. Zens and Ney
(2006) proposed a discriminatively trained phrase
orientation model and evaluated its performance as a
classifier and when plugged into a phrase-based MT
system. Their framework allows us to easily add in
extra features. Therefore we use it as a testbed to see
if we can effectively use features from Chinese typed
dependency structures to help reordering in MT.
2.1 Phrase Orientation Classifier
We build up the target language (English) translation
from left to right. The phrase orientation classifier
predicts the start position of the next phrase in the
source sentence. In our work, we use the simplest
class definition where we group the start positions
into two classes: one class for a position to the left of
the previous phrase (reversed) and one for a position
to the right (ordered).
Let c j, j? be the class denoting the movement from
source position j to source position j? of the next
phrase. The definition is:
c j, j? =
{ reversed if j? < j
ordered if j? > j
The phrase orientation classifier model is in the log-
linear form:
p?N1 (c j, j? | f J1 ,eI1, i, j)
= exp
(?Nn=1 ?nhn( f J1 ,eI1, i, j,c j, j?)
)
?c? exp
(?Nn=1 ?nhn( f J1 ,eI1, i, j,c?)
)
i is the target position of the current phrase, and f J1
and eI1 denote the source and target sentences respec-
tively. c? represents possible categories of c j, j? .
We can train this log-linear model on lots of la-
beled examples extracted from all of the aligned MT
training data. Figure 2 is an example of an aligned
sentence pair and the labeled examples that can be
extracted from it. Also, different from conventional
MERT training, we can have a large number of bi-
nary features for the discriminative phrase orienta-
tion classifier. The experimental setting will be de-
scribed in Section 4.1.
52
(21) </s>
(20) .
(19) world
(18) outside
(17) the
(16) to
(15) up
(14) opening
(13) of
(12) policy
(11) 's
(10) China
(9) from
(8) arising
(7) star
(6) bright
(5) a
(4) become
(3) already
(2) has
(1) Beihai
(0) <s>
(15)
</s>
(14)?(13)??(12)?(11)?(10)?(9)??(8)?(7)?5)?(6)?)?(4)??(3)??(2)?(1)??(0)<s>
ordered151420
ordered14618
ordered6516
reversed5715
reversed7810
reversed8109
ordered1098
reversed9137
ordered13126
ordered12115
ordered1134
ordered323
ordered211
ordered100
classj'ji
i j
Figure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with the
labeled examples for the phrase orientation classifier. Note that the alignment grid in this example is automatically
generated.
The basic feature functions are similar to what
Zens and Ney (2006) used in their MT experiments.
The basic binary features are source words within a
window of size 3 (d ? ?1,0,1) around the current
source position j, and target words within a window
of size 3 around the current target position i. In the
classifier experiments in Zens and Ney (2006) they
also use word classes to introduce generalization ca-
pabilities. In the MT setting it?s harder to incorpo-
rate the part-of-speech information on the target lan-
guage. Zens and Ney (2006) also exclude word class
information in the MT experiments. In our work
we will simply use the word features as basic fea-
tures for the classification experiments as well. As
a concrete example, we look at the labeled example
(i = 4, j = 3, j? = 11) in Figure 2. We include the
word features in a window of size 3 around j and i
as in Zens and Ney (2006), we also include words
around j? as features. So we will have nine word
features for (i = 4, j = 3, j? = 11):
Src?1:. Src0:?? Src1:?)
Src2?1:{ Src20: Src21:(
Tgt?1:already Tgt0:become Tgt1:a
2.2 Path Features Using Typed Dependencies
Assuming we have parsed the Chinese sentence that
we want to translate and have extracted the gram-
matical relations in the sentence, we design features
using the grammatical relations. We use the path be-
tween the two words annotated by the grammatical
relations. Using this feature helps the model learn
about what the relation is between the two chunks
of Chinese words. The feature is defined as follows:
for two words at positions p and q in the Chinese
53
Shared relations Chinese English
nn 15.48% 6.81%
punct 12.71% 9.64%
nsubj 6.87% 4.46%
rcmod 2.74% 0.44%
dobj 6.09% 3.89%
advmod 4.93% 2.73%
conj 6.34% 4.50%
num/nummod 3.36% 1.65%
attr 0.62% 0.01%
tmod 0.79% 0.25%
ccomp 1.30% 0.84%
xsubj 0.22% 0.34%
cop 0.07% 0.85%
cc 2.06% 3.73%
amod 3.14% 7.83%
prep 3.66% 10.73%
det 1.30% 8.57%
pobj 2.82% 10.49%
Table 1: The percentage of typed dependencies in files
1?325 in Chinese (CTB6) and English (English-Chinese
Translation Treebank)
sentence (p < q), we find the shortest path in the
typed dependency parse from p to q, concatenate all
the relations on the path and use that as a feature.
A concrete example is the sentences in Figure 3,
where the alignment grid and labeled examples are
shown in Figure 2. The glosses of the Chinese words
in the sentence are in Figure 3, and the English trans-
lation is ?Beihai has already become a bright star
arising from China?s policy of opening up to the out-
side world.? which is also listed in Figure 2.
For the labeled example (i = 4, j = 3, j? = 11),
we look at the typed dependency parse to find the
path feature between ?? and . The relevant
dependencies are: dobj(??, ?h), clf (?h, ()
and nummod( , ). Therefore the path feature is
PATH:dobjR-clfR-nummodR. We also use the direc-
tionality: we add an R to the dependency name if it?s
going against the direction of the arrow.
3 Chinese Grammatical Relations
Our Chinese grammatical relations are designed to
be very similar to the Stanford English typed depen-
dencies (de Marneffe and Manning, 2008; de Marn-
effe et al, 2006).
3.1 Description
There are 45 named grammatical relations, and a de-
fault 46th relation dep (dependent). If a dependency
matches no patterns, it will have the most generic
relation dep. The descriptions of the 45 grammat-
ical relations are listed in Table 2 ordered by their
frequencies in files 1?325 of CTB6 (LDC2007T36).
The total number of dependencies is 85748, and
other than the ones that fall into the 45 grammatical
relations, there are also 7470 dependencies (8.71%
of all dependencies) that do not match any patterns,
and therefore keep the generic name dep.
3.2 Chinese Specific Structures
Although we designed the typed dependencies to
show structures that exist both in Chinese and En-
glish, there are many other syntactic structures that
only exist in Chinese. The typed dependencies we
designed also cover those Chinese specific struc-
tures. For example, the usage of ?{? (DE) is one
thing that could lead to different English transla-
tions. In the Chinese typed dependencies, there
are relations such as cpm (DE as complementizer)
or assm (DE as associative marker) that are used
to mark these different structures. The Chinese-
specific ??? (BA) construction also has a relation
ba dedicated to it.
The typed dependencies annotate these Chinese
specific relations, but do not directly provide a map-
ping onto how they are translated into English. It
becomes more obvious how those structures affect
the ordering when Chinese sentences are translated
into English when we apply the typed dependencies
as features in the phrase orientation classifier. This
will be further discussed in Section 4.4.
3.3 Comparison with English
To compare the distribution of Chinese typed de-
pendencies with English, we extracted the English
typed dependencies from the translation of files 1?
325 in the English Chinese Translation Treebank
1.0 (LDC2007T02), which correspond to files 1?325
in CTB6. The English typed dependencies are ex-
tracted using the Stanford Parser.
There are 116,799 total English dependencies,
and 85,748 Chinese ones. On the corpus we use,
there are 45 distinct dependency types (not includ-
ing dep) in Chinese, and 50 in English. The cov-
erage of named relations is 91.29% in Chinese and
90.48% in English; the remainder are the unnamed
relation dep. We looked at the 18 shared relations
54
?? ? ?? ?? ? ? ?? ? ?? ? ? ? ?? ?
nsubj nsubjpobj lccomp loc rcmod
dobj
clfnummodadvmod
Beihai already become China to outside open during rising (DE) one measureword brightstar .prep cpm
punct
Figure 3: A Chinese example sentence labeled with typed dependencies
between Chinese and English in Table 1. Chinese
has more nn, punct, nsubj, rcmod, dobj, advmod,
conj, nummod, attr, tmod, and ccomp while English
uses more pobj, det, prep, amod, cc, cop, and xsubj,
due mainly to grammatical differences between Chi-
nese and English. For example, some determiners
in English (e.g., ?the? in (1b)) are not mandatory in
Chinese:
(1a)??=/import and export/total value
(1b) The total value of imports and exports
In another difference, English uses adjectives
(amod) to modify a noun (?financial? in (2b)) where
Chinese can use noun compounds (???/finance?
in (2a)).
(2a)?u/Tibet??/finance?/system??/reform
(2b) the reform in Tibet ?s financial system
We also noticed some larger differences between
the English and Chinese typed dependency distribu-
tions. We looked at specific examples and provide
the following explanations.
prep and pobj English has much more uses of prep
and pobj. We examined the data and found three
major reasons:
1. Chinese uses both prepositions and postposi-
tions while English only has prepositions. ?Af-
ter? is used as a postposition in Chinese exam-
ple (3a), but a preposition in English (3b):
(3a)??/1997??/after
(3b) after 1997
2. Chinese uses noun phrases in some cases where
English uses prepositions. For example, ??
-? (period, or during) is used as a noun phrase
in (4a), but it?s a preposition in English.
(4a)??/1997t/to??/1998?- /period
(4b) during 1997-1998
3. Chinese can use noun phrase modification in
situations where English uses prepositions. In
example (5a), Chinese does not use any prepo-
sitions between ?apple company? and ?new
product?, but English requires use of either
?of? or ?from?.
(5a)?*??/apple companyc??/new product
(5b) the new product of (or from) Apple
The Chinese DE constructions are also often
translated into prepositions in English.
cc and punct The Chinese sentences contain more
punctuation (punct) while the English translation
has more conjunctions (cc), because English uses
conjunctions to link clauses (?and? in (6b)) while
Chinese tends to use only punctuation (?,? in (6a)).
(6a) YJ/these?=/city??/social?/economic
0/development??/rapid??0/local
?/economic"?/strength?/clearly
/enhance
(6b) In these municipalities the social and economic de-
velopment has been rapid, and the local economic
strength has clearly been enhanced
rcmod and ccomp There are more rcmod and
ccomp in the Chinese sentences and less in the En-
glish translation, because of the following reasons:
1. Some English adjectives act as verbs in Chi-
nese. For example, c (new) is an adjectival
predicate in Chinese and the relation between
c (new) and ?? (system) is rcmod. But
?new? is an adjective in English and the En-
glish relation between ?new? and ?system? is
amod. This difference contributes to more rc-
mod in Chinese.
(7a)c/new{/(DE)X=/verify and write off
(7b) a new sales verification system
2. Chinese has two special verbs (VC): 4 (SHI)
and ? (WEI) which English doesn?t use. For
55
abbreviation short description Chinese example typed dependency counts percentagenn noun compound modifier q??e nn(?e,q?) 13278 15.48%punct punctuation 0:,?? punct(,?,?) 10896 12.71%nsubj nominal subject ?? nsubj(,??) 5893 6.87%conj conjunct (links two conjuncts) ??Z?a? conj(?a?,??) 5438 6.34%dobj direct object ???Y??G?G dobj(?Y,?G) 5221 6.09%advmod adverbial modifier \????G advmod(??,) 4231 4.93%prep prepositional modifier ?"B??Zq? prep(q?,?) 3138 3.66%nummod number modifier ?G?G nummod(G,?) 2885 3.36%amod adjectival modifier J-?? amod(??,J-) 2691 3.14%pobj prepositional object ???? pobj(??,?) 2417 2.82%rcmod relative clause modifier X?t,{<Y rcmod(<Y,?t) 2348 2.74%cpm complementizer ??{??? cpm(,{) 2013 2.35%assm associative marker ?{?? assm(?,{) 1969 2.30%assmod associative modifier ?{?? assmod(??,?) 1941 2.26%cc coordinating conjunction ??Z?a? cc(?a?,Z) 1763 2.06%clf classifier modifier ?G?G clf(?G,G) 1558 1.82%ccomp clausal complement Uq??Rzf~?? ccomp(??,Rz) 1113 1.30%det determiner YJ??? det(??,YJ) 1113 1.30%lobj localizer object ?#u lobj(u,?#) 1010 1.18%range dative object that is a quantifier phrase ?b ?7?? range(?b,?) 891 1.04%asp aspect marker ??*~ asp(?,?) 857 1.00%tmod temporal modifier 1X?t, tmod(?t,1) 679 0.79%plmod localizer modifier of a preposition ?Y?yH? plmod(?,?) 630 0.73%attr attributive ?4??7?? attr(?,??) 534 0.62%mmod modal verb modifier ?Czt?F mmod(zt,) 497 0.58%loc localizer 3??1? loc(3,1?) 428 0.50%top topic O?4??? top(4,O?) 380 0.44%pccomp clausal complement of a preposition ??\??? pccomp(?,??) 374 0.44%etc etc modifier )?s? etc(?s,) 295 0.34%lccomp clausal complement of a localizer ?)?i8??{?h lccomp(?,8) 207 0.24%ordmod ordinal number modifier ????? ordmod(?,??) 199 0.23%xsubj controlling subject Uq??Rzf~?? xsubj(Rz,Uq) 192 0.22%neg negative modifier 1X?t, neg(?t,X) 186 0.22%rcomp resultative complement ???? rcomp(??,??) 176 0.21%comod coordinated verb compound modifier ?Y"q comod(?Y,"q) 150 0.17%vmod verb modifier ??|?i??0?{*~ vmod(0?,|?) 133 0.16%prtmod particles such as?,1,u, ????Rz{?? prtmod(Rz,?) 124 0.14%ba ?ba? construction ?????5=? ba(?5,?) 95 0.11%dvpm manner DE(?) modifier ?H?3? dvpm(?H,?) 73 0.09%dvpmod a ?XP+DEV(?)? phrase that modifies VP ?H?3? dvpmod(3?,?H) 69 0.08%prnmod parenthetical modifier ???-? 1990 ? 1995? prnmod(?-, 1995) 67 0.08%cop copular ?4?{? cop(?,4) 59 0.07%pass passive marker ?????b? pass(??,?) 53 0.06%nsubjpass nominal passive subject 1??*S?{?	? nsubjpass(?*,1) 14 0.02%
Table 2: Chinese grammatical relations and examples. The counts are from files 1?325 in CTB6.
example, there is an additional relation, ccomp,
between the verb4/(SHI) and\?/reduce in
(8a). The relation is not necessary in English,
since4/SHI is not translated.
(8a) /second4/(SHI)??#/1996
?)/ChinaLl?/substantially
\?/reduce{/tariff
(8b) Second, China reduced tax substantially in
1996.
conj There are more conj in Chinese than in En-
glish for three major reasons. First, sometimes one
complete Chinese sentence is translated into sev-
eral English sentences. Our conj is defined for two
grammatical roles occurring in the same sentence,
and therefore, when a sentence breaks into multiple
ones, the original relation does not apply. Second,
we define the two grammatical roles linked by the
conj relation to be in the same word class. However,
words which are in the same word class in Chinese
may not be in the same word class in English. For
example, adjective predicates act as verbs in Chi-
nese, but as adjectives in English. Third, certain con-
structions with two verbs are described differently
between the two languages: verb pairs are described
as coordinations in a serial verb construction in Chi-
nese, but as the second verb being the complement
56
of the first verb in English.
4 Experimental Results
4.1 Experimental Setting
We use various Chinese-English parallel corpora1
for both training the phrase orientation classifier, and
for extracting statistical phrases for the phrase-based
MT system. The parallel data contains 1,560,071
sentence pairs from various parallel corpora. There
are 12,259,997 words on the English side. Chi-
nese word segmentation is done by the Stanford Chi-
nese segmenter (Chang et al, 2008). After segmen-
tation, there are 11,061,792 words on the Chinese
side. The alignment is done by the Berkeley word
aligner (Liang et al, 2006) and then we symmetrized
the word alignment using the grow-diag heuristic.
For the phrase orientation classifier experiments,
we extracted labeled examples using the parallel
data and the alignment as in Figure 2. We extracted
9,194,193 total valid examples: 86.09% of them are
ordered and the other 13.91% are reversed. To eval-
uate the classifier performance, we split these exam-
ples into training, dev and test set (8 : 1 : 1). The
phrase orientation classifier used in MT experiments
is trained with all of the available labeled examples.
Our MT experiments use a re-implementation of
Moses (Koehn et al, 2003) called Phrasal, which
provides an easier API for adding features. We
use a 5-gram language model trained on the Xin-
hua and AFP sections of the Gigaword corpus
(LDC2007T40) and also the English side of all the
LDC parallel data permissible under the NIST08
rules. Documents of Gigaword released during the
epochs of MT02, MT03, MT05, and MT06 were
removed. For features in MT experiments, we in-
corporate Moses? standard eight features as well as
the lexicalized reordering features. To have a more
comparable setting with (Zens and Ney, 2006), we
also have a baseline experiment with only the stan-
dard eight features. Parameter tuning is done with
Minimum Error Rate Training (MERT) (Och, 2003).
The tuning set for MERT is the NIST MT06 data
set, which includes 1664 sentences. We evaluate the
result with MT02 (878 sentences), MT03 (919 sen-
1LDC2002E18, LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85,
LDC2002L27 and LDC2005T34.
tences), and MT05 (1082 sentences).
4.2 Phrase Orientation Classifier
Feature Sets #features Train. Acc. Train. Dev DevAcc. (%) Macro-F Acc. (%) Macro-FMajority class - 86.09 - 86.09 -Src 1483696 89.02 71.33 88.14 69.03Src+Tgt 2976108 92.47 82.52 91.29 79.80Src+Src2+Tgt 4440492 95.03 88.76 93.64 85.58Src+Src2+Tgt+PATH 4691887 96.01 91.15 94.27 87.22
Table 3: Feature engineering of the phrase orientation
classifier. Accuracy is defined as (#correctly labeled ex-
amples) divided by (#all examples). The macro-F is an
average of the accuracies of the two classes.
The basic source word features described in Sec-
tion 2 are referred to as Src, and the target word
features as Tgt. The feature set that Zens and Ney
(2006) used in their MT experiments is Src+Tgt. In
addition to that, we also experimented with source
word features Src2 which are similar to Src, but take
a window of 3 around j? instead of j. In Table 3
we can see that adding the Src2 features increased
the total number of features by almost 50%, but also
improved the performance. The PATH features add
fewer total number of features than the lexical fea-
tures, but still provide a 10% error reduction and
1.63 on the macro-F1 on the dev set. We use the best
feature sets from the feature engineering in Table 3
and test it on the test set. We get 94.28% accuracy
and 87.17 macro-F1. The overall improvement of
accuracy over the baseline is 8.19 absolute points.
4.3 MT Experiments
In the MT setting, we use the log probability from
the phrase orientation classifier as an extra feature.
The weight of this discriminative reordering feature
is also tuned by MERT, along with other Moses
features. In order to understand how much the
PATH features add value to the MT experiments, we
trained two phrase orientation classifiers with differ-
ent features: one with the Src+Src2+Tgt feature set,
and the other one with Src+Src2+Tgt+PATH. The re-
sults are listed in Table 4. We compared to two
different baselines: one is Moses8Features which
has a distance-based reordering model, the other is
Baseline which also includes lexicalized reorder-
ing features. From the table we can see that using
the discriminative reordering model with PATH fea-
tures gives significant improvement over both base-
57
Setting #MERT features MT06(tune) MT02 MT03 MT05
Moses8Features 8 31.49 31.63 31.26 30.26Moses8Features+DiscrimRereorderNoPATH 9 31.76(+0.27) 31.86(+0.23) 32.09(+0.83) 31.14(+0.88)Moses8Features+DiscrimRereorderWithPATH 9 32.34(+0.85) 32.59(+0.96) 32.70(+1.44) 31.84(+1.58)
Baseline (Moses with lexicalized reordering) 16 32.55 32.56 32.65 31.89Baseline+DiscrimRereorderNoPATH 17 32.73(+0.18) 32.58(+0.02) 32.99(+0.34) 31.80(?0.09)Baseline+DiscrimRereorderWithPATH 17 32.97(+0.42) 33.15(+0.59) 33.65(+1.00) 32.66(+0.77)
Table 4: MT experiments of different settings on various NIST MT evaluation datasets. All differences marked in bold
are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).
??? ?? ??
det
every level product
nn
? ? ?? ? ??
products of all level
??? ?? ?? ? ? ?? ? ??whole city this year industry total output value
det nn
gross industrial output value of the whole city this year
Figure 4: Two examples for the feature PATH:det-nn and
how the reordering occurs.
lines. If we use the discriminative reordering model
without PATH features and only with word features,
we still get improvement over the Moses8Features
baseline, but the MT performance is not signifi-
cantly different from Baseline which uses lexical-
ized reordering features. From Table 4 we see that
using the Src+Src2+Tgt+PATH features significantly
outperforms both baselines. Also, if we compare be-
tween Src+Src2+Tgt and Src+Src2+Tgt+PATH, the
differences are also statistically significant, which
shows the effectiveness of the path features.
4.4 Analysis: Highly-weighted Features in the
Phrase Orientation Model
There are a lot of features in the log-linear phrase
orientation model. We looked at some highly-
weighted PATH features to understand what kind
of grammatical constructions were informative for
phrase orientation. We found that many path fea-
tures corresponded to our intuitions. For example,
the feature PATH:prep-dobjR has a high weight for
being reversed. This feature informs the model that
in Chinese a PP usually appears before VP, but in
English they should be reversed. Other features
with high weights include features related to the
DE construction that is more likely to translate to
a relative clause, such as PATH:advmod-rcmod and
PATH:rcmod. They also indicate the phrases are
more likely to be chosen in reversed order. Another
frequent pattern that has not been emphasized in the
previous literature is PATH:det-nn, meaning that a
[DT NP1NP2] in Chinese is translated into English
as [NP2 DT NP1]. Examples with this feature are
in Figure 4. We can see that the important features
decided by the phrase orientation model are also im-
portant from a linguistic perspective.
5 Conclusion
We introduced a set of Chinese typed dependencies
that gives information about grammatical relations
between words, and which may be useful in other
NLP applications as well as MT. We used the typed
dependencies to build path features and used them to
improve a phrase orientation classifier. The path fea-
tures gave a 10% error reduction on the accuracy of
the classifier and 1.63 points on the macro-F1 score.
We applied the log probability as an additional fea-
ture in a phrase-based MT system, which improved
the BLEU score of the three test sets significantly
(0.59 on MT02, 1.00 on MT03 and 0.77 on MT05).
This shows that typed dependencies on the source
side are informative for the reordering component in
a phrase-based system. Whether typed dependen-
cies can lead to improvements in other syntax-based
MT systems remains a question for future research.
Acknowledgments
The authors would like to thank Marie-Catherine de
Marneffe for her help on the typed dependencies,
and Daniel Cer for building the decoder. This work
is funded by a Stanford Graduate Fellowship to the
first author and gift funding from Google for the
project ?Translating Chinese Correctly?.
58
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 224?232, Columbus, Ohio, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8, Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC-06, pages 449?454.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Demonstration Session.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of HLT-NAACL.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101?104.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 737?745, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
Association for Computational Linguistics.
59
Coling 2010: Poster Volume, pages 1318?1326,
Beijing, August 2010
Search with Synonyms: Problems and Solutions
Xing Wei, Fuchun Peng, Huishin Tseng, Yumao Lu, Xuerui Wang, Benoit Dumoulin
Yahoo! Labs	

{xwei,fuchun,huihui,yumaol,xuerui,benoitd}@yahoo-inc.com
Abstract
Search with synonyms is a challenging
problem for Web search, as it can eas-
ily cause intent drifting. In this paper,
we propose a practical solution to this is-
sue, based on co-clicked query analysis,
i.e., analyzing queries leading to clicking
the same documents. Evaluation results
on Web search queries show that syn-
onyms obtained from this approach con-
siderably outperform the thesaurus based
synonyms, such as WordNet, in terms of
keeping search intent.
1 Introduction
Synonym discovery has been an active topic in a
variety of language processing tasks (Baroni and
Bisi, 2004; Fellbaum, 1998; Lin, 1998; Pereira
et al, 1993; Sanchez and Moreno, 2005; Turney,
2001). However, due to the difficulties of syn-
onym judgment (either automatically or manu-
ally) and the uncertainty of applying synonyms
to specific applications, it is still unclear how
synonyms can help Web scale search task. Previ-
ous work in Information Retrieval (IR) has been
focusing mainly on related words (Bai et al,
2005; Wei and Croft, 2006; Riezler et al, 2008).
But Web scale data handling needs to be precise
and thus synonyms are more appropriate than re-
lated words for introducing less noise and alle-
viating the efficiency concern of query expan-
sion. In this paper, we explore both manually-
built thesaurus and automatic synonym discov-
ery, and apply a three-stage evaluation by sep-
arating synonym accuracy from relevance judg-
ment and user experience impact.
The main difficulties of discovering synonyms
for Web search are the following:
1. Synonym discovery is context sensitive.
Although there are quite a few manually built
thesauri available to provide high quality syn-
onyms (Fellbaum, 1998), most of these syn-
onyms have the same or nearly the same mean-
ing only in some senses. If we simply replace
them in search queries in all occurrences, it is
very easy to trigger search intent drifting. Thus,
Web search needs to understand different senses
encountered in different contexts. For example,
?baby? and ?infant? are treated as synonyms in
many thesauri, but ?Santa Baby? has nothing to
do with ?infant?. ?Santa Baby? is a song title,
and the meaning of ?baby? in this entity is dif-
ferent than the usual meaning of ?infant?.
2. Context can not only limit the use of syn-
onyms, but also broaden the traditional definition
of synonyms. For instance, ?dress? and ?attire?
sometimes have nearly the same meaning, even
though they are not associated with the same en-
try in many thesauri; ?free? and ?download? are
far from synonyms in traditional definition, but
?free cd rewriter? may carry the same query in-
tent as ?download cd rewriter?.
3. There are many new synonyms devel-
oped from the Web over time. ?Mp3? and
?mpeg3? were not synonyms twenty years ago;
?snp newspaper? and ?snp online? carry the
same query intent only after snponline.com was
published. Manually editing synonym list is pro-
hibitively expensive. Thus, we need an auto-
matic synonym discovery system that can learn
from huge amount of data and update the dictio-
nary frequently.
1318
In summary, synonym discovery for Web
search is different from traditional thesaurus
mining; it needs to be context sensitive and needs
to be updated timely. To address these prob-
lems, we conduct context based synonym dis-
covery from co-clicked queries, i.e., queries that
share similar document click distribution. To
show the effectiveness of our synonym discov-
ery method on Web search, we use several met-
rics to demonstrate significant improvements:
(1) synonym discovery accuracy that measures
how well it keeps the same search intent; (2)
relevance impact measured by Discounted Cu-
mulative Gain (DCG) (Jarvelin and Kekalainen.,
2002); and (3) user experience impact measured
by online experiment.
The rest of the paper is organized as follows.
In Section 2, we first discuss related work and
differentiate our work from existing work. Then
we present the details of our synonym discov-
ery approach in Section 3. In Section 4 we show
our query rewriting strategy to include synonyms
in Web search. We conduct experiments on ran-
domly sampled Web search queries and run the
three-stage evaluation in Section 5 and analyze
the results in Section 6. WordNet based syn-
onym reformulation and a current commercial
search engine are the baselines for the three-
stage evaluation respectively. Finally we con-
clude the paper in Section 7.
2 Related Works
Automatically discovering synonyms from large
corpora and dictionaries has been popular top-
ics in natural language processing (Sanchez and
Moreno, 2005; Senellart and Blondel, 2003; Tur-
ney, 2001; Blondel and Senellart, 2002; van der
Plas and Tiedemann, 2006), and hence, there has
been a fair amount of work in calculating word
similarity (Porzel and Malaka, 2004; Richardson
et al, 1998; Strube and Ponzetto, 2006; Bolle-
gala et al, 2007) for the purpose of discovering
synonyms, such as information gain on ontology
(Resnik, 1995) and distributional similarity (Lin,
1998; Lin et al, 2003). However, the definition
of synonym is application dependent and most
of the work has been applied to a specific task
(Turney, 2001) or restricted in one domain (Ba-
roni and Bisi, 2004). Synonyms extracted us-
ing these traditional approaches cannot be easily
adopted in Web search where keeping search in-
tent is critical.
Our work is also related to semantic matching
in IR: manual techniques such as using hand-
crafted thesauri and automatic techniques such
as query expansion and clustering all attempts to
provide a solution, with varying degrees of suc-
cess (Jones, 1971; van Rijsbergen, 1979; Deer-
wester et al, 1990; Liu and Croft, 2004; Bai
et al, 2005; Wei and Croft, 2006; Cao et al,
2007). These works focus mainly on adding in
loosely semantically related words to expand lit-
eral term matching. But related words may be
too coarse for Web search considering the mas-
sive data available.
3 Synonym Discovery based on
Co-clicked Queries
In this section, we discuss our approach to syn-
onym discovery based on co-clicked queries in
Web search in detail.
3.1 Co-clicked Query Clustering
Clustering has been extensively studied in many
applications, including query clustering (Wen et
al., 2002). One of the most successful tech-
niques for clustering is based on distributional
clustering (Lin, 1998; Pereira et al, 1993). We
adopt a similar approach to our co-clicked query
clustering. Each query is associated with a set
of clicked documents, which in turn associated
with the number of views and clicks. We then
compute the distance between a pair of queries
by calculating the Jensen-Shannon(JS) diver-
gence (Lin, 1991) between their clicked URL
distributions. We start with that every query
is a separate cluster, and merge clusters greed-
ily. After clusters are generated, pairs of queries
within the same cluster can be considered as
co-clicked/related queries with a similarity score
computed from their JS divergence.
Sim(qk|ql) = DJS(qk||ql) (1)
1319
3.2 Query Pair Alignment
To make sure that words are replacement for
each other in the co-clicked queries, we align
words in the co-clicked query pairs that have
the same length (number of terms), and have
the same terms for all positions except one.
This is a simplification for complicated aligning
processes. Previous work on machine transla-
tion (Brown et al, 1993) can be used when com-
plete alignment is needed for modeling. How-
ever, as we have tremendous amount of co-
clicked query data, our restricted version of
alignment is sufficient to obtain a reasonable
number of synonyms. In addition, this restricted
approach eliminates much noise introduced in
those complicated aligning processes.
3.2.1 Synonym Discovery from Co-clicked
Query Pair
Synonyms discovered from co-clicked queries
have two aspects of word meaning: (1) gen-
eral meaning in language and (2) specific mean-
ing in the query. These two aspects are related.
For example, if two words are more likely to
carry the same meaning in general, then they are
more likely to carry the same meaning in spe-
cific queries; on the other hand, if two words of-
ten carry the same meaning in a variety of spe-
cific queries, then we tend to believe that the two
words are synonyms in general language. How-
ever, neither of these two aspects can cover the
other. Synonyms in general language may not
be used to replace each other in a specific query.
For example, ?sea? and ?ocean? have nearly the
same meaning in language, but in the specific
query ?sea boss boat?, ?sea? and ?ocean? cannot
be treated as synonyms because ?sea boss? is a
brand; also, in the specific query ?women?s wed-
ding attire?, ?dress? can be viewed as a synonym
to ?attire?, but in general language, these two
words are not synonyms. Therefore, whether
two words are synonyms or not for a specific
query is a synthesis judgment based on both of
general meaning and specific context.
We develop a three-step process for synonym
discovery based on co-clicked queries, consider-
ing the above two aspects.
Step 1: Get al synonym candidates for word
wi in general meaning.
In this step, we would like to get al syn-
onym candidates for a word. This step corre-
sponds to Aspect (1) to catch the general mean-
ing of words in language. We consider all the
co-clicked queries with the word and sum over
them, as in Eq. 2
P (wj |wi) =
?
k simk(wi ? wj)?
wj
?
k sim(wi ? wj)
(2)
where simk(wi ? wj) represents the similarity
score (see Section 3.1) of a query qk that aligns
wi to wj . So intuitively, we aggregate scores of
all query pairs that align wi to wj , and normalize
it to a probability over the vocabulary.
Step 2: Get synonyms for word wi in query
qk.
In this step, we would like to get synonyms for
a word in a specific query. We define the prob-
ability of reformulating wi with wj for query qk
as the similarity score shown in Eq. 3.
P (wj |wi, qk) = simk(wi ? wj) (3)
Step 3: Combine the above two steps.
Now we have two sets of estimates for the syn-
onym probability, which is used to reformulate
wi with wj . One set of values are based on gen-
eral language information and another set of val-
ues are based on specific queries. We apply three
combination approaches to integrate the two sets
of values for a final decision of synonym dis-
covery: (1) two independent thresholds for each
probability, (2) linear combination with a coeffi-
cient, and (3) linear combination in log scale as
in Eq. 4, with ? as a mixture coefficient.
Pqk(wj |wi) ? ? log P (wj |wi)
+(1 ? ?) log P (wj |wi, qk) (4)
In experiments we found that there is no sig-
nificant difference with the results from different
combination methods by finely tuned parameter
setting.
3.2.2 Concept based Synonyms
The simple word alignment strategy we used
can only get the synonym mapping from single
1320
term to single term. But there are a lot of phrase-
to-phrase, term-to-phrase, or phrase-to-term syn-
onym mappings in language, such as ?babe in
arms? to ?infant?, and ?nyc? to ?new york city?.
We perform query segmentation on queries to
identify concept units from queries based on
an unsupervised segmentation model (Tan and
Peng, 2008). Each unit is a single word or sev-
eral consecutive words that represent a meaning-
ful concept.
4 Synonym Handling in Web Search
The automatic synonym discovery methods de-
scribed in Section 3 generate synonym pairs for
each query. A simple and straightforward way
to use the synonym pairs would be ?equalizing?
them in search, just like the ?OR? function in
most commercial search engines.
Another method would be to re-train the
whole ranking system using the synonym fea-
ture, but it is expensive and requires a large size
training set. We consider this to be future work.
Besides general equalization in all cases, we
also apply a restriction, specially, on whether or
not to allow synonyms to participate in document
selection. For the consideration of efficiency,
most Web search engines has a document selec-
tion step to pre-select a subset of documents for
full ranking. For the general equalization, the
synonym pair is treated as the same even in the
document selection round; in a conservative vari-
ation, we only use the original word for docu-
ment selection but use the synonyms in the sec-
ond phase finer ranking.
5 Experiments
In this section, we present the experimental re-
sults for our approaches with some in-depth dis-
cussion.
5.1 Evaluation Metrics
We have several metrics to evaluate the synonym
discovery system for Web search queries. They
corresponds to the three stages during the system
development. Each of them measures a different
aspect.
Stage 1: accuracy. Because we are more in-
terested in the application of reformulating Web
search queries, our guideline to the editorial
judgment focuses on the query intent change and
context-based synonyms. For example, ?trans-
porters? and ?movers? are good synonyms in
the context of ?boat? because ?boat transporters?
and ?boat movers? keep the same search intent,
but ?ocean? is not a good synonym to ?sea? in
the query of ?sea boss boats? because ?sea boss?
is a brand name and ?ocean boss? does not re-
fer to the same brand. Results are measured with
accuracy by the number of discovered synonyms
(which reflects coverage).
Stage 2: relevance. To evaluate the effec-
tiveness of our semantic features we use DCG,
a widely-used metric for measuring Web search
relevance.
Stage 3: user experience. In addition to the
search relevance, we also evaluate the practical
user experience after logging all the user search
behaviors during a two-week online experiment.
Web CTR: the Web click through rate (Sher-
man and Deighton, 2001; Lee et al, 2005) is de-
fined as
CTR = number of clicks
total page views
,
where a page view (PV) is one result page that a
search engine returns for a query.
Abandon rate: the percentage of queries that
are abandoned by user neither clicking a result
nor issuing a query refinement.
5.2 Data
A period of Web search query log with clicked
URLs are used to generate co-clicked query set.
After word alignment that extracts the co-clicked
query pairs with same number of units and with
only one different unit, we obtain 12.1M unseg-
mented query pairs and 11.9M segmented query
pairs.
Since we run a three-stage evaluation, there
are three independent evaluation set respectively:
1. accuracy test set. For the evaluation of syn-
onym discovery accuracy, we randomly sampled
42K queries from two weeks of query log, and
1321
evaluate the effectiveness of our synonym dis-
covery model with these queries. To test the syn-
onym discovery model built on the segmented
data, we segment the queries before using them
as evaluation set.
2. relevance test set. To evaluate the relevance
impact by the synonym discovery approach, we
run experiments on another two weeks of query
log and randomly sampled 1000 queries from the
affected queries (queries that have differences in
the top 5 results after synonym handling).
3. user experience test set. The user experi-
ence test is conducted online with a commercial
search engine.
5.3 Results of Synonym Discovery
Accuracy
Here we present the results of WordNet the-
saurus based query synonym discovery, co-
clicked based term-to-term query synonym dis-
covery, and co-click concept based query syn-
onym discovery.
5.3.1 Thesaurus-based Synonym
Replacement
The WordNet thesaurus-based synonym re-
placement is a baseline here. For any word that
has synonyms in the thesaurus, thesaurus-based
synonym replacement will rewrite the word with
synonyms from the thesaurus.
Although thesaurus often provides clean in-
formation, synonym replacement based on the-
saurus does not consider query context and in-
troduces too many errors and noise. Our exper-
iments show that only 46% of the discovered
synonyms are correct synonyms in query. The
accuracy is too low to be used for Web search
queries.
5.3.2 Co-clicked Query-based Context
Synonym Discovery
Here we present the results from our approach
based on co-clicked query data (in this section
the queries are all original queries without seg-
mentation). Figure 1 shows the accuracy of syn-
onyms by the number of discovered synonyms.
By applying different thresholds as cut-off lines
to Eq. 4, we get different numbers of synonyms
from the same test set. As we can see, loosening
the threshold can give us more synonym pairs,
but it could hurt the accuracy.
Figure 1: Accuracy versus number of synonyms
with term based synonym discovery
Figure 1 demonstrates how accuracy changes
with the number of synonyms. Y-axis repre-
sents the percentage of correctly discovered syn-
onyms, and X-axis represents the number of
discovered synonyms, including both of correct
ones and wrong ones. The three different lines
represents three different parameter settings of
mixture weights (? in Eq. 4, which is 0.2, 0.3,
or 0.4 in the figure). The figure shows accuracy
drops by increasing the number of synonyms.
More synonym pairs lead to lower accuracy.
From Figure 1 we can see: Firstly, three
curves with different thresholds almost over-
lap, which means the effectiveness of synonym
discovery is not very sensitive to the mixture
weight. Secondly, accuracy is monotonically de-
creasing as more synonyms are detected. By
getting more synonyms, the accuracy decreases
from 100% to less than 80% (we are not in-
terested in accuracies lower than 80% due to
the high precision requirement of Web search
tasks, so the graph contains only high-accuracy
results). This trend also confirms the effective-
ness of our approach (the accuracy for a random
approach would be a constant).
5.3.3 Concept based Context Synonym
Discovery
We present results from our model based on
segmented co-clicked query data in this section.
1322
Original Query New Query with Synonyms Intent
Examples of thesaurus-based based synonym replacement
basement window wells drainage basement window wells drain
billabong boardshorts sale billabong boardshorts sales event same
bigger stronger faster documentary larger stronger faster documentary
yahoo hayseed
maryland judiciary case search maryland judiciary pillowcase search different
free cell phone number lookup free cell earpiece number lookup
Examples of term-to-term synonym discovery
airlines jobs airlines careers
area code finder area code search same
acai berry acai fruit
acai berry acai juice
ace hardware different
crest toothpaste coupon crest whitestrips coupon
Examples of concept based synonym discovery
ae american eagle outfitters
apartments for rent apartment rentals same
arizona time zone arizona time
cortrust bank credit card cortrust bank mastercard
david beckham beckham different
dodge caliber dodge
Table 1: Examples of query synonym discovery: the first section is thesaurus based, second sec-
tion is co-clicked data based term-to-term synonym discovery, and the last section is concept based
synonym discovery.
The modeling part is the same as the one for
Section 5.3.2, and the only difference is that
the data were segmented. We have shown in
Section 5.3.2 that the mixture weight is not an
crucial factor within a reasonable range, so we
present only the result with one mixture weight
in Figure 2. As in Section 5.3.2, the figure shows
that the accuracy of synonym discovery is sensi-
tive to the threshold. It confirms that our model
is effective and setting threshold to Eq. 4 is a fea-
sible and sound way to discover not only single
term synonyms but also phrase synonyms.
Figure 2: Accuracy versus number of synonyms
with concept based synonym discovery
Table 1 shows some anecdotal examples of
query synonyms with the thesaurus-based syn-
onym replacement, context sensitive synonym
discovery, and concept based context sensitive
synonym discovery. In contrast, the upper part
of each section shows positive examples (query
intents remain the same after synonym replace-
ment) and the lower part shows negative ex-
amples (query intents change after synonym re-
placement).
5.4 Results of Relevance Impact
We run relevance test on 1000 randomly sampled
affected queries. With the automatic synonym
discovery approach we apply our synonym han-
dling method described in Section 4. Results of
DCG improvements by different thresholds and
synonym handling settings are presented in Ta-
ble 2. Thresholds are selected empirically from
the accuracy test in Section 5.3 (we run a small
size relevance test on the accuracy test set and
set the range of thresholds based on that). Note
that in our relevance experiments we use term-
to-term synonym pairs only. For the relevance
impact of concept-based synonym discovery, we
would like to study it in our future work.
1323
From Table 2 we can see that the automatic
synonym discovery approach we presented sig-
nificantly improves search relevance on various
settings, which confirms the effectiveness of our
synonym discovery for Web search queries. We
conjecture that avoiding synonym in document
selection is of help. This is because precision is
more important to Web search than recall for the
huge amount of data available on the Web.
Relevance impact with synonym handling
doc-selection
threshold1 threshold2 participation DCG
0.8 0.02 no +1.7%
0.8 0.02 yes +1.3%
0.8 0.05 no +1.8%
0.8 0.05 yes +1.4%
Table 2: Relevance impact with synonym han-
dling by different parameter settings. ?Thresh-
old1? is the threshold for context-based similar-
ity score?Eq. 3; ?threshold2? is the threshold
for general case similarity score?Eq. 2; ?doc-
selection participation? refers to whether or not
let synonym handling participate in document
selection. All improvements are statistically sig-
nificant by Wilcox significance test.
5.5 Results of User Experience Impact
In addition to the relevance impact, we also eval-
uated the practical user experience impact by
CTR and abandon rate (defined in Section 5.1)
through a two-week online run. Results show
that the synonym discovery method presented in
this paper improves Web CTR by 2%, and de-
creases abandon rate by 11.4%. All changes
are statistically significant, which indicates syn-
onyms are indeed beneficial to user experience.
6 Discussion and Error Analysis
From Table 1, we can see that our approach can
catch not only traditional synonyms, which are
the synonyms that can be found in manually-
built thesaurus, but also context-based syn-
onyms, which may not be treated as synonyms
in a standard dictionary or thesaurus. There are
a variety of synonyms our approach discovered:
1. Synonyms that are not considered as syn-
onyms in traditional thesaurus, such as ?berry?
and ?fruit? in the context of ?acai?. ?acai berry?
and ?acai fruit? refer to the same fruit.
2. Synonyms that have different part-of-
speech features than the corresponding original
words, such as ?finder? and ?search?. Users
searching ?area code finder? and users search-
ing ?area code search? are looking for the same
content. In the context of Web search queries,
part-of-speech is not an important factor as most
queries are not grammatically perfect.
3. Synonyms that show up in recent concepts,
such as ?webmail? and ?email? in the context
of ?cox?. The new concept of ?webmail? or
?email? has not been added to many thesauri yet.
4. Synonyms not limited by length, such as
?crossword puzzles? and ?crossword?, ?homes
for sale? and ?real estate?. The segmenter
helps our system discover synonyms in various
lengths.
With these many variations, the synonyms dis-
covered by our approach are not the ?synonyms?
in the traditional meaning. They are context sen-
sitive, Web data oriented and search effective
synonyms. These synonyms are discovered by
the statistical model we presented and based on
Web search queries and clicked data.
However, the click data themselves contain a
huge amount of noise. Although they can re-
flect the users? intents in some big picture, in
many specific cases synonyms discovered from
co-clicked data are biased by the click noise. In
our application?Web search query reformula-
tion with synonyms, accuracy is the most im-
portant thing and thus we are interested in er-
ror analysis. The errors that our model makes
in synonym discovery are mainly caused by the
following reasons:
(1) There are some concepts well accepted
such as ?cnn? means ?news? and ?amtrak?
means ?train?. And users searching ?news? tend
to click CNN Web site; users searching ?train?
tend to click Amtrak Web site. With our model,
?cnn? and ?news?, ?amtrak? and ?train? are dis-
covered to be synonyms, which may hurt the
search of ?news? or ?train? in general meaning.
1324
(2) Same clicks by different intents. Although
clicking on same documents generally indicates
same search intent, different intents could re-
sult in same or similar clicks, too. For exam-
ple, the queries of ?antique style wedding rings?
and ?antique style engagement rings? carry dif-
ferent intents, but very usually, these two differ-
ent intents lead to the clicks on the same Web
site. ?Booster seats? and ?car seats?, ?brighton
handbags? and ?brighton shoes? are other two
examples in the same case. For these examples,
clicking on Web URLs are not precise enough
to reflect the subtle difference of language con-
cepts.
(3) Bias from dominant user intents. Most
people searching ?apartment? are looking for an
apartment to rent. So ?apartment for rent? and
?apartment? have similar clicked URLs. But
these two are not synonyms in language. In these
cases, popular user intents dominate and bias the
meaning of language, which causes problems.
?Airline baggage restrictions? and ?airline travel
restrictions? is another example.
(4) Antonyms. Many context-based synonym
discovery methods suffer from the antonym
problem, because antonyms can have very simi-
lar contexts. In our model, the problem has been
reduced by integrating clicked-URLs. But still,
there are some examples, such as ?spyware? and
?antispyware?, resulting in similar clicks. To
learn how to ?protect a Web site?, a user often
needs to learn what are the main methods to ?at-
tack a Web site?, and these different-intent pairs
lead to the same clicks because different intents
do not have to mean different interests in many
specific cases.
Although these problems are not common, but
when they happen, they cause a bad user search
experience. We believe a solution to these prob-
lems might need more advanced linguistic anal-
ysis.
7 Conclusions
In this paper, we have developed a synonym dis-
covery approach based on co-clicked query data,
and improved search relevance and user experi-
ence significantly based on the approach.
For future work, we are investigating more
synonym handling methods to further improve
the synonym discovery accuracy, and to handle
the discovered synonyms in more ways than just
the query side.
References
Bai, J., D. Song, P. Bruza, J.Y. Nie, and G. Cao.
2005. Query Expansion using Term Relationships
in Language Models for Information Retrieval. In
Proceedings of the ACM 14th Conference on In-
formation and Knowledge Management.
Baroni, M. and S. Bisi. 2004. Using Cooccurrence
Statistics and the Web to Discover Synonyms in a
Technical Language. In LREC.
Blondel, V. and P. Senellart. 2002. Automatic Ex-
traction of Synonyms in a Dictionary. In Proc. of
the SIAM Workshop on Text Mining.
Bollegala, D., Y. Matsuo, and M. Ishizuka. 2007.
Measuring Semantic Similarity betweenWords us-
ing Web Search Engines. In Proceedings of the
16th international conference on World Wide Web
(WWW).
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statis-
tical Machine Translation: Parameter Estimation.
Computational Linguistics, 19(2):263.
Cao, G., J.Y. Nie, and J. Bai. 2007. Using Markov
Chains to Exploit Word Relationships in Informa-
tion Retrieval. In Proceedings of the 8th Confer-
ence on Large-Scale Semantic Access to Content.
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
Latent Semantic Analysis. Journal of the Amer-
ican Society for Information Science, 41(6):391?
407.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, Mass.
Jarvelin, K. and J. Kekalainen. 2002. Cumulated
Gain-Based Evaluation Evaluation of IR Tech-
niques. ACM TOIS, 20:422?446.
Jones, K. S., 1971. Automatic Keyword Classification
for Information Retrieval. London: Butterworths.
Lee, Uichin, Zhenyu Liu, and Junghoo Cho. 2005.
Automatic Identification of User Goals in Web
Search. In In the World-Wide Web Conference
(WWW).
1325
Lin, D., S. Zhao, L. Qin, and M. Zhou. 2003. Iden-
tifying Synonyms among Distributionally Similar
Words. In Proceedings of International Joint Con-
ference on Artificial Intelligence (IJCAI).
Lin, J. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(1):145?151.
Lin, D. 1998. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of COLING/ACL-
98, pages 768?774.
Liu, X. and B. Croft. 2004. Cluster-based Retrieval
using LanguageModels. In Proceedings of SIGIR.
Pereira, F., N. Tishby, and L. Lee. 1993. Distribu-
tional Clustering of English Words. In Proceed-
ings of ACL, pages 183 ? 190.
Porzel, R. and R. Malaka. 2004. A Task-based Ap-
proach for Ontology Evaluation. In ECAI Work-
shop on Ontology Learning and Population.
Resnik, P. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Pro-
ceedings of IJCAI-95, pages 448 ? 453.
Richardson, S., W. Dolan, and L. Vanderwende.
1998. MindNet: Acquiring and Structuring Se-
mantic Information from Text. In 36th Annual
meeting of the Association for Computational Lin-
guistics.
Riezler, Stefan, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Im-
proved Query Expansion. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING?08).
Sanchez, D. and A. Moreno. 2005. Automatic Dis-
covery of Synonyms and Lexicalizations from the
Web. In Proceedings of the 8th Catalan Confer-
ence on Artificial Intelligence.
Senellart, P. and V. D. Blondel. 2003. Automatic
Discovery of Similar Words. In Berry, M., editor,
A Comprehensive Survey of Text Mining. Springer-
Verlag, New York.
Sherman, L. and J. Deighton. 2001. Banner ad-
vertising: Measuring effectiveness and optimiz-
ing placement. Journal of Interactive Marketing,
15(2):60?64.
Strube, M. and S. P. Ponzetto. 2006. WikiRe-
late! Computing Semantic Relatedness Using
Wikipedia. In Proceedings of AAAI.
Tan, B. and F. Peng. 2008. Unsupervised Query Seg-
mentation using Generative Language Models and
Wikipedia. In Proceedings of the 17th Interna-
tional World Wide Web Conference (WWW), pages
347?356.
Turney, P. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning.
van der Plas, Lonneke and Jorg Tiedemann. 2006.
Finding Synonyms using Automatic Word Align-
ment and Measures of Distributional Similarity.
In Proceedings of the COLING/ACL 2006, pages
866?873.
van Rijsbergen, C.J., 1979. Information Retrieval.
London: Butterworths.
Wei, X. and W. B. Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceed-
ings of SIGIR, pages 178?185.
Wen, J.R., J.Y. Nie, and H.J. Zhang. 2002. Query
Clustering Using User Logs. ACM Transactions
on Information Systems, 20(1):59?81.
1326
