Looking Under the Hood: Tools for Diagnosing Your Question
Answering Engine
Eric Breck?, Marc Light?, Gideon S. Mann?, Ellen Riloff?,
Brianne Brown?, Pranav Anand?, Mats Rooth?, Michael Thelen?
? The MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730, {ebreck,light}@mitre.org
? Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, gsm@cs.jhu.edu
? School of Computing, University of Utah, Salt Lake City, UT 84112, {riloff,thelenm}@cs.utah.edu
? Bryn Mawr College, Bryn Mawr, PA 19010, bbrown@brynmawr.edu
? Department of Mathematics, Harvard University, Cambridge, MA 02138, anand@fas.harvard.edu
? Department of Linguistics, Cornell University, Ithaca, NY 14853, mr249@cornell.edu
Abstract
In this paper we analyze two question
answering tasks : the TREC-8 ques-
tion answering task and a set of reading
comprehension exams. First, we show
that Q/A systems perform better when
there are multiple answer opportunities
per question. Next, we analyze com-
mon approaches to two subproblems:
term overlap for answer sentence iden-
tification, and answer typing for short
answer extraction. We present general
tools for analyzing the strengths and
limitations of techniques for these sub-
problems. Our results quantify the limi-
tations of both term overlap and answer
typing to distinguish between compet-
ing answer candidates.
1 Introduction
When building a system to perform a task, the
most important statistic is the performance on
an end-to-end evaluation. For the task of open-
domain question answering against text collec-
tions, there have been two large-scale end-to-
end evaluations: (TREC-8 Proceedings, 1999)
and (TREC-9 Proceedings, 2000). In addition, a
number of researchers have built systems to take
reading comprehension examinations designed to
evaluate children?s reading levels (Charniak et al,
2000; Hirschman et al, 1999; Ng et al, 2000;
Riloff and Thelen, 2000; Wang et al, 2000). The
performance statistics have been useful for deter-
mining how well techniques work.
However, raw performance statistics are not
enough. If the score is low, we need to under-
stand what went wrong and how to fix it. If the
score is high, it is important to understand why.
For example, performance may be dependent on
characteristics of the current test set and would
not carry over to a new domain. It would also be
useful to know if there is a particular character-
istic of the system that is central. If so, then the
system can be streamlined and simplified.
In this paper, we explore ways of gaining
insight into question answering system perfor-
mance. First, we analyze the impact of having
multiple answer opportunities for a question. We
found that TREC-8 Q/A systems performed bet-
ter on questions that had multiple answer oppor-
tunities in the document collection. Second, we
present a variety of graphs to visualize and ana-
lyze functions for ranking sentences. The graphs
revealed that relative score instead of absolute
score is paramount. Third, we introduce bounds
on functions that use term overlap1 to rank sen-
tences. Fourth, we compute the expected score of
a hypothetical Q/A system that correctly identifies
the answer type for a question and correctly iden-
tifies all entities of that type in answer sentences.
We found that a surprising amount of ambiguity
remains because sentences often contain multiple
entities of the same type.
1Throughout the text, we use ?overlap? to refer to the
intersection of sets of words, most often the words in the
question and the words in a sentence.
2 The data
The experiments in Sections 3, 4, and 5 were per-
formed on two question answering data sets: (1)
the TREC-8 Question Answering Track data set
and (2) the CBC reading comprehension data set.
We will briefly describe each of these data sets
and their corresponding tasks.
The task of the TREC-8 Question Answering
track was to find the answer to 198 questions us-
ing a document collection consisting of roughly
500,000 newswire documents. For each question,
systems were allowed to return a ranked list of
5 short (either 50-character or 250-character) re-
sponses. As a service to track participants, AT&T
provided top documents returned by their retrieval
engine for each of the TREC questions. Sec-
tions 4 and 5 present analyses that use all sen-
tences in the top 10 of these documents. Each
sentence is classified as correct or incorrect auto-
matically. This automatic classification judges a
sentence to be correct if it contains at least half
of the stemmed, content-words in the answer key.
We have compared this automatic evaluation to
the TREC-8 QA track assessors and found it to
agree 93-95% of the time (Breck et al, 2000).
The CBC data set was created for the Johns
Hopkins Summer 2000 Workshop on Reading
Comprehension. Texts were collected from the
Canadian Broadcasting Corporation web page for
kids (http://cbc4kids.ca/). They are an average
of 24 sentences long. The stories were adapted
from newswire texts to be appropriate for ado-
lescent children, and most fall into the follow-
ing domains: politics, health, education, science,
human interest, disaster, sports, business, crime,
war, entertainment, and environment. For each
CBC story, 8-12 questions and an answer key
were generated.2 We used a 650 question sub-
set of the data and their corresponding 75 stories.
The answer candidates for each question in this
data set were all sentences in the document. The
sentences were scored against the answer key by
the automatic method described previously.
2This work was performed by Lisa Ferro and Tim Bevins
of the MITRE Corporation. Dr. Ferro has professional expe-
rience writing questions for reading comprehension exams
and led the question writing effort.
3 Analyzing the number of answer
opportunities per question
In this section we explore the impact of multiple
answer opportunities on end-to-end system per-
formance. A question may have multiple answers
for two reasons: (1) there is more than one differ-
ent answer to the question, and (2) there may be
multiple instances of each answer. For example,
?What does the Peugeot company manufacture??
can be answered by trucks, cars, or motors and
each of these answers may occur in many sen-
tences that provide enough context to answer the
question. The table insert in Figure 1 shows that,
on average, there are 7 answer occurrences per
question in the TREC-8 collection.3 In contrast,
there are only 1.25 answer occurrences in a CBC
document. The number of answer occurrences
varies widely, as illustrated by the standard devia-
tions. The median shows an answer frequency of
3 for TREC and 1 for CBC, which perhaps gives
a more realistic sense of the degree of answer fre-
quency for most questions.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5 6 7 9 1 2 1 4 1 8 2 7 2 8 6 1 6 7
# Answers
%
 Q
ue
st
io
ns
TREC-8
5 0
3 5 2
7.04
3
12.94
CBC
2 1 9
2 7 4
1.25
1
0.61
# Questions
# Answers
Mean
Median
Standard Dev.
Figure 1: Frequency of answers in the TREC-8
(black bars) and CBC (white bars) data sets
To gather this data we manually reviewed 50
randomly chosen TREC-8 questions and identi-
fied all answers to these questions in our text col-
lection. We defined an ?answer? as a text frag-
ment that contains the answer string in a context
sufficient to answer the question. Figure 1 shows
the resulting graph. The x-axis displays the num-
ber of answer occurrences found in the text col-
lection per question and the y-axis shows the per-
3We would like to thank John Burger and John Aberdeen
for help preparing Figure 1.
centage of questions that had x answers. For ex-
ample, 26% of the TREC-8 questions had only
1 answer occurrence, and 20% of the TREC-8
questions had exactly 2 answer occurrences (the
black bars). The most prolific question had 67
answer occurrences (the Peugeot example men-
tioned above). Figure 1 also shows the analysis
of 219 CBC questions. In contrast, 80% of the
CBC questions had only 1 answer occurrence in
the targeted document, and 16% had exactly 2 an-
swer occurrences.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 1 0 2 0 3 0 4 0 5 0 6 0 7 0
# answers occurences per question
%
 o
f s
ys
te
m
s 
w
ith
 a
t l
ea
st
 o
ne
 c
or
re
ct
 re
sp
on
se
Point per question
Mean correct per occurrence #
Figure 2: Answer repetition vs. system response
correctness for TREC-8
Figure 2 shows the effect that multiple answer
opportunities had on the performance of TREC-8
systems. Each solid dot in the scatter plot repre-
sents one of the 50 questions we examined.4 The
x-axis shows the number of answer opportunities
for the question, and the y-axis represents the per-
centage of systems that generated a correct an-
swer5 for the question. E.g., for the question with
67 answer occurrences, 80% of the systems pro-
duced a correct answer. In contrast, many ques-
tions had a single answer occurrence and the per-
centage of systems that got those correct varied
from about 2% to 60%.
The circles in Figure 2 represent the average
percentage of systems that answered questions
correctly for all questions with the same number
of answer occurrences. For example, on average
about 27% of the systems produced a correct an-
swer for questions that had exactly one answer oc-
4We would like to thank Lynette Hirschman for suggest-
ing the analysis behind Figure 2 and John Burger for help
with the analysis and presentation.
5For this analysis, we say that a system generated a cor-
rect answer if a correct answer was in its response set.
currence, but about 50% of the systems produced
a correct answer for questions with 7 answer op-
portunities. Overall, a clear pattern emerges: the
performance of TREC-8 systems was strongly
correlated with the number of answer opportuni-
ties present in the document collection.
4 Graphs for analyzing scoring
functions of answer candidates
Most question answering systems generate sev-
eral answer candidates and rank them by defin-
ing a scoring function that maps answer candi-
dates to a range of numbers. In this section,
we analyze one particular scoring function: term
overlap between the question and answer can-
didate. The techniques we use can be easily
applied to other scoring functions as well (e.g.,
weighted term overlap, partial unification of sen-
tence parses, weighted abduction score, etc.). The
answer candidates we consider are the sentences
from the documents.
The expected performance of a system that
ranks all sentences using term overlap is 35% for
the TREC-8 data. This number is an expected
score because of ties: correct and incorrect can-
didates may have the same term overlap score. If
ties are broken optimally, the best possible score
(maximum) would be 54%. If ties are broken
maximally suboptimally, the worst possible score
(minimum) would be 24%. The corresponding
scores on the CBC data are 58% expected, 69%
maximum, and 51% minimum. We would like to
understand why the term overlap scoring function
works as well as it does and what can be done to
improve it.
Figures 3 and 4 compare correct candidates and
incorrect candidates with respect to the scoring
function. The x-axis plots the range of the scor-
ing function, i.e., the amount of overlap. The
y-axis represents Pr(overlap=x | correct) and
Pr(overlap=x | incorrect), where separate curves
are plotted for correct and incorrect candidates.
The probabilities are generated by normalizing
the number of correct/incorrect answer candidates
with a particular overlap score by the total number
of correct/incorrect candidates, respectively.
Figure 3 illustrates that the correct candidates
for TREC-8 have term overlap scores distributed
between 0 and 10 with a peak of 24% at an over-
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0 2 4 6 8 10 12 14 16 18 20N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
overlap
incorrect
correct
Figure 3: Pr(overlap=x|[in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 5 10 15 20 25 30No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
overlap
incorrect
correct
Figure 4: Pr(overlap=x|[in]correct) for CBC
lap of 2. However, the incorrect candidates have
a similar distribution between 0 and 8 with a peak
of 32% at an overlap of 0. The similarity of the
curves illustrates that it is unclear how to use the
score to decide if a candidate is correct or not.
Certainly no static threshold above which a can-
didate is deemed correct will work. Yet the ex-
pected score of our TREC term overlap system
was 35%, which is much higher than a random
baseline which would get an expected score of
less than 3% because there are over 40 sentences
on average in newswire documents.6
After inspecting some of the data directly, we
posited that it was not the absolute term overlap
that was important for judging candidate but how
the overlap score compares to the scores of other
candidates. To visualize this, we generated new
graphs by plotting the rank of a candidate?s score
6We also tried dividing the term overlap score by the
length of the question to normalize for query length but did
not find that the graph was any more helpful.
on the x-axis. For example, the candidate with
the highest score would be ranked first, the can-
didate with the second highest score would be
ranked second, etc. Figures 5 and 6 show these
graphs, which display Pr(rank=x | correct) and
Pr(rank=x | incorrect) on the y-axis. The top-
ranked candidate has rank=0.
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
-
10
00
-
90
0
-
80
0
-
70
0
-
60
0
-
50
0
-
40
0
-
30
0
-
20
0
-
10
0 0
N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
ranked overlap
incorrect
correct
Figure 5: Pr(rank=x | [in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
-45 -40 -35 -30 -25 -20 -15 -10 -5 0No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
ranked overlap
incorrect
correct
Figure 6: Pr(rank=x | [in]correct) for CBC
The ranked graphs are more revealing than the
graphs of absolute scores: the probability of a
high rank is greater for correct answers than in-
correct ones. Now we can begin to understand
why the term overlap scoring function worked as
well as it did. We see that, unlike classification
tasks, there is no good threshold for our scor-
ing function. Instead relative score is paramount.
Systems such as (Ng et al, 2000) make explicit
use of relative rank in their algorithms and now
we understand why this is effective.
Before we leave the topic of graphing scoring
functions, we want to introduce one other view of
the data. Figure 7 plots term overlap scores on
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
0 2 4 6 8 10 12 14
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
lo
g-
od
ds
 o
f c
or
re
ct
ne
ss
m
a
ss
overlap
log-odds
mass curve
Figure 7: TREC-8 log odds correct given overlap
the x-axis and the log odds of being correct given
a score on the y-axis. The log odds formula is:
log Pr(correct|overlap)Pr(incorrect|overlap)
Intuitively, this graph shows how much more
likely a sentence is to be correct versus incorrect
given a particular score. A second curve, labeled
?mass,? plots the number of answer candidates
with each score. Figure 7 shows that the odds of
being correct are negative until an overlap of 10,
but the mass curve reveals that few answer candi-
dates have an overlap score greater than 6.
5 Bounds on scoring functions that use
term overlap
The scoring function used in the previous sec-
tion simply counts the number of terms shared
by a question and a sentence. One obvious mod-
ification is to weight some terms more heavily
than others. We tried using inverse document fre-
quence based (IDF) term weighting on the CBC
data but found that it did not improve perfor-
mance. The graph analogous to Figure 6 but with
IDF term weighting was virtually identical.
Could another weighting scheme perform bet-
ter? How well could an optimal weighting
scheme do? How poorly would the maximally
suboptimal scheme do? The analysis in this sec-
tion addresses these questions. In essence the an-
swer is the following: the question and the can-
didate answers are typically short and thus the
number of overlapping terms is small ? conse-
quently, many candidate answers have exactly the
same overlapping terms and no weighting scheme
could differentiate them. In addition, subset rela-
tions often hold between overlaps. A candidate
whose overlap is a subset of a second candidate
cannot score higher regardless of the weighting
scheme.7 We formalize these overlap set relations
and then calculate statistics based on them for the
CBC and TREC data.
Question: How much was Babe Belanger paid to play
amateur basketball?
S1: She was a member of the winningest
basketball team Canada ever had.
S2: Babe Belanger never made a cent for her
skills.
S3: They were just a group of young women
from the same school who liked to
play amateur basketball.
S4: Babe Belanger played with the Grads from
1929 to 1937.
S5: Babe never talked about her fabulous career.
MaxOsets : ( {S2, S4}, {S3} )
Figure 8: Example of Overlap Sets from CBC
Figure 8 presents an example from the CBC
data. The four overlap sets are (i) Babe Belanger,
(ii) basketball, (iii) play amateur basketball, and
(iv) Babe. In any term-weighting scheme with
positive weights, a sentence containing the words
Babe Belanger will have a higher score than sen-
tences containing just Babe, and sentences with
play amateur basketball will have a higher score
than those with just basketball. However, we can-
not generalize with respect to the relative scores
of sentences containing Babe Belanger and those
containing play amateur basketball because some
terms may have higher weights than others.
The most we can say is that the highest scor-
ing candidate must be a member of {S2, S4} or
{S3}. S5 and S1 cannot be ranked highest be-
cause their overlap sets are a proper subset of
competing overlap sets. The correct answer is
S2 so an optimal weighting scheme would have
a 50% chance of ranking S2 first, assuming that
it identified the correct overlap set {S2, S4} and
then randomly chose between S2 and S4. A max-
imally suboptimal weighting scheme could rank
S2 no lower than third.
We will formalize these concepts using the fol-
lowing variables:
7Assuming that all term weights are positive.
q: a question (a set of words)
s: a sentence (a set of words)
w,v: sets of intersecting words
We define an overlap set (ow,q) to be a set of
sentences (answer candidates) that have the same
words overlapping with the question. We define a
maximal overlap set (Mq) as an overlap set that is
not a subset of any other overlap set for the ques-
tion. For simplicity, we will refer to a maximal
overlap set as a MaxOset.
ow,q = {s|s ? q = w}
?q = all unique overlap sets for q
maximal(ow,q) if ?ov,q ? ?q, w 6? v
Mq = {ow,q ? ?q | maximal(ow,q)}
Cq = {s|s correctly answers q}
We can use these definitions to give upper
and lower bounds on the performance of term-
weighting functions on our two data sets. Table 1
shows the results. The max statistic is the per-
centage of questions for which at least one mem-
ber of its MaxOsets is correct. The min statis-
tic is the percentage of questions for which all
candidates of all of its MaxOsets are correct (i.e.,
there is no way to pick a wrong answer). Finally
the expectedmax is a slightly more realistic up-
per bound. It is equivalent to randomly choosing
among members of the ?best? maximal overlap
set, i.e., the MaxOset that has the highest percent-
age of correct members. Formally, the statistics
for a set of questions Q are computed as:
max = |{q|?o ? Mq,?s ? o s.t. s ? Cq}||Q|
min = |{q|?o ? Mq,?s ? o s ? Cq}||Q|
exp. max = 1|Q| ?
?
q?Q
max
o?Mq
|{s ? o and s ? Cq}|
|o|
The results for the TREC data are considerably
lower than the results for the CBC data. One ex-
planation may be that in the CBC data, only sen-
tences from one document containing the answer
are considered. In the TREC data, as in the TREC
task, it is not known beforehand which docu-
ments contain answers, so irrelevant documents
exp. max max min
CBC training 72.7% 79.0% 24.4%
TREC-8 48.8% 64.7% 10.1%
Table 1: Maximum overlap analysis of scores
may contain high-scoring sentences that distract
from the correct sentences.
In Table 2, we present a detailed breakdown
of the MaxOset results for the CBC data. (Note
that the classifications overlap, e.g., questions that
are in ?there is always a chance to get it right?
are also in the class ?there may be a chance to
get it right.?) 21% of the questions are literally
impossible to get right using only term weight-
ing because none of the correct sentences are in
the MaxOsets. This result illustrates that maxi-
mal overlap sets can identify the limitations of a
scoring function by recognizing that some candi-
dates will always be ranked higher than others.
Although our analysis only considered term over-
lap as a scoring function, maximal overlap sets
could be used to evaluate other scoring functions
as well, for example overlap sets based on seman-
tic classes rather than lexical items.
In sum, the upper bound for term weighting
schemes is quite low and the lower bound is
quite high. These results suggest that methods
such as query expansion are essential to increase
the feature sets used to score answer candidates.
Richer feature sets could distinguish candidates
that would otherwise be represented by the same
features and therefore would inevitably receive
the same score.
6 Analyzing the effect of multiple
answer type occurrences in a sentence
In this section, we analyze the problem of extract-
ing short answers from a sentence. Many Q/A
systems first decide what answer type a question
expects and then identify instances of that type in
sentences. A scoring function ranks the possible
answers using additional criteria, which may in-
clude features of the surrounding sentence such
as term overlap with the question.
For our analysis, we will assume that two short
answers that have the same answer type and come
from the same sentence are indistinguishable to
the system. This assumption is made by many
number of percentage
questions of questions
Impossible to get it wrong 159 24%
(?ow ? Mq, ?s ? ow, s ? Cq)
There is always a chance to get it right 45 7%
(?ow ? Mq, ?s ? ow s.t. s ? Cq)
There may be a chance to get it right 310 48%
(?ow ? Mq s.t. ?s ? ow s.t. s ? Cq)
The wrong answers will always be weighted too highly 137 21%
(?ow ? Mq, ?s ? ow, s 6? Cq)
There are no correct answers with any overlap with Q 66 10%
(?s ? d, s is incorrect or s has 0 overlap)
There are no correct answers (auto scoring error) 12 2%
(?s ? d, s is incorrect)
Table 2: Maximal Overlap Set Analysis for CBC data
Q/A systems: they do not have features that can
prefer one entity over another of the same type in
the same sentence.
We manually annotated data for 165 TREC-
9 questions and 186 CBC questions to indicate
perfect question typing, perfect answer sentence
identification, and perfect semantic tagging. Us-
ing these annotations, we measured how much
?answer confusion? remains if an oracle gives you
the correct question type, a sentence containing
the answer, and correctly tags all entities in the
sentence that match the question type. For exam-
ple, the oracle tells you that the question expects
a person, gives you a sentence containing the cor-
rect person, and tags all person entities in that sen-
tence. The one thing the oracle does not tell you
is which person is the correct one.
Table 3 shows the answer types that we used.
Most of the types are fairly standard, except for
the Defaultnp and Defaultvp which are default
tags for questions that desire a noun phrase or
verb phrase but cannot be more precisely typed.
We computed an expected score for this hy-
pothetical system as follows: for each question,
we divided the number of correct candidates (usu-
ally one) by the total number of candidates of the
same answer type in the sentence. For example,
if a question expects a Location as an answer and
the sentence contains three locations, then the ex-
pected accuracy of the system would be 1/3 be-
cause the system must choose among the loca-
tions randomly. When multiple sentences contain
a correct answer, we aggregated the sentences. Fi-
nally, we averaged this expected accuracy across
all questions for each answer type.
TREC CBC
Answer Type Score Freq Score Freq
defaultnp .33 47 .25 28
organization .50 1 .72 3
length .50 1 .75 2
thingname .58 14 .50 1
quantity .58 13 .77 14
agent .63 19 .40 23
location .70 24 .68 29
personname .72 11 .83 13
city .73 3 n/a 0
defaultvp .75 2 .42 15
temporal .78 16 .75 26
personnoun .79 7 .53 5
duration 1.0 3 .67 4
province 1.0 2 1.0 2
area 1.0 1 n/a 0
day 1.0 1 n/a 0
title n/a 0 .50 1
person n/a 0 .67 3
money n/a 0 .88 8
ambigbig n/a 0 .88 4
age n/a 0 1.0 2
comparison n/a 0 1.0 1
mass n/a 0 1.0 1
measure n/a 0 1.0 1
Overall .59 165 .61 186
Overall-dflts .69 116 .70 143
Table 3: Expected scores and frequencies for each
answer type
Table 3 shows that a system with perfect ques-
tion typing, perfect answer sentence identifica-
tion, and perfect semantic tagging would still
achieve only 59% accuracy on the TREC-9 data.
These results reveal that there are often multi-
ple candidates of the same type in a sentence.
For example, Temporal questions received an ex-
pected score of 78% because there was usually
only one date expression per sentence (the correct
one), while Default NP questions yielded an ex-
pected score of 25% because there were four noun
phrases per question on average. Some common
types were particularly problematic. Agent ques-
tions (most Who questions) had an answer con-
fusability of 0.63, while Quantity questions had a
confusability of 0.58.
The CBC data showed a similar level of an-
swer confusion, with an expected score of 61%,
although the confusability of individual answer
types varied from TREC. For example, Agent
questions were even more difficult, receiving a
score of 40%, but Quantity questions were easier
receiving a score of 77%.
Perhaps a better question analyzer could assign
more specific types to the Default NP and De-
fault VP questions, which skew the results. The
Overall-dflts row of Table 3 shows the expected
scores without these types, which is still about
70% so a great deal of answer confusion remains
even without those questions. The confusability
analysis provides insight into the limitations of
the answer type set, and may be useful for com-
paring the effectiveness of different answer type
sets (somewhat analogous to the use of grammar
perplexity in speech research).
Q1: What city is Massachusetts General Hospital located
in?
A1: It was conducted by a cooperative group of on-
cologists from Hoag, Massachusetts General Hospital
in Boston, Dartmouth College in New Hampshire, UC
San Diego Medical Center, McGill University in Montreal
and the University of Missouri in Columbia.
Q2: When was Nostradamus born?
A2: Mosley said followers of Nostradamus, who lived
from 1503 to 1566, have claimed ...
Figure 9: Sentences with Multiple Items of the
Same Type
However, Figure 9 shows the fundamental
problem behind answer confusability. Many sen-
tences contain multiple instances of the same
type, such as lists and ranges. In Q1, recognizing
that the question expects a city rather than a gen-
eral location is still not enough because several
cities are in the answer sentence. To achieve bet-
ter performance, Q/A systems need use features
that can more precisely target an answer.
7 Conclusion
In this paper we have presented four analyses of
question answering system performance involv-
ing: multiple answer occurence, relative score for
candidate ranking, bounds on term overlap perfor-
mance, and limitations of answer typing for short
answer extraction. We hope that both the results
and the tools we describe will be useful to others.
In general, we feel that analysis of good perfor-
mance is nearly as important as the performance
itself and that the analysis of bad performance can
be equally important.
References
E.J. Breck, J.D. Burger, L. Ferro, L. Hirschman, D. House,
M. Light, and I. Mani. 2000. How to Evaluate your
Question Answering System Every Day and Still Get
Real Work Done. In Proceedings of the Second Con-
ference on Language Resources and Evaluation (LREC-
2000).
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kos-
mala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy,
Z. Yang, S. Zeller, and L. Zorn. 2000. Reading Compre-
hension Programs in a Statistical-Language-Processing
Class. In ANLP/NAACL Workshop on Reading Com-
prehension Tests as Evaluation for Computer-Based Lan-
guage Understanding Systems.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep Read: A Reading Comprehension System. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
H.T. Ng, L.H. Teo, and J.L.P. Kwan. 2000. A Machine
Learning Approach to Answering Questions for Reading
Comprehension Tests. In Proceedings of EMNLP/VLC-
2000 at ACL-2000.
E. Riloff and M. Thelen. 2000. A Rule-based Question
Answering System for Reading Comprehension Tests.
In ANLP/NAACL Workshop on Reading Comprehension
Tests as Evaluation for Computer-Based Language Un-
derstanding Systems.
TREC-8 Proceedings. 1999. Proceedings of the Eighth
Text Retrieval Conference (TREC8). National Institute of
Standards and Technology, Special Publication 500-246,
Gaithersburg, MD.
TREC-9 Proceedings. 2000. Proceedings of the Ninth Text
Retrieval Conference (forthcoming). National Institute
of Standards and Technology, Special Publication 500-
XXX, Gaithersburg, MD.
W. Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandy-
berry, and M.P. Harper. 2000. A Question Answering
System Developed as a Project in a Natural Language
Processing Course. In ANLP/NAACL Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 1?2,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Increasing Maintainability of NLP Evaluation Modules Through Declarative
Implementations
Terry Heinze
Research & Development Department
Thomson Corporation
Eagan, MN 55123
terry.heinze@thomson.com
Marc Light
Research & Development Department
Thomson Corporation
Eagan, MN 55123
marc.light@thomson.com
Abstract
Computing precision and recall metrics for named
entity tagging and resolution involves classifying
text spans as true positives, false positives, or false
negatives. There are many factors that make this
classification complicated for real world systems.
We describe an evaluation system that attempts to
control this complexity through a set of rules and a
forward chaining inference engine.
1 Introduction
Computing precision and recall metrics for named entity
recognition systems involves classifying each text span
that the system proposes as an entity and a subset of the
text spans that the gold data specifies as an entity. These
text spans must be classified as true positives, false posi-
tives, or false negatives.
In the simple case, it is easy to write a procedure to
walk through the list of text spans from the system and
check to see if a corresponding text span exists in the gold
data with the same label, mark the text span as true posi-
tive or false positive accordingly, and delete the span from
the gold data set. Then the procedure need only walk
through the remaining gold data set and mark these spans
as false negatives. The three predicates are the equality
of the span?s two offsets and the labels. This evaluation
procedure is useful for any natural language processing
task that involves finding and labeling text spans.
The question this poster addresses is how best to man-
age the complexity of the evaluation system that results
from adding a number of additional requirements to the
classification of text spans. The requirements may in-
clude fuzzy extent predicates, label hierarchies, confi-
dence levels for gold data, and collapsing multiple men-
tions in a document to produce a single classification. In
addition, named entity tasks often also involve resolving
a mention of an entity to an entry in an authority file (i.e.,
record in a relational database). This extension also re-
quires an interleaved evaluation where the error source is
important.
We started with a standard procedural approach, en-
coding the logic in nested conditionals. When the nesting
reached a depth of five (e.g., Figure 1), we decided to try
another approach. We implemented the logic in a set of
rules. More specifically, we used the Drools rules and for-
ward chaining engine (http://labs.jboss.com/drools/) to
classify text spans as true positives, false positives, and/or
false negatives. The procedural code was 379 lines long.
The declarative system consists of 25 rules with 150 lines
of supporting code. We find the rules more modular and
easier to modify and maintain. However, at this time, we
have no experimental result to support this opinion.
2 Added Complexity of the Classification
of Text Spans for Evaluation
Matching extents and labels: A system text span may
overlap a gold data span but leave out, say, punctuation.
This may be deemed correct but should be recorded as a
fuzzy match. A match may also exist for span labels also
since they may be organized hierarchically (e.g, cities and
countries are kinds of locations). Thus, calling a city a
location may be considered a partial match.
Annotator Confidence: We allowed our annotators to
mark text span gold data with an attribute of ?low con-
fidence.? We wanted to pass this information through to
the classification of the spans so that they might be fil-
tered out for final precision and recall if desired.
Document level statistics: Some named entity tagging
tasks are only interested in document level tagging. In
other words, the system need only decide if an entity is
mentioned in a document: how many times it is men-
tioned is unimportant.
Resolution: Many of our named entity tagging tasks
go a step further and also require linking each entity men-
tion to a record in a database of entities. For error anal-
1
ysis, we wished to note if a false negative/positive with
respect to resolution is caused by the upstream named
entity tagger. Finally, our authority files often have many
entries for the same entity and thus the gold data contains
multiple correct ids.
for (annotations)
if(extents & labels match)
if(ids match => TP res)
if(notresolved => TN res)
else if(single id => TP res)
else if(multiple ids => contitional TP res)
else error
else
if(gold id exists)
if(gold id uncertain => FP res low confidence)
else => FP res
else
if(fuzzy extents & labels match)
if(ids match)
if(no gold id => TN res)
else if(multiple ids => conditional TP res)
else => fuzzy TP res
else ...
Figure 1: Nested conditionals for instance classification
3 Using Rules to Implement the Logic of
the Classification
The rules define the necessary conditions for membership
in a class. These rules are evaluated by an inference en-
gine, which forward chains through the rule set. In this
manner, rules for fuzzy matches, for handling gold data
confidence factors, and for adding exclusionary condi-
tions could be added (or removed) from the rule set with-
out modifying procedural code.
rule ?truepositive? salience 100
sa : SourceAnnotation( assigned == false )
ta : TargetAnnotation( type == sa.type,
beginOffset == sa.beginOffset, endOffset == sa.endOffset )
then sa.setResult(?TP?);
rule ?false positive? salience 90
sa : SourceAnnotation( assigned == false )
not TargetAnnotation( type == sa.type,
beginOffset == sa.beginOffset, endOffset == sa.endOffset )
then sa.setResult(?FP?);
rule ?false negative? salience 80
ta : TargetAnnotation( assigned == false )
not SourceAnnotation( type == ta.type,
beginOffset == ta.beginOffset, endOffset == ta.endOffset )
then ta.setResult(?FN?);
Figure 2: Rules for instance classification
Three rules were needed to determine the basic col-
lection level metrics. The results of these rules were
then passed on to the next sets of rules for modification
for conditional checks. We use agenda groups and rule
salience to control the firing precedence within the rule
sets. In Figure 2, we present an example of the sort of
rules that are defined.
For example, the determination of true positives was
made by firing the ?true positive? rule whenever an an-
notation from the system matched an annotation from
the gold data. This occurred if the entity type and off-
sets were equal. This rule was given higher salience than
those for true negatives and false positives since it had the
effect of removing the most candidate annotations from
the working memory.
Note that because we use a Java implementation that
adheres to JSR94, all of the rules apply their conditions
to Java objects. The syntax for tautologies within the con-
dition statements, refer to bean properties within the en-
closing object.
In Figure 3, we show first, a modification to add a fuzzy
metric rule that checks false negative annotations to see if
they might be a fuzzy match. Second, we show a rule that
removes false positives that are defined in a stop-word
list.
rule?fuzzy check? agenda-group ?FuzzyMatch?
ta : TargetAnnotation( result == ?FN? );
sa : SourceAnnotation( type == ta.type, result == ?FP?,
ta.beginOffset < endOffset, ta.endOffset > beginOffset );
eval(ifFuzzyMatch(sa.getText(), ta.getText(), sa.getType()));
then sa.setResult(?FzTP?);
rule ?filter FP? salience 10 agenda-group ?Filter?
sa : SourceAnnotation( result == ?FP? );
eval(DexterMetrics.ifStopWord(sa.getText(), sa.getType()));
then sa.setResult(sa.getResult() + ?-ignored:stop word?);
Figure 3: Rules for modified classification
4 Conclusion
We described some of the complexities that our evalua-
tion module had to deal with and then introduce a rule-
based approach to its implementation. We feel that this
approach made our evaluation code easier to understand
and modify. Based on this positive experience, we sug-
gest that other groups try using rules in their evaluation
modules.
2
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 74?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrating High Precision Rules with Statistical Sequence Classifiers for
Accuracy and Speed
Wenhui Liao, Marc Light, and Sriharsha Veeramachaneni
Research and Development,Thomson Reuters
610 Opperman Drive, Eagan MN 55123
Abstract
Integrating rules and statistical systems is a
challenge often faced by natural language pro-
cessing system builders. A common sub-
class is integrating high precision rules with a
Markov statistical sequence classifier. In this
paper we suggest that using such rules to con-
strain the sequence classifier decoder results
in superior accuracy and efficiency. In a case
study of a named entity tagging system, we
provide evidence that this method of combina-
tion does prove efficient than other methods.
The accuracy was the same.
1 Introduction
Sequence classification lies at the core of several
natural language processing applications, such as
named entity extraction, Asian language segmen-
tation, Germanic language noun decompounding,
and event identification. Statistical models with a
Markov dependency have been successful employed
to perform these tasks, e.g., hidden Markov mod-
els (HMMs)(Rabiner, 1989) and conditional random
fields (CRFs)(Lafferty et al, 2001). These statistical
systems employ a Viterbi (Forney, 1973) decoder at
runtime to efficiently calculate the most likely la-
bel sequence based on the observed sequence and
model. Statistical machine translation systems make
use of similar decoders.
In many situations it is beneficial, and some-
times required, for these systems to respect con-
straints from high precision rules. And thus when
building working sequence labeling systems, re-
searchers/software engineers are often faced with
the task of combining these two approaches. In
this paper we argue for a particular method of com-
bining statistical models with Markov dependencies
and high precision rules. We outline a number of
ways to do this and then argue that guiding the de-
coder of the statistical system has many advantages
over other methods of combination.
But first, does the problem of combining multi-
ple approaches really happen? In our experience the
need arises in the following way: a statistical ap-
proach with a Markov component is chosen because
it has the best precision/recall characteristics and has
reasonable speed. However, a number of rules arise
for varied reasons. For example, the customer pro-
vides domain knowledge not present in the training
data or a particular output characteristic is more im-
portant that accuracy. Consider the following ficti-
tious but plausible situation: A named entity tagging
system is built using a CRF. The customer then pro-
vides a number of company names that cannot be
missed, i.e., false negatives for these companies are
catastrophic but false positives can be tolerated. In
addition, it is known that, unlike in the training data,
the runtime data will have a company name immedi-
ately before every ticker symbol. The question fac-
ing the builder of the system is how to combine the
CRF with rules based on the must-find company list
and the company-name-before-every-ticker-symbol
fact.
Similar situations arise for the other sequence tag-
ging situations mentioned above and for machine
translation. We suspect that even for non-language
applications, such as gene sequence labeling, similar
situations arise.
74
In the next section we will discuss a number of
methods for combining statistical systems and high
precision rules and argue for guiding the decoder
of the statistical model. Then in section 3, we de-
scribe an implementation of the approach and give
evidence that the speed benefits are substantial.
2 Methods for Combining a Markov
Statistical System and High Precision
Rules
One method of combination is to encode high preci-
sion rules as features and then train a new model that
includes these features. One advantage is that the
system stays a straightforward statistical system. In
addition, the rules are fully integrated into the sys-
tem allowing the statistical model weigh the rules
against other evidence. However, the model may
not give the rules high weight if training data does
not bear out their high precision or if the rule trig-
ger does not occur often enough in the training data.
Thus, despite a ?rule? feature being on, the system
may not ?follow? the rule in its result labeling. Also,
addition or modification of a rule would require a
retraining of the model for optimal accuracy. The
retraining process may be costly and/or may not be
possible in the operational environment.
Another method is to run both the statistical sys-
tem and the rules and then merge the resulting labels
giving preference to the labels resulting from the
high precision rules. The benefits are that the rules
are always followed. However, the statistical system
does not have the information needed to give an op-
timal solution based on the results of the high preci-
sion rules. In other words, the results will be incon-
sistent from the view of the statistical system; i.e., if
it had know what the rules were going to say, then it
would have calculated the remaining part of the label
sequence differently. In addition, the decoder con-
siders part of the label sequence search space that is
only going to be ruled out, pun intended, later.
Now for the preferred method: run the rules first,
then use their output to guide the decoder for the
statistical model. The benefits of this method are
that the rules are followed, the statistical system is
informed of constraints imposed by the rules and
thus the statistical system calculates optimal paths
given these constraints. In addition, the decoder
considers only those label sequences consistent with
these constraints, resulting in a smaller search space.
Thus, we would expect this method to produce both
a more accurate and a faster implementation.
Consider Figure 1 which shows a lattice that rep-
resents all the labeling sequences for the input ...
Microsoft on Monday announced a ... The possible
labels are O (out), P (person), C (company), L (lo-
cation) . Assume Microsoft is in a list of must-find
companies and that on and Monday are part of a rule
that makes them NOT names in this context. The
bold points are constraints from the high-precision
rules. In other words, only sequences that include
these bold points need to be considered.
Figure 1: Guiding decoding with high-precision rules
Figure 1 also illustrates how the constraints re-
duce the search space. Without constraints, the
search space includes 46 = 4096 sequences, while
with constraints, it includes only 43 = 64.
It should also be noted that we do not claim to
have invented the idea of constraining the decoder.
For example, in the context of active learning, where
a human corrects some of the errors made by a CRF
sequence classifier, (Culota et al, 2006) proposed a
constrained Viterbi algorithm that finds the path with
maximum probability that passes through the labels
assigned by the human. They showed that constrain-
ing the path to respect the human labeling consider-
ably improves the accuracy on the remaining tokens
in the sequence. Our contribution is noticing that
constraining the decoder is a good way to integrate
rule output.
3 A Case Study: Named Entity
Recognition
In this section, we flesh out the discussion of named
entity (NE) tagging started above. Since the entity
type of a word is determined mostly by the context
of the word, NE tagging is often posed as a sequence
75
classification problem and solved by Markov statis-
tical systems.
3.1 A Named Entity Recognition System
The system described here starts with a CRF which
was chosen because it allows for the use of numer-
ous and arbitrary features of the input sequence and
it can be efficiently trained and decoded. We used
the Mallet toolkit (McCallum, 2002) for training the
CRF but implemented our own feature extraction
and runtime system. We used standard features such
as the current word, the word to the right/left, ortho-
graphic shape of the word, membership in word sets
(e.g., common last names), features of neighboring
words, etc.
The system was designed to run on news wire text
and based on this data?s characteristics, we designed
a handful of high precision rules including:
Rule 1: if a token is in a must-tag list, this token
should be marked as Company no matter what the
context is.
Rule 2: if a capitalized word is followed by cer-
tain company suffix such as Ltd, Inc, Corp, etc., la-
bel both as Company.
Rule 3: if a token sequence is in a company list
and the length of the sequence is larger than 3, label
them as Company.
Rule 4: if a token does not include any uppercase
letters, is not pure number, and is not in an excep-
tions list, label it as not part of a name. (The ex-
ceptions list includes around 70 words that are not
capitalized but still could be an NE, such as al, at,
in, -, etc.)
Rule 5: if a token does not satisfy rule 4 but its
neighboring tokens satisfy rule 4, then if this token
is a time related word, label it as not part of a name.
(Example time tokens are January and Monday.)
The first three rules aim to find company names
and the last two to find tokens that are not part of a
name.
These rules are integrated into the system as de-
scribed in section 2: we apply the rules to the input
token sequence and then use the resulting labels, if
any, to constrain the Viterbi decoder for the CRF.
A further optimization of the system is based on
the following observation: features need not be cal-
culated for tokens that have already received labels
from the rules. (An exception to this is when fea-
tures are copied to a neighbor, e.g., the token to my
left is a number.) Thus, we do not calculate many
features of rule-labeled tokens. Note that feature ex-
traction can often be a major portion of the compu-
tational cost of sequence labeling systems (see Table
1(b))
3.2 Evidence of Computational Savings
Resulting from Our Proposed Method of
Integration
We compare the results when high-precision rules
are integrated into CRF for name entity extraction
(company, person, and location) in terms of both ac-
curacy and speed for different corpora. Three cor-
pora are used, CoNLL (CoNLL 2003 English shared
task official test set), MUC (Message Understanding
Conference), and TF (includes around 1000 news ar-
ticles from Thomson Financial).
Table 1(a) shows the results for each corpora re-
spectively. The baseline method does not use any
high-precision rules, the Post-corr uses the high-
precision rules to correct the labeling from the CRF,
and Constr-viti uses the rules to constrain the label
sequences considered by the Viterbi decoder. In gen-
eral, Constr-viti achieves slightly better precision
and recall.
(a)
(b)
Figure 2: (b) A test example : (a) without constraints; (b)
with constraints
To better understand how our strategy could im-
prove the accuracy, we did some analysis on the
76
Table 1: Experiment Results
Database Methods Precision Recall F1
CoNLL Baseline 84.38 83.02 83.69
Post-corr 85.87 84.86 85.36
Constr-viti 85.98 85.55 85.76
TF Baseline 88.39 82.42 85.30
Post-corr 87.69 88.30 87.99
Constr-viti 88.02 88.54 88.28
MUC Baseline 92.22 88.72 90.43
Post-Corr 91.28 88.87 90.06
Constr-viti 90.86 89.37 90.11
(a)Precision and Recall
Methods Rules Features Viterbi Overall
Baseline 0 0.78 0.22 1
Post-corr 0.08 0.78 0.22 1.08
Constr-vite 0.08 0.35 0.13 0.56
Baseline 0 0.85 0.15 1
Post-Corr 0.14 0.85 0.15 1.14
Constr-vite 0.14 0.38 0.1 0.62
Baseline 0 0.79 0.21 1
Post-corr 0.12 0.79 0.21 1.12
Constr-vite 0.12 0.36 0.12 0.60
(b)Time Efficiency
testing data. In one example as shown in Figure 2,
Steel works as an attorney, without high-precision
rules, Steel works is tagged as a company since it is
in our company list. Post-correction changes the la-
bel of works to O, but it is unable to fix Steel. With
our strategy, since works is pinned as O in the Vert-
ibi algorithm, Steel is tagged as Per. Thus, com-
pared to post-correction, the advantage of constrain-
ing Viterbi is that it is able to affect the whole path
where the token is, instead a token itself. However,
the improvements were not significant in our case
study. We have not done an error analysis. We can
only speculate that the high precision rules do not
have perfect precision and thus create a number of
errors that the statistical model would not have made
on its own.
We also measured how much the constrained
Viterbi method improves efficiency. We divide the
computational time to three parts: time in applying
rules, time in feature extraction, and time in Viterbi
computation. Table 1(b) lists the time efficiency. In-
stead using specific time unit (e.g. second), we use
ratio instead by assuming the overall time for the
baseline method is 1. As shown in the table, for
the three data sets, the overall time of our method
is 0.56, 0.62, and 0.60 of the time of the baseline
algorithm respectively. The post-correction method
is the most expensive one because of the extra time
spending in rules. Overall, the constrained Viterbi
method is substantially faster than the Baseline and
Post-corr methods in addition to being more accu-
rate.
4 Conclusions
The contribution of this paper is the repurposing of
the idea of constraining a decoder: we constrain the
decoder as a way to integrate high precision rules
with a statistical sequence classifier. In a case study
of named entity tagging, we show that this method
of combination does in fact increase efficiency more
than competing methods without any lose of ac-
curacy. We believe analogous situations exist for
other sequence classifying tasks such as Asian lan-
guage segmentation, Germanic language noun de-
compounding, and event identification.
References
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Artificial
Intelligence Journal, 170:1101?1122.
G. D. Forney. 1973. The viterbi algorithm. Proceedings
of the IEEE, 61(3):268?278.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289.
A.K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, pages 257?286.
77
The Language of Bioscience:
Facts, Speculations, and Statements in Between
Marc Light
Library and Information Science
Linguistics Department
University of Iowa
Iowa City, IA 52242
marc-light@uiowa.edu
Xin Ying Qiu
Management Sciences
University of Iowa
Iowa City, IA 52242
xin-qiu@uiowa.edu
Padmini Srinivasan
Library and Information Science
Management Sciences
University of Iowa
Iowa City, IA 52242
padmini-srinivasan@uiowa.edu
Abstract
We explore the use of speculative lan-
guage in MEDLINE abstracts. Results
from a manual annotation experiment sug-
gest that the notion of speculative sentence
can be reliably annotated by humans. In
addition, an experiment with automated
methods also suggest that reliable auto-
mated methods might also be developed.
Distributional observations are also pre-
sented as well as a discussion of possible
uses for a system that can recognize spec-
ulative language.
1 Introduction
The scientific process involves making hypotheses,
gathering evidence, using inductive reasoning to
reach a conclusion based on the data, and then mak-
ing new hypotheses. Scientist are often not com-
pletely certain of a conclusion. This lack of definite
belief is often reflected in the way scientists discuss
their work.
In this paper, we focus on expressions of levels of
belief: the expressions of hypotheses, tentative con-
clusions, hedges, and speculations. ?Affect? is used
in linguistics as a label for this topic. This is not a
well-known topic in the field of text processing of
bioscience literature. Thus, we present a large num-
ber of examples to elucidate the variety and nature
of the phenomena. We then return to a discussion
of the goals, importance, and possible uses of this
research.
1.1 Examples
The sentences in the following box contain frag-
ments expressing a relatively high level of specula-
tion. The level of belief expressed by an author is
often difficult to ascertain from an isolated sentence
and often the context of the abstract is needed. All
examples in the paper are from abstracts available at
the Nation Library of Medicine PubMed webpage
(currently http://www.ncbi.nlm.nih.gov/PubMed/).
The PubMed identifier is provided following each
sentence.
Pdcd4 may thus constitute a useful molecular
target for cancer prevention. (1131400)
As the GT box has also previously been shown
to play a role in gene regulation of other genes,
these newly isolated Sp2 and Sp3 proteins might
regulate expression not only of the TCR gene but
of other genes as well. (1341900)
On the basis of these complementary results, it
has been concluded that curcumin shows very
high binding to BSA, probably at the hydropho-
bic cavities inside the protein. (12870844)
Curcumin down-regulates Ki67, PCNA and mu-
tant p53 mRNAs in breast cancer cells, these
properties may underlie chemopreventive ac-
tion. (14532610)
The next examples contain fragments that are
speculative but probably less so than those above.
(As we will discuss later, it is difficult to agree on
levels of speculation.) The containing sentence does
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 17-24.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
provide some context but the rest of the abstract if
not the full text is often necessary along with enough
knowledge of field to understand text.
Removal of the carboxy terminus enables ERP
to interact with a variety of ets-binding sites
including the E74 site, the IgH enhancer pi
site, and the lck promoter ets site, suggesting
a carboxy-terminal negative regulatory domain.
(7909357)
In addition, we show that a component of the
Ras-dependent mitogen-activated protein kinase
pathway, nerve growth factor-inducible c-Jun,
exerts its effects on receptor gene promoter ac-
tivity most likely through protein-protein inter-
actions with Sp1. (11262397)
Results suggest that one of the mechanisms of
curcumin inhibition of prostate cancer may be
via inhibition of Akt. (12682902)
The previous examples contain phrases such as
most likely and suggesting, which in these cases, ex-
plicitly mark a level of belief less than 100%. The
next examples are not as explicitly marked: to date
and such as can also be used in purely definite state-
ments.
To date, we find that the signaling pathway
triggered by each type of insult is distinct.
(10556169)
However, the inability of IGF-1, insulin and
PMA to stimulate 3beta-HSD type 1 expression
by themselves in the absence of IL-4 indicates
that the multiple pathways downstream of IRS-1
and IRS-2 must act in cooperation with an IL-
4-specific signaling molecule, such as the tran-
scription factor Stat6. (11384880)
These findings highlight the feasibility of mod-
ulating HO-1 expression during hypothermic
storage to confer tissues a better protection to
counteract the damage characteristic of organ
transplantation. (12927811)
The words may and might were both used to ex-
press speculation in the examples above but are am-
biguous between expressing speculation versus pos-
sibility. The examples above are speculative and the
sentence below expresses a definite statement about
two possibilities.
The level of LFB1 binding activity in adenoid-
cystic as well as trabecular tumours shows some
variation and may either be lower or higher
than in the non-tumorous tissue. (7834800)
The sentence below involves the adjective puta-
tive in an apositive noun phrase modifier, a different
syntactic form that in the previous examples. It also
clearly shows that the speculative portion is often
confined to only a part of the information provided
in a sentence.
We report here the isolation of human zinc finger
2 (HZF2), a putative zinc-finger transcription
factor, by motif-directed differential display of
mRNA extracted from histamine-stimulated hu-
man vein endothelial cells. (11121585)
Of course, definite sentences also come in a vari-
ety. The definite sentences below vary in topic and
form.
Affinity chromatography and coimmunoprecipi-
tation assays demonstrated that c-Jun and T-Ag
physically interact with each other. (12692226)
However, NF-kappaB was increased at 3 h while
AP-1 (Jun B and Jun D) and CREB were in-
creased at 15 h. (10755711)
We studied the transcript distribution of c-jun,
junB and junD in the rat brain. (1719462)
An inclusive model for all steps in the targeting
of proteins to subnuclear sites cannot yet be pro-
posed. (11389536)
We have been talking about speculative fragments
and speculative sentences. For the rest of the paper,
we define a speculative sentence to be one that con-
tains at least one speculative fragment. A definite
sentence contains no speculative fragments. In this
study we only considered annotations at the sentence
level. However, in future work, we plan to work on
sub-sentential annotations.
1.2 Goals of our research on speculative speech
and possible uses
Our general goal is to investigate speculative speech
in bioscience literature and explore how it might be
used in HLT applications for bioscientists. A more
specific goal is to investigate the use of speculative
speech in MEDLINE abstracts because of their ac-
cessibility.
There are a number of reasons supporting the im-
portance of understanding speculative speech:
? it makes up a substantial portion of scientific
prose (we estimate that 11% of sentences in
MEDLINE abstracts contain speculative frag-
ments),
? many researchers are interested in current
trends and directions and speculations are
likely to be relevant,
? even if definite statements are of primary im-
portance, knowing that a statement is not defi-
nite, i.e. speculative, is important.
In the following, we expand upon these points in the
contexts of i) information retrieval, ii) information
extraction, and iii) knowledge discovery.
In the context of information retrieval, an exam-
ple information need might be ?I am looking for
speculations about the X gene in liver tissue.? One
of the authors spoke at a research department of a
drug company and the biologists present expressed
this sort of information need. On the other hand,
one of the authors has also encountered the opposite
need: ?I am looking for definite statements about
transcription factors that interact with NF Kappa B.?
Both these information needs would be easier to ful-
fill if automated annotation of speculative passages
was possible.
In the context of information extraction, a simi-
lar situation exists. For example, extracting tables
of protein-protein interactions would benefit from
knowing which interactions were speculative and
which were definite.
In the context of knowledge discovery (KR), spec-
ulation might play a number of roles. One possibil-
ity would be to use current speculative statements
about a topic of interest as a seed for the automated
knowledge discovery process. For example, terms
could be extracted from speculative fragments and
used to guide the initial steps of the knowledge dis-
covery process. A less direct but perhaps even more
important use is in building test/train datasets for
knowledge discovery systems. For example, let us
assume that in a 1985 publication we find a specu-
lation about two topics/concepts A and C being re-
lated and later in a 1995 document there is a definite
statement declaring that A and C are connected via
B. This pair of statements can then form the basis
of a discovery problem. We may use it to test a KR
system?s ability to predict B as the connecting as-
pect between A and C and to do this using data prior
to the 1995 publication. The same example could
also be used differently: KR systems could be as-
sessed on their ability to make a speculation between
A and C using data up to 1985 excluding the partic-
ular publication making the speculation. In this way
such pairs of temporally ordered speculative-definite
statements may be of value in KR research. Dif-
ferentiating between speculative and definite state-
ments is one part of finding such statement pairs.
2 Related work
We know of no work specifically on speculative
speech in the context of text processing of bio-
science literature. However, some work on informa-
tion extraction from bioscience literature has dealt
with speculative speech. For example, (Friedman et
al., 1994) discusses uncertainty and hedging in ra-
diology reports and their system assigns one of five
levels of certainty to extracted findings.
Text processing systems in general have focused
?factual? language. However, a growing number of
researchers have started work on other aspects of
language such as expressing opinions, style of writ-
ing, etc. For example a human language technology
workshop will be held this Spring entitled ?Explor-
ing Attitude and Affect in Text: Theories and Ap-
plications.? (Qu et al, 2004). Previous work along
these lines includes (Wilson and Wiebe, 2003). This
research focuses on newswire texts and other texts
on the topic of politics and current events.
There has been recent work on classifying sen-
tences from MEDLINE abstracts for the categories
such as object, background, conclusions (McKnight
and Srinivasan, 2003). In addition, early work,
(Liddy, 1988) built text grammars for empirical re-
search abstracts categorized and assigned structure
concerning rhetorical roles of the sentences. How-
ever, none of this work addresses the speculative vs.
definite distinction we are interested in.
There has also been some work on construct-
ing test sets for knowledge discovery. Several re-
searchers have used the discoveries by Swanson and
Smalheiser to test their own algorithms. The two
problems most commonly used in replication stud-
ies (e.g., (Weeber et al, 2001)) are their discov-
ery of a link between Raynauds disease and fish
oils (Swanson, 1986) and their discovery of several
links between migraine and magnesium (Swanson,
1988). The most comprehensive replication to date
is (Srinivasan, 2004) which employs eight Swanson
and Smalheiser discoveries as a test bed.
In the remainder of the paper, we describe a man-
ual annotation experiment we performed, give pre-
liminary results on our attempts to automatically
annotate sentences as containing speculative frag-
ments, and conclude with comments on possible fu-
ture work.
3 Manual annotation experiment
In this experiment, four human annotators manually
marked sentences as highly speculative, low specu-
lative, or definite.
Some of the questions we hoped to answer with
this experiment were: can we characterize what a
speculative sentence is (as demonstrated by good
inter-annotator agreement), can a distinction be-
tween high and low speculation be made, how much
speculative speech is there, where are speculative
sentences located in the abstract, is there variation
across topics?
The annotators were instructed to follow written
annotation guidelines which we provide in appendix
of this paper. We wanted to explore how well the an-
notators agreed on relatively abstract classifications
such as ?requires extrapolation from actual findings?
and thus we refrained from writing instructions such
as ?if the sentence contains a form of suggest, then
mark it as speculative? into the guidelines.
We chose three topics to work on and used the
following Pubmed queries to gather abstracts:
? ?gene regulation? AND ?transcription factor?
AND 1900:2001[edat]
? (crohn?s disease OR crohn disease) AND com-
plications[MeSH Subheading] AND hasab-
stract[text] AND English[Lang] AND (ho-
minidae[MeSH Terms] OR Human[MeSH
Terms])
? turmeric OR curcumin OR curcuma
The first topic is gene regulation and is about
molecular biology research on transcription factors,
promoter regions, gene expression, etc. The second
topic is Crohn?s disease which is a chronic relapsing
intestinal inflammation and has a number of genes
(CARD15) or chromosomal loci associated with it.
The third topic is turmeric (aka curcumin), a spice
widely used in Asia and highly regarded for its cu-
rative and analgesic properties. These include the
treatment of burns, stomach ulcers and ailments, and
various skin diseases. There has been a surge of in-
terest in curcumin over the last decade.
Each abstract set was prepared for annotation as
follows: the order of the abstracts was randomized
and the abstracts were broken into sentences us-
ing Mxterminator (Reynar and Ratnaparkhi, 1997).
The following people performed the annotations:
Padmini Srinivasan, who has analyzed crohns and
turmeric documents for a separate knowledge dis-
cover research task, Xin Ying Qiu, who is com-
pletely new to all three topics, Marc Light, who
has some experience with gene regulation texts (e.g.,
(Light et al, 2003)), Vladimir Leontiev, who is a re-
search scientist in an anatomy and cell biology de-
partment. It certainly would have been preferable to
have four experts on the topics do the annotation but
this was not possible.
The following manual annotations were per-
formed:
a. 63 gene regulation abstracts (all sentences) by
both Leontiev and Light,
b. 47 gene regulation additional abstracts (all sen-
tences) by Light,
c. 100 crohns abstracts (last 2 sentences) by both
Srinivasan and Qiu,
d. 400 crohns abstracts additional (last 2 sen-
tences) by Qiu,
e. 100 turmeric abstracts (all sentences) by Srini-
vasan,
f. 400 turmeric additional abstracts (last 2 sen-
tences) by Srinivasan.
The 63 double annotated gene regulation abstracts
(set a) contained 547 sentences. The additional ab-
stracts (set b) marked by Light1 contained 344 sen-
tences summing to 891 sentences of gene regula-
tion abstracts. Thus, there is an average of almost
9 sentences per gene regulation abstract. The 100
turmeric abstracts (set e) contained 738 sentences.
The other sets contain twice as many sentences as
abstracts since only the last two sentences where an-
notated.
The annotation of each sentence was performed in
the context of its abstract. This was true even when
only the last two sentences where annotated. The
annotation guidelines in the appendix were used by
all annotators. In addition, at the start of the exper-
iment general issues were discussed but none of the
specific examples in the sets a-f.
We worked with three categories Low Specula-
tive, High Speculative, and Definite. All sentences
were annotated with one of these. The general idea
behind the low speculative level was that the authors
expressed a statement in such a way that it is clear
that it follows almost directly from results but not
quite. There is a small leap of faith. A high specu-
lative statement would contain a more dramatic leap
from the results mentioned in the abstract.
Our inter-annotator agreement results are ex-
pressed in the following four tables. The first table
contains values for the kappa statistic of agreement
(see (Siegel and Castellan, 1988)) for the gene regu-
lation data (set a) and the crohns data (set c). Three
values were computed: kappa for three-way agree-
ment (High vs. Low vs. Definite), two-way (Spec-
ulative vs. Definite) and two-way (High vs. Low).
Due to the lack of any sentences marked High in
set c, a kappa value for High vs. low (HvsL) is not
possible. Kappa scores between 0.6 and 0.8 are gen-
erally considered encouraging but not outstanding.
HvsLvsD SvsD HvsL
geneReg 0.53 0.68 0.03
crohns 0.63 0.63 na
1Pun intended.
The following two tables are confusion matrices,
the first for gene regulation data (set a) and the sec-
ond for the crohns data (set c).
H L D
H 5 11 5
L 10 26 19
D 3 12 440
H L D
H 0 0 3
L 0 14 3
D 1 7 170
If we consider one of the annotators as defining
truth (gold standard), then we can compute preci-
sion and recall numbers for the other annotator on
finding speculative sentences. If we choose Leon-
tiev and Srinivasan as defining truth, then Light and
Qiu receive the scores below.
precision recall
Light 0.68 0.78
Qiu 0.70 0.64
As is evident from the confusion matrices, the
amount of data that we redundantly annotated is
small and thus the kappa numbers are at best to be
taken as trends. However, it does seem that the spec-
ulative vs. definite distinction can be made with
some reliability. In contrast, the high speculation vs.
low speculation distinction cannot.
The gene regulation annotations marked by Light
(sets a & b using only Light?s annotations) can
be used to answer questions about the position of
speculative fragments in abstracts. Consider the
histogram-like table below. The first row refers to
speculative sentences and the second to definite. The
columns refer to the last sentence of an abstract, the
penultimate, elsewhere, and a row sum. The num-
ber in brackets is the raw count. Remember that the
number of abstracts in sets a & b together is 100.
last 2nd last earlier total
S 57%(57) 23%(23) 6%(45) 14%(125)
D 43%(43) 77%(75) 94%(648) 86%(766)
It is clear that almost all of the speculations come
towards the end of the abstract. In fact the final sen-
tence contains a speculation more often than not.
In addition, consider the data where all sentences
in an abstract were annotated (sets a & b & e, us-
ing Light?s annotation of a), there were 1456 defini-
tive sentences (89%) and 173 speculative sentence
(11%). Finally, if we consider the last two sen-
tences of all the data (sets a-f), we have 1712 defini-
tive sentences (82%) and 381 speculative sentences
(18.20%).
4 Automatic classifier experiment
We decided to explore the ability of an SVM-
based text classifier to select speculative sentences
from the abstracts. For this the abstracts were
first processed using the SMART retrieval system
(Salton, 1971) in order to obtain representation vec-
tors (term-based). Alternative representations were
tried involving stemming and term weighting (no
weights versus TF*IDF weights). Since results ob-
tained were similar we present only results using
stemming and no weights.
The classifier experiments followed a 10-fold
cross-validation design. We used SV Mlight pack-
age2 with all settings at default values. We ran ex-
periments in two modes. First, we considered only
the last 2 sentences. For this we pooled all hand
tagged sentences from the three topic areas (sets a-f).
Second, we explored classification on all sentences
in the document (sets a,b,e).
If we assume a default strategy as a simple base-
line, where the majority decision is always made,
then we get an accuracy of 82% for the classifica-
tion problem on the last two sentences data set and
89% for the all sentences data set. Another baseline
option is to use a set of strings and look for them as
substrings in the sentences. The following 14 strings
were identified by Light while annotating the gene
regulation abstracts (sets a&b): suggest, potential,
likely, may, at least, in part, possibl, potential, fur-
ther investigation, unlikely, putative, insights, point
toward, promise, propose. The automated system
then looks for these substrings in a sentence and if
found, the sentence is marked as speculative and as
definite if not.
In the table below the scores for the three methods
of annotation are listed as rows. We give accuracy
on the categorization task and precision and recall
numbers for finding speculative sentences. The for-
mat is precision/recall(accuracy), all as percentages.
The Majority method, annotating every sentence as
2http://wwwai.cs.unidortmund.de/ SOFT-
WARE/SVM LIGHT/svm light.html.en
definite, does not receive precision and recall values.
The substring method was run on a subset of the
datasets where the gene regulation data (sets a&b)
was removed. (It performs extremely well on the
gene regulation data due to the fact that it was devel-
oped on that data.)
last2 all
SVM 71/39(85) 84/39(92)
Substr 55/80(87) 55/79(95)
Majority (82) (89)
Again the results are preliminary since the amount
of data is small and the feature set we explored was
limited to words. However, it should be noted that
both the substring and the SVM systems performs
well suggesting that speculation in abstracts is lex-
ically marked but in a somewhat ambiguous fash-
ion. This conclusion is also supported by the fact
that neither system used positional features and yet
the precision and recall on the all sentence data set
is similar to the last two sentences data set.
5 Conclusion and future work
The work presented here is preliminary but promis-
ing. It seems that the notion of speculative sen-
tence can be characterized enabling manual anno-
tation. However, we did not manage to characterize
the distinction between high and low speculation. In
addition, it seems likely that automated systems will
be able to achieve useful accuracy. Finally, abstracts
seem to include a fair amount of speculative infor-
mation.
Future work concerning manual annotation would
include revising the guidelines, throwing out the
High vs. Low distinction, annotating more data, an-
notating sub-sentential units, annotating the focus of
the speculation (e.g., a gene), and annotating full
text articles. We are also ignorant of work in lin-
guistics that almost certainly exists and may be in-
formative. We have started this process by consider-
ing (Hyland, 1998) and (Harris et al, 1989).
Future work concerning automatic annotation in-
cludes expanding the substring system with more
substrings and perhaps more complicated regular ex-
pressions, expanding the feature set of the SVM, try-
ing out other classification methods such as decision
trees.
Finally, we plan on building some of the applica-
tions mentioned: a speculation search engine, tran-
scription factor interaction tables with a specula-
tion/definite column, and knowledge discovery test
sets.
Acknowledgments
We would like to thank Vladimir Leontiev for his
time and effort annotating gene regulation abstracts.
In addition, we would like to thank David Eich-
mann for his assistance with our database queries.
We would also like to thank Lynette Hirschman for
assistance with the title of this paper. Finally, we
would like to thank the anonymous workshop re-
viewers for their comments.
References
C. Friedman, P. Alderson, J. Austin, J.J. Cimino, and S.B.
Johnson. 1994. A general natural-language text pro-
cessor for clinical radiology. Journal of the American
Medical Informatics Association, 1(2):161?174.
Z. Harris, M. Gottfried, T. Ryckman, P. Mattick, A. Dal-
adier, T.N. Harris, and S. Harris. 1989. The Form of
Information in Science : analysis of an immunology
sublanguage. Kluwer Academic Publishers.
K. Hyland. 1998. Hedging in Scientific Research Arti-
cles. John Benjamins B.V.
E. D. Liddy. 1988. The Discourse-Level Structure of
Natural Language Texts: An Exploratory Study of Em-
pirical Abstracts. Ph.D. thesis, Syracuse University.
M. Light, R. Arens, V. Leontiev, M. Patterson, X. Y. Qiu,
and H. Wang. 2003. Extracting transcription factor in-
teractions from medline abstracts. In Posters from the
11th International Conference on Intelligent Systems
in Molecular Biology. ISCB.
L. McKnight and P. Srinivasan. 2003. Categorization of
sentence types in medical abstracts. In Proceedings of
the 2003 AMIA conference.
Yan Qu, J. Shanahan, and J. M. Wiebe, editors. 2004.
Proceedings of the AAAI Spring Symposium on Ex-
ploring Attitude and Affect in Text: Theories and Ap-
plications. AAAI. (to appear).
J. Reynar and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
Proceedings of the Fifth Conference on Applied Natu-
ral Language Processing, pages 16?19. ACL.
G. Salton, editor. 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing.
Prentice Hall, Englewood Cliffs, NJ.
S. Siegel and N.J. Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
P. Srinivasan. 2004. Text mining: Generating hypotheses
from medline. Journal of the American Society for
Information Science and Technology. (to appear).
D.R. Swanson. 1986. Fish oil, raynaud?s syndrome, and
undiscovered public knowledge. Perspectives in Biol-
ogy and Medicine, 30:7?18.
D.R. Swanson. 1988. Migraine and magnesium: Eleven
neglected connections. Perspectives in Biology and
Medicine, 31:526?557.
M. Weeber, H. Klein, L. Berg, and R. Vos. 2001. Con-
cepts in literature-based discovery: Simulating swan-
son?s raynaud-fish oil and migraine-magnesium dis-
coveries. Journal of the American Society for Infor-
mation Science, 52(7):548?557.
T. Wilson and J. Wiebe. 2003. Annotating opinions in
the world press. In Proceedings of the 4th SIGdial
Workshop on Discourse and Dialogue (SIGdial-03).
Appendix: Annotation Guidelines
Some target uses for speculative sentence classifi-
cation:
? a speculation search site that enables scientists
and health workers to find speculative state-
ments about a topic of interest,
? a set of starting points for knowledge discovery
systems,
? a test set for knowledge discovery systems.
The purpose of the guidelines below is to instruct
annotators on which sentences should be marked as
speculative.
There are three possible annotations for a sen-
tence: Low Speculative, High Speculative, and Def-
inite. All sentences should annotated with one of
these.
A sentence may be long and contain many sub-
parts:
? if any part of it is High Speculative (HS), it
should be marked as HS,
? if it is not HS but a part of it is Low Speculative
(LS), it should be marked as LS,
? otherwise it should be marked as Definite.
It should also be mentioned that the intent of the
author is what is relevant. The annotator should try
to decide if the author meant the sentence as specu-
lative or definite. E.g., an annotator should not mark
a sentence as speculative, if the author intended the
statement to be definitive.
Below are the definitions for the categories.
? Low Speculative (LS): A sentence fragment is
LS if the author indicates that it receives di-
rect support from the work presented but there
are other possible explanations for the results
(as there always are in science). However,
the proposition (expressed in the sentence frag-
ment) is a plausible if not likely explanation.
? High Speculative (HS): A sentence fragment is
HS if the author indicates that it does not follow
from the work presented but could be extrapo-
lated from it. In other words the work provides
indirect support for the proposition.
? Definite: A sentence fragment is definite if it is
not LS or HS. Observations are generally Def-
inite as are statements about methods, previous
work, etc.
Below are tests that may be helpful for annotating
particular sentences.
? If the sentence fragment implicitly suggests fu-
ture experimentation, then it is likely to be HS.
? Paraphrased the sentence fragment using ?we
conclude?, ?we observe?, or ?we know?. If
a contradiction or cognitive dissonance occurs
then perhaps the fragment is speculative. The
contradiction will be analogous to that in ?we
definitely believe that maybe there is a chance?.
Below are a number of additional considerations.
? Our characterization of speculative speech is
meant to be broad enough to include state-
ments that are not explicitly marked as specula-
tions but are speculations made by the authors
nonetheless. For example, we would consider
a proposal that some statement is true to be a
speculative sentence.
? Mentions of speculations made in previous
work should be considered speculations, e.g.,
?It was recently proposed that ...?.
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 28?31,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Web-based Interfaces for Natural Language Processing Tools
Marc Light? and Robert Arens? and Xin Lu?
?Linguistics Department
?School of Library and Information Science
??Computer Science Department
University of Iowa
Iowa, USA 52242
{marc-light,robert-arens,xin-lu}@uiowa.edu
Abstract
We have built web interfaces to a number
of Natural Language Processing technolo-
gies. These interfaces allow students to
experiment with different inputs and view
corresponding output and inner workings
of the systems. When possible, the in-
terfaces also enable the student to mod-
ify the knowledge bases of the systems
and view the resulting change in behav-
ior. Such interfaces are important because
they allow students without computer sci-
ence background to learn by doing. Web
interfaces also sidestep issues of platform
dependency in software packages, avail-
able computer lab times, etc. We discuss
our basic approach and lessons learned.
1 Introduction
The Problem: Natural language processing (NLP)
technology is relevant to non-computer scientists:
our classes are populated by students from neuro-
science, speech pathology, linguistics, teaching of
foreign languages, health informatics, etc. To effec-
tively use NLP technology, it is helpful understand,
at some level, how it works. Hands-on experimen-
tation is an effective method for gaining such under-
standing. Unfortunately, to be able to experiment,
non-computer scientists often need to acquire some
programming skills and knowledge of the Unix op-
erating system. This can be time consuming and
tedious and can distract students from their central
goal of understanding how a technology works and
how best to employ it for their interests.
In addition, getting a technology to run on a set
lab machines can be problematic: the programs may
be developed for a different platform, e.g., a pro-
gram was developed for Linux but the lab machines
run MSWindows. Another hurdle is that machine
administrators are often loath to install applications
that they perceive as non-standard. Finally, lab times
can be restrictive and thus it is preferable to enable
students to use computers to which they have easy
access.
Our Solution: We built web interfaces to many
core NLP modules. These interfaces not only al-
low students to use a technology but also allow stu-
dents to modify and extend the technology. This en-
ables experimentation. We used server-side script-
ing languages to build such web interfaces. These
programs take input from a web browser, feed it to
the technology in question, gather the output from
the technology and send it back to the browser for
display to the student. Access to web browsers is
nearly ubiquitous and thus the issue of lab access is
side-stepped. Finally, the core technology need only
run on the web server platform. Many instructors
have access to web servers running on different plat-
forms and, in general, administering a web server is
easier than maintaining lab machines.
An Example: Finite state transduction is a core
NLP technology and one that students need to un-
derstand. The Cass partial parsing system (Abney,
1997) makes use of a cascade of FSTs. To use this
system, a student creates a grammar. This grammar
is compiled and then applied to sentences provided
28
Figure 1: Web interface to Cass
Figure 2: Cass Output
29
by the student. Prior to our work, the only interface
to Cass involved the Unix command line shell. Fig-
ure 3 shows an example session with the command
line interface. It exemplifies the sort of interface that
users must master in order to work with current hu-
man language technology.
1 emacs input.txt &
2 emacs grammar.txt &
3 source /usr/local/bin/setupEnv
3 reg gram.txt
4 Montytagger.py inTagged input.txt
5 cat inTagged |
6 wordSlashTagInput.pl |
7 cass -v -g gram.txt.fsc > cassOut
8 less cassOut
Figure 3: Cass Command Line Interface
A web-based interface hides many of the details, see
Figure 1 and Figure 2. For example, the use of an
ASCII-based text editor such as emacs become un-
necessary. In addition, the student does not need
to remembering flags such as -v -g and does not
need to know how to use Unix pipes, |, and out-
put redirection, >. None of this knowledge is ter-
ribly difficult but the amount accumulates quickly
and such information does not help the student un-
derstand how Cass works.
2 What we have built
To date, we have built web interfaces to nine NLP-
related technologies:
? the Cass parser (Abney, 1997),
? the MontyTagger Brill-style part-of-speech tag-
ger (Liu, 2004),
? the NLTK statistical part-of-speech tagger,
? a NLTK context-free grammar parser (Loper
and Bird, 2002),
? the Gsearch context-free grammar parser (Cor-
ley et al, 2001),
? the SenseRelate word sense disambiguation
system (Pedersen et al, 2005),
? a Perl Regular expression evaluator,
? a linguistic feature annotator,
? and a decision tree classifier (Witten and Frank,
1999).
These interfaces have been used in an introduction
to computational linguistics course and an introduc-
tion to creating and using corpora course. Prior to
the interface construction, no hands-on lab assign-
ments were given; instead all assignments were pen-
cil and paper. The NLP technologies listed above
were chosen because they fit into the material of the
course and because of their availability.
2.1 Allowing the student to process input
The simplest type of interface allows students to pro-
vide input and displays corresponding output. All
the interfaces above provide this ability. They all
start with HTML forms to collect input. In the sim-
plest case, PHP scripts process the forms, placing
input into files and then system calls are made to
run the NLP technology. Finally, output files are
wrapped in HTML and displayed to the user. The
basic PHP program remains largely unchanged from
one NLP technology to the next. In most cases, it
suffices to use the server file system to pass data
back and forth to the NLP program ? PHP pro-
vides primitives for creating and removing unique
temporary files. In only one case was it necessary to
use a semaphore on a hard-coded filename. We also
experimented with Java server pages and Perl CGI
scripts instead of PHP.
2.2 Allowing the student to modify knowledge
resources
The web interfaces to the Cass parser, Gsearch, and
MontyTagger allow the student to provide their cor-
responding knowledge base. For Cass and Gsearch,
an additional text box is provided for the grammars
they require. The rule sequence and lexicon that the
MontyTagger uses can be large and thus unwieldy
for a textarea form input element. We solved
the problem by preloading the textareas with a
?standard? rule sequence and lexicon which the stu-
dent can then modify. We also provided the ability to
upload the rule sequences and lexicon as files. One
problem with the file upload method is that it assume
that the students can generate ASCII-only files with
30
the appropriate line break character. This assump-
tion is often false.
An additional problem with allowing students
to modify knowledge resources is providing use-
ful feedback when these student-provided resources
contain syntax or other types of errors. At this point
we simply capture the stderr output of the pro-
gram and display it.
Finally, with some systems such as Spew
(Schwartz, 1999), and The Dada Engine (Bulhak,
1996), allowing web-based specification of knowl-
edge bases amounts to allowing the student to exe-
cute arbitrary code on the server machine, an obvi-
ous security problem.
2.3 Allowing the student to examine internal
system processing
Displaying system output with a web interface is rel-
atively easy; however, showing the internal work-
ings of a system is more challenging with a web
interface. At this point, we have only displayed
traces of steps of an algorithm. For example, the
NLTK context-free grammar parser interface pro-
vides a trace of the steps of the parsing algorithm.
One possible solution would be to generate Flash
code to animate a system?s processing.
2.4 Availability
The web pages are currently available at que.info-
science.uiowa.edu/?light/classes/compLing/ How-
ever, it is not our intent to provide server cycles for
the community but rather to provide the PHP scripts
open source so that others can run the interfaces
on their own servers. An instructor at another
university has already made use of our code.
3 Lessons learned
? PHP is easier to work with than Java Server
Pages and CGI scripts;
? requiring users to paste input into text boxes is
superior to allowing user to upload files (for se-
curity reasons and because it is easier to control
the character encoding used);
? getting debugging information back to the stu-
dent is very important;
? security is an issue since one is allowing users
to initiate computationally intensive processes;
? it is still possible for students to claim the inter-
face does not work for them (even though we
used no client-side scripting).
? Peer learning is less likely than in a lab set-
ting; however, we provided a web forum and
this seems to alleviated the problem somewhat.
4 Summary
At the University of Iowa, many students, who want
to learn about natural language processing, do not
have the requisite Unix and programming skills to
do labs using command line interfaces. In addition,
our lab machines run MSWindows, the instructors
do not administer the machines, and there are restric-
tive lab hours. Thus, until recently assignments con-
sisted of pencil-and-paper problems. We have built
web-based interfaces to a number of NLP modules
that allow students to use, modify, and learn.
References
Steven Abney. 1997. Partial parsing via finite-state cas-
cades. Natural Language Engineering, 2(4).
Andrew Bulhak. 1996. The dada engine.
http://dev.null.org/dadaengine/.
S. Corley, M. Corley, F. Keller, M. Crocker, and
S. Trewin. 2001. Finding Syntactic Structure in Un-
parsed Corpora: The Gsearch Corpus Query System.
Computers and the Humanities, 35:81?94.
Hugo Liu. 2004. Montylingua: An end-to-end natural
language processor with common sense. homepage.
Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In Proc. of the ACL-02 Workshop
on Effective Tools and Methods for Teaching Natural
Language Processing and Computational Linguistics.
Ted Pedersen, Satanjeev Banerjee, and Siddharth Pat-
wardhan. 2005. Maximizing Semantic Relatedness to
Perform Word Sense Disambiguation. Supercomput-
ing institute research report umsi 2005/25, University
of Minnesota.
Randal Schwartz. 1999. Random sentence generator.
Linux Magazine, September.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
31
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 32?36,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Making Hidden Markov Models More Transparent
Nashira Richard Lincoln? and Marc Light?
??Linguistics Department
?School of Library and Information Science
?Computer Science Department
University of Iowa
Iowa, USA 52242
{nashira-lincoln, marc-light}@uiowa.edu
Abstract
Understanding the decoding algorithm for
hidden Markov models is a difficult task
for many students. A comprehensive un-
derstanding is difficult to gain from static
state transition diagrams and tables of ob-
servation production probabilities. We
have built a number of visualizations de-
picting a hidden Markov model for part-
of-speech tagging and the operation of the
Viterbi algorithm. The visualizations are
designed to help students grasp the oper-
ation of the HMM. In addition, we have
found that the displays are useful as de-
bugging tools for experienced researchers.
1 Introduction
Hidden Markov Models (HMMs) are an important
part of the natural language processing toolkit and
are often one of the first stochastic generation mod-
els that students1 encounter. The corresponding
Viterbi algorithm is also often the first example
of dynamic programming that students encounter.
Thus, HMMs provide an opportunity to start stu-
dents on the correct path of understanding stochas-
tic models, not simply treating them as black boxes.
Unfortunately, static state transition diagrams, ta-
bles of probability values, and lattice diagrams are
not enough for many students. They have a general
idea of how a HMM works but often have common
1The Introduction to Computational Linguistics course at
the University of Iowa has no prerequisites, and over half the
students are not CS majors.
misconceptions. For example, we have found that
students often believe that as the Viterbi algorithm
calculates joint state sequence observation sequence
probabilities, the best state sequence so far is always
a prefix of global best path. This is of course false.
Working a long example to show this is very tedious
and thus text books seldom provide such examples.
Even for practitioners, HMMs are often opaque
in that the cause of a mis-tagging error is often left
uncharacterized. A display would be helpful to pin-
point why an HMM chose an incorrect state se-
quence instead of the correct one.
Below we describe two displays that attempt to
remedy the above mentioned problems and we dis-
cuss a Java implementation of these displays in the
context of a part-of-speech tagging HMM (Kupiec,
1992). The system is freely available and has an
XML model specification that allows models calcu-
lated by other methods to be viewed. (A standard
maximum likelihood estimation was implemented
and can be used to create models from tagged data.
A model is also provided.)
2 Displays
Figure 1 shows a snapshot of our first display. It
contains three kinds of information: most likely
path for input, transition probabilities, and history of
most likely prefixes for each observation index in the
Viterbi lattice. The user can input text at the bottom
of the display, e.g., Pelham pointed out that Geor-
gia voters rejected the bill. The system then runs
Viterbi and animates the search through all possible
state sequences and displays the best state sequence
prefix as it works its way through the observation
32
Figure 1: The system?s main display. Top pane: shows the state space and animates the derivation of the
most likely path for ?Pelman pointed out that Georgia voters ...?; Middle pane: a mouse-over-triggered bar
graph of out transition probabilities for a state; Bottom pane: a history of most likely prefixes for each
observation index in the Viterbi lattice. Below the panes is the input text field.
33
Figure 2: Contrast display: The user enters a sequence on the top text field and presses enter, the sequence
is tagged and displayed in both the top and bottom text fields. Finally, the user changes any incorrect tags in
the top text field and presses enter and the probability ratio bars are then displayed.
34
sequence from left to right (these are lines connect-
ing the states in Figure 1). At any point, the stu-
dent can mouse-over a state to see probabilities for
transitions out of that state (this is the bar graph in
Figure 1). Finally, the history of most likely pre-
fixes is displayed (this history appears below the bar
graph in Figure 1). We mentioned that students often
falsely believe that the most likely prefix is extended
monotonically. By seeing the path through the states
reconfigure itself in the middle of the observation se-
quence and by looking at the prefix history, a student
has a good chance of dispelling the false belief of
monotonicity.
The second display allows the user to contrast two
state sequences for the same observation sequence.
See Figure 2. For each contrasting state pairs, it
shows the ratio of the corresponding transition to
each state and it shows the ratio of the generation of
the observation conditioned on each state. For exam-
ple, in Figure 2 the transition DT?JJ is less likely
than DT?NNP. The real culprit is generation proba-
bility P(Equal|JJ) which is almost 7 times larger than
P(Equal|NNP). Later in the sequence we see a simi-
lar problem with generating opportunity from a NNP
state. These generation probabilities seem to drown
out any gains made by the likelihood of NNP runs.
To use this display, the user types in a sentence
in the box above the graph and presses enter. The
HMM is used to tag the input. The user then modi-
fies (e.g., corrects) the tag sequence and presses en-
ter and the ratio bars then appear.
Let us consider another example: in Figure 2, the
mis-tagging of raises as a verb instead of a noun at
the end of the sentence. The display shows us that
although NN?NNS is more likely than NN?VBZ,
the generation probability for raises as a verb is
over twice as high as a noun. (If this pattern of
mis-taggings caused by high generation probabil-
ity ratios was found repeatedly, we might consider
smoothing these distributions more aggressively.)
3 Implementation
The HMM part-of-speech tagging model and
corresponding Viterbi algorithm were implemented
based on their description in the updated version,
http://www.cs.colorado.edu/?martin/
SLP/updated.html , of chapter 8 of (Jurafsky
and Martin, 2000). A model was trained using
Maximum Likelihood from the UPenn Treebank
(Marcus et al, 1993). The input model file is
encoded using XML and thus models built by other
systems can be read in and displayed.
The system is implemented in Java and requires
1.4 or higher to run. It has been tested on Linux and
Apple operating systems. We will release it under a
standard open source license.
4 Summary and future work
Students (and researchers) need to understand
HMMs. We have built a display that allow users
to probe different aspects of an HMM and watch
Viterbi in action. In addition, our system provides
a display that allows users to contrast state sequence
probabilities. To drive these displays, we have built
a standard HMM system including parameter esti-
mating and decoding and provide a part-of-speech
model trained on UPenn Treebank data. The system
can also read in models constructed by other sys-
tems.
This system was built during this year?s offering
of Introduction to Computational Linguistics at the
University of Iowa. In the Spring of 2006 it will be
deployed in the classroom for the first time. We plan
on giving a demonstration of the system during a
lecture on HMMs and part-of-speech tagging. A re-
lated problem set using the system will be assigned.
The students will be given several mis-tagged sen-
tences and asked to analyze the errors and report
on precisely why they occurred. A survey will be
administered at the end and improvements will be
made to the system based on the feedback provided.
In the future we plan to implement Good-Turing
smoothing and a method for dealing with unknown
words. We also plan to provide an additional display
that shows the traditional Viterbi lattice figure, i.e.,
observations listed left-to-right, possible states listed
from top-to-bottom, and lines from left-to-right con-
necting states at observation index i with the previ-
ous states, i-1, that are part of the most likely state
sequence to i. Finally, we would like to incorpo-
rate an additional display that will provide a visual-
ization of EM HMM training. We will use (Eisner,
2002) as a starting point.
35
References
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proc. of the
ACL 2002 Workshop on effective tools and method-
ologies for teaching natural language processing and
computational linguistics.
Daniel Jurafsky and James H. Martin. 2000. Speech and
Language Processing: an introduction to natural lan-
guage processing, and computational linguistics, and
speech recognition. Prentice-Hall.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden markov model. Computer Speech and Lan-
guage, 6:225?242.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330, June.
36
