Proceedings of NAACL HLT 2007, Companion Volume, pages 37?40,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Situated Models of Meaning for Sports Video Retrieval  
Michael Fleischman 
MIT Media Lab 
mbf@mit.edu 
Deb Roy 
MIT Media Lab 
dkroy@media.mit.edu 
 
Abstract 
Situated models of meaning ground words in the 
non-linguistic context, or situation, to which they 
refer.  Applying such models to sports video re-
trieval requires learning appropriate representa-
tions for complex events.  We propose a method 
that uses data mining to discover temporal pat-
terns in video, and pair these patterns with associ-
ated closed captioning text.  This paired corpus is 
used to train a situated model of meaning that sig-
nificantly improves video retrieval performance. 
1 Introduction 
Recent advances in digital broadcasting and re-
cording allow fans access to an unprecedented 
amount of sports video.  The growing need to 
manage and search large video collections presents 
a challenge to traditional information retrieval (IR) 
technologies.  Such methods cannot be directly 
applied to video data, even when closed caption 
transcripts are available; for, unlike text docu-
ments, the occurrence of a query term in a video is 
often not enough to assume the video?s relevance 
to that query.  For example, when searching 
through video of baseball games, returning all clips 
in which the phrase ?home run? occurs, results 
primarily in video of events where a home run 
does not actually occur.  This follows from the fact 
that in sports, as in life, people often talk not about 
what is currently happening, but rather, they talk 
about what did, might, or will happen in the future.   
Traditional IR techniques cannot address such 
problems because they model the meaning of a 
query term strictly by that term?s relationship to 
other terms.  To build systems that successfully 
search video, IR techniques should be extended to 
exploit not just linguistic information but also ele-
ments of the non-linguistic context, or situation, 
that surrounds language use.  This paper presents a 
method for video event retrieval from broadcast 
sports that achieves this by learning a situated 
model of meaning from an unlabeled video corpus. 
The framework for the current model is derived 
from previous work on computational models of 
verb learning (Fleischman & Roy, 2005).  In this 
earlier work, meaning is defined by a probabilistic 
mapping between words and representations of the 
non-linguistic events to which those words refer.  
In applying this framework to events in video, we 
follow recent work on video surveillance in which 
complex events are represented as temporal rela-
tions between lower level sub-events (Hongen et 
al., 2004).  While in the surveillance domain, hand 
crafted event representations have been used suc-
cessfully, the greater variability of content in 
broadcast sports demands an automatic method for 
designing event representations.   
The primary focus of this paper is to present a 
method for mining such representations from large 
video corpora, and to describe how these represen-
tations can be mapped to natural language.  We 
focus on a pilot dataset of broadcast baseball 
games.  Pilot video retrieval tests show that using a 
situated model significantly improves perform-
ances over traditional language modeling methods. 
2 Situated Models of Meaning 
Building situated models of meaning operates in 
three phases (see Figure 1): first, raw video data is 
abstracted into multiple streams of discrete fea-
tures.  Temporal data mining techniques are then 
applied to these feature streams to discover hierar-
chical temporal patterns.  These temporal patterns 
form the event representations that are then 
mapped to words from the closed caption stream. 
2.1 Feature Extraction  
The first step in representing events in video is to 
abstract the very high dimensional raw video data 
into more semantically meaningful streams of in-
formation.  Ideally, these streams would corre-
spond to basic events that occur in sports video 
(e.g., hitting, throwing, catching, kicking, etc.). 
Due to the limitations of computer vision tech-
niques, extracting such ideal features is often in-
feasible.  However, by exploiting the ?language of
37
Figure 1.  Video processing pipeline for learning situated models of meaning. 
 
film? that is used to produce sports video, informa-
tive features can be extracted that are also easy to 
compute.  Thus, although we cannot easily identify 
a player hitting the ball, we can easily detect fea-
tures that correlate with hitting: e.g., when a scene 
focusing on the pitching mound immediately 
jumps to one zooming in on the field (Figure 1).  
While such correlations are not perfect, pilot tests 
show that baseball events can be classified using 
such features (Fleischman et. al., in prep). 
Importantly, this is the only phase of our frame-
work that is domain specific; i.e., it is the only as-
pect of the framework designed specifically for use 
with baseball data.  Although many feature types 
can be extracted, we focus on only two feature 
types: visual context, and camera motion. 
 
Visual Context 
 
Visual context features encode general properties 
of the visual scene in a video segment.  The first 
step in extracting such features is to split the raw 
video into ?shots? based on changes in the visual 
scene due to editing (e.g., jumping from a close up 
of the pitcher to a wide angle of the field).  Shot 
detection is a well studied problem in multimedia 
research; in this work, we use the method of 
Tardini et al (2005) because of its speed and 
proven performance on sports video.   
After a game is segmented into shots, each shot 
is categorized into one of three categories: pitch-
ing-scene, field-scene, or other.  Categorization is 
based on image features (e.g., color histograms, 
edge detection, motion analysis) extracted from an 
individual key frame chosen from that shot.  A de-
cision tree is trained (with bagging and boosting) 
using the WEKA machine learning toolkit that 
achieves over 97% accuracy on a held out dataset.  
Camera Motion 
 
Whereas visual context features provide informa-
tion about the global situation that is being ob-
served, camera motion features afford more precise 
information about the actions occurring in the 
video.  The intuition here is that the camera is a 
stand in for a viewer?s focus of attention.  As ac-
tion in the video takes place, the camera moves to 
follow it, mirroring the action itself, and providing 
an informative feature for event representation.   
Detecting camera motion (i.e., pan/tilt/zoom) is a 
well-studied problem in video analysis.  We use 
the system of (Bouthemy et al, 1999) which com-
putes the pan, tilt, and zoom motions using the pa-
rameters of a two-dimensional affine model fit to 
every pair of sequential frames in a video segment.  
The output of this system is then clustered into 
characteristic camera motions (e.g. zooming in fast 
while panning slightly left) using a 1st order Hid-
den Markov Model  with 15 states, implemented 
using the Graphical Modeling Toolkit (GMTK).   
2.2 Temporal Pattern Mining 
In this step, temporal patterns are mined from the 
features abstracted from the raw video data.  As 
described above, ideal semantic features (such as 
hitting and catching) cannot be extracted easily 
from video. We hypothesize that finding temporal 
patterns between scene and camera motion features 
can produce representations that are highly corre-
lated with sports events.  Importantly, such tempo-
ral patterns are not strictly sequential, but rather, 
are composed of features that can occur in complex 
and varied temporal relations to each other.  For 
example, Figure 1 shows the representation for a 
fly ball event that is composed of: a camera pan-
38
ning up followed by a camera pan down, occurring 
during a field scene, and before a pitching scene. 
Following previous work in video content classi-
fication (Fleischman et al, 2006), we use tech-
niques from temporal data mining to discover 
event patterns from feature streams.  The algorithm 
we use is fully unsupervised. It processes feature 
streams by examining the relations that occur be-
tween individual features within a moving time 
window.  Following Allen (1984), any two features 
that occur within this window must be in one of 
seven temporal relations with each other (e.g. be-
fore, during, etc.).  The algorithm keeps track of 
how often each of these relations is observed, and 
after the entire video corpus is analyzed, uses chi-
square analyses to determine which relations are 
significant.  The algorithm iterates through the 
data, and relations between individual features that 
are found significant in one iteration (e.g. 
[BEFORE, camera panning up, camera panning 
down]), are themselves treated as individual fea-
tures in the next.  This allows the system to build 
up higher-order nested relations in each iteration 
(e.g. [DURING, [BEFORE, camera panning up, 
camera panning down], field scene]]).  The tempo-
ral patterns found significant in this way are then 
used as the event representations that are then 
mapped to words. 
2.3 Linguistic Mapping 
The last step in building a situated model of mean-
ing is to map words onto the representations of 
events mined from the raw video.  We equate the 
learning of this mapping to the problem of estimat-
ing the conditional probability distribution of a 
word given a video event representation.  Similar 
to work in image retrieval (Barnard et al, 2003), 
we cast the problem in terms of Machine Transla-
tion: given a paired corpus of words and a set of 
video event representations to which they refer, we 
make the IBM Model 1 assumption and use the 
expectation-maximization method to estimate the 
parameters (Brown et al, 1993):   
?
=
+
=
m
j
ajm jvideowordpl
C
videowordp
1
)|()1()|(
     (1) 
This paired corpus is created from a corpus of 
raw video by first abstracting each video into the 
feature streams described above.  For every shot 
classified as a pitching scene, a new instance is 
created in the paired corpus corresponding to an 
event that starts at the beginning of that shot and 
ends exactly four shots after.  This definition of an 
event follows from the fact that most events in 
baseball must start with a pitch and usually do not 
last longer than four shots (Gong et al, 2004).   
For each of these events in the paired corpus, a 
representation of the video is generated by match-
ing all patterns (and the nested sub-patterns) found 
from temporal mining to the feature streams of the 
event.  These video representations are then paired 
with all the words from the closed captioning that 
occur during that event (plus/minus 10 seconds).   
3 Experiments 
Work on video IR in the news domain often fo-
cuses on indexing video data using a set of image 
classifiers that categorize shots into pre-determined 
concepts (e.g. flag, outdoors, George Bush, etc.).  
Text queries must then be translated (sometimes 
manually) in terms of these concepts (Worring & 
Snoek, 2006).  Our work focuses on a more auto-
mated approach that is closer to traditional IR tech-
niques.  Our framework extends the language 
modeling approach of Ponte and Croft (1998) by 
incorporating a situated model of meaning.   
In Ponte and Croft (1998), documents relevant to 
a query are ranked based on the probability that 
each document generated each query term.  We 
follow this approach for video events, making the 
assumption that the relevance of an event to a 
query depends both on the words associated with 
the event (i.e. what was said while the event oc-
curred), as well as the situational context modeled 
by the video event representations: 
? ??=
query
word
videowordpcaptionwordpeventqueryp )1()|()|()|( ??  (2) 
The p(word|caption) is estimated using the lan-
guage modeling technique described in Ponte and 
Croft (1998).  The p(word|video) is estimated as in 
equation 1 above.  ? is used to weight the models.  
 
Data 
 
The system has been evaluated on a pilot set of 6 
broadcast baseball games totaling about 15 hours 
and 1200 distinct events.  The data represents 
video of 9 different teams, at 4 different stadiums, 
broadcast on 4 different stations.  Highlights (i.e., 
events which terminate with the player either out 
or safe) were hand annotated, and categorized ac-
cording to the type of the event (e.g., strikeout vs. 
homerun), the location of the event (e.g., right field 
vs. infield), and the nature of the event (e.g., fly 
ball vs. line drive).  Each of these categories was 
39
used to automatically select query terms to be used 
in testing.  Similar to Berger & Lafferty (1999), the 
probability distribution of terms given a category is 
estimated using a normalized log-likelihood ratio 
(Moore, 2004), and query terms are sampled ran-
domly from this distribution.  This gives us a set of 
queries for each annotated category (e.g., strikeout: 
?miss, chasing?; flyball: ?fly, streak?).  Although 
much noisier than human produced queries, this 
procedure generates a large amount of test queries 
for which relevant results can easily be determined 
(e.g., if a returned event for the query ?fly, streak? 
is of the flyball category, it is marked relevant). 
 Experiments are reported using 6-fold cross 
validation during which five games are used to 
train the situated model while the sixth is held out 
for testing.  Because data is sparse, the situation 
model is trained only on the hand annotated high-
light events.  However, retrieval is always tested 
using both highlight and non-highlight events.  
Figure 2.  Effect of situated model on video IR. 
 
 Results 
 
Figure 2 shows results for 520 automatically gen-
erated queries of one to four words in length.  
Mean average precision (MAP), a common metric 
that combines elements of precision, recall, and 
ranking, is used to measure the relevance of the top 
five results returned for each query.  We show re-
sults for the system using only linguistic informa-
tion (i.e. ?=1), only non-linguistic information (i.e. 
?=0), and both information together (i.e. ?=0.5).   
The poor performance of the system using only 
non-linguistic information is expected given the 
limited training data and the simple features used 
to represent events.  Interestingly, using only lin-
guistic information produces similarly poor per-
formance.  This is a direct result of announcers? 
tendency to discuss topics not currently occurring 
in the video.  By combining text and video analy-
ses, though, the system performs significantly bet-
ter (p<0.01) by determining when the observed 
language actually refers to the situation at hand.  
4 Conclusion 
We have presented a framework for video retrieval 
that significantly out-performs traditional IR meth-
ods applied to closed caption text. Our new ap-
proach incorporates the visual content of baseball 
video using automatically learned event represen-
tations to model the situated meaning of words. 
Results indicate that integration of situational con-
text dramatically improves performance over tradi-
tional methods alone.  In future work we will 
examine the effects of applying situated models of 
meaning to other tasks (e.g., machine translation).   
References 
Allen, J.F. (1984). A General Model of Action and Time. Arti-
ficial Intelligence. 23(2). 
Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, D, 
and Jordan, M. (2003), "Matching Words and Pictures," 
Journal of Machine Learning Research, Vol 3. 
Berger, A.  and Lafferty, J. (1999). Information Retrieval as 
Statistical Translation. In Proceedings of SIGIR-99. 
Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A unified 
approach to shot change detection and camera motion char-
acterization. IEEE Trans. on Circuits and Systems for Video 
Technology, 9(7):1030-1044. 
Brown, P., Della Pietra, S., Della Pietra, V. Mercer, R. (1993). 
The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(10). 
Fleischman, M. and Roy, D. (2005). Intentional Context in 
Situated Language Learning.  In Proc. of 9th Conference on 
Comp. Natural Language Learning. 
Fleischman, M., DeCamp, P. Roy, D.  (2006). Mining Tempo-
ral Patterns of Movement for Video Content Classification.  
The 8th ACM SIGMM International Workshop on Multi-
media Information Retrieval. 
Fleischman, M., Roy, B., Roy, D. (in prep.). Automated Fea-
ture Engineering inBaseball Highlight Classification. 
Gong, Y., Han, M., Hua, W., Xu, W.  (2004). Maximum en-
tropy model-based baseball highlight detection and classifi-
cation.  Computer Vision and Image Understanding. 96(2). 
Hongen, S., Nevatia, R. Bremond, F. (2004). Video-based 
event recognition: activity representation and probabilistic 
recognition methods.  Computer Vision and Image Under-
standing. 96(2). pp: 129 - 162  
Moore, Robert C. (2004). Improving IBM Word Alignment 
Model 1. in Proc. of 42nd ACL.  
Ponte, J.M., and Croft, W.B. (1998). A Language Modeling 
Approach to Information Retrieval. In Proc. of SIGIR?98.  
Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005). Shot 
Detection and Motion Analysis for Automatic MPEG-7 
Annotation of Sports Videos.  In 13th International Confer-
ence on Image Analysis and Processing. 
Worring, M., Snoek, C.. (2006). Semantic Indexing and Re-
trieval of Video.  Tutorial at ACM Multimedia 
40
Proceedings of ACL-08: HLT, pages 121?129,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Grounded Language Modeling for 
Automatic Speech Recognition of Sports Video  
Michael Fleischman 
Massachusetts Institute of Technology 
Media Laboratory 
mbf@mit.edu 
Deb Roy 
Massachusetts Institute of Technology 
Media Laboratory 
dkroy@media.mit.edu 
 
 
Abstract 
Grounded language models represent the rela-
tionship between words and the non-linguistic 
context in which they are said.  This paper de-
scribes how they are learned from large cor-
pora of unlabeled video, and are applied to the 
task of automatic speech recognition of sports 
video.  Results show that grounded language 
models improve perplexity and word error 
rate over text based language models, and fur-
ther, support video information retrieval better 
than human generated speech transcriptions. 
1 Introduction 
Recognizing speech in broadcast video is a neces-
sary precursor to many multimodal applications 
such as video search and summarization (Snoek 
and Worring, 2005;).  Although performance is 
often reasonable in controlled environments (such 
as studio news rooms), automatic speech recogni-
tion (ASR) systems have significant difficulty in 
noisier settings (such as those found in live sports 
broadcasts) (Wactlar et al, 1996).  While many 
researches have examined how to compensate for 
such noise using acoustic techniques, few have 
attempted to leverage information in the visual 
stream to improve speech recognition performance 
(for an exception see Murkherjee and Roy, 2003).   
In many types of video, however, visual context 
can provide valuable clues as to what has been 
said.  For example, in video of Major League 
Baseball games, the likelihood of the phrase ?home 
run? increases dramatically when a home run has 
actually been hit.  This paper describes a method 
for incorporating such visual information in an 
ASR system for sports video.  The method is based 
on the use of grounded language models to repre-
sent the relationship between words and the non-
linguistic context to which they refer (Fleischman 
and Roy, 2007).   
Grounded language models are based on re-
search from cognitive science on grounded models 
of meaning. (for a review see Roy, 2005, and Roy 
and Reiter, 2005).  In such models, the meaning of 
a word is defined by its relationship to representa-
tions of the language users? environment.  Thus, 
for a robot operating in a laboratory setting, words 
for colors and shapes may be grounded in the out-
puts of its computer vision system (Roy & Pent-
land, 2002); while for a simulated agent operating 
in a virtual world, words for actions and events 
may be mapped to representations of the agent?s 
plans or goals (Fleischman & Roy, 2005).   
This paper extends previous work on grounded 
models of meaning by learning a grounded lan-
guage model from naturalistic data collected from 
broadcast video of Major League Baseball games.  
A large corpus of unlabeled sports videos is col-
lected and paired with closed captioning transcrip-
tions of the announcers? speech. 1  This corpus is 
used to train the grounded language model, which 
like traditional language models encode the prior 
probability of words for an ASR system.  Unlike 
traditional language models, however, grounded 
language models represent the probability of a 
word conditioned not only on the previous word(s), 
but also on features of the non-linguistic context in 
which the word was uttered.   
Our approach to learning grounded language 
models operates in two phases.  In the first phase, 
events that occur in the video are represented using 
hierarchical temporal pattern automatically mined  
                                                          
1
 Closed captioning refers to human transcriptions of speech 
embedded in the video stream primarily for the hearing im-
paired.  Closed captioning is reasonably accurate (although not 
perfect) and available on some, but not all, video broadcasts. 
121
 Figure 1.  Representing events in video.  a) Events are represented by first abstracting the raw video into visual con-
text, camera motion, and audio context features.  b) Temporal data mining is then used to discover hierarchical tem-
poral patterns in the parallel streams of features.  c) Temporal patterns found significant in each iteration are stored 
in a codebook that is used to represent high level events in video. 
 
from low level features.  In the second phase, a 
conditional probability distribution is estimated 
that describes the probability that a word was ut-
tered given such event representations. In the fol-
lowing sections we describe these two aspects of 
our approach and evaluate the performance of our 
grounded language model on a speech recognition 
task using video highlights from Major League 
Baseball games.  Results indicate improved per-
formance using three metrics: perplexity, word 
error rate, and precision on an information retrieval 
task. 
2 Representing Events in Sports Video 
Recent work in video surveillance has demon-
strated the benefit of representing complex events 
as temporal relations between lower level sub-
events (Hongen et al, 2004).  Thus, to represent 
events in the sports domain, we would ideally first 
represent the basic sub events that occur in sports 
video (e.g., hitting, throwing, catching, running, 
etc.) and then build up complex events (such as 
home run) as a set of temporal relations between 
these basic events.  Unfortunately, due to the limi-
tations of computer vision techniques, reliably 
identifying such basic events in video is not feasi-
ble.  However, sports video does have characteris-
tics that can be exploited to effectively represent 
complex events. 
Like much broadcast video, sports video is 
highly produced, exploiting many different camera 
angles and a human director who selects which 
camera is most appropriate given what is happen-
ing on the field.  The styles that different directors 
employ are extremely consistent within a sport and 
make up a ?language of film? which the machine 
can take advantage of in order to represent the 
events taking place in the video. 
Thus, even though it is not easy to automati-
cally identify a player hitting a ball in video, it is 
easy to detect features that correlate with hitting, 
e.g., when a scene focusing on the pitching mound 
immediately jumps to one zooming in on the field 
(see Figure 1).  Although these correlations are not 
perfect, experiments have shown that baseball 
events can be classified using such features 
(Fleischman et al, 2007).   
We exploit the language of film to represent 
events in sports video in two phases.  First, low 
level features that correlate with basic events in 
sports are extracted from the video stream.  Then, 
temporal data mining is used to find patterns 
within this low level event stream.   
2.1 Feature Extraction 
We extract three types of features: visual con-
text features, camera motion features, and audio 
context features.   
122
Visual Context Features 
Visual context features encode general proper-
ties of the visual scene in a video segment.  Super-
vised classifiers are trained to identify these 
features, which are relatively simple to classify in 
comparison to high level events (like home runs) 
that require more training data and achieve lower 
accuracy.  The first step in classifying visual con-
text features is to segment the video into shots (or 
scenes) based on changes in the visual scene due to 
editing (e.g. jumping from a close up to a wide 
shot of the field).  Shot detection and segmentation 
is a well studied problem; in this work we use the 
method of Tardini et al (2005).   
After the video is segmented into shots, indi-
vidual frames (called key frames) are selected and 
represented as a vector of low level features that 
describe the key frame?s color distribution, en-
tropy, etc. (see Fleischman and Roy, 2007 for the 
full list of low level features used).  The WEKA 
machine learning package is used to train a boosted 
decision tree to classify these frames into one of 
three categories: pitching-scene, field-scene, other 
(Witten and Frank, 2005).  Those shots whose key 
frames are classified as field-scenes are then sub-
categorized (using boosted decision trees) into one 
of the following categories: infield, outfield, wall, 
base, running, and misc.  Performance of these 
classification tasks is approximately 96% and 90% 
accuracy respectively. 
Camera Motion Features 
In addition to visual context features, we also 
examine the camera motion that occurs within a 
video.  Unlike visual context features, which pro-
vide information about the global situation that is 
being observed, camera motion features represent 
more precise information about the actions occur-
ring in a video.  The intuition here is that the cam-
era is a stand in for a viewer?s focus of attention.  
As actions occur in a video, the camera moves to 
follow it; this camera motion thus mirrors the ac-
tions themselves, providing informative features 
for event representation.   
Like shot boundary detection, detecting the mo-
tion of the camera in a video (i.e., the amount it 
pans left to right, tilts up and down, and zooms in 
and out) is a well-studied problem.  We use the 
system of Bouthemy et al (1999) which computes 
the camera motion using the parameters of a two-
dimensional affine model to fit every pair of se-
quential frames in a video.  A 15 state 1st order 
Hidden Markov Model, implemented with the 
Graphical Modeling Toolkit,2 then converts the 
output of the Bouthemy system into a stream of 
clustered characteristic camera motions (e.g. state 
12 clusters together motions of zooming in fast 
while panning slightly left). 
Audio Context 
The audio stream of a video can also provide use-
ful information for representing non-linguistic con-
text.  We use boosted decision trees to classify 
audio into segments of speech, excited_speech, 
cheering, and music.  Classification operates on a 
sequence of overlapping 30 ms frames extracted 
from the audio stream. For each frame, a feature 
vector is computed using, MFCCs (often used in 
speaker identification and speech detection tasks), 
as well as energy, the number of zero crossings, 
spectral entropy, and relative power between dif-
ferent frequency bands.  The classifier is applied to 
each frame, producing a sequence of class labels. 
These labels are then smoothed using a dynamic 
programming cost minimization algorithm (similar 
to those used in Hidden Markov Models).  Per-
formance of this system achieves between 78% 
and 94% accuracy.   
2.2 Temporal Pattern Mining 
Given a set of low level features that correlate with 
the basic events in sports, we can now focus on 
building up representations of complex events.  
Unlike previous work (Hongen et al, 2005) in 
which representations of the temporal relations 
between low level events are built up by hand, we 
employ temporal data mining techniques to auto-
matically discover such relations from a large cor-
pus of unannotated video. 
As described above, ideal basic events (such as 
hitting and catching) cannot be identified easily in 
sports video. By finding temporal patterns between 
audio, visual and camera motion features, how-
ever, we can produce representations that are 
highly correlated with sports events.  Importantly, 
such temporal patterns are not strictly sequential, 
but rather, are composed of features that can occur 
                                                          
2
 http://ssli.ee.washington.edu/~bilmes/gmtk/ 
123
in complex and varied temporal relations to each 
other.   
To find such patterns automatically, we follow 
previous work in video content classification in 
which temporal data mining techniques are used to 
discover event patterns within streams of lower 
level features.  The algorithm we use is fully unsu-
pervised and proceeds by examining the relations 
that occur between features in multiple streams 
within a moving time window.  Any two features 
that occur within this window must be in one of 
seven temporal relations with each other (e.g. be-
fore, during, etc.) (Allen, 1984).  The algorithm 
keeps track of how often each of these relations is 
observed, and after the entire video corpus is ana-
lyzed, uses chi-square analyses to determine which 
relations are significant.  The algorithm iterates 
through the data, and relations between individual 
features that are found significant in one iteration 
(e.g. [OVERLAP, field-scene, cheer]), are them-
selves treated as individual features in the next.  
This allows the system to build up higher-order 
nested relations in each iteration (e.g. [BEFORE, 
[OVERLAP, field-scene, cheer], field scene]]).   
The temporal patterns found significant in this 
way make up a codebook which can then be used 
as a basis for representing a video.  The term code-
book is often used in image analysis to describe a 
set of features (stored in the codebook) that are 
used to encode raw data (images or video).  Such 
codebooks are used to represent raw video using 
features that are more easily processed by the 
computer.  
Our framework follows a similar approach in 
which raw video is encoded (using a codebook of 
temporal patterns) as follows.  First, the raw video 
is abstracted into the visual context, camera mo-
tion, and audio context feature streams (as de-
scribed in Section 2.1).  These feature streams are 
then scanned, looking for any temporal patterns 
(and nested sub-patterns) that match those found in 
the codebook.  For each pattern, the duration for 
which it occurs in the feature streams is treated as 
the value of an element in the vector representation 
for that video.   
Thus, a video is represented as an n length vec-
tor, where n is the total number of temporal pat-
terns in the codebook.  The value of each element 
of this vector is the duration for which the pattern 
associated with that element was observed in the 
video.  So, if a pattern was not observed in a video 
at all, it would have a value of 0, while if it was 
observed for the entire length of the video, it would 
have a value equal to the number of frames present 
in that video.   
Given this method for representing the non-
linguistic context of a video, we can now examine 
how to model the relationship between such con-
text and the words used to describe it.  
3 Linguistic Mapping 
Modeling the relationship between words and non-
linguistic context assumes that the speech uttered 
in a video refers consistently (although not exclu-
sively) to the events being represented by the tem-
poral pattern features.  We model this relationship, 
much like traditional language models, using con-
ditional probability distributions.  Unlike tradi-
tional language models, however, our grounded 
language models condition the probability of a 
word not only on the word(s) uttered before it, but 
also on the temporal pattern features that describe 
the non-linguistic context in which it was uttered.  
We estimate these conditional distributions using a 
framework similar that used for training acoustic 
models in ASR and translation models in Machine 
Translation (MT). 
We generate a training corpus of utterances 
paired with representations of the non-linguistic 
context in which they were uttered.  The first step 
in generating this corpus is to generate the low 
level features described in Section 2.1 for each 
video in our training set.  We then segment each 
video into a set of independent events based on the 
visual context features we have extracted.  We fol-
low previous work in sports video processing 
(Gong et al, 2004) and define an event in a base-
ball video as any sequence of shots starting with a 
pitching-scene and continuing for four subsequent 
shots.  This definition follows from the fact that the 
vast majority of events in baseball start with a 
pitch and do not last longer than four shots.  For 
each of these events in our corpus, a temporal pat-
tern feature vector is generated as described in sec-
tion 2.2.  These events are then paired with all the 
words from the closed captioning transcription that 
occur during each event (plus or minus 10 sec-
onds).  Because these transcriptions are not neces-
sarily time synched with the audio, we use the 
method described in Hauptmann and Witbrock 
124
(1998) to align the closed captioning to the an-
nouncers? speech.   
Previous work has examined applying models 
often used in MT to the paired corpus described 
above (Fleischman and Roy, 2006).  Recent work 
in automatic image annotation (Barnard et al, 
2003; Blei and Jordan, 2003) and natural language 
processing (Steyvers et al, 2004), however, have 
demonstrated the advantages of using hierarchical 
Bayesian models for related tasks.  In this work we 
follow closely the Author-Topic (AT) model (Stey-
vers et al, 2004) which is a generalization of La-
tent Dirichlet Allocation (LDA) (Blei et al, 2005).3   
LDA is a technique that was developed to 
model the distribution of topics discussed in a large 
corpus of documents.  The model assumes that 
every document is made up of a mixture of topics, 
and that each word in a document is generated 
from a probability distribution associated with one 
of those topics.  The AT model generalizes LDA, 
saying that the mixture of topics is not dependent 
on the document itself, but rather on the authors 
who wrote it.  According to this model, for each 
word (or phrase) in a document, an author is cho-
sen uniformly from the set of the authors of the 
document.  Then, a topic is chosen from a distribu-
tion of topics associated with that particular author.  
Finally, the word is generated from the distribution 
associated with that chosen topic.  We can express 
the probability of the words in a document (W) 
given its authors (A) as: 
? ??
? ? ?
=
Wm Ax Tzd
xzpzmp
A
AWp )|()|(1)|(  (1) 
where T is the set of latent topics that are induced 
given a large set of training data.   
We use the AT model to estimate our grounded 
language model by making an analogy between 
documents and events in video.  In our framework, 
the words in a document correspond to the words 
in the closed captioning transcript associated with 
an event.  The authors of a document correspond to 
the temporal patterns representing the non- 
 
linguistic context of that event.  We modify the AT 
model slightly, such that, instead of selecting from 
                                                          
3
 In the discussion that follows, we describe a method for es-
timating unigram grounded language models.  Estimating 
bigram and trigram models can be done by processing on 
word pairs or triples, and performing normalization on the 
resulting conditional distributions. 
a uniform distribution (as is done with authors of 
documents), we select patterns from a multinomial 
distribution based upon the duration of the pattern.  
The intuition here is that patterns that occur for a 
longer duration are more salient and thus, should 
be given greater weight in the generative process.  
We can now rewrite (1) to give the probability of 
words during an event (W) given the vector of ob-
served temporal patterns (P) as: 
???
? ? ?
=
Wm Px Tz
xpxzpzmpPWp )()|()|()|(  (2) 
In the experiments described below we follow 
Steyver et al, (2004) and train our AT model using 
Gibbs sampling, a Markov Chain Monte Carlo 
technique for obtaining parameter estimates.  We 
run the sampler on a single chain for 200 iterations.  
We set the number of topics to 15, and normalize 
the pattern durations first by individual pattern 
across all events, and then for all patterns within an 
event.  The resulting parameter estimates are 
smoothed using a simple add N smoothing tech-
nique, where N=1 for the word by topic counts and 
N=.01 for the pattern by topic counts.   
4 Evaluation 
In order to evaluate our grounded language model-
ing approach, a parallel data set of 99 Major 
League Baseball games with corresponding closed 
captioning transcripts was recorded from live tele-
vision.  These games represent data totaling ap-
proximately 275 hours and 20,000 distinct events 
from 25 teams in 23 stadiums, broadcast on five 
different television stations.  From this set, six 
games were held out for testing (15 hours, 1200 
events, nine teams, four stations).  From this test 
set, baseball highlights (i.e., events which termi-
nate with the player either out or safe) were hand 
annotated for use in evaluation, and manually tran-
scribed in order to get clean text transcriptions for 
gold standard comparisons.  Of the 1200 events in 
the test set, 237 were highlights with a total word 
count of 12,626 (vocabulary of 1800 words). 
The remaining 93 unlabeled games are used to 
train unigram, bigram, and trigram grounded lan-
guage models.  Only unigrams, bigrams, and tri-
grams that are not proper names, appear greater 
than three times, and are not composed only of 
stop words were used.  These grounded language 
models are then combined in a backoff strategy 
125
with traditional unigram, bigram, and trigram lan-
guage models generated from a combination of the 
closed captioning transcripts of all training games 
and data from the switchboard corpus (see below).  
This backoff is necessary to account for the words 
not included in the grounded language model itself 
(i.e. stop words, proper names, low frequency 
words).  The traditional text-only language models 
(which are also used below as baseline compari-
sons) are generated with the SRI language model-
ing toolkit (Stolcke, 2002) using Chen and 
Goodman's modified Kneser-Ney discounting and 
interpolation (Chen and Goodman, 1998).  The 
backoff strategy we employ here is very simple: if 
the ngram appears in the GLM then it is used, oth-
erwise the traditional LM is used.  In future work 
we will examine more complex backoff strategies 
(Hsu, in review). 
We evaluate our grounded language modeling 
approach using 3 metrics: perplexity, word error 
rate, and precision on an information retrieval task. 
4.1 Perplexity 
Perplexity is an information theoretic measure of 
how well a model predicts a held out test set.  We 
use perplexity to compare our grounded language 
model to two baseline language models: a lan-
guage model generated from the switchboard cor-
pus, a commonly used corpus of spontaneous 
speech in the telephony domain (3.65M words; 27k 
vocab); and a language model that interpolates 
(with equal weight given to both) between the 
switchboard model and a language model trained 
only on the baseball-domain closed captioning 
(1.65M words; 17k vocab).  The results of calculat-
ing perplexity on the test set highlights for these 
three models is presented in Table 1 (lower is bet-
ter). 
Not surprisingly, the switchboard language 
model performs far worse than both the interpo-
lated text baseline and the grounded language 
model.  This is due to the large discrepancy be-
tween both the style and vocabulary of language 
about sports compared to the domain of telephony 
sampled by the switchboard corpus.  Of more in-
terest is the decrease in perplexity seen when using 
the grounded language model compared to the in-
terpolated model.  Note that these two language 
models are generated using the same speech tran-
scriptions, i.e. the closed captioning from the train-
ing games and the switchboard corpus.  However, 
whereas the baseline model remains the same for 
each of the 237 test highlights, the grounded lan-
guage model generates different word distributions 
for each highlight depending on the event features 
extracted from the highlight video. 
  
 Switchboard Interpolated 
(Switch+CC) 
Grounded 
ppl 1404 145.27 83.88 
 
Table 1.  Perplexity measures for three different lan-
guage models on a held out test set of baseball high-
lights (12,626 words).  We compare the grounded 
language model to two text based language models: one 
trained on the switchboard corpus alone; and interpo-
lated with one trained on closed captioning transcrip-
tions of baseball video.  
4.2 Word Accuracy and Error Rate 
Word error rate (WER) is a normalized measure of 
the number of word insertions, substitutions, and 
deletions required to transform the output tran-
scription of an ASR system to a human generated 
gold standard transcription of the same utterance.  
Word accuracy is simply the number of words in 
the gold standard that they system correctly recog-
nized.  Unlike perplexity which only evaluates the 
performance of language models, examining word 
accuracy and error rate requires running an entire 
ASR system, i.e. both the language and acoustic 
models.   
We use the Sphinx system to train baseball specific 
acoustic models using parallel acoustic/text data 
automatically mined from our training set.  Follow-
ing Jang and Hauptman (1999), we use an off the 
shelf acoustic model (the hub4 model) to generate 
an extremely noisy speech transcript of each game 
in our training set, and use dynamic programming 
to align these noisy outputs to the closed caption-
ing stream for those same games.  Given these two 
transcriptions, we then generate a paired acous-
tic/text corpus by sampling the audio at the time 
codes where the ASR transcription matches the 
closed captioning transcription.   
For example, if the ASR output contains the 
term sequence ?? and farther home run for David 
forty says?? and the closed captioning contains 
the sequence ??another home run for David 
Ortiz?,? the matched phrase ?home run for 
David? is assumed a correct transcription for the 
audio at the time codes given by the ASR system.  
Only looking at sequences of three words or more,  
126
76.6
80.3
89.6
70
75
80
85
90
95
switchboard interpolated grounded
W
o
rd
 
Er
ro
r 
R
at
e
 
(W
ER
)
31.3
25.4
15.1
0
5
10
15
20
25
30
35
switchboard interpolated grounded
W
o
rd
 
A
c
cu
ra
c
y 
(%
)
 
Figure 3.  Word accuracy and error rates for ASR sys-
tems using a grounded language model, a text based 
language model trained on the switchboard corpus, and 
the switchboard model interpolated with a text based 
model trained on baseball closed captions. 
 
we extract approximately 18 hours of clean paired 
data from our 275 hour training corpus.  A con-
tinuous acoustic model with 8 gaussians and 6000 
ties states is trained on this data using the Sphinx 
speech recognizer.4 
Figure 3 shows the WERs and accuracy for 
three ASR systems run using the Sphinx decoder 
with the acoustic model described above and either 
the grounded language model or the two baseline 
models described in section 4.1.  Note that per-
formance for all of these systems is very poor due 
to limited acoustic data and the large amount of 
background crowd noise present in sports video 
(and particularly in sports highlights).  Even with 
this noise, however, results indicate that the word 
accuracy and error rates when using the grounded 
language model is significantly better than both the 
switchboard model (absolute WER reduction of 
13%; absolute accuracy increase of 15.2%) and the 
switchboard interpolated with the baseball specific 
text based language model (absolute WER reduc-
tion of 3.7%; absolute accuracy increase of 5.9%).   
                                                          
4
 http://cmusphinx.sourceforge.net/html/cmusphinx.php 
Drawing conclusions about the usefulness of 
grounded language models using word accuracy or 
error rate alone is difficult.  As it is defined, these 
measures penalizes a system that mistakes ?a? for 
?uh? as much as one that mistakes ?run? for ?rum.?  
When using ASR to support multimedia applica-
tions (such as search), though, such substitutions 
are not of equal importance.  Further, while visual 
information may be useful for distinguishing the 
latter error, it is unlikely to assist with the former.  
Thus, in the next section we examine an extrinsic 
evaluation in which grounded language models are 
judged not directly on their effect on word accu-
racy or error rate, but based on their ability to sup-
port video information retrieval.  
4.3 Precision of Information Retrieval  
One of the most commonly used applications of 
ASR for video is to support information retrieval 
(IR).  Such video IR systems often use speech tran-
scriptions to index segments of video in much the 
same way that words are used to index text docu-
ments (Wactlar et al, 1996).  For example, in the 
domain of baseball, if a video IR system were is-
sued the query ?home run,? it would typically re-
turn a set of video clips by searching its database 
for events in which someone uttered the phrase 
?home run.?  Because such systems rely on ASR 
output to search video, the performance of a video 
IR system gives an indirect evaluation of the 
ASR?s quality.  Further, unlike the case with word 
accuracy or error rate, such evaluations highlight a 
systems ability to recognize the more relevant con-
tent words without being distracted by the more 
common stop words. 
Our metric for evaluation is the precision with 
which baseball highlights are returned in a video 
IR system.  We examine three systems: one that 
uses ASR with the grounded language model, a 
baseline system that uses ASR with the text only 
interpolated language model, and finally a system 
that uses human produced closed caption transcrip-
tions to index events. 
For each system, all 1200 events from the test 
set (not just the highlights) are indexed.  Queries 
are generated artificially using a method similar to 
Berger and Lafferty (1999) and used in Fleischman 
and Roy (2007).  First, each highlight is labeled 
with the event?s type (e.g. fly ball), the event?s lo-
cation (e.g. left field) and the event?s result (e.g. 
double play): 13 labels total.  Log likelihood ratios 
127
are then used to find the phrases (unigram, trigram, 
and bigram) most indicative of each label (e.g. ?fly 
ball? for category fly ball).  For each label, the 
three most indicative phrases are issued as queries 
to the system, which ranks its results using the lan-
guage modeling approach of Ponte and Croft 
(1998).  Precision is measured on how many of the 
top five returned events are of the correct category.   
Figure 4 shows the precision of the video IR 
systems based on ASR with the grounded language 
model, ASR with the text-only interpolated lan-
guage model, and closed captioning transcriptions.  
As with our previous evaluations, the IR results 
show that the system using ASR with the grounded 
language model performed better than the one us-
ing ASR with the text-only language model (5.1% 
absolute improvement).  More notably, though, 
Figure 4 shows that the system using the grounded 
language model performed better than the system 
using the hand generated closed captioning tran-
scriptions (4.6% absolute improvement).  Although 
this is somewhat counterintuitive given that hand 
transcriptions are typically considered gold stan-
dards, these results follow from a limitation of us-
ing text-based methods to index video.  
Unlike the case with text documents, the occur-
rence of a query term in a video is often not 
enough to assume the video?s relevance to that 
query.  For example, when searching through 
video of baseball games, returning all clips in 
which the phrase ?home run? occurs, results pri-
marily in video of events where a home run does 
not actually occur.  This follows from the fact that 
in sports, as in life, people often talk not about 
what is currently happening, but rather, they talk 
about what did, might, or will happen in the future.   
By taking into account non-linguistic context 
during speech recognition, the grounded language 
model system indirectly circumvents some of these 
false positive results.  This follows from the fact 
that an effect of using the grounded language 
model is that when an announcer utters a phrase 
(e.g., ?fly ball?), the system is more likely to rec-
ognize that phrase correctly if the event it refers to 
is actually occurring (e.g. if someone actually hit a 
fly ball).  Because the grounded language model 
system is biased to recognize phrases that describe 
what is currently happening, it returns fewer false 
positives and gets higher precision.  
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
0.35
ASR-LM CC ASR-GLM
Pr
ec
is
io
n
 
o
f T
o
p 
5
 
Figure 4. Precision of top five results of a video IR sys-
tem based on speech transcriptions.  Three different 
transcriptions are compared: ASR-LM uses ASR with a 
text-only interpolated language model (trained on base-
ball closed captioning and the switchboard corpus); 
ASR-GLM uses ASR with a grounded language model; 
CC uses human generated closed captioning transcrip-
tions (i.e., no ASR). 
5 Conclusions 
We have described a method for improving speech 
recognition in video.  The method uses grounded 
language modeling, an extension of tradition lan-
guage modeling in which the probability of a word 
is conditioned not only on the previous word(s) but 
also on the non-linguistic context in which the 
word is uttered.  Context is represented using hier-
archical temporal patterns of low level features 
which are mined automatically from a large unla-
beled video corpus.  Hierarchical Bayesian models 
are then used to map these representations to 
words.  Initial results show grounded language 
models improve performance on measures of per-
plexity, word accuracy and error rate, and preci-
sion on an information retrieval task. 
In future work, we will examine the ability of 
grounded language models to improve perform-
ance for other natural language tasks that exploit 
text based language models, such as Machine 
Translation.  Also, we are examining extending 
this approach to other sports domains such as 
American football.  In theory, however, our ap-
proach is applicable to any domain in which there 
is discussion of the here-and-now (e.g., cooking 
shows, etc.).  In future work, we will examine the 
strengths and limitations of grounded language 
modeling in these domains. 
128
References 
Allen, J.F. (1984). A General Model of Action and 
Time. Artificial Intelligence. 23(2). 
Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, 
D, and Jordan, M. (2003), Matching Words and 
Pictures, Journal of Machine Learning Research, 
Vol 3. 
Berger, A. and Lafferty, J. (1999). Information 
Retrieval as Statistical Translation. In Proceed-
ings of SIGIR-99. 
Blei, D. and Jordan, M. (2003). Modeling annotated 
data. Proceedings of the 26th International Confer-
ence on Research and Development in Information 
Retrieval, ACM Press, 127?134.  
Blei, D. Ng, A., and Jordan, M (2003). ?Latent Dirichlet 
allocation.? Journal of Machine Learning Research 
3:993?1022.  
Bouthemy, P., Gelgon, M., Ganansia, F. (1999). A uni-
fied approach to shot change detection and cam-
era motion characterization. IEEE Trans. on 
Circuits and Systems for Video Technology, 
9(7). 
Chen, S. F. and Goodman, J., (1998). An Empirical 
Study of Smoothing Techniques for Language Mod-
eling, Tech. Report TR-10-98, Computer Science 
Group, Harvard U., Cambridge, MA. 
Fleischman M, Roy, D. (2007).  Situated Models of 
Meaning for Sports Video Retrieval.  HLT/NAACL. 
Rochester, NY. 
Fleischman, M. and Roy, D. (2007). Unsupervised Con-
tent-Based Indexing of Sports Video Retrieval.  9th 
ACM Workshop on Multimedia Information Retrieval 
(MIR). Augsburg, Germany. 
Fleischman, M. B. and Roy, D.  (2005)  Why Verbs are 
Harder to Learn than Nouns: Initial Insights from a 
Computational Model of Intention Recognition in 
Situated Word Learning.  27th Annual Meeting of the 
Cognitive Science Society, Stresa, Italy. 
Fleischman, M., DeCamp, P. Roy, D.  (2006). Mining 
Temporal Patterns of Movement for Video Content 
Classification.  ACM Workshop on Multimedia In-
formation Retrieval. 
Fleischman, M., Roy, B., and Roy, D. (2007).  Tempo-
ral Feature Induction for Sports Highlight Classifica-
tion.  In Proceedings of ACM Multimedia.  
Augsburg, Germany. 
Gong, Y., Han, M., Hua, W., Xu, W.  (2004). Maximum 
entropy model-based baseball highlight detection and 
classification.  Computer Vision and Image Un-
derstanding. 96(2). 
Hauptmann, A. , Witbrock, M., (1998) Story Segmenta-
tion and Detection of Commercials in Broadcast 
News Video, Advances in Digital Libraries. 
Hongen, S., Nevatia, R. Bremond, F. (2004). 
Video-based event recognition: activity repre-
sentation and probabilistic recognition methods.  
Computer Vision and Image Understanding. 
96(2). 
Hsu , Bo-June (Paul). (in review). Generalized Linear 
Interpolation of Language Models. 
Jang, P., Hauptmann, A. (1999).  Learning to Recognize 
Speech by Watching Television.  IEEE Intelligent 
Systems Magazine, 14(5), pp. 51-58.  
Mukherjee, N. and Roy, D.. (2003). A Visual Context-
Aware Multimodal System for Spoken Language 
Processing. Proc. Eurospeech, 4 pages.  
Ponte, J.M., and Croft, W.B. (1998). A Language Mod-
eling Approach to Information Retrieval. In Proc. of 
SIGIR?98.  
Roy, D. (2005). . Grounding Words in Perception and 
Action: Insights from Computational Models.  TICS. 
Roy, D. and Pentland, A. (2002).  Learning Words from 
Sights and Sounds: A Computational Model. Cogni-
tive Science, 26(1). 
Roy. D. and Reiter, E. (2005). . Connecting Language to 
the World. Artificial Intelligence, 167(1-2), 1-12. 
Snoek, C.G.M. and Worring, M.. (2005).  Multimodal 
video indexing: A review of the state-of-the-art. 
Multimedia Tools and Applications, 25(1):5-35. 
Steyvers, M., Smyth, P., Rosen-Zvi, M., & Griffiths, T. 
(2004). Probabilistic Author-Topic Models for In-
formation Discovery. The Tenth ACM SIGKDD In-
ternational Conference on Knowledge Discovery and 
Data Mining. Seattle, Washington. 
Stolcke, A., (2002). SRILM - An Extensible Language 
Modeling Toolkit, in Proc. Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.  
Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005). 
Shot Detection and Motion Analysis for Automatic 
MPEG-7 Annotation of Sports Videos.  In 13th In-
ternational Conference on Image Analysis and Proc-
essing. 
Wactlar, H., Witbrock, M., Hauptmann, A., (1996 ). 
Informedia: News-on-Demand Experiments in 
Speech Recognition.  ARPA Speech Recognition 
Workshop, Arden House, Harriman, NY.  
Witten, I. and Frank, E. (2005).  Data Mining: Practical 
machine learning tools and techniques.  2nd Edition, 
Morgan Kaufmann. San Francisco, CA. 
 
129
Understanding Complex Visually Referring Utterances
Peter Gorniak
Cognitive Machines Group
MIT Media Laboratory
pgorniak@media.mit.edu
Deb Roy
Cognitive Machines Group
MIT Media Laboratory
dkroy@media.mit.edu
Abstract
We propose a computational model of
visually-grounded spatial language under-
standing, based on a study of how people
verbally describe objects in visual scenes.
We describe our implementation of word
level visually-grounded semantics and their
embedding in a compositional parsing frame-
work. The implemented system selects the
correct referents in response to a broad range
of referring expressions for a large percentage
of test cases. In an analysis of the system?s
successes and failures we reveal how visual
context influences the semantics of utterances
and propose future extensions to the model
that take such context into account.
1 Introduction
We present a study of how people describe objects in vi-
sual scenes of the kind shown in Figure 1. Based on
this study, we propose a computational model of visually-
grounded language understanding. A typical referring ex-
pression for Figure 1 might be, ?the far back purple cone
that?s behind a row of green ones?. In such tasks, speak-
ers construct expressions to guide listeners? attention to
intended objects. Such referring expressions succeed in
communication because speakers and listeners find sim-
ilar features of the visual scene to be salient, and share
an understanding of how language is grounded in terms
of these features. This work is a step towards our longer
term goals to develop a conversational robot (Roy et al,
forthcoming 2003) that can fluidly connect language to
perception and action.
To study the characteristics of descriptive spatial lan-
guage, we collected several hundred referring expres-
sions based on scenes similar to Figure 1. We analysed
the descriptions by cataloguing the visual features that
they referred to within a scene, and the range of linguistic
devices (words or grammatical patterns) that they used to
refer to those features. The combination of a visual fea-
ture and corresponding linguistic device is referred to as
a descriptive strategy.
Figure 1: A sample scene used to elicit visually-grounded
referring expressions (if this figure has been reproduced
in black and white, the light cones are green in colour, the
dark cones are purple)
We propose a set of computational mechanisms that
correspond to the most commonly used descriptive strate-
gies from our study. The resulting model has been imple-
mented as a set of visual feature extraction algorithms,
a lexicon that is grounded in terms of these visual fea-
tures, a robust parser to capture the syntax of spoken ut-
terances, and a compositional engine driven by the parser
that combines visual groundings of lexical units. We use
the term grounded semantic composition to highlight that
both the semantics of individual words and the word com-
position process itself are visually-grounded. We propose
processes that combine the visual models of words, gov-
erned by rules of syntax. In designing our system, we
made several simplifying assumptions. We assumed that
word meanings are independent of the visual scene, and
that semantic composition is a purely incremental pro-
cess. As we will show, neither of these assumptions holds
in all of our data, but our system still understands most
utterances correctly.
To evaluate the system, we collected a set of spoken
utterances from three speakers. The model was able to
correctly understand the visual referents of 59% of the
expressions (chance performance was 1/30?30i=1 1/i =
13%). The system was able to resolve a range of linguis-
tic phenomena that made use of relatively complex com-
positions of spatial semantics. We provide an analysis of
the sources of failure in this evaluation, based on which
we propose a number of improvements that are required
to achieve human level performance.
An extended report on this work can be found in (Gor-
niak and Roy, 2003).
1.1 Related Work
Winograd?s SHRDLU is a well known system that could
understand and generate natural language referring to ob-
jects and actions in a simple blocks world (Winograd,
1970). Like our system it performs semantic interpreta-
tion during parsing by attaching short procedures to lexi-
cal units. However, SHRDLU had access to a clean sym-
bolic representation of the scene and only handles sen-
tences it could parse complete. The system discussed
here works with a synthetic vision system, reasons over
geometric and other visual measures, and works from ac-
curate transcripts of noisy human speech.
Partee provides an overview of the general formal se-
mantics approach and the problems of context based
meanings and meaning compositionality from this per-
spective (Partee, 1995). Our work reflects many of the
ideas from this work, such as viewing adjectives as func-
tions, as well as idea?s from Pustejovsky?s theory of the
Generative Lexicon (GL) (Pustejovsky, 1995). How-
ever, these formal approaches operate in a symbolic do-
main and leave the details of non-linguistic influences on
meaning unspecified, whereas we take the computational
modelling of these influences as our primary concern.
Word meanings have been approached by several re-
searchers as a problem of associating visual represen-
tations, often with complex internal structure, to word
forms. Models have been suggested for visual represen-
tations underlying spatial relations (Regier and Carlson,
2001). Models for verbs include grounding their seman-
tics in the perception of actions (Siskind, 2001). Landau
and Jackendoff provide a detailed analysis of additional
visual shape features that play a role in language (Landau
and Jackendoff, 1993).
We have previously proposed methods for visually-
grounded language learning (Roy and Pentland, 2002),
understanding (Roy et al, 2002), and generation (Roy,
2002). However, the treatment of semantic composition
in these efforts was relatively primitive. While this sim-
ple approach worked in the constrained domains that we
have addressed in the past, it does not scale to the present
task.
2 A Spatial Description Task
We designed a task that requires people to describe ob-
jects in computer generated scenes containing up to 30
objects with random positions on a virtual surface. The
objects all had identical shapes and sizes, and were ei-
ther green or purple in colour. Each of the objects had a
50% chance of being green, otherwise it was purple. This
design naturally led speakers to make reference to spa-
tial aspects of the scene, rather than the individual object
properties which subjects tended to use in our previous
work (Roy, 2002). We refer to this task as the Bishop
task, and to the resulting language understanding model
and implemented system simply as Bishop.
2.1 Data Collection
Participants in the study ranged in age from 22 to 30
years, and included both native and non-native English
speakers. Pairs of participants were seated with their
backs to each other, each person looking at a computer
screen which displayed a scene such as that in Figure 1.
Each screen showed the same scene. In each pair, one
participant served as describer, and the other as listener.
The describer wore a microphone that was used to record
his or her speech. The describer used a mouse to select an
object from the scene, and then verbally described the se-
lected object to the listener. The listener?s task was to se-
lect the same object on their own computer display based
on the verbal description. If the selected objects matched,
they disappeared from the scene and the describer would
select and describe another object. If they did not match,
the describer would re-attempt the description until un-
derstood by the listener. The scene contained 30 objects
at the beginning of each session, and a session ended
when no objects remained, at which point the describer
and listener switched roles and completed a second ses-
sion (some participants fulfilled a role multiple times).
Initially, we collected 268 spoken object descriptions
from 6 participants. The raw audio was segmented using
our speech segmentation algorithm based on pause struc-
ture (Yoshida, 2002). Along with the utterances, the cor-
responding scene layout and target object identity were
recorded together with the times at which objects were
selected. This 268 utterance corpus is referred to as the
development data set. We manually transcribed each spo-
ken utterance verbatim, retaining all speech errors (false
starts and various other ungrammaticalities). Off-topic
speech events (laughter, questions about the task, other
remarks, and filled pauses) were marked as such (they do
not appear in any results we report). We wrote a simple
heuristic algorithm based on time stamps to pair utter-
ances and selections based on their time stamps. When
we report numbers of utterances in data sets in this paper,
they correspond to how many utterance-selection pairs
our pairing algorithm produces.
Once our implementation based on the development
corpus yielded acceptable results, we collected another
179 spoken descriptions from three additional partici-
pants to evaluate generalization and coverage of our ap-
proach. The discussion and analysis in the following sec-
tions focuses on the development set. In Section 6 we
discuss performance on the test set.
2.2 Descriptive Strategies for Achieving Joint
Reference
We distinguish three subsets of our development data, 1)
a set containing those utterance/selection pairs that con-
tain errors, where an error can be due to a repair or mis-
take on the human speaker?s part, a segmentation mis-
take by our speech segmenter, or an error by our utter-
ance/selection pairing algorithm, 2) a set that contains
those utterance/selection pairs that employ descriptive
strategies other than those we cover in our computational
understanding system (we cover those in Sections 2.2.1
to 2.2.5), and 3) the set of utterance/selection pairs in the
development data that are not a member of either sub-
set described above. We refer to this last subset as the
?clean? set. Note that the first two subsets are not mu-
tually exclusive. As we catalogue descriptive strategies
from the development data in the following sections, we
report two percentages for each descriptive strategy. The
first is the percentage of utterance/selection pairs that em-
ploy a specific descriptive strategy relative to all the utter-
ance/selection pairs in the development data set. The sec-
ond is the percentage of utterance/selection pairs relative
to the clean set of utterance/selection pairs, as described
above.
2.2.1 Colour
Almost every utterance employs colour to pick out ob-
jects. While designing the task, we intentionally trivial-
ized the problem of colour reference. Objects come in
only two distinct colours, green and purple. Unsurpris-
ingly, all participants used the terms ?green? and ?pur-
ple? to refer to these colours. Participants used colour to
identify one or more objects in 96% of the data, and 95%
of the clean data.
2.2.2 Spatial Regions and Extrema
The second most common descriptive strategy is to re-
fer to spatial extremes within groups of objects and to
spatial regions in the scene. The example in Figure 2
uses two spatial terms to pick out its referent: ?front? and
?left?, both of which leverage spatial extrema to direct the
listener?s attention. Multiple spatial specifications tend to
be interpreted in left to right order, that is, selecting a
group of objects matching the first term, then amongst
those choosing objects that match the second term.
?the purple one in the front left corner?
Figure 2: Example utterance specifying objects by refer-
ring to spatial extrema
Being rather ubiquitous in the data, spatial extrema
and spatial regions are often used in combination with
other descriptive strategies like grouping, but are most
frequently combined with other extrema and region spec-
ifications. Participants used single spatial extrema to
identify one or more objects in 72% of the data, and in
78% of the clean data. They used spatial region specifi-
cations in 20% of the data (also 20% of the clean data),
and combined multiple extrema or regions in 28% (30%
of the clean data).
2.2.3 Grouping
To provide landmarks for spatial relations and to spec-
ify sets of objects to select from, participants used lan-
guage to describe groups of objects. Figure 3 shows an
example of such grouping constructs, which uses a count
to specify the group (?three?). In this example, the par-
ticipant first specifies a group containing the target ob-
ject, then utters another description to select within that
group. Note that grouping alone never yields an indi-
vidual reference, so participants compose grouping con-
structs with further referential tactics (predominantly ex-
trema and spatial relations) in all cases. Participants used
grouping to identify objects in 12% of the data and 10%
of the clean data.
?there?s three on the left side; the one in the
furthest back?
Figure 3: Example utterance using grouping
2.2.4 Spatial Relations
As already mentioned in Section 2.2.3, participants
sometimes used spatial relations between objects or
groups of objects. Examples of such relations are ex-
pressed through prepositions like ?below? or ?behind? as
well as phrases like ?to the left of? or ?in front of?. Fig-
ure 4 shows an example that involves a spatial relation
between individual objects. The spatial relation is com-
bined with another strategy, here an extremum (as well
as two speech errors by the describer). Participants used
spatial relations in 6% of the data (7% of the clean data).
?there?s a purple cone that?s it?s all the way on the
left hand side but it?s it?s below another purple?
Figure 4: Example utterance specifying a spatial relation
2.2.5 Anaphora
In a number of cases participants used anaphoric refer-
ences to the previous object removed during the descrip-
tion task. Figure 5 shows a sequence of two scenes and
corresponding utterances in which the second utterance
refers back to the object selected in the first. Participants
employed spatial relations in 4% of the data (3% of the
clean data).
?the closest purple one on the far left side?
?the green one right behind that one?
Figure 5: Example sequence of an anaphoric utterance
2.2.6 Other
In addition to the phenomena listed in the preceding
sections, participants used a small number of other de-
scription strategies. Some that occurred more than once
but that we have not yet addressed in our computational
model are selection by distance (lexicalised as ?close to?
or ?next to?), selection by neighbourhood (?the green
one surrounded by purple ones?), selection by symmetry
(?the one opposite that one?), and selection by something
akin to local connectivity (?the lone one?). We anno-
tated 13% of our data as containing descriptive strategies
other than the ones covered in the preceding sections. We
marked 15% of our data as containing errors.
3 The Understanding Framework
3.1 Synthetic Vision
Instead of relying on the information we use to render the
scenes in Bishop, which includes 3D object locations and
the viewing angle, we implemented a simple synthetic vi-
sion algorithm to ease a future transfer back to a robot?s
vision system. This algorithm produces a map attribut-
ing each pixel of the rendered image to one of the objects
or the background. In addition, we use the full colour
information for each pixel drawn in the rendered scene.
We chose to work in a virtual world for this project so
that we could freely change colour, number, size, shape
and arrangement of objects to elicit interesting verbal be-
haviours in our participants.
3.2 Lexical Entries and Concepts
Conceptually, we treat lexical entries like classes in an
object oriented programming language. When instanti-
ated, they maintain an internal state that can be as simple
as a tag identifying the dimension along which to perform
an ordering, or as complex as multidimensional probabil-
ity distributions. Each entry can contain a semantic com-
poser that encapsulates the function to combine this entry
with other constituents during a parse. These composers
are described in-depth in Section 4. The lexicon used
for Bishop contains many lexical entries attaching differ-
ent semantic composers to the same word. For exam-
ple, ?left? can be either a spatial relation or an extremum,
which may be disambiguated by grammatical structure
during parsing.
During composition, structures representing the ob-
jects a constituent references are passed between lexical
entries. We refer to these structures as concepts. Each en-
try accepts zero or more concepts, and produces zero or
more concepts as the result of the composition operation.
A concept lists the entities in the world that are possible
referents of the constituent it is associated with, together
with real numbers representing their ranking due to the
last composition operation.
3.3 Parsing
We use a bottom-up chart parser to guide the interpre-
tation of phrases (Allen, 1995). Such a parser has the
advantage that it employs a dynamic programming strat-
egy to efficiently reuse already computed subtrees of the
parse. Furthermore, it produces all sub components of a
parse and thus produces a useable result without the need
to parse to a specific symbol.
Bishop performs only a partial parse, a parse that is not
required to cover a whole utterance, but simply takes the
longest referring parsed segments to be the best guess.
Unknown words do not stop the parse process. Rather,
all constituents that would otherwise end before the un-
known word are taken to include the unknown word, in
essence making unknown words invisible to the parser
and the understanding process. In this way we recover
essentially all grammatical chunks and relations that are
important to understanding in our restricted task.
We use a simple grammar containing 19 rules. With
each rule, we associate an argument structure for seman-
tic composition. When a rule is syntactically complete
during a parse, the parser checks whether the composers
of the constituents in the tail of the rule can accept the
number of arguments specified in the rule. If so, it calls
the semantic composer associated with the constituent
with the concepts yielded by its arguments to produce a
concept for the head of the rule.
4 Semantic Composition
Most of the composers presented follow the same com-
position schema: they take one or more concepts as ar-
guments and yield another concept that references a pos-
sibly different set of objects. Composers may introduce
new objects, even ones that do not exist in the scene as
such, and they may introduce new types of objects (e.g.
groups of objects referenced as if they were one object).
Most composers first convert an incoming concept to the
objects it references, and subsequently perform compu-
tations on these objects. If ambiguities persist at the end
of understanding an utterance (multiple possible referents
exist), we let Bishop choose the one with maximum ref-
erence strength.
4.1 Colour - Probabilistic Attribute Composers
As mentioned in Section 3.1, we chose not to exploit the
information used to render the scene, and therefore must
recover colour information from the final rendered im-
age. The colour average for the 2D projection of each
object varies due to occlusion by other objects, as well
as distance from and angle with the virtual camera. We
separately collected a set of labelled instances of ?green?
and ?purple? cones, and estimated a three dimensional
Gaussian distribution from the average red, green and
blue values of each pixel belonging to the example cones.
When asked to compose with a given concept, this type of
probabilistic attribute composer assigns each object refer-
enced by the source concept the probability density func-
tion evaluated at the average colour of the object.
4.2 Spatial Extrema and Spatial Regions - Ordering
Composers
To determine spatial regions and extrema, an ordering
composer orders objects along a specified feature dimen-
sion (e.g. x coordinate relative to a group) and picks ref-
erents at an extreme end of the ordering. To do so, it
assigns an exponential weight function to objects accord-
ing to ?i(1+v) for picking minimal objects, where i is the
object?s position in the sequence, v is its value along the
feature dimension specified, normalized to range between
0 and 1 for the objects under consideration. The maximal
case is weighted similarly, but using the reverse order-
ing subtracting the fraction in the exponent from 2. For
our reported results ? = 0.38. This formula lets refer-
ent weights fall off exponentially both with their posi-
tion in the ordering and their distance from the extreme
object. In that way extreme objects are isolated except
for cases in which many referents cluster around an ex-
tremum, making picking out a single referent difficult.
We attach this type of composer to words like ?leftmost?
and ?top?.
The ordering composer can also order objects accord-
ing to their absolute position, corresponding more closely
to spatial regions rather than spatial extrema relative to a
group. The reference strength formula for this version
is ?(1+
d
dmax
) where d is the euclidean distance from a
reference point, and dmax the maximum such distance
amongst the objects under consideration. This version of
the composer is attached to words like ?middle?. It has
the effect that reference weights are relative to absolute
position on the screen. An object close to the centre of
the board achieves a greater reference weight for the word
?middle?, independently of the position of other objects
of its kind. Ordering composers work across any number
of dimensions by simply ordering objects by their Eu-
clidean distance, using the same exponential falloff func-
tion as in the other cases.
4.3 Grouping Composers
For non-numbered grouping (e.g., when the describer
says ?group? or ?cones?), the grouping composer
searches the scene for groups of objects that are all within
a maximum distance threshold from another group mem-
ber. It only considers objects that are referenced by the
concept it is passed as an argument. For numbered groups
(?two?, ?three?), the composer applies the additional con-
straint that the groups have to contain the correct number
of objects. Reference strengths for the concept are de-
termined by the average distance of objects within the
group.
The output of a grouping composer may be thought
of as a group of groups. To understand the motivation
for this, consider the utterance, ?the one to the left of
the group of purple ones?. In this expression, the phrase
?group of purple ones? will activate a grouping composer
that will find clusters of purple cones. For each clus-
ter, the composer computes the convex hull (the minimal
?elastic band? that encompasses all the objects) and cre-
ates a new composite object that has the convex hull as its
shape. When further composition takes place to under-
stand the entire utterance, each composite group serves
as a potential landmark relative to ?left?.
However, concepts can be marked so that their be-
haviour changes to split apart concepts refering to groups.
For example, the composer attached to ?of? sets this flag
on concepts passing through it. Note that ?of? is only in-
volved in composition for grammar rules of the type NP
? NP P NP, but not for those performing spatial com-
positions for phrases like ?to the left of?. Therefore, the
phrase ?the frontmost one of the three green ones? will
pick the front object within the best group of three green
objects.
4.4 Spatial Relations - Spatial Composers
The spatial semantic composer employs a version of the
Attentional Vector Sum (AVS) suggested in (Regier and
Carlson, 2001). The AVS is a measure of spatial relation
meant to approximate human judgements corresponding
to words like ?above? and ?to the left of? in 2D scenes of
objects. Given two concepts as arguments, the spatial se-
mantic composer converts both into sets of objects, treat-
ing one set as providing possible landmarks, the other as
providing possible targets. The composer then calculates
the AVS for each possible combination of landmarks and
targets. Finally, the spatial composer divides the result
by the Euclidean distance between the objects? centres of
mass, to account for the fact that participants exclusively
used nearby objects to select through spatial relations.
4.5 Anaphoric Composers
Triggered by words like ?that? (as in ?to the left of that
one?) or ?previous?, an anaphoric composer produces a
concept that refers to a single object, namely the last ob-
ject removed from the scene during the session. This ob-
ject specially marks the concept as referring not to the
current, but the previous visual scene, and any further
calculations with this concept are performed in that vi-
sual context.
5 Example: Understanding a Description
?the purple one?
?one on the left?
?the purple one on the left?
Figure 6: Example: ?the purple one on the left?
Consider the scene in Figure 6, and the output of the
chart parser for the utterance, ?the purple one on the left?
in Figure 7. Starting at the top left of the parse output,
the parser finds ?the? in the lexicon as an ART (article)
with a selecting composer that takes one argument. It
finds two lexical entries for ?purple?, one marked as a
CADJ (colour adjective), and one as an N (noun). Each
of them have the same composer, a probabilistic attribute
composer marked as P(), but the adjective expects one ar-
gument whereas the noun expects none. Given that the
noun expects no arguments and that the grammar con-
tains a rule of the form NP? N, an NP (noun phrase) is
instantiated and the probabilistic composer is applied to
the default set of objects yielded by N, which consists of
all objects visible. This composer call is marked P(N) in
the chart. After composition, the NP contains a subset of
only the purple objects (Figure 6, top right). At this point
the parser applies NP?ART NP, which produces the NP
spanning the first two words and again contains only the
purple objects, but is marked as unambiguously referring
to an object. S(NP) marks the application of this selecting
composer called S.
The parser goes on to produce a similar NP covering
the first three words by combining the ?purple? CADJ
with ?one? and the result with ?the?. The ?on? P (prepo-
ART:the
CADJ:purple
N:purple
NP:P(N)
NP:S(NP)
N:one
NP:one
NP:P(N)
NP:S(NP)
P:on
ART:the
N:left
ADJ:left
N:left
NP:left
NP:left
NP:S(NP)
NP:S(NP)
NP:O.x.min(NP)
NP:O.x.min(NP)
NP:O.x.min(NP)
the purple one on the left
Figure 7: Sample parse of a referring noun phrase
sition) is left dangling for the moment as it needs a con-
stituent that follows it. It contains a modifying seman-
tic composer that simply bridges the P, applying the first
argument to the second. After another ?the?, ?left? has
several lexical entries: in its ADJ and one of its N forms
it contains an ordering semantic composer that takes a
single argument, whereas its second N form contains a
spatial semantic composer that takes two arguments to
determine a target and a landmark object. At this point
the parser can combine ?the? and ?left? into two possible
NPs, one containing the ordering and the other the spatial
composer. The first of these NPs in turn fulfills the need
of the ?on? P for a second argument according to NP?
NP P NP, performing its ordering compose first on ?one?
(for ?one on the left?), selecting all the objects on the left
(Figure 6, bottom left). The application of the ordering
composer is denoted as O.x.min(NP) in the chart, indi-
cating that this is an ordering composer ordering along
the x axis and selecting the minimum along this axis. On
combining with ?purple one?, the same composer selects
all the purple objects on the left (Figure 6, bottom right).
Finally on ?the purple one?, it produces the same set of
objects as ?purple one?, but marks the concept as unam-
biguously picking out a single object. Note that the parser
attempts to use the second interpretation of ?left? (the one
containing a spatial composer) but fails because this com-
poser expects two arguments that are not provided by the
grammatical structure of the sentence.
6 Results and Discussion
6.1 Overall Performance
In Table 1 we present overall accuracy results, indicating
for which percentage of different groups of examples our
system picked the same referent as the person describing
the object. The first line in the table shows performance
relative to the total set of utterances collected. The second
one shows the percentage of utterances our system un-
derstood correctly excluding those marked as using a de-
scriptive strategy that was not listed in Section 4, and thus
not expected to be understood by Bishop. The final line in
Table 1 shows the percentage of utterances for which our
system picked the correct referent relative to the clean de-
velopment and testing sets. Although there is obviously
room for improvement, these results are significant given
that chance performance on this task is only 13.3% and
linguistic input was transcripts of unconstrained speech.
Utterance Set Accuracy -
Development
Accuracy -
Testing
All 76.5% 58.7%
All except ?Other? 83.2% 68.8%
Clean 86.7% 72.5%
Table 1: Overall Results
Colour Due to the simple nature of colour naming in the
Bishop task, the probabilistic composers responsible
for selecting objects based on colour made no errors.
Spatial Extrema Our ordering composers correctly
identify 100% of the cases in which a participant
uses only colour and a single spatial extremum in his
or her description. Participants also favour this de-
scriptive strategy, using it with colour alone in 38%
of the clean data. In the clean training data, Bishop
understands 86.8% of all utterances employing spa-
tial extrema. Participants composed one or more
spatial region or extrema references in 30% of the
clean data. Our ordering composers correctly inter-
pret 85% of these cases, for example that in Figure 2
in Section 2.2.2. The mistakes our composers make
are usually due to overcommitment and faulty order-
ing.
Spatial Regions Description by spatial region occurs
alone in only 5% of the clean data, and together with
other strategies in 15% of the clean data. Almost
all the examples of this strategy occurring alone use
words like ?middle? or ?centre?. The top image in
Figure 8 exemplifies the use of ?middle? that our
ordering semantic composer models. The object re-
ferred to is the one closest to the centre of the board.
The bottom image in Figure 8 shows a different in-
terpretation of middle: the object in the middle of
a (linguistically not mentioned) group of objects.
Note that within the group there are two candidate
centre objects, and that the one in the front is pre-
ferred. There are also further meanings of middle
that we expand on in (Gorniak and Roy, 2003). In
summary, we can catalogue a number of different
meanings for the word ?middle? in our data that
are linguistically indistinguishable, but depend on
visual and historical context to be correctly under-
stood.
?the green one in the middle?
?the purple cone in the middle?
Figure 8: Types of ?middles?
Grouping Our composers implementing the grouping
strategies used by participants are the most simplis-
tic of all composers we implemented, compared to
the depth of the actual phenomenon of visual group-
ing. As a result, Bishop only understands 29% of ut-
terances that employ grouping in the clean training
data. More sophisticated grouping algorithms have
been proposed, such as Shi and Malik?s (2000).
Spatial Relations The AVS measure divided by distance
between objects corresponds very well to human
spatial relation judgements in this task. All the er-
rors that occur in utterances that contain spatial rela-
tions are due to the possible landmarks or targets not
being correctly identified (grouping or region com-
posers might fail to provide the correct referents).
Our spatial relation composer picks the correct ref-
erent in all those cases where landmarks and tar-
gets are the correct ones. Bishop understands 64.3%
of all utterances that employ spatial relations in the
clean training data. There are types of spatial rela-
tions such as relations based purely on distance and
combined relations (?to the left and behind?) that we
decided not to cover in this implementation, but that
occur in the data and should be covered in future ef-
forts.
Anaphora Our solution to the use of anaphora in the
Bishop task performs perfectly (100% of utter-
ances employing anaphora) in understanding refer-
ence back to a single object in the clean development
data. However, there are more complex variants of
anaphora that we do not currently cover, for example
reference back to groups of objects.
7 Future Directions
Every one of our semantic composers attempts to solve a
separate hard problem, some of which (e.g. grouping and
spatial relations) have seen long lines of work dedicated
to more sophisticated solutions than ours. The individual
problems were not the emphasis of this paper, and the
solutions presented here can be improved.
If a parse does not produce a single referent, backtrack-
ing would provide an opportunity to revise the decisions
made at various stages of interpretation until a referent
is produced. Yet backtracking only solves problems in
which the system knows that it has either failed to ob-
tain a good answer. We demonstrated cases of selection
of word meanings by visual context in our data. Here, a
good candidate solution according to one word meaning
may still produce the wrong referent due to a specific vi-
sual context. A future system should take into account
local and global visual context during composition to ac-
count for these human selection strategies.
By constructing the parse charts we obtain a rich set
of partial and full syntactic and semantic fragments offer-
ing explanations for parts of the utterance. In the future,
we plan to use this information to engage in clarification
dialogue with the human speaker.
Machine learning algorithms may be used to learn
many of the parameter settings that were set by hand in
this work, including on-line learning to adapt parame-
ters during verbal interaction. Furthermore, learning new
types of composers and appropriate corresponding gram-
matical constructs poses a difficult challenge for the fu-
ture.
8 Summary
We have presented a model of visually-grounded lan-
guage understanding. At the heart of the model is a set
of lexical items, each grounded in terms of visual fea-
tures and grouping properties when applied to objects in
a scene. A robust parsing algorithm finds chunks of syn-
tactically coherent words from an input utterance. To de-
termine the semantics of phrases, the parser activates se-
mantic composers that combine words to determine their
joint reference. The robust parser is able to process gram-
matically ill-formed transcripts of natural spoken utter-
ances. In evaluations, the system selected correct objects
in response to utterances for 76.5% of the development
set data, and for 58.7% of the test set data. On clean data
sets with various speech and processing errors held out,
performance was higher yet. We suggested several av-
enues for improving performance of the system including
better methods for spatial grouping, semantically guided
backtracking during sentence processing, the use of ma-
chine learning to replace hand construction of models,
and the use of interactive dialogue to resolve ambiguities.
In the near future, we plan to transplant Bishop into an
interactive conversational robot (Roy et al, forthcoming
2003), vastly improving the robot?s ability to comprehend
spatial language in situated spoken dialogue.
References
James Allen, 1995. Natural Language Understanding,
chapter 3. The Benjamin/Cummings Publishing Com-
pany, Inc, Redwood City, CA, USA.
Peter Gorniak and Deb Roy. 2003. Grounded composi-
tional semantics for referring noun phrases. forthcom-
ing.
B. Landau and R. Jackendoff. 1993. ?what? and ?where?
in spatial language and spatial cognition. Behavioural
and Brain Sciences, 2(16):217?238.
Barbara H. Partee. 1995. Lexical semantics and com-
positionality. In Lila R. Gleitman and Mark Liber-
man, editors, An Invitation to Cognitive Science: Lan-
guage, volume 1, chapter 11, pages 311?360. MIT
Press, Cambridge, MA.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, MA, USA.
Terry Regier and L. Carlson. 2001. Grounding spatial
language in perception: An empirical and computa-
tional investigation. Journal of Experimental Psychol-
ogy: General, 130(2):273?298.
Deb Roy and Alex Pentland. 2002. Learning words from
sights and sounds: A computational model. Cognitive
Science, 26(1):113?146.
Deb Roy, Peter J. Gorniak, Niloy Mukherjee, and Josh
Juster. 2002. A trainable spoken language understand-
ing system. In Proceedings of the International Con-
ference of Spoken Language Processing.
D. Roy, K. Hsiao, and N. Mavridis. forthcoming, 2003.
Coupling robot perception and physical simulation:
Towards sensory-motor grounded conversation.
Deb Roy. 2002. Learning visually-grounded words and
syntax for a scene description task. Computer Speech
and Language, 16(3).
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 8(22):888?
905, August.
Jeffrey Mark Siskind. 2001. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. Journal of Artificial Intelli-
gence Research, 15:31?90, August.
Terry Winograd. 1970. Procedures as a representation
for data in a computer program for understanding nat-
ural language. Ph.D. thesis, Massachusetts Institute of
Technology.
Norimasa Yoshida. 2002. Utterance segmenation for
spontaneous speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
Conversational Robots: Building Blocks for Grounding Word Meaning
Deb Roy
MIT Media Lab
dkroy@media.mit.edu
Kai-Yuh Hsiao
MIT Media Lab
eepness@mit.edu
Nikolaos Mavridis
MIT Media Lab
nmav@media.mit.edu
Abstract
How can we build robots that engage in fluid
spoken conversations with people, moving be-
yond canned responses to words and towards
actually understanding? As a step towards ad-
dressing this question, we introduce a robotic
architecture that provides a basis for grounding
word meanings. The architecture provides per-
ceptual, procedural, and affordance represen-
tations for grounding words. A perceptually-
coupled on-line simulator enables sensory-
motor representations that can shift points of
view. Held together, we show that this archi-
tecture provides a rich set of data structures
and procedures that provide the foundations for
grounding the meaning of certain classes of
words.
1 Introduction
Language enables people to talk about the world, past,
present, and future, real and imagined. For a robot to
do the same, it must ground language in its world as
mediated by its perceptual, motor, and cognitive capac-
ities. Many words that refer to entities in the world can
be grounded through sensory-motor associations. For in-
stance, the meaning of ball includes perceptual associ-
ations that encode how balls look and predictive models
of how balls behave. The representation of touch must in-
clude procedural associations that encode how to perform
the action, and perceptual encodings to recognize the ac-
tion in others. In this view, words serve as labels for per-
ceptual or action concepts. When a word is uttered, the
underlying concept is communicated since the speaker
and listener maintain similar associations. This basic
approach underlies most work to date in building ma-
chines that ground language (Bailey, 1997; Narayanan,
1997; Regier and Carlson, 2001; Roy and Pentland, 2002;
Siskind, 2001; Lammens, 1994; Steels, 2001).
Not all words, however, can be grounded in terms
of perceptual and procedural representations, even when
used in concrete situations. In fact, in even the simplest
conversations about everyday objects, events, and rela-
tions, we run into problems. Consider a person and a
robot sitting across a table from each other, engaged in
coordinated activity involving manipulation of objects.
After some interaction, the person says to the robot:
Touch the heavy blue thing that was on my left.
To understand and act on this command in context,
consider the range of knowledge representations that the
robot must bind words of this utterance to. Touch can
be grounded in a visually-guided motor program that en-
ables the robot to move towards and touch objects. This
is an example of a procedural association which also crit-
ically depends on perception to guide the action. Heavy
specifies a property of objects which involves affordances
that intertwine procedural representations with percep-
tual expectations (Gibson, 1979). Blue and left specify
visual properties. Thing must be grounded in terms of
both perception and affordances (one can see an object,
and expect to reach out and touch it). Was triggers a ref-
erence to the past. My triggers a shift of perspective in
space.
We have developed an architecture in which a physical
robot is coupled with a physical simulator to provide the
basis for grounding each of these classes of lexical se-
mantics1. This workshop paper provides an abbreviated
1We acknowledge that the words in this example, like most
words, have numerous additional connotations that are not cap-
tured by the representations that we have suggested. For exam-
ple, words such as touch, heavy and blue can be used metaphor-
ically to refer to emotional actions and states. Things are not al-
ways physical perceivable objects, my usually indicates posses-
sion, and so forth. Barwise and Perry use the phrase ?efficiency
of language? to highlight the situation-dependent reusability of
words and utterances (Barwise and Perry, 1983). However, for
version of a forthcoming paper (Roy et al, forthcoming
2003).
The robot, called Ripley, is driven by compliant actu-
ators and is able to manipulate small objects. Ripley has
cameras, touch, and various other sensors on its ?head?.
Force sensors in each actuated joint combined with po-
sition sensors provide the robot with a sense of proprio-
ception. Ripley?s visual and proprioceptive systems drive
a physical simulator that keeps a constructed version of
the world (that includes Ripley?s own physical body) in
synchronization with Ripley?s noisy perceptual input. An
object permanence module determines when to instanti-
ate and destroy objects in the model based on perceptual
evidence. Once instantiated, perception can continue to
influence the properties of an object in the model, but
knowledge of physical world dynamics is built into the
simulator and counteracts ?unreasonable? percepts.
Language is grounded in terms of associations with el-
ements of this perceptually driven world model, as well
as direct groundings in terms of sensory and motor rep-
resentations. Although the world model directly reflects
reality, the state of the model is the result of an interpre-
tation process that compiles perceptual input into a sta-
ble registration of the environment. As opposed to direct
perception, the world model affords the ability to assume
arbitrary points of view through the use of synthetic vi-
sion which operates within the physical model, enabling
a limited form of ?out of body experience?. This ability
is essential to successfully differentiate the semantics of
my left versus your left. Non-linguistic cues such as the
visual location of the communication partners can be in-
tegrated with linguistic input to context-appropriate per-
spective shifts. Shifts of perspective in time and space
may be thought of as semantic modulation functions. Al-
though the meaning of ?left? in one sense remains con-
stant across usages, the words ?my? and ?your? modu-
late the meaning by swapping frames of reference. We
suspect that successful use of language requires constant
modulations of meanings of this and related kinds.
We describe the robot and simulator, and mechanisms
for real-time coupling. We then discuss mechanisms
within this architecture designed for the purposes of
grounding the semantics of situated, natural spoken con-
versation. Although no language understanding system
has yet been constructed, we conclude by sketching how
the semantics of each of the words and the whole utter-
ance discussed above can be grounded in the data struc-
tures and processes provided by this architecture. This
work represents steps towards our long term goal of de-
veloping robots and other machines that use language in
the utterance and context that we have described, the ground-
ings listed above play essential roles. It may be argued that
other senses of words are often metaphoric extensions of these
embodied representations (Lakoff and Johnson, 1980).
human-like ways by leveraging deep, grounded represen-
tations of meaning that ?hook? into the world through
machine perception, action, and higher layers of cogni-
tive processes. The work has theoretical implications on
how language is represented and processed by machine,
and also has practical applications where natural human-
robot interaction is needed such as deep-sea robot con-
trol, remote handling of hazardous materials by robots,
and astronaut-robot communication in space.
2 Background
Although robots, speech recognizers, and speech synthe-
sizers can easily be connected in shallow ways, the re-
sults are limited to canned behavior. The proper inte-
gration of language in a robot highlights deep theoret-
ical issues that touch on virtually all aspects of artifi-
cial intelligence (and cognitive science) including per-
ception, action, memory, and planning. Along with other
researchers, we use the term grounding to refer to prob-
lem of anchoring the meaning of words and utterances in
terms of non-linguistic representations that the language
user comes to know through some combination of evolu-
tionary and lifetime learning.
A natural approach is to connect words to perceptual
classifiers so that the appearance of an object, event, or
relation in the environment can instantiate a correspond-
ing word in the robot. This basic idea has been applied
in many speech-controlled robots over the years (Brown
et al, 1992; McGuire et al, 2002; Crangle and Suppes,
1994).
Detailed models have been suggested for sensory-
motor representations underlying color (Lammens,
1994), spatial relations (Regier, 1996; Regier and Carl-
son, 2001). Models for grounding verbs include ground-
ing verb meanings in the perception of actions (Siskind,
2001), and grounding in terms of motor control programs
(Bailey, 1997; Narayanan, 1997). Object shape is clearly
important when connection language to the world, but re-
mains a challenging problem in computational models of
language grounding. Landau and Jackendoff provide a
detailed analysis of additional visual shape features that
play a role in language (Landau and Jackendoff, 1993).
In natural conversation, people speak and gesture to
coordinate joint actions (Clark, 1996). Speakers and lis-
teners use various aspects of their physical environment
to encode and decode utterance meanings. Communica-
tion partners are aware of each other?s gestures and foci
of attention and integrate these source of information into
the conversational process. Motivated by these factors,
recent work on social robots have explored mechanisms
that provide visual awareness of human partners? gaze
and other facial cues relevant for interaction (Breazeal,
2003; Scassellati, 2002).
3 Ripley: An Interactive Robot
Ripley was designed specifically for the purposes of ex-
ploring questions of grounded language, and interactive
language acquisition. The robot has a range of motions
that enables him to move objects around on a tabletop
placed in front of him. Ripley can also look up and make
?eye contact? with a human partner. Three primary con-
siderations drove the design of the robot: (1) We are in-
terested in the effects of changes of visual perspective and
their effects on language and conversation, (2) Sensory-
motor grounding of verbs. (3) Human-directed training
of motion. For example, to teach Ripley the meaning of
?touch?, we use ?show-and-tell? training in which exem-
plars of the word (in this case, motor actions) can be pre-
sented by a human trainer in tandem with verbal descrip-
tions of the action.
To address the first consideration, Ripley has cameras
placed on its head so that all motions of the body lead to
changes of view point. This design decision leads to chal-
lenges in maintaining stable perspectives in a scene, but
reflect the type of corrections that people must also con-
stantly perform. To support acquisition of verbs, Ripley
has been designed with a ?mouth? that can grasp objects
and enable manipulation. As a result, the most natural
class of verbs that Ripley will learn involve manual ac-
tions such as touching, lifting, pushing, and giving. To
address the third consideration, Ripley is actuated with
compliant joints, and has ?training handles?. In spite of
the fact that the resulting robot resembles an arm more
than a torso, it nonetheless serves our purposes as a vehi-
cle for experiments in situated, embodied, conversation.
In contrast, many humanoid robots are not actually able
to move their torso?s to a sufficient degree to obtain sig-
nificant variance in visual perspectives, and grasping is
often not achieved in these robots due to additional com-
plexities of control. This section provides a description of
Ripley?s hardware and low level sensory processing and
motor control software layers.
3.1 Mechanical Structure and Actuation
The robot is essentially an actuated arm, but since cam-
eras and other sensors are placed on the gripper, and the
robot is able to make ?eye contact?, we often think of
the gripper as the robot?s head. The robot has seven de-
grees of freedom (DOF?s) including a 2-DOF shoulder,
1-DOF elbow, 3-DOF wrist / neck, and 1-DOF gripper
/ mouth. Each DOF other than the gripper is actuated
by series-elastic actuators (Pratt et al, 2002) in which all
force from electric motors are transferred through torsion
springs. Compression sensors are placed on each spring
and used for force feedback to the low level motion con-
troller. The use of series-elastic actuators gives Ripley the
ability to precisely sense the amount of force that is being
applied at each DOF, and leads to compliant motions.
3.2 Motion Control
A position-derivative control loop is used to track target
points that are sequenced to transit smoothly from the
starting point of a motion gesture to the end point. Nat-
ural motion trajectories are learned from human teachers
through manual demonstrations.
The robot?s motion is controlled in a layered fashion.
The lowest level is implemented in hardware and consists
of a continuous control loop between motor amplifiers
and force sensors of each DOF. At the next level of con-
trol, a microcontroller implements a position-derivative
(PD) control loop with a 5 millisecond cycle time. The
microcontroller accepts target positions from a master
controller and translates these targets into force com-
mands via the PD control loop. The resulting force com-
mands are sent down stream to the motor amplifier con-
trol loop. The same force commands are also sent up
stream back to the master controller, serving as dynamic
proprioceptive force information
To train motion trajectories, the robot is put in a grav-
ity canceling motor control mode in which forces due
to gravity are estimated based on the robot?s joint po-
sitions and counteracted through actuation. While in
this mode, a human trainer can directly move the robot
through desired motion trajectories. Motion trajectories
are recorded during training. During playback, motion
trajectories can be interrupted and smoothly revised to
follow new trajectories as determined by higher level con-
trol. We have also implemented interpolative algorithms
that blend trajectories to produce new motions that be-
yond the training set.
3.3 Sensory System and Visual Processing
Ripley?s perceptual system is based on several kinds of
sensors. Two color video cameras, a three-axis tilt ac-
celerometer (for sensing gravity), and two microphones
are mounted in the head. Force sensitive resistors provide
a sense of touch on the inside and outside surfaces of the
gripper fingers. In the work reported here, we make use of
only the visual, touch, and force sensors. The remaining
sensors will be integrated in the future. The microphones
have been used to achieve sound source localization and
will play a role in maintaining ?eye contact? with com-
munication partners. The accelerometer will be used to
help correct frames of reference of visual input.
Complementing the motor system is the robot?s sensor
system. One of the most important sets of sensors is the
actuator set itself; as discussed, the actuators are force-
controlled, which means that the control loop adjusts the
force that is output by each actuator. This in turn means
that the amount of force being applied at each joint is
known. Additionally, each DOF is equipped with abso-
lute position sensors that are used for all levels of motion
control and for maintaining the zero-gravity mode.
The vision system is responsible for detecting ob-
jects in the robot?s field of view. A mixture of Gaus-
sians is used to model the background color and pro-
vides foreground/background classification. Connected
regions with uniform color are extracted from the fore-
ground regions. The three-dimensional shape of an object
is represented using histograms of local geometric fea-
tures, each of which represents the silhouette of the ob-
ject from a different viewpoint. Three dimension shapes
are represented in a view-based approach using sets of
histograms. The color of regions is represented using his-
tograms of illumination-normalized RGB values. Details
of the shape and color representations can be found in
(Roy et al, 1999).
To enable grounding of spatial terms such as ?above?
and ?left?, a set of spatial relations similar to (Regier,
1996) is measured between pair of objects. The first fea-
ture is the angle (relative to the horizon) of the line con-
necting the centers of area of an object pair. The second
feature is the shortest distance between the edges of the
objects. The third spatial feature measures the angle of
the line which connects the two most proximal points of
the objects.
The representations of shape, color, and spatial rela-
tions described above can also be generated from virtual
scenes based on Ripley?s mental model as described be-
low. Thus, the visual features can serve as a means to
ground words in either real time camera grounded vision
or simulated synthetic vision.
3.4 Visually-Guided Reaching
Ripley can reach out and touch objects by interpolating
between recorded motion trajectories. A set of sample
trajectories are trained by placing objects on the tabletop,
placing Ripley in a canonical position so that the table
is in view, and then manually guiding the robot until it
touches the object. A motion trajectory library is col-
lected in this way, with each trajectory indexed by the
position of the visual target. To reach an object in an arbi-
trary position, a linear interpolation between trajectories
is computed.
3.5 Encoding Environmental Affordances: Object
Weight and Compliance
Words such as ?heavy? and ?soft? refer to properties of
objects that cannot be passively perceived, but require
interaction with the object. Following Gibson (Gibson,
1979), we refer to such properties of objects as affor-
dances. The word comes from considerations of what
an object affords to an agent who interacts with it. For
instance, a light object can be lifted with ease as opposed
to a heavy object. To assess the weight of an unknown
object, an agent must actually lift (or at least attempt
to lift) it and gauge the level of effort required. This is
precisely how Ripley perceives weight. When an object
is placed in Ripley?s mouth, a motor routine is initiated
which tightly grasps the object and then lifts and low-
ers the object three times. While the motor program is
running, the forces experienced in each DOF (Section
3.2) are monitored. In initial word learning experiments,
Ripley is handed objects of various masses and provided
word labels. A simple Bayes classifier was trained to dis-
tinguish the semantics of ?very light?, ?light?, ?heavy?,
and ?very heavy?. In a similar vein, we also grounded
the semantics of ?hard? and ?soft? in terms of grasping
motor routines that monitor pressure changes at each fin-
gertip as a function of grip displacement.
4 A Perceptually-Driven ?Mental Model?
Ripley integrates real-time information from its visual
and proprioceptive systems to construct an ?internal
replica?, or mental model of its environment that best
explains the history of sensory data that Ripley has ob-
served2. The mental model is built upon the ODE rigid
body dynamics simulator (Smith, 2003). ODE provides
facilities for modeling the dynamics of three dimensional
rigid objects based on Newtonian physics. As Rip-
ley?s physical environment (which includes Ripley?s own
body) changes, perception of these changes drive the cre-
ation, updating, and destruction of objects in the mental
model. Although simulators are typically used in place of
physical systems, we found physical simulation to be an
ideal substrate for implementing Ripley?s mental model
(for coupled on-line simulation, see also (Cao and Shep-
herd, 1989; Davis, 1998; Surdu, 2000)).
The mental model mediates between perception of the
objective world on one hand, and the semantics of lan-
guage on the other. Although the mental model reflects
the objective environment, it is biased as a result of a pro-
jection through Ripley?s particular sensory complex. The
following sections describe the simulator, and algorithms
for real-time coupling to Ripley?s visual and propriocep-
tive systems.
The ODE simulator provides an interface for creating
and destroying rigid objects with arbitrary polyhedron ge-
ometries placed within a 3D virtual world. Client pro-
grams can apply forces to objects and update their proper-
ties during simulation. ODE computes basic Newtonian
updates of object positions at discrete time steps based
object masses and applied forces. Objects in ODE are
currently restricted to two classes. Objects in Ripley?s
workspace (the tabletop) are constrained to be spheres of
fixed size. Ripley?s body is modeled within the simula-
2Mental models have been proposed as a central mechanism
in a broad range of cognitive capacities (Johnson-Laird, 1983).
Figure 1: Ripley looks down at objects on a tabletop.
tor as a configuration of seven connected cylindrical links
terminated with a rectangular head that approximate the
dimensions and mass of the physical robot. We introduce
the following notation in order to describe the simulator
and its interaction with Ripley?s perceptual systems.
4.1 Coupling Perception to the Mental Model
An approximate physical model of Ripley?s body is built
into the simulator. The position sensors from the 7 DOFs
are used to drive a PD control loop that controls the joint
forces applied to the simulated robot. As a result, motions
of the actual robot are followed by dampened motions of
the simulated robot.
A primary motivation for introducing the mental model
is to register, stabilize, and track visually observed ob-
jects in Ripley?s environment. An object permanence
module, called the Objecter, has been developed as a
bridge between raw visual analysis and the physical sim-
ulator. When a visual region is found to stably exist for a
sustained period of time, an object is instantiated by the
Objecter in the ODE physical simulator. It is only at this
point that Ripley becomes ?aware? of the object and is
able to talk about it. Once objects are instantiated in the
mental model, they are never destroyed. If Ripley looks
away from an object such that the object moves out of
view, a representation of the object persists in the mental
model. Figure 1 shows an example of Ripley looking over
the workspace with four objects in view. In Figure 2, the
left image shows the output from Ripley?s head-mounted
camera, and the right image shows corresponding simu-
lated objects which have been registered and are being
tracked.
The Objecter consists of two interconnected compo-
nents. The first component, the 2D-Objecter, tracks two-
dimension visual regions generated by the vision sys-
tem. The 2D-Objecter also implements a hysteresis func-
tion which detects visual regions that persist over time.
Figure 2: Visual regions and corresponding simulated ob-
jects in Ripley?s mental model corresponding to the view
from Figure 1
Figure 3: By positioning a synthetic camera at the posi-
tion approximating the human?s viewpoint, Ripley is able
to ?visualize? the scene from the person?s point of view
which includes a partial view of Ripley.
The second component, the 3D-Objecter, takes as in-
put persistent visual regions from the 2D-Objecter, which
are brought into correspondence with a full three dimen-
sional physical model which is held by ODE. The 3D-
Objecter performs projective geometry calculations to ap-
proximate the position of objects in 3D based on 2D re-
gion locations combined with the position of the source
video camera (i.e., the position of Ripley?s head). Each
time Ripley moves (and thus changes his vantage point),
the hysteresis functions in the 2D-Objecter are reset, and
after some delay, persistent regions are detected and sent
to the 3D-Objecter. No updates to the mental model are
performed while Ripley is in motion. The key problem in
both the 2D- and 3D-Objecter is to maintain correspon-
dence across time so that objects are tracked and persist
in spite of perceptual gaps. Details of the Objecter will
be described in (Roy et al, forthcoming 2003).
4.2 Synthetic Vision and Imagined Changes of
Perspective
The ODE simulator is integrated with the OpenGL 3D
graphics environment. Within OpenGL, a 3D environ-
ment may be rendered from an arbitrary viewpoint by
positioning and orienting a synthetic camera and render-
Figure 4: Using virtual shifts of perspective, arbitrary
vantage points may be taken. The (fixed) location of the
human partner is indicated by the figure on the left.
ing the scene from the camera?s perspective. We take ad-
vantage of this OpenGL functionality to implement shifts
in perspective without physically moving Ripley?s pose.
For example, to view the workspace from the human part-
ner?s point of view, the synthetic camera is simply moved
to the approximate position of the person?s head (which
is currently a fixed location). Continuing our example,
Figures 3 and 4 show examples of two synthetic views
of the situation from Figures 1 and 2. The visual analy-
sis features described in Section 3.3 can be applied to the
images generated by synthetic vision.
4.3 Event-Based Memory
A simple form of run length encoding is used to com-
pactly represent mental model histories. Each time an ob-
ject changes a properties more than a set threshold using
a distance measure that combines color, size, and loca-
tion disparities, an event is detected in the mental model.
Thus stretches of time in which nothing in the environ-
ment changes are represented by a single frame of data
and a duration parameter with a relatively large value.
When an object changes properties, such as its position,
an event is recorded that only retains the begin and end
point of the trajectory but discards the actual path fol-
lowed during the motion. As a result, references to the
past are discretized in time along event boundaries. There
are many limitations to this approach to memory, but as
we shall see, it may nonetheless be useful in grounding
past tense references in natural language.
5 Putting the Pieces Together
We began by asking how a robot might ground the mean-
ing of the utterance, ?Touch the heavy blue thing that was
on my left?. We are now able to sketch an answer to this
question. Ripley?s perceptual system, and motor control
system, and mental model each contribute elements for
grounding the meaning of this utterance. In this section,
we informally show how the various components of the
architecture provide a basis for language grounding.
The semantic grounding of each word in our ex-
ample utterance is presented using algorithmic descrip-
tions reminiscent of the procedural semantics developed
by Winograd (Winograd, 1973) and Miller & Johnson
(Miller and Johnson-Laird, 1976). To begin with a simple
case, the word ?blue? is a property that may be defined as:
property Blue(x) {
c? GetColorModel(x)
return fblue(c)
}
The function returns a scalar value that indicates how
strongly the color of object x matches the expected color
model encoded in fblue. The color model would be en-
coded using the color histograms and histogram com-
parison methods described in Section 3.3. The function
GetColorModel() would retrieve the color model of x
from memory, and if not found, call on motor procedures
to look at x and construct a model.
?Touch? can be grounded in the perceptually-guided
motor procedure described in Section 3.4. This reaching
gesture terminates successfully when the touch sensors
are activated and the visual system reports that the target
x remains in view:
procedure Touch(x) {
repeat
Reach-towards(x)
until touch sensor(s) activated
if x in view then
return success
else
return failure
end if
}
Along similar lines, it is also useful to define a weigh
procedure (which has been implemented as described in
Section 3.5):
procedure Weigh(x) {
Grasp(x)
resistance? 0
while Lift(x) do
resistance? resistance + joint forces
end while
return resistance
}
Weigh() monitors the forces on the robot?s joints as it
lifts x. The accumulated forces are returned by the func-
tion. This weighing procedure provides the basis for
grounding ?heavy?:
property Heavy(x) {
w? GetWeight(x)
return fheavy(w)
}
Similar to GetColorModel(), GetWeight() would first
check if the weight of x is already known, and if not,
then it would optionally call Weigh() to determine the
weight.
To define ?the?, ?my?, and ?was?, it is useful to intro-
duce a data structure that encodes contextual factors that
are salient during language use:
structure Context {
Point-of-view
Working memory
}
The point of view encodes the assumed perspective for
interpreting spatial language. The contents of working
memory would include, by default, objects currently in
the workspace and thus instantiated in Ripley?s mental
model of the workspace. However, past tense markers
such as ?was? can serve as triggers for loading salient el-
ements of Ripley?s event-based memory into the working
model. To highlight its effect on the context data struc-
ture, Was() is defined as a context-shift function:
context-shift Was(context) {
Working memory? Salient events from mental model
history
}
?Was? triggers a request from memory (Section 4.3) for
objects which are added to working memory, making
them accessible to other processes. The determiner ?the?
indicates the selection of a single referent from working
memory:
determiner The(context) {
Select most salient element from working memory
}
In the example, the semantics of ?my? can be grounded
in the synthetic visual perspective shift operation de-
scribed in Section 4.2:
context-shift My(context) {
context.point-of-view? GetPointOfView(speaker)
}
Where GetPointOfV iew(speaker) obtains the spatial
position and orientation of the speaker?s visual input.
?Left? is also grounded in a visual property model
which computes a geometric spatial function (Section
3.3) relative to the assumed point of view:
property Left(x, context) {
trajector? GetPosition(x)
return fleft(trajector, context.point? of ? view)
}
GetPosition(), like GetColorModel() would use the
least effortful means for obtaining the position of x. The
function fleft evaluates how well the position of x fits
a spatial model relative to the point of view determined
from context.
?Thing? can be grounded as:
object Thing(x) {
if (IsTouchable(x) and IsViewable(x)) return true;
else return false
}
This grounding makes explicit use of two affordances of
a thing, that it be touchable and viewable. Touchability
would be grounded using Touch() and viewability based
on whether x has appeared in the mental model (which is
constructed based on visual perception).
The final step in interpreting the utterance is to com-
pose the semantics of the individual words in order to
derive the semantics of the whole utterance. We address
the problem of grounded semantic composition in detail
elsewhere (Gorniak and Roy, protectforthcoming 2003).
For current purposes, we assume that a syntactic parser
is able to parse the utterance and translate it into a nested
set of function calls:
Touch(The(Left(My(Heavy(Blue(Thing(Was(context)))))))))
The innermost argument, context, includes the as-
sumed point of view and contents of working memory.
Each nested function modifies the contents of context
by either shifting points of view, loading new contents
into working memory, or sorting / highlighting contents
of working memory. The Touch() procedure finally acts
on the specified argument.
This concludes our sketch of how we envision the im-
plemented robotic architecture would be used to ground
the semantics of the sample sentence. Clearly many im-
portant details have been left out of the discussion. Our
intent here is to convey only an overall gist of how lan-
guage would be coupled to Ripley. Our current work is
focused on the realization of this approach using spoken
language input.
References
D. Bailey. 1997. When push comes to shove: A computa-
tional model of the role of motor control in the acqui-
sition of action verbs. Ph.D. thesis, Computer science
division, EECS Department, University of California
at Berkeley.
Jon Barwise and John Perry. 1983. Situations and Atti-
tudes. MIT-Bradford.
Cynthia Breazeal. 2003. Towards sociable robots.
Robotics and Autonomous Systems, 42(3-4).
Michael K. Brown, Bruce M. Buntschuh, and Jay G.
Wilpon. 1992. SAM: A perceptive spoken lan-
guage understanding robot. IEEE Transactions on Sys-
tems, Man, and Cybernetics, 22 . IEEE Transactions
22:1390?1402.
F. Cao and B. Shepherd. 1989. Mimic: a robot planning
environment integrating real and simulated worlds. In
IEEE International Symposium on Intelligent Control,
page 459464.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
C. Crangle and P. Suppes. 1994. Language and Learning
for Robots. CSLI Publications, Stanford, CA.
W. J. Davis. 1998. On-line simulation: Need and
evolving research requirements. In J. Banks, editor,
Handbook of Simulation: Principles, Methodology,
Advances, Applications and Practice. Wiley.
James J. Gibson. 1979. The Ecological Approach to Vi-
sual Perception. Erlbaum.
Peter Gorniak and Deb Roy. forthcoming, 2003.
Grounded semantic composition for visual scenes.
P.N. Johnson-Laird. 1983. Mental Models: Towards a
Cognitive Science of Language, Inference, and Con-
sciousness. Cambridge University Press.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press, Chicago.
Johan M. Lammens. 1994. A computational model of
color perception and color naming. Ph.D. thesis, State
University of New York.
Barbara Landau and Ray Jackendoff. 1993. ?What? and
?where? in spatial language and spatial cognition. Be-
havioral and Brain Sciences, 16:217?265.
P. McGuire, J. Fritsch, J.J. Steil, F. Roethling, G.A. Fink,
S. Wachsmuth, G. Sagerer, and H. Ritter. 2002. Multi-
modal human-machine communication for instructing
robot grasping tasks. In Proceedings of the IEEE/RSJ
International Conference on Intelligent Robots and
Systems (IROS).
George Miller and Philip Johnson-Laird. 1976. Lan-
guage and Perception. Harvard University Press.
Srinivas Narayanan. 1997. KARMA: Knowledge-based
active representations for metaphor and aspect. Ph.D.
thesis, University of California Berkeley.
J. Pratt, B. Krupp B, and C. Morse. 2002. Series elas-
tic actuators for high fidelity force control. Industrial
Robot, 29(3):234?241.
T. Regier and L. Carlson. 2001. Grounding spatial lan-
guage in perception: An empirical and computational
investigation. Journal of Experimental Psychology,
130(2):273?298.
Terry Regier. 1996. The human semantic potential. MIT
Press, Cambridge, MA.
Deb Roy and Alex Pentland. 2002. Learning words from
sights and sounds: A computational model. Cognitive
Science, 26(1):113?146.
Deb Roy, Bernt Schiele, and Alex Pentland. 1999.
Learning audio-visual associations from sensory in-
put. In Proceedings of the International Conference
of Computer Vision Workshop on the Integration of
Speech and Image Understanding, Corfu, Greece.
Deb Roy, Kai-Yuh Hsiao, and Nick Mavridis. forthcom-
ing, 2003. Coupling robot perception and on-line sim-
ulation: Towards grounding conversational semantics.
Brian Scassellati. 2002. Theory of mind for a humanoid
robot. Autonomous Robots, 12:13?24.
Jeffrey Siskind. 2001. Grounding the Lexical Semantics
of Verbs in Visual Perception using Force Dynamics
and Event Logic. Journal of Artificial Intelligence Re-
search, 15:31?90.
R Smith. 2003. ODE: Open dynamics engine.
Luc Steels. 2001. Language games for autonomous
robots. IEEE Intelligent Systems, 16(5):16?22.
John R. Surdu. 2000. Connecting simulation to the
mission operational environment. Ph.D. thesis, Texas
A&M.
T. Winograd, 1973. A Process model of Language Un-
derstanding, pages 152?186. Freeman.
Learning Word Meanings and Descriptive Parameter Spaces from Music
Brian Whitman
MIT Media Lab
Music, Mind and Machine
Cambridge, MA USA
bwhitman@media.mit.edu
Deb Roy
MIT Media Lab
Cognitive Machines
Cambridge, MA USA
dkroy@media.mit.edu
Barry Vercoe
MIT Media Lab
Music, Mind and Machine
Cambridge, MA USA
bv@media.mit.edu
Abstract
The audio bitstream in music encodes a high
amount of statistical, acoustic, emotional and
cultural information. But music also has an
important linguistic accessory; most musical
artists are described in great detail in record
reviews, fan sites and news items. We high-
light current and ongoing research into extract-
ing relevant features from audio and simulta-
neously learning language features linked to
the music. We show results in a ?query-by-
description? task in which we learn the per-
ceptual meaning of automatically-discovered
single-term descriptive components, as well as
a method of automatically uncovering ?seman-
tically attached? terms (terms that have percep-
tual grounding.) We then show recent work in
?semantic basis functions? ? parameter spaces
of description (such as fast ... slow or male
... female) that encode the highest descriptive
variance in a semantic space.
1 Introduction
What can you learn by listening to the radio all day? If
the DJ was wordy enough, we argue that you can gain
enough knowledge of the language of perception, as well
as the grammar of description and the grammar of music.
Here we develop a system that uncovers descriptive pa-
rameters of perception completely autonomously. Rela-
tions between English adjectives and audio features are
learned using a new ?severe multi-class? algorithm based
on the support vector machine. Training data consists of
music reviews from the Internet correlated echnology and
Entertainment Media: Rights and Responsibilities with
acoustic recordings of the reviewed music. Once trained,
we obtain a perceptually-grounded lexicon of adjectives
that may be used to automatically label new music. The
1000 2000 3000 4000 50000
0.5
1
1.5 quiet
1000 2000 3000 4000 50000
0.5
1
1.5 loud
1000 2000 3000 4000 50000
0.5
1
1.5 funky
1000 2000 3000 4000 50000
0.5
1
1.5 lonesome
Figure 1: Mean spectral characteristics of four different
terms uncovered by the spectral frame-based single term
attachment system. Magnitude of frequency on the y-
axis, frequency in Hz on the x-axis.
predictive accuracy of the perceptual models are evalu-
ated on unseen test music-review data samples. We con-
sider terms with high predictive accuracy (i.e., that agree
with word usage of musical reviews not used during train-
ing) to be well grounded. We extend our prior work by
introducing a ?linguistic expert,? in the form of a lexical
knowledge base that provides human-encoded symbolic
knowledge about lexical relations. We apply lexical re-
lations to well grounded adjectives to determine the per-
ceptual correlates of opposition. This enables us to move
from isolated word groundings to a gradation system by
discovering the perceptual basis underlying lexical oppo-
sition of adjective pairs (fast ... slow, hard ... soft, etc.).
Once we have uncovered these gradations, we effectively
obtain a set of ?semantic basis functions? which can be
used to characterize music samples based on their per-
ceptual projections onto these lexically determined basis
functions.
Term Precision Term Precision
acoustic 23.2% annoying 0.0%
classical 27.4% dangerous 0.0%
clean 38.9% gorgeous 0.0%
dark 17.1% hilarious 0.0%
electronic 11.7% lyrical 0.0%
female 32.9% sexy 1.5%
happy 13.8% troubled 0.0%
romantic 23.1% typical 0.0%
upbeat 21.0% wicked 0.0%
vocal 18.6% worldwide 2.8%
Table 1: Selected adjective terms and their weighted pre-
cision in predicting a description of as-yet ?unheard? mu-
sic in the frame-based single term attachment system.
The very low baseline and noisy ground truth contribute
to low overall scores, but the difference between ?un-
groundable? and high-scoring terms are significant? for
example, the system cannot find a spectral definition of
?sexy.?
2 Background
In the general audio domain, work has recently been done
(Slaney, 2002) that links sound samples to description us-
ing the labeled descriptions on the sample sets. In the vi-
sual domain, some work has been undertaken attempting
to learn a link between language and multimedia. The
lexicon-learning aspects in (Duygulu et al, 2002) study
a set of fixed words applied to an image database and
use a method similar to EM (expectation-maximization)
to discover where in the image the terms (nouns) ap-
pear. (Barnard and Forsyth, 2000) outlines similar work.
Regier has studied the visual grounding of spatial terms
across languages, finding subtle effects that depend on
the relative shape, size, and orientation of objects (Regier,
1996). Work on motion verb semantics include both pro-
cedural (action) based representations building on Petri
Net formalisms (Bailey, 1997; Narayanan, 1997) and en-
codings of salient perceptual features (Siskind, 2001). In
(Roy, 1999), we explored aspects of learning shape and
color terms, and took first steps in perceptually-grounded
grammar acquisition.
We refer to a word as ?grounded? if we are able to de-
termine reliable perceptual or procedural associations of
the word that agree with normal usage. However, encod-
ing single terms in isolation is only a first step in sensory-
motor grounding. Lexicographers have traditionally stud-
ied lexical semantics in terms of lexical relations such
as opposition, hyponymy, and meronymy (Cruse, 1986).
We have made initial investigations into the perceptual
grounding of lexical relations. We argue that gradations
or linguistic parameter spaces (such as fast ... slow or
big ... small) are necessary to describe high-dimensional
perceptual input.
Figure 2: The ?Radio, Radio? platform for autonomously
learning language and music models. A bank of systems
(with a distributed computing back-end connecting them)
listens to multiple genres of radio streams and hones an
acoustic model. When a new artist is detected from the
metadata, our cultural representation crawler extracts lan-
guage used to describe the artist and adds to our language
model. Concurrently, we learn relations between the mu-
sic and language models to ground language terms in per-
ception.
Our first approach to this problem was in (Whitman
and Rifkin, 2002), in which we learned the descriptions
of music by a combination of automated web crawls for
artist description and analysis of the spectral content of
their music. The results for that work, which appear
in Figure 1 and Table 1, show that we can accurately
predict (well above an impossibly low baseline) a label
on a held-out test set of music. We also see encour-
aging results in the set of terms that were accurately
predicted. In effect we can draw an imaginary line in
the form of a confidence threshold around our results
and assign certain types of terms ?grounded? while oth-
ers are ?ungroundable.? In Table 1 above, we note that
terms like ?electronic? and ?vocal? that would appear in
the underlying perceptual feature space get high scores
while more culturally-influenced terms like ?gorgeous?
and ?sexy? do not do as well. We have recently extended
this work (Whitman et al, 2003) by learning parameters
in the same manner. Just because we know the spectral
shape of ?quiet? and ?loud? (as in Figure 1) we cannot
infer any sort of connecting space between them unless
we know that they are antonyms. In this work, we infer
such gradation spaces through the use of a lexical knowl-
edge base, ?grounding? such parameters through percep-
tion. As well, to capture important time-aware gradations
such as ?fast...slow? we introduce a new machine listening
np Term Score
beth gibbons 0.1648
trip hop 0.1581
dummy 0.1153
goosebumps 0.0756
soulful melodies 0.0608
rounder records 0.0499
dante 0.0499
may 1997 0.0499
sbk 0.0499
grace 0.0499
adj Term Score
cynical 0.2997
produced 0.1143
smooth 0.0792
dark 0.0583
particular 0.0571
loud 0.0558
amazing 0.0457
vocal 0.0391
unique 0.0362
simple 0.0354
Table 2: Top 10 terms (noun phrase and adjective sets) for
the musical group ?Portishead? from community meta-
data.
representation that allows for far more perceptual gener-
ality in the time domain than our previous work?s single
frame-based power spectral density. Our current platform
for retrieving audio and description is shown in Figure 2.
We acknowledge previous work on the computational
study of adjectival scales as in (Hatzivassiloglou and
McKeown, 1993), where a system could group gradation
scales using a clustering algorithm. The polar represen-
tation of adjectives discussed in (Miller, 1990) also influ-
enced our system.
3 Automatically Uncovering Description
We propose an unsupervised model of language feature
collection that is based on description by observation,
that is, learning target classifications by reading about the
musical artists in reviews and discussions.
3.1 Community Metadata
Our model is called community metadata (Whitman and
Lawrence, 2002) and has been successfully used in style
detection (Whitman and Smaragdis, 2002) and artist sim-
ilarity prediction (Ellis et al, 2002). It creates a ma-
chine understandable representation of artist description
by searching the Internet for the artist name and perform-
ing light natural language processing on the retrieved
pages. We split the returned documents into classes en-
compassing n-grams (terms of word length n), adjectives
(using a part-of-speech tagger (Brill, 1992)) and noun
phrases (using a lexical chunker (Ramshaw and Mar-
cus, 1995).) Each pair {artist, term} retrieved is given
an associated salience weight, which indicates the rela-
tive importance of term as associated to artist. These
saliences are computed using a variant of the popular TF-
IDF measure gaussian weighted to avoid highly specific
and highly general terms. (See Table 2 for an example.)
One important feature of community metadata is its time-
sensitivity; terms can be crawled once a week and we can
take into account trajectories of community-level opinion
about certain artists.
Although tempting, we are reticent to make the claim
that the community metadata vectors computationally ap-
proach the ?linguistic division of labor? proposed in (Put-
nam, 1987) as each (albeit unaware) member of the net-
worked community is providing a small bit of informa-
tion and description about the artist in question. We feel
that the heavily biased opinion extracted from the Inter-
net is best treated as an approximation of a ?ground truth
description.? Factorizing the Internet community into rel-
atively coherent smaller communities to obtain sharpened
lexical groundings is part of future work. However, we
do in fact find that the huge amount of information we
retrieve from these crawls average out to a good general
idea of the artists.
4 Time-Aware Machine Listening
We aim for a representation of audio content that cap-
tures as much perceptual content as possible and ask the
system to find patterns on its own. Our representation is
based on the MPEG-7 (Casey, 2001) standard for con-
tent understanding and metadata organization.1 The re-
sult of an MPEG-7 encoding is a discrete state number
l (l = [1...n]) for each 1100 th of a second of input au-
dio. We histogram the state visits into counts for each
n-second piece of audio.
5 Relating Audio to Description
Given an audio and text model, we next discuss how to
discover relationships between them. The approach we
use is the same as our previous work, where we place
the problem as a multi-class classification problem. Our
input observations are the audio-derived features, and
in training, each audio feature is associated with some
salience weight of each of the 200,000 possible terms that
our community metadata crawler discovered. In a recent
test, training 703 separate SVMs on a small adjective set
in the frame-based single term system took over 10 days.
In most machine learning classifiers, time is dependent on
the number of classes. As well, due to the unsupervised
and automatic nature of the description classes, many are
incorrect (such as when an artist is wrongly described)
or unimportant (as in the case of terms such as ?talented?
or ?cool?? meaningless to the audio domain.) Lastly, be-
cause the decision space over the entire artist space is so
large, most class outputs are negative. This creates a bias
problem for most machine learning algorithms. We next
show our attempt at solving these sorts of problems us-
ing a new classifier technique based on the support vector
machine.
1Our audio representation is fully described in (Whitman et
al., 2003).
5.1 Regularized Least-Squares Classification
Regularized Least-Squares Classification (Rifkin, 2002)
allows us to solve ?severe multi-class? problems where
there are a great number of target classes and a fixed set
of source observations. It is related to the Support Vector
Machine (Vapnik, 1998) in that they are both instances
of Tikhonov regularization (Evgeniou et al, 2000), but
whereas training a Support Vector Machine requires the
solution of a constrained quadratic programming prob-
lem, training RLSC only requires solving a single system
of linear equations. Recent work (Fung and Mangasar-
ian, 2001), (Rifkin, 2002) has shown that the accuracy of
RLSC is essentially identical to that of SVMs.
We arrange our observations in a Gram matrix K,
where Kij ? Kf (xi, xj) using the kernel function Kf .
Kf (x1, x2) is a generalized dot product (in a Reproduc-
ing Kernel Hilbert Space (Aronszajn, 1950)) between xi
and xj. We use the Gaussian kernel
Kf (x1, x2) = e
? (|x1?x2|)
2
?2 (1)
where ? is a parameter we keep at 0.5.
Then, training an RLSC system consists of solving the
system of linear equations
(K +
I
C
)c = y, (2)
where C is a user-supplied regularization constant. The
resulting real-valued classification function f is
f(x) =
?`
i=1
ciK(x, xi). (3)
The crucial property of RLSC is that if we store the in-
verse matrix (K+ IC )
?1
, then for a new right-hand side y,
we can compute the new c via a simple matrix multipli-
cation. This allows us to compute new classifiers (after
arranging the data and storing it in memory) on the fly
with simple matrix multiplications.
5.2 Evaluation for a ?Query-by-Description? Task
To evaluate our connection-finding system, we compute
the weighted precision P (at) of predicting the label t for
audio derived features of artist a. We train a new ct for
each term t against the training set. ft(x) for the test set
is computed over each audio-derived observation frame
x and term t. If the sign of ft(x) is the same as our
supposed ?ground truth? for that {artist, t}, (i.e. did the
audio frame for an artist correctly resolve to a known de-
scriptive term?) we consider the prediction successful.
Due to the bias problem mentioned earlier, the evaluation
is then computed on the test set by computing a ?weighted
precision?: where P (ap) indicates overall positive accu-
racy (given an audio-derived observation, the probabil-
ity that a positive association to a term is predicted) and
Perception
LexicalKnowledge BaseDescription by Observation
Figure 3: Overview of our parameter grounding method.
Semantically attached terms are discovered by finding
strong connections to perception. We then ask a ?pro-
fessional? in the form of a lexical knowledge base about
antonymial relations. We use those relations to infer gra-
dations in perception.
P (an) indicates overall negative accuracy, P (a) is de-
fined as P (ap)P (an), which should remain significant
even in the face of extreme negative output class bias.
Now we sort the list of P (at) and set an arbitrary
threshold ?. In our implementation, we use ? = 0.1. Any
P (at) greater than ? is considered ?grounded.? In this
manner we can use training accuracy to throw away badly
scoring classes and then figure out which were incorrect
or unimportant.
6 Linguistic Experts for Parameter
Discovery
Given a set of ?grounded? single terms, we now discuss
our method for uncovering parameter spaces among those
terms and learning the knobs to vary their gradation. Our
model states that certain knowledge is not inferred from
sensory input or intrinsic knowledge but rather by query-
ing a ?linguistic expert.? If we hear ?loud? audio and we
hear ?quiet? audio, we would need to know that those
terms are antonymially related before inferring the gra-
dation space between them.
6.1 WordNet
WordNet (Miller, 1990) is a lexical database hand-
developed by lexicographers. Its main organization is
the ?synset?, a group of synonymous words that may re-
place each other in some linguistic context. The mean-
ing of a synset is captured by its lexical relations, such
as hyponymy, meronymy, or antonymy, to other synsets.
WordNet has a large community of users and various
APIs for accessing the information automatically. Ad-
jectives in WordNet are organized in two polar clusters of
synsets, which each focal synset (the head adjective) link-
ing to some antonym adjective. The intended belief is that
northern - southern playful - serious
unlimited - limited naive - sophisticated
foreign - native consistent - inconsistent
outdoor - indoor foreign - domestic
dissonant - musical physical - mental
opposite - alternate censored - uncensored
unforgettable - forgettable comfortable - uncomfortable
concrete - abstract untamed - tame
partial - fair empirical - theoretical
atomic - conventional curved - straight
lean - rich lean - fat
Table 3: Example synant relations.
descriptive relations are stored as polar gradation spaces,
implying that we can?t fully understand ?loud? without
also understanding ?quiet.? We use these antonymial re-
lations to build up a new relation that encodes as much
antonymial expressivity as possible, which we describe
below.
6.2 Synant Sets
We define a set of lexical relations called synants, which
consist of every antonym of a source term along with ev-
ery antonym of each synonym and every synonym of each
antonym. In effect, we recurse through WordNet?s tree
one extra level to uncover as many antonymial relations
as possible. For example, ?quiet??s anchor antonym is
?noisy,? but ?noisy? has other synonyms such as ?clan-
gorous? and ?thundering.? By uncovering these second-
order antonyms in the synant set, we hope to uncover as
much gradation expressivity as possible. Some example
synants are shown in Table 3.
The obvious downside of computing the synant set is
that they can quickly lose synonymy? following from the
example above, we can go from ?quiet? to its synonym
?untroubled,? which leads to an synantonymial relation
of ?infested.? We also expect problems due to our lack of
sense tagging: ?quiet? to its fourth sense synonym ?re-
strained? to its antonym ?demonstrative,? for example,
probably has little to do with sound. But in both cases
we rely again on the sheer size of our example space;
with so many possible adjective descriptors and the large
potential size of the synant set, we expect our connection-
finding machines to do the hard work of throwing away
the mistakes.
7 Innate Dimensionality of Parameters
Now that we have a set of grounded antonymial adjec-
tives pairs, we would like to investigate the mapping in
perceptual space between each pair. We can do this with
a multidimensional scaling (MDS) algorithm. Let us call
all acoustically derived data associated with one adjec-
tive as X1 and all data associated with the syn-antonym
X2. An MDS algorithm can be used to find a multidi-
mensional embedding of the data based on pairwise sim-
ilarity distances between data points. The similarity dis-
tances between music samples is based on the represen-
tations described in the previous section. Consider first
only the data from X1. The perceptual diversity of this
data will reflect the fact that it represents numerous artists
and songs. Overall, however, we would predict that a low
dimensional space can embed X1 with low stress (i.e.,
good fit to the data) since all samples of X1 share a de-
scriptive label that is well grounded. Now consider the
embedding of the combined data set of X1 and X2. In
this case, the additional dimensions needed to accomo-
date the joint data will reflect the relation between the
two datasets. Our hypothesis was that the additional per-
ceptual variance of datasets formed by combining pairs
of datasets on the basis of adjective pairs which are (1)
well grounded, and (2) synants, would small compared to
combinations in which either of these two combinations
did not hold. Following are intial results supporting this
hypothesis.
7.1 Nonlinear Dimensionality Reduction
Classical dimensional scaling systems such as MDS or
PCA can efficiently learn a low-dimensional weighting
but can only use euclidean or tangent distances between
observations to do so. In complex data sets, the distances
might be better represented as a nonlinear function to
capture inherent structure in the dimensions. Especially
in the case of music, time variances among adjacent ob-
servations could be encoded as distances and used in the
scaling. We use the Isomap algorithm from (Tenenbaum
et al, 2000) to capture this inherent nonlinearity and
structure of the audio features. Isomap scales dimensions
given a NxN matrix of distances between every observa-
tion in N . It roughly computes global geodesic distance
by adding up a number of short ?neighbor hops? (where
the number of neighbors is a tunable parameter, here we
use k = 20) to get between two arbitrarily far points in in-
put space. Schemes like PCA or MDS would simply use
the euclidean distance to do this, where Isomap operates
on prior knowledge of the structure within the data. For
our purposes, we use the same gaussian kernel function
as we do for RLSC (Equation 1) for a distance metric,
which has proved to work well for most music classifica-
tion tasks.
Isomap can embed in a set of dimensions beyond the
target dimension to find the best fit. By studying the
residual variance of each embedding, we can look for the
?elbow? (the point at which the variance falls off to the
minimum)? and treat that embedding as the innate one.
We use this variance to show that our highly-grounded
parameter spaces can be embedded in less dimensions
than ungrounded ones.
8 Experiments and Results
In the following section we describe our experiments us-
ing the aforementioned models and show how we can au-
tomatically uncover the perceptual parameter spaces un-
derlying adjective oppositions.
8.1 Audio dataset
We use audio from the NECI Minnowmatch testbed
(Whitman et al, 2001). The testbed includes on average
ten songs from each of 1,000 albums from roughly 500
artists. The album list was chosen from the most popular
songs on OpenNap, a popular peer-to-peer music sharing
service, in August of 2001. We do not separate audio-
derived features among separate songs since our connec-
tions in language are at the artist level (community meta-
data refers to an artist, not an album or song.) Therefore,
each artist a is represented as a concatenated matrix of
Fa computed from each song performed by that artist.
Fa contains N rows of 40-dimensional data. Each ob-
servation represents 10 seconds of audio data. We choose
a random sampling of artists for both training and testing
(25 artists each, 5 songs for a total of N observations for
testing and training) from the Minnowmatch testbed.
8.2 RLSC for Audio to Term Relation
Each artist in the testbed has previously been crawled for
community metadata vectors, which we associate with
the audio vectors as a yt truth vector. In this experiment,
we limit our results to adjective terms only. The entire
community metadata space of 500 artists ended up with
roughly 2,000 unique adjectives, which provide a good
sense of musical description. The other term types (n-
grams and noun phrases) are more useful in text retrieval
tasks, as they contain more specific information such as
band members, equipment or song titles. Each audio ob-
servation in N is associated with an artist a, which in
turn is related to the set of adjectives with pre-defined
salience. (Salience is zero if the term is not related, un-
bounded if related.) We are treating this problem as clas-
sification, not regression, so we assign not-related terms
a value of -1 and positively related terms are regularized
to 1.
We compute a ct for each adjective term t on the train-
ing set after computing the stored kernel. We use a C
of 10. After all the cts are stored to disk we then bring
out the held-out test set and compute relative adjective
weighted prediction accuracy P (a) for each term. The
results (in Table 4) are similar to our previous work but
we note that our new representation allows us to capture
more time- and structure-oriented terms. We see that the
time-aware MPEG-7 representation creates a far better
sense of perceptual salience than our prior frame-based
power spectral density estimation, which threw away all
short- and mid-time features.
Term Precision Term Precision
busy 42.2% artistic 0.0%
steady 41.5% homeless 0.0%
funky 39.2% hungry 0.0%
intense 38.4% great 0.0%
acoustic 36.6% awful 0.0%
african 35.3% warped 0.0%
melodic 27.8% illegal 0.0%
romantic 23.1% cruel 0.0%
slow 21.6% notorious 0.0%
wild 25.5% good 0.0%
young 17.5% okay 0.0%
Table 4: Select adjective terms discovered by the time-
aware adjective grounding system. Overall, the attached
term list is more musical due to the increased time-aware
information in the representation.
Parameter Precision
big - little 30.3%
present - past 29.3%
unusual - familiar 28.7%
low - high 27.0%
male - female 22.3%
hard - soft 21.9%
loud - soft 19.8%
smooth - rough 14.6%
clean - dirty 14.0%
vocal - instrumental 10.5%
minor - major 10.2%
Table 5: Select automatically discovered parameter
spaces and their weighted precision. The top are the most
semantically significant description spaces for music un-
derstanding uncovered autonomously by our system.
8.3 Finding Parameter Spaces using WordNet
Lexical Relations
We now take our new single-term results and ask our pro-
fessional for help in finding parameters. For all adjec-
tives over our predefined ? we retrieve a restricted synant
set. This restricted set only retrieves synants that are
in our community metadata space: i.e. we would not
return ?soft? as a synant to ?loud? if we did not have
community-derived ?soft? audio. The point here is to only
find synantonymial relations that we have perceptual data
to ?ground? with. We rank our synant space by the mean
of the P (a) of each polar term. For example, P (asoft)
was 0.12 and we found a synant ?loud? in our space with
a P (aloud) of 0.26, so our P (aloud...soft) would be 0.19.
This allows us to sort our parameter spaces by the maxi-
mum semantic attachment. We see results of this process
in Table 5.
We consider this result our major finding: from lis-
tening to a set of albums and reading about the artists, a
computational system has automatically derived the opti-
0 5 10 15 20
0.8
0.85
0.9
0.95 male ? female
0 5 10 15 200.4
0.6
0.8
1 major ? minor
0 5 10 15 200.8
0.85
0.9
0.95
1 loud ? soft
0 5 10 15 200.7
0.8
0.9
1 low ? high
0 5 10 15 200.92
0.94
0.96
0.98
1 alive ? dead
0 5 10 15 200.8
0.85
0.9
0.95
1 quiet ? soft
Res
idua
l Va
rian
ce 
Dimension 
Figure 4: Residual variance elbows (marked by arrows)
for different parameter spaces. Note the clear elbows
for grounded parameter spaces, while less audio-derived
spaces such as ?alive - dead? maintain a high variance
throughout. Bad antonym relations such as ?quiet - soft?
also have no inherent dimensionality.
mal (strongest connection to perception) semantic grada-
tion spaces to describe the incoming observation. These
are not the most statistically significant bases but rather
the most semantically significant bases for understanding
and retrieval.
8.4 Making Knobs and Uncovering Dimensionality
We would like to show the results of such understanding
at work in a classification or retrieval interface, so we then
have another algorithm learn the d-dimensional mapping
of the two polar adjectives in each of the top n parameter
spaces. We also use this algorithm to uncover the natural
dimensionality of the parameter space.
For each parameter space a1 ... a2, we take all obser-
vations automatically labeled by the test pass of RLSC as
a1 and all as a2 and separate them from the rest of the
observations. The observations Fa1 are concatenated to-
gether with Fa2 serially, and we choose an equal number
of observations from both to eliminate bias. We take this
subset of observation Fa12 and embed it into a distance
matrix D with the gaussian kernel in Equation 1. We feed
D to Isomap and ask for a one-dimensional embedding
of the space. The result is a weighting that we can feed
completely new unlabeled audio into and retrieve scalar
values for each of these parameters. We would like to
propose that the set of responses from each of our new
?semantic experts? (weight matrices to determine param-
eter values) define the most expressive semantic repre-
sentation possible for music.
By studying the residual variances of Isomap as in Fig-
ure 4, we can see that Isomap finds inherent dimension-
ality for our top grounded parameter spaces. But for ?un-
grounded? parameters or non-antonymial spaces, there is
less of a clear ?elbow? in the variances indicating a natural
embedding. For example, we see from Figure 4 that the
?male - female? parameter (which we construe as gender
of artist or vocalist) has a lower inherent dimensional-
ity than the more complex ?low - high? parameter and is
lower yet than the ungroundable (in audio) ?alive - dead.?
These results allow us to evaluate our parameter discov-
ery system (in which we show that groundable terms have
clearer elbows) but also provide an interesting window
into the nature of descriptions of perception.
9 Conclusions
We show that we can derive the most semantically sig-
nificant description spaces automatically, and also form
them into a knob for future classification, retrieval and
even synthesis. Our next steps involve user studies of
music description, an attempt to discover if the meaning
derived by community metadata matches up with individ-
ual description, and a way to extract a user model from
language to specify results based on prior experience.
We are also currently working on new automatic lexi-
cal relation discovery techniques. For example, from the
set of audio observations, we can infer antonymial rela-
tions without the use of an expert by finding optimally
statistically separable observations. As well, meronymy,
hyponymy and synonymy can be inferred by studying ar-
tificial combinations of observation (the mixture of ?loud?
and ?peaceful? might not resolve but the mixture of ?sexy?
and ?romantic? might.)
From the perspective of computational linguistics, we
see a rich area of future exploration at the boundary of
perceptual computing and lexical semantics. We have
drawn upon WordNet to strengthen our perceptual repre-
sentations, but we believe the converse is also true. These
experiments are a step towards grounding WordNet in
machine perception.
References
N. Aronszajn. 1950. Theory of reproducing kernels.
Transactions of the American Mathematical Society,
68:337?404.
D. Bailey. 1997. When push comes to shove: A compu-
tational model of the role of motor control in the ac-
quisition of action verbs. Ph.D. thesis, University of
California at Berkeley.
K. Barnard and D. Forsyth. 2000. Learning the seman-
tics of words and pictures.
Eric Brill. 1992. A simple rule-based part-of-speech tag-
ger. In Proc. ANLP-92, 3rd Conference on Applied
Natural Language Processing, pages 152?155, Trento,
IT.
Michael Casey. 2001. General sound recognition and
similarity tools. In MPEG-7 Audio Workshop W-6 at
the AES 110th Convention, May.
D.A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press.
P. Duygulu, K. Barnard, J.F.G. De Freitas, and D.A.
Forsyth. 2002. Object recognition as machine transla-
tion: Learning a lexicon for a fixed image vocabulary.
Dan Ellis, Brian Whitman, Adam Berezweig, and Steve
Lawrence. 2002. The quest for ground truth in musical
artist similarity. In Proc. International Symposium on
Music Information Retrieval ISMIR-2002.
Theodoros Evgeniou, Massimiliano Pontil, and Tomaso
Poggio. 2000. Regularization networks and support
vector machines. Advanced In Computational Mathe-
matics, 13(1):1?50.
Glenn Fung and O. L. Mangasarian. 2001. Proximal
support vector classifiers. In Provost and Srikant, edi-
tors, Proc. Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 77?86. ACM.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1993. Towards the automatic identification of adjecti-
val scales: Clustering adjectives according to meaning.
In Proceedings of the 31st Annual Meeting of the ACL.
G.A. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Narayanan. 1997. Knowledge-based Action Repre-
sentations for Metaphor and Aspect (KARMA). Ph.D.
thesis, University of California at Berkeley.
H. Putnam. 1987. Representation and Reality. MIT
Press.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proc. Third
Workshop on Very Large Corpora, pages 82?94, Som-
erset, New Jersey. Association for Computational Lin-
guistics.
T. Regier. 1996. The human semantic potential. MIT
Press, Cambridge, MA.
Ryan M. Rifkin. 2002. Everything Old Is New Again:
A Fresh Look at Historical Approaches to Machine
Learning. Ph.D. thesis, Massachusetts Institute of
Technology.
D. Roy. 1999. Learning Words from Sights and Sounds:
A Computational Model. Ph.D. thesis, Massachusetts
Institute of Technology.
J. Siskind. 2001. Grounding the Lexical Semantics of
Verbs in Visual Perception using Force Dynamics and
Event Logic. Journal of Artificial Intelligence Re-
search, 15:31?90.
Malcolm Slaney. 2002. Semantic-audio retrieval. In
Proc. 2002 IEEE International Conference on Acous-
tics, Speech and Signal Processing, May.
J.B. Tenenbaum, V. de Silva, and J.C. Langford. 2000. A
global geometric framework for nonlinear dimension-
ality reduction. Science, 290:2319?2323.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Brian Whitman and S. Lawrence. 2002. Inferring de-
scriptions and similarity for music from community
metadata. In Proc. Int. Computer Music Conference
2002 (ICMC), pages 591?598, September.
Brian Whitman and Ryan Rifkin. 2002. Musical query-
by-description as a multi-class learning problem. In
Proc. IEEE Multimedia Signal Processing Conference
(MMSP), December.
Brian Whitman and Paris Smaragdis. 2002. Combin-
ing musical and cultural features for intelligent style
detection. In Proc. Int. Symposium on Music Inform.
Retriev. (ISMIR), pages 47?52, October.
Brian Whitman, Gary Flake, and Steve Lawrence. 2001.
Artist detection in music with minnowmatch. In Proc.
2001 IEEE Workshop on Neural Networks for Signal
Processing, pages 559?568. Falmouth, Massachusetts,
September 10?12.
Brian Whitman, Deb Roy, and Barry Vercoe. 2003.
Grounding a lexicon and lexical relations from ma-
chine perception of music. submitted.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 104?111, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
 Intentional Context in  
Situated Natural Language Learning  
 
Michael Fleischman and Deb Roy 
Cognitive Machines  
The Media Laboratory  
Massachusetts Institute of Technology 
mbf@mit.edu, dkroy@media.mit.edu 
 
Abstract 
Natural language interfaces designed for 
situationally embedded domains (e.g. 
cars, videogames) must incorporate 
knowledge about the users? context to 
address the many ambiguities of situated 
language use. We introduce a model of 
situated language acquisition that operates 
in two phases.  First, intentional context is 
represented and inferred from user actions 
using probabilistic context free grammars.  
Then, utterances are mapped onto this 
representation in a noisy channel 
framework.  The acquisition model is 
trained on unconstrained speech collected 
from subjects playing an interactive game, 
and tested on an understanding task. 
1 Introduction 
As information technologies move off of our 
desktops and into the world, the need for Natural 
Language Processing (NLP) systems that exploit 
information about the environment becomes 
increasingly apparent.  Whether in physical 
environments (for cars and cell phones) or in 
virtual ones (for videogames and training 
simulators), applications are beginning to demand 
language interfaces that can understand 
unconstrained speech about constrained domains.  
Unlike most text-based NLP research, which 
focuses on open-domain problems, work we refer 
to as situated NLP focuses on improving language 
processing by exploiting domain-specific 
information about the non-linguistic situational 
context of users? interactions.  For applications 
where agents interact in shared environments, such 
information is critical for successful 
communication. 
Previous work in situated NLP has focused on 
methods for grounding the meaning of words in 
physical and virtual environments.  The motivation 
for this work comes from the inability of text-
based NLP technologies to offer viable models of 
semantics for human computer interaction in 
shared environments.  For example, imagine a 
situation in which a human user is interacting with 
a robotic arm around a table of different colored 
objects.  If the human were to issue the command 
?give me the blue one,? both the manually-coded 
(Lenat, 1995; Fellbaum, 1998) and statistical 
models (Manning and Schutze, 2000) of meaning 
employed in text-based NLP are inadequate; for, in 
both models, the meaning of a word is based only 
on its relations to other words.  However, in order 
for the robot to successfully ?give me the blue 
one,? it must be able to link the meaning of the 
words in the utterance to its perception of the 
environment (Roy, Hsiao, & Mavridis, 2004).  
Thus, recent work on grounding meaning has 
focused on how words and utterances map onto 
physical descriptions of the environment: either in 
the form of perceptual representations (Roy, in 
press, Siskind, 2001, Regier, 1996) or control 
schemas (Bailey, 1997 Narayanan, 1999).1 
While such physical descriptions are useful 
representations for some classes of words (e.g., 
colors, shapes, physical movements), they are 
insufficient for more abstract language, such as 
that which denotes intentional action.  This 
insufficiency stems from the fact that intentional 
actions (i.e. actions performed with the purpose of 
achieving a goal) are highly ambiguous when 
described only in terms of their physically 
observable characteristics.  For example, imagine a 
situation in which one person moves a cup towards 
another person and utters the unknown word 
                                                 
1 Note that Narayanan?s work moves away from purely 
physical to metaphorical levels of description.  
104
 ?blicket.?  Now, based only on the physical 
description of this action, one might come to think 
of ?blicket? as meaning anything from ?give cup?, 
to ?offer drink?, to ?ask for change.?  This 
ambiguity stems from the lack of contextual 
information that strictly perceptual descriptions of 
action provide.  
This research presents a methodology for 
modeling the intentional context of utterances and 
describes how such representations can be used in 
a language learning task.  We decompose language 
learning into two phases: intention recognition and 
linguistic mapping.  In the first phase, we model 
intentional action using a probabilistic context free 
grammar.  We use this model to parse sequences of 
observed physical actions, thereby inferring a 
hierarchical tree representation of a user?s 
intentions.  In the second phase, we use a noisy 
channel model to learn a mapping between 
utterances and nodes in that tree representation.  
We present pilot situated language acquisition 
experiments using a dataset of paired spontaneous 
speech and action collected from human subjects 
interacting in a shared virtual environment.  We 
evaluate the acquired model on a situated language 
understanding task. 
2 Intention Recognition 
The ability to infer the purpose of others? actions 
has been proposed in the psychological literature 
as essential for language learning in children 
(Tommasello, 2003, Regier, 2003).  In order to 
understand how such intention recognition might 
be modeled in a computational framework, it is 
useful to specify the types of ambiguities that make 
intentional actions difficult to model.  Using as an 
example the situation involving the cup described 
above, we propose that this interaction 
demonstrates two distinct types of ambiguity.  The 
first type, which we refer to as a vertical ambiguity 
describes the ambiguity between the ?move cup? 
vs. ?offer drink? meanings of ?blicket.?  Here the 
ambiguity is based on the level of description that 
the speaker intended to convey.  Thus, while both 
meanings are correct (i.e., both meanings 
accurately describe the action), only one 
corresponds to the word ?blicket.?   
The second type of ambiguity, referred to as 
horizontal ambiguity describes the ambiguity 
between the ?offer drink? vs. ?ask for change? 
interpretations of ?blicket.?  Here there is an 
ambiguity based on what actually is the intention 
behind the physical action.  Thus, it is the case that 
only one of these meaning corresponds to ?blicket? 
and the other meaning is not an accurate 
description of the intended action. 
Figure 1 shows a graphical representation of 
these ambiguities.  Here the leaf nodes represent a 
basic physical description of the action, while the 
root nodes represent the highest-level actions for 
which the leaf actions were performed2.  Such a 
tree representation is useful in that it shows both 
the horizontal ambiguity that exists between the 
nodes labeled ?ask for change? and ?offer drink,? 
as well as the vertical ambiguity that exits between 
the nodes labeled ?offer drink? and ?move cup.? 
 
Figure 1:  Graphical representation of vertical and 
horizontal ambiguities for actions. 
 
In order to exploit the intuitive value of such a 
tree representation, we model intention recognition 
using probabilistic context free grammars 
(PCFG)3.  We develop a small set of production 
rules in which the left hand side represents a higher 
order intentional action (e.g., ?offer drink?), and 
the right hand side represents a sequence of lower 
level actions that accomplish it (e.g. ?grasp cup?, 
?move cup?, ?release cup?).  Each individual 
action (i.e. letter in the alphabet of the PCFG) is 
further modeled as a simple semantic frame that 
contains roles for an agent, an object, an action, 
and multiple optional modifier roles (see inset 
figure 1).  While in this initial work productions 
are created by hand (a task made feasible by the 
                                                 
2 In other words, high-level actions (e.g. ?be polite) are 
preformed by means of the performance of lower-level 
actions (e.g. ?offer drink?). 
3 The idea of a ?grammar of behavior? has a rich history 
in the cognitive sciences dating back at least to Miller et 
al., 1960 
O ffe r  D r in k
M o v e  C u p
o b s e r v e d  a c t io n
A c tio n :   O f fe r  
A g e n t:  P e r s o n 1
P a t ie n t:  P e r s o n 2
O b je c t:   D r in k
A s k  fo r  C h a n g e
B e  P o l i te
L i f t  C u p S l id e  C u p
? B lic k e t?
105
 constrained nature of situated domains) learning 
such rules automatically is discussed in section 4.2. 
Just as in the plan recognition work of Pynadath, 
(1999), we cast the problem of intention 
recognition as a probabilistic parsing problem in 
which sequences of physical actions are used to 
infer an abstract tree representation.  Resolving 
horizontal ambiguities thus becomes equivalent to 
determining which parse tree is most likely given a 
sequence of events.  Further, resolving vertical 
ambiguities corresponds to determining which 
level node in the inferred tree is the correct level of 
description that the speaker had in mind.   
3 Linguistic Mapping 
Given a model of intention recognition, the 
problem for a language learner becomes one of 
mapping spoken utterances onto appropriate 
constituents of their inferred intentional 
representations.  Given the intention representation 
above, this is equivalent to mapping all of the 
words in an utterance to the role fillers of the 
appropriate semantic frame in the induced 
intention tree.  To model this mapping procedure, 
we employ a noisy channel model in which the 
probability of inferring the correct meaning given 
an utterance is approximated by the (channel) 
probability of generating that utterance given that 
meaning, times the (source) prior probability of the 
meaning itself (see Equation 1). 
 
?)|( utterancemeaningp               (1) 
         )1()()|( ?? ?? meaningpmeaningutterancep  
 
Here utterance refers to some linguistic unit 
(usually a sentence) and meaning refers to some 
node in the tree (represented as a semantic frame) 
inferred during intention recognition4.  We can use 
the probability associated with the inferred tree (as 
given by the PCFG parser) as the source 
probability.  Further, we can learn the channel 
probabilities in an unsupervised manner using a 
variant of the EM algorithm similar to machine 
translation (Brown et al, 1993), and statistical 
language understanding (Epstein, 1996).   
4 Pilot Experiments 
4.1 Data Collection 
                                                 
4 ? refers to a weighting coefficient. 
In order to avoid the many physical and perceptual 
problems that complicate work with robots and 
sensor-grounded data, this work focuses on 
language learning in virtual environments.  We 
focus on multiplayer videogames , which support 
rich types of social interactions.  The complexities 
of these environments highlight the problems of 
ambiguous speech described above, and 
distinguish this work from projects characterized 
by more simplified worlds and linguistic 
interactions, such as SHRDLU (Winograd, 1972).  
Further, the proliferation of both commercial and 
military applications (e.g., Rickel et al, 2002) 
involving such virtual worlds suggests that they 
will continue to become an increasingly important 
area for natural language research in the future.   
 
 
Figure 2: Screen shot of Neverwinter Nights game used 
in experimentation. 
 
In order to test our model, we developed a virtual 
environment based on the multi-user videogame 
Neverwinter Nights.5  The game, shown in Figure 
2, provides useful tools for generating modules in 
which players can interact.  The game was 
instrumented such that all players? speech/text 
language and actions are recorded during game 
play.  For data collection, a game was designed in 
which a single player must navigate their way 
through a cavernous world, collecting specific 
objects, in order to escape.  Subjects were paired 
such that one, the novice, would control the virtual 
character, while the other, the expert, guided her 
through the world.  While the expert could say 
anything in order to tell the novice where to go and 
what to do, the novice was instructed not to speak, 
but only to follow the commands of the expert.  
 
                                                 
5 http://nwn.bioware.com/ 
106
 RightClickDoor RightClickFloor RightClickFloor RightClickFloor LeftClickDoor
?ok go into the room? ?go over to that door? ?now open the door?
Expert?s utterances:
Novice?s actions:
RightClickDoor RightClickFloor RightClickFloor RightClickFloor LeftClickDoor
MoveThruRoomOpenDoor OpenDoor
FindAxe PickUpAxe
GetAxe
Intention Recognition
Action:  Get
Agent:   Player
Object:  Axe
GetAxe -> GoToAxe TakeAxe
FindAxe -> Open Move Open
OpenDoor -> ClickDoor
Behavior Grammar
Action:  Open
Agent:   Player
Object:  Door
RightClickDoor RightClickFloor RightClickFloor RightClickFloor LeftClickDoor
MoveThruRoomOpenDoor OpenDoor
FindAxe PickUpAxe
GetAxe
Linguistic Mapping
?now open the door?
P(words|roles)
Figure 3.  Experimental methodology: a) subjects? speech and action sequences are recorded; b) an intentional tree is 
inferred over the sequence of observed actions using a PCFG parser; c) the linguistic mapping algorithm examines 
the mappings between the utterance and all possible nodes to learn the best mapping of words given semantic roles. 
 
The purpose behind these restrictions was to elicit 
free and spontaneous speech that is only 
constrained by the nature of the task.  This 
environment seeks to emulate the type of speech 
that a real situated language system might 
encounter: i.e., natural in its characteristics, but 
limited in its domain of discourse.  
The subjects in the data collection were 
university graduate and undergraduate students.  
Subjects (8 male, 4 female) were staggered such 
that the novice in one trial became the expert in the 
next.  Each pair played the game at least five times, 
and for each of those trials, all speech from the 
expert and all actions from the novice were 
recorded.  Table 1 shows examples of utterances 
recorded from game play, the observed actions 
associated with them, and the actions? inferred 
semantic frame. 
 
Utterance Action Frame 
ok this time you are 
gonna get the axe first 
MOVE  
ROOM1 
act: GET  
obj: AXE 
through the red archway 
on your right 
MOVE  
ROOM2 
act: MOVE 
goal: ARCH 
manner: THRU 
now open that door CLICK_ON 
LEVER 
act: OPEN  
obj: DOOR 
ok now take the axe CLICK_ON 
CHEST 
act: TAKE 
obj: AXE 
source: CHEST 
Table 1: Representative test utterances collected from 
subjects with associated game actions and frames 
 
Data collection produces two parallel streams of 
information: the sequence of actions taken by the 
novice and the audio stream produced by the 
expert (figure 3a).  The audio streams are 
automatically segmented into utterances using a 
speech endpoint detector, which are then 
transcribed by a human annotator.  Each action in 
the sequence is then automatically parsed, and each 
node in the tree is replaced with a semantic frame 
(figure 3b).6  The data streams are then fed into the 
linguistic mapping algorithms as a parallel corpus 
of the expert?s transcribed utterances and the 
inferred semantic roles associated with the 
novice?s actions (figure 3c). 
4.2 Algorithms 
Intention Recognition 
 
As described in section 2, we represent the task 
model associated with the game as a set of 
production rules in which the right hand side 
consists of an intended action (e.g. ?find key?) and 
the left hand side consists of a sequence of sub-
actions that are sufficient to complete that action 
(e.g. ?go through door, open chest, pick_up key?).  
By applying probabilities to the rules, intention 
recognition can be treated as a probabilistic context 
free parsing problem, following Pynadath, 1999.  
For these initial experiments we have hand-
annotated the training data in order to generate the 
grammar used for intention recognition, estimating 
their maximum likelihood probabilities over the 
training set.  In future work, we intend to examine 
how such grammars can be learned in conjunction 
with the language itself; extending research on 
learning task models (Nicolescu and Mataric, 
2003) and work on learning PCFGs (Klein and 
Manning, 2004) with our own work on 
unsupervised language learning. 
Given the PCFG, we use a probabilistic Earley 
parser (Stolcke, 1994), modified slightly to output 
                                                 
6 We use 65 different frames, comprised of 35 unique 
role fillers. 
107
 partial trees (with probabilities) as each action is 
observed.  Figure 4 shows a time slice of an 
inferred intention tree after a player mouse clicked 
on a lever in the game.  Note that both the vertical 
and horizontal ambiguities that exist for this action 
in the game parallel the ambiguities shown in 
Figure 1.  As described above, each node in the 
tree is represented as a semantic frame (see figure 
4 insets), whose roles are aligned to the words in 
the utterances during the linguistic mapping phase. 
 
Linguistic Mapping 
 
The problem of learning a mapping between 
linguistic labels and nodes in an inferred 
intentional tree is recast as one of learning the 
channel probabilities in Equation 1.  Each node in 
a tree is treated as a simple semantic frame and the 
role fillers in these frames, along with the words in 
the utterances, are treated as a parallel corpus.  
This corpus is used as input to a standard 
Expectation Maximization algorithm that estimates 
the probabilities of generating a word given the 
occurrence of a role filler.  We follow IBM Model 
1 (Brown et al, 1993) and assume that each word 
in an utterance is generated by exactly one role in 
the parallel frame  
Using standard EM to learn the role to word 
mapping is only sufficient if one knows to which 
level in the tree the utterance should be mapped.  
However, because of the vertical ambiguity 
inherent in intentional actions, we do not know in 
advance which is the correct utterance-to-level 
mapping.  To account for this, we extend the 
standard EM algorithm as follows (see figure 3c): 
1) set uniform likelihoods for all utterance-to-
level mappings  
2) for each mapping, run standard EM 
3) merge output distributions of EM (weighting 
each by its mapping likelihood) 
4) use merged distribution to recalculate 
likelihoods of all utterance-to-level mappings 
5) goto step 2 
4.3 Experiments  
Methodologies for evaluating language acquisition 
tasks are not standardized.  Given our model, there 
exists the possibility of employing intrinsic 
measures of success, such as word alignment 
accuracy.  However, we choose to measure the 
success of learning by examining the related (and 
more natural) task of language understanding.   
For each subject pair, the linguistic mapping 
algorithms are trained on the first four trials of 
game play and tested on the final trial.  (This gives 
on average 130 utterances of training data and 30 
utterances of testing data per pair.)  For each 
utterance in the test data, we calculate the 
likelihood that it was generated by each frame seen 
in testing.  We select the maximum likelihood 
frame as the system?s hypothesized meaning for 
the test utterance, and examine both how often the 
maximum likelihood estimate exactly matches the 
true frame (frame accuracy), and how many of the 
role fillers within the estimated frame match the 
role fillers of the true frame (role accuracy).7   
 
Figure 4: Inferred intention tree (with semantic 
frames) from human subject game play. 
 
For each subject, the algorithm?s parameters are 
optimized using data from all other subjects. We 
assume correct knowledge of the temporal 
alignment between utterances and actions.  In 
future work, we will relax this assumption to 
explore the effects of not knowing which actions 
correspond to which utterances in time. 
To examine the performance of the model, three 
experiments are presented.  Experiment 1 
examines the basic performance of the algorithms 
on the language understanding task described 
above given uniform priors.  The system is tested 
under two conditions: 1) using the extended EM 
algorithm given an unknown utterance-to-level 
alignment, and 2) using the standard EM algorithm 
given the correct utterance-to-level alignment.   
Experiment 2 tests the benefit of incorporating 
intentional context directly into language 
understanding.  This is done by using the parse 
probability of each hypothesized intention as the 
                                                 
7 See Fleischman and Roy (2005) for experiments 
detailing performance on specific word categories. 
F in d  K e y  E x it  L e v e l
G o  T h r o u g h  D o o r
O p e n  D o o r
P u l l  L e v e r T u r n  K n o b
c l ic k _ o n  l e v e r
A c t io n :  M o v e  
A g e n t:  P la y e r
O b je c t:  D o o r
M a n n e r : T h ro u g h
A c tio n :  G e t  
A g e n t:  P la y e r
O b je c t:  K e y
S o u r c e : C h e s t
A c t io n :  E x it
A g e n t:  
P la y e r
O b je c t:  L e v e l
A c t io n :  O p e n
A g e n t:  
P la y e r
O b je c t:  D o o r
108
 source probability in Equation 1.  Thus, given an 
utterance to understand, we cycle through all 
possible actions in the grammar, parse each one as 
if it were observed, and use the probability 
generated by the parser as its prior probability.  By 
changing the weighting coefficient (?) between the 
source and channel probabilities, we show the 
range of performances of the system from using no 
context at all (?=1) to using only context itself 
(?=0) in understanding.   
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5: Comparison of models trained with utterance-
to-level alignment both known and unknown.  
Performance is on a language understanding task 
(baseline equivalent to choosing most frequent frame) 
 
Experiment 3 studies to what extent inferred tree 
structures are necessary when modeling language 
acquisition.  Although, in section 1, we have 
presented intuitive reasons why such structures are 
required, one might argue that inferring trees over 
sequences of observed actions might not actually 
improve understanding performance when 
compared to a model trained only on the observed 
actions themselves.  This hypothesis is tested by 
comparing a model trained given the correct 
utterance-to-level alignment (described in 
experiment 1) with a model in which each 
utterance is aligned to the leaf node (i.e. observed 
action) below the correct level of alignment.  For 
example, in figure 4, this would correspond to 
mapping the utterance ?go through the door?, not 
to ?GO THROUGH DOOR?, but rather to 
?CLICK_ON LEVER.?   
4.4 Results 
Experiment 1: We present the average performance 
over all subject pairs, trained with the correct 
utterance-to-level alignment both known and 
unknown, and compare it to a baseline of choosing 
the most frequent frame from the training data.  
Figure 5 shows the percentage of maximum 
likelihood frames chosen by the system that 
exactly match the intended frame (frame 
accuracy), as well as, the percentage of roles from 
the maximum likelihood frame that overlap with 
roles in the intended frame (role accuracy). 
As expected, the understanding performance 
goes down for both frames and roles when the 
correct utterance-to-level alignment is unknown.  
Interestingly, while the frame performance 
declines by 14.3%, the performance on roles only 
declines 6.4%.  This difference is due primarily to 
the fact that, while the mapping from words to 
action role fillers is hindered by the need to 
examine all alignments, the mapping from words 
to object role fillers remains relatively robust.  This 
is due to the fact that while each level of intention 
carries a different action term, often the objects 
described at different levels remain the same.  For 
example, in figure 4, the action fillers ?TAKE?, 
?MOVE?, ?OPEN?, and ?PULL? occur only once 
along the path.  However, the object filler 
?DOOR? occurs multiple times.  Thus, the chance 
that the role filler ?DOOR? correctly maps to the 
word ?door? is relatively high compared to the role 
filler ?OPEN? mapping to the word ?open.?8  
 
 
 
 
 
 
 
 
 
 
Figure 6: Frame accuracy as a function of ? value (Eq. 
1) trained on unknown utterance-to-level alignments. 
 
Experiment 2: Figure 6 shows the average frame 
accuracy of the system trained without knowing 
the correct utterance-to-level alignment, as a 
function of varying the ? values from Equation 1.  
The graph shows that including intentional context 
does improve system performance when it is not 
given too much weight (i.e., at relatively high 
alpha values).  This suggests that the benefit of 
intentional context is somewhat outweighed by the 
power of the learned role to word mappings.  
                                                 
8 This asymmetry for learning words about actions vs. 
objects is well known in psychology (Gleitman, 1990) 
and is addressed directly in Fleischman and Roy, 2005. 
2 5 %
2 7 %
2 9 %
3 1 %
3 3 %
3 5 %
3 7 %
3 9 %
4 1 %
4 3 %
4 5 %
4 7 %
4 9 %
0 . 2 0 . 4 0 . 6 0 . 8 1
?
Fr
am
e 
A
cc
ur
ac
y
0 %
1 0 %
2 0 %
3 0 %
4 0 %
5 0 %
6 0 %
7 0 %
8 0 %
9 0 %
f r a m e  a c c u r a c y r o le  a c c u r a c y
b a s e l in e u n k n o w n  le v e l k n o w n  le v e l
109
 Looking closer, we find a strong negative 
correlation (r=-0.81) between the understanding 
performance using only channel probabilities (?=1) 
and the improvement obtained by including the 
intentional context.  In other words, the better one 
does without context, the less context improves 
performance.  Thus, we expect that in noisier 
environments (such as when speech recognition is 
employed) where channel probabilities are less 
reliable, employing intentional context will be 
even more advantageous. 
 
Experiment 3: Figure 7 shows the average 
performance on both frame and role accuracy for 
systems trained without using the inferred tree 
structure (on leaf nodes only) and on the full tree 
structure (given the correct utterance-to-level 
alignment).  Baselines are calculated by choosing 
the most frequent frame from training.9 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
frame accuracy role accuracy
baseline (observed) observed baseline (inferred) inferred
 
Figure 7: Comparison of models trained on inferred 
intentional tree vs. directly on observed actions  
 
It is clear from the figure that understanding 
performance is higher when the intentional tree is 
used in training.  This is a direct result of the fact 
that speakers often speak about high-level 
intentions with words that do not directly refer to 
the observed actions.  For example, after opening a 
door, experts often say: ?go through the door,? for 
which the observed action is a simple movement 
(e.g., ?MOVE ROOMx?).  Also, by referring to 
high-level intentions, experts can describe 
sequences of actions that are not immediately 
referred to.  For example, an expert might say: ?get 
the key? to describe a sequence of actions that 
begins with ?CLICK_ON CHEST.?  Thus, the 
result of not learning over a parsed hierarchical 
                                                 
9 Note that baselines are different for the two conditions, 
because there are a differing number of frames used in 
the leaf node only condition.   
representation of intentions is increased noise, and 
subsequently, poorer understanding performance.  
5 Discussion 
The results from these experiments, although 
preliminary, indicate that this model of language 
acquisition performs well above baseline on a 
language understanding task.  This is particularly 
encouraging given the unconstrained nature of the 
speech on which it was trained.  Thus, even free 
and spontaneous speech can be handled when 
modeling a constrained domain of discourse.10   
In addition to performing well given difficult 
data, the experiments demonstrate the advantages 
of using an inferred intentional representation both 
as a contextual aid to understanding and as a 
representational scaffolding for language learning.  
More important than these preliminary results, 
however, is the general lesson that this work 
suggests about the importance of knowledge 
representations for situated language acquisition.   
As discussed in section 2, learning language 
about intentional action requires dealing with two 
distinct types of ambiguity.  These difficulties 
cannot be handled by merely increasing the 
amount of data used, or switching to a more 
sophisticated learning algorithm.  Rather, dealing 
with language use for situated applications requires 
building appropriate knowledge representations 
that are powerful enough for unconstrained 
language, yet scalable enough for practical 
applications.  The work presented here is an initial 
demonstration of how the semantics of 
unconstrained speech can be modeled by focusing 
on constrained domains.   
As for scalability, it is our contention that for 
situated NLP, it is not a question of being able to 
scale up a single model to handle open-domain 
speech. The complexity of situated communication 
requires the use of domain-specific knowledge for 
modeling language use in different contexts.  Thus, 
with situated NLP systems, it is less productive to 
focus on how to scale up single models to operate 
beyond their original domains.  Rather, as more 
individual applications are tackled (e.g. cars, 
                                                 
10 Notably, situated applications for which natural 
language interfaces are required typically have limited 
domains (e.g., talking to one?s car doesn?t require open-
domain language processing). 
110
 phones, videogames, etc.) the interesting question 
becomes one of how agents can learn to switch 
between different models of language as they 
interact in different domains of discourse. 
6 Conclusion 
We have introduced a model of language 
acquisition that explicitly incorporates intentional 
contexts in both learning and understanding.  We 
have described pilot experiments on paired 
language and action data in order to demonstrate 
both the model?s feasibility as well as the efficacy 
of using intentional context in understanding.  
Although we have demonstrated a first step toward 
an advanced model of language acquisition, there 
is a great deal that has not been addressed.  First, 
what is perhaps most obviously missing is any 
mention of syntax in the language learning process 
and its role in bootstrapping for language 
acquisition.  Future work will focus on moving 
beyond the IBM Model 1 assumptions, to develop 
more syntactically-structured models.   
Further, although the virtual environment used in 
this research bears similarity to situated 
applications that demand NL interfaces, it is not 
known exactly how well the model will perform 
?in the real world.?  Future work will examine 
installing models in real world applications. In 
parallel investigations, we will explore our method 
as a cognitive model of human language learning. 
Finally, as was mentioned previously, the task 
model for this domain was hand annotated and, 
while the constrained nature of the domain 
simplified this process, further work is required to 
learn such models jointly with language.   
In summary, we have presented first steps 
toward tackling problems of ambiguity inherent in 
grounding the semantics of situated language. We 
believe this work will lead to practical applications 
for situated NLP, and provide new tools for 
modeling human cognitive structures and 
processes underlying situated language use 
(Fleischman and Roy, 2005). 
Acknowledgments 
Peter Gorniak developed the software to capture 
data from the videogame used in our experiments. 
References 
D. Bailey, J Feldman, S. Narayanan., & G. Lakoff.. 
Embodied lexical development. 19th Cognitive 
Science Society Meeting. Mahwah, NJ, 1997. 
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra & 
R. L. Mercer. ?The Mathematics of Sta-tistical 
Machine Translation: Parameter Estimation,? 
Computational Linguistics 19(2). 1993. 
M Epstein  Statistical Source Channel Models for 
Natural Language Understanding Ph. D. thesis, 
New York University, September, 1996  
C. Fellbaum WordNet: An On-line Lexical Database 
and Some of its Applications. MIT Press, 1998. 
M. Fleischman and D.K. Roy.  Why Verbs are 
Harder to Learn than Nouns: Initial Insights from a 
Computational Model of Intention Recognition in 
Situated Word Learning.  CogSci. Italy, 2005. 
L. Gleitman. "The structural sources of verb 
meanings." Language Acquisition, 1(1), 1990. 
D. Klein and C. Manning, "Corpus-Based Induction 
of Syntactic Structure: Models of Dependency and 
Constituency", Proc. of the 42nd ACL, 2004 [ 
D. B Lenat,. CYC: A Large-Scale Investment in 
Knowledge Infrastructure". Comm. of ACM, 1995. 
C. Manning, H. Schutze,. Foundations of Statistical 
Natural Language Processing. MIT Press, 2001. 
G. A. Miller, E. Galanter, and K. Pribram 1960. Plans 
and the Structure of Behavior. New York: Halt. 
S. Narayanan.. Moving right along: A computational 
model of metaphoric reasoning about events. In 
Proc. of AAAI. Orlando, FL, 1999. 
M. Nicolescu, M. Mataric?, Natural Methods for 
Robot Task Learning: Instructive Demonstration, 
Generalization and Practice, AGENTS, Australia, 2003.  
D. Pynadath, 1999. Probabilistic Grammars for Plan 
Recognition. Ph.D. Thesis, University of Michigan.  
T. Regier. The human semantic potential. MIT Press, 
Cambridge, MA, 1996. 
T. Regier. Emergent constraints on word-learning: A 
computational review. TICS, 7, 263-268, 2003.  
J. Rickel, S. Marsella, J. Gratch, R. Hill, D. Traum 
and W. Swartout, "Towards a New Generation of 
Virtual Humans for Interactive Experiences," in 
IEEE Intelligent Systems July/August 2002. 
D.Roy, K. Hsiao, and N. Mavridis. Mental imagery 
for a conversational robot. IEEE Trans. on 
Systems, Man, and Cybernetics, 34(3) 2004. 
D. Roy. (in press). Grounding Language in the 
World: Schema Theory Meets Semiotics. AI. 
J.  Siskind. Grounding the Lexical Semantics of 
Verbs in Visual Perception using Force Dynamics 
and Event Logic. JAIR, 2001. 
A. Stolcke. Bayesian Learning of Probabilistic 
Language Models. Ph.d., UC Berkeley, 1994. 
M. Tomasello. Constructing a Language: A Usage-
Based Theory of Language Acquisition. Harvard 
University Press, 2003. 
T. Winograd. Understanding Natural Language. 
Academic Press, 1972. 
111
Extracting aspects of determiner meaning
from dialogue in a virtual world environment
Hilke Reckman, Jeff Orkin, and Deb Roy
MIT Media Lab
{reckman,jorkin,dkroy}@media.mit.edu
Abstract
We use data from a virtual world game for automated learning of words and grammatical con-
structions and their meanings. The language data are an integral part of the social interaction in the
game and consist of chat dialogue, which is only constrained by the cultural context, as set by the
nature of the provided virtual environment. Building on previous work, where we extracted a vocab-
ulary for concrete objects in the game by making use of the non-linguistic context, we now target
NP/DP grammar, in particular determiners. We assume that we have captured the meanings of a set
of determiners if we can predict which determiner will be used in a particular context. To this end we
train a classifier that predicts the choice of a determiner on the basis of features from the linguistic
and non-linguistic context.
1 Introduction
Determiners are among those words whose meanings are hardest to define in a dictionary. In NLP,
determiners are often considered ?stop words? that are not relevant for understanding the content of a
document and should be removed before any interesting processing is done. On the other hand, it has
been shown that children are sensitive to determiner choice already at a very early age, using these
function words in figuring out what content nouns are intended to refer to. Meanings of determiners have
been argued to include important pragmatic and discourse-related functions.
We have a corpus of dialogue that is grounded in a virtual environment. This means that in our data
there is a relation between what people are saying and what they are doing, providing cues as to what
they mean by the words and constructions they use. We have chosen to use a virtual world environment
to collect data in, rather than a real world environment, because relatively rich virtual worlds are by now
available that are able to provide an interesting level of grounding, whereas making sense of real world
scenes using computer vision is still very challenging. In addition, this choice allows us to conveniently
collect data online1.
Although there exists a rich body of computational linguistics research on learning from corpus data,
these corpora usually consist of text only. Only recently corpora that include non-linguistic context have
started to be collected and used for grounded learning of semantics (Chen et al, 2010; Frank et al,
2009; Fleischman and Roy, 2005; Gorniak and Roy, 2005). This kind of work offers new and insightful
perspectives on learning meanings of natural language words and constructions, based on the idea that
our own knowledge of natural language meanings is grounded in action and perception (Roy, 2005), and
that language is a complex adaptive system which evolves in a community through grounded interaction
(e.g. Steels, 2003). So far the language in virtually grounded datasets has often been restricted to either
descriptions or directives, so utterances can be paired fairly directly with the actions they describe. The
interaction in our data is much freer. That means that it is more representative for the data that human
learners get, and that our methods can be applied to a wider variety of data, possibly also to datasets
1Von Ahn and Dabbish (2004) were among the first to realize the potential of collecting human knowledge data online, in a
game setup, collecting a large image-labeling corpus.
245
that have not been collected specifically for this purpose. A related project is KomParse (Klu?wer et al,
2010). Piantadosi et al (2008) developed a Bayesian model that learns compositional semantic meanings
of different kinds of words, including quantifiers, but from completely artificial data.
Our research focuses on learning from data, rather than through interaction, though the latter may be
possible in a later stage of the project. An example of a virtual world project where language is learned
through interaction is ?Wubble World? (Hewlett et al, 2007). In the Give Challenge (Byron et al, 2009)
a virtual world setup is used to evaluate natural language generation systems.
In previous work we have extracted words and multi-word expressions that refer to a range of objects
that are prominent in our virtual environment (Reckman et al, 2010). Now we investigate if aspects
of determiner meaning can be learned from this dataset. The extracted knowledge of nouns makes
the learning of determiners possible. We study what factors contribute to the choice of the determiner
and how they relate to each other, by training a decision tree classifier using these factors as features.
The decision tree provides insight in which features are actually used, in which order, and to which
effect. The accuracy of the resulting classifier on a test set should give us an impression of how well
we understand the use of the different determiners. Although one may argue that this study is about use
rather than about meaning, we take it that meaning can only be learned through use, and it is meaning
that we are ultimately interested in. One of the overarching questions we are concerned with is what
knowledge about language and how it works is needed to extract knowledge about constructions and their
meanings from grounded data. Practically, a computational understanding of determiners will contribute
to determining the reference of referential expressions, particularly in situated dialogue, and to generating
felicitous referential expressions (cf. Belz et al, 2010).
We first introduce our dataset. Then we discuss the automated extraction of determiners. Subse-
quently, we motivate the features we use, present our classifier experiments, and discuss the results.
2 Data: The Restaurant Game
Orkin and Roy (2007) showed in The Restaurant Game project that current computer game technology
allows for simulating a restaurant at a high level-of-detail, and exploiting the game-play experiences
of thousands of players to capture a wider coverage of knowledge than what could be handcrafted by
a team of researchers. The restaurant theme was inspired by the idea of Schank and Abelson (1977),
who argued that the understanding of language requires the representation of common ground for ev-
eryday scenarios. The goal is automating characters with learned behavior and dialogue. The ongoing
Restaurant Game project has provided a rich dataset for linguistic and AI research. In an online two-
player game humans are anonymously paired to play the roles of customers and waitresses in a virtual
restaurant (http://theRestaurantGame.net). Players can chat with open-ended typed text, move around
the 3D environment, and manipulate 47 types of interactive objects through a point-and-click interface
(see figure 1). Every object provides the same interaction options: pick up, put down, give, inspect, sit
on, eat, and touch, but objects respond to these actions in different ways. The chef and bartender are
hard-coded to produce food items based on keywords in chat text. A game takes about 10-15 minutes to
play. Everything players say and do is logged in time-coded text files on our servers. Although player
interactions vary greatly, we have demonstrated that enough people do engage in common behavior that
it is possible for an automatic system to learn statistical models of typical behavior and language that
correlate highly with human judgment of typicality (Orkin and Roy, 2007).
Over 10.000 games have been collected. The dialogue is grounded in two (partially overlapping)
ways. Not only is there a simulated physical environment with objects that can be manipulated in various
ways, but also social patterns of recurring events provide an anchor for making sense of the dialogue.
Previous research results include a first implementation of a planner that drives AI characters playing the
game (Orkin and Roy, 2009).
The intuition is that a human student of English starting from scratch (but with some common sense
knowledge about restaurants), could learn quite a bit of English from studying the Restaurant Game
episodes; possibly enough to play the game. We try to computationally simulate such a learning process.
246
Figure 1: Screen-shots from The Restaurant Game, from left to right: third-person perspective, waitress?s
perspective with dialogue, menu for interacting with objects.
3 Extracting nouns
Previously, we extracted a vocabulary of referring expressions for a set of concrete objects, based on
which words and phrases have the highest relative frequency in the contexts in which the objects are
used (see figure 2). We extracted words and phrases that can refer to the food and drink items on the
restaurant?s menu, the menu, and the bill, and some other items. These expressions represent the core
nominal phrases in the game. We will use these expressions as a starting point to extract determiners
and nominal modifiers. We restrict ourselves to the ordered food and drink items, the menu and the bill,
expecting that these show a somewhat uniform and interesting behavior, as they are the objects that can
appear and disappear during the course of a game.
food type referring expressions
SOUP
?soup?
?vegetable soup?
?soup du jour?
?soup de jour?
SALAD ?salad?
?cobb salad?
SPAGHETTI ?spaghetti??spaghetti marinara?
FILET ?steak? ?filet??filet mignon?
SALMON ?salmon??grilled salmon?
LOBSTER ?lobster?
?lobster thermador?
CHEESECAKE
?cheesecake?
?cheese? ?cake?
?cherry cheesecake?
?cheese cake?
PIE ?pie??berry pie?
TART ?tart?
?nectarine tart?
drink type referring expressions
WATER ?water?
TEA ?tea?
COFFEE ?coffee?
BEER ?beer?
REDWINE ?red? ?wine?
?red wine?
WHITEWINE ?white?
?white wine?
item type referring expressions
MENU ?menu?
BILL ?bill?
?check?
Figure 2: Extracted referring expressions for relevant items.
The referring expressions for these object types have been extracted in an unsupervised manner
making use of the relative frequency of words and phrases in the context of the objects being used.
Words, bigrams and trigrams were validated against each other with the use of one threshold. For more
detail see (Reckman et al, 2010).
4 Extracting determiners
Extracting determiners totally unsupervised is a non-trivial task. Attempts to use the existing fully un-
supervised grammar induction algorithm ADIOS (Solan et al, 2005) did not give us the results we were
hoping for. Instead, we decided to make use of the knowledge of nouns that we already have and target
247
determiners directly, rather than having to induce a full grammar. In future work we will look into using
alternative grammar induction systems, for a wider range of learning tasks.
We first narrowed down our search space by collecting words that are positively associated with
the position directly to the left of the nominal expression above a high recall, low precision threshold
(phi=0.01)2. This should favor determiners and other nominal modifiers over, for example, verbs.
We expect determiners to appear with a wider range of different nouns than adjectival modifiers do.
Especially in this restricted domain, adjectives are more likely to be restricted to specific object types.
We consider pre-nominal terms that are general enough to appear with more than 5 different objects (out
of 17) to be determiner candidates. We also check that our candidates can be preceded by an utterance
boundary.
The word the is most strongly associated with the relevant position, combines with most different
nouns, and can occur as only element between a boundary and a noun. We therefore assume that at
least the is a determiner. We order the other candidates according to their similarity to the, measured
as the cosine distance in a vector-space, with their two words to the left and to the right as dimensions.
We accept words as determiners in order of similarity to the, starting with the most similar word, after
checking that they are in complementary distribution with all of the already accepted words, i.e. that the
word does not occur adjacent to any of those. This gives us the following determiners: the, my, your,
some, a, another, our, one, ur, two, 2.3
We can then identify adjectival modifiers by looking at what occurs between determiners and nouns.
By checking what else these modifiers can be preceded by (that is also in complementary distribution
with known determiners), we can do another round of determiner search, and that lets us add any to
our list. As nouns can also be immediately preceded by an utterance boundary, we establish that the
determiner position is not obligatorily filled.
Of course this is not a complete set of determiners, but they appear to be the most prominent ones in
the game. Real quantifiers are relatively rare and that is to be expected, given the setting. Perhaps more
surprisingly, this and that are not associated with the left-of-noun position. It turns out that they are not
used very frequently as determiners in the game, and much more as pronouns. In future work we will
extract pronouns, by looking for single words that have a distribution that is similar to the distribution of
full noun phrases with a determiner.
In the process of extracting determiners, we also extract adjectives and modifiers such as glass of.
With little extra effort we can build a vocabulary of these as well, including information as to which
nouns they are associated with. Their meanings, however, are in most cases not sufficiently grounded in
the game to be understood. We may in a more advanced stage of the project be able to figure out that the
adjective free makes the item less likely to appear on the bill, but the meaning of hot will always remain
unclear, as temperature is not modeled in the game. Finding words associated with the position to the left
of specific nouns can also help us further improve our vocabulary of referring expressions, for example
by identifying veg and veggie as alternatives for vegetable in vegetable soup4.
We took a shortcut by directly targeting the position left of the noun. This involves language-specific
knowledge about English. To make this method applicable to different languages and only use very
general knowledge at the start, we would first have to find out what the position of the determiner is.
This may be to the right of the noun or affixed to it. Not all languages have articles, but we can expect
determiners like my, your, another etc. to occur either adjacent to5, or morphologically expressed on the
noun6. In previous work we have shown how a construction for coordination can be extracted (Reckman
2The phi-score is a chi-square based association metric. Manning and Schu?tze (2000) argue that such metrics are suitable
to quantify collocational effects. We also used it in extracting the referring expressions.
3For the experiments we replace ur by your, and 2 by two. We assume this could in principle be done automatically, although
especially in the latter case this is not trivial.
4We do already have a list of spelling variants for all the terms, but veg and veggie were too different from the canonical
form to get through the edit-distance filter
5Obviously we do not catch floating quantifiers this way. We might catch their non-floating counterparts and then discover
that they occur in other positions as well.
6Several unsupervised morphological analyzers have been developed, which should in principle be run in an early stage of
learning. For English however, the only interesting morphology at play here is plural formation.
248
et al, 2010). Coordination, to our knowledge, occurs in all languages and this is probably a feature of
general human cognition, so it makes sense to assume it exists in a language and look for it in the data. It
can then be used as a probe on structure. Categories that are grammatically intimately connected to nouns
are more likely to be repeated in a coordination involving two nouns. If we look at our English data, for
example, we see that a lot more material tends to occur between and and the second noun-conjunct, than
between the first noun-conjunct and and, which suggests that things that are grammatically close to the
noun occur to the left of it.
5 Features
In this section we motivate the features we will use. To capture the full meaning of determiners, we
would probably have to model the mental states of the players. However, what we aim at here is a
preliminary understanding of determiners as a step towards the understanding of full sentences, and the
resolution of NP reference and co-reference, which would be prerequisites for any serious modeling of
mental states. So we are interested in what can be learned from directly observable features. The features
are theoretically motivated, and reflect the nature of the referent, whether the referent has been mentioned
before, whether the referent is present, and who the speaker and addressee are.
The first feature is object type. There are 17 different objects that we take into account: BEER,
BILL, CHEESECAKE, COFFEE, FILET, LOBSTER, MENU, PIE, REDWINE, SALAD, SALMON, SOUP,
SPAGHETTI, TART, TEA, WATER, and WHITEWINE. We expect this feature to matter, because in a
restaurant situation one usually orders ?the spaghetti?, but ?a beer?. This may be to some extent dependent
on what is on the menu, but not completely. Regardless of what is on the menu, ordering ?the Heineken?
seems to be more unusual than ordering ?the Merlot?. This may mean that our data is not entirely
representative of the general case, because of our restaurant setting. However, it cannot be excluded that
similar effects play a role in other settings, too. There is of course the effect of mass versus count nouns,
too, but this may be a bit masked, because of unit expressions like glass of. We chose to not include
these unit expressions as a feature, because the decision to use such modifiers can be considered part of
the decision on which determiner to use. So using the modifier as a feature, would be giving away part
of the solution to the determiner-choice problem.
The second feature captures the notion of discourse-old versus discourse-new. We distinguish be-
tween cases where an object of a particular type is mentioned for the first time, and where it has already
been mentioned before. In the latter case, we take it that the discourse referent has already been intro-
duced. The expected effect is that first mentions tend to be indefinite.7 This is only an approximation,
because sometimes a second object of the same type is introduced and we do not resolve the reference
of our instances.
The third and fourth features incorporate present versus future presence of the object, plus the posi-
tion of the utterance with respect to the central action involving the object. We keep track of the previous
and following action in which the object is involved. Actions of interest are restricted to the appearance
of the object and and its central action: ?eating? for food and drink items, ?looking at? for the menu, and
?paying? for the bill. Being involved in such an action also implies presence. Other intervening actions
are ignored. The features are ?preceding action? and ?following action?, and the values are ?appearance?,
?main action?, and ?none?. We expect indefinites before appearance, when the object is not yet present.
Note that these features rely entirely on non-linguistic context.
The fifth and sixth features identify speaker and addressee. The speaker can be the customer or the
waitress. For the addressee the relevant distinction is whether the staff (chef and bartender) are addressed
or not. We expect a tendency of the waitress using your when talking to the customer, and of the customer
using my more often. We expect more indefinites or absence of a determiner when the staff is spoken to.
These features are central to dialogue, and may reveal differences between the roles.
7This is a typical feature for languages that have articles, and may be expressed through other means in other languages.
249
6 Experiments
We use the decision tree classifier from the Natural Language ToolKit for Python (Loper and Bird, 2002)
and train and test it through 10-fold cross-validation on 74304 noun phrases from 5000 games, 23776 of
which actually have determiners. The noun phrases used all contain nouns that can refer to the selected
objects, though we cannot guarantee that they were intended to do so in all cases. In fact, we have seen
examples where this is clearly not the case, and for example filet, which normally refers to the FILET
object, is used in the context of salmon. This means that there is a level of noise in our data.
The instances where the determiner is absent are very dominant, and this part of the data is necessarily
noisy, because of rare determiners that we?ve missed8, and possibly rather heterogeneous, as there are
many reasons why people may choose to not type a determiner in chat. Therefore we focus on the
experiments where we have excluded these cases, as the results are more interesting. We will refer to
the data that excludes instances with no determiner as the restricted dataset. When instances with no
determiner are included, we will talk about the full dataset.
6.1 Baselines
In the experiments we compare the results of using the features to two different baselines. The simplest
baseline is to always choose the most frequent determiner. For the instances that have overt determiners,
the most frequent one is the. Always choosing the gives us a mean accuracy of 0.364. If we include
the instances with no overt determiners, that gives us a much higher baseline of 0.680, when the no
determiner option is always chosen. We call this the simple baseline.
The second baseline is the result of using only the object feature, and forms the basis of our experi-
ments. We call this the object-only baseline. On the restricted dataset the resulting classifier assigns the
determiner a to the objects BEER, COFFEE, PIE, REDWINE, SALAD, TEA, WATER, and WHITEWINE,
and the determiner the to BILL, CHEESECAKE, FILET, LOBSTER, MENU, SALMON, SOUP, SPAGHETTI,
and TART. This yields a mean accuracy of 0.520, which is a considerable improvement over the sim-
ple baseline that is relevant for this part of the data. If we look at the confusion matrix in figure 3 that
summarizes the results of all 10 object-only runs we see that the objects? preferences for definite versus
indefinite determiners are also visible in the way instances with determiners other than the and a are
misclassified. Instances with definite determiners are more often classified as the, and indefinites as a.
a another any my one our some the two your
a <4984> . . . . . . 2912 . .
another 608 <.> . . . . . 76 . .
any 56 . <.> . . . . 24 . .
my 238 . . <.> . . . 742 . .
one 354 . . . <.> . . 241 . .
our 28 . . . . <.> . 178 . .
some 1109 . . . . . <.> 438 . .
the 1270 . . . . . . <7383> . .
two 191 . . . . . . 58 <.> .
your 805 . . . . . . 2075 . <.>
Figure 3: Confusion matrix for the object-only baseline.
On the full dataset, the classifier assigns the to instances of BILL and MENU and no determiner
to everything else, reflecting the count/mass distinction, and resulting in a mean accuracy of 0.707.
This is also a statistically significant improvement over its baseline, but much less spectacular. The
definite/indefinite distinction that we saw with the restricted dataset, does not really emerge here.
8It is also hard to reliably recognize misspelled determiners as determiners tend to be very short words.
250
6.2 Adding the other features
In the core experiments of this paper we always use the object feature as a basis and measure the effect of
adding the other features, separately and in combination. All differences reported are significant, unless
stated otherwise. The table in figure 5 at the end of the section summarizes the results.
If we add the feature of whether the item has been mentioned before or not, we get more indefi-
nites, as was to be expected. On the restricted dataset, the MENU, PIE, and TART objects get a if not
mentioned previously, and the otherwise. The mean accuracy is 0.527, which is a statistically significant
improvement over the object-only baseline (the improvement is consistent over all 10 runs), but it seems
rather small, nevertheless. (Using the discourse feature without the object feature gives a score of 0.377.)
Adding information as to whether the customer has seen the menu does not make any difference. On the
full dataset the discourse feature matters only for MENU, which gets a if not previously mentioned. The
mean accuracy is 0.709.
If, instead, we add the action features we get a somewhat more substantial improvement for the
restricted dataset; a mean accuracy of 0.561. We also get a wider range of determiners: your tends to be
chosen after appearing and before eating, another after eating, and a between no action and appearing.
The order in which the following and preceding action features are applied by the classifier differs per
object. (The action features without the object feature give a mean accuracy score of 0.427.) For the full
dataset the mean accuracy is 0.714, again a consistent, but marginal improvement. However, a, the and
your are the only determiners used, in addition to the no determiner option.
Adding the speaker and addressee features to the object feature base gives the classifier a better grip
on your. More indefinites are used when the staff is addressed, your when the customer is spoken to.
However, my is still not picked up. The speaker and addressee features are used in both orders. The mean
accuracy is 0.540, which is better than with the discourse feature, but worse than with the action features.
(The speaker and addressee features without the object feature give a mean accuracy score of 0.424.) In
the case of the full dataset, the new features are barely used, and there is no consistent improvement over
the different runs. The mean accuracy is 0.711.
If we combine the action features and speaker/addressee features on top of the object feature basis,
we see a substantial improvement again for the restricted dataset. The mean accuracy is 0.592. Finally,
we get some cases of my being correctly classified, and also your is correctly classified significantly
more often than in the previous experiments. The object feature always comes first in the decision tree.
For the other features, all relative orders are attested. Adding the ?previously-mentioned? feature to
this combination (see also figure 4) improves this result a little bit more, to a mean accuracy of 0.594,
although we can expect the information contained in it to have a large overlap with the information in
other features, for example, items mentioned for the first time will typically not have appeared yet.
a another any my one our some the two your
a <5732> 163 1 11 20 . 70 1773 1 125
another 175 <350> . 2 . . 48 70 . 39
any 44 4 <.> . . . 2 29 . 1
my 154 19 . <9> . . 20 765 . 13
one 437 20 . 2 <16> . 4 70 . 46
our 29 1 . . . <.> 1 161 . 14
some 881 48 . 6 3 . <114> 421 . 74
the 1332 74 . 33 8 . 34 <6131> . 1040
two 191 10 . 2 . . 1 45 <.> .
your 218 88 . . . . 20 781 . <1773>
(row = reference; col = test)
Figure 4: Confusion matrix for the object, action, speaker/addressee and discourse features combined.
251
6.3 Linguistic context and dialogue acts
It will be part of future research to distinguish the different dialogue acts that the nominal phrases that
we studied can be part of. Identifying the ?task? that an expression is part of may have a similar effect.
Tasks of the type ?customer gets seated?, ?waitress serves food?, ?customer eats meal?, etc. are annotated
for supervised learning, and may consist of several actions and utterances (Orkin et al, 2010).
To give an indication that the dialogue act that an expression is part of may be informative as to the
correct choice of the determiner, we have done an extra experiment, where we have used the word before
and the word after the DP as features. This gives a tremendous amount of feature values, which are not
very insightful, due to the lack of generalization, and are a near guarantee for over-fitting. However,
it does yield an improvement over using the object-only baseline. Moreover, the preceding word and
following word features are now applied before the object feature. The mean accuracy in this experiment
was 0.562, which is comparable to the experiment with object and action features. At the same time we
get a wider range of determiners than we have had before, including some correctly classified instances
of our. On the full dataset we even get a higher accuracy score than in any of the other experiments:
0.769, also with a much wider range of determiners. We suspect that this local linguistic context gives
quite good cues as to whether the expression is part of a proper sentence or not, and that in the former
case an overt determiner is much more likely9. The results of all experiments are summarized in figure 5.
restricted full
simple baseline 0.364 0.680
object-only baseline 0.520 0.707
object + discourse 0.527 0.709
object + action 0.561 0.714
object + speaker 0.540 0.711
object + action + speaker 0.592 0.721
object + action + speaker + discourse 0.594 0.721
object + surrounding words 0.562 0.769
Figure 5: Summary of the testing results.
7 Discussion
Maybe the most surprising outcome is that the object type turns out to be the main factor in choosing
the determiner in this virtual restaurant setting. It would beinteresting to see this reproduced on the
data of two new games that are currently being developed, with novel scenarios, locations and objects.
At the same time, it is a strength of our approach, that we can simulate a specific setting and capture
its ideosyncrasies, learning domain-specific aspects of language, and hopefully eventually learn what
generalizes across different scenarios.
For the restricted dataset we see that, consistently, indefinites are mostly misclassified as a, and
definites mostly as the. If we evaluate only for definiteness, we get a mean accuracy of 0.800 for the case
with all features combined. We could distinguish these two classes of determiners on the basis of the
similarity of each determiner to the two dominant types. It is, however, the object feature that seems to
be mainly responsible for the gain in definiteness accuracy with respect to the simple baseline.
It is unsurprising that we haven?t learned much about one and two, except that they pattern with
indefinites, as we haven?t included features that have to do with the number of objects. There actually
are more numerals that appear in the game, but did not make it into our list of determiners, because
they did not occur with enough different objects. In the general case, we are doubtful that numerals are
sufficiently grounded in this game for their exact meanings to be learned. It may however be possible to
learn a one-two-many kind of distinction. This would also involve looking into plural morphology, and
remains for future research.
9We have observed that in several games people tend to just sum up food items, without embedding them in a sentence.
252
We also haven?t learned anything about our, except that it patterns with definites. It is not quite clear
what kind of features would be relevant to our in this setting.
For the possessive pronouns your and my we have learned that one tends to be linked to the waitress
as a speaker (and the customer as addressee) and the other to the customer. It will be challenging to reach
an understanding that goes deeper than this10. The range of interactions in the game may be too limited
to learn the meanings of all determiners in their full generality.
While we have treated a and another as different determiners, we have included cases of some more
under some. It may be worthwhile to include some more (and perhaps any more and one more as well) as
a separate determiner. However, our best classifier so far still cannot distinguish between a and another
very well.
The experiments with linguistic context suggest that dialogue act may make for an additional, pow-
erful, albeit indirect, feature. The fact that it helps to know when the main action involving the object
took place, rather than just its appearance, may also be taken to point in the same direction, as people
tend to say different kinds of things about an object before and after the main action.
Using a classifier seems to be a reasonable way of testing how well we understand determiners, as
long as our features provide insight. Although there is still a lot of room for improvement, there is likely
to be a ceiling effect at some point, because sometimes more than one option is felicitous. We also have
to keep in mind that chat is likely to be more variable than normal written or spoken language.
8 Conclusion
We have carried out an exploratory series of experiments, to see if meanings of determiners, a very
abstract linguistic category, could be learned from virtually grounded dialogue data. We have trained a
classifier on a set of theoretically motivated features, and used the testing phase to evaluate how well
these features predict the choice of the determiner.
Altogether, the results are encouraging. If we exclude instances with no determiner we reach an
accuracy of 0.594 over a baseline of 0.364. The features that identify the dialogue participants and
surrounding actions, including appearance, play an important role in this result, even though the object
type remains the main factor. A clear dichotomy between definite and indefinite determiners emerges.
The results for the complete dataset are a bit messier, and need more work.
In future work we will identify utterance types, or dialogue acts, that also rely on surrounding actions
and on the speaker and addressee. We will also look into resolving reference and co-reference.
Acknowledgments
This research was funded by a Rubicon grant from the Netherlands Organisation for Scientific Research
(NWO), project nr. 446-09-011.
References
Belz, A., E. Kow, J. Viethen, and A. Gatt (2010). Generating referring expressions in context: The
GREC task evaluation challenges. In Empirical Methods in Natural Language Generation, pp. 294?
327. Springer.
Byron, D., A. Koller, K. Striegnitz, J. Cassell, R. Dale, J. Moore, and J. Oberlander (2009). Report on
the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE). In Proceedings
of the 12th European Workshop on Natural Language Generation, pp. 165?173. ACL.
Chen, D., J. Kim, and R. Mooney (2010). Training aMultilingual Sportscaster: Using Perceptual Context
to Learn Language. Journal of Artificial Intelligence Research 37, 397?435.
10For their personal pronoun counterparts you and I we might stand a better chance.
253
Fleischman, M. and D. Roy (2005). Why verbs are harder to learn than nouns: Initial insights from a
computational model of intention recognition in situated word learning. In 27th Annual Meeting of
the Cognitive Science Society, Stresa, Italy.
Frank, M., N. Goodman, and J. Tenenbaum (2009). Using speakers? referential intentions to model early
cross-situational word learning. Psychological Science 20(5), 578.
Gorniak, P. and D. Roy (2005). Probabilistic grounding of situated speech using plan recognition and
reference resolution. In Proceedings of the 7th international conference on Multimodal interfaces, pp.
143. ACM.
Hewlett, D., S. Hoversten, W. Kerr, P. Cohen, and Y. Chang (2007). Wubble world. In Proceedings of
the 3rd Conference on Artificial Intelligence and Interactive Entertainment.
Klu?wer, T., P. Adolphs, F. Xu, H. Uszkoreit, and X. Cheng (2010). Talking NPCs in a virtual game
world. In Proceedings of the ACL 2010 System Demonstrations, pp. 36?41. ACL.
Loper, E. and S. Bird (2002). NLTK: The natural language toolkit. In Proceedings of the ACL-02
Workshop on Effective tools and methodologies for teaching natural language processing and compu-
tational linguistics-Volume 1, pp. 70. ACL.
Manning, C. and H. Schu?tze (2000). Foundations of statistical natural language processing. MIT Press.
Orkin, J. and D. Roy (2007). The restaurant game: Learning social behavior and language from thousands
of players online. Journal of Game Development 3(1), 39?60.
Orkin, J. and D. Roy (2009). Automatic learning and generation of social behavior from collective
human gameplay. In Proceedings of The 8th International Conference on Autonomous Agents and
Multiagent Systems-Volume 1, pp. 385?392. International Foundation for Autonomous Agents and
Multiagent Systems.
Orkin, J., T. Smith, H. Reckman, and D. Roy (2010). Semi-Automatic Task Recognition for Interactive
Narratives with EAT & RUN. In Proceedings of the 3rd Intelligent Narrative Technologies Workshop
at the 5th International Conference on Foundations of Digital Games (FDG).
Piantadosi, S., N. Goodman, B. Ellis, and J. Tenenbaum (2008). A Bayesian model of the acquisition of
compositional semantics. In Proceedings of the Thirtieth Annual Conference of the Cognitive Science
Society. Citeseer.
Reckman, H., J. Orkin, and D. Roy (2010). Learning meanings of words and constructions, grounded in
a virtual game. In Proceedings of the 10th Conference on Natural Language Processing (KONVENS).
Roy, D. (2005). Semiotic schemas: A framework for grounding language in action and perception.
Artificial Intelligence 167(1-2), 170?205.
Schank, R. and R. Abelson (1977). Scripts, plans, goals and understanding: An inquiry into human
knowledge structures. Lawrence Erlbaum Associates Hillsdale, NJ.
Solan, Z., D. Horn, E. Ruppin, and S. Edelman (2005). Unsupervised learning of natural languages.
Proceedings of the National Academy of Sciences of the United States of America 102(33), 11629.
Steels, L. (2003). Evolving grounded communication for robots. Trends in cognitive sciences 7(7),
308?312.
Von Ahn, L. and L. Dabbish (2004). Labeling images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing systems, pp. 319?326. ACM.
254
