Simple but effective feedback generation to tutor abstract problem solving
Xin Lu, Barbara Di Eugenio, Stellan Ohlsson and Davide Fossati
University of Illinois at Chicago
Chicago, IL 60607, USA
xinlu@northsideinc.com
bdieugen,stellan,dfossa1@uic.edu
Abstract
To generate natural language feedback for an
intelligent tutoring system, we developed a
simple planning model with a distinguishing
feature: its plan operators are derived auto-
matically, on the basis of the association rules
mined from our tutorial dialog corpus. Auto-
matically mined rules are also used for real-
ization. We evaluated 5 different versions of
a system that tutors on an abstract sequence
learning task. The version that uses our plan-
ning framework is significantly more effective
than the other four versions. We compared this
version to the human tutors we employed in
our tutorial dialogs, with intriguing results.
1 Introduction
Intelligent Tutoring Systems (ITSs) are software
systems that provide individualized instruction, like
human tutors do in one-on-one tutoring sessions.
Whereas ITSs have been shown to be effective in
engendering learning gains, they still are not equiv-
alent to human tutors. Hence, many researchers
are exploring Natural Language (NL) as the key to
bridging the gap between human tutors and current
ITSs. A few results are now available, that show that
ITS with relatively sophisticated language interfaces
are more effective than some other competitive con-
dition (Graesser et al, 2004; Litman et al, 2006;
Evens and Michael, 2006; Kumar et al, 2006; Van-
Lehn et al, 2007; Di Eugenio et al, 2008). Ascer-
taining which specific features of the NL interaction
are responsible for learning still remains an open re-
search question.
In our experiments, we contrasted the richness
with which human tutors respond to student ac-
tions with poorer forms of providing feedback, e.g.
only graphical. Our study starts exploring the role
that positive feedback plays in tutoring and in ITSs.
While it has long been observed that most tutors tend
to avoid direct negative feedback, e.g. (Fox, 1993;
Moore et al, 2004), ITSs mostly provide negative
feedback, as they react to student errors.
In this paper, we will first briefly describe our tu-
torial dialog collection. We will then present the
planning architecture that underlies our feedback
generator. Even if our ITS does not currently al-
low for student input, our generation architecture is
inspired by state-of-the art tutorial dialog manage-
ment (Freedman, 2000; Jordan et al, 2001; Zinn et
al., 2002). One limitation of these approaches is that
plan operators are difficult to maintain and extend,
partly because they are manually defined and tuned.
Crucially, our plan operators are automatically de-
rived via the association rules mined from our cor-
pus. Finally, we will devote a substantial amount
of space to evaluation. Our work is among the first
to show not only that a more sophisticated language
interface results in more learning, but that it favor-
ably compares with human tutors. Full details on
our work can be found in (Lu, 2007).
2 Task and curriculum
Our domain concerns extrapolating letter patterns,
such as inferring EFMGHM, given the pattern
ABMCDM and the new initial letter E. This task is
used in cognitive science to investigate human in-
formation processing (Kotovsky and Simon, 1973;
Reed and Johnson, 1994; Nokes and Ohlsson, 2005).
The curriculum we designed consists of 13 patterns
of increasing length and difficulty; it was used un-
changed in both our human data collection and ITS
experiments. The curriculum is followed by two
104
Tutor Moves
Answer student?s questions
Evaluate student?s actions
Summarize what done so far
Prompt student into activity
Diagnose what student is doing
Instruct
Demonstrate how to solve (portions of) problem
Support ? Encourage student
Conversation ? Acknowledgments, small talk
Student Moves
Question
Explain what student said or did
Reflect ? Evaluate own understanding
Answer tutor?s question
Action Response ? Perform non-linguistic action
(e.g. write letter down)
Complete tutor?s utterance
Conversation ? Acknowledgments, small talk
Table 1: Tutor and student moves
post-test problems, each 15 letter long: subjects (all
from the psychology subject pool) have n trials to
extrapolate each pattern, always starting from a dif-
ferent letter (n = 6 in the human conditions, and
n = 10 in the ITS conditions). While the ear-
lier example was kept simple for illustrative pur-
poses, our patterns become very complex. Start-
ing e.g. from the letter L, we invite the reader to
extrapolate problem 9 in our curriculum: BDDFF-
FCCEEGGGC, or the second test problem: ACZD-
BYYDFXGEWWGI.1
3 Human dialogs
Three tutors ? one expert, one novice, and one
(the lecturer) experienced in teaching, but not in
one-on-one tutoring ? were videotaped as they in-
teracted with 11 subjects each.2 A repeated mea-
sures ANOVA, followed by post-hoc tests, revealed
that students with the expert tutor performed signifi-
cantly better than the students with the other two tu-
tors on both test problems (p < 0.05 in both cases).
36 dialog excerpts were transcribed, taken from
1The solutions are, respectively: LNNPPPMMOOQQQM,
and LNZOMYYOQXRPWWRT .
2One goal of ours was to ascertain whether expert tutors are
indeed more effective than non-expert tutors, not at all a fore-
gone conclusion since very few studies have contrasted expert
and non expert tutors, e.g. (Glass et al, 1999).
18 different subjects (6 per tutor), for a total of
about 2600 tutor utterances and 660 student ut-
terances (transcription guidelines were taken from
(MacWhinney, 2000)). For each subject, these two
dialog excerpts cover the whole interaction with the
tutor on one easy and one difficult problem (# 2 and
9 respectively). 2 groups of 2 coders each, annotated
half of the transcripts each, with dialogue moves.
Our move inventory comprises 9 tutor moves and
7 student moves, as listed in Table 1.3 Table 2
presents an annotated fragment from one of the di-
alogues with the expert tutor. Kappa measures for
intercoder agreement had values in the following
ranges, according to the scale in (Rietveld and van
Hout, 1993): for tutor moves, from moderate (0.4
for Support) to excellent (0.82 for Prompt); for stu-
dent moves, from substantial (0.64 for Explanation)
to excellent (0.82 for Question, 0.97 for ActionRe-
sponse). Whereas some of these Kappa measures
are lower than what we had strived for, we decided
to nonetheless use the move inventory in its entirety,
after the coders reconciled their codings. In fact, our
ultimate evaluation measure concerns learning, and
indeed the ITS version that uses that entire move in-
ventory engenders the most learning. Please see (Lu
et al, 2007) for a detailed analysis of these dialogs
and for a discussion of differences among the tutors
in terms of tutor and student moves.
The transcripts were further annotated by one
coder for tutor attitude (whether the tutor agrees
with the student?s response ? positive, negative, neu-
tral), for correctness of student move and for stu-
dent confidence (positive, negative, neutral). Stu-
dent hesitation time (long, medium, short) was es-
timated by the transcribers. Additionally, we an-
notated for the problem features under discussion.
Of the 8 possible relationships between letters, most
relevant to the examples discussed in this paper are
forward, backward, progression and marker. E.g. in
ABMCDM, M functions as chunk marker, and the
sequence moves forward by one step, both within
one chunk and across chunks. Within and across are
two out of 4 relationship scopes, which encode the
coverage of a particular relationship within the se-
quence.
3There is no explicit tutor question move because we focus
on the goal of a tutor?s question, either prompt or diagnose.
105
Line Utterances Annotation
38 Tutor: how?d you actually get the n in the first place? Diagnose
39 Student: from here I count from c to g and then just from n to r. Answer
40 Tutor: okay so do the c to g. Prompt
41 Tutor: do it out loud so I can hear you do it. Prompt
42 Student: c d e f. Explain
43 Student: so it?s three spaces. Answer
44 Tutor: okay so it?s three spaces in between. Summarize
Table 2: An annotated fragment from a dialogue with the expert tutor
4 Learning rules to provide feedback
Once the corpus was annotated, we mined the ex-
pert tutor portion via Classification based on Associ-
ations (CBA) (Liu et al, 1998). CBA generates un-
derstandable rules and has been effectively applied
to various domains. CBA finds all rules that exist
in the data, which is especially important for small
training sets such as ours.
To modularize what the rules should learn, we de-
composed what the tutor should do into two com-
ponents pertaining to content: letter relationship and
relationship scope; and two components pertaining
to how to deliver that content: tutor move and tutor
attitude. Hence, we derived 4 sets of tutorial rules.
Features used in the rules are those annotated on the
tutoring dialogs, plus the student?s Knowledge State
(KS) on each type of letter relationship rel, com-
puted as follows:
KS(rel) = b
p? 0.5 + w
t
? 5c (1)
p is the number of partially correct student inputs, w
is the number of wrong student inputs and t is the to-
tal number of student inputs (?inputs? here are only
those relevant to the relationship rel, as logged by
the ITS from the beginning of the session). KS(rel)
ranges from 0 to 5. The higher the value, the worse
the performance on rel. The scale of 5 was chosen
to result in just enough values for KS(rel) to be use-
ful for classification.
We ran experiments with different lengths of dia-
log history, but using only the last utterance gave us
the best results. Three of the four rule sets have ac-
curacies between 88 and 90% (results are based on
6-way cross-validation, and the cardinality of each
set of rules is in the low hundreds. ). Whereas the
tutor move rule set only has 57% accuracy, as for
some of the low Kappa values mentioned earlier, our
relation-marker = No, relation-forward = Yes,
student-move = ActionResponse
? relation-forward = Yes
(Confidence = 100%, Support = 4.396%)
correctness = wrong, scope-within = No,
KS(backward) = 0, relation-forward = Yes
? tutor-move = Summarize
(Confidence = 100%, Support = 6.983%)
correctness = wrong, relation-forward = Yes,
KS(forward) = 1, hesitation = no
? tutor-attitude = negative
(Confidence = 100%, Support = 1.130%)
Figure 1: Example Tutorial Rules
ultimate evaluation measure is that the NL feedback
based on these rules does improve learning.
Figure 1 shows three example rules, for choosing
relationship, move and attitude respectively ? we?ll
discuss two of them. The first rule predicts that the
ITS will continue focusing on the forward relation,
if it was focusing on forward and not on marker, and
the student just input something. The second rule
chooses the summarize move if the student made a
mistake, the ITS was focusing on forward but not on
relationships within chunks, and the student showed
perfect knowledge of backward.
Two strength measurements are associated with
each rule X ? y. A rule holds with confidence
conf if conf% of cases that containX also contain y;
and with support sup if sup% of cases contain X or
y. Rules are ordered, with confidence having prece-
dence over support. Ties over confidence are solved
via support; any remaining ties are solved according
to the order rules were generated.
106
For each Tut-Move-Rule TMRi,k whose Left-Hand Side LHS matches ISi do:
1. Create and Populate New Plan pi,k:
(a) preconditions = ISi; actions = tutor move from TMRi,k; strength = confidence and support from TMRi,k
(b) Fill remaining slots in pi,k:
i. contents = relationship ? scope (from highest ranked rules that match ISi from relationship and scope
rule sets);
ii. modifiers = attitude (from highest ranked rule that matches ISi from tutor attitude rule set)
2. Augment Plan: do the following n times :
(a) make copy of ISi and name it ISi+1;
(b) change agent to ?tutor?;
(c) change corresponding elements in ISi+1 to move, attitude, letter relationship and scope from pi,k;
(d) from the two rule sets for tutor move and tutor attitude, retrieve highest ranked rules that match ISi+1,
TMRi+1,j and TARi+1,j
(e) add to actions tutor move from TMRi+1,j ; add to modifiers tutor attitude from TARi+1,j
Figure 2: Plan generation
5 From rules to plans
For our task of extrapolating abstract sequences, we
built a model-tracing ITS by means of the Tutoring
Development Kit (TDK) (Koedinger et al, 2003).
Model-tracing ITSs codify cognitive skills via pro-
duction rules. The student?s solution is monitored
by rules that fire according to the underlying model.
When the student steps differ from that model, an
error is recognized. A portion of the student inter-
face of the ITS is shown in Figure 4a. It mainly in-
cludes two rows, one showing the Example Pattern,
the other for the student to input the New Pattern
extrapolated starting with the letter in the first cell.
In model-tracing ITSs, production rules provide
the capability to generate simple template-based
messages. We developed a more sophisticated NL
feedback generator consisting of three major mod-
ules: update, planning and feedback realization.
The update module maintains the context, rep-
resented by the Information State (IS) (Larsson
and Traum, 2000), which captures the overall dia-
log context and interfaces with external knowledge
sources (e.g., curriculum, tutorial rules) and the pro-
duction rule system. As the student performs a new
action, the IS is updated. The planning module gen-
erates or revises the system plan and selects the next
tutoring move based on the newly updated IS. At
last the feedback realization module transforms this
move into NL feedback.
The planning module consists of three compo-
nents, plan generation, plan selection and plan mon-
itoring. A plan includes an ordered collection of tu-
toring moves meant to help the student correctly fill
a single cell. The structure of our plans is shown in
Figure 3.
Plan generation generates a plan set which con-
tains one plan for each tutor move rule that matches
the current ISi. Each of these plans is augmented at
plan generation time by ?simulating? the next ISi+1
that would result if the move is executed but its ef-
fects are not achieved. The algorithm is sketched
in Figure 2. The planner iterates through the tutor
move rule set.4 Recall that our four rule sets are to-
tally ordered. Also, note that each rule set contains a
default rule that fires when no rule matches the cur-
rent ISi. In Step 1b, at every iteration only the rules
that have not been checked yet from those three rule
sets are considered. In Step 2, n is set to 3, i.e., each
plan contains 3 additional moves and corresponding
attitudes, which will provide hints when no response
from the student occurs. Three hints plus one orig-
inal move makes 4, which is the average number of
moves in one turn of the expert tutor.
An example plan is shown in Figure 3. It is gen-
erated in reaction to the mistake in Figure 4a, and by
4Since there is no language input, rules which include stu-
dent moves other than ActionResponse in their LHS will never
be activated. Additionally, we recast tutor answers as confirm
moves, since students cannot ask questions.
107
Preconditions (same as the IS in Figure 4b)
Effects student?s input = W
Contents relationship = forward
scope = across
Actions summarize, evaluate, prompt,
summarize
Modifiers negative, negative, neutral, neutral
Strength conf = 100%, sup = 6.983%
Figure 3: An Example Plan
firing, among others, the rules in Figure 1. The IS in
Figure 4b reflects some of the history of this interac-
tion (in the slots Relationships, Scopes and KS), and
as such corresponds to the situation depicted in Fig-
ure 4a in a specific context (this plan was generated
in one of our user experiments).
The plan selection component retrieves the high-
est ranked plan in the newly generated plan set, se-
lects a template for each tutoring move in its ?Ac-
tions? slot and puts each tutoring move onto the di-
alog move (DM) stack. Earlier we mentioned that
rules are totally ordered according to confidence,
then support and finally rule generation order. When
a plan set contains more than one plan, plans are
also totally ordered, since they inherit strength mea-
surements from the rule that engenders the first tutor
move in the Actions slot.
After the student receives the message which
realizes the top tutoring move in the DM stack,
plan monitoring checks whether its intended effects
have been obtained. If the effects have not been ob-
tained, and the student?s input is unchanged, the next
move from the DM stack will be executed to pro-
vide the student with hint messages until either the
student?s input changes or the DM stack becomes
empty. If the DM stack becomes empty, the next
plan is selected from the original plan set and the
tutoring moves within that plan are pushed onto the
DM stack. Whenever the student?s input changes,
or after every plan in the plan set has been selected,
control returns to plan generation.
The realization module. A tutor move is pushed
onto the DM stack by plan selection together with
a template to realize it. 50 templates were writ-
ten manually upon inspection of the expert tutor di-
alogs. Since several templates can realize each tutor
move, we used CBA to learn rules to choose among
templates. Features used to learn when to use each
(a) A Student Action in Problem 4
1. Agent: student (producer of current move);
2. Relationships: forward, progress in length
3. Scopes: across (for ?forward?), within (for
?progress in length?);
4. Agent?s move: action response;
5. Agent?s attitude: positive (since student shows
no hesitation before inputting letter);
6. Correctness: wrong (correct letter is W);
7. Student?s input: X;
8. Student?s selection: 4th cell in New Pattern row;
9. Hesitation time: no;
10. Student?s knowledge state (KS): 1 (on ?for-
ward?), 3 (on ?progress in length?).
(b) The corresponding IS
Figure 4: A snapshot of an ITS interaction
template also include the tutor attitude. For the first
Summarize move in the plan in Figure 3, given the
IS in Figure 4b, the rule in Fig. 5 will fire (tutor at-
titude does not affect this specific rule). As a result,
the following feedback message is generated: ?From
V to X, you are going forward 2 in the alphabet.?
6 Evaluation
To demonstrate the utility of our feedback genera-
tor, we developed five different versions of our ITS,
named according to how feedback is generated:
1. No feedback: The ITS only provides the basic
interface, so that subjects can practice solving
the 13 problems in the curriculum, but does not
provide any kind of feedback.
2. Color only: The ITS provides graphic feed-
back by turning the input green if it is correct
or red if it is wrong.
3. Negative: In addition to the color feedback, the
108
scope-within = No, relation-marker = No,
relation-forward = Yes, move= Summarize ?
template = TPL11
[where TPL11: From ?<reference-pattern>?
to ?<input>?, you are going <input-relation>
<input-number> in the alphabet.]
Figure 5: Example Realization Rule
ITS provides feedback messages when the in-
put is wrong.
4. Positive: In addition to the color feedback, the
ITS provides feedback messages when the in-
put is correct.
5. Model: In addition to the color feedback, the
ITS provides feedback messages generated by
the feedback generator just described.
Feedback is given for each input letter. Positive
and negative verbal feedback messages are given
out whenever the student?s input is correct or incor-
rect, respectively. Positive feedback messages con-
firm the correct input and explain the relationships
which this input is involved in. Negative feedback
messages flag the incorrect input and deliver hints.
The feedback messages for the ?negative? and ?pos-
itive? versions were developed earlier in the project,
to avoid repetitions and inspired by the expert tu-
tor?s language but before we performed any anno-
tation and mining. They are directly generated by
TDK production rules.
Although in reality positive and negative feedback
are both present in tutoring sessions, one study for
the letter pattern task shows that positive/negative
feedback, given independently, perform different
functions (Corrigan-Halpern and Ohlsson, 2002). In
addition, our negative condition is meant to embody
the ?classical? model-tracing ITS, that only reacts
to student errors. Hence, in our experiments, we
elected to keep these two types of feedback separate,
other than in the ?model? version of the ITS.
To evaluate the five versions of the ITS, we ran a
between-subjects study in which each group of sub-
jects interacted with one version of the system. A
group of control subjects took the post-test with no
training at all but only read a short description of the
Score
Condition Prob 1 Prob 2 Total
0 Control 36.50 32.84 69.34
1 No feedback 58.21 75.27 133.48
2 Color only 68.32 66.30 134.62
3 Negative 70.33 66.06 141.83
4 Positive 75.06 79.00 154.06
5 Model 91.95 101.76 193.71
Table 3: Average Post-test Scores of the ITS
domain.5 Subjects were trained to solve the same
13 problems in the curriculum that were used in the
human tutoring condition. They also did the same
post-test (2 problems, each pattern 15 letters long).
For each post-test problem, each subject had 10 tri-
als, where each trial started with a new letter.
6.1 Results
Table 3 reports the average post-test scores of the
six groups of subjects, corresponding to the five ver-
sions of the ITS and the control condition. Perfor-
mance on each problem is measured by the number
of correct letters out of a total of 150 letters (15 let-
ters by 10 trials); hence, cumulative post-test score,
is the number of correct letters out of 300 possible.
A note before we proceed. In ITS research it is
common to administer the same test before (pre-test)
and after treatment (post-test), but we only have the
post-test. The pre/post-test paradigm is used for two
reasons. First, for evaluation proper, to gauge learn-
ing gains. Second, to verify that the groups have the
same level of pre-tutoring ability, as shown when the
pre-tests of the different groups are statistically in-
distinguishable, and hence, that they can be rightly
compared. Even without a pre-test we can assess
this. An ANOVA on ?time spent on the first 3 prob-
lems? revealed no significant differences across the
different groups. Since time spent on the first 3 prob-
lems is highly correlated with post-test score (multi-
ple regression, p < 0.03), this provides indirect evi-
dence that all subjects before treatment have equiva-
lent ability for this task. Hence, we can trust that our
evaluation, in terms of absolute scores, does reveal
differences between conditions.
Our main findings are based on one-way
ANOVAs, followed by Tukey post-hoc tests:
5The number of subjects in each condition varies from 32 to
38. Groups differ in size because of technical problems.
109
? A main effect of ITS (p ? 0.05). Subjects who
interacted with any version of the ITS had sig-
nificantly higher total post-test scores than sub-
jects in the control condition.
? A main effect of modeled feedback (p< 0.05).
Subjects who interacted with the ?model? ver-
sion of the ITS had significantly higher total
post-test scores than control subjects, and sub-
jects with any other version of the ITS.
? No other effects. Subjects trained by the three
versions ?color only?, ?negative?, ?positive?,
did not have significantly higher total post-test
scores than subjects with the ?no feedback?
version; neither did subjects trained by the two
versions ?negative?, ?positive?, wrt subjects
with the ?color-only? version.
If we examine individual problems, the same pat-
tern of results hold, other than, interestingly, the
model and positive versions are not significantly dif-
ferent any more. As customary, we also analyze ef-
fect sizes , i.e., how much more subjects learn with
the ?model? ITS in comparison to the other condi-
tions. On the Y axis, Figure 6 shows Cohen?s d, a
common measure of effect size. Each point repre-
sents the difference between the means of the scores
in the ?model? ITS and in one of the other condi-
tion, divided by the standard deviation of either con-
dition. According to (Cohen, 1988), the effect sizes
shown in Figure 6 are large as concerns the compari-
son with the ?no feedback?, ?color only? and ?nega-
tive? conditions, and moderate as concerns the ?pos-
itive? condition.6
ITSs and Human Tutors. After we established
that, at least cumulatively, the ?model? ITS is more
effective than the other ITSs, we wanted to assess
how well the ?model? ITS fares in comparison to
the expert tutor it is modeled on. Since in the human
data each post-test problem consists of only 6 trials,
the first 6 trials per problem from the ITSs are used
to run this comparison, for a maximum total score
of 180 (15 letters by 6 trials, by 2 problems). Fig-
ure 7 shows the overall post-test performance of all
9 conditions. The error bars in the figure represent
the standard deviations.
6A very large effect size with respect to control is not shown
in Figure 6.
Figure 6: Effect sizes: how much more subjects
learn with the ?model? ITS
Figure 7: Post-test performance ? all conditions
Paired t-tests between the model ITS and each of
the human tutors show that:7
? on problem 1, the ?model? ITS is indistinguish-
able from the expert tutor, and is significantly
better than the novice and the lecturer
(p = 0.05 and p = 0.039 respectively);
? on problem 2, the model ITS is significantly
worse than the expert tutor (p = 0.020), and
is not different from the other two tutors;
? cumulatively, there are no significant differ-
ences between the ?model? ITS and any of the
three human tutors.
7A 9-way ANOVA among all conditions is not appropriate,
since in a sense we have two ?super conditions?, human and
ITS. It is better to compare the ?model? ITS to each of the human
tutors via t-tests, as a follow-up to the differences highlighted by
the separate analyses on the two ?super conditions?.
110
7 Discussion and Conclusions
Our results add to the growing body of evidence
that language feedback engenders more learning not
only than simple practice, but also, than less sophis-
ticated language feedback. Importantly, our ?model?
ITS appears intriguingly close to our expert tutor in
effectiveness: on post-test problem 1, it is as effec-
tive as the expert tutor himself, and significantly bet-
ter than the other two tutors, as the expert tutor is. It
appears our ?model? ITS does capture at least some
features of successful tutoring.
As concerns the specific language the ITS gen-
erates, we compared different ways of providing
verbal feedback. A subject receives both positive
and negative verbal feedback when interacting with
the ?model? version, while a subject receives only
one type of verbal feedback when interacting with
the ?positive? and ?negative? versions (recall that
in all these versions including the ?model? ITS the
red/green graphical feedback is provided on every
input). While we cannot draw definite conclusions
regarding the functions of positive and negative
feedback, since the ?model? version provides other
tutorial moves beyond positive / negative feedback,
we have suggestive evidence that negative feedback
by itself is not as effective. Additionally, positive
feedback appears to play an important role. First,
the ?model? and the ?positive? versions are statisti-
cally equivalent when we analyze performance on
individual problems. Further, in the ?model? ver-
sion, the ratio of positive to negative messages turns
out to be 9 to 1. In our tutoring dialogs, positive
feedback still outnumbers negative feedback, but by
a lower margin, 4 to 1. We hypothesize that convey-
ing a positive attitude in an ITS is perhaps even more
important than in human tutoring since a human has
many more ways of conveying subtle shades of ap-
proval and disapproval.
From the NLG point of view, we have presented
a simple generation architecture that turns out to be
rather effective. Among its clear limitations are the
lack of hierarchical planning, and the fact that dif-
ferent components of a plan are generated indepen-
dently one from the other. Among its strengths are
that the plan operators are derived automatically via
the rules we mined, both for content planning and,
partly, for realization.
It clearly remains to be seen whether our NLG
framework can easily be ported to other domains
? the issue is not domain dependence, but whether
a more complex domain will require some form of
hierarchical planning. We are now working in the
domain of Computer Science data structures and al-
gorithms, where we continue exploring the role of
positive feedback. We collected data with two tu-
tors in that domain, and there again, we found that
in the human data positive feedback occurs about 8
times more often than negative feedback. We are
now annotating the data to mine it as we did here,
and developing the core ITS.
Acknowledgments
This work was supported by awards N00014-00-
1-0640 and N00014-07-1-0040 from the Office of
Naval Research, by Campus Research Board S02
and S03 awards from the University of Illinois at
Chicago, and in part, by awards IIS 0133123 and
ALT 0536968 from the National Science Founda-
tion.
References
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences (2nd ed.). Hillsdale, NJ: Lawrence
Earlbaum Associates.
Andrew Corrigan-Halpern and Stellan Ohlsson. 2002.
Feedback effects in the acquisition of a hierarchical
skill. In Proceedings of the 24th Annual Conference
of the Cognitive Science Society.
Barbara Di Eugenio, Davide Fossati, Susan Haller, Dan
Yu, and Michael Glass. 2008. Be brief, and they
shall learn: Generating concise language feedback for
a computer tutor. International Journal of AI in Edu-
cation, 18(4). To appear.
Martha W. Evens and Joel A. Michael. 2006. One-on-
one Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional systems.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Reva K. Freedman. 2000. Plan-based dialogue manage-
ment in a physics tutor. In Proceedings of the Sixth
Applied Natural Language Conference, Seattle, WA,
May.
Michael Glass, Jung Hee Kim, Martha W. Evens, Joel A.
Michael, and Allen A. Rovick. 1999. Novice vs. ex-
pert tutors: A comparison of style. In MAICS-99, Pro-
ceedings of the Tenth Midwest AI and Cognitive Sci-
ence Conference, pages 43?49, Bloomington, IN.
111
Arthur C. Graesser, S. Lu, G.T. Jackson, H. Mitchell,
M. Ventura, A. Olney, and M.M. Louwerse. 2004.
AutoTutor: A tutor with dialogue in natural language.
Behavioral Research Methods, Instruments, and Com-
puters, 36:180?193.
Pamela Jordan, Carolyn Penstein Rose?, and Kurt Van-
Lehn. 2001. Tools for authoring tutorial dialogue
knowledge. In Proceedings of AI in Education 2001
Conference.
Kenneth R. Koedinger, Vincent Aleven, and Neil T. Hef-
fernan. 2003. Toward a rapid development environ-
ment for cognitive tutors. In 12th Annual Conference
on Behavior Representation in Modeling and Simula-
tion.
K. Kotovsky and H. Simon. 1973. Empirical tests of a
theory of human acquisition of information-processing
analysis. British Journal of Psychology, 61:243?257.
Rohit Kumar, Carolyn P. Rose?, Vincent Aleven, Ana Igle-
sias, and Allen Robinson. 2006. Evaluating the Ef-
fectiveness of Tutorial Dialogue Instruction in an Ex-
ploratory Learning Context. In Proceedings of the
Seventh International Conference on Intelligent Tutor-
ing Systems, Jhongli, Taiwan, June.
Staffan Larsson and David R. Traum. 2000. Information
state and dialogue management in the trindi dialogue
move engine toolkit. Natural Language Engineering,
6(3-4):323?340.
Diane J. Litman, Carolyn P. Rose?, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man. 2006. Spoken versus typed human and computer
dialogue tutoring. International Journal of Artificial
Intelligence in Education, 16:145?170.
Bing Liu, Wynne Hsu, and Yiming Ma. 1998. Inte-
grating classification and association rule mining. In
Knowledge Discovery and Data Mining, pages 80?86,
New York, August.
Xin Lu, Barbara Di Eugenio, Trina Kershaw, Stellan
Ohlsson, and Andrew Corrigan-Halpern. 2007. Ex-
pert vs. non-expert tutoring: Dialogue moves, in-
teraction patterns and multi-utterance turns. In CI-
CLING07, Proceedings of the 8th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics, pages 456?467. Best Student Paper
Award.
Xin Lu. 2007. Expert tutoring and natural language
feedback in intelligent tutoring systems. Ph.D. thesis,
University of Illinois - Chicago.
Brian MacWhinney. 2000. The CHILDES project. Tools
for analyzing talk: Transcription Format and Pro-
grams, volume 1. Lawrence Erlbaum, Mahwah, NJ,
third edition.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian
Varges, and Claus Zinn. 2004. Generating Tutorial
Feedback with Affect. In FLAIRS04, Proceedings of
the Seventeenth International Florida Artificial Intel-
ligence Research Society Conference.
Timothy J. Nokes and Stellan Ohlsson. 2005. Compar-
ing multiple paths to mastery: What is learned? Cog-
nitive Science, 29:769?796.
Jonathan Reed and Peder Johnson. 1994. Assessing im-
plicit learning with indirect tests: Determining what is
learned about sequence structure. Journal of Exper-
imental Psychology: Learning, Memory, and Cogni-
tion, 20(3):585?594.
Toni Rietveld and Roeland van Hout. 1993. Statistical
Techniques for the Study of Language and Language
Behaviour. Mouton de Gruyter, Berlin - New York.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,
Pamela W. Jordan, Andrew Olney, and Carolyn P.
Rose?. 2007. When are tutorial dialogues more effec-
tive than reading? Cognitive Science, 31(1):3?62.
Claus Zinn, Johanna D. Moore, and Mark G. Core. 2002.
A 3-tier planning architecture for managing tutorial
dialogue. In ITS 2002, 6th. Intl. Conference on In-
telligent Tutoring Systems, pages 574?584, Biarritz,
France.
112
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 65?75,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exploring Effective Dialogue Act Sequences
in One-on-one Computer Science Tutoring Dialogues
Lin Chen, Barbara Di Eugenio
Computer Science
U. of Illinois at Chicago
lchen43,bdieugen@uic.edu
Davide Fossati
Computer Science
Carnegie Mellon U. in Qatar
davide@fossati.us
Stellan Ohlsson, David Cosejo
Psychology
U. of Illinois at Chicago
stellan,dcosej1@uic.edu
Abstract
We present an empirical study of one-on-
one human tutoring dialogues in the domain
of Computer Science data structures. We
are interested in discovering effective tutor-
ing strategies, that we frame as discovering
which Dialogue Act (DA) sequences corre-
late with learning. We employ multiple lin-
ear regression, to discover the strongest mod-
els that explain why students learn during
one-on-one tutoring. Importantly, we define
?flexible? DA sequence, in which extraneous
DAs can easily be discounted. Our experi-
ments reveal several cognitively plausible DA
sequences which significantly correlate with
learning outcomes.
1 Introduction
One-on-one tutoring has been shown to be a very ef-
fective form of instruction compared to other educa-
tional settings. Much research on discovering why
this is the case has focused on the analysis of the
interaction between tutor and students (Fox, 1993;
Graesser et al, 1995; Lepper et al, 1997; Chi et al,
2001). In the last fifteen years, many such analyses
have been approached from a Natural Language Pro-
cessing (NLP) perspective, with the goal of build-
ing interfaces that allow students to naturally inter-
act with Intelligent Tutoring Systems (ITSs) (Moore
et al, 2004; Cade et al, 2008; Chi et al, 2010).
There have been two main types of approaches to
the analysis of tutoring dialogues. The first kind
of approach compares groups of subjects interact-
ing with different tutors (Graesser et al, 2004; Van-
Lehn et al, 2007), in some instances contrasting the
number of occurrences of relevant features between
the groups (Evens and Michael, 2006; Chi et al,
2010). However, as we already argued in (Ohlsson
et al, 2007), this code-and-count methodology only
focuses on what a certain type of tutor (assumed to
be better according to certain criteria) does differ-
ently from another tutor, rather than on strategies
that may be effective independently from their fre-
quencies of usage by different types of tutor. Indeed
we had followed this same methodology in previous
work (Di Eugenio et al, 2006), but a key turning
point for our work was to discover that our expert
and novice tutors were equally effective (please see
below).
The other kind of approach uses linear regression
analysis to find correlations between dialogue fea-
tures and learning gains (Litman and Forbes-Riley,
2006; Di Eugenio et al, 2009). Whereas linear
regression is broadly used to analyze experimental
data, only few analyses of tutorial data or tutoring
experiments use it. In this paper, we follow
Litman and Forbes-Riley (2006) in correlating se-
quences of Dialogue Acts (DAs) with learning gains.
We extend that work in that our bigram and trigram
DAs are not limited to tutor-student DA bigrams ?
Litman and Forbes-Riley (2006) only considers bi-
grams where one DA comes from the tutor?s turn
and one from the student?s turn, in either order. Im-
portantly, we further relax constraints on how these
sequences are built, in particular, we are able to
model DA sequences that include gaps. This allows
us to discount the noise resulting from intervening
DAs that do not contribute to the effectiveness of
the specific sequence. For example, if we want to
65
explore sequences in which the tutor first provides
some knowledge to solve the problem (DPI) and
then knowledge about the problem (DDI) (DPI and
DDI will be explained later), an exchange such as
the one in Figure 1 should be taken into account
(JAC and later LOW are the tutors, students are indi-
cated with a numeric code, such as 113 in Figure 1).
However, if we just use adjacent utterances, the ok
from the student (113) interrupts the sequence, and
we could not take this example into account. By al-
lowing gaps in our sequences, we test a large number
of linear regression models, some of which result in
significant models that can be used as guidelines to
design an ITS. Specifically, these guidelines will be
used for further improvement of iList, an ITS that
provides feedback on linked list problems and that
we have developed over the last few years. Five
different versions of iList have been evaluated with
220 users (Fossati et al, 2009; Fossati et al, 2010).
iList is available at http://www.digitaltutor.net, and
has been used by more than 550 additional users at
15 different institutions.
JAC: so we would set k equal to e and then delete. [DPI]
113: ok.
JAC: so we?ve inserted this whole list in here.[DDI]
113: yeah.
Figure 1: {DPI, DDI} Sequence Excerpt
The rest of the paper is organized as follows.
In Section 2, we describe the CS-Tutoring corpus,
including data collection, transcription, and anno-
tation. In Section 3, we introduce our methodol-
ogy that combines multiple linear regression with n-
grams of DAs that allow for gaps. We discuss our
experiments and results in Section 4.
2 The CS Tutoring Corpus
2.1 Data Collection
During the time span of 3 semesters, we collected a
corpus of 54 one-on-one tutoring sessions on Com-
puter Science data structures: linked list, stack and
binary search tree. (In the following context, we
will refer them as Lists, Stacks and Trees). Each stu-
dent only participated in one session, and was ran-
domly assigned to one of two tutors: LOW, an expe-
rienced Computer Science professor, with more than
30 years of teaching experience; or JAC, a senior un-
dergraduate student in Computer Science, with only
one semester of previous tutoring experience. In the
end 30 students interacted with LOW and 24 with
JAC.
Students took a pre-test right before the tutoring
session, and an identical post-test immediately after.
The test had two problems on Lists, two problems on
Stacks, and four problems on Trees. Each problem
was graded out of 5 points, for a possible maximum
score of 10 points each for Lists and Stacks, and 20
points for Trees. Pre and post-test scores for each
topic were later normalized to the [0..1] interval, and
learning gains were computed.
Table 1 includes information on session length.
Note that for each topic, the number of sessions is
lower than 54. The tutor was free to tutor on what
he felt was more appropriate, after he was given an
informal assessment of the student?s performance on
the pre-test (tutors were not shown pre-tests to avoid
that they?d tutor to the pre-test only). Hence, not
every student was tutored on every topic.
Topic N Session length (minutes)Min Max Total ? ?
Lists 52 3.4 41.4 750.4 14.4 5.8
Stacks 46 0.3 9.4 264.5 5.8 1.8
Trees 53 9.1 40.0 1017.6 19.2 6.6
Sessions 54 12.8 61.1 2032.5 37.6 6.1
Table 1: CS Tutoring Corpus - Descriptives
Each tutoring session was videotaped. The cam-
era was pointing at the sheets of paper on which tu-
tors and students were writing during the session.
The videos were all transcribed. The transcripts
were produced according to the rules and conven-
tions described in the transcription manual of the
CHILDES project (MacWhinney, 2000). Dialogue
excerpts included in this paper show some of the
transcription conventions. For example, ?+...?
denotes trailing, ?xxx? unintelligible speech and
?#? a short pause (see Figure 2). The CHILDES
transcription manual also provides directions on ut-
terance segmentation.
An additional group of 53 students (control
group) took the pre- and post-tests, but instead of
participating in a tutoring session they attended a
40 minute lecture about an unrelated CS topic. The
rationale for such a control condition was to assess
66
LOW: what?s the if? [Prompt]
LOW: well of course, don?t do this if t two is null so if t
two isn?t null we can do that and xxx properly # thinking
I put it in here. [DPI]
LOW: or else if t two is null that?s telling us that this is
the +. . . [Prompt,FB]
Figure 2: {Prompt,DPI,FB} sequence excerpt
whether by simply taking the pre-test students would
learn about data-structures, and hence, to tease out
whether any learning we would see in the tutored
conditions would be indeed due to tutoring.
The learning gain, expressed as the difference
between post-score and pre-score, of students that
received tutoring was significantly higher than the
learning gain of the students in the control group, for
all the topics. This was showed by ANOVA between
the aggregated group of tutored students and the
control group, and was significant at the p < 0.01
for each topic. There was no significant difference
between the two tutored conditions in terms of learn-
ing gain. The fact that students did not learn more
with the experienced tutor was an important finding
that led us to question the approach of comparing
and contrasting more and less experienced tutors.
Please refer to (Di Eugenio et al, 2009) for further
descriptive measurements of the corpus.
2.2 Dialogue Act Annotation
Many theories have been proposed as concerns DAs,
and there are many plausible inventories of DAs, in-
cluding for tutorial dialogue (Evens and Michael,
2006; Litman and Forbes-Riley, 2006; Boyer et al,
2010). We start from a minimalist point of view,
postulating that, according to current theories of
skill acquisition (Anderson, 1986; Sun et al, 2005;
Ohlsson, 2008), at least the following types of tuto-
rial intervention can be explained in terms of why
and how they might support learning:
1. A tutor can tell the student how to perform the
task.
2. A tutor can state declarative information about
the domain.
3. A tutor can provide feedback:
(a) positive, to confirm that a correct but tentative
step is in fact correct;
(b) negative, to help a student detect and correct an
error.
We first read through the entire corpus and exam-
ined it for impressions and trends, as suggested by
(Chi, 1997). Our informal assessment convinced us
that our minimalist set of tutoring moves was an ap-
propriate starting point. For example, contrary to
much that has been written about an idealized so-
cratic type of tutoring where students build knowl-
edge by themselves (Chi et al, 1994), our tutors
are rather directive in style, namely, they do a lot
of telling and stating. Indeed our tutors talk a lot,
to the tune of producing 93.5% of the total words!
We translated the four types above into the follow-
ing DAs: Direct Procedural Instruction (DPI), Di-
rect Declarative Instruction (DDI), Positive Feed-
back (+FB), and Negative Feedback (-FB). Besides
those 4 categories, we additionally annotated the
corpus for Prompt (PT), since our tutors did explic-
itly invite students to be active in the interaction.
We also annotated for Student Initiative (SI), to cap-
ture active participation on the part of the student?s.
SI occurs when the student proactively produces a
meaningful utterance, by providing unsolicited ex-
planation (see Figures 6 and 4), or by asking ques-
tions. As we had expected, SIs are not as frequent as
other moves (see below). However, this is precisely
the kind of move that a regression analysis would
tease out from others, if it correlates with learning,
even if it occurs relatively infrequently. This indeed
happens in two models, see Table 8.
Direct Procedural Instruction(DPI) occurs when
the tutor directly tells the student what task to per-
form. More specifically:
? Utterances containing correct steps that lead to
the solution of a problem, e.g. see Figure 1.
? Utterances containing high-level steps or sub-
goals (it wants us to put the new node that con-
tains G in it, after the node that contains B).
? Utterances containing tactics and strategies (so
with these kinds of problems, the first thing I
have to say is always draw pictures).
? Utterances where the tutor talked in the first-
person but in reality the tutor instructed the stu-
dent on what to do (So I?m pushing this value
onto a stack. So I?m pushing G back on).
Direct Declarative Instruction (DDI) occurred
when the tutor provided facts about the domain or
67
a specific problem. The key to determine if an ut-
terance is DDI is that the tutor is telling the student
something that he or she ostensibly does not already
know. Common sense knowledge is not DDI ( ten
is less than eleven ). Utterances annotated as DDI
include:
? Providing general knowledge about data struc-
tures (the standard format is right child is al-
ways greater than the parent, left child is al-
ways less than the parent).
? Telling the student information about a specific
problem (this is not a binary search tree).
? Conveying the results of a given action (so now
since we?ve eliminated nine, it?s gone).
? Describing pictures of data structures (and then
there is a link to the next node).
Prompts (PT) occur when the tutor attempts to
elicit a meaningful contribution from the student.
We code for six types of tutor prompts, including:
? Specific prompt: An attempt to get a specific
response from the student (that?s not b so what
do we want to do?).
? Diagnosing: The tutor attempts to determine
the student?s knowledge state (why did you put
a D there?).
? Confirm-OK: The tutor attempts to determine if
the student understood or if the student is pay-
ing attention (okay, got that idea?).
? Fill-in-the-blank: The tutor does not complete
an utterance thereby inviting the student to
complete the utterance, e.g. see Figure 2.
Up to now we have discussed annotations for ut-
terances that do not explicitly address what the stu-
dent has said or done. However, many tutoring
moves concern providing feedback to the student.
Indeed as already known but not often acted upon in
ITS interfaces, tutors do not just point out mistakes,
but also confirm that the student is making correct
steps. While the DAs discussed so far label single
utterances, our positive and negative feedback (+FB
and -FB) annotations comprise a sequence of con-
secutive utterances, that starts where the tutor starts
providing feedback. We opted for a sequence of ut-
terances rather than for labeling one single utterance
because we found it very difficult to pick one single
utterance as the one providing feedback, when the
tutor may include e.g. an explanation that we con-
sider to be part of feedback. Positive feedback oc-
curs when the student says or does something cor-
rect, either spontaneously or after being prompted
by the tutor. The tutor acknowledges the correctness
of the student?s utterance, and possibly elaborates on
it with further explanation. Negative feedback oc-
curs when the student says or does something incor-
rect, either spontaneously or after being prompted
by the tutor. The tutor reacts to the mistake and pos-
sibly provides some form of explanation.
After developing a first version of the coding
manual, we refined it iteratively. During each itera-
tion, two human annotators independently annotated
several dialogues for one DA at a time, compared
outcomes, discussed disagreements, and fine-tuned
the scheme accordingly. This process was repeated
until a sufficiently high inter-coder agreement was
reached. The Kappa values we obtained in the fi-
nal iteration of this process are listed in Table 2
(Di Eugenio and Glass, 2004; Artstein and Poesio,
2008). In Table 2, the ?Double Coded*? column
refers to the sessions that we double coded to cal-
culate the inter-coder agreement. This number does
not include the sessions which were double coded
when coders were developing the coding manual.
The numbers of double-coded sessions differ by DA
since it depends on the frequency on the particular
DA (recall that we coded for one DA at a time).
For example, since Student Initiatives (SI) are not as
frequent, we needed to double code more sessions
to find a number of SI?s high enough to compute a
meaningful Kappa (in our whole corpus, there are
1157 SIs but e.g. 4957 Prompts).
Category Double Coded* Kappa
DPI 10 .7133
Feedback 5 .6747
DDI 10 .8018
SI 14 .8686
Prompt 8 .9490
Table 2: Inter-Coder Agreement in Corpus
The remainder of the corpus was then indepen-
dently annotated by the two annotators. For our
final corpus, for the double coded sessions we did
not come to a consensus label when disagreements
arose; rather, we set up a priority order based on
68
topic and coder (e.g., during development of the
coding scheme, when coders came to consensus
coding, which coder?s interpretation was chosen
more often), and we chose the annotation by a cer-
tain coder based on that order.
As a final important note, given our coding
scheme some utterances have more than one label
(see Figures 2 and 4), whereas others are not la-
belled at all. Specifically, most student utterances,
and some tutor utterances, are not labelled (see Fig-
ures 1 and 4).
3 Method
3.1 Linear Regression Models
In this work, we adopt a multiple regression model,
because it can tell us how much variation in learning
outcomes is explained by the variation of individual
features in the data. The features we use include pre-
test score, the length of the tutoring sessions, and
DAs, both the single DAs we annotated for and DA
n-grams, i.e. DA sequences of length n. Pre-test
score is always included since the effect of previ-
ous knowledge on learning is well established, and
confirmed in our data (see all Models 1 in Table 4);
indeed multiple linear regression allows us to factor
out the effect of previous knowledge on learning, by
quantifying the predictive power of features that are
added beyond pre-test score.
3.2 n-gram Dialogue Act Model
n-grams (sequences of n units, such as words, POS
tags, dialogue acts) have been used to derive lan-
guage models in computational linguistics for a long
time, and have proven effective in tasks like part-of-
speech tagging, spell checking.
Our innovation with regard to using DA n-grams
is to allow gaps in the sequence. This allows us
to extract the sequences that are really effective,
and to eliminate noise. Note that from the point
of view of an effective sequence, noise is anything
that does not contribute to the sequence. For ex-
ample, a tutor?s turn may be interrupted by a stu-
dent?s acknowledgments, such as ?OK? or ?Uh-hah?
(see Figure 1). Whereas these acknowledgments
perform fundamental functions in conversation such
as grounding (Clark, 1992), they may not directly
correlate with learning (a hypothesis to test). If we
counted them in the sequence, they would contribute
two utterances, transforming a 3 DA sequence into a
5 DA sequence. As well known, the higher the n, the
sparser the data becomes, i.e., the fewer sequences
of length n we find, making the task of discover-
ing significant correlations all the harder. Note that
some of the bigrams in (Litman and Forbes-Riley,
2006) could be considered to have gaps, since they
pair one student move (say SI) with each tutor move
contained in the next tutor turn (eg, in our Figure 6
they would derive two bigrams [SI, FB], and [SI,
Prompt]). However, this does not result in a system-
atic exploration of all possible sequences of a certain
length n, with all possible gaps of length up to m, as
we do here.
The tool that allows us to leave gaps in sequences
is part of Apache Lucene,1 an open source full text
search library. It provides strong capabilities to
match and count efficiently. Our counting method
is based on two important features provided by
Lucene, that we already used in other work (Chen
and Di Eugenio, 2010) to detect uncertainty in dif-
ferent types of corpora.
? Synonym matching: We can specify several
different tokens at the same position in a field
of a document, so that each of them can be used
to match the query.
? Precise gaps: With Lucene, we can precisely
specify the gap between the matched query and
the indexed documents (sequences of DAs in
our case) using a special type of query called
SpanNearQuery.
To take advantage of Lucene as described above,
we use the following algorithm to index our corpus.
1. For each Tutor-Topic session, we generate n-
gram utterance sequences ? note that these are
sequences of utterances at this point, not of
DAs.
2. We prune utterance sequences where either 0
or only 1 utterance is annotated with a DA, be-
cause we are mining sequences with at least 2
DAs. Recall that given our annotation, some ut-
terances are not annotated (see e.g. Figure 1).
3. After pruning, for each utterance sequence, we
generate a Lucene document: each DA label on
an utterance will be treated as a token, multiple
1http://lucene.apache.org/
69
labels on the same utterance will be treated as
?synonyms?.
By indexing annotations as just described, we
avoid the problem of generating too many combina-
tions of labels. After indexing, we can use SpanN-
earQuery to query the index. SpanNearQuery allows
us to specify the position distance allowed between
each term in the query.
Figure 3 is the field of the generated Lucene doc-
ument corresponding to the utterance sequences in
Figure 4. We can see that each utterance of the tu-
tor is tagged with 2 DAs. Those 2 DAs produce 2
tokens, which are put into the same position. The
tokens in the same position act as synonyms to each
other during the query.
Figure 3: Lucene Document Example for DAs
258: okay.
JAC: its right child is eight. [DDI, FB]
258: uh no it has to be greater than ten. [SI]
JAC: right so it?s not a binary search tree # it?s not a b s t,
right? [DDI,Prompt]
Figure 4: {FB, SI, DDI} is most effective in Trees
4 Experiments and Results
Here we build on our previous results reported
in (Di Eugenio et al, 2009). There we had shown
that, for lists and stacks, models that include positive
and negative feedback are significant and explain
more of the variance with respect to models that only
include pre-test score, or include pre-test score and
session length. Table 4 still follows the same ap-
proach, but adds to the regression models the addi-
tional DAs, DPI, DDI, Prompt and SI that had not
been included in that earlier work. The column M
refers to three types of models, Model 1 only in-
cludes Pre-test, Model 2 adds session length to Pre-
test, and Model 3 adds to Pre-test all the DAs. As ev-
idenced by the table, only DPI provides a marginally
significant contribution, and only for lists. Note that
length is not included in Model 3?s. We did run all
the equivalent models to Model 3?s including length.
The R2?s stay the same (literally, to the second dec-
imal digit), or minimally decrease. However, in all
these Model 3+?s that include length no DA is sig-
nificant, hence we consider them as less explana-
tory than the Model 3?s in Table 4: finding that a
longer dialogue positively affects learning does not
tell us what happens during that dialogue which is
conducive to learning.
Note that the ? weights on the pre-test are al-
ways negative in every model, namely, students with
higher pre-test scores learn less than students with
lower pre-test scores. This is an example of the well-
known ceiling effect: students with more previous
knowledge have less learning opportunity. Also no-
ticeable is that the R2 for the Trees models are much
higher than for Lists and Stacks, and that for Trees
no DA is significant (although there will be signifi-
cant trigram models that involve DAs for Trees). We
have observed that Lists are in general more diffi-
cult than Stacks and Trees (well, at least than binary
search trees) for students.
Topic Pre-Test ? Gain ?
Lists .40 .27 .14 .25
Stacks .29 .30 .31 .24
Trees .50 .26 .30 .24
Table 3: Learning gains and t-test statistics
Indeed Table 3 shows that in the CS-tutoring cor-
pus the average learning gain is only .14 for Lists,
but .31 for Stacks and .30 for Trees; whereas stu-
dents have the lowest pre-test score on Stacks, and
hence they have more opportunities for learning,
they learn as much for Trees, but not for Lists.
We now examine whether DA sequences help us
explain why student learn. We have run 24 sets of
linear regression experiments, which are grouped as
the following 6 types of models.
? With DA bigrams (DA sequences of length 2):
? Gain ? DA Bigram
? Gain ? DA Bigram + Pre-test Score
? Gain ? DA Bigram + Pre-test Score +
Session Length
? With DA trigrams (DA sequences of length 3):
? Gain ? DA Trigram
? Gain ? DA Trigram + Pre-test Score
? Gain ? DA Trigram + Pre-test Score +
Session Length
For each type of model:
70
Topic M Predictor ? R2 P
Lists
1 Pre-test ?.47 .20 < .001
2
Pre-test ?.43 .29 < .001Length .01 < .001
3
Pre-test ?.500
.377
< .001
+FB .020 < .01
-FB .039 ns
DPI .004 < .1
DDI .001 ns
SI .005 ns
Prompt .001 ns
Stacks
1 Pre-test ?.46 .296 < .001
2 Pre-test ?.46 .280 < .001Length ?.002 ns
3
Pre-test ?.465
.275
< .001
+FB ?.017 < .01
-FB ?.045 ns
DPI .007 ns
DDI .001 ns
SI .008 ns
Prompt ?.006 ns
Trees
1 Pre-test ?.739 .676 < .001
2 Pre-test ?.733 .670 < .001Length .001 ns
3
Pre-test ?.712
.667
< .001
+FB ?.002 ns
-FB ?.018 ns
DPI ?.001 ns
DDI ?.001 ns
SI ?.001 ns
Prompt ?.001 ns
All
1 Pre-test ?.505 .305 < .001
2 Pre-test ?.528 .338 < .001Length .06 < .001
3
Pre-test ?.573
.382
< .001
+FB .009 < .001
-FB ?.024 ns
DPI .001 ns
DDI .001 ns
SI .001 ns
Prompt .001 ns
Table 4: Linear Regression ? Human Tutoring
1. We index the corpus according to the length of
the sequence (2 or 3) using the method we in-
troduced in section 3.2.
2. We generate all the permutations of all the DAs
we annotated for within the specified length;
count the number of occurrences of each per-
mutation using Lucene?s SpanNearQuery al-
lowing for gaps of specified length. Gaps can
span from 0 to 3 utterances; for example, the
excerpt in Figure 1 will be counted as a {DPI,
DDI} bigram with a gap of length 1. Gaps can
be discontinuous.
3. We run linear regressions2 on the six types of
models listed above, generating actual models
by replacing a generic DA bi- or tri-gram with
each possible DA sequence we generated in
step 2.
4. We output those regression results, in which the
whole model and every predictor are at least
marginally significant (p < 0.1).
The number of generated significant models is
shown in Figure 5. In the legend of the Figure,
B stands for Bigram DA sequence, T stands for
Trigram DA sequence, L stands for session Length,
P stands for Pre-test score. Not surprisingly, Fig-
ure 5 shows that, as the allowed gap increases in
length, the number of significant models increases
too, which give us more models to analyze.
0
10
20
30
40
50
60
Gap Allowed
N
um
be
r o
f S
ig
ni
fic
an
t M
od
el
s
0 1 2 3
?
?
?
??
?
?
?
?
?
Predictors
T
T+P
T+P+L
B
B+P
B+P+L
Figure 5: Gaps Allowed vs. Significant Models
Figure 5 shows that there are a high number of
significant models. In what follows we will present
first of all those that improve on the models that
do not use sequences of DAs, as presented in Ta-
ble 4. Improvement here means not only that the
R2 is higher, but that the model is more appropriate
as an approximation of a tutor strategy, and hence,
constitutes a better guideline for an ITS. For exam-
ple, take model 3 for Lists in Table 4. It tells us
that positive feedback (+FB) and direct procedural
instruction (DPI) positively correlate with learning
2We used rJava, http://www.rforge.net/rJava/
71
gains. However, this obviously cannot mean that our
ITS should only produce +FB and DPI. The ITS is
interacting with the student, and it needs to tune its
strategies according to what happens in the interac-
tion; model 3 doesn?t even tell us if +FB and DPI
should be used together or independently. Models
that include sequences of DAs will be more useful
for the design of an ITS, since they point out what
sequences of DAs the ITS may use, even if they still
don?t answer the question, when should the ITS en-
gage in a particular sequence ? we have addressed
related issues in our work on iList (Fossati et al,
2009; Fossati et al, 2010).
4.1 Bigram Models
{DPI, Feedback} Model Indeed the first signifi-
cant models that include a DA bigram include the
{DPI, Feedback} DA sequence. Note that we distin-
guish between models that employ Feedback (FB)
without distinguishing between positive and nega-
tive feedback; and models where the type of feed-
back is taken into account (+FB, -FB). Table 5 shows
that for Lists, a sequence that includes DPI followed
by any type of feedback (Feedback, +FB, -FB) pro-
duces significant models when the model includes
pre-test. Table 5 and all tables that follow include
the column Gap that indicates the length of the gap
within the DA sequence with which that model was
obtained. When, as in Table 5, multiple numbers
appear in the Gap column, this indicates that the
model is significant with all those gap settings. We
only show the ?, R2 and P values for the gap length
which generates the highest R2 for a model, and the
corresponding gap length is in bold font: for exam-
ple, the first model for Lists in Table 5 is obtained
with a gap length = 2. For Lists, these models are not
as predictive as Model 3 in Table 4, however we be-
lieve they are more useful from an ITS design point
of view: they tell us that when the tutor gives direct
instruction on how to solve the problem, within a
short span of dialogue the tutor produces feedback,
since (presumably) the student will have tried to ap-
ply that DPI. For Stacks, a {DPI, -FB} model (with-
out taking pre-test into account) significantly corre-
lates (p < 0.05) with learning gain, and marginally
significantly correlates with learning gain when the
model also includes pre-test score. This latter model
is actually more predictive than Model 3 for Stacks
in Table 4 that includes +FB but not DPI. We can
see the ? weight is negative for the sequence {DPI,
-FB} in the Stacks model. No models including the
bigram {DPI, -FB} are significant for Trees.
Topic Predictor ? R2 P Gap
Lists
DPI, -FB .039 .235 <.001 2, 3Pre-test ?.513 < .001
DPI, +FB .019
.339
<.001
0, 1, 2, 3Pre-test ?.492 < .001
Length .011 < 0.05
DPI, FB .016
.333
<.05
0, 1, 2, 3Pre-test ?.489 < .001
Length .011 < 0.05
Stacks
DPI, -FB ?.290 .136 <.05 0, 1, 2, 3
DPI, -FB ?.187 .342 <.1 0, 1, 2, 3Pre-test ?.401 < .001
Table 5: DPI, Feedback Model
{FB, DDI} Model A natural question arises:
since Feedback following DPI results in significant
models, are there any significant models which in-
clude sequences whose first component is a Feed-
back move? We found only two that are signif-
icant, when Feedback is followed by DDI (Direct
Declarative Instruction). Note that here we are not
distinguishing between negative and positive feed-
back. Those models are shown in Table 6. The
Lists model is not more effective than the original
Model 3 for Lists in Table 4, but the model for Trees
is slightly more explanatory than the best model
for Trees in that same table, and includes a bigram
model, whereas in Table 4, only pre-test is signifi-
cant for Trees.
Topic Predictor ? R2 P Gap
Lists
FB, DDI .1478
.321
<.1
1Pre-test ?.470 < .001
Length .011 < .05
Trees FB, DDI .0709 .6953 <.05 0Pre-test ?.7409 < .001
Table 6: {FB, DDI} Model
4.2 Trigram Models
{DPI, FB, DDI} Model Given our significant bi-
gram models for DPI followed by FB, and FB fol-
lowed by DDI, it is natural to ask whether the com-
bined trigram model {DPI, FB, DDI} results in a
significant model. It does for the topic List, as
shown in table 7, however again the R2 is lower than
72
that of Model 3 in Table 4. This suggests that an ef-
fective tutoring sequence is to provide instruction on
how to solve the problem (DPI), then Feedback on
what the student does, and finally some declarative
instruction (DDI).
Topic Predictor ? R2 P Gap
Lists
DPI, FB, DDI .156
.371
<.01
1Pre-test ?.528 < .001
Length .012 < .05
Table 7: {DPI, FB, DDI} Model
More effective trigram models include Prompt
and SI. Up to now, only one model including se-
quences of DAs was superior to the simpler models
in Table 4. Interestingly, different trigrams that still
include some form of Feedback, DPI or DDI, and
then either Prompt or SI (Student Initiative) result in
models that exhibit slightly higher R2; additionally
in all these models the trigram predictor is highly
significant. These models are listed in table 8 (note
that the two Trees models differ because in one FB is
generic Feedback, irregardless of orientation, in the
other it?s +FB, i.e., positive feedback). In detail, im-
provements in R2 are 0.0382 in topic Lists, 0.12 in
topic Stacks and 0.0563 in topic Trees. The highest
improvement is in Stacks.
Topic Predictor ? R2 P Gap
Lists
PT,DPI,FB .266
.415
<.01
0Pre-test ?.463 < .001
Length .011 < .05
Stacks DDI,FB,PT ?.06 .416 <.01 1Pre-test ?.52 < .001
Trees +FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Trees FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Table 8: Highest R2 Models
It is interesting to note that the model for Lists add
Prompt at the beginning to a bigram that had already
been found to contribute to a significant model. For
Trees, likewise, we add another DA to the bigram
{FB,DDI} that had been found to be significant; this
time, it is Student Initiative (SI) and it occurs in
the middle. This indicates that, after the tutor pro-
vides feedback, the student takes the initiative, and
the tutor responds with one piece of information the
student didn?t know (DDI). Of course, the role of
Prompts and SI is not surprising, although interest-
ingly they are significant only in association with
certain tutor moves. It is well known that students
learn more when they build knowledge by them-
selves, either by taking the initiative (SI), or after
the tutor prompts them to do so (Chi et al, 1994;
Chi et al, 2001).
LOW: it?s backwards # it?s got four elements, but they
are backwards. [DDI]
234: so we have do it again. [SI]
LOW: so do it again. [FB]
LOW: do what again? [Prompt]
Figure 6: {DDI, FB, PT} is most effective in Stacks
4.3 Other models
We found other significant models, specifically,
{DDI,DPI} for all three topics, and {-FB,SI} for
Lists. However, their R2 are very low, and much
lower than any of the other models presented so
far. Besides models that include only one DA se-
quence and pre-test score to predict learning gain,
we also ran experiments to see if adding multiple
DA sequences to pre-test score will lead to signifi-
cant models ? namely, we experimented with mod-
els which include two sequences as predictors, say,
the two bigrams {-FB,SI} and {FB,DDI}. However,
no significant models were found.
5 Conclusions
In this paper, we explored effective tutoring strate-
gies expressed as sequence of DAs. We first pre-
sented the CS-Tutoring corpus. By relaxing the DA
n-gram definition via the fuzzy matching provided
by Apache Lucene, we managed to discover several
DA sequences that significantly correlate with learn-
ing gain. Further, we discovered models with higher
R2 than models which include only one single DA,
which are also more informative from the point of
view of the design of interfaces to ITSs.
6 Acknowledgments
This work was mainly supported by ONR (N00014-
00-1-0640), and by the UIC Graduate College
(2008/2009 Dean?s Scholar Award). Partial sup-
port is also provided by NSF (ALT-0536968, IIS-
0905593).
73
References
John R. Anderson. 1986. Knowledge compilation: The
general learning mechanism. Machine learning: An
artificial intelligence approach, 2:289?310.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596. Survey Article.
Kristy Elizabeth Boyer, Robert Phillips, Amy Ingram,
Eun Young Ha, Michael Wallis, Mladen Vouk, and
James Lester. 2010. Characterizing the effectiveness
of tutorial dialogue with Hidden Markov Models. In
Intelligent Tutoring Systems, pages 55?64. Springer.
Whitney L. Cade, Jessica L. Copeland, Natalie K. Per-
son, and Sidney K. D?Mello. 2008. Dialogue modes
in expert tutoring. In Intelligent Tutoring Systems,
volume 5091 of Lecture Notes in Computer Science,
pages 470?479. Springer Berlin / Heidelberg.
Lin Chen and Barbara Di Eugenio. 2010. A lucene
and maximum entropy model based hedge detection
system. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 114?119, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Michelene T. H. Chi, Stephanie A. Siler, Takashi Ya-
mauchi, and Robert G. Hausmann. 2001. Learning
from human tutoring. Cognitive Science, 25:471?533.
Min Chi, Kurt VanLehn, and Diane Litman. 2010. The
more the merrier? Examining three interaction hy-
potheses. In Proceedings of the 32nd Annual Confer-
ence of the Cognitive Science Society (CogSci2010),
Portland,OR.
Michelene T.H. Chi. 1997. Quantifying qualitative anal-
yses of verbal data: A practical guide. Journal of the
Learning Sciences, 6(3):271?315.
Herbert H. Clark. 1992. Arenas of Language Use. The
University of Chicago Press, Chicago, IL.
Barbara Di Eugenio and Michael Glass. 2004. The
Kappa statistic: a second look. Computational Lin-
guistics, 30(1):95?101. Squib.
Barbara Di Eugenio, Trina C. Kershaw, Xin Lu, Andrew
Corrigan-Halpern, and Stellan Ohlsson. 2006. To-
ward a computational model of expert tutoring: a first
report. In FLAIRS06, the 19th International Florida
AI Research Symposium, Melbourne Beach, FL.
Barbara Di Eugenio, Davide Fossati, Stellan Ohlsson,
and David Cosejo. 2009. Towards explaining effec-
tive tutorial dialogues. In Annual Meeting of the Cog-
nitive Science Society, pages 1430?1435, Amsterdam,
July.
Martha W. Evens and Joel A. Michael. 2006. One-on-
one Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Davide Fossati, Barbara Di Eugenio, Christopher Brown,
Stellan Ohlsson, David Cosejo, and Lin Chen. 2009.
Supporting Computer Science curriculum: Exploring
and learning linked lists with iList. IEEE Transac-
tions on Learning Technologies, Special Issue on Real-
World Applications of Intelligent Tutoring Systems,
2(2):107?120, April-June.
Davide Fossati, Barbara Di Eugenio, Stellan Ohlsson,
Christopher Brown, and Lin Chen. 2010. Generat-
ing proactive feedback to help students stay on track.
In ITS 2010, 10th International Conference on Intelli-
gent Tutoring Systems. Poster.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional systems.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Arthur C. Graesser, Natalie K. Person, and Joseph P.
Magliano. 1995. Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive
Psychology, 9:495?522.
Arthur C. Graesser, Shulan Lu, George Tanner Jack-
son, Heather Hite Mitchell, Mathew Ventura, Andrew
Olney, and Max M. Louwerse. 2004. AutoTutor:
A tutor with dialogue in natural language. Behav-
ioral Research Methods, Instruments, and Computers,
36:180?193.
Mark R. Lepper, Michael F. Drake, and Teresa
O?Donnell-Johnson. 1997. Scaffolding techniques of
expert human tutors. In K. Hogan and M. Pressley, ed-
itors, Scaffolding student learning: Instructional ap-
proaches and issues. Cambridge, MA: Brookline.
Diane Litman and Kate Forbes-Riley. 2006. Correla-
tions between dialogue acts and learning in spoken
tutoring dialogues. Natural Language Engineering,
12(02):161?176.
Brian MacWhinney. 2000. The Childes Project: Tools
for Analyzing Talk: Transcription format and pro-
grams, volume 1. Psychology Press, 3 edition.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian
Varges, and Claus Zinn. 2004. Generating Tutorial
Feedback with Affect. In FLAIRS04, Proceedings of
the Seventeenth International Florida Artificial Intel-
ligence Research Society Conference.
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Da-
vide Fossati, Xin Lu, and Trina C. Kershaw. 2007.
Beyond the code-and-count analysis of tutoring dia-
logues. In Proceedings of the 13th International Con-
ference on Artificial Intelligence in Education, pages
349?356, Los Angeles, CA, July. IOS Press.
Stellan Ohlsson. 2008. Computational models of skill
acquisition. The Cambridge handbook of computa-
tional psychology, pages 359?395.
74
Ron Sun, Paul Slusarz, and Chris Terry. 2005. The Inter-
action of the Explicit and the Implicit in Skill Learn-
ing: A Dual-Process Approach. Psychological Re-
view, 112:159?192.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,
Pamela W. Jordan, Andrew Olney, and Carolyn P.
Rose?. 2007. When are tutorial dialogues more effec-
tive than reading? Cognitive Science, 31(1):3?62.
75
