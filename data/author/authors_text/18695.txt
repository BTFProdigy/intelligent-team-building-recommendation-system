Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 155?164,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Unsupervised Solution Post Identification from Discussion Forums
Deepak P
IBM Research - India
Bangalore, India
deepak.s.p@in.ibm.com
Karthik Visweswariah
IBM Research - India
Bangalore, India
v-karthik@in.ibm.com
Abstract
Discussion forums have evolved into a de-
pendable source of knowledge to solve
common problems. However, only a mi-
nority of the posts in discussion forums
are solution posts. Identifying solution
posts from discussion forums, hence, is an
important research problem. In this pa-
per, we present a technique for unsuper-
vised solution post identification leverag-
ing a so far unexplored textual feature, that
of lexical correlations between problems
and solutions. We use translation mod-
els and language models to exploit lex-
ical correlations and solution post char-
acter respectively. Our technique is de-
signed to not rely much on structural fea-
tures such as post metadata since such
features are often not uniformly available
across forums. Our clustering-based itera-
tive solution identification approach based
on the EM-formulation performs favor-
ably in an empirical evaluation, beating
the only unsupervised solution identifica-
tion technique from literature by a very
large margin. We also show that our unsu-
pervised technique is competitive against
methods that require supervision, outper-
forming one such technique comfortably.
1 Introduction
Discussion forums have become a popular knowl-
edge source for finding solutions to common prob-
lems. StackOverflow
1
, a popular discussion forum
for programmers is among the top-100 most vis-
ited sites globally
2
. Now, there are discussion fo-
rums for almost every major product ranging from
1
http://www.stackoverflow.com
2
http://www.alexa.com/siteinfo/stackoverflow.com
automobiles
3
to gadgets such as those of Mac
4
or
Samsung
5
. These typically start with a registered
user posting a question/problem
6
to which other
users respond. Typical response posts include so-
lutions or clarification requests, whereas feedback
posts form another major category of forum posts.
As is the case with any community of humans,
discussion forums have their share of inflamma-
tory remarks too. Mining problem-solution pairs
from discussion forums has attracted much atten-
tion from the scholarly community in the recent
past. Since the first post most usually contains
the problem description, identifying its solutions
from among the other posts in the thread has been
the focus of many recent efforts (e.g., (Gandhe et
al., 2012; Hong and Davison, 2009)). Extract-
ing problem-solution pairs from forums enables
the usage of such knowledge in knowledge reuse
frameworks such as case-based reasoning (Kolod-
ner, 1992) that use problem-solution pairs as raw
material. In this paper, we address the problem
of unsupervised solution post identification
7
from
discussion forums.
Among the first papers to address the solution
identification problem was the unsupervised ap-
proach proposed by (Cong et al, 2008). It em-
ploys a graph propagation method that prioritizes
posts that are (a) more similar to the problem post,
(b) more similar to other posts, and (c) authored
by a more authoritative user, to be labeled as so-
lution posts. Though seen to be effective in iden-
tifying solutions from travel forums, the first two
assumptions, (a) and (b), were seen to be not very
3
http://www.cadillacforums.com/
4
https://discussions.apple.com/
5
http://www.galaxyforums.net/
6
We use problem and question, as well as solution and
answer interchangeably in this paper.
7
This problem has been referred to as answer extraction
by some papers earlier. However, we use solution identifica-
tion to refer to the problem since answer and extraction have
other connotations in the Question-Answering and Informa-
tion Extraction communities respectively.
155
reliable in solution identification in other kinds of
discussion boards. (Catherine et al, 2012) reports
a study that illustrates that non-solution posts are,
on an average, as similar to the problem as solution
posts in technical forums. The second assump-
tion (i.e., (b) above) was also not seen to be use-
ful in discussion forums since posts that are highly
similar to other posts were seen to be complaints,
repetitive content being more pervasive among
complaint posts than solutions (Catherine et al,
2013). Having exhausted the two obvious textual
features for solution identification, subsequent ap-
proaches have largely used the presence of lexi-
cal cues signifying solution-like narrative (e.g., in-
structive narratives such as ?check the router for
any connection issues?) as the primary content-
based feature for solution identification.
All solution identification approaches
since (Cong et al, 2008) have used super-
vised methods that require training data in the
form of labeled solution and non-solution posts.
The techniques differ from one another mostly
in the non-textual features that are employed in
representing posts. A variety of high precision as-
sumptions such as solution post typically follows
a problem post (Qu and Liu, 2011), solution posts
are likely to be within the first few posts, solution
posts are likely to have been acknowledged by
the problem post author (Catherine et al, 2012),
users with high authoritativeness are likely to
author solutions (Hong and Davison, 2009), and
so on have been seen to be useful in solution
identification. Being supervised methods, the
above assumptions are implicitly factored in
by including the appropriate feature (e.g., post
position in thread) in the feature space so that the
learner may learn the correlation (e.g., solution
posts typically are among the first few posts)
using the training data. Though such assumptions
on structural features, if generic enough, may be
built into unsupervised techniques to aid solution
identification, the variation in availability of
such features across forums limits the usage of
models that rely heavily on structural features.
For example, some forums employ chronological
order based flattening of threads (Seo et al, 2009)
making reply-to information unavailable; models
that harness reply-to features would then have
limited utility on identifying solutions within
such flattened threads. On medical forums,
privacy considerations may force forum data to
be dumped without author information, making a
host of author-id based features unavailable. On
datasets that contain data from across forums,
the model may have to be aware of the absence
of certain features in subsets of the data, or be
modeled using features that are available on all
threads.
Our Contribution: We propose an unsuper-
vised method for solution identification. The cor-
nerstone of our technique is the usage of a hith-
erto unexplored textual feature, lexical correla-
tions between problems and solutions, that is ex-
ploited along with language model based charac-
terization of solution posts. We model the lexical
correlation and solution post character using reg-
ularized translation models and unigram language
models respectively. To keep our technique appli-
cable across a large variety of forums with vary-
ing availability of non-textual features, we design
it to be able to work with minimal availability of
non-textual features. In particular, we show that
by using post position as the only non-textual fea-
ture, we are able to achieve accuracies compara-
ble to supervision-based approaches that use many
structural features (Catherine et al, 2013).
2 Related Work
In this section, we provide a brief overview of pre-
vious work related to our problem. Though most
of the answer/solution identification approaches
proposed so far in literature are supervised meth-
ods that require a labeled training corpus, there are
a few that require limited or no supervision. Ta-
ble 1 provides an overview of some of the more
recent solution identification techniques from lit-
erature, with a focus on some features that we wish
to highlight. The common observation that most
problem-solving discussion threads have a prob-
lem description in the first post has been explic-
itly factored into many techniques; knowing the
problem/question is important for solution iden-
tification since author relations between problem
and other posts provide valuable cues for solution
identification. Most techniques use a variety of
such features as noted in Section 1. SVMs have
been the most popular method for supervised and
semi-supervised learning for the task of solution
identification.
Of particular interest to us are approaches that
use limited or no supervision, since we focus on
unsupervised solution identification in this paper.
156
Paper Reference Supervision Assumptions on Features other than Learning
Problem Position Post Content Used Technique
(Qu and Liu, 2011) Supervised First Post likely HMM assumes Naive Bayes
to be problem solution follows problem & HMM
(Ding et al, 2008) Supervised First Post Post Position, Author, CRFs
Context Posts
(Kim et al, 2010) Supervised None Post Position, Author, MaxEnt,
Previous Posts, Profile etc. SVM, CRF
(Hong and Davison, 2009) Supervised First Post Post Position, Author, SVM
Author Authority
(Catherine et al, 2012) Supervised First Post Post Position, Author, Problem SVM
Author?s activities wrt Post
(Catherine et al, 2013) Limited First Post Post Position/Rating, Author, SVMs &
Supervision Author Rating, Post Ack Co-Training
(Cong et al, 2008) Unsupervised None Author, Author Authority, Graph
Relation to Problem Author Propagation
Our Method Unsupervised First Post Post Position Translation
Models & LM
Table 1: Summary of Some Solution Identification Techniquess
The only unsupervised approach for the task, that
from (Cong et al, 2008), uses a graph propaga-
tion method on a graph modeled using posts as
vertices, and relies on the assumptions that posts
that bear high similarity to the problem and other
posts and those authored by authoritative users are
more likely to be solution posts. Some of those as-
sumptions, as mentioned in Section 1, were later
found to be not generalizable to beyond travel fo-
rums. The semi-supervised approach presented
in (Catherine et al, 2013) uses a few labeled
threads to bootstrap SVM based learners which are
then co-trained in an iterative fashion. In addition
to various features explored in literature, they use
acknowledgement modeling so that posts that have
been acknowledged positively may be favored for
being labeled as solutions.
We will use translation and language models
in our method for solution identification. Usage
of translation models for modeling the correlation
between textual problems and solutions have been
explored earlier starting from the answer retrieval
work in (Xue et al, 2008) where new queries were
conceptually expanded using the translation model
to improve retrieval. Translation models were also
seen to be useful in segmenting incident reports
into the problem and solution parts (Deepak et al,
2012); we will use an adaptation of the generative
model presented therein, for our solution extrac-
tion formulation. Entity-level translation models
were recently shown to be useful in modeling cor-
relations in QA archives (Singh, 2012).
3 Problem Definition
Let a thread T from a discussion forum be made
up of t posts. Since we assume, much like
many other earlier papers, that the first post is
the problem post, the task is to identify which
among the remaining t ? 1 posts are solutions.
There could be multiple (most likely, different)
solutions within the same thread. We may now
model the thread T as t ? 1 post pairs, each
pair having the problem post as the first element,
and one of the t ? 1 remaining posts (i.e., re-
ply posts in T ) as the second element. Let C =
{(p
1
, r
1
), (p
2
, r
2
), . . . , (p
n
, r
n
)} be the set of such
problem-reply pairs from across threads in the dis-
cussion forum. We are interested in finding a sub-
set C
?
of C such that most of the pairs in C
?
are
problem-solution pairs, and most of those in C?C
?
are not so. In short, we would like to find problem-
solution pairs from C such that the F-measure
8
for
solution identification is maximized.
4 Our Approach
4.1 The Correlation Assumption
Central to our approach is the assumption of lex-
ical correlation between the problem and solution
8
http://en.wikipedia.org/wiki/F1 score
157
texts. At the word level, this translates to assum-
ing that there exist word pairs such that the pres-
ence of the first word in the problem part pre-
dicts the presence/absence of the second word in
the solution part well. Though not yet harnessed
for solution identification, the correlation assump-
tion is not at all novel. Infact, the assumption
that similar problems have similar solutions (of
which the correlation assumption is an offshoot)
forms the foundation of case-based reasoning sys-
tems (Kolodner, 1992), a kind of knowledge reuse
systems that could be the natural consumers of
problem-solution pairs mined from forums. The
usage of translation models in QA retrieval (Xue et
al., 2008; Singh, 2012) and segmentation (Deepak
et al, 2012) were also motivated by the correlation
assumption. We use an IBM Model 1 translation
model (Brown et al, 1990) in our technique; sim-
plistically, such a model m may be thought of as
a 2-d associative array where the value m[w
1
][w
2
]
is directly related to the probability ofw
1
occuring
in the problem when w
2
occurs in the solution.
4.2 Generative model for Solution Posts
Consider a unigram language model S
S
that mod-
els the lexical characteristics of solution posts, and
a translation model T
S
that models the lexical cor-
relation between problems and solutions. Our gen-
erative model models the reply part of a (p, r) pair
(in which r is a solution) as being generated from
the statistical models in {S
S
, T
S
} as follows.
? For each word w
s
occuring in r,
1. Choose z ? U(0, 1)
2. If z ? ?, Choose w ? Mult(S
S
)
3. Else, Choose w ? Mult(T
p
S
)
where T
p
S
denotes the multionomial distribu-
tion obtained from T
S
conditioned over the words
in the post p; this is obtained by assigning each
candidate solution word w a weight equal to
avg{T
S
[w
?
][w]|w
?
? p}, and normalizing such
weights across all solution words. In short, each
solution word is assumed to be generated from
the language model or the translation model (con-
ditioned on the problem words) with a probabil-
ity of ? and 1 ? ? respectively, thus accounting
for the correlation assumption. The generative
model above is similar to the proposal in (Deepak
et al, 2012), adapted suitably for our scenario. We
model non-solution posts similarly with the sole
difference being that they would be sampled from
the analogous models S
N
and T
N
that characterize
behavior of non-solution posts.
Example: Consider the following illustrative
example of a problem and solution post:
? Problem: I am unable to surf the web on the
BT public wifi.
? Solution: Maybe, you should try disconnect-
ing and rejoining the network.
Of the solution words above, generic words
such as try and should could probably be ex-
plained by (i.e., sampled from) the solution lan-
guage model, whereas disconnect and rejoin could
be correlated well with surf and wifi and hence are
more likely to be supported better by the transla-
tion model.
4.3 Clustering-based Approach
We propose a clustering based approach so as to
cluster each of the (p, r) pairs into either the so-
lution cluster or the non-solution cluster. The ob-
jective function that we seek to maximize is the
following:
?
(p,r)?C
{
F ((p, r),S
S
, T
S
) if label((p,r))=S
F ((p, r),S
N
, T
N
) if label((p,r))=N
(1)
F ((p, r),S, T ) indicates the conformance of
the (p, r) pair (details in Section 4.3.1) with the
generative model that uses the S and T models as
the language and translation models respectively.
The clustering based approach labels each (p, r)
pair as either solution (i.e., S) or non-solution (i.e.,
N ). Since we do not know the models or the la-
belings to start with, we use an iterative approach
modeled on the EM meta-algorithm (Dempster et
al., 1977) involving iterations, each comprising of
an E-step followed by the M-step. For simplicity
and brevity, instead of deriving the EM formula-
tion, we illustrate our approach by making an anal-
ogy with the popular K-Means clustering (Mac-
Queen, 1967) algorithm that also uses the EM for-
mulation and crisp assignments of data points like
we do. K-Means is a clustering algorithm that
clusters objects represented as multi-dimensional
points into k clusters where each cluster is rep-
resented by the centroid of all its members. Each
iteration in K-Means starts off with assigning each
158
In K-Means In Our Approach
Data Multi-dimensional Points (p, r) pairs
Cluster Model Respective Centroid Vector Respective S and T Models for each cluster
Initialization Random Choice of Centroids Models learnt using (p, r) pairs labeled
using the Post Position of r
E-Step label(d) = label((p, r)) = argmax
i
F ((p, r),S
i
, T
i
)
argmin
i
dist(d, centroid
i
) (Sec 4.3.1), and learn solution word
source probabilities (Sec 4.3.2)
M-Step centroid
i
= avg{d|label(d) = i} Re-learn S
S
and T
S
using pairs labeled S
S
N
and T
N
using pairs labeled N (Sec 4.3.3)
Output The clustering of points (p, r) pairs labeled as S
Table 2: Illustrating Our Approach wrt K-Means Clustering
data object to its nearest centroid, followed by re-
computing the centroid vector based on the assign-
ments made. The analogy with K-Means is illus-
trated in Table 2.
Though the analogy in Table 2 serves to provide
a high-level picture of our approach, the details re-
quire further exposition. In short, our approach is
a 2-way clustering algorithm that uses two pairs of
models, [S
S
, T
S
] and [S
N
, T
N
], to model solution
pairs and non-solution pairs respectively. At each
iteration, the post-pairs are labeled as either solu-
tion (S) or non-solution (N ) based on which pair
of models they better conform to. Within the same
iteration, the four models are then re-learnt using
the labels and other side information. At the end
of the iterations, the pairs labeled S are output as
solution pairs. We describe the various details in
separate subsections herein.
4.3.1 E-Step: Estimating Labels
As outlined in Table 2, each (p, r) pair would
be assigned to one of the classes, solution or
non-solution, based on whether it conforms better
with the solution models (i.e., S
S
& T
S
) or non-
solution models (S
N
& T
N
), as determined using
the F ((p, r),S, T ) function, i.e.,
label((p, r)) = argmax
i?{S,N}
F ((p, r),S
i
, T
i
)
F (.) falls out of the generative model:
F ((p, r),S, T ) =
?
w?r
??S[w]+(1??)?T
p
[w]
where S[w] denotes the probability of w from
S and T
p
[w] denotes the probability of w from
the multinomial distribution derived from T con-
ditioned over the words in p, as in Section 4.2.
4.3.2 E-Step: Estimating Reply Word Source
Since the language and translation models operate
at the word level, the objective function entails that
we let the models learn based on their fractional
contribution of the words from the language and
translation models. Thus, we estimate the propor-
tional contribution of each word from the language
and translation models too, in the E-step. The frac-
tional contributions of the word w ? r in the (p, r)
pair labeled as solution (i.e., S) is as follows:
f
(p,r)
S
S
(w) =
S
S
[w]
S
S
[w] + T
p
S
[w]
f
(p,r)
T
S
(w) =
T
p
S
[w]
S
S
[w] + T
p
S
[w]
The fractional contributions are just the actual
supports for the word w, normalized by the to-
tal contribution for the word from across the two
models. Similar estimates, f
(p,r)
S
N
(.) and f
(p,r)
S
N
(.)
are made for reply words from pairs labeled N .
In our example from Section 4.2, words such as
rejoin are likely to get higher f
(p,r)
T
S
(.) scores due
to being better correlated with problem words and
consequently better supported by the translation
model; those such as try may get higher f
(p,r)
S
S
(.)
scores.
4.3.3 M-Step: Learning Models
We use the labels and reply-word source estimates
from the E-step to re-learn the language and trans-
lation models in this step. As may be obvious
from the ensuing discussion, those pairs labeled
as solution pairs are used to learn the S
S
and T
S
models and those labeled as non-solution pairs are
159
used to learn the models with subscript N . We let
each reply word contribute as much to the respec-
tive language and translation models according to
the estimates in Section 4.3.2. In our example, if
the word disconnect is assigned a source proba-
bility of 0.9 and 0.1 for the translation and lan-
guage models respectively, the virtual document-
pair from (p, r) that goes into the training of the
respective T model would assume that disconnect
occurs in r with a frequency of 0.9; similarly, the
respective S would account for disconnect with a
frequency of 0.1. Though fractional word frequen-
cies are not possible in real documents, statistical
models can accomodate such fractional frequen-
cies in a straightforward manner. The language
models are learnt only over the r parts of the (p, r)
pairs since they are meant to characterize reply be-
havior; on the other hand, translation models learn
over both p and r parts to model correlation.
Regularizing the T models: In our formula-
tion, the language and translation models may be
seen as competing for ?ownership? of reply words.
Consider the post and reply vocabularies to be
of sizes A and B respectively; then, the transla-
tion model would have A ? B variables, whereas
the unigram language model has only B variables.
This gives the translation model an implicit edge
due to having more parameters to tune to the data,
putting the language models at a disadvantage.
To level off the playing field, we use a regular-
ization
9
operation in the learning of the transla-
tion models. The IBM Model 1 learning pro-
cess uses an internal EM approach where the E-
step estimates the alignment vector for each prob-
lem word; this vector indicates the distribution of
alignments of the problem word across the solu-
tion words. In our example, an example alignment
vector for wifi could be: {rejoin : 0.4, network :
0.4, disconnect : 0.1, . . .}. Our regularization
method uses a parameter ? to discard the long tail
in the alignment vector by resetting entries hav-
ing a value ? ? to 0.0 followed by re-normalizing
the alignment vector to add up to 1.0. Such prun-
ing is performed at each iteration in the learn-
ing of the translation model, so that the following
M-steps learn the probability matrix according to
such modified alignment vectors.
The semantics of the ? parameter may be in-
9
We use the word regularization in a generic sense to
mean adapting models to avoid overfitting; in particular, it
may be noted that we are not using popular regularization
methods such as L1-regularization.
Alg. 1 Clustering-based Solution Identification
Input. C, a set of (p, r) pairs
Output. C
?
, the set of identified solution pairs
Initialization
1. ?(p, r) ? C
2. if(r.postpos = 2) label((p, r)) = S
3. else label((p, r)) = N
4. Learn S
S
& T
S
using pairs labeled S
5. Learn S
N
& T
N
using pairs labeled N
EM Iterations
6. while(not converged ?#Iterations < 10)
E-Step:
7. ?(p, r) ? C
8. label((p, r)) = argmax
i
F ((p, r),S
i
, T
i
)
9. ?w ? r
10. Estimate f
(p,r)
S
label(p,r)
(w) , f
(p,r)
T
label(p,r)
(w)
M-Step:
11. Learn S
S
& T
S
from pairs labeled S
using the f
(p,r)
S
S
(.) f
(p,r)
T
S
(.) estimates
12. Learn S
N
& T
N
from pairs labeled N
using the f
(p,r)
S
N
(.) f
(p,r)
T
N
(.) estimates
Output
13. Output (p, r) pairs from C with
label((p, r)) = S as C
?
tuitively outlined. If we would like to allow align-
ment vectors to allow a problem word to align with
upto two reply words, we would need to set ? to
a value close to 0.5(=
1
2
); ideally though, to al-
low for the mass consumed by an almost inevitable
long tail of very low values in the alignment vec-
tor, we would need to set it to slightly lower than
0.5, say 0.4.
4.3.4 Initialization
K-Means clustering mostly initializes centroid
vectors randomly; however, it is non-trivial to ini-
tialize the complex translation and language mod-
els randomly. Moreover, an initialization such that
the S
S
and T
S
models favor the solution pairs
more than the non-solution pairs is critical so that
they may progressively lean towards modeling so-
lution behaviour better across iterations. Towards
this, we make use of a structural feature; in partic-
ular, adapting the hypothesis that solutions occur
in the first N posts (Ref. (Catherine et al, 2012)),
we label the pairs that have the the reply from the
second post (note that the first post is assumed to
be the problem post) in the thread as a solution
160
post, and all others as non-solution posts. Such
an initialization along with uniform reply word
source probabilities is used to learn the initial es-
timates of the S
S
, T
S
, S
N
and T
N
models to be
used in the E-step for the first iteration. We will
show that we are able to effectively perform solu-
tion identification using our approach by exploit-
ing just one structural feature, the post position,
as above. However, we will also show that we can
exploit other features as and when available, to de-
liver higher accuracy clusterings.
4.3.5 Method Summary
The overall method comprising the steps that
have been described is presented in Algorithm 1.
The initialization using the post position (Ref.
Sec 4.3.4) is illustrated in Lines 1-5, whereas the
EM-iterations form Steps 6 through 12. Of these,
the E-step incorporates labeling (Line 8) as de-
scribed in Sec 4.3.1 and reply-word source estima-
tion (Line 10) detailed in Sec 4.3.2. The models
are then re-learnt in the M-Step (Lines 11-12) as
outlined in Sec 4.3.3. At the end of the iterations
that may run up to 10 times if the labelings do not
stabilize earlier, the pairs labeled S are output as
identified solutions (Line 13).
Time Complexity: Let n denote |C|, and the
number of unique words in each problem and re-
ply post be a and b respectively. We will de-
note the vocabulary size of problem posts as A
and that of reply posts as B. Learning of the
language and translation models in each iteration
costs O(nb + B) and O(k
?
(nab + AB)) respec-
tively (assuming the translation model learning
runs for k
?
iterations). The E-step labeling and
source estimation cost O(nab) each. For k iter-
ations of our algorithm, this leads to an overall
complexity of O(kk
?
(nab+AB)).
5 Experimental Evaluation
We use a crawl of 140k threads from Apple Dis-
cussion forums
10
. Out of these, 300 threads (com-
prising 1440 posts) were randomly chosen and
each post was manually tagged as either solution
or non-solution by the authors of (Catherine et al,
2013) (who were kind enough to share the data
with us) with an inter-annotator agreement
11
of
0.71. On an average, 40% of replies in each thread
and 77% of first replies were seen to be solutions,
10
http://discussions.apple.com
11
http://en.wikipedia.org/wiki/Cohen?s kappa
Figure 1: F% (Y) vs. #Iterations (X)
T
S
ProblemWord, SolutionWord T
S
[p][s]
network, guest 0.0754
connect, adaptor 0.0526
wireless, adaptor 0.0526
translat, shortcut 0.0492
updat, rebuilt 0.0405
S
S
SolutionWord S
S
[s]
your 0.0115
try 0.0033
router 0.0033
see 0.0033
password 0.0023
Table 4: Sample T
S
and S
S
Estimates
leading to an F-measure of 53% for our initializa-
tion heuristic. We use the F-measure
12
for solu-
tion identification, as the primary evaluation mea-
sure. While we vary the various parameters sep-
arately in order to evaluate the trends, we use a
dataset of 800 threads (containing the 300 labeled
threads) and set ? = 0.5 and ? = 0.4 unless other-
wise mentioned. Since we have only 300 labeled
threads, accuracy measures are reported on those
(like in (Catherine et al, 2013)). We pre-process
the post data by stemming words (Porter, 1980).
5.1 Quality Evaluation
In this study, we compare the performance of our
method under varying settings of ? against the
only unsupervised approach for solution identi-
fication from literature, that from (Cong et al,
2008). We use an independent implementation
of the technique using Kullback-Leibler Diver-
gence (Kullback, 1997) as the similarity measure
between posts; KL-Divergence was seen to per-
form best in the experiments reported in (Cong et
al., 2008).
Table 3 illustrates the comparative performance
12
http://en.wikipedia.org/wiki/F1 score
161
Technique Precision Recall F-Measure
Unsupervised Graph Propagation (Cong et al, 2008) 29.7 % 55.6 % 38.7 %
Our Method with only Translation Models (? = 0.0) 41.8 % 86.8 % 56.5 %
Our Method with only Language Models (? = 1.0) 63.2 % 62.1 % 62.6 %
Our Method with Both Models (? = 0.5) 61.3 % 66.9 % 64.0 %
Methods using Supervision (Catherine et al, 2013)
ANS CT 40.6 % 88.0 % 55.6 %
ANS-ACK PCT 56.8 % 84.1 % 67.8%
Table 3: Quality Evaluation
Figure 2: F% (Y) vs. ? (X) Figure 3: F% (Y) vs. ? (X) Figure 4: F% (Y) vs. #Threads (X)
on various quality metrics, of which F-Measure is
typically considered most important. Our pure-
LM
13
setting (i.e., ? = 1) was seen to perform up
to 6 F-Measure points better than the pure-TM
14
setting (i.e., ? = 0), whereas the uniform mix is
seen to be able to harness both to give a 1.4 point
(i.e., 2.2%) improvement over the pure-LM case.
The comparison with the approach from (Cong et
al., 2008) illustrates that our method is very clearly
the superior method for solution identification out-
performing the former by large margins on all the
evaluation measures, with the improvement on F-
measure being more than 25 points.
Comparison wrt Methods from (Catherine et
al., 2013): Table 3 also lists the performance of
SVM-based methods from (Catherine et al, 2013)
that use supervised information for solution iden-
tification, to help put the performance of our tech-
nique in perspective. Of the two methods therein,
ANS CT is a more general method that uses two
views (structural and lexical) of solutions which
are then co-trained. ANS-ACK PCT is an en-
hanced method that requires author-id informa-
tion and a means of classifying posts as acknowl-
edgements (which is done using additional super-
vision); a post being acknowledged by the prob-
lem author is then used as a signal to enhance
the solution-ness of a post. In the absence of
author information (such as may be common in
13
Language Model
14
Translation Model
privacy-constrained domains such as medical fo-
rums) and extrinsic information to enable identify
acknowledgements, ANS CT is the only technique
available. Our technique is seen to outperform
ANS CT by a respectable margin (8.6 F-measure
points) while trailing behind the enhanced ANS-
ACK PCT method with a reasonably narrow 3.8
F-measure point margin. Thus, our unsupervised
method is seen to be a strong competitor even for
techniques using supervision outlined in (Cather-
ine et al, 2013), illustrating the effectiveness of
LM and TM modeling of reply posts.
Across Iterations: For scenarios where com-
putation is at a premium, it is useful to know how
quickly the quality of solution identification sta-
bilizes, so that the results can be collected after
fewer iterations. Figure 1 plots the F-measure
across iterations for the run with ? = 0.5, ? = 0.4
setting, where the F-measure is seen to stabilize in
as few as 4-5 iterations. Similar trends were ob-
served for other runs as well, confirming that the
run may be stopped as early as after the fourth it-
eration without considerable loss in quality.
Example Estimates from LMs and TMs: In
order to understand the behavior of the statistical
models, we took the highest 100 entries from both
S
S
and T
S
and attempted to qualitatively evalu-
ate semantics of the words (or word pairs) corre-
sponding to those. Though the stemming made it
hard to make sense of some entries, we present
some of the understandable entries from among
162
the top-100 in Table 4. The first three entries from
T
S
deal with connection issues for which adaptor
or guest account related solutions are proposed,
whereas the remaining have something to do with
the mac translator app and rebuilding libraries af-
ter an update. The top words from S
S
include im-
perative words and words from solutions to com-
mon issues that include actions pertaining to the
router or password.
5.2 Varying Parameter Settings
We now analyse the performance of our approach
against varying parameter settings. In particular,
we vary ? and ? values and the dataset size, and
experiment with some initialization variations.
Varying ?: ? is the weighting parameter that
indicates the fraction of weight assigned to LMs
(vis-a-vis TMs). As may be seen from Figure 2,
the quality of the results as measured by the F-
measure is seen to peak around the middle (i.e.,
? = 0.5), and decline slowly towards either ex-
treme, with a sharp decline at ? = 0 (i.e., pure-
TM setting). This indicates that a uniform mix is
favorable; however, if one were to choose only one
type of model, usage of LMs is seen to be prefer-
able than TMs.
Varying ? : ? is directly related to the extent of
pruning of TMs, in the regularization operation;
all values in the alignment vector ? ? are pruned.
Thus, each problem word is roughly allowed to be
aligned with at most ?
1
?
solution words. The
trends from Figure 3 suggests that allowing a prob-
lem word to be aligned to up to 2.5 solution words
(i.e., ? = 0.4) is seen to yield the best performance
though the quality decline is graceful towards ei-
ther side of the [0.1, 0.5] range.
Varying Data Size: Though more data always
tends to be beneficial since statistical models ben-
efit from redundancy, the marginal utility of ad-
ditional data drops to very small levels beyond
a point; we are interested in the amount of data
beyond which the quality of solution identifica-
tion flattens out. Figure 4 suggests that there is
a sharp improvement in quality while increasing
the amount of data from 300 threads (i.e., 1440
(p, r) pairs) to 550 (2454 pairs), whereas the in-
crement is smaller when adding another 250 pairs
(total of 3400 pairs). Beyond 800 threads, the F-
measure was seen to flatten out rapidly and stabi-
lize at ? 64%.
Initialization: In Apple discussion forums,
posts by Apple employees that are labeled with
the Apple employees tag (approximately ? 7% of
posts in our dataset) tend to be solutions. So are
posts that are marked Helpful (? 3% of posts) by
other users. Being specific to Apple forums, we
did not use them for initialization in experiments
so far with the intent of keeping the technique
generic. However, when such posts are initial-
ized as solutions (in addition to first replies as we
did earlier), the F-score for solution identification
for our technique was seen to improve slightly, to
64.5% (from 64%). Thus, our technique is able
to exploit any extra solution identifying structural
features that are available.
6 Conclusions and Future Work
We considered the problem of unsupervised so-
lution post identification from discussion forum
threads. Towards identifying solutions to the prob-
lem posed in the initial post, we proposed the us-
age of a hitherto unexplored textual feature for
the solution identification problem; that of lexical
correlations between problems and solutions. We
model and harness lexical correlations using trans-
lation models, in the company of unigram lan-
guage models that are used to characterize reply
posts, and formulate a clustering-based EM ap-
proach for solution identification. We show that
our technique is able to effectively identify solu-
tions using just one non-content based feature, the
post position, whereas previous techniques in liter-
ature have depended heavily on structural features
(that are not always available in many forums) and
supervised information. Our technique is seen to
outperform the sole unsupervised solution identi-
fication technique in literature, by a large margin;
further, our method is even seen to be competi-
tive to recent methods that use supervision, beat-
ing one of them comfortably, and trailing another
by a narrow margin. In short, our empirical analy-
sis illustrates the superior performance and estab-
lishes our method as the method of choice for un-
supervised solution identification.
Exploration into the usage of translation models
to aid other operations in discussion forums such
as proactive word suggestions for solution author-
ing would be interesting direction for follow-up
work. Discovery of problem-solution pairs in
cases where the problem post is not known before-
hand, would be a challenging problem to address.
163
References
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79?85.
Rose Catherine, Amit Singh, Rashmi Gangadharaiah,
Dinesh Raghu, and Karthik Visweswariah. 2012.
Does similarity matter? the case of answer extrac-
tion from technical discussion forums. In COLING
(Posters), pages 175?184.
Rose Catherine, Rashmi Gangadharaiah, Karthik
Visweswariah, and Dinesh Raghu. 2013. Semi-
supervised answer extraction from discussion fo-
rums. In IJCNLP.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 467?474. ACM.
P. Deepak, Karthik Visweswariah, Nirmalie Wiratunga,
and Sadiq Sani. 2012. Two-part segmentation of
text documents. In CIKM, pages 793?802.
Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), pages 1?
38.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xianyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of questions from online
forums. In ACL.
Ankur Gandhe, Dinesh Raghu, and Rose Catherine.
2012. Domain adaptive answer extraction for dis-
cussion boards. In Proceedings of the 21st interna-
tional conference companion on World Wide Web,
pages 501?502. ACM.
Liangjie Hong and Brian D Davison. 2009. A
classification-based approach to question answering
in discussion boards. In Proceedings of the 32nd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 171?
178. ACM.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010.
Tagging and linking web forum posts. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 192?202. Asso-
ciation for Computational Linguistics.
Janet L Kolodner. 1992. An introduction to case-based
reasoning. Artificial Intelligence Review, 6(1):3?34.
Solomon Kullback. 1997. Information theory and
statistics. Courier Dover Publications.
James MacQueen. 1967. Some methods for classi-
fication and analysis of multivariate observations.
In Proceedings of the fifth Berkeley symposium on
mathematical statistics and probability, volume 1,
page 14. California, USA.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program: electronic library and information
systems, 14(3):130?137.
Zhonghua Qu and Yang Liu. 2011. Finding problem
solving threads in online forum. In IJCNLP, pages
1413?1417.
Jangwon Seo, W Bruce Croft, and David A Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM conference
on Information and knowledge management, pages
1907?1910. ACM.
Amit Singh. 2012. Entity based q&a retrieval. In
EMNLP-CoNLL, pages 1266?1277.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives.
In SIGIR, pages 475?482.
164
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 12?21,
Dublin, Ireland, August 23-24 2014.
Generating a Word-Emotion Lexicon from #Emotional Tweets
Anil Bandhakavi
1
Nirmalie Wiratunga
1
Deepak P
2
Stewart Massie
1
1
IDEAS Research Institute, Robert Gordon University, Scotland, UK
2
IBM Research - India, Bangalore, India
{a.s.bandhakavi,n.wiratunga}@rgu.ac.uk
deepaksp@acm.org, s.massie@rgu.ac.uk
Abstract
Research in emotion analysis of text sug-
gest that emotion lexicon based features
are superior to corpus based n-gram fea-
tures. However the static nature of the
general purpose emotion lexicons make
them less suited to social media analysis,
where the need to adopt to changes in vo-
cabulary usage and context is crucial. In
this paper we propose a set of methods to
extract a word-emotion lexicon automati-
cally from an emotion labelled corpus of
tweets. Our results confirm that the fea-
tures derived from these lexicons outper-
form the standard Bag-of-words features
when applied to an emotion classification
task. Furthermore, a comparative analysis
with both manually crafted lexicons and
a state-of-the-art lexicon generated using
Point-Wise Mutual Information, show that
the lexicons generated from the proposed
methods lead to significantly better classi-
fication performance.
1 Introduction
Emotion mining or affect sensing is the compu-
tational study of natural language expressions in
order to quantify their associations with different
emotions (e.g. anger, fear, joy, sadness and sur-
prise). It has a number of applications for the in-
dustry, commerce and government organisations,
but uptake has arguably been slow. This in part is
due to the challenges involved with modelling sub-
jectivity and complexity of the emotive content.
However, use of qualitative metrics to capture
emotive strength and extraction of features from
these metrics has in recent years shown promise
(Shaikh, 2009). A general-purpose emotion lexi-
con (GPEL) is a commonly used resource that al-
lows qualitative assessment of a piece of emotive
text. Given a word and an emotion, the lexicon
provides a score to quantify the strength of emo-
tion expressed by that word. Such lexicons are
carefully crafted and are utilised by both super-
vised and unsupervised algorithms to directly ag-
gregate an overall emotion score or indirectly de-
rive features for emotion classification tasks (Mo-
hammad, 2012a), (Mohammad, 2012b).
Socio-linguistics suggest that social media is a
popular means for people to converse with individ-
uals, groups and the world in general (Boyd et al.,
2010). These conversations often involve usage of
non-standard natural language expressions which
consistently evolve. Twitter and Facebook were
credited for providing momentum for the 2011
Arab Spring and Occupy Wall street movements
(Ray, 2011),(Skinner, 2011). Therefore efforts to
model social conversations would provide valu-
able insights into how people influence each other
through emotional expressions. Emotion analysis
in such domains calls for automated discovery of
lexicons. This is so since learnt lexicons can in-
tuitively capture the evolving nature of vocabulary
in such domains better than GPELs.
In this work we show how an emotion la-
belled corpus can be leveraged to generate a word-
emotion lexicon automatically. Key to this is the
availability of a labelled corpus which may be ob-
tained using a distance-supervised approach to la-
belling (Wang et al., 2012). In this paper we pro-
pose three lexicon generation methods and evalu-
ate the quality of these by deploying them in an
emotion classification task. We show through our
experiments that the word-emotion lexicon gener-
ated using the proposed methods in this paper sig-
nificantly outperforms GPELs such as WordnetAf-
fect, NRC word-emotion association lexicon and a
leaxicon learnt using Point-wise Mutual Informa-
tion (PMI). Additionally, our lexicons also outper-
form the traditional Bag-of-Words representation.
The rest of the paper is organised as follows: In
12
Section 2 we present the related work. In Section
3 we outline the problem. In Section 4 we for-
mulate the different methods proposed to generate
the word-emotion lexicons. In Section 5 we dis-
cuss experimental results followed by conclusions
and future work in Section 6.
2 Related Work
Computational emotion analysis, draws from cog-
nitive and physiology studies to establish the key
emotion categories; and NLP and text mining re-
search to establish features designed to represent
emotive content. Emotion analysis has been ap-
plied in a variety of domains: fairy tales (Fran-
cisco and Gervas, 2006; Alm et al., 2005);
blogs (Mihalcea and Liu, 2006; Neviarouskaya et
al., 2010), novels (John et al., 2006), chat mes-
sages (E.Holzman and William M, 2003; Ma et
al., 2005; Mohammad and Yang, 2011) and emo-
tional events on social media content(Kim et al.,
2009). Comparative studies on emotive word dis-
tributions on micro-blogs and personal content
(e.g. love letters, suicide notes) have shown that
emotions such as disgust are expressed well in
tweets. Further, expression of emotion in tweets
and love letters have been shown to have similari-
ties(K. Roberts and Harabagiu, 2012).
Emotion classification frameworks provide in-
sights into human emotion expressions (Ekman,
1992; Plutchik, 1980; Parrott, 2001). The emo-
tions proposed by (Ekman, 1992) are popular in
emotion classification tasks (Mohammad, 2012b;
Aman and Szpakowicz, 2008). Recently there has
also been interest in extending this basic emo-
tion framework to model more complex emotions
(such as politeness, rudeness, deception, depres-
sion, vigour and confusion) (Pearl and Steyvers,
2010; Bollen et al., 2009). A common theme
across these approaches involves the selection
of emotion-rich features and learning of relevant
weights to capture emotion strength (Mohammad,
2012a; Qadir and Riloff, 2013).
Usefulness of a lexicon: Lexicons such as
Wordnet Affect (Strapparava and Valitutti, 2004)
and NRC (Saif M. Mohammad, 2013)) are
very valuable resources from which emotion
features can be derived for text representation.
These are manually crafted and typically con-
tain emotion-rich formal vocabulary. Hybrid ap-
proaches that combine features derived from these
static lexicons with n-grams have resulted in bet-
ter performance than either alone (Mohammad,
2012b),(Aman and Szpakowicz, 2008). However
the informal and dynamic nature of social me-
dia content makes it harder to adopt these lexi-
cons for emotion analysis. An alternative strategy
is to derive features from a dynamic (i.e., learnt)
lexicon. Here association metrics such as Point-
wise Mutual Information (PMI) can be used to
model emotion polarity between a word and emo-
tion labelled content (Mohammad, 2012a). Such
approaches will be used as baselines to compare
against our proposed lexicon generation strategies.
There are other lexicon generation methods pro-
posed by Rao .et. al (Yanghui Rao and Chen,
2013) and Yang .et. al (Yang et al., 2007). We do
not consider these in our comparative evaluation
since these methods require rated emotion labels
and emoticon classes respectively.
Lexicon generation, relies on the availability of
a labelled corpus from which the word-emotion
distributions can be discovered. For this pur-
pose we exploit a distance-supervised approach
where indirect cues are used to unearth implicit
(or distant) labels that are contained in the cor-
pus (Alec Go and Huang, 2009). We adopt
the approach as in (Wang et al., 2012) to cor-
pus labelling where social media content, and in
particular Twitter content is sampled for a pre-
defined set of hashtag cues (P. Shaver, 1987) .
Here each set of cues represent a given emotion
class. Distant-supervision is particularly suited to
Twitter-like platforms because people use hash-
tags to extensively convey or emphasis the emo-
tion behind their tweets (e.g., That was my best
weekend ever.#happy!! #satisfied!). Also given
that tweets are length restricted (140 characters),
modelling the emotional orientation of words in
a Tweet is easier compared to longer documents
that are likely to capture complex and mixed emo-
tions. This simplicity and access to sample data
has made Twitter one of the most popular domains
for emotion analysis research (Wang et al., 2012;
Qadir and Riloff, 2013).
3 Problem Definition
We now outline the problem formally. We start
with a set of documents D = {d
1
, d
2
, . . . , d
n
}
where each document d
i
has an associated label
C
d
i
indicating the emotion class to which d
i
be-
longs. We consider the case where the documents
are tweets. For example, a tweet d
i
nice sunday
13
#awesome may have a label joy indicating that the
tweet belongs to the joy emotion class. We also as-
sume that the labels C
d
i
come from a pre-defined
set of six emotion classes anger, fear, joy, sad, sur-
prise, love. Since our techniques are generic and
do not depend on the number of emotion classes,
we will denote the emotion classes as {C
j
}
N
j=1
.
Let there be K words extracted from the training
documents, denoted as {w
i
}
K
i=1
. Our task is to de-
rive a lexiconLex that quantifies the emotional va-
lence of words (from the tweets in D) to emotion
classes. In particular, the lexicon may be thought
of as a 2d-associative array where Lex[w][c] indi-
cates the emotional valence of the word w to the
emotion class c. When there is no ambiguity, we
will use Lex(i, j) to refer to the emotional valence
of word w
i
to the emotion class C
j
. We will quan-
tify the goodness of the lexicons that are generated
using various methods by measuring their perfor-
mance in an emotion classification task.
4 Lexicon Generation Methods
We now outline the various methods for lexicon
generation. We first start off with a simple tech-
nique for learning lexicons based on just term fre-
quencies (which we will later use as a baseline
technique), followed by more sophisticated meth-
ods that are based on conceptual models on how
tweets are generated.
4.1 Term Frequency based Lexicon
A simple way to measure the emotional valence of
the word w
i
to the emotion class C
j
is to compute
the probability of occurrence of w
i
in a tweet la-
belled as C
j
, normalized by its probability across
all classes. This leads to:
Lex(i, j) =
p(w
i
|C
j
)
?
N
k=1
p(w
i
|C
k
)
(1)
where the conditional probability is simply
computed using term frequencies.
p(w
i
|C
j
) =
freq(w
i
, C
j
)
freq(C
j
)
(2)
where freq(w
i
, C
j
) is the number of times
w
i
occurs in documents labeled with class C
j
.
freq(C
j
) is the total number of documents in C
j
.
4.2 Iterative methods for Lexicon Generation
The formulation in the previous section generates
a word-emotion matrix L by observing the term
frequencies within a class. However term frequen-
cies alone do not capture the term-class associa-
tions, because not all frequently occurring terms
exhibit the characteristics of a class. For exam-
ple, a term sunday that occurs in a tweet nice sun-
day #awesome labelled joy is evidently not indica-
tive of the class joy; however, the frequency based
computation increments the weight of sunday wrt
the class joy by virtue of this occurrence. In the
following sections, we propose generative models
that seek to remedy such problems of the simple
term frequency based lexicon.
4.2.1 Generative models for Documents
As discussed above, though a document is labelled
with an emotion class, not all terms relate strongly
to the labelled emotion. Some documents may
have terms conveying a different emotion than
what the document is labelled with, since the la-
bel is chosen based on the most prominent emo-
tion in the tweet. Additionally, some words could
be emotion-neutral (e.g., sunday in our example
tweet) and could be conveying non-emotional in-
formation. We now describe two generative mod-
els that account for such considerations, and then
outline methods to learn lexicons based on them.
Mixture of Classes Model: Let L
C
k
be the
unigram language model (Liu and Croft, 2005)
that expresses the lexical character for the emotion
class C
k
; though microblogs are short text frag-
ments, language modeling approaches have been
shown to be effective in similarity assesment be-
tween them (Deepak and Chakraborti, 2012). We
model a document d
i
to be generated from across
the emotion class language models:
1. For each word w
j
in document d
i
,
(a) Lookup the unit vector [?
(1)
d
ij
, . . . , ?
(N)
d
ij
];
This unit vector defines a probability
distribution over the language models.
(b) Choose a language model L from
among the K LMs, in accordance with
the vector
(c) Samplew
j
in accordance with the multi-
nomial distribution L
If d
i
is labelled with the emotion class C
d
i
, it is
likely that the value of ?
(n)
d
ij
is high for words in d
i
since it is likely that majority of the words are sam-
pled from the L
C
d
i
language model. The posterior
probability in accordance with this model can then
be intuitively formulated as:
14
P (d
i
, C
d
i
|?) =
?
w
j
?d
i
N
?
x=1
?
(x)
d
ij
? L
C
x
(w
j
) (3)
where ? is the parameters {L
C
j
}
N
j=1
, ? and C
d
i
is the class label for document d
i
.
Class and Neutral Model: We now introduce
another model where the words in a document are
assumed to be sampled from either the language
model of the corresponding (i.e., labelled) emo-
tion class or from the neutral language model, L
C
.
Thus, the generative model for a document d
i
la-
belled with emotion classC
d
i
would be as follows:
1. For each word w
j
in document d
i
,
(a) Lookup the weight ?
d
ij
; this parameter
determines the mix of the labelled emo-
tion class and the neutral class, for w
j
in
d
i
(b) Choose L
C
k
with a probability of ?
d
ij
,
and L
C
with a probability of 1.0? ?
d
ij
(c) Samplew
j
in accordance with the multi-
nomial distribution of the chosen lan-
guage model
The posterior probability in accordance with
this model can be intuitively formulated as :
P (d
i
, C
d
i
|?) =
?
w
j
?d
i
?
d
ij
? L
C
d
i
(w
j
)
+ (1? ?
d
ij
)? L
C
(w
j
)
(4)
where ? is the parameters {L
C
j
}
N
j=1
, L
C
, ? .
Equation 3 models a document to exhibit char-
acteristics of many classes with different levels
of magnitude. Equation 4 models a document to
be a composition of terms that characterise one
class and other general terms; a similar formula-
tion where a document is modeled using a mix of
two models has been shown to be useful in charac-
terizing problem-solution documents (Deepak et
al., 2012; Deepak and Visweswariah, 2014). The
central idea of the expectation maximization (EM)
algorithm is to maximize the probability of the
data, given the language models {L
C
j
}
N
j=1
and
L
C
. The term weights are estimated from the lan-
guage models (E-step) and the language models
are re-estimated (M-step) using the term weights
from the E-step. Thus the maximum likelihood
estimation process in EM alternates between the
E-step and the M-step. In the following sections
we detail the EM process for the two generative
models separately. We compare and contrast the
two variants of the EM algorithm in Table 1.
4.2.2 EM with Mixture of Classes Model
We will use a matrix based representation for the
language model and the lexicon, to simplify the il-
lustration of the EM steps. Under the matrix nota-
tion, L
(p)
denotes theK?N matrix at the p
th
iter-
ation where the i
th
column is the language model
corresponding to the i
th
class, i.e., L
C
i
. The p
th
E-
step estimates the various ?
d
ij
vectors for all doc-
uments based on the language models in L
(p?1)
,
whereas the M-step re-learns the language models
based on the ? values from the E-step. The steps
are detailed as follows:
E-Step: The ?
(n)
d
ij
is simply estimated to the
fractional support for the j
th
word in the i
th
docu-
ment (denoted as w
ij
) from the n
th
class language
model:
?
(n)
d
ij
=
L
(p?1)
C
n
(w
ij
)
?
x
L
(p?1)
C
x
(w
ij
)
(5)
M-Step: As mentioned before in Table 1 this
step learns the language models from the ? esti-
mates of the previous step. As an example, if a
wordw is estimated to have come from the joy lan-
guage model with a weight (i.e., ?) 0.5, it would
contribute 0.5 as its count to the joy language
model. Thus, every occurrence of a word is split
across language models using their corresponding
? estimates:
L
(p)
C
n
[w] =
?
i
?
j
I(w
ij
= w)? ?
(n)
d
ij
?
i
?
j
?
(n)
d
ij
(6)
where the indicator function I(w
ij
= w) evalu-
ates to 1 if w
ij
= w is satisfied and 0 otherwise.
After any M-Step, the lexicon can be obtained
by normalizing the L
(p)
language models so that
the weights for each word adds up to 1.0. i.e.,
Lex
(p)
(i, j) =
L
(p)
C
j
[w
i
]
?
K
x=1
L
(p)
C
x
[w
i
]
(7)
In the above equation, the suffix (i, j) refers to
the i
th
word in the j
th
class, confirming to our 2d-
array representation of the language models.
15
Table 1: EM Algorithm variants
States EM with mixture of classes model EM with class and neutral model
INPUT Training data T Training data T
OUTPUT Word-Emotion Lexicon Word-Emotion Lexicon
Initialisation Learn the initial language models
{L
C
j
}
N
j=1
Learn the initial language models
{L
C
j
}
N
j=1
and L
C
Convergence While not converged or #Iterations
< ?, a threshold
While not converged or #Iterations
< ?, a threshold
E-step Estimate the ?
d
ij
s based on the
current estimate of {L
C
j
}
N
j=1
(Sec
4.2.2)
Estimate ?
d
ij
based on the current
estimate of {L
C
j
}
N
j=1
and L
C
(Sec
4.2.3)
M-step Estimate the language models
{L
C
j
}
N
j=1
using ?
d
ij
s (Sec 4.2.2)
Estimate the language models
{L
C
j
}
N
j=1
and L
C
using ?
d
ij
(Sec
4.2.3)
Lexicon Induction Induce a word-emotion lexicon
from {L
C
j
}
N
j=1
(Sec 4.2.2)
Induce a word-emotion lexicon
from {L
C
j
}
N
j=1
and L
C
(Sec 4.2.3)
4.2.3 EM with Class and Neutral Model
The main difference in this case, when compared
to the previous is that we need to estimate a neutral
language model L
C
in addition to the class spe-
cific models. We also have fewer parameters to
learn since the ?
d
ij
is a single value rather than a
vector of N values as in the previous case.
E-Step: ?
d
ij
is estimated to the relative weight
of the wordw
ij
from across the language model of
the corresponding class, and the neutral model:
?
d
ij
=
L
(p?1)
C
d
i
(w
ij
)
L
(p?1)
C
d
i
(w
ij
) + L
(p?1)
C
(w
ij
)
(8)
Where C
d
i
denotes the class corresponding to
the label of the document d
i
.
M-Step: In a slight contrast from the M-Step
for the earlier case as shown in Table 1, a word
estimated to have a weight (i.e., ? value) of 0.2
would contribute 20% of its count to the cor-
responding class? language model, while the re-
maining would go to the neutral language model
L
C
. Since the class-specific and neutral language
models are estimated differently, we have two sep-
arate equations:
L
(p)
C
n
[w] =
?
i,label(d
i
)=C
n
?
j
I(w
ij
= w)? ?
d
ij
?
i,label(d
i
)=C
n
?
j
?
d
ij
(9)
L
(p)
C
[w] =
?
i
?
j
I(w
ij
= w)? (1.0? ?
d
ij
)
?
i
?
j
(1.0? ?
d
ij
)
(10)
where label(d
i
) = C
n
As is obvious, the class-
specific language models are contributed to by
the documents labelled with the class whereas the
neutral language model has contributions from all
documents. The normalization to achieve the lexi-
con is exactly the same as in the mixture of classes
case, and hence, is omitted here.
4.2.4 EM Initialization
In the case of iterative approaches like EM, the ini-
tialization is often considered crucial. In our case,
we initialize the unigram class language models
by simply aggregating the scores of the words in
tweets labelled with the respective class. Thus, the
joy language model would be the initialized to be
the maximum likelihood model to explain the doc-
uments labelled joy. In the case of the class and
neutral generative model, we additionally build
the neutral language model by aggregating counts
across all the documents in the corpus (regardless
of what their emotion label is).
5 Experiments
In this section we detail our experimental evalu-
ation. We begin with the details about the Twit-
ter data used in our experiments. We then dis-
cuss how we created the folds for a cross valida-
tion experiment. Thereafter we detail the classifi-
16
cation task used to evaluate the word-emotion lex-
icon. Finally we discuss the performance of our
proposed methods for lexicon generation in com-
parison with other manually crafted lexicons, PMI
based method for lexicon generation and the stan-
dard BoW in an emotion classification task.
5.1 Twitter Dataset
The data set used in our experiments was a corpus
of emotion labelled tweets harnessed by (Wang et
al., 2012). The data set was available in the form
of tweet ID?s and the corresponding emotion la-
bel. The emotion labels comprised namely : anger,
fear, joy, sadness, surprise, love and thankfulness.
We used the Twitter search API
1
to obtain the
tweets by searching with the corresponding tweet
ID. After that we decided to consider only tweets
that belong to the primary set of emotions defined
by Parrott (Parrott, 2001). The emotion classes in
our case included anger, fear, joy, sadness, sur-
prise and love. We had a collection of 0.28 mil-
lion tweets which we used to carry out a 10 fold
cross-validation experiment.
We decided to generate the folds manually,in
order to compare the performance of the differ-
ent algorithms used in our experiments. We split
the collection of 0.28 million tweets into 10 equal
size sets to generate 10 folds with different train-
ing and test sets in each fold. Also all the folds in
our experiments were obtained by stratified sam-
pling, ensuring that we had documents represent-
ing all the classes in both the training and test sets.
We used the training data in each fold to generate
the word-emotion lexicon and measured the per-
formance of it on the test data in an emotion clas-
sification task. Table 2 shows the average distri-
bution of the different classes namely: anger, fear,
joy, sadness, surprise and love over the 10 folds.
Observe that emotions such as joy and sadness had
a very high number of representative documents
. Emotions such as anger,love and fear were the
next most represented emotions. The emotion sur-
prise had very few representative documents com-
pared to that of the other emotions.
5.2 Evaluating the word-emotion lexicon
We adopted an emotion classification task in order
to evaluate the quality of the word-emotion lexi-
con generated using the proposed methods. Also
research in emotion analysis of text suggest that
1
https://dev.twitter.com/docs/using-search
Table 2: Average distribution of emotions across
the folds
Emotion Training Test
Anger 58410 6496
Fear 13692 1548
Joy 74108 8235
Sadness 63711 7069
Surprise 2533 282
Love 31127 3464
Total 243855 27095
lexicon based features were effective compared to
that of n-gram features in an emotion classifica-
tion of text (Aman and Szpakowicz, 2008; Mo-
hammad, 2012a). Therefore we decided to use the
lexicon to derive features for text representation.
We followed a similar procedure as in (Moham-
mad, 2012a) to define integer valued features for
text representation. We define one feature for each
emotion to capture the number of words in a train-
ing/test document that are associated with the cor-
responding emotion. The feature vector for a train-
ing/test document was constructed using the word-
emotion lexicon. Given a training/test document
d we construct the corresponding feature vector
d
?
=< count(e
1
), count(e
2
), . . . , count(e
m
)) >
of length m (in our case m is 6), wherein
count(e
i
) represents the number of words in d that
exhibit emotion e
i
. count(e
i
) is computed as:
count(e
i
) =
?
w?d
I( max
j=1,...,m
Lex(w, j) = C
i
)
(11)
where I(. . .) is the indicator function as used
previously. For example if a document has 1 joy
word, 2 love words and 1 surprise word the feature
vector for the document would be (0, 0, 1, 0, 1, 2).
We used the different lexicon generation methods
discussed in sections 4.1, 4.2.2 and 4.2.3 to con-
struct the feature vectors for the documents. In the
case of the lexicon generated as in section 4.2.3
the max in equation 11 is computed over m + 1
columns. We also used the lexicon generation
method proposed in (Mohammad, 2012a) to con-
struct the feature vectors. PMI was used in (Mo-
hammad, 2012a) to generate a word-emotion lexi-
con which is as follows :
Lex(i, j) = log
freq(w
i
, C
j
) ? freq(?C
j
)
freq(C
j
) ? freq(w
i
,?C
j
)
(12)
17
where freq(w
i
, C
j
) is the number of times n-
gram w
i
occurs in a document labelled with emo-
tion C
j
, freq(w
i
,?C
j
) is the number of times n-
gram w
i
occurs in a document not labelled with
emotion C
j
. freq(C
j
) and freq(?C
j
) are the
number of documents labelled with emotion C
j
and ?C
j
respectively.
Apart from the aforementioned automatically
generated lexicons we also used manually crafted
lexicons such as WordNet Affect (Strapparava and
Valitutti, 2004) and the NRC word-emotion as-
sociation lexicon (Saif M. Mohammad, 2013) to
construct the feature vectors for the documents.
Unlike the automatic lexicons, the general purpose
lexicons do not offer numerical scores. There-
fore we looked for presence/absence of words in
the lexicons to obtain the feature vectors. Fur-
thermore we also represented documents in the
standard BoW representation. We performed fea-
ture selection using the metric Chisquare
2
, to se-
lect the top 500 features to represent documents.
Since tweets are very short we incorporated a bi-
nary representation for BoW instead of term fre-
quency. For classification we used a multiclass
SVM classifier
3
and all the experiments were con-
ducted using the data mining software Weka
2
. We
used standard metrics such as Precision, Recall
and F-measure to compare the performance of the
different algorithms. In the following section we
analyse the experimental results for TF-lex (Sec
4.1), EMallclass-lex (Sec 4.2.2), EMclass-corpus-
lex (Sec 4.2.3), PMI-lex (Mohammad, 2012a),
WNA-lex (Strapparava and Valitutti, 2004), NRC-
lex (Saif M. Mohammad, 2013) and BoW in an
emotion classification task. Also in the case of
EM based methods we experimented with differ-
ent threshold limits ? shown in Table 1. We report
the results only w.r.t ? = 1 due to space limitations.
5.3 Results and Analysis
Table 3 shows the F-scores obtained for differ-
ent methods for each emotion. Observe that the
F-score for each emotion shown in Table 3 for a
method is the average F-score obtained over the
10 test sets (one per fold). We carried a two tail
paired t-test
4
between the baselines and our pro-
posed methods to measure statistical significance
for performance on the test set in each fold. From
2
http://www.cs.waikato.ac.nz/ml/weka/
3
http://www.csie.ntu.edu.tw/ cjlin/liblinear/
4
http://office.microsoft.com/en-gb/excel-help/ttest-
HP005209325.aspx
the t-test we observed that our proposed methods
are statistically significant over the baselines with
a confidence of 95% (i.e with p value 0.05). Also
note that the best results obtained for an emotion
are highlighted in bold. It is evident from the re-
sults that the manually crafted lexicons Wornd-
net Affect and the NRC word-emotion association
lexicon are significantly outperformed by all the
automatically generated lexicons for all emotions.
Also the BoW model significantly outperforms the
manually crafted lexicons suggesting that these
lexicons are not sufficiently effective for emotion
mining in a domain like Twitter.
When compared with BoW the PMI-lex pro-
posed by (Mohammad, 2012a) achieves a 2% gain
w.r.t emotion love, a 0.6% gain w.r.t emotion joy
and 1.28% gain w.r.t emotion sadness. However
in the case of emotions such as fear and sur-
prise BoW achieves significant gains of 11.17%
and 20.96% respectively. The results suggest that
the PMI-lex was able to leverage the availability
of adequate training examples to learn the pat-
terns about emotions such as anger, joy, sadness
and love. However given that not all emotions are
widely expressed a lexicon generation method that
relies heavily on abundant training data could be
ineffective to mine less represented emotions.
Now we analyse the results obtained for the lex-
icons generated from our proposed methods and
compare them with BoW and PMI-lex. From
the results obtained for our methods in Table 3
it suggests that our methods achieve the best F-
scores for 4 emotions namely anger, fear, sad-
ness and love out of the 6 emotions. In par-
ticular the EM-class-corpus-lex method obtains
the best F-score for 3 emotions namely anger,
sadness and love. When compared with BoW
and PMI-lex, EM-class-corpus-lex obtains a gain
of 0.85% and 0.93% respectively w.r.t emotion
anger, 1.85% and 0.57% respectively w.r.t emo-
tion sadness, 18.67% and 16.88% respectively
w.r.t emotion love. Our method TF-lex achieves a
gain of 5.47% and 16.64% respectively over BoW
and PMI-lex w.r.t emotion fear. Furthermore w.r.t
emotion surprise all our proposed methods outper-
form PMI-lex. However BoW still obtains the best
F-score for emotion surprise.
When we compared the results between our
own methods EM-class-corpus-lex obtains the
best F-scores for emotions anger, joy, sadness and
love. We expected that modelling a document
18
Table 3: Emotion classification results
Method Average F-Score
Anger Fear Joy Sadness Surprise Love
Baselines
WNA-lex 25.82% 6.61% 12.94% 8.76% 0.76% 2.67%
NRC-lex 21.37% 3.97% 16.04% 8.87% 1.54% 7.22%
Bow 56.5% 13.56% 63.34% 50.57% 21.65% 20.52%
PMI-lex 56.42% 2.39% 63.4% 50.57% 0.69% 22.31%
Our Learnt Lexicons
TF-lex 55.85% 19.03% 62.01% 50.54% 11.29% 37.69%
EMallclass-lex 56.64% 14.53% 61.89% 50.48% 12.33% 38.13%
EMclass-corpus-lex 57.35% 16.1% 62.74% 51.14% 12.05% 39.19%
to exhibit more than one emotion (EM-allclass-
lex) would better distinguish the class boundaries.
However given that tweets are very short it was
observed that modelling a document as a mixture
of emotion terms and general terms (EM-class-
corpus-lex) yielded better results. However we ex-
pect EM-allclass-lex to be more effective in other
domains such as blogs, discussion forums wherein
the text size is larger compared to tweets.
Table 4 summarizes the overall F-scores ob-
tained for the different methods. Note that the
F-scores shown in Table 4 are the average over-
all F-scores over the 10 test sets. Again we con-
ducted a two tail paired t-test
4
between the base-
lines and our proposed methods to measure the
performance gains. It was observed that all our
proposed methods are statistically significant over
the baselines with a confidence of 95% (i.e with
p value 0.05). In Table 4 we italicize all our best
performing methods and highlight in bold the best
among them. From the results it is evident that our
proposed methods obtain significantly better F-
scores over all the baselines with EM-class-corpus
achieving the best F-score with a gain of 3.21%,
2.9%, 39.03% and 38.7% over PMI-lex, BoW,
WNA-lex and NRC-lex respectively. Our findings
reconfirm previous findings in the literature that
emotion lexicon based features improve over cor-
pus based n-gram features in a emotion classifica-
tion task. Also our findings suggest that domain
specific automatic lexicons are significantly better
over manually crafted lexicons.
6 Conclusions and Future Work
We proposed a set of methods to automatically ex-
tract a word-emotion lexicon from an emotion la-
belled corpus. Thereafter we used the lexicons to
Table 4: Overall F-scores
Method Avg Overall F-
score
Baselines
WNA-lex 13.17%
NRC-lex 13.50%
Bow 49.30%
PMI-lex 48.99%
Our automatic lexicons
TF-lex 51.45%
EMallclass-lex 51.38%
EMclass-corpus-lex 52.20%
derive features for text representation and showed
that lexicon based features significantly outper-
form the standard BoW features in the emotion
classification of tweets. Furthermore our lexicons
achieve significant improvements over the general
purpose lexicons and the PMI based automatic
lexicon in the classification experiments. In fu-
ture we intend to leverage the lexicons to design
different text representations and also test them
on emotional content from other domains. Auto-
matically generating human-interpretable models
(e.g., (Balachandran et al., 2012)) to accompany
emotion classifier decisions is another interesting
direction for future work.
References
Richa Bhayani Alec Go and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing.
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine learn-
ing for text-based emotion prediction. In Proceed-
19
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, HLT ?05, pages 579?586, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
S. Aman and S. Szpakowicz. 2008. Using roget?s the-
saurus for fine-grained emotion recognition. In In-
ternational Joint Conference on Natural Language
Processing.
Vipin Balachandran, Deepak P, and Deepak Khemani.
2012. Interpretable and reconfigurable clustering
of document datasets by deriving word-based rules.
Knowl. Inf. Syst., 32(3):475?503.
Johan Bollen, Alberto Pepe, and Huina Mao. 2009.
Modelling public mood and emotion : Twitter senti-
ment and socio-economic phenomena. In CoRR.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of the 2010
43rd Hawaii International Conference on System
Sciences, Washington, DC, USA.
P. Deepak and Sutanu Chakraborti. 2012. Finding rel-
evant tweets. In WAIM, pages 228?240.
P. Deepak and Karthik Visweswariah. 2014. Unsu-
pervised solution post identification from discussion
forums. In ACL.
P. Deepak, Karthik Visweswariah, Nirmalie Wiratunga,
and Sadiq Sani. 2012. Two-part segmentation of
text documents. In CIKM, pages 793?802.
Lars E.Holzman and Pottenger William M. 2003.
Classification of emotions in internet chat : An
application of machine learning using speech
phonemes. Technical report, Technical report,
Leigh University.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3):169?200.
Virginia Francisco and Pablo Gervas. 2006. Auto-
mated mark up of affective information in english
text. Text, Speech and Dialouge, volume 4188 of
Lecture Notes in Computer Science:375?382.
David John, Anthony C. Boucouvalas, and Zhe Xu.
2006. Representing emotinal momentum within ex-
pressive internet communication. In In Proceed-
ings of the 24th IASTED international conference on
Internet and multimedia systems and applications,
pages 183-188, Anaheim, CA, ACTA Press.
J. Johnson J. Guthrie K. Roberts, M.A. Roach and S.M.
Harabagiu. 2012. ?empatweet: Annotating and de-
tecting emotions on twitter?,. In in Proc. LREC,
2012, pp.3806-3813.
Elsa Kim, Sam Gilbert, J.Edwards, and Erhardt Graeff.
2009. Detecting sadness in 140 characters: Senti-
ment analysis of mourning of michael jackson on
twitter.
Xiaoyong Liu and W Bruce Croft. 2005. Statistical
language modeling for information retrieval. Tech-
nical report, DTIC Document.
Chunling Ma, Helmut Prendinger, and Mitsuru
Ishizuka. 2005. Emotion estimation and reasoning
based on affective textual interaction. In First In-
ternational Conference on Affective Computing and
Intelligent Interaction (ACII-2005), pages 622-628,
Beijing, China.
Rada Mihalcea and Hugo Liu. 2006. A corpus-based
approach for finding happiness. In In AAAI-2006
Spring Symposium on Computational Approaches to
Analysing Weblogs, pages 139-144. AAAI press.
Saif M. Mohammad and Tony Yang. 2011. Tracking
seniment in mail : How genders differ on emotional
axes. In In Proceedings of the 2nd Workshop on
Computational Approaches to Subjectivity and Sen-
timent Analysis(WASSA 2011), pages 70- 79, Port-
land, Oregon. Association for Computational Lin-
guistics.
Saif Mohammad. 2012a. #emotional tweets. In
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012).
Saif M. Mohammad. 2012b. Portable features for clas-
sifying emotional text. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 587-591, Montreal , Canada.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2010. Recognition of affect, judg-
ment, and appreciation in text. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10, pages 806?814, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
D. Kirson P. Shaver, J. Schwartz. 1987. Emotion
knowledge: Further exploration of a prototype ap-
proach. Journal of Personality and Social Psychol-
ogy, Vol 52 no 6:1061 ? 1086.
W Parrott. 2001. Emotions in social psychology. Psy-
chology Press, Philadelphia.
Lisa Pearl and Mark Steyvers. 2010. Identifying emo-
tions, intentions and attitudes in text using a game
with a purpose. In In Proceedings of the NAACL-
HLT 2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text, Los
Abgeles, California.
R. Plutchik. 1980. A general psychoevolutionary the-
ory of emotion. In R. Plutchik & H. Kellerman
(Eds.), Emotion: Theory, research, and experience:,
Vol. 1. Theories of emotion (pp. 3-33). New York:
Academic:(pp. 3?33).
20
Ashequl Qadir and Ellen Riloff. 2013. Bootstrapped
learning of emotion hashtahs #hashtags4you. In
In the 4th Workshop on Computational Approaches
to Subjectivity, Sentiment & Social Media Analysis
(WASSA 2013).
Tapas Ray. 2011. The ?story? of digital excess in rev-
olutions of the arab spring. Journal of Media Prac-
tice, 12(2):189?196.
Peter D. Turney Saif M. Mohammad. 2013. Crowd-
sourcing a word-emotion association lexicon. Com-
putational Intelligence, 29 (3), 436-465, Wiley
Blackwell Publishing Ltd, 2013, 29(3):436?465.
Prendinger H. Ishizuka M. Shaikh, M.A.M., 2009. A
Linguistic Interpretation of the OCC Emotion Model
for Affect Sensing from Text, chapter 4, pages 45?73.
Julia Skinner. 2011. Social media and revolu-
tion: The arab spring and the occupy movement
as seen though three information studies paradigms.
Sprouts: Working papers on Information Systems,
11(169).
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: an affective extension of wordnet.
Technical report, ITC-irst, Istituto per la Ricerca
Scienti?ca e Tecnologica I-38050 Povo Trento Italy.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter ?big
data? for automatic emotion identification. In Pro-
ceedings of the 2012 ASE/IEEE International Con-
ference on Social Computing and 2012 ASE/IEEE.
C. Yang, K. H. Y. Lin, and H. H. Chen. 2007. Emo-
tion classification using web blog corpora. In Pro-
ceedings of the IEEE/WIC/ACM International Con-
ference on Web Intelligence, WI ?07, pages 275?278,
Washington, DC, USA. IEEE Computer Society.
Liu Wenyin Qing Li Yanghui Rao, Xiaojun Quan and
Mingliang Chen. 2013. Building word-emotion
mapping dictionary for online news. In In Pro-
ceedings of the 4th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis, WASSA 2013.
21
