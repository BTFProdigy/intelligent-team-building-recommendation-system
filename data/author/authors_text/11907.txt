Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280?1288,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Improving Nominal SRL in Chinese Language with Verbal SRL In-
formation and Automatic Predicate Recognition 
 
Junhui Li?   Guodong Zhou??   Hai Zhao??  Qiaoming Zhu?  Peide Qian? 
? Jiangsu Provincial Key Lab for Computer Information Processing Technologies
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
? Department of Chinese, Translation and Linguistics 
City University of HongKong, China 
Email: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn
 
                                                          
? Corresponding author 
Abstract 
This paper explores Chinese semantic role la-
beling (SRL) for nominal predicates. Besides 
those widely used features in verbal SRL, 
various nominal SRL-specific features are 
first included. Then, we improve the perform-
ance of nominal SRL by integrating useful 
features derived from a state-of-the-art verbal 
SRL system. Finally, we address the issue of 
automatic predicate recognition, which is es-
sential for a nominal SRL system. Evaluation 
on Chinese NomBank shows that our research 
in integrating various features derived from 
verbal SRL significantly improves the per-
formance. It also shows that our nominal SRL 
system much outperforms the state-of-the-art 
ones. 
1. Introduction 
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning. 
Due to the difficulty in deep semantic parsing, 
most of previous work focuses on shallow se-
mantic parsing, which assigns a simple structure 
(such as WHO did WHAT to WHOM, WHEN, 
WHERE, WHY, HOW) to each predicate in a 
sentence. In particular, the well-defined seman-
tic role labeling (SRL) task has been drawing 
more and more attention in recent years due to 
its importance in deep NLP applications, such as 
question answering (Narayanan and Harabagiu, 
2004), information extraction (Surdeanu et al, 
2003), and co-reference resolution (Ponzetto and 
Strube, 2006). Given a sentence and a predicate 
(either a verb or a noun) in it, SRL recognizes 
and maps all the constituents in the sentence into 
their corresponding semantic arguments (roles) 
of the predicate. According to the predicate 
types, SRL could be divided into SRL for verbal 
predicates (verbal SRL, in short) and SRL for 
nominal predicates (nominal SRL, in short). 
During the past few years, verbal SRL has 
dominated the research on SRL with the avail-
ability of FrameNet (Baker et al, 1998), Prop-
Bank (Palmer et al, 2005), and the consecutive 
CoNLL shared tasks (Carreras and M?rquez, 
2004 & 2005) in English language. As a com-
plement to PropBank on verbal predicates, 
NomBank (Meyers et al, 2004) annotates nomi-
nal predicates and their corresponding semantic 
roles using similar semantic framework as 
PropBank. As a representative, Jiang and Ng 
(2006) pioneered the exploration of various 
nominal SRL-specific features besides the tradi-
tional verbal SRL-related features on NomBank. 
They achieved the performance of 72.73 and 
69.14 in F1-measure on golden and automatic 
syntactic parse trees, respectively, given golden 
nominal predicates. 
For SRL in Chinese, Sun and Jurafsky (2004) 
and Pradhan et al (2004) pioneered the research 
on Chinese verbal and nominal SRLs, respec-
tively, on small private datasets. Taking the ad-
vantage of recent release of Chinese PropBank 
(Xue and Palmer, 2003) and Chinese NomBank 
(Xue, 2006a), Xue and his colleagues (Xue and 
Palmer 2005; Xue 2006b; Xue, 2008) pioneered 
the exploration of Chinese verbal and nominal 
SRLs, given golden predicates. Among them, 
Xue and Palmer (2005) studied Chinese verbal 
SRL using Chinese PropBank and achieved the 
performance of 91.3 and 61.3 in F1-measure on 
golden and automatic syntactic parse trees, re-
spectively. Xue (2006b) extended their study on 
Chinese nominal SRL and attempted to improve 
the performance of nominal SRL by simply in-
1280
 cluding the Chinese PropBank training instances 
into the training data for nominal SRL on Chi-
nese NomBank. However, such integration was 
empirically proven unsuccessful due to the dif-
ferent nature of certain features for verbal and 
nominal SRLs. Xue (2008) further improved the 
performance on both verbal and nominal SRLs 
with a better syntactic parser and new features. 
Ding and Chang (2008) focused on argument 
classification for Chinese verbal predicates with 
hierarchical feature selection strategy. They 
achieved the classification precision of 94.68% 
on golden parse trees on Chinese PropBank. 
This paper focuses on Chinese nominal SRL. 
This is done by adopting a traditional verbal 
SRL architecture to handle Chinese nominal 
predicates with additional nominal SRL-specific 
features. Moreover, we significantly enhance the 
performance of nominal SRL by properly inte-
grating various features derived from verbal 
SRL. Finally, this paper investigates the effect of 
automatic nominal predicate recognition on the 
performance of Chinese nominal SRL. Although 
previous research (e.g. CoNLL?2008) in English 
nominal SRL reveals the importance of auto-
matic predicate recognition, there has no re-
ported research on automatic predicate 
recognition in Chinese nominal SRL. 
The rest of this paper is organized as follows: 
Section 2 introduces Chinese NomBank while 
the baseline nominal SRL system is described in 
Section 3 with traditional and nominal SRL-
specific features. Then, the baseline nominal 
SRL system is improved by integrating useful 
features derived from verbal SRL (Section 4) 
and extended with automatic recognition of 
nominal predicates (Section 5). Section 6 gives 
experimental results and discussion. Finally, 
Section 7 concludes the paper.    
2. Chinese NomBank 
Chinese NomBank (Xue, 2006a) adopts similar 
semantic framework as NomBank, and focuses 
on Chinese nominal predicates with their argu-
ments in Chinese TreeBank. The semantic ar-
guments include:  
1) Core arguments: Arg0 to Arg5. Generally, 
Arg0 and Arg1 denotes the agent and the 
patient, respectively, while arguments from 
Arg2 to Arg5 are predicate-specific.  
2) Adjunct arguments, which are universal to 
all predicates, e.g. ArgM-LOC for locative, 
and ArgM-TMP for temporal. 
 
All the arguments are annotated on parse tree 
nodes with their boundaries aligning with the 
spans of tree nodes. Figure 1 gives an example 
with two nominal predicates and their respective 
arguments, while the nominal predicate ???
/investment? has two core arguments, ?NN(??
/foreign businessman)? as Arg0 and ?NN(??
/bank)? as Arg1, and the other nominal predicate 
??? /loan? also has two core arguments, 
?NP(???? /Bank of China)? as Arg1 and 
Figure 1: Two nominal predicates and their arguments in the style of NomBank. 
? 
?? ?? ??
??
???
P 
NN NN NN
VV
NN NN 
Arg0/Rel1 Rel1 Arg1/Rel1
NP 
PP 
Arg0/Rel2 
ArgM-MNR/Rel2 Rel2 
NP 
CD
QP
NP
VP
VP
??? ?? 
? 
NN NN 
PU 
NP 
Arg1/Rel2 
IP
?? ?? 
Sup/Rel2
Bank of China 
to 
Foreign  Investment  Bank 
provide
4 billion
RMB loan 
. 
Bank of China provides 4 billion RMB loan to Foreign Investment Bank. 
1281
 ?PP(??????? /to Foreign Investment 
Bank)? as Arg0,  and 1 adjunct argument, 
?NN(???/RMB)? as ArgM-MNR, denoting 
the manner of loan. It is worth noticing that 
there is a (Chinese) NomBank-specific label in 
Figure 1, Sup (support verb) (Xue, 2006a), in 
helping introduce the arguments, which occur 
outside the nominal predicate-headed noun 
phrase. This is illustrated by the nominal predi-
cate ???/loan?, whose Arg0 and Arg1 are both 
realized outside the nominal predicate-headed 
noun phrase, NP(????????/4 billion 
RMB loan). Normally, a verb is marked as a 
support verb only when it shares some argu-
ments with the nominal predicate. 
3. Baseline: Chinese Nominal SRL 
Popular SRL systems usually formulate SRL as 
a classification problem, which annotates each 
constituent in a parse tree with a semantic role 
label or with the non-argument label NULL. Be-
sides, we divide the system into three consecu-
tive phases so as to overcome the imbalance 
between the training instances of the NULL 
class and those of any other argument classes.  
Argument pruning. Here, several heuristic 
rules are adopted to filter out constituents, which 
are most likely non-arguments. According to the 
argument structures of nominal predicates, we 
categorize arguments into two types: arguments 
inside NP (called inside arguments) and argu-
ments introduced via a support verb (called out-
side arguments), and handle them separately. 
For the inside arguments, the following three 
heuristic rules are applied to find inside argu-
ment candidates: 
z All the sisters of the predicate are candi-
dates. 
z If a CP or DNP node is a candidate, its chil-
dren are candidates too. 
z For any node X, if its parent is an ancestral 
node of the predicate, and the internal 
nodes along the path between X and the 
predicate are all NPs, then X is a candidate. 
For outside arguments, we look for the sup-
port verb of the focus nominal predicate, and 
then adopt the rules as proposed in Xue and 
Palmer (2005) to find the candidates for the sup-
port verb, since outside argument candidates are 
introduced via this support verb. That to say, the 
argument candidates of the support verb are re-
garded as outside argument candidates of the 
nominal predicate. However, as support verbs 
are not annotated explicitly in the testing phase, 
we identify intervening verbs as alternatives to 
support verbs in both training and testing phases 
with the path between the nominal predicate and 
intervening verb in the form of 
?VV<VP>[NP>]+NN?, where ?[NP>]+? denotes 
one or more NPs.  Our statistics on Chinese 
NomBank shows that 51.96% of nominal predi-
cates have no intervening verb while 48.04% of 
nominal predicates have only one intervening 
verb. 
Taken the nominal predicate ???/loan? in 
Figure 1 as an example, NN(???/RMB) and 
QP(??? /4 billion) are identified as inside 
argument candidates, while PP(??????
?/to Foreign Investment Bank) and NP(???
?/Bank of China) are identified as outside ar-
gument candidates via the support verb VV(?
?/provide). 
Argument identification. A binary classifier 
is applied to determine the candidates as either 
valid arguments or non-arguments. It is worth 
pointing out that we only mark those candidates 
that are most likely to be NULL (with probabil-
ity > 0.90) as non-arguments. Our empirical 
study shows that this little trick much benefits 
nominal SRL, since argument identification for 
nominal predicates is much more difficult than 
that for verbal predicates and thus many argu-
ments would have been falsely marked as non-
arguments if the threshold is set as 0.5. 
Argument classification. A multi-class classi-
fier is employed to label identified arguments 
with specific argument labels (including the 
NULL class for non-argument). 
In the following, we first adapt some tradi-
tional features, which have been proven effec-
tive in verbal SRL, to nominal SRL, and then 
introduce several nominal SRL-specific features. 
3.1. Traditional Features 
Using the feature naming convention as adopted 
in Jiang and Ng (2006), Table 1 lists the tradi-
tional features, where ?I? and ?C? indicate the 
features for argument identification and classifi-
cation, respectively. Among them, the predicate 
class (b2) feature was first introduced in Xue 
and Palmer (2005) to overcome the imbalance of 
the predicate distribution in that some predicates 
can be only found in the training data while 
some predicates in the testing data are absent 
from the training data. In particular, the verb 
class is classified along three dimensions: the 
number of arguments, the number of framesets 
and selected syntactic alternations. For example, 
1282
 the verb class of ?C1C2a? means that it has two 
framesets, with the first frameset having one 
argument and the second having two arguments. 
The symbol ?a? in the second frameset repre-
sents a type of syntactic alternation. 
 
Feature Remarks: b1-b5(C, I), b6-b7(C) 
b1 Predicate: the nominal predicate itself. (??
/loan) 
b2 Predicate class: the verb class that the predi-
cate belongs to. (C4a) 
b3 Head word (b3H) and its POS (b3P).  (??
/bank, NN) 
b4 Phrase type: the syntactic category of the 
constituent. (NP) 
b5 Path: the path from the constituent to the 
nominal predicate. 
 (NP<IP>VP>VP>NP>NP>NN) 
b6 Position: the positional relationship of the 
constituent with the predicate. ?left? or 
?right?. (left) 
b7 First word (b7F) and last word (b7L) of the 
focus constituent. (??/China, ??/bank) 
Combined features: b11-b14(C, I), b15(C) 
b11: b1&b4;       b12: b1&b3H;       b13: b2&b4;  
b14: b2&b3H;    b15: b5&b6 
Table 1: Traditional features and their instantiations 
for argument identification and classification, with 
NP(????/Bank of China)  as the focus constitu-
ent and NN(??/loan) as the nominal predicate, re-
garding Figure 1. 
3.2. Nominal SRL-specific Features 
To capture more useful information in the predi-
cate-argument structure, we also study addi-
tional features which provide extra information. 
Statistics on Chinese NomBank show that about 
40% of pruned inside candidates are arguments. 
Since inside arguments usually locate near to the 
nominal predicate, its surroundings are expected 
to be helpful in SRL. Table 2 shows the features 
in better capturing the details between inside 
arguments and nominal predicates. Specially, 
features ai6 and ai7 are sister-related features, 
inspired by the features related with the 
neighboring arguments in Jiang and Ng (2006). 
Statistics on NomBank and Chinese Nom-
Bank show that about 20% and 22% of argu-
ments are introduced via a support verb, 
respectively. Since a support verb pivots outside 
arguments and the nominal predicate on its two 
sides, support verbs play an important role in 
labeling these arguments. Here, we also identify 
intervening verbs as alternatives to support verbs 
since support verbs are not explicitly in the test-
ing phase. Table 3 lists the intervening verb-
related features (ao1-ao4, ao11-ao14) employed 
in this paper. 
 
Feature Remarks 
ai1 Whether the focus constituent is adjacent to 
the predicate. Yes or No. (Yes) 
ai2 The headword (ai2H) and pos (ai2P) of the 
predicate?s nearest right sister. (??/bank, 
NN) 
ai3 Whether the predicate has right sisters. Yes 
or No. (Yes) 
ai4 Compressed path of b5: compressing se-
quences of identical labels into one. 
(NN<NP>NN) 
ai5 Whether the predicate has sisters. Yes or 
No. (Yes) 
ai6 For each sister of the focus constituent, 
combine b3H&b4&b5&b6. ( ? ?
/bank&NN & NN<NP>NN&right) 
ai7 Coarse version of ai6, b4&b6. (NN&right) 
Table 2: Additional features and their instantiations  
for inside argument candidates, with ?NN(??
/foreign businessman)? as the focus constituent and 
?NN(?? /investment)? as the nominal predicate, 
regarding Figure1. 
 
Feature Remarks 
ao1 Intervening verb itself. (??/provide) 
ao2 The verb class that the intervening verb 
belongs to. (C3b) 
ao3 The path from the focus constituent to the 
intervening verb. (NP<IP>VP>VP>VV) 
ao4 The compressed path of ao3: compressing 
sequences of identical labels into one. 
(NP<IP>VP>VV) 
Combined features: ao11-ao14 
ao11: ao1&ao3;      ao12: ao1&ao4;    
ao13: ao2&ao3;      ao14: ao2&ao4. 
Table 3: Additional features and their instantiations 
for outside argument candidates, with ?NP(????
/Bank of China)? as the focus constituent and ???
/loan? as the nominal predicate, regarding Figure1. 
Feature selection. Some Features proposed 
above may not be effective in tasks of identifica-
tion and classification. We adopt the greedy fea-
ture selection algorithm as described in Jiang 
and Ng (2006) to pick up positive features em-
pirically and incrementally according to their 
contributions on the development data. The al-
gorithm repeatedly selects one feature each time 
which contributes most, and stops when adding 
any of the remaining features fails to improve 
the performance. As far as the SRL task con-
cerned, the whole feature selection process could 
be done as follows: 1). Feature selection for ar-
gument identification: run the selection algo-
1283
 rithm with the basic set of features (b1-b5, b11-
b14) to pick up effective features from (ai1-ai7, 
ao1-ao4, ao11-ao14); 2). Feature selection for 
argument classification: fix the output returned 
in step1 as the feature set of argument identifica-
tion, and run the selection algorithm with the 
basic set of features (b1-b7, b11-b15) to select 
positive features from (ai1-ai7, ao1-ao4, ao11-
ao14) for argument classification. 
4. Integrating Features derived from 
Verbal SRL 
Since Chinese PropBank and NomBank are an-
notated on the same data set with the same lexi-
cal guidelines (e.g. frame files), it may be 
interesting to investigate the contribution of 
Chinese verbal SRL on the performance of Chi-
nese nominal SRL. In the frame files, argument 
labels are defined with regard to their semantic 
roles to the predicate, either a verbal or nominal 
predicate. For example, in the frame file of 
predicate ???/loan?, the borrower is always 
labeled with Arg0 and the lender labeled with 
Arg1. This can be demonstrated by the follow-
ing two sentences: ???/loan? is annotated as a 
nominal and a verbal predicate in S1 and S2, 
respectively. 
S1 [Arg1 ????/Bank of China] [Arg0 ???
????/to Foreign Investment Bank] ??
/provide [Rel??/loan] 
S2  [Arg0 ????/Bank of China] [Arg1 ???
????/from Foreign Investment Bank] [Rel 
??/loan] 
Therefore, it is straightforward to augment 
nominal training instances with verbal ones. 
However, Xue (2006b) found that simply adding 
the training instances for verbal SRL to the 
training data for nominal SRL and indiscrimi-
nately extracting the same features in both ver-
bal and nominal SRLs hurt the performance. 
This may be due to that certain features (e.g. the 
path feature) are much different for verbal and 
nominal SRLs. This can be illustrated in sen-
tences S1 and S2: the verbal instances in S2 are 
negative for semantic role labeling of the nomi-
nal predicate ???/loan? in S1, since ????
?/Bank of China? takes opposite roles in S1 
and S2. So does ????????/(from/to) 
Foreign Investment Bank?. 
Although several support verb-related features 
(ao1-ao4, ao11-ao14) have been proposed, one 
may still ask how large the role support verbs 
can play in nominal SRL. It is interesting to note 
that outside arguments and the highest NP 
phrase headed by the nominal predicate are also 
annotated as arguments of the support verb in 
Chinese PropBank. For example, Chinese Prop-
Bank marks ?????/Bank of China? as Arg0 
and ?????????/4 billion RMB loan? 
as Arg1 for verb ???/provide? in Figure1. Let 
OA be the outside argument, VV be the support 
verb, and NP be the highest NP phrase headed 
by the nominal predicate NN, then there exists a 
pattern ?OA VV NN? in the sentence, where the 
support verb VV plays a certain role in trans-
ferring roles between OA and NN. For example, 
if OA is the agent of VV, then OA is also the 
agent of phrase VP(VV NN). Like the example 
in Figure1, supposing a NP is the agent of sup-
port verb ???/provide? as well as VP phrase 
(??????????? /provide 4 billion 
RMB loan?), we can infer that the NP is the 
lender of the nominal predicate ???/loan? in-
dependently on any other information, such as 
the NP content and the path from the NP to the 
nominal predicate ???/loan?.  
Let C be the focus constituent, V be the inter-
vening verb, and NP be the highest NP headed 
by the nominal predicate. Table 4 shows the fea-
tures (ao5-ao8, p1-p7) derived from verbal SRL. 
In this paper, we develop a state-of-the-art Chi-
nese verbal SRL system, similar to the one as 
shown in Xue (2008), to achieve the goal. Based 
on golden parse trees on Chinese PropBank, our 
Chinese verbal SRL system achieves the per-
formance of 92.38 in F1-measure, comparable to 
Xue (2008) which achieved the performance of 
92.0 in F1-measure. 
 
Feature Remarks 
ao5 Whether C is an argument for V. Yes or No
ao6 The semantic role of C for V. 
ao7 Whether NP is an argument for V. Yes or No
ao8 The semantic role of NP for V. 
Combined features: p1-p7 
p1: ao1&ao5;         p2: ao1&ao6;    p3: ao1&ao5&b1; 
p4: ao1&ao6&b1;  p5: ao1&apo7;  p6: ao1&ao8;  
p7: ao5&ao7. 
Table 4: Features derived from verbal SRL. 
5. Automatic Predicate Recognition 
Unlike Chinese PropBank where almost all the 
verbs are annotated as predicates, Chinese Nom-
Bank only marks those nouns having arguments 
as predicates. Statistics on Chinese NomBank 
show that only 17.5% of nouns are marked as 
predicates. It is possible that a noun is a predi-
1284
 cate in some cases but not in others. Previous 
Chinese nominal SRL systems (Xue, 2006b; 
Xue, 2008) assume that nominal predicates have 
already been manually annotated and thus are 
available. To our best knowledge, there is no 
report on addressing automatic recognition of 
nominal predicates on Chinese nominal SRL. 
Automatic recognition of nominal predicates 
can be cast as a binary classification (e.g., Predi-
cate vs. Non-Predicate) problem. This paper 
employs the convolution tree kernel, as proposed 
in Collins and Duffy (2001), on automatic rec-
ognition of nominal predicates. 
Given the convolution tree kernel, the key 
problem is how to extract a parse tree structure 
from the parse tree for a nominal predicate can-
didate. In this paper, the parse tree structure is 
constructed as follows: 1) starting from the 
predicate candidate?s POS node, collect all of its 
sister nodes (with their headwords); 2). recur-
sively move one level up and collect all of its 
sister nodes (with their headwords) till reaching 
a non-NP node. Specially, in order to explicitly 
mark the positional relation between a node and 
the predicate candidate, all nodes on the left side 
of the candidate are augmented with tags 1 and 2 
for nodes on the right side. Figure 2 shows an 
example of the parse tree structure with regard 
to the predicate candidate ???/loan? as shown 
in Figure 1. 
In our extra experiments we found global sta-
tistic features (e.g. g1-g5) about the predicate 
candidate are helpful in a feature vector-based 
method for predicate recognition. Figure 2 
makes an attempt to utilize those features in ker-
nel-based method. We have explored other ways 
to include those global features. However, the 
way in Figure 2 works best.  
 
 
Let the predicate candidate be w0, and its left 
and right neighbor words be w-1 and w1, respec-
tively. The five global features are defined as 
follows. 
g1 Whether w0 is ever tagged as a verb in the 
training data? Yes or No. 
g2 Whether w0 is ever annotated as a nominal 
predicate in the training data? Yes or No. 
g3 The most likely label for w0 when it occurs 
together with w-1 and w1. 
g4 The most likely label for w0 when it occurs 
together with w-1. 
g5 The most likely label for w0 when it occurs 
together with w1. 
6. Experiment Results and Discussion 
We have evaluated our Chinese nominal SRL 
system on Chinese NomBank with Chinese 
PropBank 2.0 as its counterpart. 
6.1. Experimental Settings 
This version of Chinese NomBank consists of 
standoff annotations on the files (chtb_001 to 
1151.fid) of Chinese Penn TreeBank 5.1. Fol-
lowing the experimental setting in Xue (2008), 
648 files (chtb_081 to 899.fid) are selected as 
the training data, 72 files (chtb_001 to 040.fid 
and chtb_900 to 931.fid) are held out as the test 
data, and 40 files (chtb_041 to 080.fid) as the 
development data, with 8642, 1124, and 731 
propositions, respectively. 
As Chinese words are not naturally segmented 
in raw sentences, two Chinese automatic parsers 
are constructed: word-based parser (assuming 
golden word segmentation) and character-based 
parser (with automatic word segmentation). 
Here, Berkeley parser (Petrov and Klein, 2007)1 
is chosen as the Chinese automatic parser. With 
regard to character-based parsing, we employ a 
Chinese word segmenter, similar to Ng and Low 
(2004), to obtain the best automatic segmenta-
tion result for a given sentence, which is then 
fed into Berkeley parser for further syntactic 
parsing. Both the word segmenter and Berkeley 
parser are developed with the same training and 
development datasets as our SRL experiments. 
The word segmenter achieves the performance 
of 96.1 in F1-measure while the Berkeley parser 
gives a performance of 82.5 and 85.5 in F1-
measure on golden and automatic word segmen-
tation, respectively2.  
??? 1 In addition, SVMLight with the tree kernel 
function (Moschitti, 2004) 3  is selected as our 
classifier. In order to handle multi-classification 
                                                          
1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ 
2 POSs are not counted in evaluating the performance of 
word-based syntactic parser, but they are counted in evalu-
ating the performance of character-based parser. Therefore 
the F1-measure for the later is higher than that for the for-
mer. 
3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 
Figure 2: Semantic sub-tree for nominal predicate
RMB 
?? 
loan 
?? 1 
provide 
??? 1 
4 billion 
VV1 
NN1 NN 
NPQP1 
NP
VP 
g1 ?. g5
1285
 problem in argument classification, we apply the 
one vs. others strategy, which builds K classifi-
ers so as to separate one class from all others. 
For argument identification and classification, 
we adopt the linear kernel and the training pa-
rameter C is fine-tuned to 0.220. For automatic 
recognition of nominal predicates, the training 
parameter C and the decay factor ?  in the con-
volution tree kernel are fine-tuned to 2.0 and 0.2, 
respectively. 
6.2. Results with Golden Parse Trees and 
Golden Nominal Predicates 
Effect of nominal SRL-specific features 
 
 Rec.(%) Pre.(%) F1 
traditional features 62.83 73.58 67.78 
+nominal SRL-specific  
features 
69.90 75.11 72.55 
Table 5: The performance of nominal SRL on the 
development data with golden parse trees and golden 
nominal predicates 
After performing the greedy feature selection 
algorithm on the development data, features 
{ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as pro-
posed in Section 3.2, are selected consecutively 
for argument identification, while features {ai7, 
ao1, ai1, ao2, ai5, ao4} are selected for argument 
classification. Table 5 presents the SRL results 
on the development data. It shows that nominal 
SRL-specific features significantly improve the 
performance from 67.78 to 72.55 ( ) 
in F1-measure. 
05.0;2 <p?
Effect of features derived from verbal SRL 
 
Features Rec.(%) Pre.(%) F1 
baseline 67.86 73.63 70.63  
+ao5 68.15 73.60 70.77 (+0.14)
+ao6 67.66 72.80 70.14 (-0.49)
+ao7 68.20 75.41 71.62 (+0.99)
+ao8 68.30 75.39 71.67 (+1.04)
+p1 67.91 74.40 71.00 (+0.37)
+p2 67.76 74.20 70.83 (+0.20)
+p3 67.96 74.69 71.16 (+0.53)
+p4 68.01 74.18 70.96 (+0.33)
+p5 68.01 75.01 71.39 (+0.76)
+p6 68.20 75.12 71.49 (+0.86)
+p7 68.40 75.70 71.87 (+1.24)
Table 6: Effect of features derived from verbal SRL 
on the performance of nominal SRL on the test data 
with golden parse trees and golden nominal predi-
cates. The first row presents the performance using 
traditional and nominal SRL-specific features. 
 
 
 Rec.(%) Pre.(%) F1 
baseline  67.86 73.63 70.63 
+features derived 
from verbal SRL
68.40 77.51 72.67 
Xue (2008) 66.1 73.4 69.6 
Table 7: The performance of nominal SRL on the test 
data with golden parse trees and golden nominal 
predicates 
 
Table 6 shows the effect of features derived 
from verbal SRL in an incremental way. It 
shows that only the feature ao6 has negative ef-
fect due to its strong relevance with intervening 
verbs and thus not included thereafter. Table 7 
shows the performance on the test data with or 
without using the features derived from the ver-
bal SRL system. It shows these features signifi-
cantly improve the performance ( ) 
on nominal SRL. Table 7 also shows our system 
outperforms Xue (2008) by 3.1 in F1-measure. 
05.0;2 <p?
6.3. Results with Automatic Parse Trees 
and Golden Nominal Predicates 
In previous section we have assumed the avail-
ability of golden parse trees during the testing 
process. Here we conduct experiments on auto-
matic parse trees, using the Berkeley parser. 
Since arguments come from constituents in 
parse trees, those arguments, which do not align 
with any syntactic constituents, are simply dis-
carded. Moreover, for any nominal predicate 
segmented incorrectly by the word segmenter, 
all its arguments are unable to be labeled neither. 
Table 8 presents the SRL performance on the 
test data by using automatic parse trees. It shows 
that the performance drops from 72.67 to 60.87 
in F1-measure when replacing golden parse trees 
with word-based automatic ones, partly due to 
the absence of 6.9% arguments in automatic 
trees, and wrong POS tagging of nominal predi-
cates. Table 8 also compares our system with 
Xue (2008). It shows that our system also out-
performs Xue (2008) on Chinese NomBank. 
 Rec. (%) Pre. (%) F1 
This paper 56.95(53.55) 66.74(66.69) 60.87(59.40)
Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3) 
Table 8: The performance of nominal SRL on the test 
data with automatic parse trees and golden predicates. 
Here, the numbers outside the parentheses indicate 
the performance using a word-based parser, while the 
numbers inside indicate the performance using a 
character-based parser4. 
                                                          
4 About 1.6% nominal predicates are mistakenly segmented 
by the character-based parser, thus their arguments are 
missed directly. 
1286
 6.4. Results with Automatic Nominal Predi-
cates 
So far nominal predicates are assumed to be 
manually annotated and available. Here we turn 
to a more realistic scenario in which both the 
parse tree and nominal predicates are automati-
cally obtained. In the following, we first report 
the results of automatic nominal predicate rec-
ognition and then the results of nominal SRL on 
automatic recognition of nominal predicates. 
Results of nominal predicate recognition 
Parses g1-g5 Rec.(%) Pre.(%) F1 
no 91.46 88.93 90.18 golden 
yes 92.62 89.36 90.96 
word-based yes 86.39 81.80 84.03 
character-based yes 84.79 81.94 83.34 
Table 9: The performance of automatic nominal 
predicate recognition on the test data 
 
Table 9 lists the predicate recognition results, 
using the parse tree structure, as shown in Sec-
tion 5, and the convolution tree kernel, as pro-
posed in Collins and Duffy (2001). The second 
column (g1-g5) indicates whether the global fea-
tures (g1-g5) are included in the parse tree struc-
ture. We have also defined a simple rule that 
treats a noun which is ever a verb or a nominal 
predicate in the training data as a nominal predi-
cate. Based on golden parse trees, the rule re-
ceives the performance of 81.40 in F1-measure. 
This suggests that our method significantly out-
performs the simple rule-based one. Table 9 also 
shows that: 
z As a complement to local structural informa-
tion, global features improve the performance 
of automatic nominal predicate recognition 
by 0.78 in F1-measure. 
z The word-based syntactic parser decreases 
the F1-measure from 90.96 to 84.03, mostly 
due to the POSTagging errors between NN 
and VV, while the character-based syntactic 
parser further drops the F1-measure by 0.69, 
due to automatic word segmentation. 
Results with automatic predicates 
 
Parses Predicates Rec.(%) Pre.(%) F1 
golden 68.40 77.51 72.67 golden 
automatic 65.07 74.65 69.53 
golden 55.95 66.74 60.87 word-
based automatic 52.67 59.56 55.90 
golden 53.55 66.69 59.40 character-
based automatic 50.66 59.60 54.77 
Table 10: The performance of nominal SRL on the 
test data with the choices of golden/automatic parse 
trees and golden/automatic predicates 
In order to have a clear performance comparison 
among nominal SRL on golden/automatic parse 
trees and golden/automatic predicates, Table 10 
lists all the results in those scenarios. 
6.5. Comparison 
Chinese nominal SRL vs. Chinese verbal SRL 
Comparison with Xue (2008) shows that the per-
formance of Chinese nominal SRL is about 20 
lower (e.g. 72.67 vs. 92.38 in F1-measure) than 
that of Chinese verbal SRL, partly due to the 
smaller amount of annotated data (about 1/5) in 
Chinese NomBank than that in Chinese Prop-
Bank. Moreover, according to Chinese Nom-
Bank annotation criteria (Xue 2006a), even 
when a noun is a true deverbal noun, not all of 
its modifiers are legitimate arguments or ad-
juncts of this predicate. Only arguments that can 
co-occur with both the nominal and verbal forms 
of the predicate are considered in the NomBank 
annotation. This means that the judgment of ar-
guments is semantic rather than syntactic. These 
facts may also partly explain the lower nominal 
SRL performance, especially the performance of 
argument identification. This can be illustrated 
by the statistics on the development data that 
96% (40%) of verbal (nominal) predicates? sis-
ters are annotated as arguments. Finally, the 
predicate-argument structure of nominal predi-
cates is more flexible and complicated than that 
of verbal predicates as illustrated in Xue (2006a). 
Chinese nominal SRL vs. English nominal 
SRL 
Liu and Ng (2007) reported the performance of 
77.04 and 72.83 in F1-measure on English Nom-
Bank when golden and automatic parse trees are 
used, respectively. Taking into account that Chi-
nese verbal SRL achieves comparable perform-
ance with English verbal SRL on golden parse 
trees, the performance gap between Chinese and 
English nominal SRL (e.g. 72.67 vs. 77.04 in 
F1-measure) presents great challenge for Chi-
nese nominal SRL. Moreover, while automatic 
parse trees only decrease the performance of 
English nominal SRL by about 4.2 in F1-
measure, automatic parse trees significantly de-
crease the performance of Chinese nominal SRL 
by more than 12 in F1-measure due to the much 
lower performance of Chinese syntactic parsing. 
7. Conclusion 
In this paper we investigate nominal SRL in 
Chinese language. In particular, some nominal 
SRL-specific features are included to improve 
1287
 the performance. Moreover, various features 
derived from verbal SRL are properly integrated 
into nominal SRL. Finally, a convolution tree 
kernel is adopted to address the issue of auto-
matic nominal predicates recognition, which is 
essential in a nominal SRL system.  
To our best knowledge, this is the first re-
search on 
1) Exploring Chinese nominal SRL on auto-
matic parse trees with automatic predicate 
recognition; 
2) Successfully integrating features derived 
from Chinese verbal SRL into Chinese nomi-
nal SRL with much performance improve-
ment. 
Acknowledgement  
This research was supported by Project 
60673041 and 60873150 under the National 
Natural Science Foundation of  China, Project 
2006AA01Z147 under the  ?863? National 
High-Tech Research and Development of China, 
and Project BK2008160 under the Natural Sci-
ence Foundation of the Jiangsu province of 
China. We also want to thank Dr. Nianwen Xue 
for share of the verb class file. We also want to 
thank the reviewers for insightful comments. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING-ACL 1998. 
Xavier Carreras and Lluis M?rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2004.  
Xavier Carreras and Lluis M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Michael Collins and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS 2001.  
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings 
of EMNLP 2008. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
role Labeling of NomBank: a Maximum Entropy 
Approach. In Proceedings of EMNLP 2006.  
Chang Liu and Hwee Tou Ng. 2007. Learning Predic-
tive Structures for Semantic Role Labeling of 
NomBank. In Proceedings of ACL 2007. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Yong, and R. Grishman. 2004. Anno-
tating Noun Argument Structure for NomBank. In 
Proceedings of LREC 2004.  
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Pro-
ceedings of ACL 2004. 
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In 
Proceedings of COLING 2004.  
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings 
of EMNLP 2004. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics. 
Slav Petrov. and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of 
NAACL 2007.  
Simone Paolo Ponzetto and Michael Strube. 2006. 
Semantic Role Labeling for Coreference Resolu-
tion. In Proceedings of EACL 2006. 
Sameer Pradhan, Honglin Sun, Wayne Ward, James 
H. Martin, and Dan Jurafsky. 2004. Parsing Ar-
guments of Nominalizations in English and Chi-
nese. In Proceedings of NAACL-HLT 2004.  
Honglin Sun and Daniel Jurafsky. 2004. Shallow 
Semantic Parsing of Chinese. In Proceedings of 
NAACL 2004.  
Mihai Surdeanu, Sanda Harabagiu, John Williams 
and Paul Aarseth. 2003. Using Predicate-argument 
Structures for Information Extraction. In Proceed-
ings of ACL 2003. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of 
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
the Propositions in the Penn Chinese TreeBank. In 
Proceedings of 2nd SIGHAN Workshop on Chinese 
Language Processing.  
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese verbs. In 
Proceedings of IJCAI 2005.  
Nianwen Xue. 2006a. Annotating the Predicate-
Argument Structure of Chinese Nominalizations. 
In Proceedings of the LREC 2006. 
Nianwen Xue. 2006b. Semantic Role Labeling of 
Nominalized Predicates in Chinese. In Proceed-
ings of HLT-NAACL 2006. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
1288
Semi-Supervised Learning for Relation Extraction  
 
ZHOU GuoDong    LI JunHui    QIAN LongHua    ZHU Qiaoming 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow Univ., Suzhou, China 215006 
Email : {gdzhou, lijunhui, qianlonghua, qmzhu}@suda.edu.cn 
 
Abstract 
This paper proposes a semi-supervised learn-
ing method for relation extraction. Given a 
small amount of labeled data and a large 
amount of unlabeled data, it first bootstraps a 
moderate number of weighted support vectors 
via SVM through a co-training procedure with 
random feature projection and then applies a 
label propagation (LP) algorithm via the boot-
strapped support vectors. Evaluation on the 
ACE RDC 2003 corpus shows that our method 
outperforms the normal LP algorithm via all 
the available labeled data without SVM boot-
strapping. Moreover, our method can largely 
reduce the computational burden. This sug-
gests that our proposed method can integrate 
the advantages of both SVM bootstrapping 
and label propagation.  
1 Introduction 
Relation extraction is to detect and classify various 
predefined semantic relations between two entities 
from text and can be very useful in many NLP ap-
plications such as question answering, e.g. to an-
swer the query ?Who is the president of the United 
States??, and information retrieval, e.g. to expand 
the query ?George W. Bush? with ?the president of 
the United States? via his relationship with ?the 
United States?. 
During the last decade, many methods have 
been proposed in relation extraction, such as su-
pervised learning (Miller et al2000; Zelenko et al
2003; Culota and Sorensen 2004; Zhao and Grish-
man 2005; Zhang et al2006; Zhou et al2005, 
2006), semi-supervised learning (Brin 1998; 
Agichtein and Gravano 2000; Zhang 2004; Chen et 
al 2006), and unsupervised learning (Hasegawa et 
al 2004; Zhang et al2005). Among these methods, 
supervised learning-based methods perform much 
better than the other two alternatives. However, 
their performance much depends on the availability 
of a large amount of manually labeled data and it is 
normally difficult to adapt an existing system to 
other applications and domains. On the other hand, 
unsupervised learning-based methods do not need 
the definition of relation types and the availability 
of manually labeled data. However, they fail to 
classify exact relation types between two entities 
and their performance is normally very low. To 
achieve better portability and balance between hu-
man efforts and performance, semi-supervised 
learning has drawn more and more attention re-
cently in relation extraction and other NLP appli-
cations. 
This paper proposes a semi-supervised learning 
method for relation extraction. Given a small 
amount of labeled data and a large amount of unla-
beled data, our proposed method first bootstraps a 
moderate number of weighted support vectors from 
all the available data via SVM using a co-training 
procedure with random feature projection and then 
applies a label propagation (LP) algorithm to cap-
ture the manifold structure in both the labeled and 
unlabeled data via the bootstrapped support vectors. 
Compared with previous methods, our method can 
integrate the advantages of both SVM bootstrap-
ping in learning critical instances for the labeling 
function and label propagation in capturing the 
manifold structure in both the labeled and unla-
beled data to smooth the labeling function. 
The rest of this paper is as follows. In Section 2, 
we review related semi-supervised learning work 
in relation extraction. Then, the LP algorithm via 
bootstrapped support vectors is proposed in Sec-
tion 3 while Section 4 shows the experimental re-
sults. Finally, we conclude our work in Section 5.  
2 Related Work 
Generally, supervised learning is preferable to un-
supervised learning due to prior knowledge in the 
32
annotated training data and better performance. 
However, the annotated data is usually expensive 
to obtain. Hence, there has been growing interest in 
semi-supervised learning, aiming at inducing clas-
sifiers by leveraging a small amount of labeled 
data and a large amount of unlabeled data. Related 
work in relation extraction using semi-supervised 
learning can be classified into two categories: 
bootstrapping-based (Brin 1998; Agichtein and 
Gravano 2000; Zhang 2004) and label propaga-
tion(LP)-based (Chen et al2006).  
Currently, bootstrapping-based methods domi-
nate semi-supervised learning in relation extraction. 
Bootstrapping works by iteratively classifying 
unlabeled instances and adding confidently classi-
fied ones into labeled data using a model learned 
from augmented labeled data in previous iteration. 
Brin (1998) proposed a bootstrapping-based 
method on the top of a self-developed pattern 
matching-based classifier to exploit the duality 
between patterns and relations. Agichtein and Gra-
vano (2000) shared much in common with Brin 
(1998). They employed an existing pattern match-
ing-based classifier (i.e. SNoW) instead. Zhang 
(2004) approached the much simpler relation clas-
sification sub-task by bootstrapping on the top of 
SVM. Although bootstrapping-based methods have 
achieved certain success, one problem is that they 
may not be able to well capture the manifold struc-
ture among unlabeled data. 
As an alternative to the bootstrapping-based 
methods, Chen et al(2006) employed a LP-based 
method in relation extraction. Compared with 
bootstrapping, the LP algorithm can effectively 
combine labeled data with unlabeled data in the 
learning process by exploiting the manifold struc-
ture (e.g. the natural clustering structure) in both 
the labeled and unlabeled data. The rationale be-
hind this algorithm is that the instances in high-
density areas tend to carry the same labels. The LP 
algorithm has also been successfully applied in 
other NLP applications, such as word sense disam-
biguation (Niu et al2005), text classification 
(Szummer and Jaakkola 2001; Blum and Chawla 
2001; Belkin and Niyogi 2002; Zhu and Ghahra-
mani 2002; Zhu et al2003; Blum et al2004), and 
information retrieval (Yang et al2006). However, 
one problem is its computational burden, espe-
cially when a large amount of labeled and unla-
beled data is taken into consideration. 
In order to take the advantages of both boot-
strapping and label propagation, our proposed 
method propagates labels via bootstrapped support 
vectors. On the one hand, our method can well 
capture the manifold structure in both the labeled 
and unlabeled data. On the other hand, our method 
can largely reduce the computational burden in the 
normal LP algorithm via all the available data. 
3 Label Propagation via Bootstrapped 
Support Vectors 
The idea behind our LP algorithm via bootstrapped 
support vectors is that, instead of propagating la-
bels through all the available labeled data, our 
method propagates labels through critical instances 
in both the labeled and unlabeled data. In this pa-
per, we use SVM as the underlying classifier to 
bootstrap a moderate number of weighted support 
vectors for this purpose. This is based on an as-
sumption that the manifold structure in both the 
labeled and unlabeled data can be well preserved 
through the critical instances (i.e. the weighted 
support vectors bootstrapped from all the available 
labeled and unlabeled data). The reason why we 
choose SVM is that it represents the state-of-the-
art in machine learning research and there are good 
implementations of the algorithm available. In par-
ticular, SVMLight (Joachims 1998) is selected as 
our classifier. For efficiency, we apply the one vs. 
others strategy, which builds K classifiers so as to 
separate one class from all others. Another reason 
is that we can adopt the weighted support vectors 
returned by the bootstrapped SVMs as the critical 
instances, via which label propagation is done.  
3.1 Bootstrapping Support Vectors 
This paper modifies the SVM bootstrapping algo-
rithm BootProject(Zhang 2004) to bootstrap sup-
port vectors. Given a small amount of labeled data 
and a large amount of unlabeled data, the modified 
BootProject algorithm bootstraps on the top of  
SVM by iteratively classifying  unlabeled  in-
stances  and moving   confidently  classified  ones  
into  labeled data using a model learned from the 
augmented labeled data in previous  iteration,  until 
not enough unlabeled instances can be classified 
confidently. Figure 1 shows the modified BootPro-
ject algorithm for bootstrapping support vectors.  
 
33
_________________________________________ 
Assume: 
L :  the labeled data; 
U :  the unlabeled data; 
S :  the batch size (100 in our experiments); 
P :  the number of views(feature projections); 
r :   the number of classes (including all the rela-
tion (sub)types and the non-relation)  
 
BEGIN 
REPEAT 
FOR i = 1 to P DO 
Generate projected feature space iF  from 
the original feature space F ; 
Project both L  and U  onto iF , thus gener-
ate iL  and iU ; 
Train SVM classifier ijSVM  on iL  for each 
class )1( rjr j K= ; 
Run ijSVM  on iU  for each class 
)1( rjr j K=  
END FOR 
Find (at most) S instances in U  with the 
highest agreement (with threshold 70% in 
our experiments) and the highest average 
SVM-returned confidence value (with 
threshold 1.0 in our experiments); 
Move them from U to L; 
UNTIL not enough unlabeled instances (less 
than 10 in our experiments) can be confidently 
classified; 
Return all the (positive and negative) support 
vectors  included in all the latest SVM classifi-
ers ijSVM  with their collective weight (abso-
lute alpha*y) information as the set of 
bootstrapped support vectors to act as the la-
beled data in the LP algorithm; 
Return U (those hard cases which can not be 
confidently classified) to act as the unlabeled 
data in the LP algorithm; 
END 
_________________________________________ 
Figure 1: The algorithm  
for bootstrapping support vectors 
 
In particular, this algorithm generates multiple 
overlapping ?views? by projecting from the origi-
nal feature space. In this paper, feature views with 
random feature projection, as proposed in Zhang 
(2004), are explored. Section 4 will discuss this 
issue in more details. During the iterative training 
process, classifiers trained on the augmented la-
beled data using the projected views are then asked 
to vote on the remaining unlabeled instances and 
those with the highest probability of being cor-
rectly labeled are chosen to augment the labeled 
data.  
During the bootstrapping process, the support 
vectors included in all the trained SVM classifiers 
(for all the relation (sub)types and the non-relation) 
are bootstrapped (i.e. updated) at each iteration. 
When the bootstrapping process stops, all the 
(positive and negative) support vectors included in 
the SVM classifiers are returned as bootstrapped 
support vectors with their collective weights (abso-
lute a*y) to act as the labeled data in the LP algo-
rithm and all the remaining unlabeled instances (i.e. 
those hard cases which can not be confidently clas-
sified in the bootstrapping process) in the unla-
beled data are returned to act as the unlabeled data 
in the LP algorithm. Through SVM bootstrapping, 
our LP algorithm will only depend on the critical 
instances (i.e. support vectors with their weight 
information bootstrapped from all the available 
labeled and unlabeled data) and those hard in-
stances, instead of all the available labeled and 
unlabeled data.  
3.2 Label Propagation 
In the LP algorithm (Zhu and Ghahramani 2002), 
the manifold structure in data is represented as a 
connected graph. Given the labeled data (the above 
bootstrapped support vectors with their weights) 
and unlabeled data (the remaining hard instances in 
the unlabeled data after bootstrapping, including 
all the test instances for evaluation), the LP algo-
rithm first represents labeled and unlabeled in-
stances as vertices in a connected graph, then 
propagates the label information from any vertex 
to nearby vertex through weighted edges and fi-
nally infers the labels of unlabeled instances until a 
global stable stage is achieved. Figure 2 presents 
the label propagation algorithm on bootstrapped 
support vectors in details. 
 
34
_________________________________________
Assume:  
Y : the rn * labeling matrix, where ijy  repre-
sents the probability of vertex )1( nixi K=  
with label )1( rjr j K=  (including the non-
relation label); 
LY : the top l  rows of 
0Y . LY corresponds to the 
l  labeled instances; 
UY : the bottom u  rows of 
0Y . UY corresponds 
to the u  unlabeled instances; 
T : a nn *  matrix, with ijt  is the probability 
jumping from vertex ix to vertex jx ; 
 
BEGIN (the algorithm) 
Initialization:  
1) Set the iteration index 0=t ;  
2) Let 0Y  be the initial soft labels attached to 
each vertex;  
3) Let 0LY  be consistent with the labeling in 
the labeled (including all the relation 
(sub)types and the non-relation) data, where 
0
ijy = the weight of the bootstrapped support 
vector if ix  has label jr  (Please note that 
jr  can be the non-relation label) and 0 oth-
erwise;  
4) Initialize 0UY ; 
REPEAT 
Propagate the labels of any vertex to nearby 
vertices by tt YTY =+1 ; 
Clamp the labeled data, that is, replace 1+tLY  
with 0LY ; 
UNTIL Y converges(e.g. 1+tLY  converges to 
0
LY ); 
Assign each unlabeled instance with a label: for 
)( nilxi ?p , find its label with 
j
ijymaxarg ; 
END (the algorithm) 
_________________________________________ 
Figure 2: The LP algorithm 
 
 
Here, each vertex corresponds to an instance, 
and the edge between any two instances ix  and jx  
is weighted by ijw  to measure their similarity. In 
principle, larger edge weights allow labels to travel 
through easier. Thus the closer the instances are, 
the more likely they have similar labels. The algo-
rithm first calculates the weight ijw  using a kernel, 
then transforms it to ?
=
=?=
n
k
kjijij wwijpt
1
/)( , 
which measures the probability of propagating a 
label from instance jx to instance ix , and finally 
normalizes ijt row by row using ?
=
=
n
k
ikijij ttt
1
/  to 
maintain the class probability interpretation of the 
labeling matrix Y .  
During the label propagation process, the label 
distribution of the labeled data is clamped in each 
loop using the weights of the bootstrapped support 
vectors and acts like forces to push out labels 
through the unlabeled data. With this push origi-
nates from the labeled data, the label boundaries 
will be pushed much faster along edges with larger 
weights and settle in gaps along those with lower 
weights. Ideally, we can expect that ijw  across 
different classes should be as small as possible and 
ijw  within the same class as big as possible. In this 
way, label propagation happens within the same 
class most likely. 
This algorithm has been shown to converge to 
a unique solution (Zhu and Ghahramani 2002), 
which can be obtained without iteration in theory, 
and the initialization of YU0 (the unlabeled data) is 
not important since YU0 does not affect its estima-
tion. However, proper initialization of YU0 actually 
helps the algorithm converge more rapidly in prac-
tice. In this paper, each row in YU0 is initialized to 
the average similarity with the labeled instances. 
4 Experimentation 
This paper uses the ACE RDC 2003 corpus pro-
vided by LDC for evaluation. This corpus is gath-
ered from various newspapers, newswires and 
broadcasts.  
 
35
Method 
LP via bootstrapped 
(weighted) SVs 
LP via bootstrapped  
(un-weighted) SVs 
LP w/o SVM  
bootstrapping 
SVM 
(BootProject) SVM  
Bootstrapping 
5% 46.5 (+1.4) 44.5 (+1.7) 43.1 (+1.0) 35.4 (-) 40.6 (+0.9) 
10% 48.6 (+1.7) 46.5 (+2.1) 45.2 (+1.5) 38.6 (-) 43.1 (+1.4) 
25% 51.7 (+1.9) 50.4 (+2.3) 49.6 (+1.8) 43.9 (-) 47.8 (+1.7) 
50% 53.6 (+1.8) 52.6 (+2.2) 52.1 (+1.7) 47.2 (-) 50.5 (+1.6) 
75% 55.2 (+1.3) 54.5 (+1.8) 54.2 (+1.2) 53.1 (-) 53.9 (+1.2) 
100% 56.2 (+1.0) 55.8 (+1.3) 55.6 (+0.8) 55.5 (-) 55.8 (+0.7) 
Table 1: Comparison of different methods using a state-of-the-art linear kernel on the ACE RDC 2003 
corpus (The numbers inside the parentheses indicate the increases in F-measure if we add the ACE RDC 
2004 corpus as the unlabeled data) 
4.1 Experimental Setting 
In the ACE RDC 2003 corpus, the training data 
consists of 674 annotated text documents (~300k 
words) and 9683 instances of relations. During 
development, 155 of 674 documents in the training 
set are set aside for fine-tuning. The test set is held 
out only for final evaluation. It consists of 97 
documents (~50k words) and 1386 instances of 
relations. The ACE RDC 2003 task defines 5 rela-
tion types and 24 subtypes between 5 entity types, 
i.e. person, organization, location, facility and GPE. 
All the evaluations are measured on the 24 sub-
types including relation identification and classifi-
cation. 
In all our experiments, we iterate over all pairs 
of entity mentions occurring in the same sentence 
to generate potential relation instances1. For better 
evaluation, we have adopted a state-of-the-art lin-
ear kernel as similarity measurements. In our linear 
kernel, we apply the same feature set as described 
in a state-of-the-art feature-based system (Zhou et 
al 2005): word, entity type, mention level, overlap, 
base phrase chunking, dependency tree, parse tree 
and semantic information. Given above various 
lexical, syntactic and semantic features, multiple 
overlapping feature views are generated in the 
bootstrapping process using random feature projec-
tion (Zhang 2004). For each feature projection in 
bootstrapping support vectors, a feature is ran-
domly selected with probability p and therefore the 
eventually projected feature space has p*F features 
                                                           
1  In this paper, we only measure the performance of 
relation extraction on ?true? mentions with ?true? 
chaining of co-reference (i.e. as annotated by the cor-
pus annotators) in the ACE corpora. We also explic-
itly model the argument order of the two mentions 
involved and only model explicit relations because of 
poor inter-annotator agreement in the annotation of 
implicit relations and their limited number. 
on average, where F is the size of the original fea-
ture space. In this paper, p and the number of dif-
ferent views are fine-tuned to 0.5 and 10 2 
respectively using 5-fold cross validation on the 
training data of the ACE RDC 2003 corpus. 
4.2 Experimental Results 
Table 1 presents the F-measures 3  (the numbers 
outside the parentheses) of our algorithm using the 
state-of-the-art linear kernel on different sizes of 
the ACE RDC training data with all the remaining 
training data and the test data4  as the unlabeled 
data on the ACE RDC 2003 corpus. In this paper, 
we only report the performance (averaged over 5 
trials) with the percentages of 5%, 10%, 25%, 50%, 
75% and 100%5. For example, our LP algorithm 
via bootstrapped (weighted) support vectors 
achieves the F-measure of 46.5 if using only 5% of 
the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data. Table 1 
                                                           
2 This suggests that the modified BootProject algorithm 
in the bootstrapping phase outperforms the SelfBoot 
algorithm (with p=1.0 and m=1) which uses all the 
features as the only view. In the related NLP literature, 
co-training has also shown to typically outperform 
self-bootstrapping. 
3 Our experimentation also shows that most of perform-
ance improvement with either bootstrapping or label 
propagation comes from gain in recall. Due to space 
limitation, this paper only reports the overall F-
measure. 
4  In our label propagation algorithm via bootstrapped 
support vectors, the test data is only included in the 
second phase (i.e. the label propagation phase) and not 
used in the first phase (i.e. bootstrapping support vec-
tors). This is to fairly compare different semi-
supervised learning methods. 
5 We have tried less percentage than 5%. However, our 
experiments show that using much less data will suffer 
from performance un-stability. Therefore, we only re-
port the performance with percentage not less than 5%. 
36
also compares our method with SVM and the 
original SVM bootstrapping algorithm BootPro-
ject(i.e. bootstrapping on the top of SVM with fea-
ture projection, as proposed in Zhang (2004)). 
Finally, Table 1 compares our LP algorithm via 
bootstrapped (weighted by default) support vectors 
with other possibilities, such as the scheme via 
bootstrapped (un-weighted, i.e. the importance of 
support vectors is not differentiated) support vec-
tors and the scheme via all the available labeled 
data (i.e. without SVM bootstrapping). Table 1 
shows that: 
1) Inclusion of unlabeled data using semi-
supervised learning, including the SVM boot-
strapping algorithm BootProject, the normal 
LP algorithm via all the available labeled and 
unlabeled data without SVM bootstrapping, 
and our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors, 
consistently improves the performance, al-
though semi-supervised learning has shown to 
typically decrease the performance when a lot 
of (enough) labeled data is available (Nigam 
2001).  This may be due to the insufficiency of 
labeled data in the ACE RDC 2003 corpus. 
Actually, most of relation subtypes in the two 
corpora much suffer from the data sparseness 
problem (Zhou et al2006).  
2) All the three LP algorithms outperform the 
state-of-the-art SVM classifier and the SVM 
bootstrapping algorithm BootProject. Espe-
cially, when a small amount of labeled data is 
available, the performance improvements by 
the LP algorithms are significant. This indi-
cates the usefulness of the manifold structure 
in both labeled and unlabeled data and the 
powerfulness of the LP algorithm in modeling 
such information.  
3) Our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors out-
performs the normal LP algorithm via all the 
available labeled data w/o SVM bootstrapping. 
For example, our LP algorithm via boot-
strapped (weighted) support vectors outper-
forms the normal LP algorithm from 0.6 to 3.4 
in F-measure on the ACE RDC 2003 corpus 
respectively when the labeled data ranges from 
100% to 5%. This suggests that the manifold 
structure in both the labeled and unlabeled data 
can be well preserved via bootstrapped support 
vectors, especially when only a small amount 
of labeled data is available. This implies that 
weighted support vectors may represent the 
manifold structure (e.g. the decision boundary 
from where label propagation is done) better 
than the full set of data ? an interesting result 
worthy more quantitative and qualitative justi-
fication in the future work.   
4) Our LP algorithms via bootstrapped (weighted) 
support vectors perform better than LP algo-
rithms via bootstrapped (un-weighted) support 
vectors by ~1.0 in F-measure on average. This 
suggests that bootstrapped support vectors with 
their weights can better represent the manifold 
structure in all the available labeled and unla-
beled data than bootstrapped support vectors 
without their weights. 
5) Comparison of SVM, SVM bootstrapping and 
label propagation with bootstrapped (weighted) 
support vectors shows that both bootstrapping 
and label propagation contribute much to the 
performance improvement. 
Table 1 also shows the increases in F-measure 
(the numbers inside the parentheses) if we add all 
the instances in the ACE RDC 20046 corpus into 
the ACE RDC 2003 corpus in consideration as 
unlabeled data in all the four semi-supervised 
learning methods. It shows that adding more unla-
beled data can consistently improve the perform-
ance. For example, compared with using only 5% 
of the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data, including 
the ACE RDC 2004 corpus as the unlabeled data 
increases the F-measures of 1.4 and 1.0 in our LP 
algorithm and the normal LP algorithm respec-
tively. Table 1 shows that the contribution grows 
first when the labeled data begins to increase and 
reaches a maximum of ~2.0 in F-measure at a cer-
tain point. 
Finally, it is found in our experiments that 
critical and hard instances normally occupy only 
15~20% (~18% on average) of all the available 
labeled and unlabeled data. This suggests that, 
through bootstrapped support vectors, our LP algo-
                                                           
6  Compared with the ACE RDC 2003 task, the ACE 
RDC 2004 task defines two more entity types, i.e. 
weapon and vehicle, much more entity subtypes, and 
different 7 relation types and 23 subtypes between 7 
entity types. The ACE RDC 2004 corpus from LDC 
contains 451 documents and 5702 relation instances. 
37
rithm can largely reduce the computational burden 
since it only depends on the critical instances (i.e. 
bootstrapped support vectors with their weights) 
and those hard instances.   
5 Conclusion 
This paper proposes a new effective and efficient 
semi-supervised learning method in relation ex-
traction. First, a moderate number of weighted 
support vectors are bootstrapped from all the avail-
able labeled and unlabeled data via SVM through a 
co-training procedure with feature projection. Here, 
a random feature projection technique is used to 
generate multiple overlapping feature views in 
bootstrapping using a state-of-the-art linear kernel. 
Then, a LP algorithm is applied to propagate labels 
via the bootstrapped support vectors, which, to-
gether with those hard unlabeled instances and the 
test instances, are represented as vertices in a con-
nected graph. During the classification process, the 
label information is propagated from any vertex to 
nearby vertex through weighted edges and finally 
the labels of unlabeled instances are inferred until a 
global stable stage is achieved.  In this way, the 
manifold structure in both the labeled and unla-
beled data can be well captured by label propaga-
tion via bootstrapped support vectors. Evaluation 
on the ACE RDC 2004 corpus suggests that our LP 
algorithm via bootstrapped support vectors can 
take the advantages of both SVM bootstrapping 
and label propagation.  
For the future work, we will systematically 
evaluate our proposed method on more corpora 
and explore better metrics of measuring the simi-
larity between two instances. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation of 
China and Project 2006AA01Z147 under the ?863? 
National High-Tech Research and Development of 
China. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Agichtein E. and Gravano L. (2000). Snowball: 
Extracting relations from large plain-text collec-
tions. Proceedings of the 5th ACM International 
Conference on Digital Libraries 
(ACMDL?2000). 
Belkin, M. and Niyogi, P. (2002). Using Manifold 
Structure for Partially Labeled Classification. 
NIPS 15. 
Blum A. and Chawla S. (2001). Learning from la-
beled and unlabeled data using graph mincuts. 
ICML?2001. 
Blum A., Lafferty J., Rwebangira R and Reddy R. 
(2004). Semi-supervised learning using random-
ized mincuts. ICML?2004. 
Brin S. (1998). Extracting patterns and relations 
from world wide web. Proceedings of WebDB 
Workshop at 6th International Conference on 
Extending Database Technology:172-183. 
Charniak E. (2001). Immediate-head Parsing for 
Language Models. ACL?2001: 129-137. Tou-
louse, France 
Chen J.X., Ji D.H., Tan C.L. and Niu Z.Y. (2006). 
Relation extraction using label propagation 
based semi-supervised learning. COLING-
ACL?2006: 129-136. July 2006. Sydney, Austra-
lia. 
Culotta A. and Sorensen J. (2004). Dependency 
tree kernels for relation extraction. ACL?2004. 
423-429. 21-26 July 2004. Barcelona, Spain. 
Hasegawa T., Sekine S. and Grishman R. (2004). 
Discovering relations among named entities 
form large corpora. ACL?2004. Barcelona, Spain. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to ex-
tract information from text. ANLP?2000. 226-
233. 29 April  - 4 May 2000, Seattle, USA 
Moschitti A. (2004). A study on convolution ker-
nels for shallow semantic parsing. 
ACL?2004:335-342. 
Nigam K.P. (2001). Using unlabeled data to im-
prove text classification. Technical Report 
CMU-CS-01-126. 
Niu Z.Y., Ji D.H., and Tan C.L. (2005). Word 
Sense Disambiguation Using Label Propagation 
Based Semi-supervised Learning. 
ACL?2005:395-402., Ann Arbor, Michigan, 
USA. 
Szummer, M., & Jaakkola, T. (2001). Partially La-
beled Classification with Markov Random 
Walks. NIPS 14. 
38
Yang L.P., Ji D.H., Zhou G.D. and Nie Y. (2006). 
Document Re-ranking using cluster validation 
and label propagation. CIKM?2006. 5-11 Nov 
2006. Arlington, Virginia, USA. 
Zelenko D., Aone C. and Richardella. (2003). Ker-
nel methods for relation extraction. Journal of 
Machine Learning Research. 3(Feb):1083-1106. 
Zhang M., Su J., Wang D.M., Zhou G.D. and Tan 
C.L. (2005). Discovering Relations from a 
Large Raw Corpus Using Tree Similarity-based 
Clustering, IJCNLP?2005, Lecture Notes in Arti-
ficial Intelligence (LNAI 3651). 378-389. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). 
A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured 
Features. COLING-ACL-2006: 825-832. Sydney, 
Australia 
Zhang Z. (2004). Weakly supervised relation clas-
sification for information extraction. 
CIKM?2004. 8-13 Nov 2004. Washington D.C. 
USA. 
Zhao S.B. and Grishman R. (2005). Extracting re-
lations with integrated information using kernel 
methods. ACL?2005: 419-426. Univ of Michi-
gan-Ann Arbor,  USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). 
Exploring various knowledge in relation extrac-
tion. ACL?2005. 427-434. 25-30 June, Ann Ar-
bor, Michgan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling 
commonality among related classes in relation 
extraction, COLING-ACL?2006: 121-128. Syd-
ney, Australia. 
Zhu, X. and Ghahramani, Z. (2002). Learning from 
Labeled and Unlabeled Data with Label 
Propagation. CMU CALD Technical Report. 
CMU-CALD-02-107. 
Zhu, X., Ghahramani, Z. and Lafferty, J. (2003). 
Semi-Supervised Learning Using Gaussian 
Fields and Harmonic Functions. ICML?2003. 
39
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 671?679,
Beijing, August 2010
Learning the Scope of Negation via Shallow Semantic Parsing 
Junhui Li  Guodong Zhou?  Hongling Wang  Qiaoming Zhu 
School of Computer Science and Technology 
        Soochow University at Suzhou 
{lijunhui, gdzhou, redleaf, qmzhu}@suda.edu.cn 
 
                                                          
? Corresponding author 
Abstract 
In this paper we present a simplified shallow 
semantic parsing approach to learning the 
scope of negation (SoN). This is done by 
formulating it as a shallow semantic parsing 
problem with the negation signal as the 
predicate and the negation scope as its ar-
guments. Our parsing approach to SoN 
learning differs from the state-of-the-art 
chunking ones in two aspects. First, we ex-
tend SoN learning from the chunking level 
to the parse tree level, where structured syn-
tactic information is available. Second, we 
focus on determining whether a constituent, 
rather than a word, is negated or not, via a 
simplified shallow semantic parsing frame-
work. Evaluation on the BioScope corpus 
shows that structured syntactic information 
is effective in capturing the domination rela-
tionship between a negation signal and its 
dominated arguments. It also shows that our 
parsing approach much outperforms the 
state-of-the-art chunking ones. 
1 Introduction 
Whereas negation in predicate logic is 
well-defined and syntactically simple, negation 
in natural language is much complex. Gener-
ally, learning the scope of negation involves 
two subtasks: negation signal finding and nega-
tion scope finding. The former decides whether 
the words in a sentence are negation signals 
(i.e., words indicating negation, e.g., no, not, 
fail, rather than), where the semantic informa-
tion of the words, rather than the syntactic in-
formation, plays a critical role. The latter de-
termines the sequences of words in the sen-
tence which are negated by the given negation 
signal. Compared with negation scope finding, 
negation signal finding is much simpler and has 
been well resolved in the literature, e.g. with 
the accuracy of 95.8%-98.7% on the three 
subcorpora of the Bioscope corpus (Morante 
and Daelemans, 2009). In this paper, we focus 
on negation scope finding instead. That is, we 
assume golden negation signal finding. 
Finding negative assertions is essential in 
information extraction (IE), where in general, 
the aim is to derive factual knowledge from 
free text. For example, Vincze et al (2008) 
pointed out that the extracted information 
within the scopes of negation signals should 
either be discarded or presented separately 
from factual information. This is especially 
important in the biomedical domain, where 
various linguistic forms are used extensively to 
express impressions, hypothesized explanations 
of experimental results or negative findings. 
Szarvas et al (2008) reported that 13.45% of 
the sentences in the abstracts subcorpus of the 
BioScope corpus and 12.70% of the sentences 
in the full papers subcorpus of the Bioscope 
corpus contain negative assertions. In addition 
to the IE tasks in the biomedical domain, SoN 
learning has attracted more and more attention 
in some natural language processing (NLP) 
tasks, such as sentiment classification (Turney, 
2002). For example, in the sentence ?The chair 
is not comfortable but cheap?, although both 
the polarities of the words ?comfortable? and 
?cheap? are positive, the polarity of ?the chair? 
regarding the attribute ?cheap? keeps positive 
while the polarity of ?the chair? regarding the 
attribute ?comfortable? is reversed due to the 
negation signal ?not?.  
Most of the initial research on SoN learning 
focused on negated terms finding, using either 
some heuristic rules (e.g., regular expression), 
or machine learning methods (Chapman et al, 
2001; Huang and Lowe, 2007; Goldin and 
Chapman, 2003). Negation scope finding has 
been largely ignored until the recent release of 
671
the BioScope corpus (Szarvas et al, 2008; 
Vincze et al, 2008). Morante et al (2008) and 
Morante and Daelemans (2009) pioneered the 
research on negation scope finding by formu-
lating it as a chunking problem, which classi-
fies the words of a sentence as being inside or 
outside the scope of a negation signal. How-
ever, this chunking approach suffers from low 
performance, in particular on long sentences, 
due to ignoring structured syntactic information. 
For example, given golden negation signals on 
the Bioscope corpus, Morante and Daelemans 
(2009) only got the performance of 50.26% in 
PCS (percentage of correct scope) measure on 
the full papers subcorpus (22.8 words per sen-
tence on average), compared to 87.27% in PCS 
measure on the clinical reports subcorpus (6.6 
words per sentence on average). 
This paper explores negation scope finding 
from a parse tree perspective and formulates it 
as a shallow semantic parsing problem, which 
has been extensively studied in the past few 
years (Carreras and M?rquez, 2005). In par-
ticular, the negation signal is recast as the pre-
dicate and the negation scope is recast as its 
arguments. The motivation behind is that 
structured syntactic information plays a critical 
role in negation scope finding and should be 
paid much more attention, as indicated by pre-
vious studies in shallow semantic parsing 
(Gildea and Palmer, 2002; Punyakanok et al, 
2005). Our parsing approach to negation scope 
finding differs from the state-of-the-art chunk-
ing ones in two aspects. First, we extend nega-
tion scope finding from the chunking level into 
the parse tree level, where structured syntactic 
information is available. Second, we focus on 
determining whether a constituent, rather than a 
word, is negated or not. Evaluation on the 
BioScope corpus shows that our parsing ap-
proach much outperforms the state-of-the-art 
chunking ones. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 in-
troduces the Bioscope corpus on which our 
approach is evaluated. Section 4 describes our 
parsing approach by formulating negation 
scope finding as a simplified shallow semantic 
parsing problem. Section 5 presents the ex-
perimental results. Finally, Section 6 concludes 
the work. 
2 Related Work 
While there is a certain amount of literature 
within the NLP community on negated terms 
finding (Chapman et al, 2001; Huang and 
Lowe, 2007; Goldin and Chapman, 2003), 
there are only a few studies on negation scope 
finding (Morante et al, 2008; Morante and 
Daelemans, 2009).  
Negated terms finding  
Rule-based methods dominated the initial re-
search on negated terms finding. As a repre-
sentative, Chapman et al (2001) developed a 
simple regular expression-based algorithm to 
detect negation signals and identify medical 
terms which fall within the negation scope. 
They found that their simple regular expres-
sion-based algorithm can effectively identify a 
large portion of the pertinent negative state-
ments from discharge summaries on determin-
ing whether a finding or disease is absent. Be-
sides, Huang and Lowe (2007) first proposed 
some heuristic rules from a parse tree perspec-
tive to identify negation signals, taking advan-
tage of syntactic parsing, and then located ne-
gated terms in the parse tree using a corre-
sponding negation grammar. 
As an alternative to the rule-based methods, 
various machine learning methods have been 
proposed for finding negated terms. As a rep-
resentative, Goldin and Chapman (2003) a-
dopted both Na?ve Bayes and decision trees to 
distinguish whether an observation is negated 
by the negation signal ?not? in hospital reports.  
Negation scope finding  
Morante et al (2008) pioneered the research on 
negation scope finding, largely due to the 
availability of a large-scale annotated corpus, 
the Bioscope corpus. They approached the ne-
gation scope finding task as a chunking prob-
lem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecu-
tiveness of the negation scope. Morante and 
Daelemans (2009) further improved the per-
formance by combing several classifiers.  
Similar to SoN learning, there are some ef-
forts in the NLP community on learning the 
scope of speculation. As a representative, 
?zg?r and Radev (2009) divided speculation 
672
learning into two subtasks: speculation signal 
finding and speculation scope finding. In par-
ticular, they formulated speculation signal 
finding as a classification problem while em-
ploying some heuristic rules from the parse tree 
perspective on speculation scope finding. 
3 Negation in the BioScope Corpus 
This paper employs the BioScope corpus 
(Szarvas et al, 2008; Vincze et al, 2008)1, a 
freely downloadable negation resource from 
the biomedical domain, as the benchmark cor-
pus. In this corpus, every sentence is annotated 
with negation signals and speculation signals 
(if it has), as well as their linguistic scopes. 
Figure 1 shows a self-explainable example. In 
this paper, we only consider negation signals, 
rather than speculation ones. Our statistics 
shows that 96.57%, 3.23% and 0.20% of nega-
tion signals are represented by one word, two 
words and three or more words, respectively. 
Additional, adverbs (e.g., not, never) and de-
terminers (e.g., no, neither) occupy 45.66% and 
30.99% of negation signals, respectively. 
 
The Bioscope corpus consists of three sub-
corpora: the full papers and the abstracts from 
the GENIA corpus (Collier et al, 1999), and 
clinical (radiology) reports. Among them, the 
full papers subcorpus and the abstracts subcor-
pus come from the same genre, and thus share 
some common characteristics in statistics, such 
as the number of words in the negation scope to 
the right (or left) of the negation signal and the 
average scope length. In comparison, the clini-
cal reports subcorpus consists of clinical radi-
ology reports with short sentences. For detailed 
statistics about the three subcorpora, please see 
Morante and Daelemans (2009). 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
For preprocessing, all the sentences in the 
Bioscope corpus are tokenized and then parsed 
using the Berkeley parser2 (Petrov and Klein, 
2007) trained on the GENIA TreeBank (GTB) 
1.0 (Tateisi et al, 2005)3, which is a bracketed 
corpus in (almost) PTB style. 10-fold 
cross-validation on GTB1.0 shows that the 
parser achieves the performance of 86.57 in 
F1-measure. It is worth noting that the GTB1.0 
corpus includes all the sentences in the ab-
stracts subcorpus of the Bioscope corpus. 
4 Negation Scope Finding via Shallow 
Semantic Parsing 
In this section, we first formulate the negation 
scope finding task as a shallow semantic pars-
ing problem. Then, we deal with it using a sim-
plified shallow semantic parsing framework.  
4.1 Formulating Negation Scope Finding  
as a Shallow Semantic Parsing Prob-
lem 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the 
constituents in the sentence into their corre-
sponding semantic arguments (roles) of the 
predicate. As far as negation scope finding 
considered, the negation signal can be regarded 
as the predicate4, while the scope of the nega-
tion signal can be mapped into several con-
stituents which are negated and thus can be 
regarded as the arguments of the negation sig-
nal. In particular, given a negation signal and 
its negation scope which covers wordm, ?, 
wordn, we adopt the following two heuristic 
rules to map the negation scope of the negation 
signal into several constituents which can be 
deemed as its arguments in the given parse tree. 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus. 
1) The negation signal itself and all of its an-
cestral constituents are non-arguments. 
2) If constituent X is an argument of the given 
negation signal, then X should be the high-
est constituent dominated by the scope of 
wordm, ?, wordn. That is to say, X?s parent 
constituent must cross-bracket or include 
the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA  
4 If a negation signal consists of multiply words 
(e.g., rather than), the last word (e.g., than) is cho-
sen to represent the negation signal. 
673
 Figure 2: An illustration of a negation signal and its arguments in a parse tree. 
These findings 
indicates 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities 
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
predicate
arguments
 
The first rule ensures that no argument cov-
ers the negation signal while the second rule 
ensures no overlap between any two arguments. 
For example, in the sentence ?These findings 
indicate that corticosteroid resistance can not 
be explained by abnormalities?, the negation 
signal ?can not? has the negation scope ?corti-
costeroid resistance can not be explained by 
abnormalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation signal 
?can not? while its arguments include three 
constituents {NP4,5, MD6,6, and VP8,11}. It is 
worth noting that according to the above rules, 
negation scope finding via shallow semantic 
parsing, i.e. determining the arguments of a 
given negation signal, is robust to some varia-
tions in parse trees. This is also empirically 
justified by our later experiments. For example, 
if the VP6,11 in Figure 2 is incorrectly expanded 
by the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, 
the negation scope of the negation signal ?can 
not? can still be correctly detected as long as 
{NP4,5, MD6,6, VB8,8, and VP9,11} are predicted 
as the arguments of the negation signal ?can 
not?. 
Compared with common shallow semantic 
parsing which needs to assign an argument 
with a semantic label, negation scope finding 
does not involve semantic label classification 
and thus could be divided into three consequent 
phases: argument pruning, argument identifica-
tion and post-processing. 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the nega-
tion signal-scope structures in negation scope 
finding can be also classified into several cer-
tain types and argument pruning can be done 
by employing several heuristic rules to filter 
out constituents, which are most likely 
non-arguments of a negation signal. Similar to 
the heuristic algorithm as proposed in Xue and 
Palmer (2004) for argument pruning in com-
mon shallow semantic parsing, the argument 
pruning algorithm adopted here starts from 
designating the negation signal as the current 
node and collects its siblings. It then iteratively 
moves one level up to the parent of the current 
node and collects its siblings. The algorithm 
ends when it reaches the root of the parse tree. 
To sum up, except the negation signal and its 
ancestral constituents, any constituent in the 
parse tree whose parent covers the given nega-
tion signal will be collected as argument can-
didates. Taking the negation signal node 
?RB7,7? in Figure 2 as an example, constituents 
{MD6,6, VP8,11, NP4,5, IN3,3, VBP2,2, and NP0,1} 
are collected as its argument candidates conse-
quently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine 
the argument candidates as either valid argu-
ments or non-arguments. Similar to argument 
674
identification in common shallow semantic 
parsing, the structured syntactic information 
plays a critical role in negation scope finding.  
Basic Features 
Table 1 lists the basic features for argument 
identification. These features are also widely 
used in common shallow semantic parsing for 
both verbal and nominal predicates (Xue, 2008; 
Li et al, 2009). 
Feature Remarks 
b1 Negation: the stem of the negation signal, 
e.g., not, rather_than. (can_not) 
b2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
b3 Path: the syntactic path from the argument 
candidate to the negation signal. 
(NP<S>VP>RB) 
b4 Position: the positional relationship of the
argument candidate with the negation sig-
nal. ?left? or ?right?. (left) 
Table 1: Basic features and their instantiations for 
argument identification in negation scope finding, 
with NP4,5 as the focus constituent (i.e., the argu-
ment candidate) and ?can not? as the given negation 
signal, regarding Figure 2. 
Additional Features 
To capture more useful information in the ne-
gation signal-scope structures, we also explore 
various kinds of additional features. Table 2 
shows the features in better capturing the de-
tails regarding the argument candidate and the 
negation signal. In particular, we categorize the 
additional features into three groups according 
to their relationship with the argument candi-
date (AC, in short) and the given negation sig-
nal (NS, in short). 
Some features proposed above may not be 
effective in argument identification. Therefore, 
we adopt the greedy feature selection algorithm 
as described in Jiang and Ng (2006) to pick up 
positive features incrementally according to 
their contributions on the development data. 
The algorithm repeatedly selects one feature 
each time which contributes most, and stops 
when adding any of the remaining features fails 
to improve the performance. As far as the ne-
gation scope finding task concerned, the whole 
feature selection process could be done by first 
running the selection algorithm with the basic 
features (b1-b4) and then incrementally picking 
up effective features from (ac1-ac6, AC1-AC2, 
ns1-ns4, NS1-NS2, nsac1-nsac2, and NSAC1 
-NSAC7). 
Feature Remarks 
argument candidate (AC) related 
ac1 the headword (ac1H) and its POS (ac1P). 
(resistance, NN) 
ac2 the left word (ac2W) and its POS (ac2P). 
(that, IN) 
ac3 the right word (ac3W) and its POS (ac3P). 
(can, MD) 
ac4 the phrase type of its left sibling (ac4L) 
and its right sibling (ac4R). (NULL, VP) 
ac5 the phrase type of its parent node. (S) 
ac6 the subcategory. (S:NP+VP) 
combined features (AC1-AC2) 
b2&fc1H, b2&fc1P 
negation signal (NS) related 
ns1 its POS. (RB) 
ns2 its left word (ns2L) and right word (ns2R). 
(can, be) 
ns3 the subcategory. (VP:MD+RB+VP) 
ns4 the phrase type of its parent node. (VP) 
combined features (NS1-NS2) 
b1&ns2L, b1&ns2R 
NS-AC-related 
nsac1 the compressed path of b3: compressing 
sequences of identical labels into one.  
(NP<S>VP>RB) 
nsac2 whether AC and NS are adjacent in posi-
tion. ?yes? or ?no?. (no) 
combined features (NSAC1-NSAC7) 
b1&b2, b1&b3, b1&nsac1, b3&NS1, b3&NS2, 
b4&NS1, b4&NS2 
Table 2: Additional features and their instantiations 
for argument identification in negation scope find-
ing, with NP4,5 as the focus constituent (i.e., the 
argument candidate) and ?can not? as the given 
negation signal, regarding Figure 2. 
4.4 Post-Processing 
Although a negation signal in the BioScope 
corpus always has only one continuous block 
as its negation scope (including the negation 
signal itself), the negation scope finder may 
result in discontinuous negation scope due to 
independent prediction in the argument identi-
fication phase. Given the golden negation sig-
nals, we observed that 6.2% of the negation 
scopes predicted by our negation scope finder 
are discontinuous.  
Figure 3 demonstrates the projection of all 
the argument candidates into the word level. 
According to our argument pruning algorithm 
in Section 4.2, except the words presented by 
675
the negation signal, the projection covers the 
whole sentence and each constituent (LACi or 
RACj in Figure 3) receives a probability distri-
bution of being an argument of the given nega-
tion signal in the argument identification phase. 
 Since a negation signal is deemed inside of its 
negation scope in the BioScope corpus, our 
post-processing algorithm first includes the 
negation signal in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the negation signal 
itself, the leftmost word of constituent LACi 
(1<=i<=m). Supposing LACi receives prob-
ability of Pi being an argument, we use the fol-
lowing formula to determine LACk* whose 
leftmost word represents the boundary of the 
left scope. If k*=0, then the negation signal 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ? P?
                                                          
 
Similarly, the right boundary of the given 
negation signal can be decided. 
5 Experimentation 
We have evaluated our shallow semantic pars-
ing approach to negation scope finding on the 
BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante 
and Daelemans (2009), the abstracts subcorpus 
is randomly divided into 10 folds so as to per-
form 10-fold cross validation, while the per-
formance on both the papers and clinical re-
ports subcorpora is evaluated using the system 
trained on the whole abstracts subcorpus. In 
addition, SVMLight5 is selected as our classi-
fier. In particular, we adopt the linear kernel 
and the training parameter C is fine-tuned to 
0.2. 
1
5 http://svmlight.joachims.org/ 
The evaluation is made using the accuracy. 
We report the accuracy using three measures: 
PCLB and PCRB, which indicate the percent-
ages of correct left boundary and right bound-
ary respectively, PCS, which indicates the per-
centage of correct scope as a whole.  
LACm   ?.   LAC1 RAC1   ?.   RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
5.2 Experimental Results on Golden Parse 
Trees 
In order to select beneficial features from the 
additional features proposed in Section 4.3, we 
randomly split the abstracts subcorpus into 
training and development datasets with propor-
tion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 
features {NSAC5, ns2R, NS1, ac1P, ns3, 
NSAC7, ac4R} are selected consecutively for 
argument identification. Table 3 presents the 
effect of selected features in an incremental 
way on the development data. It shows that the 
additional features significantly improve the 
performance by 11.66% in PCS measure from 
74.93% to 86.59% ( ). 2; 0.0p? <
 
Feature PCLB PCRB PCS 
Baseline 84.26 88.92 74.93 
+NSAC5 90.96 88.92 81.34 
+ns2R 91.55 88.92 81.92 
+NS1 92.42 89.50 83.09 
+ac1P 93.59 89.50 84.26 
+ns3 93.88 90.09 84.84 
+NSAC7 94.75 89.80 85.42 
+ac4R 95.04 90.67 86.59 
Table 3: Performance improvement (%) of includ-
ing the additional features in an incremental way on 
the development data (of the abstracts subcorpus). 
However, Table 3 shows that the additional 
features behave quite differently in terms of 
PCLB and PCRB measures. For example, 
PCLB measure benefits more from features 
NSAC5, ns2R, NS1, ac1P, and NSAC7 while 
PCRB measure benefits more from features 
NS1 and ac4R. It also shows that the features 
(e.g., NSAC5, ns2R, NS1, NSAC7) related to 
neighboring words of the negation signal play a 
critical role in recognizing both left and right 
boundaries. This may be due to the fact that 
neighboring words usually imply sentential 
information. For example, ?can not be? indi-
cates a passive clause while ?did not? indicates 
an active clause. Table 3 also shows that the 
recognition of left boundaries is much easier 
than that of right boundaries. This may be due 
676
to the fact that 83.6% of negation signals have 
themselves as the left boundaries in the ab-
stracts subcorpus.  
gument candidate is outside or cross-brackets 
with the golden negation scope, then it is a 
non-argument. The oracle performance is pre-
sented in the rows of oracle in Table 5 and Ta-
ble 6. 
Table 4 presents the performance on the ab-
stracts subcorpus by performing 10-fold 
cross-validation. It shows that the additional 
features significantly improve the performance 
over the three measures ( ). 2; 0.0p? <
Table 5 and Table 6 show that: 
1) Automatic syntactic parsing lowers the per-
formance of negation scope finding on the 
abstracts subcorpus in all three measures (e.g. 
from 83.10 to 81.84 in PCS). As expected, 
the parser trained on the whole GTB1.0 
corpus works better than that trained on 
6,691 sentences (e.g. 64.02 Vs. 62.70, and 
89.79 Vs. 85.21 in PCS measure on the full 
papers and the clinical reports subcorpora, 
respectively). However, the performance de-
crease shows that negation scope finding is 
not as sensitive to automatic syntactic pars-
ing as common shallow semantic parsing, 
whose performance might decrease by about 
~10 in F1-measure (Toutanova et al, 2005). 
This indicates that negation scope finding 
via shallow semantic parsing is robust to 
some variations in the parse trees. 
1
Feature PCLB PCRB PCS 
Baseline 84.29 87.82 74.05 
+selected features 93.06 88.96 83.10 
Table 4: Performance (%) of negation scope finding 
on the abstracts subcorpus using 10-fold 
cross-validation.  
5.3 Experimental Results on Automatic 
Parse Trees 
The GTB1.0 corpus contains 18,541 sentences 
in which 11,850 of them (63.91%) overlap with 
the sentences in the abstracts subcorpus6. In 
order to get automatic parse trees for the sen-
tences in the abstracts subcorpus, we train the 
Berkeley parser with the remaining 6,691 sen-
tences in GTB1.0. The Berkeley parser trained 
on 6,691 sentences achieves the performance of 
85.22 in F1-measure on the other sentences in 
GTB1.0. For both the full papers and clinical 
reports subcorpora, we get their automatic 
parse trees by using two Berkeley parsers: one 
trained on 6,691 sentences in GBT1.0, and the 
other trained on all the sentences in GTB1.0.  
2) autoparse(test) consistently outperforms 
autoparse(t&t) on both the abstracts and the 
full papers subcorpora. However, it is sur-
prising to find that autoparse(t&t) achieves 
better performance on the clinical reports 
subcorpus than autoparse(test). This may be 
due to the special characteristics of the 
clinical reports subcorpus, which mainly 
consists of much shorter sentences with 6.6 
words per sentence on average, and better 
adaptation of the argument identification 
classifier to the variations in the automatic 
parse trees. 
To test the performance on automatic parse 
trees, we employ two different configurations. 
First, we train the argument identification clas-
sifier on the abstracts subcorpus using auto-
matic parse trees produced by Berkeley parser 
trained on 6,691 sentences. The experimental 
results are presented in the rows of auto-
parse(t&t) in Table 5 and Table 6. Then, we 
train the argument identification classifier on 
the abstracts subcorpus using golden parse 
trees. The experimental results are presented in 
the rows of autoparse(test) in Table 5 and Ta-
ble 6.  
3) The performance on all three subcorpora 
indicates that the recognition of right 
boundary is much harder than that of left 
boundary. This may be due to the longer 
right boundary on an average. Our statistics 
shows that the average left/right boundaries 
are 1.1/6.9, 0.1/3.7, and 1.2/6.5 words on the 
abstracts, the full papers and the clinical re-
ports subcorpora, respectively. 
We also report an oracle performance to ex-
plore the best possible performance of our sys-
tem by assuming that our negation scope finder 
can always correctly determine whether a can-
didate is an argument or not. That is, if an ar-
4) The oracle performance is less sensitive to 
automatic syntactic parsing. In addition, 
given the performance gap between the per-
formance of our negation scope finder and 
the oracle performance, there is still much 
room for further performance improvement. 
                                                          
6 There are a few cases where two sentences in the 
abstracts subcorpus map into one sentence in GTB. 
677
 Abstracts Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 91.97 87.82 80.88 85.45 67.20 59.26 97.48 88.30 85.89
autoparse(test) 92.71 88.33 81.84 87.57 68.78 62.70 97.48 87.73 85.21
oracle 99.72 94.59 94.37 98.94 84.13 83.33 99.89 98.39 98.39
Table 5: Performance (%) of negation scope finding on the three subcorpora by using automatic parser trained 
with 6,691 sentences in GTB1.0.  
 Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 85.98 67.99 60.32 97.48 92.66 90.48 
autoparse(test) 87.83 70.11 64.02 97.36 92.20 89.79 
oracle 98.94 83.86 83.07 99.77 97.94 97.82 
Table 6: Performance (%) of negation scope finding on the two subcorpora by using automatic parser trained 
with all the sentences in GTB1.0.  
 
Method Abstracts Papers Clinical 
M et al (2008) 57.33 n/a n/a 
M & D (2009) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Our final system 81.84 64.02 89.79 
Table 7: Performance comparison over the PCS 
measure (%) of our system with other 
state-of-the-art ones.  
Table 7 compares our performance in PCS 
measure with related work. It shows that even 
our baseline system with four basic features as 
presented in Table 1 performs better than 
Morante et al (2008) and Morante and Daele-
mans(2009). This indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syn-
tactic information on negation scope finding. It 
also shows that our final system significantly 
outperforms the state-of-the-art ones using a 
chunking approach, especially on the abstracts 
and full papers subcorpora. However, the im-
provement on the clinical reports subcorpus is 
less apparent, partly due to the fact that the 
sentences in this subcorpus are much simpler 
(with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Following are two typical sen-
tences from the clinical reports subcorpus, 
where the negation scope covers the whole sen-
tence (except the period punctuation). Such 
sentences account for 57% of negation sen-
tences in the clinical reports subcorpus. 
 
6 Conclusion 
In this paper we have presented a simplified 
shallow semantic parsing approach to negation 
scope finding by formulating it as a shallow 
semantic parsing problem, which has been ex-
tensively studied in the past few years. In par-
ticular, we regard the negation signal as the 
predicate while mapping the negation scope 
into several constituents which are deemed as 
arguments of the negation signal. Evaluation on 
the Bioscope corpus shows the appropriateness 
of our shallow semantic parsing approach and 
that structured syntactic information plays a 
critical role in capturing the domination rela-
tionship between a negation signal and its ne-
gation scope. It also shows that our parsing 
approach much outperforms the state-of-the-art 
chunking ones. To our best knowledge, this is 
the first research on exploring negation scope 
finding via shallow semantic parsing. 
Future research will focus on joint learning 
of negation signal and its negation scope find-
ings. Although Morante and Daelemans (2009) 
reported the performance of 95.8%-98.7% on 
negation signal finding, it lowers the perform-
ance of negation scope finding by about 
7.29%-16.52% in PCS measure.  
Acknowledgments 
This research was supported by Projects 
60683150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 20093201110006 under the Specialized 
Research Fund for the Doctoral Program of 
Higher Education of China. 
(1) No evidence of focal pneumonia . 
 
(2) No findings to account for symptoms . 
678
References 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 
2001. A Simple Algorithm for Identifying Ne-
gated Findings and Diseases in Discharge Sum-
maries. Journal of Biomedical Informatics, 34: 
301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et 
al. 1999. The GENIA project: corpus-based 
knowledge acquisition and information extrac-
tion from genome research papers. In Proceed-
ings of EACL 1999.  
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Parsing for Predicate Argument Rec-
ognition. In Proceedings of ACL 2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. 
Learning to Detect Negation with ?Not? in Medi-
cal Texts. In Proceedings of SIGIR 2003. 
Yang Huang and Henry Lowe. 2007. A Novel Hy-
brid Approach to Automated Negation Detection 
in Clinical Radiology Reports. Journal of the 
American Medical Informatics Association, 14(3): 
304-311. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic Role Labeling of NomBank: A Maximum En-
tropy Approach. In Proceedings of EMNLP 
2006. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information 
and Automatic Predicate Recognition. In Pro-
ceedings of EMNLP 2009. 
Roser Morante, Anthony Liekens, and Walter 
Daelemans. 2008. Learning the Scope of Nega-
tion in Biomedical Texts. In Proceedings of 
EMNLP 2008. 
Roser Morante and Walter Daelemans. 2009. A 
Metalearning Approach to Processing the Scope 
of Negation. In Proceedings of CoNLL 2009. 
Arzucan ?zg?r; Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific 
Text. In Proceedings of EMNLP 2009. 
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceedings of 
NAACL 2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 
2005. The Necessity of Syntactic Parsing for 
Semantic Role Labeling. In Proceedings of IJCAI 
2005. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
annotation for negation, uncertainty and their 
scope in biomedical texts. In Proceedings of 
BioNLP 2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. In Proceedings of IJCNLP 2005, 
Companion volume. 
Kristina Toutanova, Aria Haghighi, and Christopher 
D. Manning. 2005. Joint Learning Improves Se-
mantic Role Labeling. In Proceedings of ACL 
2005. 
Peter D. Turney. 2002. Thumbs Up or Thumbs 
Down? Semantic Orientation Applied to Unsu-
pervised Classification of Reviews. In Proceed-
ings of ACL 2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. In Pro-
ceedings of EMNLP 2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
 
 
679
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714?724,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Unified Framework for Scope Learning via Simplified Shallow Seman-
tic Parsing 
 
Qiaoming Zhu    Junhui Li    Hongling Wang    Guodong Zhou?  
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
{qmzhu, lijunhui, hlwang, gdzhou}@suda.edu.cn 
 
 
                                                          
? Corresponding author 
Abstract 
This paper approaches the scope learning 
problem via simplified shallow semantic pars-
ing. This is done by regarding the cue as the 
predicate and mapping its scope into several 
constituents as the arguments of the cue. 
Evaluation on the BioScope corpus shows that 
the structural information plays a critical role 
in capturing the relationship between a cue 
and its dominated arguments. It also shows 
that our parsing approach significantly outper-
forms the state-of-the-art chunking ones. Al-
though our parsing approach is only evaluated 
on negation and speculation scope learning 
here, it is portable to other kinds of scope 
learning.  
1 Introduction 
Recent years have witnessed an increasing interest 
in the analysis of linguistic scope in natural lan-
guage. The task of scope learning deals with the 
syntactic analysis of what part of a given sentence 
is under user?s special interest. For example, of 
negation assertion concerned, a negation cue (e.g., 
not, no) usually dominates a fragment of the given 
sentence, rather than the whole sentence, especially 
when the sentence is long. Generally, scope learn-
ing involves two subtasks: cue recognition and its 
scope identification. The former decides whether a 
word or phrase in a sentence is a cue of a special 
interest, where the semantic information of the 
word or phrase, rather than the syntactic informa-
tion, plays a critical role. The latter determines the 
sequences of words in the sentence which are 
dominated by the given cue.  
Recognizing the scope of a special interest (e.g., 
negative assertion and speculative assertion) is es-
sential in information extraction (IE), whose aim is 
to derive factual knowledge from free text. For 
example, Vincze et al (2008) pointed out that the 
extracted information within the scope of a nega-
tion or speculation cue should either be discarded 
or presented separately from factual information. 
This is especially important in the biomedical and 
scientific domains, where various linguistic forms 
are used extensively to express impressions, hy-
pothesized explanations of experimental results or 
negative findings. Besides, Vincez et al (2008) 
reported that 13.45% and 17.70% of the sentences 
in the abstracts subcorpus of the BioScope corpus 
contain negative and speculative assertions, respec-
tively, while 12.70% and 19.44% of the sentences 
in the full papers subcorpus contain negative and 
speculative assertions, respectively. In addition to 
the IE tasks in the biomedical domain, negation 
scope learning has attracted increasing attention in 
some natural language processing (NLP) tasks, 
such as sentiment classification (Turney, 2002). 
For example, in the sentence ?The chair is not 
comfortable but cheap?, although both the polari-
ties of the words ?comfortable? and ?cheap? are 
positive, the polarity of ?the chair? regarding the 
attribute ?cheap? keeps positive while the polarity 
of ?the chair? regarding the attribute ?comfortable? 
is reversed due to the negation cue ?not?. Similarly, 
seeing the increasing interest in speculation scope 
learning, the CoNLL?2010 shared task (Farkas et 
al., 2010) aims to detect uncertain information in 
resolving the scopes of speculation cues. 
Most of the initial research in this literature fo-
cused on either recognizing negated terms or iden-
tifying speculative sentences, using some heuristic 
714
rules (Chapman et al, 2001; Light et al, 2004), 
and machine learning methods (Goldin and Chap-
man, 2003; Medlock and Briscoe, 2007). However, 
scope learning has been largely ignored until the 
recent release of the BioScope corpus (Szarvas et 
al., 2008), where negation/speculation cues and 
their scopes are annotated explicitly. Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b) pioneered the research on scope learning 
by formulating it as a chunking problem, which 
classifies the words of a sentence as being inside or 
outside the scope of a cue. Alternatively, ?zg?r 
and Radev (2009) and ?vrelid et al (2010) defined 
heuristic rules for speculation scope learning from 
constituency and dependency parse tree perspec-
tives, respectively. 
Although the chunking approach has been 
evaluated on negation and speculation scope learn-
ing and can be easily ported to other scope learning 
tasks, it ignores syntactic information and suffers 
from low performance. Alternatively, even if the 
rule-based methods may be effective for a special 
scope learning task (e.g., speculation scope learn-
ing), it is not readily adoptable to other scope 
learning tasks (e.g., negation scope learning). In-
stead, this paper explores scope learning from 
parse tree perspective and formulates it as a simpli-
fied shallow semantic parsing problem, which has 
been extensively studied in the past few years 
(Carreras and M?rquez, 2005). In particular, the 
cue is recast as the predicate and the scope is recast 
as the arguments of the cue. The motivation behind 
is that the structured syntactic information plays a 
critical role in scope learning and should be paid 
much more attention, as indicated by previous 
studies in shallow semantic parsing (Gildea and 
Palmer, 2002; Punyakanok et al, 2005). Although 
our approach is evaluated only on negation and 
speculation scope learning here, it is portable to 
other kinds of scope learning. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 intro-
duces the Bioscope corpus on which our approach 
is evaluated. Section 4 describes our parsing ap-
proach by formulating scope learning as a simpli-
fied shallow semantic parsing problem. Section 5 
presents the experimental results. Finally, Section 
6 concludes the work. 
 
 
2 Related Work  
Most of the previous research on scope learning 
falls into negation scope learning and speculation 
scope learning.  
Negation Scope Learning 
Morante et al (2008) pioneered the research on 
negation scope learning, largely due to the avail-
ability of a large-scale annotated corpus, the Bio-
scope corpus. They approached negation cue 
recognition as a classification problem and formu-
lated negation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecutive-
ness of the negation scope. Morante and Daele-
mans (2009a) further improved the performance by 
combing several classifiers and achieved the accu-
racy of ~98% for negation cue recognition and the 
PCS (Percentage of Correct Scope) of ~74% for 
negation scope identification on the abstracts sub-
corpus. However, this chunking approach suffers 
from low performance, in particular on long sen-
tences. For example, given golden negation cues 
on the Bioscope corpus, Morante and Daelemans 
(2009a) only got the performance of 50.26% in 
PCS on the full papers subcorpus (22.8 words per 
sentence on average), compared to 87.27% in PCS 
on the clinical reports subcorpus (6.6 words per 
sentence on average). 
Speculation Scope Learning 
Similar to Morante and Daelemans (2009a), 
Morante and Daelemans (2009b) formulated 
speculation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the speculation scope, 
with proper post-processing to ensure consecutive-
ness of the speculation scope. They concluded that 
their method for negation scope identification is 
portable to speculation scope identification. How-
ever, of speculation scope identification concerned, 
it also suffers from low performance, with only 
60.59% in PCS for the clinical reports subcorpus 
of short sentences. 
Alternatively, ?zg?r and Radev (2009) em-
ployed some heuristic rules from constituency 
parse tree perspective on speculation scope identi-
fication. Given golden speculation cues, their rule-
based method achieves the accuracies of 79.89% 
715
and 61.13% on the abstracts and the full papers 
subcorpora, respectively. The more recent 
CoNLL?2010 shared task was dedicated to the de-
tection of speculation cues and their linguistic 
scope in natural language processing (Farkas et al, 
2010). As a representative, ?vrelid et al (2010) 
adopted some heuristic rules from dependency 
parse tree perspective to identify their speculation 
scopes. 
3 Cues and Scopes in the BioScope Cor-
pus 
This paper employs the BioScope corpus (Szarvas 
et al, 2008; Vincze et al, 2008) 1 , a freely 
downloadable resource from the biomedical do-
main, as the benchmark corpus. In this corpus, 
every sentence is annotated with negation cues and 
speculation cues (if it has), as well as their linguis-
tic scopes. Figure 1 shows a self-explainable ex-
ample. It is possible that a negation/speculation cue 
consists of multiple words, i.e., ?can not?/?indicate 
that? in Figure 1. 
 
The Bioscope corpus consists of three sub-
corpora: biological full papers from FlyBase and 
from BMC Bioinformatics, biological paper ab-
stracts from the GENIA corpus (Collier et al, 
1999), and clinical (radiology) reports. Among 
them, the full papers subcorpus and the abstracts 
subcorpus come from the same genre, and thus 
share some common characteristics in statistics, 
such as the number of words in the nega-
tion/speculation scope to the right (or left) of the 
negation/speculation cue and the average scope 
length. In comparison, the clinical reports subcor-
pus consists of clinical radiology reports with short 
sentences. For detailed statistics and annotation 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
guidelines about the three subcorpora, please see 
Morante and Daelemans (2009a & 2009b). 
For preprocessing, all the sentences in the Bio-
scope corpus are tokenized and then parsed using 
the Berkeley parser (Petrov and Klein, 2007) 2  
trained on the GENIA TreeBank (GTB) 1.0 
(Tateisi et al, 2005)3, which is a bracketed corpus 
in (almost) PTB style. 10-fold cross-validation on 
GTB1.0 shows that the parser achieves the per-
formance of 86.57 in F1-measure. It is worth not-
ing that the GTB1.0 corpus includes all the 
sentences in the abstracts subcorpus of the Bio-
scope corpus. 
4 Scope Learning via Simplified Shallow 
Semantic Parsing 
In this section, we first formulate the scope learn-
ing task as a simplified shallow semantic parsing 
problem. Then, we deal with it using a simplified 
shallow semantic parsing framework. 
4.1 Formulating Scope Learning as a Simpli-
fied Shallow Semantic Parsing Problem 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the con-
stituents in the sentence into their corresponding 
semantic arguments (roles) of the predicate or not. 
As far as scope learning considered, the cue can be 
regarded as the predicate4, while its scope can be 
mapped into several constituents dominated by the 
cue and thus can be regarded as the arguments of 
the cue. In particular, given a cue and its scope 
which covers wordm, ?, wordn, we adopt the fol-
lowing two heuristic rules to map the scope of the 
cue into several constituents which can be deemed 
as its arguments in the given parse tree. 
1) The cue itself and all of its ancestral constituents 
are non-arguments. 
2) If constituent X is an argument of the given cue, 
then X should be the highest constituent domi-
nated by the scope of wordm, ?, wordn. That is 
to say, X?s parent constituent must cross-bracket 
or include the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA 
4 If a speculation cue consists of multiply words (e.g., whether 
or not), the first word (e.g., whether) is chosen to represent the 
speculation signal. However, the last word (e.g., not) is chosen 
to represent the negation cue if it consists of multiple words 
(e.g., can not). 
716
 Figure 2: Examples of a negation/speculation cue and its arguments in a parse tree 
These findings 
indicate 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
neg-predicate
neg-arguments
spec-predicate
spec-argument
 
The first rule ensures that no argument covers 
the cue while the second rule ensures no overlap 
between any two arguments. These two constraints 
between a cue and its arguments are consistent 
with shallow semantic parsing (Carreras and 
M?rquez, 2005). For example, in the sentence 
?These findings indicate that corticosteroid resis-
tance can not be explained by abnormalities?, the 
negation cue ?can not? has the negation scope 
?corticosteroid resistance can not be explained by 
abnormalities? while the speculation cue ?indicate 
that? has the speculation scope ?indicate that cor-
ticosteroid resistance can not be explained by ab-
normalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation cue ?can 
not? while its arguments include three constituents 
{NP4,5, MD6,6, and VP8,11}. Similarly, the node 
?VBP2,2? (i.e., indicate) represents the  speculation 
cue ?indicate that? while its arguments include one 
constituent SBAR3,11. It is worth noting that ac-
cording to the above rules, scope learning via shal-
low semantic parsing, i.e. determining the 
arguments of a given cue, is robust to some varia-
tions in the parse trees. This is also empirically 
justified by our later experiments. For example, if 
the VP6,11 in Figure 2 is incorrectly expanded by 
the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, the 
negation scope of the negation cue ?can not? can 
still be correctly detected as long as {NP4,5, MD6,6, 
VB8,8, and VP9,11} are predicated as the arguments 
of the negation cue ?can not?. 
Compared with common shallow semantic pars-
ing which needs to assign an argument with a se-
mantic label, scope identification does not involve 
semantic label classification and thus could be di-
vided into three consequent phases: argument 
pruning, argument identification and post-
processing. 
 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the cue-scope 
structures in scope learning can be also classified 
into several certain types and argument pruning 
can be done by employing several heuristic rules 
accordingly to filter out constituents, which are 
most likely non-arguments of a given cue. Similar 
to the heuristic algorithm proposed in Xue and 
Palmer (2004) for argument pruning in common 
shallow semantic parsing, the argument pruning 
algorithm adopted here starts from designating the 
cue node as the current node and collects its sib-
lings. It then iteratively moves one level up to the 
parent of the current node and collects its siblings. 
The algorithm ends when it reaches the root of the 
parse tree. To sum up, except the cue node itself 
and its ancestral constituents, any constituent in the 
parse tree whose parent covers the given cue will 
be collected as argument candidates. Taking the 
negation cue node ?RB7,7? in Figure 2 as an exam-
ple, constituents {MD6,6, VP8,11, NP4,5, IN3,3,  
717
 
 
Feature Remarks 
B1 Cue itself: the word of the cue, e.g., not,
rather_than. (can_not) 
B2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
B3 Path: the syntactic path from the argument 
candidate to the cue. (NP<S>VP>RB) 
B4 Position: the positional relationship of the
argument candidate with the cue. ?left? or 
?right?. (left) 
Table 1: Basic features and their instantiations for ar-
gument identification in scope learning, with NP4,5 as 
the focus constituent (i.e., the argument candidate) and 
?can not? as the given cue, regarding Figure 2. 
 
 
Feature Remarks 
Argument Candidate (AC) related 
AC1 The headword (AC1H) and its POS
(AC1P). (resistance, NN) 
AC2 The left word (AC2W) and its POS
(AC2P). (that, IN) 
AC3 The right word (AC3W) and its POS
(AC3P). (can, MD) 
AC4 The phrase type of its left sibling (AC4L)
and its right sibling (AC4R). (NULL, VP)
AC5 The phrase type of its parent node. (S) 
AC6 The subcategory. (S:NP+VP) 
Cue/Predicate (CP) related 
CP1 Its POS. (RB) 
CP2 Its left word (CP2L) and right word
(CP2R). (can, be) 
CP3 The subcategory. (VP:MD+RB+VP) 
CP4 The phrase type of its parent node. (VP) 
Combined Features related with the Argument Candi-
date  (CFAC1-CFAC2) 
b2&AC1H, b2&AC1P 
Combined Features related with the given
Cue/Predicate  (CFCP1-CFCP2) 
B1&CP2L, B1&CP2R 
Combined Features related with both the Argument 
Candidate and the given Cue/Predicate (CFACCP1-
CFACCP7) 
B1&B2, B1&B3, B1&CP1, B3&CFCP1, B3&CFCP2, 
B4&CFCP1, B4&CFCP2 
Table 2: Additional features and their instantiations for 
argument identification in scope identification, with 
NP4,5 as the focus constituent (i.e., the argument candi-
date) and ?can not? as the given cue, regarding Figure 2. 
 
VBP2,2, and NP0,1} are collected as its argument 
candidates consequently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine the 
argument candidates as either valid arguments or 
non-arguments. Similar to argument identification 
in common shallow semantic parsing, the struc-
tured syntactic information plays a critical role in 
scope learning. 
Basic Features 
Table 1 lists the basic features for argument identi-
fication. These features are also widely used in 
common shallow semantic parsing for both verbal 
and nominal predicates (Xue, 2008; Li et al, 2009). 
Additional Features 
To capture more useful information in the cue-
scope structures, we also explore various kinds of 
additional features. Table 2 shows the features in 
better capturing the details regarding the argument 
candidate and the cue. In particular, we categorize 
the additional features into three groups according 
to their relationship with the argument candidate 
(AC, in short) and the given cue/predicate (CP, in 
short). 
Some features proposed above may not be effec-
tive in argument identification. Therefore, we 
adopt the greedy feature selection algorithm as de-
scribed in Jiang and Ng (2006) to pick up positive 
features incrementally according to their contribu-
tions on the development data. The algorithm re-
peatedly selects one feature each time, which con-
tributes most, and stops when adding any of the 
remaining features fails to improve the perform-
ance. 
4.4 Post-Processing 
Although a cue in the BioScope corpus always has 
only one continuous block as its scope (including 
the cue itself), the scope identifier may result in 
discontinuous scope due to independent predica-
tion in the argument identification phase. Given the 
golden negation/speculation cues, we observe that 
6.2%/9.1% of the negation/speculation scopes pre-
dicted by our scope identifier are discontinuous. 
718
 
Figure 3 demonstrates the projection of all the 
argument candidates into the word level. Accord-
ing to our argument pruning algorithm in Section 
4.2, except the words presented by the cue, the pro-
jection covers the whole sentence and each con-
stituent (LACi or RACj in Figure 3) receives a 
probability distribution of being an argument of the 
given cue in the argument identification phase. 
Since a cue is deemed inside its scope in the 
BioScope corpus, our post-processing algorithm 
first includes the cue in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the cue itself, the left-
most word of constituent LACi (1<=i<=m). Sup-
posing LACi receives probability of Pi being an 
argument, we use the following formula to deter-
mine LACk* whose leftmost word represents the 
boundary of the left scope. If k*=0, then the cue 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ?  P?
Similarly, the right boundary of the given cue 
can be decided. 
4.5 Cue Recognition 
Automatic recognition of cues of a special interest 
is the prerequisite for a scope learning system. The 
approaches to recognizing cues of a special interest 
usually fall into two categories: 1) substring 
matching approaches, which require a set of cue 
words or phrases in advance (e.g., Light et al, 
2004); 2) machine learning approaches, which 
train a classifier with either supervised or semi-
supervised learning methods (e.g., ?zg?r and 
Radev, 2009; Szarvas, 2008). Without loss of gen-
erality, we adopt a machine learning approach and 
train a classifier with supervised learning. In par-
ticular, we make an independent classification for 
each word with a BIO label to indicate whether it 
is the first word of a cue, inside a cue, or outside of 
it, respectively. 
LACm    ?.      LAC1 RAC1      ?.    RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
Inspired by previous studies on similar tasks 
such as WSD and nominal predicate recognition in 
shallow semantic parsing (Lee and Ng, 2002; Li et 
al., 2009), where various features on the word it-
self, surrounding words and syntactic information 
have been successfully used, we believe that such 
information is also valuable to automatic recogni-
tion of cues. Table 3 shows the features employed 
for cue recognition. In particular, we categorize 
these features into three groups: 1) features about 
the cue candidate itself (CC in short); 2) features 
about surrounding words (SW in short); and 3) 
structural features derived from the syntactic parse 
tree (SF in short).
 
Feature Remarks 
Cue Candidate (CC) related 
CC1 The cue candidate itself. (indicate) 
CC2 The stem of the cue candidate. (indicate)
CC3 The POS tag of the cue candidate. (VBP)
Surrounding Words (SW) related 
SW1 The left surrounding words with the win-
dow size of 3. (these, findings) 
SW2 The right surrounding words with the 
window size of 3. (that, corticosteroid,
resistance) 
Structural Features (SF) 
SF1 The subcategory of the candidate node.  
(VP-->VBP+SBAR) 
SF2 The subcategory of the candidate node?s 
parent. (S-->NP+VP) 
SF3 POS tag of the candidate node + Phrase 
type of its parent node + Phrase type of its 
grandpa node. (VBP + VP + S) 
Table 3: Features and their instantiations for cue recog-
nition, with VBP2,2 as the cue candidate, regarding Fig-
ure 2. 
5 Experimentation 
We have evaluated our simplified shallow seman-
tic parsing approach to negation and speculation 
scope learning on the BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b), the abstracts subcorpus is randomly di-
vided into 10 folds so as to perform 10-fold cross-
validation, while the performance on both the pa-
719
pers and clinical reports subcorpora is evaluated 
using the system trained on the whole abstracts 
subcorpus. In addition, SVMLight  is selected as 
our classifier. 
5
For cue recognition, we report its performance 
using precision/recall/F1-measure. For scope iden-
tification, we report the accuracy in PCS (Percent-
age of Correct Scopes) when the golden cues are 
given, and report precision/recall/F1-measure 
when the cues are automatically recognized. 
5.2 Experimental Results on Golden Parse 
Trees and Golden Cues 
In order to select beneficial features from the addi-
tional features proposed in Section 4.3, we ran-
domly split the abstracts subcorpus into the 
training data and the development data with pro-
portion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 7 
features {CFACCP5, CP2R, CFCP1, AC1P, CP3, 
CFACCP7, AC4R} are selected consecutively for 
negation scope identification while 11 features 
{CFACCP5, AC2W, CFACCP2, CFACCP4, AC5, 
CFCP1, CFACCP7, CFACCP1, CP4, AC3P, 
CFAC2} are selected for speculation scope identi-
fication. Table 4 gives the contribution of addi-
tional features on the development data. It shows 
that the additional features significantly improve 
the performance by 11.66% in accuracy from 
74.93% to 86.59% ( ) for negation scope 
identification and improve the performance by 
11.07% in accuracy from 77.29% to 88.36% 
( ) for speculation scope identification. 
The feature selection experiments suggest that the 
features (e.g., CFACCP5, AC2W, CFCP1) related 
to neighboring words of the cue play a critical role 
for both negation and speculation scope identifica-
tion. This may be due to the fact that neighboring 
words usually imply important sentential informa-
tion. For example, ?can not be? indicates a passive 
clause while ?did not? indicates an active clause. 
2; 0.0p? < 1
1
                                                          
2; 0.0p? <
Since the additional selected features signifi-
cantly improve the performance for both negation 
and speculation scope identification, we will in-
clude those additional selected features in all the 
remaining experiments. 
 
 
5 http://svmlight.joachims.org/ 
Task Features Acc (%) 
Baseline 74.93 Negation scope 
identification +selected features 86.59 
Baseline 77.29 Speculation scope 
identification +selected features 88.36 
Table 4: Contribution of additional selected features on 
the development dataset of the abstracts subcorpus 
 
Since all the sentences in the abstracts subcorpus 
are included in the GTB1.0 corpus while we do not 
have golden parse trees for the sentences in the full 
papers and the clinical reports subcorpora, we only 
evaluate the performance of scope identification on 
the abstracts subcorpus with golden parse trees. 
Table 5 presents the performance on the abstracts 
subcorpus by performing 10-fold cross-validation. 
It shows that given golden parse trees and golden 
cues, speculation scope identification achieves 
higher performance (e.g., ~3.3% higher in accu-
racy) than negation scope identification. This is 
mainly due to the observation on the BioScope 
corpus that the scope of a speculation cue can be 
usually characterized by its POS and the syntactic 
structures of the sentence where it occurs. For ex-
ample, the scope of a verb in active voice usually 
starts at the cue itself and ends at its object (e.g., 
the speculation cue ?indicate that? in Figure 2 
scopes the fragment of ?indicate that corticoster-
oid resistance can not be explained by abnormali-
ties?). Moreover, the statistics on the abstracts 
subcorpus shows that the number of arguments per 
speculation cue is smaller than that of arguments 
per negation cue (e.g., 1.5 vs. 1.8). 
 
Task Acc (%) 
Negation scope identification 83.10 
Speculation scope identification 86.41 
Table 5: Accuracy (%) of scope identification with 
golden parse trees and golden cues on the abstracts sub-
corpus using 10-fold cross-validation 
 
It is worth nothing that we adopted the post-
processing algorithm proposed in Section 4.4 to 
ensure the continuousness of identified scope. As 
to examine the effectiveness of the algorithm, we 
abandon the proposed algorithm by simply taking 
the left and right-most boundaries of any nodes in 
the tree which are classified as in scope. Experi-
ments on the abstracts subcorpus using 10-fold 
cross-validation shows that the simple post-
processing rule gets the performance of 80.59 and 
86.08 in accuracy for negation and speculation 
720
scope identification, respectively, which is lower 
than the performance in Table 5 achieved by our 
post-processing algorithm.  
5.3 Experimental Results on Automatic 
Parse Trees and Golden Cues 
The GTB1.0 corpus contains 18,541 sentences in 
which 11,850 of them (63.91%) overlap with the 
sentences in the abstracts subcorpus6. In order to 
get automatic parse trees, we train the Berkeley 
parser with the remaining 6,691 sentences in 
GTB1.0, which achieves the performance of 85.22 
in F1-measure on the remaining 11,850 sentences 
in GTB1.0. Table 6 shows the performance of 
scope identification on automatic parse trees and 
golden cues. In addition, we also report an oracle 
performance to explore the best possible perform-
ance of our system by assuming that our scope 
finder can always correctly determine whether a 
candidate is an argument or not. That is, if an ar-
gument candidate falls within the golden scope, 
then it is a argument. This is to measure the impact 
of automatic syntactic parsing itself. Table 6 shows 
that: 
1) For both negation and speculaiton scope 
identification, automatic syntactic parsing 
lowers the performance on the abstracts 
subcorpus (e.g., from 83.10% to 81.84% in 
accuracy for negation scope identification and 
from 86.41% to 83.74% in accuracy for 
speculaiton scope identification). However, the 
performance drop shows that both negation and 
speculation scope identification are not as 
senstive to automatic syntactic parsing as 
common shallow semantic parsing, whose 
performance might decrease by about ~10 in F1-
measure (Toutanova et al, 2005). This indicates 
that scope identification via simplified shallow 
semantic parsing is robust to some variations in 
the parse trees.  
2) Although speculation scope identification 
consistently achieves higher performance than 
negaiton scope identification when golden parse 
trees are availabe, speculation scope 
identification achieves comparable performance 
with negation scope identification on the 
abstracts subcorpus and the full papers 
                                                          
6 There are a few cases where two sentences in the abstracts 
subcorpus map into one sentence in GTB1.0. 
subcorpus while speculation scope identification 
even performs ~20% lower in accuracy than 
negation scope identification on the clinical 
report subcorpus. This is largely due to that 
specuaiton scope identification is more sensitive 
to syntactic parsing errors than negation scope 
identification due to the wider scope of a 
speculation cue while the sentences of the 
clinical reports come from a different genre, 
which indicates low performance in syntactic 
parsing.  
3) Given the performance gap between the 
performance of our scope finder and the oracle 
performance, there is still much room for further 
performance improvement. 
 
Task Method Abstracts Papers Clinical
auto 81.84 62.70 85.21 Negation scope 
identification oracle 94.37 83.33 98.39 
auto 83.74 61.29 67.90 Speculation scope
identification oracle 95.69 83.72 83.29 
Table 6: Accuracy (%) of scope identification on the 
three subcorpora using automatic parser trained on 
6,691 sentences in GTB1.0 
 
Task Method Abstracts Papers Clinical
M et al (2008) 57.33 n/a n/a 
M & D (2009a) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Negation 
scope 
identification 
Our final  81.84 64.02 89.79 
M & D (2009b) 77.13 47.94 60.59 
? & R (2009) 79.89 61.13 n/a 
Our baseline 77.39 54.55 61.92 
Speculation 
scope 
identification 
Our final  83.74 63.49 68.78 
Table 7: Performance comparison of our system with 
the state-of-the-art ones in accuracy (%). Note that all 
the performances achieved on the full papers subcorpus 
and the clinical subcorpus are achieved using the whole 
GTB1.0 corpus of 18,541 sentences while all the per-
formances achieved on the abstract subcorpus are 
achieved using 6,691 sentences from GTB1.0 due to 
overlap of the abstract subcorpus with GTB1.0. 
 
Table 7 compares our performance with related 
ones. It shows that even our baseline system with 
the four basic features presented in Table 1 
achieves comparable performance with Morante et 
al. (2008) and Morante and Daelemans (2009a & 
2009b). This further indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syntactic 
information on scope identification. It also shows 
that our final system significantly outperforms the 
721
state-of-the-art ones using a chunking approach, 
especially on the abstracts and full papers subcor-
pora. However, the improvement on the clinical 
reports subcorpora for negation scope identifica-
tion is much less apparent, partly due to the fact 
that the sentences in this subcorpus are much sim-
pler (with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Table 7 also shows that our parsing 
approach to speculation scope identification out-
performs the rule-based method in ?zg?r and 
Radev (2009), where 10-fold cross-validation is 
performed on both the abstracts and the full papers 
subcorpora. 
5.4 Experimental Results with Automatic 
Parse Trees and Automatic Cues 
So far negation/speculation cues are assumed to be 
manually annotated and available. Here we turn to 
a more realistic scenario in which cues are auto-
matically recognized. In the following, we first 
report the results of cue recognition and then the 
results of scope identification with automatic cues. 
Cue Recognition 
Task Features R (%) P (%) F1 
CC + SW 93.80 94.39 94.09Negation cue  
recognition CC+SW+SF 95.50 95.72 95.61
CC + SW 83.77 92.04 87.71Speculation cue  
recognition CC+SW+SF 84.33 93.07 88.49
Table 8: Performance of automatic cue recognition with 
gold parse trees on the abstracts subcorpus using 10-fold 
cross-validation 
 
Table 8 lists the performance of cue recognition on 
the abstracts subcorpus, assuming all words in the 
sentences as candidates. It shows that as a com-
plement to features derived from word/pos infor-
mation (CC+SW features), structural features (SF 
features) derived from the syntactic parse tree sig-
nificantly improve the performance of cue recogni-
tion by about 1.52 and 0.78 in F1-measure for 
negation and speculation cue recognition, respec-
tively, and thus included thereafter. In addition, we 
have also experimented on only these words, 
which happen to be a cue or inside a cue in the 
training data as cue candidates. However, this ex-
perimental setting achieves a lower performance 
than that when all words are considered. 
 
Task Corpus R (%) P (%) F1 
Abstracts 94.99 94.35 94.67 
Papers 90.48 87.47 88.95 
Negation cue 
recognition 
Clinical 86.81 88.54 87.67 
Abstracts 83.74 93.14 88.19 
Papers 73.02 82.31 77.39 
Speculation cue 
recognition 
Clinical 33.33 91.77 48.90 
Table 9: Performance of automatic cue recognition with 
automatic parse trees on the three subcorpora 
 
Table 9 presents the performance of cue recog-
nition achieved with automatic parse trees on the 
three subcorpora. It shows that: 
1) The performance gap of cue recognition 
between golden parse trees and automatic parse 
trees on the abstracts subcorpus is not salient 
(e.g., 95.61 vs. 94.67 in F1-measure for negation 
cues and 88.49 vs. 88.19 for speculation cues), 
largely due to the features defined for cue 
recognition are local and insenstive to syntactic 
variations. 
2) The performance of negation cue recognition is 
higher than that of speculation cue recognition 
on all the three subcorpora. This is prabably due 
to the fact that the collection of negation cue 
words or phrases is limitted while speculation 
cue words or phrases are more open. This is 
illustrated by our statistics that about only 1% 
and 1% of negation cues in the full papers and 
the clinical reports subcorpora are absent from 
the abstracts subcorpus, compared to about 6% 
and 20% for speculation cues. 
3) Unexpected, the recall of speculation cue 
recognition on the clinical reports subcorpus is 
very low (i.e., 33.33% in recall measure). This is 
probably due to the absence of about 20% 
speculation cues from the training data of the 
abstracts subcorpus. Moreover, the speculation 
cue ?or?, which accounts for about 24% of 
specuaiton cues in the clinical reports subcorpus, 
only acheives about 2% in recall largely due to 
the errors caused by the classifier trained on the 
abstracts subcorpus, where only about 11% of 
words ?or? are annotated as speculation cues. 
Scope Identification with Automatic Cue Rec-
ognition 
Table 10 lists the performance of both negation 
and speculation scope identification with automatic 
cues and automatic parse trees. It shows that auto-
matic cue recognition lowers the performance by 
722
3.34, 6.80, and 8.38 in F1-measure for negation 
scope identification on the abstracts, the full papers 
and the clinical reports subcorpora, respectively, 
while it lowers the performance by 6.50, 13.14 and 
31.23 in F1-measures for speculation scope identi-
fication on the three subcorpora, respectively, sug-
gesting the big challenge of cue recognition in the 
two scope learning tasks. 
 
Task Corpus R (%) P (%) F1 
Abstracts 78.77 78.24 78.50
Papers 58.20 56.27 57.22
Negation scope 
identification 
Clinical 80.62 82.22 81.41
Abstracts 73.34 81.58 77.24
Papers 47.51 53.55 50.35
Speculation scope 
identification 
Clinical 25.59 70.46 37.55
Table 10: Performance of both negation and speculation 
scope identification with automatic cues and automatic 
parse trees 
6 Conclusion  
In this paper we have presented a new approach to 
scope learning by formulating it as a simplified 
shallow semantic parsing problem, which has been 
extensively studied in the past few years. In par-
ticular, we regard the cue as the predicate and map 
its scope into several constituents which are 
deemed as arguments of the cue. Evaluation on the 
Bioscope corpus shows the appropriateness of our 
parsing approach and that structured syntactic in-
formation plays a critical role in capturing the 
domination relationship between a cue and its 
dominated arguments. It also shows that our pars-
ing approach outperforms the state-of-the-art 
chunking ones. Although our approach is only 
evaluated on negation and speculation scope learn-
ing here, it is portable to other kinds of scope 
learning. 
For the future work, we will explore tree kernel-
based methods to further improve the performance 
of scope learning in better capturing the structural 
information, and apply our parsing approach to 
other kinds of scope learning. 
Acknowledgments 
This research was supported by Projects 60873150, 
60970056, and 90920004 under the National Natu-
ral Science Foundation of China, Project 
20093201110006 under the Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion of China. 
References  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction 
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. CoNLL? 2005. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34: 301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et al 
1999. The GENIA Project: Corpus-Based Knowl-
edge Acquisition and Information Extraction from 
Genome Research Papers. EACL?1999. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-2010 
Shared Task: Learning to Detect Hedges and their 
Scope in Natural Language Text. CoNLL?2010: 
Shared Task. 
Daniel Gildea and Martha Palmer. 2002. The Necessity 
of Parsing for Predicate Argument Recognition. 
ACL?2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to Detect Negation with ?Not? in Medical Texts. 
SIGIR?2003. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
Role Labeling of NomBank: A Maximum Entropy 
Approach. EMNLP? 2006. 
Yoong Keok Lee and Hwee Tou Ng. 2002. An Empiri-
cal Evaluation of Knowledge Sources and Learning 
Algorithms for Word Sense Disambiguation. 
EMNLP?2002. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. 2009. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information and 
Automatic Predicate Recognition. EMNLP? 2009. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Specula-
tions, and Statements in Between. BioLink?2004. 
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific 
Literature. ACL?2007. 
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bio-
medical Texts. EMNLP?2008. 
723
Roser Morante and Walter Daelemans. 2009a. A 
Metalearning Approach to Processing the Scope of 
Negation. CoNLL?2009. 
Roser Morante and Walter Daelemans. 2009b. Learning 
the Scope of Hedge Cues in Biomedical Texts. 
BioNLP?2009. 
Lilja ?vrelid, Erik Velldal, and Stephan Oepen. 2010. 
Syntactic Scope Resolution in Uncertainty Analysis. 
COLING?2010. 
Arzucan ?zg?r, Dragomir R. Radev. 2009. Detecting 
Speculations and their Scopes in Scientific Text. 
EMNLP?2009. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. NAACL?2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2005. 
The Necessity of Syntactic Parsing for Semantic Role 
Labeling. IJCAI? 2005. 
Gy?rgy Szarvas. 2008. Hedge Classification in Bio-
medical Texts with a Weakly Supervised Selection of 
Keywords. ACL?2008. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, and 
J?nos Csirik. 2008. The BioScope corpus: Annota-
tion for Negation, Uncertainty and their Scope in 
Biomedical Texts. BioNLP?2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. IJCNLP?2005 (Companion volume). 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL?2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The BioScope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. EMNLP?2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates with 
Semantic Roles. Computational Linguistics, 
34(2):225-255.  
724
Proceedings of NAACL-HLT 2013, pages 540?549,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based
Translation
Junhui Li
University of Maryland
College Park, USA
lijunhui@umiacs.umd.edu
Philip Resnik
University of Maryland
College Park, USA
resnik@umd.edu
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Abstract
Incorporating semantic structure into a
linguistics-free translation model is chal-
lenging, since semantic structures are
closely tied to syntax. In this paper, we
propose a two-level approach to exploiting
predicate-argument structure reordering in a
hierarchical phrase-based translation model.
First, we introduce linguistically motivated
constraints into a hierarchical model, guiding
translation phrase choices in favor of those
that respect syntactic boundaries. Second,
based on such translation phrases, we propose
a predicate-argument structure reordering
model that predicts reordering not only
between an argument and its predicate, but
also between two arguments. Experiments on
Chinese-to-English translation demonstrate
that both advances significantly improve
translation accuracy.
1 Introduction
Hierarchical phrase-based (HPB) translation mod-
els (Chiang, 2005; Chiang, 2007) that utilize syn-
chronous context free grammars (SCFG) have been
widely adopted in statistical machine translation
(SMT). Although formally syntactic, such models
rarely respect linguistically-motivated syntax, and
have no formal notion of semantics. As a re-
sult, they tend to produce translations containing
both grammatical errors and semantic role confu-
sions. Our goal is to take advantage of syntactic
and semantic parsing to improve translation qual-
ity of HPB translation models. Rather than intro-
ducing semantic structure into the HPB model di-
rectly, we construct an improved translation model
by incorporating linguistically motivated syntactic
constraints into a standard HPB model. Once the
translation phrases are linguistically constrained, we
are able to propose a predicate-argument reorder-
ing model. This reordering model aims to solve
two problems: ensure that arguments are ordered
properly after translation, and to ensure that the
proper argument structures even exist, for instance
in the case of PRO-drop languages. Experimental
results on Chinese-to-English translation show that
both the hard syntactic constraints and the predicate-
argument reordering model obtain significant im-
provements over the syntactically and semantically
uninformed baseline.
In principle, semantic frames (or, more specifi-
cally, predicate-argument structures: PAS) seem to
be a promising avenue for translational modeling.
While languages might diverge syntactically, they
are less likely to diverge semantically. This has
previously been recognized by Fung et al (2006),
who report that approximately 84% of semantic
role mappings remained consistent across transla-
tions between English and Chinese. Subsequently,
Zhuang and Zong (2010) took advantage of this
consistency to jointly model semantic frames on
Chinese/English bitexts, yielding improved frame
recognition accuracy on both languages.
While there has been some encouraging work on
integrating syntactic knowledge into Chiang?s HPB
model, modeling semantic structure in a linguisti-
cally naive translation model is a challenge, because
the semantic structures themselves are syntactically
motivated. In previous work, Liu and Gildea (2010)
model the reordering/deletion of source-side seman-
tic roles in a tree-to-string translation model. While
it is natural to include semantic structures in a tree-
based translation model, the effect of semantic struc-
tures is presumably limited, since tree templates
themselves have already encoded semantics to some
540
extent. For example, template (VP (VBG giving)
NP#1 NP#2) entails NP#1 as receiver and NP#2 as
thing given. Xiong et al (2012) model the reorder-
ing between predicates and their arguments by as-
suming arguments are translated as a unit. However,
they only considered the reordering between argu-
ments and their predicates.
2 Syntactic Constraints for HPB
Translation Model
In this section, we briefly review the HPB model,
then present our approach to incorporating syntactic
constraints into it.
2.1 HPB Translation Model
In HPB models, synchronous rules take the form
X ? ??, ?,??, where X is the non-terminal sym-
bol, ? and ? are strings of lexical items and non-
terminals in the source and target side, respectively,
and ? indicates the one-to-one correspondence be-
tween non-terminals in ? and ?. Each such rule
is associated with a set of translation model fea-
tures {?i}, including phrase translation probabil-
ity p (? | ?) and its inverse p (? | ?), the lexical
translation probability plex (? | ?) and its inverse
plex (? | ?), and a rule penalty that affects prefer-
ence for longer or shorter derivations. Two other
widely used features are a target language model
feature and a target word penalty.
Given a derivation d, its translation probability is
estimated as:
P (d) ?
?
i
?i (d)
?i
(1)
where ?i is the corresponding weight of feature ?i.
See (Chiang, 2007) for more details.
2.2 Syntactic Constraints
Translation rules in an HPB model are extracted
from initial phrase pairs, which must include at least
one word inside one phrase aligned to a word inside
the other, such that no word inside one phrase can
be aligned to a word outside the other phrase. It
is not surprising to observe that initial phrases fre-
quently are non-intuitive and inconsistent with lin-
guistic constituents, because they are based only on
statistical word alignments. Nothing in the frame-
work actually requires linguistic knowledge.
Koehn et al (2003) conjectured that such non-
intuitive phrases do not help in translation. They
tested this conjecture by restricting phrases to syn-
tactically motivated constituents on both the source
and target side: only those initial phrase pairs are
subtrees in the derivations produced by the model.
However, their phrase-based translation experiments
(on Europarl data) showed the restriction to syn-
tactic constituents is actually harmful, because too
many phrases are eliminated. The idea of hard syn-
tactic constraints then seems essentially to have been
abandoned: it doesn?t appear in later work.
On the face of it, there are many possible rea-
sons Koehn et al (2003)?s hard constraints did not
work, including, for example, tight restrictions that
unavoidably exclude useful phrases, and practical is-
sues like the quality of parse trees. Although en-
suing work moved in the direction of soft syntactic
constraints (see Section 6), our ultimate goal of cap-
turing predicate-argument structure requires linguis-
tically valid syntactic constituents, and therefore we
revisit the idea of hard constraints, avoiding prob-
lems with their strictness by relaxing them in three
ways.
First, requiring source phrases to be subtrees in
a linguistically informed syntactic parse eliminates
many reasonable phrases. Consider the English-
Chinese phrase pair ?the red car, hongse de qiche?.1
It is easily to get a translation entry for the whole
phrase pair. By contrast, the phrase pair ?the red,
hongse de? is typically excluded because it does not
correspond to a complete subtree on the source side.
Yet translating the red is likely to be more useful
than translating the red car, since it is more general:
it can be followed by any other noun translation. To
this end, we relax the syntactic constraints by allow-
ing phrases on the source side corresponding to ei-
ther one subtree or sibling subtrees with a common
parent node in the syntactic parse. For example, the
red in Figure 1(a) is allowed since it spans two sub-
trees that have a common parent node NP.
Second, we might still exclude useful phrases be-
cause the syntactic parses of some languages, like
Chinese, prefer deep trees, resulting in a head and
its modifiers being distributed across multiple struc-
tural levels. Consider the English sentence I still
1We use English as source language for better readability.
541
like the red car very much and its syntactic structure
as shown in Figure 1(a). Phrases I still, still like,
I still like are not allowed, since they don?t map to
either a subtree or sibling subtrees. Logically, how-
ever, it might make sense not just to include phrases
mapping to (sibling) subtrees, but to include phrases
mapping to subtrees with the same head. To this end,
we flatten the syntactic parse so that a head and all its
modifiers appear at the same level. Another advan-
tage of this flattened structure is that flattened trees
are more reliable than unflattened ones, in the sense
that some bracketing errors in unflattened trees can
be eliminated during tree flattening. Figure 1(b) il-
lustrates flattening a syntactic parse by moving the
head (like) and all its modifiers (I, still, the red car,
and very much) to the same level.
Third, initial phrase pair extraction in Chiang?s
HPB generates a very large number of rules, which
makes training and decoding very slow. To avoid
this, a widely used strategy is to limit initial phrases
to a reasonable length on either side during rule ex-
traction (e.g., 10 in Chiang (2007)). A correspond-
ing constraint to speed up decoding prohibits any X
from spanning a substring longer than a fixed length,
often the same as the maximum phrase length in rule
extraction. Although the initial phrase length limita-
tion mainly keeps non-intuitive phrases out, it also
closes the door on some useful phrases. For ex-
ample, a translation rule ?I still like X, wo rengran
xihuan X? will be prohibited if the non-terminal X
covers 8 or more words. In contrast, our hard con-
straints have already filtered out dominating non-
intuitive phrases; thus there is more room to include
additional useful phrases. As a result, we can switch
off the constraints on initial phrase length in both
training and decoding.
2.3 Reorderable Glue Rules
In decoding, if no good rule (e.g., a rule whose left-
hand side is X) can be applied or the length of the
potential source span is larger than a pre-defined
length, a glue rule (either S ? ?X1, X1? or S ?
?S1X2, S1X2?) will be used to simply stitch two
consequent translated phrases together in monotonic
way. This will obviously prevent some reasonable
translation derivations because in certain cases, the
order of phrases may be inverted on the target side.
Moreover, even that the syntactic constraints dis-
a. Word alignment for an English-Chinese sentence pair 
with the parse tree for the English sentence 
I 
ADVP
like the red car very much
NP 
still 
VBP NP ADVP
S
b. Flattened parse tree for the English sentence
S
?  ??   ??  ??  ?? ???
wo rengran feichang xihua hongse de qiche 
VP
VP 
I 
ADVP 
like the red car very much
NP 
still 
VBP NP ADVP
Figure 1: Example of flattening parse tree.
cussed above make translation nodeXs are syntacti-
cally informed, stitching translated phrases from left
to right will unavoidably generate non-syntactically
informed node Ss. For example, the combination of
X (like) and X (the) does not make much sense in
linguistic perspective.
Alternatively, we replace glue rules of HPB with
reorderable ones:
? T ? ?X1, X1?
? T ? ?T1T2, T1T2?
? T ? ?T1T2, T2T1?
where the second (third) rule combines two trans-
lated phrases in a monotonic (inverted) way. Specif-
ically, we set the translation probability of the first
translation rule as 1 while estimating the probabil-
ities of the other two rules from training data. In
both training and decoding, we require the phrases
covered by T to satisfy our syntactic constraints.
Therefore, all translation nodes (both Xs and T s)
in derivations are syntactically informed, providing
room to explore PAS reordering in HPB model.
3 PAS Reordering Model
Ideally, we aim to model PAS reordering based on
the true semantic roles of both the source and tar-
get side, as to better cater not only consistence but
542
I 
AM-TMP 
like the red car very much
A0 
still 
VBP A1 AM-MNR
?  ??   ??  ??  ?? ? ??
wo rengran feichang xihua hongse de qiche
a. Word alignment for an English-Chinese sentence 
pair with semantic roles for the English sentence 
PAS-S 
AM-TMP 2A0 1  VBP3 A1 4  AM-MNR 5  
PAS-T 
X 2  X 1  X 5  X 3  X 4  
b. PAS-S and PAS-T for predicate like  
Figure 2: Example of PAS on both the source and target
side. Items are aligned by indices.
divergence between semantic frames of the source
and target language. However, considering there is
no efficient way of jointly performing MT and SRL,
accurate SRL on target side can only be done after
translation. Similar to related work (Liu and Gildea,
2010; Xiong et al, 2012), we obtain the PAS of
the source language (PAS-S) via a shallow seman-
tic parser and project the PAS of the target language
(PAS-T) using the word alignment derived from the
translation process. Specifically, we use PropBank
standard (Palmer et al, 2005; Xue, 2008) which de-
fines a set of numbered core arguments (i.e., A0-A5)
and adjunct-like arguments (e.g., AM-TMP for tem-
poral, AM-MNR for manner). Figure 2(b) shows
an example of PAS projection from source language
to target language.2 The PAS reordering model de-
scribes the probability of reordering PAS-S into PAS-
T. Given a predicate p, it takes the following form:
P (PAS-T | PAS-S, PRE=p) (2)
Note that cases for untranslated roles can be natu-
rally reflected in our PAS reordering model. For ex-
ample, if the argument IA0 is untranslated in Figure
2, its PAS-T will be X2X5X3X4.
2In PAS-S, we use parts-of-speech (POS) of predicates to
distinguish different types of verbs since the semantic structures
of Chinese adjective verbs are different from those of others.
3.1 Probability Estimation
While it is hard and unnecessary to translate a pred-
icate and all its associated arguments with one rule,
especially if the sentence is long, a practicable way,
as most decoders do, is to translate them in multi-
ple level rules. In addition, some adjunct-like argu-
ments are optional, or structurally dispensable part
of a sentence, which may result in data sparsity is-
sue. Based on these observations, we decompose
Formula 2 into two parts: predicate-argument re-
ordering and argument-argument reordering.
Predicate-Argument Reordering estimates the
reordering probability between a predicate and one
of its arguments. Taking predicate like and its argu-
ment A1 the red car in Figure 2(a) as an example,
the predicate-argument pattern on the source side
(PA-S) is VBP1 A12 while the predicate-argument
pattern on the target side (PA-T) is X1X2. The re-
ordering probability is estimated as:
PP-A (PA-T=X1 X2 | PA-S=VBP1 A12, PRE=like) =
Count (PA-T=X1 X2, PA-S=VBP1 A12, PRE=like)
?
T ??(PA-S) Count (PA-T=T , PA-S=VBP1 A12, PRE=like)
(3)
where ? (PA-S) enumerates all possible reorder-
ings on the target side. Moreover, we take the pred-
icate lexicon of predicate into account. To avoid
data sparsity, we set a threshold (e.g., 100) to re-
tain frequent predicates. For infrequent predicates,
their probabilities are smoothed by replacing predi-
cate lexicon with its POS. Finally, if source side pat-
terns are infrequent (e.g., less than 10) for frequent
predicates, their probabilities are smoothed as well
with the same way.
Argument-Argument Reordering estimates the
reordering probability between two arguments, i.e.,
argument-argument pattern on the source side (AA-
S) and its counterpart on the target side (AA-T).
However, due to that arguments are driven and piv-
oted by their predicates, we also include predicate
in patterns of AA-S and AA-T. Let?s revisit Fig-
ure 2(a). A1 the red car and AM-MNR very much
are inverted on the target side, whose probability is
estimated as:
PA-A (AA-T=X3 X1 X2 | AA-S=VBP1 A12 AM-MNR3, PRE=like)
(4)
Similarly we smooth the probabilities by distin-
guishing frequent predicates from infrequent ones,
543
as well as frequent patterns from infrequent ones.
3.2 Integrating the PAS Reordering Model into
the HPB Model
We integrate the PAS reordering model into the HPB
SMT by adding a new feature into the log-linear
translation model. Unlike the conventional phrase
and lexical translation features whose values are
phrase pair-determined and thus can be calculated
offline, the value of the PAS reordering model can
only be obtained with being aware of the predicate-
argument structures a hypothesis may cover. Before
we present the algorithm of integrating the PAS re-
ordering model, we define a few functions by assum-
ing p for a predicate, a for an argument, and H for a
hypothesis:
? A (i, j, p): returns arguments of p which are
fully located within the span from word i to j
on the source side. For example, in Figure 2,
A (4, 8, like) = {A1, AM -MRN}.3
? B (i, j, p): returns true if p is located within [i, j];
otherwise returns false.
? C (a, p): returns true if predicate-argument reorder-
ing for a and p has not calculated yet; otherwise re-
turns false.
? D (a1, a2, p): returns true if argument-argument
reordering for p?s arguments a1 and a2 has not cal-
culated yet; otherwise returns false.
? PP -A (H, a, p): according to Eq. 3, returns the
probability of predicate-argument reordering of a
and p, given a and p are covered by H . The po-
sitional relation of a and p on the target side can be
detected according to translation derivation of H .
? PA-A (H, a1, a2, p): according to Eq. 4, returns
the probability of argument-argument reordering of
p?s arguments a1 and a2, given a1, a2 and p are cov-
ered by H .
Algorithm 1 integrates the PAS reordering model
into a CKY-style decoder whenever a new hypothe-
sis is generated. Given a hypothesis H , it first looks
for predicates and their arguments which are covered
3The hard constraints make sure a valid source text span
would never fully cover some roles while partially cover other
roles. For example, phrases like the red, the read car very in
Figure 1 are invalid.
Algorithm 1: Integrating the PAS reordering
model into a CKY-style decoder
Input: Sentence f in the source language
Predicate-Argument Structures of f
Hypothesis H spanning from word i to j
Output: Log-Probability of the PAS reordering
model
1. set prob = 0.0
2. for predicate p in f , such that B (i, j, p) is true
3. ARG = A (i, j, p)
4. for a ? ARG such that C (a, p) is true
5. prob+= logPP -A (H, a, p)
6. for a1, a2 ? ARG such that a1 6= a2 and
D (a1, a2, p) is true
7. prob+= logPA-A (H, a1, a2, p)
8. return prob
by H (line 2-3). Then it respectively calculates the
probabilities of predicate-argument reordering and
argument-argument reordering(line 4-7).
4 Experiments
We have presented our two-level approach to in-
corporating syntactic and semantic structures in a
HPB system. In this section, we test the effect of
such structural information on a Chinese-to-English
translation task. The baseline system is a reproduc-
tion of Chiang?s (2007) HPB system. The bilin-
gual training data contains 1.5M sentence pairs with
39.4M Chinese words and 46.6M English words.4
We obtain the word alignments by running GIZA++
(Och and Ney, 2000) on the corpus in both direc-
tions and applying ?grow-diag-final-and? refinement
(Koehn et al, 2003). We use the SRI language mod-
eling toolkit to train a 5-gram language model on the
Xinhua portion of the Gigaword corpus and standard
MERT (Och, 2003) to tune the feature weights on
the development data.
To obtain syntactic parse trees for instantiating
syntactic constraints and predicate-argument struc-
tures for integrating the PAS reordering model, we
first parse the source sentences with the Berkeley
Parser (Petrov and Klein, 2007) trained on Chinese
TreeBank 6.0 and then ran the Chinese semantic role
4This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
544
System MT 02 MT 04 MT 05 Ave.
max-phrase-length=10
max-char-span=10
base HPB 40.00 35.33 32.97 36.10
+ basic constraints + unflattened tree 33.90 32.00 29.83 31.91
+ our constraints + unflattened tree 38.47 34.51 32.15 35.04
+ our constraints + flattened tree 38.55 35.38 32.44 35.46
max-phrase-length=?
max-char-span=?
+ basic constraints + unflattened tree 35.38 32.89 30.42 32.90
+ our constraints + unflattened tree 39.41 36.02 33.21 36.21
+ our constraints + flattened tree 40.01 36.24 33.65 36.71
Table 1: Effects of hard constraints. Here max-phrase-length is for maximum initial phrase length in training and
max-char-span for maximum phrase length can be covered by non-terminal X in decoding.
labeler (Li et al, 2010) on all source parse trees to
annotate semantic roles for all verbal predicates.
We use the 2003 NIST MT evaluation test data
(919 sentence pairs) as the development data, and
the 2002, 2004 and 2005 NIST MT evaluation
test data (878, 1788 and 1082 sentence pairs, re-
spectively) as the test data. For evaluation, the
NIST BLEU script (version 11b) is used to calcu-
late the NIST BLEU scores, which measures case-
insensitive matching of n-grams with n up to 4. To
test whether a performance difference is statistically
significant, we conduct significance tests following
the paired bootstrapping approach (Koehn, 2004).
4.1 Effects of Syntactic Constraints
We have also tested syntactic constraints that simply
require phrases on the source side to map to a sub-
tree (called basic constraints). Similar to requiring
initial phrases on the source side to satisfy the con-
straints in training process, we only perform chart
parsing on text spans which satisfy the constraints
in decoding process. Table 1 shows the results of
applying syntactic constraints with different experi-
mental settings. From the table, we have the follow-
ing observations.
? Consistent with the conclusion in Koehn et
al. (2003), using the basic constraints is harmful to
HPB. Fortunately, our constraints consistently work
better than the basic constraints.
? Relaxing maximum phrase length in training and
maximum char span length in decoding, we obtain
an average improvement of about 1.0?1.2 BLEU
points for systems with both basic constraints and
our constraints. It is worth noting that after re-
laxing the lengths, the system with our constraints
performs on a par with the base HPB system (e.g.,
36.21 vs. 36.10).
System MT 02 MT 04 MT 05 Ave.
base HPB 40.00 35.33 32.97 36.10
+our constraints 40.01 36.24++ 33.65+ 36.71
with reorderable
glue rules
40.70+ 36.00+ 33.67+ 36.79
+PAS model 40.41+ 36.73++?? 34.24++? 37.13
Table 2: Effects of reorderable glue rules and the PAS
reordering model. +/++: significant over base HPB at
0.05/0.01; */**: significant over the system with reorder-
able glue rules at 0.05/0.01.
? Flattening parse trees further improves 0.4?0.5
BLEU points on average for systems with our syn-
tactic constraints. Our final system with constraints
outperforms the base HPB system with an average
of 0.6 BLEU points improvement (36.71 vs. 36.10).
Another advantage of applying syntactic constraints
is efficiency. By comparing the base HPB system
and the system with our syntactic constraints (i.e.,
the last row in Table 1), it is not surprising to ob-
serve that the size of rules extracted from training
data drops sharply from 193M in base HPB sys-
tem to 60M in the other. Moreover, the system
with constraints needs less decoding time than base
HPB does. Observation on 2002 NIST MT test data
(26 words per sentence on average) shows that basic
HPB system needs to fill 239 cells per sentence on
average in chart parsing while the other only needs
to fill 108 cells.
4.2 Effects of Reorderable Glue Rules
Based on the system with our syntactic constraints
and relaxed phrase lengths in training and decoding,
we replace traditional glue rules with reorderable
glue rules. Table 2 shows the results, from which
we find that the effect of reorderable glue rules is
elusive: surprisingly, it achieves 0.7 BLEU points
545
sentence length 1-10 11-20 21-30 31-40 41+ all
sentence count 337 1001 1052 768 590 3748
base HPB 32.21 37.51 36.71 34.96 35.00 35.73
+our constraints 31.70 37.57 37.10 36.20++ 35.78++ 36.39++
Table 3: Experimental results over different sentence
length on the three test sets. +/++: significant over base
HPB at 0.05/0.01.
improvement on NIST MT 2002 test set while hav-
ing negligible or even slightly negative impact on the
other two test sets. The reason of reorderable glue
rules having limited influence on translation results
over monotonic only glue rules may be due to that
the monotonic reordering overwhelms the inverted
one: estimated from training data, the probability of
the monotonic glue rule is 95.5%.
4.3 Effects of the PAS Reordering Model
Based on the system with reorderable glue rules, we
examine whether the PAS reordering model is capa-
ble of improving translation performance. The last
row in Table 2 presents the results . It shows the sys-
tem with the PAS reordering model obtains an aver-
age of 0.34 BLEU points over the system without it
(e.g., 37.13 vs. 36.79). It is interesting to note that it
achieves significant improvement on NIST MT 2004
and 2005 test sets (p < 0.05) while slightly lowering
performance on NIST MT 2002 test set (p > 0.05):
the surprising improvement of applying reorderable
glue rules on NIST MT 2002 test set leaves less
room for further improvement. Finally, it shows we
obtain an average improvement of 1.03 BLEU points
on the three test sets over the base HPB system.
5 Discussion and Future Work
The results in Table 1 demonstrate that significant
and sometimes substantial gains over baseline can
be obtained by incorporating hard syntactic con-
straints into the HPB model. Due to the capability of
translation phrases of arbitrary length, we conjecture
that the improvement of our system over the baseline
HPB system mostly comes from long sentences. To
test the conjecture, we combine all test sentences in
the three test sets and group them in terms of sen-
tence length. Table 3 presents the sentence distri-
bution and BLEU scores over different length. The
results validate our assumption that the system with
constraints outperforms the base systems on long
sentences (e.g., sentences with 20+ words).
Figure 3 displays a translation example which
shows the difference between the base HPB
system and the system with constraints. The
inappropriate translation of the base HPB
system can be mainly blamed on the rule
?
X[2,5] ? ?2??3X[4,5], X[4,5] the development of
?
,
where ?2 ??3 , a part of the subtree [0, 3] span-
ning from word 0 to 3, is translated immediately
to the right of X[2,5], making a direct impact that
subtree [0, 3] is translated discontinuously on the
target side. On the contrary, we can see that our
constraints are able to help select appropriate phrase
segments with respect to its syntactic structure.
Although our syntactic constraints apply on the
source side, they are completely ignorant of syn-
tax on the target side, which might result in ex-
cluding some useful translation rules. Let?s re-
visit the sentence in Figure 3, where we can see
that a transition rule spanning from word 0 to 5,
say
?
X[0,5] ? X[0,3]?4???5, X[0,3] depends on
?
is intuitive: the syntactic structure on the target side
satisfies the constraints, although that of the source
side doesn?t. One natural extension of this work,
therefore, would be to relax the constraints by in-
cluding translation rules whose syntactic structure
of either the source side or the target side satisfies
the constraints.
To illustrate how the PAS reordering model im-
pacts translation output, Figure 4 displays two trans-
lation examples of systems with or without it. The
predicate ??/convey in the first example has three
core arguments, i.e., A0, A2, and A1. The difference
between the two outputs is the reordering of A1 and
A2 while the PAS reordering model gives priority to
pattern VV A1 A2. In the second example, we clearly
observe two serious translation errors in the system
without PAS reordering model: ??/themA1 is un-
translated; ??/chinaA0 is moved to the immediate
right of predicate ??/allow and plays as direct ob-
ject.
Including the PAS reordering model improves the
BLEU scores. One further direction to refine the ap-
proach is to alleviate verb sparsity via verb classes.
Another direction is to include useful context in es-
timating reordering probability. For example, the
content of a temporal argument AM-TMP can be a
546
( ( ( ( ??   ?? )   ?)    ?? )   ?   ( ???   ( ( ( ??    ?? )   ? )   ??) )  ?)
    0      1      2      3     4       5        6        7     8      9      10 
X [0,1] : lot X [4,5] : depends on
X [2,5] : X [4,5]  the development of
X [6,9] : the devet. of the world sit. X [10,10] : . 
X [7,7] : sit.
X [6,7] : world X [7,7]
X [ 9,9] : devet.X [0,1] : lot 
X [0,3] : X [0,1]  development 
X [5,9] : depends on X [9,9]  of the X [6,7]  
X [0,10] : X [0,3]  X [5,9] .  
Figure 3: A translation example of the base HPB system (above) and the system with constraints (below).
 
[ ?] A0  [ ?] AM-ADVP  [ ? ?] A2  [ ??] PRE  ?? [ ?? ?? ? ??] A1   A0 1   AM-ADVP 2   A2 3   VV4   A1 5  
[south korean] [will] [deliver] hope [resume talks message] [to the dprk]       X 1   X 2   X 4   X 5   X 3  
[korean] [will] [convey] [to the] hope of [resuming talks information]         X 1   X 2   X 4   X 3   X 5  
Source 
w/o 
with 
Ref. south korean conveys its desire to resume talking with north korean                  ---- 
[ ??] A0  [ ???] AM-TMP  [ ??] PRE  [ ??] A1  [ ?? ?? ??] A2  ?       A0 1  AM-TMP 2  VV3  A1 4  A2 5  
[china] [friday] [allowed] [them] [to seoul th rough the philippines] .           X 1   X 2   X 3   X 4   X 5  
[friday] [allowed] [china] [to seoul through the philippines] .                  X 2   X 3   X 1   X 5  
Source 
w/o 
with 
Ref. in friday, china allowed them to travel to seoul through philippines .                  ---- 
Figure 4: Two translation examples of the system with/without PAS reordering model
short/simple phrase (e.g., friday) or a long/complex
one (e.g., when I was 20 years old), which has im-
pact on its reordering in translation.
6 Related Work
While there has been substantial work on linguis-
tically motivated SMT, we limit ourselves here
to several approaches that leverage syntactic con-
straints yet still allow cross-constituent transla-
tions. In terms of tree-based SMT with cross-
constituent translations, Cowan et al (2006) al-
lowed non-constituent sub phrases on the source
side and adopted phrase-based translation model for
modifiers in clauses. Marcu (2006) and Galley
et al (2006) inserted artificial constituent nodes in
parsing tree as to capture useful but non-constituent
phrases. The parse tree binarization approach
(Wang et al, 2007; Marcu, 2007) and the forest-
based approach (Mi et al, 2008) would also cover
non-constituent phrases to some extent. Shen et
al. (2010) defined well-formed dependency struc-
ture to cover uncompleted dependency structure in
translation rules. In addition to the fact that the
constraints of Shen et al (2010) and this paper
are based on different syntactic perspectives (i.e.,
dependency structure vs. constituency structure),
the major difference is that in this work we don?t
limit the length of phrases to a fixed maximum size
(e.g., 10 in Hiero). Consequently, we obtain some
translation rules that are not found in Hiero sys-
tems constrained by the length. In terms of (hi-
erarchical) phrase-based SMT with syntactic con-
straints, particular related to constituent boundaries,
Koehn et al (2003) tested constraints allowing con-
stituent matched phrases only. Chiang (2005) and
Cherry (2008) used a soft constraint to award or pe-
nalize hypotheses which respect or violate syntactic
boundaries. Marton and Resnik (2008) further ex-
plored the idea of soft constraints by distinguishing
among constituent types. Xiong et al (2009; 2010)
presented models that learn phrase boundaries from
aligned dataset.
On the other hand, semantics motivated SMT has
also seen an increase in activity recently. Wu and
547
Fung (2009) re-ordered arguments on the target side
translation output, seeking to maximize the cross-
lingual match of the semantic frames of the re-
ordered translation to that of the source sentence.
Liu and Gildea (2010) added two types of semantic
role features into a tree-to-string translation model.
Although Xiong et al (2012) and our work are both
focusing on source side PAS reordering, our model
differs from theirs in two main aspects: 1) we con-
sider reordering not only between an argument and
its predicate, but also between two arguments; and
2) our reordering model can naturally model cases
of untranslated arguments or predicates.
7 Conclusion
In this paper, we have presented an approach to
incorporating syntactic and semantic structures for
the HPB translation model. To accommodate the
close tie of semantic structures to syntax, we first
revisited the idea of hard syntactic constraints, and
we demonstrated that hard constraints can, in fact,
lead to significant improvement in translation qual-
ity when applied to Chiang?s HPB framework. Then
our PAS reordering model, thanks to the constraints
which guided translation phrases in favor of syntac-
tic boundaries, made further improvements by pre-
dicting reordering not only between an argument
and its predicate, but also between two arguments.
In the future work, we will extend the PAS reorder-
ing model to include useful context, e.g., the head
words and the syntactic categories of arguments.
Acknowledgments
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA. The
authors would like to thank three anonymous re-
viewers for providing helpful suggestions, and also
acknowledge Ke Wu and other CLIP labmates in
MT group for useful discussions. We also thank
creators of the valuable off-the-shelf NLP packages,
such as GIZA++ and Berkeley Parser.
References
Colin Cherry. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings of
ACL-HLT 2008, pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree transla-
tion. In Proceedings of EMNLP 2006, pages 232?241.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai
Wu. 2006. Automatic learning of Chinese-English
semantic structure mapping. In Proceedings of SLT
2006, pages 230?233.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of ACL-COLING 2006, pages 961?968.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings of ACL 2010, pages 1108?1117.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of COL-
ING 2010, pages 716?724.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of EMNLP 2006, pages 44?52.
Steve DeNeefe; Kevin Knight; Wei Wang; Daniel Marcu.
2007. What can syntax-based mt learn from phrase-
based mt? In Proceedings of EMNLP-CoNLL 2007,
pages 755?763.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-HLT 2008,
pages 192?199.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
548
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT 2007, pages 404?411.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine translation.
Computational Linguistics, 36(4):649?671.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of EMNLP-
CoNLL 2007, pages 746?754.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
NAACL-HLT 2009, pages 13?16.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009.
A syntax-driven bracketing model for phrase-based
translation. In Proceedings of ACL-IJCNLP 2009,
pages 315?323.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In Proceedings of NAACL-HLT 2010, pages 136?144.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of ACL 2012, pages 902?911.
Nianwen Xue. 2008. Automatic labeling of semantic
roles. Computational Linguistics, 34(4):225?255.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
EMNLP 2010, pages 304?314.
549
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108?1117,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Joint Syntactic and Semantic Parsing of Chinese 
Junhui Li  and  Guodong Zhou 
School of Computer Science & Technology
Soochow University 
Suzhou, China 215006 
{lijunhui, gdzhou}@suda.edu.cn
Hwee Tou Ng 
Department of Computer Science 
National University of Singapore 
13 Computing Drive, Singapore 117417
nght@comp.nus.edu.sg 
 
Abstract 
This paper explores joint syntactic and seman-
tic parsing of Chinese to further improve the 
performance of both syntactic and semantic 
parsing, in particular the performance of se-
mantic parsing (in this paper, semantic role 
labeling). This is done from two levels. Firstly, 
an integrated parsing approach is proposed to 
integrate semantic parsing into the syntactic 
parsing process. Secondly, semantic informa-
tion generated by semantic parsing is incorpo-
rated into the syntactic parsing model to better 
capture semantic information in syntactic 
parsing. Evaluation on Chinese TreeBank, 
Chinese PropBank, and Chinese NomBank 
shows that our integrated parsing approach 
outperforms the pipeline parsing approach on 
n-best parse trees, a natural extension of the 
widely used pipeline parsing approach on the 
top-best parse tree. Moreover, it shows that 
incorporating semantic role-related informa-
tion into the syntactic parsing model signifi-
cantly improves the performance of both syn-
tactic parsing and semantic parsing. To our 
best knowledge, this is the first research on 
exploring syntactic parsing and semantic role 
labeling for both verbal and nominal predi-
cates in an integrated way. 
1 Introduction 
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning. 
Due to the difficulty in deep semantic parsing, 
most previous work focuses on shallow semantic 
parsing, which assigns a simple structure (such 
as WHO did WHAT to WHOM, WHEN, 
WHERE, WHY, HOW) to each predicate in a 
sentence. In particular, the well-defined semantic 
role labeling (SRL) task has been drawing in-
creasing attention in recent years due to its im-
portance in natural language processing (NLP) 
applications, such as question answering (Nara-
yanan and Harabagiu, 2004), information extrac-
tion (Surdeanu et al, 2003), and co-reference 
resolution (Kong et al, 2009). Given a sentence 
and a predicate (either a verb or a noun) in the 
sentence, SRL recognizes and maps all the con-
stituents in the sentence into their corresponding 
semantic arguments (roles) of the predicate. In 
both English and Chinese PropBank (Palmer et 
al., 2005; Xue and Palmer, 2003), and English 
and Chinese NomBank (Meyers et al, 2004; Xue, 
2006), these semantic arguments include core 
arguments (e.g., Arg0 for agent and Arg1 for 
recipient) and adjunct arguments (e.g., 
ArgM-LOC for locative argument and 
ArgM-TMP for temporal argument). According 
to predicate type, SRL can be divided into SRL 
for verbal predicates (verbal SRL, in short) and 
SRL for nominal predicates (nominal SRL, in 
short).  
With the availability of large annotated cor-
pora such as FrameNet (Baker et al, 1998), 
PropBank, and NomBank in English, data-driven 
techniques, including both feature-based and 
kernel-based methods, have been extensively 
studied for SRL (Carreras and M?rquez, 2004; 
Carreras and M?rquez, 2005; Pradhan et al, 
2005; Liu and Ng, 2007). Nevertheless, for both 
verbal and nominal SRL, state-of-the-art systems 
depend heavily on the top-best parse tree and 
there exists a large performance gap between 
SRL based on the gold parse tree and the 
top-best parse tree. For example, Pradhan et al 
(2005) suffered a performance drop of 7.3 in 
F1-measure on English PropBank when using the 
top-best parse tree returned from Charniak?s 
parser (Charniak, 2001). Liu and Ng (2007) re-
ported a performance drop of 4.21 in F1-measure 
on English NomBank.  
Compared with English SRL, Chinese SRL 
suffers more seriously from syntactic parsing. 
Xue (2008) evaluated on Chinese PropBank and 
showed that the performance of Chinese verbal 
SRL drops by about 25 in F1-measure when re-
placing gold parse trees with automatic ones. 
Likewise, Xue (2008) and Li et al (2009) re-
ported a performance drop of about 12 in 
F1-measure in Chinese NomBank SRL. 
1108
While it may be difficult to further improve 
syntactic parsing, a promising alternative is to 
perform both syntactic and semantic parsing in 
an integrated way. Given the close interaction 
between the two tasks, joint learning not only 
allows uncertainty about syntactic parsing to be 
carried forward to semantic parsing but also al-
lows useful information from semantic parsing to 
be carried backward to syntactic parsing.  
This paper explores joint learning of syntactic 
and semantic parsing for Chinese texts from two 
levels. Firstly, an integrated parsing approach is 
proposed to benefit from the close interaction 
between syntactic and semantic parsing. This is 
done by integrating semantic parsing into the 
syntactic parsing process. Secondly, various se-
mantic role-related features are directly incorpo-
rated into the syntactic parsing model to better 
capture semantic role-related information in syn-
tactic parsing. Evaluation on Chinese TreeBank, 
Chinese PropBank, and Chinese NomBank 
shows that our method significantly improves the 
performance of both syntactic and semantic 
parsing. This is promising and encouraging. To 
our best knowledge, this is the first research on 
exploring syntactic parsing and SRL for verbal 
and nominal predicates in an integrated way.  
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 pre-
sents our baseline systems for syntactic and se-
mantic parsing. Section 4 presents our proposed 
method of joint syntactic and semantic parsing 
for Chinese texts. Section 5 presents the experi-
mental results. Finally, Section 6 concludes the 
paper. 
2 Related Work 
Compared to the large body of work on either 
syntactic parsing (Ratnaparkhi, 1999; Collins, 
1999; Charniak, 2001; Petrov and Klein, 2007), 
or SRL (Carreras and M?rquez, 2004; Carreras 
and M?rquez, 2005; Jiang and Ng, 2006), there is 
relatively less work on their joint learning.  
Koomen et al (2005) adopted the outputs of 
multiple SRL systems (each on a single parse 
tree) and combined them into a coherent predi-
cate argument output by solving an optimization 
problem. Sutton and McCallum (2005) adopted a 
probabilistic SRL system to re-rank the N-best 
results of a probabilistic syntactic parser. How-
ever, they reported negative results, which they 
blamed on the inaccurate probability estimates 
from their locally trained SRL model.  
As an alternative to the above pseudo-joint 
learning methods (strictly speaking, they are still 
pipeline methods), one can augment the syntactic 
label of a constituent with semantic information, 
like what function parsing does (Merlo and Mu-
sillo, 2005). Yi and Palmer (2005) observed that 
the distributions of semantic labels could poten-
tially interact with the distributions of syntactic 
labels and redefined the boundaries of constitu-
ents. Based on this observation, they incorpo-
rated semantic role information into syntactic 
parse trees by extending syntactic constituent 
labels with their coarse-grained semantic roles 
(core argument or adjunct argument) in the sen-
tence, and thus unified semantic parsing and 
syntactic parsing. The actual fine-grained seman-
tic roles are assigned, as in other methods, by an 
ensemble classifier. However, the results ob-
tained with this method were negative, and they 
concluded that semantic parsing on PropBank 
was too difficult due to the differences between 
chunk annotation and tree structure. Motivated 
by Yi and Palmer (2005), Merlo and Musillo 
(2008) first extended a statistical parser to pro-
duce a richly annotated tree that identifies and 
labels nodes with semantic role labels as well as 
syntactic labels. Then, they explored both 
rule-based and machine learning techniques to 
extract predicate-argument structures from this 
enriched output. Their experiments showed that 
their method was biased against these roles in 
general, thus lowering recall for them (e.g., pre-
cision of 87.6 and recall of 65.8).  
There have been other efforts in NLP on joint 
learning with various degrees of success. In par-
ticular, the recent shared tasks of CoNLL 2008 
and 2009 (Surdeanu et al, 2008; Hajic et al, 
2009) tackled joint parsing of syntactic and se-
mantic dependencies. However, all the top 5 re-
ported systems decoupled the tasks, rather than 
building joint models. Compared with the disap-
pointing results of joint learning on syntactic and 
semantic parsing, Miller et al (2000) and Finkel 
and Manning (2009) showed the effectiveness of 
joint learning on syntactic parsing and some 
simple NLP tasks, such as information extraction 
and name entity recognition. In addition, at-
tempts on joint Chinese word segmentation and 
part-of-speech (POS) tagging (Ng and Low, 
2004; Zhang and Clark, 2008) also illustrate the 
benefits of joint learning. 
 
1109
 3 Baseline: Pipeline Parsing on 
Top-Best Parse Tree 
In this section, we briefly describe our approach 
to syntactic parsing and semantic role labeling, 
as well as the baseline system with pipeline 
parsing on the top-best parse tree. 
3.1 Syntactic Parsing 
Our syntactic parser re-implements Ratnaparkhi 
(1999), which adopts the maximum entropy 
principle. The parser recasts a syntactic parse 
tree as a sequence of decisions similar to those 
of a standard shift-reduce parser and the parsing 
process is organized into three left-to-right 
passes via four procedures, called TAG, 
CHUNK, BUILD, and CHECK. 
First pass. The first pass takes a tokenized sen-
tence as input, and uses TAG to assign each 
word a part-of-speech.  
Second pass. The second pass takes the output 
of the first pass as input, and uses CHUNK to 
recognize basic chunks in the sentence.  
Third pass. The third pass takes the output of 
the second pass as input, and always alternates 
between BUILD and CHECK in structural pars-
ing in a recursive manner. Here, BUILD decides 
whether a subtree will start a new constituent or 
join the incomplete constituent immediately to 
its left. CHECK finds the most recently pro-
posed constituent, and decides if it is complete.  
3.2 Semantic Role Labeling 
Figure 1 demonstrates an annotation example of 
Chinese PropBank and NomBank. In the figure, 
the verbal predicate ???/provide? is annotated 
with three core arguments (i.e., ?NP (??
/Chinese ??/govt.)? as Arg0, ?PP (?/to ?
?/N. Korean ??/govt.)? as Arg2, and ?NP 
(???/RMB ??/loan)? as Arg1), while the 
nominal predicate ???/loan? is annotated with 
two core arguments (i.e., ?NP (??/Chinese ?
?/govt.)? as Arg1 and ?PP (?/to ??/N. Ko-
rean ??/govt.)? as Arg0), and an adjunct ar-
gument (i.e., ?NN ( ? ? ? /RMB)? as 
ArgM-MNR, denoting the manner of loan). It is 
worth pointing out that there is a (Chinese) 
NomBank-specific label in Figure 1, Sup (sup-
port verb) (Xue, 2006), to help introduce the 
arguments which occur outside the nominal pre-
dicate-headed noun phrase. In (Chinese) Nom-
Bank, a verb is considered to be a support verb 
only if it shares at least an argument with the 
nominal predicate. 
3.2.1 Automatic Predicate Recognition 
Automatic predicate recognition is a prerequisite 
for the application of SRL systems. For verbal 
predicates, it is very easy. For example, 99% of 
verbs are annotated as predicates in Chinese 
PropBank. Therefore, we can simply select any 
word with a part-of-speech (POS) tag of VV, 
VA, VC, or VE as verbal predicate. 
Unlike verbal predicate recognition, nominal 
predicate recognition is quite complicated. For 
Figure 1: Two predicates (Rel1 and Rel2) and their arguments in the style of Chinese PropBank and NomBank. 
?
to ?? 
N. Korean
??
govt.
?? 
provide
P 
NR NN
VV
NN NN 
NP
PP 
Arg0/Rel2 
Arg2/Rel1 
ArgM-MNR/Rel2 Rel2 
NP
VP
VP
???
RMB
??
loan 
? 
. 
NR NN
PU 
NP 
Arg1/Rel2
Arg0/Rel1
IP
?? 
Chinese 
?? 
govt. 
Sup/Rel2
Rel1
Chinese government provides RMB loan to North Korean government. 
Arg1/Rel1
TOP
1110
example, only 17.5% of nouns are annotated as 
predicates in Chinese NomBank. It is quite 
common that a noun is annotated as a predicate 
in some cases but not in others. Therefore, au-
tomatic predicate recognition is vital to nominal 
SRL. In principle, automatic predicate recogni-
tion can be cast as a binary classification (e.g., 
Predicate vs. Non-Predicate) problem. For no-
minal predicates, a binary classifier is trained to 
predict whether a noun is a nominal predicate or 
not. In particular, any word POS-tagged as NN 
is considered as a predicate candidate in both 
training and testing processes. Let the nominal 
predicate candidate be w0, and its left and right 
neighboring words/POSs be w-1/p-1and w1/p1, 
respectively. Table 1 lists the feature set used in 
our model. In Table 1, local features present the 
candidate?s contextual information while global 
features show its statistical information in the 
whole training set. 
 
Type Description 
w0, w-1, w1, p-1, p1 local 
features The first and last characters of the candidate
Whether w0 is ever tagged as a verb in the 
training data? Yes/No 
Whether w0 is ever annotated as a nominal 
predicate in the training data? Yes/No 
The most likely label for w0 when it occurs 
together with w-1 and w1. 
The most likely label for w0 when it occurs 
together with w-1. 
 
 
global 
features 
The most likely label for w0 when it occurs 
together with w1. 
Table 1: Feature set for nominal predicate recognition 
 
3.2.2 SRL for Chinese Predicates 
Our Chinese SRL models for both verbal and 
nominal predicates adopt the widely-used SRL 
framework, which divides the task into three 
sequential sub-tasks: argument pruning, argu-
ment identification, and argument classification. 
In particular, we follow Xue (2008) and Li et al 
(2009) to develop verbal and nominal SRL 
models, respectively. Moreover, we have further 
improved the performance of Chinese verbal 
SRL by exploring additional features, e.g., voice 
position that indicates the voice maker (BA, BEI) 
is before or after the constituent in focus, the 
rule that expands the parent of the constituent in 
focus, and the core arguments defined in the 
predicate?s frame file. For nominal SRL, we 
simply use the final feature set of Li et al (2009). 
As a result, our Chinese verbal and nominal SRL 
systems achieve performance of 92.38 and 72.67 
in F1-measure respectively (on golden parse 
trees and golden predicates), which are compa-
rable to Xue (2008) and Li et al (2009). For 
more details, please refer to Xue (2008) and Li 
et al (2009). 
3.3 Pipeline Parsing on Top-best Parse 
Tree 
Similar to most of the state-of-the-art systems 
(Pradhan et al, 2005; Xue, 2008; Li et al, 2009), 
the top-best parse tree is first returned from our 
syntactic parser and then fed into the SRL sys-
tem. Specifically, the verbal (nominal) SRL la-
beler is in charge of verbal (nominal) predicates, 
respectively. For each sentence, since SRL is 
only performed on one parse tree, only con-
stituents in it are candidates for semantic argu-
ments. Therefore, if no constituent in the parse 
tree can map the same text span to an argument 
in the manual annotation, the system will not get 
a correct annotation. 
4 Joint Syntactic and Semantic Parsing 
In this section, we first explore pipeline parsing 
on N-best parse trees, as a natural extension of 
pipeline parsing on the top-best parse tree. Then, 
joint syntactic and semantic parsing is explored 
for Chinese texts from two levels. Firstly, an 
integrated parsing approach to joint syntactic 
and semantic parsing is proposed. Secondly, 
various semantic role-related features are di-
rectly incorporated into the syntactic parsing 
model for better interaction between the two 
tasks. 
4.1 Pipeline Parsing on N-best Parse Trees 
The pipeline parsing approach employed in this 
paper is largely motivated by the general 
framework of re-ranking, as proposed in Sutton 
and McCallum (2005). The idea behind this ap-
proach is that it allows uncertainty about syntac-
tic parsing to be carried forward through an 
N-best list, and that a reliable SRL system, to a 
certain extent, can reflect qualities of syntactic 
parse trees. Given a sentence x, a joint parsing 
model is defined over a semantic frame F and a 
parse tree t in a log-linear way: 
( )
( ) ( ) ( )
, |
1 log | , log |
Score F t x
P F t x P t x? ?= ? +    (1) 
where P(t|x) is returned by a probabilistic syn-
tactic parsing model, e.g., our syntactic parser, 
and P(F|t, x) is returned by a probabilistic se-
mantic parsing model, e.g. our verbal & nominal 
1111
  
SRL systems. In our pipeline parsing approach, 
P(t|x) is calculated as the product of all involved 
decisions? probabilities in the syntactic parsing 
model, and P(F|t, x) is calculated as the product 
of all the semantic role labels? probabilities in a 
sentence (including both verbal and nominal 
SRL). That is to say, we only consider those 
constituents that are supposed to be arguments. 
Here, the parameter ?  is a balance factor in-
dicating the importance of the semantic parsing 
model. 
In particular, (F*, t*) with maximal Score(F, 
t|x) is selected as the final syntactic and seman-
tic parsing results. Given a sentence, N-best 
parse trees are generated first using the syntactic 
parser, and then for each parse tree, we predict 
the best SRL frame using our verbal and nomi-
nal SRL systems. 
4.2 Integrated Parsing 
Although pipeline parsing on N-best parse trees 
could relieve severe dependence on the quality 
of the top-best parse tree, there is still a potential 
drawback: this method suffers from the limited 
scope covered by the N-best parse trees since the 
items in the parse tree list may be too similar, 
especially for long sentences. For example, 
50-best parse trees can only represent a combi-
nation of 5 to 6 binary ambiguities since 2^5 < 
50 < 2^6. 
Ideally, we should perform SRL on as many 
parse trees as possible, so as to enlarge the 
search scope. However, pipeline parsing on all 
possible parse trees is time-consuming and thus 
unrealistic. As an alternative, we turn to inte-
grated parsing, which aims to perform syntactic 
and semantic parsing synchronously. The key 
idea is to construct a parse tree in a bottom-up 
way so that it is feasible to perform SRL at suit-
able moments, instead of only when the whole 
parse tree is built. Integrated parsing is practica-
ble, mostly due to the following two observa-
tions: (1) Given a predicate in a parse tree, its 
semantic arguments are usually siblings of the 
predicate, or siblings of its ancestor. Actually, 
this special observation has been widely em-
ployed in SRL to prune non-arguments for a 
verbal or nominal predicate (Xue, 2008; Li et al, 
2009). (2) SRL feature spaces (both in fea-
ture-based method and kernel-based method) 
mostly focus on the predicate-argument structure 
of a given (predicate, argument) pair. That is to 
say, once a predicate-argument structure is 
formed (i.e., an argument candidate is connected 
with the given predicate), there is enough con-
textual information to predict their SRL relation. 
As far as our syntactic parser is concerned, we 
invoke the SRL systems once a new constituent 
covering a predicate is complete with a ?YES? 
decision from the CHECK procedure. Algorithm 
Algorithm 1. The algorithm integrating syntactic parsing and SRL. 
Assume: 
  t: constituent which is complete with ?YES? decision of CHECK procedure 
  P: number of predicates 
  Pi: ith predicate 
  S: SRL result, set of predicates and its arguments 
BEGIN 
   srl_prob = 0.0; 
   FOR i=1 to P DO 
      IF t covers Pi THEN 
         T = number of children of t; 
         FOR j=1 to T DO 
             IF t?s jth child Chj does not cover Pi THEN 
                 Run SRL given predicate Pi and constituent Chj to get their semantic role 
lbl and its probability prob; 
                 IF lbl does not indicate non-argument THEN 
                    srl_prob += log( prob ); 
                    S = S ? {(Pi, Chj, lbl)}; 
                 END IF 
             END IF 
         END FOR 
      END IF 
   END FOR 
   return srl_prob; 
END 
1112
1 illustrates the integration of syntactic and se-
mantic parsing. For the example shown in Fig-
ure 2, the CHECK procedure predicts a ?YES? 
decision, indicating the immediately proposed 
constituent ?VP (?? /provide ??? /RMB 
??/loan)? is complete. So, at this moment, the 
verbal SRL system is invoked to predict the se-
mantic label of the constituent ?NP (???
/RMB ??/loan)?, given the verbal predicate 
?VV (??/provide)?. Similarly, ?PP (?/to ?
?/N. Korean ??/govt.)? would also be se-
mantically labeled as soon as ?PP (?/to ??/N. 
Korean ??/govt.)? and ?VP (??/provide ?
??/RMB ??/loan)? are merged into a big-
ger VP. In this way, both syntactic and semantic 
parsing are accomplished when the root node 
TOP is formed. It is worth pointing out that all 
features (Xue, 2008; Li et al, 2009) used in our 
SRL model can be instantiated and their values 
are same as the ones when the whole tree is 
available. In particular, the probability computed 
from the SRL model is interpolated with that of 
the syntactic parsing model in a log-linear way 
(with equal weights in our experiments). This is 
due to our hypothesis that the probability re-
turned from SRL model is helpful to joint syn-
tactic and semantic parsing, considering the 
close interaction between the two tasks. 
 
 
4.3 Integrating Semantic Role-related 
Features into Syntactic Parsing Model 
The integrated parsing approach as shown in 
Section 4.2 performs syntactic and semantic 
parsing synchronously. In contrast to traditional 
syntactic parsers where no semantic role-related 
information is used, it may be interesting to in-
vestigate the contribution of such information in 
the syntactic parsing model, due to the availabil-
ity of such information in the syntactic parsing 
process. In addition, it is found that 11% of pre-
dicates in a sentence are speculatively attached 
with two or more core arguments with the same 
label due to semantic parsing errors (partly 
caused by syntactic parsing errors in automatic 
parse trees). This is abnormal since a predicate 
normally only allows at most one argument of 
each core argument role (i.e., Arg0-Arg4). 
Therefore, such syntactic errors should be 
avoidable by considering those arguments al-
ready obtained in the bottom-up parsing process. 
On the other hand, taking those expected seman-
tic roles into account would help the syntactic 
parser. In terms of our syntactic parsing model, 
this is done by directly incorporating various 
semantic role-related features into the syntactic 
parsing model (i.e., the BUILD procedure) when 
the newly-formed constituent covers one or 
more predicates. 
For the example shown in Figure 2, once the 
constituent ?VP (?? /provide ??? /RMB 
??/loan)?, which covers a verbal predicate 
?VV (??/provide)?, is complete, the verbal 
SRL model would be triggered first to mark 
constituent ?NP (???/RMB ??/loan)? as 
ARG1, given predicate ?VV (??/provide)?. 
Then, the BUILD procedure is called to make 
the BUILD decision for the newly-formed con-
stituent ?VP (??/provide ???/RMB ??
/loan)?. Table 2 lists various semantic 
role-related features explored in our syntactic 
parsing model and their instantiations with re-
gard to the example shown in Figure 2. In Table 
2, feature sf4 gives the possible core semantic 
roles that the focus predicate may take, accord-
ing to its frame file; feature sf5 presents the se-
mantic roles that the focus predicate has already 
occupied; feature sf6 indicates the semantic 
roles that the focus predicate is expecting; and 
SF1-SF8 are combined features. Specifically, if 
the current constituent covers n predicates, then 
14 * n features would be instantiated. Moreover, 
we differentiate whether the focus predicate is 
verbal or nominal, and whether it is the head 
word of the current constituent. 
Feature Selection. Some features proposed 
above may not be effective in syntactic parsing. 
Here we adopt the greedy feature selection algo-
rithm as described in Jiang and Ng (2006) to 
select useful features empirically and incremen-
tally according to their contributions on the de-
velopment data. The algorithm repeatedly se-
lects one feature each time which contributes the 
most, and stops when adding any of the remain-
Figure 2: An application of CHECK with YES as the 
decision. Thus, VV (??/provide) and NP (???
/RMB ??/loan) reduce to a big VP. 
P NP 
PP 
Start_VP / NO 
VV NP
???
RMB
??
loan
NN NN?? 
provide 
?
to 
NR NN 
?? 
N. Korean 
?? 
govt. 
? ?
VP YES?
1113
ing features fails to improve the syntactic pars-
ing performance. 
 
Feat. Description 
sf1 Path: the syntactic path from C to P. (VP>VV)
sf2 Predicate: the predicate itself. (??/provide)
sf3 Predicate class (Xue, 2008): the class that P 
belongs to. (C3b) 
sf4 Possible roles: the core semantic roles P may 
take. (Arg0, Arg1, Arg2) 
sf5 Detected roles: the core semantic roles already 
assigned to P. (Arg1) 
sf6 Expected roles:  possible semantic roles P is 
still expecting. (Arg0, Arg2) 
SF1 For each already detected argument, its role 
label + its path from P. (Arg1+VV<VP>NP) 
SF2 sf1 + sf2. (VP>VV+??/provide) 
SF3 sf1 + sf3. (VP>VV+C3b) 
SF4 Combined possible argument roles. 
(Arg0+Arg1+Arg2) 
SF5 Combined detected argument roles. (Arg1) 
SF6 Combined expected argument roles. 
(Arg0+Arg2) 
SF7 For each expected semantic role, sf1 + its role 
label. (VP>VV+Arg0, VP>VV+Arg2) 
SF8 For each expected semantic role, sf2 + its role 
label. 
 (??/provide+Arg0, ??/provide+Arg2) 
Table 2: SRL-related features and their instantiations 
for syntactic parsing, with ?VP (??/provide ??
?/RMB ??/loan)? as the current constituent C 
and ???/provide? as the focus predicate P, based 
on Figure 2. 
5 Experiments and Results 
We have evaluated our integrated parsing ap-
proach on Chinese TreeBank 5.1 and corre-
sponding Chinese PropBank and NomBank.  
5.1 Experimental Settings 
This version of Chinese PropBank and Chinese 
NomBank consists of standoff annotations on 
the file (chtb 001 to 1151.fid) of Chinese Penn 
TreeBank 5.1. Following the experimental set-
tings in Xue (2008) and Li et al (2009), 648 
files (chtb 081 to 899.fid) are selected as the 
training data, 72 files (chtb 001 to 040.fid and 
chtb 900 to 931.fid) are held out as the test data, 
and 40 files (chtb 041 to 080.fid) are selected as 
the development data. In particular, the training, 
test, and development data contain 31,361 
(8,642), 3,599 (1,124), and 2,060 (731) verbal 
(nominal) propositions, respectively. 
For the evaluation measurement on syntactic 
parsing, we report labeled recall, labeled preci-
sion, and their F1-measure. Also, we report re-
call, precision, and their F1-measure for evalua-
tion of SRL on automatic predicates, combining 
verbal SRL and nominal SRL. An argument is 
correctly labeled if there is an argument in man-
ual annotation with the same semantic label that 
spans the same words. Moreover, we also report 
the performance of predicate recognition. To see 
whether an improvement in F1-measure is statis-
tically significant, we also conduct significance 
tests using a type of stratified shuffling which in 
turn is a type of compute-intensive randomized 
tests. In this paper, ?>>>?, ?>>?, and ?>? denote 
p-values less than or equal to 0.01, in-between 
(0.01, 0.05], and bigger than 0.05, respectively. 
We are not aware of any SRL system comb-
ing automatic predicate recognition, verbal SRL 
and nominal SRL on Chinese PropBank and 
NomBank. Xue (2008) experimented independ-
ently with verbal and nominal SRL and assumed 
correct predicates. Li et al (2009) combined 
nominal predicate recognition and nominal SRL 
on Chinese NomBank. The CoNLL-2009 shared 
task (Hajic et al, 2009) included both verbal and 
nominal SRL on dependency parsing, instead of 
constituent-based syntactic parsing. Thus the 
SRL performances of their systems are not di-
rectly comparable to ours. 
5.2 Results and Discussions 
Results of pipeline parsing on N-best parse 
trees. While performing pipeline parsing on 
N-best parse trees, 20-best (the same as the heap 
size in our syntactic parsing) parse trees are ob-
tained for each sentence using our syntactic 
parser as described in Section 3.1. The balance 
factor ?  is set to 0.5 indicating that the two 
components in formula (1) are equally important. 
Table 3 compares the two pipeline parsing ap-
proaches on the top-best parse tree and the 
N-best parse trees. It shows that the approach on 
N-best parse trees outperforms the one on the 
top-best parse tree by 0.42 (>>>) in F1-measure 
on SRL. In addition, syntactic parsing also bene-
fits from the N-best parse trees approach with 
improvement of 0.17 (>>>) in F1-measure. This 
suggests that pipeline parsing on N-best parse 
trees can improve both syntactic and semantic 
parsing. 
It is worth noting that our experimental results 
in applying the re-ranking framework in Chinese 
pipeline parsing on N-best parse trees are very 
encouraging, considering the pessimistic results 
of Sutton and McCallum (2005), in which the 
re-ranking framework failed to improve the per-
formance on English SRL. It may be because, 
1114
unlike Sutton and McCallum (2005), P(F, t|x) 
defined in this paper only considers those con-
stituents which are identified as arguments. This 
can effectively avoid the noises caused by the 
predominant non-argument constituents. More-
over, the huge performance gap between Chi-
nese semantic parsing on the gold parse tree and 
that on the top-best parse tree leaves much room 
for performance improvement. 
 
Method Task R (%) P (%) F1 
Syntactic 76.68 79.12 77.88
SRL 62.96 65.04 63.98
Predicate 94.18 92.28 93.22
V-SRL 65.33 68.52 66.88
V-Predicate 89.52 93.12 91.29
N-SRL 49.58 48.19 48.88
Pipeline on top 
-best parse tree 
N-Predicate 86.83 71.76 78.58
Syntactic 76.89 79.25 78.05
SRL 62.99 65.88 64.40
Predicate 94.07 92.22 93.13
V-SRL 65.41 69.09 67.20
V-Predicate 89.66 93.02 91.31
N-SRL 49.24 49.46 49.35
Pipeline on 20 
-best parse trees 
N-Predicate 86.65 72.15 78.74
Syntactic 77.14 79.01 78.07
SRL 62.67 67.67 65.07
Predicate 93.97 92.42 93.19
V-SRL 65.37 70.27 67.74
V-Predicate 90.08 92.87 91.45
N-SRL 48.02 52.83 50.31
Integrated 
parsing 
N-Predicate 85.41 73.23 78.85
Syntactic 77.47 79.58 78.51
SRL 63.14 68.17 65.56
Predicate 93.97 92.52 93.24
V-SRL 65.74 70.98 68.26
V-Predicate 89.86 93.17 91.49
N-SRL 48.80 52.67 50.66
Integrated 
parsing with 
semantic 
role-related 
features 
N-Predicate 85.85 72.78 78.78
Table 3: Syntactic and semantic parsing performance 
on test data (using gold standard word boundaries). 
?V-? denotes ?verbal? while ?N-?denotes ?nominal?. 
 
Results of integrated parsing. Table 3 also 
compares the integrated parsing approach with 
the two pipeline parsing approaches. It shows 
that the integrated parsing approach improves 
the performance of both syntactic and semantic 
parsing by 0.19 (>) and 1.09 (>>>) respectively 
in F1-measure over the pipeline parsing ap-
proach on the top-best parse tree. It is also not 
surprising to find out that the integrated parsing 
approach outperforms the pipeline parsing ap-
proach on 20-best parse trees by 0.67 (>>>) in 
F1-measure on SRL, due to its exploring a larger 
search space, although the integrated parsing 
approach integrates the SRL probability and the 
syntactic parsing probability in the same manner 
as the pipeline parsing approach on 20-best 
parse trees. However, the syntactic parsing per-
formance gap between the integrated parsing 
approach and the pipeline parsing approach on 
20-best parse trees is negligible.  
Results of integrated parsing with semantic 
role-related features. After performing the 
greedy feature selection algorithm on the devel-
opment data, features {SF3, SF2, sf5, sf6, SF4} 
as proposed in Section 4.3 are sequentially se-
lected for syntactic parsing. As what we have 
assumed, knowledge about the detected seman-
tic roles and expected semantic roles is helpful 
for syntactic parsing. Table 3 also lists the per-
formance achieved with those selected features. 
It shows that the integration of semantic 
role-related features in integrated parsing sig-
nificantly enhances both the performance of syn-
tactic and semantic parsing by 0.44 (>>>) and 
0.49 (>>) respectively in F1-measure. In addi-
tion, it shows that it outperforms the wide-
ly-used pipeline parsing approach on top-best 
parse tree by 0.63 (>>>) and 1.58 (>>>) in 
F1-measure on syntactic and semantic parsing, 
respectively. Finally, it shows that it outper-
forms the widely-used pipeline parsing approach 
on 20-best parse trees by 0.46 (>>>) and 1.16 
(>>>) in F1-measure on syntactic and semantic 
parsing, respectively. This is very encouraging, 
considering the notorious difficulty and 
complexity of both the syntactic and semantic 
parsing tasks. 
Table 3 also shows that our proposed method 
works well for both verbal SRL and nominal 
SRL. In addition, it shows that the performance 
of predicate recognition is very stable due to its 
high dependence on POS tagging results, rather 
than syntactic parsing results. Finally, it is not 
surprising to find out that the performance of 
predicate recognition when mixing verbal and 
nominal predicates is better than the perform-
ance of either verbal predicates or nominal 
predicates.  
5.3 Extending the Word-based Syntactic 
Parser to a Character-based Syntactic Parser 
The above experimental results on a word-based 
syntactic parser (assuming correct word seg-
mentation) show that both syntactic and seman-
tic parsing benefit from our integrated parsing 
approach. However, observing the great chal-
lenge of word segmentation in Chinese informa-
1115
tion processing, it is still unclear whether and 
how much joint learning benefits charac-
ter-based syntactic and semantic parsing. In this 
section, we extended the Ratnaparkhi parser 
(1999) to a character-based parser (with auto-
matic word segmentation), and then examined 
the effectiveness of joint learning.  
Given the three-pass process in the 
word-based syntactic parser, it is easy to extend 
it to a character-based parser for Chinese texts. 
This can be done by only replacing the TAG 
procedure in the first pass with a POSCHUNK 
procedure, which integrates Chinese word seg-
mentation and POS tagging in one step, follow-
ing the method described in (Ng and Low 2004). 
Here, each character is annotated with both a 
boundary tag and a POS tag. The 4 possible 
boundary tags include ?B? for a character that 
begins a word and is followed by another char-
acter, ?M? for a character that occurs in the 
middle of a word, ?E? for a character that ends a 
word, and ?S? for a character that occurs as a 
single-character word. For example, ????
/Beijing city/NR? would be decomposed into 
three units: ? ? /north/B_NR?, ? ?
/capital/M_NR?, and ??/city/E_NR?. Also, ??
/is/VC? would turn into ??/is/S_VC?. Through 
POSCHUNK, all characters in a sentence are 
first assigned with POS chunk labels which must 
be compatible with previous ones, and then 
merged into words with their POS tags. For ex-
ample, ??/north/B_NR?, ??/capital/M_NR?, 
and ??/city/E_NR? will be merged as ????
/Beijing/NR?, ??/is/S_VC? will become ??
/is/VC?. Finally the merged results of the PO-
SCHUNK are fed into the CHUNK procedure of 
the second pass. 
Using the same data split as the previous ex-
periments, word segmentation achieves perfor-
mance of 96.3 in F1-measure on the test data. 
Table 4 lists the syntactic and semantic parsing 
performance by adopting the character-based 
parser.  
Table 4 shows that integrated parsing benefits 
syntactic and semantic parsing when automatic 
word segmentation is considered. However, the 
improvements are smaller due to the extra noise 
caused by automatic word segmentation. For 
example, our experiments show that the per-
formance of predicate recognition drops from 
93.2 to 90.3 in F1-measure when replacing cor-
rect word segmentations with automatic ones. 
 
 
Method Task R (%) P (%) F1 
Syntactic 82.23 84.28 83.24Pipeline on top-best 
parse tree SRL 60.40 62.75 61.55
Syntactic 82.25 84.29 83.26Pipeline on 20-best 
parse trees SRL 60.17 63.63 61.85
Syntactic 82.51 84.31 83.40Integrated parsing  
with semantic 
role-related features
SRL 60.09 65.35 62.61
Table 4: Performance with the character-based pars-
er1 (using automatically recognized word bounda-
ries). 
6 Conclusion 
In this paper, we explore joint syntactic and se-
mantic parsing to improve the performance of 
both syntactic and semantic parsing, in particular 
that of semantic parsing. Evaluation shows that 
our integrated parsing approach outperforms the 
pipeline parsing approach on N-best parse trees, 
a natural extension of the widely-used pipeline 
parsing approach on the top-best parse tree. It 
also shows that incorporating semantic informa-
tion into syntactic parsing significantly improves 
the performance of both syntactic and semantic 
parsing. This is very promising and encouraging, 
considering the complexity of both syntactic and 
semantic parsing. 
To our best knowledge, this is the first suc-
cessful research on exploring syntactic parsing 
and semantic role labeling for verbal and nomi-
nal predicates in an integrated way.  
Acknowledgments 
The first two authors were financially supported 
by Projects 60683150, 60970056, and 90920004 
under the National Natural Science Foundation 
of China. This research was also partially sup-
ported by a research grant R-252-000-225-112 
from National University of Singapore Aca-
demic Research Fund. We also want to thank the 
reviewers for insightful comments. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING-ACL 1998. 
Xavier Carreras and Lluis M?rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2004.  
                                                          
1 POS tags are included in evaluating the perform-
ance of a character-based syntactic parser. Thus it 
cannot be directly compared with the word-based one 
where correct word segmentation is assumed. 
1116
Xavier Carreras and Lluis M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Eugene Charniak. 2001. Immediate-Head Parsing for 
Language Models. In Proceedings of ACL 2001. 
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Jenny Rose Finkel and Christopher D. Manning. 
2009. Joint Parsing and Named Entity Recognition. 
In Proceedings of NAACL 2009. 
Jan Hajic, Massimiliano Ciaramita, Richard Johans-
son, et al 2009. The CoNLL-2009 Shared Task: 
Syntactic and Semantic Dependencies in Multiple 
Languages. In Proceedings of CoNLL 2009. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
Role Labeling of NomBank: A Maximum Entropy 
Approach. In Proceedings of EMNLP 2006.  
Fang Kong, Guodong Zhou, and Qiaoming Zhu. 2009. 
Employing the Centering Theory in Pronoun 
Resolution from the Semantic Perspective. In 
Proceedings of EMNLP 2009.  
Peter Koomen, Vasin Punyakanok, Dan Roth, 
Wen-tau Yih. 2005. Generalized Inference with 
Multiple Semantic Role Labeling Systems. In 
Proceedings of CoNLL 2005. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. 2009. Improving Nominal SRL in 
Chinese Language with Verbal SRL information 
and Automatic Predicate Recognition. In Pro-
ceedings of EMNLP 2009. 
Chang Liu and Hwee Tou Ng. 2007. Learning Pre-
dictive Structures for Semantic Role Labeling of 
NomBank. In Proceedings of ACL 2007. 
Paola Merlo and Gabriele Mussillo. 2005. Accurate 
Function Parsing. In Proceedings of EMNLP 2005. 
Paola Merlo and Gabriele Musillo. 2008. Semantic 
Parsing for High-Precision Semantic Role Label-
ling. In Proceedings of CoNLL 2008. 
Adam Meyers, Ruth Reeves, Catherine Macleod, 
Rachel Szekely, Veronika Zielinska, Brian Young, 
and Ralph Grishman. 2004. Annotating Noun Ar-
gument Structure for NomBank. In Proceedings of 
LREC 2004. 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A Novel Use of Statistical 
Parsing to Extract Information from Text. In Pro-
ceedings of ANLP 2000. 
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In 
Proceedings of COLING 2004. 
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese 
Part-of-Speech Tagging: One-at-a-Time or 
All-at-Once? Word-Based or Character-Based? In 
Proceedings of EMNLP 2004. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics, 
31, 71-106. 
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of 
NAACL 2007. 
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005. Support Vector Learning for Semantic 
Argument Classification. Machine Learning, 2005, 
60:11-39.  
Adwait Ratnaparkhi. 1999. Learning to Parse Natural 
Language with Maximum Entropy Models. Ma-
chine Learning, 34, 151-175. 
Mihai Surdeanu, Sanda Harabagiu, John Williams 
and Paul Aarseth. 2003. Using Predi-
cate-Argument Structures for Information Extrac-
tion. In Proceedings of ACL 2003. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of 
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008. 
Charles Sutton and Andrew McCallum. 2005. Joint 
Parsing and Semantic Role Labeling. In Proceed-
ings of CoNLL2005. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
the Propositions in the Penn Chinese TreeBank. In 
Proceedings of the 2nd SIGHAN Workshop on 
Chinese Language Processing. 
Nianwen Xue. 2006. Annotating the Predi-
cate-Argument Structure of Chinese Nominaliza-
tions. In Proceedings of LREC 2006.  
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
Szu-ting Yi and Martha Palmer. 2005. The Integra-
tion of Syntactic Parsing and Semantic Role La-
beling. In Proceedings of CoNLL 2005. 
Yue Zhang and Stephen Clark. 2008. Joint Word 
Segmentation and POS Tagging Using a Single 
Perceptron. In Proceedings of ACL 2008. 
 
1117
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33?37,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Head-Driven Hierarchical Phrase-based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
This paper presents an extension of Chi-
ang?s hierarchical phrase-based (HPB) model,
called Head-Driven HPB (HD-HPB), which
incorporates head information in translation
rules to better capture syntax-driven infor-
mation, as well as improved reordering be-
tween any two neighboring non-terminals at
any stage of a derivation to explore a larger
reordering search space. Experiments on
Chinese-English translation on four NIST MT
test sets show that the HD-HPB model signifi-
cantly outperforms Chiang?s model with aver-
age gains of 1.91 points absolute in BLEU.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. Due to lack of linguistic knowledge,
Chiang?s HPB model contains only one type of non-
terminal symbol X , often making it difficult to se-
lect the most appropriate translation rules.1 What
is more, Chiang?s HPB model suffers from limited
phrase reordering combining translated phrases in a
monotonic way with glue rules. In addition, once a
1Another non-terminal symbol S is used in glue rules.
glue rule is adopted, it requires all rules above it to
be glue rules.
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble. Inspired by previous work in parsing (Char-
niak, 2000; Collins, 2003), our Head-Driven HPB
(HD-HPB) model is based on the intuition that lin-
guistic heads provide important information about a
constituent or distributionally defined fragment, as
in HPB. We identify heads using linguistically mo-
tivated dependency parsing, and use their POS to
refine X. In addition HD-HPB provides flexible re-
ordering rules freely mixing translation and reorder-
ing (including swap) at any stage in a derivation.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
33
 ??/NR 
Ouzhou 
??/NN 
baguo 
??/AD 
lianming 
??/VV 
zhichi 
??/NR 
meiguo 
??/NN 
lichang 
root 
Eight European countries jointly support America?s stand 
Figure 1: An example word alignment for a Chinese-
English sentence pair with the dependency parse tree for
the Chinese sentence. Here, each Chinese word is at-
tached with its POS tag and Pinyin.
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simulta-
neously using a context-free grammar. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), given a
word sequence f ij from position i to position j, we
first find heads and then concatenate the POS tags
of these heads as f ij?s non-terminal symbol. Specif-
ically, we adopt unlabeled dependency structure to
derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 1
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
According to the occurrence of terminals in
translation rules, we group rules in the HD-HPB
model into two categories: head-driven hierarchical
rules (HD-HRs) and non-terminal reordering rules
(NRRs), where the former have at least one terminal
on both source and target sides and the later have no
terminals. For rule extraction, we first identify ini-
tial phrase pairs on word-aligned sentence pairs by
using the same criterion as most phrase-based trans-
lation models (Och and Ney, 2004) and Chiang?s
HPB model (Chiang, 2005; Chiang, 2007). We
extract HD-HRs and NRRs based on initial phrase
pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that contain
other phrases and then replace sub-phrases with POS
tags corresponding to their heads. Given the word
alignment in Figure 1, Table 1 demonstrates the dif-
ference between hierarchical rules in Chiang (2007)
and HD-HRs defined here.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair on the source side, there are
four possible positional relationships for their target
side translations (we use Y as a variable for non-
terminals on the source side while all non-terminals
on the target side are labeled as X):
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
34
phrase pairs hierarchical rule head-driven hierarchical rule
lichang, stand X?lichang, stand
NN?lichang,
X?stand
meiguo lichang1, America?s stand1 X?meiguo X1, America?s X1
NN?meiguo NN1,
X?America?s X1
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 lichang,
support America?s1 stand
X?X1 lichang,
X1 stand
VV?VV-NR1 lichang,
X?X1 stand
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
2.3 Features and Decoding
Given e for the translation output in the target lan-
guage, s and t for strings of terminals and non-
terminals on the source and target side, respectively,
we use a feature set analogous to the default feature
set of Chiang (2007), including:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
Our decoder is based on CKY-style chart parsing
with beam search and searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang?s HPB model, it is pos-
sible for a non-terminal generated by a NRR to be
included afterwards by a HD-HR or another NRR.
3 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for our
HD-HPB and HPB systems, including initial phrase
length (as 10) in training, the maximum number of
non-terminals (as 2) in translation rules, maximum
number of non-terminals plus terminals (as 5) on
the source, beam threshold ? (as 10?5) (to discard
derivations with a score worse than ? times the best
score in the same chart cell), beam size b (as 200)
(i.e. each chart cell contains at most b derivations).
For Moses HPB, we use ?grow-diag-final-and? to
obtain symmetric word alignments, 10 for the max-
imum phrase length, and the recommended default
values for all other parameters.
We train our model on a dataset with ?1.5M sen-
tence pairs from the LDC dataset.2 We use the
2002 NIST MT evaluation test data (878 sentence
pairs) as the development data, and the 2003, 2004,
2005, 2006-news NIST MT evaluation test data
(919, 1788, 1082, and 616 sentence pairs, respec-
tively) as the test data. To find heads, we parse the
source sentences with the Berkeley Parser3 (Petrov
and Klein, 2007) trained on Chinese TreeBank 6.0
and use the Penn2Malt toolkit4 to obtain (unlabeled)
dependency structures.
We obtain the word alignments by running
2This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
35
GIZA++ (Och and Ney, 2000) on the corpus in both
directions and applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
BLEU scores. To test whether a performance differ-
ence is statistically significant, we conduct signifi-
cance tests following the paired bootstrap approach
(Koehn, 2004). In this paper,?**? and?*? de-
note p-values less than 0.01 and in-between [0.01,
0.05), respectively.
Table 2 lists the rule table sizes. The full rule ta-
ble size (including HD-HRs and NRRs) of our HD-
HPB model is ?1.5 times that of Chiang?s, largely
due to refining the non-terminal symbol X in Chi-
ang?s model into head-informed ones in our model.
It is also unsurprising, that the test set-filtered rule
table size of our model is only ?0.7 times that of Chi-
ang?s: this is due to the fact that some of the refined
translation rule patterns required by the test set are
unattested in the training data. Furthermore, the rule
table size of NRRs is much smaller than that of HD-
HRs since a NRR contains only two non-terminals.
Table 3 lists the translation performance with
BLEU scores. Note that our re-implementation of
Chiang?s original HPB model performs on a par with
Moses HPB. Table 3 shows that our HD-HPB model
significantly outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU (and sim-
ilar improvements over Moses HPB).
Table 3 shows that the head-driven scheme out-
performs a SAMT-style approach (for each test set
p < 0.01), indicating that head information is more
effective than (partial) CFG categories. Taking lian-
ming zhichi in Figure 1 as an example, HD-HPB
labels the span VV, as lianming is dominated by
zhichi, effecively ignoring lianming in the transla-
tion rule, while the SAMT label is ADVP:AD+VV5
which is more susceptible to data sparsity. In addi-
tion, SAMT resorts to X if a text span fails to satisify
pre-defined categories. Examining initial phrases
5the constituency structure for lianming zhichi is (VP (ADVP
(AD lianming)) (VP (VV zhichi) ...)).
System Total MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6 2.8 4.7 3.3 3.0 3.4
HD-HPB 59.5/0.6 1.9/0.1 3.4/0.2 2.3/0.2 2.0/0.1 2.4/0.2
Table 2: Rule table sizes (in million) of different mod-
els. Note: 1) For HD-HPB, the rule sizes separated by /
indicate HD-HRs and NRRs, respectively; 2) Except for
?Total?, the figures correspond to rules filtered on the cor-
responding test set.
System MT 03 MT 04 MT 05 MT 06 Avg.
Moses HPB 32.94* 35.16 32.18 29.88* 32.54
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50** 37.61** 34.56** 31.78** 34.86
SAMT-HPB 34.07 36.52** 32.90* 30.66 33.54
HD-HR+Glue 34.58** 36.55** 33.84** 31.06 34.01
Table 3: BLEU (%) scores of different models. Note:
1) SAMT-HPB indicates our HD-HPB model with non-
terminal scheme of Zollmann and Venugopal (2006);
2) HD-HR+Glue indicates our HD-HPB model replac-
ing NRRs with glue rules; 3) Significance tests for
Moses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glue
are done against HPB.
extracted from the SAMT training data shows that
28% of them are labeled as X.
In order to separate out the individual contribu-
tions of the novel HD-HRs and NRRs, we carry out
an additional experiment (HD-HR+Glue) using HD-
HRs with monotonic glue rules only (adjusted to re-
fined rule labels, but effectively switching off the ex-
tra reordering power of full NRRs). Table 3 shows
that on average more than half of the improvement
over HPB (Chiang and Moses) comes from the re-
fined HD-HRs, the rest from NRRs.
Examining translation rules extracted from the
training data shows that there are 72,366 types of
non-terminals with respect to 33 types of POS tags.
On average each sentence employs 16.6/5.2 HD-
HRs/NRRs in our HD-HPB model, compared to
15.9/3.6 hierarchical rules/glue rules in Chiang?s
model, providing further indication of the impor-
tance of NRRs in translation.
4 Conclusion
We present a head-driven hierarchical phrase-based
(HD-HPB) translation model, which adopts head in-
formation (derived through unlabeled dependency
analysis) in the definition of non-terminals to bet-
ter differentiate among translation rules. In ad-
36
dition, improved and better integrated reordering
rules allow better reordering between consecutive
non-terminals through exploration of a larger search
space in the derivation. Experimental results on
Chinese-English translation across four test sets
demonstrate significant improvements of the HD-
HPB model over both Chiang?s HPB and a source-
side SAMT-style refined version of HPB.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
37
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1123?1133,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Unified Model for Soft Linguistic Reordering Constraints
in Statistical Machine Translation
Junhui Li
?
Yuval Marton
?
Philip Resnik
?
Hal Daum
?
e III
?
?
UMIACS, University of Maryland, College Park, MD
{lijunhui, resnik, hal}@umiacs.umd.edu
?
Microsoft Corp., City Center Plaza, Bellevue, WA
yumarton@microsoft.com
Abstract
This paper explores a simple and effec-
tive unified framework for incorporating
soft linguistic reordering constraints into a
hierarchical phrase-based translation sys-
tem: 1) a syntactic reordering model
that explores reorderings for context free
grammar rules; and 2) a semantic re-
ordering model that focuses on the re-
ordering of predicate-argument structures.
We develop novel features based on both
models and use them as soft constraints
to guide the translation process. Ex-
periments on Chinese-English translation
show that the reordering approach can sig-
nificantly improve a state-of-the-art hier-
archical phrase-based translation system.
However, the gain achieved by the seman-
tic reordering model is limited in the pres-
ence of the syntactic reordering model,
and we therefore provide a detailed analy-
sis of the behavior differences between the
two.
1 Introduction
Reordering models in statistical machine transla-
tion (SMT) model the word order difference when
translating from one language to another. The
popular distortion or lexicalized reordering mod-
els in phrase-based SMT make good local pre-
dictions by focusing on reordering on word level,
while the synchronous context free grammars in
hierarchical phrase-based (HPB) translation mod-
els are capable of handling non-local reordering
on the translation phrase level. However, reorder-
ing, especially without any help of external knowl-
edge, remains a great challenge because an ac-
curate reordering is usually beyond these word
level or translation phrase level reordering mod-
els? ability. In addition, often these translation
models fail to respect linguistically-motivated syn-
tax and semantics. As a result, they tend to pro-
duce translations containing both syntactic and se-
mantic reordering confusions. In this paper our
goal is to take advantage of syntactic and seman-
tic parsing to improve translation quality. Rather
than introducing reordering models on either the
word level or the translation phrase level, we pro-
pose a unified approach to modeling reordering on
the linguistic unit level, e.g., syntactic constituents
and semantic roles. The reordering unit falls into
multiple granularities, from single words to more
complex constituents and semantic roles, and of-
ten crosses translation phrases. To show the ef-
fectiveness of our reordering models, we integrate
both syntactic constituent reordering models and
semantic role reordering models into a state-of-
the-art HPB system (Chiang, 2007; Dyer et al,
2010). We further contrast it with a stronger base-
line, already including fine-grained soft syntac-
tic constraint features (Marton and Resnik, 2008;
Chiang et al, 2008). The general ideas, however,
are applicable to other translation models, e.g.,
phrase-based model, as well.
Our syntactic constituent reordering model con-
siders context free grammar (CFG) rules in the
source language and predicts the reordering of
their elements on the target side, using word align-
ment information. Due to the fact that a con-
stituent, especially a long one, usually maps into
multiple discontinuous blocks in the target lan-
guage, there is more than one way to describe the
monotonicity or swapping patterns; we therefore
design two reordering models: one is based on the
leftmost aligned target word and the other based
on the rightmost target word.
While recently there has also been some encour-
aging work on incorporating semantic structure
(or, more specifically, predicate-argument struc-
ture: PAS) reordering in SMT, it is still an open
question whether semantic structure reordering
1123
strongly overlaps with syntactic structure reorder-
ing, since the semantic structure is closely tied to
syntax. To this end, we employ the same reorder-
ing framework as syntactic constituent reordering
and focus on semantic roles in a PAS. We then an-
alyze the differences between the syntactic and se-
mantic features.
The contributions of this paper include the fol-
lowing:
? We introduce novel soft reordering con-
straints, using syntactic constituents or se-
mantic roles, composed over word alignment
information in translation rules used during
decoding time;
? We introduce a unified framework to incor-
porate syntactic and semantic reordering con-
straints;
? We provide a detailed analysis providing in-
sight into why the semantic reordering model
is significantly less effective when syntactic
reordering features are also present.
The rest of the paper is organized as follows.
Section 2 provides an overview of HPB transla-
tion model. Section 3 describes the details of our
unified reordering models. Section 4 gives our ex-
perimental results and Section 5 discusses the be-
havior difference between syntactic constituent re-
ordering and semantic role reordering. Section 6
reviews related work and, finally Section 7 con-
cludes the paper.
2 HPB Translation Model: an Overview
In HPB models (Chiang, 2007), synchronous rules
take the formX ? ??, ?,??, whereX is the non-
terminal symbol, ? and ? are strings of lexical
items and non-terminals in the source and target
side, respectively, and ? indicates the one-to-one
correspondence between non-terminals in ? and ?.
Each such rule is associated with a set of transla-
tion model features {?
i
}, such as phrase transla-
tion probability p (? | ?) and its inverse p (? | ?),
the lexical translation probability p
lex
(? | ?) and
its inverse p
lex
(? | ?), and a rule penalty that af-
fects preference for longer or shorter derivations.
Two other widely used features are a target lan-
guage model feature and a target word penalty.
Given a derivation d, its translation log-
probability is estimated as:
logP (d) ?
?
i
?
i
?
i
(d)
(1)
	 ?
PAS	 ?
A0	 ?(NP)	 ? TMP	 ?(NP)	 ? Pre	 ?(VBD)	 ? A1	 ?(NP)	 ?Applicants	 ?	 ?	 ?	 ?	 ?	 ?yesterday	 ?	 ?	 ?	 ?	 ?	 ?filled	 ?	 ?	 ?	 ?	 ?	 ?the	 ?forms	 ?
Figure 1: Example of predicate-argument struc-
ture.
where ?
i
is the corresponding weight of feature ?
i
.
See (Chiang, 2007) for more details.
3 Unified Linguistic Reordering Models
As mentioned earlier, the linguistic reordering unit
is the syntactic constituent for syntactic reorder-
ing, and the semantic role for semantic reordering.
The syntactic reordering model takes a CFG rule
(e.g., VP ? VP PP PP) and models the reorder-
ing of the constituents on the left hand side by ex-
amining their translation or visit order according
to the target language. For the semantic reorder-
ing model, it takes a PAS and models its reorder-
ing on the target side. Figure 1 shows an example
of a PAS where the predicate (Pre) has two core
arguments (A0 and A1) and one adjunct (TMP).
Note that we refer all core arguments, adjuncts,
and predicates as semantic roles; thus we say the
PAS in Figure 1 has 4 roles. According to the an-
notation principles in (Chinese) PropBank (Palmer
et al, 2005; Xue and Palmer, 2009), all the roles
in a PAS map to a corresponding constituent in the
parse tree, and these constituents (e.g., NPs and
VBD in Figure 1) do not overlap with each other.
Next, we use a CFG rule to describe our syn-
tactic reordering model. Treating the two forms
of reorderings in a unified way, the semantic re-
ordering model is obtainable by regarding a PAS
as a CFG rule and considering a semantic role as a
constituent.
Because the translation of a source constituent
might result in multiple discontinuous blocks,
there can be several ways to describe or group
the reordering patterns. Therefore, we design
two general constituent reordering sub-models.
One is based on the leftmost aligned word (left-
most reordering model) and the other is based on
the rightmost aligned word (rightmost reordering
model), as follows. Figure 2 shows the model-
ing steps for the leftmost reordering model. Fig-
ure 2(a) is an example of a CFG rule in the source
1124
	 ?
XP	 ?XP1	 ? XP2	 ? XP3	 ? XP4	 ?f3	 ?	 ?f4	 ? f5	 ?	 ? f6	 ?	 ?f7	 ? f8	 ?...	 ?
...	 ?
...	 ?
...	 ?
?	 ?	 ?	 ?e2	 ?	 ?	 ?	 ?	 ?e3	 ?	 ?	 ?	 ?e4	 ?	 ?	 ?	 ?e5	 ?	 ?	 ?	 ?e6	 ?	 ?	 ?	 ?e7	 ?	 ?	 ?	 ?e8	 ?	 ?	 ?	 ?e9	 ?	 ??	 ? XP1	 ? XP2	 ? XP3	 ? XP4	 ?e2	 ? e3	 ? e5	 ?(a)	 ?a	 ?CFG	 ?rule	 ?and	 ?its	 ?alignment	 ? (b)	 ?leftmost	 ?aligned	 ?target	 ?words	 ?
XP1	 ? XP2	 ? XP3	 ? XP4	 ?1	 ? 4	 ? 2	 ? 3	 ? XP1	 ? XP2	 ? XP3	 ? XP4	 ?DM	 ? DS	 ? M	 ?(c)	 ?visit	 ?order	 ? (d)	 ?reordering	 ?types	 ?
Figure 2: Modeling process illustration for leftmost reordering model.
parse tree and its word alignment links to the target
language. Note that constituent XP
4
, which covers
word f
8
, has no alignment. Then for each XP
i
, we
find the leftmost target word which is aligned to a
source word covered by XP
i
. Figure 2(b) shows
that the leftmost target words for XP
1
, XP
2
, and
XP
3
are e
2
, e
5
, and e
3
, respectively, while XP
4
has no aligned target word. Then we get visit
order V = {v
i
} for {XP
i
} in the transformation
from Figure 2(b) to Figure 2(c), with the follow-
ing strategies for special cases:
? if the first constituent XP
1
is unaligned, we
add a NULL word at the beginning of the tar-
get side and link XP
1
to the NULL word;
? if a constituent XP
i
(i > 1) is unaligned, we
add a link to the target word which is aligned
to XP
i?1
, e.g., XP
4
will be linked to e
3
; and
? if k constituents XP
m
1
. . .XP
m
k
(m
1
<
. . . < m
k
) are linked to the same target word,
then v
m
i
= v
m
i+1
? 1, e.g., since XP
3
and
XP
4
are both linked to e
3
, then v
3
= v
4
? 1.
Finally Figure 2(d) converts the visit order V =
{v
1
, . . . v
n
} into a sequence of leftmost reordering
types LRT = {lrt
1
, . . . , lrt
n?1
}. For every two
adjacent constituents XP
i
and XP
i+1
with corre-
sponding visit order v
i
and v
i+1
, their reordering
could be one of the following:
? Monotone (M) if v
i+1
= v
i
+ 1;
? Discontinuous Monotone (DM) if v
i+1
> v
i
+ 1;
? Swap (S) if v
i+1
= v
i
? 1;
? Discontinuous Swap (DS) if v
i+1
< v
i
? 1.
Up to this point, we have generated a se-
quence of leftmost reordering types LRT =
{lrt
1
, . . . , lrt
n?1
} for a given CFG rule cfg:
XP ? XP
1
. . .XP
n
. The leftmost reordering
model takes the following form:
score
lrt
(cfg) = P
l
(lrt
1
, . . . , lrt
n?1
| ? (cfg))
(2)
where ? (cfg) indicates the surrounding context of
the CFG. By assuming that any two reordering
types in LRT = {lrt
1
, . . . , lrt
n?1
} are indepen-
dent of each other, we reformulate Eq. 2 into:
score
lrt
(cfg) =
n?1
?
i=1
P
l
(lrt
i
| ? (cfg))
(3)
Similarly, the sequence of rightmost reordering
types RRT can be decided for a CFG rule XP ?
XP
1
. . .XP
n
.
Accordingly, for a PAS pas: PAS ? R
1
. . .R
n
,
we can obtain its sequences of leftmost and right-
most reordering types by using the same way de-
scribed above.
3.1 Probability Estimation
In order to predict either the leftmost or right-
most reordering type for two adjacent constituents,
we use a maximum entropy classifier to esti-
mate the probability of the reordering type rt ?
{M,DM,S,DS} as follows:
P (rt | ? (cfg)) =
exp (
?
k
?
k
f
k
(rt, ? (cfg)))
?
rt
?
exp (
?
k
?
k
f
i
(rt
?
, ? (cfg)))
(4)
where f
k
are binary features, ?
k
are the weights of
these features. Most of our features f
k
are syntax-
based. For XP
i
and XP
i+1
in cfg, the features
1125
#Index Feature
cf1 L(XP
i
) & L(XP
i+1
) & L(XP)
cf2
for each XP
j
(j < i)
L(XP
i
) & L(XP
i+1
) & L(XP) & L(XP
j
)
cf3
for each XP
j
(j > i+ 1)
L(XP
i
) & L(XP
i+1
) & L(XP) & L(XP
j
)
cf4 L(XP
i
) & L(XP
i+1
) & P(XP
i
)
cf5 L(XP
i
) & L(XP
i+1
) &H(XP
i
)
cf6 L(XP
i
) & L(XP
i+1
) & P(XP
i+1
)
cf7 L(XP
i
) & L(XP
i+1
) &H(XP
i+1
)
cf8 L(XP
i
) & L(XP
i+1
) & S(XP
i
)
cf9 L(XP
i
) & L(XP
i+1
) & S(XP
i+1
)
cf10 L(XP
i
) & L(XP)
cf11 L(XP
i+1
) & L(XP)
Table 1: Features adopted in the syntactic leftmost
and rightmost reordering models. L (XP) returns
the syntactic category of XP, e.g., NP, VP, PP etc.;
H (XP) returns the head word of XP; P (XP) re-
turns the POS tagger of the head word; S (XP)
returns the translation status of XP on the target
language: un. if it is untranslated; cont. if it is
a continuous block; and discont. if it maps into
multiple discontinuous blocks.
are aimed to examine which of them should be
translated first. Therefore, most features share two
common components: the syntactic categories of
XP
i
and XP
i+1
. Table 1 shows the features used in
syntactic leftmost and rightmost reordering mod-
els. Note that we use the same features for both.
Although the semantic reordering model is
structured in precisely the same way, we use dif-
ferent feature sets to predict the reordering be-
tween two semantic roles. Given the two adjacent
roles R
i
and R
i+1
in a PAS pas, Table 2 shows the
features that are used in the semantic leftmost and
rightmost reordering models.
3.2 Integrating into the HPB Model
For models with syntactic reordering, we add two
new features (i.e., one for the leftmost reorder-
ing model and the other for the rightmost reorder-
ing model) into the log-linear translation model in
Eq. 1. Unlike the conventional phrase and lexi-
cal translation features, whose values are phrase
pair-determined and thus can be calculated offline,
the value of the reordering features can only be
obtained during decoding time, and requires word
alignment information as well. Before we present
the algorithm integrating the reordering models,
we define the following functions by assuming
XP
i
and XP
i+1
are the constituent pair of interest
in CFG rule cfg, H is the translation hypothesis
and a is its word alignment:
#Index Feature
rf1
R(R
i
) &R(R
i+1
) & P(pas)
R(R
i
) &R(R
i+1
)
rf2
for each R
j
(j < i)
R(R
i
) &R(R
i+1
) &R(R
j
) & P(pas)
R(R
i
) &R(R
i+1
) &R(R
j
)
rf3
for each R
j
(j > i+ 1)
R(R
i
) &R(R
i+1
) &R(R
j
) & P(pas)
R(R
i
) &R(R
i+1
) &R(R
j
)
rf4 R(R
i
) &R(R
i+1
) & P(R
i
)
rf5 R(R
i
) &R(R
i+1
) &H(R
i
)
rf6 R(R
i
) &R(R
i+1
) & L(R
i
)
rf7 R(R
i
) &R(R
i+1
) & P(R
i+1
)
rf8 R(R
i
) &R(R
i+1
) &H(R
i+1
)
rf9 R(R
i
) &R(R
i+1
) & L(R
i+1
)
rf10 R(R
i
) &R(R
i+1
) & S(R
i
)
rf11 R(R
i
) &R(R
i+1
) & S(R
i+1
)
rf12
R(R
i
) & P(pas)
R(R
i
)
rf13
R(R
i+1
) & P(pas)
R(R
i+1
)
Table 2: Features adopted in the semantic leftmost
and rightmost reordering models. P (pas) returns
the predicate content of pas;R (R) returns the role
type of R, e.g., Pred, A0, TMP, etc. For features
rf1, rf2, rf3, rf12 and rf13, we include another ver-
sion which excludes the predicate content P(pas)
for reasons of sparsity.
? F
1
(w
1
, w
2
, XP): returns true if constituent XP is
within the span from word w
1
to w
2
; otherwise returns
false.
? F
2
(H, cfg, XP
i
, XP
i+1
) returns true if the reordering
of the pair ?XP
i
, XP
i+1
? in rule cfg has not been calcu-
lated yet; otherwise returns false.
? F
3
(H, a, XP
i
, XP
i+1
) returns the leftmost and right-
most reordering types for the constituent pair ?XP
i
,
XP
i+1
?, given alignment a, according to Section 3.
? F
4
(rt, cfg, XP
i
, XP
i+1
) returns the probability of
leftmost reordering type rt for the constituent pair
?XP
i
, XP
i+1
? in rule cfg.
? F
5
(rt, cfg, XP
i
, XP
i+1
) returns the probability of
rightmost reordering type rt for the constituent pair
?XP
i
, XP
i+1
? in rule cfg.
Algorithm 1 integrates the syntactic leftmost
and rightmost reordering models into a CKY-style
decoder whenever a new hypothesis is generated.
Given a hypothesis H with its alignment a, it tra-
verses all CFG rules in the parse tree and sees if
two adjacent constituents are conditioned to trig-
ger the reordering models (lines 2-4). For each
pair of constituents, it first extracts its leftmost and
rightmost reordering types (line 6) and then gets
their respective probabilities returned by the max-
imum entropy classifiers defined in Section 3.1
1126
Algorithm 1: Integrating the syntactic reordering models
into a CKY-style decoder
Input: Sentence f in the source language
Parse tree t of f
All CFG rules {cfg} in t
Hypothesis H spanning from word w
1
to w
2
Alignment a of H
Output: Log-Probabilities of the syntactic leftmost
and rightmost reordering models
1. set l prob = r
p
rob = 0.0
2. foreach cfg in {cfg}
3. foreach pair XP
i
and XP
i+1
in cfg
4. if F
1
(w
1
, w
2
, XP
i
) = false or
F
1
(w
1
, w
2
, XP
i+1
) = false or
F
2
(H, cfg, XP
i
, XP
i+1
) = false
5. continue
6. (l type, r type) = F
3
(H, a, XP
i
, XP
i+1
)
7. l prob += logF
4
(l type, cfg,XP
i
,XP
i+1
)
8. r prob += logF
5
(r type, cfg,XP
i
,XP
i+1
)
9. return (l prob, r prob)
(lines 7-8). Then the algorithm returns two log-
probabilities of the syntactic reordering models.
Note that Function F
1
returns true if hypothesis
H fully covers, or fully contains, constituentXP
i
,
regardless of the reordering type of XP
i
. Do not
confuse any parsing tag XP
i
with the nameless
variables X
i
in Hiero or cdec rules.
For the semantic reordering models, we also
add two new features into the log-linear transla-
tion model. To get the two semantic reordering
model feature values, we simply use Algorithm 1
and its associated functions from F
1
to F
5
replac-
ing a CFG rule cfg with a PAS pas, and a con-
stituent XP
i
with a semantic role R
i
. Algorithm 1
therefore permits a unified treatment of syntactic
and PAS-based reordering, even though it is ex-
pressed in terms of syntactic reordering here for
ease of presentation.
4 Experiments
We have presented our unified approach to in-
corporating syntactic and semantic soft reorder-
ing constraints in an HPB system. In this section,
we test its effectiveness in Chinese-English trans-
lation.
4.1 Experimental Settings
For training we use 1.6M sentence pairs of the
non-UN and non-HK Hansards portions of NIST
MT training corpora, segmented with the Stan-
ford segmenter (Tseng et al, 2005). The En-
glish data is lowercased, tokenized and aligned
with GIZA++ (Och and Ney, 2000) to obtain bidi-
rectional alignments, which are symmetrized us-
ing the grow-diag-final-and method (Koehn et al,
2003). We train a 4-gram LM on the English
side of the corpus with 600M additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We use the HPB decoder cdec (Dyer et
al., 2010), with Mr. Mira (Eidelman et al, 2013),
which is a k-best variant of MIRA (Chiang et al,
2008), to tune the parameters of the system.
We use NIST MT 06 dataset (1664 sentence
pairs) for tuning, and NIST MT 03, 05, and 08
datasets (919, 1082, and 1357 sentence pairs, re-
spectively) for evaluation.
1
We use BLEU (Pap-
ineni et al, 2002) for both tuning and evaluation.
To obtain syntactic parse trees and semantic
roles on the tuning and test datasets, we first
parse the source sentences with the Berkeley
Parser (Petrov and Klein, 2007), trained on the
Chinese Treebank 7.0 (Xue et al, 2005). We
then pass the parses to a Chinese semantic role
labeler (Li et al, 2010), trained on the Chinese
PropBank 3.0 (Xue and Palmer, 2009), to anno-
tate semantic roles for all verbal predicates (part-
of-speech tag VV, VE, or VC).
Our basic baseline system employs 19 basic
features: a language model feature, 7 transla-
tion model features, word penalty, unknown word
penalty, the glue rule, date, number and 6 pass-
through features. Our stronger baseline employs,
in addition, the fine-grained syntactic soft con-
straint features of Marton and Resnik (2008), here-
after MR08. The syntactic soft constraint features
include both MR08 exact-matching and cross-
boundary constraints (denoted XP= and XP+).
Since the syntactic parses of the tuning and test
data contain 29 types of constituent labels and 35
types of POS tags, we have 29 types of XP+ fea-
tures and 64 types of XP= features.
4.2 Model Training
To train the syntactic and semantic reordering
models, we use a gold alignment dataset.
2
It con-
tains 7,870 sentences with 191,364 Chinese words
and 261,399 English words. We first run syn-
1
http://www.itl.nist.gov/iad/mig//tests/mt
2
This dataset includes LDC2006E86, and newswire
parts of LDC2012T16, LDC2012T20, LDC2012T24, and
LDC2013T05. Indeed, the reordering models can also be
trained on the MT training data with its automatic alignment.
However, our preliminary experiments showed that the re-
ordering models trained on gold alignment yielded higher im-
provement.
1127
Reordering
Type
Syntactic Semantic
l-m r-m l-m r-m
M 73.5 80.6 63.8 67.9
DM 3.9 3.3 14.0 12.0
S 19.5 13.2 13.1 10.7
DS 3.2 3.0 9.1 9.5
#instance 199,234 66,757
Table 3: Reordering type distribution over the re-
ordering model?s training data. Hereafter, l-m and
r-m are for leftmost and rightmost, respectively.
tactic parsing and semantic role labeling on the
Chinese sentences, then train the models by us-
ing MaxEnt toolkit with L1 regularizer (Tsuruoka
et al, 2009).
3
Table 3 shows the reordering type
distribution over the training data. Interestingly,
about 17% of the syntactic instances and 16% of
the semantic instances differ in their leftmost and
rightmost reordering types, indicating that the left-
most/rightmost distinction is informative. We also
see that the number of semantic instances is about
1/3 of that of syntactic instances, but the entropy
of the semantic reordering classes is higher, indi-
cating the reordering of semantic roles is harder
than that of syntactic constituents.
A deeper examination of the reordering model?s
training data reveals that some constituent pairs
and semantic role pairs have a preference for a
specific reordering type (monotone or swap). In
order to understand how well the MR08 system
respects their reordering preference, we use the
gold alignment dataset LDC2006E86, in which
the source sentences are from the Chinese Tree-
bank, and thus both the gold parse trees and gold
predicate-argument structures are available. Ta-
ble 4 presents examples comparing the reordering
distribution between gold alignment and the out-
put of the MR08 system. For example, the first
row shows that based on the gold alignment, for
?PP,VP?, 16% are in monotone and 76% are in
swap reordering. However, our MR08 system out-
puts 46% of them in monotone and and 50% in
swap reordering. Hence, the reordering accuracy
for ?PP,VP? is 54%. Table 4 also shows that the
semantic reordering between core arguments and
predicates (e.g., ?Pred,A1?, ?A0,Pred?) has a less
ambiguous pattern than that between adjuncts and
other roles (e.g., ?LOC,Pred?, ?A0,TMP?), indicat-
ing the higher reordering flexibility of adjuncts.
3
http://www.logos.ic.i.u-tokyo.ac.jp/?tsuruoka/maxent/
Const. Pair
Gold MR08 output
M S M S acc.
PP VP 16 76 46 50 54
NP LC 26 74 58 42 50
DNP NP 24 72 78 19 39
CP NP 26 67 84 10 33
NP DEG 39 61 31 69 66
... ... ...
all 81 13 79 14 80
Role Pair
Gold MR08 output
M S M S acc.
Pred A1 84 6 82 9 72
A0 Pred 82 11 79 8 75
LOC Pred 17 30 36 25 49
A0 TMP 35 25 61 6 45
TMP Pred 30 22 49 19 43
... ... ...
all 63 13 73 9 64
Table 4: Examples of the reordering distribution
(%) of gold alignment and the MR08 system out-
put. For simplicity, we only focus on (M)onotone
and (S)wap based on leftmost reordering.
4.3 Translation Experiment Results
Our first group of experiments investigates
whether the syntactic reordering models are able
to improve translation quality in terms of BLEU.
To this end, we respectively add our syntactic re-
ordering models into both the baseline and MR08
systems. The effect is shown in the rows of ?+ syn-
reorder? in Table 5. From the table, we have the
following two observations.
? Although the HPB model is capable of
handling non-local phrase reordering using
synchronous context free grammars, both
our syntactic leftmost reordering model and
rightmost model are still able to achieve im-
provement over both the baseline and MR08.
This suggests that our syntactic reordering
features interact well with the MR08 syntac-
tic soft constraints: the XP+ and XP= fea-
tures focus on a single constituent each, while
our reordering features focus on a pair of con-
stituents each.
? There is no clear indication of whether the
leftmost reordering model works better than
the other. In addition, integrating both the
leftmost and rightmost reordering models has
limited improvement over a single reordering
model.
Our second group of experiments is to vali-
date the semantic reordering models. Results are
1128
System
Tuning Test
MT06 MT03 MT05 MT08 Avg.
Baseline 34.1 36.1 32.3 27.4 31.9
+
syn-
reorder
l-m 35.2 36.9? 33.6? 28.4? 33.0
r-m 35.2 37.2? 33.7? 28.6? 33.2
both 35.6 37.1? 33.6? 28.8? 33.1
+
sem-
reorder
l-m 34.4 36.7? 33.0? 27.8? 32.5
r-m 34.5 36.7? 33.1? 27.8? 32.5
both 34.5 37.0? 33.6? 27.7? 32.8
+syn+sem 35.5 37.3? 33.7? 29.0? 33.3
MR08 35.6 37.4 34.2 28.7 33.4
+
syn-
reorder
l-m 36.0 38.2? 35.0? 29.2? 34.1
r-m 36.0 38.1? 34.8? 29.2? 34.0
both 35.9 38.2? 35.3? 29.5? 34.3
+
sem-
reorder
l-m 35.8 37.6? 34.7? 28.7 33.7
r-m 35.8 37.4 34.5? 28.8 33.6
both 35.8 37.6? 34.7? 28.8 33.7
+syn+sem 36.1 38.4? 35.2? 29.5? 34.4
Table 5: System performance in BLEU scores.
?/?: significant over baseline or MR08 at 0.01
/ 0.05, respectively, as tested by bootstrap re-
sampling (Koehn, 2004)
shown in the rows of ?+ sem-reorder? in Table 5.
Here we observe:
? The semantic reordering models also achieve
significant gain of 0.8 BLEU on average over
the baseline system, demonstrating the ef-
fectiveness of PAS-based reordering. How-
ever, the gain diminishes to 0.3 BLEU on the
MR08 system.
? The syntactic reordering models outperform
the semantic reordering models on both the
baseline and MR08 systems.
Finally, we integrate both the syntactic and se-
mantic reordering models into the final system.
The two models collectively achieve a gain of up
to 1.4 BLEU over the baseline and 1.0 BLEU over
MR08 on average, which is shown in the rows of
?+syn+sem? in Table 5.
5 Discussion
The trend of the results, summarized as perfor-
mance gain over the baseline and MR08 systems
averaged over all test sets, is presented in Table 6.
The syntactic reordering models outperform the
semantic reordering models, and the gain achieved
by the semantic reordering models is limited in the
presence of the MR08 syntactic features. In this
section, we look at MR08 system and the systems
improving it to explore the behavior differences
between the two reordering models.
Coverage analysis: Our statistics show that
syntactic reordering features (either leftmost or
System Baseline MR08
+syn-reorder 1.2 0.9
+sem-reorder 0.8 0.3
+ both 1.4 1.0
Table 6: Performance gain in BLEU over baseline
and MR08 systems averaged over all test sets.
rightmost) are called 24 times per sentence on av-
erage. This is compared to only 9 times per sen-
tence for semantic reordering features. This is not
surprising since the semantic reordering features
are exclusively attached to predicates, and the span
set of the semantic roles is a strict subset of the
span set of the syntactic constituents; only 22% of
syntactic constituents are semantic roles. On aver-
age, a sentences has 4 PASs and each PAS contains
3 semantic roles. Of all the semantic role pairs,
44% are in the same CFG rules, indicating that this
part of semantic reordering has overlap with syn-
tactic reordering. Therefore, the PAS model has
fewer opportunities to influence reordering.
Reordering accuracy analysis: The reordering
type distribution on the reordering model training
data in Table 3 suggests that semantic reordering
is more difficult than syntactic reordering. To val-
idate this conjecture on our translation test data,
we compare the reordering performance among
the MR08 system, the improved systems and the
maximum entropy classifiers. For the test set, we
have four reference translations. We run GIZA++
on the data combination of our translation train-
ing data and test data to get the alignment for the
test data and each reference translation. Once we
have the (semi-)gold alignment, we compute the
gold reordering types between two adjacent syn-
tactic constituents or semantic roles. Then we
evaluate the automatic reordering outputs gener-
ated from both our translation systems and max-
imum entropy classifiers. Table 7 shows the ac-
curacy averaged over the four gold reordering sets
(the four reference translations). It shows that 1)
as expected, our classifiers do worse on the harder
semantic reordering prediction than syntactic re-
ordering prediction; 2) thanks to the high accu-
racy obtained by the maxent classifiers, integrat-
ing either the syntactic or the semantic reorder-
ing constraints results in better reordering perfor-
mance from both syntactic and semantic perspec-
tives; 3) in terms of the mutual impact, the syn-
tactic reordering models help improving seman-
tic reordering more than the semantic reordering
1129
System
Syntactic Semantic
l-m r-m l-m r-m
MR08 75.0 78.0 66.3 68.5
+syn-reorder 78.4 80.9 69.0 70.2
+sem-reorder 76.0 78.8 70.7 72.7
+both 78.6 81.7 70.6 72.1
Maxent Classifier 80.7 85.6 70.9 73.5
Table 7: Reordering accuracy on four gold sets.
System
Syntactic Semantic
l-m r-m l-m r-m
+syn-reorder 1.2 1.2 - -
+sem-reorder - - 0.7 0.9
+both 1.2 1.0 0.5 0.4
Table 8: Reordering feature weights.
models help improving syntactic reordering; and
4) the rightmost models have a learnability advan-
tage over the leftmost models, achieving higher
accuracy across the board.
Feature weight analysis: Table 8 shows the
syntactic and semantic reordering feature weights.
It shows that the semantic feature weights de-
crease in the presence of the syntactic features, in-
dicating that the decoder learns to trust semantic
features less in the presence of the more accurate
syntactic features. This is consistent with our ob-
servation that semantic reordering is harder than
syntactic reordering, as seen in Tables 3 and 7.
Potential improvement analysis: Table 7 also
shows that our current maximum entropy classi-
fiers have room for improvement, especially for
semantic reordering. In order to explore the error
propagation from the classifiers themselves and
explore the upper bound for improvement from the
reordering models, we perform an ?oracle? study,
letting the classifiers be aware of the ?gold? re-
ordering type between two syntactic constituents
or two semantic roles, and returning a higher prob-
ability for the gold reordering type and a smaller
one for the others (i.e., we set 0.9 for the gold
System MT 03 MT 05 MT 08 Avg.
Non-
Oracle
MR08 37.4 34.2 28.7 33.4
+syn-
reorder
38.2 35.3 29.5 34.3
+sem-
reorder
37.6 34.7 28.8 33.7
+ both 38.4 35.2 29.5 34.4
Oracle
+syn-
reorder
39.2 35.9 29.6 34.9
+sem-
reorder
37.9 34.8 28.9 33.9
+ both 39.1 36.0 29.8 35.0
Table 9: Performance (BLEU score) comparison
between non-oracle and oracle experiments.
reordering type, and let the other non-gold three
types share 0.1). Again, to get the gold reorder-
ing type, we run GIZA++ to get the alignment for
tuning/test source sentences and each of four ref-
erence translations. We report the averaged per-
formance by using the gold reordering type ex-
tracted from the four reference translations. Ta-
ble 9 compares the performance between the non-
oracle and oracle settings. We clearly see that us-
ing gold syntactic reordering types significantly
improves the performance (e.g., 34.9 vs. 33.4 on
average) and there is still some room for improve-
ment by building a better maximum entropy clas-
sifiers (e.g., 34.9 vs. 34.3). To our surprise, how-
ever, the improvement achieved by gold semantic
reordering types is still small (e.g., 33.9 vs. 33.4),
suggesting that the potential improvement of se-
mantic reordering models is much more limited.
And we again see that the improvement achieved
by semantic reordering models is limited in the
presence of the syntactic reordering models.
6 Related Work
Syntax-based reordering: Some previous work
pre-ordered words in the source sentences, so that
the word order of source and target sentences is
similar. The reordering rules were either manu-
ally designed (Collins et al, 2005; Wang et al,
2007; Xu et al, 2009; Lee et al, 2010) or auto-
matically learned (Xia and McCord, 2004; Gen-
zel, 2010; Visweswariah et al, 2010; Khalilov
and Sima?an, 2011; Lerner and Petrov, 2013), us-
ing syntactic parses. Li et al (2007) focused on
finding the n-best pre-ordered source sentences by
predicting the reordering of sibling constituents,
while Yang et al (2012) obtained word order by
using a reranking approach to reposition nodes in
syntactic parse trees. Both are close to our work;
however, our model generates reordering features
that are integrated into the log-linear translation
model during decoding.
Another approach in previous work added soft
constraints as weighted features in the SMT de-
coder to reward good reorderings and penalize bad
ones. Marton and Resnik (2008) employed soft
syntactic constraints with weighted binary features
and no MaxEnt model. They did not explicitly
target reordering (beyond applying constraints on
HPB rules). Although employing linguistically
motivated labels in SCFG is capable of captur-
ing constituent reorderings (Chiang, 2010; Mylon-
1130
akis and Sima?an, 2011), the rules are sparser than
SCFG with nameless non-terminals (i.e., Xs) and
soft constraints. Ge (2010) presented a syntax-
driven maximum entropy reordering model that
predicted the source word translation order. Gao
et al (2011) employed dependency trees to predict
the translation order of a word and its head word.
Huang et al (2013) predicted the translation order
of two source words.
4
Our work, which shares this
approach, differs from their work primarily in that
our syntactic reordering models are based on the
constituent level, rather than the word level.
Semantics-based reordering: Semantics-
based reordering has also seen an increase
in activity recently. In the pre-ordering ap-
proach, Wu et al (2011) automatically learned
pre-ordering rules from PAS. In the soft con-
straint or reordering model approach, Liu and
Gildea (2010) modeled the reordering/deletion
of source-side semantic roles in a tree-to-string
translation model. Xiong et al (2012) and Li et
al. (2013) predicted the translation order between
either two arguments or an argument and its
predicate. Instead of decomposing a PAS into
individual units, Zhai et al (2013) constructed
a classifier for each source side PAS. Finally in
the post-processing approach category, Wu and
Fung (2009) performed semantic role labeling
on translation output and reordered arguments to
maximize the cross-lingual match of the semantic
frames between the source sentence and the target
translation. To our knowledge, their semantic
reordering models were PAS-specific. In contrast,
our model is universal and can be easily adopted
to model the reordering of other linguistic units
(e.g., syntactic constituents). Moreover, we
have studied the effectiveness of the semantic
reordering model in different scenarios.
Non-syntax-based reorderings in HPB: Re-
cently we have also seen work on lexicalized re-
ordering models without syntactic information in
HPB (Setiawan et al, 2009; Huck et al, 2013;
Nguyen and Vogel, 2013). The non-syntax-
based reordering approach models the reorder-
ing of translation words/phrases while the syntax-
based approach models the reordering of syn-
tactic constituents. Although there are overlaps
between translation phrases and syntactic con-
stituents, it is reasonable to think that the two re-
4
Note that they obtained the translation order of source
word pairs by predicting the reordering of adjacent con-
stituents, which was quite close to our work.
ordering approaches can work together well and
even complement each other, as the linguistic pat-
terns they capture differ substantially. Setiawan
et al (2013) modeled the orientation decisions
between anchors and two neighboring multi-unit
chunks which might cross phrase or rule bound-
aries. Last, we also note that recent work on non-
syntax-based reorderings in (flat) phrase-based
models (Cherry, 2013; Feng et al, 2013) can also
be potentially adopted to hpb models.
7 Conclusion and Future Work
In this paper, we have presented a unified reorder-
ing framework to incorporate soft linguistic con-
straints (of syntactic or semantic nature) into the
HPB translation model. The syntactic reordering
models take CFG rules and model their reordering
on the target side, while the semantic reordering
models work with PAS. Experiments on Chinese-
English translation show that the reordering ap-
proach can significantly improve a state-of-the-art
hierarchical phrase-based translation system. We
have also discussed the differences between the
two linguistic reordering models.
There are many directions in which this work
can be continued. First, the syntactic reordering
model can be extended to model reordering among
constituents that cross CFG rules. Second, al-
though we do not see obvious gain from the se-
mantic reordering model when the syntactic model
is adopted, it might be beneficial to further jointly
consider the two reordering models, focusing on
where each one does well. Third, to better exam-
ine the overlap or synergy between our approach
and the non-syntax-based reordering approach, we
will conduct direct comparisons and combinations
with the latter.
Acknowledgments
This research was supported in part by the
BOLT program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0012-
12-C-0015. Any opinions, findings, conclusions
or recommendations expressed in this paper are
those of the authors and do not necessarily re-
flect the view of DARPA. The authors would like
to thank three anonymous reviewers for providing
helpful comments, and also acknowledge Ke Wu,
Vladimir Eidelman, Hua He, Doug Oard, Yuening
Hu, Jordan Boyd-Graber, and Jyothi Vinjumur for
useful discussions.
1131
References
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of ACL 1996, pages 310?
318.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of HLT-NAACL 2013, pages 22?31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008, pages 224?233.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL 2010,
pages 1443?1452.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL 2010 System Demonstra-
tions, pages 7?12.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. mira: Open-
source large-margin structured learning on mapre-
duce. In Proceedings of ACL 2013 System Demon-
strations, pages 199?204.
Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Proceedings of ACL
2013, pages 322?332.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hier-
archical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Niyu Ge. 2010. A direct syntax-driven reordering
model for phrase-based machine translation. In Pro-
ceedings of HLT-NAACL 2010, pages 849?857.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of COLING 2010, pages 376?
384.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In Proceedings of
EMNLP 2013, pages 556?566.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for
hierarchical machine translation. In Proceedings of
WMT 2013, pages 452?463.
Maxim Khalilov and Khalil Sima?an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proceedings of IJCNLP 2011,
pages 38?46.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent reordering and syntax models for
English-to-Japanese statistical machine translation.
In Proceedings of COLING 2010, pages 626?634.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP 2013, pages 513?523.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proceedings of ACL 2007,
pages 720?727.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings of ACL 2010, pages 1108?1117.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013.
Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In Proceedings of
HLT-NAACL 2013, pages 540?549.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
COLING 2010, pages 716?724.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT 2008, pages
1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguistic
annotations. In Proceedings of ACL 2011, pages
642?652.
ThuyLinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into a chart-
based decoder for machine translation. In Proceed-
ings of ACL 2013, pages 1587?1596.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
1132
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311?318.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007, pages 404?411.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
Proceedings of ACL-IJCNLP 2009, pages 324?332.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin
Shen. 2013. Two-neighbor orientation model with
cross-boundary global contexts. In Proceedings of
ACL 2013, pages 1264?1274.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
168?171.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of ACL-IJCNLP 2009,
pages 477?485.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of COLING 2010, pages
1119?1127.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP
2007, pages 737?745.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
HLT-NAACL 2009: short papers, pages 13?16.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of IJCNLP 2011, pages 29?
37.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of COLING 2004, pages
508?514.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of ACL 2012, pages 902?
911.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of HLT-NAACL 2009, pages 245?253.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL 2012, pages 912?920.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2013. Handling ambiguities of bilingual
predicate-argument structures for statistical machine
translation. In Proceedings of ACL 2013, pages
1127?1136.
1133
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 232?242,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Using Syntactic Head Information in Hierarchical Phrase-Based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
Chiang?s hierarchical phrase-based (HPB)
translation model advances the state-of-the-art
in statistical machine translation by expanding
conventional phrases to hierarchical phrases
? phrases that contain sub-phrases. How-
ever, the original HPB model is prone to over-
generation due to lack of linguistic knowl-
edge: the grammar may suggest more deriva-
tions than appropriate, many of which may
lead to ungrammatical translations. On the
other hand, limitations of glue grammar rules
in the original HPB model may actually pre-
vent systems from considering some reason-
able derivations. This paper presents a sim-
ple but effective translation model, called the
Head-Driven HPB (HD-HPB) model, which
incorporates head information in translation
rules to better capture syntax-driven informa-
tion in a derivation. In addition, unlike the
original glue rules, the HD-HPB model allows
improved reordering between any two neigh-
boring non-terminals to explore a larger re-
ordering search space. An extensive set of ex-
periments on Chinese-English translation on
four NIST MT test sets, using both a small
and a large training set, show that our HD-
HPB model consistently and statistically sig-
nificantly outperforms Chiang?s model as well
as a source side SAMT-style model.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. However, due to lack of linguistic
knowledge, Chiang?s HPB model contains only one
type of non-terminal symbol X , often making it
difficult to select the most appropriate translation
rules.1
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble.
By contrast, and inspired by previous work in
parsing (Charniak, 2000; Collins, 2003), our Head-
Driven HPB (HD-HPB) model is based on the in-
tuition that linguistic heads provide important in-
formation about a constituent or distributionally de-
fined fragment, as in HPB. We identify heads using
linguistically motivated dependency parsing, and
use head information to refine X.
Furthermore, Chiang?s HPB model suffers from
limited phrase reordering by combining translated
1Another non-terminal symbol S is used in glue rules.
232
 (a) (b) 
zuotian chuxi huiyi 
attended a meeting yesterday 
X2 X1 
X1 X2 
S2 
S1 
S2 
S1 
zuotian chuxi huiyi 
attended a meeting yesterday 
X4 X3 
X3 X4 
X2 
X1 
X2 
S2 
X1 
S1 
S1 
X1 
Figure 1: Example of derivations disallowed in Chiang?s
HPB model. The rules with dotted lines are not covered
in Chiang?s model.
phrases in a monotonic way with glue rules. In
addition, once a glue rule is adopted, it requires
all rules above it to be glue rules. For exam-
ple, given a Chinese-English sentence pair (?
?/zuotian1 ??/chuxi2 ??/huiyi3, Attended2 a3
meeting3 yesterday1), a correct translation is impos-
sible via HPB derivations in Figure 1. For the deriva-
tion in Figure 1(a), swap reordering in the glue rule
(i.e., S1 ? ?S2X2, X2S2?) is disallowed and, even
if such a swap reordering is available, it lacks useful
information for rule selection. For the derivation in
Figure 1(b), the combination of two non-terminals
(i.e., X2 ? ?X3X4, X3X4?) is disallowed to form
a new non-terminal which in turn is a sub-phrase of
a hierarchical rule. These limitations prevent tra-
ditional HPB systems from even considering some
reasonable derivations.
To tackle the problem of glue rules, He (2010) ex-
tended the HPB model by using bracketing transduc-
tion grammar (Wu, 1996) instead of the monotone
glue rules, and trained an extra classifier for glue
rules to predict reorderings of neighboring phrases.
By contrast, our HD-HPB model refines the non-
terminal symbol X with syntactic head informa-
tion and provides flexible reordering rules, including
swap, which can mix freely with hierarchical trans-
lation rules for better interleaving of translation and
reordering in translation derivations.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
The paper is structured as follows: Section 2
describes the synchronous context-free grammar
(SCFG) in our HD-HPB translation model. Sec-
tion 3 presents our model and features, followed by
the decoding algorithm in Section 4. We report ex-
perimental results in Section 5. Finally we conclude
in Section 6.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simultane-
ously using a context-free grammar. In particular,
each synchronous rule rewrites a non-terminal into
a pair of strings, s and t, where s (or t) contains ter-
minals and non-terminals from the source (or target)
language and there is a one-to-one correspondence
between the non-terminal symbols on both sides.
A good and informative inventory of non-terminal
symbols is always important, especially for a suc-
cessful SCFG-based translation model. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), ideally
non-terminals should capture important information
of the word sequences they cover to be able to prop-
erly discriminate between similar and different word
sequences during translation. This motivates our
approach to provide syntax-enriched non-terminal
symbols. Given a word sequence f ij from position i
to position j, we refine the non-terminal symbol X
to reflect some of the internal syntactic structure of
233
?
?
/N
R
 
O
uz
ho
u 
?
?
/N
N 
ba
gu
o 
?
?
/A
D
li
an
mi
ng
?
?
/V
V
zh
ic
hi
 
?
?
/N
R 
me
ig
uo
 
?
/P du
i
?
?
/N
N
ce
li
e 
?
/N
R
yi
 
ro
ot
E
ig
ht
 
E
ur
op
ea
n 
co
un
tr
ie
s
jo
in
tly
su
pp
or
t
A
m
er
ic
a?
s
st
an
d
ag
ai
ns
t
Ir
aq
Figure 2: An example word alignment for a Chinese-English sentence pair with the dependency parse tree for the
Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin.
the word sequence covered by X . A correct transla-
tion rule selection therefore not only maps terminals
into terminals, but is both constrained and guided
by syntactic information in the non-terminals. At
the same time, it is not clear whether an ?ideal? ap-
proach that captures a full syntactic analysis of the
string fragment covered by a non-terminal is feasi-
ble: the diversity of syntactic structures could make
training impossible and lead to serious data sparse-
ness issues. As a compromise, given a word se-
quence f ij , we first find heads and then concatenate
the POS tags of these heads as f ij?s non-terminal
symbol.2 Our approach is guided by the intuition
that linguistic heads provide important information
about a constituent or distributionally defined frag-
ment, as in HPB. Specifically, we adopt dependency
structure to derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 2
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
2Note that instead of POS tags, it is also possible to use other
types of syntactic information associated with heads to refine
non-terminal symbols (Section 5.5.2).
In our HD-HPB model, the SCFG is defined as
a tuple ??, N,?,?,<?, where ? is a set of source
language terminals,N is a set of non-terminals cate-
gorizing terminals in ?, ? is a set of target language
terminals, ? is a set of non-terminals categorizing
terminals in ?, and < is a set of translation rules.
A rule ? in < is in the form of ?Ps ? s, Pt ? t, ??,
where:
? Ps ? N and Pt ? ?;
? s ? (? ?N)+ and t ? (? ? ?)+
? ? is a bijection between non-terminals in s and t.
According to the occurrence of terminals in s and
t, we group the rules in the HD-HPB model into two
categories: head-driven hierarchical rules (HD-HRs)
and non-terminal reordering rules (NRRs), where
the former have at least one terminal on both source
and target sides and the later have no terminals. For
rule extraction, we first identify initial phrase pairs
on word-aligned sentence pairs by using the same
criterion as most phrase-based translation models
(Och and Ney, 2004) and Chiang?s HPB model (Chi-
ang, 2005; Chiang, 2007). We extract HD-HRs and
NRRs based on initial phrase pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that con-
tain other phrases and then replace sub-phrases with
their corresponding non-terminal symbols. Given
the word alignment as shown in Figure 2, Table 1
demonstrates the difference between hierarchical
rules in Chiang (2007) and HD-HRs defined here.
234
phrase pairs hierarchical rule head-driven hierarchical rule
celie, stand X?celie, stand
NN?celie,
X?stand
dui yi celie1, stand1 against Iraq X?dui yi X1, X1 against Iraq
NN?dui yi NN1,
X?X1 against Iraq
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 dui yi celie2,
support America?s1 stand2 against Iraq
X?X1 dui yi X2,
X1 X2 against Iraq
VV?VV-NR1 dui yi NN2,
X?X1 X2 against Iraq
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair
?
f ij , e
i?
j?
?
, we check all other
initial phrase pairs
?
fkl , e
k?
l?
?
which satisfy k = j+1
(i.e., phrase fkl is located immediately to the right
of f ij in the source language). For their target
side translations, there are four possible positional
relationships: monotone, discontinuous monotone,
swap, and discontinuous swap. In order to differen-
tiate non-terminals from those in the target language
(i.e., X), we use Y as a variable for non-terminals in
the source language, and obtain four types of NRRs:
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
For example in Figure 2, the NRR for initial
phrase pairs ?zhichi meiguo, support America?s?
and ?dui yi celie, stand against Iraq? would be
?V V ? V V -NR1NN2, X ? X1X2?.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
3 Log-linear Model and Features
Following Och and Ney (2002), we depart from the
traditional noisy-channel approach and use a general
log-linear model. Let d be a derivation from sen-
tence f in the source language to sentence e in the
target language. The probability of d is defined as:
P (d) ?
?
i
?i (d)
?i (1)
where ?i are features defined on derivations and
?i are feature weights. In particular, we use a fea-
ture set analogous to the default feature set of Chi-
ang (2007), which includes:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
235
Algorithm 1: Decoding Algorithm
Input: Sentence f1n in the source language
Dependency structure of f1n
HD-HR rule set HDHR
NRR rule set NRR
Initial phrase length K
Output: Best derivation d?
1. set chart[i, j]=NIL (1 ? i ? j ? n);
2. for l from 1 to n do
3. for all i, j such that j ? i = l do
4. if l ? K do
5. for all derivations d derived from
HDHR spanning from i to j do
6. add d into chart[i, j]
7. for all derivations d derived from
NRR spanning from i to j do
8. add d into chart[i, j]
9. set d? as the top derivation of chart[1, n]
10.return d?
It is worth pointing out that we define translation
probabilities for NRRs only for the direction from
source language to target language, although trans-
lation probabilities for HD-HRs are defined for both
directions. This is mostly due to the fact that a NRR
excludes terminals and has only two options on the
target side (i.e., either X ? X1X2 or X ? X2X1).
4 Decoding
Our decoder is based on CKY-style chart parsing
with beam search. Given an input sentence f , it finds
a sentence e in the target language derived from the
best derivation d? among all possible derivations D:
d? = arg max
d?D
P (D) (2)
Algorithm 1 presents the decoding process. Given
a source sentence, it searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang (2007), it is possible for
a non-terminal generated by a NRR to be included
afterwards by a HD-HR or another NRR. Similar to
Chiang (2007) in generating k-best derivations from
i to j, we make use of cube pruning (Huang and Chi-
ang, 2005) with an integrated language model for
each derivation.
5 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for HD-
HPB, HPB and SAMT-HPB systems, including ini-
tial phrase length (as 10) in training, the maximum
number of non-terminals (as 2) in translation rules,
maximum number of non-terminals plus terminals
(as 5) on the source, prohibition of non-terminals
to be adjacent on the source, beam threshold ? (as
10?5) (to discard derivations with a score worse than
? times the best score in the same chart cell), beam
size b (as 200) (i.e. each chart cell contains at most
b derivations). For Moses HPB, we use ?grow-diag-
final-and? to obtain symmetric word alignments, 10
for the maximum phrase length, and the recom-
mended default values for all other parameters.
5.1 Experimental Settings
To examine the efficacy of our approach on training
datasets of different scales, we first train translation
models on a small-sized corpus, and then scale to a
larger one. We use the 2002 NIST MT evaluation
test data (878 sentence pairs) as the development
data, and the 2003, 2004, 2005, 2006-news NIST
MT evaluation test data (919, 1788, 1082, and 616
sentence pairs, respectively) as the test data. To find
heads, we parse the source sentences with the Berke-
ley Parser3 (Petrov and Klein, 2007) trained on Chi-
nese TreeBank 6.0 and use the Penn2Malt toolkit4
to obtain dependency structures.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2000) on the corpus in
both directions, applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
236
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
NIST and the BLEU scores, which measures case-
insensitive matching of n-grams with n up to 4. To
test whether a performance difference is statistically
significant, we conduct significance tests following
the paired bootstrap approach (Koehn, 2004). In this
paper, ?**? and ?*? denote p-values less than
0.01 and in-between [0.01, 0.05), respectively.
5.2 Results on Small Data
To test the HD-HPB models, we firstly carried out
experiments using the FBIS corpus as training data,
which contains ?240K sentence pairs. Table 2 lists
the rule table sizes. The full rule table size (includ-
ing HD-HRs and NRRs) of our HD-HPB model is
about 1.5 times that of Chiang?s, largely due to re-
fining the non-terminal symbolX in Chiang?s model
into head-informed ones in our model. It is also
unsurprising, that the test set-filtered rule table size
of our model is only about 0.8 times that of Chi-
ang?s: this is due to the fact that some of the re-
fined translation rule patterns required by the test
set are unattested in the training data. Furthermore,
the rule table size of NRRs is much smaller than
that of HD-HRs since a NRR contains only two
non-terminals. Table 3 lists the translation perfor-
mance with NIST and BLEU scores. Note that our
re-implementation of Chiang?s original HPB model
performs on a par with Moses HPB. Table 3 shows
that our HD-HPB model significantly outperforms
Chiang?s HPB model with an average improvement
of 1.32 in BLEU and 0.16 in NIST (and similar im-
provements over Moses HPB).
Although HD-HPB has small size of phrase ta-
bles compared to HPB, it still consumes more time
in decoding (e.g., 15.1 vs. 11.0), mostly due to the
flexible reordering of NRRs.
5.3 Results on Large Data
We also conduct experiments on larger training
data with ?1.5M sentence pairs from the LDC
dataset.5 Table 4 lists the rule table sizes and Ta-
ble 5 presents translation performance with NIST
5This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
and BLEU scores. It shows that our HD-HPB model
consistently outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU and 0.35
in NIST (similar for Moses HPB). Compared to the
improvement achieved on the small data, it is en-
couraging to see that our HD-HPB model benefits
more from larger training data with little adverse ef-
fect on decoding time which increases only slightly
from 15.1 to 16.6 seconds per sentence.
5.4 Comparison with SAMT-HPB
Comparing the performance of SAMT-HPB with
regular HPB in Table 3 and Table 5, it is interest-
ing to see that in general the SAMT-style approach
leads to a deterioration of translation performance
for the small training set (e.g., 30.09 for SAMT-HPB
vs. 30.64 for HPB) while it comes into its own for
the large training set (e.g., 33.54 for SAMT-HPB vs.
32.95 for HPB), indicating that the SAMT-style ap-
proach is more prone to data sparseness than HPB
(or, indeed, HD-HPB).
Comparing the performance of SAMT-HPB with
HD-HPB, shows that our head-driven non-terminal
refining approach consistently outperforms the
SAMT-style approach on an extensive set of ex-
periments (for each test set p < 0.01), indicating
that head information is more effective than (par-
tial) CFG categories. To make the comparison fair,
it is important to note that our implementation of
source-side SAMT-HPB includes the same sophis-
ticated non-terminal re-ordering NRR rules as HD-
HPB (Section 2.2 ). Thus the performance differ-
ences reported here are not due to different reorder-
ing capabilities, but to the discriminative impact of
the head information in HD-HPB over SAMT-style
annotation. Taking lianming zhichi in Figure 2 as an
example, HD-HPB labels the span VV, as lianming
is dominated by zhichi, effecively ignoring lianming
in the translation rule, while the SAMT label is
ADVP:AD+VV6 which is more susceptible to data
sparsity (Table 2 and Table 4). In addition, SAMT
resorts to X if a text span fails to satisify pre-defined
categories. Examining initial phrases extracted from
the SAMT training data shows that 28% of them are
labeled as X. Finally, for Chinese syntactic analy-
6The constituency structure for lianming zhichi is (VP
(ADVP (AD lianming)) (VP (VV zhichi) ...)).
237
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6M 2.8M 4.7M 3.3M 3.0M 3.4M
HD-HPB 59.5/0.6M 1.9/0.1M 3.4/0.2M 2.3/0.2M 2.0/0.1M 2.4/0.2M
SAMT-HPB 70.1/0.4M 2.2/0.2M 4.0/0.2M 2.7/0.2M 2.3/0.2M 2.8/0.2M
Table 2: Rule table sizes of different models trained on small data. Note: 1) SAMT-HPB indicates our HD-HPB model
with the non-terminal scheme of Zollmann and Venugopal (2006); 2) For HD-HPB and SAMT-HPB, the rule sizes
separated by / indicate HD-HRs and NRRs, respectively; 2) Except for ?Total Rules?, the figures correspond to rules
filtered on the corresponding test set.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.377 29.67 8.209 33.60 7.571 29.49 6.773 28.90 7.483 30.42 NA
HPB 8.137 29.75 9.050 34.06 8.264 30.09 7.788 28.64 8.310 30.64 11.0
HD-HPB 8.308 31.01** 9.211 35.11** 8.426 31.57** 7.930 30.15** 8.469 31.96 15.1
SAMT-HPB 7.886 29.14* 8.703 33.32** 7.961 29.49* 7.307 28.41 7.964 30.09 17.3
HD-HR+Glue 7.966 29.51 8.826 33.68 8.116 29.84 7.474 28.51 8.095 30.39 5.4
Table 3: NIST and BLEU (%) scores of different models trained on small data. Note: 1) HD-HR+Glue indicates our
HD-HPB model replacing NRRs with glue rules; 2) Significance tests for Moses HPB, HD-HPB, SAMT-HPB and
HD-HR+Glue are done against HPB.
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 206.8M 11.3M 17.6M 12.9M 10.4M 13.0M
HD-HPB 318.6/2.3M 7.3/0.3M 12.2/0.4M 8.5/0.3M 6.7/0.2M 8.7/0.3M
SAMT-HPB 371.0/1.1M 8.6/0.3M 14.3/0.4M 10.1/0.3M 7.9/0.3M 10.2/0.3M
Table 4: Rule table sizes of different models trained on large data.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.914 32.94* 8.429 35.16 7.962 32.18 6.483 29.88* 7.697 32.54 NA
HPB 8.583 33.59 9.114 35.39 8.465 32.20 7.532 30.60 8.423 32.95 13.7
HD-HPB 8.885 35.50** 9.494 37.61** 8.871 34.56** 7.839 31.78** 8.772 34.86 16.6
SAMT-HPB 8.644 34.07 9.245 36.52** 8.618 32.90* 7.543 30.66 8.493 33.54 19.1
HD-HR+Glue 8.831 34.58** 9.435 36.55** 8.821 33.84** 7.863 31.06 8.737 34.01 6.7
Table 5: NIST and BLEU (%) scores of different models trained on large data. Note: System labels and significance
testing as in Table 3.
238
sis, dependency structure is more reliable than con-
stituency structure. Moreover, SAMT-HPB takes
more time in decoding than HD-HPB due to larger
phrase tables.
5.5 Discussion
5.5.1 Individual Contribution of HD-HRs and
NRRs
Examining translation output shows that on aver-
age each sentence employs 16.6/5.2 HD-HRs/NRRs
in our HD-HPB model, compared to 15.9/3.6 hier-
archical rules/glue rules in Chiang?s model, provid-
ing further indication of the importance of NRRs in
translation. In order to separate out the individual
contributions of the novel HD-HRs and NRRs, we
carry out an additional experiment (HD-HR+Glue)
using HD-HRs with monotonic glue rules only (ad-
justed to refined rule labels, but effectively switching
off the extra reordering power of full NRRs) both
on the small and the large datasets, with interest-
ing results: Table 3 (HD-HR+Glue) shows that for
the small training set most of the improvement of
our full HD-HPB model comes from the NRRs, as
RR+Glue performs on the same level as Chiang?s
original and Moses HPB (the differences are not
statistically significant), perhaps indicating sparse-
ness for the refined HD-HRs given the small train-
ing set. Table 5 shows that for the large training
set, HD-HRs come into their own: on average more
than half of the improvement over HPB (Chiang and
Moses) comes from the refined HD-HRs, the rest
from NRRs.
It is not surprising that compared to the others
HD-HR+Glue takes much less time in decoding.
This is due to the fact that 1) compared to HPB, the
refined translation rule patterns on the source side
have fewer entries in phrase table; 2) compared to
HD-HPB, HD-HR+Glue switches off the extra re-
ordering of NRRs. The decoding time for HD-HPB
and HD-HR+Glue suggests that NRRs are more than
doubling the time required to decode.
5.5.2 Different Head Label Sets
Examining initial phrases extracted from the large
size training data shows that there are 63K types
of refined non-terminals with respect to 33 types of
POS tags. Considering the sparseness in translation
rules caused by this comparatively detained POS tag
set, we carry out an experiment with a reduced set
of non-terminal types by using a less granular POS
tag set (C-HPB). Moreover, due to the fact that con-
catenation of POS tags of heads mostly captures in-
ternal structure of a text span, it is interesting to ex-
amine the effect of other syntactic labels, in partic-
ular dependency labels, to try to better capture the
impact of the external context on the text span. To
this end, we replace the POS tag of head with its
incoming dependency label (DL-HPB), or the com-
bination of (the original fine-grained) POS tag and
its dependency label (POS-DL-HPB). For C-HPB
we use the coarse POS tag set obtained by group-
ing the 33 types of Chinese POS tags into 11 types
following Xia (2000). For example, we generalize
all verbal tags (e.g., VA, VC, VE, and VV ) and all
nominal tags (e.g., NR, NT, and NN) into Verb and
Noun, respectively. We use the dependency labels
in Penn2Malt which defines 9 types of dependency
labels for Chinese, including AMOD, DEP, NMOD,
P, PMOD, ROOT, SBAR, VC, and VMOD.7
Table 6 shows the results trained on large data.
Although the number of non-terminal types de-
creased sharply from 63K to 3K, using the coarse
POS tag set in C-HPB surprisingly lowers the per-
formance with 1.1 BLEU scores on average (e.g.,
33.75 vs. 34.86), indicating that grouping POS
tags using simple linguistic rules is inappropriate for
HD-HPB. We still believe that this initial negative
finding should be supplemented by future work on
groupping POS tags using machine learning tech-
niques considering contextual information.
Table 6 also shows that replacing POS tags
of heads with their dependency labels (DL-HPB)
substantially lowers the average performance from
34.86 on BLEU score to 32.54, probably due to
the very coarse granularity of the dependency la-
bels used. In addition, replacing non-terminal label
with more refined tags (e.g., combination of original
POS tag and dependency label) also lowers trans-
lation performance (POS-DL-HPB). Further experi-
ments with more fine-grained dependency labels are
required.
7Some other types of dependency labels (e.g., SUB, OBJ)
are generated from function tags which are not available in our
automatic parse trees.
239
V
V-
N
R
1 d
u
i 
yi
 
N
N
2 
V
V
?
 
,
 
X
?
 
X
1 
X
2 
ag
ai
n
st
 
Ir
aq
 
(b)
 
zh
ic
hi
 
m
ei
gu
o
1 
du
i y
i c
el
ie
2,
 
 
su
pp
o
rt
 
A
m
er
ic
a?
s 1
 
st
an
d 2
 
ag
ai
n
st
 
Ir
aq
 
 
V
V-
N
R
?
 
zh
ic
hi
 
m
ei
gu
o
 ,
 
X
?
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
(a)
 
zh
ic
hi
 
m
ei
gu
o
,
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
Figure 3: Examples of pharse pairs and their head-driven
translation rules with dependency relation, regarding Fig-
ure 2
System MT 03 MT 04 MT 05 MT 06 Avg.
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50 37.61 34.56 31.78 34.86
C-HPB 34.10 36.43 33.46 31.00 33.75
DL-HPB 32.81 35.19 32.27 29.89 32.54
POS-DL-HPB 34.08 36.78 33.14 30.43 33.61
HD-DEP-HPB 35.48 38.17 34.81 32.38 35.21
Table 6: BLEU (%) scores of models trained on large
data.
5.5.3 Encoding Full Dependency Relations in
Translation Rule
Xie et al (2011) present a dependency-to-string
translation model with a complete dependency struc-
ture on the source side and a moderate average im-
provement of 0.46 BLEU over the HPB baseline. By
contrast, in our HD-HPB approach, dependency in-
formation is used to identify heads in the strings cov-
ered by non-terminals in HD-HR rules, and to refine
non-terminal labels accordingly, with an average im-
provement of 1.91 in BLEU over the HPB baseline
(when trained on the large data). This raises the
question whether and to what extent complete (un-
labeled) dependency information between the string
and the heads in head-labeled non-terminal parts of
the source side of SCFGs in HD-HPB can further
improve results.
Given the source side of a translation rule (ei-
ther HD-HR or NRR), say Ps ? s1 . . . sm (where
each si is either a terminal or a head POS in a re-
fined non-terminal), in a further set of experiments
we keep the full unlabeled dependency relations be-
tween s1 . . . sm so as to capture contextual syntactic
information in translation rules. For example, on the
source side of Figure 3 (b) where VV-NR maps into
words zhichi and meiguo while NN maps into word
celie, we keep the full unlabeled dependency rela-
tions among words {zhichi, meiguo, dui, yi, celie}.
HD-DEP-HPB (Table 6) augments translation rules
in HD-HPB with full dependency relations on the
source side. This further boosts the performance
by 0.35 BLEU scores on average over HD-HPB and
outperforms the HPB baseline by 2.26 BLEU scores
on average.
5.5.4 Error Analysis
We carried out a manual error analysis compar-
ing the outputs of our HD-HPB system with those
of Chiang?s (both trained on the large data). We ob-
serve that improved BLEU score often correspond to
better topological ordering of phrases in the hierar-
chical structure of the source side, with a direct im-
pact on which words in a source sentence should be
translated first, and which later. As ungrammatical
translations are often due to inappropriate topologi-
cal orderings of phrases in the hierarchical structure,
guiding the translation through appropriate topolog-
ical ordering should improve translation quality. To
give an example, consider the following input sen-
tence from the 04 NIST MT test data and its two
translation results:
? Input: ??0 ??1 ?2 ?3 ??4 ????5 ?
?6 ?7 ??8 ??9
? HPB: chinese delegation to us dollar purchase of
more high technology equipment
? HD-HPB: chinese delegation went to the united
states to buy more us high - tech equipment
Figure 4 demonstrates the topological orderings
in the two hierarchical structures. In addition to dis-
fluency and some grammar errors (e.g., a main verb
is missing), the basic HPB system also makes mis-
takes in reordering (e.g., ??4 ????5 ??6
translated as dollar purchase of more). The poor
translation quality, unsurprisingly, is caused by in-
appropriate topological ordering (Figure 4(a)). By
comparison, the topological ordering reflected in the
hierarchical structure of our HD-HPB model bet-
ter respects syntactic structure (Figure 4(b)). Let
240
??
?
0?
??
?
1?
?? 2?
?? 3?
??
?
4?
??
??
?
5?
??
?
6?
?? 7?
??
?
8?
??
?
9?
X [4
?4]
?
X [6
?6]
?
X [4
?6]
?
X [3
?7]
?
X [3
?8]
?
X [2
?9]
?
X [1
?9]
?
X [0
?9]
?
S [0
?9]
?
(a)
.?T
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?C
hia
ng
?s?
HP
B.?
(b
).?I
mp
ro
ve
d?t
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?H
D?
HP
B.
1.?
S [0
?9]
??
?X [
0?9
],??
????
????
????
????
??X
[0?
9]
?
2.?
X [0
?9]
??
???
[0?
0]
?X [
1?9
],??
????
????
????
????
???c
hin
es
e?X
[1?
9]?
3.?
X [1
?9]
??
???
[1?
1]
?X [
2?9
],??
????
????
????
????
??d
ele
ga
tio
n?X
[2
?9
]?
4.?
X [2
?9]
??
?? [
2?2
]?X
[3
?8]
???
[9?
9],?
?
????
????
????
????
??to
?X [
3?8
]?e
qu
ipm
en
t?
5.?
X [3
?8]
??
?X [
3?7
]??
?,?
?
X [3
?7]
?te
ch
no
log
y?
6.?
X [3
?7]
??
?? [
3?3
]?X
[4
?6]
?? [
7?
7],?
?
us
?X [
4?6
]?h
igh
?
7.?
X [4
?6]
??
?X [
4?4
]??
??
? [5
?5]
?X [
6?6
],?
X [6
?6]
?X [
4?
4]?o
f?m
or
e?
8.?
X [4
?4]
??
???
[4?
4]
,??
pu
rch
as
e?
9.?
X [6
?6]
??
???
[6?
6]
,??
do
lla
r
1. ?
VV
[0
?9]
??
?N
N [
0?1
]?V
V [2
?9
],??
????
????
????
?X?
?
?X [
0?
1]?X
[2
?9]
?
2.?
NN
[0?
1]
??
???
[0?
0]?N
N [
1?
1]
,??
????
????
????
??X
??
?ch
ine
se
?X [
1?
1]??
3.?
NN
[1?
1]
??
???
[1
?1]
,??
????
????
????
??X
??
?de
leg
ati
on
?
4.?
VV
[2
?9]
??
?? [
2?2
]??
[3?
3]?
VV
[4?
9]
,???
????
?X?
?
?w
en
t?t
o?t
he
?un
ite
d?s
tat
es
?to
?X [
4?
9]?
5.?
VV
[4
?9]
??
?VV
?M
[4
?6
]??
[7?
7]
???
[8
?8]
?N
N [
9?9
],?
????
????
????
X??
?X [
4?6
]?h
igh
??t
ec
h?X
[9?
9]??
6.?
VV
?M
[4
?6]
??
???
[4
?4]
?M
[5?
6]
,?
????
????
????
????
??X
??
?bu
y?X
[5?
6]
??
7.?
M
[5
?6]
??
?CD
[5?
5]?M
[6
?6]
,??
????
????
????
X??
?X [
5?5
]?X
[6
?6]
?
8.?
CD
[5
?5]
??
???
??
[5
?5]
,??
????
????
????
?X?
?
?m
or
e?
9.?
M
[6
?6]
??
???
[6?
6]
,??
????
????
????
X??
?us
?
10
.?N
N [
9?9
]??
???
[9
?9]
,??
????
????
????
?X?
?
?eq
uip
me
nt
?
ro
ot
?? NR 0 
?? NN 1 
? VV 2
? NR 3
??
 
VV 4 
??
?? CD 5 
?? M 6 
? JJ 7
?? NN 8 
?? NN 9
CD
[5-5
]
M
[6-6
]
M
[5-6
]
VV-M
[4-6
]
VV
[4-9
]
VV
[2-9
] 
NN
 
VV
[0-9
]
[0-1
]
NN
NN
[1-1
]
[9-9
]
Figure 4: An example Chinese sentence and its two hierarchical structures. Note: subscript [i-j] represents spanning
from word i to word j on the source side.
us refer to the HD-HPB hierarchical structure on
the source side as translation parse tree and to the
treebank-based parser derived tree as syntactic parse
tree from which we obtain unlabeled dependency
structure. Examining the translation parse trees of
our HD-HPB model shows that phrases with 1/2/3/4
heads account for 64.9%/23.1%/8.8%/3.2%, respec-
tively. Compared to 37.9% of the phrases in the
translation parse trees of the HPB model, 43.2% of
the phrases of our HD-HPB model correspond to a
linguistically motivated constituent in the syntactic
parse tree with exactly the same text span. In sum,
therefore, instead of simply enforcing hard linguistic
constraints imposed by a full syntactic parse struc-
ture, our model opts for a successful mix of linguis-
tically motivated and combinatorial (matching sub-
phrases in HPB) constraints.
6 Conclusion
In this paper, we present a head-driven hierarchi-
cal phrase-based translation model, which adopts
head information (derived through unlabeled depen-
dency analysis) in the definition of non-terminals
to better differentiate among translation rules. In
addition, improved and better integrated reorder-
ing rules allow better reordering between consecu-
tive non-terminals through exploration of a larger
search space in the derivation. Our model main-
tains the strengths of Chiang?s HPB model while at
the same time it addresses the over-generation prob-
lem caused by using a uniform non-terminal symbol.
Experimental results on Chinese-English translation
across a wide range of training and test sets demon-
strate significant and consistent improvements of our
HD-HPB model over Chiang?s HPB model as well
as over a source side version of the SAMT-style
model.
Currently, we only consider head information in a
word sequence. In the future work, we will exploit
more syntactic and semantic information to system-
atically and automatically define the inventory of
non-terminals (in source and target). For example,
for a non-terminal symbol VV, we believe it will
benefit translation if we use fine-grained dependency
labels (subject, object etc.) used to link it to its gov-
erning head elsewhere in the translation rule.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
241
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT 2005, pages 53?64.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of ACL
1996, pages 152?158.
Fei Xia. 2000. The part-of-speech tagging guidelines for
the Penn Chinese Treebank (3.0). Technical Report
IRCS-00-07, University of Pennsylvania Institute for
Research in Cognitive Science Technical.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of EMNLP 2011, pages
216?226.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
242
