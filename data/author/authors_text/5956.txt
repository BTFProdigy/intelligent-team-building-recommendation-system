R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 542 ? 552, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Lexicon-Constrained Character Model for Chinese 
Morphological Analysis 
Yao Meng, Hao Yu, and Fumihito Nishino 
Fujitsu R&D Center Co., Ltd, Room B1003, Eagle Run Plaza, No. 26 Xiaoyun Road,  
Chaoyang District, Bejing, 100016, P. R. China 
{Mengyao, Yu, Nishino}@frdc.fujitsu.com 
Abstract. This paper proposes a lexicon-constrained character model that com-
bines both word and character features to solve complicated issues in Chinese 
morphological analysis. A Chinese character-based model constrained by a 
lexicon is built to acquire word building rules. Each character in a Chinese sen-
tence is assigned a tag by the proposed model. The word segmentation and part-
of-speech tagging results are then generated based on the character tags. The 
proposed method solves such problems as unknown word identification, data 
sparseness, and estimation bias in an integrated, unified framework. Preliminary 
experiments indicate that the proposed method outperforms the best SIGHAN 
word segmentation systems in the open track on 3 out of the 4 test corpora. Ad-
ditionally, our method can be conveniently integrated with any other Chinese 
morphological systems as a post-processing module leading to significant im-
provement in performance. 
1   Introduction 
Chinese morphological analysis is a fundamental problem that has been studied ex-
tensively [1], [2], [3], [4], [5], [6], [7], [8]. Researchers make use of word or character 
features to cope with this problem. However, neither of them seems completely satis-
factory. 
In general, a simple word-based approach can achieve about 90% accuracy for 
segmentation with a medium-size dictionary. However, since no dictionary includes 
every Chinese word, the unknown word (or Out Of Vocabulary, OOV) problem [9], 
[10] can severely affect the performance of word-based approaches. Furthermore, 
word-based models have an estimation bias when faced with segmentation candidates 
with different numbers of words. For example, in the standard hidden Markov model, 
the best result, ?
=
?
==
n
i
iiii
TT
tttptwpWTpT
1
11
* )...|()|(maxarg)|( maxarg , is related to the number of 
the words in the segmentation candidates. As such, a candidate with fewer words is 
preferred over those with more words in the selection process. Therefore, most word-
based models are likely to fail when a combinational ambiguity1 sequence is separated 
into multiple words.  
                                                          
1
  A typical segmentation ambiguity, it refers to a situation in which the same Chinese sequence 
may be one word or several words in different contexts. 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 543 
Compared with Chinese words, Chinese characters are relatively less unambigu-
ous. The Chinese character set is very limited. Therefore, unknown characters occur 
rarely in a sentence. The grammatical advantages of characters have inspired re-
searchers to adopt character features in Chinese morphology and parsing [5], [6], [11], 
[12]. However, it is difficult to incorporate necessary word features, such as the form 
of a Chinese word and its fixed part-of-speech tags, in most character-based ap-
proaches. For this reason, character-based approaches have not achieved satisfactory 
performance in large-scale open tests.  
In this paper, we propose a lexicon-constrained character model to combine the 
merits of both approaches. We explore how to capture the Chinese word building 
rules using a statistical method, which reflects the regularities in the word formation 
process. First, a character hidden Markov method assigns the candidate tags to each 
character. Next, a large-size word list combined with linguistic information is used to 
filter out erroneous candidates. Finally, segmentation and part-of-speech tagging for 
the sentence are provided based on the character tags.  
The proposed model solves the problems of unknown word detection, word seg-
mentation and part-of-speech tagging using both word and character features. Addi-
tionally, our module is a post-processing module, which can be coupled to any exist-
ing Chinese morphological system; and it can readily recall some of the unknown 
words omitted by the system, and as a result, significantly improves the overall per-
formance. Evaluations of the proposed system on SIGHAN open test sets indicate that 
our method outperforms the best bakeoff results on 3 test sets, and ranks 2nd in the 4th 
test set [9].  
2   A Lexicon-Constrained Character Model for Chinese 
Morphology 
2.1   An Elementary Model to Describe Chinese Word Building Rules  
It is recognized that there are some regularities in the process of forming words from 
Chinese characters. This in general can be captured by word building rules. In this 
paper, we explore a statistical model to acquire such rules. The following are some 
definitions used in the proposed model. 
[Def. 1] character position feature 
We use four notations to denote the position of a character in a Chinese word. ?F? 
means the first character of the word, ?L? the last character, ?M? is a character within 
it and ?S? the word itself.  
[Def. 2] character tag set 
It is the product of the set of character position features and the set of part-of-
speech tags.  
Character tag set ={xy| setwordx  POS ? , },,,{ LMFSy ? }, where, x denotes one 
part-of-speech (POS) tag and y a character position feature. Together they are used to 
define the rules of Chinese word formation.  
[Def. 3] character tagging 
Given a Chinese sentence; character tagging is the process for assigning a character 
tag to each character in the sentence.  
544 Y. Meng, H. Yu, and F. Nishino 
Word building rules are acquired based on the relation between the character and 
the corresponding character tag. Word segmentation and part-of-speech tagging can 
be achieved easily based on the result of character tagging. For example, a character 
with ?xS? is a single character word with the part-of-speech tag ?x?; a character se-
quence starting with ?xF? and ending with ?xL? is a multiple character word with the 
part-of-speech tag ?x?.   
The elementary model adopts the character bi-gram hidden Markov model. In hid-
den Markov model, given the sentence,
nn ccccs 121 ...: ? , and character tagging result 
nn xyxyxyxyt 121 ...: ? , the probability of result t of s is estimated as: 
?
=
??
?=
ni
iiiii xycpxyxyxypstp
,1
12 )|() |()|(  (1) 
The best character tagging result for the sentences is given by equation (2):  
?
=
??
?=
ni
iiiii
t
xycpxyxyxypt
,1
12
* )|()|(maxarg  (2) 
We used the People's Daily Corpus of 1998 [13] to train this model. Also we 
adopted a 100,000-word dictionary listing all valid part-of-speech tags for each Chi-
nese word in the training phase to solve the data sparseness problem. The training data 
are converted into character tagging data through the following steps: a single charac-
ter word with ?x? is converted into the character marked with tag ?xS?; a two-character 
word with ?x? is converted into a first character with ?xF? and a second character with 
?xL?; a word with more than two characters with ?x? are converted into a first character 
with ?xF?, middle characters with ?xM? and last character with ?xL?. We adopt the POS 
tag set from the People's Daily Corpus, which consists of 46 tags. Taking into account 
of the four position features, the final character tag set is comprised of 184 tags. 
The emitted probability and transition probability of the model are estimated by the 
maximum likelihood method. The emitted probability is counted by the training Cor-
pus and the dictionary, where the Chinese words in the dictionary are counted one 
time. The transition probability is trained from the training Corpus only. 
2.2   An Improved Character-Based Model Using Lexicon Constraints 
We tested the above model based on the SIGHAN open test set [9]. The average pre-
cision for word segmentation was more than 88%. This means that most of the word 
building rules in Chinese have been obtained by the elementary model. However, the 
performance was relatively inferior to other word segmentation systems. It indicated 
that the model needed more features to learn word building rules. In error analysis, we 
found that the elementary model was so flexible that it produced many pseudo-words 
and invalid part-of-speech tags. In practice, a Chinese word is a stable sequence of 
Chinese characters, whose formation and part-of-speech tags are fixed by long-term 
usage. It seemed that only character position and meaning cannot describe a word 
building rule effectively.  
We also observed that word segmentation systems based on a simple dictionary 
matching algorithm and a few linguistic rules could achieve about 90% accuracy [14]. 
This suggested that a lexicon may have contribution to word building rules. Thus, we 
tried to incorporate a lexicon to the model to improve the performance. 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 545 
The major errors in the elementary model were pseudo words and invalid part-of-
speech (POS) tags. We proposed two constraints based on the lexicon to deal with 
these errors: 
1. If a possible word produced from the elementary model is in the word-
dictionary, the character tag of the characters forming this word should be 
consistent with the part-of-speech tag of the word in the dictionary.  
2. If a possible word produced is not in the dictionary, it must include one or 
more single characters, and none of which may be subsumed by any word in 
the dictionary in the current context. 
The first constraint eliminates invalid character tags. For example, the character  
?? ? has six character tags: ?aF? (first in adjective) , ?dF? (first in adverb), ?nF? (first 
in noun), ?nrF? (first in person name), ?tF? (first in time), and ?vF? (first in verb). The 
character ??? has five character tags: ?dL?, ?nL?, ?nrL?, ?tL?, and ?vL?. The combina-
tion of the two characters produces the possible word ????, which includes five 
possible word part-of-speech tags: ?d?, ?n?, ?nr?, ?t?, and ?v? based on these character 
tags. But ???? is a word in the dictionary, which only has two valid part-of-speech 
tags, namely, ?time? and ?person name?. Obviously, the part-of-speech tags: ?d?, ?n? 
and ?v? of ???? are invalid. Accordingly, the tags ?aF?, ?dF?, ?nF? , ?vF? on ??? and 
the tags ?dL?, ?nL?, ?vL? on ??? are also invalid. So they should be pruned from the 
candidates of the character tagging.  
The second constraint prunes pseudo words in the elementary model. Many studies 
in dictionary-based segmentation treat unknown words as sequences of single charac-
ters [1], [14]. The second constraint ensures that the new word produced by the ele-
mentary model must have one or more ?unattached? single characters (not subsumed 
by any other words). For example, the sequence ?????? (program error) will 
combine the pseudo word ???? because of the tag ?nF? on ??? and the tag ?nL? on 
???. The second constraint will prune ???? since ???? (program) and ???? 
(error) are already in the dictionary and there is no ?unattached? single character in it. 
Accordingly, the tag ?nF? on ??? and the tag ?nL? on ??? will be deleted from the 
candidates of character tagging.  
The following experiments show the lexicon-based constraints are very effective in 
eliminating error cases. The elementary model faces an average of 9.3 character tags 
for each character. The constraints will prune 70% of these error tags from it. As a 
result, the performance of character tagging is improved.  
It is worth noting that the lexicon in the elementary model cannot distort the prob-
ability of the character tagging results in the model. The pruned cases are invalid 
cases which cannot occur in the training data because all the words and POS tags in 
the training data are valid. Thus, the model built from the training data is not affected 
by the pruning process.  
2.3   Case Study 
In this subsection, we illustrate the advantages of the proposed method for Chinese 
morphology with an example.  
546 Y. Meng, H. Yu, and F. Nishino 
Example: ?????????????? 
(Xiaoming will analyze the program errors tomorrow).  
Where, ???? is an unknown word (person name), and the sequence ???? is a 
combinational ambiguity (either ???? (put up with) or ???+ ??? (will)). Here is 
how our approach works. 
Step 1: List all the character tags for each character. Figure 1 shows the character 
tags in the sequence ?????? . 
?
aF dF nF nrF nM nrM nsM qM vM aL dL vL aS 
?
aF dF nF nrF vF tF nM lM tM aL dL nrL aS 
?
aF dF nF nrF vF tF nM lM tM aL dL nrL aS 
?
nF tF nrM dL nL nrL tL vL
 
Fig. 1. Candidates for the sequence ?????? 
In this step we are able to find possible unknown words based on character position 
features. For example, the character tags in ?????? combine four possible un-
known words: ????, ?????, ????? , and ??????.   
Step 2: Prune the invalid candidates using constraints. 
The first constraint prunes some invalid character tags. For example, ???? can be 
either an adverb (d) or a personal name (nr); ???? is a time (t) word. The other part-
of-speech tags of these two words will be deleted. With the second constraint, we can 
delete ????? because ???? and ???? are words in the dictionary. However, ??
?? , ?????, and ?????? will be kept because ??? is a ?unattached? single 
character. The remaining candidates are shown in figure 2.  
?
aF dF nF nrF nM nrM nsM qM vM aL dL vL aS
?
 dF  nrF   nM lM tM    aS
?
     tF nM lM tM  dL nrL aS
?
nF tF nrM   nrL tL  
 
Fig. 2. Remaining Candidates for the sequence ?????? 
Step 3: Choose the best character tagging result based on the proposed character 
hidden Markov model.  
The best character tagging result is chosen using equation 2 in Section 2.1. The 
ambiguities in segmentation and word POS tagging are solved in the character tag-
ging process.  
Consider the combinational ambiguity ???? in the following 2 candidates: 
Candidate 1: ???/nr ??/t ?/d ?/d ??/n ??/n ??/v ??/v? 
Candidate 2: ???/nr ??/t ??/v ??/n ??/n ??/v ??/v? 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 547 
In word-based linear model, the erroneous candidate 2 will be prior to the correct 
candidate 1 since the model counts 9 nodes in candidate 1 but 8 nodes in candidate 2. 
However, there is no such bias in the character model because the number of charac-
ters does not change. The combinational ambiguity ???? will be denoted as ??/dS 
?/dS? or ??/vF ?/vL?. The number of nodes in all candidates of character tagging is 
the same.  
At last, the correct result ??/nrF ?/nrL ?/tF ?/tL ?/dS ?/dS?/nF ?/nL ?/nF 
?/nL ?/vF ?/vL ?/vF ?/vL? is selected, and the corresponding morphological result 
is: ???/nr ??/t ?/d ?/d ??/n ??/n ?? /v ??/v ?.  
The above steps show the proposed approach solves the various issues related to 
Chinese morphology by a concise character tagging process where word building is 
revealed. 
3   Experiments and Discussion 
We evaluated the proposed character method using the SIGHAN Backoff data, i.e. the 
one-month People's Daily Corpus of 1998, and the first version of Penn Chinese Tree-
bank [15]. We compared our approach against two state-of-the-art systems: one is 
based on a bi-gram word segmentation model [7], and the other based on a word-
based hidden Markov model [3]. For simplicity, we only considered three kinds of 
unknown words (personal name, location name, and organization name) in the all 
methods.  
The same corpus and word-dictionary were used to train the above three systems. 
The training data set was the 5-month People's Daily Corpus of 1998, which con-
tained approximately 6,300,000 words and 46 word part-of-speech tags. The system 
dictionary contained 100,000 words and the valid part-of-speech tag(s) of each word.  
On average, there were 1.3 part-of-speech tags for a word in the dictionary.  
In the following, chr-HMM refers to the proposed elementary model; chr-
HMM+Dic refers to the character model improved by integrating linguistic informa-
tion. W-Bigram is the word-based bi-gram system, and W-HMM is the word-based 
hidden Markov system. 
3.1   Morphological Experimental Results 
We examined the performance of our model in comparison against W-Bigram and W-
HMM. Table 1 compares the segmentation performance of our model against that of 
other models. Table 2 shows the accuracy in unknown word identification. Table 3 
illustrates the performance of the part-of-speech tagging. The experiments in Table 1 
and Table 2 were examined using the SIGHAN open test corpora. The experiments in 
Table 3 were performed again on the one-month People's Daily Corpus (PD corpus) 
and 4,000 sentences in the Penn Chinese Treebank (Penn CTB). We only examined 4 
major word categories in the Penn Chinese Treebank due to inconsistency in the part-
of-speech tag sets between the two corpora. The 4 major word categories were: noun 
(shown as NN, NR in Penn CTB; n, nr, ns, nz in PD corpus), verb (VV in Penn CTB; 
v, vd, vn in PD corpus), adjective (JJ in Penn CTB; a, ad, an in PD corpus) and adverb 
(AD in Penn CTB; d in PD corpus). 
548 Y. Meng, H. Yu, and F. Nishino 
Segmentation and word POS tagging performance is measured in precision (P%), 
recall (R%) and F-score (F). Unknown words (NW) are those words not found in our 
word-dictionary, which include named entities and other new words. The unknown 
word rate (NW-Rate), the precision on unknown words (NW-Precision) and recall on 
total unknown words (NW-Recall) are given by:   
NW-Rate= identifiedNW  of  #  total
rdsunknown wo of  #
 NW-Precision = identifiedNW  of # total
rdsunknown wo  validof #
 
NW-Recall = data in testingNW  of # total
rdunknown wo  validof #
 
Table 1 shows that the above three systems achieve similar performances on the 
PK testing corpus. All of them were trained by the People's Daily corpus. For this 
reason, their performances were similar when the testing data had similar styles. But 
for other texts, the proposed character model performed much better than the word-
based models in both recall and precision. This indicated that our approach performed 
better for unseen data.  
Table 2 shows that our method for unknown word identification also outperforms 
the word-based method. We notice that word-based approaches and character-based 
approaches have similar precision on unknown word identification, however word-
based approaches have much lower recall than character-based ones. The main reason 
for this is that word-based systems focus only on unknown words with proper word 
structures, but cannot recognize newly generated words, rare words, and other new 
words unlisted in the dictionary. A very high proportion of these types of unknown 
word in the SIGHAN testing data affects the recall of the word-based methods on 
unknown words. The experiments reveal that our method could effectively identify all 
kinds of new words. This is because our model has defined word building rules for all 
kinds of words. 
Without a widely recognized testing standard, it is very hard to evaluate the per-
formance on part-of-speech tagging. The results in Penn Chinese Treebank was better 
than that in the People's Daily Corpus since we examined all 42 POS tags in the Peo-
ple's Daily Corpus, but we only tested four major POS tags in Penn Chinese Tree-
bank. Our approach is better than the word-based method for two test data sets. How-
ever, we could not conclude that our method was superior to the word-based method 
because of the limited testing approaches and testing data. A thorough empirical com-
parison among different approaches should be investigated in the future.  
Table 1. Comparison of word segmentation based on SIGHAN open test sets 
 PK CTB HK AS 
 R%/ P% F R%/ P% F R%/ P% F R%/ P% F 
Chr-HMM 91.9/91.8 91.8 86.9/87.3 87.1 87.7/86.7 87.2 89.9/89.1 89.5 
Chr-HMM+Dic 95.9/96.7 96.3 92.7/93.5 93.1 91.1/91.9 91.5 92.3/93.9 93.1 
W-Bigram 94.7/95.4 95.1 87.4/86.8 87.1 88.7/83.7 86.3 87.9/85.1 86.5 
W-HMM 94.6/95.1 94.9 88.6/89.2 88.9 90.7/89.1 89.9 90.7/87.2 89.0 
Rank 1 in SIG 96.3/95.6 96.0 91.6/90.7 91.2 95.8/95.4 95.6 91.5/89.4 90.5 
Rank 2 in SIG 96.3/94.3 95.3 91.1/89.1 90.1 90.9/86.3 88.6 89.2/85.3 87.3 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 549 
Table 2. Accuracy of unknown word identification for SIGHAN open test sets 
 PK CTB HK AS 
Chr-HMM UWR% P% R% UWR% P% R% UWR% P% R% UWR% P% R% 
Chr-HMM+Dic 2.3 56.2 54.8 10.4 68.8 64.4 9.7 61.4 58.4 8 65.4 62.9 
W-Bigram 2.3 54.7 53.6 10.4 53.9 23.8 9.7 53.0 29.6 8 64.6 35.3 
W-HMM 2.3 58.1 51.3 10.4 68.3 37.2 9.7 62.3 40.7 8 68.4 41.1 
Table 3. Comparison of word part-of-speech tagging 
 People Daily Penn CTB 
 P% R% F-score P% R% F-score 
Chr-HMM 82.4% 82.5% 82.5 89.7% 88.5% 89.1 
Chr-HMM+Dic 89.3 87.8 88.6 92.5 91.5 92.0 
W-HMM 86.2% 85.4% 85.7 91.1% 90.8% 91.0 
From Table 1 and Table 3, we notice that chr-HMM achieved 88% accuracy in 
word segmentation and 80% in part-of-speech tagging without a word-dictionary. 
Chr-HMM is a state-of-the-art Chinese morphology system without a word-
dictionary. Its performance is comparable to some dictionary-based approaches (e.g., 
forward-maximum). This result indicates that our model has effectively captured most 
of the Chinese word building rules.  
The results also show that chr-HMM+Dic outperformed the best SIGHAN word 
segmentation system on 3 out of the 4 SIGHAN open track test corpora, and achieved 
top 2 in the case of HK testing corpus.  
3.2   Incorporation with Other Systems  
The advantage of the proposed model is proficiency in describing word building rules 
and since many existing NLP application systems are weak in identifying new words, 
it is intuitive to integrate our model to existing systems and serves as a post-
processing subsystem. In this subsection, we show how existing word segmentation 
systems could be improved using chr-HMM.  
Given a segmentation result, we assume that unidentified new words may be a se-
quence of unattached characters. That is, all multiple-character words in the given 
result are considered correct, while single words, which might include unidentified 
new words will be rechecked by the chr-HMM. The entire process involves 3 steps: 
1. Only character tags that are consistent with the position of the character in the 
word are listed for multi-character words.  
2. The unattached characters are tagged with all possible character tags. In this 
way, the original segmentation result is converted into a group of character 
tagging candidates.  
3. We then input these character tagging candidates into the chr-HMM to select 
the best one.  
550 Y. Meng, H. Yu, and F. Nishino 
Consider an original result:  
?? [?  ?  ?  ?  ?] ??  [?  ? ] (Jordan bounced back strongly from the 
bottom yesterday) 
The parts in brackets are the sequence of single characters where the new words 
may appear. The chr-HMM will list all possible character tags for these ?unattached? 
characters. The parts outside the brackets are multiple-character words identified by 
the original system. They are assumed correct and maintain also positional informa-
tion. Only the character tags, which are consistent with the positions of the character 
in the word are listed. The character tagging candidates for the above sample is given 
in Figure 3: 
?  ? ?  
vL ? nL nsL ? 
 
tL vL nM nrL vM
  
? nL lM nsF nL 
 
nM ?
nrF nrL tL nM vF nrF nM vF vL vF vL
nF nL nL aM pS nF vF nF nL aF nL
aF aL tS tS dS nS nS aF aL aS vS
? ? ? ? ? ? ? ? ? ? ?
 
Fig. 3. Character tagging candidates for rechecking 
Chr-HMM is then applied to the character tagging candidates and the best charac-
ter tagging selected based on the probability of the candidates is output as the result. 
In this example, the result is: ??? (Jordan) ?? (yesterday) ? (from) ?? (earth) 
?? (strongly) ?? (bound)?. The three missing new words in the original system 
are identified by this post-processing subsystem.  
We re-assigned the word segmentation results for all participants who have given 
permission to release data from the SIGHAN site (available for download from 
http://www.sighan.org/bakeoff2003 ). Table 4 enlists the performance of SIGHAN 
open test with and without chr-HMM. The participant numbers correspond to the sites 
listed in [9].  
Table 4. Comparison of results with and without chr-HMM 
Corpus Site  R% P% F 
AS 03 Before After 
89.2 
90.8 
85.3 
92.0 
87.2 
91.4 
01 Before After 
88.7 
90.1 
87.6 
91.8 
88.1 
90.9 
03 Before After 
85.3 
86.4 
80.6 
87.8 
82.9 
87.1 CTB 
10 Before After 
91.1 
91.0 
89.1 
93.5 
90.1 
92.3 
HK 03 Before After 
90.9 
89.4 
86.3 
91.0 
88.6 
90.2 
03 Before After 
94.1 
94.4 
91.1 
95.3 
92.5 
94.9 PK 
10 Before After 
96.3 
95.6 
95.6 
97.7 
95.9 
96.7 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 551 
From Table 4, it is obvious that word segmentation precision increases signifi-
cantly, and at the same time, the corresponding recall remains the same or slightly 
declined. This implies that the chr-HMM retains the correct words by the original 
system and concurrently decreases significantly its errors.  
4   Related Work  
Although character features are very important in Chinese morphology, research in 
character-based approach is unpopular.  Chooi-Ling Goh et al [16], Jianfeng Gao et 
al. [8] and Huaping Zhang [3] adopted character information to handle unknown 
words; X. Luo [11], Yao Meng [12] and Shengfen Luo [17] each presented character-
based parsing models for Chinese parsing or new-word extraction. T. Nakagawa used 
word-level information and character-level information for word segmentation [6]. 
Hwee Tou Ng et al [5] investigated word-based and character-based approaches and 
proposed a maximum entropy character-based POS analyzer. Although the character 
tags proposed in this paper are essentially similar to some of the previous work men-
tioned above, here our focus is to integrate various word features with the character-
based model in such a way that the probability of the model is undistorted. The pro-
posed model is effective in acquiring word building rules. To our knowledge, our 
work is the first character-based approach, which outperforms the word-based ap-
proaches for SIGHAN open test. Also, our approach is versatile and can be easily 
integrated with existing morphological systems to achieve improved performance.  
5   Conclusion and Future Works  
A lexicon-constrained character model is proposed to capture word building rules 
using word features and character features. The combination of word and character 
features improves the performance of word segmentation and part-of-speech tagging. 
The proposed model can solve complicated issues in Chinese morphological analysis. 
The Chinese morphological analysis is generalized into a process of specific character 
tagging and word filtering. A lexicon supervises the character-based model to elimi-
nate invalid character tagging candidates.  
Our system outperformed the best SIGHAN word segmentation system in 3 out of 
the 4 SIGHAN open test sets. To our knowledge, our work is the first character-based 
approach, which performs better than word-based approaches for SIGHAN open test. 
In addition, the proposed method is versatile and can be easily integrated to any exist-
ing Chinese morphological system as a post-processing subsystem leading to en-
hanced performance.  
In this paper, we focused on word features in character-based mode, and adopted 
HMM as the statistical model to identify the rules. Other statistical models, such as 
maximum entropy, boosting, support vector machine, etc., may also be suitable for 
this application. They are worth investigating. The data sparseness problem is practi-
cally non-existent in the character-based model for the Chinese character set is lim-
ited. However, odd characters are occasionally found in Chinese personal or place 
names. Some rules using named entity identification technique may help smoothen 
552 Y. Meng, H. Yu, and F. Nishino 
this. In a broader view, the word building rules proposed in our model is simple 
enough for linguistic studies to better understand for example formation of Chinese 
words or even the Chinese language itself. 
References 
1. Andi Wu. Chinese Word Segmentation in MSR-NLP. In Proc. of SIGHAN Workshop, 
Sapporo, Japan, (2003) 127-175 
2. GuoDong Zhou and Jian Su. A Chinese Efficient Analyzer Integrating Word Segmenta-
tion, Part-Of-Speech Tagging, Partial Parsing and Full Parsing. In Proc. Of SIGHAN 
Workshop, Sapporo, Japan, (2003) 78-83 
3. Huaping Zhang, Hong-Kui Yu et al. HHMM-based Chinese Lexical Analyzer ICTCLAS. 
In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 184-187 
4. Nianwen Xue and Libin Shen. Chinese Word Segmentation as LMR Tagging. In Proc. Of 
SIGHAN Workshop, Sapporo, Japan, (2003) 176-179 
5. Hwee Tou Ng, Low, Jin Kiat. Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-
Once? Word-Based or Character-Based? In Proc. of EMNLP, Barcelona, Spain, (2004) 
277-284 
6. Tetsuji Nakagawa. Chinese and Japanese Word Segmentation Using Word-level and 
Character-level Information, In Proc. of the 20th COLING, Geneva, Switzerland, (2004) 
466-472 
7. Guohong Fu and Kang-Kwong Luke. A Two-stage Statistical Word Segmentation System 
for Chinese. In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 156-157 
8. Jianfeng Gao, Andi Wu, Chang-Ning Huang et al Adaptive Chinese Word Segmentation. 
In Proc. of 42nd ACL. Barcelona, Spain, (2004) 462-469  
9. Richard Sproat and Thomas Emerson. The First International Chinese Word Segmentation 
Bakeoff. In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 133-143 
10. X. Luo. A Maximum Entropy Chinese Character-based Parser. In Proc. of EMNLP. Sap-
poro, Japan, (2003) 192-199 
11. Honglan Jin, Kam-Fai Wong, ?A Chinese Dictionary Construction Algorithm for Informa-
tion Retrieval?, ACM Transactions on Asian Language Information Processing, 1(4):281-
296, Dec. 2002. 
12. Yao Meng, Hao Yu and Fumihito Nishino. 2004. Chinese New Word Identification Based 
on Character Parsing Model. In Proc. of 1st  IJCNLP, Hainan, China, (2004) 489-496 
13. Shiwen Yu, Huiming Duan, etal. ?????????????????. ?????
?v(5), (2002) 49-64, 58-65 
14. Maosong Sun and Benjamin K. T? Sou. Ambiguity Resolution in Chinese Word Segmenta-
tion. In Proc. of 10th Pacific Asia Conference on Language, Information & Computation, 
(1995) 121-126 
15. Nianwen Xue, Fu-Dong Chiou and Martha Palmer. Building a Large-scale Annotated 
Chinese Corpus. In Proc. of the 19th COLING. Taibei, Taiwan, (2002)  
16. Chooi-Ling GOH, Masayuki Asahara, Yuji Matsumoto. Chinese Unknown Word Identifi-
cation Using Character-based Tagging and Chunking. In Proc. of the 41st ACL, Interac-
tive Poster/Demo Sessions, Sapporo, Japan, (2003) 197-200 
17. Shengfen Luo, Maosong Sun. 2003, Two-character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual Measure, In Proc. of the 2nd SIGHAN Workshop, Sap-
poro, Japan, (2003) 20-30 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 1004 ? 1016, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Web-Based Terminology Translation Mining 
Gaolin Fang, Hao Yu, and Fumihito Nishino 
Fujitsu Research and Development Center, Co., LTD. Beijing 100016, China  
{glfang, yu, nishino}@frdc.fujitsu.com 
Abstract. Mining terminology translation from a large amount of Web data can 
be applied in many fields such as reading/writing assistant, machine translation 
and cross-language information retrieval. How to find more comprehensive re-
sults from the Web and obtain the boundary of candidate translations, and how 
to remove irrelevant noises and rank the remained candidates are the challeng-
ing issues. In this paper, after reviewing and analyzing all possible methods of 
acquiring translations, a feasible statistics-based method is proposed to mine 
terminology translation from the Web. In the proposed method, on the basis of 
an analysis of different forms of term translation distributions, character-based 
string frequency estimation is presented to construct term translation candidates 
for exploring more translations and their boundaries, and then sort-based subset 
deletion and mutual information methods are respectively proposed to deal with 
subset redundancy information and prefix/suffix redundancy information 
formed in the process of estimation. Extensive experiments on two test sets of 
401 and 3511 English terms validate that our system has better performance.  
1   Introduction 
The goal of Web-based terminology translation mining is to mine the translations of 
terminologies or proper nouns which cannot be looked up in the dictionary from the 
Web using a statistical method, and then construct an application system for read-
ing/writing assistant (e.g. Mont Blanc????, ???). Translators and technical 
researchers cannot yet obtain an accurate translation after many lookup efforts when 
they encounter terminology or proper noun during translating or writing foreign lan-
guage. According to Web statistics by Google, 76.59% of Web pages are English. In 
China, statistical results by China Internet Network Information Center in July 2004 
show that the number of Internet users has reached 94 million, and nearly 87.4% of 
users have educational backgrounds beyond high school. These users can smoothly read 
general English pages, but some terminologies in the Web hamper them to exactly un-
derstand the whole content. Some skilled users perhaps resort to a Web search engine, 
but they cannot obtain effective information from a large amount of retrieved irrelevant 
pages and redundancy information. Thus, it is necessary to provide a system to auto-
matically mine translation knowledge of terms or proper nouns using abundant Web 
information so as to help users accurately read or write foreign language.  
The system of Web-based terminology translation mining has many applications. 
1) Reading/writing assistant, as one part of computer-assisted language learning 
(CALL) used in the E-learning. During reading or writing, users often meet terms 
 Web-Based Terminology Translation Mining 1005 
whose translations cannot be found in the dictionary, but this system can help them 
mine native and accurate translations from the Web. 2) The tool for constructing bi-
lingual dictionary. The system can not only provide translation candidates for compil-
ing bilingual lexicon, but also evaluate or rescore the candidate list of the dictionary. 
The constructed dictionary can be further applied in cross-language information re-
trieval (CLIR) and machine translation. 3) As one of the typical application paradigms 
of the combination of CLIR and Web mining. 
There are some issues that need to be solved using Web information to mine termi-
nology translation: 1) How to find more comprehensive results, i.e. mining all possi-
ble forms of annotation pairs in the Web. 2) How to obtain the boundary of candidate 
translations, especially for the language without the boundary mark such as Chinese 
and Japanese. Because we don?t know the translation is at left or right, and what is 
between the pair, and where is the candidate endpoint? 3) How to remove the noises 
formed in the statistics and rank the remained candidates.  
On the basis of reviewing all possible methods of acquiring translations, a feasible 
statistics-based method is proposed to mine terminology translation from the Web. In 
the proposed method, after analyzing different forms of term translation distributions, 
character-based string frequency estimation is employed to construct term candidate 
translations for exploring more translations and their boundaries, and then the candi-
date noises formed in the process of statistics are defined as two categories: subset 
redundancy information and prefix/suffix redundancy information. Sort-based subset 
deletion and mutual information methods are respectively proposed to deal with two 
redundancy information. Experiments on two test sets of 401 and 3511 English terms 
show that our system has better performance. In all reported literatures, our experi-
ment is the first time for the extensive research on Web-based terminology translation 
mining on the largest scale. 
2   Related Work 
Automatic acquisition of bilingual word pairs or translations has been extensively 
researched in the literature. The methods of acquiring translations are usually summa-
rized as four categories: 1) acquiring translation from parallel corpora, 2) acquiring 
translation from a combination of translations of constituent words, 3) acquiring 
translation from bilingual annotation in the Web, and 4) acquiring translation from 
non-parallel corpora.  
1)  Acquiring translation from parallel corpora. 
Acquiring bilingual lexicon or translations from parallel corpora (including sentence 
alignment and paragraph alignment) is to utilize statistics information such as co-
occurrence, position, and length between source word and translation equivalence in 
parallel texts as an evaluation criterion to obtain one-to-one map word pairs. Many 
previous researches focused on extracting bilingual lexicon from parallel corpora, and 
readers can refer to the reviews [1], [2] for the details. However, due to the restriction 
of current available parallel corpora of different languages, together with the fact that 
corpus annotation requires a lot of manpower and resources, researchers have at-
tempted to extract translations from non-parallel corpus or Web data. As opposed to 
extracting from parallel corpora, there are no corresponding units in non-parallel 
1006 G. Fang, H. Yu, and F. Nishino 
corpora so that statistics information such as co-occurrence, position and length be-
come unreliable. New statistical clues have to be proposed to build the relationship 
for acquiring translation pairs from non-parallel corpora, which is more difficult to 
handle than in parallel corpora.  
2) Acquiring translation from a combination of translations of constituent words. 
Grefenstette [3] employed an example-based approach to obtain compound word 
translations. His method first combined possible translations of each constituent, and 
then searched them in WWW, where the retrieved number was viewed as an evalua-
tion criterion. Experiments on a set of 724 German words and a set of 1140 Spanish 
terms showed that the accuracies of English translations were about 87% and 86%, 
respectively.  
Cao and Li [4] proposed a dictionary-based translation combination method to col-
lect translation candidates of English base noun phrases, and then employed a naive 
Bayesian classifier and TF-IDF vector constructed with EM algorithm as evaluation 
criterions for translation selection. In an experiment with 1000 English base noun 
phrases, the coverage of acquiring translations was 91.4%, and the accuracy of top 3 
choices was 79.8%. The system was further improved in the literature [5]. 
Navigli et al [6] proposed an ontology learning method for acquiring terminology 
translations from English to Italian. His method was based on bilingual lexicon and 
semantic relation between the constituents of source language derived from ontology 
learning, where disambiguated terms dramatically reduced the number of alternative 
translations and their combinations. This system can automatically extract the transla-
tions of 405 complex terms in the tourism domain. 
Using the translation combination of each constituent to acquire the translation of a 
multiword term is very suitable for translation acquisitions of base noun phrases. 
However, terminologies and technical terms often consist of unknown words, and 
their translations are seldom the combination of each constituent. Thus, the result of 
direct combination is not very desirable for terminology translation acquisition.  
3)  Acquiring translation from bilingual annotation in the Web. 
Nagata et al [7] proposed an empirical function of the byte distance between Japa-
nese and English terms as an evaluation criterion to extract the translation of Japanese 
word, and their results could be used as a Japanese-English dictionary. Preliminary 
experiments on the 50 word pairs showed that an accuracy of top 50 candidates 
reached 56%. The reasons for such experimental results have two aspects: first, the 
system didn?t further deal with candidate noises for mining useful knowledge; second, 
this system only handled top 100 Web pages retrieved from search engine. In fact, 
previous 100 Web pages seldom contain effective bilingual annotation information 
only directly using keyword search rather than imposing other restrictions. Thus, this 
problem should be further researched for practical applications. Since his research 
focused on finding English translation given a Japanese term, the segmentation of 
Japanese could be avoided. However, our problem is to find Chinese equivalent using 
English term, so we have to cope with how to obtain the correct boundary of Chinese 
translations. Therefore, the issue and the proposed method in this paper are distinctly 
different with Nagata?s.  
 Web-Based Terminology Translation Mining 1007 
4)  Acquiring translation from non-parallel corpora. 
Acquiring translation from non-parallel corpora is based on the clue that the context 
of the source term is very similar to that of the target translation in a large amount of 
corpora. In 1995, Rapp [8] assumed that there is a correlation between the patterns of 
word co-occurrence in non-parallel texts of different languages, and then proposed a 
matrix permutation method to match these patterns. However, computational limita-
tion hampered further extension of this method. In 1996, Tannaka and Iwasaki [9] 
demonstrated how to extract lexical translation candidates from non-aligned corpora 
using the similar idea. In 1999, this method was developed and improved by Rapp 
[10]. Rather than computing the co-occurrence relation matrix between one word and 
all words, the matrix between one word and a small base lexicon are estimated. Ex-
periments on 100 German words indicated that an accuracy of top 1 English transla-
tion was 72%, and top 10 was 89%. This system was only suitable for the situation of 
one word to one word, and didn?t further research on the translation acquisition from 
multiword to multiword. 
In 1995, Fung [11] proposed a ?context heterogeneity? method to compute the 
measure similarity between word and its translation for finding translation candidates. 
In the experiment with 58 English words, an accuracy of 50% is obtained in the top 
10 Chinese word candidates. Based on this work, Fung presented the word relation 
matrix to find the translation pair in 1997 [12]. This method respectively computed 
the correlation vectors between source word and seed word, target word and seed 
word. In 19 Japanese term test set, the accuracy of English translations reached 30%. 
In 1998, the method was improved to extend to non-parallel, comparable texts for 
translation acquisition [13]. This system use TF/IDF as the feature, and different 
measure functions as the similarity computation between the candidate pair. However, 
the system was restricted to the assumption that there are no missing translations and 
all translations are included in the candidate word list.  
Shahzad et al [14] first extracted the sentence corpora that are likely to contain the 
target translation using bilingual dictionary and transformation table. And then, the 
heuristics method was employed to obtain the correct candidate by analyzing the 
relations of source compound nouns and using partial context information. Experi-
ments on the 10 compound nouns showed that the average accuracy and recall were 
respectively 34% and 60%. 
As shown from the current situation of translation acquisition from non-parallel 
corpora, all experiments above are basically performed on small-scaled word set, and 
their results are very inspiring but difficult to put into practical use. Furthermore, most 
experimental methods are only suitable for one word translation, i.e. the word number 
ratio of translation pair is on a basis of 1:1. Thus, there are many issues to be further 
researched before it is used to explore new translation in the application area.  
From the review above, we know that Method 1 requires a large number of parallel 
corpora, and Method 2 and Method 4 have some limitations when they are applied to 
acquire the terminology translation, and Method 3 makes the best of mass Web re-
sources and is a feasible approach. When people use Asia language such as Chinese, 
Japanese, and Korean to write, especially scientific article or technical paper, they 
often annotate the associated English meaning after the terminology. With the devel-
opment of Web and the open of accessible electronic documents, digital library, and 
1008 G. Fang, H. Yu, and F. Nishino 
scientific articles, these resources will become more and more abundant. Thus, 
Method 3 is a feasible way to solve the terminology translation acquisition, which is 
also validated by the following experiments. 
3   The Framework of the Terminology Translation Mining System 
The Web-based terminology  translation  mining  system  is  depicted  in Fig. 1 as 
follows: 
 
Query  
?Mont Blanc?
WWW 
(bilingual annotation)
Rank & sort 
candidates
String frequency 
estimation 
Result 
????? 
Web page 
download 
module
Candidate noises 
& solutions 
HTML  
analysis 
Web page collection 
Terminology translation mining 
 
Fig. 1.  The Web-based terminology translation mining system 
The system consists of two parts: Web page collection and terminology translation 
mining. Web page collection includes download module and HTML analysis module. 
The function of download module is to collect these Web pages with terms? associ-
ated bilingual annotations, and then the pages are inputted into HTML analysis mod-
ule. In HTML analysis, Web pages are built as a tree structure from which possible 
features for the bilingual pair and text information in the HTML page are simultane-
ously extracted. 
Terminology translation mining includes string frequency estimation, candidate 
noises and their solutions, and rank & sort candidates. Translation candidates are 
constructed through string frequency estimation module, and then we analyze their 
noises and propose the corresponding methods to handle them. At last, the approach 
combining the possible features such as frequency, distribution, length proportion, 
distance, keywords and key symbols is employed to rank these candidates. 
In Web pages, there are a variety of bilingual annotation forms. Correctly exploring 
all kinds of forms can make the mining system extract the comprehensive translation 
results. After analyzing a large amount of Web page examples, we summarize transla-
tion distribution forms as the following six categories: 1) Direct annotation 2) Sepa-
rate annotation 3) Subset form 4) Table form 5) List form 6) Explanation form. Direct 
annotation is the most widely used form in the Web, where English meaning often 
 Web-Based Terminology Translation Mining 1009 
follows after Chinese terminology, and some have symbol marks such as bracket 
parentheses and bracket, and some have nothing, e.g. ????Mont Blanc?. Separate 
annotation is referred to as the case that there are some Chinese words or English 
letters between the translation pair, e.g. ?????,???universal life insurance?. 
Subset form is that the extracted translation pair is a subset of existing bilingual pair, 
for example, during searching the term ?Mont Blanc?, the term pair ????????
(Chamonix Mont Blanc)? also provides the valid information. Table or list form is the 
Web page in the form of table or list. Explanation form is the explanation and illustra-
tion for technical terms.  
 
Fig. 2. The examples of translation distribution forms, (a) Direct annotation, some has no mark 
(a1), and some have some symbol marks (a2, a3) (b) Separate annotation, there are English 
letters (b1) or some Chinese words (b2, b3) between the translation pair  (c) Subset form  (d) 
Table form  (e) List form  (f) Explanation form 
4   Statistics Based Translation Finding 
4.1   Character-Based String Frequency Estimation 
All kinds of possible translation forms of terminologies in the Web can be effectively 
and comprehensively mined through character-based string frequency estimation. The 
proposed method with Chinese character as the basic unit of statistics can not only 
obtain the correct boundary of the translation candidate, but also conveniently explore 
these Chinese candidate terminologies that usually consist of unknown words or un-
known compound words. 
String frequency information is one of the important clues during extracting candi-
date translations. Its estimation method has a direct influence on the system perform-
ance efficiency. The method combing hash index and binary search is employed to 
(a1) (a2) (a3) 
(b1) (b2) (b3) 
(c) (d) (e) (f) 
1010 G. Fang, H. Yu, and F. Nishino 
construct the index for all translation candidates. The definition of hash function is 
calculated according to 6763 Chinese characters in GB2312 system with a one-to-one 
map. Hash function is formulized as:  
otherwise
c
c
cc
cc
Y 215
176215
6763
5)161()176(94
)161()176(94
0
0
10
10
>
??
??
??
?
??+?
?+?
=  ,                    (1) 
where 10 ,cc  are respectively the unsigned encoding values of the first, second bytes 
of first Chinese character of candidate items. All strings are partitioned into different 
blocks in terms of the first Chinese character with the hash function above, where the 
strings with the same first character are sorted by lexicographic order, and the strings 
with non-Chinese character as the first position are indexed to the value of 6763. 
Here, GB2312 is employed as our statistics standard. Other encoding system is con-
verted to the corresponding characters in GB2312, and the characters will be omitted 
if there is no counterpart. The reasons for this strategy are as follows: 1) terminology 
seldom consists of rare words out of GB2312, 2) the index space is dramatically re-
duced using GB2312 rather than the Unicode encoding so as to quicken the estimation 
speed. 
The terminology to be looked up is inputted into search engine, and the relevant 
Web pages with this term?s associated bilingual annotation are collected. Web pages 
are transformed into text through HTML analysis module. The term position is lo-
cated as the center point through keyword search, and then string frequency and dis-
tribution estimation is performed in a window of 100 bytes. In Web pages, terminol-
ogies are often written as different forms because of the effect of noise. For example, 
the term ?Mont Blanc? may be written as ?MONT BLANC?, ?Mont-Blanc?, ?Mont 
??Blanc?, and ?MontBlanc?. For finding different forms of keywords in the Web, the 
fuzzy string matching approach is proposed. This method takes 26 English letters in 
the keyword as effective matching symbols, while ignoring the blank space and other 
symbols. In the matched text, only these English letters are viewed as effective items 
for comparison. Using this method can effectively locate different forms of terms and 
therefore obtain comprehensive translation candidates.  
The process of string frequency estimation is described as follows. In the windows 
with keyword as the center, each character is built as a beginning index, and then the 
string candidates are constructed with the increase of the string in the form of one 
Chinese character unit. Since terminology translation usually consists of unknown 
words or compound words, character is employed as the basic unit of statistics rather 
than word so as to explore these unknown term translations as more as possible. 
String candidates are indexed in the database with hash and binary search method, if 
there exists the same item as the inputted candidate, its frequency is increased by 1, 
otherwise, this candidate is added to the database at this position. After handling one 
Web page, the distribution information is also estimated at the same time. In the pro-
gramming implementation, the table of stop words and some heuristic rules of the 
beginning and end with respect to the keyword position are constructed to accelerate 
the statistics process.  
 Web-Based Terminology Translation Mining 1011 
4.2   Translation Noises and Their Solutions 
All possible forms of terminology translations can be comprehensively mined after 
character-based string frequency estimation. However, there are many irrelevant items 
and redundancy noises formed in the process of mining. These noises are defined as 
the following two categories. 
1) Subset redundancy information. The characteristic of this kind information is 
that this item is a subset of one item, but its frequency is lower than that item. For 
example: ?Mont Blanc???(38) ??(27) ??(11)?, where ????, ???? belong 
to subset redundancy information. They should be removed. 
2) Prefix/suffix redundancy information. The characteristic of this kind information 
is that this item is the prefix or suffix of one item, but its frequency is greater than that 
item. For example: 1. ?Mont Blanc ??(16) ???(9) ???(8)?, 2. ?Credit Rating
??(12) ????(10)?, 3. ?Knowledge Portal ????(33) ??????(30)?. 
In Example 1, the item ???? is suffix redundancy information and should be re-
moved. In Example 2, the item ???? is prefix redundancy information and should 
also be removed. In Example 3, the term ?????? is in accord with the definition 
of suffix redundancy information, but this term is a correct candidate. Thus, the prob-
lem of prefix/suffix redundancy information is so complex that we need an evaluation 
method to decide to retain or drop this candidate.  
 
Fig. 3. The description of the sort-based subset deletion algorithm 
4.2.1   Sort-Based Subset Deletion Method 
Aiming at subset redundancy information, we propose sort-based subset deletion 
method to handle it. Because subset redundancy information is an intermediate of 
estimating terminology translations, its information is basically contained by the 
1. Sort by entropy value 
2. Sort by boundary[*] for the same entropy 
3. Sort by length and lexical sort for the same entropy and boundary 
4. int nNum = 0;    //record the number of remained candidates 
5. for(int i=0; i<m_nDataNum; i++)  { 
6.  int nIsSubString = FALSE; 
7.   if(nNum == 0)    //for the first item to be remained 
8. Judge whether to remain this item using boundary and length proportion 
information; 
9.   else  { 
10.         for(int j=0; j< nNum; j++)  { 
11. Judge if the ith candidate is a subset of the jth, and doesn?t emerge in 
the isolated form, if yes   
12. {    nIsSubString = TRUE;    break;    } 
13.            } 
14.  } 
15.  if(!nIsSubString)  { 
16.         Move the ith candidate information to nNum position, and save; 
17.         The saved number nNum++; 
18.  } 
19. } 
20. m_nDataNum = nNum; //Save the total number. 
[*]Note: refer to the case that the string has the distinct left and right boundary in the Web 
1012 G. Fang, H. Yu, and F. Nishino 
longer string candidate with higher frequency. Therefore, this problem can be well 
solved by first sorting and then judging if this item is a subset of the preceding candi-
dates. The detailed algorithm is described in Fig. 3. 
4.2.2   Mutual Information Based Method 
Prefix/suffix redundancy information is very complicated to deal with. In some cases, 
previous candidate is a correct translation and should be retained, while in other cases, 
it is a noise and should be deleted. In this paper, mutual information based method is 
proposed to decide if the candidate should be retained or deleted. 
The concept of information entropy is first proposed by Shannon in 1948. Entropy 
is a measure of uncertainty of a random variable, and defined as:  
 ??=
=
k
i
ii xpxpXH
1
2 )(log)()( ,                                       (2) 
where )( ixp  is a probability function of a random variable X=xi. 
Mutual information is a concept of information theory, and is a measure of the 
amount of information that one random variable contains about another variable. The 
mutual information of two events X and Y is defined as: 
 ),()()(),( YXHYHXHYXI ?+= ,                                (3) 
where H(X) and H(Y) are respectively the entropies of the random variables of X and 
Y, and H(X,Y) is the co-occurrence entropy of X and Y.  
Mutual information reflects a closeness degree of the combination of X and Y. If 
there is no interesting relationship between X and Y, I(X,Y)=0, that is, X and Y are 
independent each other. If there is a genuine association between X and Y, the co-
occurrence of XY will be bigger than the random individual occurrence chance of X 
or Y, and consequently I>>0. In this case, the possibility as a fixed compound phrase 
of XY becomes very big. Small mutual information hints that the combination of X 
and Y is very loose, and therefore there is a great possibility of a boundary between 
two words X, Y.  
String frequency estimation is performed on different Web pages. In each Web 
page there is more than one occurrence for a candidate translation. Mapping this esti-
mation process to the entropy calculation, we define Nnxp ii /)( = , where ni denotes 
the number of a translation candidate in one Web page, and N represents the total 
number of this candidate. We define k as the number of the estimated Web pages. The 
calculation of entropy is rewritten as:  
  Nnn
NN
n
N
nXH
k
i
ii
k
i
ii
2
1
2
1
2 loglog
1log)( +??=??=
==
.                  (4) 
Through this formula, the candidate entropy can be computed directly rather than 
after counting all Web data. Therefore, it can reduce the time of statistics.  
Entropy can not only reflect the frequency information N, but also the distribution 
information in different Webs. The higher the frequency is, and the larger the entropy 
is. If the distribution is more uniform, this entropy value will become bigger. This is 
also in accord with our intuition.  
 Web-Based Terminology Translation Mining 1013 
Given two candidate patterns of 1t , 2t  in the set of translation candidates, 
)()( 21 tCtC > , where C denotes the frequency of estimation. For suffix redundancy 
information, )( 21 tsufft = ; for prefix redundancy information, )( 21 tpreft = . Accord-
ing to the definition of mutual information, )()()()( 21212 tHttHtHtI ??+= .  
The mutual information based method for prefix/suffix redundancy information is 
described as follows. First, judge if the condition of 95.0)(/)( 11 ?? tCttC
i
i  or 
95.0)(/)( 11 ?? tCttC
i
i  is satisfied, where the candidates itt1  represent the items that 
do not contained each other in the windows of 10 candidates after the candidate 1t . If 
the condition is met, then delete 1t . In an example of ?Dendritic Cell ??(62) ??
???(40) ????(15) ?????(4)?, because (40+15+4)/62=0.952>0.95, the 
candidate ???? is deleted. If prefix/suffix redundancy information don?t satisfy the 
condition above, then judge the condition of )()( 21 tItI <? , if yes, then delete 1t , 
otherwise retain it. The value of ? is determined by the experiments, and the following 
experimental results demonstrate that ?=0.85 is the best parameter. 
5   Experiments 
Our experimental database consists of two sets of 401 English-Chinese term pairs and 
3511 English-Chinese term pairs in the financial domain. There is no intersection 
between two sets. Each terminology often consists of 1-6 English words, and the 
associated translation contains 2-8 Chinese characters. In the test set of 401 terms, 
there are more than one Chinese translation for one English term, and only one Chi-
nese translation for 3511 term pairs. The top n accuracy is defined as the percentage 
of terms whose top n translations include correct translation in the term pairs.  
55
60
65
70
75
80
85
90
95
100
Top1 Top3 Top5 Top10 Top30
The calculated number
A
c c
u
ra
c y
?=0.9
?=0.85
?=0.82
?=0.8
?=0.7
 
Fig. 4.  The relationship between the parameter ? and the accuracy 
For testing in what condition, mutual information based method is the best to deal 
with the prefix/suffix redundancy information. The parameter of ? is respectively set 
1014 G. Fang, H. Yu, and F. Nishino 
to 0.7, 0.8, 0.82, 0.85, and 0.9 in the experiment on the test set of 401 terms. Experi-
mental results are shown in Fig. 4. From the figure, we know that ?=0.85 is the  
best parameter. 
60
65
70
75
80
85
90
95
100
50 100 150 200 250 300
The number of Web pages
A
c c
u r
a c
y
N=1
N=3
N=5
 
Fig. 5.  The relationship between the number of Web pages and the accuracy 
A second experiment is to analyze the number of Web pages influencing the term 
translation accuracy. The experiments are respectively performed on 50, 100, 150, 
200, 250, and 300 Web pages retrieved from the Web. Experimental results are illus-
trated in Fig. 5, where N=1, 3, 5 represent the results of top 1, top 3, and top 5. As 
seen from the figure, the result of using 200 Web pages is best. When the Web pages 
increase more than 200 Web pages, the performance isn?t improved distinctly, while 
the computation cost grows. In the case of 200 Web pages, the Chinese translation 
accuracy of top 1 is 71.8%, and top 3 is 94.5%, and top 5 is 97% on the test set of 401 
English terms (see Table 1). 
Table 1.  Experimental results on a test set of 401 terms 
Candidates Top30 Top10 Top5 Top3 Top1 
Accuracy 99.5% 99% 97% 94.5% 71.8% 
Using the previous trained parameters, we perform term translation mining ex-
periments in the test set of 3511 terms. Experimental results are listed in Table 2. 
From this table, the accuracy of top 3 is 83.6%. Experiments also validate that the 
accuracy of top 30 is nearly equal to the coverage of translations (the percentage of 
term translations found by our system). This is because there is no change on the 
accuracy when increasing the candidate number after top 30.  
Table 2.  Experimental results on a test set of 3511 terms 
Candidates Top30 Top10 Top5 Top3 Top1 
Accuracy 95.4% 93.8% 89.1% 83.6% 56.4% 
 Web-Based Terminology Translation Mining 1015 
6   Conclusions 
In this paper, after reviewing and analyzing all possible methods of acquiring trans-
lations, a statistics-based method is proposed to mine terminology translation from 
the Web. In the proposed method, character-based string frequency estimation is 
first presented to construct term translation candidates, and then sort-based subset 
deletion and mutual information methods are respectively proposed to deal with two 
redundancy information formed in the process of estimation. Experiments on two 
vocabularies of 401 and 3511 English terms show that our system has better per-
formance, about 94.5% and 83.6% in the top 3 Chinese candidates. The contribu-
tions of this paper focus on the following two aspects: 1) On the basis of reviewing 
all possible methods of acquiring translations and analyzing different forms of term 
translation distribution, a statistics-based method is proposed to mine terminology 
translation from the Web. 2) The candidate noises are defined as two categories: 
subset redundancy information and prefix/suffix redundancy information. Sort-
based subset deletion and mutual information methods are respectively proposed to 
deal with two redundancy information.  
References 
1. Somers, H.: Bilingual Parallel Corpora and Language Engineering. Proc. Anglo-Indian 
Workshop "Language Engineering for South-Asian languages", (2001) 
2. V?ronis, J.: Parallel Text Processing - Alignment and Use of Translation Corpora. The 
Netherlands: Kluwer Academic Publishers, (2000) 
3. Grefenstette, G.: The WWW as a Resource for Example-Based MT Tasks. Proc. ASLIB 
Translating and the Computer 21 Conference, (1999) 
4. Cao, Y., Li, H.: Base Noun Phrase Translation Using Web Data and the EM Algorithm. 
Proc. 19th Int?l Conf. Computational Linguistics, (2002) 127-133 
5. Li, H., Cao, Y., Li, C.: Using Bilingual Web Data to Mine and Rank Translations. IEEE 
Intelligent Systems. 4 (2003) 54-59 
6. Navigli, R., Velardi, P., Gangemi, A.: Ontology Learning and Its Application to Auto-
mated Terminology Translation. IEEE Intelligent Systems. 1 (2003) 22-31 
7. Nagata, M., Saito, T., Suzuki, K.: Using the Web as a Bilingual Dictionary.  
Proc. ACL 2001 Workshop Data-Driven Methods in Machine Translation, (2001)  
95?102 
8. Rapp, R.: Identifying Word Translations in Nonparallel Texts. Proc. 33th Annual Meeting 
of the Association for Computational Linguistics, (1995) 320-322 
9. Tanaka, K., Iwasaki, H.: Extraction of Lexical Translation from Non-Aligned Corpora, 
Proc. 16th Int?l Conf. Computational Linguistics, (1996) 580-585 
10. Rapp, R.: Automatic Identification of Word Translations from Unrelated English and 
German Corpora. Proc. 37th Annual Meeting Assoc. Computational Linguistics, (1999) 
519-526 
11. Fung, P.: Compiling Bilingual Lexicon Entries from a Non-Parallel English-Chinese Cor-
pus. Proc. Third Annual Workshop on Very Large Corpora, (1995) 173-183 
 
1016 G. Fang, H. Yu, and F. Nishino 
12. Fung, P.: Finding Terminology Translations from Nonparallel Corpora. Proc. Fifth Annual 
Workshop on Very Large Corpora (WVLC'97), (1997) 192-202 
13. Fung P., Yee, L.P.: An IR Approach for Translation New Words from Nonparallel, Com-
parable Texts. Proc. 17th Int?l Conf. Computational Linguistics and 36th Annual Meeting 
of the Association for Computational Linguistics, (1998) 414-420 
14. Shahzad, I., Ohtake, K., Masuyama, S., Yamamoto, K.: Identifying Translations of Com-
pound Nouns Using Non-Aligned Corpora. Proc. Workshop on Multilingual Information 
Processing and Asian Language Processing, (1999) 108-113 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 199?206,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese-English Term Translation Mining Based on 
Semantic Prediction  
 
 
Gaolin Fang, Hao Yu, and Fumihito Nishino 
Fujitsu Research and Development Center, Co., LTD. Beijing 100016, China  
{glfang, yu, nishino}@cn.fujitsu.com
 
  
 
Abstract 
Using abundant Web resources to mine 
Chinese term translations can be applied 
in many fields such as reading/writing as-
sistant, machine translation and cross-
language information retrieval. In mining 
English translations of Chinese terms, 
how to obtain effective Web pages and 
evaluate translation candidates are two 
challenging issues. In this paper, the ap-
proach based on semantic prediction is 
first proposed to obtain effective Web 
pages. The proposed method predicts 
possible English meanings according to 
each constituent unit of Chinese term, and 
expands these English items using 
semantically relevant knowledge for 
searching. The refined related terms are 
extracted from top retrieved documents 
through feedback learning to construct a 
new query expansion for acquiring more 
effective Web pages. For obtaining a cor-
rect translation list, a translation 
evaluation method in the weighted sum of 
multi-features is presented to rank these 
candidates estimated from effective Web 
pages. Experimental results demonstrate 
that the proposed method has good per-
formance in Chinese-English term trans-
lation acquisition, and achieves 82.9% 
accuracy. 
1 Introduction 
The goal of Web-based Chinese-English (C-E) 
term translation mining is to acquire translations 
of terms or proper nouns which cannot be looked 
up in the dictionary from the Web using a statis-
tical method, and then construct an application 
system for reading/writing assistant (e.g., ???
?The Romance of Three Kingdoms). During 
translating or writing foreign language articles, 
people usually encounter terms, but they cannot 
obtain native translations after many lookup ef-
forts. Some skilled users perhaps resort to a Web 
search engine, but a large amount of retrieved 
irrelevant pages and redundant information ham-
per them to acquire effective information. Thus, 
it is necessary to provide a system to automati-
cally mine translation knowledge of terms using 
abundant Web information so as to help users 
accurately read or write foreign language articles.  
The system of Web-based term translation 
mining has many applications. 1) Read-
ing/writing assistant. 2) The construction tool of 
bilingual or multilingual dictionary for machine 
translation. The system can not only provide 
translation candidates for compiling a lexicon, 
but also rescore the candidate list of the diction-
ary. We can also use English as a medium lan-
guage to build a lexicon translation bridge 
between two languages with few bilingual anno-
tations (e.g., Japanese and Chinese). 3) Provide 
the translations of unknown queries in cross-
language information retrieval (CLIR). 4) As one 
of the typical application paradigms of the com-
bination of CLIR and Web mining. 
Automatic acquisition of bilingual translations 
has been extensively researched in the literature. 
The methods of acquiring translations are usually 
summarized as the following six categories. 1) 
Acquiring translations from parallel corpora. To 
reduce the workload of manual annotations, re-
searchers have proposed different methods to 
automatically collect parallel corpora of different 
language versions from the Web (Kilgarriff, 
2003). 2) Acquiring translations from non-
parallel corpora (Fung, 1997; Rapp, 1999). It is 
based on the clue that the context of source term 
is very similar to that of target translation in a 
large amount of corpora. 3) Acquiring transla-
tions from a combination of translations of con-
stituent words (Li et al, 2003). 4) Acquiring 
translations using cognate matching (Gey, 2004) 
199
or transliteration (Seo et al, 2004). This method 
is very suitable for the translation between two 
languages with some intrinsic relationships, e.g., 
acquiring translations from Japanese to Chinese 
or from Korean to English. 5) Acquiring transla-
tions using anchor text information (Lu et al, 
2004). 6) Acquiring translations from the Web. 
When people use Asia language (Chinese, Japa-
nese, and Korean) to write, they often annotate 
associated English meanings after terms. With 
the development of Web and the open of accessi-
ble electronic documents, digital library, and sci-
entific articles, these resources will become more 
and more abundant. Thus, acquiring term transla-
tions from the Web is a feasible and effective 
way. Nagata et al (2001) proposed an empirical 
function of the byte distance between Japanese 
and English terms as an evaluation criterion to 
extract translations of Japanese words, and the 
results could be used as a Japanese-English dic-
tionary.   
Cheng et al (2004) utilized the Web as the 
corpus source to translate English unknown que-
ries for CLIR. They proposed context-vector and 
chi-square methods to determine Chinese transla-
tions for unknown query terms via mining of top 
100 search-result pages from Web search engines.  
Zhang and Vines (2004) proposed using a Web 
search engine to obtain translations of Chinese 
out-of-vocabulary terms from the Web to im-
prove CLIR performance. The method used Chi-
nese as query items, and retrieved previous 100 
document snippets by Google, and then estimated 
possible translations using co-occurrence infor-
mation.  
From the review above, we know that previous 
related researches didn?t concern the issue how to 
obtain effective Web pages with bilingual 
annotations, and they mainly utilized the 
frequency feature as the clue to mine the 
translation. In fact, previous 100 Web results 
seldom contain effective English equivalents. 
Apart from the frequency information, there are 
some other features such as distribution, length 
ratio, distance, keywords, key symbols and 
boundary information which have very important 
impacts on term translation mining. In this paper, 
the approach based on semantic prediction is 
proposed to obtain effective Web pages; for 
acquiring a correct translation list, the evaluation 
strategy in the weighted sum of multi-features is 
employed to rank the candidates.  
The remainder of this paper is organized as 
follows. In Section 2, we give an overview of the 
system. Section 3 proposes effective Web page 
collection. In Section 4, we introduce translation 
candidate construction and noise solution. Sec-
tion 5 presents candidate evaluation based on 
multi-features. Section 6 shows experimental 
results. The conclusion is drawn in the last sec-
tion. 
2 System Overview 
The C-E term translation mining system based on 
semantic prediction is illustrated in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. The Chinese-English term translation min-
ing system based on semantic prediction 
 
The system consists of two parts: Web page 
handling and term translation mining. Web page 
handling includes effective Web page collection 
and HTML analysis. The function of effective 
Web page collection is to collect these Web 
pages with bilingual annotations using semantic 
prediction, and then these pages are inputted into 
HTML analysis module, where possible features 
and text information are extracted. Term transla-
tion mining includes candidate unit construction, 
candidate noise solution, and rank&sort candi-
dates. Translation candidates are formed through 
candidate unit construction module, and then we 
analyze their noises and propose the correspond-
ing methods to handle them. At last, the approach 
using multi-features is employed to rank these 
candidates. 
Correctly exploring all kinds of bilingual anno-
tation forms on the Web can make a mining sys-
tem extract comprehensive translation results. 
After analyzing a large amount of Web page ex-
amples, translation distribution forms is summa-
rized as six categories in Figure 2: 1) Direct 
annotation (a). some have nothing (a1), and some 
have symbol marks (a2, a3) between the pair; 2) 
Separate annotation. There are English letters (b1) 
or some Chinese words (b2, b3) between the pair; 
3) Subset form (c); 4) Table form (d); 5) List 
form (e); and 6) Explanation form (f).  
 
 
Query  
????? 
WWW
Features
1. Frequency 
2. Distribution 
3. Distance 
4. Length ratio
5. Key symbols
and boundary 
Rank & sort 
candidates
Candidate unit 
construction Result 
?Mont Blanc?
Effective 
 Web page 
collection 
HTML 
analysis 
Candidate noise 
solution 
200
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The examples of translation distribution 
forms 
3 Effective Web page collection 
For mining the English translations of Chinese 
terms and proper names, we must obtain effective 
Web pages, that is, collecting these Web pages 
that contain not only Chinese characters but also 
the corresponding English equivalents. However, 
in a general Web search engine, when you input a 
Chinese technical term, the number of retrieved 
relevant Web pages is very large. It is infeasible 
to download all the Web pages because of a huge 
time-consuming process. If only the 100 abstracts 
of Web pages are used for the translation estima-
tion just as in the previous work, effective Eng-
lish equivalent words are seldom contained for 
most Chinese terms in our experiments, for ex-
ample: ?????, ????, ?????, ??
??. In this paper, a feasible method based on 
semantic prediction is proposed to automatically 
acquire effective Web pages. In the proposed 
method, possible English meanings of every con-
stituent unit of a Chinese term are predicted and 
further expanded by using semantically relevant 
knowledge, and these expansion units with the 
original query are inputted to search bilingual 
Web pages. In the retrieved top-20 Web pages, 
feedback learning is employed to extract more 
semantically-relevant terms by frequency and 
average length. The refined expansion terms, to-
gether with the original query, are once more sent 
to retrieve effective relevant Web pages. 
3.1 Term expansion  
Term expansion is to use predictive semantically-
relevant terms of target language as the expan-
sion of queries, and therefore resolve the issue 
that top retrieved Web pages seldom contain ef-
fective English annotations. Our idea is based on 
the assumption that the meanings of Chinese 
technical terms aren?t exactly known just through 
their constituent characters and words, but the 
closely related semantics and vocabulary infor-
mation may be inferred and predicted. For exam-
ple, the corresponding unit translations of a term 
?????? are respectively: three(?), country, 
nation(?), act, practice(?), and meaning, jus-
tice(?). As seen from these English translations, 
we have a general impression of ?things about 
three countries?. After expanding, the query item 
for the example above becomes "????"+ 
(three | country | nation | act | practice | meaning | 
justice). The whole procedure consists of three 
steps: unit segmentation, item translation knowl-
edge base construction, and expansion knowl-
edge base evaluation. 
Unit segmentation. Getting the constituent 
units of a technical term is a segmentation proce-
dure. Because most Chinese terms consist of out-
of-vocabulary words or meaningless characters, 
the performance using general word segmenta-
tion programs is not very desirable. In this paper, 
a segmentation method is employed to handle 
term segmentation so that possible meaningful 
constituent units are found. In the inner structure 
of proper nouns or terms, the rightmost unit usu-
ally contains a headword to reflect the major 
meaning of the term. Sometimes, the modifier 
starts from the leftmost point of a term to form a 
multi-character unit. As a result, forward maxi-
mum matching and backward maximum match-
ing are respectively conducted on the term, and 
all the overlapped segmented units are added to 
candidate items. For example, for the term 
?abcd?, forward segmented units are ?ab cd?, 
backward are ?a bcd?, so ?ab cd a bcd? will be 
viewed as our segmented items. 
Item translation knowledge base construc-
tion. Because the segmented units of a technical 
term or proper name often consist of abbreviation 
items with shorter length, limited translations 
provided by general dictionaries often cannot 
satisfy the demand of translation prediction. Here, 
a semantic expansion based method is proposed 
to construct item translation knowledge base. In 
this method, we only keep these nouns or adjec-
tive items consisting of 1-3 characters in the dic-
tionary. If an item length is greater than two 
characters and contains any item in the knowl-
edge base, its translation will be added as transla-
tion candidates of this item. For example, the 
Chinese term ????? can be segmented into 
the units ???? and ???, where ??? has only 
two English meanings ?section, thigh? in the dic-
tionary. However, we can derive its meaning us-
(a1) (a2) (a3)
(b1) (b2) (b3)
(c) (d) (e) (f)
201
ing the longer word including this item such as 
???, ???. Thus, their respective translations 
?stock, stockholder? are added into the knowl-
edge base list of ??? (see Figure 3).  
 
 
 
 
 
 
Figure 3. An expansion example in the dictionary 
knowledge base 
Expansion knowledge base evaluation. To 
avoid over-expanding of translations for one item, 
using the retrieved number from the Web as our 
scoring criterion is employed to remove irrele-
vant expansion items and rank those possible 
candidates. For example, ??? and its expansion 
translation ?stock? are combined as a new query 
?? stock ????. It is sent to a general search 
engine like Google to obtain the count number, 
where only the co-occurrence of ? ? ? and 
?stock? excluding the word ???? is counted. 
The retrieved number is about 316000. If the oc-
currence number of an item is lower than a cer-
tain threshold (100), the evaluated translation 
will not be added to the item in the knowledge 
base. Those expanded candidates for the item in 
the dictionary are sorted through their retrieved 
number. 
3.2 Feedback learning 
Though pseudo-relevance feedback (PRF) has 
been successfully used in the information re-
trieval (IR), whether PRF in single-language IR 
or pre-translation PRF and post-translation PRF 
in CLIR, the feedback results are from source 
language to source language or target language to 
target language, that is, the language of feedback 
units is same as the retrieval language. Our novel 
is that the input language (Chinese) is different 
from the feedback target language (English), that 
is, realizing the feedback from source language to 
target language, and this feedback technique is 
also first applied to the term mining field. 
After the expansion of semantic prediction, the 
predicted meaning of an item has some devia-
tions with its actual sense, so the retrieved docu-
ments are perhaps not our expected results. In 
this paper, a PRF technique is employed to ac-
quire more accurate, semantically relevant terms. 
At first, we collect top-20 documents from search 
results after term expansion, and then select 
target language units from these documents, 
get language units from these documents, which 
are highly related with the original query in 
source language. However, how to effectively 
select these units is a challenging issue. In the 
literature, researchers have proposed different 
methods such as Rocchio?s method or Robert-
son?s probabilistic method to solve this problem. 
After some experimental comparisons, a simple 
evaluation method using term frequency and av-
erage length is presented in this paper. The 
evaluation method is defined as follows: 
1)(
1)()(
+?
+= ttftw , where N
tsDt
N
i i?
=? =1 ),()(  (1) 
?(t) represents the average length between the 
source word s and the target candidate t. If the 
greater that the average length is, the relevance 
degree between source terms and candidates will 
become lower. The purpose of adding ?(t) to 1 
is to avoid the divide overflow in the case that the 
average length is equal to zero. Di(s,t) denotes the 
byte distance between source words and target 
candidates, and N represents the total number of 
candidate occurrences in the estimated Web 
pages. This evaluation method is very suitable for 
the discrimination of these words with lower, but 
same term frequencies. In the ranked candidates 
after PRF feedback, top-5 candidates are selected 
as our refined expansion items. In the previous 
example, the refined expansion items are: King-
doms, Three, Romance, Chinese, Traditional. 
These refined expansion terms, together with the 
original query, "????"+(Kingdoms | Three | 
Romance | Chinese | Traditional) are once more 
sent to retrieve relevant results, which are viewed 
as effective Web pages used in the process of the 
following estimation. 
4 Translation candidate construction and 
noise solution 
The goal of translation candidate construction is 
to construct and mine all kinds of possible trans-
lation forms of terms from the Web, and effec-
tively estimate their feature information such as 
frequency and distribution. In the transferred text, 
we locate the position of a query keyword, and 
then obtain a 100-byte window with keyword as 
the center. In this window, each English word is 
built as a beginning index, and then string candi-
dates are constructed with the increase of string 
in the form of one English word unit. String can-
didates are indexed in the database with hash and 
binary search method. If there exists the same 
item as the inputted candidate, its frequency is 
increased by 1, otherwise, this candidate is added 
?
?? ?? 
202
to this position of the database. After handling 
one Web page, the distribution information is 
also estimated at the same time. In the program-
ming implementation, the table of stop words and 
some heuristic rules of the beginning and end 
with respect to the keyword position are em-
ployed to accelerate the statistics process. 
The aim of noise solution is to remove these ir-
relevant items and redundant information formed 
in the process of mining. These noises are de-
fined as the following two categories.  
1) Subset redundancy. The characteristic is 
that this item is a subset of one item, but its fre-
quency is lower than that item. For example, ??
???License plate number (6), License plate 
(5)?, where the candidate ?License plate? belongs 
to subset redundancy. They should be removed.   
2) Affix redundancy. The characteristic is that 
this item is the prefix or suffix of one item, but its 
frequency is greater than that item. For example, 
1. ?????: Three Kingdoms (30), Romance 
of the Three Kingdoms (22), The Romance of 
Three Kingdoms (7)?, 2. ???? : Blue Chip 
(35), Blue Chip Economic Indicators (10)?. In 
Example 1, the item ?Three Kingdoms? is suffix 
redundancy and should be removed. In Example 
2, the term ?Blue Chip? is in accord with the 
definition of prefix redundancy information, but 
this term is a correct translation candidate. Thus, 
the problem of affix redundancy information is 
so complex that we need an evaluation method to 
decide to retain or drop the candidate.  
To deal with subset redundancy and affix 
redundancy information, sort-based subset 
deletion and mutual information methods are 
respectively proposed. More details refer to our 
previous paper (Fang et al, 2005). 
5 Candidate evaluation based on multi-
features 
5.1 Possible features for translation pairs 
Through analyzing mass Web pages, we obtain 
the following possible features that have impor-
tant influences on term translation mining. They 
include: 1) candidate frequency and its distribu-
tion in different Web pages, 2) length ratio be-
tween source terms and target candidates (S-T), 3) 
distance between S-T, and 4) keywords, key 
symbols and boundary information between S-T. 
1) Candidate frequency and its distribution  
Translation candidate frequency is the most 
important feature and is the basis of decision-
making. Only the terms whose frequencies are 
greater than a certain threshold are further con-
sidered as candidates in our system. Distribution 
feature reflects the occurrence information of one 
candidate in different Webs. If the distribution is 
very uniform, this candidate will more possibly 
become as the translation equivalent with a 
greater weight. This is also in accord with our 
intuition. For example, the translation candidates 
of the term ?????? include ?put option? and 
?short put?, and their frequencies are both 5. 
However, their distributions are ?1, 1, 1, 1, 1? 
and ?2, 2, 1?. The distribution of ?put option? is 
more uniform, so it will become as a translation 
candidate of ?????? with a greater weight.  
2) Length ratio between S-T 
The length ratio between S-T should satisfy 
certain constraints. Only the word number of a 
candidate falls within a certain range, the possi-
bility of becoming a translation is great.  
To estimate the length ratio relation between 
S-T, we conduct the statistics on the database 
with 5800 term translation pairs. For example, 
when Chinese term has three characters, i.e. W=3, 
the probability of English translations with two 
words is largest, about P(E=2 |W =3)= 78%, and 
there is nearly no occurrence out of the range of 
1-4. Thus, different weights can be impacted on 
different candidates by using statistical distribu-
tion information of length ratio. The weight con-
tributing to the evaluation function is set 
according to these estimated probabilities in the 
experiments. 
3) Distance between S-T 
Intuitively, if the distance between S-T is 
longer, the probability of being a translation pair 
will become smaller. Using this knowledge we 
can alleviate the effect of some noises through 
impacting different weights when we collect pos-
sible correct candidates far from the source term.  
To estimate the distance between S-T, experi-
ments are carried on 5800*200 pages with 5800 
term pairs, and statistical results are depicted as 
the histogram of distances in Figure 4. 
0
2000
4000
6000
8000
10000
12000
14000
-100 -75 -50 -25 0 25 50 75 100
 
Figure 4. The histogram of distances between S-T 
203
 
In the figure, negative value represents that 
English translation located in front of the Chinese 
term, and positive value represents English trans-
lation is behind the Chinese term. As shown from 
the figure, we know that most candidates are dis-
tributed in the range of -60-60 bytes, and few 
occurrences are out of this range. The numbers of 
translations appearing in front of the term and 
after the term are nearly equal. The curve looks 
like Gaussian probability distribution, so Gaus-
sian models are proposed to model it. By the 
curve fitting, the parameters of Gaussian models 
are obtained, i.e. u=1 and sigma=2. Thus, the 
contribution probability of distance to the ranking 
function is formulized as 
8/)1),(( 2
22
1),( ??= jiDD ejip
?
, where D(i,j) repre-
sents the byte distance between the source term i 
and the candidate j.  
4) Keywords, key symbols and boundary in-
formation between S-T 
Some Chinese keywords or capital English ab-
breviation letters between S-T can provide an 
important clue for the acquisition of possible cor-
rect translations. These Chinese keywords in-
clude the words such as ????, ????, 
????, ?????, ????, ???, 
???, ???, ????, ????, ?
????. The punctuations between S-T can also 
provide very strong constraints, for example, 
when the marks ?? ?( ) [ ]? exist, the probabil-
ity of being a translation pair will greatly increase. 
Thus, correctly judging these cases can not only 
make translation finding results more compre-
hensive, but also increase the possibility that this 
candidate is as one of correct translations. 
Boundary information refers to the fact that the 
context of candidates on the Web has distinct 
mark information, for example, the position of 
transition from continuous Chinese to English, 
the place with bracket ellipsis and independent 
units in the HTML text. 
5.2 Candidate evaluation method 
After translation noise handling, we evaluate 
candidate translations so that possible candidates 
get higher scores. The method in the weighted 
sum of multi-features including: candidate fre-
quency, distribution, length ratio, distance, key-
words, key symbols and boundary information 
between S-T, is proposed to rank the candidates. 
The evaluation method is formulized as follows:  
? ?
++=
=
N
i
j DL wjijiptsptScore
1
1 )),(),(([),()( ??  
)]),(),((max2 wjijipD
j
?? + , 121 =+ ??    (2) 
In the equation, Score(t) is proportional to 
),( tspL , N and ),( jipD . If the bigger these com-
ponent values are, the more they contribute to the 
whole evaluation formula, and correspondingly 
the candidate has higher score. The length ratio 
relation ),( tspL  reflects the proportion relation 
between S-T as a whole, so its weight will be 
impacted on the Score(t) in the macro-view. The 
weights are trained through a large amount of 
technical terms and proper nouns, where each 
relation corresponds to one probability. N de-
notes the total number of Web pages that contain 
candidates, and partly reflects the distribution 
information of candidates in different Web pages. 
If the greater N is, the greater Score(t) will be-
come. The distance relation ),( jipD  is defined as 
the distance contribution probability of the jth 
source-candidate pair on the ith Web pages, 
which is impacted on every word pair emerged 
on the Web in the point of micro-view. Its calcu-
lation formula is defined in Section 5.1. The 
weights of 1?  and 2?  represent the proportion of 
term frequency and term distribution, and 1?  de-
notes the weight of the total number of one can-
didate occurrences, and 2?  represents the weight 
of counting the nearest distance occurrence for 
each Web page. wji ),(?  is the contribution prob-
ability of keywords, key symbols and boundary 
information. If there are predefined keywords, 
key symbols, and boundary information between 
S-T, i.e., 1),( =ji? , then the evaluation formula 
will give a reward w, otherwise, 0),( =ji?  indi-
cate that there is no impact on the whole equation. 
6 Experiments 
Our experimental data consist of two sets: 400 C-
E term pairs and 3511 C-E term pairs in the fi-
nancial domain. There is no intersection between 
the two sets. Each term often consists of 2-8 Chi-
nese characters, and the associated translation 
contains 2-5 English words. In the test set of 400 
terms, there are more than one English translation 
for every Chinese term, and only one English 
translation for 3511 term pairs. In the test sets, 
Chinese terms are inputted to our system on 
batch, and their corresponding translations are 
viewed as a criterion to evaluate these mined 
candidates. The top n accuracy is defined as the 
204
percentage of terms whose top n translations in-
clude correct translations in the term pairs. A se-
ries of experiments are conducted on the two test 
sets.  
Experiments on the number of feedback 
pages: To obtain the best parameter of feedback 
Web pages that influence the whole system accu-
racy, we perform the experiments on the test set 
of 400 terms. The number of feedback Web 
pages is respectively set to 0, 10, 20, 30, and 40. 
N=1, 3, 5 represent the accuracies of top 1, 3, and 
5. From the feedback pages, previous 5 semanti-
cally-relevant terms are extracted to construct a 
new query expansion for retrieving more effec-
tive Web pages. Translation candidates are mined 
from these effective pages, whose accuracy 
curves are depicted in Figure 5. 
60
65
70
75
80
85
90
95
100
0 10 20 30 40
The number of feedback Web pages
Ac
cu
rac
y
N=1
N=3
N=5
 
Figure 5.  The number of feedback Web pages 
 
As seen from the figure above, when the num-
ber of feedback Web pages is 20, the accuracy 
reaches the best. Thus, the feedback parameter in 
our experiments is set to 20. 
Experiments on the parameter 1? : In the 
candidate evaluation method using multi-features, 
the parameter of 1?  need be chosen through the 
experiments. To obtain the best parameter, the 
experiments are set as follows. The accuracy of 
top 5 candidates is viewed as a performance cri-
terion. The parameters are respectively set from 0 
to 1 with the increase of 0.1 step. The results are 
listed in Figure 6. As seen from the figure, 
1? =0.4 is best parameter, and therefore 2? =0.6. 
In the following experiments, the parameters are 
all set to this value. 
80
85
90
95
100
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Parameter
Ac
cu
rac
y
Figure 6.  The relation between the parameter 1?  and 
the accuracy 
Experiments on the test set of 400 terms us-
ing different methods: The methods respec-
tively without prediction(NP), with prediction(P), 
with prediction and feedback(PF) only using term 
frequency (TM), and with prediction and feed-
back using multi-features(PF+MF) are employed 
on the test set of 400 terms. The results are listed 
in Table 1. As seen from this table, if there is no 
semantic prediction, the obtained translations 
from Web pages are about 48% in the top 30 
candidates. This is because general search en-
gines will retrieve more relevant Chinese Web 
pages rather than those effective pages including 
English meanings. Thus, the semantic prediction 
method is employed. Experiments demonstrate 
the method with semantic prediction distinctly 
improves the accuracy, about 36.8%. To further 
improve the performance, the feedback learning 
technique is proposed, and it increases the aver-
age accuracy of 6.5%. Though TM is very effec-
tive in mining the term translation, the multi-
feature method fully utilizes the context of can-
didates, and therefore obtains more accurate re-
sults, about 92.8% in the top 5 candidates. 
 
Table 1. The term translation results using different 
methods 
 Top30 Top10 Top5 Top3 Top1
NP 48.0 47.5 46.0 44.0 28.0
P 84.8 83.3 82.3 79.3 60.8
PF+TM 91.3 90.8 90.3 88.3 71.0
PF+MF 95.0 94.5 92.8 91.5 78.8
 
Experiments on a large vocabulary: To vali-
date our system performance, experiments are 
carried on a large vocabulary of 3511 terms using 
different methods. One method is to use term 
frequency (TM) as an evaluation criterion, and 
the other method is to use multi-features (MF) as 
an evaluation criterion. Experimental results are 
shown as follows. 
 
Table 2. The term translation results on a large vo-
cabulary 
 Top30 Top10 Top5 Top3 Top1
TM 82.5 81.2 78.3 73.5 49.4
MF 89.1 88.4 86.0 82.9 58.2
 
From Table 2, we know the accuracy with top 
5 candidates is about 86.0%. The method using 
multi-features is better than that of using term 
frequency, and improves an average accuracy of 
7.94% 
Some examples of acquiring English transla-
tions of Chinese terms are provided in Table 3. 
1?
205
Only top 3 English translations are listed for each 
Chinese term.  
 
Table 3.  Some C-E mining examples  
Chinese 
terms 
The list of English translations  
(Top 3) 
???? 
The Three Kingdoms 
The Romance of the Three Kingdoms
The Romance of Three Kingdoms 
???? 
Merit student 
"Three Goods" student 
Excellent League member 
??? 
Blue Chip 
Blue Chips 
Blue chip stocks 
??? 
Mont Blanc 
Mont-Blanc 
Chamonix Mont-Blanc 
????? 
Burmuda Triangle 
Bermuda Triangle 
The Bermuda Triangle 
??? 
License plate number 
Vehicle plate number 
Vehicle identification no 
 
7 Conclusions  
In this paper, the method based on semantic 
prediction is first proposed to acquire effective 
Web pages. The proposed method predicts 
possible meanings according to each constituent 
unit of Chinese term, and expands these items for 
searching using semantically relevant knowledge, 
and then the refined related terms are extracted 
from top retrieved documents through feedback 
learning to construct a new query expansion for 
acquiring more effective Web pages. For obtain-
ing a correct translation list, the translation 
evaluation method using multi-features is pre-
sented to rank these candidates. Experimental 
results show that this method has good perform-
ance in Chinese-English translation acquisition, 
about 82.9% accuracy in the top 3 candidates. 
References  
P.J. Cheng, J.W. Teng, R.C. Chen, et al 2004. Trans-
lating unknown queries with web corpora for 
cross-language information retrieval, Proc. ACM 
SIGIR, pp. 146-153. 
G.L. Fang, H. Yu, and F. Nishino. 2005. Web-Based 
Terminology Translation Mining, Proc. IJCNLP, 
pp. 1004-1016. 
P. Fung. 1997. Finding terminology translations from 
nonparallel corpora, Proc. Fifth Annual Work-
shop on Very Large Corpora (WVLC'97), pp. 
192-202. 
F.C. Gey. 2004. Chinese and Korean topic search of 
Japanese news collections, In Working Notes of 
the Fourth NTCIR Workshop Meeting, Cross-
Lingual Information Retrieval Task, pp. 214-218. 
A. Kilgarriff and G. Grefenstette. 2003. Introduction 
to the special issue on the Web as corpus, Com-
putational Linguistics, 29(3): 333-348.  
H. Li, Y. Cao, and C. Li. 2003.Using bilingual web 
data to mine and rank translations, IEEE Intelli-
gent Systems, 18(4): 54-59. 
W.H. Lu, L.F. Chien, and H.J. Lee. 2004. Anchor text 
mining for translation of Web queries: A transi-
tive translation approach, ACM Trans. Informa-
tion System, 22(2): 242-269. 
M. Nagata, T. Saito, and K. Suzuki. 2001. Using the 
web as a bilingual dictionary, Proc. ACL 2001 
Workshop Data-Driven Methods in Machine 
Translation, pp. 95-102. 
R. Rapp. 1999. Automatic identification of word 
translations from unrelated English and German 
corpora, Proc. 37th Annual Meeting Assoc. Com-
putational Linguistics, pp. 519-526. 
H.C. Seo, S.B. Kim, H.G. Lim and H.C. Rim. 2004. 
KUNLP system for NTCIR-4 Korean-English 
cross language information retrieval, In Working 
Notes of the Fourth NTCIR Workshop Meeting, 
Cross-Lingual Information Retrieval Task, pp. 
103-109. 
Y. Zhang and P. Vines. 2004. Using the web for 
automated translation extraction in cross-
language information retrieval, Proc. ACM 
SIGIR, pp. 162-169. 
206
