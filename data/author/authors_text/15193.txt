Learning to Shift the Polarity of Words for Sentiment Classification
Daisuke Ikeda? Hiroya Takamura? Lev-Arie Ratinov?? Manabu Okumura?
?Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology
ikeda@lr.pi.titech.ac.jp
??Department of Computer Science, University of Illinois at Urbana-Champaign
ratinov2@uiuc.edu
?Precision and Intelligence Laboratory, Tokyo Institute of Technology
{takamura,oku}@pi.titech.ac.jp
Abstract
We propose a machine learning based
method of sentiment classification of sen-
tences using word-level polarity. The polari-
ties of words in a sentence are not always the
same as that of the sentence, because there
can be polarity-shifters such as negation ex-
pressions. The proposed method models
the polarity-shifters. Our model can be
trained in two different ways: word-wise and
sentence-wise learning. In sentence-wise
learning, the model can be trained so that the
prediction of sentence polarities should be
accurate. The model can also be combined
with features used in previous work such
as bag-of-words and n-grams. We empiri-
cally show that our method almost always
improves the performance of sentiment clas-
sification of sentences especially when we
have only small amount of training data.
1 Introduction
Due to the recent popularity of the internet, individ-
uals have been able to provide various information
to the public easily and actively (e.g., by weblogs
or online bulletin boards). The information often in-
cludes opinions or sentiments on a variety of things
such as new products. A huge amount of work has
been devoted to analysis of the information, which
is called sentiment analysis. The sentiment analysis
has been done at different levels including words,
sentences, and documents. Among them, we focus
on the sentiment classification of sentences, the task
to classify sentences into ?positive? or ?negative?,
because this task is fundamental and has a wide ap-
plicability in sentiment analysis. For example, we
can retrieve individuals? opinions that are related to
a product and can find whether they have the positive
attitude to the product.
There has been much work on the identification of
sentiment polarity of words. For instance, ?beauti-
ful? is positively oriented, while ?dirty? is negatively
oriented. We use the term sentiment words to refer
to those words that are listed in a predefined polar-
ity dictionary. Sentiment words are a basic resource
for sentiment analysis and thus believed to have a
great potential for applications. However, it is still
an open problem how we can effectively use sen-
timent words to improve performance of sentiment
classification of sentences or documents.
The simplest way for that purpose would be the
majority voting by the number of positive words and
the number of negative words in the given sentence.
However, the polarities of words in a sentence are
not always the same as that of the sentence, be-
cause there can be polarity-shifters such as nega-
tion expressions. This inconsistency of word-level
polarity and sentence-level polarity often causes er-
rors in classification by the simple majority voting
method. A manual list of polarity-shifters, which
are the words that can shift the sentiment polarity of
another word (e.g., negations), has been suggested.
However, it has limitations due to the diversity of
expressions.
Therefore, we propose a machine learning based
method that models the polarity-shifters. The model
can be trained in two different ways: word-wise
296
and sentence-wise. While the word-wise learn-
ing focuses on the prediction of polarity shifts, the
sentence-wise learning focuses more on the predic-
tion of sentence polarities. The model can also be
combined with features used in previous work such
as bag-of-words, n-grams and dependency trees. We
empirically show that our method almost always im-
proves the performance of sentiment classification
of sentences especially when we have only small
amount of training data.
The rest of the paper is organized as follows. In
Section 2, we briefly present the related work. In
Section 3, we discuss well-known methods that use
word-level polarities and describe our motivation. In
Section 4, we describe our proposed model, how to
train the model, and how to classify sentences using
the model. We present our experiments and results
in Section 5. Finally in Section 6, we conclude our
work and mention possible future work.
2 Related Work
Supervised machine learning methods including
Support Vector Machines (SVM) are often used in
sentiment analysis and shown to be very promising
(Pang et al, 2002; Matsumoto et al, 2005; Kudo and
Matsumoto, 2004; Mullen and Collier, 2004; Ga-
mon, 2004). One of the advantages of these meth-
ods is that a wide variety of features such as depen-
dency trees and sequences of words can easily be in-
corporated (Matsumoto et al, 2005; Kudo and Mat-
sumoto, 2004; Pang et al, 2002). Our attempt in this
paper is not to use the information included in those
substructures of sentences, but to use the word-level
polarities, which is a resource usually at hand. Thus
our work is an instantiation of the idea to use a re-
source on one linguistic layer (e.g., word level) to
the analysis of another layer (sentence level).
There have been some pieces of work which fo-
cus on multiple levels in text. Mao and Lebanon
(2006) proposed a method that captures local senti-
ment flow in documents using isotonic conditional
random fields. Pang and Lee (2004) proposed to
eliminate objective sentences before the sentiment
classification of documents. McDonald et al (2007)
proposed a model for classifying sentences and doc-
uments simultaneously. They experimented with
joint classification of subjectivity for sentence-level,
and sentiment for document-level, and reported that
their model obtained higher accuracy than the stan-
dard document classification model.
Although these pieces of work aim to predict not
sentence-level but document-level sentiments, their
concepts are similar to ours. However, all the above
methods require annotated corpora for all levels,
such as both subjectivity for sentences and senti-
ments for documents, which are fairly expensive to
obtain. Although we also focus on two different lay-
ers, our method does not require such expensive la-
beled data. What we require is just sentence-level
labeled training data and a polarity dictionary of sen-
timent words.
3 Simple Voting by Sentiment Words
One of the simplest ways to classify sentences us-
ing word-level polarities would be a majority voting,
where the occurrences of positive words and those
of negative words in the given sentence are counted
and compared with each other. However, this major-
ity voting method has several weaknesses. First, the
majority voting cannot take into account at all the
phenomenon that the word-level polarity is not al-
ways the same as the polarity of the sentence. Con-
sider the following example:
I have not had any distortion problems
with this phone and am more pleased with
this phone than any I?ve used before.
where negative words are underlined and positive
words are double-underlined. The example sentence
has the positive polarity, though it locally contains
negative words. The majority voting would misclas-
sify it because of the two negative words.
This kind of inconsistency between sentence-level
polarity and word-level polarity often occurs and
causes errors in the majority voting. The reason
is that the majority voting cannot take into ac-
count negation expressions or adversative conjunc-
tions, e.g., ?I have not had any ...? in the example
above. Therefore, taking such polarity-shifting into
account is important for classification of sentences
using a polarity dictionary. To circumvent this prob-
lem, Kennedy and Inkpen (2006) and Hu and Liu
(2004) proposed to use a manually-constructed list
of polarity-shifters. However, it has limitations due
to the diversity of expressions.
297
Another weakness of the majority voting is that
it cannot be easily combined with existing methods
that use the n-gram model or tree structures of the
sentence as features. The method we propose here
can easily be combined with existing methods and
show better performance.
4 Word-Level Polarity-Shifting Model
We assume that when the polarity of a word is dif-
ferent from the polarity of the sentence, the polarity
of the word is shifted by its context to adapt to the
polarity of the sentence. Capturing such polarity-
shifts will improve the classification performance of
the majority voting classifier as well as of more so-
phisticated classifiers.
In this paper, we propose a word polarity-shifting
model to capture such phenomena. This model is
a kind of binary classification model which deter-
mines whether the polarity is shifted by its context.
The model assigns a score sshift(x, S) to the senti-
ment word x in the sentence S. If the polarity of x
is shifted in S, sshift(x, S) > 0. If the polarity of x
is not shifted in S, sshift(x, S) ? 0. Let w be a pa-
rameter vector of the model and ? be a pre-defined
feature function. Function sshift is defined as
sshift(x, S) = w ? ?(x, S). (1)
Since this model is a linear discriminative model,
there are well-known algorithms to estimate the pa-
rameters of the model.
Usually, such models are trained with each occur-
rence of words as one instance (word-wise learning).
However, we can train our model more effectively
with each sentence being one instance (sentence-
wise learning). In this section, we describe how to
train our model in two different ways and how to
apply the model to a sentence classification.
4.1 Word-wise Learning
In this learning method, we train the word-level
polarity-shift model with each occurrence of sen-
timent words being an instance. Training exam-
ples are automatically extracted by finding sentiment
words in labeled sentences. In the example of Sec-
tion 3, for instance, both negative words (?distor-
tion? or ?problems?) and a positive word (?pleased?)
appear in a positive sentence. We regard ?distortion?
and ?problems?, whose polarities are different from
that of the sentence, as belonging to the polarity-
shifted class. On the contrary, we regard ?pleased?,
whose polarity is the same as that of the sentence, as
not belonging to polarity-shifted class.
We can use the majority voting by those (possi-
bly polarity-shifted) sentiment words. Specifically,
we first classify each sentiment word in the sentence
according to whether the polarity is shifted or not.
Then we use the majority voting to determine the
polarity of the sentence. If the first classifier classi-
fies a positive word into the ?polarity-shifted? class,
we treat the word as a negative one. We expect that
the majority voting with polarity-shifting will out-
perform the simple majority voting without polarity-
shifting. We actually use the weighted majority vot-
ing, where the polarity-shifting score for each senti-
ment word is used as the weight of the vote by the
word. We expect that the score works as a confi-
dence measure.
We can formulate this method as follows. Here,
N and P are respectively defined as the sets of neg-
ative sentiment words and positive sentiment words.
For instance, x ? N means that x is a negative word.
We also write x ? S to express that the word x oc-
curs in S.
First, let us define two scores, scorep(S) and
scoren(S), for the input sentence S. The scorep(S)
and the scoren(S) respectively represent the num-
ber of votes for S being positive and the number
of votes for S being negative. If scorep(S) >
scoren(S), we regard the sentence S as having the
positive polarity, otherwise negative. We suppose
that the following relations hold for the scores:
scorep(S) =
?
x?P?S
?sshift(x, S) +
?
x?N?S
sshift(x, S), (2)
scoren(S) =
?
x?P?S
sshift(x, S) +
?
x?N?S
?sshift(x, S). (3)
When either a polarity-unchanged positive word
(sshift(x, S) ? 0) or a polarity-shifted negative
word occurs in the sentence S, scorep(S) increases.
We can easily obtain the following relation between
two scores:
scorep(S) = ?scoren(S). (4)
298
Since, according to this relation, scorep(S) >
scoren(S) is equivalent to scorep(S) > 0, we use
only scorep(S) for the rest of this paper.
4.2 Sentence-wise Learning
The equation (2) can be rewritten as
scorep(S) =
?
x?S
sshift(x, S)I(x)
=
?
x?S
w ? ?(x, S)I(x)
= w ?
{
?
x?S
?(x, S)I(x)
}
, (5)
where I(x) is the function defined as follows:
I(x) =
?
?
?
?
?
+1 if x ? N ,
?1 if x ? P ,
0 otherwise.
(6)
This scorep(S) can also be seen as a linear discrimi-
native model and the parameters of the model can be
estimated directly (i.e., without carrying out word-
wise learning). Each labeled sentence in a corpus
can be used as a training instance for the model.
In this method, the model is learned so that the
predictive ability for sentence classification is opti-
mized, instead of the predictive ability for polarity-
shifting. Therefore, this model can remain indeci-
sive on the classification of word instances that have
little contextual evidence about whether polarity-
shifting occurs or not. The model can rely more
heavily on word instances that have much evidence.
In contrast, the word-wise learning trains the
model with all the sentiment words appearing in a
corpus. It is assumed here that all the sentiment
words have relations with the sentence-level polar-
ity, and that we can always find the evidence of the
phenomena that the polarity of a word is different
from that of a sentence. Obviously, this assump-
tion is not always correct. As a result, the word-wise
learning sometimes puts a large weight on a context
word that is irrelevant to the polarity-shifting. This
might degrade the performance of sentence classifi-
cation as well as of polarity-shifting.
4.3 Hybrid Model
Both methods described in Sections 4.1 and 4.2
are to predict the sentence-level polarity only with
the word-level polarity. On the other hand, sev-
eral methods that use another set of features, for ex-
ample, bag-of-words, n-grams or dependency trees,
were proposed for the sentence or document classi-
fication tasks. We propose to combine our method
with existing methods. We refer to it as hybrid
model.
In recent work, discriminative models including
SVM are often used with many different features.
These methods are generally represented as
score?p(X) = w? ? ??(X), (7)
where X indicates the target of classification, for ex-
ample, a sentence or a document. If score?p(X) > 0,
X is classified into the target class. ??(X) is a fea-
ture function. When the method uses the bag-of-
words model, ?? maps X to a vector with each ele-
ment corresponding to a word.
Here, we define new score function scorecomb(S)
as a linear combination of scorep(S), the score
function of our sentence-wise learning, and
score?p(S), the score function of an existing
method. Using this, we can write the function as
scorecomb(S) = ?scorep(S) + (1 ? ?)score?p(S)
= ?
?
x?S
w ? ?(x, S)I(x) + (1 ? ?)w? ? ??(S)
= wcomb ?
?
?
?
x?S
?(x, S)I(x), (1 ? ?)??(S)
?
. (8)
Note that ?? indicates the concatenation of two vec-
tors, wcomb is defined as ?w, w?? and ? is a param-
eter which controls the influence of the word-level
polarity-shifting model. This model is also a dis-
criminative model and we can estimate the param-
eters with a variety of algorithms including SVMs.
We can incorporate additional information like bag-
of-words or dependency trees by ??(S).
4.4 Discussions on the Proposed Model
Features such as n-grams or dependency trees can
also capture some negations or polarity-shifters. For
example, although ?satisfy? is positive, the bigram
model will learn ?not satisfy? as a feature corre-
lated with negative polarity if it appears in the train-
ing data. However, the bigram model cannot gener-
alize the learned knowledge to other features such
299
Table 1: Statistics of the corpus
customer movie
# of Labeled Sentences 1,700 10,662
Available 1,436 9,492
# of Sentiment Words 3,276 26,493
Inconsistent Words 1,076 10,674
as ?not great? or ?not disappoint?. On the other
hand, our polarity-shifter model learns that the word
?not? causes polarity-shifts. Therefore, even if there
was no ?not disappoint? in training data, our model
can determine that ?not disappoint? has correlation
with positive class, because the dictionary contains
?disappoint? as a negative word. For this reason,
the polarity-shifting model can be learned even with
smaller training data.
What we can obtain from the proposed method is
not only a set of polarity-shifters. We can also obtain
the weight vector w, which indicates the strength of
each polarity-shifter and is learned so that the pre-
dictive ability of sentence classification is optimized
especially in the sentence-wise learning. It is impos-
sible to manually determine such weights for numer-
ous features.
It is also worth noting that all the models proposed
in this paper can be represented as a kernel function.
For example, the hybrid model can be seen as the
following kernel:
Kcomb(S1, S2) = ?
?
xi?S1
?
xj?S2
K((xi, S1), (xj , S2))
+(1 ? ?)K ?(S1, S2). (9)
Here, K means the kernel function between
words and K ? means the kernel function be-
tween sentences respectively. In addition,
?
xi
?
xjK((xi, S1), (xj , S2)) can be seen as
an instance of convolution kernels, which was
proposed by Haussler (1999). Convolution kernels
are a general class of kernel functions which are
calculated on the basis of kernels between substruc-
tures of inputs. Our proposed kernel treats sentences
as input, and treats sentiment words as substructures
of sentences. We can use high degree polynomial
kernels as both K which is a kernel between sub-
structures, i.e. sentiment words, of sentences, and
K ? which is a kernel between sentences to make the
classifiers take into consideration the combination
of features.
5 Evaluation
5.1 Datasets
We used two datasets, customer reviews 1 (Hu
and Liu, 2004) and movie reviews 2 (Pang and
Lee, 2005) to evaluate sentiment classification of
sentences. Both of these two datasets are often
used for evaluation in sentiment analysis researches.
The number of examples and other statistics of the
datasets are shown in Table 1.
Our method cannot be applied to sentences which
contain no sentiment words. We therefore elimi-
nated such sentences from the datasets. ?Available?
in Table 1 means the number of examples to which
our method can be applied. ?Sentiment Words?
shows the number of sentiment words that are found
in the given sentences. Please remember that senti-
ment words are defined as those words that are listed
in a predefined polarity dictionary in this paper. ?In-
consistent Words? shows the number of the words
whose polarities conflicted with the polarity of the
sentence.
We performed 5-fold cross-validation and used
the classification accuracy as the evaluation mea-
sure. We extracted sentiment words from General
Inquirer (Stone et al, 1996) and constructed a polar-
ity dictionary. After some preprocessing, the dictio-
nary contains 2,084 positive words and 2,685 nega-
tive words.
5.2 Experimental Settings
We employed the Max Margin Online Learning
Algorithms for parameter estimation of the model
(Crammer et al, 2006; McDonald et al, 2007).
In preliminary experiments, this algorithm yielded
equal or better results compared to SVMs. As the
feature representation, ?(x, S), of polarity-shifting
model, we used the local context of three words
to the left and right of the target sentiment word.
We used the polynomial kernel of degree 2 for
polarity-shifting model and the linear kernel for oth-
1http://www.cs.uic.edu/?liub/FBS/FBS.
html
2http://www.cs.cornell.edu/people/pabo/
movie-review-data/
300
Table 2: Experimental results of the sentence classi-
fication
methods customer movie
Baseline 0.638 0.504
BoW 0.790 0.724
2gram 0.809 0.756
3gram 0.800 0.762
Simple-Voting 0.716 0.624
Negation Voting 0.733 0.658
Word-wise 0.783 0.699
Sentence-wise 0.806 0.718
Hybrid BoW 0.827 0.748
Hybrid 2gram 0.840 0.755
Hybrid 3gram 0.837 0.758
Opt 0.840 0.770
ers, and feature vectors are normalized to 1. In hy-
brid models, the feature vectors,
?
x?S ?(x, S)I(x)
and ??(S) are normalized respectively.
5.3 Comparison of the Methods
We compared the following methods:
? Baseline classifies all sentences as positive.
? BoW uses unigram features. 2gram uses uni-
grams and bigrams. 3gram uses unigrams, bi-
grams, and 3grams.
? Simple-Voting is the most simple majority vot-
ing with word-level polarity (Section 3).
? Negation Voting proposed by Hu and
Liu (2004) is the majority voting that takes
negations into account. As negations, we
employed not, no, yet, never, none, nobody,
nowhere, nothing, and neither, which are taken
from (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2006; Hu and Liu, 2004) (Section 3).
? Word-wise was described in Section 4.1.
? Sentence-wise was described in Section 4.2.
? Hybrid BoW, hybrid 2gram, hybrid 3gram
are combinations of sentence-wise model and
respectively BoW, 2gram and 3gram (Section
4.3). We set ? = 0.5.
Table 2 shows the results of these experiments.
Hybrid 3gram, which corresponds to the proposed
method, obtained the best accuracy on customer re-
view dataset. However, on movie review dataset,
the proposed method did not outperform 3gram. In
Section 5.4, we will discuss this result in details.
Comparing word-wise to simple-voting, the accu-
racy increased by about 7 points. This means that
the polarity-shifting model can capture the polarity-
shifts and it is an important factor for sentiment clas-
sification. In addition, we can see the effectiveness
of sentence-wise, by comparing it to word-wise in
accuracy.
?Opt? in Table 2 shows the results of hybrid mod-
els with optimal ? and combination of models. The
optimal results of hybrid models achieved the best
accuracy on both datasets.
We show some dominating polarity-shifters ob-
tained through learning. We obtained many nega-
tions (e.g., no, not, n?t, never), modal verbs (e.g.,
might, would, may), prepositions (e.g., without, de-
spite), comma with a conjunction (e.g., ?, but? as
in ?the case is strong and stylish, but lacks a win-
dow?), and idiomatic expressions (e.g., ?hard resist?
as in ?it is hard to resist?, and ?real snooze?).
5.4 Effect of Training Data Size
When we have a large amount of training data, the n-
gram classifier can learn well whether each n-gram
tends to appear in the positive class or the negative
class. However, when we have only a small amount
of training data, the n-gram classifier cannot capture
such tendency. Therefore the external knowledge,
such as word-level polarity, could be more valuable
information for classification. Thus it is expected
that the sentence-wise model and the hybrid model
will outperform n-gram classifier which does not
take word-level polarity into account, more largely
with few training data.
To verify this conjecture, we conducted experi-
ments by changing the number of the training ex-
amples, i.e., the labeled sentences. We evaluated
three models: sentence-wise, 3gram model and hy-
brid 3gram on both customer review and movie re-
view.
Figures 1 and 2 show the results on customer re-
view and movie review respectively. When the size
of the training data is small, sentence-wise outper-
301
Figure 1: Experimental results on customer review
Figure 2: Experimental results on movie review
forms 3gram on both datasets. We can also see that
the advantage of sentence-wise becomes smaller as
the amount of training data increases, and that the
hybrid 3gram model almost always achieved the best
accuracy among the three models. Similar behaviour
was observed when we ran the same experiments
with 2gram or BoW model. From these results, we
can conclude that, as we expected above, the word-
level polarity is especially effective when we have
only a limited amount of training data, and that the
hybrid model can combine two models effectively.
6 Conclusion
We proposed a model that captures the polarity-
shifting of sentiment words in sentences. We also
presented two different learning methods for the
model and proposed an augmented hybrid classifier
that is based both on the model and on existing clas-
sifiers. We evaluated our method and reported that
the proposed method almost always improved the
accuracy of sentence classification compared with
other simpler methods. The improvement was more
significant when we have only a limited amount of
training data.
For future work, we plan to explore new feature
sets appropriate for our model. The feature sets we
used for evaluation in this paper are not necessar-
ily optimal and we can expect a better performance
by exploring appropriate features. For example, de-
pendency relations between words or appearances of
conjunctions will be useful. The position of a word
in the given sentence is also an important factor in
sentiment analysis (Taboada and Grieve, 2004). Fur-
thermore, we should directly take into account the
fact that some words do not affect the polarity of the
sentence, though the proposed method tackled this
problem indirectly. We cannot avoid this problem
to use word-level polarity more effectively. Lastly,
since we proposed a method for the sentence-level
sentiment prediction, our next step is to extend the
method to the document-level sentiment prediction.
Acknowledgement
This research was supported in part by Overseas Ad-
vanced Educational Research Practice Support Pro-
gram by Ministry of Education, Culture, Sports, Sci-
ence and Technology.
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online Passive-
Aggressive Algorithms. In Journal of Machine Learn-
ing Research, Vol.7, Mar, pp.551?585, 2006.
Michael Gamon. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and
the role of linguistic analysis. In Proceedings of the
20th International Conference on Computational Lin-
guistics (COLING-2004) , pp.841?847, 2004.
David Haussler. Convolution Kernels on Discrete Struc-
tures, Technical Report UCS-CRL-99-10, University
of California in Santa Cruz, 1999.
Minqing Hu and Bing Liu. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteeth
National Conference on Artificial Intellgience (AAAI-
2004) , pp.755?560, San Jose, USA, July 2004.
302
Alistair Kennedy and Diana Inkpen. Sentiment Classi-
fication of Movie and Product Reviews Using Con-
textual Valence Shifters. In Workshop on the Analysis
of Formal and Informal Information Exchange during
Negotiations (FINEXIN-2005), 2005.
Taku Kudo and Yuji Matsumoto. A Boosting Algorithm
for Classification of Semi-Structured Text. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2004), pp.301?
308, 2004.
Yu Mao and Guy Lebanon. Isotonic Conditional Ran-
dom Fields and Local Sentiment Flow. In Proceedings
of the Newral Information Processing Systems (NIPS-
2006), pp.961?968, 2006.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. Sentiment Classification using Word Sub-
Sequences and Dependency Sub-Trees. In Proceed-
ings of the 9th Pacific-Asia International Conference
on Knowledge Discovery and Data Mining (PAKDD-
2005), pp.301?310 , 2005.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. Structured Models for Fine-to-
Coarse Sentiment Analysis. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL-2007), pp.432?439, 2007.
Tony Mullen and Nigel Collier. Sentiment analysis us-
ing support vector machines with diverse informa-
tion sources. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004), pp.412?418, 2004.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
Thumbs up? Sentiment Classification using Machine
Learning Techniques. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2002), pp.76?86, 2002.
Bo Pang and Lillian Lee. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summarization
Based on Minimum Cuts. In Proceedings of the 42th
Annual Meeting of the Association for Computational
Linguistics (ACL-2004), pp.271?278, 2004.
Bo Pang and Lillian Lee. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2005), pp.115?124, 2005.
Livia Polanyi and Annie Zaenen. Contextual Valence
Shifters. In AAAI Spring Symposium on Exploring At-
titude and Affect in Text: Theories and Applications
(AAAI-EAAT2004), 2004.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. The General Inquirer: A Com-
puter Approach to Content Analysis. The MIT Press,
1996.
Maite Taboada and Jack Grieve. Analyzing Appraisal
Automatically. In AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Applica-
tions (AAAI-EAAT2004), pp.158?161, 2004.
303
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280?287,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Guiding Semi-Supervision with Constraint-Driven Learning
Ming-Wei Chang Lev Ratinov Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, ratinov2, danr}@uiuc.edu
Abstract
Over the last few years, two of the main
research directions in machine learning of
natural language processing have been the
study of semi-supervised learning algo-
rithms as a way to train classifiers when the
labeled data is scarce, and the study of ways
to exploit knowledge and global information
in structured learning tasks. In this paper,
we suggest a method for incorporating do-
main knowledge in semi-supervised learn-
ing algorithms. Our novel framework unifies
and can exploit several kinds of task specic
constraints. The experimental results pre-
sented in the information extraction domain
demonstrate that applying constraints helps
the model to generate better feedback during
learning, and hence the framework allows
for high performance learning with signif-
icantly less training data than was possible
before on these tasks.
1 Introduction
Natural Language Processing (NLP) systems typi-
cally require large amounts of knowledge to achieve
good performance. Acquiring labeled data is a dif-
ficult and expensive task. Therefore, an increasing
attention has been recently given to semi-supervised
learning, where large amounts of unlabeled data are
used to improve the models learned from a small
training set (Collins and Singer, 1999; Thelen and
Riloff, 2002). The hope is that semi-supervised or
even unsupervised approaches, when given enough
knowledge about the structure of the problem, will
be competitive with the supervised models trained
on large training sets. However, in the general
case, semi-supervised approaches give mixed re-
sults, and sometimes even degrade the model per-
formance (Nigam et al, 2000). In many cases, im-
proving semi-supervised models was done by seed-
ing these models with domain information taken
from dictionaries or ontology (Cohen and Sarawagi,
2004; Collins and Singer, 1999; Haghighi and Klein,
2006; Thelen and Riloff, 2002). On the other hand,
in the supervised setting, it has been shown that
incorporating domain and problem specific struc-
tured information can result in substantial improve-
ments (Toutanova et al, 2005; Roth and Yih, 2005).
This paper proposes a novel constraints-based
learning protocol for guiding semi-supervised learn-
ing. We develop a formalism for constraints-based
learning that unifies several kinds of constraints:
unary, dictionary based and n-ary constraints, which
encode structural information and interdependencies
among possible labels. One advantage of our for-
malism is that it allows capturing different levels of
constraint violation. Our protocol can be used in
the presence of any learning model, including those
that acquire additional statistical constraints from
observed data while learning (see Section 5. In the
experimental part of this paper we use HMMs as the
underlying model, and exhibit significant reduction
in the number of training examples required in two
information extraction problems.
As is often the case in semi-supervised learning,
the algorithm can be viewed as a process that im-
proves the model by generating feedback through
280
labeling unlabeled examples. Our algorithm pushes
this intuition further, in that the use of constraints
allows us to better exploit domain information as a
way to label, along with the current learned model,
unlabeled examples. Given a small amount of la-
beled data and a large unlabeled pool, our frame-
work initializes the model with the labeled data and
then repeatedly:
(1) Uses constraints and the learned model to label
the instances in the pool.
(2) Updates the model by newly labeled data.
This way, we can generate better ?training? ex-
amples during the semi-supervised learning process.
The core of our approach, (1), is described in Sec-
tion 5. The task is described in Section 3 and the
Experimental study in Section 6. It is shown there
that the improvement on the training examples via
the constraints indeed boosts the learned model and
the proposed method significantly outperforms the
traditional semi-supervised framework.
2 Related Work
In the semi-supervised domain there are two main
approaches for injecting domain specific knowledge.
One is using the prior knowledge to accurately tailor
the generative model so that it captures the domain
structure. For example, (Grenager et al, 2005) pro-
poses Diagonal Transition Models for sequential la-
beling tasks where neighboring words tend to have
the same labels. This is done by constraining the
HMM transition matrix, which can be done also for
other models, such as CRF. However (Roth and Yih,
2005) showed that reasoning with more expressive,
non-sequential constraints can improve the perfor-
mance for the supervised protocol.
A second approach has been to use a small high-
accuracy set of labeled tokens as a way to seed and
bootstrap the semi-supervised learning. This was
used, for example, by (Thelen and Riloff, 2002;
Collins and Singer, 1999) in information extraction,
and by (Smith and Eisner, 2005) in POS tagging.
(Haghighi and Klein, 2006) extends the dictionary-
based approach to sequential labeling tasks by prop-
agating the information given in the seeds with con-
textual word similarity. This follows a conceptually
similar approach by (Cohen and Sarawagi, 2004)
that uses a large named-entity dictionary, where the
similarity between the candidate named-entity and
its matching prototype in the dictionary is encoded
as a feature in a supervised classifier.
In our framework, dictionary lookup approaches
are viewed as unary constraints on the output states.
We extend these kinds of constraints and allow for
more general, n-ary constraints.
In the supervised learning setting it has been es-
tablished that incorporating global information can
significantly improve performance on several NLP
tasks, including information extraction and semantic
role labeling. (Punyakanok et al, 2005; Toutanova
et al, 2005; Roth and Yih, 2005). Our formalism
is most related to this last work. But, we develop a
semi-supervised learning protocol based on this for-
malism. We also make use of soft constraints and,
furthermore, extend the notion of soft constraints to
account for multiple levels of constraints? violation.
Conceptually, although not technically, the most re-
lated work to ours is (Shen et al, 2005) that, in
a somewhat ad-hoc manner uses soft constraints to
guide an unsupervised model that was crafted for
mention tracking. To the best of our knowledge,
we are the first to suggest a general semi-supervised
protocol that is driven by soft constraints.
We propose learning with constraints - a frame-
work that combines the approaches described above
in a unified and intuitive way.
3 Tasks, Examples and Datasets
In Section 4 we will develop a general framework
for semi-supervised learning with constraints. How-
ever, it is useful to illustrate the ideas on concrete
problems. Therefore, in this section, we give a brief
introduction to the two domains on which we tested
our algorithms. We study two information extrac-
tion problems in each of which, given text, a set of
pre-defined fields is to be identified. Since the fields
are typically related and interdependent, these kinds
of applications provide a good test case for an ap-
proach like ours.1
The first task is to identify fields from citations
(McCallum et al, 2000) . The data originally in-
cluded 500 labeled references, and was later ex-
tended with 5,000 unannotated citations collected
from papers found on the Internet (Grenager et al,
2005). Given a citation, the task is to extract the
1The data for both problems is available at:
http://www.stanford.edu/ grenager/data/unsupie.tgz
281
(a) [ AUTHOR Lars Ole Andersen . ] [ TITLE Program analysis and specialization for the C programming language . ] [TECH-REPORT PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , ] [ DATE May 1994 . ]
(b) [ AUTHOR Lars Ole Andersen . Program analysis and ] [TITLE specialization for the ] [EDITOR C ] [ BOOKTITLE
Programming language ] [ TECH-REPORT . PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , May ] [ DATE
1994 . ]
Figure 1: Error analysis of a HMM model. The labels are annotated by underline and are to the right of
each open bracket. The correct assignment was shown in (a). While the predicted label assignment (b) is
generally coherent, some constraints are violated. Most obviously, punctuation marks are ignored as cues
for state transitions. The constraint ?Fields cannot end with stop words (such as ?the?)? may be also good.
fields that appear in the given reference. See Fig. 1.
There are 13 possible fields including author, title,
location, etc.
To gain an insight to how the constraints can guide
semi-supervised learning, assume that the sentence
shown in Figure 1 appears in the unlabeled data
pool. Part (a) of the figure shows the correct la-
beled assignment and part (b) shows the assignment
labeled by a HMM trained on 30 labels. However,
if we apply the constraint that state transition can
occur only on punctuation marks, the same HMM
model parameters will result in the correct labeling
(a). Therefore, by adding the improved labeled as-
signment we can generate better training samples
during semi-supervised learning. In fact, the punc-
tuation marks are only some of the constraints that
can be applied to this problem. The set of constraints
we used in our experiments appears in Table 1. Note
that some of the constraints are non-local and are
very intuitive for people, yet it is very difficult to
inject this knowledge into most models.
The second problem we consider is extracting
fields from advertisements (Grenager et al, 2005).
The dataset consists of 8,767 advertisements for
apartment rentals in the San Francisco Bay Area
downloaded in June 2004 from the Craigslist web-
site. In the dataset, only 302 entries have been la-
beled with 12 fields, including size, rent, neighbor-
hood, features, and so on. The data was prepro-
cessed using regular expressions for phone numbers,
email addresses and URLs. The list of the con-
straints for this domain is given in Table 1. We im-
plement some global constraints and include unary
constraints which were largely imported from the
list of seed words used in (Haghighi and Klein,
2006). We slightly modified the seedwords due to
difference in preprocessing.
4 Notation and Definitions
Consider a structured classification problem, where
given an input sequence x = (x1, . . . , xN ), the taskis to find the best assignment to the output variables
y = (y1, . . . , yM ). We denote X to be the space ofthe possible input sequences and Y to be the set of
possible output sequences.
We define a structured output classifier as a func-
tion h : X ? Y that uses a global scoring function
f : X ?Y ? R to assign scores to each possible in-
put/output pair. Given an input x, a desired function
f will assign the correct output y the highest score
among all the possible outputs. The global scoring
function is often decomposed as a weighted sum of
feature functions,
f(x, y) =
M
?
i=1
?ifi(x, y) = ? ? F (x, y).
This decomposition applies both to discriminative
linear models and to generative models such as
HMMs and CRFs, in which case the linear sum
corresponds to log likelihood assigned to the in-
put/output pair by the model (for details see (Roth,
1999) for the classification case and (Collins, 2002)
for the structured case). Even when not dictated by
the model, the feature functions fi(x, y) used arelocal to allow inference tractability. Local feature
function can capture some context for each input or
output variable, yet it is very limited to allow dy-
namic programming decoding during inference.
Now, consider a scenario where we have a set
of constraints C1, . . . , CK . We define a constraint
C : X ? Y ? {0, 1} as a function that indicates
whether the input/output sequence violates some de-
sired properties. When the constraints are hard, the
solution is given by
argmax
y?1C(x)
? ? F (x, y),
282
(a)-Citations
1) Each field must be a consecutive list of words, and can
appear at most once in a citation.
2) State transitions must occur on punctuation marks.
3) The citation can only start with author or editor.
4) The words pp., pages correspond to PAGE.
5) Four digits starting with 20xx and 19xx are DATE.
6) Quotations can appear only in titles.
7) The words note, submitted, appear are NOTE.
8) The words CA, Australia, NY are LOCATION.
9) The words tech, technical are TECH REPORT.
10) The words proc, journal, proceedings, ACM are JOUR-NAL or BOOKTITLE.
11) The words ed, editors correspond to EDITOR.
(b)-Advertisements
1) State transitions can occur only on punctuation marks or
the newline symbol.
2) Each field must be at least 3 words long.
3) The words laundry, kitchen, parking are FEATURES.
4) The words sq, ft, bdrm are SIZE.
5) The word $, *MONEY* are RENT.
6) The words close, near, shopping are NEIGHBORHOOD.
7) The words laundry kitchen, parking are FEATURES.
8) The (normalized) words phone, email are CONTACT.
9) The words immediately, begin, cheaper are AVAILABLE.
10) The words roommates, respectful, drama are ROOM-MATES.
11) The words smoking, dogs, cats are RESTRICTIONS.
12) The word http, image, link are PHOTOS.
13) The words address, carlmont, st, cross are ADDRESS.
14) The words utilities, pays, electricity are UTILITIES.
Table 1: The list of constraints for extracting fields
from citations and advertisements. Some constraints
(represented in the first block of each domain) are
global and are relatively difficult to inject into tradi-
tional models. While all the constraints hold for the
vast majority of the data, some of them are violated
by some correct labeled assignments.
where 1C(x) is a subset of Y for which all Ci as-sign the value 1 for the given (x, y).
When the constraints are soft, we want to in-
cur some penalty for their violation. Moreover, we
want to incorporate into our cost function a mea-
sure for the amount of violation incurred by vi-
olating the constraint. A generic way to capture
this intuition is to introduce a distance function
d(y, 1Ci(x)) between the space of outputs that re-spect the constraint,1Ci(x), and the given output se-quence y. One possible way to implement this dis-
tance function is as the minimal Hamming distance
to a sequence that respects the constraint Ci, that is:
d(y, 1Ci(x)) = min(y??1C(x)) H(y, y?). If the penaltyfor violating the soft constraint Ci is ?i, we write the
score function as:
argmax
y
? ? F (x, y) ?
K
?
i=1
?id(y, 1Ci(x)) (1)
We refer to d(y, 1C(x)) as the valuation of theconstraint C on (x, y). The intuition behind (1) is as
follows. Instead of merely maximizing the model?s
likelihood, we also want to bias the model using
some knowledge. The first term of (1) is used to
learn from data. The second term biases the mode
by using the knowledge encoded in the constraints.
Note that we do not normalize our objective function
to be a true probability distribution.
5 Learning and Inference with Constraints
In this section we present a new constraint-driven
learning algorithm (CODL) for using constraints to
guide semi-supervised learning. The task is to learn
the parameter vector ? by using the new objective
function (1). While our formulation allows us to
train also the coefficients of the constraints valua-
tion, ?i, we choose not to do it, since we view this asa way to bias (or enforce) the prior knowledge into
the learned model, rather than allowing the data to
brush it away. Our experiments demonstrate that the
proposed approach is robust to inaccurate approxi-
mation of the prior knowledge (assigning the same
penalty to all the ?i ).We note that in the presence of constraints, the
inference procedure (for finding the output y that
maximizes the cost function) is usually done with
search techniques (rather than Viterbi decoding,
see (Toutanova et al, 2005; Roth and Yih, 2005) for
a discussion), we chose beamsearch decoding.
The semi-supervised learning with constraints is
done with an EM-like procedure. We initialize the
model with traditional supervised learning (ignoring
the constraints) on a small labeled set. Given an un-
labeled set U , in the estimation step, the traditional
EM algorithm assigns a distribution over labeled as-
signmentsY of each x ? U , and in the maximization
step, the set of model parameters is learned from the
distributions assigned in the estimation step.
However, in the presence of constraints, assigning
the complete distributions in the estimation step is
infeasible since the constraints reshape the distribu-
tion in an arbitrary way. As in existing methods for
training a model by maximizing a linear cost func-
tion (maximize likelihood or discriminative maxi-
283
mization), the distribution over Y is represented as
the set of scores assigned to it; rather than consid-
ering the score assigned to all y?s, we truncate the
distribution to the top K assignments as returned
by the search. Given a set of K top assignments
y1, . . . , yK , we approximate the estimation step by
assigning uniform probability to the top K candi-
dates, and zero to the other output sequences. We
denote this algorithm top-K hard EM. In this pa-
per, we use beamsearch to generate K candidates
according to (1).
Our training algorithm is summarized in Figure 2.
Several things about the algorithm should be clari-
fied: the Top-K-Inference procedure in line 7, the
learning procedure in line 9, and the new parameter
estimation in line 9.
The Top-K-Inference is a procedure that returns
the K labeled assignments that maximize the new
objective function (1). In our case we used the top-
K elements in the beam, but this could be applied
to any other inference procedure. The fact that the
constraints are used in the inference procedure (in
particular, for generating new training examples) al-
lows us to use a learning algorithm that ignores the
constraints, which is a lot more efficient (although
algorithms that do take the constraints into account
can be used too). We used maximum likelihood es-
timation of ? but, in general, perceptron or quasi-
Newton can also be used.
It is known that traditional semi-supervised train-
ing can degrade the learned model?s performance.
(Nigam et al, 2000) has suggested to balance the
contribution of labeled and unlabeled data to the pa-
rameters. The intuition is that when iteratively esti-
mating the parameters with EM, we disallow the pa-
rameters to drift too far from the supervised model.
The parameter re-estimation in line 9, uses a similar
intuition, but instead of weighting data instances, we
introduced a smoothing parameter ? which controls
the convex combination of models induced by the la-
beled and the unlabeled data. Unlike the technique
mentioned above which focuses on naive Bayes, our
method allows us to weight linear models generated
by different learning algorithms.
Another way to look the algorithm is from the
self-training perspective (McClosky et al, 2006).
Similarly to self-training, we use the current model
to generate new training examples from the unla-
Input:
Cycles: learning cycles
Tr = {x, y}: labeled training set.
U : unlabeled dataset
F : set of feature functions.
{?i}: set of penalties.
{Ci}: set of constraints.
?: balancing parameter with the supervised model.
learn(Tr, F ): supervised learning algorithm
Top-K-Inference:
returns top-K labeled scored by the cost function (1)CODL:
1. Initialize ?0 = learn(Tr, F ).2. ? = ?0.3. For Cycles iterations do:
4. T = ?
5. For each x ? U
6. {(x, y1), . . . , (x, yK)} =
7. Top-K-Inference(x, ?, F, {Ci}, {?i})
8. T = T ? {(x, y1), . . . , (x, yK)}
9. ? = ??0 + (1 ? ?)learn(T, F )
Figure 2: COnstraint Driven Learning (CODL). In
Top-K-Inference, we use beamsearch to find the K-
best solution according to Eq. (1).
beled set. However, there are two important differ-
ences. One is that in self-training, once an unlabeled
sample was labeled, it is never labeled again. In
our case all the samples are relabeled in each iter-
ation. In self-training it is often the case that only
high-confidence samples are added to the labeled
data pool. While we include all the samples in the
training pool, we could also limit ourselves to the
high-confidence samples. The second difference is
that each unlabeled example generates K labeled in-
stances. The case of one iteration of top-1 hard EM
is equivalent to self training, where all the unlabeled
samples are added to the labeled pool.
There are several possible benefits to using K > 1
samples. (1) It effectively increases the training set
by a factor of K (albeit by somewhat noisy exam-
ples). In the structured scenario, each of the top-K
assignments is likely to have some good components
so generating top-K assignments helps leveraging
the noise. (2) Given an assignment that does not sat-
isfy some constraints, using top-K allows for mul-
tiple ways to correct it. For example, consider the
output 11101000 with the constraint that it should
belong to the language 1?0?. If the two top scoring
corrections are 11111000 and 11100000, consider-
ing only one of those can negatively bias the model.
284
6 Experiments and Results
In this section, we present empirical results of our
algorithms on two domains: citations and adver-
tisements. Both problems are modeled with a sim-
ple token-based HMM. We stress that token-based
HMM cannot represent many of our constraints. The
function d(y, 1C(x)) used is an approximation of aHamming distance function, discussed in Section 7.
For both domains, and all the experiments, ? was
set to 0.1. The constraints violation penalty ? is set
to ? log 10?4 and ? log 10?1 for citations and ad-
vertisements, resp.2 Note that all constraints share
the same penalty. The number of semi-supervised
training cycles (line 3 of Figure 2) was set to 5. The
constraints for the two domains are listed in Table 1.
We trained models on training sets of size vary-
ing from 5 to 300 for the citations and from 5 to
100 for the advertisements. Additionally, in all the
semi-supervised experiments, 1000 unlabeled exam-
ples are used. We report token-based3 accuracy on
100 held-out examples (which do not overlap neither
with the training nor with the unlabeled data). We
ran 5 experiments in each setting, randomly choos-
ing the training set. The results reported below are
the averages over these 5 runs.
To verify our claims we implemented several
baselines. The first baseline is the supervised learn-
ing protocol denoted by sup. The second baseline
was a traditional top-1 Hard EM also known as
truncated EM4 (denoted by H for Hard). In the third
baseline, denoted H&W, we balanced the weight
of the supervised and unsupervised models as de-
scribed in line 9 of Figure 2. We compare these base-
lines to our proposed protocol, H&W&C, where we
added the constraints to guide the H&W protocol.
We experimented with two flavors of the algorithm:
the top-1 and the top-K version. In the top-K ver-
sion, the algorithm uses K-best predictions (K=50)
for each instance in order to update the model as de-
scribed in Figure 2.
The experimental results for both domains are in
given Table 2. As hypothesized, hard EM sometimes
2The guiding intuition is that ?F (x, y) corresponds to a log-
likelihood of a HMM model and ? to a crude estimation of the
log probability that a constraint does not hold. ? was tuned on
a development set and kept fixed in all experiments.
3Each token (word or punctuation mark) is assigned a state.
4We also experimented with (soft) EM without constraints,
but the results were generally worse.
(a)- Citations
N Inf. sup. H H&W H&W&C H&W&C
(Top-1) (Top-K)
5 no I 55.1 60.9 63.6 70.6 71.0
I 66.6 69.0 72.5 76.0 77.8
10 no I 64.6 66.8 69.8 76.5 76.7
I 78.1 78.1 81.0 83.4 83.8
15 no I 68.7 70.6 73.7 78.6 79.4
I 81.3 81.9 84.1 85.5 86.2
20 no I 70.1 72.4 75.0 79.6 79.4
I 81.1 82.4 84.0 86.1 86.1
25 no I 72.7 73.2 77.0 81.6 82.0
I 84.3 84.2 86.2 87.4 87.6
300 no I 86.1 80.7 87.1 88.2 88.2
I 92.5 89.6 93.4 93.6 93.5
(b)-Advertisements
N Inf. sup. H H&W H&W&C H&W&C
(Top-1) (Top-K)
5 no I 55.2 61.8 60.5 66.0 66.0
I 59.4 65.2 63.6 69.3 69.6
10 no I 61.6 69.2 67.0 70.8 70.9
I 66.6 73.2 71.6 74.7 74.7
15 no I 66.3 71.7 70.1 73.0 73.0
I 70.4 75.6 74.5 76.6 76.9
20 no I 68.1 72.8 72.0 74.5 74.6
I 71.9 76.7 75.7 77.9 78.1
25 no I 70.0 73.8 73.0 74.9 74.8
I 73.7 77.7 76.6 78.4 78.5
100 no I 76.3 76.2 77.6 78.5 78.6
I 80.4 80.5 81.2 81.8 81.7
Table 2: Experimental results for extracting fields
from citations and advertisements. N is the number
of labeled samples. H is the traditional hard-EM and
H&W weighs labeled and unlabeled data as men-
tioned in Sec. 5. Our proposed model is H&W&C,
which uses constraints in the learning procedure. I
refers to using constraints during inference at eval-
uation time. Note that adding constraints improves
the accuracy during both learning and inference.
degrade the performance. Indeed, with 300 labeled
examples in the citations domain, the performance
decreases from 86.1 to 80.7. The usefulness of in-
jecting constraints in semi-supervised learning is ex-
hibited in the two right most columns: using con-
straints H&W&C improves the performance over
H&W quite significantly.
We carefully examined the contribution of us-
ing constraints to the learning stage and the testing
stage, and two separate results are presented: test-
ing with constraints (denoted I for inference) and
without constraints (no I). The I results are consis-
tently better. And, it is also clear from Table 2,
that using constraints in training always improves
285
the model and the amount of improvement depends
on the amount of labeled data.
Figure 3 compares two protocols on the adver-
tisements domain: H&W+I, where we first run the
H&W protocol and then apply the constraints dur-
ing testing stage, and H&W&C+I, which uses con-
straints to guide the model during learning and uses
it also in testing. Although injecting constraints in
the learning process helps, testing with constraints is
more important than using constraints during learn-
ing, especially when the labeled data size is large.
This confirms results reported for the supervised
learning case in (Punyakanok et al, 2005; Roth and
Yih, 2005). However, as shown, our proposed al-
gorithm H&W&C for training with constraints is
critical when the amount labeled data is small.
Figure 4 further strengthens this point. In the cita-
tions domain, H&W&C+I achieves with 20 labeled
samples similar performance to the supervised ver-
sion without constraints with 300 labeled samples.
(Grenager et al, 2005) and (Haghighi and Klein,
2006) also report results for semi-supervised learn-
ing for these domains. However, due to differ-
ent preprocessing, the comparison is not straight-
forward. For the citation domain, when 20 labeled
and 300 unlabeled samples are available, (Grenager
et al, 2005) observed an increase from 65.2% to
71.3%. Our improvement is from 70.1% to 79.4%.
For the advertisement domain, they observed no im-
provement, while our model improves from 68.1%
to 74.6% with 20 labeled samples. Moreover, we
successfully use out-of-domain data (web data) to
improve our model, while they report that this data
did not improve their unsupervised model.
(Haghighi and Klein, 2006) also worked on one of
our data sets. Their underlying model, Markov Ran-
dom Fields, allows more expressive features. Nev-
ertheless, when they use only unary constraints they
get 53.75%. When they use their final model, along
with a mechanism for extending the prototypes to
other tokens, they get results that are comparable to
our model with 10 labeled examples. Additionally,
in their framework, it is not clear how to use small
amounts of labeled data when available. Our model
outperforms theirs once we add 10 more examples.
 0.65
 0.7
 0.75
 0.8
 0.85
100252015105
H+N+I
H+N+C+I
Figure 3: Comparison between H&W+I and
H&W&C+I on the advertisements domain. When
there is a lot of labeled data, inference with con-
straints is more important than using constraints dur-
ing learning. However, it is important to train with
constraints when the amount of labeled data is small.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
100252015105
sup. (300)
H+N+C+I
Figure 4: With 20 labeled citations, our algorithm
performs competitively to the supervised version
trained on 300 samples.
7 Soft Constraints
This section discusses the importance of using soft
constraints rather than hard constraints, the choice
of Hamming distance for d(y, 1C(x)) and how weapproximate it. We use two constraints to illustrate
the ideas. (C1): ?state transitions can only occur onpunctuation marks or newlines?, and (C2): ?the field
TITLE must appear?.
First, we claim that defining d(y, 1C(x)) to bethe Hamming distance is superior to using a binary
value, d(y, 1C(x)) = 0 if y ? 1C(x) and 1 other-wise. Consider, for example, the constraint C1 inthe advertisements domain. While the vast majority
of the instances satisfy the constraint, some violate
it in more than one place. Therefore, once the binary
distance is set to 1, the algorithm looses the ability to
discriminate constraint violations in other locations
286
of the same instance. This may hurt the performance
in both the inference and the learning stage.
Computing the Hamming distance exactly can
be a computationally hard problem. Further-
more, it is unreasonable to implement the ex-
act computation for each constraint. Therefore,
we implemented a generic approximation for the
hamming distance assuming only that we are
given a boolean function ?C(yN ) that returnswhether labeling the token xN with state yN vio-lates constraint with respect to an already labeled
sequence (x1, . . . , xN?1, y1, . . . , yN?1). Then
d(y, 1C(x)) =
?N
i=1 ?C(yi). For example,consider the prefix x1, x2, x3, x4, which con-tains no punctuation or newlines and was labeled
AUTH, AUTH, DATE, DATE. This labeling
violates C1, the minimal hamming distance is 2, andour approximation gives 1, (since there is only one
transition that violates the constraint.)
For constraints which cannot be validated based
on prefix information, our approximation resorts to
binary violation count. For instance, the constraint
C2 cannot be implemented with prefix informationwhen the assignment is not complete. Otherwise, it
would mean that the field TITLE should appear as
early as possible in the assignment.
While (Roth and Yih, 2005) showed the signif-
icance of using hard constraints, our experiments
show that using soft constraints is a superior op-
tion. For example, in the advertisements domain,
C1 holds for the large majority of the gold-labeledinstances, but is sometimes violated. In supervised
training with 100 labeled examples on this domain,
sup gave 76.3% accuracy. When the constraint vio-
lation penalty ? was innity (equivalent to hard con-
straint), the accuracy improved to 78.7%, but when
the penalty was set to ?log(0.1), the accuracy of the
model jumped to 80.6%.
8 Conclusions and Future Work
We proposed to use constraints as a way to guide
semi-supervised learning. The framework devel-
oped is general both in terms of the representation
and expressiveness of the constraints, and in terms
of the underlying model being learned ? HMM in
the current implementation. Moreover, our frame-
work is a useful tool when the domain knowledge
cannot be expressed by the model.
The results show that constraints improve not
only the performance of the final inference stage but
also propagate useful information during the semi-
supervised learning process and that training with
the constraints is especially significant when the
number of labeled training data is small.
Acknowledgments: This work is supported by NSF SoD-
HCER-0613885 and by a grant from Boeing. Part of this work
was done while Dan Roth visited the Technion, Israel, sup-
ported by a Lady Davis Fellowship.
References
W. Cohen and S. Sarawagi. 2004. Exploiting dictionaries in
named entity extraction: Combining semi-markov extraction
processes and data integration methods. In Proc. of the ACMSIGKDD.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP.
M. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. of EMNLP.
T. Grenager, D. Klein, and C. Manning. 2005. Unsupervised
learning of field segmentation models for information extrac-
tion. In Proc. of the Annual Meeting of the ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In Proc. of HTL-NAACL.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy markov models for information extraction and seg-
mentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proceedings of HLT-NAACL.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text
classification from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103?134.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learn-
ing and inference over constrained output. In Proc. of IJCAI.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML.
D. Roth. 1999. Learning in natural language. In Proc. of IJCAI,
pages 898?904.
W. Shen, X. Li, and A. Doan. 2005. Constraint-based entity
matching. In Proc. of AAAI).
N. Smith and J. Eisner. 2005. Contrastive estimation: Training
log-linear models on unlabeled data. In Proc. of the AnnualMeeting of the ACL.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern contexts.
In Proc. of EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of theAnnual Meeting of the ACL.
287
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147?155,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Design Challenges and Misconceptions in Named Entity Recognition? ? ?
Lev Ratinov Dan Roth
Computer Science Department
University of Illinois
Urbana, IL 61801 USA
{ratinov2,danr}@uiuc.edu
Abstract
We analyze some of the fundamental design
challenges and misconceptions that underlie
the development of an efficient and robust
NER system. In particular, we address issues
such as the representation of text chunks, the
inference approach needed to combine local
NER decisions, the sources of prior knowl-
edge and how to use them within an NER
system. In the process of comparing several
solutions to these challenges we reach some
surprising conclusions, as well as develop an
NER system that achieves 90.8 F1 score on
the CoNLL-2003 NER shared task, the best
reported result for this dataset.
1 Introduction
Natural Language Processing applications are char-
acterized by making complex interdependent deci-
sions that require large amounts of prior knowledge.
In this paper we investigate one such application?
Named Entity Recognition (NER). Figure 1 illus-
trates the necessity of using prior knowledge and
non-local decisions in NER. In the absence of mixed
case information it is difficult to understand that
? The system and the Webpages dataset are available at:
http://l2r.cs.uiuc.edu/?cogcomp/software.php
? This work was supported by NSF grant NSF SoD-HCER-
0613885, by MIAS, a DHS-IDS Center for Multimodal In-
formation Access and Synthesis at UIUC and by an NDIIPP
project from the National Library of Congress.
? We thank Nicholas Rizzolo for the baseline LBJ NER
system, Xavier Carreras for suggesting the word class models,
and multiple reviewers for insightful comments.
SOCCER - [PER BLINKER] BAN LIFTED .
[LOC LONDON] 1996-12-06 [MISC Dutch] forward
[PER Reggie Blinker] had his indefinite suspension
lifted by [ORG FIFA] on Friday and was set to make
his [ORG Sheffield Wednesday] comeback against
[ORG Liverpool] on Saturday . [PER Blinker] missed
his club?s last two games after [ORG FIFA] slapped a
worldwide ban on him for appearing to sign contracts for
both [ORG Wednesday] and [ORG Udinese] while he was
playing for [ORG Feyenoord].
Figure 1: Example illustrating challenges in NER.
?BLINKER? is a person. Likewise, it is not obvi-
ous that the last mention of ?Wednesday? is an orga-
nization (in fact, the first mention of ?Wednesday?
can also be understood as a ?comeback? which hap-
pens on Wednesday). An NER system could take ad-
vantage of the fact that ?blinker? is also mentioned
later in the text as the easily identifiable ?Reggie
Blinker?. It is also useful to know that Udinese
is a soccer club (an entry about this club appears
in Wikipedia), and the expression ?both Wednesday
and Udinese? implies that ?Wednesday? and ?Udi-
nese? should be assigned the same label.
The above discussion focuses on the need for ex-
ternal knowledge resources (for example, that Udi-
nese can be a soccer club) and the need for non-
local features to leverage the multiple occurrences
of named entities in the text. While these two needs
have motivated some of the research in NER in
the last decade, several other fundamental decisions
must be made. These include: what model to use for
147
sequential inference, how to represent text chunks
and what inference (decoding) algorithm to use.
Despite the recent progress in NER, the effort has
been dispersed in several directions and there are no
published attempts to compare or combine the re-
cent advances, leading to some design misconcep-
tions and less than optimal performance. In this
paper we analyze some of the fundamental design
challenges and misconceptions that underlie the de-
velopment of an efficient and robust NER system.
We find that BILOU representation of text chunks
significantly outperforms the widely adopted BIO.
Surprisingly, naive greedy inference performs com-
parably to beamsearch or Viterbi, while being con-
siderably more computationally efficient. We ana-
lyze several approaches for modeling non-local de-
pendencies proposed in the literature and find that
none of them clearly outperforms the others across
several datasets. However, as we show, these contri-
butions are, to a large extent, independent and, as we
show, the approaches can be used together to yield
better results. Our experiments corroborate recently
published results indicating that word class models
learned on unlabeled text can significantly improve
the performance of the system and can be an al-
ternative to the traditional semi-supervised learning
paradigm. Combining recent advances, we develop
a publicly available NER system that achieves 90.8
F1 score on the CoNLL-2003 NER shared task, the
best reported result for this dataset. Our system is ro-
bust ? it consistently outperforms all publicly avail-
able NER systems (e.g., the Stanford NER system)
on all three datasets.
2 Datasets and Evaluation Methodology
NER system should be robust across multiple do-
mains, as it is expected to be applied on a diverse set
of documents: historical texts, news articles, patent
applications, webpages etc. Therefore, we have con-
sidered three datasets: CoNLL03 shared task data,
MUC7 data and a set of Webpages we have anno-
tated manually. In the experiments throughout the
paper, we test the ability of the tagger to adapt to new
test domains. Throughout this work, we train on the
CoNLL03 data and test on the other datasets without
retraining. The differences in annotation schemes
across datasets created evaluation challenges. We
discuss the datasets and the evaluation methods be-
low.
The CoNLL03 shared task data is a subset of
Reuters 1996 news corpus annotated with 4 entity
types: PER,ORG, LOC, MISC. It is important to
notice that both the training and the development
datasets are news feeds from August 1996, while the
test set contains news feeds from December 1996.
The named entities mentioned in the test dataset are
considerably different from those that appear in the
training or the development set. As a result, the test
dataset is considerably harder than the development
set. Evaluation: Following the convention, we re-
port phrase-level F1 score.
The MUC7 dataset is a subset of the North
American News Text Corpora annotated with a wide
variety of entities including people, locations, or-
ganizations, temporal events, monetary units, and
so on. Since there was no direct mapping from
temporal events, monetary units, and other entities
from MUC7 and the MISC label in the CoNLL03
dataset, we measure performance only on PER,ORG
and LOC. Evaluation: There are several sources
of inconsistency in annotation between MUC7 and
CoNLL03. For example, since the MUC7 dataset
does not contain the MISC label, in the sentence
?balloon, called the Virgin Global Challenger? , the
expression Virgin Global Challenger should be la-
beled as MISC according to CoNLL03 guidelines.
However, the gold annotation in MUC7 is ?balloon,
called the [ORG Virgin] Global Challenger?. These
and other annotation inconsistencies have prompted
us to relax the requirements of finding the exact
phrase boundaries and measure performance using
token-level F1.
Webpages - we have assembled and manually an-
notated a collection of 20 webpages, including per-
sonal, academic and computer-science conference
homepages. The dataset contains 783 entities (96-
loc, 223-org, 276-per, 188-misc). Evaluation: The
named entities in the webpages were highly am-
biguous and very different from the named entities
seen in the training data. For example, the data in-
cluded sentences such as : ?Hear, O Israel, the Lord
our God, the Lord is one.? We could not agree on
whether ?O Israel? should be labeled as ORG, LOC,
or PER. Similarly, we could not agree on whether
?God? and ?Lord? is an ORG or PER. These issues
148
led us to report token-level entity-identification F1
score for this dataset. That is, if a named entity to-
ken was identified as such, we counted it as a correct
prediction ignoring the named entity type.
3 Design Challenges in NER
In this section we introduce the baseline NER sys-
tem, and raise the fundamental questions underlying
robust and efficient design. These questions define
the outline of this paper. NER is typically viewed
as a sequential prediction problem, the typical mod-
els include HMM (Rabiner, 1989), CRF (Lafferty
et al, 2001), and sequential application of Per-
ceptron or Winnow (Collins, 2002). That is, let
x = (x1, . . . , xN ) be an input sequence and y =
(y1, . . . , yN ) be the output sequence. The sequential
prediction problem is to estimate the probabilities
P (yi|xi?k . . . xi+l, yi?m . . . yi?1),
where k, l and m are small numbers to allow
tractable inference and avoid overfitting. This con-
ditional probability distribution is estimated in NER
using the following baseline set of features (Zhang
and Johnson, 2003): (1) previous two predictions
yi?1 and yi?2 (2) current word xi (3) xi word type
(all-capitalized, is-capitalized, all-digits, alphanu-
meric, etc.) (4) prefixes and suffixes of xi (5) tokens
in the window c = (xi?2, xi?1, xi, xi+1, xi+2) (6)
capitalization pattern in the window c (7) conjunc-
tion of c and yi?1.
Most NER systems use additional features, such
as POS tags, shallow parsing information and
gazetteers. We discuss additional features in the fol-
lowing sections. We note that we normalize dates
and numbers, that is 12/3/2008 becomes *Date*,
1980 becomes *DDDD* and 212-325-4751 becomes
*DDD*-*DDD*-*DDDD*. This allows a degree of ab-
straction to years, phone numbers, etc.
Our baseline NER system uses a regularized aver-
aged perceptron (Freund and Schapire, 1999). Sys-
tems based on perceptron have been shown to be
competitive in NER and text chunking (Kazama and
Torisawa, 2007b; Punyakanok and Roth, 2001; Car-
reras et al, 2003) We specify the model and the fea-
tures with the LBJ (Rizzolo and Roth, 2007) mod-
eling language. We now state the four fundamental
design decisions in NER system which define the
structure of this paper.
Algorithm Baseline system Final System
Greedy 83.29 90.57
Beam size=10 83.38 90.67
Beam size=100 83.38 90.67
Viterbi 83.71 N/A
Table 1: Phrase-level F1 performance of different inference
methods on CoNLL03 test data. Viterbi cannot be used in the
end system due to non-local features.
Key design decisions in an NER system.
1) How to represent text chunks in NER system?
2) What inference algorithm to use?
3) How to model non-local dependencies?
4) How to use external knowledge resources in NER?
4 Inference & Chunk Representation
In this section we compare the performance of sev-
eral inference (decoding) algorithms: greedy left-
to-right decoding, Viterbi and beamsearch. It may
appear that beamsearch or Viterbi will perform
much better than naive greedy left-to-right decoding,
which can be seen as beamsearch of size one. The
Viterbi algorithm has the limitation that it does not
allow incorporating some of the non-local features
which will be discussed later, therefore, we cannot
use it in our end system. However, it has the appeal-
ing quality of finding the most likely assignment to
a second-order model, and since the baseline fea-
tures only have second order dependencies, we have
tested it on the baseline configuration.
Table 1 compares between the greedy decoding,
beamsearch with varying beam size, and Viterbi,
both for the system with baseline features and for the
end system (to be presented later). Surprisingly, the
greedy policy performs well, this phenmenon was
also observed in the POS tagging task (Toutanova
et al, 2003; Roth and Zelenko, 1998). The impli-
cations are subtle. First, due to the second-order of
the model, the greedy decoding is over 100 times
faster than Viterbi. The reason is that with the
BILOU encoding of four NE types, each token can
take 21 states (O, B-PER, I-PER , U-PER, etc.). To
tag a token, the greedy policy requires 21 compar-
isons, while the Viterbi requires 213, and this analy-
sis carries over to the number of classifier invoca-
tions. Furthermore, both beamsearch and Viterbi
require transforming the predictions of the classi-
149
Rep. CoNLL03 MUC7
Scheme Test Dev Dev Test
BIO 89.15 93.61 86.76 85.15
BILOU 90.57 93.28 88.09 85.62
Table 2: End system performance with BILOU and BIO
schemes. BILOU outperforms the more widely used BIO.
fiers to probabilities as discussed in (Niculescu-
Mizil and Caruana, 2005), incurring additional time
overhead. Second, this result reinforces the intuition
that global inference over the second-order HMM
features does not capture the non-local properties
of the task. The reason is that the NEs tend to
be short chunks separated by multiple ?outside? to-
kens. This separation ?breaks? the Viterbi decision
process to independent maximization of assignment
over short chunks, where the greedy policy performs
well. On the other hand, dependencies between iso-
lated named entity chunks have longer-range depen-
dencies and are not captured by second-order tran-
sition features, therefore requiring separate mecha-
nisms, which we discuss in Section 5.
Another important question that has been stud-
ied extensively in the context of shallow parsing and
was somewhat overlooked in the NER literature is
the representation of text segments (Veenstra, 1999).
Related works include voting between several rep-
resentation schemes (Shen and Sarkar, 2005), lex-
icalizing the schemes (Molina and Pla, 2002) and
automatically searching for best encoding (Edward,
2007). However, we are not aware of similar work
in the NER settings. Due to space limitations, we do
not discuss all the representation schemes and com-
bining predictions by voting. We focus instead on
two most popular schemes? BIO and BILOU. The
BIO scheme suggests to learn classifiers that iden-
tify the Beginning, the Inside and the Outside of
the text segments. The BILOU scheme suggests
to learn classifiers that identify the Beginning, the
Inside and the Last tokens of multi-token chunks
as well as Unit-length chunks. The BILOU scheme
allows to learn a more expressive model with only
a small increase in the number of parameters to be
learned. Table 2 compares the end system?s perfor-
mance with BIO and BILOU. Examining the results,
we reach two conclusions: (1) choice of encod-
ing scheme has a big impact on the system perfor-
mance and (2) the less used BILOU formalism sig-
nificantly outperforms the widely adopted BIO tag-
ging scheme. We use the BILOU scheme throughout
the paper.
5 Non-Local Features
The key intuition behind non-local features in NER
has been that identical tokens should have identi-
cal label assignments. The sample text discussed
in the introduction shows one such example, where
all occurrences of ?blinker? are assigned the PER
label. However, in general, this is not always the
case; for example we might see in the same doc-
ument the word sequences ?Australia? and ?The
bank of Australia?. The first instance should be la-
beled as LOC, and the second as ORG. We consider
three approaches proposed in the literature in the fol-
lowing sections. Before continuing the discussion,
we note that we found that adjacent documents in
the CoNLL03 and the MUC7 datasets often discuss
the same entities. Therefore, we ignore document
boundaries and analyze global dependencies in 200
and 1000 token windows. These constants were se-
lected by hand after trying a small number of val-
ues. We believe that this approach will also make
our system more robust in cases when the document
boundaries are not given.
5.1 Context aggregation
(Chieu and Ng, 2003) used features that aggre-
gate, for each document, the context tokens appear
in. Sample features are: the longest capitilized se-
quence of words in the document which contains
the current token and the token appears before a
company marker such as ltd. elsewhere in text.
In this work, we call this type of features con-
text aggregation features. Manually designed con-
text aggregation features clearly have low coverage,
therefore we used the following approach. Recall
that for each token instance xi, we use as features
the tokens in the window of size two around it:
ci = (xi?2, xi?1, xi, xi+1, xi+2). When the same
token type t appears in several locations in the text,
say xi1 , xi2 , . . . , xiN , for each instance xij , in ad-
dition to the context features cij , we also aggregate
the context across all instances within 200 tokens:
C = ?j=Nj=1 cij .
150
CoNLL03 CoNLL03 MUC7 MUC7 Web
Component Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + Context Aggregation 85.40 89.99 79.16 71.53 70.76
3) (1) + Extended Prediction History 85.57 90.97 78.56 74.27 72.19
4) (1)+ Two-stage Prediction Aggregation 85.01 89.97 75.48 72.16 72.72
5) All Non-local Features (1-4) 86.53 90.69 81.41 73.61 71.21
Table 3: The utility of non-local features. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and
Webpages. No single technique outperformed the rest on all domains. The combination of all techniques is the most robust.
5.2 Two-stage prediction aggregation
Context aggregation as done above can lead to ex-
cessive number of features. (Krishnan and Manning,
2006) used the intuition that some instances of a to-
ken appear in easily-identifiable contexts. Therefore
they apply a baseline NER system, and use the re-
sulting predictions as features in a second level of in-
ference. We call the technique two-stage prediction
aggregation. We implemented the token-majority
and the entity-majority features discussed in (Krish-
nan and Manning, 2006); however, instead of docu-
ment and corpus majority tags, we used relative fre-
quency of the tags in a 1000 token window.
5.3 Extended prediction history
Both context aggregation and two-stage prediction
aggregation treat all tokens in the text similarly.
However, we observed that the named entities in the
beginning of the documents tended to be more easily
identifiable and matched gazetteers more often. This
is due to the fact that when a named entity is intro-
duced for the first time in text, a canonical name is
used, while in the following discussion abbreviated
mentions, pronouns, and other references are used.
To break the symmetry, when using beamsearch or
greedy left-to-right decoding, we use the fact that
when we are making a prediction for token instance
xi, we have already made predictions y1, . . . , yi?1
for token instances x1, . . . , xi?1. When making the
prediction for token instance xi, we record the la-
bel assignment distribution for all token instances
for the same token type in the previous 1000 words.
That is, if the token instance is ?Australia?, and in
the previous 1000 tokens, the token type ?Australia?
was twice assigned the label L-ORG and three times
the label U-LOC, then the prediction history feature
will be: (L?ORG : 25 ;U ? LOC : 35).
5.4 Utility of non-local features
Table 3 summarizes the results. Surprisingly, no
single technique outperformed the others on all
datasets. The extended prediction history method
was the best on CoNLL03 data and MUC7 test set.
Context aggregation was the best method for MUC7
development set and two-stage prediction was the
best for Webpages. Non-local features proved less
effective for MUC7 test set and the Webpages. Since
the named entities in Webpages have less context,
this result is expected for the Webpages. However,
we are unsure why MUC7 test set benefits from non-
local features much less than MUC7 development
set. Our key conclusion is that no single approach
is better than the rest and that the approaches are
complimentary- their combination is the most stable
and best performing.
6 External Knowledge
As we have illustrated in the introduction, NER is
a knowledge-intensive task. In this section, we dis-
cuss two important knowledge resources? gazetteers
and unlabeled text.
6.1 Unlabeled Text
Recent successful semi-supervised systems (Ando
and Zhang, 2005; Suzuki and Isozaki, 2008) have
illustrated that unlabeled text can be used to im-
prove the performance of NER systems. In this
work, we analyze a simple technique of using word
clusters generated from unlabeled text, which has
been shown to improve performance of dependency
parsing (Koo et al, 2008), Chinese word segmen-
tation (Liang, 2005) and NER (Miller et al, 2004).
The technique is based on word class models, pio-
neered by (Brown et al, 1992), which hierarchically
151
CoNLL03 CoNLL03 MUC7 MUC7 Web
Component Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46
3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26
4) All External Knowledge 88.55 92.49 84.50 83.23 74.44
Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages.
clusters words, producing a binary tree as in Fig-
ure 2.
Figure 2: An extract from word cluster hierarchy.
The approach is related, but not identical, to dis-
tributional similarity (for details, see (Brown et al,
1992) and (Liang, 2005)). For example, since the
words Friday and Tuesday appear in similar con-
texts, the Brown algorithm will assign them to the
same cluster. Successful abstraction of both as a
day of the week, addresses the data sparsity prob-
lem common in NLP tasks. In this work, we use the
implementation and the clusters obtained in (Liang,
2005) from running the algorithm on the Reuters
1996 dataset, a superset of the CoNLL03 NER
dataset. Within the binary tree produced by the al-
gorithm, each word can be uniquely identified by
its path from the root, and this path can be com-
pactly represented with a bit string. Paths of dif-
ferent depths along the path from the root to the
word provide different levels of word abstraction.
For example, paths at depth 4 closely correspond
to POS tags. Since word class models use large
amounts of unlabeled data, they are essentially a
semi-supervised technique, which we use to consid-
erably improve the performance of our system.
In this work, we used path prefixes of length
4,6,10, and 20. When Brown clusters are used as
features in the following sections, it implies that all
features in the system which contain a word form
will be duplicated and a new set of features con-
taining the paths of varying length will be intro-
duced. For example, if the system contains the fea-
ture concatenation of the current token and the sys-
tem prediction on the previous word, four new fea-
tures will be introduced which are concatenations
of the previous prediction and the 4,6,10,20 length
path-representations of the current word.
6.2 Gazetteers
An important question at the inception of the NER
task was whether machine learning techniques are
necessary at all, and whether simple dictionary
lookup would be sufficient for good performance.
Indeed, the baseline for the CoNLL03 shared task
was essentially a dictionary lookup of the enti-
ties which appeared in the training data, and it
achieves 71.91 F1 score on the test set (Tjong and
De Meulder, 2003). It turns out that while prob-
lems of coverage and ambiguity prevent straightfor-
ward lookup, injection of gazetteer matches as fea-
tures in machine-learning based approaches is crit-
ical for good performance (Cohen, 2004; Kazama
and Torisawa, 2007a; Toral and Munoz, 2006; Flo-
rian et al, 2003). Given these findings, several ap-
proaches have been proposed to automatically ex-
tract comprehensive gazetteers from the web and
from large collections of unlabeled text (Etzioni
et al, 2005; Riloff and Jones, 1999) with lim-
ited impact on NER. Recently, (Toral and Munoz,
2006; Kazama and Torisawa, 2007a) have success-
fully constructed high quality and high coverage
gazetteers from Wikipedia.
In this work, we use a collection of 14 high-
precision, low-recall lists extracted from the web
that cover common names, countries, monetary
units, temporal expressions, etc. While these
gazetteers have excellent accuracy, they do not pro-
vide sufficient coverage. To further improve the
coverage, we have extracted 16 gazetteers from
Wikipedia, which collectively contain over 1.5M en-
tities. Overall, we have 30 gazetteers (available
for download with the system), and matches against
152
CoNLL03 CoNLL03 MUC7 MUC7 Web
Component Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + External Knowledge 88.55 92.49 84.50 83.23 74.44
3) (1) + Non-local 86.53 90.69 81.41 73.61 71.21
4) All Features 90.57 93.50 89.19 86.15 74.53
5) All Features (train with dev) 90.80 N/A 89.19 86.15 74.33
Table 5: End system performance by component. Results confirm that NER is a knowledge-intensive task.
each one are weighted as a separate feature in the
system (this allows us to trust each gazetteer to a dif-
ferent degree). We also note that we have developed
a technique for injecting non-exact string matching
to gazetteers, which has marginally improved the
performance, but is not covered in the paper due to
space limitations. In the rest of this section, we dis-
cuss the construction of gazetteers from Wikipedia.
Wikipedia is an open, collaborative encyclopedia
with several attractive properties. (1) It is kept up-
dated manually by it collaborators, hence new enti-
ties are constantly added to it. (2) Wikipedia con-
tains redirection pages, mapping several variations
of spelling of the same name to one canonical en-
try. For example, Suker is redirected to an entry
about Davor S?uker, the Croatian footballer (3) The
entries in Wikipedia are manually tagged with cate-
gories. For example, the entry about the Microsoft
in Wikipedia has the following categories: Companies
listed on NASDAQ; Cloud computing vendors; etc.
Both (Toral and Munoz, 2006) and (Kazama and
Torisawa, 2007a) used the free-text description of
the Wikipedia entity to reason about the entity type.
We use a simpler method to extract high coverage
and high quality gazetteers from Wikipedia. By
inspection of the CoNLL03 shared task annotation
guidelines and of the training set, we manually ag-
gregated several categories into a higher-level con-
cept (not necessarily NER type). When a Wikipedia
entry was tagged by one of the categories in the ta-
ble, it was added to the corresponding gazetteer.
6.3 Utility of External Knowledge
Table 4 summarizes the results of the techniques
for injecting external knowledge. It is important
to note that, although the world class model was
learned on the superset of CoNLL03 data, and al-
though the Wikipedia gazetteers were constructed
Dataset Stanford-NER LBJ-NER
MUC7 Test 80.62 85.71
MUC7 Dev 84.67 87.99
Webpages 72.50 74.89
Reuters2003 test 87.04 90.74
Reuters2003 dev 92.36 93.94
Table 6: Comparison: token-based F1 score of LBJ-NER and
Stanford NER tagger across several domains
based on CoNLL03 annotation guidelines, these fea-
tures proved extremely good on all datasets. Word
class models discussed in Section 6.1 are computed
offline, are available online1, and provide an alter-
native to traditional semi-supervised learning. It is
important to note that the word class models and the
gazetteers and independednt and accumulative. Fur-
thermore, despite the number and the gigantic size
of the extracted gazetteers, the gazeteers alone are
not sufficient for adequate performance. When we
modified the CoNLL03 baseline to include gazetteer
matches, the performance went up from 71.91 to
82.3 on the CoNLL03 test set, below our baseline
system?s result of 83.65. When we have injected the
gazetteers into our system, the performance went up
to 87.22. Word class model and nonlocal features
further improve the performance to 90.57 (see Ta-
ble 5), by more than 3 F1 points.
7 Final System Performance Analysis
As a final experiment, we have trained our system
both on the training and on the development set,
which gave us our best F1 score of 90.8 on the
CoNLL03 data, yet it failed to improve the perfor-
mance on other datasets. Table 5 summarizes the
performance of the system.
Next, we have compared the performance of our
1http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz
153
system to that of the Stanford NER tagger, across the
datasets discussed above. We have chosen to com-
pare against the Stanford tagger because to the best
of our knowledge, it is the best publicly available
system which is trained on the same data. We have
downloaded the Stanford NER tagger and used the
strongest provided model trained on the CoNLL03
data with distributional similarity features. The re-
sults we obtained on the CoNLL03 test set were
consistent with what was reported in (Finkel et al,
2005). Our goal was to compare the performance of
the taggers across several datasets. For the most re-
alistic comparison, we have presented each system
with a raw text, and relied on the system?s sentence
splitter and tokenizer. When evaluating the systems,
we matched against the gold tokenization ignoring
punctuation marks. Table 6 summarizes the results.
Note that due to differences in sentence splitting, to-
kenization and evaluation, these results are not iden-
tical to those reported in Table 5. Also note that in
this experiment we have used token-level accuracy
on the CoNLL dataset as well. Finally, to complete
the comparison to other systems, in Table 7 we sum-
marize the best results reported for the CoNLL03
dataset in literature.
8 Conclusions
We have presented a simple model for NER that
uses expressive features to achieve new state of the
art performance on the Named Entity recognition
task. We explored four fundamental design deci-
sions: text chunks representation, inference algo-
rithm, using non-local features and external knowl-
edge. We showed that BILOU encoding scheme sig-
nificantly outperforms BIO and that, surprisingly, a
conditional model that does not take into account in-
teractions at the output level performs comparably
to beamsearch or Viterbi, while being considerably
more efficient computationally. We analyzed sev-
eral approaches for modeling non-local dependen-
cies and found that none of them clearly outperforms
the others across several datasets. Our experiments
corroborate recently published results indicating that
word class models learned on unlabeled text can
be an alternative to the traditional semi-supervised
learning paradigm. NER proves to be a knowledge-
intensive task, and it was reassuring to observe that
System Resources Used F1
+ LBJ-NER Wikipedia, Nonlocal Fea-
tures, Word-class Model
90.80
- (Suzuki and
Isozaki, 2008)
Semi-supervised on 1G-
word unlabeled data
89.92
- (Ando and
Zhang, 2005)
Semi-supervised on 27M-
word unlabeled data
89.31
- (Kazama and
Torisawa, 2007a)
Wikipedia 88.02
- (Krishnan and
Manning, 2006)
Non-local Features 87.24
- (Kazama and
Torisawa, 2007b)
Non-local Features 87.17
+ (Finkel et al,
2005)
Non-local Features 86.86
Table 7: Results for CoNLL03 data reported in the literature.
publicly available systems marked by +.
knowledge-driven techniques adapt well across sev-
eral domains. We observed consistent performance
gains across several domains, most interestingly in
Webpages, where the named entities had less context
and were different in nature from the named entities
in the training set. Our system significantly outper-
forms the current state of the art and is available to
download under a research license.
Apendix? wikipedia gazetters & categories
1)People: people, births, deaths. Extracts 494,699 Wikipedia
titles and 382,336 redirect links. 2)Organizations: cooper-
atives, federations, teams, clubs, departments, organizations,
organisations, banks, legislatures, record labels, constructors,
manufacturers, ministries, ministers, military units, military
formations, universities, radio stations, newspapers, broad-
casters, political parties, television networks, companies, busi-
nesses, agencies. Extracts 124,403 titles and 130,588 redi-
rects. 3)Locations: airports, districts, regions, countries, ar-
eas, lakes, seas, oceans, towns, villages, parks, bays, bases,
cities, landmarks, rivers, valleys, deserts, locations, places,
neighborhoods. Extracts 211,872 titles and 194,049 redirects.
4)Named Objects: aircraft, spacecraft, tanks, rifles, weapons,
ships, firearms, automobiles, computers, boats. Extracts 28,739
titles and 31,389 redirects. 5)Art Work: novels, books, paint-
ings, operas, plays. Extracts 39,800 titles and 34037 redirects.
6)Films: films, telenovelas, shows, musicals. Extracts 50,454
titles and 49,252 redirects. 7)Songs: songs, singles, albums.
Extracts 109,645 titles and 67,473 redirects. 8)Events: playoffs,
championships, races, competitions, battles. Extracts 20,176 ti-
tles and 15,182 redirects.
154
References
R. K. Ando and T. Zhang. 2005. A high-performance
semi-supervised learning method for text chunking. In
ACL.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D.
Pietra, and J. C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. Learn-
ing a perceptron-based named entity chunker via on-
line recognition feedback. In CoNLL.
H. Chieu and H. T. Ng. 2003. Named entity recognition
with a maximum entropy approach. In Proceedings of
CoNLL.
W. W. Cohen. 2004. Exploiting dictionaries in named
entity extraction: Combining semi-markov extraction
processes and data integration methods. In KDD.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
L. Edward. 2007. Finding good sequential model struc-
tures using output transformations. In EMNLP).
O. Etzioni, M. J. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
J. Kazama and K. Torisawa. 2007a. Exploiting wikipedia
as external knowledge for named entity recognition. In
EMNLP.
J. Kazama and K. Torisawa. 2007b. A new perceptron al-
gorithm for sequence labeling with non-local features.
In EMNLP-CoNLL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
V. Krishnan and C. D. Manning. 2006. An effective two-
stage model for exploiting non-local dependencies in
named entity recognition. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML. Mor-
gan Kaufmann.
P. Liang. 2005. Semi-supervised learning for natural
language. Masters thesis, Massachusetts Institute of
Technology.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In HLT-NAACL.
A. Molina and F. Pla. 2002. Shallow parsing using spe-
cialized hmms. The Journal of Machine Learning Re-
search, 2:595?613.
A. Niculescu-Mizil and R. Caruana. 2005. Predicting
good probabilities with supervised learning. In ICML.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
IEEE.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In AAAI.
N. Rizzolo and D. Roth. 2007. Modeling discriminative
global inference. In ICSC.
D. Roth and D. Zelenko. 1998. Part of speech tagging us-
ing a network of linear separators. In COLING-ACL.
H. Shen and A. Sarkar. 2005. Voting between multiple
data representations for text chunking. Advances in
Artificial Intelligence, pages 389?400.
J. Suzuki and H. Isozaki. 2008. Semi-supervised sequen-
tial labeling and segmentation using giga-word scale
unlabeled data. In ACL.
E. Tjong, K. and F. De Meulder. 2003. Introduction
to the conll-2003 shared task: Language-independent
named entity recognition. In CoNLL.
A. Toral and R. Munoz. 2006. A proposal to automat-
ically build and maintain gazetteers for named entity
recognition by using wikipedia. In EACL.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL.
J. Veenstra. 1999. Representing text chunks. In EACL.
T. Zhang and D. Johnson. 2003. A robust risk mini-
mization based named entity recognition system. In
CoNLL.
155
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1234?1244, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning-based Multi-Sieve Co-reference Resolution with Knowledge?
Lev Ratinov
Google Inc.?
ratinov@google.com
Dan Roth
University of Illinois at Urbana-Champaign
danr@illinois.edu
Abstract
We explore the interplay of knowledge and
structure in co-reference resolution. To inject
knowledge, we use a state-of-the-art system
which cross-links (or ?grounds?) expressions
in free text to Wikipedia. We explore ways
of using the resulting grounding to boost the
performance of a state-of-the-art co-reference
resolution system. To maximize the utility of
the injected knowledge, we deploy a learning-
based multi-sieve approach and develop novel
entity-based features. Our end system outper-
forms the state-of-the-art baseline by 2 B3 F1
points on non-transcript portion of the ACE
2004 dataset.
1 Introduction
Co-reference resolution is the task of grouping men-
tions to entities. For example, consider the text snip-
pet in Fig. 11. The correct output groups the men-
tions {m1,m2,m5} to one entity while leaving m3
?We thank Nicholas Rizzolo and Kai Wei Chang for their
invaluable help with modifying the baseline co-reference sys-
tem. We thank the anonymous EMNLP reviewers for con-
structive comments. This research was supported by the Army
Research Laboratory (ARL) under agreement W911NF-09-2-
0053 and by the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, conclusions or recommendations are
those of the authors and do not necessarily reflect the view of
the ARL, DARPA, AFRL, or the US government.
? The majority of this work was done while the first author
was at the University of Illinois.
1Throughout this paper, curly brackets {} denote the extent
and square brackets [] denote the head.
?After the {[vessel]}m1 suffered a catastrophic torpedo
detonation, {[Kursk]}m2 sank in the waters of {[Barents
Sea]}m3 with all hands lost. Though rescue attempts were
offered by a nearby {Norwegian [ship]}m4 , Russia declined
initial rescue offers, and all 118 sailors and officers aboard
{[Kursk]}m5 perished.?
Figure 1: Example illustrating the challenges in co-reference
resolution.
and m4 as singletons. Resolving co-reference is fun-
damental for understanding natural language. For
example in Fig. 1, to infer that Kusrk has suffered
a torpedo detonation, we have to understand that
{[vessel]}m1 refers to {[Kursk]}m2.
This inference is typically trivial for humans, but
proves extremely challenging for state-of-the-art co-
reference resolution systems. We believe that it is
world knowledge that gives people the ability to un-
derstand text with such ease. A human reader can in-
fer that since Kursk sank, it must be a vessel and ves-
sels which suffer catastrophic torpedo detonations
can sink. Moreover, some readers might just know
that Kursk is a Russian submarine named after the
city of Kursk, where the largest tank battle in his-
tory took place in 1943. In this work we are using
Wikipedia as a source of encyclopedic knowledge.
The key contributions of this work are:
(1) Using Wikipedia to assign a set of knowledge
attributes to mentions in a context-sensitive way.
For example, for the text in Fig. 1, our system as-
signs to the mention ?Kursk? the nationalities: Rus-
sian, Soviet and the attributes ship, incident, subma-
rine, shipwreck (as opposed to city or battle). We
are using a publicly available system for context-
1234
sensitive disambiguation to Wikipedia. We then
extract attributes from the cross-linked Wikipedia
pages (described in Sec. 3.1), assign these attributes
to the document mentions (Sec. 3.2) and develop
knowledge-rich compatibility metric between men-
tions (Sec. 3.3)2.
(2) Integrating the strength of rule-based systems
such as (Haghighi and Klein, 2009; Raghunathan et
al., 2010) into a machine learning framework. We
are using a multi-sieve approach (Raghunathan et
al., 2010), which splits pairwise ?co-reference? vs.
?non-coreference? decisions to different types and
attempts to make the easy decisions first (Goldberg
and Elhadad, 2010). Our multi-sieve approach is
different from (Raghunathan et al 2010) in sev-
eral respects: (a) our sieves are machine-learning
classifiers, (b) the same pair of mentions can fall
into multiple sieves, (c) later sieves can override
the decisions made by earlier sieves, allowing to re-
cover from errors as additional evidence becomes
available. In our running example, the decision
of whether {[vessel]}m1 refers to {[Kursk]}m2 is
made before the decision of whether {[vessel]}m1
refers to {Norwegian [ship]}m4 since decisions in
the same sentence are believed to be easier than
cross-sentence ones. We describe our learning-
based multi-sieve approach in Sec. 4.
(3) A novel approach for entity-based features. As
sieves of classifiers are applied, our system attempts
to model entities and share the attributes between the
mentions belonging to the same entity. Once the de-
cision that {[vessel]}m1 and {[Kursk]}m2 co-refer is
made, we want the two mentions to share the Rus-
sian nationality. This allows us to avoid erroneously
linking {[vessel]}m1 to {Norwegian [ship]}m4 de-
spite vessel and ship being synonyms in Word-
Net. However, in this work we allow the sieves to
make conflicting decisions on the same pair of men-
tions. Hence, obtaining entities and their attributes
by straightforward transitive closure of co-reference
predictions is impossible. We describe our approach
for leveraging possibly contradicting predictions in
Sec. 5.
(4) By adding word-knowledge features and us-
2The extracted attributes and the related re-
sources are available for public download at
http://cogcomp.cs.illinois.edu/Data/
Ace2004CorefWikiAttributes.zip
Input: document d; mentions M = {m1, . . . ,mN}
1) For each mi ? M , assign it a Wikipedia page pi in a
context-sensitive way (pi may be null).
- If pi 6= null: extract knowledge attributes from pi and
assign to m.
- Else extract knowledge attributes directly from m via
noun-phrase parsing techniques (Vadas and Curran, 2008).
3) Let Q = {(mi,mj)}i6=j , be the queue of mention
pairs approximately sorted by ?easy-first? (Goldberg and
Elhadad, 2010).
4) Let G be a partial clustering graph.
5) While Q is not empty
- Extract a pair p = (mi,mj) from Q.
- Using the knowledge attributes of mi and mj as well as
the structure of G, classify whether p is co-referent.
- Update G with the classification decision.
6) Construct an end clustering from G.
Figure 2: High-level system architecture.
ing learning-based multi-sieve approach, we im-
prove the performance of the state-of-the-art system
of (Bengtson and Roth, 2008) by 3 MUC, 2 B3 and
2 CEAF F1 points on the non-transcript portion of
the ACE 2004 dataset. We report our experimen-
tal results in Sec. 6 and conclude with discussion in
Sec. 7.
We conclude the introduction by giving a high-
level overview of our system in Fig. 2.
2 Baseline System
In this work, we are using the state-of-the-art sys-
tem of (Bengtson and Roth, 2008), which relies
on a pairwise scoring function pc to assign an or-
dered pair of mentions a probability that they are
coreferential. It uses a rich set of features includ-
ing: string edit distance, gender match, whether the
mentions appear in the same sentence, whether the
heads are synonyms in WordNet etc. The function
pc is modeled using regularized averaged percep-
tron for a tuned number of training rounds, learn-
ing rate and margin. For the end system, we keep
these parameters intact, our only modifications will
be adding knowledge-rich features and adding inter-
mediate classification sieves to the training and the
inference, which we will discuss in the following
sections.
At inference time, given a document d and a
pairwise co-reference scoring function pc, (Bengt-
son and Roth, 2008) generate a graph Gd accord-
1235
ing to the Best-Link decision model (Ng and Cardie,
2002) as follows. For each mention m in docu-
ment d, let Bm be the set of mentions appearing
before m in d. Let a be the highest scoring an-
tecedent: a = argmaxb?Bm(pc(b,m)). We will add
the edge (a,m) to Gd if pc(a,m) predicts the pair to
be co-referent with a confidence exceeding a chosen
threshold, then we take the transitive closure3.
The properties of the Best-Link inference are il-
lustrated in Fig. 3. At this stage, we ask the reader
to ignore the knowledge attributes at the bottom of
the figure. Let us assume that the pairwise classi-
fier labeled the mentions (m2,m5) co-referent be-
cause they have identical surface form; mentions
(m1,m4) are co-referred because the heads are syn-
onyms in WordNet. Let us assume that since m1
and m2 appear in the same sentence, the pairwise
classifier managed to leverage the dependency parse
tree to correctly co-ref the pair (m1,m2). The tran-
sitive closure would correctly link (m1,m5) despite
the incorrect prediction of the pairwise classifier on
(m1,m5), and would incorrectly link m4 with all
other mentions because of the incorrect pairwise
prediction on (m1,m4) and despite the correct pre-
dictions on (m2,m4) and (m4,m5).
Figure 3: A sample output of a pairwise co-reference classifier.
The full edges represent a co-ref prediction and the empty edges
represent a non-coref prediction. A set of knowledge attributes
for selected mentions is shown as well.
3 Wikipedia as Knowledge
In this section we describe our methodology for us-
ing Wikipedia as a knowledge resource. In Sec. 3.1
we cover the process of knowledge extraction from
3We use Platt Scaling while (Bengtson and Roth, 2008) used
the raw output value of pc.
Wikipedia pages. We describe how to inject this
knowledge into mentions in Sec. 3.2. The bottom
part of Fig. 3 illustrates the knowledge attributes our
system injects to two sample mentions at this stage.
Finally, in Sec. 3.3 we describe a compatibility met-
ric our system learns over the injected knowledge.
3.1 Wikipedia Knowledge Attributes
Our goal in this section is to extract from Wikipedia
pages a compact and highly-accurate set of knowl-
edge attributes, which nevertheless possesses dis-
criminative power for co-reference4. We concentrate
on three types of knowledge attributes: fine-grained
semantic categories, gender information and nation-
ality where applicable.
Each Wikipedia page is assigned a set of cat-
egories. There are over 100K categories in
Wikipedia, many are extremely fine-grained and
contain very few pages. The value of the Wikipedia
category structure for knowledge acquisition has
long been noticed in several influential works, such
as (Suchanek et al 2007; Nastase and Strube, 2008)
to name a few. However, while the recall of the
above resources is excellent, we found their preci-
sion insufficient for our purposes. We have imple-
mented a simple high-precision low-recall heuris-
tic for extracting the head words of Wikipedia cat-
egories as follows.
We noticed that Wikipedia categories have a sim-
ple structure of either <noun-phrase> or <noun-
phrase><relation-token><noun-phrase>, where
in the second case the category information is al-
ways on the left. Therefore, we first remove the
text succeeding a set of carefully chosen relation to-
kens5. With this heuristic ?Recipients of the Gold
Medal of the Royal Astronomical Society? becomes
just ?Recipients?; ?Populated places in Africa? be-
comes ?places?; however ?Institute for Advanced
Study faculty? becomes ?Institute? (rather than
?faculty?). At the second step, we apply the Illi-
nois POS tagger and keep only the tokens labeled as
NNS. This step allows us to exclude singular nouns
incorrectly identified as heads, such as ?Institute?
above. To further reduce the noise in the category
4We justify the reasons for our choice of high-precision low-
recall knowledge extraction in Sec. 3.2.
5The selected set was: {of, in, with, from, ?,?, at, who,
which, for, and, by}
1236
extraction, we also remove all rare category tokens
which appeared in less than 100 titles ending up with
2088 fine-grained entity types. We manually map
popular fine-grained categories to coarser-grained
ones, more consistent with ACE entity typing. A
sample of the mapping is shown in the table below:
Fine-grained Coarse-grained
departments, organizations, banks, . . . ORG
venues, trails, areas, buildings, . . . LOC
countries, towns, villages, . . . GPE
churches, highways, schools, . . . FACILITY
Manual inspection of the extracted category key-
words has led us to believe that this heuristic
achieves a higher precision at a considerable loss
of recall when compared to the more sophisticated
approach of (Nastase and Strube, 2008), which
correctly identifies ?faculty? as the head of ?Insti-
tute for Advanced Study faculty?, but incorrectly
identifies ?statistical organizations? as the head of
?Presidents of statistical organizations? in about
half the titles containing the category6.
We assign gender to the titles using the follow-
ing simple heuristic. The first paragraph of each
Wikipedia article provides a very brief summary
of the entity in focus. If the first paragraph of a
Wikipedia page contains the pronoun ?she?, but not
?he?, the article is considered to be about a female
(and vice-versa). However, when the page is as-
signed a non-person-related fine-grained NE type
(e.g. school) and at the same time is not assigned
a person-related fine-grained NE type (e.g. novel-
ist), we mark the page as inanimate regardless of the
presence of he/she pronouns. The nationality is as-
signed by matching the tokens in the original (un-
processed) categories of the Wikipedia page to a list
of countries. We assign nationality not only to the
Wikipedia titles, but also to single tokens. For each
token, we track the list of titles it appears in, and if
the union of the nationalities assigned to the titles it
appears in is less than 7, we mark the token compat-
ible with these nationalities. This allows us to iden-
tify Ivan Lebedev as Russian and Ronen Ben-Zohar
as Israeli, even though Wikipedia may not contain
pages for these specific people.
6 (Nastase and Strube, 2008) analyze a set of categories S
assigned to Wikipedia page p jointly, hence the same category
expression can be interpreted differently, depending on S.
3.2 Injecting Knowledge Attributes
Once we have extracted the knowledge attributes of
Wikipedia pages, we need to inject them into the
mentions. (Rahman and Ng, 2011) used YAGO for
similar purposes, but noticed that knowledge injec-
tion is often noisy. Therefore they used YAGO only
for mention pairs where one mention was an NE
of type PER/LOC/ORG and the other was a com-
mon noun. This implies that all MISC NEs were
discarded, and all NE-NE pairs were discarded as
well. We also note that (Rahman and Ng, 2011)
reports low utility of FrameNet-based features. In
fact, when incrementally added to other features in
cluster-ranking model the FrameNet-based features
sometimes led to performance drops. This observa-
tion has motivated our choice of high-precision low-
recall heuristic in Sec. 3.1 and will motivate us to
add features conservatively when building attribute
compatibility metric in Sec. 3.3.
Additionally, while (Rahman and Ng, 2011) uses
the union of all possible meanings a mention may
have in Wikipedia, we deploy GLOW (Ratinov et
al., 2011)7, a context-sensitive system for disam-
biguation to Wikipedia. Using context-sensitive dis-
ambiguation to Wikipedia as well as high-precision
set of knowledge attributes allows us to inject the
knowledge to more mention pairs when compared
to (Rahman and Ng, 2011). Our exact heuristic for
injecting knowledge attributes to mentions is as fol-
lows:
Named Entities with Wikipedia Disambiguation
If the mention head is an NE matched to a Wikipedia
page p by GLOW, we import all the knowledge at-
tributes from p. GLOW allows us to map ?Ephraim
Sneh? to http://en.wikipedia.org/wiki/Efraim Sneh
and to assign it the Israeli nationality, male gender,
and the fine-grained entity types: {member, politi-
cian, person, minister, alumnus, physician, gen-
eral}.
Head and Extent Keywords
If the mention head is not mapped to Wikipedia by
GLOW and the head contains keywords which ap-
pear in the list of 2088 fine-grained entity types,
then the rightmost such keyword is added to the list
of mention knowledge attributes. If the head does
7Available at: http://cogcomp.cs.illinois.
edu/page/software_view/Wikifier
1237
not contain any entity-type keywords but the extent
does, we add the rightmost such keyword of the ex-
tent. In both cases, we apply the heuristic of re-
moving clauses starting with a select set of punctua-
tions, prepositions and pronouns, annotating what is
left with POS tagger and restricting to noun tokens
only8. This allows us to inject knowledge to men-
tions unmapped to Wikipedia, such as: ?{current
Cycle World publisher [Larry Little]}?, which is as-
signed the attribute publisher but not world or cy-
cle. Likewise, ?{[Joseph Conrad Parkhurst], who
founded the motorcycle magazine Cycle World in
1962 }?, is not assigned the attribute magazine,
since the text following ?who? is discarded.
3.3 Learning Attributes Compatibility
In the previous section we have assigned knowledge
attributes to the mentions. Some of this information,
such as gender and coarse-grained entity types are
also modeled in the baseline system of (Bengtson
and Roth, 2008). Our goal is to build a compatibility
metric on top of this redundant, yet often inconsis-
tent information.
The majority of the features we are using are
straightforward, such as: (1) whether the two men-
tions mapped to the same Wikipedia page, (2)
gender agreement (both Wikipedia and dictionary-
based), (3) nationality agreement (here we measure
only whether the sets intersect, since mentions can
have multiple nationalities in the real world), (4)
coarse-grained entity type match, etc.
The only non-trivial feature is measuring com-
patibility between sets of fine-grained entity types,
which we describe below. Let us assume that men-
tion m1 was assigned the set of fine-grained entity
types S1 and the mention m2 was assigned the set
of fine-grained entity types S2. We record whether
S1 and S2 share elements. If they do, than, in addi-
tion to the Boolean feature, the list of the shared el-
ements also appears as a list of discrete features. We
do the same for the most similar and most dissimilar
elements of S1 and S2 (along with their discretized
similarity score) according to a WordNet-based sim-
ilarity metric of (Do et al 2009). The reason for ex-
plicitly listing the shared, the most similar and dis-
8This heuristic is similar to the one we used for extracting
Wikipedia category headwords and seems to be a reasonable
baseline for parsing noun structures (Vadas and Curran, 2008).
similar elements is that the WordNet similarity does
not always correspond to co-reference compatibil-
ity. For example, the pair (company, rival) has a
low similarity score according to WordNet, but char-
acterizes co-referent mentions. On the other hand,
the pair (city, region) has a high WordNet simi-
larity score, but characterizes non-coreferent men-
tions. We want to allow our system to ?memorize?
the discrepancy between the WordNet similarity and
co-reference compatibility of specific pairs.
We also note that we generate a set of selected
conjunctive features, most notably of fine-grained
categories with NER predictions. The reason is
that the pair of mentions ?(Microsoft, Google)? are
not co-referent despite the fact that they both have
the company attribute. On the other hand ?(Mi-
crosoft, Redmond-based company)? is a co-referent
pair. To capture this difference, we generate the
feature ORG-ORG&&share attribute for the first
pair, and ORG-O&&share attribute for the second
pair9. These features are also used in conjunction
with string edit distance. Therefore, if our system
sees two named entities which share the same fine-
grained type but have a large string edit distance, it
will label the pair as non-coref.
4 Learning-based Multi-Sieve Aproach
State-of-the-art machine-learning co-ref systems,
e.g. (Bengtson and Roth, 2008; Rahman and Ng,
2011) train a single model for predicting co-
reference of all mention pairs. However, rule-based
systems, e.g. (Haghighi and Klein, 2009; Raghu-
nathan et al 2010) characterize mention pairs by
discourse structure and linguistic properties and ap-
ply rules in a prescribed order (high-precision rules
first). Somewhat surprisingly, such hybrid approach
of applying rules on top of structures produced by
statistical tools (such as dependency parse trees) per-
forms better than pure machine-learning approach10.
In this work, we attempt to integrate the strength
of linguistically motivated rule-based systems with
the robustness of a machine learning approach. We
started with a hypothesis that different types of men-
9The head of ?Redmond-based company? is ?company?,
which is not a named entity, and is marked O.
10(Raghunathan et al 2010) recorded the best result on
CoNLL 2011 shared task.
1238
tion pairs may require a different co-ref model. For
example, consider the text below:
Queen Rania of Jordan , Egypt?s [Suzanne Mubarak]m1 and
others were using their charisma and influence to campaign
for equality of the sexes. [Mubarak]m2 , wife of Egyptian
President [Hosni Mubarak]m3 , and one of the conference
organizers, said they must find ways to . . .
There is a subtle difference between mention pairs
(m1,m2) and (m2,m3). One of the differences is
purely structural. The first pair appears in different
sentences, while the second pair ? in the same sen-
tence. It turns out that string edit distance feature be-
tween two named entities has different ?semantics?
depending on whether the two mentions appear in
the same sentence. The reason is that to avoid redun-
dancy, humans refer to the same entity differently
within the sentence, preferring titles, nicknames and
pronouns. Therefore, when a similar-looking named
entities appear in the same sentence, they are ac-
tually likely to refer to different entities. On the
other hand, in the sentence ?Reggie Jackson, nick-
named Mr. October . . . ? we have to rely heavily on
sentence structure rather than string edit distance to
make the correct co-ref prediction.
Trained on Sieve-specific
Sieve All Data Training
AllSentencePairs 61.37 67.46
ClosestNonProDiffSent 60.71 63.33
NonProSameSentence 62.97 63.80
NerMentionsDiffSent 86.44 87.12
SameSentenceOneNer 64.10 68.88
Adjacent 71.00 78.80
SameSenBothNer 75.30 73.75
Nested 76.11 79.00
Table 1: F1 performance on co-referent mention pairs by sieve
type when trained with all data versus sieve-specific data only.
Our second intuition is that ?easy-first? inference
is necessary to effectively leverage knowledge. For
example, in Fig. 3, our goal is to link vessel to
Kursk and assign it the Russian/Soviet nationality
prior to applying the pairwise co-reference classi-
fier on (vessel, Norwegian ship). Therefore, our
goal is to apply the pairwise classifier on pairs in
prescribed order and to propagate the knowledge
across mentions. The ordering should be such that
(a) maximum amount of information is injected at
early stages (b) the precision at the early stages is as
high as possible (Raghunathan et al 2010). Hence,
we divide the mention pairs as follows:
Nested: are pairs such as ?{{[city]m1} of [Jerusalem]m2}?
where the extent of one of the mentions contains the extent of
the other. For some mentions, the extent is the entire clause, so
we also added a requirement that mention heads are at most 7
tokens apart. Intuitively, it is the easiest case of co-reference.
There are 5,804 training samples and 992 testing samples, out
of which 208 are co-referent.
SameSenBothNer: are pairs of named entities which appear
in the same sentence. We already saw an example for this
case involving [Mubarak]m2 and [Hosni Mubarak]m3. There
are 13,041 training samples and 1,746 testing samples, out of
which 86 are co-referent.
Adjacent: are pairs of mentions which appear closest to each
other on the dependency tree. We note that most of the nested
pairs are also adjacent. There are training 5,872 samples and
895 testing samples, out of which 219 are co-referent.
SameSentenceOneNer: are pairs which appear in the same
sentence and exactly one of the mentions is a named entity, and
the other is not a pronoun. Typical pairs are ?Israel-country?,
as opposed to ?Bill Clinton - reporter?. This type of pairs is
fairly difficult, but our hope is to use encyclopedic knowledge
to boost the performance. There are 15,715 training samples
and 2,635 testing samples, out of which 207 are co-referent.
NerMentionsDiffSent: are pairs of mentions in different sen-
tences, both of which are named entities. There are 189,807
training samples and 24,342 testing samples, out of which 1,628
are co-referent.
NonProSameSentence: are pairs in the same sentence, where
both mentions are non-pronouns. This sieve includes all the
pairs in the SameSentenceOneNer sieve. Typical pairs are
?city-capital? and ?reporter-celebrity?. There are 33,895
training samples and 5,393 testing samples, out of which 336
are co-referent..
ClosestNonProDiffSent: are pairs of mentions in different sen-
tences with no other mentions between the two. 3,707 train-
ing samples and 488 testing samples, out of which 38 are co-
referent.
AllSentencePairs: All mention pairs within same sentence.
There are 49,953 training samples and 7,809 testing samples,
out of which 846 are co-referent.
TopSieve: The set of mention pairs classified by the baseline
system. 525,398 training samples and 85,358 testing samples,
out of which 1,387 are co-referent.
In Tab. 1 we compare the performance at each
sieve in two scenarios11. First, we train with the en-
tire 525,398 training samples, and then we train on
11The data is described in Sec. 6.1.
1239
whatever training data is available for the specific
sieve12. We were surprised to see that the F1 on the
nested mentions, when trained on the 5,804 sieve-
specific samples improves to 79.00 versus 76.11
when trained on the 525,398 top sieve samples.
There are several things to note when interpreting
the results in Tab 1. First, the sheer ratio of positive
to negative samples fluctuates drastically. For exam-
ple, 208 out of the 992 testing samples at the nested
sieve are positive, while there are only 86 positive
samples out of 1,746 testing samples in the Same-
SenBothNer sieve. It seems unreasonable to use the
same model for inference at both sieves. Second, the
data for intermediate sieves is not always a subset of
the top sieve. The reason is that top sieve extracts
a positive instance only for the closest co-referent
mentions, while sieves such as AllSentencePairs ex-
tract samples for all co-referent pairs which appear
in the same sentence. Third, while our division to
sieves may resemble witchcraft, it is motivated by
the intuition that mentions appearing close to one
another are easier instances of co-ref as well as lin-
guistic insights of (Raghunathan et al 2010).
5 Entity-Based Features
In this section we describe our approach for build-
ing entity-based features. Let {C1, C2, . . . CN} be
the set of sieve-specific classifiers. In our case, C1 is
the nested mention pairs classifier, C2 is the Same-
SenBothNer classifier, and C9 is the top sieve clas-
sifier. We design entity-based features so that the
subsequent sieves ?see? the decisions of the previ-
ous sieves and use entity-based features based on the
intermediate clustering. However, unlike (Raghu-
nathan et al 2010), we allow the subsequent sieves
to change the decisions made by the lower sieves
(since additional information becomes available).
5.1 Intermediate Clustering Features (IC)
Let Ri(m) be the set of all mentions which, when
paired with the mention m, form valid sample pairs
for sieve i. E.g. in our running example of Fig. 1,
12We report pairwise performance on mention pairs because
it is the more natural metric for the intermediate sieves. We
report only performance on co-referent pairs, because for many
sieves, such as the top sieve, 99% of the mention pairs are non-
coreferent, hence the baseline of labeling all samples as non-
coreferent would result in 99% accuracy. We are interested in a
more challenging baseline, the co-referent pairs.
R2([Kursk]m2) = {[Barents Sea]m3}, since both
m1 and m2 are NEs and appear in the same sen-
tence. Let R+i (m) be the set of all mentions which
were labeled as co-referent to the mention m by the
classifier Ci (including m, which is co-referent to
itself). We define R?i (m) similarly. We denote the
union of mentions co-refed to m during inference
up to sieve i as E+i (m) = ?
i?1
j=1R
+
j (m). Similarly,
E?i (m) = ?
i?1
j=1R
?
j (m). Using these definitions
we can introduce entity-based prediction features
which allow subsequent sieves to use information
aggregated from previous sieves:
ICRi (mj ,mk) =
?
?
?
?1 mj ? R?i?1(mk)
+1 mj ? R+i?1(mk)
0 Otherwise
ICEi (mj ,mk) =
?
?
?
?1 mj ? E?i?1(mk)
+1 mj ? E+i?1(mk)
0 Otherwise
ICRi stores the pairwise prediction history, thus
when classifying a pair (mj ,mk) at sieve i, a
classifier can see the predictions of all the previous
sieves applicable on that pair. ICEi stores the
transitive closures of the sieve-specific predictions.
We note that both ICRi and IC
E
i can have the values
+1 and -1 active at the same time if intermediate
sieve classifiers generated conflicting predictions.
However, a classifier at sieve i will use as features
both ICR1 ,. . . IC
R
i?1 and IC
E
1 ,. . . IC
E
i?1, thus it
will know the lowest sieve at which the conflicting
evidence occurs. The classifier at sieve i also
uses set identity, set containment, set overlap and
other set comparison features between E+/?i?1 (mj)
and E+/?i?1 (mk). We check whether the sets have
symmetric difference, whether the size of the
intersection between the two sets is at least half
the size of the smallest set etc. We also generate
subtypes of set comparison features when restricting
the elements to NE-mentions and non-pronominal
mentions (e.g ?what percent of named entities do
the sets have in common??).
5.2 Surface Form Compatibility (SFC)
The intermediate clustering features do not allow us
to generalize predictions from pairs of mentions to
pairs of surface strings. For example, if we have
three mentions: {[vessel]m1 , [Kursk]m2 , [Kursk]m5},
then the prediction on the pair (m1,m2) will not be
1240
(B)aseline (B)+Knowledge (B)+Predictions (B)+Knowledge+Predictions
TopSieve 66.58 69.08 68.77 70.43
AllSentencePairs 67.46 71.79 69.59 73.50
ClosestNonProDiffSent 63.33 65.62 65.57 70.76
NonProSameSentence 63.80 69.62 67.03 71.11
NerMentionsDiffSent 87.12 88.23 88.68 89.07
SameSentenceOneNer 68.88 70.58 67.89 73.17
Adjacent 78.80 81.32 80.00 81.79
SameSenBothNer 73.75 80.50 77.21 80.98
Nested 79.00 83.59 80.65 83.37
Table 2: Utility of knowledge and prediction features (F1 on co-referent mention pairs) by inference sieves. Both knowledge and
entity-based features significantly and independently improve the performance for all sieves. The goal of entity-based features is
to propagate knowledge effectively, thus it is encouraging that the combination of entity-based and knowledge features performed
significantly better than any of the approaches individually at the top sieve.
used for the prediction on the pair (m1,m5), even
though in both pairs we are asking whether Kursk
can be referred to as vessel. The surface form com-
patibility features mirror the intermediate clustering
features, but relax mention IDs and replace them
by surface forms. Similarly to intermediate cluster-
ing features, both +1 and -1 values can be active at
the same time. We also generate subtypes of set-
comparison features for NE-mentions and optionally
stemmed non-pronominal mentions. For example,
in a text discussing President Clinton and President
Putin, some instances of the surface from president
will refer to Putin but not Clinton and vice-versa.
Therefore, both for (Putin, president) and for (Clin-
ton, president), the surface from compatibility will
be +1 and -1 simultaneously. This indicates to the
system that Putin can be referred to as president, but
president can refer to other entities in the document
as well.
6 Experiments and Results
6.1 Data
We use the official ACE 2004 English training
data (NIST, 2004). We started with the data split
used in (Culotta et al 2007), which used 336 doc-
uments for training and 107 documents for testing.
We note that ACE data contains both newswire text
and transcripts. In this work, we are using NLP tools
such as POS tagger, named entity recognizer, shal-
low parser, and a disambiguation to Wikipedia sys-
tem to inject expressive features into a co-reference
system.
Unfortunately, current state-of-the-art NLP tools
do not work well on transcribed text. Therefore, we
discard all the transcripts. Our criteria was simple.
The ACE annotators have marked the named enti-
ties both in newswire and in the transcripts. We kept
only those documents which contained named en-
tities (according to manual ACE annotation) and at
least 1/3 of the named entities started with a capital
letter. After this pre-processing step, we were left
with 275 out of the original 336 training documents,
and 42 out of the 107 testing documents.
For the experiments throughout this paper, fol-
lowing Culotta et al(Culotta et al 2007) and much
other work, to make experiments more compara-
ble across systems, we assume that perfect mention
boundaries and mention type labels are given. How-
ever, we do not use the gold named entity types such
as person/location/facility etc. available in the data.
In all experiments we automatically split words and
sentences, and annotate the text with part-of-speech
tags, named entities and cross-link concepts from
the text to Wikipedia using publicly available tools.
6.2 Ablation Study
In Tab. 2 we report the pairwise F1 scores on co-
referent mention pairs broken down by sieve and
using different components. This allows us to see,
for example, that adding only the knowledge at-
tributes improved the performance at NonProSame-
Sentence sieve from 63.80 to 69.62. We have or-
dered the sieves according to our initial intuition of
?easy first?. We were surprised to see that co-ref res-
olution for named entities in the same sentence was
harder than cross-sentence (73.75 vs. 87.12 base-
1241
 75
 76
 77
 78
 79
 80
 81
 82
0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0
F1
 - M
UC
Confidence threshold for a positive prediction
Baseline
Knowledge&Predictions
KnowledgeOnly
PredictionsOnly
 81.5
 82
 82.5
 83
 83.5
 84
 84.5
 85
 85.5
0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0
F1
 - B
3
Confidence threshold for a positive prediction
Baseline
Knowledge&Predictions
KnowledgeOnly
PredictionsOnly
 73
 74
 75
 76
 77
 78
 79
 80
0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0
F1
 - C
EA
F
Confidence threshold for a positive prediction
Baseline
Knowledge&Predictions
KnowledgeOnly
PredictionsOnly
Figure 4: End performance for various systems.
line F1). We were also surprised to see that resolv-
ing all mention pairs within sentence when includ-
ing pronouns was easier than resolving pairs where
both mentions were non-pronouns (67.46 vs. 63.80
baseline F1).
We note that conceptually, the nested
(B)+Predictions sieve should be identical to
the baseline. However, in practice, the surface
form compatibility (SFC) features are generated
for the nested sieve as well. Given two mentions
m1 and m2, the SFC features capture how many
surface forms E+(m1) and E+(m2) share. At the
nested sieve, E+(m) and R+(m) are just m, which
is identical to string comparison features already
existing in the baseline system. While the SFC
features do not add new information, they influence
the weight the features get (essentially leading to
a different regularization), which in turn leads to
slightly different results.
6.3 End system performance
Recall that the Best-Link algorithm applies transi-
tive closure on the graph generated by thresholding
the pairwise co-reference scoring function pc. The
lower the threshold on the positive prediction, the
lower is the precision and the higher is the recall. In
Fig. 4 we compare the end clustering quality across
a variety of thresholds and for various system fla-
vors using three metrics: MUC (Vilain et al 1995),
B3 (Bagga and Baldwin, 1998) and CEAF (Luo,
2005)13. The purpose of this comparison is to see
the impact of the knowledge and the prediction fea-
tures on the final output and to see whether the per-
formance gains are due to (mis-)tuning of one of
the systems or are they consistent across a variety
of thresholds.
The end performance of the baseline system
on our training/testing split peaks at around 78.39
MUC, 83.03 B3 and 77.52 CEAF, which is higher
(e.g. 3 B3 F1 points) than the originally reported
result on the entire dataset (which includes the tran-
scripts). This is expected, since well-formed text is
easier to process than transcripts. We note that our
baseline is a state-of-the art system which recorded
the highest B3 and BLANC scores at CoNLL 2011
shared task and took the third place overall. Fig. 4
shows a minimum improvement of 3 MUC, 2 B3
and 1.25 CEAF F1 points across all thresholds when
comparing the baseline to our end system. Surpris-
ingly, the knowledge features outperformed predic-
tion features on pairwise, MUC and B3 metrics, but
not on the CEAF metric. This shows that pairwise
performance is not always indicative of cluster-level
performance for all metrics.
7 Conclusions and Related Work
To illustrate the strengths of our approach, let us
consider the following text:
Another terminal was made available in {[Jiangxi]m1}, an
{inland [province]m2}. . . . The previous situation whereby
large amount of goods for {Jiangxi [province]m3} had to
be re-shipped through Guangzhou and Shanghai will be
changed completely.
The baseline system assigns each mention to a
separate cluster. The pairs (m1,m2) and (m1,m3)
13In the interest of space, we refer the reader to the literature
for details about the different metrics.
1242
are misclassified because the baseline classifier does
not know that Jiangxi is a province and the preposi-
tion an before m2 is interpreted to mean it is a pre-
viously unmentioned entity. The pair (m2,m3) is
misclassified because identical heads have different
modifiers, as in (big province, small province). Our
end system first co-refs (m1,m2) at the AllSameSen-
tence sieve due to the knowledge features, and then
co-refs (m1,m3) at the top sieve due to surface form
compatibility features indicating that province was
observed to refer to Jiangxi in the document. The
transitivity of Best-Link takes care of (m2,m3).
However, our approach has multiple limitations.
Entity-based features currently do not propagate
knowledge attributes directly, but through aggregat-
ing pairwise predictions at knowledge-infused inter-
mediate sieves. We rely on gold mention bound-
aries and exhaustive gold co-reference annotation.
This prevented us from applying our approach to
the Ontonotes dataset where singleton clusters and
co-referent nested mentions are removed. There-
fore the gold annotation for training several sieves
of our scheme is missing (e.g. nested mentions).
Another limitation is our somewhat preliminary di-
vision to sieves. (Vilalta and Rish, 2003) have ex-
perimented with approaches for automatic decom-
position of data to subclasses and learning multiple
models to improve data separability. We hope that
similar approach would be useful for co-reference
resolution. Ideally, we want to make ?simple de-
cisions? first, similarly to what was done in (Gold-
berg and Elhadad, 2010) for dependency parsing,
and model clustering as a structured problem, sim-
ilarly to (Joachims et al 2009; Wick et al 2011).
However, our experience with multi-sieve approach
with classifiers suggests that a single model would
not perform well for both lower sieves with little
entity-based information and higher sieves with a lot
of entity-based features. Addressing the aforemen-
tioned challenges is a subject for future work.
There has been an increasing interest in
knowledge-rich co-reference resolution (Ponzetto
and Strube, 2006; Haghighi and Klein, 2010; Rah-
man and Ng, 2011). We use Wikipedia differently
from (Ponzetto and Strube, 2006) who focus on
using WikiRelate, a Wikipedia-based relatedness
metric (Strube and Ponzetto, 2006). (Rahman and
Ng, 2011) used the union of all possible inter-
pretations a mention may have in YAGO, which
means that Michael Jordan could be co-refed both
to a scientist and basketball player in the same
document. Additionally, (Rahman and Ng, 2011)
use exact word matching, relying on YAGO?s ability
to extract a comprehensive set of facts offline14. We
are the first to use context-sensitive disambiguation
to Wikipedia, which received a lot of attention re-
cently (Bunescu and Pasca, 2006; Cucerzan, 2007;
Mihalcea and Csomai, 2007; Milne and Witten,
2008; Ratinov et al 2011). We extract context-
sensitive, high-precision knowledge attributes from
Wikipedia pages and apply (among other features)
WordNet similarity metric on pairs of knowledge
attributes to determine attribute compatibility.
We have integrated the strengths of rule-based
systems such as (Haghighi and Klein, 2009; Raghu-
nathan et al 2010) into a multi-sieve machine learn-
ing framework. We show that training sieve-specific
models significantly increases the performance on
most intermediate sievesieves.
We develop a novel approach for entity-based in-
ference. Unlike (Rahman and Ng, 2011) who con-
struct entities left-to-right, and similarly to (Raghu-
nathan et al 2010) we resolve easy instances of co-
ref to reduce error propagation in entity-based fea-
tures. Unlike (Raghunathan et al 2010), we al-
low later stages of inference to change the decisions
made at lower stages as additional entity-based evi-
dence becomes available.
By adding word-knowledge features and refin-
ing the inference, we improve the performance of a
state-of-the-art system of (Bengtson and Roth, 2008)
by 3 MUC, 2 B3 and 2 CEAF F1 points on the non-
transcript portion of the ACE 2004 dataset.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
14YAGO uses WordNet to expand its set of facts. For ex-
ample, if Martha Stewart is assigned the meaning personality
from category head words analysis, YAGO adds the meaning
celebrity because personality is a direct hyponym of celebrity in
WordNet. However, this is done offline in a context-insensitive
way, which is inherently limited.
1243
R. C. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
EACL.
S. Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In EMNLP-
CoNLL.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL, pages 81?88.
Q. Do, D. Roth, M. Sammons, Y. Tu, and V. Vydiswaran.
2009. Robust, light-weight approaches to compute
lexical similarity. Technical report, University of Illi-
nois at Urbana-Champaign.
A. Fader, S. Soderland, and O. Etzioni. 2009. Scaling
wikipedia-based named entity disambiguation to arbi-
trary web text. In WikiAI (IJCAI workshop).
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In NAACL.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In EMNLP.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In HLT-ACL. As-
sociation for Computational Linguistics.
T. Joachims, T. Hofmann, Y. Yue, and C. Yu. 2009.
Predicting structured objects with support vector ma-
chines. Communications of the ACM, Research High-
light, 52(11):97?104, November.
X. Luo. 2005. On coreference resolution performance
metrics. In HLT.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM.
D. Milne and I. H. Witten. 2008. Learning to link with
wikipedia. In CIKM.
V. Nastase and M. Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In AAAI.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
NIST. 2004. The ace evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
S. P. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, wordnet and wikipedia for coreference
resolution. In HLT-ACL.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010.
A multi-pass sieve for coreference resolution. In
EMNLP.
A. Rahman and V. Ng. 2011. Coreference resolution
with world knowledge. In HLT-ACL.
L. Ratinov, D. Downey, M. Anderson, and D. Roth.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the Twenty-First National Conference
on Artificial Intelligence, July.
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: A core of semantic knowledge. In WWW.
D. Vadas and J. R. Curran. 2008. Parsing noun phrase
structure with CCG. In ACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC6, pages 45?52.
R. Vilalta and I. Rish. 2003. A decomposition of classes
via clustering to explain and improve naive bayes. In
ECML.
M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta, and
A. McCallum. 2011. Samplerank: Training factor
graphs with atomic gradients. In ICML.
1244
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384?394,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Word representations:
A simple and general method for semi-supervised learning
Joseph Turian
De?partement d?Informatique et
Recherche Ope?rationnelle (DIRO)
Universite? de Montre?al
Montre?al, Que?bec, Canada, H3T 1J4
lastname@iro.umontreal.ca
Lev Ratinov
Department of
Computer Science
University of Illinois at
Urbana-Champaign
Urbana, IL 61801
ratinov2@uiuc.edu
Yoshua Bengio
De?partement d?Informatique et
Recherche Ope?rationnelle (DIRO)
Universite? de Montre?al
Montre?al, Que?bec, Canada, H3T 1J4
bengioy@iro.umontreal.ca
Abstract
If we take an existing supervised NLP sys-
tem, a simple and general way to improve
accuracy is to use unsupervised word
representations as extra word features. We
evaluate Brown clusters, Collobert and
Weston (2008) embeddings, and HLBL
(Mnih & Hinton, 2009) embeddings
of words on both NER and chunking.
We use near state-of-the-art supervised
baselines, and find that each of the three
word representations improves the accu-
racy of these baselines. We find further
improvements by combining different
word representations. You can download
our word features, for off-the-shelf use
in existing NLP systems, as well as our
code, here: http://metaoptimize.
com/projects/wordreprs/
1 Introduction
By using unlabelled data to reduce data sparsity
in the labeled training data, semi-supervised
approaches improve generalization accuracy.
Semi-supervised models such as Ando and Zhang
(2005), Suzuki and Isozaki (2008), and Suzuki
et al (2009) achieve state-of-the-art accuracy.
However, these approaches dictate a particular
choice of model and training regime. It can be
tricky and time-consuming to adapt an existing su-
pervised NLP system to use these semi-supervised
techniques. It is preferable to use a simple and
general method to adapt existing supervised NLP
systems to be semi-supervised.
One approach that is becoming popular is
to use unsupervised methods to induce word
features?or to download word features that have
already been induced?plug these word features
into an existing system, and observe a significant
increase in accuracy. But which word features are
good for what tasks? Should we prefer certain
word features? Can we combine them?
A word representation is a mathematical object
associated with each word, often a vector. Each
dimension?s value corresponds to a feature and
might even have a semantic or grammatical
interpretation, so we call it a word feature.
Conventionally, supervised lexicalized NLP ap-
proaches take a word and convert it to a symbolic
ID, which is then transformed into a feature vector
using a one-hot representation: The feature vector
has the same length as the size of the vocabulary,
and only one dimension is on. However, the
one-hot representation of a word suffers from data
sparsity: Namely, for words that are rare in the
labeled training data, their corresponding model
parameters will be poorly estimated. Moreover,
at test time, the model cannot handle words that
do not appear in the labeled training data. These
limitations of one-hot word representations have
prompted researchers to investigate unsupervised
methods for inducing word representations over
large unlabeled corpora. Word features can be
hand-designed, but our goal is to learn them.
One common approach to inducing unsuper-
vised word representation is to use clustering,
perhaps hierarchical. This technique was used by
a variety of researchers (Miller et al, 2004; Liang,
2005; Koo et al, 2008; Ratinov & Roth, 2009;
Huang & Yates, 2009). This leads to a one-hot
representation over a smaller vocabulary size.
Neural language models (Bengio et al, 2001;
Schwenk & Gauvain, 2002; Mnih & Hinton,
2007; Collobert & Weston, 2008), on the other
hand, induce dense real-valued low-dimensional
384
word embeddings using unsupervised approaches.
(See Bengio (2008) for a more complete list of
references on neural language models.)
Unsupervised word representations have
been used in previous NLP work, and have
demonstrated improvements in generalization
accuracy on a variety of tasks. But different word
representations have never been systematically
compared in a controlled way. In this work, we
compare different techniques for inducing word
representations, evaluating them on the tasks of
named entity recognition (NER) and chunking.
We retract former negative results published in
Turian et al (2009) about Collobert and Weston
(2008) embeddings, given training improvements
that we describe in Section 7.1.
2 Distributional representations
Distributional word representations are based
upon a cooccurrence matrix F of size W?C, where
W is the vocabulary size, each row Fw is the ini-
tial representation of word w, and each column Fc
is some context. Sahlgren (2006) and Turney and
Pantel (2010) describe a handful of possible de-
sign decisions in contructing F, including choice
of context types (left window? right window? size
of window?) and type of frequency count (raw?
binary? tf-idf?). Fw has dimensionality W, which
can be too large to use Fw as features for word w in
a supervised model. One can map F to matrix f of
size W ? d, where d  C, using some function g,
where f = g(F). fw represents word w as a vector
with d dimensions. The choice of g is another de-
sign decision, although perhaps not as important
as the statistics used to initially construct F.
The self-organizing semantic map (Ritter &
Kohonen, 1989) is a distributional technique
that maps words to two dimensions, such that
syntactically and semantically related words are
nearby (Honkela et al, 1995; Honkela, 1997).
LSA (Dumais et al, 1988; Landauer et al,
1998), LSI, and LDA (Blei et al, 2003) induce
distributional representations over F in which
each column is a document context. In most of the
other approaches discussed, the columns represent
word contexts. In LSA, g computes the SVD of F.
Hyperspace Analogue to Language (HAL) is
another early distributional approach (Lund et al,
1995; Lund & Burgess, 1996) to inducing word
representations. They compute F over a corpus of
160 million word tokens with a vocabulary size W
of 70K word types. There are 2?W types of context
(columns): The first or second W are counted if the
word c occurs within a window of 10 to the left or
right of the word w, respectively. f is chosen by
taking the 200 columns (out of 140K in F) with
the highest variances. ICA is another technique to
transform F into f . (Va?yrynen & Honkela, 2004;
Va?yrynen & Honkela, 2005; Va?yrynen et al,
2007). ICA is expensive, and the largest vocab-
ulary size used in these works was only 10K. As
far as we know, ICA methods have not been used
when the size of the vocab W is 100K or more.
Explicitly storing cooccurrence matrix F can be
memory-intensive, and transforming F to f can
be time-consuming. It is preferable that F never
be computed explicitly, and that f be constructed
incrementally. R?ehu?r?ek and Sojka (2010) describe
an incremental approach to inducing LSA and
LDA topic models over 270 millions word tokens
with a vocabulary of 315K word types. This is
similar in magnitude to our experiments.
Another incremental approach to constructing f
is using a random projection: Linear mapping g is
multiplying F by a random matrix chosen a pri-
ori. This random indexing method is motivated
by the Johnson-Lindenstrauss lemma, which states
that for certain choices of random matrix, if d is
sufficiently large, then the original distances be-
tween words in F will be preserved in f (Sahlgren,
2005). Kaski (1998) uses this technique to pro-
duce 100-dimensional representations of docu-
ments. Sahlgren (2001) was the first author to use
random indexing using narrow context. Sahlgren
(2006) does a battery of experiments exploring
different design decisions involved in construct-
ing F, prior to using random indexing. However,
like all the works cited above, Sahlgren (2006)
only uses distributional representation to improve
existing systems for one-shot classification tasks,
such as IR, WSD, semantic knowledge tests, and
text categorization. It is not well-understood
what settings are appropriate to induce distribu-
tional word representations for structured predic-
tion tasks (like parsing and MT) and sequence la-
beling tasks (like chunking and NER). Previous
research has achieved repeated successes on these
tasks using clustering representations (Section 3)
and distributed representations (Section 4), so we
focus on these representations in our work.
3 Clustering-based word representations
Another type of word representation is to induce
a clustering over words. Clustering methods and
385
distributional methods can overlap. For example,
Pereira et al (1993) begin with a cooccurrence
matrix and transform this matrix into a clustering.
3.1 Brown clustering
The Brown algorithm is a hierarchical clustering
algorithm which clusters words to maximize the
mutual information of bigrams (Brown et al,
1992). So it is a class-based bigram language
model. It runs in time O(V ?K2), where V is the size
of the vocabulary and K is the number of clusters.
The hierarchical nature of the clustering means
that we can choose the word class at several
levels in the hierarchy, which can compensate for
poor clusters of a small number of words. One
downside of Brown clustering is that it is based
solely on bigram statistics, and does not consider
word usage in a wider context.
Brown clusters have been used successfully in
a variety of NLP applications: NER (Miller et al,
2004; Liang, 2005; Ratinov & Roth, 2009), PCFG
parsing (Candito & Crabbe?, 2009), dependency
parsing (Koo et al, 2008; Suzuki et al, 2009), and
semantic dependency parsing (Zhao et al, 2009).
Martin et al (1998) presents algorithms for
inducing hierarchical clusterings based upon word
bigram and trigram statistics. Ushioda (1996)
presents an extension to the Brown clustering
algorithm, and learn hierarchical clusterings of
words as well as phrases, which they apply to
POS tagging.
3.2 Other work on cluster-based word
representations
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce.
HMMs can be used to induce a soft clustering,
specifically a multinomial distribution over pos-
sible clusters (hidden states). Li and McCallum
(2005) use an HMM-LDA model to improve
POS tagging and Chinese Word Segmentation.
Huang and Yates (2009) induce a fully-connected
HMM, which emits a multinomial distribution
over possible vocabulary words. They perform
hard clustering using the Viterbi algorithm.
(Alternately, they could keep the soft clustering,
with the representation for a particular word token
being the posterior probability distribution over
the states.) However, the CRF chunker in Huang
and Yates (2009), which uses their HMM word
clusters as extra features, achieves F1 lower than
a baseline CRF chunker (Sha & Pereira, 2003).
Goldberg et al (2009) use an HMM to assign
POS tags to words, which in turns improves
the accuracy of the PCFG-based Hebrew parser.
Deschacht and Moens (2009) use a latent-variable
language model to improve semantic role labeling.
4 Distributed representations
Another approach to word representation is to
learn a distributed representation. (Not to be
confused with distributional representations.)
A distributed representation is dense, low-
dimensional, and real-valued. Distributed word
representations are called word embeddings. Each
dimension of the embedding represents a latent
feature of the word, hopefully capturing useful
syntactic and semantic properties. A distributed
representation is compact, in the sense that it can
represent an exponential number of clusters in the
number of dimensions.
Word embeddings are typically induced us-
ing neural language models, which use neural
networks as the underlying predictive model
(Bengio, 2008). Historically, training and testing
of neural language models has been slow, scaling
as the size of the vocabulary for each model com-
putation (Bengio et al, 2001; Bengio et al, 2003).
However, many approaches have been proposed
in recent years to eliminate that linear dependency
on vocabulary size (Morin & Bengio, 2005;
Collobert & Weston, 2008; Mnih & Hinton, 2009)
and allow scaling to very large training corpora.
4.1 Collobert and Weston (2008) embeddings
Collobert and Weston (2008) presented a neural
language model that could be trained over billions
of words, because the gradient of the loss was
computed stochastically over a small sample of
possible outputs, in a spirit similar to Bengio and
Se?ne?cal (2003). This neural model of Collobert
and Weston (2008) was refined and presented in
greater depth in Bengio et al (2009).
The model is discriminative and non-
probabilistic. For each training update, we
read an n-gram x = (w1, . . . ,wn) from the corpus.
The model concatenates the learned embeddings
of the n words, giving e(w1) ? . . . ? e(wn), where
e is the lookup table and ? is concatenation.
We also create a corrupted or noise n-gram
x? = (w1, . . . ,wn?q, w?n), where w?n , wn is chosen
uniformly from the vocabulary.1 For convenience,
1In Collobert and Weston (2008), the middle word in the
386
we write e(x) to mean e(w1) ? . . . ? e(wn). We
predict a score s(x) for x by passing e(x) through
a single hidden layer neural network. The training
criterion is that n-grams that are present in the
training corpus like x must have a score at least
some margin higher than corrupted n-grams like
x?. Specifically: L(x) = max(0, 1? s(x) + s(x?)). We
minimize this loss stochastically over the n-grams
in the corpus, doing gradient descent simultane-
ously over the neural network parameters and the
embedding lookup table.
We implemented the approach of Collobert and
Weston (2008), with the following differences:
? We did not achieve as low log-ranks on the
English Wikipedia as the authors reported in
Bengio et al (2009), despite initially attempting
to have identical experimental conditions.
?We corrupt the last word of each n-gram.
? We had a separate learning rate for the em-
beddings and for the neural network weights.
We found that the embeddings should have a
learning rate generally 1000?32000 times higher
than the neural network weights. Otherwise, the
unsupervised training criterion drops slowly.
? Although their sampling technique makes train-
ing fast, testing is still expensive when the size of
the vocabulary is large. Instead of cross-validating
using the log-rank over the validation data as
they do, we instead used the moving average of
the training loss on training examples before the
weight update.
4.2 HLBL embeddings
The log-bilinear model (Mnih & Hinton, 2007) is
a probabilistic and linear neural model. Given an
n-gram, the model concatenates the embeddings
of the n ? 1 first words, and learns a linear model
to predict the embedding of the last word. The
similarity between the predicted embedding and
the current actual embedding is transformed
into a probability by exponentiating and then
normalizing. Mnih and Hinton (2009) speed up
model evaluation during training and testing by
using a hierarchy to exponentially filter down
the number of computations that are performed.
This hierarchical evaluation technique was first
proposed by Morin and Bengio (2005). The
model, combined with this optimization, is called
the hierarchical log-bilinear (HLBL) model.
n-gram is corrupted. In Bengio et al (2009), the last word in
the n-gram is corrupted.
5 Supervised evaluation tasks
We evaluate the hypothesis that one can take an
existing, near state-of-the-art, supervised NLP
system, and improve its accuracy by including
word representations as word features. This
technique for turning a supervised approach into a
semi-supervised one is general and task-agnostic.
However, we wish to find out if certain word
representations are preferable for certain tasks.
Lin and Wu (2009) finds that the representations
that are good for NER are poor for search query
classification, and vice-versa. We apply clus-
tering and distributed representations to NER
and chunking, which allows us to compare our
semi-supervised models to those of Ando and
Zhang (2005) and Suzuki and Isozaki (2008).
5.1 Chunking
Chunking is a syntactic sequence labeling task.
We follow the conditions in the CoNLL-2000
shared task (Sang & Buchholz, 2000).
The linear CRF chunker of Sha and Pereira
(2003) is a standard near-state-of-the-art baseline
chunker. In fact, many off-the-shelf CRF imple-
mentations now replicate Sha and Pereira (2003),
including their choice of feature set:
? CRF++ by Taku Kudo (http://crfpp.
sourceforge.net/)
? crfsgd by Le?on Bottou (http://leon.
bottou.org/projects/sgd)
? CRFsuite by by Naoaki Okazaki (http://
www.chokkan.org/software/crfsuite/)
We use CRFsuite because it makes it sim-
ple to modify the feature generation code,
so one can easily add new features. We
use SGD optimization, and enable negative
state features and negative transition fea-
tures. (?feature.possible transitions=1,
feature.possible states=1?)
Table 1 shows the features in the baseline chun-
ker. As you can see, the Brown and embedding
features are unigram features, and do not partici-
pate in conjunctions like the word features and tag
features do. Koo et al (2008) sees further accu-
racy improvements on dependency parsing when
using word representations in compound features.
The data comes from the Penn Treebank, and
is newswire from the Wall Street Journal in 1989.
Of the 8936 training sentences, we used 1000
randomly sampled sentences (23615 words) for
development. We trained models on the 7936
387
? Word features: wi for i in {?2,?1, 0,+1,+2},
wi ? wi+1 for i in {?1, 0}.
? Tag features: wi for i in {?2,?1, 0,+1,+2},
ti ? ti+1 for i in {?2,?1, 0,+1}. ti ? ti+1 ? ti+2
for i in {?2,?1, 0}.
? Embedding features [if applicable]: ei[d] for i
in {?2,?1, 0,+1,+2}, where d ranges over the
dimensions of the embedding ei.
? Brown features [if applicable]: substr(bi, 0, p)
for i in {?2,?1, 0,+1,+2}, where substr takes
the p-length prefix of the Brown cluster bi.
Table 1: Features templates used in the CRF chunker.
training partition sentences, and evaluated their
F1 on the development set. After choosing hy-
perparameters to maximize the dev F1, we would
retrain the model using these hyperparameters on
the full 8936 sentence training set, and evaluate
on test. One hyperparameter was l2-regularization
sigma, which for most models was optimal at 2 or
3.2. The word embeddings also required a scaling
hyperparameter, as described in Section 7.2.
5.2 Named entity recognition
NER is typically treated as a sequence prediction
problem. Following Ratinov and Roth (2009), we
use the regularized averaged perceptron model.
Ratinov and Roth (2009) describe different
sequence encoding like BILOU and BIO, and
show that the BILOU encoding outperforms BIO,
and the greedy inference performs competitively
to Viterbi while being significantly faster. Ac-
cordingly, we use greedy inference and BILOU
text chunk representation. We use the publicly
available implementation from Ratinov and Roth
(2009) (see the end of this paper for the URL). In
our baseline experiments, we remove gazetteers
and non-local features (Krishnan & Manning,
2006). However, we also run experiments that
include these features, to understand if the infor-
mation they provide mostly overlaps with that of
the word representations.
After each epoch over the training set, we
measured the accuracy of the model on the
development set. Training was stopped after the
accuracy on the development set did not improve
for 10 epochs, generally about 50?80 epochs
total. The epoch that performed best on the
development set was chosen as the final model.
We use the following baseline set of features
from Zhang and Johnson (2003):
? Previous two predictions yi?1 and yi?2
? Current word xi
? xi word type information: all-capitalized,
is-capitalized, all-digits, alphanumeric, etc.
? Prefixes and suffixes of xi, if the word contains
hyphens, then the tokens between the hyphens
? Tokens in the window c =
(xi?2, xi?1, xi, xi+1, xi+2)
? Capitalization pattern in the window c
? Conjunction of c and yi?1.
Word representation features, if present, are used
the same way as in Table 1.
When using the lexical features, we normalize
dates and numbers. For example, 1980 becomes
*DDDD* and 212-325-4751 becomes *DDD*-
*DDD*-*DDDD*. This allows a degree of abstrac-
tion to years, phone numbers, etc. This delexi-
calization is performed separately from using the
word representation. That is, if we have induced
an embedding for 12/3/2008 , we will use the em-
bedding of 12/3/2008 , and *DD*/*D*/*DDDD*
in the baseline features listed above.
Unlike in our chunking experiments, after we
chose the best model on the development set, we
used that model on the test set too. (In chunking,
after finding the best hyperparameters on the
development set, we would combine the dev
and training set and training a model over this
combined set, and then evaluate on test.)
The standard evaluation benchmark for NER
is the CoNLL03 shared task dataset drawn from
the Reuters newswire. The training set contains
204K words (14K sentences, 946 documents), the
test set contains 46K words (3.5K sentences, 231
documents), and the development set contains
51K words (3.3K sentences, 216 documents).
We also evaluated on an out-of-domain (OOD)
dataset, the MUC7 formal run (59K words).
MUC7 has a different annotation standard than
the CoNLL03 data. It has several NE types that
don?t appear in CoNLL03: money, dates, and
numeric quantities. CoNLL03 has MISC, which
is not present in MUC7. To evaluate on MUC7,
we perform the following postprocessing steps
prior to evaluation:
1. In the gold-standard MUC7 data, discard
(label as ?O?) all NEs with type NUM-
BER/MONEY/DATE.
2. In the predicted model output on MUC7 data,
discard (label as ?O?) all NEs with type MISC.
388
These postprocessing steps will adversely affect
all NER models across-the-board, nonetheless
allowing us to compare different models in a
controlled manner.
6 Unlabled Data
Unlabeled data is used for inducing the word
representations. We used the RCV1 corpus, which
contains one year of Reuters English newswire,
from August 1996 to August 1997, about 63
millions words in 3.3 million sentences. We
left case intact in the corpus. By comparison,
Collobert and Weston (2008) downcases words
and delexicalizes numbers.
We use a preprocessing technique proposed
by Liang, (2005, p. 51), which was later used
by Koo et al (2008): Remove all sentences that
are less than 90% lowercase a?z. We assume
that whitespace is not counted, although this
is not specified in Liang?s thesis. We call this
preprocessing step cleaning.
In Turian et al (2009), we found that all
word representations performed better on the
supervised task when they were induced on the
clean unlabeled data, both embeddings and Brown
clusters. This is the case even though the cleaning
process was very aggressive, and discarded more
than half of the sentences. According to the
evidence and arguments presented in Bengio et al
(2009), the non-convex optimization process for
Collobert and Weston (2008) embeddings might
be adversely affected by noise and the statistical
sparsity issues regarding rare words, especially
at the beginning of training. For this reason, we
hypothesize that learning representations over the
most frequent words first and gradually increasing
the vocabulary?a curriculum training strategy
(Elman, 1993; Bengio et al, 2009; Spitkovsky
et al, 2010)?would provide better results than
cleaning.
After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences (41% of
the original). The cleaned RCV1 corpus has 269K
word types. This is the vocabulary size, i.e. how
many word representations were induced. Note
that cleaning is applied only to the unlabeled data,
not to the labeled data used in the supervised tasks.
RCV1 is a superset of the CoNLL03 corpus.
For this reason, NER results that use RCV1
word representations are a form of transductive
learning.
7 Experiments and Results
7.1 Details of inducing word representations
The Brown clusters took roughly 3 days to induce,
when we induced 1000 clusters, the baseline in
prior work (Koo et al, 2008; Ratinov & Roth,
2009). We also induced 100, 320, and 3200
Brown clusters, for comparison. (Because Brown
clustering scales quadratically in the number of
clusters, inducing 10000 clusters would have
been prohibitive.) Because Brown clusters are
hierarchical, we can use cluster supersets as
features. We used clusters at path depth 4, 6, 10,
and 20 (Ratinov & Roth, 2009). These are the
prefixes used in Table 1.
The Collobert and Weston (2008) (C&W)
embeddings were induced over the course of a
few weeks, and trained for about 50 epochs. One
of the difficulties in inducing these embeddings is
that there is no stopping criterion defined, and that
the quality of the embeddings can keep improving
as training continues. Collobert (p.c.) simply
leaves one computer training his embeddings
indefinitely. We induced embeddings with 25, 50,
100, or 200 dimensions over 5-gram windows.
In comparison to Turian et al (2009), we use
improved C&W embeddings in this work:
? They were trained for 50 epochs, not just 20
epochs.
? We initialized all embedding dimensions uni-
formly in the range [-0.01, +0.01], not [-1,+1].
For rare words, which are typically updated only
143 times per epoch2, and given that our embed-
ding learning rate was typically 1e-6 or 1e-7, this
means that rare word embeddings will be concen-
trated around zero, instead of spread out randomly.
The HLBL embeddings were trained for 100
epochs (7 days).3 Unlike our Collobert and We-
ston (2008) embeddings, we did not extensively
tune the learning rates for HLBL. We used a learn-
ing rate of 1e-3 for both model parameters and
embedding parameters. We induced embeddings
with 100 dimensions over 5-gram windows, and
embeddings with 50 dimensions over 5-gram win-
dows. Embeddings were induced over one pass
2A rare word will appear 5 (window size) times per
epoch as a positive example, and 37M (training examples per
epoch) / 269K (vocabulary size) = 138 times per epoch as a
corruption example.
3The HLBL model updates require fewer matrix mul-
tiplies than Collobert and Weston (2008) model updates.
Additionally, HLBL models were trained on a GPGPU,
which is faster than conventional CPU arithmetic.
389
approach using a random tree, not two passes with
an updated tree and embeddings re-estimation.
7.2 Scaling of Word Embeddings
Like many NLP systems, the baseline system con-
tains only binary features. The word embeddings,
however, are real numbers that are not necessarily
in a bounded range. If the range of the word
embeddings is too large, they will exert more
influence than the binary features.
We generally found that embeddings had zero
mean. We can scale the embeddings by a hy-
perparameter, to control their standard deviation.
Assume that the embeddings are represented by a
matrix E:
E ? ? ? E/stddev(E) (1)
? is a scaling constant that sets the new standard
deviation after scaling the embeddings.
(a)
 93.6
 93.8
 94
 94.2
 94.4
 94.6
 94.8
 0.001  0.01  0.1  1
Va
lid
ati
on
 F1
Scaling factor ?
C&W, 50-dim
HLBL, 50-dimC&W, 200-dimC&W, 100-dim
HLBL, 100-dimC&W, 25-dim
baseline
(b)
 89
 89.5
 90
 90.5
 91
 91.5
 92
 92.5
 0.001  0.01  0.1  1
Va
lid
ati
on
 F1
Scaling factor ?
C&W, 200-dimC&W, 100-dimC&W, 25-dimC&W, 50-dim
HLBL, 100-dim
HLBL, 50-dim
baseline
Figure 1: Effect as we vary the scaling factor ? (Equa-
tion 1) on the validation set F1. We experiment with
Collobert and Weston (2008) and HLBL embeddings of var-
ious dimensionality. (a) Chunking results. (b) NER results.
Figure 1 shows the effect of scaling factor ?
on both supervised tasks. We were surprised
to find that on both tasks, across Collobert and
Weston (2008) and HLBL embeddings of various
dimensionality, that all curves had similar shapes
and optima. This is one contributions of our
work. In Turian et al (2009), we were not
able to prescribe a default value for scaling the
embeddings. However, these curves demonstrate
that a reasonable choice of scale factor is such that
the embeddings have a standard deviation of 0.1.
7.3 Capacity of Word Representations
(a)
 94.1
 94.2
 94.3
 94.4
 94.5
 94.6
 94.7
 100  320  1000  3200
 25  50  100  200
Va
lid
ati
on
 F1
# of Brown clusters
# of embedding dimensions
C&W
HLBL
Brown
baseline
(b)
 90
 90.5
 91
 91.5
 92
 92.5
 100  320  1000  3200
 25  50  100  200
Va
lid
ati
on
 F1
# of Brown clusters
# of embedding dimensions
C&W
Brown
HLBL
baseline
Figure 2: Effect as we vary the capacity of the word
representations on the validation set F1. (a) Chunking
results. (b) NER results.
There are capacity controls for the word
representations: number of Brown clusters, and
number of dimensions of the word embeddings.
Figure 2 shows the effect on the validation F1 as
we vary the capacity of the word representations.
In general, it appears that more Brown clusters
are better. We would like to induce 10000 Brown
clusters, however this would take several months.
In Turian et al (2009), we hypothesized on
the basis of solely the HLBL NER curve that
higher-dimensional word embeddings would give
higher accuracy. Figure 2 shows that this hy-
pothesis is not true. For NER, the C&W curve is
almost flat, and we were suprised to find the even
25-dimensional C&W word embeddings work so
well. For chunking, 50-dimensional embeddings
had the highest validation F1 for both C&W and
HLBL. These curves indicates that the optimal
capacity of the word embeddings is task-specific.
390
System Dev Test
Baseline 94.16 93.79
HLBL, 50-dim 94.63 94.00
C&W, 50-dim 94.66 94.10
Brown, 3200 clusters 94.67 94.11
Brown+HLBL, 37M 94.62 94.13
C&W+HLBL, 37M 94.68 94.25
Brown+C&W+HLBL, 37M 94.72 94.15
Brown+C&W, 37M 94.76 94.35
Ando and Zhang (2005), 15M - 94.39
Suzuki and Isozaki (2008), 15M - 94.67
Suzuki and Isozaki (2008), 1B - 95.15
Table 2: Final chunking F1 results. In the last section, we
show how many unlabeled words were used.
System Dev Test MUC7
Baseline 90.03 84.39 67.48
Baseline+Nonlocal 91.91 86.52 71.80
HLBL 100-dim 92.00 88.13 75.25
Gazetteers 92.09 87.36 77.76
C&W 50-dim 92.27 87.93 75.74
Brown, 1000 clusters 92.32 88.52 78.84
C&W 200-dim 92.46 87.96 75.51
C&W+HLBL 92.52 88.56 78.64
Brown+HLBL 92.56 88.93 77.85
Brown+C&W 92.79 89.31 80.13
HLBL+Gaz 92.91 89.35 79.29
C&W+Gaz 92.98 88.88 81.44
Brown+Gaz 93.25 89.41 82.71
Lin and Wu (2009), 3.4B - 88.44 -
Ando and Zhang (2005), 27M 93.15 89.31 -
Suzuki and Isozaki (2008), 37M 93.66 89.36 -
Suzuki and Isozaki (2008), 1B 94.48 89.92 -
All (Brown+C&W+HLBL+Gaz), 37M 93.17 90.04 82.50
All+Nonlocal, 37M 93.95 90.36 84.15
Lin and Wu (2009), 700B - 90.90 -
Table 3: Final NER F1 results, showing the cumulative
effect of adding word representations, non-local features, and
gazetteers to the baseline. To speed up training, in combined
experiments (C&W plus another word representation),
we used the 50-dimensional C&W embeddings, not the
200-dimensional ones. In the last section, we show how
many unlabeled words were used.
7.4 Final results
Table 2 shows the final chunking results and Ta-
ble 3 shows the final NER F1 results. We compare
to the state-of-the-art methods of Ando and Zhang
(2005), Suzuki and Isozaki (2008), and?for
NER?Lin and Wu (2009). Tables 2 and 3 show
that accuracy can be increased further by combin-
ing the features from different types of word rep-
resentations. But, if only one word representation
is to be used, Brown clusters have the highest ac-
curacy. Given the improvements to the C&W em-
beddings since Turian et al (2009), C&W em-
beddings outperform the HLBL embeddings. On
chunking, there is only a minute difference be-
tween Brown clusters and the embeddings. Com-
(a)
 0
 50
 100
 150
 200
 250
0 1 10 100 1K 10K 100K 1M
# o
f p
er-
tok
en
 er
ror
s (t
est
 se
t)
Frequency of word in unlabeled data
C&W, 50-dim
Brown, 3200 clusters
(b)
 0
 50
 100
 150
 200
 250
0 1 10 100 1K 10K 100K 1M
# o
f p
er-
tok
en
 er
ror
s (t
est
 se
t)
Frequency of word in unlabeled data
C&W, 50-dim
Brown, 1000 clusters
Figure 3: For word tokens that have different frequency
in the unlabeled data, what is the total number of per-token
errors incurred on the test set? (a) Chunking results. (b) NER
results.
bining representations leads to small increases in
the test F1. In comparison to chunking, combin-
ing different word representations on NER seems
gives larger improvements on the test F1.
On NER, Brown clusters are superior to the
word embeddings. Since much of the NER F1
is derived from decisions made over rare words,
we suspected that Brown clustering has a superior
representation for rare words. Brown makes
a single hard clustering decision, whereas the
embedding for a rare word is close to its initial
value since it hasn?t received many training
updates (see Footnote 2). Figure 3 shows the total
number of per-token errors incurred on the test
set, depending upon the frequency of the word
token in the unlabeled data. For NER, Figure 3 (b)
shows that most errors occur on rare words, and
that Brown clusters do indeed incur fewer errors
for rare words. This supports our hypothesis
that, for rare words, Brown clustering produces
better representations than word embeddings that
haven?t received sufficient training updates. For
chunking, Brown clusters and C&W embeddings
incur almost identical numbers of errors, and
errors are concentrated around the more common
391
words. We hypothesize that non-rare words have
good representations, regardless of the choice
of word representation technique. For tasks like
chunking in which a syntactic decision relies upon
looking at several token simultaneously, com-
pound features that use the word representations
might increase accuracy more (Koo et al, 2008).
Using word representations in NER brought
larger gains on the out-of-domain data than on the
in-domain data. We were surprised by this result,
because the OOD data was not even used during
the unsupervised word representation induction,
as was the in-domain data. We are curious to
investigate this phenomenon further.
Ando and Zhang (2005) present a semi-
supervised learning algorithm called alternating
structure optimization (ASO). They find a low-
dimensional projection of the input features that
gives good linear classifiers over auxiliary tasks.
These auxiliary tasks are sometimes specific
to the supervised task, and sometimes general
language modeling tasks like ?predict the missing
word?. Suzuki and Isozaki (2008) present a semi-
supervised extension of CRFs. (In Suzuki et al
(2009), they extend their semi-supervised ap-
proach to more general conditional models.) One
of the advantages of the semi-supervised learning
approach that we use is that it is simpler and more
general than that of Ando and Zhang (2005) and
Suzuki and Isozaki (2008). Their methods dictate
a particular choice of model and training regime
and could not, for instance, be used with an NLP
system based upon an SVM classifier.
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce. Since they can scale
to millions of phrases, and they train over 800B
unlabeled words, they achieve state-of-the-art
accuracy on NER using their phrase clusters.
This suggests that extending word representa-
tions to phrase representations is worth further
investigation.
8 Conclusions
Word features can be learned in advance in an
unsupervised, task-inspecific, and model-agnostic
manner. These word features, once learned, are
easily disseminated with other researchers, and
easily integrated into existing supervised NLP
systems. The disadvantage, however, is that ac-
curacy might not be as high as a semi-supervised
method that includes task-specific information
and that jointly learns the supervised and unsu-
pervised tasks (Ando & Zhang, 2005; Suzuki &
Isozaki, 2008; Suzuki et al, 2009).
Unsupervised word representations have been
used in previous NLP work, and have demon-
strated improvements in generalization accuracy
on a variety of tasks. Ours is the first work to
systematically compare different word repre-
sentations in a controlled way. We found that
Brown clusters and word embeddings both can
improve the accuracy of a near-state-of-the-art
supervised NLP system. We also found that com-
bining different word representations can improve
accuracy further. Error analysis indicates that
Brown clustering induces better representations
for rare words than C&W embeddings that have
not received many training updates.
Another contribution of our work is a default
method for setting the scaling parameter for
word embeddings. With this contribution, word
embeddings can now be used off-the-shelf as
word features, with no tuning.
Future work should explore methods for
inducing phrase representations, as well as tech-
niques for increasing in accuracy by using word
representations in compound features.
Replicating our experiments
You can visit http://metaoptimize.com/
projects/wordreprs/ to find: The word
representations we induced, which you can
download and use in your experiments; The code
for inducing the word representations, which you
can use to induce word representations on your
own data; The NER and chunking system, with
code for replicating our experiments.
Acknowledgments
Thank you to Magnus Sahlgren, Bob Carpenter,
Percy Liang, Alexander Yates, and the anonymous
reviewers for useful discussion. Thank you to
Andriy Mnih for inducing his embeddings on
RCV1 for us. Joseph Turian and Yoshua Bengio
acknowledge the following agencies for re-
search funding and computing support: NSERC,
RQCHP, CIFAR. Lev Ratinov was supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
author and do not necessarily reflect the view of
the Air Force Research Laboratory (AFRL).
392
References
Ando, R., & Zhang, T. (2005). A high-
performance semi-supervised learning method
for text chunking. ACL.
Bengio, Y. (2008). Neural net language models.
Scholarpedia, 3, 3881.
Bengio, Y., Ducharme, R., & Vincent, P. (2001).
A neural probabilistic language model. NIPS.
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin,
C. (2003). A neural probabilistic language
model. Journal of Machine Learning Research,
3, 1137?1155.
Bengio, Y., Louradour, J., Collobert, R., &
Weston, J. (2009). Curriculum learning. ICML.
Bengio, Y., & Se?ne?cal, J.-S. (2003). Quick train-
ing of probabilistic neural nets by importance
sampling. AISTATS.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003).
Latent dirichlet alocation. Journal of Machine
Learning Research, 3, 993?1022.
Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra,
V. J. D., & Lai, J. C. (1992). Class-based n-gram
models of natural language. Computational
Linguistics, 18, 467?479.
Candito, M., & Crabbe?, B. (2009). Improving gen-
erative statistical parsing with semi-supervised
word clustering. IWPT (pp. 138?141).
Collobert, R., & Weston, J. (2008). A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
ICML.
Deschacht, K., & Moens, M.-F. (2009). Semi-
supervised semantic role labeling using the
Latent Words Language Model. EMNLP (pp.
21?29).
Dumais, S. T., Furnas, G. W., Landauer, T. K.,
Deerwester, S., & Harshman, R. (1988). Using
latent semantic analysis to improve access to
textual information. SIGCHI Conference on
Human Factors in Computing Systems (pp.
281?285). ACM.
Elman, J. L. (1993). Learning and development
in neural networks: The importance of starting
small. Cognition, 48, 781?799.
Goldberg, Y., Tsarfaty, R., Adler, M., & Elhadad,
M. (2009). Enhancing unlexicalized parsing
performance using a wide coverage lexicon,
fuzzy tag-set mapping, and EM-HMM-based
lexical probabilities. EACL.
Honkela, T. (1997). Self-organizing maps of
words for natural language processing applica-
tions. Proceedings of the International ICSC
Symposium on Soft Computing.
Honkela, T., Pulkki, V., & Kohonen, T. (1995).
Contextual relations of words in grimm tales,
analyzed by self-organizing map. ICANN.
Huang, F., & Yates, A. (2009). Distributional rep-
resentations for handling sparsity in supervised
sequence labeling. ACL.
Kaski, S. (1998). Dimensionality reduction by
random mapping: Fast similarity computation
for clustering. IJCNN (pp. 413?418).
Koo, T., Carreras, X., & Collins, M. (2008).
Simple semi-supervised dependency parsing.
ACL (pp. 595?603).
Krishnan, V., & Manning, C. D. (2006). An
effective two-stage model for exploiting non-
local dependencies in named entity recognition.
COLING-ACL.
Landauer, T. K., Foltz, P. W., & Laham, D. (1998).
An introduction to latent semantic analysis.
Discourse Processes, 259?284.
Li, W., & McCallum, A. (2005). Semi-supervised
sequence modeling with syntactic topic models.
AAAI.
Liang, P. (2005). Semi-supervised learning
for natural language. Master?s thesis, Mas-
sachusetts Institute of Technology.
Lin, D., & Wu, X. (2009). Phrase clustering
for discriminative learning. ACL-IJCNLP (pp.
1030?1038).
Lund, K., & Burgess, C. (1996). Producing
highdimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods,
Instrumentation, and Computers, 28, 203?208.
Lund, K., Burgess, C., & Atchley, R. A. (1995).
Semantic and associative priming in high-
dimensional semantic space. Cognitive Science
Proceedings, LEA (pp. 660?665).
Martin, S., Liermann, J., & Ney, H. (1998). Algo-
rithms for bigram and trigram word clustering.
Speech Communication, 24, 19?37.
Miller, S., Guinness, J., & Zamanian, A. (2004).
Name tagging with word clusters and discrim-
inative training. HLT-NAACL (pp. 337?342).
393
Mnih, A., & Hinton, G. E. (2007). Three
new graphical models for statistical language
modelling. ICML.
Mnih, A., & Hinton, G. E. (2009). A scalable
hierarchical distributed language model. NIPS
(pp. 1081?1088).
Morin, F., & Bengio, Y. (2005). Hierarchical
probabilistic neural network language model.
AISTATS.
Pereira, F., Tishby, N., & Lee, L. (1993). Distri-
butional clustering of english words. ACL (pp.
183?190).
Ratinov, L., & Roth, D. (2009). Design chal-
lenges and misconceptions in named entity
recognition. CoNLL.
Ritter, H., & Kohonen, T. (1989). Self-organizing
semantic maps. Biological Cybernetics,
241?254.
Sahlgren, M. (2001). Vector-based semantic
analysis: Representing word meanings based
on random labels. Proceedings of the Semantic
Knowledge Acquisition and Categorisation
Workshop, ESSLLI.
Sahlgren, M. (2005). An introduction to random
indexing. Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge
Engineering (TKE).
Sahlgren, M. (2006). The word-space model:
Using distributional analysis to represent syn-
tagmatic and paradigmatic relations between
words in high-dimensional vector spaces.
Doctoral dissertation, Stockholm University.
Sang, E. T., & Buchholz, S. (2000). Introduction
to the CoNLL-2000 shared task: Chunking.
CoNLL.
Schwenk, H., & Gauvain, J.-L. (2002). Connec-
tionist language modeling for large vocabulary
continuous speech recognition. International
Conference on Acoustics, Speech and Signal
Processing (ICASSP) (pp. 765?768). Orlando,
Florida.
Sha, F., & Pereira, F. C. N. (2003). Shal-
low parsing with conditional random fields.
HLT-NAACL.
Spitkovsky, V., Alshawi, H., & Jurafsky, D.
(2010). From baby steps to leapfrog: How ?less
is more? in unsupervised dependency parsing.
NAACL-HLT.
Suzuki, J., & Isozaki, H. (2008). Semi-supervised
sequential labeling and segmentation using
giga-word scale unlabeled data. ACL-08: HLT
(pp. 665?673).
Suzuki, J., Isozaki, H., Carreras, X., & Collins, M.
(2009). An empirical study of semi-supervised
structured conditional models for dependency
parsing. EMNLP.
Turian, J., Ratinov, L., Bengio, Y., & Roth, D.
(2009). A preliminary evaluation of word
representations for named-entity recognition.
NIPS Workshop on Grammar Induction, Repre-
sentation of Language and Language Learning.
Turney, P. D., & Pantel, P. (2010). From frequency
to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research.
Ushioda, A. (1996). Hierarchical clustering of
words. COLING (pp. 1159?1162).
Va?yrynen, J., & Honkela, T. (2005). Compar-
ison of independent component analysis and
singular value decomposition in word context
analysis. AKRR?05, International and Interdis-
ciplinary Conference on Adaptive Knowledge
Representation and Reasoning.
Va?yrynen, J. J., & Honkela, T. (2004). Word cat-
egory maps based on emergent features created
by ICA. Proceedings of the STeP?2004 Cogni-
tion + Cybernetics Symposium (pp. 173?185).
Finnish Artificial Intelligence Society.
Va?yrynen, J. J., Honkela, T., & Lindqvist, L.
(2007). Towards explicit semantic features
using independent component analysis. Pro-
ceedings of the Workshop Semantic Content
Acquisition and Representation (SCAR). Stock-
holm, Sweden: Swedish Institute of Computer
Science.
R?ehu?r?ek, R., & Sojka, P. (2010). Software frame-
work for topic modelling with large corpora.
LREC.
Zhang, T., & Johnson, D. (2003). A robust risk
minimization based named entity recognition
system. CoNLL.
Zhao, H., Chen, W., Kit, C., & Zhou, G.
(2009). Multilingual dependency learning: a
huge feature engineering method to semantic
dependency parsing. CoNLL (pp. 55?60).
394
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1375?1384,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Local and Global Algorithms for Disambiguation to Wikipedia
Lev Ratinov 1 Dan Roth1 Doug Downey2 Mike Anderson3
1University of Illinois at Urbana-Champaign
{ratinov2|danr}@uiuc.edu
2Northwestern University
ddowney@eecs.northwestern.edu
3Rexonomy
mrander@gmail.com
Abstract
Disambiguating concepts and entities in a con-
text sensitive way is a fundamental problem
in natural language processing. The compre-
hensiveness of Wikipedia has made the on-
line encyclopedia an increasingly popular tar-
get for disambiguation. Disambiguation to
Wikipedia is similar to a traditional Word
Sense Disambiguation task, but distinct in that
the Wikipedia link structure provides addi-
tional information about which disambigua-
tions are compatible. In this work we analyze
approaches that utilize this information to ar-
rive at coherent sets of disambiguations for a
given document (which we call ?global? ap-
proaches), and compare them to more tradi-
tional (local) approaches. We show that previ-
ous approaches for global disambiguation can
be improved, but even then the local disam-
biguation provides a baseline which is very
hard to beat.
1 Introduction
Wikification is the task of identifying and link-
ing expressions in text to their referent Wikipedia
pages. Recently, Wikification has been shown to
form a valuable component for numerous natural
language processing tasks including text classifica-
tion (Gabrilovich and Markovitch, 2007b; Chang et
al., 2008), measuring semantic similarity between
texts (Gabrilovich and Markovitch, 2007a), cross-
document co-reference resolution (Finin et al, 2009;
Mayfield et al, 2009), and other tasks (Kulkarni et
al., 2009).
Previous studies on Wikification differ with re-
spect to the corpora they address and the subset
of expressions they attempt to link. For exam-
ple, some studies focus on linking only named en-
tities, whereas others attempt to link all ?interest-
ing? expressions, mimicking the link structure found
in Wikipedia. Regardless, all Wikification systems
are faced with a key Disambiguation to Wikipedia
(D2W) task. In the D2W task, we?re given a text
along with explicitly identified substrings (called
mentions) to disambiguate, and the goal is to out-
put the corresponding Wikipedia page, if any, for
each mention. For example, given the input sen-
tence ?I am visiting friends in <Chicago>,? we
output http://en.wikipedia.org/wiki/Chicago ? the
Wikipedia page for the city of Chicago, Illinois, and
not (for example) the page for the 2002 film of the
same name.
Local D2W approaches disambiguate each men-
tion in a document separately, utilizing clues such
as the textual similarity between the document and
each candidate disambiguation?s Wikipedia page.
Recent work on D2W has tended to focus on more
sophisticated global approaches to the problem, in
which all mentions in a document are disambiguated
simultaneously to arrive at a coherent set of dis-
ambiguations (Cucerzan, 2007; Milne and Witten,
2008b; Han and Zhao, 2009). For example, if a
mention of ?Michael Jordan? refers to the computer
scientist rather than the basketball player, then we
would expect a mention of ?Monte Carlo? in the
same document to refer to the statistical technique
rather than the location. Global approaches utilize
the Wikipedia link graph to estimate coherence.
1375
m1 = Taiwan m2 = China m3 = Jiangsu Province..............
t1 = Taiwan t5 =People's Republic of China t7 = Jiangsu
..............
Document text  with mentions
t2 = Chinese Taipei t3 =Republic of China t4 = China t6 = History of China
?(m1, t1)
?(m1, t2)
?(m1, t3)
?(t1, t7) ?(t3, t7) ?(t5, t7)
Figure 1: Sample Disambiguation to Wikipedia problem with three mentions. The mention ?Jiangsu? is unambiguous.
The correct mapping from mentions to titles is marked by heavy edges
In this paper, we analyze global and local ap-
proaches to the D2W task. Our contributions are
as follows: (1) We present a formulation of the
D2W task as an optimization problem with local and
global variants, and identify the strengths and the
weaknesses of each, (2) Using this formulation, we
present a new global D2W system, called GLOW. In
experiments on existing and novel D2W data sets,1
GLOW is shown to outperform the previous state-
of-the-art system of (Milne and Witten, 2008b), (3)
We present an error analysis and identify the key re-
maining challenge: determining when mentions re-
fer to concepts not captured in Wikipedia.
2 Problem Definition and Approach
We formalize our Disambiguation to Wikipedia
(D2W) task as follows. We are given a document
d with a set of mentions M = {m1, . . . ,mN},
and our goal is to produce a mapping from the set
of mentions to the set of Wikipedia titles W =
{t1, . . . , t|W |}. Often, mentions correspond to a
concept without a Wikipedia page; we treat this case
by adding a special null title to the set W .
The D2W task can be visualized as finding a
many-to-one matching on a bipartite graph, with
mentions forming one partition and Wikipedia ti-
tles the other (see Figure 1). We denote the output
matching as an N -tuple ? = (t1, . . . , tN ) where ti
is the output disambiguation for mention mi.
1The data sets are available for download at
http://cogcomp.cs.illinois.edu/Data
2.1 Local and Global Disambiguation
A local D2W approach disambiguates each men-
tion mi separately. Specifically, let ?(mi, tj) be a
score function reflecting the likelihood that the can-
didate title tj ?W is the correct disambiguation for
mi ? M . A local approach solves the following
optimization problem:
??local = argmax?
N
?
i=1
?(mi, ti) (1)
Local D2W approaches, exemplified by (Bunescu
and Pasca, 2006) and (Mihalcea and Csomai, 2007),
utilize ? functions that assign higher scores to titles
with content similar to that of the input document.
We expect, all else being equal, that the correct
disambiguations will form a ?coherent? set of re-
lated concepts. Global approaches define a coher-
ence function ?, and attempt to solve the following
disambiguation problem:
?? = argmax
?
[
N
?
i=1
?(mi, ti) + ?(?)] (2)
The global optimization problem in Eq. 2 is NP-
hard, and approximations are required (Cucerzan,
2007). The common approach is to utilize the
Wikipedia link graph to obtain an estimate pairwise
relatedness between titles ?(ti, tj) and to efficiently
generate a disambiguation context ??, a rough ap-
proximation to the optimal ??. We then solve the
easier problem:
?? ? argmax
?
N
?
i=1
[?(mi, ti) +
?
tj???
?(ti, tj)] (3)
1376
Eq. 3 can be solved by finding each ti and then map-
ping mi independently as in a local approach, but
still enforces some degree of coherence among the
disambiguations.
3 Related Work
Wikipedia was first explored as an information
source for named entity disambiguation and in-
formation retrieval by Bunescu and Pasca (2006).
There, disambiguation is performed using an SVM
kernel that compares the lexical context around the
ambiguous named entity to the content of the can-
didate disambiguation?s Wikipedia page. However,
since each ambiguous mention required a separate
SVM model, the experiment was on a very limited
scale. Mihalcea and Csomai applied Word Sense
Disambiguation methods to the Disambiguation to
Wikipedia task (2007). They experimented with
two methods: (a) the lexical overlap between the
Wikipedia page of the candidate disambiguations
and the context of the ambiguous mention, and (b)
training a Naive Bayes classiffier for each ambigu-
ous mention, using the hyperlink information found
in Wikipedia as ground truth. Both (Bunescu and
Pasca, 2006) and (Mihalcea and Csomai, 2007) fall
into the local framework.
Subsequent work on Wikification has stressed that
assigned disambiguations for the same document
should be related, introducing the global approach
(Cucerzan, 2007; Milne and Witten, 2008b; Han and
Zhao, 2009; Ferragina and Scaiella, 2010). The two
critical components of a global approach are the se-
mantic relatedness function ? between two titles,
and the disambiguation context ??. In (Milne and
Witten, 2008b), the semantic context is defined to
be a set of ?unambiguous surface forms? in the text,
and the title relatedness ? is computed as Normal-
ized Google Distance (NGD) (Cilibrasi and Vitanyi,
2007).2 On the other hand, in (Cucerzan, 2007) the
disambiguation context is taken to be all plausible
disambiguations of the named entities in the text,
and title relatedness is based on the overlap in cat-
egories and incoming links. Both approaches have
limitations. The first approach relies on the pres-
2(Milne and Witten, 2008b) also weight each mention in ??
by its estimated disambiguation utility, which can be modeled
by augmenting ? on per-problem basis.
ence of unambiguous mentions in the input docu-
ment, and the second approach inevitably adds ir-
relevant titles to the disambiguation context. As we
demonstrate in our experiments, by utilizing a more
accurate disambiguation context, GLOW is able to
achieve better performance.
4 System Architecture
In this section, we present our global D2W system,
which solves the optimization problem in Eq. 3. We
refer to the system as GLOW, for Global Wikifica-
tion. We use GLOW as a test bed for evaluating local
and global approaches for D2W. GLOW combines
a powerful local model ? with an novel method
for choosing an accurate disambiguation context ??,
which as we show in our experiments allows it to
outperform the previous state of the art.
We represent the functions ? and ? as weighted
sums of features. Specifically, we set:
?(m, t) =
?
i
wi?i(m, t) (4)
where each feature ?i(m, t) captures some aspect
of the relatedness between the mention m and the
Wikipedia title t. Feature functions ?i(t, t?) are de-
fined analogously. We detail the specific feature
functions utilized in GLOW in following sections.
The coefficients wi are learned using a Support Vec-
tor Machine over bootstrapped training data from
Wikipedia, as described in Section 4.5.
At a high level, the GLOW system optimizes the
objective function in Eq. 3 in a two-stage process.
We first execute a ranker to obtain the best non-null
disambiguation for each mention in the document,
and then execute a linker that decides whether the
mention should be linked to Wikipedia, or whether
instead switching the top-ranked disambiguation to
null improves the objective function. As our exper-
iments illustrate, the linking task is the more chal-
lenging of the two by a significant margin.
Figure 2 provides detailed pseudocode for GLOW.
Given a document d and a set of mentions M , we
start by augmenting the set of mentions with all
phrases in the document that could be linked to
Wikipedia, but were not included in M . Introducing
these additional mentions provides context that may
be informative for the global coherence computation
(it has no effect on local approaches). In the second
1377
Algorithm: Disambiguate to Wikipedia
Input: document d, Mentions M = {m1, . . . ,mN}
Output: a disambiguation ? = (t1, . . . , tN ).
1) Let M ? = M? { Other potential mentions in d}
2) For each mention m?i ? M ?, construct a set of disam-
biguation candidates Ti = {ti1, . . . , tiki}, t
i
j 6= null
3) Ranker: Find a solution ? = (t?1, . . . , t?|M?|), where
t?i ? Ti is the best non-null disambiguation of m?i.
4) Linker: For each m?i, map t?i to null in ? iff doing so
improves the objective function
5) Return ? entries for the original mentions M .
Figure 2: High-level pseudocode for GLOW.
step, we construct for each mention mi a limited set
of candidate Wikipedia titles Ti thatmi may refer to.
Considering only a small subset of Wikipedia titles
as potential disambiguations is crucial for tractabil-
ity (we detail which titles are selected below). In the
third step, the ranker outputs the most appropriate
non-null disambiguation ti for each mention mi.
In the final step, the linker decides whether the
top-ranked disambiguation is correct. The disam-
biguation (mi, ti) may be incorrect for several rea-
sons: (1) mention mi does not have a corresponding
Wikipedia page, (2) mi does have a corresponding
Wikipedia page, but it was not included in Ti, or
(3) the ranker erroneously chose an incorrect disam-
biguation over the correct one.
In the below sections, we describe each step of the
GLOW algorithm, and the local and global features
utilized, in detail. Because we desire a system that
can process documents at scale, each step requires
trade-offs between accuracy and efficiency.
4.1 Disambiguation Candidates Generation
The first step in GLOW is to extract all mentions that
can refer to Wikipedia titles, and to construct a set
of disambiguation candidates for each mention. Fol-
lowing previous work, we use Wikipedia hyperlinks
to perform these steps. GLOW utilizes an anchor-
title index, computed by crawling Wikipedia, that
maps each distinct hyperlink anchor text to its tar-
get Wikipedia titles. For example, the anchor text
?Chicago? is used in Wikipedia to refer both to the
city in Illinois and to the movie. Anchor texts in the
index that appear in document d are used to supple-
ment the mention setM in Step 1 of the GLOW algo-
rithm in Figure 2. Because checking all substrings
Baseline Feature: P (t|m), P (t)
Local Features: ?i(t,m)
cosine-sim(Text(t),Text(m)) : Naive/Reweighted
cosine-sim(Text(t),Context(m)): Naive/Reweighted
cosine-sim(Context(t),Text(m)): Naive/Reweighted
cosine-sim(Context(t),Context(m)): Naive/Reweighted
Global Features: ?i(ti, tj)
I[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ] : avg/max
I[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/max
Table 1: Ranker features. I[ti?tj ] is an indicator variable
which is 1 iff ti links to tj or vise-versa. I[ti?tj ] is 1 iff
the titles point to each other.
in the input text against the index is computation-
ally inefficient, we instead prune the search space
by applying a publicly available shallow parser and
named entity recognition system.3 We consider only
the expressions marked as named entities by the
NER tagger, the noun-phrase chunks extracted by
the shallow parser, and all sub-expressions of up to
5 tokens of the noun-phrase chunks.
To retrieve the disambiguation candidates Ti for
a given mention mi in Step 2 of the algorithm, we
query the anchor-title index. Ti is taken to be the
set of titles most frequently linked to with anchor
text mi in Wikipedia. For computational efficiency,
we utilize only the top 20 most frequent target pages
for the anchor text; the accuracy impact of this opti-
mization is analyzed in Section 6.
From the anchor-title index, we compute two lo-
cal features ?i(m, t). The first, P (t|m), is the frac-
tion of times the title t is the target page for an an-
chor text m. This single feature is a very reliable
indicator of the correct disambiguation (Fader et al,
2009), and we use it as a baseline in our experiments.
The second, P (t), gives the fraction of all Wikipedia
articles that link to t.
4.2 Local Features ?
In addition to the two baseline features mentioned in
the previous section, we compute a set of text-based
3Available at http://cogcomp.cs.illinois.edu/page/software.
1378
local features ?(t,m). These features capture the in-
tuition that a given Wikipedia title t is more likely to
be referred to by mention m appearing in document
d if the Wikipedia page for t has high textual simi-
larity to d, or if the context surrounding hyperlinks
to t are similar to m?s context in d.
For each Wikipedia title t, we construct a top-
200 token TF-IDF summary of the Wikipedia page
t, which we denote as Text(t) and a top-200 to-
ken TF-IDF summary of the context within which
t was hyperlinked to in Wikipedia, which we denote
as Context(t). We keep the IDF vector for all to-
kens in Wikipedia, and given an input mention m in
a document d, we extract the TF-IDF representation
of d, which we denote Text(d), and a TF-IDF rep-
resentation of a 100-token window around m, which
we denote Context(m). This allows us to define
four local features described in Table 1.
We additionally compute weighted versions of
the features described above. Error analysis has
shown that in many cases the summaries of the dif-
ferent disambiguation candidates for the same sur-
face form s were very similar. For example, con-
sider the disambiguation candidates of ?China? and
their TF-IDF summaries in Figure 1. The major-
ity of the terms selected in all summaries refer to
the general issues related to China, such as ?legal-
ism, reform, military, control, etc.?, while a minority
of the terms actually allow disambiguation between
the candidates. The problem stems from the fact
that the TF-IDF summaries are constructed against
the entire Wikipedia, and not against the confusion
set of disambiguation candidates of m. Therefore,
we re-weigh the TF-IDF vectors using the TF-IDF
scheme on the disambiguation candidates as a ad-
hoc document collection, similarly to an approach
in (Joachims, 1997) for classifying documents. In
our scenario, the TF of the a token is the original
TF-IDF summary score (a real number), and the IDF
term is the sum of all the TF-IDF scores for the to-
ken within the set of disambiguation candidates for
m. This adds 4 more ?reweighted local? features in
Table 1.
4.3 Global Features ?
Global approaches require a disambiguation context
?? and a relatedness measure ? in Eq. 3. In this sec-
tion, we describe our method for generating a dis-
ambiguation context, and the set of global features
?i(t, t?) forming our relatedness measure.
In previous work, Cucerzan defined the disam-
biguation context as the union of disambiguation
candidates for all the named entity mentions in the
input document (2007). The disadvantage of this ap-
proach is that irrelevant titles are inevitably added to
the disambiguation context, creating noise. Milne
and Witten, on the other hand, use a set of un-
ambiguous mentions (2008b). This approach uti-
lizes only a fraction of the available mentions for
context, and relies on the presence of unambigu-
ous mentions with high disambiguation utility. In
GLOW, we utilize a simple and efficient alternative
approach: we first train a local disambiguation sys-
tem, and then use the predictions of that system as
the disambiguation context. The advantage of this
approach is that unlike (Milne and Witten, 2008b)
we use all the available mentions in the document,
and unlike (Cucerzan, 2007) we reduce the amount
of irrelevant titles in the disambiguation context by
taking only the top-ranked disambiguation per men-
tion.
Our global features are refinements of previously
proposed semantic relatedness measures between
Wikipedia titles. We are aware of two previous
methods for estimating the relatedness between two
Wikipedia concepts: (Strube and Ponzetto, 2006),
which uses category overlap, and (Milne and Wit-
ten, 2008a), which uses the incoming link structure.
Previous work experimented with two relatedness
measures: NGD, and Specificity-weighted Cosine
Similarity. Consistent with previous work, we found
NGD to be the better-performing of the two. Thus
we use only NGD along with a well-known Pon-
twise Mutual Information (PMI) relatedness mea-
sure. Given a Wikipedia title collection W , titles
t1 and t2 with a set of incoming links L1, and L2
respectively, PMI and NGD are defined as follows:
NGD(L1, L2) = Log(Max(|L1|, |L2|))? Log(|L1 ? L2|)Log(|W |)? Log(Min(|L1|, |L2|))
PMI(L1, L2) = |L1 ? L2|/|W ||L1|/|W ||L2|/|W |
The NGD and the PMI measures can also be com-
puted over the set of outgoing links, and we include
these as features as well. We also included a fea-
ture indicating whether the articles each link to one
1379
another. Lastly, rather than taking the sum of the re-
latedness scores as suggested by Eq. 3, we use two
features: the average and the maximum relatedness
to ??. We expect the average to be informative for
many documents. The intuition for also including
the maximum relatedness is that for longer docu-
ments that may cover many different subtopics, the
maximum may be more informative than the aver-
age.
We have experimented with other semantic fea-
tures, such as category overlap or cosine similar-
ity between the TF-IDF summaries of the titles, but
these did not improve performance in our experi-
ments. The complete set of global features used in
GLOW is given in Table 1.
4.4 Linker Features
Given the mention m and the top-ranked disam-
biguation t, the linker attempts to decide whether t is
indeed the correct disambiguation of m. The linker
includes the same features as the ranker, plus addi-
tional features we expect to be particularly relevant
to the task. We include the confidence of the ranker
in t with respect to second-best disambiguation t?,
intended to estimate whether the ranker may have
made a mistake. We also include several properties
of the mention m: the entropy of the distribution
P (t|m), the percent of Wikipedia titles in which m
appears hyperlinked versus the percent of times m
appears as plain text, whether m was detected by
NER as a named entity, and a Good-Turing estimate
of how likely m is to be out-of-Wikipedia concept
based on the counts in P (t|m).
4.5 Linker and Ranker Training
We train the coefficients for the ranker features us-
ing a linear Ranking Support Vector Machine, using
training data gathered from Wikipedia. Wikipedia
links are considered gold-standard links for the
training process. The methods for compiling the
Wikipedia training corpus are given in Section 5.
We train the linker as a separate linear Support
Vector Machine. Training data for the linker is ob-
tained by applying the ranker on the training set. The
mentions for which the top-ranked disambiguation
did not match the gold disambiguation are treated
as negative examples, while the mentions the ranker
got correct serve as positive examples.
Mentions/Distinct titles
data set Gold Identified Solvable
ACE 257/255 213/212 185/184
MSNBC 747/372 530/287 470/273
AQUAINT 727/727 601/601 588/588
Wikipedia 928/813 855/751 843/742
Table 2: Number of mentions and corresponding dis-
tinct titles by data set. Listed are (number of men-
tions)/(number of distinct titles) for each data set, for each
of three mention types. Gold mentions include all dis-
ambiguated mentions in the data set. Identified mentions
are gold mentions whose correct disambiguations exist in
GLOW?s author-title index. Solvable mentions are identi-
fied mentions whose correct disambiguations are among
the candidates selected by GLOW (see Table 3).
5 Data sets and Evaluation Methodology
We evaluate GLOW on four data sets, of which
two are from previous work. The first data set,
from (Milne and Witten, 2008b), is a subset of the
AQUAINT corpus of newswire text that is annotated
to mimic the hyperlink structure in Wikipedia. That
is, only the first mentions of ?important? titles were
hyperlinked. Titles deemed uninteresting and re-
dundant mentions of the same title are not linked.
The second data set, from (Cucerzan, 2007), is taken
from MSNBC news and focuses on disambiguating
named entities after running NER and co-reference
resolution systems on newsire text. In this case,
all mentions of all the detected named entities are
linked.
We also constructed two additional data sets. The
first is a subset of the ACE co-reference data set,
which has the advantage that mentions and their
types are given, and the co-reference is resolved. We
asked annotators on Amazon?s Mechanical Turk to
link the first nominal mention of each co-reference
chain to Wikipedia, if possible. Finding the accu-
racy of a majority vote of these annotations to be
approximately 85%, we manually corrected the an-
notations to obtain ground truth for our experiments.
The second data set we constructed, Wiki, is a sam-
ple of paragraphs from Wikipedia pages. Mentions
in this data set correspond to existing hyperlinks in
the Wikipedia text. Because Wikipedia editors ex-
plicitly link mentions to Wikipedia pages, their an-
chor text tends to match the title of the linked-to-
page?as a result, in the overwhelming majority of
1380
cases, the disambiguation decision is as trivial as
string matching. In an attempt to generate more
challenging data, we extracted 10,000 random para-
graphs for which choosing the top disambiguation
according to P (t|m) results in at least a 10% ranker
error rate. 40 paragraphs of this data was utilized for
testing, while the remainder was used for training.
The data sets are summarized in Table 2. The ta-
ble shows the number of annotated mentions which
were hyperlinked to non-null Wikipedia pages, and
the number of titles in the documents (without
counting repetitions). For example, the AQUAINT
data set contains 727 mentions,4 all of which refer
to distinct titles. The MSNBC data set contains 747
mentions mapped to non-null Wikipedia pages, but
some mentions within the same document refer to
the same titles. There are 372 titles in the data set,
when multiple instances of the same title within one
document are not counted.
To isolate the performance of the individual com-
ponents of GLOW, we use multiple distinct metrics
for evaluation. Ranker accuracy, which measures
the performance of the ranker alone, is computed
only over those mentions with a non-null gold dis-
ambiguation that appears in the candidate set. It is
equal to the fraction of these mentions for which the
ranker returns the correct disambiguation. Thus, a
perfect ranker should achieve a ranker accuracy of
1.0, irrespective of limitations of the candidate gen-
erator. Linker accuracy is defined as the fraction of
all mentions for which the linker outputs the correct
disambiguation (note that, when the title produced
by the ranker is incorrect, this penalizes linker accu-
racy). Lastly, we evaluate our whole system against
other baselines using a previously-employed ?bag of
titles? (BOT) evaluation (Milne and Witten, 2008b).
In BOT, we compare the set of titles output for a doc-
ument with the gold set of titles for that document
(ignoring duplicates), and utilize standard precision,
recall, and F1 measures.
In BOT, the set of titles is collected from the men-
tions hyperlinked in the gold annotation. That is,
if the gold annotation is { (China, People?s Repub-
lic of China), (Taiwan, Taiwan), (Jiangsu, Jiangsu)}
4The data set contains votes on how important the mentions
are. We believe that the results in (Milne and Witten, 2008b)
were reported on mentions which the majority of annotators
considered important. In contrast, we used all the mentions.
Generated data sets
Candidates k ACE MSNBC AQUAINT Wiki
1 81.69 72.26 91.01 84.79
3 85.44 86.22 96.83 94.73
5 86.38 87.35 97.17 96.37
20 86.85 88.67 97.83 98.59
Table 3: Percent of ?solvable? mentions as a function
of the number of generated disambiguation candidates.
Listed is the fraction of identified mentions m whose
target disambiguation t is among the top k candidates
ranked in descending order of P (t|m).
and the predicted anotation is: { (China, People?s
Republic of China), (China, History of China), (Tai-
wan, null), (Jiangsu, Jiangsu), (republic, Govern-
ment)} , then the BOT for the gold annotation is:
{People?s Republic of China, Taiwan, Jiangsu} , and
the BOT for the predicted annotation is: {People?s
Republic of China, History of China, Jiangsu} . The
title Government is not included in the BOT for pre-
dicted annotation, because its associate mention re-
public did not appear as a mention in the gold anno-
tation. Both the precision and the recall of the above
prediction is 0.66. We note that in the BOT evalua-
tion, following (Milne and Witten, 2008b) we con-
sider all the titles within a document, even if some
the titles were due to mentions we failed to identify.5
6 Experiments and Results
In this section, we evaluate and analyze GLOW?s
performance on the D2W task. We begin by eval-
uating the mention detection component (Step 1 of
the algorithm). The second column of Table 2 shows
how many of the ?non-null? mentions and corre-
sponding titles we could successfully identify (e.g.
out of 747 mentions in the MSNBC data set, only
530 appeared in our anchor-title index). Missing en-
tities were primarily due to especially rare surface
forms, or sometimes due to idiosyncratic capitaliza-
tion in the corpus. Improving the number of iden-
tified mentions substantially is non-trivial; (Zhou et
al., 2010) managed to successfully identify only 59
more entities than we do in the MSNBC data set, us-
ing a much more powerful detection method based
on search engine query logs.
We generate disambiguation candidates for a
5We evaluate the mention identification stage in Section 6.
1381
Data sets
Features ACE MSNBC AQUAINT Wiki
P (t|m) 94.05 81.91 93.19 85.88
P (t|m)+Local
Naive 95.67 84.04 94.38 92.76
Reweighted 96.21 85.10 95.57 93.59
All above 95.67 84.68 95.40 93.59
P (t|m)+Global
NER 96.21 84.04 94.04 89.56
Unambiguous 94.59 84.46 95.40 89.67
Predictions 96.75 88.51 95.91 89.79
P (t|m)+Local+Global
All features 97.83 87.02 94.38 94.18
Table 4: Ranker Accuracy. Bold values indicate the
best performance in each feature group. The global ap-
proaches marginally outperform the local approaches on
ranker accuracy , while combing the approaches leads to
further marginal performance improvement.
mention m using an anchor-title index, choosing
the 20 titles with maximal P (t|m). Table 3 eval-
uates the accuracy of this generation policy. We
report the percent of mentions for which the cor-
rect disambiguation is generated in the top k can-
didates (called ?solvable? mentions). We see that
the baseline prediction of choosing the disambigua-
tion t which maximizes P (t|m) is very strong (80%
of the correct mentions have maximal P (t|m) in all
data sets except MSNBC). The fraction of solvable
mentions increases until about five candidates per
mention are generated, after which the increase is
rather slow. Thus, we believe choosing a limit of 20
candidates per mention offers an attractive trade-off
of accuracy and efficiency. The last column of Ta-
ble 2 reports the number of solvable mentions and
the corresponding number of titles with a cutoff of
20 disambiguation candidates, which we use in our
experiments.
Next, we evaluate the accuracy of the ranker. Ta-
ble 4 compares the ranker performance with base-
line, local and global features. The reweighted lo-
cal features outperform the unweighted (?Naive?)
version, and the global approach outperforms the
local approach on all data sets except Wikipedia.
As the table shows, our approach of defining the
disambiguation context to be the predicted dis-
ambiguations of a simpler local model (?Predic-
tions?) performs better than using NER entities as
in (Cucerzan, 2007), or only the unambiguous enti-
Data set Local Global Local+Global
ACE 80.1 ? 82.8 80.6 ? 80.6 81.5 ? 85.1
MSNBC 74.9 ? 76.0 77.9 ? 77.9 76.5 ? 76.9
AQUAINT 93.5 ? 91.5 93.8 ? 92.1 92.3 ? 91.3
Wiki 92.2 ? 92.0 88.5 ? 87.2 92.8 ? 92.6
Table 5: Linker performance. The notation X ? Y
means that when linking all mentions, the linking accu-
racy is X , while when applying the trained linker, the
performance is Y . The local approaches are better suited
for linking than the global approaches. The linking accu-
racy is very sensitive to domain changes.
System ACE MSNBC AQUAINT Wiki
Baseline: P (t|m) 69.52 72.83 82.67 81.77
GLOW Local 75.60 74.39 84.52 90.20
GLOW Global 74.73 74.58 84.37 86.62
GLOW 77.25 74.88 83.94 90.54
M&W 72.76 68.49 83.61 80.32
Table 6: End systems performance - BOT F1. The per-
formance of the full system (GLOW) is similar to that of
the local version. GLOW outperforms (Milne and Witten,
2008b) on all data sets.
ties as in (Milne and Witten, 2008b).6 Combining
the local and the global approaches typically results
in minor improvements.
While the global approaches are most effective for
ranking, the linking problem has different charac-
teristics as shown in Table 5. We can see that the
global features are not helpful in general for predict-
ing whether the top-ranked disambiguation is indeed
the correct one.
Further, although the trained linker improves ac-
curacy in some cases, the gains are marginal?and
the linker decreases performance on some data sets.
One explanation for the decrease is that the linker
is trained on Wikipedia, but is being tested on non-
Wikipedia text which has different characteristics.
However, in separate experiments we found that
training a linker on out-of-Wikipedia text only in-
creased test set performance by approximately 3
percentage points. Clearly, while ranking accuracy
is high overall, different strategies are needed to
achieve consistently high linking performance.
A few examples from the ACE data set help il-
6In NER we used only the top prediction, because using all
candidates as in (Cucerzan, 2007) proved prohibitively ineffi-
cient.
1382
lustrate the tradeoffs between local and global fea-
tures in GLOW. The global system mistakenly links
?<Dorothy Byrne>, a state coordinator for the
Florida Green Party, said . . . ? to the British jour-
nalist, because the journalist sense has high coher-
ence with other mentions in the newswire text. How-
ever, the local approach correctly maps the men-
tion to null because of a lack of local contextual
clues. On the other hand, in the sentence ?In-
stead of Los Angeles International, for example,
consider flying into <Burbank> or John Wayne Air-
port in Orange County, Calif.?, the local ranker
links the mention Burbank to Burbank, California,
while the global system correctly maps the entity to
Bob Hope Airport, because the three airports men-
tioned in the sentence are highly related to one an-
other.
Lastly, in Table 6 we compare the end system
BOT F1 performance. The local approach proves
a very competitive baseline which is hard to beat.
Combining the global and the local approach leads
to marginal improvements. The full GLOW sys-
tem outperforms the existing state-of-the-art system
from (Milne and Witten, 2008b), denoted as M&W,
on all data sets. We also compared our system with
the recent TAGME Wikification system (Ferragina
and Scaiella, 2010). However, TAGME is designed
for a different setting than ours: extremely short
texts, like Twitter posts. The TAGME RESTful API
was unable to process some of our documents at
once. We attempted to input test documents one sen-
tence at a time, disambiguating each sentence inde-
pendently, which resulted in poor performance (0.07
points in F1 lower than the P (t|m) baseline). This
happened mainly because the same mentions were
linked to different titles in different sentences, lead-
ing to low precision.
An important question is why M&W underper-
forms the baseline on the MSNBC and Wikipedia
data sets. In an error analysis, M&W performed
poorly on the MSNBC data not due to poor disam-
biguations, but instead because the data set contains
only named entities, which were often delimited in-
correctly by M&W. Wikipedia was challenging for
a different reason: M&W performs less well on the
short (one paragraph) texts in that set, because they
contain relatively few of the unambiguous entities
the system relies on for disambiguation.
7 Conclusions
We have formalized the Disambiguation to
Wikipedia (D2W) task as an optimization problem
with local and global variants, and analyzed the
strengths and weaknesses of each. Our experiments
revealed that previous approaches for global disam-
biguation can be improved, but even then the local
disambiguation provides a baseline which is very
hard to beat.
As our error analysis illustrates, the primary re-
maining challenge is determining when a mention
does not have a corresponding Wikipedia page.
Wikipedia?s hyperlinks offer a wealth of disam-
biguated mentions that can be leveraged to train
a D2W system. However, when compared with
mentions from general text, Wikipedia mentions
are disproportionately likely to have corresponding
Wikipedia pages. Our initial experiments suggest
that accounting for this bias requires more than sim-
ply training a D2W system on a moderate num-
ber of examples from non-Wikipedia text. Apply-
ing distinct semi-supervised and active learning ap-
proaches to the task is a primary area of future work.
Acknowledgments
This research supported by the Army Research
Laboratory (ARL) under agreement W911NF-09-
2-0053 and by the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. The third
author was supported by a Microsoft New Faculty
Fellowship. Any opinions, findings, conclusions or
recommendations are those of the authors and do not
necessarily reflect the view of the ARL, DARPA,
AFRL, or the US government.
References
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), Trento, Italy, pages 9?16, April.
Ming-Wei Chang, Lev Ratinov, Dan Roth, and Vivek
Srikumar. 2008. Importance of semantic represen-
tation: dataless classification. In Proceedings of the
1383
23rd national conference on Artificial intelligence -
Volume 2, pages 830?835. AAAI Press.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Trans. on Knowl. and
Data Eng., 19(3):370?383.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling wikipedia-based named entity disam-
biguation to arbitrary web text. In Proceedings of
the WikiAI 09 - IJCAI Workshop: User Contributed
Knowledge and Artificial Intelligence: An Evolving
Synergy, Pasadena, CA, USA, July.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, Proceedings of the 19th ACM con-
ference on Information and knowledge management,
pages 1625?1628. ACM.
Tim Finin, Zareen Syed, James Mayfield, Paul Mc-
Namee, and Christine Piatko. 2009. Using Wikitol-
ogy for Cross-Document Entity Coreference Resolu-
tion. In Proceedings of the AAAI Spring Symposium
on Learning by Reading and Learning to Read. AAAI
Press, March.
Evgeniy Gabrilovich and Shaul Markovitch. 2007a.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, pages 1606?1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007b.
Harnessing the expertise of 70,000 human editors:
Knowledge-based feature generation for text catego-
rization. J. Mach. Learn. Res., 8:2297?2345, Decem-
ber.
Xianpei Han and Jun Zhao. 2009. Named entity dis-
ambiguation by leveraging wikipedia semantic knowl-
edge. In Proceeding of the 18th ACM conference on
Information and knowledge management, CIKM ?09,
pages 215?224, New York, NY, USA. ACM.
Thorsten Joachims. 1997. A probabilistic analysis of
the rocchio algorithm with tfidf for text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ?97, pages
143?151, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 457?466, New York, NY, USA. ACM.
James Mayfield, David Alexander, Bonnie Dorr, Jason
Eisner, Tamer Elsayed, Tim Finin, Clay Fink, Mar-
jorie Freedman, Nikesh Garera, James Mayfield, Paul
McNamee, Saif Mohammad, Douglas Oard, Chris-
tine Piatko, Asad Sayeed, Zareen Syed, and Ralph
Weischede. 2009. Cross-Document Coreference Res-
olution: A Key Technology for Learning by Reading.
In Proceedings of the AAAI 2009 Spring Symposium
on Learning by Reading and Learning to Read. AAAI
Press, March.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, pages 233?242, New York, NY, USA.
ACM.
David Milne and Ian H. Witten. 2008a. An effec-
tive, low-cost measure of semantic relatedness ob-
tained from wikipedia links. In In the Wikipedia and
AI Workshop of AAAI.
David Milne and Ian H. Witten. 2008b. Learning to link
with wikipedia. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
CIKM ?08, pages 509?518, New York, NY, USA.
ACM.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In proceedings of the 21st national confer-
ence on Artificial intelligence - Volume 2, pages 1419?
1424. AAAI Press.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 1335?1343, Beijing, China,
August. Coling 2010 Organizing Committee.
1384
