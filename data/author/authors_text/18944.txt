Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30?41,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring the utility of joint morphological and syntactic learning
from child-directed speech
Stella Frank
sfrank@inf.ed.ac.uk
Frank Keller
keller@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Sharon Goldwater
sgwater@inf.ed.ac.uk
Abstract
Children learn various levels of linguistic
structure concurrently, yet most existing mod-
els of language acquisition deal with only
a single level of structure, implicitly assum-
ing a sequential learning process. Developing
models that learn multiple levels simultane-
ously can provide important insights into how
these levels might interact synergistically dur-
ing learning. Here, we present a model that
jointly induces syntactic categories and mor-
phological segmentations by combining two
well-known models for the individual tasks.
We test on child-directed utterances in English
and Spanish and compare to single-task base-
lines. In the morphologically poorer language
(English), the model improves morphological
segmentation, while in the morphologically
richer language (Spanish), it leads to better
syntactic categorization. These results provide
further evidence that joint learning is useful,
but also suggest that the benefits may be dif-
ferent for typologically different languages.
1 Introduction
Models of language acquisition seek to infer lin-
guistic structure from data with minimal amounts of
prior knowledge, in order to discover which char-
acteristics of the input data are useful for learn-
ing, and thus potentially utilised by human learners.
Most previous work has focused on learning individ-
ual aspects of linguistic structure. However, children
clearly learn multiple aspects in parallel, rather than
sequentially, implying that models of language ac-
quisition should also incorporate joint learning. Joint
models investigate the interaction between different
levels of linguistic structure during learning. These
interactions are often (but not necessarily) synergis-
tic, enabling better, more robust, learning by making
use of cues from multiple sources. Recent models
using joint learning to model language acquisition
have spanned various domains including phonology,
word segmentation, syntax and semantics (Feldman
et al, 2009; Elsner et al, 2012; Doyle and Levy,
2013; Johnson, 2008; Kwiatkowski et al, 2012).
In this paper we examine the joint learning of
syntactic categories and morphology, which are ac-
quired by children at roughly the same age (Clark,
2003b), implying possible interactions in the learn-
ing process. Both morphology and word order de-
pend on categorising words based on their morpho-
syntactic function. However, previous models of
syntactic category learning have relied principally
on surrounding context, i.e., word order constraints,
whereas models of morphology use word-internal
cues. Our joint model integrates both sources of
information, allowing the model to flexibly weigh
them according to their utility.
Languages differ in the richness of their mor-
phology and strictness of word order. These char-
acteristics appear to be (anti)correlated, with rich
morphology co-occurring with free word order and
vice versa (Blake, 2001; McFadden, 2003). The
timecourse of acquisition is also influenced by lan-
guage typology: learners of morphologically rich
languages become productive in morphology ear-
lier (Xanthos et al, 2011), suggesting that richer
morphology may be more salient for learners than
impoverished morphology. Sentence comprehension
in children also shows cross-linguistic differences
in the cues used to make sense of non-canonical
sentence structure: learners of a morphologically
rich language (Turkish) disregard word order in
30
favour of morphology, whereas learners of En-
glish favour word order (Slobin, 1982; MacWhin-
ney et al, 1984). These interactions between mor-
phology and word order suggest that a joint model
will be better able to support the differences in cue
strength (rich morphology versus strict word order),
and thus be more language-general, than single-task
models.
Both syntactic category and morphology induc-
tion have been the focus of much recent work. (See
Hammarstro?m and Borin (2011) for an overview
of unsupervised morphology learning, likewise
Christodoulopoulos et al (2010) for a comparison
of part of speech/syntactic category induction sys-
tems.) However, given the tightly coupled nature of
these two tasks, there has been surprisingly little
work in joint learning of morphology and syntac-
tic categories. Systems for inducing syntactic cat-
egories often make use of morpheme-like features,
such as word-final characters (Smith and Eisner,
2005; Haghighi and Klein, 2006; Berg-Kirkpatrick
et al, 2010; Lee et al, 2010), or model words
at the character-level (Clark, 2003a; Blunsom and
Cohn, 2011), but do not include morphemes ex-
plicitly. Other systems (Dasgupta and Ng, 2007;
Christodoulopoulos et al, 2011) use morphologi-
cal segmentations learned by a separate morphology
model as features in a pipeline approach.
Models of morphology induction generally oper-
ate over a lexicon, i.e. a list of word types, rather
than token corpora (Goldsmith, 2006; Creutz and
Lagus, 2007; Kurimo et al, 2010). These models
find morphological categories on the basis of word-
internal features, without taking syntactic context
into account (which is of course not available in a
lexicon).
Lee et al (2011) and Sirts and Aluma?e (2012)
present models that infer morphological segmenta-
tions and syntactic categories jointly, although Lee
et al (2011) do not evaluate the inferred syntactic
categories. Both make use of a word-type constraint
which limits each word form to a single analysis
(i.e., all instances of ducks are assigned to a single
category and will have the same morpheme analy-
sis, ignoring the gold standard distinction between a
plural noun and third person singular verb). This can
make inference more tractable, and often increases
performance, but does not respect the ambiguity in-
herent in natural language, both over syntactic cat-
egories and morphological analyses. The degree of
ambiguity is language dependent, so that even if a
type-constraint is perhaps relatively unproblematic
in English, it will pose problems in morphologically
richer languages. Furthermore, these two models
make use of an array of heuristics that may not allow
them to be easily generalisable across languages and
datasets (e.g., likelihood scaling (Sirts and Aluma?e,
2012), sequential suffix matching (Lee et al, 2011)).
In this paper, we present a joint model composed
of two well-known individual models. This allows
us to cleanly investigate the effects of joint learning
and its potential benefits over the single task models.
The simplicity of our models also allows us to avoid
modelling and inference heuristics.
Previous models have used adult-directed written
texts, which differs significantly from the type of
language available to child learners. We test our joint
model on child-directed utterances in English (a
morphologically poor language) and Spanish (with
richer morphology)1. Our results indicate that our
joint model is able to flexibly accommodate lan-
guages with differing levels of morphological rich-
ness. The joint model matches the performance of
single task models on both tasks, demonstrating that
the additional complexity is not a problem (i.e., it
does not add noise). Moreover, the joint model im-
proves performance significantly on the task corre-
sponding to the language?s weaker cue, indicating a
transfer of information from the stronger cue. The
fact that the nature of this improvement varies by
language provides evidence that joint learning can
effectively accommodate typological diversity.
2 Model
The task is to assign word tokens to part of speech
categories and simultaneously segment the tokens
into morphemes. We assume a relatively simple yet
commonly used concatenative morphology which
models a word as a stem plus (possibly null) suffix2.
1There are languages with much richer morphology than
Spanish, but none with a child-directed corpus suitably anno-
tated for evaluation.
2Fullwood and O?Donnell (2013) recently presented a
model of non-concatenative morphology that could be inte-
grated into this model; however, it does not perform well on En-
glish (and presumably other mostly concatenative languages).
31
Since this is an unsupervised model, the inferred cat-
egories and morphemes lack meaningful labels, but
ideally will correspond to gold standard categories
and morphemes.
2.1 Word Order
We model a sequence of words as a Hidden Markov
Model (HMM) with a non-parametric emission dis-
tribution. As usual, the latent states of the HMM rep-
resent syntactic categories. The tag sequence is gen-
erated by a trigram Dirichlet-multinomial distribu-
tion, where transition parameters ? are drawn from
a symmetric Dirichlet distribution with the hyperpa-
rameter ?t . Each tag ti in the sequence is then drawn
from the transition distribution conditioned on the
previous two tags:
?(t,t ?) ? Dir(?t)
ti = t|ti?1 = t
?, ti?2 = t
??,?? Mult(?(t ?,t ??))
This model is token-based, permitting different
tokens of the same word type to have different
syntactic categories. Most recent models have in-
cluded a constraint forcing all tokens of a given
type into the same category, which improves per-
formance but often complicates inference. The
Bayesian HMM?s performance is therefore not state-
of-the-art, but is comparable to other token-based
models (Christodoulopoulos et al, 2010) and the
model is easy to extend within the Bayesian frame-
work, allowing us to compare multiple versions.
This part of the model is parametric, operat-
ing over a fixed number of tags T , and is iden-
tical to the formulation of tag transitions in the
Bayesian HMM (Goldwater and Griffiths, 2007).
However, we replace the BHMM?s emission dis-
tribution with the morphologically-informed distri-
butions described below. As in the BHMM, the
emission distributions are conditioned on the tag,
i.e., each tag has its own morphology.
2.2 Morphology
The morphology model introduced by Goldwater
et al (2006) generates morphological analyses for a
set of tokens. These analyses consist of a tag plus a
stem and suffix pair, which are concatenated to form
the observed words. Both stem s and suffix f are
generated from Dirichlet-multinomials conditioned
on the tag t:
?? Dir(??)
t|?? Mult(?)
?? Dir(?s)
s|t,?? Mult(?t)
?? Dir(? f )
f |t,?? Mult(?t)
The ?s are hyperparameters governing the Dirich-
let distributions from which the multinomials ?,?,?
are drawn. In turn, t,s, and f are drawn from these
multinomials.
The probability of a word under this model is the
sum of the probabilities of all possible analyses l =
(t,s, f ):
P0(w) =?
l
P0(l) = ?
t,s, f s.t.
s? f=w
P(s|t)P( f |t)P(t) (1)
where s? f = w denotes that the concatenation of
stem and suffix results in the word w.
On its own, this distribution over morphologi-
cal analyses makes independence assumptions that
are too strong: most word tokens of a word type
have the same analysis, but P0 will re-generate
that analysis for every token. To resolve this prob-
lem, a Pitman-Yor process (PYP) is placed over the
generating distribution above. The Pitman-Yor pro-
cess has been found to be useful for representing
the power-law distributions common in natural lan-
guage (Teh, 2006; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011).
The distribution of draws from a Pitman-Yor pro-
cess (which, in our case, determines the distribu-
tion of word tokens with each morphological anal-
ysis) is commonly described using the metaphor of
a Chinese restaurant. A series of customers (tokens
z = z1 . . .zN) enter a restaurant with an infinite num-
ber of initially empty tables. Upon entering, each
customer is seated at a table k with probability
p(zi = k|z1 . . .zi?1,a,b) = (2)
{
nk?a
i?1+b if 1? k ? K
Ka+b
i?1+b if k = K +1
32
tk sk
fk lk
K
zi
wi
N
Figure 1: Plate diagram depicting the morphology model
(adapted from Goldwater et al (2006)). Hyperparameters
have been omitted for clarity. The left-hand plate depicts
the base distribution P0; note that the morphological anal-
yses lk are generated deterministically as (tk,sk, fk). The
observed words wi are also deterministic given zi = k and
lk, since wi = sk? fk.
where nk is the number of customers already sitting
at table k, K is the total number of tables occupied by
the i?1 previous customers, and 0? a < 1 and b? 0
are hyperparameters of the process. The probability
of being seated at a table increases with the number
of customers already seated at that table, creating a
?rich-get-richer? power-law distribution of tokens to
tables; a and b control the amount of reuse of exist-
ing tables, with smaller values leading to more reuse.
Crucially, each table serves a dish generated by
the base distribution P0?i.e., the dish is a morpho-
logical analysis lk = (t,s, f )?and all the customers
seated at the same table share the same dish, which
is generated only once (at the point when that table
is first occupied). The model can thus reuse the anal-
ysis for a particular word and avoid regenerating the
same analysis multiple times. Note that multiple ta-
bles may have identical analyses, lk = lk? . Figure 1
illustrates how the full PYP morphology model gen-
erates the observed sequence of word tokens.
2.3 Combined Model
The full model (Figure 2) combines the latent tag se-
quence with the morphology model. Tag tokens are
generated conditioned on local context, not the base
distribution, as in the morphology model. Instead of
a single PYP generating morphological analyses for
all tokens, as in the Goldwater et al (2006) model,
we have a separate PYP for each tag type, i.e., each
tag has its own restaurant with its own customers
(the tokens labeled with that tag) and its own mor-
phological analyses. The distribution of customers
ti?2 ti?1 ti zi
wi
sk fk
lk
N
Kt
T
Figure 2: Plate diagram depicting the joint model. Hyper-
parameters have been omitted for clarity. The L-shaped
plate contains the tokens, while the square plates contain
the morphological analyses. The t are latent tags, zi is an
assignment to a morphological analysis lk = (sk, fk), and
wi is the observed word. T is the number of distinct tags,
and Kt the number of tables used by tag type t.
in each of the tag-specific restaurants is still deter-
mined by Equation 2, except that all of the counts
and indices are with respect to only the tokens and
tables assigned to that tag.
Each tag-specific PYP (restaurant) also has a sep-
arate base distribution, P(t)0 , resulting in distinct dis-
tributions over stems and suffixes for each tag. The
analyses generated by the base distributions consist
of (stem, suffix) pairs; the tag is given by the identity
of the generating PYP.
P(t)0 (w) =?
l
P(t)0 (l = (s, f )) = ?
s, f s.t.
s? f=w
P(s|t)P( f |t)
(3)
The full joint posterior distribution of a sequence
of words, tags, and morpheme analyses is shown in
Figure 3. Note that all tag-specific morphology mod-
els share the same Pitman-Yor parameters a and b.
3 Inference
We use Gibbs sampling for inference over the three
sets of discrete variables: tags t, their assignments to
morphological analyses (tables) z, and the analyses
themselves l.
Each iteration of the sampler has two stages: First
the morphological analyses l are sampled, and then
each token samples a new tag and a new assignment
to an analysis/table. Because the table assignments
33
P(t, l,z|?t ,a,b,?s,? f ) =P(t|?t)P(l|t,?s,? f )P(z|a,b) (4)
P(t|?t) =
N
?
i=2
P(ti|ti?1, ti?2,t1...i?1,?t) =
T
?
t,t ?=1
?(T?t)
?(ntt ? +T?t)
T
?
t ??=1
?(ntt ?t ?? +?t)
?(?t)
(5)
P(l|t,?s,? f ) =
T
?
t=1
Kt
?
k=1
Pt(lk = (s, f )|l1...k?1,?s,? f ) (6)
=
T
?
t=1
?(S?s)
?(mt +S?s)
?(F? f )
?(mt +F? f )
S
?
s=1
?(mts +?s)
?(?s)
F
?
f=1
?(mt f +? f )
?(? f )
(7)
P(z|a,b) =
T
?
t=1
Nt
?
i=1
P(zi|t,z1...i?1,a,b) (8)
=
T
?
t=1
?(1+b)
?(nt +b)
Kt
?
k=1
(ka+b)
?(nk?a)
?(1?a)
(9)
Figure 3: The posterior distribution of our joint model. Because the sequence of words w is deterministic given
analyses l and assignments to analyses (tables) z, the joint posterior over all variables P(w,t, l,z|?t ,a,b,?s,? f ) is
equal to P(t, l,z|?t ,a,b,?s,? f ) when lzi = wi for all i, and 0 otherwise. We give equations for the non-zero case. ns
refer to token counts, ms to table counts. We add two dummy tokens at the start, end, and between sentences to pad
the context history.
are conditioned on tags (i.e., a token must be as-
signed to a table in the correct PYP restaurant) re-
sampling the tag requires immediate resampling of
the table assignment as well.
3.1 Initialization
The tags are initialized uniformly at random. For
each token, a segmentation point is chosen uni-
formly at random (we disallow segmentations with
a null stem). If this segmentation is new within the
PYP associated with that token?s tag, a new table is
created for the token in that PYP. If it matches an ex-
isting analysis, zi is sampled from the existing tables
k plus a possible new table k?.
3.2 Morphological Analyses
Each lk represents the morphological analysis for the
set of tokens assigned to table k. Resampling the
segmentation point (stem and suffix identity) of the
analysis changes the segmentation of all of the word
tokens assigned to that analysis. Note that the tag is
not included in lk in the combined model, because
the tag identity is dependent on the local contexts of
all the tokens seated at the table.
Analyses are sampled from a product of Dirichlet-
multinomial posteriors as follows:
p(lk = (s, f )|t, l
\k) =
m\ks +?s
m\k +S?s
m\kf +? f
m\k +F? f
(10)
where ms and m f are the number of analyses for
this tag that share a stem or suffix with lk, and m
is the total number of analyses for this tag. S and
F are the total number of stems and suffixes in the
model. l\k indicates that the current analysis lk has
been removed from the distribution and the appro-
priate counts, to create the correct conditioning dis-
tribution for the Gibbs sampler.
3.3 Tags
Tags are sampled from the product of posteri-
ors of the transition and emission distributions.
The transition distribution is a standard Dirichlet-
multinomial posterior. Calculating the emission dis-
tribution probability, i.e. the marginal probability of
the word given the tag, involves summing over the
probability of all the existing tables in the given PYP
that emit the correct word, plus the probability of
a new table being created, which also includes the
probability of a new analysis from P(t)0 .
34
More precisely, tags are sampled from the follow-
ing distribution:
p(ti = t|wi = w,t\i,z\i, l,?t ,a,b) (11)
? p(ti = t|ti?1, ti?2,t\i,?t)? p(w|t,z\i, l)
= p(ti = t|ti?1, ti?2,t\i,?t)
? ( ?
k s.t. lk=w
p(zi = k|t,w,z\i)+ p(zi = knew|t,w,z\i))
=
nti?2ti?1t +?t
nti?2ti?1 +T?t
? ( ?
k s.t. lk=w
nk?a
nt +b
+
Kta+b
nt +b
P(t)0 (w))
where lk = w matches tables compatible with w,
i.e., the concatenation of stem and suffix form the
word, slk ? flk = w. nk is the number of words as-
signed to the table k and Kt is the total number of
tables in the PYP for tag t. Note that all counts are
obtained after the removal of the current ti and zi,
i.e., from t\i and z\i.
3.4 Table Assignments
Once a new tag has been sampled for a token, the ta-
ble assignment must be resampled conditioned on
the new tag. The assignment zi is drawn over all
compatible tables in the tag?s PYP (that is, where
lk = w), plus a possible new table:
p(zi = k|ti = t,w,z\i,a,b) ? (12)
{
nk?a
nt+b
if 1? k ? Kt
Kt a+b
nt+b
P(t)0 (w) if k = Kt +1
P(t)0 is calculated by summing over the probability
of all possible segmentations for a new analysis for
word wi, using Equation 3. If a new table is drawn
(k > Kt) then we also sample a new analysis for that
table from P(t)0 .
4 Preliminary Experiments
An important argument for joint learning is that it
affords increased flexibility and robustness across a
wider range of input data. A model that relies on
word order cannot learn syntactic categories from a
morphologically complex language with free word
order; likewise a model attempting to categorise
words using morphology alone will fail on a lan-
guage without morphology. An effective joint model
Language A
abdc fefh pomo rtut usst
cdcc bcba gghh npop npoo
cdca aaaa fefh hfeg pnon
Language B
noom.no usrs.st bbdb.ac cbab.cc cdaa.cc
rttt.uu cbab.aa mnom.oo ccda.bc onmm.om
rruu.ts npop.mm gehg.fh trrt.uu tssu.uu
Table 1: Example sentences in the synthetic languages.
Words in Category 1 are made of characters a-d, Cate-
gory 2 e-h, Category 3 m-p, Category 4 r-u. Suffixes in
Language B are separated with periods (.) for illustrative
purposes only.
will be able to make use of the different cues in both
language types in a flexible way.
In order to test the proposed model, we run two
experiments on synthetic languages, which simulate
languages in which either word order or morphology
is the sole cue. Most natural languages fall between
these extremes, but these experiments show that our
model can capture the full spectrum.
Language A is a strict word order language lack-
ing morphology. It has a vocabulary of 200 word
types, split into four different categories. The 50
word types in each category are created by com-
bining four letters, with replacement, into four-letter
words, with a different set of letters used in each cat-
egory3. Words within a category may thus share be-
ginning or ending characters, which could be posited
as stems or suffixes by the model, but since only
50 of 256 possible strings are used, there will be
no strong evidence for consistent stem and suffixes
(i.e. stems appearing with multiple suffixes and vice
versa). Each sentence in Language A consists of five
words in one of twenty possible category sequences.
In these sequences, each category is either followed
by itself or the next category (i.e. [2,2,2,3,4] is valid
but [2,4,3,1,4] is not). Word order is thus strongly
constrained by category membership.
Language B has free word order, with category
membership signalled by suffixes. Words are cre-
3We achieved the same results with a language using the
same four characters in all categories, but using different char-
acters makes the categories human-readable. The model does
not have a orthographic/phonological component and so will
not recognise the within-category similarity, other than possi-
bly positing spurious stems or suffixes.
35
-70000
-60000
-50000
-40000
-30000
-20000
-10000
 0
 0  200  400  600  800  1000
L
o
g
 
P
r
o
b
a
b
i
l
i
t
y
Iteration
LangA TotalLangA TransitionsLangA MorphologyLangB TotalLangB TransitionsLangB Morphology
Figure 4: Log probability of the sampler state over 1000
iterations on Languages A and B.
ated by the concatenation of a stem and a suffix,
where the stems are the same as the words in lan-
guage A (50 stems in each of four categories). One
of six category-specific suffixes is appended to each
stem, resulting in 300 word types per category. Each
suffix is two letters long, created by combining three
possible letters (the same letters used to create the
stems), thus making mis-segmentation possible (for
instance, up to three of the suffixes could have the
same final letter). Sentences are again five words
long, but the sequence of categories is drawn at ran-
dom, resulting in uniformly random word order. See
Table 1 for example sentences in both languages.
We create a 5000 word corpus for each language,
and run our model on these corpora. Hyperparame-
ters are set to the same values in both languages4.
We run the sampler on each dataset for 1000 it-
erations with simulated annealing. In both cases,
the correct solution is found by iteration 500. Fig-
ure 4 shows that the morphology component con-
tinues to increase the log probability by increasing
the number of tokens seated at a table. Note that
the correct solution in Language A involves learn-
ing a very peaked transition distribution as well as an
even more extreme distribution over suffixes (where
only the null suffix has high probability), whereas
the same distributions in Language B are much flat-
ter. The fact that the same hyperparameter setting is
4The PYP parameters are set to a = 0.1,b = 1.0 and the
HMM transition parameter ?t = 1.0; the parameters in the base
distribution are ?s,? f = 0.001,?k = 0.5.
able to correctly identify the two language extremes
indicates that the model is robust to hyperparameter
values.
These experiments demonstrate that our joint
model is able to learn correctly even when only ei-
ther morphology or word order is informative in a
language. We now turn to acquisition data from nat-
ural languages in which both morphology and word
order are useful cues but to varying degrees.
5 CDS Experiments
5.1 Data
We use two corpora, Eve (Brown, 1973) and Or-
nat (Ornat, 1994), from the CHILDES database
(MacWhinney, 2000). These corpora consist of the
child-directed utterances heard by two children,
the former learning English and the latter Spanish.
These have been annotated for part of speech cate-
gories and morphemes.
The CHILDES corpora are tagged with a very rich
set of part of speech tags (74 tags), which we col-
lapse to a smaller set of tags5. The Eve corpus has
61224 tokens and is thus larger than the Spanish cor-
pus, which has 40497 tokens. However, the English
corpus has only 17 gold suffix types, while Spanish
has 83. The increased richness of Spanish morphol-
ogy also has an effect on the number of word types in
the corpus: the Spanish dataset has 3046 word types,
whereas the larger English dataset has only 1957.
Morphology is annotated using a stem-affix en-
coding which does not directly correspond to our
segmentation-based model. The word running is an-
notated as run-ING, jumping as jump-ING; the anno-
tation is thus agnostic about ortho-morphemic seg-
mentation (i.e., whether to segment as run.ning or
runn.ing), whereas the model is forced to choose
a segmentation point. Syncretic suffixes (sharing
an identical surface form) are disambiguated: sings
is annotated as sing-3S, plums as plum-PL. Con-
versely, the annotation scheme merges allomorphs
into a single suffix: infinitive verbs in Spanish,
for instance, are encoded as ending with -INF,
corresponding to -ar, -er, and -ir surface forms.
5These are 13 for English (ADJ, ADV, AUX, CONJ, DET,
INF, NOUN, NEG, OTH, PART, PREP, PRO, VERB) and 10
for Spanish, since the gold standard does not distinguish AUX,
PART or INF.
36
We ignore irregular/non-affixing forms annotated
with & (e.g. was, annotated as be&PAST) and
use only hyphen-separated suffixes to evaluate.
Where multiple suffixes are concatenated together
(e.g., dog-DIM-PL) we treat this as a single suffix
(-DIM-PL) for evaluation purposes.
In Spanish, many words are annotated as having
a suffix of effectively zero length, e.g. the imper-
ative gusta is annotated as gusta-2S&IMP. We re-
place these suffixes (where the stem is equal to the
word) with a null suffix, excluding them from eval-
uation, as they are impossible for a segmentation-
based model to find.
5.2 Evaluation
Tags are evaluated using VM (Rosenberg and
Hirschberg, 2007), as has become standard for this
task (Christodoulopoulos et al, 2010). VM is a mea-
sure of the normalised cross-entropy between gold
and proposed clusters; it ranges between 0 and 100,
with higher scores being better.
We also use VM to evaluate the morphological
segmentation: all tokens with a common suffix are
clustered together, and these clusters are compared
against the gold suffix clusters6. Using a clustering
metric avoids the need to evaluate against a gold seg-
mentation point (which the annotation lacks). Tag
membership is added to the non-null model suffixes,
so that a final -s suffix found in tag 2 is distinguished
from the same suffix found in tag 8 (creating suffixes
-s-T8 and -s-T2), analogous to the gold annotation
distinction between syncretic morphemes -PL and
-3S.
Note that ceiling performance of our model on
Suffix VM will be below 100, since our model can-
not cluster allomorphs, which are represented by a
single abstract morpheme in the gold standard.
5.3 Baselines
We test the full model, MORTAG, against a number
of variations to investigate the advantages of jointly
modelling the two tasks.
Two variants remove the transition distributions,
and thus local syntactic context, from the model.
6We also evaluated stem morpheme clusters and found near-
ceiling performance due to the high number of null-suffix words
in both corpora.
MORTAGNOTRANS is the full model without tran-
sitions between tag tokens; morphology PYP draws
remain conditioned on token tags. We add a Dirich-
let prior over tags (?t = 0.1) to encourage tag spar-
sity (analogous to the transition distribution in the
full model). MORCLUSTERS is the original model
of Goldwater et al (2006), in which tags (called
clusters in the original) are drawn by P0.
MORTAGNOSEG is a variant in which the only
available suffix is the null suffix; thus segmentations
are trivial and only tags are inferred. This model
is approximately equivalent to a simple Bayesian
HMM but with the addition of PYPs within the
emission distribution. We also evaluate against tags
found by the BHMM, with a Dirichlet-multinomial
emission distribution and no morphology.
MORTAGTRUETAGS is the full model but with all
tags fixed to their gold values. This model gives us
oracle-type results for morphology. (Due to the an-
notation scheme used in CHILDES, oracle morpho-
logical segmentations are unavailable, so we were
unable to test a model with gold morphology and in-
ferred tags.)
5.4 Experimental Procedure
Hyperparameter values for the Pitman-Yor process
were found using grid search on a development set
(Section 10 of Eve and Section 8 of Ornat; these sec-
tions are removed from the dataset we report results
on). We use the values which give the best Suffix
VM performance on the development data; however
we stress that the development results did not vary
greatly over a wide range of hyperparameter values,
and only deteriorated significantly at extreme values
of a.
There are a number of other hyperparameters in
the model which we set to fixed values. The transi-
tion hyperparameter ?t is set to 0.1 in all models.
We set the hyperparameters for the stem and suf-
fix distributions in the morphology base distribution
P0 to 0.001 for both ?s and ? f ; ?k over tags in the
MORCLUSTERS model is set to 0.5. The number of
possible stems and suffixes is given by the dataset: in
the Eve dataset there are 5339 candidate stems and
6617 candidate suffixes; in the Ornat dataset these
numbers are 8649 and 6598, respectively. The num-
ber of tags available to the model is set to the number
of gold tags in the data.
37
Tag VM Suffix VM
MORTAG 59.1(1.9) 41.9(10.0)
MORCLUSTERS 22.4(1.0)? 28.0(11.9)?
MORTAGNOTRANS 19.3(1.2)? 24.4(5.2)?
MORTAGNOSEG 59.4(1.7) ?
BHMM 56.2(2.3)? ?
MORTAGTRUETAGS ? 42.5(5.2)
Table 2: English Eve corpus results. Standard deviations
are in parentheses; ? denotes a significant difference from
the MORTAG model.
Sampling is run for 5000 iterations with anneal-
ing. Inspection of the posterior log-likelihood indi-
cates that the models converge after about 1000 it-
erations. We run inference over all models ten times
and report the average performance. Significance is
reported using the non-parametric Wilcoxon rank-
sum test with a significance level of ?< 0.05.
5.5 Results: English
Results on the English Eve corpus are shown in Ta-
ble 2. We use PYP parameters a = 0.3 and b = 10,
though we found similar performance over a wide
range of values of a and b. Our results show a clear
improvement in the morphological segmentations
found by the joint model and stable tagging perfor-
mance across all models with context information.
The syntactic clusters found by models using
only morphological patterns, MORTAGNOTRANS
and MORCLUSTERS, are clearly inferior and lead to
low Tag VM results. The models with local syntac-
tic context all perform approximately equally well
in terms of finding tags. We find no improvement on
tagging performance in English when adding mor-
phology, compared to the MORTAGNOSEG base-
line in which words are not segmented. However, we
do see a small but significant improvement over the
BHMM for both of these models, due to the replace-
ment of the multinomial emission distribution in the
BHMM with the PYP.
Morphological segmentations, as measured by
Suffix VM, clearly improve with the addition of lo-
cal contexts (and the ensuing better tags): the full
model outperforms the baselines without syntactic
contexts. On this dataset, the joint MORTAG model
even matches the performance of the model us-
Tag VM Suffix VM
MORTAG 43.4(2.6) 41.4(2.5)
MORCLUSTERS 20.3(2.5)? 46.5(3.2)
MORTAGNOTRANS 14.4(1.7)? 36.4(2.0)?
MORTAGNOSEG 39.6(3.7)? ?
BHMM 36.4(0.7)? ?
MORTAGTRUETAGS ? 59.8(0.4)?
Table 3: Spanish Ornat corpus results. Standard devia-
tions are in parentheses; ? denotes a significant difference
from the MORTAG model.
ing oracle tags. The standard deviation over Suf-
fix VM scores is quite large for MORTAG and
MORCLUSTERS; this is due to frequent words hav-
ing two high probability segmentations (most no-
tably is, which in some runs was segmented as i.s).
5.6 Results: Spanish
For the Spanish Ornat corpus, we found slightly dif-
ferent optimal PYP hyperparameters and set a = 0.1
and b = 0.1. Results are shown in Table 3.
The Spanish results pattern in the opposite way
as English. Here we see a statistically significant
improvement in tagging performance of the full
joint model over both models without morphology
(MORTAGNOSEG and BHMM). Models without
context information again find much worse tags,
mainly because (as in English) function words are
not identifiable by suffixes.
However, the full model does not find better mor-
phological segmentations than the MORCLUSTERS
model, despite better tags (the two models? Suffix
VM scores are not statistically significantly differ-
ent). We also see that the difference between the seg-
mentations found by the model using gold tags and
estimated tags is quite large. This is due to the ora-
cle model finding the rarer suffixes which were not
distinguished by the models with noisier tags. This
demonstrates the importance of syntactic categorisa-
tion for the morpheme induction task, and suggests
that a more sophisticated tagging model (with better
performance) may yet improve morpheme segmen-
tation performance in Spanish.
38
6 Conclusion
We have presented a model of joint syntactic cate-
gory and morphology induction. Operating within a
generative Bayesian framework means that combin-
ing single-task components is straightforward and
well-founded. Our model is token-based, allowing
for syntactic and morphemic ambiguity.
To our knowledge, this is the first joint model to
be tested on child-directed speech data, which is less
complex than the newswire corpora used by previ-
ous joint models. Child-directed speech may be sim-
ple enough for joint learning not to be necessary: our
results indicate the contrary, namely that joint learn-
ing is indeed helpful when learning from realistic
acquisition data.
We tested this model on two languages with dif-
ferent morphological characteristics. On English, a
language with relatively little morphology, espe-
cially in child directed speech, we found that bet-
ter categorisation of words yielded much better mor-
phology in terms of suffixes learned. Conversely, in
Spanish we saw less difference on the morphology
task between models with categories inferred solely
from morphemic patterns and models that also used
local syntactic context for categorisation. However,
in Spanish we saw an improvement in the tagging
task when morphology information was included.
This suggests that English and Spanish make dif-
ferent word-order and morphology trade-offs. In En-
glish, local context provides at least as much in-
formation as morphology in terms of determining
the correct syntactic category, but knowing a good
estimate of the correct syntactic category is use-
ful for determining a word?s morphology. In Span-
ish, a word?s morphology can more easily be deter-
mined simply by looking at frequent suffixes within
a purely morphological system. On the other hand,
word order is freer, making local syntactic context
unreliable, so taking morphological information into
account can improve tagging. These differences be-
tween languages demonstrate the benefits of joint
learning, which enables the learner to more flexibly
utilise the information available in the input data.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,
John DeNero, and Dan Klein. Painless unsuper-
vised learning with features. In Proceedings of the
North American Association for Computational
Linguistics (NAACL), 2010.
Barry J. Blake. Case. Cambridge University Press,
2001.
Phil Blunsom and Trevor Cohn. A hierarchical
Pitman-Yor process HMM for unsupervised part
of speech induction. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2011.
Roger Brown. A first language: The early stages.
Harvard University Press, Cambridge, MA, 1973.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. A Bayesian mixture model
for part-of-speech induction using multiple fea-
tures. In Proceedings of the 16th Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), 2011.
Alexander Clark. Combining distributional and
morphological information for part of speech in-
duction. In Proceedings of the 10th annual Meet-
ing of the European Association for Computa-
tional Linguistics (EACL), 2003a.
Eve V. Clark. First Language Acquisition. Cam-
bridge University Press, 2003b.
Mathias Creutz and Krista Lagus. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing, 4(1):1?34, 2007.
Sajib Dasgupta and Vincent Ng. Unsupervised part-
of-speech acquisition for resource-scarce lan-
guages. In Proceedings of the 12th Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), 2007.
Gabriel Doyle and Roger Levy. Combining multi-
ple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL-HLT 2013, pages
117?126, 2013.
39
Micha Elsner, Sharon Goldwater, and Jacob Eisen-
stein. Bootstrapping a unified model of lexical
and phonetic acquisition. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2012.
Naomi Feldman, Thomas Griffiths, and James Mor-
gan. Learning phonetic categories by learning a
lexicon. In Proceedings of the 31st Annual Con-
ference of the Cognitive Science Society (CogSci),
2009.
Michelle A. Fullwood and Timothy J. O?Donnell.
Learning non-concatenative morphology. In Pro-
ceedings of the Workshop on Cognitive Modeling
and Computational Linguistics, 2013.
John Goldsmith. An algorithm for the unsupervised
learning of morphology. Natural Language Engi-
neering, 12(4):353?371, December 2006.
Sharon Goldwater and Thomas L. Griffiths. A
fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Interpolating between types and to-
kens by estimating power-law generators. In Ad-
vances in Neural Information Processing Systems
18, 2006.
Aria Haghighi and Dan Klein. Prototype-driven
grammar induction. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2006.
Harald Hammarstro?m and Lars Borin. Unsupervised
learning of morphology. Computational Linguis-
tics, 37(2):309?350, 2011.
Mark Johnson. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2008.
Mikko Kurimo, Sami Virpioja, and Ville T. Turunen.
Proceedings of the MorphoChallenge 2010 work-
shop. Technical Report TKK-ICS-R37, Aalto
University School of Science and Technology, Es-
poo, Finland, 2010.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-
moyer, and Mark Steedman. A probabilistic
model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL), 2012.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Simple type-level unsupervised POS tagging.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of
Fifteenth Conference on Computational Natural
Language Learning, 2011.
Brian MacWhinney. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ, 2000.
Brian MacWhinney, Elizabeth Bates, and Reinhold
Kliegl. Cue validity and sentence interpretation
in English, German, and Italian. Journal of Ver-
bal Learning and Verbal Behavior, 23:127?150,
1984.
Thomas McFadden. On morphological case and
word-order freedom. In Proceedings of the An-
nual Meeting of the Berkeley Linguistics Society,
volume 29, pages 295?306, 2003.
S. Lopez Ornat. La adquisicion de la lengua espag-
nola. Siglo XXI, Madrid, 1994.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
12th Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), 2007.
Kairit Sirts and Tanel Aluma?e. A hierarchical
Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of
the Conference of the North American Chapter
of the Association for Computational Linguistics
(NAACL), 2012.
Dan Slobin. Universal and particular in the acqui-
sition of language. In Eric Wanner and Lila R.
Gleitman, editors, Language acquisition: the state
40
of the art, pages 128?170. Cambridge University
Press, 1982.
Noah A. Smith and Jason Eisner. Contrastive esti-
mation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL), 2005.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
2006.
Aris Xanthos, Sabine Laaha, Steven Gillis, Ursula
Stephany, Ayhan Aksu-Koc?, Anastasia Christofi-
dou, Natalia Gagarina, Gordana Hrzica, F. Ni-
han Ketrez, Marianne Kilani-Schoch, Katharina
Korecky-Kro?ll, Melita Kovac?evic?, Klaus Laalo,
Marijan Palmovic?, Barbara Pfeiler, Maria D.
Voeikova, and Wolfgang U. Dressler. On the role
of morphological richness in the early develop-
ment of noun and verb inflection. First Language,
31(4):461?479, 2011.
41
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1073?1083,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Weak semantic context helps phonetic learning
in a model of infant language acquisition
Stella Frank
sfrank@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Naomi H. Feldman
nhf@umd.edu
Department of Linguistics
University of Maryland
College Park, MD, 20742, USA
Sharon Goldwater
sgwater@inf.ed.ac.uk
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Abstract
Learning phonetic categories is one of the
first steps to learning a language, yet is hard
to do using only distributional phonetic in-
formation. Semantics could potentially be
useful, since words with different mean-
ings have distinct phonetics, but it is un-
clear how many word meanings are known
to infants learning phonetic categories. We
show that attending to a weaker source of
semantics, in the form of a distribution over
topics in the current context, can lead to
improvements in phonetic category learn-
ing. In our model, an extension of a pre-
vious model of joint word-form and pho-
netic category inference, the probability of
word-forms is topic-dependent, enabling
the model to find significantly better pho-
netic vowel categories and word-forms than
a model with no semantic knowledge.
1 Introduction
Infants begin learning the phonetic categories of
their native language in their first year (Kuhl et al,
1992; Polka and Werker, 1994; Werker and Tees,
1984). In theory, semantic information could offer
a valuable cue for phoneme induction
1
by helping
infants distinguish between minimal pairs, as lin-
guists do (Trubetzkoy, 1939). However, due to a
widespread assumption that infants do not know the
meanings of many words at the age when they are
learning phonetic categories (see Swingley, 2009
for a review), most recent models of early phonetic
category acquisition have explored the phonetic
learning problem in the absence of semantic infor-
mation (de Boer and Kuhl, 2003; Dillon et al, 2013;
1
The models in this paper do not distinguish between pho-
netic and phonemic categories, since they do not capture
phonological processes (and there are also none present in
our synthetic data). We thus use the terms interchangeably.
Feldman et al, 2013a; McMurray et al, 2009; Val-
labha et al, 2007).
Models without any semantic information are
likely to underestimate infants? ability to learn pho-
netic categories. Infants learn language in the wild,
and quickly attune to the fact that words have (pos-
sibly unknown) meanings. The extent of infants?
semantic knowledge is not yet known, but existing
evidence shows that six-month-olds can associate
some words with their referents (Bergelson and
Swingley, 2012; Tincoff and Jusczyk, 1999, 2012),
leverage non-acoustic contexts such as objects or ar-
ticulations to distinguish similar sounds (Teinonen
et al, 2008; Yeung and Werker, 2009), and map
meaning (in the form of objects or images) to new
word-forms in some laboratory settings (Friedrich
and Friederici, 2011; Gogate and Bahrick, 2001;
Shukla et al, 2011). These findings indicate that
young infants are sensitive to co-occurrences be-
tween linguistic stimuli and at least some aspects
of the world.
In this paper we explore the potential contribu-
tion of semantic information to phonetic learning
by formalizing a model in which learners attend to
the word-level context in which phones appear (as
in the lexical-phonetic learning model of Feldman
et al, 2013a) and also to the situations in which
word-forms are used. The modeled situations con-
sist of combinations of categories of salient ac-
tivities or objects, similar to the activity contexts
explored by Roy et al (2012), e.g.,?getting dressed?
or ?eating breakfast?. We assume that child learn-
ers are able to infer a representation of the situ-
ational context from their non-linguistic environ-
ment. However, in our simulations we approximate
the environmental information by running a topic
model (Blei et al, 2003) over a corpus of child-
directed speech to infer a topic distribution for each
situation. These topic distributions are then used as
input to our model to represent situational contexts.
The situational information in our model is simi-
1073
lar to that assumed by theories of cross-situational
word learning (Frank et al, 2009; Smith and Yu,
2008; Yu and Smith, 2007), but our model does not
require learners to map individual words to their ref-
erents. Even in the absence of word-meaning map-
pings, situational information is potentially useful
because similar-sounding words uttered in similar
situations are more likely to be tokens of the same
lexeme (containing the same phones) than similar-
sounding words uttered in different situations.
In simulations of vowel learning, inspired by
Vallabha et al (2007) and Feldman et al (2013a),
we show a clear improvement over previous mod-
els in both phonetic and lexical (word-form) cate-
gorization when situational context is used as an
additional source of information. This improve-
ment is especially noticeable when the word-level
context is providing less information, arguably the
more realistic setting. These results demonstrate
that relying on situational co-occurrence can im-
prove phonetic learning, even if learners do not yet
know the meanings of individual words.
2 Background and overview of models
Infants attend to distributional characteristics of
their input (Maye et al, 2002, 2008), leading to
the hypothesis that phonetic categories could be
acquired on the basis of bottom-up distributional
learning alone (de Boer and Kuhl, 2003; Vallabha
et al, 2007; McMurray et al, 2009). However, this
would require sound categories to be well sepa-
rated, which often is not the case?for example,
see Figure 1, which shows the English vowel space
that is the focus of this paper.
Recent work has investigated whether infants
could overcome such distributional ambiguity by
incorporating top-down information, in particular,
the fact that phones appear within words. At six
months, infants begin to recognize word-forms
such as their name and other frequently occurring
words (Mandel et al, 1995; Jusczyk and Hohne,
1997), without necessarily linking a meaning to
these forms. This ?protolexicon? can help differen-
tiate phonetic categories by adding word contexts
in which certain sound categories appear (Swingley,
2009; Feldman et al, 2013b). To explore this idea
further, Feldman et al (2013a) implemented the
Lexical-Distributional (LD) model, which jointly
learns a set of phonetic vowel categories and a set
of word-forms containing those categories. Simula-
tions showed that the use of lexical context greatly
500100015002000250030003500 F2
200
400
600
800
1000
1200
F1
oa
uw
aw
oo
uh
ah
er
ehae
ihei
iy
Figure 1: The English vowel space (generated from
Hillenbrand et al (1995), see Section 6.2), plotted
using the first two formants.
improved phonetic learning.
Our own Topic-Lexical-Distributional (TLD)
model extends the LD model to include an addi-
tional type of context: the situations in which words
appear. To motivate this extension and clarify the
differences between the models, we now provide
a high-level overview of both models; details are
given in Sections 3 and 4.
2.1 Overview of LD model
Both the LD and TLD models are computational-
level models of phonetic (specifically, vowel) cat-
egorization where phones (vowels) are presented
to the model in the context of words.
2
The task is
to infer a set of phonetic categories and a set of
lexical items on the basis of the data observed for
each word token x
i
. In the original LD model, the
observations for token x
i
are its frame f
i
, which
consists of a list of consonants and slots for vowels,
and the list of vowel tokensw
i
. (The TLD model
includes additional observations, described below.)
A single vowel token, w
ij
, is a two dimensional
vector representing the first two formants (peaks
in the frequency spectrum, ordered from lowest to
highest). For example, a token of the word kitty
would have the frame f
i
= k t , containing two
consonant phones, /k/ and /t/, with two vowel phone
slots in between, and two vowel formant vectors,
2
For a related model that also tackles the word segmenta-
tion problem, see Elsner et al (2013). In a model of phono-
logical learning, Fourtassi and Dupoux (submitted) show that
semantic context information similar to that used here remains
useful despite segmentation errors.
1074
wi0
= [464, 2294] and w
i1
= [412, 2760].
3
Given the data, the model must assign each
vowel token to a vowel category, w
ij
= c. Both
the LD and the TLD models do this using inter-
mediate lexemes, `, which contain vowel category
assignments, v
`j
= c, as well as a frame f
`
. If a
word token is assigned to a lexeme, x
i
= `, the
vowels within the word are assigned to that lex-
eme?s vowel categories, w
ij
= v
`j
= c.
4
The word
and lexeme frames must match, f
i
= f
`
.
Lexical information helps with phonetic catego-
rization because it can disambiguate highly over-
lapping categories, such as the ae and eh categories
in Figure 1. A purely distributional learner who ob-
serves a cluster of data points in the ae-eh region is
likely to assume all these points belong to a single
category because the distributions of the categories
are so similar. However, a learner who attends to
lexical context will notice a difference: contexts
that only occur with ae will be observed in one part
of the ae-eh region, while contexts that only oc-
cur with eh will be observed in a different (though
partially overlapping) space. The learner then has
evidence of two different categories occurring in
different sets of lexemes.
Simulations with the LD model show that using
lexical information to constrain phonetic learning
can greatly improve categorization accuracy (Feld-
man et al, 2013a), but it can also introduce errors.
When two word tokens contain the same consonant
frame but different vowels (i.e., minimal pairs),
the model is more likely to categorize those two
vowels together. Thus, the model has trouble distin-
guishing minimal pairs. Although young children
also have trouble with minimal pairs (Stager and
Werker, 1997; Thiessen, 2007), the LD model may
overestimate the degree of the problem. We hypoth-
esize that if a learner is able to associate words with
the contexts of their use (as children likely are), this
could provide a weak source of information for dis-
ambiguating minimal pairs even without knowing
their exact meanings. That is, if the learner hears
kV
1
t and kV
2
t in different situational contexts, they
are likely to be different lexical items (and V
1
and
V
2
different phones), despite the lexical similarity
between them.
3
In simulations we also experiment with frames in which
consonants are not represented perfectly.
4
The notation is overloaded: w
ij
refers both to the vowel
formants and the vowel category assignments, and x
i
refers
to both the token identity and its assignment to a lexeme.
2.2 Overview of TLD model
To demonstrate the benefit of situational informa-
tion, we develop the Topic-Lexical-Distributional
(TLD) model, which extends the LD model by as-
suming that words appear in situations analogous
to documents in a topic model. Each situation h
is associated with a mixture of topics ?
h
, which is
assumed to be observed. Thus, for the ith token in
situation h, denoted x
hi
, the observed data will be
its frame f
hi
, vowels w
hi
, and topic vector ?
h
.
From an acquisition perspective, the observed
topic distribution represents the child?s knowledge
of the context of the interaction: she can distin-
guish bathtime from dinnertime, and is able to rec-
ognize that some topics appear in certain contexts
(e.g. animals on walks, vegetables at dinnertime)
and not in others (few vegetables appear at bath-
time). We assume that the child would learn these
topics from observing the world around her and
the co-occurrences of entities and activities in the
world. Within any given situation, there might be
a mixture of different (actual or possible) topics
that are salient to the child. We assume further that
as the child learns the language, she will begin to
associate specific words with each topic as well.
Thus, in the TLD model, the words used in a sit-
uation are topic-dependent, implying meaning, but
without pinpointing specific referents. Although the
model observes the distribution of topics in each
situation (corresponding to the child observing her
non-linguistic environment), it must learn to asso-
ciate each (phonetically and lexically ambiguous)
word token with a particular topic from that distri-
bution. The occurrence of similar-sounding words
in different situations with mostly non-overlapping
topics will provide evidence that those words be-
long to different topics and that they are therefore
different lexemes. Conversely, potential minimal
pairs that occur in situations with similar topic dis-
tributions are more likely to belong to the same
topic and thus the same lexeme.
Although we assume that children infer topic
distributions from the non-linguistic environment,
we will use transcripts from CHILDES to create the
word/phone learning input for our model. These
transcripts are not annotated with environmental
context, but Roy et al (2012) found that topics
learned from similar transcript data using a topic
model were strongly correlated with immediate ac-
tivities and contexts. We therefore obtain the topic
distributions used as input to the TLD model by
1075
training an LDA topic model (Blei et al, 2003)
on a superset of the child-directed transcript data
we use for lexical-phonetic learning, dividing the
transcripts into small sections (the ?documents? in
LDA) that serve as our distinct situations h. As
noted above, the learned document-topic distribu-
tions ? are treated as observed variables in the
TLD model to represent the situational context. The
topic-word distributions learned by LDA are dis-
carded, since these are based on the (correct and
unambiguous) words in the transcript, whereas the
TLD model is presented with phonetically ambigu-
ous versions of these word tokens and must learn to
disambiguate them and associate them with topics.
3 Lexical-Distributional Model
In this section we describe more formally the gen-
erative process for the LD model (Feldman et al,
2013a), a joint Bayesian model over phonetic cat-
egories and a lexicon, before describing the TLD
extension in the following section.
The set of phonetic categories and the lexicon are
both modeled using non-parametric Dirichlet Pro-
cess priors, which return a potentially infinite num-
ber of categories or lexemes. A DP is parametrized
as DP (?,H), where ? is a real-valued hyperpa-
rameter andH is a base distribution.H may be con-
tinuous, as when it generates phonetic categories
in formant space, or discrete, as when it generates
lexemes as a list of phonetic categories.
A draw from a DP, G ? DP (?,H), returns
a distribution over a set of draws from H , i.e., a
discrete distribution over a set of categories or lex-
emes generated by H . In the mixture model setting,
the category assignments are then generated from
G, with the datapoints themselves generated by the
corresponding components fromH . IfH is infinite,
the support of the DP is likewise infinite. During
inference, we marginalize over G.
3.1 Phonetic Categories: IGMM
Following previous models of vowel learning (de
Boer and Kuhl, 2003; Vallabha et al, 2007; Mc-
Murray et al, 2009; Dillon et al, 2013) we assume
that vowel tokens are drawn from a Gaussian mix-
ture model. The Infinite Gaussian Mixture Model
(IGMM) (Rasmussen, 2000) includes a DP prior,
as described above, in which the base distribution
H
C
generates multivariate Gaussians drawn from
a Normal Inverse-Wishart prior.
5
Each observation,
a formant vector w
ij
, is drawn from the Gaussian
corresponding to its category assignment c
ij
:
?
c
,?
c
? H
C
= NIW(?
0
,?
0
, ?
0
) (1)
G
C
? DP (?
c
, H
C
) (2)
c
ij
? G
C
(3)
w
ij
|c
ij
= c ? N(?
c
,?
c
) (4)
The above model generates a category assignment
c
ij
for each vowel token w
ij
. This is the baseline
IGMM model, which clusters vowel tokens using
bottom-up distributional information only; the LD
model adds top-down information by assigning cat-
egories in the lexicon, rather than on the token
level.
3.2 Lexicon
In the LD model, vowel phones appear within
words drawn from the lexicon. Each such lexeme
is represented as a frame plus a list of vowel cate-
gories v
`
. Lexeme assignments for each token are
drawn from a DP with a lexicon-generating base
distribution H
L
. The category for each vowel to-
ken in the word is determined by the lexeme; the
formant values are drawn from the corresponding
Gaussian as in the IGMM:
G
L
? DP (?
l
, H
L
) (5)
x
i
= ` ? G
L
(6)
w
ij
|v
`j
= c ? N(?
c
,?
c
) (7)
H
L
generates lexemes by first drawing the num-
ber of phones from a geometric distribution and the
number of consonant phones from a binomial dis-
tribution. The consonants are then generated from a
DP with a uniform base distribution (but note they
are fixed at inference time, i.e., are observed cate-
gorically), while the vowel phones v
`
are generated
by the IGMM DP above, v
`j
? G
C
.
Note that two draws from H
L
may result in iden-
tical lexemes; these are nonetheless considered to
be separate (homophone) lexemes.
4 Topic-Lexical-Distributional Model
The TLD model retains the IGMM vowel phone
component, but extends the lexicon of the LD
model by adding topic-specific lexicons, which cap-
ture the notion that lexeme probabilities are topic-
dependent. Specifically, the TLD model replaces
5
This compound distribution is equivalent to
?
c
? IW(?
0
, ?
0
), ?
c
|?
c
? N(?
0
,
?
c
?
0
)
1076
the Dirichlet Process lexicon with a Hierarchical
Dirichlet Process (HDP; Teh (2006)). In the HDP
lexicon, a top-level global lexicon is generated as
in the LD model. Topic-specific lexicons are then
drawn from the global lexicon, containing a subset
of the global lexicon (but since the size of the global
lexicon is unbounded, so are the topic-specific lex-
icons). These topic-specific lexicons are used to
generate the tokens in a similar manner to the LD
model. There are a fixed number of lower level
topic-lexicons; these are matched to the number
of topics in the LDA model used to infer the topic
distributions (see Section 6.4).
More formally, the global lexicon is generated
as a top-level DP: G
L
? DP (?
l
, H
L
) (see Sec-
tion 3.2; remember H
L
includes draws from the
IGMM over vowel categories). G
L
is in turn used
as the base distribution in the topic-level DPs,
G
k
? DP (?
k
, G
L
). In the Chinese Restaurant
Franchise metaphor often used to describe HDPs,
G
L
is a global menu of dishes (lexemes). The topic-
specific lexicons are restaurants, each with its own
distribution over dishes; this distribution is defined
by seating customers (word tokens) at tables, each
of which serves a single dish from the menu: all
tokens x at the same table t are assigned to the
same lexeme `
t
. Inference (Section 5) is defined
in terms of tables rather than lexemes; if multiple
tables draw the same dish from G
L
, tokens at these
tables share a lexeme.
In the TLD model, tokens appear within situa-
tions, each of which has a distribution over topics
?
h
. Each token x
hi
has a co-indexed topic assign-
ment variable, z
hi
, drawn from ?
h
, designating the
topic-lexicon from which the table for x
hi
is to be
drawn. The formant values for w
hij
are drawn in
the same way as in the LD model, given the lexeme
assignment at x
hi
. This results in the following
model, shown in Figure 2:
G
L
? DP (?
l
, H
L
) (8)
G
k
? DP (?
k
, G
L
) (9)
z
hi
?Mult(?
h
) (10)
x
hi
= t|z
hi
= k ? G
k
(11)
w
hij
|x
hi
= t, v
`
t
j
= c ? N(?
c
,?
c
) (12)
5 Inference: Gibbs Sampling
We use Gibbs sampling to infer three sets of vari-
ables in the TLD model: assignments to vowel cat-
egories in the lexemes, assignments of tokens to
?
0
, ?
0
,?
0
, ?
0
H
C
G
C
?
c
?
c
,?
c
?
?
H
L
G
L
?
l
G
k
?
k
K
z
hi
x
hi
f
hi
w
hij
|w
hi
|
|x
h
|
D
?
h
Figure 2: TLD model, depicting, from left to right,
the IGMM component, the LD lexicon compo-
nent, the topic-specific lexicons, and finally the
token x
hi
, appearing in document h, with observed
vowel formants w
hij
and frame f
hi
. The lexeme
assignment x
hi
and the topic assignment z
hi
are
inferred, the latter using the observed document-
topic distribution ?
h
. Note that f
i
is deterministic
given the lexeme assignment. Squared nodes depict
hyperparameters. ? is the set of hyperparameters
used by H
L
when generating lexical items (see
Section 3.2).
topics, and assignments of tokens to tables (from
which the assignment to lexemes can be read off).
5.1 Sampling lexeme vowel categories
Each vowel in the lexicon must be assigned to a
category in the IGMM. The posterior probability of
a category assignment is composed of the DP prior
over categories and the likelihood of the observed
vowels belonging to that category. We use w
`j
to
denote the set of vowel formants at position j in
words that have been assigned to lexeme `. Then,
P (v
`j
= c|w,x, `
\`
)
? P (v
`j
= c|`
\`
)p(w
`j
|v
`j
= c,w
\`j
) (13)
The first (DP prior) factor is defined as:
P (v
`j
= c|v
\`j
) =
{
n
c
P
c
n
c
+?
c
if c exists
?
c
P
c
n
c
+?
c
if c new
(14)
where n
c
is the number of other vowels in the lex-
icon, v
\lj
, assigned to category c. Note that there
is always positive probability of creating a new
category.
The likelihood of the vowels is calculated by
marginalizing over all possible means and vari-
ances of the Gaussian category parameters, given
1077
the NIW prior. For a single point (if |w
`j
| = 1),
this predictive posterior is in the form of a Student-t
distribution; for the more general case see Feldman
et al (2013a), Eq. B3.
5.2 Sampling table & topic assignments
We jointly sample x and z, the variables assigning
tokens to tables and topics. Resampling the table
assignment includes the possibility of changing to
a table with a different lexeme or drawing a new
table with a previously seen or novel lexeme. The
joint conditional probability of a table and topic
assignment, given all other current token assign-
ments, is:
P (x
hi
= t, z
hi
= k|w
hi
, ?
h
, t
\hi
, `,w
\hi
)
= P (k|?
h
)P (t|k, `
t
, t
\hi
)
?
c?C
p(w
hi?
|v
`
t
?
= c,w
\hi
) (15)
The first factor, the prior probability of topic k
in document h, is given by ?
hk
obtained from the
LDA. The second factor is the prior probability of
assigning word x
i
to table t with lexeme ` given
topic k. It is given by the HDP, and depends on
whether the table t exists in the HDP topic-lexicon
for k and, likewise, whether any table in the topic-
lexicon has the lexeme `:
P (t|k, `, t
\hi
) ?
?
?
?
?
?
n
kt
n
k
+?
k
if t in k
?
k
n
k
+?
k
m
`
m+?
l
if t new, ` known
?
k
n
k
+?
k
?
`
m+?
l
if t and ` new
(16)
Here n
kt
is the number of other tokens at table t,
n
k
are the total number of tokens in topic k, m
`
is the number of tables across all topics with the
lexeme `, and m is the total number of tables.
The third factor, the likelihood of the vowel for-
mantsw
hi
in the categories given by the lexeme v
l
,
is of the same form as the likelihood of vowel cate-
gories when resampling lexeme vowel assignments.
However, here it is calculated over the set of vow-
els in the token assigned to each vowel category
(i.e., the vowels at indices where v
`
t
?
= c). For a
new lexeme, we approximate the likelihood using
100 samples drawn from the prior, each weighted
by ?/100 (Neal, 2000).
5.3 Hyperparameters
The three hyperparameters governing the HDP over
the lexicon, ?
l
and ?
k
, and the DP over vowel cate-
gories, ?
c
, are estimated using a slice sampler. The
remaining hyperparameters for the vowel category
and lexeme priors are set to the same values used
by Feldman et al (2013a).
6 Experiments
6.1 Corpus
We test our model on situated child directed speech,
taken from the C1 section of the Brent corpus in
CHILDES (Brent and Siskind, 2001; MacWhinney,
2000). This corpus consists of transcripts of speech
directed at infants between the ages of 9 and 15
months, captured in a naturalistic setting as par-
ent and child went about their day. This ensures
variability of situations.
Utterances with unintelligible words or quotes
are removed. We restrict the corpus to content
words by retaining only words tagged as adj,
n, part and v (adjectives, nouns, particles, and
verbs). This is in line with evidence that infants
distinguish content and function words on the basis
of acoustic signals (Shi and Werker, 2003). Vowel
categorization improves when attending only to
more prosodically and phonologically salient to-
kens (Adriaans and Swingley, 2012), which gen-
erally appear within content, not function words.
The final corpus consists of 13138 tokens and 1497
word types.
6.2 Hillenbrand Vowels
The transcripts do not include phonetic information,
so, following Feldman et al (2013a), we synthe-
size the formant values using data from Hillenbrand
et al (1995). This dataset consists of a set of 1669
manually gathered formant values from 139 Amer-
ican English speakers (men, women and children)
for 12 vowels. For each vowel category, we con-
struct a Gaussian from the mean and covariance of
the datapoints belonging to that category, using the
first and second formant values measured at steady
state. We also construct a second dataset using only
datapoints from adult female speakers.
Each word in the dataset is converted to a phone-
mic representation using the CMU pronunciation
dictionary, which returns a sequence of Arpabet
phoneme symbols. If there are multiple possible
pronunciations, the first one is used. Each vowel
phoneme in the word is then replaced by formant
values drawn from the corresponding Hillenbrand
Gaussian for that vowel.
1078
6.3 Merging Consonant Categories
The Arpabet encoding used in the phonemic rep-
resentation includes 24 consonants. We construct
datasets both using the full set of consonants?the
?C24? dataset?and with less fine-grained conso-
nant categories. Distinguishing all consonant cate-
gories assumes perfect learning of consonants prior
to vowel categorization and is thus somewhat unre-
alistic (Polka and Werker, 1994), but provides an
upper limit on the information that word-contexts
can give.
In the ?C15? dataset, the voicing distinction is
collapsed, leaving 15 consonant categories. The
collapsed categories are B/P, G/K, D/T, CH/JH,
V/F, TH/DH, S/Z, SH/ZH, R/L while HH, M, NG,
N, W, Y remain separate phonemes. This dataset
mirrors the finding in Mani and Plunkett (2010) that
12 month old infants are not sensitive to voicing
mispronunciations.
The ?C6? dataset distinguishes between only
6 coarse consonant phonemes, corresponding to
stops (B,P,G,K,D,T), affricates (CH,JH), fricatives
(V, F, TH, DH, S, Z, SH, ZH, HH), nasals (M,
NG, N), liquids (R, L), and semivowels/glides (W,
Y). This dataset makes minimal assumptions about
the category categories that infants could use in this
learning setting.
Decreasing the number of consonants increases
the ambiguity in the corpus: bat not only shares
a frame (b t) with boat and bite, but also, in the
C15 dataset, with put, pad and bad (b/p d/t), and
in the C6 dataset, with dog and kite, among many
others (STOP STOP). Table 1 shows the percent-
age of types and tokens that are ambiguous in each
dataset, that is, words in frames that match multiple
wordtypes. Note that we always evaluate against
the gold word identities, even when these are not
distinguished in the model?s input. These datasets
are intended to evaluate the degree of reliance on
consonant information in the LD and TLD models,
and to what extent the topics in the TLD model can
replace this information.
6.4 Topics
The input to the TLD model includes a distribution
over topics for each situation, which we infer in
advance from the full Brent corpus (not only the
C1 subset) using LDA. Each transcript in the Brent
corpus captures about 75 minutes of parent-child
interaction, and thus multiple situations will be
included in each file. The transcripts do not delimit
Dataset C24 C15 C6
Input Types 1487 1426 1203
Frames 1259 1078 702
Ambig Types % 27.2 42.0 80.4
Ambig Tokens % 41.3 56.9 77.2
Table 1: Corpus statistics showing the increasing
amount of ambiguity as consonant categories are
merged. Input types are the number of word types
with distinct input representations (as opposed to
gold orthographic word types, of which there are
1497). Ambiguous types and tokens are those with
frames that match multiple (orthographic) word
types.
situations, so we do this somewhat arbitrarily by
splitting each transcript after 50 CDS utterances,
resulting in 203 situations for the Brent C1 dataset.
As well as function words, we also remove the
five most frequent content words (be, go, get, want,
come). On average, situations are only 59 words
long, reflecting the relative lack of content words
in CDS utterances.
We infer 50 topics for this set of situations using
the mallet toolkit (McCallum, 2002). Hyperpa-
rameters are inferred, which leads to a dominant
topic that includes mainly light verbs (have, let,
see, do). The other topics are less frequent but cap-
ture stronger semantic meaning (e.g. yummy, peach,
cookie, daddy, bib in one topic, shoe, let, put, hat,
pants in another). The word-topic assignments are
used to calculate unsmoothed situation-topic distri-
butions ? used by the TLD model.
6.5 Evaluation
We evaluate against adult categories, i.e., the ?gold-
standard?, since all learners of a language even-
tually converge on similar categories. (Since our
model is not a model of the learning process, we
do not compare the infant learning process to the
learning algorithm.) We evaluate both the inferred
phonetic categories and words using the clustering
evaluation measure V-Measure (VM; Rosenberg
and Hirschberg, 2007).
6
VM is the harmonic mean
of two components, similar to F-score, where the
components (VC and VH) are measures of cross
entropy between the gold and model categorization.
6
Other clustering measures, such as 1-1 matching and
pairwise precision and recall (accuracy and completeness)
showed the same trends, but VM has been demonstrated to
be the most stable measure when comparing solutions with
varying numbers of clusters (Christodoulopoulos et al, 2010).
1079
24 Cons 15 Cons 6 Cons
75
80
85
90
Dataset
V
M
LD-all
TLD-all
LD-w
TLD-w
Figure 3: Vowel evaluation. ?all? refers to datasets
with vowels synthesized from all speakers, ?w? to
datasets with vowels synthesized from adult female
speakers? vowels. The bars show a 95% Confidence
Interval based on 5 runs. IGMM-all results in a VM
score of 53.9 (CI=0.5); IGMM-w has a VM score
of 65.0 (CI=0.2), not shown.
For vowels, VM measures how well the inferred
phonetic categorizations match the gold categories;
for lexemes, it measures whether tokens have been
assigned to the same lexemes both by the model
and the gold standard. Words are evaluated against
gold orthography, so homophones, e.g. hole and
whole, are distinct gold words.
6.6 Results
We compare all three models?TLD, LD, and
IGMM?on the vowel categorization task, and
TLD and LD on the lexical categorization task
(since IGMM does not infer a lexicon). The datasets
correspond to two sets of conditions: firstly, either
using vowel categories synthesized from all speak-
ers or only adult female speakers, and secondly,
varying the coarseness of the observed consonant
categories. Each condition (model, vowel speak-
ers, consonant set) is run five times, using 1500
iterations of Gibbs sampling with hyperparameter
sampling. Overall, we find that TLD outperforms
the other models in both tasks, across all condi-
tions.
Vowel categorization results are shown in Fig-
ure 3. IGMM performs substantially worse than
both TLD and LD, with scores more than 30 points
lower than the best results for these models, clearly
showing the value of the protolexicon and repli-
500100015002000250030003500 F2
200
400
600
800
1000
1200
F1
Figure 4: Vowels found by the TLD model; su-
pervowels are indicated in red. The gold-standard
vowels are shown in gold in the background but are
mostly overlapped by the inferred categories.
cating the results found by Feldman et al (2013a)
on this dataset. Furthermore, TLD consistently out-
performs the LD model, finding better phonetic
categories, both for vowels generated from the com-
bined categories of all speakers (?all?) and vowels
generated from adult female speakers only (?w?),
although the latter are clearly much easier for both
models to learn. Both models perform less well
when the consonant frames provide less informa-
tion, but the TLD model performance degrades less
than the LD performance.
Both the TLD and the LD models find ?super-
vowel? categories, which cover multiple vowel cat-
egories and are used to merge minimal pairs into a
single lexical item. Figure 4 shows example vowel
categories inferred by the TLD model, including
two supervowels. The TLD supervowels are used
much less frequently than the supervowels found
by the LD model, containing, on average, only two-
thirds as many tokens.
Figure 5 shows that TLD also outperforms LD
on the lexeme/word categorization task. Again per-
formance decreases as the consonant categories
become coarser, but the additional semantic infor-
mation in the TLD model compensates for the lack
of consonant information. In the individual com-
ponents of VM, TLD and LD have similar VC
(?recall?), but TLD has higher VH (?precision?),
demonstrating that the semantic information given
by the topics can separate potentially ambiguous
words, as hypothesized.
Overall, the contextual semantic information
1080
24 Cons 15 Cons 6 Cons
92
94
96
98
100
Dataset
V
M
LD-all
TLD-all
LD-w
TLD-w
Figure 5: Lexeme evaluation. ?all? refers to datasets
with vowels synthesized from all speakers, ?w? to
datasets with vowels synthesized from adult female
speakers? vowels.
added in the TLD model leads to both better pho-
netic categorization and to a better protolexicon,
especially when the input is noisier, using degraded
consonants. Since infants are not likely to have per-
fect knowledge of phonetic categories at this stage,
semantic information is a potentially rich source
of information that could be drawn upon to offset
noise from other domains. The form of the seman-
tic information added in the TLD model is itself
quite weak, so the improvements shown here are in
line with what infant learners could achieve.
7 Conclusion
Language acquisition is a complex task, in which
many heterogeneous sources of information may
be useful. In this paper, we investigated whether
contextual semantic information could be of help
when learning phonetic categories. We found that
this contextual information can improve phonetic
learning performance considerably, especially in
situations where there is a high degree of pho-
netic ambiguity in the word-forms that learners
hear. This suggests that previous models that have
ignored semantic information may have underesti-
mated the information that is available to infants.
Our model illustrates one way in which language
learners might harness the rich information that is
present in the world without first needing to acquire
a full inventory of word meanings.
The contextual semantic information that the
TLD model tracks is similar to that potentially
used in other linguistic learning tasks. Theories
of cross-situational word learning (Smith and Yu,
2008; Yu and Smith, 2007) assume that sensitivity
to situational co-occurrences between words and
non-linguistic contexts is a precursor to learning the
meanings of individual words. Under this view, con-
textual semantics is available to infants well before
they have acquired large numbers of semantic min-
imal pairs. However, recent experimental evidence
indicates that learners do not always retain detailed
information about the referents that are present in a
scene when they hear a word (Medina et al, 2011;
Trueswell et al, 2013). This evidence poses a di-
rect challenge to theories of cross-situational word
learning. Our account does not necessarily require
learners to track co-occurrences between words
and individual objects, but instead focuses on more
abstract information about salient events and topics
in the environment; it will be important to investi-
gate to what extent infants encode this information
and use it in phonetic learning.
Regardless of the specific way in which infants
encode semantic information, our method of adding
this information by using LDA topics from tran-
script data was shown to be effective. This method
is practical because it can approximate semantic
information without relying on extensive manual
annotation.
The LD model extended the phonetic catego-
rization task by adding word contexts; the TLD
model presented here goes even further, adding
larger situational contexts. Both forms of top-down
information help the low-level task of classifying
acoustic signals into phonetic categories, furthering
a holistic view of language learning with interac-
tion across multiple levels.
Acknowledgments
This work was supported by EPSRC grant
EP/H050442/1 and a James S. McDonnell Founda-
tion Scholar Award to the final author.
References
Frans Adriaans and Daniel Swingley. Distribu-
tional learning of vowel categories is supported
by prosody in infant-directed speech. In Pro-
ceedings of the 34th Annual Conference of the
Cognitive Science Society (CogSci), 2012.
E. Bergelson and D. Swingley. At 6-9 months,
human infants know the meanings of many
1081
common nouns. Proceedings of the National
Academy of Sciences, 109(9):3253?3258, Feb
2012.
David M. Blei, Thomas L. Griffiths, Michael I. Jor-
dan, and Joshua B. Tenenbaum. Hierarchical
topic models and the nested Chinese restaurant
process. In Advances in Neural Information Pro-
cessing Systems 16, 2003.
Michael R. Brent and Jeffrey M. Siskind. The role
of exposure to isolated words in early vocabulary
development. Cognition, 81(2):B33?B44, 2001.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. Two decades of unsuper-
vised POS induction: How far have we come?
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 575?584, Cambridge, MA, Octo-
ber 2010. Association for Computational Lin-
guistics.
Bart de Boer and Patricia K. Kuhl. Investigating the
role of infant-directed speech with a computer
model. Acoustics Research Letters Online, 4(4):
129, 2003.
Brian Dillon, Ewan Dunbar, and William Idsardi. A
single-stage approach to learning phonological
categories: Insights from Inuktitut. Cognitive
Science, 37(2):344?377, Mar 2013.
Micha Elsner, Sharon Goldwater, Naomi Feldman,
and Frank Wood. A cognitive model of early
lexical acquisition with phonetic variability. In
Proceedings of the 18th Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), 2013.
Naomi H. Feldman, Thomas L. Griffiths, Sharon
Goldwater, and James L. Morgan. A role for the
developing lexicon in phonetic category acquisi-
tion. Psychological Review, 2013a.
Naomi H. Feldman, Emily B. Myers, Katherine S.
White, Thomas L. Griffiths, and James L. Mor-
gan. Word-level information influences phonetic
learning in adults and infants. Cognition, 127(3):
427?438, 2013b.
Abdellah Fourtassi and Emmanuel Dupoux. A rudi-
mentary lexicon and semantics help bootstrap
phoneme acquisition. Submitted.
Michael C. Frank, Noah D. Goodman, and
Joshua B. Tenenbaum. Using speakers? refer-
ential intentions to model early cross-situational
word learning. Psychological Science, 20(5):
578?585, 2009.
Manuela Friedrich and Angela D. Friederici. Word
learning in 6-month-olds: Fast encoding?weak
retention. Journal of Cognitive Neuroscience, 23
(11):3228?3240, Nov 2011.
Lakshmi J. Gogate and Lorraine E. Bahrick. In-
tersensory redundancy and 7-month-old infants?
memory for arbitrary syllable-object relations.
Infancy, 2(2):219?231, Apr 2001.
J. Hillenbrand, L. A. Getty, M. J. Clark, and
K. Wheeler. Acoustic characteristics of Ameri-
can English vowels. Journal of the Acoustical
Society of America, 97(5 Pt 1):3099?3111, May
1995.
P. W. Jusczyk and Elizabeth A. Hohne. Infants?
memory for spoken words. Science, 277(5334):
1984?1986, Sep 1997.
Patricia K. Kuhl, Karen A. Williams, Francisco
Lacerda, Kenneth N. Stevens, and Bjorn Lind-
blom. Linguistic experience alters phonetic per-
ception in infants by 6 months of age. Science,
255(5044):606?608, 1992.
Brian MacWhinney. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, 2000.
D. R. Mandel, P. W. Jusczyk, and D. B. Pisoni.
Infants? recognition of the sound patterns of their
own names. Psychological Science, 6(5):314?
317, Sep 1995.
Nivedita Mani and Kim Plunkett. Twelve-month-
olds know their cups from their keps and tups.
Infancy, 15(5):445470, Sep 2010.
Jessica Maye, Daniel J. Weiss, and Richard N.
Aslin. Statistical phonetic learning in infants:
facilitation and feature generalization. Develop-
mental Science, 11(1):122?134, Jan 2008.
Jessica Maye, Janet F Werker, and LouAnn Gerken.
Infant sensitivity to distributional information
can affect phonetic discrimination. Cognition,
82(3):B101?B111, Jan 2002.
Andrew McCallum. MALLET: A machine learn-
ing for language toolkit, 2002.
Bob McMurray, Richard N. Aslin, and Joseph C.
Toscano. Statistical learning of phonetic cate-
gories: insights from a computational approach.
Developmental Science, 12(3):369?378, May
2009.
1082
Tamara Nicol Medina, Jesse Snedeker, John C.
Trueswell, and Lila R. Gleitman. How words
can and cannot be learned by observation. Pro-
ceedings of the National Academy of Sciences,
108(22):9014?9019, 2011.
Radford Neal. Markov chain sampling methods
for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9:
249?265, 2000.
Linda Polka and Janet F. Werker. Developmen-
tal changes in perception of nonnative vowel
contrasts. Journal of Experimental Psychology:
Human Perception and Performance, 20(2):421?
435, 1994.
Carl Rasmussen. The infinite Gaussian mixture
model. In Advances in Neural Information Pro-
cessing Systems 13, 2000.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of
the 12th Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2007.
Brandon C. Roy, Michael C. Frank, and Deb Roy.
Relating activity contexts to early word learning
in dense longitudinal data. In Proceedings of the
34th Annual Conference of the Cognitive Science
Society (CogSci), 2012.
Rushen Shi and Janet F. Werker. The basis of pref-
erence for lexical words in 6-month-old infants.
Developmental Science, 6(5):484?488, 2003.
M. Shukla, K. S. White, and R. N. Aslin. Prosody
guides the rapid mapping of auditory word forms
onto visual objects in 6-mo-old infants. Proceed-
ings of the National Academy of Sciences, 108
(15):6038?6043, Apr 2011.
Linda B. Smith and Chen Yu. Infants rapidly learn
word-referent mappings via cross-situational
statistics. Cognition, 106(3):1558?1568, 2008.
Christine L. Stager and Janet F. Werker. Infants
listen for more phonetic detail in speech percep-
tion than in word-learning tasks. Nature, 388:
381?382, 1997.
D. Swingley. Contributions of infant word learning
to language development. Philosophical Trans-
actions of the Royal Society B: Biological Sci-
ences, 364(1536):3617?3632, Nov 2009.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
pages 985 ? 992, Sydney, 2006.
Tuomas Teinonen, Richard N. Aslin, Paavo Alku,
and Gergely Csibra. Visual speech contributes to
phonetic learning in 6-month-old infants. Cogni-
tion, 108:850?855, 2008.
Erik D. Thiessen. The effect of distributional infor-
mation on children?s use of phonemic contrasts.
Journal of Memory and Language, 56(1):16?34,
Jan 2007.
R. Tincoff and P. W. Jusczyk. Some beginnings of
word comprehension in 6-month-olds. Psycho-
logical Science, 10(2):172?175, Mar 1999.
Ruth Tincoff and Peter W. Jusczyk. Six-month-
olds comprehend words that refer to parts of the
body. Infancy, 17(4):432444, Jul 2012.
N. S. Trubetzkoy. Grundz?uge der Phonologie. Van-
denhoeck und Ruprecht, G?ottingen, 1939.
John C. Trueswell, Tamara Nicol Medina, Alon
Hafri, and Lila R. Gleitman. Propose but ver-
ify: Fast mapping meets cross-situational word
learning. Cognitive Psychology, 66:126?156,
2013.
G. K. Vallabha, J. L. McClelland, F. Pons, J. F.
Werker, and S. Amano. Unsupervised learning
of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sci-
ences, 104(33):13273?13278, Aug 2007.
Janet F. Werker and Richard C. Tees. Cross-
language speech perception: Evidence for per-
ceptual reorganization during the first year of
life. Infant Behavior and Development, 7:49?63,
1984.
H. Henny Yeung and Janet F. Werker. Learning
words? sounds before learning how words sound:
9-month-olds use distinct objects as cues to cat-
egorize speech information. Cognition, 113(2):
234?243, Nov 2009.
Chen Yu and Linda B. Smith. Rapid word learning
under uncertainty via cross-situational statistics.
Psychological Science, 18(5):414?420, 2007.
1083
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 1?8,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Using Sentence Type Information for Syntactic Category Acquisition
Stella Frank (s.c.frank@sms.ed.ac.uk)
Sharon Goldwater (sgwater@inf.ed.ac.uk)
Frank Keller (keller@inf.ed.ac.uk)
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
Abstract
In this paper we investigate a new source of
information for syntactic category acquisition:
sentence type (question, declarative, impera-
tive). Sentence type correlates strongly with
intonation patterns in most languages; we hy-
pothesize that these intonation patterns are a
valuable signal to a language learner, indicat-
ing different syntactic patterns. To test this hy-
pothesis, we train a Bayesian Hidden Markov
Model (and variants) on child-directed speech.
We first show that simply training a separate
model for each sentence type decreases perfor-
mance due to sparse data. As an alternative, we
propose two new models based on the BHMM
in which sentence type is an observed variable
which influences either emission or transition
probabilities. Both models outperform a stan-
dard BHMM on data from English, Cantonese,
and Dutch. This suggests that sentence type
information available from intonational cues
may be helpful for syntactic acquisition cross-
linguistically.
1 Introduction
Children acquiring the syntax of their native language
have access to a large amount of contextual informa-
tion. Acquisition happens on the basis of speech, and
the acoustic signal carries rich prosodic and intona-
tional information that children can exploit. A key task
is to separate the acoustic properties of a word from the
underlying sentence intonation. Infants become attuned
to the pragmatic and discourse functions of utterances
as signalled by intonation extremely early; in this they
are helped by the fact that intonation contours of child
and infant directed speech are especially well differen-
tiated between sentence types (Stern et al, 1982; Fer-
nald, 1989). Children learn to use appropriate intona-
tional melodies to communicate their own intentions at
the one word stage, before overt syntax develops (Snow
and Balog, 2002).
It follows that sentence type information (whether a
sentence is declarative, imperative, or a question), as
signaled by intonation, is readily available to children
by the time they start to acquire syntactic categories.
Sentence type also has an effect on sentence structure
in many languages (most notably on word order), so
we hypothesise that sentence type is a useful cue for
syntactic category learning. We test this hypothesis by
incorporating sentence type information into an unsu-
pervised model of part of speech tagging.
We are unaware of previous work investigating the
usefulness of this kind of information for syntactic
category acquisition. In other domains, intonation has
been used to identify sentence types as a means of im-
proving speech recognition language models. Specifi-
cally, (Taylor et al, 1998) found that using intonation
to recognize dialogue acts (which to a significant extent
correspond to sentence types) and then using a special-
ized language model for each type of dialogue act led
to a significant decrease in word error rate.
In the remainder of this paper, we first present the
Bayesian Hidden Markov Model (BHMM; Goldwater
and Griffiths (2007)) that is used as the baseline model
of category acquisition, as well as our extensions to
the model, which incorporate sentence type informa-
tion. We then discuss the distinctions in sentence type
that we used and our evaluation measures, and finally
our experimental results. We perform experiments on
corpora in four different languages: English, Spanish,
Cantonese, and Dutch. Our results on Spanish show no
difference between the baseline and the models incor-
porating sentence type, possibly due to the small size of
the Spanish corpus. Results on all other corpora show
a small improvement in performance when sentence
type is included as a cue to the learner. These cross-
linguistic results suggest that sentence type may be a
useful source of information to children acquiring syn-
tactic categories.
2 BHMM Models
2.1 Standard BHMM
We use a Bayesian HMM (Goldwater and Griffiths,
2007) as our baseline model. Like a standard trigram
HMM, the BHMM assumes that the probability of tag
ti depends only on the previous two tags, and the proba-
bility of word wi depends only on ti. This can be written
as
ti|ti?1 = t, ti?2 = t
?,?(t,t
?) ? Mult(?(t,t
?)) (1)
wi|ti = t,?
(t) ? Mult(?(t)) (2)
where ?(t,t
?) are the parameters of the multinomial dis-
tribution over following tags given previous tags (t, t ?)
1
and ?(t) are the parameters of the distribution over out-
puts given tag t. The BHMM assumes that these param-
eters are in turn drawn from symmetric Dirichlet priors
with parameters ? and ?, respectively:
?(t,t
?)|? ? Dirichlet(?) (3)
?(t)|? ? Dirichlet(?) (4)
Using these Dirichlet priors allows the multinomial dis-
tributions to be integrated out, leading to the following
predictive distributions:
P(ti|t?i,?) =
C(ti?2, ti?1, ti)+?
C(ti?2, ti?1)+T?
(5)
P(wi|ti, t?i,w?i,?) =
C(ti,wi)+?
C(ti)+Wti?
(6)
where t?i = t1 . . . ti?1, w?i = w1 . . .wi?1, C(ti?2, ti?1, ti)
and C(ti,wi) are the counts of the trigram (ti?2, ti?1, ti)
and the tag-word pair (ti,wi) in t?i and w?i, T is the
size of the tagset, and Wti is the number of word types
emitted by ti.
Based on these predictive distributions, (Goldwa-
ter and Griffiths, 2007) develop a Gibbs sampler for
the model, which samples from the posterior distri-
bution over tag sequences t given word sequences w,
i.e., P(t|w,?,?)? P(w|t,?)P(t|?). This is done by us-
ing Equations 5 and 6 to iteratively resample each tag
ti given the current values of all other tags.1 The re-
sults show that the BHMM with Gibbs sampling per-
forms better than the standard HMM using expectation-
maximization. In particular, the Dirichlet priors in the
BHMM constrain the model towards sparse solutions,
i.e., solutions in which each tag emits a relatively small
number of words, and in which a tag transitions to few
following tags. This type of model constraint allows
the model to find solutions which correspond to true
syntactic parts of speech (which follow such a sparse,
Zipfian distribution), unlike the uniformly-sized clus-
ters found by standard maximum likelihood estimation
using EM.
In the experiments reported below, we use the Gibbs
sampler described by (Goldwater and Griffiths, 2007)
for the BHMM, and modify it as necessary for our own
extended models. We also follow (Goldwater and Grif-
fiths, 2007) in using Metropolis-Hastings sampling for
the hyperparameters, which are inferred automatically
in all experiments. A separate ? parameter is inferred
for each tag.
2.2 BHMM with Sentence Types
We wish to add a sentence type feature to each time-
step in the HMM, signalling the current sentence type.
We treat sentence type (s) as an observed variable, on
the assumption that it is observed via intonation or
1Slight corrections need to be made to Equation 5 to ac-
count for sampling tags from the middle of the sequence
rather than from the end; these are given in (Goldwater and
Griffiths, 2007) and are followed in our own samplers.
ti?2 ti?1 ti
wisi
?
?
N
Figure 1: Graphical model representation of the
BHMM-T, which includes sentence type as an ob-
served variable on tag transitions (but not emissions).
punctuation features (not part of our model), and these
features are informative enough to reliably distinguish
sentence types (as speech recognition tasks have found
to be the case, see Section 1).
In the BHMM, there are two obvious ways that sen-
tence type could be incorporated into the generative
model: either by affecting the transition probabilities
or by affecting the emission probabilities. The first case
can be modeled by adding si as a conditioning variable
when choosing ti, replacing line 1 from the BHMM
definition with the following:
ti|si = s, ti?1 = t, ti?2 = t
?,?(t,t
?) ? Mult(?(s,t,t
?)) (7)
We will refer to this model, illustrated graphically in
Figure 1, as the BHMM-T. It assumes that the distribu-
tion over ti depends not only on the previous two tags,
but also on the sentence type, i.e., that different sen-
tence types tend to have different sequences of tags.
In contrast, we can add si as a conditioning variable
for wi by replacing line 2 from the BHMM with
wi|si = s, ti = t,?
(t) ? Mult(?(s,t)) (8)
This model, the BHMM-E, assumes that different sen-
tence types tend to have different words emitted from
the same tag.
The predictive distributions for these models are
given in Equations 9 (BHMM-T) and 10 (BHMM-E):
P(ti|t?i,si,?) =
C(ti?2, ti?1, ti,si)+?
C(ti?2, ti?1,si)+T?
(9)
P(wi|ti,si,?) =
C(ti,wi,si)+?
C(ti,si)+Wti?
(10)
Of course, we can also create a third new model,
the BHMM-B, in which sentence type is used to con-
dition both transition and emission probabilities. This
model is equivalent to training a separate BHMM on
each type of sentence (with shared hyperparameters).
Note that introducing the extra conditioning variable
in these models has the consequence of splitting the
counts for transitions, emissions, or both. The split dis-
tributions will therefore be estimated using less data,
which could actually degrade performance if sentence
type is not a useful variable.
2
Our prediction is that sentence type is more likely
to be useful as a conditioning variable for transition
probabilities (BHMM-T) than for emission probabili-
ties (BHMM-E). For example, the auxiliary inversion
in questions is likely to increase the probability of
the AUX? PRO transition, compared to declaratives.
Knowing that the sentence is a question may also af-
fect emission probabilities, e.g., it might increase the
probability the word you given a PRO and decrease the
probability of I; one would certainly expect wh-words
to have much higher probability in wh-questions than
in declaratives. However, many other variables also af-
fect the particular words used in a sentence (princi-
pally, the current semantic and pragmatic context). We
expect that sentence type plays a relatively small role
compared to these other factors. The ordering of tags
within an utterance, on the other hand, is principally
constrained by sentences type (especially in the short
and grammatically simple utterances found in child-
directed speech).
3 Sentence Types
We experiment with a number of sentence-type cate-
gories, leading to increasingly fine grained distinctions.
The primary distinction is between questions (Q) and
declaratives (D). Questions are marked by punctuation
(in writing) or by intonation (in speech), as well as by
word order or other morpho-syntactic markers in many
languages.
Questions may be separated into categories, most
notably wh-questions and yes/no-questions. Many lan-
guages (including several English dialects) have dis-
tinct intonation patterns for wh- and yes/no-questions
(Hirst and Cristo, 1998).
Imperatives are a separate type from declaratives,
with distinct word order and intonation patterns.
Declaratives may be further subdivided into frag-
ments and full sentences. We define fragments as ut-
terances without a verb (including auxiliary verbs).
As an alternate sentence-level feature to sentence
type, we use length. Utterances are classified accord-
ing to their length, as either shorter or longer than av-
erage. Shorter utterances are more likely to be frag-
ments and may have distinct syntactic patterns. How-
ever these patterns are likely to be less strong than in
the above type-based types. In effect this condition is
a pseudo-baseline, testing the effects of less- or non-
informative sentence features on our proposed models.
4 Evaluation Measures
Evaluation of fully unsupervised part of speech tagging
is known to be problematic, due to the fact that the
part of speech clusters found by the model are unla-
beled, and do not automatically correspond to any of
the gold standard part of speech categories. We report
three evaluation measures in our experiments, in order
to avoid the weaknesses inherent in any single measure
and in an effort to be comparable to previous work.
Matched accuracy (MA), also called many-to-one
accuracy, is a commonly used measure for evaluating
unlabeled clusterings in part of speech tagging. Each
unlabeled cluster is given the label of the gold category
with which it shares the most members. Given these la-
bels, accuracy can be measured as usual, as the percent-
age of tokens correctly labeled. Note that multiple clus-
ters may have the same label if several clusters match
the same gold standard category. This can lead to a de-
generate solution if the model is allowed an unbounded
number of categories, in which each word is in a sepa-
rate cluster. In less extreme cases, it makes comparing
MA across clustering results with different numbers of
clusters difficult. Another serious issue with MA is the
?problem of matching? (Meila, 2007): matched accu-
racy only evaluates whether or not the items in the clus-
ter match the majority class label. The non-matching
items within a cluster might all be from a second gold
class, or they might be from many different classes. In-
tuitively, the former clustering should be evaluated as
better, but matched accuracy is the same for both clus-
terings.
Variation of Information (VI) (Meila, 2007) is a
clustering evaluation measure that avoids the match-
ing problem. It measures the amount of information
lost and gained when moving between two clusterings.
More precisely:
V I(C,K) = H(C)+H(K)?2I(C,K)
= H(C|K)+H(K|C)
A lower score implies closer clusterings, since each
clustering has less information not shared with the
other: two identical clusterings have a VI of zero. How-
ever, VI?s upper bound is dependent on the maximum
number of clusters in C or K, making it difficult to com-
pare clustering results with different numbers of clus-
ters.
As a third, and, in our view, most informative
measure, we use V-measure (VM; Rosenberg and
Hirschberg (2007)). Like VI, VM uses the conditional
entropy of clusters and categories to evaluate cluster-
ings. However, it also has the useful characteristic of
being analogous to the precision and recall measures
commonly used in NLP. Homogeneity, the precision
analogue, is defined as
V H = 1?
H(C|K)
H(C)
.
VH is highest when the distribution of categories
within each cluster is highly skewed towards a small
number of categories, such that the conditional entropy
is low. Completeness (recall) is defined symmetrically
to VH as:
VC = 1?
H(K|C)
H(K)
.
VC measures the conditional entropy of the clusters
within each gold standard category, and is highest if
each category maps to a single cluster so that each
3
Eve Manchester
Sentence type Counts |w| Counts |w|
Total 13494 4.39 13216 4.23
D
Total 8994 4.48 8315 3.52
I 623 4.87 757 4.22
F 2996 1.73 4146 1.51
Q
Total 4500 4.22 4901 5.44
wh 2105 4.02 1578 4.64
Short utts 5684 1.89 6486 1.74
Long utts 7810 6.21 6730 6.64
Table 1: Counts of sentence types in the Eve and
Manchester training set. (Test and dev sets are approx-
imately 10% of the size of training.) |w| is the average
length in words of utterances of this type. D: declar-
atives, I: imperatives, F: fragments, Q: questions, wh:
wh-questions.
model cluster completely contains a category. The V-
measure VM is simply the harmonic mean of VH and
VC, analogous to traditional F-score. Unlike MA and
VI, VM is invariant with regards to both the number of
items in the dataset and to the number of clusters used,
and consequently it is best suited for comparing results
across different corpora.
5 English experiments
5.1 Corpora
We use the Eve corpus (Brown, 1973) and the
Manchester corpus (Theakston et al, 2001) from the
CHILDES collection (MacWhinney, 2000). The Eve
corpus is a longitudinal study of a single US Ameri-
can child from the age of 1.5 to 2.25 years, whereas
the Manchester corpus follows a cohort of 12 British
children from the ages of 2 to 3. Using both corpora
ensures that any effect is not due to a particular child,
and is not specific to a type of English.
From both corpora we remove all utterances spoken
by a child; the remaining utterances are nearly exclu-
sively child-directed speech (CDS). We use the full Eve
corpus and a similarly sized subset of the Manchester
corpus, consisting of the first 70 CDS utterances from
each file. Files from the chronological middle of each
corpus are set aside for development and testing (Eve:
file 10 for testing, 11 for dev; Manchester: file 17 from
each child for testing, file 16 for dev).
Both corpora have been tagged using the relatively
rich CHILDES tagset, which we collapse to a smaller
set of thirteen tags: adjectives, adverbs, auxiliaries,
conjunctions, determiners, infinitival-to, nouns, nega-
tion, participles, prepositions, pronouns, verbs and
other (communicators, interjections, fillers and the
like). wh-words are tagged as adverbs (why,where,
when and how, or pronouns (who and the rest).
Table 1 show the sizes of the training sets, and
the breakdown of sentence types within them. Each
sentence type can be identified using a distinguish-
ing characteristic. Sentence-final punctuation is used to
differentiate between questions and declaratives; wh-
questions are then further differentiated by the pres-
ence of a wh-word. Imperatives are separated from the
declaratives by a heuristic (since CHILDES does not
have an imperative verb tag): if an utterance includes
a base verb within the first two words, without a pro-
noun proceeding it (with the exception of you, as in
you sit down right now), the utterance is coded as an
imperative. Fragments are also identified using the tag
annotations, namely by the lack of a verb or auxiliary
tag in an utterance.
The CHILDES annotation guide specifies that the
question mark is to be used with any utterance with ?fi-
nal rising contour?, even if syntactically the utterance
might appear to be a declarative or exclamation. The
question category consequently includes echo ques-
tions (Finger stuck?) and non-inverted questions (You
want me to have it?).
5.2 Inference and Evaluation Procedure
Unsupervised models do not suffer from overfitting,
so generally it is thought unnecessary to use separate
training and testing data, with results being reported
on the entire set of input data. However, there is still
a danger, in the course of developing a model, of over-
fitting in the sense of becoming too finely attuned to a
particular set of input data. To avoid this, we use sep-
arate test and development sets. The BHMM is trained
on (train+dev) or (train+test), but evaluation scores are
computed based on the dev or test portions of the data
only. 2
We run the Gibbs sampler for 2000 iterations, with
hyperparameter resampling and simulated annealing.
Each iteration produces an assignment of tags to the
tokens in the corpus; the final iteration is used for eval-
uation purposes. Since Gibbs sampling is a stochas-
tic algorithm, we run all models multiple times (three,
except where stated otherwise) and report average val-
ues for all evaluation measures, as well as confidence
intervals. We run our experiments using a variety of
sentence type features, ranging from the coarse ques-
tion/declarative (Q/D) distinction to the full five types.
For reasons of space we do not report all results here,
instead confining ourselves to representative samples.
5.3 BHMM-B: Type-specific Sub-Models
When separate sub-models are used for each sen-
tence type, as in the BHMM-B, where both transition
and emission probabilities are conditioned on sentence
type, the hidden states (tags) in each sub-model do
not correspond to each other, e.g., a hidden state 9 in
one sub-model is not the same state 9 in another sub-
model. Consequently, when evaluating the tagged out-
put, each sentence type must be evaluated separately
(otherwise the evaluation would equate declaratives-
tag-9 with questions-tag-9).
2The results presented in this paper are all evaluated on
the dev set; preliminary test set results on the Eve corpus
show the same patterns.
4
Model VM VC VH VI MA
wh-questions
BHMM: 63.0 (1.0) 59.8 (0.4) 66.6 (1.8) 1.63 (0.03) 70.7 (2.7)
BHMM-B: 58.7 (2.0) 58.2 (2.1) 59.2 (2.0) 1.74 (0.09) 59.7 (2.0)
Other Questions
BHMM: 65.6 (1.4) 62.7 (1.3) 68.7 (1.5) 1.62 (0.06) 74.5 (0.5)
BHMM-B: 64.4 (3.6) 62.6 (4.4) 66.2 (2.8) 1.65 (0.19) 70.8 (2.5)
Declaratives
BHMM: 60.9 (1.3) 58.7 (1.1) 63.3 (1.6) 1.84 (0.06) 73.5 (0.8)
BHMM-B: 58.0 (1.2) 55.5 (1.1) 60.7 (1.5) 1.99 (0.06) 69.0 (1.5)
Table 2: Results for BHMM-B on W/Q/D sentence types (dev set evaluation) in the Manchester corpus, compared
to the standard BHMM. The confidence interval is indicated in parentheses. Note that lower VI is better.
Model VM VC VH VI MA
BHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)
Q/D: 61.2 (1.2) 58.6 (1.2) 64.0 (1.4) 1.88 (0.06) 72.1 (1.5)
W/Q/D: 61.0 (1.7) 59.0 (1.5) 63.0 (2.0) 1.86 (0.08) 69.6 (2.2)
F/I/D/Q/W: 61.7 (1.7) 58.9 (1.8) 64.8 (1.6) 1.80 (0.09) 70.5 (1.3)
Table 3: Results for BHMM-E on the Eve corpus (dev set evaluation), compared to the standard BHMM. The
confidence interval is indicated in parentheses.
Table 2 shows representative results for the W/Q/D
condition on the Manchester corpus, separated into wh-
questions, other questions, and declaratives. For each
sentence type, the BHMM-B performs significantly
worse than the BHMM. The wh-questions sub-model,
which is trained on the smallest subset of the input cor-
pus, performs the worst across all measures except VI.
This suggests that lack of data is why these sub-models
perform worse than the standard model.
5.4 BHMM-E: Type-specific Emissions
Having demonstrated that using entirely separate sub-
models does not improve tagging performance, we turn
to the BHMM-E, in which emission probability distri-
butions are sentence-type specific, but transition prob-
abilities are shared between all sentence types.
The results in Table 3 show that BHMM-E does re-
sult in slightly better tagging performance as evaluated
by VI (lower VI is better) and VM and its components.
Matched accuracy does not capture this same trend. In-
specting the clusters found by the model, we find that
clusters for the most part do match gold categories. The
tokens that do not fall into the highest matching gold
categories are not distributed randomly, however; for
instance, nouns and pronouns often end up in the same
cluster. VI and VM capture these secondary matches
while MA does not. Some small gold categories (e.g.
the single word infinitival-to and negation-not cate-
gories) are often merged by the model into a single
cluster, with the result that MA considers nearly half
the cluster as misclassified.
The largest increase in performance with regards to
the standard BHMM is obtained by adding the distinc-
tion between declaratives and questions. Thereafter,
adding the wh-question, fragment and imperative sen-
tence types does not worsen performance, but also does
not significantly improve performance on any measure.
5.5 BHMM-T: Type-specific Transitions
Lastly, the BHMM-T shares emission probabilities
among sentence types and uses sentence type specific
transition probabilities.
Results comparing the standard BHMM with the
BHMM-T with sentence-type-specific transition prob-
abilities are presented in Table 4. Once again, VM
and VI show a clear trend: the models using sen-
tence type information outperform both the standard
BHMM and models splitting according to utterance
length (shorter/longer than average). MA shows no sig-
nificant difference in performance between the differ-
ent models (aside from clearly showing that utterance
length is an unhelpful feature). The fact that the MA
measure shows no clear change in performance is likely
a fault of the measure itself; as explained above, VI and
VM take into account the distribution of words within
a category, which MA does not.
As with the BHMM-E, the improvements to VM and
VI are obtained by distinguishing between questions
and declaratives, and then between wh- and other ques-
tions. Both of these distinctions are marked by intona-
tion in English. In contrast, distinguishing fragments
and imperatives, which are less easily detected by in-
tonation, provides no obvious benefit in any case. Us-
ing sentence length as a feature degrades performance
considerably, confirming that improvements in perfor-
mance are due to sentence types capturing useful infor-
mation about the tagging task, and not simply due to
splitting the input in some arbitrary way.
One reason for the improvement when adding the
wh-question type is that the models are learning to
identify and cluster the wh-words in particular. If we
evaluate the wh-words separately, VM rises from 32.3
5
Model VM VC VH VI MA
Eve
BHMM: 59.4 (0.2) 56.9 (0.2) 62.3 (0.2) 1.96 (0.01) 72.2 (0.2)
Q/D: 60.9 (0.5) 58.3 (0.4) 63.7 (0.6) 1.89 (0.02) 72.7 (0.3)
W/Q/D: 62.5 (1.2) 60.0 (1.3) 65.2 (1.0) 1.81 (0.06) 72.9 (0.8)
F/I/D/Q/W: 62.2 (1.5) 59.5 (1.6) 65.2 (1.3) 1.77 (0.08) 71.5 (1.4)
Length: 57.9 (1.2) 55.3 (1.1) 60.7 (1.3) 2.04 (0.06) 69.7 (2.0)
Manchester
BHMM: 60.2 (0.9) 57.6 (0.9) 63.1 (1.0) 1.92 (0.05) 72.1 (0.7)
Q/D: 61.5 (0.7) 59.2 (0.6) 63.9 (0.9) 1.84 (0.03) 71.6 (1.5)
W/Q/D: 62.7 (0.2) 60.6 (0.2) 65.0 (0.3) 1.78 (0.01) 71.2 (0.6)
F/I/D/Q/W: 62.5 (0.4) 60.3 (0.5) 64.9 (0.4) 1.79 (0.02) 71.3 (0.9)
Length: 58.1 (0.7) 55.6 (0.8) 60.8 (0.6) 2.02 (0.04) 71.0 (0.6)
Table 4: Results on the Eve and Manchester corpora for the various sentence types in the BHMM and BHMM-T
models. The confidence interval is indicated in parentheses.
in the baseline BHMM to 41.5 in the W/Q/D condition
with the BHMM-T model and 46.8 with the BHMM-
E model. Performance for the non-wh-words also im-
proves in the W/Q/D condition, albeit less dramati-
cally: from 61.1 in the baseline BHMM to 63.6 with
BHMM-T and 62.0 with BHMM-E. The wh-question
type enables the models to pick up on the defining char-
acteristics of these sentences, namely wh-words.
We predicted the sentence-type specific transition
probabilities in the BHMM-T to be more useful than
the sentence-type specific emission probabilities in the
BHMM-E. The BHMM-T does perform slightly better
than the BHMM-E, however, the effect is small. Word
or tag order may be the most overt difference between
questions and declaratives in English, but word choice,
especially the use of wh-words varies sufficiently be-
tween sentence types for sentence-type specific emis-
sion probabilities to be equally useful.
6 Crosslinguistic Experiments
In the previous section we found that sentence type
information improved syntactic categorisation in En-
glish. In this section, we evaluate the BHMM?s perfor-
mance on a range of languages other than English, and
investigate whether sentence type information is use-
ful across languages. To our knowledge this is the first
application of the BHMM to non-English data.
Nearly all human languages distinguish between
yes/no-questions and declaratives in intonation; ques-
tions are most commonly marked by rising intonation
(Hirst and Cristo, 1998). wh-questions do not always
have a distinct intonation type, but they are signalled
by the presence of members of the small class of wh-
words.
The CHILDES collection includes tagged corpora
for Spanish and Cantonese: the Ornat corpus (Ornat,
1994) and the Lee Wong Leung (LWL) corpus (Lee
et al, 1994) respectively. To cover a greater variety of
word order patterns, a Dutch corpus of adult dialogue
(not CDS) is also tested. We describe each corpus in
turn below; Table 5 describes their relative sizes.
Total Ds all Qs wh-Qs
Spanish 8759 4825 3934 1507
|w| 4.29 4.41 4.14 3.72
Cantonese 12544 6689 5855 2287
|w| 4.16 3.85 4.52 4.80
Dutch 8967 7812 1155 363
|w| 6.16 6.19 6.00 7.08
Table 5: Counts of sentence types in the training sets
for Spanish. Cantonese and Dutch. (Test and dev sets
are approximately 10% of the size of training.) |w| is
the average length in words of utterances of this type.
D: declaratives, Qs: questions, wh-Qs: wh-questions.
6.1 Spanish
The Ornat corpus is a longitudinal study of a single
child between the ages of one and a half and nearly
four years, consisting of 17 files. Files 08/09 are used
testing/development.
We collapse the Spanish tagset used in the Ornat cor-
pus in a similar fashion to the English corpora. There
are 11 tags in the final set: adjectives, adverbs, con-
juncts, determiners, nouns, prepositions, pronouns, rel-
ative pronouns, auxiliaries, verbs, and other.
Spanish wh-questions are formed by fronting the
wh-word (but without the auxiliary verbs added in
English); yes/no-questions involve raising the main
verb (again without the auxiliary inversion in English).
Spanish word order in declaratives is generally freer
than English word order. Verb- and object-fronting is
more common, and pronouns may be dropped (since
verbs are marked for gender and number).
6.2 Cantonese
The LWL corpus consists of transcripts from a set of
children followed over the course of a year, totalling
128 files. The ages of the children are not matched, but
they range between one and three years old. Our train-
ing set consists of the first 500 utterances of all train-
ing files, in order to create a data set of similar size as
the other corpora used. Files from children aged two
6
years and five months are used as the test set; files from
two years and six months are the development set files
(again, the first 500 utterances from each of these make
up the test/dev corpus).
The tagset used in the LWL is larger than the En-
glish corpus. It consists of 20 tags: adjective, ad-
verb, aspectual marker, auxiliary or modal verb, clas-
sifier, communicator, connective, determiners, genitive
marker, preposition or locative, noun, negation, pro-
nouns, quantifiers, sentence final particle, verbs, wh-
words, foreign word, and other. We remove all sen-
tences that are encoded as being entirely in English but
leave single foreign, mainly English, words (generally
nouns) in a Cantonese context.
Cantonese follows the same basic SVO word order
as English, but with a much higher frequency of topic-
raising. Questions are not marked by different word or-
der. Instead, particles are inserted to signal questioning.
These particles can signal either a yes/no-question or a
wh-question; in the case of wh-questions they replace
the item being questioned (e.g., playing-you what?),
without wh-raising as in English or Spanish. Despite
the use of tones in Cantonese, questions are marked
with rising final intonation.
6.3 Dutch
The Corpus of Spoken Dutch (CGN) contains Dutch
spoken in a variety of settings. We use the ?spontaneous
conversation? component, consisting of 925 files, since
it is the most similar to CDS. However, the utterances
are longer, and there are far fewer questions, especially
wh-questions (see Table 5).
The corpus does not have any meaningful timeline,
so we designated all files with numbers ending in 0 as
test files and files ending in 9 as dev files. The first
60 utterances from each file were used, to create train-
ing/test/dev sets similar in size to the other corpora.
The coarse CGN tagset consists of 11 tags, which
we used directly: adjective, adverb, conjunction, deter-
miner, interjection, noun, number, pronoun/determiner,
preposition, and verb.
Dutch follows verb-second word order in main
clauses and SOV word order in embedded clauses.
Yes/no-questions are created by verb-fronting, as in
Spanish. wh-questions involve a wh-word at the begin-
ning of the utterance followed by the verb in second
position.
6.4 Results
We trained standard BHMM, BHMM-T and BHMM-E
models in the same manner as with the English corpora.
Given the poor performance of the BHMM-B, we did
not test it here.
Due to inconsistent annotation and lack of familiar-
ity with the languages we tested only two sentence type
distinctions, Q/D and W/Q/D. Punctuation was used
to distinguish between questions and declaratives. wh-
questions were identified by using a list of wh-words
for Spanish and Dutch; for Cantonese we relied on the
wh-word tag annotation.
Results are shown in Table 6. Since the corpora
are different sizes and use tagsets of varying sizes, VI
and MA results are not comparable between corpora.
VM (and VC and VH) are more robust, but even so
cross-corpora comparisons should be made carefully.
The English corpora VM scores are significantly higher
(around 10 points higher) than the non-English corpora
scores.
In Cantonese and Dutch, the W/Q/D BHMM-T
model performs best; in both cases significantly bet-
ter than the BHMM. In Cantonese, the separation of
wh-questions improves tagging significantly in both the
BHMM-T and BHMM-E models, whereas simply sep-
arating questions and declaratives helps far less. In
the Dutch corpus, wh-questions improved only in the
BHMM-T, not in the BHMM-E.
The Spanish models have higher variance, due to the
small size of the corpus. Due to the high variance, there
are no significant differences between any of the con-
ditions; it is also difficult to spot a trend.
7 Future Work
We have shown sentence type information to be use-
ful for syntactic tagging. However, the BHMM-E and
BHMM-T models are successful in part however be-
cause they also share information as well as split it;
the completely split BHMM-B does not perform well.
Many aspects of tagging do not change significantly
between sentence types. Within a noun phrase, the or-
dering of determiners and nouns is the same whether
it is in a question or an imperative, and to a large ex-
tent the determiners and nouns used will be the same
as well. Learning these patterns over as much input as
possible is essential. Therefore, the next step in this line
of work will be to add a general (corpus-level) model
alongside type-specific models. Ideally, the model will
learn when to use the type-specific model (when tag-
ging the beginning of questions, for instance) and when
to use the general model (when tagging NPs). Such a
model would make use of sentence-type information in
a better way, hopefully leading to further improvements
in performance. A further, more sophisticated model
could learn the useful sentence types distinctions auto-
matically, perhaps forgoing the poorly performing im-
perative or fragment types we tested here in favor of a
more useful type we did not identify.
8 Conclusions
We set out to investigate a hitherto unused source of
information for models of syntactic category learning,
namely intonation and its correlate, sentence type. We
showed that this information is in fact useful, and in-
cluding it in a Bayesian Hidden Markov Model im-
proved unsupervised tagging performance.
We found tagging performance increases if sentence
type information is used to generate either transition
probabilities or emission probabilities in the BHMM.
However, we found that performance decreases if sen-
tence type information is used to generate both transi-
7
Model VM VC VH VI MA
Spanish
BHMM: 49.4 (1.8) 47.2 (1.9) 51.8 (1.8) 2.27 (0.09) 61.5 (2.1)
BHMM-E Q/D: 49.4 (1.5) 47.0 (1.4) 52.1 (1.7) 2.28 (0.06) 60.9 (2.6)
BHMM-E W/Q/D: 48.7 (2.5) 46.4 (2.4) 51.2 (2.6) 2.31 (0.11) 60.2 (3.0)
BHMM-T Q/D: 49.0 (1.7) 46.7 (1.6) 51.6 (1.7) 2.30 (0.07) 60.9 (2.5)
BHMM-T W/Q/D: 49.5 (2.5) 47.2 (2.3) 52.1 (2.8) 2.27 (0.11) 61.0 (3.0)
Cantonese
BHMM: 49.4 (0.8) 44.5 (0.7) 55.4 (1.0) 2.60 (0.04) 67.2 (1.0)
BHMM-E Q/D: 50.7 (1.6) 45.4 (1.5) 57.5 (1.7) 2.55 (0.09) 69.0 (1.0)
BHMM-E W/Q/D: 52.3 (0.3) 46.9 (0.3) 59.3 (0.4) 2.46 (0.02) 69.4 (0.9)
BHMM-T Q/D: 50.3 (0.9) 45.0 (0.9) 57.0 (1.0) 2.57 (0.05) 68.4 (0.8)
BHMM-T W/Q/D: 52.2 (0.8) 46.8 (0.9) 59.1 (0.7) 2.47 (0.05) 69.9 (1.9)
Dutch
BHMM: 48.4 (0.7) 47.1 (0.8) 49.7 (0.7) 2.38 (0.04) 62.3 (0.3)
BHMM-E Q/D: 48.4 (0.4) 47.3 (0.4) 49.7 (0.5) 2.37 (0.02) 62.2 (0.3)
BHMM-E W/Q/D 47.6 (0.3) 46.3 (0.4) 48.8 (0.2) 2.41 (0.02) 61.2 (1.1)
BHMM-T Q/D: 47.9 (0.5) 46.7 (0.4) 49.1 (0.5) 2.40 (0.02) 61.5 (0.4)
BHMM-T W/Q/D: 49.6 (0.2) 48.5 (0.2) 50.8 (0.2) 2.31 (0.10) 64.1 (0.2)
Table 6: Results for BHMM, BHMM-E, and BHMM-T on non-English corpora.
tion and emission probabilities (which is equivalent to
training a separate BHMM for each sentence type).
To test the generality of our findings, we carried out a
series of cross-linguistic experiments, integrating sen-
tence type information in unsupervised tagging mod-
els for Spanish, Cantonese, and Dutch. The results for
Cantonese and Dutch mirrored those for English, show-
ing a small increase in tagging performance for models
that included sentence type information. For Spanish,
no improvement was observed.
References
Roger Brown. 1973. A first language: The early
stages. Harvard University Press.
Anne Fernald. 1989. Intonation and communicative
intent in mothers? speech to infants: Is the melody
the message? Child Development, 60(6):1497?
1510.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics.
Daniel Hirst and Albert Di Cristo, editors. 1998. Into-
nation systems: a survey of twenty languages. Cam-
bridge University Press.
Thomas H.T. Lee, Colleen H Wong, Samuel Leung,
Patricia Man, Alice Cheung, Kitty Szeto, and Cathy
S P Wong. 1994. The development of grammatical
competence in cantonese-speaking children. Techni-
cal report, Report of RGC earmarked grant 1991-94.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ.
Marina Meila. 2007. Comparing clusterings ? an in-
formation based distance. Journal of Multivariate
Analysis, 98:873?895.
Susana Lopez Ornat. 1994. La adquisicion de la
lengua espagnola. Siglo XXI, Madrid.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In EMNLP.
David Snow and Heather Balog. 2002. Do chil-
dren produce the melody before the words? A re-
view of developmental intonation research. Lingua,
112:1025?1058.
Daniel N. Stern, Susan Spieker, and Kristine MacK-
ain. 1982. Intonation contours as signals in mater-
nal speech to prelinguistic infants. Developmental
Psychology, 18(5):727?735.
Paul A. Taylor, S. King, S. D. Isard, and H. Wright.
1998. Intonation and dialogue context as constraints
for speech recognition. Language and Speech,
41(3):493?512.
Anna Theakston, Elena Lieven, Julian M. Pine, and
Caroline F. Rowland. 2001. The role of per-
formance limitations in the acquisition of verb-
argument structure: an alternative account. Journal
of Child Language, 28:127?152.
8
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 14?18,
Gothenburg, Sweden, April 26 2014.
c
?2014 Association for Computational Linguistics
Learning the hyperparameters to learn morphology
Stella Frank
ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
sfrank@inf.ed.ac.uk
Abstract
We perform hyperparameter inference
within a model of morphology learn-
ing (Goldwater et al., 2011) and find
that it affects model behaviour drastically.
Changing the model structure successfully
avoids the unsegmented solution, but re-
sults in oversegmentation instead.
1 Introduction
Bayesian models provide a sound statistical
framework in which to explore aspects of language
acquisition. Explicitly specifying the causal and
computational structure of a model enables the in-
vestigation of hypotheses such as the feasibility of
learning linguistic structure from the available in-
put (Perfors et al., 2011), or the interaction of dif-
ferent linguistic levels (Johnson, 2008a). However,
these models can be sensitive to small changes in
(hyper-)parameters settings. Robustness in this re-
spect is important, since positing specific parame-
ter values is cognitively implausible.
In this paper we revisit a model of morphol-
ogy learning presented by Goldwater and col-
leagues in Goldwater et al. (2006) and Goldwa-
ter et al. (2011) (henceforth GGJ). This model
demonstrated the effectiveness of non-parametric
stochastic processes, specifically the Pitman-Yor
Process, for interpolating between types and to-
kens. Language learners are exposed to tokens,
but many aspects of linguistic structure are lexical;
identifying which tokens belong to the same lexi-
cal type is crucial. Surface form is not always suf-
ficient, as in the case of ambiguous words. More-
over, morphology in particular is influenced by
vocabulary-level type statistics (Bybee, 1995), so
it is important for a model to operate on both lev-
els: token statistics from realistic (child-directed)
input, and type-level statistics based on the token
analyses.
The GGJ model learns successfully given fixed
hyperparameter values in the Pitman-Yor Process.
However, we show that when these hyperparam-
eters are inferred, it collapses to a token-based
model with a trivial morphology. In this paper
we discuss the reasons for this problematic be-
haviour, which are relevant for other models based
on Pitman-Yor Processes with discrete base dis-
tributions, common in natural language tasks. We
investigate some potential solutions, by chang-
ing the way morphemes are generated within the
model. Our results are mixed; we avoid the hyper-
parameter problem, but learn overly compact mor-
pheme lexicons.
2 The Pitman-Yor Process
The Pitman-Yor Process G ? PYP(a, b,H
0
) (Pit-
man and Yor, 1997; Teh, 2006) generates distribu-
tions over the space of the base distribution H
0
,
with the hyperparameters a and b governing the
extent of the shift from H
0
. Draws from G have
values from H
0
, but with probabilities given by
the PYP. For example, in a unigram PYP language
model with observed words, H
0
may be a uni-
form distribution over the vocabulary, U(
1
T
). The
PYP shifts this distribution to the power-law dis-
tribution over tokens found in natural language,
allowing words to have much higher (and lower)
than uniform probability. We will continue using
the language model example in this section, since
the subsequent morphology model is effectively a
complex unigram language model in which word
types correspond to morphological analyses. In
our presentation, we pay particular attention to the
role of the hyperparameter a, since this value gov-
erns the power-law behaviour of the PYP (Buntine
and Hutter, 2010).
When G is marginalised out, the result is the
PYP Chinese Restaurant Process, which is a use-
ful representation of the distribution of observa-
tions (word tokens) to values from H
0
(types). In
14
this restaurant, customers (tokens) arrive and are
seated at one of a potentially infinite number of
tables. Each table receives a dish (type) from the
base distribution when the first customer is seated
there; thereafter all subsequent customers adopt
the same dish. The probability of customer z
i
be-
ing seated at a table k depends on the number of
customers already seated at that table n
k
. Popu-
lar tables will attract more customers, generating a
Zipfian distribution over customers at tables.
This Zipfian/power-law behaviour can be simi-
lar to that of the natural language data, and is the
principal motivation behind using the PYP. How-
ever, it is only valid for the distribution of cus-
tomers to tables. When the base distribution is dis-
crete ? as in our language model example and
the morphology model ? the same dish may be
served at multiple tables. In most cases, the dis-
tribution of interest is generally that of customers
(tokens) to dishes (types), rather than to tables,
suggesting a preference for a setting in which each
dish appears at few tables. This is dependent on a
(constrained to be 0 ? a < 1), and to a lesser ex-
tent on b: If a is small, each dish will be served at
a single table, resulting in the type-token and the
table-customer power-laws matching. If a is near
1, however, the probability of more than a single
customer being seated at a table is small, and the
distribution of dishes being eaten by the customers
will match the base distribution, rather than being
adapted by the caching mechanism of the PYP.
The expected number of tables K grows as
O(N
a
) (see Buntine and Hutter (2010) for an ex-
act formulation). The number of word types in
the data gives us a minimum number of tables,
K ? T . When a is small (less than 0.5), the num-
ber of expected tables is significantly less than the
number of types in a non-trivial dataset, suggest-
ing a lower bound for values of a.
In our language model, the posterior probability
of assigning a wordw
i
to a table k with dish `
k
and
n
k
previous customers is:
p(w
i
= k|w
1
. . . w
i?1
, a, b) ? (1)
{
(n
k
? a)I(w
i
= `
k
) if 1 ? k ? K
(Ka+ b)H
0
(w
i
) if k = K + 1
where I(w
i
= `
k
) returns 1 if the token and the
dish match, and 0 otherwise. We see that in order
to prefer assigning customers to already occupied
tables, we need H
0
(w)(Ka+ b) < n
k
? a. Given
K ? T , and setting H
0
=
1
T
, we can approxi-
mate this with
1
T
(Ta + b) < n
k
? a. From this
we obtain a <
1
2
(n
k
?
b
T
), which indicates that in
order for tables with a single customer (n
k
= 1) to
attract further customers, a must be smaller than
0.5. Thus, there is a tension between the number
of tables required by the data and our desire to
reuse tables. One solution is to fix a to an arbi-
trary, sufficiently small value, as GGJ do in their
experiments. In contrast, in this paper we infer a
and b along with the other parameters, and change
the other free variable, the base distribution H
0
.
3 Morphology
The morphology model introduced by GGJ has a
base distribution that generates not simply word
types, as in the language model example, but mor-
phological analyses. These are relatively simple,
consisting of stem+suffix segmentation and a clus-
ter membership. The probability of a word is the
sum of the probability of all cluster c, stem s, suf-
fix f tuples:
H
0
(w) =
?
(c,s,f)
p(c)p(s|c)p(f |c)I(w = s.f)
(2)
with the stems and the suffixes being gener-
ated from cluster-specific distributions. In the
GGJ model, all three distributions (cluster, stem,
suffix) are finite conjugate symmetric Dirichlet-
Multinomial (DirMult) distributions. We retain the
DirMult over clusters, but change the morpheme-
generating distributions.
The DirMult is equivalent to a Dirichlet Process
prior (DP) with a finite base distribution; we use
this representation because it allows us to replace
the base distributions flexibly. A DP(?,H
0
) is also
equivalent to a PYP with a = 0, and thus also can
be represented with a Chinese Restaurant Process,
but in this case we sum over all tables to obtain the
predictive probability of a (say) stem:
p(s|?
s
, H
S
) =
m
s
+ ?
s
H
S
?
s
?
m
s
?
+ ?
s
(3)
Note that the counts m
s
refer to stems generated
within the base distribution, not to token counts
within the PYP.
The original GGJ model, ORIG, is equivalent to
setting H
S
for stems to U(
1
S
), and likewise H
F
=
U(
1
F
), where S and F are the number of possible
stems and suffixes in the dataset (i.e., all possible
prefix and suffix strings, including a null string).
15
There are two difficulties with this model.
Firstly, it assumes a closed vocabulary and re-
quires setting S and F in advance, by looking at
the data. As a cognitive model, this is awkward,
since it assumes a fixed, relatively small number
of possible morphemes.
Secondly, when the PYP hyperparameters are
inferred, a is set to be nearly 1, resulting in a model
with as many tokens as tables. This behaviour is
due to the interaction between vocabulary size and
base distribution probabilities outlined in the pre-
vious section: this base distribution assigns rel-
atively high probability to words, so new tables
have high probability; as the number of tables in-
creases (from its fairly large minimum), the op-
timal a for this table configuration also increases,
resulting in convergence at the token-based model.
We investigate two alternate base distribution
over stems and suffixes, both of which extend the
space of possible morphemes, thereby lowering
the overall probability of the observed words.
DP-CHAR generates morphemes by first gener-
ating a length l ? Poisson(?). Characters
are then drawn from a uniform distribution,
c
0...l
? U(1/|Chars|). A morpheme?s prob-
ability decreases exponentially by length, re-
sulting in a strong preference for shorter mor-
phemes.
DP-UNI simply extends the original uniform dis-
tribution to s and f ? U(1/1e6), in effect
moving probability mass to a large number
of unseen morphemes. It is thus similar to
DP-CHAR without the length preference.
4 Inference
We follow the same inference procedure as GGJ,
using Gibbs sampling. The sampler iterates be-
tween inferring each token?s table assignment and
resampling the table labels (see GGJ for details).
Within the morphology base distribution, the
prior for the DirMult over clusters is set to ?
k
=
0.5. To replicate the original DirMult model
1
, we
set ?
s
= 0.001S and ?
f
= 0.001F . In the other
models, ?
s
= ?
f
= 1. Within DP-CHAR, ? = 6
for stems, 0.5 for suffixes.
1
In this model, the predictive posterior is defined as
p(s|?, S) =
m
s
+?
m
.
+S?
, using an alternate definition of ?.
Eve (Orth.) Ornat (Orth.)
a Tables/Type a Tables/Type
ORIG 0.96 21.2 0.97 10.64
DP-CHAR 0.46 1.4 0.56 1.17
DP-UNI 0.81 7.3 0.70 2.33
Table 1: Final values for a on the orthographic En-
glish and Spanish datasets, as well as the average
number of tables for each word type. The 95%
confidence interval across three runs is ? 0.01.
(Phonological Eve is similar to Orthographic Eve.)
4.1 Sampling Hyperparameters
We sample PYP a and b hyperparameters using
a slice sampler
2
. Previous work with this model
has always fixed these values, generally finding
small a to be optimal and b to have little effect.
In experiments with fixed hyperparameters, we set
a = b = 0.1.
To sample the hyperparameters, we place vague
priors over them: a ? Beta(1, 1) and b ?
Gamma(10, 0.1). The slice sampler samples a new
value for a and b after every 10 iterations of Gibbs
sampling.
5 Experiments
5.1 Datasets
Our datasets consist of the adult utterances
from two morphologically annotated corpora from
CHILDES, an English corpus, Eve (Brown, 1973),
and a Spanish corpus, Ornat (Ornat, 1994). Mor-
phology is marked by a grammatical suffix on the
stem, e.g. doggy-PL. Words marked with irregular
morphology are unsegmented.
The two languages, while related, have differ-
ing degrees of affixation: the English Eve corpus
consists of 63 315 tokens (5% suffixed) and 1 988
types (28% suffixed); the Ornat corpus has 43 796
tokens (23% suffixed) and 3 157 types (50% suf-
fixed). The English corpus has 17 gold suffix
types, while Spanish has 72.
We also use the phonologically encoded Eve
dataset used by GGJ. This dataset does not ex-
actly correspond to the orthographic version, due
to discrepancies in tokenisation, so we are unable
to evaluate this dataset quantitatively.
2
Mark Johnson?s implementation, available at
http://web.science.mq.edu.au/
?
mjohnson/
Software.htm
16
Eve (Orth.) Ornat (Orth.) Eve (Phon.)
% Seg |L| VM % Seg |L| VM % Seg |L|
Gold 5 23 (5)
ORIG Fix 7 1680 46.42(10.8) 14 2488 46.63(2.7) 17 1619
ORIG Inf 1 1893 9.94(1.0) 4 2769 17.80(3.7) 1 1984
DP-CHAR Fix 52 1331 15.33(0.3) 83 1828 35.76(1.1) 47 1289
DP-CHAR Inf 50 1330 16.15(0.4) 85 1824 36.47(0.5) 33 1317
DP-UNI Fix 38 1394 17.28(1.7) 51 1874 39.58(0.8) 36 1392
DP-UNI Inf 15 1574 31.54(3.1) 31 1983 42.48(1.1) 21 1500
Table 2: Final morphology results. ?Fix? refers to models with fixed PYP hyperparameters (a = b = 0.1),
while ?Inf? models have inferred hyperparameters. % Seg shows the percentage of tokens that have a non-
null suffix, while |L| is the size of the morpheme lexicon. VM is shown with 95% confidence intervals.
5.2 Results
For each setting, we report the average over three
runs of 1000 iterations of Gibbs sampling without
annealing, using the last iteration for evaluation.
Table 1 shows what happens when hyperpa-
rameters are inferred: ORIG finds a token-based
solution, with as many tables as tokens, while
DP-CHAR is the opposite, with a small a allowing
for just over one table for each word type. DP-UNI
is between these two extremes. b is consistently
between 1 and 3, confirming it has little effect.
The effect of the hyperparameters can be seen
in the morphology results, shown in Table 2.
DP-CHAR is robust across hyperparameter val-
ues, finding the same type-based solution with
fixed and inferred hyperparameters, while the
other models have very different results depending
on the hyperparameter settings. ORIG with fixed
hyperparameters performs best, with the highest
VM score (a clustering measure, Rosenberg and
Hirschberg (2007)) and a level of segmentation
close to the correct one. However, with inferred
hyperparameters, this model severely underseg-
ments: it finds the unsegmented maximum likeli-
hood solution, where all tokens are generated from
the stem distribution (Goldwater, 2007).
The models with alternate base distributions go
to the other extreme, oversegmenting the corpus.
As generating new morphemes becomes less prob-
able, the pressure to find the most compact mor-
pheme lexicon grows. This leads to oversegmen-
tation due to many spurious suffixes. The length
penalty in DP-CHAR exacerbates this problem, but
it can be seen in the DP-UNI solutions as well,
particularly when hyperparameters are fixed to en-
courage a type-based solution.
6 Conclusion
The base distribution in the original GGJ model
assigned a relatively high probability to unseen
morphemes, allowing the model to generate new
analyses for seen words instead of reusing old
analyses and leading to undersegented token-
based solutions. The alternative base distributions
proposed here were effective in finding type-based
solutions. However, these over-segmented solu-
tions clearly do not match the true morphology,
indicating that the model structure is inadequate.
One reason may be that the model structure
is overly simple. The model is faced with an ar-
guably more difficult task than a human learner,
who has access to semantic, syntactic, and phono-
logical cues. Adding these types of information
has been shown to help morphology learning in
similar models (Johnson, 2008b; Sirts and Gold-
water, 2013; Frank et al., 2013).
Similarly, the morphological ambiguity that is
captured by a model operating over tokens (and
ignored in better-performing models that allow
only a single analysis for each word type: Poon
et al. (2009); Lee et al. (2011); Sirts and Alum?ae
(2012)) can often be disambiguated using seman-
tic and syntactic information. A model that gener-
ates a single analysis per meaningful (semantically
and syntactically distinct) word-form could avoid
the potential problems of spurious re-generation
seen in the original GGJ model as well as the
converse problem of under-generation in our al-
ternatives. Such a model might also map onto the
human lexicon (which demonstrably avoids both
problems) in a more realistic way.
17
References
Roger Brown. A first language: The early
stages. Harvard University Press, Cambridge,
MA, 1973.
Wray Buntine and Marcus Hutter. A Bayesian
view of the Poisson-Dirichlet process. 2010.
URL arXiv:1007.0296.
Joan Bybee. Regular morphology and the lexi-
con. Language and Cognitive Processes, 10:
425?455, 1995.
Stella Frank, Frank Keller, and Sharon Goldwater.
Exploring the utility of joint morphological and
syntactic learning from child-directed speech.
In Proceedings of the 18th Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP), 2013.
Sharon Goldwater. Nonparametric Bayesian Mod-
els of Lexical Acquisition. PhD thesis, Brown
University, 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Interpolating between types and to-
kens by estimating power-law generators. In
Advances in Neural Information Processing
Systems 18, 2006.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Producing power-law distributions
and damping word frequencies with two-stage
language models. Journal of Machine Learning
Research, 12:2335?2382, 2011.
Mark Johnson. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), 2008a.
Mark Johnson. Unsupervised word segmentation
for Sesotho using Adaptor Grammars. In Pro-
ceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology
and Phonology, pages 20?27, June 2008b.
Yoong Keok Lee, Aria Haghighi, and Regina
Barzilay. Modeling syntactic context improves
morphological segmentation. In Proceedings of
Fifteenth Conference on Computational Natural
Language Learning (CONLL), 2011.
S. Lopez Ornat. La adquisicion de la lengua es-
pagnola. Siglo XXI, Madrid, 1994.
Amy Perfors, Joshua B. Tenenbaum, and Terry
Regier. The learnability of abstract syntactic
principles. Cognition, 118(3):306 ? 338, 2011.
Jim Pitman and Marc Yor. The two-parameter
Poisson-Dirichlet distribution derived from a
stable subordinator. Annals of Probability, 25
(2):855?900, 1997.
Hoifung Poon, Colin Cherry, and Kristina
Toutanova. Unsupervised morphological
segmentation with log-linear models. In
Proceedings of the Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL), 2009.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of
the 12th Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2007.
Kairit Sirts and Tanel Alum?ae. A hierarchical
Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of
the Conference of the North American Chapter
of the Association for Computational Linguis-
tics (NAACL), 2012.
Kairit Sirts and Sharon Goldwater. Minimally-
supervised morphological segmentation using
adaptor grammars. Transactions of the Associa-
tion for Computational Linguistics, 1:231?242,
2013.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
Sydney, 2006.
18
