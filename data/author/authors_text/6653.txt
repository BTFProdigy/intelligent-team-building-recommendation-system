R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 22 ? 33, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
The Use of Monolingual Context Vectors for Missing 
Translations in Cross-Language Information Retrieval 
Yan Qu1, Gregory Grefenstette2, and David A. Evans1 
1
 Clairvoyance Corporation, 5001 Baum Boulevard, Suite 700, 
Pittsburgh, PA, 15213, USA 
{yqu, dae}@clairvoyancecorp.com 
2
 LIC2M/SCRI/LIST/DTSI/CEA, B.P.6, 
92265 Fontenay-aux-Roses Cedex, France 
{Gregory.Grefenstette}@cea.fr 
Abstract. For cross-language text retrieval systems that rely on bilingual dic-
tionaries for bridging the language gap between the source query language and 
the target document language, good bilingual dictionary coverage is imperative.  
For terms with missing translations, most systems employ some approaches for 
expanding the existing translation dictionaries.  In this paper, instead of lexicon 
expansion, we explore whether using the context of the unknown terms can help 
mitigate the loss of meaning due to missing translation.  Our approaches consist 
of two steps: (1) to identify terms that are closely associated with the unknown 
source language terms as context vectors and (2) to use the translations of the 
associated terms in the context vectors as the surrogate translations of the un-
known terms.  We describe a query-independent version and a query-dependent 
version using such monolingual context vectors.  These methods are evaluated 
in Japanese-to-English retrieval using the NTCIR-3 topics and data sets.  Em-
pirical results show that both methods improved CLIR performance for short 
and medium-length queries and that the query-dependent context vectors per-
formed better than the query-independent versions. 
1   Introduction 
For cross-language text retrieval systems that rely on bilingual dictionaries for bridg-
ing the language gap between the source query language and the target document 
language, good bilingual dictionary coverage is imperative [8,9].  Yet, translations for 
proper names and special terminology are often missing in available dictionaries.  
Various methods have been proposed for finding translations of names and terminol-
ogy through transliteration [5,11,13,14,16,18,20] and corpus mining [6,7,12,15,22].  
In this paper, instead of attempting to find the candidate translations of terms without 
translations to expand existing translation dictionaries, we explore to what extent 
simply using text context can help mitigate the missing translation problem and for 
what kinds of queries.  The context-oriented approaches include (1) identifying words 
that are closely associated with the unknown source language terms as context vectors 
and (2) using the translations of the associated words in the context vectors as the 
surrogate translations of the unknown words. We describe a query-independent  
 The Use of Monolingual Context Vectors for Missing Translations 23 
version and a query-dependent version using such context vectors.  We evaluate these 
methods in Japanese-to-English retrieval using the NTCIR-3 topics and data sets.  In 
particular, we explore the following questions: 
? Can translations obtained from context vectors help CLIR performance? 
? Are query-dependent context vectors more effective than query-independent 
context vectors for CLIR? 
In the balance of this paper, we first describe related work in Section 2.  The methods 
of obtaining translations through context vectors are presented in Section 3.  The CLIR 
evaluation system and evaluation results are presented in Section 4 and Section 5, re-
spectively.  We summarize the paper in Section 6. 
2   Related Work 
In dictionary-based CLIR applications, approaches for dealing with terms with missing 
translations can be classified into three major categories.  The first is a do-nothing ap-
proach by simply ignoring the terms with missing translations.  The second category 
includes attempts to generate candidate translations for a subset of unknown terms, such 
as names and technical terminology, through phonetic translation between different 
languages (i.e., transliteration) [5,11,13,14,16,18,20].  Such methods generally yield 
translation pairs with reasonably good accuracy reaching about 70% [18].  Empirical 
results have shown that the expanded lexicons can significantly improve CLIR system 
performance [5,16,20].  The third category includes approaches for expanding existing 
bilingual dictionaries by exploring multilingual or bilingual corpora.  For example, the 
?mix-lingual? feature of the Web has been exploited for locating translation pairs by 
searching for the presence of both Chinese and English text in a text window [22].  In 
work focused on constructing bilingual dictionaries for machine translation, automatic 
translation lexicons are compiled using either clean aligned parallel corpora [12,15] or 
non-parallel comparable corpora [6,7].  In work with non-parallel corpora, contexts of 
source language terms and target language terms and a seed translation lexicon are 
combined to measure the association between the source language terms and potential 
translation candidates in the target language.  The techniques with non-parallel corpora 
save the expense of constructing large-scale parallel corpora with the tradeoff of lower 
accuracy, e.g., about 30% accuracy for the top-one candidate [6,7].  To our knowledge, 
the usefulness of such lexicons in CLIR systems has not been evaluated. 
While missing translations have been addressed in dictionary-based CLIR systems, 
most of the approaches mentioned above attempt to resolve the problem through dic-
tionary expansion.  In this paper, we explore non-lexical approaches and their effective-
ness on mitigating the problem of missing translations.  Without additional lexicon 
expansion, and keeping the unknown terms in the source language query, we extract 
context vectors for these unknown terms and obtain their translations as the surrogate 
translations for the original query terms.  This is motivated by the pre-translation feed-
back techniques proposed by several previous studies [1,2].  Pre-translation feedback 
has been shown to be effective for resolving translation ambiguity, but its effect on 
recovering the lost meaning due to missing translations has not been empirically evalu-
ated.  Our work provides the first empirical results for such an evaluation. 
24 Y. Qu, G. Grefenstette, and D.A. Evans 
3   Translation via Context Vectors 
3.1   Query-Independent Context Vectors 
For a source language term t, we define the context vector of term t as:  
tC = ?? ittttt ,...,,,, 4321  
where terms 1t  to it  are source language terms that are associated with term t within 
a certain text window in some source language corpus.  In this report, the associated 
terms are terms that co-occur with term t above a pre-determined cutoff threshold. 
Target language translations of term t are derived from the translation of the known 
source language terms in the above context vectors: 
trans(t) = <trans(t1), trans(t2), ?, trans(tn)> 
Selection of the source language context terms for the unknown term above is only 
based on the association statistics in an independent source language corpus.  It does 
not consider other terms in the query as context; thus, it is query independent.  Using 
the Japanese-to-English pair as an example, the steps are as follows: 
1. For a Japanese term t that is unknown to the bilingual dictionary, extract 
concordances of term t within a window of P bytes (we used P=200 bytes 
or 100 Japanese characters) in a Japanese reference corpus. 
2. Segment the extracted Japanese concordances into terms, removing stop-
words. 
3. Select the top N (e.g., N=5) most frequent terms from the concordances to 
form the context vector for the unknown term t. 
4. Translate these selected concordance terms in the context vector into Eng-
lish to form the pseudo-translations of the unknown term t. 
Note that, in the translation step (Step 4) of the above procedure, the source lan-
guage association statistics for selecting the top context terms and frequencies of their 
translations are not used for ranking or filtering any translations.  Rather, we rely on 
the Cross Language Information Retrieval system?s disambiguation function to select 
the best translations in context of the target language documents [19]. 
3.2   Query-Dependent Context Vectors 
When query context is considered for constructing context vectors and pseudo-
translations, the concordances containing the unknown terms are re-ranked based on 
the similarity scores between the window concordances and the vector of the known 
terms in the query.  Each window around the unknown term is treated as a document, 
and the known query terms are used.  This is based on the assumption that the top 
ranked concordances are likely to be more similar to the query; subsequently, the 
context terms in the context vectors provide better context for the unknown term.  
Again, using the Japanese-English pair as an example, the steps are as follows: 
 The Use of Monolingual Context Vectors for Missing Translations 25 
1. For a Japanese term t unknown to the bilingual dictionary, extract a window of 
text of P bytes (we used P=200 bytes or 100 Japanese characters) around 
every occurrence of term t in a Japanese reference corpus. 
2. Segment the Japanese text in each window into terms and remove stopwords. 
3. Re-rank the window based on similarity scores between the terms found in the 
window and the vector of the known query terms. 
4. Obtain the top N (e.g., N=5) most frequently occurring terms from the top M 
(e.g., M=100) ranking windows to form the Japanese context vector for the 
unknown term t. 
5. Translate each term in the Japanese context vector into English to form the 
pseudo-translations of the unknown term t. 
The similarity scores are based on Dot Product. 
The main difference between the two versions of context vectors is whether the 
other known terms in the query are used for ranking the window concordances.  
Presumably, the other query terms provide a context-sensitive interpretation of the 
unknown terms.  When M is extremely large, however, the query-dependent version 
should approach the performance of the query-independent version. 
We illustrate both versions of the context vectors with topic 23 
(????????????? ?President Kim Dae-Jung's policy toward Asia?) 
from NTCIR-3: 
First, the topic is segmented into terms, with the stop words removed: 
???; ???; ???; ?? 
Then, the terms are categorized as ?known? vs. ?unknown? based on the bilingual 
dictionary: 
Unknown:  
Query23: ??? 
Known: 
Query23:??? 
Query23:??? 
Query23:?? 
Next, concordance windows containing the unknown term ??? are extracted:  
?????????????????????????????????    
????????????????????????????????? 
???????????????????????????????? 
???????????????????????????  
?? 
Next, the text in each window is segmented by a morphological processor into 
terms with stopwords removed [21]. 
In the query-independent version, we simply select the top 5 most frequently oc-
curring terms in the concordance windows.  The top 5 source language context terms 
for ??? are: 
26 Y. Qu, G. Grefenstette, and D.A. Evans 
3527:? 
3399:?? 
3035:??? 
2658:?? 
901:??????
1
 
Then, the translations of the above context terms are obtained from the bilingual 
dictionary to provide pseudo-translations for the unknown term ???, with the 
relevant translations in italics: 
??? ? ? ? gold 
??? ? ? ? metal 
??? ? ? ? money 
??? ??? ? ? 
??? ? ??? ? chief executive 
??? ? ??? ? president 
??? ? ??? ? presidential 
??? ? ?? ? korea 
??? ??????? ? ? 
With the query-dependent version, the segmented concordances are ranked by 
comparing the similarity between the concordance vector and the known term vector.  
Then we take the 100 top ranking concordances and, from this smaller set, select the 
top 5 most frequently occurring terms.  This time, the top 5 context terms are: 
1391:??? 
1382:? 
1335:?? 
1045:?? 
379:?????? 
In this example, the context vectors from both versions are the same, even though 
the terms are ranked in different orders.  The pseudo-translations from the context 
vectors are: 
??? ? ??? ? chief executive 
??? ? ??? ? president 
??? ? ??? ? presidential 
??? ? ? ? gold 
??? ? ? ? metal 
??? ? ? ? money 
??? ??? ? ? 
??? ? ?? ? korea 
??? ??????? ? ? 
                                                          
1
 Romanization of the katakana name ?????? could produce a correct transliteration of 
the name in English, which is not addressed in this paper.  Our methods for name translitera-
tion can be found in [18,20]. 
 The Use of Monolingual Context Vectors for Missing Translations 27 
4   CLIR System 
We evaluate the usefulness of the above two methods for obtaining missing transla-
tions in our Japanese-to-English retrieval system.  Each query term missing from our 
bilingual dictionary is provided with pseudo-translations using one of the methods.  
The CLIR system involves the following steps: 
First, a Japanese query is parsed into terms2 with a statistical part of speech tagger 
and NLP module [21].  Stopwords are removed from query terms.  Then query terms 
are split into a list of known terms, i.e., those that have translations from bilingual 
dictionaries, and a list of unknown terms, i.e., those that do not have translations from 
bilingual dictionaries.  Without using context vectors for unknown terms, translations 
of the known terms are looked up in the bilingual dictionaries and our disambiguation 
module selects the best translation for each term based on coherence measures be-
tween translations [19]. 
The dictionaries we used for Japanese to English translation are based on edict3, 
which we expanded by adding translations of missing English terms from a core Eng-
lish lexicon by looking them up using BabelFish4.  Our final dictionary has a total of 
210,433 entries.  The English corpus used for disambiguating translations is about 
703 MB of English text from NTCIR-4 CLIR track5.  For our source language corpus, 
we used the Japanese text from NTCIR-3. 
When context vectors are used to provide translations for terms missing from our dic-
tionary, first, the context vectors for the unknown terms are constructed as described 
above.  Then the same bilingual lexicon is used for translating the context vectors to 
create a set of pseudo-translations for the unknown term t.  We keep all the pseudo-
translations as surrogate translations of the unknown terms, just as if they really were 
the translations we found for the unknown terms in our bilingual dictionary. 
We use a corpus-based translation disambiguation method for selecting the best 
English translations for a Japanese query word.  We compute coherence scores of 
translated sequences created by obtaining all possible combinations of the translations 
in a source sequence of n query words (e.g., overlapping 3-term windows in our ex-
periments).  The coherence score is based on the mutual information score for each 
pair of translations in the sequence.  Then we take the sum of the mutual information 
scores of all translation pairs as the score of the sequence.  Translations with the high-
est coherence scores are selected as best translations.  More details on translation 
disambiguation can be found in [19]. 
Once the best translations are selected, indexing and retrieval of documents in the 
target language is based on CLARIT [4].  For this work, we use the dot product func-
tion for computing similarities between a query and a document: 
                                                          
2
 In these experiments, we do not include multiple-word expression such as ???? (war 
crime) as terms, because translation of most compositional multiple-word expressions can be 
generally constructed from translations of component words (?? and ??) and our empiri-
cal evaluation has not shown significant advantages of a separate model of phrase translation. 
3
 http://www.csse.monash.edu.au/~jwb/j_edict.html 
4
 http://world.altavista.com/ 
5
 http://research.nii.ac.jp/ntcir/ntcir-ws4/clir/index.html 
28 Y. Qu, G. Grefenstette, and D.A. Evans 
)()(),( tWtWDPsim
DPt
DP?
??
?=  . (1) 
where WP(t) is the weight associated with the query term t and WD(t) is the weight 
associated with the term t in the document D.  The two weights are computed as  
follows: 
)()()( tIDFtTFtW DD ?=  . (2) 
)()()()( tIDFtTFtCtW PP ??=  . (3) 
where IDF and TF are standard inverse document frequency and term frequency sta-
tistics, respectively.  IDF(t) is computed with the target corpus for retrieval.  The 
coefficient C(t) is an ?importance coefficient?, which can be modified either manually 
by the user or automatically by the system (e.g., updated during feedback). 
For query expansion through (pseudo-) relevance feedback, we use pseudo-
relevance feedback based on high-scoring sub-documents to augment the queries.  
That is, after retrieving some sub-documents for a given topic from the target corpus, 
we take a set of top ranked sub-documents, regarding them as relevant sub-documents 
to the query, and extract terms from these sub-documents.  We use a modified Roc-
chio formula for extracting and ranking terms for expansion: 
NumDoc
DocSetD
t
D
TF
tIDFtRocchio
?
??=
)(
)()(
 
(4) 
where IDF(t) is the Inverse Document Frequency of term t in reference database, 
NumDoc the number of sub-documents in the given set of sub-documents, and TFD(t) 
the term frequency score for term t in sub-document D. 
Once terms for expansion are extracted and ranked, they are combined with the 
original terms in the query to form an expanded query. 
exp
QQk
new
Q +?=  (5) 
in which Qnew, Qorig, Qexp stand for the new expanded query, the original query, and 
terms extracted for expansion, respectively.  In the experiments reported in Section 5, 
we assign a constant weight to all expansion terms (e.g., 0.5) 
5   Experiments 
5.1   Experiment Setup 
For evaluation, we used NTCIR-3 Japanese topics6.  Of the 32 topics that have rele-
vance judgments, our system identifies unknown terms as terms not present in our 
expanded Japanese-to-English dictionary described above.  The evaluation of the  
 
                                                          
6
 http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings3/index.html 
 The Use of Monolingual Context Vectors for Missing Translations 29 
effect of using context vectors is based only on the limited number of topics that con-
tain these unknown terms.  The target corpus is the NTCIR-3 English corpus, which 
contains 22,927 documents.  The statistics about the unknown terms for short (i.e., the 
title field only), medium (i.e., the description field only), and long (i.e., the descrip-
tion and the narrative fields) queries are summarized below.  The total number of 
unknown terms that we treated with context vectors was 83 (i.e., 6+15+62). 
 Short Medium Long 
No. of topics containing unknown terms 57 148 249 
Avg No. of terms in topics (total) 3.2 (16) 5.4 (75) 36.2 (86.9) 
Avg. No. of unknown terms (total) 1 (6) 1.1 (15) 2.610 (62) 
For evaluation, we used the mean average precision and recall for the top 1000 
documents and also precision@30, as defined in TREC retrieval evaluations. 
We compare three types of runs, both with and without post-translation pseudo-
relevance feedback.   
? Runs without context vectors (baselines) 
? Runs with query-dependent context vectors  
? Runs with query-independent context vectors 
5.2   Empirical Observations 
Tables 1-4 present the performance statistics for the above runs.  For the runs with 
translation disambiguation (Tables 1-2), using context vectors improved overall re-
call, average precision, and precision at 30 documents for short queries.  Context 
vectors moderately improved recall, average precision (except for the query inde-
pendent version), and precision at 30 documents for medium length queries. 
For the long queries, we do not observe any advantages of using either query-
dependent or query-independent versions of the context vectors.  This is probably 
because the other known terms in long queries provide adequate context for recover-
ing the loss of missing translation of the unknown terms.  Adding candidate transla-
tions from context vectors only makes the query more ambiguous and inexact. 
When all translations were kept (Tables 3-4), i.e., when no translation disambigua-
tion was performed, we only see overall improvement in recall for short and medium- 
length queries.  We do not see any advantage of using context vectors for improving 
average precision or precision at 30 documents.  For longer queries, the performance 
statistics were overall worse than the baseline.  As pointed out in [10], when all trans-
lations are kept without proper weighting of the translations, some terms get more 
favorable treatment than other terms simply because they contain more translations.  
So, in models where all translations are kept, proper weighting schemes should be 
developed, e.g., as suggested in related research [17]. 
                                                          
7
 Topics 4, 23, 26, 27, 33. 
8
 Topics 4, 5, 7, 13, 14, 20, 23, 26, 27, 28, 29, 31, 33, 38. 
9
 Topics 2, 4, 5, 7, 9, 13, 14, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 31, 33, 37, 38, 42, 43, 50. 
10
 The average number of unique unknown terms is 1.4. 
30 Y. Qu, G. Grefenstette, and D.A. Evans 
Table 1.  Performance statistics for short, medium, and long queries.  Translations were disam-
biguated; no feedback was used. Percentages show change over the baseline runs. 
No Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 28/112 0.1181 0.05 
With context vectors 
(query independent) 
43/112 
(+53.6%) 
0.1295 
(+9.7%) 
0.0667 
(+33.4%) 
With context vectors  
(query dependent) 
43/112 
(+53.6%) 
0.1573 
(+33.2%) 
0.0667 
(+33.4) 
Medium 
Baseline 113/248 0.1753 0.1231 
With context vectors 
(query independent) 
114/248 
(+0.9%) 
0.1588 
(-9.5%) 
0.1256 
(+2.0%) 
With context vectors  
(query dependent) 
115/248 
(+1.8%) 
0.1838 
(+4.8%) 
0.1282 
(+4.1%) 
Long 
Baseline 305/598 0.1901 0.1264 
With context vectors 
(query independent) 
308/598 
(+1.0%) 
0.1964 
(+3.3%) 
0.1125 
(-11.0%) 
With context vectors  
(query dependent) 
298/598 
(-2.3%) 
0.1883 
(-0.9%) 
0.1139 
(-9.9%) 
Table 2. Performance statistics for short, medium, and long queries.  Translations were 
disambiguated; for pseudo-relevance feedback, the top 30 terms from top 20 subdocuments 
were selected based on the Rocchio formula.  Percentages show change over the baseline runs. 
With Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 15/112 0.1863 0.0417 
With context vectors 
(query independent) 
40/112 
(+166.7%) 
0.1812 
(-2.7%) 
0.0417 
(+0.0%) 
With context vectors  
(query dependent) 
40/112 
(+166.7%) 
0.1942 
(+4.2%) 
0.0417 
(+0.0%) 
Medium 
Baseline 139/248 0.286 0.1513 
With context vectors 
(query independent) 
137 
(-1.4%) 
0.2942 
(+2.9%) 
0.1538 
(+1.7%) 
With context vectors  
(query dependent) 
141 
(+1.4%) 
0.3173 
(+10.9%) 
0.159 
(+5.1%) 
Long 
Baseline 341/598 0.2575 0.1681 
With context vectors 
(query independent) 
347/598 
(+1.8%) 
0.2598 
(+0.9%) 
0.1681 
(+0.0%) 
With context vectors 
(query dependent) 
340/598 
(-0.3%) 
0.2567 
(-0.3%) 
0.1639 
(-2.5%) 
 The Use of Monolingual Context Vectors for Missing Translations 31 
Table 3. Performance statistics for short, medium, and long queries.  All translations were kept 
for retrieval; pseudo-relevance feedback was not used.  Percentages show change over the 
baseline runs. 
No Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 33/112 0.1032 0.0417 
With context vectors 
(query independent) 
57/112 
(+72.7%) 
0.0465 
(-54.9%) 
0.05 
(+19.9%) 
With context vectors 
(query dependent) 
41/112 
(+24.2%) 
0.1045 
(-0.2%) 
0.0417 
(+0%) 
Medium 
Baseline 113/248 0.1838 0.0846 
With context vectors 
(query independent) 
136/248 
(+20.4%) 
0.1616 
(-12.1%) 
0.0769 
(-9.1%) 
With context vectors 
(query dependent) 
122/248 
(+8.0%) 
0.2013 
(+9.5%) 
0.0769 
(-9.1%) 
Long 
Baseline 283 0.1779 0.0944 
With context vectors 
(query independent) 
295/598 
(+4.2%) 
0.163 
(-8.4%) 
0.0917 
(-2.9%) 
With context vectors 
(query dependent) 
278/598 
(-1.8%) 
0.1566 
(-12.0%) 
0.0931 
(-1.4%) 
Table 4. Performance statistics for short, medium, and long queries.  All translations were kept 
for retrieval; for pseudo-relevance feedback, the top 30 terms from top 20 subdocuments were 
selected base on the Rocchio formula.  Percentages show change over the baseline runs. 
With Feedback Recall Avg. Precision Prec@30 
Short 
Baseline 40/112 0.1733 0.0417 
With context vectors 
(query independent) 
69/112 
(+72.5%) 
0.1662 
(-4.1%) 
0.1583 
(+279.6%) 
With context vectors 
(query dependent) 
44/112 
(+10.0%) 
0.1726 
(-0.4%) 
0.0417 
(+0.0%) 
Medium 
Baseline 135/248 0.2344 0.1256 
With context vectors 
(query independent) 
161/248 
(+19.3%) 
0.2332 
(-0.5%) 
0.1333 
(+6.1%) 
With context vectors 
(query dependent) 
139/248 
(+3.0%) 
0.2637 
(+12.5%) 
0.1154 
(-8.1%) 
Long 
Baseline 344/598 0.2469 0.1444 
With context vectors 
(query independent) 
348/598 
(+1.2%) 
0.2336 
(-5.4%) 
0.1333 
(-7.7%) 
With context vectors  
(query dependent) 
319/598 
(-7.3%) 
0.2033 
(-17.7%) 
0.1167 
(-19.2%) 
32 Y. Qu, G. Grefenstette, and D.A. Evans 
6   Summary and Future Work 
We have used context vectors to obtain surrogate translations for terms that appear in 
queries but that are absent from bilingual dictionaries.  We have described two types 
of context vectors: a query-independent version and a query-dependent version.  In 
the empirical evaluation, we have examined the interaction between the use of context 
vectors with other factors such as translation disambiguation, pseudo-relevance feed-
back, and query lengths.  The empirical findings suggest that using query-dependent 
context vectors together with post-translation pseudo-relevance feedback and transla-
tion disambiguation can help to overcome the meaning loss due to missing transla-
tions for short queries.  For longer queries, the longer context in the query seems to 
make the use of context vectors unnecessary. 
The paper presents only our first set on experiments of using context to recover 
meaning loss due to missing translations.  In our future work, we will verify the ob-
servations with other topic sets and database sources; verify the observations with 
other language pairs, e.g., Chinese-to-English retrieval; and experiment with different 
parameter settings such as context window size, methods for context term selection, 
different ways of ranking context terms, and the use of the context term ranking in 
combination with disambiguation for translation selection. 
References 
1. Ballesteros, L., and Croft, B.:  Dictionary Methods for Cross-Language Information Re-
trieval.  In Proceedings of Database and Expert Systems Applications (1996) 791?801. 
2. Ballesteros, L., Croft, W. B.:  Resolving Ambiguity for Cross-Language Retrieval.  In 
Proceedings of SIGIR (1998) 64?71. 
3. Billhardt, H., Borrajo, D., Maojo, V.:  A Context Vector Model for Information Retrieval.  
Journal of the American Society for Information Science and Technology, 53(3) (2002) 
236?249. 
4. Evans, D. A., Lefferts, R. G.:  CLARIT?TREC Experiments. Information Processing and 
Management, 31(3) (1995) 385?395. 
5. Fujii, A., Ishikawa, T.:  Japanese/English Cross-Language Information Retrieval: Explora-
tion of Query Translation and Transliteration.  Computer and the Humanities, 35(4) (2001) 
389?420. 
6. Fung, P.:  A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora to 
Non-parallel Corpora.  In Proceedings of AMTA (1998) 1?17. 
7. Fung, P., Yee, L. Y.:  An IR Approach for Translating New Words from Nonparallel, 
Comparable Texts.  In Proceedings of COLING-ACL (1998) 414?420. 
8. Hull, D. A., Grefenstette, G.: Experiments in Multilingual Information Retrieval. In Pro-
ceedings of the 19th Annual International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (1996) 49?57. 
9. Grefenstette, G.: Evaluating the Adequacy of a Multilingual Transfer Dictionary for Cross 
Language Information Retrieval.  In Proceedings of LREC (1998) 755?758. 
10. Grefenstette, G.:  The Problem of Cross Language Information Retrieval. In G. Grefen-
stette, ed., Cross Language Information Retrieval, Kluwer Academic Publishers (1998)  
1?9. 
 The Use of Monolingual Context Vectors for Missing Translations 33 
11. Grefenstette, G., Qu, Y., Evans, D. A.:  Mining the Web to Create a Language Model for 
Mapping between English Names and Phrases and Japanese.  In Proceedings of the 2004 
IEEE/WIC/ACM International Conference on Web Intelligence (2004) 110?116. 
12. Ido, D., Church, K., Gale, W. A.:  Robust Bilingual Word Alignment for Machine Aided 
Translation.  In Proceedings of the Workshop on Very Large Corpora: Academic and In-
dustrial Perspectives (1993) 1?8. 
13. Jeong, K. S., Myaeng, S, Lee, J. S., Choi, K. S.:  Automatic Identification and Back-
transliteration of Foreign Words for Information Retrieval. Information Processing and 
Management, 35(4) (1999) 523?540. 
14. Knight, K, Graehl, J.:  Machine Transliteration. Computational Linguistics: 24(4) (1998) 
599?612. 
15. Kumano, A., Hirakawa, H.:  Building an MT dictionary from Parallel Texts Based on Lin-
guistic and Statistical Information.  In Proceedings of the 15th International Conference on 
Computational Linguistics (COLING) (1994) 76?81. 
16. Meng, H., Lo, W., Chen, B., Tang, K.: Generating Phonetic Cognates to Handel Named 
Entities in English-Chinese Cross-Language Spoken Document Retrieval.  In Proc of the 
Automatic Speech Recognition and Understanding Workshop (ASRU 2001) (2001). 
17. Pirkola, A., Puolamaki, D., Jarvelin, K.: Applying Query Structuring in Cross-Language 
Retrieval.  Information Management and Processing: An International Journal.  Vol 39 (3) 
(2003) 391?402. 
18. Qu, Y., Grefenstette, G.:  Finding Ideographic Representations of Japanese Names in Latin 
Scripts via Language Identification and Corpus Validation.  In Proceedings of the 42nd 
Annual Meeting of the Association for Computational Linguistics (2004) 183?190. 
19. Qu, Y., Grefenstette, G., Evans, D. A.: Resolving Translation Ambiguity Using Monolin-
gual Corpora. In Peters, C., Braschler, M., Gonzalo, J. (eds): Advances in Cross-Language 
Information Retrieval: Third Workshop of the Cross-Language Evaluation Forum, CLEF 
2002, Rome, Italy, September 19?20, 2002. Lecture Notes in Computer Science, Vol 
2785. Springer (2003) 223?241. 
20. Qu, Y., Grefenstette, G., Evans, D. A:  Automatic Transliteration for Japanese-to-English 
Text Retrieval.  In Proceedings of the 26th Annual International ACM SIGIR Conference 
on Research and Development in Information Retrieval (2003) 353?360. 
21. Qu, Y., Hull, D. A., Grefenstette, G., Evans, D. A., Ishikawa, M., Nara, S., Ueda, T., 
Noda, D., Arita, K., Funakoshi, Y., Matsuda, H.:  Towards Effective Strategies for Mono-
lingual and Bilingual Information Retrieval: Lessons Learned from NTCIR-4.  ACM 
Transactions on Asian Language Information Processing.  (to appear) 
22. Zhang, Y., Vines, P:  Using the web for automated translation extraction in cross-language 
information retrieval.  In Proceedings of the 27th Annual International ACM SIGIR Con-
ference on Research and Development in Information Retrieval (2004) 162?169. 
 
Finding Ideographic Representations of Japanese Names Written in Latin 
Script via Language Identification and Corpus Validation 
Yan Qu 
Clairvoyance Corporation 
5001 Baum Boulevard, Suite 700 
Pittsburgh, PA 15213-1854, USA 
yqu@clairvoyancecorp.com 
Gregory Grefenstette? 
LIC2M/LIST/CEA 
18, route du Panorama, BP 6 
Fontenay-aux-Roses, 92265 France 
Gregory.Grefenstette@cea.fr 
 
Abstract 
Multilingual applications frequently involve 
dealing with proper names, but names are 
often missing in bilingual lexicons. This 
problem is exacerbated for applications 
involving translation between Latin-scripted 
languages and Asian languages such as 
Chinese, Japanese and Korean (CJK) where 
simple string copying is not a solution. We 
present a novel approach for generating the 
ideographic representations of a CJK name 
written in a Latin script.  The proposed 
approach involves first identifying the origin 
of the name, and then back-transliterating the 
name to all possible Chinese characters using 
language-specific mappings.  To reduce the 
massive number of possibilities for 
computation, we apply a three-tier filtering 
process by filtering first through a set of 
attested bigrams, then through a set of attested 
terms, and lastly through the WWW for a final 
validation.  We illustrate the approach with 
English-to-Japanese back-transliteration.  
Against test sets of Japanese given names and 
surnames, we have achieved average 
precisions of 73% and 90%, respectively. 
1 Introduction 
Multilingual processing in the real world often 
involves dealing with proper names. Translations 
of names, however, are often missing in bilingual 
resources.  This absence adversely affects 
multilingual applications such as machine 
translation (MT) or cross language information 
retrieval (CLIR) for which names are generally 
good discriminating terms for high IR performance 
(Lin et al, 2003).  For language pairs with 
different writing systems, such as Japanese and 
English, and for which simple string-copying of a 
name from one language to another is not a 
solution, researchers have studied techniques for 
transliteration, i.e., phonetic translation across 
languages.  For example, European names are 
often transcribed in Japanese using the syllabic  
katakana alphabet.  Knight and Graehl (1998) used 
a bilingual English-katakana dictionary, a 
katakana-to-English phoneme mapping, and the 
CMU Speech Pronunciation Dictionary to create a 
series of weighted finite-state transducers between 
English words and katakana that produce and rank 
transliteration candidates. Using similar methods, 
Qu et al (2003) showed that integrating 
automatically discovered transliterations of 
unknown katakana sequences, i.e. those not 
included in a large Japanese-English dictionary 
such as EDICT1, improves CLIR results. 
Transliteration of names between alphabetic and 
syllabic scripts has also been studied for languages 
such as Japanese/English (Fujii & Ishikawa, 2001), 
English/Korean (Jeong et al, 1999), and 
English/Arabic (Al-Onaizan and Knight, 2002). 
In work closest to ours, Meng et al(2001), 
working in cross-language retrieval of phonetically 
transcribed spoken text, studied how to 
transliterate names into Chinese phonemes (though 
not into Chinese characters).  Given a list of 
identified names, Meng et al first separated the 
names into Chinese names and English names. 
Romanized Chinese names were detected by a left-
to-right longest match segmentation method, using 
the Wade-Giles2 and the pinyin syllable inventories 
in sequence.  If a name could be segmented 
successfully, then the name was considered a 
Chinese name.  As their spoken document 
collection had already been transcribed into pinyin, 
retrieval was based on pinyin-to-pinyin matching; 
pinyin to Chinese character conversion was not 
addressed.  Names other than Chinese names were 
considered as foreign names and were converted 
into Chinese phonemes using a language model 
derived from a list of English-Chinese equivalents, 
both sides of which were represented in phonetic 
equivalents. 
                                                    
? The work was done by the author while at 
Clairvoyance Corporation. 
1
 http://www.csse.monash.edu.au/~jwb/edict.html 
2
 http://lcweb.loc.gov/catdir/pinyin/romcover.html 
The above English-to-Japanese or English-to-
Chinese transliteration techniques, however, only 
solve a part of the name translation problem. In 
multilingual applications such as CLIR and 
Machine Translation, all types of names must be 
translated. Techniques for name translation from 
Latin scripts into CJK scripts often depend on the 
origin of the name. Some names are not 
transliterated into a nearly deterministic syllabic 
script but into ideograms that can be associated 
with a variety of pronunciations. For example, 
Chinese, Korean and Japanese names are usually 
written using Chinese characters (or kanji) in 
Japanese, while European names are transcribed 
using katakana characters, with each character 
mostly representing one syllable. 
In this paper, we describe a method for 
converting a Japanese name written with a Latin 
alphabet (or romanji), back into Japanese kanji3. 
Transcribing into Japanese kanji is harder than 
transliteration of a foreign name into syllabic 
katakana, since one phoneme can correspond to 
hundreds of possible kanji characters. For example, 
the sound ?kou? can be mapped to 670 kanji 
characters. 
Our method for back-transliterating Japanese 
names from English into Japanese consists of the 
following steps: (1) language identification of the 
origins of names in order to know what language-
specific transliteration approaches to use, (2) 
generation of possible transliterations using sound 
and kanji mappings from the Unihan database (to 
be described in section 3.1) and then transliteration 
validation through a three-tier filtering process by 
filtering first through a set of attested bigrams, then 
through a set of attested terms, and lastly through 
the Web. 
The rest of the paper is organized as follows: in 
section 2, we describe and evaluate our name 
origin identifier; section 3 presents in detail the 
steps for back transliterating Japanese names 
written in Latin script into Japanese kanji 
representations; section 4 presents the evaluation 
setup and section 5 discusses the evaluation 
results; we conclude the paper in section 6. 
2 Language Identification of Names 
Given a name in English for which we do not 
have a translation in a bilingual English-Japanese 
dictionary, we first have to decide whether the 
name is of Japanese, Chinese, Korean or some 
European origin.  In order to determine the origin 
of names, we created a language identifier for 
names, using a trigram language identification 
                                                    
3
 We have applied the same technique to Chinese and 
Korean names, though the details are not presented here. 
method (Cavner and Trenkle, 1994).  During 
training, for Chinese names, we used a list of 
11,416 Chinese names together with their 
frequency information4.  For Japanese names, we 
used the list of 83,295 Japanese names found in 
ENAMDICT5.  For English names, we used the list 
of 88,000 names found at the US. Census site6 .  
(We did not obtain any training data for Korean 
names, so origin identification for Korean names is 
not available.)  Each list of names7 was converted 
into trigrams; the trigrams for each list were then 
counted and normalized by dividing the count of 
the trigram by the number of all the trigrams. To 
identify a name as Chinese, Japanese or English 
(Other, actually), we divide the name into trigrams, 
and sum up the normalized trigram counts from 
each language.  A name is identified with the 
language which provides the maximum sum of 
normalized trigrams in the word. Table 1 presents 
the results of this simple trigram-based language 
identifier over the list of names used for training 
the trigrams. 
The following are examples of identification 
errors: Japanese names recognized as English, e.g., 
aa, abason, abire, aebakouson; Japanese names 
recognized as Chinese, e.g., abeseimei, abei, adan, 
aden, afun, agei, agoin.  These errors show that the 
language identifier can be improved, possibly by 
taking into account language-specific features, 
such as the number of syllables in a name.  For 
origin detection of Japanese names, the current 
method works well enough for a first pass with an 
accuracy of 92%. 
Input 
 names 
As 
JAP 
As 
CHI 
As 
ENG 
Accuracy 
Japanese 76816 5265 1212 92% 
Chinese 1147 9947 321 87% 
English 12115 14893 61701 70% 
Table 1: Accuracy of language origin 
identification for names in the training set (JAP, 
CHI, and ENG stand for Japanese, Chinese, and 
English, respectively) 
                                                    
4
 http://www.geocities.com/hao510/namelist/ 
5
 http://www.csse.monash.edu.au/~jwb/ 
enamdict_doc.html 
6
 http://www.census.gov/genealogy/names 
7
 Some names appear in multiple name lists: 452 of the 
names are found both in the Japanese name list and in 
the Chinese name list; 1529 names appear in the 
Japanese name list and the US Census name list; and 
379 names are found both in the Chinese name list and 
the US Census list. 
3 English-Japanese Back-Transliteration 
Once the origin of a name in Latin scripts is 
identified, we apply language-specific rules for 
back-transliteration.  For non-Asian names, we use 
a katakana transliteration method as described in 
(Qu et al, 2003).  For Japanese and Chinese 
names, we use the method described below. For 
example, ?koizumi? is identified as a name of 
Japanese origin and thus is back-transliterated to 
Japanese using Japanese specific phonetic 
mappings between romanji and kanji characters. 
3.1 Romanji-Kanji Mapping 
To obtain the mappings between kanji characters 
and their romanji representations, we used the 
Unihan database, prepared by the Unicode 
Consortium 8 .  The Unihan database, which 
currently contains 54,728 kanji characters found in 
Chinese, Japanese, and Korean, provides rich 
information about these kanji characters, such as 
the definition of the character, its values in 
different encoding systems, and the 
pronunciation(s) of the character in Chinese (listed 
under the feature kMandarin in the Unihan 
database), in Japanese (both the On reading and the 
Kun reading 9 : kJapaneseKun and 
kJapaneseOn), and in Korean (kKorean).  For 
example, for the kanji character   , coded with 
Unicode hexadecimal character 91D1, the Unihan 
database lists 49 features; we list below its 
pronunciations in Japanese, Chinese, and Korean: 
U+91D1  kJapaneseKun    KANE 
U+91D1  kJapaneseOn     KIN KON 
U+91D1  kKorean KIM KUM 
U+91D1  kMandarin       JIN1 JIN4 
In the example above, 
 
 is represented in its 
Unicode scalar value in the first column, with a 
feature name in the second column and the values 
of the feature in the third column.  The Japanese 
Kun reading of 
 
 is KANE, while the Japanese On 
readings of 
 
 is KIN and KON. 
From the Unicode database, we construct 
mappings between Japanese readings of a character 
in romanji and the kanji characters in its Unicode 
representation.  As kanji characters in Japanese 
names can have either the Kun reading or the On 
                                                    
8
 http://www.unicode.org/charts/unihan.html 
9
 Historically, when kanji characters were introduced 
into the Japanese writing system, two methods of 
transcription were used.  One is called ?on-yomi? (i.e., 
On reading), where the Chinese sounds of the characters 
were adopted for Japanese words.  The other method is 
called ?kun-yomi? (i.e., Kun reading), where a kanji 
character preserved its meaning in Chinese, but was 
pronounced using the Japanese sounds. 
reading, we consider both readings as candidates 
for each kanji character.  The mapping table has a 
total of 5,525 entries.  A typical mapping is as 
follows: 
kou U+4EC0 U+5341 U+554F U+5A09 
U+5B58 U+7C50 U+7C58 ...... 
in which the first field specifies a pronunciation in 
romanji, while the rest of the fields specifies the 
possible kanji characters into which the 
pronunciation can be mapped. 
There is a wide variation in the distribution of 
these mappings.  For example, kou can be the 
pronunciation of 670 kanji characters, while the 
sound katakumi can be mapped to only one kanji 
character. 
3.2 Romanji Name Back-Transliteration 
In theory, once we have the mappings between 
romanji characters and the kanji characters, we can 
first segment a Japanese name written in romanji 
and then apply the mappings to back-transliterate 
the romanji characters into all possible kanji 
representations.  However, for some segmentation, 
the number of the possible kanji combinations can 
be so large as to make the problem 
computationally intractable.  For example, 
consider the short Japanese name ?koizumi.? This 
name can be segmented into the romanji characters 
?ko-i-zu-mi? using the Romanji-Kanji mapping 
table described in section 3.1, but this 
segmentation then has 182*230*73*49 (over 149 
million) possible kanji combinations.  Here, 182, 
239, 73, and 49 represents the numbers of possible 
kanji characters for the romanji characters ?ko?, 
?i?, ?zu?, and ?mi?, respectively. 
In this study, we present an efficient procedure 
for back-transliterating romanji names to kanji 
characters that avoids this complexity.  The 
procedure consists of the following steps: (1) 
romanji name segmentation, (2) kanji name 
generation, (3) kanji name filtering via 
monolingual Japanese corpus, and (4) kanji-
romanji combination filtering via WWW.  Our 
procedure relies on filtering using corpus statistics 
to reduce the hypothesis space in the last three 
steps.  We illustrate the steps below using the 
romanji name ?koizumi? (  . 
3.2.1 Romanji Name Segmentation 
With the romanji characters from the Romanji-
Kanji mapping table, we first segment a name 
recognized as Japanese into sequences of romanji 
characters.  Note that a greedy segmentation 
method, such as the left-to-right longest match 
method, often results in segmentation errors.  For 
example, for ?koizumi?, the longest match 
segmentation method produces segmentation ?koi-
zu-mi?, while the correct segmentation is ?ko-
izumi?. 
Motivated by this observation, we generate all 
the possible segmentations for a given name.  The 
possible segmentations for ?koizumi? are: 
ko-izumi 
koi-zu-mi 
ko-i-zu-mi 
3.2.2 Kanji Name Segmentation 
Using the same Romanji-Kanji mapping table, 
we obtain the possible kanji combinations for a 
segmentation of a romanji name produced by the 
previous step.  For the segmentation ?ko-izumi?, 
we have a total of 546 (182*3) combinations (we 
use the Unicode scale value to represent the kanji 
characters and use spaces to separate them): 
U+5C0F U+6CC9 
U+53E4 U+6CC9 
 ...... 
We do not produce all possible combinations. As 
we have discussed earlier, such a generation 
method can produce so many combinations as to 
make computation infeasible for longer 
segmentations.  To control this explosion, we 
eliminate unattested combinations using a bigram 
model of the possible kanji sequences in Japanese. 
From the Japanese evaluation corpus of the 
NTCIR-4 CLIR track 10 , we collected bigram 
statistics by first using a statistical part-of-speech 
tagger of Japanese (Qu et al, 2004).  All valid 
Japanese terms and their frequencies from the 
tagger output were extracted.  From this term list, 
we generated kanji bigram statistics (as well as an 
attested term list used below in step 3). With this 
bigram-based model, our hypothesis space is 
significantly reduced.  For example, with the 
segmentation ?ko-i-zu-mi?, even though ?ko-i? can 
have 182*230 possible combinations, we only 
retain the 42 kanji combinations that are attested in 
the corpus. 
Continuing with the romanji segments ?i-zu?, we 
generate the possible kanji combinations for ?i-zu? 
that can continue one of the 42 candidates for ?ko-
i?.  This results in only 6 candidates for the 
segments ?ko-i-zu?. 
Lastly, we consider the romanji segments ?zu-
mi?, and retain with only 4 candidates for the 
segmentation ?ko-i-zu-mi? whose bigram 
sequences are attested in our language model: 
U+5C0F U+53F0 U+982D U+8EAB 
U+5B50 U+610F U+56F3 U+5B50 
U+5C0F U+610F U+56F3 U+5B50 
U+6545 U+610F U+56F3 U+5B50 
                                                    
10
 http://research.nii.ac.jp/ntcir-ws4/clir/index.html 
Thus, for the segmentation ?ko-i-zu-mi?, the 
bigram-based language model effectively reduces 
the hypothesis space from 182*230*73*49 
possible kanji combinations to 4 candidates.  For 
the other alternative segmentation ?koi-zu-mi?, no 
candidates can be generated by the language 
model. 
3.2.3 Corpus-based Kanji name Filtering 
In this step, we use a monolingual Japanese 
corpus to validate whether the kanji name 
candidates generated by step (2) are attested in the 
corpus.  Here, we simply use Japanese term list 
extracted from the segmented NTCIR-4 corpus 
created for the previous step to filter out unattested 
kanji combinations.  For the segmentation ?ko-
izumi?, the following kanji combinations are 
attested in the corpus (preceded by their frequency 
in the corpus): 
4167    koizumi 
16   koizumi 
4   koizumi 
None of the four kanji candidates from the 
alternate segmentation ?ko-i-zu-mi? is attested in 
the corpus.  While step 2 filters out candidates 
using bigram sequences, step 3 uses corpus terms 
in their entirety to validate candidates. 
3.2.4 Romanji-Kanji Combination Validation 
Here, we take the corpus-validated kanji 
candidates (but for which we are not yet sure if 
they correspond to the same reading as the original 
Japanese name written in romanji) and use the 
Web to validate the pairings of kanji-romanji 
combinations (e.g.,    AND koizumi).  This is 
motivated by two observations.  First, in contrast to 
monolingual corpus, Web pages are often mixed-
lingual. It is often possible to find a word and its 
translation on the same Web pages. Second, person 
names and specialized terminology are among the 
most frequent mixed-lingual items.  Thus, we 
would expect that the appearance of both 
representations in close proximity on the same 
pages gives us more confidence in the kanji 
representations.  For example, with the Google 
search engine, all three kanji-romanji combinations 
for ?koizumi? are attested: 
23,600 pages --     koizumi 
302 pages --    koizumi 
1 page --   koizumi 
Among the three, the    koizumi combination 
is the most common one, being the name of the 
current Japanese Prime Minister. 
4 Evaluation 
In this section, we describe the gold standards 
and evaluation measures for evaluating the 
effectiveness of the above method for back-
transliterating Japanese names. 
4.1 Gold Standards 
Based on two publicly accessible name lists and 
a Japanese-to-English name lexicon, we have 
constructed two Gold Standards.  The Japanese-to-
English name lexicon is ENAMDICT 11 , which 
contains more than 210,000 Japanese-English 
name translation pairs. 
Gold Standard ? Given Names (GS-GN): to 
construct a gold standard for Japanese given 
names, we obtained 7,151 baby names in romanji 
from http://www.kabalarians.com/.  Of these 7,151 
names, 5,115 names have kanji translations in the 
ENAMDICT12.  We took the 5115 romanji names 
and their kanji translations in the ENAMDICT as 
the gold standard for given names. 
Gold Standard ? Surnames (GS-SN): to 
construct a gold standard for Japanese surnames, 
we downloaded 972 surnames in romanji from 
http://business.baylor.edu/Phil_VanAuken/Japanes
eSurnames.html.  Of these names, 811 names have 
kanji translations in the ENAMDICT.  We took 
these 811 romanji surnames and their kanji 
translations in the ENAMDICT as the gold 
standard for Japanese surnames. 
4.2 Evaluation Measures 
Each name in romanji in the gold standards has 
at least one kanji representation obtained from the 
ENAMDICT.  For each name, precision, recall, 
and F measures are calculated as follows: 
? Precision: number of correct kanji output / 
total number of kanji output 
? Recall: number of correct kanji output / total 
number of kanji names in gold standard 
? F-measure: 2*Precision*Recall / (Precision + 
Recall) 
Average Precision, Average Recall, and Average 
F-measure are computed over all the names in the 
test sets. 
5 Evaluation Results and Analysis 
5.1 Effectiveness of Corpus Validation 
Table 2 and Table 3 present the precision, recall, 
and F statistics for the gold standards GS-GN and 
                                                    
11
 http://mirrors.nihongo.org/monash/ 
enamdict_doc.html 
12
 The fact that above 2000 of these names were 
missing from ENAMDICT is a further justification for a 
name translation method as described in this paper. 
GS-SN, respectively.  For given names, corpus 
validation produces the best average precision of 
0.45, while the best average recall is a low 0.27.  
With the additional step of Web validation of the 
romanji-kanji combinations, the average precision 
increased by 62.2% to 0.73, while the best average 
recall improved by 7.4% to 0.29.  We observe a 
similar trend for surnames.  The results 
demonstrate that, through a large, mixed-lingual 
corpus such as the Web, we can improve both 
precision and recall for automatically 
transliterating romanji names back to kanji. 
 Avg 
Prec 
Avg 
Recall 
F 
(1) Corpus 0.45 0.27 0.33 
(2) Web 
(over (1)) 
0.73 
(+62.2%) 
0.29 
(+7.4%) 
0.38 
(+15.2%) 
Table 2: The best Avg Precision, Avg Recall, 
and Avg F statistics achieved through corpus 
validation and Web validation for GS-GN. 
 Avg 
Prec 
Avg 
Recall 
F 
(1) Corpus 0.69 0.44 0.51 
(2) Web 
(over (1)) 
0.90 
(+23.3%) 
0.45 
(+2.3%) 
0.56 
(+9.8%) 
Table 3: The best Avg Precision, Avg Recall, 
and Avg F statistics achieved through corpus 
validation and Web validation for GS-SN. 
We also observe that the performance statistics 
for the surnames are significantly higher than those 
of the given names, which might reflect the 
different degrees of flexibility in using surnames 
and given names in Japanese.  We would expect 
that the surnames form a somewhat closed set, 
while the given names belong to a more open set.  
This may account for the higher recall for 
surnames. 
5.2 Effectiveness of Corpus Validation 
If the big, mixed-lingual Web can deliver better 
validation than the limited-sized monolingual 
corpus, why not use it at every stage of filtering? 
Technically, we could use the Web as the ultimate 
corpus for validation at any stage when a corpus is 
required.  In practice, however, each Web access 
involves additional computation time for file IO, 
network connections, etc.  For example, accessing 
Google took about 2 seconds per name13; gathering 
                                                    
13
 We inserted a 1 second sleep between calls to the 
search engine so as not to overload the engine. 
statistics for about 30,000 kanji-romanji 
combinations14 took us around 15 hours. 
In the procedure described in section 3.2, we 
have aimed to reduce computation complexity and 
time at several stages.  In step 2, we use bigram-
based language model from a corpus to reduce the 
hypothesis space.  In step 3, we use corpus filtering 
to obtain a fast validation of the candidates, before 
passing the output to the Web validation in step 4.  
Table 4 illustrates the savings achieved through 
these steps. 
 GS-GN GS-SN 
All possible 2.0e+017 296,761,622,763 
2gram model 21,306,322 
(-99.9%) 
2,486,598 
(-99.9%) 
Corpus 
validate 
30,457 
(-99.9%) 
3,298 
(-99.9%) 
Web validation 20,787 
(-31.7%) 
2,769 
(-16.0%) 
Table 4: The numbers of output candidates of 
each step to be passed to the next step.  The 
percentages specify the amount of reduction in 
hypothesis space. 
5.3 Thresholding Effects 
We have examined whether we should discard 
the validated candidates with low frequencies 
either from the corpus or the Web.  The cutoff 
points examined include initial low frequency 
range 1 to 10 and then from 10 up to 400 in with 
increments of 5.  Figure 1 and Figure 2 illustrate 
that, to achieve best overall performance, it is 
beneficial to discard candidates with very low 
frequencies, e.g., frequencies below 5.  Even 
though we observe a stabling trend after reaching 
certain threshold points for these validation 
methods, it is surprising to see that, for the corpus 
validation method with GS-GN, with stricter 
thresholds, average precisions are actually 
decreasing.  We are currently investigating this 
exception. 
5.4 Error Analysis 
Based on a preliminary error analysis, we have 
identified three areas for improvements. 
First, our current method does not account for 
certain phonological transformations when the 
On/Kun readings are concatenated together.  
Consider the name ?matsuda? (   ).  The 
segmentation step correctly segmented the romanji 
to ?matsu-da?.  However, in the Unihan database, 
                                                    
14
 At this rate, checking the 21 million combinations 
remaining after filtering with bigrams using the Web 
(without the corpus filtering step) would take more than 
a year.   
the Kun reading of   is ?ta?, while its On reading 
is ?den?.  Therefore, using the mappings from the 
Unihan database, we failed to obtain the mapping 
between the pronunciation ?da? and the kanji  , 
which resulted in both low precision and recall for 
?matsuda?.  This suggests for introducing 
language-specific phonological transformations or 
alternatively fuzzy matching to deal with the 
mismatch problem. 
 
Avg Precision - GS_GN
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 6 15 50 10
0
15
0
20
0
25
0
30
0
35
0
40
0
Threshold for frequency cutoff
Av
g 
Pr
ec
is
io
n
corpus+web corpus
 
Figure 1: Average precisions achieved via both 
corpus and corpus+Web validation with different 
frequency-based cutoff thresholds for GS-GN  
Avg Precision - GS_SN
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 6 15 50 10
0
15
0
20
0
25
0
30
0
35
0
40
0
Threshold for frequency cutoff
Av
g 
Pr
ec
is
io
n
corpus+web corpus
 
Figure 2: Average precisions achieved via both 
corpus and corpus+Web validation with different 
frequency-based cutoff thresholds for GS-SN 
Second, ENAMDICT contains mappings 
between kanji and romanji that are not available 
from the Unihan database.  For example, for the 
name ?hiroshi? in romanji, based on the mappings 
from the Unihan database, we can obtain two 
possible segmentations: ?hiro-shi? and ?hi-ro-shi?.  
Our method produces two- and three-kanji 
character sequences that correspond to these 
romanji characters.  For example, corpus validation 
produces the following kanji candidates for 
?hiroshi?: 
 
 
2    hiroshi 
10    hiroshi 
5    hiroshi 
1    hiroshi 
2 	
  hiroshi 
11 	  hiroshi 
33 	  hiroshi 
311   hiroshi 
ENAMDCIT, however, in addition to the 2- and 
3-character kanji names, also contains 1-character 
kanji names, whose mappings are not found in the 
Unihan database, e.g., 

Hiroshi 

Hiroshi 

Hiroshi 

Hiroshi 

Hiroshi 

Hiroshi 
This suggests the limitation of relying solely on 
the Unihan database for building mappings 
between romanji characters and kanji characters.  
Other mapping resources, such as ENAMDCIT, 
should be considered in our future work. 
Third, because the statistical part-of-speech 
tagger we used for Japanese term identification 
does not have a lexicon of all possible names in 
Japanese, some unknown names, which are 
incorrectly separated into individual kanji 
characters, are therefore not available for correct 
corpus-based validation.  We are currently 
exploring methods using overlapping character 
bigrams, instead of the tagger-produced terms, as 
the basis for corpus-based validation and filtering. 
6 Conclusions 
In this study, we have examined a solution to a 
previously little treated problem of transliterating 
CJK names written in Latin scripts back into their 
ideographic representations.  The solution involves 
first identifying the origins of the CJK names and 
then back-transliterating the names to their 
respective ideographic representations with 
language-specific sound-to-character mappings.  
We have demonstrated that a simple trigram-based 
language identifier can serve adequately for 
identifying names of Japanese origin.  During 
back-transliteration, the possibilities can be 
massive due to the large number of mappings 
between a Japanese sound and its kanji 
representations.  To reduce the complexity, we 
apply a three-tier filtering process which eliminates 
most incorrect candidates, while still achieving an 
F measure of 0.38 on a test set of given names, and 
an F measure of 0.56 on a test of surnames. The 
three filtering steps involve using a bigram model 
derived from a large segmented Japanese corpus, 
then using a list of attested corpus terms from the 
same corpus, and lastly using the whole Web as a 
corpus. The Web is used to validate the back-
transliterations using statistics of pages containing 
both the candidate kanji translation as well as the 
original romanji name. 
Based on the results of this study, our future 
work will involve testing the effectiveness of the 
current method in real CLIR applications, applying 
the method to other types of proper names and 
other language pairs, and exploring new methods 
for improving precision and recall for romanji 
name back-transliteration.  In cross-language 
applications such as English to Japanese retrieval, 
dealing with a romaji name that is missing in the 
bilingual lexicon should involve (1) identifying the 
origin of the name for selecting the appropriate 
language-specific mappings, and (2) automatically 
generating the back-transliterations of the name in 
the right orthographic representations (e.g., 
Katakana representations for foreign Latin-origin 
names or kanji representations for native Japanese 
names).  To further improve precision and recall, 
one promising technique is fuzzy matching (Meng 
et al 2001) for dealing with phonological 
transformations in name generation that are not 
considered in our current approach (e.g., 
?matsuda? vs ?matsuta?).  Lastly, we will explore 
whether the proposed romanji to kanji back-
transliteration approach applies to other types of 
names such as place names and study the 
effectiveness of the approach for back-
transliterating romanji names of Chinese origin and 
Korean origin to their respective kanji 
representations. 
References  
Yaser Al-Onaizan and Kevin Knight. 2002. 
Machine Transliteration of Names in Arabic 
Text. Proc. of ACL Workshop on Computational 
Approaches to Semitic Languages 
William B. Cavnar and John M. Trenkle. 1994. N-
gram based text categorization. In 3rd Annual 
Symposium on Document Analysis and 
Information Retrieval, 161-175 
Atsushi Fujii and Tetsuya Ishikawa. 2001.  
Japanese/English Cross-Language Information 
Retrieval: Exploration of Query Translation and 
Transliteration.  Computer and the Humanities,  
35( 4): 389?420 
K. S. Jeong, Sung-Hyon Myaeng, J. S. Lee, and K. 
S. Choi. 1999. Automatic identification and 
back-transliteration of foreign words for 
information retrieval. Information Processing 
and Management, 35(4): 523-540 
Kevin Knight and Jonathan Graehl. 1998.  
Machine Transliteration. Computational 
Linguistics: 24(4): 599-612 
Wen-Cheng Lin, Changhua Yang and Hsin-Hsi 
Chen. 2003. Foreign Name Backward 
Transliteration in Chinese-English Cross-
Language Image Retrieval, In Proceedings of 
CLEF 2003 Workshop, Trondheim, Norway. 
Helen Meng, Wai-Kit Lo, Berlin Chen, and Karen 
Tang. 2001.  Generating Phonetic Cognates to 
Handel Named Entities in English-Chinese 
Cross-Language Spoken Document Retrieval.  In 
Proc of the Automatic Speech Recognition and 
Understanding Workshop (ASRU 2001) Trento, 
Italy, Dec. 
Yan Qu, Gregory Grefenstette, David A. Evans. 
2003. Automatic transliteration for Japanese-to-
English text retrieval. In Proceedings of SIGIR 
2003: 353-360 
Yan Qu, Gregory Grefenstette, David A. Hull, 
David A. Evans, Toshiya Ueda, Tatsuo Kato, 
Daisuke Noda, Motoko Ishikawa, Setsuko Nara, 
and Kousaku Arita. 2004. Justsystem-
Clairvoyance CLIR Experiments at NTCIR-4 
Workshop.  In Proceedings of the NTCIR-4 
Workshop. 
 
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploring Semantic Constraints for Document Retrieval 
 
Hua Cheng, Yan Qu, Jesse Montgomery, David A. Evans 
Clairvoyance Corporation 
5001 Baum Blvd., Suite 700, Pittsburgh, PA 15213, U.S.A. 
{H.Cheng, Y.Qu, J.Montgomery, dae}@clairvoyancecorp.com 
 
  
 
Abstract 
In this paper, we explore the use of struc-
tured content as semantic constraints for 
enhancing the performance of traditional 
term-based document retrieval in special 
domains. First, we describe a method for 
automatic extraction of semantic content 
in the form of attribute-value (AV) pairs 
from natural language texts based on 
domain models constructed from a semi-
structured web resource. Then, we ex-
plore the effect of combining a state-of-
the-art term-based IR system and a sim-
ple constraint-based search system that 
uses the extracted AV pairs. Our evalua-
tion results have shown that such combi-
nation produces some improvement in IR 
performance over the term-based IR sys-
tem on our test collection. 
1 Introduction 
The questions of where and how sophisticated 
natural language processing techniques can im-
prove traditional term-based information re-
trieval have been explored for more than a dec-
ade. A considerable amount of work has been 
carried out that seeks to leverage semantic in-
formation for improving traditional IR. Early 
TREC systems such as INQUERY handled both 
natural language and semi-structured queries and 
tried to search for constraint expressions for 
country and time etc. in queries (Croft et al, 
1994). Later work, as discussed in (Strzalkowski 
et al, 1996), has focused on exploiting semantic 
information at the word level, including various 
attempts at word-sense disambiguation, e.g., 
(Voorhees, 1998), or the use of special-purpose 
terms; other approaches have looked at phrase-
level indexing or full-text query expansion. No 
approaches to date, however, have sought to em-
ploy semantic information beyond the word 
level, such as that expressed by attribute-value 
(AV) pairs, to improve term-based IR.  
Attribute-value pairs offer an abstraction for 
instances of many application domains. For ex-
ample, a person can be represented by a set of 
attributes such as name, date-of-birth, job title, 
and home address, and their associated values; a 
house has a different set of attributes such as ad-
dress, size, age and material; many product 
specifications can be mapped directly to AV 
pairs. AV pairs represent domain specific seman-
tic information for domain instances. 
Using AV pairs as semantic constraints for re-
trieval is related to some recent developments in 
areas such as Semantic Web retrieval, XML 
document retrieval, and the integration of IR and 
databases. In these areas, structured information 
is generally assumed. However, there is abundant 
and rich information that exists in unstructured 
text only. The goal of this work includes first to 
explore a method for automatically extracting 
structured information in the form of AV pairs 
from text, and then to utilize the AV pairs as se-
mantic constraints for enhancing traditional 
term-based IR systems. 
The paper is organized as follows. Section 2 
describes our method of adding AV annotations 
to text documents that utilizes a domain model 
automatically extracted from the Web. Section 3 
presents two IR systems using a vector space 
model and semantic constraints respectively, as 
well as a system that combines the two. Section 4 
describes the data set and topic set for evaluating 
the IR systems. In Section 5, we compare the 
performance of the three IR systems, and draw 
initial conclusions on how NLP techniques can 
improve traditional IR in specific domains. 
2 Domain-Driven AV Extraction 
This section describes a method that automati-
cally discovers attribute-value structures from 
unstructured texts, the result of which is repre-
sented as texts annotated with semantic tags.   
33
We chose the digital camera domain to illus-
trate and evaluate the methodology described in 
this paper. We expect this method to be applica-
ble to all domains whose main features can be 
represented as a set of specifications.  
2.1 Construction of Domain Model 
A domain model (DM) specifies a terminology 
of concepts, attributes and values for describing 
objects in a domain. The relationships between 
the concepts in such a model can be heterogene-
ous (e.g., the link between two concepts can 
mean inheritance or containment).  In this work, 
a domain model is used for establishing a vo-
cabulary as well as for establishing the attribute-
value relationship between phrases.  
For the digital camera domain, we automati-
cally constructed a domain model from existing 
Web resources. Web sites such as epinions.com 
and dpreview.com generally present information 
about cameras in HTML tables generated from 
internal databases. By querying these databases 
and extracting table content from the dynamic 
web pages, we can automatically reconstruct the 
databases as domain models that could be used 
for NLP purposes. These models can optionally 
be organized hierarchically. Although domain 
models generated from different websites of the 
same domain are not exactly the same, they often 
share many common features.  
From the epinions.com product specifications 
for 1157 cameras, we extracted a nearly compre-
hensive domain model for digital cameras, con-
sisting of a set of attributes (or features) and their 
possible values. A portion of the model is repre-
sented as follows: 
{Digital Camera} 
    <Brand> <Price> <Lens> 
{Brand} 
    (57) Canon 
    (33) Nikon 
{Price} $ 
    (136) 100 - 200 
    (100) >= 400 
{Lens} 
    <Optical Zoom> <Focus Range> 
{Optical Zoom} x 
    (17) 4 
    (3) 2.5 
{Focus Range} in., ? 
    (2) 3.9 - infinity 
    (1) 12 - infinity 
In this example, attributes are shown in curly 
brackets and sub-attributes in angle brackets. 
Attributes are followed by possible units for their 
numerical values. Values come below the attrib-
utes, headed by their frequencies in all specifica-
tions. The frequency information (in parentheses) 
is used to calculate term weights of attributes and 
values. 
Specifications in HTML tables generally do 
not specify explicitly the type restrictions on val-
ues (even though the types are typically defined 
in the underlying databases).  As type restrictions 
contain important domain information that is 
useful for value extraction, we recover the type 
restrictions by identifying patterns in values. For 
example, attributes such as price or dimension 
usually have numerical values, which can be ei-
ther a single number (?$300?), a range (?$100 - 
$200?), or a multi-dimensional value (?4 in. x 3 
in. x 2 in.?), often accompanied by a unit, e.g., $ 
or inches, whereas attributes such as brand and 
accessory usually have string values, e.g., 
?Canon? or ?battery charger?.  
We manually compile a list of units for identi-
fying numerical values, which is partially do-
main general. We identify range and multi-
dimensional values using such patterns as ?A ? 
B?, ?A to B?, ?less than A?, and ?A x B?, etc. 
Numerical values are then normalized to a uni-
form format. 
2.2 Identification of AV Pairs 
Based on the constructed domain model, we can 
identify domain values in unstructured texts and 
assign attribute names and domains to them. We 
focus on extracting values of a domain attribute.  
Attribute names appearing by themselves are not 
of interest here because attribute names alone 
cannot establish attribute-value relations. How-
ever, identification of attribute names is neces-
sary for disambiguation. 
The AV extraction procedure contains the fol-
lowing steps: 
1. Use MINIPAR (Lin, 1998) to generate 
dependency parses of texts. 
2. For all noun phrase chunks in parses, it-
eratively match sub-phrases of each 
chunk with the domain model to find all 
possible matches of attribute names and 
values above a threshold: 
? A chunk contains all words up to 
the noun head (inclusive); 
? Post-head NP components (e.g., 
PP and clauses) are treated as 
separate chunks. 
3. Disambiguate values with multiple at-
tribute assignments using the sentence 
context, with a preference toward closer 
context based on dependency. 
34
4. Mark up the documents with XML tags 
that represent AV pairs. 
Steps 2 and 3 are the center of the AV extrac-
tion process, where different strategies are em-
ployed to handle values of different types and 
where ambiguous values are disambiguated. We 
describe these strategies in detail below. 
Numerical Value 
Numerical values are identified based on the unit 
list and the range and multi-dimensional number 
patterns described earlier in Section 2.1. The 
predefined mappings between units and attrib-
utes suggest attribute assignment. It is possible 
that one unit can be mapped to multiple attrib-
utes.  For example, ?x? can be mapped to either 
optical zoom or digital zoom, both of which are 
kept as possible candidates for future disam-
biguation. For range and multi-dimensional 
numbers, we find all attributes in the domain 
model that have at least one matched range or 
multi-dimensional value, and keep attributes 
identified by either a unit or a pattern as candi-
dates. Numbers without a unit can only be 
matched exactly against an existing value in the 
domain model. 
String Value 
Human users often refer to a domain entity in 
different ways in text. For example, a camera 
called ?Canon PowerShot G2 Black Digital 
Camera? in our domain model is seldom men-
tioned exactly this way in ads or reviews, but 
rather as ?Canon PowerShot G2?, ?Canon G2?, 
etc.  However, a domain model generally only 
records full name forms rather than their all pos-
sible variations. This makes the identification of 
domain values difficult and invalidates the use of 
a trained classifier that needs training samples 
consisting of a large variety of name references. 
An added difficulty is that web texts often 
contain grammatical errors and incomplete sen-
tences as well as large numbers of out-of-
vocabulary words and, therefore, make the de-
pendency parses very noisy. As a result, effec-
tiveness of extraction algorithms based on certain 
dependency patterns can be adversely affected.  
Our approach makes use of the more accurate 
parser functionalities of part-of-speech tagging 
and phrase boundary detection, while reducing 
the reliance on low level dependency structures. 
For noun phrase chunks extracted from parse 
trees, we iteratively match all sub-phrases of 
each chunk with the domain model to find 
matching attributes and values above a threshold. 
It is often possible to find multiple AV pairs in a 
single NP chunk. 
Assigning domain attributes to an NP is essen-
tially a classification problem. In our domain 
model, each attribute can be seen as a target class 
and its values as the training set. For a new 
phrase, the idea is to find the value in the domain 
model that is most similar and then assign the 
attribute of this nearest neighbor to the phrase. 
This motivates us to adopt K Nearest Neighbor 
(KNN) (Fix and Hodges, 1951) classification for 
handling NP values. The core of KNN is a simi-
larity metric. In our case, we use word editing 
distance (Wagner and Fischer, 1974) that takes 
into account the cost of word insertions, dele-
tions, and substitutions. We compute word edit-
ing distance using dynamic programming tech-
niques. 
Intuitively, words do not carry equal weights 
in a domain. In the earlier example, words such 
as ?PowerShot? and ?G2? are more important 
than ?digital? and ?camera?, so editing costs for 
such words should be higher. This draws an 
analogy to the metric of Inverse Document Fre-
quency (IDF) in the IR community, used to 
measure the discriminative capability of a term 
in a document collection. If we regard each value 
string as a document, we can use IDF to measure 
the weight of each term in a value string to em-
phasize important domain terms and de-
emphasize more general ones. The normalized 
cost is computed as: 
)log(/)/log( TNNTN  
where TN is the total number of values for an 
attribute, and N is the number of values where a 
term occurs. This equation assigns higher cost to 
more discriminative terms and lower cost to 
more general terms.  It is also used to compute 
costs of terms in attribute names.  For words not 
appearing in a class the cost is 1, the maximum 
cost. 
The distance between a new phrase and a DM 
phrase is then calculated using word editing cost 
based on the costs of substitution, insertion, and 
deletion, where 
Costsub = (CostDM + Costnew) / 2 
Costins = Costnew 
Costdel = CostDM 
Costedit = min(Costsub, Costins, Costdel) 
 
where CostDM is the cost of a word in a domain 
value (i.e., its normalized IDF score), and Costnew 
35
is that of a word in the new phrase. The cost is 
also normalized by the larger of the weighted 
lengths of the two phrases.  We use a threshold 
of 0.6 to cut off phrases with higher cost.  
For a phrase that returns only a couple of 
matches, the similarity, i.e., the matching prob-
ability, is computed as 1 - Costedit; otherwise, the 
similarity is the maximum likelihood of an at-
tribute based on the number of returned values 
belonging to this attribute. 
Disambiguation by Sentence Context 
The AV identification process often returns mul-
tiple attribute candidates for a phrase that needs 
to be further disambiguated. The words close to 
the phrase usually provide good indications of 
the correct attribute names. Motivated by this 
observation, we design the disambiguation pro-
cedure as follows. First we examine the sibling 
nodes of the target phrase node in the depend-
ency structure for a mention of an attribute name 
that overlaps with a candidate. Next, we recur-
sively traverse upwards along the dependency 
tree until we find an overlap or reach the top of 
the tree. If an overlap is found, that attribute be-
comes the final assignment; otherwise, the at-
tribute with the highest probability is assigned. 
This method gives priority to the context closest 
(in terms of dependency) to the target phrase. For 
example, in the sentence ?The 4x stepless digital 
zoom lets you capture intricate details? (parse 
tree shown below), ?4x? can be mapped to both 
optical zoom and digital zoom, but the sentence 
context points to the second candidate.  
 
3 Document Retrieval Systems 
This section introduces three document retrieval 
systems: the first one retrieves unstructured texts 
based on vector space models, the second one 
takes advantage of semantic structures con-
structed by the methods in Section 2, and the last 
one combines the first two systems. 
3.1 Term-Based Retrieval (S1) 
Our system for term-based retrieval from un-
structured text is based on the CLARIT system, 
implementing a vector space retrieval model (Ev-
ans and Lefferts, 1995; Qu et al, 2005). The 
CLARIT system identifies terms in documents 
and constructs its index based on NLP-
determined linguistic constituents (NPs, sub-
phrases and words). The index is built upon full 
documents or variable-length subdocuments. We 
used subdocuments in the range of 8 to 12 sen-
tences as the basis for indexing and scoring 
documents in our experiments. 
Various similarity measures are supported in 
the model. For the experiments described in the 
paper, we used the dot product function for com-
puting similarities between a query and a docu-
ment: 
 
where WQ(t) is the weight associated with the 
query term t and WD(t) is the weight associated 
with the term t in the document D. The two 
weights were computed as follows: 
 
 
where IDF and TF are standard inverse docu-
ment frequency and term frequency statistics, 
respectively. IDF(t) was computed with the tar-
get corpus for retrieval. The coefficient C(t) is an 
?importance coefficient?, which can be modified 
either manually by the user or automatically by 
the system (e.g., updated during feedback). 
For term-based document retrieval, we have 
also experimented with pseudo relevance feed-
back (PRF) with various numbers of retrieved 
documents and various numbers of terms from 
such documents for query expansion. While PRF 
did result in improvement in performance, it was 
not significant. This is probably due to the fact 
that in this restricted domain, there is not much 
vocabulary variation and thus the advantage of 
using query expansion is not fully realized. 
3.2 Constraint-Based Retrieval (S2) 
The constraint-based retrieval approach searches 
through the AV-annotated document collection 
based on the constraints extracted from queries. 
Given a query q, our constraint-based system 
scores each document in the collection by com-
paring the extracted AV pairs with the con-
straints in q. Suppose q has a constraint c(a, v) 
that restricts the value of the attribute a to v, 
where v can be either a concrete value (e.g., 5 
megapixels) or a range (e.g., less than $400). If a 
)()()()( tIDFtTFtCtW QQ ??=
).()()( tIDFtTFtW DD ?=
).()(),( tWtWDQsim D
DQt
Q ?= ?
??
36
is present in a document d with a value v? that 
satisfies v, that is, v?= v if v is a concrete value or 
v? falls in the range defined by v, d is given a 
positive score w. However, if v? does not satisfy 
v, then d is given a negative score -w. No men-
tion of a does not change the score of d, except 
that, when c is a string constraint, we use a back-
off model that awards d a positive score w if it 
contains v as a substring. The final score of d 
given q is the sum of all scores for each con-
straint in q, normalized by the maximum score 
for q: ?
=
n
i
iiwc
1
, where ci is one of the n con-
straints specified in q and wi its score. 
We rank the documents by their scores. This 
scoring schema facilitates a sensible cutoff point, 
so that a constraint-based retrieval system can 
return 0 or fewer than top N documents when a 
query has no or very few relevant documents.  
3.3 Combined Retrieval (S3) 
Lee (1997) analyzed multiple post-search data 
fusion methods using TREC3 ad hoc retrieval 
data and explained the combination of different 
search results on the grounds that different runs 
retrieve similar sets of relevant documents, but 
different sets of non-relevant documents. The 
combination methods therefore boost the ranks 
of the relevant documents. One method studied 
was the summation of individual similarities, 
which bears no significant difference from the 
best approach (i.e., further multiply the summa-
tion with the number of nonzero similarities). 
Our system therefore adopts the summation 
method for its simplicity. Because the scores 
from term-based and constraint-based retrieval 
are normalized, we simply add them together for 
each document retrieved by both approaches and 
re-rank the documents based on their new scores. 
More sophisticated combination methods can be 
explored here, such as deciding which score to 
emphasize based on the characterizations of the 
queries, e.g., whether a query has more numeri-
cal values or string values. 
4 Experimental Study 
In this section, we describe the experiments we 
performed to investigate combining terms and 
semantic constraints for document retrieval. 
4.1 Data Sets 
To construct a domain corpus, we used search 
results from craigslist.org.  We chose the ?for 
sale ? electronics? section for the ?San Francisco 
Bay Area?.  We then submitted the search term 
?digital camera? in order to retrieve advertise-
ments.  After manually removing duplicates and 
expired ads, our corpus consisted of 437 ads 
posted between 2005-10-28 and 2005-11-07.  A 
typical ad is illustrated below, with a small set of 
XML tags specifying the fields of the title of the 
ad (title), date of posting (date), ad body (text), 
ad id (docno), and document (doc).  The length 
of the documents varies considerably, from 5 or 
6 sentences to over 70 (with specifications cop-
ied from other websites).  The ads have an aver-
age length of 230 words. 
 
<doc> 
<docno>docid519</docno> 
<title>brand new 12 mega pixel digital cam-
era</title> 
<date>2005-11-07,  8:27AM PST</date> 
<text> 
BRAND NEW 12 mega pixel digital cam-
era..............only $400, 
-12 Mega pixels (4000x3000) Max Resolution 
-2.0 Color LCD Display 
-8x Digital Zoom 
-16MB Built-In (internal) Memory 
-SD or MMC card (external) Memory 
-jpeg picture format 
ALSO COMES WITH SOFTWARE & CABLES 
</text> 
</doc> 
 
The test queries were constructed based on 
human written questions from the Digital Pho-
tography Review website (www.dpreview.com) 
Q&A forums, which contain discussions from 
real users about all aspects of digital photogra-
phy. Often, users ask for suggestions on purchas-
ing digital cameras and formulate their needs as a 
set of constraints. These queries form the base of 
our topic collection.  
The following is an example of such a topic 
manually annotated with the semantic constraints 
of interest to the user:  
<topic> 
<id>1</id> 
<query> 
I wanted to know what kind of Digital SLR cam-
era I should buy. I plan to spend nothing higher 
than $1500. I was told to check out the Nikon 
D70.  
</query> 
<constraint> 
<hard: type = ?SLR? /> 
<hard: price le $1500 /> 
<soft: product_name = ?Nikon D70? /> 
</constraint> 
</topic> 
37
In this example, the user query text is in the 
query field and the manually extracted AV con-
straints based on the domain model are in the 
constraint field. Two types of constraints are 
distinguished: hard and soft. The hard constraints 
must be satisfied while the soft constraints can be 
relaxed. Manual determination of hard vs. soft 
constraints is based on the linguistic features in 
the text. Automatic constraint extraction goes 
one step beyond AV extraction for the need to 
identify relations between attributes and values, 
for example, ?nothing higher than? indicates a 
?<=? relationship. Such constraints can be ex-
tracted automatically from natural text using a 
pattern-based method. However, we have yet to 
produce a rich set of patterns addressing con-
straints. In addition, such query capability can be 
simulated with a form-based parametric search 
interface. 
In order to make a fair comparison between 
systems, we use only phrases in the manually 
extracted constraints as queries to system S1. For 
the example topic, S1 extracted the NP terms 
?SLR?, ?1500? and ?Nikon D70?. During re-
trieval, a term is further decomposed into its sub-
terms for similarity matching. For instance, the 
term ?Nikon D70? is decomposed into subterms 
?Nikon? and ?D70? and thus documents that 
mention the individual subterms can be retrieved. 
For this topic, the system S2 produced annota-
tions as those shown in the constraint field.  
Table 1 gives a summary of the distribution 
statistics of terms and constraints for 30 topics 
selected from the Digital Photography Review 
website. 
 
 Average Min Max 
No. of terms 13.2 2 31 
No. of constraints 3.2 1 7 
No. of hard constraints 2.4 1 6 
No. of soft constraints 0.8 0 3 
No. of string constraints 1.4 0 5 
No. of numerical constraints 1.8 0 4 
Table 1: Summary of the distribution statistics of 
terms and constraints in the test topics 
 
4.2 Relevance Judgments 
Instead of using human subjects to give rele-
vance judgments for each document and query 
combination, we use a human annotator to mark 
up all AV pairs in each document, using the 
GATE annotation tool (Cunningham et al 2002). 
The attribute set contains the 40 most important 
attributes for digital cameras based on automati-
cally computed term distributions in our data set. 
The inter-annotator agreement (without annotator 
training) as measured by Kappa is 0.72, which 
suggests satisfactory agreement.  
Annotating AV pairs in all documents gives us 
the capability of making relevance judgments 
automatically, based on the number of matches 
between the AV pairs in a document and the 
constraints in a topic. This automatic approach is 
reasonable because unlike TREC queries which 
are short and ambiguous, the queries in our ap-
plication represent very specific information 
needs and are therefore much longer. The lack of 
ambiguity makes our problem closer to boolean 
search with structured queries like SQL than tra-
ditional IR search. In this case, a human assessor 
should give the same relevance judgments as our 
automatic system if they follow the same instruc-
tions closely. An example instruction could be ?a 
document is relevant if it describes a digital cam-
era whose specifications satisfy at least one con-
straint in the query, otherwise it is not relevant? 
(similar to the narrative field of a TREC topic).  
We specify two levels of relevance: strict and 
relaxed. Strict means that all hard constraints of a 
topic have to be satisfied for a document to be 
relevant to the topic, whereas relaxed means that 
at least half of the hard constraints have to be 
satisfied. Soft constraints play no role in a rele-
vance judgment. The advantage of the automatic 
approach is that when the levels of relevance are 
modified for different application purposes, the 
relevance judgment can be recomputed easily, 
whereas in the manual approach, the human as-
sessor has to examine all documents again. 
 
0
20
40
60
80
100
120
140
160
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Topic Number
N
um
be
r o
f R
el
ev
an
t D
oc
s
strict_judgments relaxed_judgments   
Figure 1: Distribution of relevant documents 
across topics for relaxed and strict judgments 
 
Figure 1 shows the distributions of the rele-
vant documents for the test topic set.  With strict 
judgments, only 20 out of the 30 topics have 
relevant documents, and among them 6 topics 
38
have fewer than 10 relevant documents. The top-
ics with many constraints are likely to result in 
low numbers of relevant documents. The average 
numbers of relevant documents for the set are 
57.3 for relaxed judgments, and 18 for strict 
judgments.  
5  Results and Discussion 
Our goal is to explore whether using semantic 
information would improve document retrieval, 
taking into account the errors introduced by se-
mantic processing. We therefore evaluate two 
aspects of our system: the accuracy of AV ex-
traction and the precision of document retrieval.  
5.1 Evaluate AV Extraction 
We tested the AV extraction system on a portion 
of the annotated documents, which contains 253 
AV pairs.  Of these pairs, 151 have string values, 
and the rest have numerical values. 
The result shows a prediction accuracy of 
50.6%, false negatives (missing AV pairs) 
35.2%, false positives 11%, and wrong predica-
tions 3%. Some attributes such as brand and 
resolution have higher extraction accuracy than 
other attributes such as shooting mode and di-
mension. An analysis of the missing pairs reveals 
three main sources of error: 1) an incomplete 
domain model, which misses such camera Con-
dition phrases as ?minor surface scratching?; 2) a 
noisy domain model, due to the automatic nature 
of its construction; 3) parsing errors caused by 
free-form human written texts. Considering that 
the predication accuracy is calculated over 40 
attributes and that no human labor is involved in 
constructing the domain model, we consider our 
approach a satisfactory first step toward explor-
ing the AV extraction problem. 
5.2 Evaluate AV-based Document Retrieval 
The three retrieval systems (S1, S2, and S3) each 
return top 200 documents for evaluation. Figure 
2 summarizes the precision they achieved against 
both the relaxed and strict judgments, measured 
by the standard TREC metrics (PN ? Precision at 
N, MAP ? Mean Average Precision, RP ? R-
Precision)1. For both judgments, the combined 
                                                 
1 Precision at N is the precision at N document cutoff point; 
Average Precision is the average of the precision value ob-
tained after each relevant document is retrieved, and Mean 
Average Precision is the average of AP over all topics; R-
Precision is the precision after R documents have been re-
trieved, where R is the number of relevant documents for 
the topic. 
system S3 achieved higher precision and recall 
than S1 and S2 by all metrics. In the case of re-
call, the absolute scores improve at least nine 
percent. Table 2 shows a pairwise comparison of 
the systems on three of the most meaningful 
TREC metrics, using paired T-Test; statistically 
significant results are highlighted. The table 
shows that the improvement of S3 over S1 and 
S2 is significant (or very nearly) by all metrics 
for the relaxed judgment. However, for the strict 
judgment, none of the improvements are signifi-
cant. The reason might be that one third of the 
topics have no relevant documents in our data set. 
This reduces the actual number of topics for 
evaluation. In general, the performance of all 
three systems for the strict judgment is worse 
than that for the relaxed, likely due to the lower 
number of relevant documents for this category 
(averaged at 18 per topic), which makes it a 
harder IR task. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P10 MAP RP Recall
S1_strict S2_strict S3_strict S1_relaxed S2_relaxed S3_relaxed  
Figure 2: System performance as measured by 
TREC metrics, averaged over all topics with non-
zero relevant documents 
 
Paired T-Test (p) P10 AP RP 
(S1,S2) .22 .37 .65 
(S2,S3) 1 .004 .10 
strict 
(S1,S3) .17 .48 .45 
(S1,S2) .62 .07 .56 
(S2,S3) .056 <.0001 .0007 
relaxed 
(S1,S3) .04 .02 .03 
Table 2: Paired T-Test (with two-tailed distribu-
tion) between systems over all topics 
 
The constraint-based system S2 produces 
higher initial precision than S1 as measured by 
P10. However, semantic constraints contribute 
less and less as more documents are retrieved. 
The performance of S2 is slightly worse than S1 
as measured by AP and RP, which is likely due 
to errors from AV extraction. None of the met-
rics is statistically significant.  
39
Topic-by-topic analysis gives us a more de-
tailed view of the behavior of the three systems.  
Figure 3 shows the performance of the systems 
measured by P10, sorted by that of S3. In gen-
eral, the performance of S1 and S2 deviates sig-
nificantly for individual topics. However, the 
combined system, S3, seems to be able to boost 
the good results from both systems for most top-
ics.  We are currently exploring the factors that 
contribute to the performance boost. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
16 8 15 18 4 26 30 11 24 28 7 10 2 5 13 20 21 27 3 25 1 12 17 29 19 6 23 14 22
Topic Numbers
Pr
ec
is
io
n 
@
 1
0
S1_relaxed S2_relaxed S3_relaxed  
Figure 3: Precision@10 for relaxed judgment 
 
A closer look at topics where S3 improves 
significantly over S1 and S2 at P10 reveals that 
the combined lists are biased toward the docu-
ments returned by S2, probably due to the higher 
scores assigned to documents by S2 than those 
by S1. This suggests the need for better score 
normalization methods that take into account the 
advantage of each system. 
In conclusion, our results show that using se-
mantic information can improve IR results for 
special domains where the information need can 
be specified as a set of semantic constraints. The 
constraint-based system itself is not robust 
enough to be a standalone IR system, and has to 
be combined with a term-based system to 
achieve satisfactory results. The IR results from 
the combined system seem to be able to tolerate 
significant errors in semantic annotation, consid-
ering that the accuracy of AV-extraction is about 
50%. It remains to be seen whether similar im-
provement in retrieval can be achieved in general 
domains such as news articles. 
6 Summary 
This paper describes our exploratory study of 
applying semantic constraints derived from at-
tribute-value pair annotations to traditional term-
based document retrieval. It shows promising 
results in our test domain where users have spe-
cific information needs. In our ongoing work, we 
are expanding the test topic set for the strict 
judgment as well as the data set, improving AV 
extraction accuracy, analyzing how the combined 
system improves upon individual systems, and 
exploring alternative ways of combining seman-
tic constraints and terms for better retrieval. 
References 
Hamish Cunningham, Diana Maynard, Kalina 
Bontcheva and Valentin Tablan.  2002. GATE: A 
Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. Pro-
ceedings of the 40th Anniversary Meeting of the 
Association for Computational Linguistics 
(ACL'02). Philadelphia. 
Bruce Croft, James Callan and John Broglio. 1994. 
TREC-2 Routing and Ad-Hoc Retrieval Evaluation 
Using the INQUERY System. In Proceedings of 
the 2nd Text Retrieval Conference, NIST Special 
Putlication 500-215. 
David A. Evans and Robert Lefferts. 1995.  CLARIT-
TREC experiments.  Information Processing and 
Management, 31(3), 385-395. 
E. Fix and J. Hodges. 1951. Discriminatory Analysis, 
Nonparametric Discrimination: Consistency Prop-
erties. Technical Report, USAF School of Aviation 
Medicine, Texas. 
Joon Ho Lee. 1997. Analyses of Multiple Evidence 
Combination. Proceedings of the 20th Annual In-
ternational ACM-SIGIR Conference on Research 
and Development in Information Retrieval. Phila-
delphia, pp. 267-276. 
Dekang Lin. 1998. Dependency-based Evaluation of 
MINIPAR. Workshop on the Evaluation of Parsing 
Systems, Spain.  
Yan Qu, David A. Hull, Gregory Grefenstette, David 
A. Evans, et al 2005. Towards Effective Strategies 
for Monolingual and Bilingual Information Re-
trieval: Lessons Learned from NTCIR-4. ACM 
Transactions on Asian Language Information 
Processing, 4(2): 78-110. 
Robert Wagner and Michael Fischer. 1974. The 
String-to-string Correction Problem. Journal of the 
Association for Computing Machinery, 21(1):168-
173.  
Tomek Strzalkowski, Louise Guthrie, Jussi Karigren, 
Jim Leistensnider, et al 1996. Natural language in-
formation retrieval, TREC-5 report. In Proceedings 
of the 5th Text Retrieval Conference (TREC-5), pp. 
291-314, Gaithersburg, Maryland. 
Ellen Voorhees. 1998. Using WordNet for text re-
trieval. In Wordnet, an Electronic Lexical Data-
base, pp 285-303. The MIT Press. 
 
40
