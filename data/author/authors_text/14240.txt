Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 222?225,
Paris, October 2009. c?2009 Association for Computational Linguistics
Interactive Predictive Parsing 1
Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez and Jose?-Miguel Bened??
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
Cam?? de Vera s/n, Valencia 46022 (Spain)
{rsanchez, jandreu, jbenedi}dsic.upv.es
Abstract
This paper introduces a formal framework
that presents a novel Interactive Predic-
tive Parsing schema which can be oper-
ated by a user, tightly integrated into the
system, to obtain error free trees. This
compares to the classical two-step schema
of manually post-editing the erroneus con-
stituents produced by the parsing system.
We have simulated interaction and cal-
culated evalaution metrics, which estab-
lished that an IPP system results in a high
amount of effort reduction for a manual
annotator compared to a two-step system.
1 Introduction
The aim of parsing is to obtain the linguistic in-
terpretation of sentences, that is, their underlying
syntactic structure. This task is one of the fun-
damental pieces needed by a computer to uns-
derstand language as used by humans, and has
many applications in Natural Language Process-
ing (Lease et al, 2006).
A wide array of parsing methods exist, in-
cluding those based on Probabilistic Context-Free
Grammars (PCFGs). (Charniak, 2000; Collins,
2003; Johnson, 1998; Klein and Manning, 2003;
Matsuzaki et al, 2005; Petrov and Klein, 2007).
The most impressive results are achieved by sub-
tree reranking systems, as shown in the semi-
supervised method of (McClosky et al, 2006),
or the forest reranking approximation of (Huang,
2008) in which packed parse forests (compact
structures that contain many possible tree deriva-
tions) are used.
These state-of-the-art parsers provide trees of
excelent quality. However, perfect results are vir-
1Work supported by the MIPRCV ?Consolider Inge-
nio 2010? (CSD2007-00018), iTransDoc (TIN2006-15694-
CO2-01) and Prometeo (PROMETEO/2009/014) reserach
projects, and the FPU fellowship AP2006-01363.
tually never achieved. If the need of one-hundred-
percent error free trees arises, the supervision of a
user that post-edits and corrects the errors is un-
avoidable.
Error free trees are needed in many tasks such as
handwritten mathematical expressions recognition
(Yamamoto et al, 2006), or creation of new gold
standard treebanks (Delaclergerie et al, 2008)).
For example, in the creation of the Penn Tree-
bank grammar, a basic two-stage setup was em-
ployed: a rudimentary parsing system providad a
skeletal syntactic representation, which then was
manually corrected by human annotators (Marcus
et al, 1993).
In this paper, we introduce a new formal frame-
work that tightly integrates the user within the
parsing system itself, rather than keeping him iso-
lated from the automatic tools used in a classi-
cal two-step approach. This approach introduces
the user into the parsing system, and we will call
it ?interactive predictive parsing?, or simply IPP.
An IPP system is interactive because the user is in
continuous contact with the parsing process, send-
ing and receiving feedback. An IPP system is also
predictive because it reacts to the user corrections:
it predicts and suggest new parse trees taking into
account the new gold knowledge received from
the user. Interactive predictive methods have been
studied and successfully used in fields like Auto-
matic Text Recognition (Toselli et al, 2008) and
Statistical Machine Translation (Barrachina et al,
2009; Vidal et al, 2006) to ease the work of tran-
scriptor and translators.
Assessment of the amount of effort saved by the
IPP system will be measured by automatically cal-
culated metrics.
2 Interactive Predictive Parsing
A tree t, associated to a string x1|x|, is composed
by substructures that are usually referred as con-
stituents or edges. A constituent cAij is a span de-
222
fined by a nonterminal symbol (or syntactic tag) A
that covers the substring xij .
Assume that using a given probabilistic context-
free grammar G as the model, the parser analyzes
the input sentence x = x1 . . . x|x| and produces
the parse tree t?
t? = argmax
t?T
pG(t|x), (1)
where pG(t|x) is the probability of parse tree t
given the input string x using model G, and T is
the set of all possible parse trees for x.
In an interactive predictive scenario, after ob-
taining the (probably incorrect) best tree t?, the user
is able to modify the edges cAij that are incorrect.
The system reacts to each of the corrections intro-
duced by the human by proposing a new t?? that
takes into account the corrected edge. The order
in which incorrect constituents are reviewed deter-
mines the amount of effort reduction given by the
degree of correctness of the subsequent proposed
trees.
There exist several ways in which a human ana-
lyzes a sentende. A top-to-bottom may be consid-
ered natural way of proceeding, and we follow this
approach in this work. This way, when a higher
level constituent is corrected, possible erroneous
constituents at lower levels are expectedly auto-
matically recalculated.
The introduced IPP interaction process is sim-
ilar to the ones already established in Computer-
Assisted Text Recognition and Computer-Assisted
Translation 1.
Within the IPP framework, the user reviews the
constituents contained in the tree to assess their
correctness. When the user find an incorrect edge
he modifies it, setting the correct label and span.
This action implicitly validates a subtree that is
composed by the corrected edge plus all its ances-
tor edges, which we will call the validated prefix
tree tp. When the user replaces the constituent cAij
with the correct one c?Aij , the validated prefix tree
is:
tp(c?Aij ) = {cBmn : m ? i, n ? j
d(cBmn) ? d(c?Aij )}
(2)
with d(cDpq) being the depth of constituent cDpq.
1In these fields, the user reads the sentence from left to
right. When the user finds and corrects an erroneus word, he
is implicitly validating the prefix sentence up to that word.
The remaining suffix sentence is recalculated by the system
taking into account the validated prefix sentece.
When a constituent correction is performed, the
prefix tree tp(c?Aij ) is fixed and a new tree t?? that
takes into account the prefix is proposed
t?? = argmax
t?T
pG(t|x, tp(c?Aij )). (3)
Given that we are working with context-free
grammars, the only subtree that effectively needs
to be recalcuted is the one starting from the par-
ent of the corrected edge. Let the corrected edge
be c?Aij and its parent cDst, then the following tree is
proposed
t?? = argmax
t?T
pG(t|x, tp) = (t? \ t?Dst) ? t??Dst , (4)
with
t??Dst = argmax
tDst?Tst
pG(tDst|xmn, c?Aij ) . (5)
Expression (4) represents the newly proposed
tree t??, which consists of original proposed tree
t? minus the subpart of the original proposed tree
t?Dst (whose root is the parent of the corrected edge
cDst) plus the newly calculated subtree t??
D
st (whose
root is also the parent of the corrected constituent
cDst, but also takes into account the corrected one
as shown in Expression (5)).
In Figure 1 we show an example that intends to
clarify the interactive predictive process. First, the
system provides a proposed parse tree (Fig. 1.a).
Then the user, which has in his mind the correct
reference tree, notices that it has two wrong con-
stituents (cX23 and cZ44) (Fig. 1.b), and choses to re-
place cX23 by cB22 (Fig. 1.c). Here, cB22 corresponds
to c?Aij from expressions (3) and (5).
As the user does this correction, the system au-
tomatically validates the correct prefix: all the an-
cestors of the modified constituent (dashed line in
the figure, tp(c?Aij ) from expression (2)). The sys-
tem also invalidates the subtrees related to the cor-
rected constituent (dotted line line in the figure, t?Dst
from expression (4)).
Finally, the system automatically predicts a new
subtree (t??Dst from expression (4)) (Fig. 1.d). No-
tice how cZ34 changes its span and cD44 is introduced
which provides the correct reference parse.
Within the example shown in Figure 1, the user
would obtain the gold tree with just one correction,
rather than the three operations needed on a two-
step system (one deletion, one substitution and one
insertion).
223
SB Z
Y
ba c d
A
DC
(a) Reference tree
S
ba c d
A
CB
X
Y
Z
(b) Iteration 0:
Proposed out-
put tree 1
S
ba c d
A
CB
X Z 423 4
Y
(c) Iteration 0: Er-
roneus constituents
S
ba c d
A
B 2
2 ?
? ?
Y
(d) Iteration 1:
User corrected
constituent
S
B Z
Y
ba c d
A
DC
3
4
(e) Iteration 1:
Proposed output
tree 2
Figure 1: Synthetic example of user interaction with the IPP system.
3 IPP Evaluation
The objective of the experimentation presented
here is to evaluate the amount of effort saved for
the user using the IPP system, compared to the ef-
fort required to manually correct the trees without
the use of an interactive system. In this section, we
define a standard automatic evaluation protocol,
akin to the ones used in Computer-Aided Trans-
lation and Computer Aided Text Recognition.
In the absence of testing of an interactive sys-
tem with real users, the gold reference trees were
used to simulate system interaction by a human
corrector. In order to do this, the constituents in
the proposed tree were automatically reviewed in a
preorder manner 2. In each step, the constituent in
the proposed tree was compared to the correspond-
ing one in the reference tree: if the constituent was
equivalent no action was taken. When one incor-
rect constituent was found in the proposed tree, it
was replaced by the correct one from the reference
tree. This precise step simulated what a human su-
pervisor would do, that is, to type the correct con-
stituent in place of the erroneus one.
The system then performed the predictive step
(i.e. recalculation of subtrees related to the cor-
rected constituent). We kept a correction count,
which was incremented by one after each predic-
tive step.
3.1 Evaluation metrics
For evaluation, first we report a metric represent-
ing the amount of human correcting work needed
to obtain the gold tree in a classical two-step pro-
cess (i.e. the number of operations needed to post-
edit the proposed tree in orther to obtain the gold
2Interaction in this ordered manner guaranteed that the
evaluation protocol only needed to modify the label A and
the end point j of a given edge cAij , while i remained valid
given the modifications of previous constituents.
one). We then compare this value to a metric that
measures the amount of effort needed to obtain
the gold tree with the human interacting within the
presented IPP system.
Parsing quality is generally assessed by the clas-
sical evaluation metrics, precission, recall and F-
measure. We defined the following metric that
measures the amount of effort needed in order to
post-edit a proposed tree and obtain the gold ref-
erence parse tree, akin to the Word Error Rate
used in Statistical Machine Translation and related
fields:
? Tree Constituent Error Rate (TCER): Min-
imum number of constituent substitution,
deletion and insertion operations needed to
convert the proposed parse tree into the corre-
sponding gold reference tree, divided by the
total number of constituents in the reference
tree 3.
The TCER is in fact strongly related to the F-
measure: the higher the F-measure is, the lower
TCER will be.
Finally, the relevant evaluation metric that as-
sessed the IPP system performance represents the
amount effort that the operator would have to
spend using the system in order to obtain the gold
tree, and is directly comparable to the TCER:
? Tree Constituent Action Rate (TCAC): Num-
ber of constituent corrections performed us-
ing the IPP system to obtain the reference
tree, divided by the total number of con-
stituents in the reference tree.
4 Experimental results
An IPP system was implemented over the classical
CYK-Viterbi algorithm. Experimentation was run
3Edit distance is calcualted over the ordered set of tree
constituents. This is an approximation of the edit distance
between trees.
224
over the Penn Tree bank: sections 2 to 21 were
used to obtain a vanilla Penn Treebank Grammar;
test set was the whole section 23.
We obtained several binarized versions of the
train grammar for use with the CYK. The Chom-
sky Normal Form (CNF) transformation method
from the NLTK4 was used to obtain several right-
factored binary grammars of different sizes 5.
A basic schema was introduced for parsing sen-
tences with out-of-vocabulary words: when an
input word could not be derived by any of the
preterminals in the vanilla treebank grammar, a
very small probability for that word was uniformly
added to all of the preterminals.
Results for the metrics discussed on section 3.1
for different markovizations of the train grammar
can be seen in Table 1. We observe that the perc-
etage of corrections needed using the IPP system
is much lower than the rate of needed corrections
just post-editing the proposed trees: from 42% to
46% in effort reduction by the human supervisor.
These results clearly show that an interactive
predictive system can relieve manual annotators of
a lot of burden in their task.
Note that the presented experiments were done
using parsing models that perform far from the lat-
est F1 results; their intention was to assess the util-
ity of the IPP schema. Expected relative reduc-
tions with IPP systems incorporating state-of-the-
art parsers would not be so large.
PCFG Baseline IPP RelRedF1 TCER TCAC
h=0, v=1 0.67 0.40 0.22 45%
h=0, v=2 0.68 0.39 0.21 46%
h=0, v=3 0.70 0.38 0.22 42%
Table 1: Results for the test set: F1 and TCER
for the baseline system; TCAC for the IPP system;
relative reduction beteween TCER and TCAC.
5 Conclusions
We have introduced a novel Interactive Predictive
Parsing framewrok which can be operated by a
user to obtain error free trees. We have simulated
interaction with this system and calculated evalau-
tion metrics, which established that an IPP system
results in a high amount of effort reduction for a
manual annotator compared to a two-step system.
4http://nltk.sourceforge.net/
5This method implements the vertical (v value) and hori-
zontal (h value) markovizations (Klein and Manning, 2003).
Near term future work includes applying the
IPP scenario to state-of-the-art reranking and pars-
ing systems, as well as in the development of adap-
tative parsing systems
References
Barrachina, Sergio, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jess Toms, En-
rique Vidal, Juan-Miguel Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. In Com-
putational Linguistics, 35(1) 3-28.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In NAACL ?00, 132-139.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics, 29(4):589-637.
De la Clergerie, ?Eric, Olivier Hamon, Djamel Mostefa,
Christelle Ayache, Patrick Paroubek and Anne Vil-
nat. 2008. PASSAGE: from French Parser Evalua-
tion to Large Sized Treebank. In LREC?08.
Huang, Liang. 2008. Forest reranking: discriminative
parsing with non-local features. In ACL ?08.
Johnson, Mark. 1998. PCFG models of linguistic
tree representation. In Computational Linguistics,
24:613-632.
Klein, Dan and Chistopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In ACL ?03, 423-430.
Lease, Matthew, Eugene Charniak, Mark Johnson and
David McClosky. 2006. A look at parsing and its
applications. In National Conference on Artificial
Intelligence, vol. 21-II, 1642-1645.
Marcus, Mitchell P., Mary Ann Marcinkiewicz and
Beatrice Santorini. 1995. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics 19(2), 313-330.
Matsuzaki, Takuya, Yasuke Miyao and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL ?05, 75-82.
McClosky, David, Eugene Charniak and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL ?06
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT ?07.
Toselli, Alejandro, Vero?nica Romero and Enrique Vi-
dal. 2008. Computer Assisted Transcription of Text
Images and Multimodal Interaction. In MLMI ?08.
Vidal, Enrique, Francisco Casacuberta, Luis Ro-
driguez, Jorge Civera and Carlos D. Martnez Hinare-
jos. 2006. Computer-assisted translation using
speech recognition. In IEEE Trans. on Audio,
Speech, and Language Processing, 14(3), 941-951.
Yamamoto, Ryo, Shinji Sako, Takuya Nishimoto and
Shigeki Sagayama. 2006. On-line recognition
of handwritten mathematical expressions based on
stroke-based stochastic context-free grammar. In
10th International Workshop on Frontiers in Hand-
writing Recognition.
225
Coling 2010: Poster Volume, pages 1220?1228,
Beijing, August 2010
Confidence Measures for Error Discrimination
in an Interactive Predictive Parsing Framework1
Ricardo Sa?nchez-Sa?ez, Joan Andreu Sa?nchez and Jose? Miguel Bened
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{rsanchez,jandreu,jbenedi}@dsic.upv.es
Abstract
We study the use of Confidence Measures
(CM) for erroneous constituent discrimi-
nation in an Interactive Predictive Parsing
(IPP) framework. The IPP framework al-
lows to build interactive tree annotation
systems that can help human correctors
in constructing error-free parse trees with
little effort (compared to manually post-
editing the trees obtained from an auto-
matic parser). We show that CMs can
help in detecting erroneous constituents
more quickly through all the IPP process.
We present two methods for precalculat-
ing the confidence threshold (globally and
per-interaction), and observe that CMs re-
main highly discriminant as the IPP pro-
cess advances.
1 Introduction
Within the Natural Language Processing (NLP)
field, we can tell apart two different usage scenar-
ios for automatic systems that output or work with
natural language. On one hand, we have the cases
in which the output of such systems is expected to
be used in a vanilla fashion, that is, without val-
idating or correcting the results produced by the
system. Within this usage scheme, the most im-
portant factor of a given automatic system is the
quality of the results. Although memory and com-
putational requirements of such systems are usu-
ally taken into account, the ultimate aim of most
1Work partially supported by the Spanish MICINN under
the MIPRCV ?Consolider Ingenio 2010? (CSD2007-00018),
MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-
TEO/2009/014) research projects, and the FPU fellowship
AP2006-01363.
research that relates to this scenario is to minimize
the amount of error (measured with metrics like
Word Error Rate, BLEU, F-Measure, etc.) present
within the results that are being produced.
The second usage scenario arises when there
exists the need for perfect and completely error-
free results, for example, flawlessly translated
sentences or correctly annotated syntactic trees.
In such cases, the intervention of a human valida-
tor/corrector is unavoidable. The corrector will
review and validate the results, making the suit-
able modifications before the system output can
be employed. In these kind of tasks, the most im-
portant factor to be minimized is the human ef-
fort that has to be applied to transform the sys-
tem?s potentially incorrect output into validated
and error-free output. Measuring user effort has
an intrinsic subjectivity that makes it hard to be
quantitatized. Given that the user effort is usually
inversely proportional to the quality of the system
output, most research about problems associated
to this scenario t to minimize just the system?s er-
ror rate as well.
Interactive Predictive NLP Systems
Only recently, more comparable and repro-
ducible evaluation methods for Interactive Natural
Language Systems have started to be developed,
within the context of Interactive Predictive Sys-
tems (IPS). These systems formally integrate the
correcting user into the loop, making him part of
the system right at its theoretical framework. IPSs
allow for human correctors to spare effort because
the system updates its output after each individ-
ual user correction, potentially fixing several er-
rors at each step. Interactive Predictive methods
have been studied and successfully used in fields
1220
like Handwritten Text Recognition (HTR) (Toselli
et al, 2008) and Statistical Machine Translation
(SMT) (Vidal et al, 2006; Barrachina et al, 2009)
to ease the work of transcriptors and translators.
In IPS related research the importance of the
system base error rate per se is diminished. In-
stead, the intention is to measure how well the
user and the system work together. For this, for-
mal user simulation protocols together with new
objective effort evaluation metrics such as the
Word Stroke Ratio (WSR) (Toselli et al, 2008) or
the Key-Stroke and Mouse-Ratio (KSMR) (Bar-
rachina et al, 2009) started to be used as a
benchmark. These ratios reflect the amount of
user effort (whole-word corrections in the case of
WSR; keystrokes plus mouse actions in the case of
KSMR) given a certain output. To get the amount
of user effort into context they should be measured
against the corresponding error ratios of compara-
ble non-interactive systems: Word Error Rate for
WSR and Character Error Rate for KSMR.
This dichotomy in evaluating either system per-
formance or user effort applies to Syntactic Pars-
ing as well. The objective of parsing is to pre-
cisely determine the syntactic structure of sen-
tences written in one of the several languages that
humans use. Very bright research has been carried
out in this field, resulting in several top perform-
ing completely automatic parsers (Collins, 2003;
Klein and Manning, 2003; McClosky et al, 2006;
Huang, 2008; Petrov, 2010). However, these pro-
duce results that are erroneous to some extent, and
as such unsuitable for some applications without a
previous manual correction. There are many prob-
lems where error-free results consisting in per-
fectly annotated trees are needed, such as hand-
written mathematical expression recognition (Ya-
mamoto et al, 2006) or construction of large new
gold treebanks (de la Clergerie et al, 2008).
When using automatic parsers as a baseline for
building perfect syntactic trees, the role of the
human annotator is usually to post-edit the trees
and correct the errors. This manner of operat-
ing results in the typical two-step process for er-
ror correcting, in which the system first gener-
ates the whole output and then the user verifies
or amends it. This paradigm is rather inefficient
and uncomfortable for the human annotator. For
example, a basic two-stage setup was employed
in the creation of the Penn Treebank annotated
corpus: a rudimentary parsing system provided a
skeletal syntactic representation, which then was
manually corrected by human annotators (Marcus
et al, 1994). Additional works within this field
have presented systems that act as a computerized
aid to the user in obtaining the perfect annotation
(Carter, 1997; Oepen et al, 2004; Hiroshi et al,
2005). Subjective measuring of the effort needed
to obtain perfect annotations was reported in some
of these works, but we feel that a more compara-
ble metric is needed.
With the objective of reducing the user effort
and making the laborious task of tree annotation
easier, the authors of (Sa?nchez-Sa?ez et al, 2009a)
devised an Interactive Predictive Parsing (IPP)
framework. That work embeds the human cor-
rector into the automatic parser, and allows him
to interact in real time within the system. In this
manner, the system can use the readily available
user feedback to make predictions about the parts
of the trees that have not been validated by the
corrector. The authors simulated user interaction
and calculated effort evaluation metrics, establish-
ing that an IPP system results in amounts slightly
above 40% of effort reduction for a manual anno-
tator compared to a two-step system.
Confidence Measures in NLP
Annotating trees syntactically, even with the
aid of automatic systems, generally requires hu-
man intervention with a high degree of special-
ization. This fact partially justifies the shortage
in large manually annotated treebanks. Endeavors
directed at easing the burden for the experts per-
forming this task could be of great help.
One approach that can be followed in reducing
user effort within an IPS is adding information
that helps the user to locate the individual errors
in a sentence, so he can correct them in a hastier
fashion. The use of the Confidence Measure (CM)
formalism goes in this direction, allowing us to
assign a probability of correctness for individual
erroneous constituents of a more complex output
block of a NLP system.
In fields such as HTR, SMT or Automatic
Speech Recognition (ASR), the output sentences
1221
have a global probability (or score) that reflects
the likeness of the output sentence being correct.
CMs allow precision beyond the sentence level in
predicting errors: they can be used to label the in-
dividual words as either correct or incorrect. Au-
tomatic systems can use CMs to help the user in
identifying the erroneous parts of the output in a
faster way or to aid with the amendments by sug-
gesting replacement words that are likely to be
correct.
Previous research shows that CMs have been
successfully applied within the ASR (Wessel et
al., 2001), HTR (Tarazo?n et al, 2009; Serrano
et al, 2010) and SMT (Ueffing and Ney, 2007)
fields. In these works, the ability of CMs in de-
tecting erroneous constituents is assessed by the
classical confidence metrics: the Confidence Er-
ror Rate (CER) and the Receiver Operating Char-
acteristic (ROC) (Ueffing and Ney, 2007).
However, until recent advances, the use of CMs
remained largely unexplored in Parsing. Assess-
ing the correctness of the different parts of a pars-
ing tree can be useful in improving the efficiency
and usability of an IPP system, not only by tag-
ging parts with low confidence for the user to re-
view, but also by automating part of the correction
process itself by presenting constituents that yield
a higher confidence when an error is confirmed by
the user.
CMs for parsing in the form of combinations
of features calculated from n-best lists were pro-
posed in (Bened?? et al, 2007). Later on, the au-
thors of (Sa?nchez-Sa?ez et al, 2009b) introduced
a statistical method for calculating a CM for each
of the constituents in a parse tree. In that work,
CMs are calculated using the posterior probability
of each tree constituent, approach which is similar
to the word-graph based methods in the ASR and
SMT fields.
In this paper, we apply Confidence Measures
to the Interactive Predictive Parsing framework to
asses how CMs are increasingly more accurate as
the user validates subtrees within the interactive
process. We prove that after each correction per-
formed by the user, the CMs of the remaining un-
validated constituents are more helpful to detect
errors.
2 Interactive Predictive Parsing
In this section we review the IPP framework
(Sa?nchez-Sa?ez et al, 2009a) and its underlying
operation protocol. In parsing, a syntactic tree t,
attached to a string x = x1 . . . x|x| is composed
by substructures called constituents. A constituent
cAij is defined by the nonterminal symbol (either
a syntactic label or a POS tag) A and its span
ij (the starting and ending indexes which delimit
the part of the input sentence encompassed by the
constituent).
Here follows a general formulation for the non-
interactive syntactic parsing scenario, which will
allow us to better introduce the IPP formulation.
Assume that using a given parsing model G, the
parser analyzes the input sentence x and produces
the most probable parse tree
t? = argmax
t?T
pG(t|x), (1)
where pG(t|x) is the probability of the parse tree
t given the input string x using model G, and T is
the set of all possible parse trees for x.
In the IPP framework, the manual corrector
provides feedback to the system by correcting any
of the constituents cAij from t?. The system reacts
to each of the corrections performed by the human
annotator by proposing a new t?? that takes into ac-
count the correction.
Within the IPP framework, the user reviews the
constituents contained in the tree to assess their
correctness. When the user finds an incorrect con-
stituent he modifies it, setting the correct span and
label. This action implicitly validates what it is
called the validated prefix tree tp.
We define the validated prefix tree to be com-
posed by the partially corrected constituent, all
of its ancestor constituents, and all constituents
whose end span is lower than the start span of the
corrected constituent. When the user replaces the
constituent cAij with the correct one c?Aij , the vali-
dated prefix tree is
tp(c?Aij ) = {cBmn : m ? i, n ? j ,
d(cBmn) ? d(c?Aij )} ?
{cDpq : q < i }
(2)
1222
with d(cZab) being the depth (distance from root)
of constituent cZab.
The validated prefix tree is parallel to the vali-
dated sentence prefix commonly used in Interac-
tive Machine Translation or Interactive Handwrit-
ten Recognition, and is established after each user
action.
This particular definition of the prefix tree de-
termines the fact that the user is expected to re-
view the parse tree in a preorder fashion (left-to-
right depth-first). Note that this specific explo-
ration order allows us to simulate the user inter-
action for the experimentation, as we will explain
below. Also note that other types of prefixes could
be defined, allowing for different tree review or-
ders.
Within the IPP formulation, when a constituent
correction is performed, the prefix tree tp(c?Aij ) is
validated and a new tree t?? that takes into account
the prefix is proposed. Incorporating this new
evidence into expression (1) yields the following
equation
t?? = argmax
t?T
pG(t|x, tp(c?Aij )). (3)
Given the properties of Probabilistic Context-
Free Grammars (PCFG) the only subtree that ef-
fectively needs to be recalculated is the one start-
ing from the parent of the corrected constituent.
This way, just the descendants of the newly intro-
duced constituent, as well as its right hand siblings
(along with their descendants) are calculated.
2.1 User Interaction Operation
The IPP formulation allows for a very straightfor-
ward operation protocol that is performed by the
manual corrector, in which he validates or corrects
the successive output parse trees:
1. The IPP system proposes a full parse tree t
for the input sentence.
2. Then, the user finds the first incorrect con-
stituent exploring the tree in a certain ordered
manner (preorder in our case, given by the
tree prefix definition) and amends it, by mod-
ifying its span and/or label (implicitly vali-
dating the prefix tree tp).
3. The IPP system produces the most probable
tree that is compatible with the validated pre-
fix tree tp as shown in expression (3).
4. These steps are iterated until a final, perfect
parse tree is produced by the system and val-
idated by the user.
It is worth noting that within this protocol, con-
stituents can be automatically deleted or inserted
at the end of any subtree in the syntactic struc-
ture by adequately modifying the span of the left-
neighbouring constituent.
The IPP interaction process is similar to the
ones already established in HTR and SMT. In
these fields, the user reads the output sentence
from left to right. When the user finds and corrects
an erroneous word, he is implicitly validating the
prefix sentence up to that word. The remaining
suffix sentence is recalculated by the system tak-
ing into account the validated prefix sentence.
Fig. 1 shows an example that intends to clar-
ify the Interactive Predictive process. First, the
system provides a tentative parse tree (Fig. 1.b).
Then the user, which has the correct reference tree
(Fig. 1.a) in mind, notices that it has two wrong
constituents (cX23 and cZ44) (Fig. 1.c), and chooses
to replace cX23 by cB22 (Fig. 1.d). Here, cB22 cor-
responds to c?Aij of expression (3). As the user
does this correction, the system automatically val-
idates the prefix (dashed line in Fig. 1.d, tp(c?Aij )
of expression (2)). The system also invalidates
the subtrees outside the prefix (dotted line line in
Fig. 1.d). Finally, the system automatically pre-
dicts a new subtree (Fig. 1.e). Notice how cZ34
changes its span and cD44 is introduced which pro-
vides the correct reference parse.
For further exemplification, Sa?nchez-Sa?ez
et al (2010) demonstrate an IPP based
annotation tool that can be accessed at
http://cat.iti.upv.es/ipp/.
Within the IPP scenario, the user has to man-
ually review all the system output and correct or
validate it, which is still a considerable amount of
effort. CMs can ease this work by helping to spot
the erroneous constituents.
1223
SB Z
Y
ba c d
A
DC
(a) Reference tree
S
ba c d
A
CB
X
Y
Z
(b) Iteration 0: Pro-
posed output tree 1
S
ba c d
A
CB
X Z 423 4
Y
(c) Iteration 0: Erro-
neous constituents
Y
S
ba c d
A
B 2
2 ?
? ?
(d) Iteration 1:
User corrected
constituent
S
B Z
Y
ba c d
A
DC
3
4
(e) Iteration 1:
Proposed output
tree 2
Figure 1: Synthetic example of user interaction with the IPP system.
3 Confidence Measures
Probabilistic calculation of Confidence Measures
(Sa?nchez-Sa?ez et al, 2009b) for all tree con-
stituents can be introduced within the IPP process.
The CM of each constituent is its posterior
probability, which can be considered as a measure
of the degree to which the constituent is believed
to be correct for a given input sentence x. This is
formulated as follows
pG(cAij |x) =
pG(cAij ,x)
pG(x)
=
?
t??T ; c?Aij ?t? ?(c
A
ij , c?Aij ) pG(t?|x)
pG(x)
(4)
with ?() being the Kronecker delta function. Nu-
merator in expression (4) stands for the probabil-
ity of all parse trees for x that contain the con-
stituent cAij (see Fig. 2).
S
A
?A(i, j)
?A(i, j)
x1 xi?1 xi xj xj+1 x|x|
Figure 2: The product of the inside and outside
probabilities for each constituent comprises the
upper part of expression (5)
The posterior probability is computed with the
inside ? and outside ? probabilities (Baker, 1979)
C(tAij) = pG(cAij |x) =
pG(cAij ,x)
pG(x)
= ?A(i, j) ?A(i, j)?S(1, |x|)
.
(5)
It should be clear that the calculation of con-
fidence measures reviewed here is generalizable
for any problem that employs PCFGs, and not
just NLP tasks. In the experiments presented in
the following section we show that CMs are in-
creasingly discriminant when used within the IPP
framework to detect erroneous constituents.
4 Experiments
Evaluation of the quality of CMs within the IPP
framework is done in a completely automatic
fashion by simulating user interaction. Section 4.1
introduces the evaluation protocol and metrics
measuring CM quality (i.e., their ability to de-
tect incorrect constituents). The experimentation
framework and the results are discussed in sec-
tion 4.2.
4.1 Evaluation Methods
4.1.1 IPP Evaluation
A good measure of the performance of an In-
teractive Predictive System is the amount of ef-
fort saved by the users of such a system. It is
subjective and expensive to test an IPS with real
users, so these systems are usually evaluated us-
ing automatically calculated metrics that assess
the amount of effort saved by the user.
1224
As already mentioned, the objective of an IPP
based system is to be employed by annotators to
construct correct syntactic trees with less effort.
Evaluation of an IPP system was previously done
by comparing the IPP usage effort (the number of
corrections using the IPP system) against the es-
timated effort required to manually post-edit the
trees after obtaining them with a traditional au-
tomatic parsing system (the amount of incorrect
constituents) (Sa?nchez-Sa?ez et al, 2009a).
In the case of IPP, the gold reference trees are
used to simulate system interaction by a human
corrector and provide a comparable benchmark.
This automatic evaluation protocol is similar to
the one presented in section 2.1:
1. The IPP system proposes a full parse tree t
for the input sentence.
2. The user simulation subsystem finds the first
incorrect constituent by exploring the tree in
the order defined by the prefix tree definition
(preorder) and comparing it with the refer-
ence. When the first erroneous constituent
is found, it is amended by being replaced in
the output tree by the correct one, operation
which implicitly validates the prefix tree tp.
3. The IPP system produces the most probable
tree that is compatible with the validated pre-
fix tree tp.
4. These steps are iterated until a final, perfect
parse tree is produced by the IPP system and
validated against the reference by the user
simulation subsystem.
In this work, metrics assessing the quality of
CM are introduced within this automatic protocol.
We calculate and report them after each of the it-
erations in the IPP process.
4.1.2 Confidence Measure Evaluation
Metrics
The CM of each tree constituent, computed as
shown in expression (4) can be seen as its prob-
ability of being correct. Once all CM are calcu-
lated, a confidence threshold ? ? [0, 1] can be
chosen. Constituents are then marked using ? : the
ones with a confidence above this threshold are
marked as correct, and the rest as incorrect. Com-
paring the confidence marks in the output tree
with the reference, we obtain the false rejection
Nf (?) ? [0, Nc] (number of correct constituents
in the output tree wrongly marked as incorrect by
their CM) and the true rejection Nt(?) ? [0, Ni]
(number of incorrect constituents in the output
tree that are indeed detected as incorrect by their
confidence).
The amount of correct and incorrect con-
stituents in each tree is Nc and Ni respectively. In
the ideal case of perfectly error discriminant CM,
using the best threshold would yield Nf (?) = 0
and Nt(?) = Ni.
A evaluation metric that assess the ability of
CMs in telling apart correct constituents from in-
correct ones is the Confidence Error Rate (CER):
CER(?) = Nf (?) + (Ni ?Nt(?))Nc +Ni
. (6)
The CER is the number of errors incurred by the
CMs divided by the total number of constituents.
The CER can be compared with the Absolute
Constituent Error Rate (ACER), which is the CER
obtained assuming that all constituents are marked
as correct (the only possible assumption when CM
are not available):
ACER = CER(0) = NiNc +Ni
. (7)
4.2 Experimental Framework
Our experiments were carried out over the Wall
Street Journal Penn Treebank (PTB) manually an-
notated corpus. Three sets were defined over the
PTB: train (sections 2 to 21), test (section 23),
and development (the first 346 sentences of sec-
tion 24). Before carrying out experimentation, the
NoEmpties transformation was applied to all sets
(Klein and Manning, 2001).
We implemented the CYK-Viterbi parsing al-
gorithm as the parse engine within the IPP
framework. This algorithm uses grammars in
the Chomsky Normal Form (CNF) so we em-
ployed the open source Natural Language Toolkit2
(NLTK) to obtain several right-factored binary
2http://www.nltk.org/
1225
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(a) PCFG: h=0,v=1
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(b) PCFG: h=0,v=2
Figure 3: CER results over IPP system interaction. Threshold fixed at before the interactive process.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(a) PCFG: h=0,v=1
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
CE
R
Th
re
sh
ol
d
Interaction
Thr.
ACER
CER
(b) PCFG: h=0,v=2
Figure 4: CER results over IPP system interaction. Threshold optimized for each step of the interactive
process.
grammars with different markovization parame-
ters from the training set (Klein and Manning,
2003).
The purpose of our experimentation is to de-
termine if CMs can successfully discriminate er-
roneous constituents from correct ones within an
IPP process, that is, if they help the user to find
errors in a hastier manner. For this we need to
assess if there exists discriminant information in
the CMs corresponding to the constituents of the
unvalidated part of the successive IPP-proposed
trees.
With this objective in mind, we introduced a
CM calculation step after each user interaction
within the IPP process. CMs for all constituents in
each tree were obtained as described in section 3.
After each simulated interaction, we also calcu-
lated the ACER and CER over all the syntactic
constituents of the whole test set.
Each IPP user interaction yields a parse tree
which can be seen as the concatenation of two
parts: the validated prefix tree (which is known
to be correct because the user, or the user simula-
tion subsystem in this case, has already reviewed
it) and a new suffix tree which is calculated by
the IPP system based on the validated prefix, as
shown in section 2.
The fact that the validated prefix is already
known to be correct is taken into account by the
CM calculation process, and the confidence of the
constituents in the prefix tree is automatically set
to their maximum score, equal to 1. This fact
causes that the CMs become more discriminant
after each interaction, because a larger part of the
1226
tree (the prefix) has a completely correct confi-
dence. The key point here is to measure if this
increasingly reduced CER (CM error rate) main-
tains its advantage over the also increasingly re-
duced ACER (absolute constituent error rate with-
out taking CMs into account) which would mean
that the CMs retain their discriminant power and
can be useful as an aid for a human annotator us-
ing an IPP system.
Two batches of experiments were performed
and, in each of them, two different markovizations
of the vanilla PCFG were tested as the parsing
model.
In the first battery of experiments, the confi-
dence threshold ? was optimized over the devel-
opment set before starting the IPP process, re-
maining the same during the user interaction. The
results can be seen in Fig. 3, which shows the
obtained baseline ACER and the CER (the con-
fidence assessing metric) for the test set after each
user interaction. We see how CMs retain all of
their error detection capabilities during the IPP
process: in the h0v1 PCFG they are able to dis-
cern about 25% of incorrect constituents at most
stages of the IPP process, with a slight bump up to
27% after about 7 user interactions; for the h0v2
PCFG they are able to detect about 18% of incor-
rect constituents at the first interactions, but go up
to detect 27% of errors after about 7 or more in-
teractions.
In the second experimental setup, a different
threshold for each interaction step was calcu-
lated by performing the IPP user simulation pro-
cess over the development set and optimizing
the threshold value. The results can be seen in
Fig. 4. We observe improvements in the discrim-
inant ability of confidence values after 8 user in-
teractions, with them being capable to detect more
errors towards the end of each IPP session: about
34% of errors for h0v1, and 49% of them for h0v2.
The calculated thresholds have also been plot-
ted in the aforementioned figures. For the per-
interaction threshold experimentation, we can see
how the threshold gets fine-tuned as the IPP pro-
cess advances. The lower threshold values for the
last interactions were expected due to the fact that
more constituents have been validated and have
the maximum confidence. This method for pre-
calculating one specific threshold for each of the
iterations could be useful when incorporating CM
to a real IPP based annotator.
5 Conclusions and Future Work
We have proved that using Confidence Measures
can be used to discriminate incorrect constituents
from correct ones over an Interactive Predictive
Parsing process. We have show two methods
for calculating the threshold used to mark con-
stituents as correct/incorrect, showing the advan-
tage of precalculating a specific threshold for each
of the interaction steps.
Immediate future work involves implementing
CMs as a visual aid in a real IPP system like
the one presented in (Sa?nchez-Sa?ez et al, 2010).
Through he use of CMs, all constituents in the
successive trees could be color-coded according
to their correctness confidence, so the user could
focus and make corrections faster.
Future research paths can deal with applying
CMs to improve the output of completely auto-
matic parsers, for example, using them as a com-
ponent of an n-best re-ranking system.
Additionally, the IPP framework is also suit-
able for studying and applying training algorithms
within the Active Learning and Adaptative/Online
Parsing paradigms. This kind of systems could
improve their models at operating time, by incor-
porating new ground truth data as it is provided by
the user.
References
Baker, JK. 1979. Trainable grammars for speech
recognition. Journal of the Acoustical Society of
America, 65:132.
Barrachina, S., O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,
E. Vidal, and J.M. Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. Compu-
tational Linguistics, 35(1):3?28.
Bened??, J.M., J.A. Sa?nchez, and A. Sanch??s. 2007.
Confidence measures for stochastic parsing. In
Proc. of RANLP, pages 58?63, Borovets, Bulgaria,
27-29 September.
Carter, D. 1997. The TreeBanker. A tool for super-
vised training of parsed corpora. In Proc. of EN-
VGRAM Workshop, pages 9?15.
1227
Collins, M. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
de la Clergerie, E.V., O. Hamon, D. Mostefa, C. Ay-
ache, P. Paroubek, and A. Vilnat. 2008. Passage:
from French parser evaluation to large sized tree-
bank. Proc. of LREC, 100:2.
Hiroshi, I., N. Masaki, H. Taiichi, T. Takenobu, and
T. Hozumi. 2005. eBonsai: An integrated environ-
ment for annotating treebanks. In Proc. of IJCNLP,
pages 108?113.
Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
Klein, D. and C.D. Manning. 2001. Parsing with
treebank grammars: Empirical bounds, theoretical
models, and the structure of the Penn treebank. In
Proc. of ACL, pages 338?345, Morristown, USA.
ACL.
Klein, D. and C.D. Manning. 2003. Accurate unlex-
icalized parsing. In Proc. of ACL, volume 1, pages
423?430, Morristown, USA. ACL.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McClosky, D., E. Charniak, and M. Johnson. 2006.
Effective self-training for parsing. In Proc. of
NAACL-HLT, pages 152?159.
Oepen, S., D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods. Research on Lan-
guage & Computation, 2(4):575?596.
Petrov, S. 2010. Products of Random Latent Variable
Grammars. Proc. of NAACL-HLT.
Sa?nchez-Sa?ez, R., J.A. Sa?nchez, and J.M. Bened??.
2009a. Interactive predictive parsing. In Proc. of
IWPT?09, pages 222?225, Paris, France, October.
ACL.
Sa?nchez-Sa?ez, R., J.A. Sa?nchez, and J.M. Bened??.
2009b. Statistical confidence measures for proba-
bilistic parsing. In Proc. of RANLP, pages 388?392,
Borovets, Bulgaria, September.
Sa?nchez-Sa?ez, R., L.A. Leiva, J.A. Sa?nchez, and J.M.
Bened??. 2010. Interactive predictive parsing using
a web-based architecture. In Proc. of NAACL-HLT,
Los Angeles, United States of America, June.
Serrano, N., A. Sanchis, and A. Juan. 2010. Bal-
ancing error and supervision effort in interactive-
predictive handwriting recognition. In Proc. of IUI,
pages 373?376. ACM.
Tarazo?n, L., D. Pe?rez, N. Serrano, V. Alabau,
O. Ramos Terrades, A. Sanchis, and A. Juan. 2009.
Confidence Measures for Error Correction in Inter-
active Transcription of Handwritten Text. In Proc.
of ICIAP, pages 567?574, Vietri sul Mare, Italy,
September. LNCS.
Toselli, A.H., V. Romero, and E. Vidal. 2008. Com-
puter assisted transcription of text images and mul-
timodal interaction. In Proc. MLMI, volume 5237,
pages 296?308. Springer.
Ueffing, N. and H. Ney. 2007. Word-level confidence
estimation for machine translation. Computational
Linguistics, 33(1):9?40.
Vidal, E., F. Casacuberta, L. Rodr??guez, J. Civera, and
C. Mart??nez. 2006. Computer-assisted translation
using speech recognition. IEEE TASLP, 14(3):941?
951.
Wessel, F., R. Schluter, K. Macherey, and H. Ney.
2001. Confidence measures for large vocabu-
lary continuous speech recognition. IEEE TSAP,
9(3):288?298.
Yamamoto, R., S. Sako, T. Nishimoto, and
S. Sagayama. 2006. On-line recognition of
handwritten mathematical expressions based on
stroke-based stochastic context-free grammar. In
Proc of ICFHR, pages 249?254.
1228
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 37?40,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interactive Predictive Parsing using a Web-based Architecture?
Ricardo Sa?nchez-Sa?ez? Luis A. Leiva? Joan-Andreu Sa?nchez? Jose?-Miguel Bened???
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{rsanchez,luileito,jandreu,jbenedi}@{?dsic,?iti}.upv.es
Abstract
This paper introduces a Web-based demon-
stration of an interactive-predictive framework
for syntactic tree annotation, where the user is
tightly integrated into the interactive parsing
system. In contrast with the traditional post-
editing approach, both the user and the sys-
tem cooperate to generate error-free annotated
trees. User feedback is provided by means of
natural mouse gestures and keyboard strokes.
1 Introduction
There is a whole family of problems within the pars-
ing world where error-free results, in the form of
perfectly annotated trees, are needed. Constructing
error-free trees is a necessity in many tasks, such
as handwritten mathematical expression recognition
(Yamamoto et al, 2006), or new gold standard tree-
bank creation (de la Clergerie et al, 2008). It is
a fact that current state-of-the-art syntactic parsers
provide trees that, although of excellent quality, still
contain errors. Because of this, the figure of a human
corrector who supervises the annotation process is
unavoidable in this kind of problems.
When using automatic parsers as a baseline for
building perfect syntactic trees, the role of the hu-
man annotator is to post-edit the trees and correct the
errors. This manner of operating results in the typ-
ical two-step process for error correcting, in which
the system first generates the whole output and then
?Work partially supported by the Spanish MICINN under
the MIPRCV ?Consolider Ingenio 2010? (CSD2007-00018),
MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-
TEO/2009/014) research projects, and the FPU fellowship
AP2006-01363. The authors wish to thank Vicent Alabau for
his invaluable help with the CAT-API library.
the user verifies or amends it. This paradigm is
rather inefficient and uncomfortable for the human
annotator. For example, in the creation of the Penn
Treebank annotated corpus, a basic two-stage setup
was employed: a rudimentary parsing system pro-
vided a skeletal syntactic representation, which then
was manually corrected by human annotators (Mar-
cus et al, 1994). Other tree annotating tools within
the two-step paradigm exist, such as the TreeBanker
(Carter, 1997) or the Tree Editor TrEd1.
With the objective of reducing the user effort and
making this laborious task easier, we devised an In-
teractive Predictive framework. Our aim is to put
the user into the loop, embedding him as a part of
the automatic parser, and allowing him to interact in
real time within the system. In this manner, the sys-
tem can use the readily available user feedback to
make predictions about the parts that have not been
validated by the corrector.
In this paper, we present a Web-based demo,
which implements the Interactive Predictive Parsing
(IPP) framework presented in (Sa?nchez-Sa?ez et al,
2009). User feedback (provided by means of key-
board and mouse operations) allows the system to
predict new subtrees for unvalidated parts of the an-
notated sentence, which in turn reduces the human
effort and improves annotation efficiency.
As a back-end for our demo, we use a more pol-
ished version of the CAT-API library, the Web-based
Computer Assisted Tool introduced in (Alabau et al,
2009). This library allows for a clean application de-
sign, in which both the server side (the parsing en-
gine) and the client side (which draws the trees, cap-
tures and interprets the user feedback, and requests
1http://ufal.mff.cuni.cz/?pajas/tred/
37
(a) System: output tree 1 (b) User: span modification (c) System: output tree 2
Figure 1: An interaction example on the IPP system.
parsed subtrees to the server) are independent. One
of the features that steam from the CAT-API library
is the ability for several annotators to work concur-
rently on the same problem-set, each in a different
client computer sharing the same parsing server.
Interactive predictive methods have been success-
fully demonstrated to ease the work of transcrip-
tors and translators in fields like Handwriting Text
Recognition (Romero et al, 2009; Toselli et al,
2008) and Statistical Machine Translation (Ortiz et
al., 2010; Vidal et al, 2006). This new paradigm
enables the collaboration between annotators across
the globe, granting them a physical and geographical
freedom that was inconceivable in the past.
2 Interactive Predictive Parsing
A tree t, associated to a string x1|x|, is composed
by substructures that are usually referred as con-
stituents. A constituent cAij is defined by the non-
terminal symbol A (either a syntactic label or a POS
tag) and its span ij (the starting and ending indexes
which delimit the part of the input sentence encom-
passed by the constituent).
Here follows a general formulation for the non-
interactive parsing scenario. Using a grammatical
model G, the parser analyzes the input sentence x =
{x1, . . . , x|x|} and produces the parse tree t?
t? = arg max
t?T
pG(t|x), (1)
where pG(t|x) is the probability of parse tree t given
the input string x using model G, and T is the set of
all possible parse trees for x.
In the interactive predictive scenario, after obtain-
ing the (probably incorrect) best tree t?, the user is
able to individually correct any of its constituents
cAij . The system reacts to each of the corrections in-
troduced by the human, proposing a new t?? that takes
into account the afore-mentioned corrections.
The action of modifying an incorrect constituent
(either setting the correct span or the correct label)
implicitly validates a subtree that is composed by
the partially corrected constituent, all of its ancestor
constituents, and all constituents whose end span is
lower than the start span of the corrected constituent.
We will name this subtree the validated prefix tree
tp. When the user replaces the constituent cAij with
the correct one c?Aij , the validated prefix tree is:
tp(c?Aij ) = {cBmn : m ? i, n ? j,
d(cBmn) ? d(c?Aij )} ?
{cDpq : p >= 1 , q < i}
(2)
with d(cBmn) being the depth of constituent cBmn.
When a constituent correction is performed, the
prefix tree tp(c?Aij ) is fixed and a new tree t?? that takes
into account the prefix is proposed
t?? = arg max
t?T
pG(t|x, tp(c?Aij )). (3)
Given that we are working with context-free
grammars, the only subtree that effectively needs to
be recalculated is the one starting from the parent of
the corrected constituent.
3 Demo outline
A preview version of the demonstration can be ac-
cessed at http://cat.iti.upv.es/ipp/.
The user is presented with the sentences in the se-
lected corpus, and starts parsing them one by one.
They make corrections in the trees both with the key-
board and the computer mouse. The user feedback
38
is decoded on the client side which in turn requests
subtrees to the parse engine.
Two kind of operations can be performed over
constituents: span modification (performed either by
dragging a line from the constituent to the word that
corresponds to the span?s upper index, or deleting
a tree branch by clicking on it), and label substitu-
tion (done by typing the correct one on its text field).
Modifying the span of a constituent invalidates its
label, so the server recalculates it as part of the suf-
fix. Modifying the label of a constituent validates its
span.
When the user is about to perform an opera-
tion, the affected constituent and the prefix that will
be validated are highlighted. The target span of
the modified constituent is visually shown as well.
When the user obtains the correctly annotated tree,
they can accept it by by clicking on a new sentence.
As already mentioned, the user is tightly inte-
grated into the interactive parsing process. They fol-
low a predetermined protocol in which they correct
and/or validate the annotated parse trees:
1. The parsing server proposes a full parse tree t
for the input sentence. The tree t is shown to
the user by the client (Fig. 1a).
2. The user finds the first2 incorrect constituent c
and starts amending it, either by changing its
label or changing its span (Fig. 1b, note how
the label is greyed out as it is discarded with
the span modification). This operation implic-
itly validates the prefix tree tp (highlighted in
green).
3. The system decodes the user feedback (i.e.,
mouse gestures or keyboard strokes) which can
either affect the label or the span of the incor-
rect constituent c:
(a) If the span of c is modified, the label is
not assumed to be correct. A partial con-
stituent c?, which includes span but no la-
bel, is decoded from the user feedback.
(b) If the label of c is modified, the span is
assumed to be correct. The corrected con-
stituent c? is decoded from the user feed-
back.
2The tree visiting order is left-to-right depth-first.
This step only deals with analysing the user
feedback, the parsing server will not be con-
tacted until the next step.
4. Either the partially corrected constituent c? or
the corrected constituent c? is then used by the
client to create a new extended consolidated
prefix that combines the validated prefix and the
user feedback: either tpc? or tpc?. The client
sends the extended prefix tree to the parsing
server and requests a suitable continuation for
the parse tree, or tree suffix ts:
(a) If the extended prefix is partial (tpc?), the
first element of ts is the label completing
c?, followed by the remaining calculated
whole constituents.
(b) If the extended prefix is complete (tpc?),
the parsing server produces a suitable tree
suffix ts which contains the remaining cal-
culated whole constituents.
5. The client concatenates the suffix returned by
the server to the validated extended prefix, and
shows the whole tree to the client (Fig. 1c).
6. These previous steps are iterated until a final,
perfect parse tree is produced by the server and
validated by the user.
Note that within this protocol, constituents can be
deleted or inserted by adequately modifying the span
of the left-neighbouring constituent.
4 Demo architecture
The proposed system coordinates client-side script-
ing with server-side technologies, by using the CAT-
API library (Alabau et al, 2009).
4.1 Server side
The server side of our system is a parsing en-
gine based on a customized CYK-Viterbi parser,
which uses a Probabilistic Context-Free Grammar in
Chomsky Normal Form obtained from sections 2 to
21 of the UPenn Treebank as a model (see (Sa?nchez-
Sa?ez et al, 2009) for details).
The client can request to the parsing server the
best subtree for any given span of the input string.
For each requested subtree, the client can either pro-
vide the starting label or not. If the starting subtree
39
label is not provided, the server calculates the most
probable label. The server also performs transparent
tree debinarization/binarization when communicat-
ing with the client.
4.2 Client side
The client side has been designed taking into ac-
count ergonomic issues in order to facilitate the in-
teraction.
The prototype is accessed through a Web browser,
and the only requirement is the Flash plugin (98% of
market penetration) installed in the client machine.
The hardware requirements in the client are very
low on the client side, as the parsing is process per-
formed remotely on the server side: any computer
(including netbooks) capable of running a modern
Web browser is enough.
Each validated user interaction is saved as a log
file on the server side, so a tree?s annotation session
can be later resumed.
4.2.1 Communication protocol
This demo exploits the WWW to enable the con-
nection of simultaneous accesses across the globe.
This architecture also provides cross-platform com-
patibility and requires neither computational power
nor disk space on the client?s machine.
Client and server communicate via asynchronous
HTTP connections, providing thus a richer interac-
tive experience ? no page refreshes is required when
parsing a new sentence. Moreover, the Web client
communicates with the IPP engine through binary
TCP sockets. Thus, response times are quite slow ? a
desired requirement for the user?s solace. Addition-
ally, cross-domain requests are possible, so the user
could switch between different IPP engines within
the same UI.
5 Evaluation results
We have carried out experiments that simulate user
interaction using section 23 of the Penn Treebank.
The results suggest figures ranging from 42% to
46% of effort saving compared to manually post-
editing the trees without an interactive system. In
other words, for every 100 erroneous constituents
produced by a parsing system, an IPP user would
correct only 58 (the other 42 constituents being au-
tomatically recalculated by the IPP system). Again,
see (Sa?nchez-Sa?ez et al, 2009) for the details on ex-
perimentation.
5.1 Conclusions and future work
We have introduced a Web-based interactive-
predictive system that, by using a parse engine in
an integrated manner, aids the user in creating cor-
rectly annotated syntactic trees. Our system greatly
reduces the human effort required for this task com-
pared to using a non-interactive automatic system.
Future work includes improvements to the client
side (e.g., confidence measures as a visual aid, mul-
timodality), as well as exploring other kinds of pars-
ing algorithms for the server side (e.g., adaptative
parsing).
References
V. Alabau, D. Ortiz, V. Romero, and J. Ocampo. 2009. A
multimodal predictive-interactive application for com-
puter assisted transcription and translation. In ICMI-
MLMI ?09, 227?228.
D. Carter. 1997. The TreeBanker. A tool for supervised
training of parsed corpora. In ENVGRAM?97, 9?15.
E.V. de la Clergerie, O. Hamon, D. Mostefa, C. Ayache,
P. Paroubek, and A. Vilnat. 2008. Passage: from
French parser evaluation to large sized treebank. In
LREC?08, 100:P2.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
D. Ortiz, L. A. Leiva, V. Alabau, and F. Casacuberta.
2010. Interactive machine translation using a web-
based architecture. In IUI?10, 423?425.
V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.
2009. Interactive multimodal transcription of text
imagse using a web-based demo system. In IUI?09,
477?478.
R. Sa?nchez-Sa?ez, J.A. Sa?nchez, and J.M. Bened??. 2009.
Interactive predictive parsing. In IWPT?09, 222?225.
A.H. Toselli, V. Romero, and E. Vidal. 2008. Computer
assisted transcription of text images and multimodal
interaction. In MLMI?08, 5237: 296?308.
E. Vidal, F. Casacuberta, L. Rodr??guez, J. Civera, and
C. Mart??nez. 2006. Computer-assisted translation us-
ing speech recognition. IEEE Trans. on Audio, Speech
and Language Processing, 14(3):941?951.
R. Yamamoto, S. Sako, T. Nishimoto, and S. Sagayama.
2006. On-line recognition of handwritten mathe-
matical expressions based on stroke-based stochastic
context-free grammar. In 10th Frontiers in Handwrit-
ing Recognition, 249?254.
40
