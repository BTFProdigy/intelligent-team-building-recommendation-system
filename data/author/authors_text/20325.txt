Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 503?507,
Dublin, Ireland, August 23-24, 2014.
SA-UZH: Verb-based Sentiment Analysis
Nora Hollenstein, Michi Amsler, Martina Bachmann, Manfred Klenner
Institute of Computational Linguistics, University of Zurich
Binzmuehlestrasse 14, CH-8050 Zurich, Switzerland
{hollenstein,mamsler,bachmann,klenner}@ifi.uzh.ch
Abstract
This paper describes the details of our
system submitted to the SemEval-2014
shared task about aspect-based sentiment
analysis on review texts. We participated
in subtask 2 (prediction of the polarity
of aspect terms) and 4 (prediction of the
polarity of aspect categories). Our ap-
proach to determine the sentiment of as-
pect terms and categories is based on lin-
guistic preprocessing, including a com-
positional analysis and a verb resource,
task-specific feature engineering and su-
pervised machine learning techniques. We
used a Logistic Regression classifier to
make predictions, which were ranked
above-average in the shared task.
1 Introduction
Aspect-based sentiment analysis refers to the
problem of predicting the polarity of an explicit
or implicit mention of a target in a sentence or
text. The SemEval-2014 shared task required sen-
timent analysis of laptop and restaurant reviews
on sentence level and comprised four subtasks
(Pontiki et al., 2014). The organizers created and
shared manually labelled domain-specific training
and test data sets. Two of the four subtasks dealt
with determining the sentiment of a given aspect
term (explicitly mentioned) or aspect category (ex-
plicitly or implicitly mentioned) in a sentence.
The subtasks we participated in do not include the
recognition of aspects. Given the sentence ?The
sushi rolls were perfect, but overall it was too ex-
pensive.?, ?sushi rolls? is an aspect term, and the
corresponding aspect categories are ?food? and
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
?price?. The correct predictions would be the fol-
lowing:
? Subtask 2 (aspect terms): {sushi rolls? pos-
itive}
? Subtask 4 (aspect categories): {food ? posi-
tive, price ? negative}
To solve these tasks, we introduce a Logis-
tic Regression Model for target-specific sentiment
analysis. Features are derived from a fine-grained
polarity lexicon, a verb resource specifying expec-
tations and effects of the verbs functional roles,
and a compositional analysis. In our experiments
on the restaurant and laptop reviews data for the
SemEval-2014 shared task, we found that im-
provements over the baseline are possible for all
classes except ?conflict?.
2 Related Work
We focus on the question whether fine-grained lin-
guistic sentiment analysis improves target-specific
polarity classification. Existing approaches to
aspect-based sentiment detection have focused on
different aspects of this task, e.g. the identifi-
cation of targets and their components (Popescu
and Etzioni, 2005) and sentence-level composition
(Moilanen and Pulman, 2007). Ding et al. (2008)
and Hu and Liu (2004) produced lexicon-based
approaches, which perform quite well in a large
number of domains, and Blair-Goldensohn et al.
(2008) combined lexicon-based methods and su-
pervised learning. Jiang et al. (2011) used a depen-
dency parser to generate a set of aspect dependent
features for classification. For our system we built
a sentiment composition resembling the one of
L?aubli et al. (2012), which was developed for Ger-
man. Moreover, our verb resource has some simi-
larity with the one of Neviarouskaya et al. (2009):
both rely on verb classes and utilize verb-specific
503
behavior. However, only we specify the individ-
ual verb?s (default) perspective on each role (and
are, thus, able to count polar propagations). See
also Reschke and Anand (2011), who describe in
detail how polar (verb) complements combine to
verb frame polarity (again without recording and
using role perspectives as we do).
3 System Description
In this section we present the details of our senti-
ment analysis system. We used the same prepro-
cessing and learning algorithm for both subtasks
(2 & 4). Only the feature extraction was expanded
in subtask 4 for determining the polarities of as-
pect categories (see section 3.3). The data sets
consisted of restaurant and laptop reviews, which
comprise about 3?000 manually classified target-
specific sentences for each domain.
3.1 Sentiment Composition
The fundamental steps of our sentiment analysis
system are parsing the sentences, rule-based sen-
timent analysis using a polarity lexicon and a verb
resource, feature extraction and training a machine
learning algorithm. In this section we will de-
scribe the composition of the lexicon as well as the
structure of the sentiment composition pipeline.
Category Example
POS strong ?awesome?
POS weak ?adequate?
NEG strong ?catastrophe?
NEG weak ?demotivated?
POS active ?generous?
POS passive ?noteworthy?
NEG active ?rebellion?
NEG passive ?orphaned?
Table 1: Additional categories in our fine-grained
polarity lexicon
The same polarity lexicon was used for both
domains. After mapping the polarities from the
lexicon to the words and multi-word expressions,
we calculated the polarity of nominal (NPs) and
prepositional phrases (PPs) by means of lexical
marking and the syntactic analysis of a depen-
dency parser (Choi and Palmer, 2011). We did not
implement any rules for neutral phrases, all words
and phrases not marked as positive or negative are
considered as neutral. In general, the polarities are
propagated bottom-up to their respective heads of
the NPs/PPs in composition with the subordinates.
Shifters and negation words are also taken into ac-
count. The parser output is converted into a con-
straint grammar (CG) format for the subsequent
analysis of words and phrases. To conduct this
composition of polarity for the phrases we imple-
mented a CG with the vislcg3 tools (VISL-group,
2013). The next stage of our sentiment detection
is the verb resource, which was also implemented
with the vislcg3 tools and will be explained in the
next section.
3.2 Verb-based Sentiment Analysis
In order to combine the composition of the po-
lar phrases with verb information, we encoded the
impact of the verbs on polarity using three di-
mensions: effects, expectations and verb polarity.
While effects should be understood as the outcome
instantiated through the verb, expectations can be
understood as anticipated polarities induced by the
verb. Effects and expectations are assigned to sub-
jects or objects, not to the verb itself. A positive
or negative verb effect propagates from the verb to
a subject or object if the latter receives the polar-
ity of the verb. For a verb expectation, the subject
or object is expected to be polar and thus receives
a polarity even if the sentiment composition re-
sulted neutral (see examples below). The verb po-
larity as such is the evaluation of the whole verbal
phrase. Moreover, we process predicative and pas-
sive verbs, adapting the effects and expectations to
the syntactic structure.
Since these effects and expectations match di-
rectly to the subject and objects of a sentence,
they are of great use detecting the polarity of as-
pect terms (which are predominantly subjects or
objects). We present the following examples ex-
tracted from the training data to illustrate three di-
mensions annotated by the verb analysis:
? Example of a positive effect on the direct ob-
ject of a sentence induced by the verb: ?I
love (verb POS) the operating system and the
preloaded software (POS EFF).?
? Example for a negative expectation on a
prepositional object induced by the verb:
?[...] the guy, who constantly com-
plains (verb NEG) about the noise level
(NEG EXP).?
? Example of positive predicative effects
with an auxiliary, non-polar verb: ?Ser-
504
vice (POS predicative) is (verb PRED) great,
takeout (POS predicative) is (verb PRED)
good too.?
Furthermore, we make a distinction between the
different prepositions a verb can invoke and the
succeeding semantic changes. For example, the
verb ?to die? can be annotated in three different
manners, depending on the prepositional object:
1. ?My phone died (verb NEG).?
2. ?Their pizza (POS EFF) is to die (verb POS)
for.?
3. ?He died (verb NEG) of cancer
(NEG EXP).?
To summarize, in addition to verb polarity, we
introduce effects and expectations to verb frames,
which are determined through the syntactic pattern
found, the bottom-up calculated phrase polarities
and the meaning of the verb itself. We manually
categorized approx. 300 of the most frequent pos-
itive and negative English verbs and their respec-
tive verb frames.
Laptop reviews
Feature Occurrences in %
Verbs effects 367 12.05
Verb expectations 6 0.02
Predicatives 298 9.78
Polar verbs 530 17.39
Restaurant reviews
Feature Occurrences in %
Verbs effects 246 8.09
Verb expectations 12 0.04
Predicatives 378 12.43
Polar verbs 521 17.13
Table 2: Occurrences and percentage of sentences
of annotated polar verb features in the training data
of the shared task
In table 2, we illustrate the relevance of the lin-
guistic features of this verb resource by showing in
how many sentences of the training set these anno-
tations appear. Since we merely annotated the verb
frames of the most frequent English verbs, it is
conceivable that this resource may have a consid-
erably greater effect if more domain-specific verbs
are modelled.
After this final sentiment composition step, all
derived polarity chunks are converted into a set of
features for machine learning algorithms.
3.3 Feature Extraction
In a first step of our system, the sentences are
parsed, phrase polarities are calculated and verb
effects and expectations are assigned. Subse-
quently, a feature extractor, which extracts and ag-
gregates polar information, operates on the out-
put. The Simple Logistic Regression classifier
from weka Hall et al. (2009) is then trained on
these features.
We developed a feature extraction pipeline that
retrieves information about various polarity levels
in words, syntactic functions and phrases of the
sentences in the data set. In order to use our senti-
ment composition approach for machine learning,
we extract three different sets of features, result-
ing in a total of 32 features for subtask 2 and 39
features for subtask 4.
In short, the feature sets are constructed as fol-
lows:
? Lexicon-based features: These features com-
prise simple frequency counts of positive and
negative words in the sentences and binary
features showing whether any positive or
negative, strong or active tokens are present
at all. Furthermore, these features not only
include absolute counts but also token ratios.
? Composition-based features: This feature set
describes the information found in nomi-
nal, prepositional and verbal phrases, such
as the number of positive/negative phrase
heads or predicative verb effects found. It
is also possible to distinguish between fea-
tures which represent frequency counts and
features which represent polarity ratios.
? Target-specific features: This set includes
features from the previous two sets in con-
nection with the aspect terms, e.g. whether
the aspect term has a verb expectation or
whether the aspect term is the head of a neg-
ative/positive phrase, the subject or direct ob-
ject, etc. In this set we also include accu-
mulative features that represent the complete
amount of polar information in connection
with an aspect term.
? (only for subtask 4) Category-specific fea-
tures: These features are based on a co-
occurrence analysis of the most frequent
words used in each category. That is to
505
say, we calculated the frequencies of all po-
lar nouns, verbs and adjectives that appear in
sentences of the same category in order to
find category-specific words which have an
influence on the polarity. This set includes
features such as the number of category-
specific words occurring in the sentence, etc.
For the classification of the aspect terms and
categories of the sentences into the four classes
(positive, negative, neutral and conflict), we
trained a Simple Logistic Regression classifier on
the features described above. We also explored
other machine learning algorithms such as SVMs
and artificial neural networks, however, the Logis-
tic Regression proved to yield the best results.
4 Results & Discussion
In this section we present and discuss the results
of our system in the SemEval 2014 shared task.
The results of our submission for subtasks 2 and 4,
compared to the majority baselines, can be found
in table 3. Our system performs significantly bet-
ter on restaurant reviews than on laptop reviews,
probably due to the fact that our polarity lexi-
con comprises more restaurant-specific vocabu-
lary than computer-specific vocabulary.
Subtask Data Baseline Acc.
(2) Laptops 47.06 58.30
(2) Restaurants 57.8 70.98
(4) Restaurants 59.84 73.10
Table 3: Shared-Task results for subtask 2 (aspect
term polarity) and subtask 4 (aspect category po-
larity)
In both subtasks, calculating the polarity of the
aspect terms and the aspect categories, the class
positive scores better than the three other classes.
In all data sets and all subtasks positive was the
majority class of the four-partite classification:
42% in the aspect terms of the laptop reviews, 59%
in the aspect terms and aspect categories of the
laptop reviews equally (measured in the training
data). Thus, it is not surprising that the most fre-
quent error of our system is to categorize neutral
aspect terms and categories as positive.
We do not achieve any improvements for the
class conflict. The latter is very hard to detect, not
only because this class is difficult to define but also
because of the lack of training data given for this
class. This could not be improved even though
we included lexical features to address this par-
ticular class, for example, Boolean features show-
ing whether an adversative conjunction is present
in the sentence or whether the count of positive
chunks equals the count of negative chunks in the
same sentence. These features are in line with
the theory that aspects are considered controver-
sial if positive and negative occurrences are bal-
anced and no polarity clearly prevails. Further-
more, the conflictive facet of a sentence is fre-
quently not represented in the words (e.g. ?It has
no camera, but I can always buy and install one
easy.?; camera = conflict). Thus, it becomes chal-
lenging to generate features for this class conflict
with a lexicon-based approach.
Furthermore, since our verb resource was newly
implemented, there are still many verbs (espe-
cially domain-specific verbs) which will have to
be modelled in addition to the most frequent En-
glish verbs included in the analysis by now. An-
other limitation of our current system is the fact
that verb negation is not yet implemented: We
process negation occurring in noun phrases (e.g.
?a not so tasty chicken curry?), but not when the
negation word relates to the verb (e.g. ?we didn?t
complain?).
In summary, our aspect-based sentiment anal-
ysis pipeline takes into consideration many lin-
guistic characteristics relevant for detecting opin-
ion, and still provides the possibility to expand our
compositional resources.
5 Conclusion
Given the above-average results obtained in the
shared task system ranking, we conclude that the
method for aspect-based sentiment analysis in re-
view texts presented in this paper yields competi-
tive results. We showed that the performance for
this task can be improved by using linguistically
motivated features for all classes except conflict.
We presented a supervised aspect-based senti-
ment analysis system to detect target-specific po-
larity with features derived from a fine-grained po-
larity lexicon, a verb resource and compositional
analysis based on a dependency parser. Our results
have shown that deeper linguistic analysis can pos-
itively influence the detection of target-specific
polarities on sentence level in review texts.
506
Acknowledgements
We would like to thank the organizers of the
shared task for their effort, as well as the reviewers
for their helpful comments on the paper.
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan
McDonald, Tyler Neylon, George A. Reis, , and
Jeff Reynar. Building a sentiment summarizer
for local service reviews. In WWW Workshop on
NLP in the Information Explosion Era, 2008.
Jinho D. Choi and Martha Palmer. Getting the
most out of transition-based dependency pars-
ing. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguis-
tics: Human Language Technologies, HLT ?11,
pages 687?692, Stroudsburg, PA, USA, 2011.
ACL.
Xiaowen Ding, Bing Liu, and Philip S. Yu. A
holistic lexicon-based approach to opinion min-
ing. In Proceedings of the 2008 International
Conference on Web Search and Data Mining,
WSDM ?08, pages 231?240, New York, NY,
USA, 2008. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H.
Witten. The weka data mining software: An
update. SIGKDD Explor. Newsl., 11(1):10?18,
November 2009.
Minqing Hu and Bing Liu. Mining and sum-
marizing customer reviews. In Proceedings of
the Tenth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Min-
ing, KDD ?04, pages 168?177, New York, NY,
USA, 2004. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu,
and Tiejun Zhao. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 151?160. Associa-
tion for Computational Linguistics, 2011.
Samuel L?aubli, Mario Schranz, Urs Christen, and
Manfred Klenner. Sentiment Analysis for Me-
dia Reputation Research. In Proceedings of
KONVENS 2012 (PATHOS 2012 workshop),
pages 274?281, Vienna, Austria, 2012.
Karo Moilanen and Stephen Pulman. Sentiment
composition. In Proceedings of RANLP-2007,
pages 378?382, Borovets, Bulgaria, 2007.
Alena Neviarouskaya, Helmut Prendinger, and
Mitsuru Ishizuka. Semantically distinct verb
classes involved in sentiment analysis. IADIS
AC (1), 2009.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos,
and Suresh Manandhar. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. In Proceed-
ings of the 8th International Workshop on Se-
mantic Evaluation (SemEval 2014), Dublin, Ire-
land, 2014.
Ana-Maria Popescu and Oren Etzioni. Extraction
of product features and opinions from reviews.
In Proceedings of HLT-EMNLP-05, pages 339?
349, Vancouver, Canada, 2005.
Kevin Reschke and Pranav Anand. Extracting
contextual evaluativity. In Proceedings of the
Ninth International Conference on Computa-
tional Semantics, pages 370?374, 2011.
VISL-group. http://beta.visl.sdu.dk/cg3.html. In-
stitute of Language and Communication (ISK),
University of Southern Denmark, 2013.
507
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 18?23,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Inducing Domain-specific Noun Polarity Guided by Domain-independent
Polarity Preferences of Adjectives
Manfred Klenner
Computational Linguistics
University of Zurich
Switzerland
klenner@cl.uzh.ch
Michael Amsler
Computational Linguistics
University of Zurich
Switzerland
mamsler@ifi.uzh.ch
Nora Hollenstein
Computational Linguistics
University of Zurich
Switzerland
hollenstein@ifi.uzh.ch
Abstract
In this paper, we discuss how domain-
specific noun polarity lexicons can be in-
duced. We focus on the generation of
good candidates and compare two ma-
chine learning scenarios in order to estab-
lish an approach that produces high pre-
cision. Candidates are generated on the
basis of polarity preferences of adjectives
derived from a large domain-independent
corpus. The polarity preference of a word,
here an adjective, reflects the distribution
of positive, negative and neutral arguments
the word takes (here: its nominal head).
Given a noun modified by some adjectives,
a vote among the polarity preferences of
these adjectives establishes a good indica-
tor of the polarity of the noun. In our ex-
periments with five domains, we achieved
f-measure of 59% up to 88% on the basis
of two machine learning approaches car-
ried out on top of the preference votes.
1 Introduction
Polarity lexicons are crucial for fine-grained sen-
timent analysis. For instance, in approaches
carrying out sentiment composition (Moilanen
and Pulman, 2007), where phrase-level polar-
ity is composed out of word level polarity (e.g.
disappointed
?
hope
+
? NP
?
). However, often
freely available lexicons are domain-independent,
which is a problem with domain-specific texts,
since lexical gaps reduce composition anchors.
But how many domain-specific words do we have
to expect? Is it a real or rather a marginal problem?
In our experiments, we found that domain-specific
nouns do occur quite often - so they do matter. In
one of our domains, we identified about 1000 neg-
ative nouns, 409 were domain-specific. In that do-
main, the finance sector, more than 13?000 noun
types exist that do not occur at all in the DeWac
corpus - a large Web corpus (in German) with over
90 Million sentences. Thus, most of them must
be regarded as domain-specific. It would be quite
time-consuming to go through all of them in order
to identify and annotate the polar ones. Could we,
rather, predict good candidates? We would need
polarity predictors - words that take other, polar
words e.g. as their heads. If they, moreover, had
a clear-cut preference, i.e. they mostly took one
kind of polar words, say negative, then they were
perfect predictors of the polarity of nouns. We
found that adjectives (e.g. acute) can be used as
such polarity predictors (e.g. acute mostly takes
negative nouns, denoted n
?
, e.g. acute pain)).
Our hypothesis is that the polarity prefer-
ences of adjectives are (more or less) domain-
independent. We can learn the preferences from
domain-independent texts and apply it to domain-
specific texts and get good candidates of domain-
specific polar nouns. Clearly, if the polarity pref-
erences of an adjective are balanced (0.33 for each
polarity), than the predictions could not help at all.
But if one polarity clearly prevails, we might even
get a good performance by just classifying the po-
larity of unknown nouns in a domain according to
the dominant polarity preference of the adjectives
they co-occur with.
In this paper, we show how to generate
such a preference model on the basis of a
large, domain-independent German corpus and
a domain-independent German polarity lexicon.
We use this model to generate candidate nouns
from five domain-specific text collections - rang-
ing from 3?200 up to 37?000 texts per domain.
In order to see how far an automatic induction
of a domain-specific noun lexicon could go, we
also experimented with machine learning scenar-
ios on the output of the baseline system. We ex-
perimented with a distributional feature setting on
the basis of unigrams and used the Maximum En-
18
tropy learner, Megam (Daum?e III, 2004), to learn
a classifier. We also worked with Weka (Frank et
al., 2010) and features derived from the German
polarity lexicon. Both approaches yield significant
gains in terms of precision - so they realize a high-
precision scenario.
2 Inducing the Preference Model
We seek to identify adjectives which impose a
clear-cut polar preference on their head nouns.
The polarity preference of an adjective reflects the
distribution of positive, negative and neutral nouns
the adjective modifies given to some text corpus.
We used the domain-independent DeWac corpus
(Baroni M., 2009) comprising about 90 million
German sentences. We selected those adjectives
that frequently co-occurred with polar nouns from
PoLex, a freely available German polarity lexicon
(Clematide and Klenner, 2010). Since the original
polarity lexicon contained no neutral nouns, we
first identified 2100 neutral nouns and expanded
the lexicon
1
. Altogether 5?500 nouns were avail-
able, 2100 neutral, 2100 negative and 1250 pos-
itive. For each adjective, we counted how often
it took (i.e. modified) positive, negative or neu-
tral nouns in the DeWac corpus and determined
their polarity preferences for each class (positive,
negative and neutral). This way, 28?500 adjec-
tives got a probability distribution, most of them,
however, with a dominating neutral polarity pref-
erence. Two lexicons were derived from it: a pos-
itive and a negative polarity preference lexicon.
An adjective obeys a polar polarity preference if
the sum of its positive and negative polarity pref-
erences is higher than its neutral preference. If the
positive preference is higher than the negative, the
adjective is a positive polarity predictor, otherwise
it is a negative polarity predictor. This procedure
leaves us with 506 adjectives, 401 negative polar-
ity predictors and 105 positive polarity predictors.
Figure 1 shows some examples of negative polar-
ity predictors. It reveals that, for instance, the ad-
jective akut (acute) is mostly coupled with neg-
ative nouns (61.50%). Nouns not in PoLex that
co-occur with an adjective are not considered. We
assume that these unknown nouns of an adjective
follow the same distribution that we are sampling
from the known co-occurring nouns. Note that po-
1
We searched for nouns that frequently co-occurred with
the same adjectives the polar nouns from the polarity lexicon
did and stopped annotating when we reached 2?100 neutral
nouns.
larity predictors not necessarily must have a prior
polarity itself. Actually, only 3 of the 12 adjectives
from Figure 1 do have a prior polarity (indicated as
n
?
). For instance, the adjective pl?otzlich (immedi-
ate) is not polar but has a negative polarity pref-
erence. The polarity preference of a word is not
useful in composition, it just reveals the empirical
(polar) context of the word. If, however, the polar-
ity of the context word is unknown, the preference
might license an informed polarity guess.
adjective English POS NEG #n
?
arg
?
bad/very 02.65 55.14 301
heftig intensive 07.73 48.77 814
v?ollig total 25.79 42.43 787
akut acute 06.27 61.50 478
latent latent 07.96 47.76 402
ziemlich rather 14.16 52.36 233
drohend
?
threatening 35.1 52.54 824
pl?otzlich immediate 17.78 41.82 703
gravierend grave 04.5 48.5 400
chronisch chronic 03.26 72.11 398
schleichend subtle 03.76 52.97 319
hemmunglos
?
unscrupulous 15.49 43.19 213
Figure 1: Negative Polarity Predictors
Here is the formula for the estimation of the
negative polarity preference as given in Figure 1
(n
?
denotes a negative noun from PoLex, a
j
an
adjective modifying an instance of n
?
)
2
:
prefn
?
(a
j
) =
#a
j
n
?
#a
+,?,=
j
Note that we count the number of adj-noun
types (#a
j
n
?
), not tokens. #a
+,?,=
j
is the num-
ber of adj-noun types of the adjective a
j
for all
classes: positive (+), negative(-) and neutral (=).
Figure 2 gives examples of positive polarity pre-
dictors with some of their nouns.
German English POS
ungetr?ubt unclouded joy
unbeirrbar unerring hope
?uberstr?omend overwhelming happiness
bewunderswert mirable competence
falschverstanden falsely-understood tolerance
wiedergewonnen regained freedom
Figure 2: Positive Polarity Predictors
3 Applying the Preference Model
We applied the preference model to texts from five
domains: banks (37?346 texts), transport (3221),
2
This could be interpreted as the conditional probability
of a negative noun given the adjective.
19
insurance (4768), politics (3208) and pharma
(4790). These texts have been manually classified
over the last 15 years by an institute carrying out
media monitoring
3
, not only wrt. their domain,
but also wrt. target-specific polarity (we just use
the domain annotation, currently).
The polarity of a noun is predicted by the vote
of the adjectives it occurred with. The following
formula shows the polarity prediction pol
+,?,=
for
the class negative (pol
?
):
pol
?
(n
i
) = A
i
?
?
a
j
?PM
?
??(a
j
,n
i
)
prefn
?
(a
j
)
A
i
is the number of adjectives that modify the
noun n
i
in the domain-specific texts. PM
?
is the
set of adjectives from the polarity model (PM )
with a negative polarity preference and (a
j
, n
i
) is
true, if the adjective a
j
modifies the noun n
i
ac-
cording to the domain-specific documents.
4 Improving the Predictions
The preference model serves two purposes: it gen-
erates a list of candidates for polar nouns and it
establishes a baseline. We experimented with two
feature settings in order to find out whether we
could improve on these results.
In the first setting, the WK setting, we wanted to
exploit the fact that for some adjectives that mod-
ify a noun, we know their prior polarity (from the
polarity lexicon). These adjectives do not nec-
essarily have a clear positive or negative polarity
preference. If not, then they are not used in the
prediction of the noun polarity.
But could the co-occurrence of a noun with ad-
jectives bearing a prior polarity also be indicative
of the noun polarity? For instance, if a noun is
coupled frequently and exclusively with negative
adjectives. Does this indicate something? Ones
intuition might mislead, but a machine learning
approach could reveal correlations. We used Sim-
ple Logistic Regression (SRL) from Weka and the
following features:
1. the number of positive adjectives with a prior
polarity that modify the noun
2. the number of negative adjectives with a prior
polarity that modify the noun
3
We would like to thank the f?og institute (cf.
www.foeg.uzh.ch/) for these data (mainly newspaper texts in
German).
3. the difference between 1) and 2): absolute
and ratio
4. the ratio of positive and negative adjectives
5. two binary features indicating the majority
class
6. three features for the output of the prefer-
ence model: the positive, negative and neu-
tral scores: pol
?
, pol
+
, pol
=
, respectively.
In the second setting, the MG setting, we trained
Megam, a Maximum Entropy learner, among the
following lines: we took all polar nouns from
PoLex and extracted from the DeWac corpus all
sentences containing these nouns. For each noun,
all (context) words (nouns, adjectives, verbs) co-
occurring with it in these sentences are used as
bag of words training vectors. In other words, we
learned a tri-partite classifier to predict the polarity
class (positive, negative or neutral) given a target
noun and its context, i.e. those nouns co-occurring
with it in a text collection.
5 Experiments
The goal of our experiments were the prediction
of positive and negative domain-specific nouns in
five domains. We used our preference model to
generate candidates. Then we manually annotated
the results in order to obtain a domain-specific
gold standard. We evaluated the output of the
preference model relative to the new gold stan-
dards and we run our experiments with Megam
and Weka?s Simple Logistic Regression (SRL).
Megam and Weka?s SLR were trained on the basis
of the positive, negative and neutral nouns from
PoLex and the DeWac corpus.
Figure 3 shows the results. #PM gives the num-
ber of nouns predicted by the preference model
to be negative (e.g. 220 in the politics domain).
These are the nouns we annotated for polarity
and that formed our gold standard afterwards (e.g.
75.90 out of 110 predicted are true negative nouns
and are kept as the gold standard). Since the gen-
eration of the gold standard is based on the prefer-
ence model?s output, its recall is 1. We cannot fix
the real recall since this would require to manu-
ally classify all nouns occurring in those texts (e.g.
13?000 in the banks domain). However, since we
wanted to compare the machine learning perfor-
mance with the preference model, we had to mea-
20
ID domain texts #PM prec f #WK prec rec f #MG prec rec f
D1 politics 3208 220 75.90 86.29 195 78.97 92.22 83.26 130 81.54 63.48 69.13
D2 transport 3221 141 71.63 83.47 127 73.22 92.07 80.57 64 78.12 49.50 58.54
D3 insurance 4768 255 76.86 86.91 238 78.57 95.40 85.13 155 79.35 62.75 69.09
D4 pharma 4790 257 71.59 83.44 228 76.75 95.11 81.69 137 87.83 65.40 68.35
D5 banks 37346 1013 70.38 88.02 825 77.84 90.07 79.02 437 81.23 49.78 58.32
Figure 3: Prediction of Negative Nouns
sure recall, otherwise we could not determine the
overall performance.
From Figure 3 we can see that the preference
model (PM) performs best in terms of f-measure
(in bold). Of course, recall (i.e. 1, not shown) is
idealized, since we took the output of the prefer-
ence model to generate the gold standard. Note
however that this was our premise, that we needed
an approach that delivers good candidates, other-
wise we were lost given the vast amount of can-
didate nouns (e.g. remember the 13?000 nouns in
the finance sector).
German English
Wertverminderung impairment of assets
Stagflation stagflation
Geldschwemme money glut
?
Uberhitzungssymptom overheating symptom
Hyperinflation hyperinflation
Euroschw?ache weakness of the euro
Werterosion erosion in value
Nachfrage?uberhang surplus in demand
Margendruck pressure on margins
Klumpenrisiko cluster risk
Virus virus
Handekzem hand eczema
Schweinegrippe swine flu
Geb?armutterriss ruptured uterus
Alzheimer Alzheimer
Sehst?orung defective eye sight
Tinnitus tinnitus
Figure 4: Domain-specific Negative Nouns
Figure 4 shows examples of negative nouns
from two domains: banks and pharma. But: are all
found nouns domain-specific negative nouns? In
the bank domain, we have manually annotated for
domain specificity: out of 1013 nouns predicted
to be negative by the model, 409 actually were
domain-specific (40.3 %)
4
. The other nouns could
also be in a domain-independent polarity lexicon.
Now, we turn to the prediction of positive
domain-specific nouns. It is not really surpris-
ing that the preference model is unbalanced - that
there are far more negative than positive polarity
predictors: 401 compared to 105. PoLex, the pool
4
51 of the 131 (38.93%) as positive classified nouns actu-
ally were domain-specific.
of nouns used for learning of the polarity prefer-
ences already is unbalanced (2100 negative com-
pared to 1250 positive nouns). Also, the major-
ity of the texts in our five domains are negative
(all texts are annotated for document-level polar-
ity). It is obvious then that our model is better
in the prediction of negative than positive polarity.
Actually, our base model comprising 105 positive
polarity predictors does not trigger often within
the whole corpus. For instance, only 10 predic-
tions were made in the banks domain, despite the
37?346 texts. Clearly, newspaper texts often are
critical and thus more negative than positive vo-
cabulary is used. This explains the very low recall.
However, what if we relaxed our model? If we,
for example, keep those adjectives in our model
that have a positive polarity preference > 0.35, at
least 35 out of 100 nouns co-occurring with those
adjectives should be positive.
ID #1 prec #2 prec #3 prec #4 prec
D1 18 66.6 25 60.0 25 60.0 8 50
D2 14 85.7 16 75.0 0 0 3 33.3
D3 13 69.2 15 60.0 5 100 1 100
D4 13 84.6 15 80.0 9 55.5 2 100
D5 135 76.2 174 71.2 58 87.9 40 82.5
Figure 5: Prediction of Positive Nouns
We report the results of two runs. The first one,
labelled #1, where adjectives are used to predict a
positive noun polarity if they have a positive po-
larity preference > 0.35 and where the negative
polarity preference is < 0.1. In the second run, la-
belled #2, we only require the positive preference
to be > 0.35. Table 5 shows the results. We also
show the results of Weka (label #3) and Megam
(label #4) for the candidates generated by #2.
Compared to the negative settings, the number
of found positive nouns is rather low. For instance,
in the banks domain, 174 nouns were suggested
compared to 1013 negative ones. However, pre-
cision has not dropped and it is especially higher
than the threshold value of 0.35 seemed to indi-
cate (as discussed previously). Weka (#3) and
Megam (#4) again show better precision, however
21
the number of found nouns is too low (in a setting
that suffers already from low numbers). Figure 6
shows a couple of found positive nouns.
German English
Versammlungsfreiheit freedom of assembly
Ausl?anderintegration integration of foreigners
Einlagesicherung deposit protection
Lohntransparenz wage transparency
Haushaltsdisziplin budgetary discipline
Vertriebsst?arke marketing strength
Anlegervertrauen confidence of investors
Kritikf?ahigkeit ability for criticism
F?uhrungskompetenz leadership competencies
Figure 6: Predicted Positive Nouns
So far, we have discussed a binary approach
where each class (positive, negative) was predicted
and classified independently and where especially
no adjectives with a neutral preference where con-
sidered. What happens if we include these adjec-
tives? The results are given in Figure 7.
domain #neg prec #pos prec
banks 288 80.16 3 66.66
pharma 141 70.92 32 68.75
transport 78 67.94 0 0
politics 115 76.52 0 0
insurance 132 66.66 0 0
Figure 7: Unrestricted Prediction of Noun Polarity
Although precision is good, the results are very
conservative, e.g. in the banks domain, only 288
nouns were found compared to 1013 nouns given
the binary mode. Recall and f-measure are lower
compared to the binary setting. The huge amount
of neutral preference adjectives (about 28?000)
seems to neutralize polar tendencies. But even
then, some predictions survive - so these contexts
seem to be strong.
6 Related Work
The expansion or creation of sentiment lexicons
has been investigated in many variations from dif-
ferent perspectives and for various goals. Liu
and Zhang (2012) subdivide the work in this field
into three groups: manual approaches, dictionary-
based approaches and corpus-based approaches.
While the manual approach is time-consuming, it
is still often used to create core lexicons which are
not domain-specific, e.g. (Taboada et al., 2011).
The dictionary-based approaches which are also
called thesaurus-based approaches (Huang et al.,
2014) try to make use of existing dictionaries or
thesauri like WordNet (e.g. (Esuli and Sebastiani,
2006; Baccianella et al., 2010; Neviarouskaya et
al., 2011)) while the corpus-based approaches rely
on statistical measures based on different con-
cepts, for example, sentiment consistency (Hatzi-
vassiloglou and McKeown, 1997), pointwise mu-
tual information (Turney, 2002), context co-
herency (Kanayama and Nasukawa, 2006), double
propagation (Qiu et al., 2011) or label propagation
(Huang et al., 2014). Our approach is based on the
use of an existing dictionary and of an domain-
independent corpus. But rather than using the cor-
pus to directly detect new entries for the lexicon,
we use it to derive the polarity preference of adjec-
tives which in turn is used to generate candidates
from the domain-specific corpus.
The model most similar to our approach is
(Klenner and Petrakis, 2014), where the contex-
tual and prior polarity of nouns is learned from the
polarity preference of verbs for the verb?s direct
object. However, no attempt is made to induce
domain-specific polarity as we do. We also fo-
cus on the polarity preference of adjectives and we
also try to improve precision by machine learning.
7 Conclusions
We have introduced a plain model for the in-
duction of domain-specific noun lexicons. First,
the polarity preferences of adjectives are learned
from domain-independent text and from a gen-
eral polarity lexicon. A voting approach then pre-
dicts noun polarity from adjective noun pairings
sampled from domain-specific texts. The predic-
tions based only on adjectives acting as positive
or negative polarity predictors perform astonish-
ingly well. Machine Learning can be used to im-
prove precision at the cost of recall. Our approach
thus even might be useful for fully automatic gen-
eration of a high precision, domain-specific prior
noun polarity lexicons.
In future work, we will apply our approach to
other languages than German. We then will also
have to cope with multiword expressions as well,
since compounds not longer - as in German - come
as single words. We also would like to carry out
an extrinsic evaluation in order to see how big the
impact of an induced domain-specific lexicon on
polarity text classification actually is.
22
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proc. of LREC 2010, volume 10, pages 2200?
2204.
Ferraresi A. Zanchetta E. Baroni M., Bernardini S.
2009. The WaCky Wide Web: A collection of very
large linguistically processed Web-crawled corpora.
Language Resources and Evaluation, 43(3):209?
226.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the First Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 7?13.
Hal Daum?e III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper avail-
able at http://pub.hal3.name#daume04cg-bfgs, im-
plementation available at http://hal3.name/megam.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proc. of LREC 2006, volume 6,
pages 417?422.
Eibe Frank, Mark Hall, Geoffrey Holmes, Richard
Kirkby, Bernhard Pfahringer, Ian H. Witten, and Len
Trigg. 2010. Weka-A Machine Learning Work-
bench for Data Mining. In Oded Maimon and Lior
Rokach, editors, Data Mining and Knowledge Dis-
covery Handbook, chapter 66, pages 1269?1277.
Springer US, Boston, MA.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the ACL 1997, pages 174?181.
Association for Computational Linguistics.
Sheng Huang, Zhendong Niu, and Chongyang Shi.
2014. Automatic construction of domain-specific
sentiment lexicon based on constrained label prop-
agation. Knowledge-Based Systems, 56:191?200.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proc. of EMNLP
2006, pages 355?363. Association for Computa-
tional Linguistics.
Manfred Klenner and Stefanos Petrakis. 2014. Induc-
ing the contextual and prior polarity of nouns from
the induced polarity preference of verbs. Data &
Knowledge Engineering, 90:13?21.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415?463. Springer.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proc. of RANLP 2007, pages 378?
382, Borovets, Bulgaria, September 27-29.
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru
Ishizuka. 2011. Sentiful: A lexicon for sentiment
analysis. Affective Computing, IEEE Transactions
on, 2(1):22?36.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised clas-
sification of reviews. In Proc. of the ACL 2002,
pages 417?424. Association for Computational Lin-
guistics.
23
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 85?94,
Dublin, Ireland, August 23 2014.
Compilation of a Swiss German Dialect Corpus
and its Application to PoS Tagging
Nora Hollenstein
University of Zurich
hollenstein@cl.uzh.ch
No
?
emi Aepli
University of Zurich
noemi.aepli@uzh.ch
Abstract
Swiss German is a dialect continuum whose dialects are very different from Standard German,
the official language of the German part of Switzerland. However, dealing with Swiss German in
natural language processing, usually the detour through Standard German is taken. As writing in
Swiss German has become more and more popular in recent years, we would like to provide data
to serve as a stepping stone to automatically process the dialects. We compiled NOAH?s Corpus
of Swiss German Dialects consisting of various text genres, manually annotated with Part-of-
Speech tags. Furthermore, we applied this corpus as training set to a statistical Part-of-Speech
tagger and achieved an accuracy of 90.62%.
1 Introduction
Swiss German is not an official language of Switzerland, rather it includes dialects of Standard German,
which is one of the four official languages. However, it is different from Standard German in terms of
phonetics, lexicon, morphology and syntax. Swiss German is not dividable into a few dialects, in fact it is
a dialect continuum with a huge variety. Swiss German is not only a spoken dialect but increasingly used
in written form, especially in less formal text types. Often, Swiss German speakers write text messages,
emails and blogs in Swiss German. However, in recent years it has become more and more popular and
authors are publishing in their own dialect. Nonetheless, there is neither a writing standard nor an official
orthography, which increases the variations dramatically due to the fact that people write as they please
with their own style.
So far, there are almost no natural language processing (NLP) tools for Swiss German (Scherrer and
Owen, 2010). Considering the fact that the major part of communication between Swiss people of the
German part is in dialect, we would like to start building NLP tools for Swiss German dialects.
Furthermore, it is an attempt to deal with dialect varieties directly instead of taking the detour through
the standard of a language. Speakers of various dialects increasingly communicate through social media
in their own varieties. These interactions are relatively easily accessible and could be used as a source
of data. However, there is a lack of natural language processing tools for dialects, which need to be
developed first in order to process these data automatically.
We start with training a model for a Swiss German Part-of-Speech tagger, which is one of the first steps
dealing with the automatic processing of natural language. Based on a part-of-speech tagged corpus, fur-
ther processes like semantical analysis, syntactical parsing or even applications like machine translation
can be conducted.
In order to train a PoS tagger we need a corpus annotated with parts-of-speech. As such data does not
exist yet, we compiled NOAH?s Corpus of Swiss German Dialects containing Swiss German texts of dif-
ferent genres, and annotated it manually. This is an iterative process alternating between running/training
a PoS tagger and manually annotating/correcting the output. The corpus we present in this paper consists
of 73,616 manually annotated tokens covering many dialect variations of the German-speaking part of
Switzerland.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
85
In the next section, we will mention some related work before we will have a closer look at the Swiss
German dialects and its differences to Standard German in section 3. In section 4 we introduce our
corpus including the adapted tagset before we present the application of our corpus to the Part-of-Speech
tagging task in section 5.
2 Related Work
Most natural language processing applications focus on standardised, written language varieties, but
from a methodological as well as a practical point of view, it is interesting to develop NLP methods for
variational linguistics. Even though there are no other resources of this size and no studies on PoS tagging
for written Swiss German, there have been a few approaches which share some common aspects with our
work. While there are some corpora of spoken texts, such as the Archimob project (Dejung et al., 1999)
which comprises transcribed interviews, it is difficult to find resources to build a written Swiss German
corpus. One of the rare written resources is the sms4science project (D?urscheid and Stark, 2011), a
collection of text messages in all official languages of Switzerland as well as Swiss German dialects.
Concerning Part-of-Speech tagging for non-standard dialects, there are some approaches addressing
linguistic varieties in historical texts, Hinrichs and Zastrow (2012) and Rayson et al. (2007) for German
and English respectively. Furthermore, Diab (2009), Habash and Rambow (2009) and Duh and Kirchhoff
(2005) worked on PoS tagging for Arabic dialects. The latter developed a minimally supervised PoS
tagger for an Egyptian Arabic dialect, which does not have a standard orthography either, without using
any dialect-specific tools.
As far as Swiss German NLP goes, there are approaches to dialect identification (Scherrer and Owen,
2010), dialect machine translation (Scherrer, 2012) and morphology generation (Scherrer, 2013).
3 Swiss German
Swiss German belongs to the Alemannic group of dialects, a branch of the Germanic language family.
This group can be split into three linguistic divisions; Low, High and Highest Alemannic, each of which
contains a few regions of Switzerland. There is no strict border between the Swiss German dialects and
the other Alemannic dialects, rather it is referred to as a dialect continuum. Unlike the continuum among
Swiss German dialects, there is a strict separation between Swiss German and Standard German. When
it comes to the dialects of Swiss German, one can find the concept of diglossia. Diglossia is defined
as a situation in which two languages (or two varieties of the same language) are used under different
conditions within a language community. In the case of the German language, Standard German is used
in Switzerland nearly exclusively in written context while Swiss German is in daily use, mostly in spoken
form but also in informal written contexts (Siebenhaar and Wyler, 1997). However, this distinction is
becoming more and more blurred. Schools are one of a few environments where Standard German
is expected to be used in spoken language. Unlike the situation in other languages, it is standard in
Switzerland to use dialect even in formal situations. In Swiss media, both TV and radio, Swiss German
is well represented and commonly used.
With the introduction of emails, text messages, blogs and chats, Swiss German is taking over more
and more space in written contexts. Nowadays, especially for the younger generations, it is completely
normal to write in Swiss German. However, it is not limited to the private communication. In fact, it
is even becoming a cult status to write and publish in Swiss German. Many authors, among them for
example Lenz (2013), Schobinger (2014) and Kaiser (2012) write books in their dialect, and newspaper
agencies publish newspapers in Swiss German, e.g. Blick am Abend (Ringier AG, 2013, 2014). Even the
Swiss company Swatch has published their annual report 2012 in addition to Standard German, French
and English also Swiss German (The Swatch Group AG, 2012). This hype does not seem to cease, in
the contrary. Speaking a certain dialect is part of the identification. Swiss are proud of their dialect,
which makes it possible to identify their home region if they move to another canton. Despite the big
differences, speakers of various dialects usually understand each other, except a few German varieties of
the canton Valais which others usually have troubles understanding (Keller, 1961).
86
3.1 Differences to Standard German
Swiss German differs from Standard German in many aspects such as phonetics, lexicon, morphology
and syntax. One of the most significant differences is the vocabulary, which even introduces a new word
class not in use in Standard German (see section 4.2). In Swiss German, the Standard German words
are sometimes used in a different manner. For instance, in some cases the genus may change: the word
Radio (radio) as a masculine word (in Swiss German) instead of neutral (in Standard German). However,
there are not merely differences between Swiss and Standard German, but also between the different
dialectal regions. Scherrer (2011) differs between variations which apply for the whole Swiss German
speaking area and differences which appear only in certain dialects and not outside of Allemanic dialects.
The differences between the dialects are partly due to the influence from other languages. For instance
dialects closer to the French speaking part of Switzerland use different grammatical constructions than
Eastern Swiss dialects. In this section we describe some examples of disparities between the Swiss
German dialects and Standard German.
In Swiss German there is no preterite tense (?Pr?ateritum?) and the pluperfect (?Plusquamperfekt?)
is used extremely rarely. Both of them are expressed using the present perfect (?Perfekt?) or rather a
duplication of it (for an example see table 1). Another difference exists with regards to verb tenses
and the use of the auxiliary verbs sein (to be) and haben (to have). For instance, if you are cold, in
Switzerland you would say Ich ha chalt., where ha is the first person singular of ?to have?. However, to
express yourself in this situation in Standard German, the auxilary verb ?to be? is used: Mir ist kalt.
Furthermore, there is more freedom in the order of words of a sentence, especially concerning verbs
(for an example see table 1) as well as more possibilities to correctly arrange phrases. The overt specifi-
cation of the subject is another difference. In Swiss German the subject can be dropped in many cases,
the information about the person is then usually given in the conjugation of the verb. In the question
Chunnsch au? (Swiss German) vs. Kommst du auch? (Standard German) (Are you coming too?), the
subject du is not explicitly expressed in the Swiss German version but only in the second person singular
conjugation of the verb.
Regarding nouns, the four cases of Standard German (nominative, accusative, dative and genitive) are
not all in use in the dialects (Siebenhaar and Voegeli, 1997). Swiss German speakers generally neither
speak nor write in the genitive case, apart from a few exceptions e.g. in the dialect of the canton Valais.
The genitive is replaced by a possessive dative or a phrase using prepositions. This means, in order to
express the German phrase die Ohren des Hasen (the bunny?s ears), either the possessive dative am Haas
sini Ohr?a or a preposition d Ohr?a vom Haas (where vom is a fusion of an preposition von and an article
dem) is used. Moreover, nominative and accusative forms only differ in personal pronouns, whereas the
dative case, if used, is marked with its own determiner and endings for adjectives and nouns.
There are many phenomena, which are treated differently not only in regards to Standard German
but also in different dialects. First of all, the lexicon varies a lot. The variations do not only include
different pronunciation but also completely different words. For instance in some regions of Switzerland,
the Standard German word Butter (butter) is used (even though with a masculine article instead of the
feminine one, which is correct in Standard German). In other regions, however, different words such as
Anke are used instead. Another variation concerns the order of verbs if there is more than one of them in
a sentence. It is often inverted compared to Standard German, but this varies according to the dialect. To
express a final clause with um . . . zu (in order to) for instance, people in eastern Switzerland would use
the concatenation zum. Closer to the French speaking part though, the construction f?ur . . . z is commonly
used, which marks the similarity to the French pour . . . .
The following sentences in table 1 contain examples of both kinds of differences. On the one hand,
there are the Standard German preterite forms liess and hatte, which are expressed in the perfect tense
across dialects: hat . . . (gehen) lassen and hat gehabt. On the other hand, the order of the verbs in the
perfect construction (het gha vs. gha h?at) as well as the final clause with um . . . zu differs from dialect to
dialect.
Considering the way people write in Swiss German reveals another characteristic. The aforementioned
lack of a spelling standard causes variations not only between different authors but also within texts of
87
Dialect around Bern Si het ne la ga, w?u er ne gnue G?aud het gha, f ?ur es Billet z?l?ose.
Dialect around Zurich Si h
?
at ihn gah lah, wil er n?od gnueg G?ald gha h
?
at, zum es Billet l?ose.
Standard German Sie liess ihn gehen, weil er nicht genug Geld hatte, um ein Billet zu kaufen.
English She let him go because he did not have enough money to buy a ticket.
Table 1: Differences between dialects and Standard German
the same author. As people write how they speak, they are not consistent and may spell the same word
differently in the same sentence. They are also free to merge any words, which is quite common. Joining
words into compounds is not an unseen phenomena in Standard German either. However, a compound
is a word consisting of more than one stem, which can act as one word with one corresponding part-of-
speech (usually the one of the last part), e.g. Skilift (ski lift). In Swiss German, the process of merging
words rather resembles the phenomena of clitics, i.e. phonologically bound to another word (Loos et al.,
2004). For example g?ommer is Swiss German for gehen wir (we go). G?ommer can not be split into
verb and pronoun, as the separate occurrences would be g?ond (first person plural of to go) and mir (we).
Thus, such merged words are grammatically different words which, however, are phonologically bound
and can not stand alone. One phonological word (realised as one alphabetic string limited by white
spaces) can even contain the subject, an object and the finite verb of the sentence (see section 4.2 for an
example). This means it can not be assigned to one part-of-speech. In section 4.2 we present how we
deal with them in the part-of-speech tagging task.
To strengthen our argumentation for the necessity of a Swiss German PoS tagger we compare our
results of the training with our corpus with the performance of a Standard German tagger. We run the
German model of the most common tagger for Standard German, the TreeTagger (Schmid, 1995), on our
Swiss German test set. The tagger reaches an accuracy of 50.8%, which is significantly lower than the
result after the training with our corpus.
As we have shown in this section, the dialects of Swiss German differ in many aspects from Standard
German. It is not only a different pronounciation or spelling with some variations in the vocabulary.
It also involves syntactic differences and constructions which are ungrammatical when transferred to
German. Therefore we argue against a normalisation of Swiss German as a mapping to Standard German,
a frequently proposed approach dealing with varieties.
4 Corpus Creation
We compiled a Swiss German dialect corpus in order to provide resources to work with Swiss German.
Furthermore, we applied the corpus to the basic natural language processing task of Part-of-Speech
tagging as a first application. Therefore, we specified a tagset for Swiss German and annotated the
corpus according to this tagset.
4.1 NOAH?s Corpus of Swiss German Dialects
We present NOAH?s Corpus of Swiss German Dialects, a unique resource for Swiss German. We com-
piled a Swiss German corpus containing manually annotated part-of-speech tags of 73,616 tokens. As
the first annotated resource for written texts in Swiss German dialects, the goal is to cover various text
genres as well as different dialects from all regions of Switzerland. NOAH?s Corpus is freely available
for research.
1
In NOAH?s Corpus, we include articles from the Alemannic Wikipedia (Wikipedia, The Free Encyclo-
pedia, 2011) in five major dialects (Aarau, Basel, Bern, Zurich and the Eastern part of Switzerland) and a
Swiss German special edition of the newspaper ?Blick am Abend? (Ringier AG, 2013), which was pub-
lished in 2013. In addition, we added sections of the Swiss German dialect version of the official annual
report of the Swatch company from 2012 (The Swatch Group AG, 2012). Furthermore, we incorporated
extracts of novels from the Swiss author Viktor Schobinger (Viktor Schobinger, 2013) which are written
exclusively in the Zurich dialect. Finally, we also included three blogs from BlogSpot in various dialects
as a web resource. The detailed token quantities for each text source are shown in table 2.
1
http://www.cl.uzh.ch/research/downloads.html
88
Text source No. of tokens
Alemannic Wikipedia 20,135
Swatch Annual Report 2012 13,386
Novels from Viktor Schobinger 11,165
Newspaper articles 11,259
Blogs 17,671
Total 73,616
Table 2: Corpus composition
Manning (2011) suggests that the largest opportunities for improvement in part-of-speech tagging lies
in improving the tagset and the accuracy of annotation, even though a perfect annotation of words into
discrete lexical categories is not possible because some words do not fall clearly into one category. Thus,
since the consistency of annotations in natural language corpora is of great importance for PoS tagging
performance, we put great emphasis on the manual annotations. After the annotation of the corpus by
native speakers, various consistency checks were conducted. For instance, we checked words with low
probabilities in the tagging model and we also conducted random checks for cases of difficult tags.
4.2 Tagset
As the basic tagset we use the Stuttgart-T?ubingen-TagSet (STTS), which is the standard for German
(Schiller et al., 1999). Because of the differences between German and the Swiss German dialects we
additionally introduced the tag PTKINF as well as the adding of a ?+?-sign to any PoS tag.
The newly introduced tag PTKINF represents an infinitive particle suggested by Glaser (2003). It
is a commonly used and therefore widely analysed phenomenon for Swiss German dialects with no
corresponding word or construction in German. In Swiss German people say Ich go go poschte. (I?m
going shopping.). The second go corresponds to the finite verb gehen (to go) in the according Standard
German sentence Ich gehe einkaufen. The first go, however, does not exist in the Standard German
version. This particle is probably originally derived from gehen. However, as a particle it exceeds the
use in gehen (Glaser, 2003). This infinitive particle go (derived from gehen; to go) also comes in other
forms like for instance cho (derived from kommen; to come) and afa (probably derived from anfangen;
to begin). In our corpus we found 37 occurrences of this tag.
Furthermore, we introduce special tags for merged words. Since Swiss German does not have official
spelling rules, words can be freely joined. Splitting these words in a pre-processing step would be one
approach to deal with them. However, it is not always clear where to split them and would result in
strange words as the words phonologically assimilate when merged with others (see section 3.1). Also
Manning (2011) suggests that splitting tags seems to be largely a waste of time for the goal of improving
PoS tagging numbers.
Instead of splitting, we identify these merged words by using the corresponding STTS-tag for the first
part and add a plus sign to show that a given word consists of more than one simple word. There are
sequences of words that are commonly joined, but also less common combinations can appear as it de-
pends on the preferences of the writer. A commonly joined sequence is, for instance, VAFIN+PPER,
a personal pronoun attached to a finite auxiliary verb, e.g. hets for German hat es (there is). An ex-
ample for a less commonly joined sequence would be a concatenation of three different parts of speech
VVFIN+PIS+PPER such as bruchtmese for the German words braucht man sie (one uses/needs it). Fig-
ure 3 shows some more examples of the most frequent combinations (e.g. a verb, a conjunction or a
particle followed by a pronoun). We found 1008 occurrences of merged words, which represent 1.37%
of all tokens in the corpus.
The STTS-tagset already contains one tag that is a combination of two, namely the APPRART, con-
sisting of a preposition APPR and an article ART. This is used for words like beim, which is composed of
bei and dem. However, these are ?normal? Standard German prepositions. This is not the case with the
word combinations in Swiss German writing habits, where any words of completely different parts-of-
speech can be merged together. Using the approach of simply joining the corresponding part-of-speech
tags of the words like the APPRART-case, we would end up with an infinite tagset. Thus, the approach
89
PoS tag Swiss German Standard German English
VAFIN+ isches ist es is it
KOUS+ dasme dass man that one
VMFIN+ chame kann man can one
PTKZU+ zfl?ug?a zu fliegen to fly
ADV+ deetobe dort oben up there
Table 3: PoS tags for compound words
of adding a plus sign allows us to have a clearly defined tagset. Another advantage is that it is possible
to identify all the concatenated words easily, looking for PoS tags with a ?+?-sign attached. Once the list
of all occurrences is given, the corresponding tags can still be modified according to one?s requirements
for further processing in a text or corpus. Moreover, there is not a huge loss of information due to the
omitted part-of-speech information for the other word part(s). For many combinations it is very clear
which part of speech follows. Coming across a PTKZU+ for example, the only possibility for the second
part is a verb in the infinitive, a fact that can be inferred from the grammar.
5 Evaluation of PoS Tagging
In order to achieve the best results we trained different statistical, open source PoS taggers: TreeTagger
(Schmid, 1995), hunpos tagger (Hal?acsy et al., 2007), RFTagger (Schmid and Laws, 2008), Wapiti CRF
Tagger (Lavergne et al., 2010), TnT (Trigrams?n?Tags) tagger (Brants, 2000) and BTagger (Gesmundo
and Samard?zi?c, 2012). The BTagger and the TnT tagger reach the best results for our corpus, therefore
we did a more detailed evaluation of the tagging results based on these two taggers.
5.1 Results
We evaluated the performance of the BTagger and the TnT tagger over our corpus with 10-fold cross
validation. The folds we created are non-stratified, i.e. not contiguous sentences. This is because our
corpus consists of diverse kinds of text. If we train the tagger on the whole corpus with diverse kinds
of text and then evaluate only on blogs for instance, we will not get a fair result. Thus, in order to get
balanced test sets, we chose the sentence for the 10 folds randomly. With the whole corpus as training
set, we reach an accuracy of 90.62% with the BTagger and 90.14% with the TnT tagger (see table 4).
Considering the 26.36% unknown tokens in average over all test sets, the accuracy for the unknown
tokens is surprisingly high.
Accuracy BTagger TnT tagger
Unknown tokens 77.99% 72.39%
Known tokens 93.34% 93.26%
Overall 90.62% 90.14%
Table 4: Accuracy of taggers over the whole corpus
As stated in section 4.1, our corpus contains texts from different genres. Therefore we additionally
evaluated the different text genres individually. The results are shown in table 5. The Wikipedia articles
score best with 90.92% accuracy. This is due to the fact that it is the biggest part of the corpus with
20,135 tokens (one third). In addition, the amount of unknown words is not as high as in other texts
because the variety of different words is limited to one topic per article. The literary texts are on the
second place. This corpus part is only half of the size of the Wikipedia articles. However, the texts are all
extracted from the criminal novels of Viktor Schobinger. This means, they are written in one dialect by
one person, which reduces the number of orthographic varieties and thus the number of unknown tokens.
As table 5 shows, the novels have only 16% of unknown tokens, less than all the other parts.
Furthermore, we analysed the relation between the size of the corpus and the accuracy we achieved
(see figure 1). In the case of Swiss German we found that the accuracy increases significantly until
approximately 40,000 tokens. Increasing the size of the corpus beyond this amount of tokens is helpful
90
Accuracy Accuracy Accuracy Number of
Text type overall unknown tokens known tokens unknown tokens
Wikipedia articles 90.92% 75.64% 94.60% 22.7%
Literary texts (novels) 89.37% 70.41% 92.89% 16.0%
Annual report 88.82% 76.95% 92.72% 24.7%
Blogs 88.10% 71.69% 91.73% 18.2%
Newspaper articles 87.17% 71.19% 93.15% 27.4%
Table 5: Results for the different text genres with the BTagger
to cover a larger amount of orthographic varieties and reducing the number of unknown words, but does
not considerably improve the accuracy of known tokens.
Another fact that stands out in figure 1 is the difference of the tagger performances for a training set
of 10,000 tokens. This is due to the fact that that the BTagger makes use of context information and
thus emphasises the transition probability by learning sequences of tags. Therefore, not a huge amount
of data is needed to get a comparably good performance (Gesmundo and Samard?zi?c, 2012). The TnT
tagger, on the other hand, emphasises the emission probability and does not generalise as well.
Figure 1: Relation between PoS tagging accuracy and corpus size for the TnT tagger (grey line) and the
slightly better results from the BTagger (black line).
In section 3.1, discussing the differences between Standard German and Swiss German, we argue
that Standard German tools are not capable of dealing with Swiss German dialects. As an additional
experiment we extend our Swiss German corpus with a Standard German corpus to see if the addition of
information of Standard German data improves the result. We combined our Swiss German corpus with
the T?uBa-D/Z German Treebank (Telljohann et al., 2006), which contains more than 1,300,000 tokens.
The results on a 10-fold cross validation reached an accuracy of 87.6% which is lower than the results
for the Swiss German corpus by itself. This implies that the addition of Standard German training data
to our Swiss German corpus is not helpful for the training of a Swiss German PoS tagger.
91
5.2 Error Analysis
The most frequent errors were the confusion of nouns (NN) and proper names (NE), which represent
ca. 15% of all errors. This is also a common problem for Standard German due to the capitalisation of
nouns. The different kinds of adjectives and the adverbs as well as various types of verbs are also often
mistaken, but these are confusions inside one part-of-speech category. Furthermore, there are many
mistakes between articles and some types of pronouns, especially personal and demonstrative. However,
this is not surprising as they often have the same form. For example the German indefinite article ein is
often realised as es in Swiss German, the definite article das as s. The Swiss German es also stands for
the German neutral personal pronoun es if it is not abbreviated to s. This issue is exemplified in table 6.
PoS tag Swiss German example Standard German English
ART (definite) es Buech ein Buch a book
ART (indefinite) s Buech das Buch the book
PPER Es isch rot. Es ist rot. It is red.
PPER S r?agnet. Es regnet. It is raining.
Table 6: Example of the same types with different PoS tags and meanings
5.3 Discussion & Future Work
We achieved reasonable PoS tagging results for the Swiss German dialects considering the low amount
of available resources. As stated in section 3, we are dealing with a dialect continuum missing an orthog-
raphy standard. We neither select one specific dialect (or region) of Switzerland nor do we normalise the
data in any way. Thus, our data contains a high amount of hapax legomena, i.e. words which only appear
once. This fact explains the considerably lower accuracy for unknown tokens compared to taggers for
standardised languages. Furthermore, we include different sources and different text genres in one cor-
pus, which does not simplify the work for a statistical PoS tagger. Thus, it is conceivable that accuracy
improvements may be achieved by concentrating on one particular dialect.
In future work we will enlarge NOAH?s Corpus of Swiss German Dialects by including more texts
per dialect in order to reduce the number of unknown tokens. Another approach we are pursuing is
to develop a procedure based on lexical distance measures and syntactical patterns in order to map the
different orthographic version of a token, so that the tagger can benefit from these mappings. This
procedure may also serve as a starting point towards the lemmatisation of Swiss German texts.
The goal of improving Part-of-Speech tagging for Swiss German as well as extending the corpus is to
enable and facilitate the development of further NLP tasks, such as dependency parsing, opinion mining
or deeper dialectology studies.
6 Conclusion
We have presented our work on compiling a corpus of Swiss German dialects and its application to the
training of a Part-of-Speech tagger. As a first resource, our corpus is a stepping stone for natural language
processing for the Swiss German dialect area. Training the BTagger on our corpus results in an accuracy
of 90.62%. With little post processing effort on the tagger output, a PoS-annotated corpus for Swiss
German can be obtained and thus resources extended.
NOAH?s Corpus of Swiss German Dialects contains 73,616 tokens from texts of different genres in
different dialects, manually annotated with PoS tags. We are happy to share it with interested parties.
The corpus including the PoS tags can be downloaded in XML format.
Acknowledgements
We are grateful to the Institute of Computational Linguistics of the University of Zurich for their support.
We would like to thank Martin Volk and Simon Clematide for valuable comments and suggestions.
Furthermore, many thanks to Tanja Samard?zi?c for inputs concerning the PoS taggers and David Klaper
for providing some of the raw data for the corpus.
92
References
Thorsten Brants. TnT: a statistical part-of-speech tagger. In Proceedings of the sixth conference on Ap-
plied natural language processing, pages 224?231. Association for Computational Linguistics, 2000.
Christof Dejung, Thomas Gull, and Tanja Wirz. Landigeist und Judenstempel: Erinnerungen einer
Generation 19301945. Limmat Verlag, 1999.
Mona Diab. Second generation AMIRA tools for Arabic processing: Fast and robust tokenization, POS
tagging, and base phrase chunking. In 2nd International Conference on Arabic Language Resources
and Tools, 2009.
Kevin Duh and Katrin Kirchhoff. POS tagging of dialectal Arabic: a minimally supervised approach. In
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 55?62.
Association for Computational Linguistics, 2005.
Christa D?urscheid and Elisabeth Stark. SMS4science: An international corpus-based texting project and
the specific challenges for multilingual Switzerland. Digital Discourse: Language in the New Media,
pages 299?320, 2011.
Andrea Gesmundo and Tanja Samard?zi?c. Lemmatisation as a tagging task. In Proceedings of the 50th
Annual Meeting of the Association for Computational Linguistics, pages 368?372. ACL, 2012.
Elvira Glaser. Schweizerdeutsche Syntax: Ph?anomene und Entwicklungen. In Beat Dittli, An-
nelies H?acki Buhofe, and Walter Haas, editors, G?ommer MiGro?, pages 39?66, Freiburg, Schweiz,
2003.
Nizar Habash and Owen Rambow. Arabic tokenization, part-of-speech tagging and morphological dis-
ambiguation in one fell swoop. In Proceedings of the 43rd Annual Meeting on Association for Com-
putational Linguistics, 2009.
P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz. HunPos - an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions, pages 209?212, Prague, Czech Republic, 2007.
Erhard Hinrichs and Thomas Zastrow. Linguistic annotations for a diachronic corpus of German. In
Proceedings of the 10th Workshop on Treebanks and Linguistic Theories, Heidelberg, 2012.
Renato Kaiser. UUFPASS
?
A, N
?
OD AAPASS
?
A! Der gesunde Menschenversand, 2012.
R.E. Keller. German dialects: phonology and morphology, with selected texts. Manchester University
Press, 1961.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. Practical Very Large Scale CRFs. In Proceedings
the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513,
Uppsala, Sweden, July 2010. Association for Computational Linguistics.
Pedro Lenz. I bi meh aus eine. Cosmos Verlag AG, 2013.
Eugene Loos, Susan Anderson, Day Dwight, Paul Jordan, and Douglas Wingate. Glossary of linguis-
tic terms. http://www-01.sil.org/linguistics/GlossaryOfLinguisticTerms/WhatIsACliticGrammar.htm,
2004.
Christopher D. Manning. Part-of-speech tagging from 97% to 100%: is it time for some linguistics?
Computational Linguistics and Intelligent Text Processing, pages 171?189, 2011.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan Culpeper, and Nicholas Smith. Tagging the Bard:
Evaluating the accuracy of a modern POS tagger on Early Modern English corpora. 2007.
Ringier AG. Blick am Abig. http://epaper.blick.ch/webreader/baa/download/?doc=BAA280513ZH, May
2013.
Ringier AG. Blick am Abig. http://epaper.blick.ch/webreader/baa/download/?doc=BAA020614ZH, June
2014.
93
Yves Scherrer. Syntactic transformations for Swiss German dialects. In First Workshop on Algorithms
and Resources for Modelling of DIalects and Language Vareities, Edinburgh, 2011. EMNLP.
Yves Scherrer. Machine translation into multiple dialects: The example of Swiss German. 7th SIDG
Congress - Dialect 2.0, 2012.
Yves Scherrer. Continuous variation in computational morphology - the example of Swiss German. In
TheoreticAl and Computational MOrphology: New Trends and Synergies (TACMO), Gen`eve, Suisse,
2013. 19th International Congress of Linguists. URL http://hal.inria.fr/hal-00851251.
Yves Scherrer and Rambow Owen. Natural Language Processing for the Swiss German Dialect Area.
In Proceedings of the Conference on Natural Language Processing (KONVENS), pages 93?102,
Saarbr?ucken, Germany, 2010.
Anne Schiller, Simone Teufel, Christine St?ockert, and Christine Thielen. Guidelines f?ur das Taging
deutscher Textkorpora mit STTS, August 1999.
Helmut Schmid. Improvements in Part-of-Speech Tagging with an Application to German. In Proceed-
ings of the ACL SIGDAT-Workshop, Dublin, 1995.
Helmut Schmid and Florian Laws. Estimation of Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. COLING, 2008.
Viktor Schobinger. Der
?
A?aschmen und de scht`u`urzmord. Schobinger-Verlaag, 2014.
Beat Siebenhaar and Walter Voegeli. 6 Mundart und Hochdeutsch im Vergleich. In Mundart und
Hochdeutsch im Unterricht. Orientierungshilfen f?ur Lehrer, 1997.
Beat Siebenhaar and Alfred Wyler. Dialekt und Hochsprache in der deutschsprachigen Schweiz. 1997.
Heike Telljohann, Erhard W. Hinrichs, Sandra K?ubler, Heike Zinsmeister, and Kathrin Beck. Stylebook
for the T?ubingen Treebank of Written German (T?uBa-D/Z). Technical report, Universit?at T?ubingen,
2006.
The Swatch Group AG. Swatch Group Gesch?aftsbericht 2012. http://www.swatchgroup.com/de/investor
relations/jahres und halbjahresberichte/fruehere jahres und halbjahresberichte, 2012.
Viktor Schobinger. Viktor?s z?urit?u(?u)tsch. http://www.zuerituetsch.ch/index.html, 2013.
Wikipedia, The Free Encyclopedia. Alemannic Wikipedia. http://als.wikipedia.org/wiki/Wikipedia:
Houptsyte, 2011.
94
