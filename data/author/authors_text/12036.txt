Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 61?69,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
NE Tagging for Urdu based on Bootstrap POS Learning 
 
 
Smruthi Mukund Rohini K. Srihari 
Dept. of Computer Science and Engineering Dept. of Computer Science and Engineering 
University at Buffalo, SUNY University at Buffalo, SUNY 
Amherst, NY, USA Amherst, NY, USA 
smukund@buffalo.edu rohini@cedar.buffalo.edu 
 
 
 
 
 
 
Abstract 
Part of Speech (POS) tagging and Named Ent-
ity (NE) tagging have become important com-
ponents of effective text analysis. In this 
paper, we propose a bootstrapped model that 
involves four levels of text processing for Ur-
du. We show that increasing the training data 
for POS learning by applying bootstrapping 
techniques improves NE tagging results. Our 
model overcomes the limitation imposed by 
the availability of limited ground truth data 
required for training a learning model. Both 
our POS tagging and NE tagging models are 
based on the Conditional Random Field 
(CRF) learning approach. To further enhance 
the performance, grammar rules and lexicon 
lookups are applied on the final output to cor-
rect any spurious tag assignments. We also 
propose a model for word boundary segmen-
tation where a bigram HMM model is trained 
for character transitions among all positions in 
each word. The generated words are further 
processed using a probabilistic language mod-
el. All models use a hybrid approach that 
combines statistical models with hand crafted 
grammar rules. 
1 Introduction 
The work here is motivated by a desire to under-
stand human sentiment and social behavior through 
analysis of verbal communication. Newspapers 
reflect the collective sentiments and emotions of 
the people and in turn the society to which they 
cater to. Not only do they portray an event that has 
taken place as is, but they also reveal details about 
the intensity of fear, imagination, happiness and 
other emotions that people express in relation to 
that event.  Newspaper write ups, when analyzed 
over these factors - emotions, reactions and beha-
vior - can give a broader perspective on the culture, 
beliefs and the extent to which the people in the 
region are tolerant towards other religions. Our 
final goal is to automate this kind of behavioral 
analysis on newspaper articles for the Urdu lan-
guage. Annotated corpus that tag six basic human 
emotions, ?happy?, ?fear?, ?sad?, ?surprise?, ?an-
ger? and ?disgust?, based on the code book devel-
oped using the MPQA standards as guideline, is 
currently being developed.  Articles from two lead-
ing Urdu newswires, BBC Urdu1 and Jung Daily2 
form our corpus.  
In order to achieve our goal, it was required to 
generate the basic tools needed for efficient text 
analysis. This includes NE tagging and its precur-
sor, POS tagging. However, Urdu, despite being 
spoken by over 100 million people, (Gordon, 
2005) is still a less privileged language when it 
comes to the availability of resources on the inter-
net. Developing tools for a language with limited 
resources is a challenge, but necessary, as the vo-
lume of Urdu text on the internet is rising. Huda 
(2001) shows that Urdu has now gained impor-
tance on the web, making it the right time to tackle 
these issues. 
It is useful to first examine some basic proper-
ties of Urdu and how they affect the cascade of 
NLP steps in text analysis. Urdu has the nastaleeq 
and nasq style of writing that is similar to Arabic 
                                                           
1 http://www.bbc.co.uk/urdu/ 
2 http://www.jang.net/urdu/ 
61
and flows from right to left (Ahmad et al, 2001). It 
also adopts some of its vocabulary from Arabic. 
However, the grammar and semantics of the lan-
guage is similar to Hindi and this makes it very 
different from Arabic. For effective text analysis, a 
thorough syntactic and semantic understanding of 
the language is required. Detailed grammatical 
analysis provided by Platts (1909) and Schmidt 
(1999) can be used for this purpose. The first step 
in the information retrieval pipeline is tokeniza-
tion. Unlike English, where the word delimiter is 
mostly a space, Urdu is more complex. There are 
space insertion as well as space deletion problems. 
This makes tokenization a difficult task. The word 
segmentation model that we propose here com-
bines the statistical approach that considers bigram 
transition of characters based on their positions in a 
word and morphological rules with lexicon loo-
kups. 
 POS tagging comes next in the NLP text analy-
sis pipeline. The accuracy of the tagging model 
varies, depending on the tagsets used and the do-
main of the ground truth data. There are two main 
tagsets designed for Urdu, the CRULP tagset3 and 
the U1-tagset (Hardie 2003). The U1-tagset, re-
leased as a part of EMILLE4 corpus, is based on 
the EAGLES standards (Leech and Wilson 1999). 
We decided to use the standards proposed by 
CRULP for the following reasons. 
 
1. The tagset, though not as detailed as the 
one proposed in U1-tagset, covers all the 
basic requirements needed to achieve our 
final goal. 
2. The tagged corpus provided by CRULP is 
newswire material, similar to our final 
corpus. 
 
A person, when asked to identify an NE tagged 
word in a sentence would typically try to first find 
the word associated with a proper noun or a noun, 
and then assign a suitable NE tag based on the con-
text. A similar approach is used in our model, 
where the learning happens on the data that is POS 
tagged as well as NE tagged. Features are learnt 
from the POS tags as well as the NE tags. The final 
output of our complete model returns the POS tags 
                                                           
3 
http://www.crulp.org/Downloads/ling_resources/parallelcorpu
s/Urdu POS Tagset.pdf 
4 http://www.emille.lancs.ac.uk/ 
and NE tags associated with each word. Since we 
have limited data for training both the POS as well 
as the NE models, we propose a technique called 
bootstrapping that helps in maximizing the learn-
ing for efficient tagging. 
The remainder of the paper is organized as fol-
lows. Section 2 discusses the resources assimilated 
for the work followed by tokenization and word 
segmentation in Section 3. Section 4 gives a de-
tailed explanation of our model starting with a 
brief introduction of the learning approach used. 
Rules used for POS tagging and NE tagging are 
mentioned in subsections of Section 4. Section 5 
presents the results and Section 6 concludes the 
paper. In each section, wherever relevant, previous 
work and drawbacks are presented. 
2 Resources  
Based on the style of writing for Urdu, different 
encoding standards have been proposed. Urdu 
Zabta Takthi - the national standard code page for 
Urdu and Unicode - international standard for mul-
tilingual characters are the two proposed and wide-
ly used encoding standards. BBC Urdu and Jung 
Daily are both encoded with Unicode standards 
and are good sources of data. The availability of 
online resources for Urdu is not as extensive as 
other Asian languages like Chinese and Hindi. 
However, Hussain (2008) has done a good job in 
assimilating most of the resources available on the 
internet. The lexicon provided as a part of the 
EMILLE (2003) data set for Urdu has about 
200,000 words. CRL5 has released a lexicon of 
8000 words as a part of their Urdu data collection. 
They also provide an NE tagged data set mostly 
used for morphological analysis. The lexicon in-
cludes POS information as well. CRULP6 has also 
provided a lexicon of 149,466 words that contains 
places, organizations and names of people. As part 
of the Urdu morphological analyzer provided by 
Humayoun (2007), a lexicon of about 4,500 unique 
words is made available. There are a few Urdu-
English dictionaries available online and the first 
online dictionary, compiled by Siddiqi (2008), 
provides about 24,000 words with their meanings 
in English.  
Getting all the resources into one single compi-
lation is a challenge. These resources were brought 
                                                           
5 http://crl.nmsu.edu/Resources/lang_res/urdu.html 
6 http://www.crulp.org/software/ling_resources/wordlist.htm 
62
together and suitably compiled into a format that 
can be easily processed by Semantex (Srihari, 
2008), a text extraction platform provided by Janya 
Inc7. Lists of places, organizations and names of 
famous personalities in Pakistan were also com-
piled using the Urdu-Wikipedia8 and NationalMas-
ter9. A list of most common names in Pakistan was 
composed by retrieving data from the various 
name databases available on the internet.   
The word segmentation model uses the Urdu 
corpus released by CRULP as the training data. 
This dataset is well segmented. POS tagging model 
uses data provided by CRULP and NE tagging 
model uses data provided by CRL. 
3 Word Segmentation and Tokenization  
Urdu is a language that has both the space inser-
tion and space deletion problems. The Urdu word 
segmentation problem as mentioned by Durrani 
(2007) is triggered by its orthographic rules and 
confusions about the definition of a word. Durrani 
summarizes effectively, all the problems associated 
with Urdu word segmentation. Of all the different 
techniques explored to achieve this objective, tra-
ditional techniques like longest and maximum 
matching depend mostly on the availability of a 
lexicon that holds all the morphological forms of a 
word. Such a lexicon is difficult to obtain. It is 
shown by Theeramunkong et al, (2001), that for a 
Thai segmentation system, the efficiency drops 
considerably (from 97% to 82%) making this ap-
proach highly lexicon dependent.  
Statistical based techniques have applied proba-
bilistic models to solve the problem of word seg-
mentation. Bigram and trigram models are most 
commonly employed. Using feature based tech-
niques for POS tagging is also very common. 
These techniques overcome the limitations of sta-
tistical models by considering the context around 
the word for specific words and collocations. There 
are other models that generate segments by consi-
dering word level collation as well as syllable level 
collocation.  
However, for a language like Urdu, a model that 
is purely statistical will fail to yield good segmen-
tation results. A mixed model that considers the 
morphological as well as semantic features of the 
                                                           
7 http://www.janyainc.com/ 
8 http://ur.wikipedia.com/wiki/ 
9 http://www.nationmaster.com/index.php 
language facilitates better performance as shown 
by Durrani (2007) where the word segmentation 
model uses a lexicon for proper nouns and a statis-
tical model that trains over the n-gram probability 
of morphemes. Maximum matching technique is 
used to generate word boundaries of the ortho-
graphic words that are formed and these are later 
verified using the POS information. The segments 
thus generated are ranked and the best ones are 
accepted. Statistical models that consider character 
based, syllable based and word based probabilities 
have shown to perform reasonably well. The Thai 
segmentation problem was solved by Pornprasert-
kul (1994) using the character based approach. In 
our model, we use a combination of character 
based statistical approach and grammar rules with 
lexicon lookups to generate word boundaries. 
Urdu segmentation problem can be looked at as 
an issue of inserting spaces between characters. All 
letters in Urdu, with a few exceptions, have three 
forms - initial, medial and final. (We do not con-
sider the detached form for word formation). 
Words are written by joining the letters together 
and based on the position of the letter in the word, 
suitable forms are applied. This property of word 
formation is the crux of our model. The bigram 
probability of occurrences of each of these charac-
ters, based on their positions, is obtained by train-
ing over a properly segmented training set. For 
unknown characters, unknown character models 
for all the three position of occurrences are also 
trained. The probability of word occurrence is 
noted. Along with this, a lexicon rich enough to 
hold all possible common words is maintained. 
However, this lexicon does not contain proper 
nouns. A new incoming sentence that is not seg-
mented correctly is taken and suitable word boun-
daries are generated by using a combination of 
morphological rules, lexicon lookups, bigram word 
probabilities and bigram HMM character model. 
The following probabilities are estimated and max-
imized at character level using the Viterbi algo-
rithm. The following are the calculated 
probabilities:  
 
(i) )|( )(1)( initialkmedialk chchP ? - is the prob-
bility of character k being in medial 
form given character k-1 is in initial 
form. 
63
(ii) )|( )(1)( initialkfinalk chchP ? - is the proba-
bility of character k being in final form 
given character k-1 is in initial form. 
(iii) )|( )(1)( medialkfinalk chchP ?  - is the proba-
bility of character k being in final form 
given character k-1 is in medial form. 
(iv) )|( )(1)( medialkmedialk chchP ? - is the proba-
bility of character k being in medial 
form given character k-1 is in medial 
form. 
(v) )|( )(1)( finalkinitialk chchP ? - is the proba-
bility of character k being in initial 
form given character k-1 is in final 
form. 
 
Each word thus formed successfully is then veri-
fied for morphological correctness. If the word is 
not valid morphologically, then the window is 
moved back over 3 characters and at every step the 
validity of occurrence of the word is noted. Simi-
larly, the window is moved 3 characters ahead and 
the validity of the word is verified. All words 
formed successfully are taken and further 
processed using a language model that considers 
the bigram occurrence for each word. The un-
known word probability is considered here as well. 
The word with maximum probability is taken as 
valid in the given context.  
Let >< 321 www  be the word formed by the 
moving window. Then, the word selected, ws, is 
given by 
 
(vi) 
??
??
?
??
??
?
=
)(|)(
)(|)(
)(|)(
max
3
2
1
prev
prev
prev
s
wPwP
wPwP
wPwP
w  
where wprev  is the previous word. 
 
It is also noted that the number of times a transi-
tion happens from a syllable set with consonants to 
a syllable set with vowels, in a word, is no longer 
than four in most cases as noted below. This factor 
is also considered for terminating the Viterbi algo-
rithm for each word.  
 
 Ir | aad | ah - three transitions 
 
Some of the morphological rules considered 
while deciding the word boundaries are given be-
low. Word boundary is formed when  
1. The word ends with ''?? - un Gunna 
2. The character transitions over to digits 
3. Punctuations marks are encountered ('-' is 
also included) 
4. No two 'ye' - choti ye come back to back 
5. No characters occur in detached form un-
less they are initials or abbreviations fol-
lowed by a period 
6. If current character is 'alif' and the pre-
vious character is 'ee' - bari ye then the 
word boundary occurs after 'alif' 
Some of the drawbacks seen in this model are 
mainly on account of improper identification of 
proper nouns. If a proper noun is not well seg-
mented, the error propagates through the sentence 
and typically the next two or three words fail to get 
segmented correctly. Also, in Urdu, some words 
can be written in more than one ways. This mostly 
depends on the diacritics and ambiguity between 
bari and choti 'ye'. The training data as well as the 
test data were not normalized before training. The 
model shows a precision of 83%. We realized that 
the efficiency of this model can be improved if 
phoneme level transitions were taken into consid-
eration. Training has to be increased over more 
proper nouns and a lexicon for proper nouns loo-
kup has to be maintained. Diacritics that are typi-
cally used for beautification should be removed. 
Words across the documents need to be normalized 
to one accepted format to assure uniqueness.  This 
involves considerable amount of work and hence, 
in order to prevent the propagation of error into the 
NLP text analysis pipeline, we decided to test our 
subsequent models using pre-segmented data, in-
dependent of our word segmentation model. 
4 Learning Approaches  
A Conditional Random Field (CRF), is an undi-
rected graphical model used for sequential learn-
ing. The tasks of POS tagging and NE tagging are 
both sequential learning tasks and hence this learn-
ing approach is a reasonable choice. What follows 
is a brief outline about CRF. Interested readers are 
referred to Lafferty et al, (2001), for more infor-
mation on CRF.  
4.1 Conditional Random Fields (CRF) 
64
A linear chain CRF defines a single log-linear 
probabilistic distribution over the possible tag se-
quences y for a sentence x 
??
= =
?=
T
t
K
k
tttkk xyytfxZ
xyp
1 1
1 ),,,(exp)(
1)|( ?  
where  fk(t, yt, yt-1, xt) is typically a binary function 
indicating the presence of feature k, ?k is the weight 
of the feature, and Z(x) is a normalization function. 
? ??
= =
?=
y
T
t
K
k
tttkk xyytfxZ
1 1
1 ),,,(exp)( ?  
This modeling allows us to define features on 
states (the POS/NE tags) and edges (pairs of adja-
cent POS/NE tags) combined with observations 
(eg. words and POS tags for NE estimation). The 
weights of the features are determined such that 
they maximize the conditional log-likelihood of the 
training data:  
( )? == i ii xypL 1 )()( )|(log)( ?? .  
For the actual implementation, CRF++10, an 
open source tool that uses the CRF learning algo-
rithm is used. The L-BFGS algorithm11 is used for 
optimization. 
4.2 %E Tagging using POS information 
POS tagging is a precursor for all text analysis 
tasks. Assigning POS tags to words without any 
ambiguity depends on contextual information and 
extracting this information is a challenge. For a 
language like English, several techniques have 
been proposed that can be broadly classified into 
statistical, rule based and hybrid approaches (Ek-
bal, 2007). The general consensus is that ap-
proaches like MEMM and HMM, that work well 
for Hindi, would work well for Urdu as well, since 
Urdu is grammatically similar to Hindi (Platts, 
1909).  However, the linguistic and morphological 
rules used in the post processing steps differ from 
Hindi because of Urdu?s borrowed vocabulary and 
                                                           
10 http://crfpp.sourceforge.net/ 
11 http://www.mcs.anl.gov/index.php 
style of writing from Arabic. Also, the requirement 
for such models to work well is the availability of 
large training data. 
Building NE recognizers for languages like Ur-
du is difficult as there are no concepts like capitali-
zation of characters. Also, most names of people 
have specific meanings associated with them and 
can easily be found in a dictionary with different 
associated meanings. Various learning approaches 
have been proposed for this task, HMM based 
learning approach (Bikel et al, 1999), Maximum 
Entropy Approach (Borthwick, 1999) and CRF 
approach (McCallum, 2003) are the most popular. 
Ashish et al, (2009) show an SVM based approach 
also works well for such tasks. To overcome the 
problem of limited data availability, we present a 
method to increase the amount of training data that 
is available, by using a technique called bootstrap-
ping. 
We do not have a training corpus that is manual-
ly tagged for both POS and NE. Our training data 
consists of two different datasets. The dataset used 
for POS tagging is provided by CRULP and is 
tagged using their tagset. The dataset used for NE 
tagging is provided by CRL as a part of their Urdu 
resource package. The CRL tagset consists of 
LOCATION, PERSON, ORGANIZATION, DATE 
and TIME tags. We use only the first three tags in 
this work. 
Our aim is to achieve effective POS tagging and 
NE tagging by maximizing the use of the available 
training data. The CRULP dataset (which we call 
datasetPOS) is a corpus of 150,000 words that are 
only POS tagged and the CRL dataset (which we 
call datasetNE) is a corpus of 50,000 words that are 
only NE tagged. First, we trained a CRF model on 
datasetNE that uses only the NE information to per-
form NE recognition. This one stage model was 
not effective due to the sparseness of the NE tags 
in the dataset. The model requires more data while 
training. The obvious and frequently tried ap-
proach (Thamar, 2004) is to use the POS informa-
tion.  
Figure 1 shows a two stage model that uses POS 
information to perform NE tagging. The first stage 
POSA performs POS tagging by using a CRF 
trained model to assign POS tags to each word in a 
sentence of datasetNE. The second stage NEA per-
forms NE tagging by using another CRF trained 
model that uses both the POS information as well 
65
as the NE information, to perform effective NE 
tagging. 
 
 
Figure 1. Two stage model for NE tagging using POS 
information 
 
However, although the accuracy of NE tagging 
improved over the one stage model, there was 
scope for further improvement. It is obvious that 
all the NE tagged words should have the proper 
noun (NNP) POS tag associated. But, when POS 
tags were generated for the NE tagged ground truth 
data in datasetNE, most of the words were either 
tagged as adjectives (JJ) or common nouns (NN).  
Most tags that come after case markers (CM) were 
adjectives (JJ) in the training data. Very few ac-
counted for proper nouns after case markers. This 
adversely affected the NE tagger output. It was 
also noticed that the POS tagger tagged most of the 
proper nouns (NNP) as common nouns (NN) be-
cause of the sparseness of the proper noun tag in 
the POS ground truth data set datasetPOS. This ob-
servation made us look to bootstrapping techniques 
for effective learning.  
We propose a four stage model as shown in Fig-
ure 2, for NE tagging. Three of the stages are 
trained using the CRF learning approach and one 
stage uses a rule based approach.  All four stages 
are trained using unigram features on tags and 
words and bigram features on tags. The POS 
tagged dataset, datasetPOS, consists of words and 
associated POS tags and the NE tagged dataset, 
datasetNE, consists of words and associated NE 
tags. We divide both datasets into training and test-
ing partitions. datasetPOS is divided into trainsetPOS 
and testsetPOS and datasetNE is divided into train-
setNE and testsetNE. 
 
 
Figure 2. Four stage model for NE tagging using POS 
information with bootstrapping 
 
In the model shown in Figure 2, POSA stage is a 
CRF based stage that is trained using trainsetPOS. 
Once trained, the POSA stage takes as input a sen-
tence and generates the associated POS tag for 
each word in that sentence.  
In order to increase the NNP tag associations to 
improve NE tagging, we generate POS tags for the 
NE training data in trainsetNE using the POSA 
stage. The POS tags generated at the POSA stage 
are called POSint. The POScorrection stage takes as 
input trainsetNE along with its associated POS tags, 
POSint. At this stage, correction rules - that change 
the POS tags of NE associated words to proper 
noun (NNP), assign Case Markers (CM) before 
and after the NE tags and verify proper tagging of 
Cardinals (CD) - are applied. The corrected POS 
tags are called POScorrected. A consolidated POS 
training set consisting of entries from both train-
setPOS and trainsetNE (with POScorrected generated as 
output from the POScorrection stage) is used to train 
the CRF based POSB stage. This stage is the final 
POS tagging stage. Test data consisting of sen-
tences (words) from testsetNE is sent as input to 
stage POSB and the output generated at stage POSB 
is the POS tag associated with each input word of a 
sentence. The NEB stage is a CRF based NE tagger 
that is trained on a dataset consisting of word and 
associated NE tags from trainsetNE and associated 
POS tags from POScorrected. This stage learns from 
the POS information and the NE information pro-
vided in the training data. Once trained, the NEB 
stage takes as input words from testsetNE and asso-
ciated POS tags (obtained at stage POSB) and ge-
nerates NE tags. 
The domain we are interested in is newswire 
material, and these articles are written in the ?jour-
66
nalistic? or ?news writing? style12. The articles are 
objective and follow a Subject-Object-Verb struc-
ture. Related information is usually presented with-
in close sentence proximity. This makes it possible 
to hand-craft grammar rules for the discovery of 
NE tags with fine granularity. The final POS 
tagged and NE tagged data generated as outputs at 
stage POSB and stage NEB respectively of the four 
stage model, are processed using rules and lexicon 
lookups to further improve the overall tagging ac-
curacy of the model. Rules used are mostly domain 
specific. The rules were applied to the model using 
Semantex. 
4.3 Rules for POS Tagging 
1. Our model tags all the Question Words 
(QW) like ????? - kya as pronoun (PR). All 
such occurrences are assigned QW tag. 
2. If the word is ????? ? kya and the previous 
tag is an adjective (JJ) and the next tag is a 
phrase marker (PM) then assign a light 
verb tag (VBL) else assign a verb (VB) tag 
to the word. 
3. It was observed that there were spurious 
instances of proper nouns getting tagged as 
nouns. In order to correct this error, if a 
word ends with any of the characters 
shown below, and the word was tagged as 
a noun, then the tag on the word was 
changed to a proper noun.  
?%?, ??? ,???, ?()?, ?*+?, 
?,-?, ????, ?  0*- ?, ???? 
4. All valid cardinals were tagged as nouns or 
proper nouns by the model. This was re-
solved by looking for a digit in the string.  
4.4 Rules for %E Tagging 
1. Words like ?????? (court), ??????? (bu-
reau), ????? (army) etc. are looked up. If 
there are any nouns or proper nouns above 
these within a window of two, then the tag 
on this word is ORGANIZATION. 
2. Words like ??????? (organization), ?????? 
are marked ORGANIZATION if the pre-
vious word is a proper noun. 
3. Lexicon look up for names of places is per-
formed and the POS tag of the next word 
that is found is checked. If this tag is a 
                                                           
12 http://en.wikipedia.org/wiki/News_writing 
Case Marker (CM) with a feminine gend-
er, like ???? (main) or ?????, then the 
word is marked with a LOCATION tag. 
4. If a proper noun that is selected ends with 
a suffix ?pur?, ?bad, ?dad? and has the 
same constraint as mentioned in rule 3, 
then the LOCATION tag is assigned to it 
as well. 
5 Results 
The NE tagging performance, for both the two 
stage model and the four stage model, are eva-
luated using Precision (P), Recall (R) and F-Score 
(FS) metrics, the equations for which are given 
below. 
(vii) NEs  taggedof No.
NEs taggedcorrectly  of No. P =  
(viii) setin test  NEs of no. Total
NEs  taggedof No.R =  
(ix) 
 PR
RPFS +=
2  
 
We performed a 10 fold cross validation test to 
determine the performance of the model. The data-
set is divided into 10 subsets of approximately 
equal size. One subset is withheld for testing and 
the remaining 9 subsets are used for training. This 
process is repeated for all 10 subsets and an aver-
age result is computed. The 10 fold validation test 
for NE tagging was performed for both the two 
stage as well as the four stage models. 
 
Set P R FS P R FS
1 48.09 73.25 58.06 60.54 78.7 68.44
2 38.94 72.42 50.65 60.29 80.46 68.93
3 56.98 74.38 64.53 60.54 79.74 68.83
4 38.44 78.05 51.51 60.54 80.79 69.21
5 32.29 75.91 45.31 60.79 80.34 69.21
6 44.82 88.02 59.4 59.31 79.93 68.09
7 45.75 69.75 55.26 61.04 81.73 69.89
8 43.52 71.5 54.11 60.05 80.36 68.74
9 44.64 81.97 57.8 59.93 81.09 68.92
10 44.17 78.18 56.45 60.67 79.22 68.72
Avg 43.764 76.343 55.308 60.37 80.236 68.898
Four Stage ModelTwo Stage Model
 
Table 1. NE tagging results for the two stage and four 
stage models 
 
It can be seen from Table 1 that the four stage 
model outperforms the two stage model with the 
67
average F-Score being 55.31% for the two stage 
model and 68.89% for the four stage model. 
Table 2 shows the POS tagging results for stages 
POSA and POSB. The POSB stage performs margi-
nally better than the POSA stage. 
 
Set P Set P
1 84.38 1 83.97
2 89.32 2 89.84
3 88.09 3 88.48
4 89.45 4 89.66
5 89.66 5 89.76
6 90.57 6 90.63
7 81.1 7 89.24
8 89.47 8 89.5
9 89 9 89.12
10 89.12 10 89.25
Avg 88.016 Avg 88.945
POSB ResultsPOSA Results
 
Table 2. POS tagging results for the two stage (POSA) 
and four stage (POSB) models 
 
Although for POS tagging, the improvement is 
not very significant between the two models, tags 
like light verbs (VBLI), auxiliary verbs (AUXA 
and AUXT), adjectives (JJ), demonstratives (DM) 
and nouns (NN, NNC, NNCM, NNCR) get tagged 
with higher accuracy in the four stage model as 
shown in Table 3. This improvement becomes evi-
dent in the NE test set. Unfortunately, since this 
data has no associated POS tagged ground truth, 
the results cannot be quantified. The trainsetPOS 
training data had very few instances of proper 
nouns (NNP) occurring after case markers (CM) 
and so most of the proper nouns were getting 
tagged as either adjectives (JJ) or common nouns 
(NN). After providing more training data to stage 
POSB, the model could effectively learn proper 
nouns. Spurious tagging of adjectives (JJ) and 
common nouns (NN) reduced while more proper 
nouns (NNP, NNPC) were tagged accurately and 
this allowed the NE stage to apply its learning effi-
ciently to the NE test set thereby improving the NE 
tagging results.  
The two stage model tagged 238 NE tagged 
words as proper nouns out of 403 NE words. The 
four stage model tagged 340 NE tagged words as 
proper nouns out of 403 NE words. The four stage 
model shows an improvement of 25.3% over the 
two stage model. The results reported for NE and 
POS tagging models are without considering rules 
or lexicon lookups. 
 
Tag FS Tag FS
AUXA 0.801 AUXA 0.816
AUXT 0.872 AUXT 0.898
DM 0.48 DM 0.521
JJ 0.751 JJ 0.765
NN 0.85 NN 0.858
NNC 0.537 NNC 0.549
NNCM 0.909 NNCM 0.923
NNCR 0.496 NNCR 0.51
RB 0.785 RB 0.834
VBLI 0.67 VBLI 0.693
VBT 0.553 VBT 0.586
POSA Output POSB Output
 
Table 3. POS tagging results for stages POSA and POSB 
 
In order to further improve the POS tagged re-
sults and NE tagged results, the rules mentioned in 
sections 4.3 and 4.4 and lexicon lookups were ap-
plied. Table 4 shows the result for NE tagging with 
an overall F-Score of 74.67% 
 
Tag P R FS
LOCATION 0.78 0.793 0.786
ORGANIZATION 0.775 0.731 0.752
PERSON 0.894 0.595 0.714
NEA Output
 
Table 4. NE tagging results after applying rules for test 
results in Table 1 
6. Conclusion and Future Work  
This work was undertaken as a precursor to 
achieve our final objective as discussed in Section 
1. The basic idea here is to increase the size of the 
available training data, by using bootstrapping, so 
as to maximize learning for NE tagging. The pro-
posed four stage model shows an F-Score of 68.9% 
for NE tagging which is much higher than that ob-
tained by the simple two stage model. 
A lot of avenues remain to be explored to fur-
ther improve the performance of the model. One 
approach would be to use the bootstrapping tech-
nique for NE data as well. However, the rules re-
quired can be complicated. More hand crafted rules 
and detailed lexicon lookups can result in better 
NE tagging. We have also noticed certain ambigui-
ties in tagging PERSON and LOCATION. Rules 
that resolve this ambiguity can be explored. 
68
References  
Raymond G. Gordon Jr. (ed.). 2005. Ethnologue: Lan-
guages of the World, Fifteenth edition. Dallas, TX.: 
SIL International 
Kashif Huda. 2001. An Overview of Urdu on the Web. 
 Annual of Urdu Studies Vol 20. 
Zaheer Ahmad, Jehanzeb Khan Orakzai, Inam Shamsh-
er, Awais Adnan. 2007. Urdu astaleeq Character 
Recognition. Proceedings of World Academy of 
Science, Engineering and Technology. Volume 26, 
ISSN 2070-3740. 
John T. Platts. 1967. A grammar of the Hindustani or 
Urdu language.  Munshiram Manoharlal Delhi. 
R. L. Schmidt. 1999. Urdu: an essential grammar. 
London: Routledge. 
Sarmad Hussain. 2008. Resources for Urdu Language 
Processing. The 6th Workshop on Asian Language 
Resources. 
P. Baker, A. Hardie, T. McEnery, B.D. Jayaram. 2003. 
Corpus Data for South Asian Language Processing. 
Proceedings of the 10th Annual Workshop for South 
Asian Language Processing, EACL. 
M. Humayoun, H. Hammarstrm, A. Ranta. 2007. Urdu 
Morphology, Orthography and Lexicon Extraction. 
CAASL-2: The Second Workshop on Computational 
Approaches to Arabic Script-based Languages, LSA 
2007 Linguistic Institute, Stanford University. 
Waseem Siddiqi, Shahab Alam. 2008. Online Urdu-
English and English-Urdu dictionary. 
N. Durrani. 2007. Typology of Word and Automatic 
Word Segmentation in Urdu Text Corpus. National 
University of Computer and Emerging Sciences, La-
hore, Pakistan. 
T. Theeramunkong, S. Usanavasin. 2001. on-
Dictionary Based Thai Word Segmentation Using 
decision trees. In proceedings of the First Interna-
tional Conference on Human Language Technology 
Research, San Diego, California, USA. 
A. Pornprasertkul. 1994. Thai Syntactic Analysis. Ph.D 
Thesis, Asian Institute of Technology. 
Ismat Javed. 1981.  ??  ????? ????. Taraqqi Urdu Bureau, 
New Delhi. 
Abdul M. Haq. 1987.  ????  ??? ? ???. Amjuman-e-
Taraqqi Urdu (Hindi). 
Hassan Sajjad. 2007. Statistical Part of Speech Tagger 
for Urdu. National University of Computer and 
Emerging Sciences, Lahore, Pakistan. 
John D. Lafferty, Andrew McCallum, Fernando C.N. 
Pereira. 2001. Conditional Random Fields: Probabi-
listicModels for Segmenting and Labeling Sequence 
Data. Proceedings of the Eighteenth International 
Conference on Machine Learning, pp. 282-289. 
John Chen. 2006. How to use Sequence Tagger. Seman-
tex Documentation, Janya Inc. 
Bikel, D.M., Schwartz, R.L., Weischedel, R.M.1999. 
An Algorithm that Learns What?s in a ame. Ma-
chine Learning 34(1-3), pp. 211?231. 
Borthwick, A. 1999. Maximum Entropy Approach to 
amed Entity Recognition. PhD thesis, New York 
University. 
McCallum, A., Li, W. 2003. Early results for amed 
Entity Recognition with Conditional Random Fields, 
Feature Induction and Web-enhanced Lexicons. In 
Proceedings of CoNLL. 
A. Hardie. 2003. Developing a tagset for automated 
part-of-speech tagging in Urdu. Department of Lin-
guistics and Modern English Language, University 
of Lancaster. 
Leech, G and Wilson, A. 1999. Standards for tagsets. 
Edited version of EAGLES Recommendations for the 
Morphosyntactic Annotation of Corpora. In van Hal-
teren, H (ed.) Syntactic wordclass tagging. Dor-
drecht: Kluwer Academic Publishers. 
Awaghad Ashish Krishnarao, Himanshu Gahlot, Amit 
Srinet and D. S. Kushwaha.  2009. A Comparative 
Study of amed Entity Recognition for Hindi Using 
Sequential Learning Algorithms. In IEEE Interna-
tional Advance Computing Conference (IACC '09), 
Thapar University, India. March 6-7. 
Thamar Solario. 2004. Improvement of amed Entity 
Tagging by Machine Learning, Technical Report 
CCC-04-004, Coordinacin de Ciencias Computatio-
nales. 
Ekbal, A. and Bandyopadhyay, S. 2007. A Hidden 
Markov Model Based amed Entity Recognition Sys-
tem: Bengali and Hindi as Case Studies. Springer 
LNCS, Vol. 4815, pp. 545. 
R. K. Srihari, W. Li, C. Niu and T. Cornell,"InfoXtract: 
A Customizable Intermediate Level Information Ex-
traction Engine," Journal of atural Language En-
gineering, Cambridge U. Press, 14(1), 2008, pp..33-
69. 
 
69
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 797?805,
Beijing, August 2010
Using Cross-Lingual Projections to Generate Semantic Role  
Labeled Corpus for Urdu - A Resource Poor Language 
Smruthi Mukund 
CEDAR 
University at Buffalo 
smukund@buffalo.edu 
Debanjan Ghosh 
Thomson Reuters R&D 
debanjan.ghosh@ 
thomsonreuters.com 
Rohini K. Srihari 
CEDAR 
University at Buffalo 
rohini@cedar.buffalo.edu 
 
Abstract 
In this paper we explore the possibility of 
using cross lingual projections that help 
to automatically induce role-semantic 
annotations in the PropBank paradigm 
for Urdu, a resource poor language. This 
technique provides annotation projections 
based on word alignments. It is relatively 
inexpensive and has the potential to re-
duce human effort involved in creating 
semantic role resources. The projection 
model exploits lexical as well as syntac-
tic information on an English-Urdu paral-
lel corpus. We show that our method ge-
nerates reasonably good annotations with 
an accuracy of 92% on short structured 
sentences. Using the automatically gen-
erated annotated corpus, we conduct pre-
liminary experiments to create a semantic 
role labeler for Urdu. The results of the 
labeler though modest, are promising and 
indicate the potential of our technique to 
generate large scale annotations for Urdu.  
1 Introduction 
Semantic Roles (also known as thematic roles) 
help to understand the semantic structure of a 
document (Fillmore, 1968). At a fundamental 
level, they help to capture the similarities and 
differences in the meaning of verbs via the ar-
guments they define by generalizing over surface 
syntactic configurations.  In turn, these roles aid 
in domain independent understanding as the se-
mantic frames and semantic understanding sys-
tems do not depend on the syntactic configura-
tion for each new application domain. Identify-
ing semantic roles benefit several language 
processing tasks - information extraction (Sur-
deanu et al, 2003), text categorization (Moschitti, 
2008) and finding relations in textual entailment 
(Burchardt and Frank 2006). 
Automatically identifying semantic roles is of-
ten referred to as shallow semantic parsing (Gil-
dea and Jurafsky, 2002). For English, this 
process is facilitated by the existence of two 
main SRL annotated corpora ? FrameNet (Baker 
et al, 1998) and PropBank (Palmer et al, 2005). 
Both datasets mark almost all surface realizations 
of semantic roles. FrameNet has 800 semantic 
frames that cover 120,000 example sentences1. 
PropBank has annotations that cover over 
113,000 predicate-argument structures. Clearly 
English is well supported with resources for se-
mantic roles. However, there are other widely 
spoken resource poor languages that are not as 
privileged. The PropBank based resources avail-
able for languages like Chinese (Xue and Palmer, 
2009), Korean (Palmer et al, 2006) and Spanish 
(Taule, 2008) are only about two-thirds the size 
of the English PropBank.  
Several alternative techniques have been ex-
plored in the literature to generate semantic role 
labeled corpora for resource poor languages as 
providing manually annotated data is time con-
suming and involves intense human labor. Am-
bati and Chen (2007) have conducted an exten-
sive survey and outlined the benefits of using 
parallel corpora to transfer annotations. A wide 
range of annotations from part of speech (Hi and 
Hwa, 2005) and chunks (Yarowsky et al, 2001) 
to word senses (Diab and Resnik, 2002), depen-
dencies (Hwa et al, 2002) and semantic roles 
(Pado and Lapata, 2009) have been successfully 
transferred between languages. FrameNet style 
annotations in Chinese is obtained by mapping 
English FrameNet entries directly to concepts 
listed in HowNet2 (online ontology for Chinese) 
with an accuracy of 68% (Fung and Chen, 2004). 
                                                 
1 Wikipedia - http://en.wikipedia.org/wiki/PropBank 
2 http://www.keenage.com/html/e_index.html 
797
Fung et al (2007) analyze an automatically an-
notated English-Chinese parallel corpus and 
show high cross-lingual agreement for PropBank 
roles (range of 75%-95% based on the roles).  
In this paper we explore the possibility of us-
ing English-Urdu parallel corpora to generate 
SRL annotations for Urdu, a less commonly 
taught language (LCTL). Earlier attempts to gen-
erate SRL corpora using annotation projections 
have been for languages such as German, French 
(Pado and Lapata, 2009) and Italian (Moschitti, 
2009) that have high vocabulary overlap with 
English. Also, German belongs to the same lan-
guage family as English (Germanic family). Ur-
du on the other hand is an Indic language that is 
grammatically very different and shares almost 
no vocabulary with English.  
The technique of cross lingual projections war-
rants good BLEU score that ensures correct word 
alignments. According to NIST 2008 Open Ma-
chine Translation challenge 3 , a 0.2280 best 
BLEU score was achieved for Urdu to English 
translation. This is comparable to the BLEU 
scores achieved for German to English ? 0.253 
and French to English ? 0.3 (Koehn, 2005). But, 
for SRL transfer, perfect word alignment is not 
mandatory as SRL requires semantic correspon-
dence only. According to Fillmore (1982) se-
mantic frames are based on conceptual structures. 
They are generalizations over surface structures 
and hence less prone to syntactic variations. 
Since English and Urdu have a reasonable se-
mantic correspondence (Example 3), we believe 
that the projections when capped with a post 
processing step will considerably reduce the 
noise induced by inaccurate alignments and pro-
duce acceptable mappings. 
Hindi is syntactically similar to Urdu. These 
languages are standardized forms of Hindustani. 
They are free word order languages and follow a 
general SOV (Subject-Object-Verb) structure. 
Projection approach has been used by (Mukerjee 
et al, 2006) and (Sinha, 2009) to transfer verb 
predicates from English onto Hindi. Sinha (2009) 
achieves a 90% F-Measure in verb predicate 
transfer from English to Hindi. This shows that 
using cross lingual transfer approach to obtain 
semantic annotations for Urdu from English is an 
idea worth exploring. 
                                                 
3http://www.itl.nist.gov/iaui/894.01/tests/mt/2008/doc/mt08
_official_results_v0.html 
1.1 Approach 
Our approach leverages existing English 
PropBank annotations provided via the SemLink4 
corpus. SemLink provides annotations for 
VerbNet using the pb (PropBank) attribute. By 
using English-Urdu parallel corpus we acquire 
verb predicates and their arguments. When we 
transfer verb predicates (lemmas), we also 
transfer pb attributes. We obtain annotation 
projections from the parallel corpora as follows:  
1. Take a pair of sentences E (in English) and U 
(in Urdu) that are translations of each other.  
2. Annotate E with semantic roles. 
3. Project the annotations from E onto U using 
word alignment information, lexical 
information and linguistic rules that involve 
syntactic information. 
There are several challenges to the annotation 
projection technique. Dorr (1994) presents some 
major lexical-semantic divergence problems 
applicable in this scenario:  
(a) Thematic Divergence - In some cases, al-
though there exists semantic parallelism, the 
theme of the English sentence captured in 
the subject changes into an object in the Ur-
du sentence (Example 1). 
(b) Conflatational Divergence - Sometimes tar-
get translations spans over a group of words 
(Example 1: plays is mapped to kirdar ada). 
Trying to ascertain this word span for se-
mantic roles is difficult as the alignments 
can be incomplete and very noisy. 
(c) Demotional divergence and Structural di-
vergence - Despite semantic relatedness, in 
some sentence pairs, alignments obtained 
from simple projections generate random 
matchings as the usage is syntactically dis-
similar (Example 2). 
Handling all challenges adds complexity to our 
model. The heuristic rules that we implement are 
guided by linguistic knowledge of Urdu. This 
increases the effectiveness of the alignments. 
 
Example 1: 
I 
(subject) 
am Angry at Reheem 
(object) 
 
Raheem 
(subject)  
mujhe 
(object) 
Gussa dilate hai 
(Raheem brings anger in me) 
                                                 
4 http://verbs.colorado.edu/semlink/ 
798
Example 2: (noun phrase to prepositional pharse) 
Ali attended work today 
 
Ali aaj daftar mein haazir tha 
(Ali was present at work today) 
2 Generating Parallel Corpora 
PropBank provides SRL annotated corpora for 
English. It uses predicate independent labels 
(ARG0, ARG1, etc.) which indicate how a verb 
relates to its arguments. The argument types are 
consistent across all uses of a single verb and do 
not consider the sense of the verb. We use the 
PropBank annotations provided for the Wall 
Street Journal (WSJ) part of the Penn Tree bank 
corpus (Marcus et al, 2004). The arguments of a 
verb are labeled sequentially from ARG0 to 
ARG5 where ARG0 is the proto-typical Agent, 
ARG1 is the proto-typical patient, ARG2 is the 
recipient, and so on. There are other adjunct tags 
in the dataset that are indicated by ARGM that 
include tags for location (ARGM-LOC), tempor-
al tags (ARGM-TMP) etc.  
An Urdu corpus of 6000 sentences corres-
ponding to 317 WSJ articles of Penn Tree Bank 
corpus is provided by CRULP5 (used in the NIST 
2008 machine translation task). We consider 
2350 English sentences with PropBank annota-
tions that have corresponding Urdu translations 
(CRULP corpus) for our experiments. 
2.1 Sentence Alignment 
Sentence alignment is a prerequisite for any pa-
rallel corpora processing. As the first step, we 
had to generate a perfect sentence aligned paral-
lel corpus as the translated sentences, despite 
belonging to the same domain (WSJ ? Penn tree 
bank), had several errors in demarcating the sen-
tence boundaries.  
Sentence alignment between English and Urdu 
is achieved over two iterations. In the first itera-
tion, the length of each sentence is calculated 
based on the occurrence of words belonging to 
important part of speech categories such as prop-
er nouns, adjectives and verbs. Considering main 
POS categories for length assessment helps over-
come the conflatational divergence issue. For 
each English sentence, Urdu sentences with the 
same length are considered to be probable candi-
                                                 
5http://www.crulp.org/ 
dates for alignment. In the second iteration, an 
Urdu-English lexicon is used on the Urdu corpus 
and English translations are obtained. An Eng-
lish-Urdu sentence pair with maximum lexical 
match is considered to be sentence aligned.  
Clearly this method is highly dependent on the 
existence of an exhaustive Urdu-English dictio-
nary. The lexicons that we use to perform loo-
kups are collected by mining Wikipedia and oth-
er online resources (Mukund et al, 2010). How-
ever, lexicon lookups will fail for Out-Of-
Vocabulary words. There could also be a colli-
sion if Urdu sentences have English transliterated 
words (Example 3, ?office?). Such errors are 
manually verified for correctness. 
 
Example 3: 
Kya  aaj tum office gaye ? 
 
Did you go to the office today ? 
2.2 Word Alignment 
In the case of generating word alignments it is 
beneficial to calculate alignments in both transla-
tion directions (English ? Urdu and Urdu - Eng-
lish). This nature of symmetry will help to re-
duce alignment errors. We use the Berkeley 
Aligner6 word alignment package which imple-
ments a joint training model with posterior de-
coding (Liang et al, 2006) to consider bidirec-
tional alignments. Predictions are made based on 
the agreements obtained by two bidirectional 
models in the training phase. The intuitive objec-
tive function that incorporates data likelihood 
and a measure of agreement between the models 
is maximized using an EM-like algorithm. This 
alignment model is known to provide 29% re-
duction in AER over IBM model 4 predictions.  
On our data set the word alignment accuracy 
is 71.3% (calculated over 200 sentence pairs). In 
order to augment the alignment accuracy, we 
added 3000 Urdu-English words and phrases ob-
tained from the Urdu-English dictionary to our 
parallel corpus. The alignment accuracy im-
proved by 3% as the lexicon affects the word co-
occurrence count. 
Word alignment in itself does not produce ac-
curate semantic role projections from English to 
Urdu. This is because the verb predicates in Urdu 
can span more than one token. Semantic roles 
                                                 
6 http://nlp.cs.berkeley.edu/Main.html 
799
can cover sentential constituents of arbitrary 
length, and simply using word alignments for 
projection is likely to result in wrong role spans. 
Also, alignments are not obtained for all words. 
This could lead to missing projections. 
One way to correct these alignment errors is to 
devise token based heuristic rules. This is not 
very beneficial as writing generic rules is diffi-
cult and different errors demand specific rules. 
We propose a method that considers POS, tense 
and chunk information along with word align-
ments to project annotations. 
 
 
Figure 1: Projection model 
 
Our proposed approach can be explained in 
two stages as shown in figure 1. In Stage 1 only 
verb predicates are transferred from English to 
Urdu. Stage 2 involves transfer of arguments and 
depends on the output of Stage 1. Predicate 
transfer cannot rely entirely on word alignments 
(?3). Rules devised around the chunk boundaries 
boost the verb predicate recognition rate. 
Any verb group sequence consisting of a main 
verb and its auxiliaries are marked as a verb 
chunk. Urdu data is tagged using the chunk tag 
set proposed exclusively for Indian languages by 
Bharati et al, (2006). Table 1 shows the tags that 
are important for this task. 
 
Verb Chunk Description 
VGF Verb group is finite  (decided by the auxiliaries) 
VGNF Verb group for non-finite adverbial and adjectival chunk 
VGNN Verb group has a gerund 
Table 1: Verb chunk tags in Urdu 
The sentence aligned parallel corpora that we 
feed as input to our model is POS tagged for both 
English and Urdu. Urdu data is also tagged for 
chunk boundaries and morphological features 
like tense, gender and number information.  
Named Entities are also marked on the Urdu data 
set as they help in tagging the ARGM arguments. 
All the NLP taggers (POS, NE, Chunker, and 
Morphological Analyzer) used in this work are 
detailed in Mukund et al, (2010). 
English data is not chunked using a conven-
tional chunk tagger. Each English sentence is 
split into virtual phrases at boundaries deter-
mined by the following parts of speech ? IN, TO, 
MD, POS, CC, DT, SYM,: (Penn Tree Bank tag-
set). These tags represent positions in a sentence 
that typically mark context transitions (they are 
mostly the closed class words). We show later 
how these approximate chunks assist in correct-
ing predicate mappings. 
We use an Urdu-English dictionary (?2.1) that 
assigns English meanings to Urdu words in each 
sentence. Using translation information from a 
dictionary can help transfer verb predicates when 
the translation equivalent preserves the lexical 
meaning of the source language.  
The first rule that gets applied for predicate 
transfer is based on lexicon lookup. If the Eng-
lish verb is found to be a synonym to an Urdu 
word that is part of a verb chunk, then the lemma 
associated with the English word is transferred to 
the entire verb chunk in Urdu. However not all 
translations? equivalents are lexically synonym-
ous. Sometimes the word used in Urdu is differ-
ent in meaning to that in English but relevant in 
the context (lexical divergence).  
The word alignments considered in proximity 
to the approximate English chunks come to res-
cue in such scenarios. Here, for all the words 
occurring in each Urdu verb chunk, correspond-
ing English aligned words are found from the 
word alignments. If the words that are found be-
long to the same approximate English chunk, 
then the verb predicate of that chunk (if present) 
is projected onto the verb chunk in Urdu. This 
heuristic technique increases the verb projection 
accuracy by about 15% as shown in ?4. 
The Penn tree bank tag set for English part of 
speech has different tags for verbs based on the 
tense information. VBD is used to indicate past 
tense, and VBP and VBZ for present tense. Urdu 
800
also has the tense information associated with the 
verbs in some cases. We exploit this similarity to 
project the verb predicates from English onto 
Urdu. 
The adverbial chunk in Urdu includes pure ad-
verbial phrases. These chunks also form part of 
the verb predicates.  
   S 
 
 
RBP          NP                        VGNF 
 
RB         NN   VB     AUXA    
(??????/dobara)     (???/jaan)  (????/dali)        (???/gayi) 
[English meaning ? Revitalized] 
Figure 2: example for demotional divergence 
 
E.g. consider the English word ?revitalized? 
(figure 2). This is tagged VBD. However, the Ur-
du equivalent of this word is ??????? ??? ???? ???? 
(dobara jaan daali gayi ~ to put life in again). 
The POS tags are ?RB, NN, VB, AUXA? (adverb, 
noun, verb, aspectual auxiliary). The word ?do-
bara? is a part of the adverbial chunk RBP and 
the infinite verb chunk VGNF spans across the 
last two words ?daali gayi?. ?jaan? is a noun 
chunk. This kind of demotional divergence is 
commonly observed in languages like Hindi and 
Urdu. In order to consider this entire phrase to be 
the Urdu equivalent representation of the English 
word ?revitalized?, a rule for adverbial chunk is 
included as the last step to account for un-
accommodated English verbs in the projections. 
In the PropBank corpus, predicate argument re-
lations are marked for almost all occurrences of 
non-copula verbs. We however do not have POS 
tags that help to identify non-copula words. 
Words that can be auxiliary verbs occur as non-
copula verbs in Urdu. We maintain a list of such 
auxiliary verbs. When the verb chunk in Urdu 
contains only one word and belongs to the list, 
we simply ignore the verb chunk and proceed to 
the next chunk. This avoids several false posi-
tives in verb projections.  
Stage 2 of the model includes the transfer of 
arguments. In order to see how well our method 
works, we project all argument annotations from 
English onto Urdu. We do not consider word 
alignments for arguments with proper nouns. The 
double metaphone algorithm (Philips 2000) is 
applied on both English NNP (proper noun) 
tagged words as well as English transliterated 
Urdu (NNP) tagged words. Arguments from 
English are mapped onto Urdu for word pairs 
with the same metaphone code. 
For other arguments, we consider word align-
ments in proximity to verb predicates. The argu-
ment boundaries are determined based on chunk 
and POS information. We observe that our me-
thod projects the annotations associated with 
nouns fairly well. However, when the arguments 
contain adjectives, the boundaries are disconti-
nuous. In such cases, we consider the entire 
chunk without the case marker as a probable 
candidate for the projected argument. We also 
have some issues with the ARGM-MOD argu-
ments in that they overlap with the verb predi-
cates. When the verb predicate that it overlaps 
with is a complex predicate, we consider the en-
tire verb chunk to be the Urdu equivalent candi-
date argument. These rules along with word 
alignments yield fairly accurate projections.  
The rules that we propose are dependent on the 
POS, chunk and tense information that are lan-
guage specific. Hence our method is language 
independent only to the extent that the new lan-
guage considered should have similar syntactic 
structure as Urdu. Indic languages fall in this 
category. 
3 Verb Predicates 
Detecting verb predicates can be a challenging 
task especially if very reliable and efficient tools 
such as POS tagger and chunkers are not availa-
ble. We apply the POS tagger (CRULP tagset, 
88% F-Score) and Chunker (Hindi tagset, 90% 
F-Score) provided by Mukund et al, (2010) on 
the Urdu data set and show that syntactic infor-
mation helps to compensate alignment errors. 
Stanford POS tagger7 (Penn Tree bank tagset) is 
applied on the English data set. 
Predicates can be simple predicates that lie 
within the chunk boundary or complex predicates 
when they span across chunk boundaries. When 
verbs in English are expressed in Urdu/Hindi, in 
several cases, more than one word is used to 
achieve perfect translation. In English the tense 
of the verb is mostly captured by the verb mor-
pheme such as ?asked? ?said? ?saying?. In Ur-
du the tense is mostly captured by the auxiliary 
verbs. So a single word English verb such as 
?talking? would be translated into two words 
                                                 
7 http://nlp.stanford.edu/software/tagger.shtml 
801
?batein karna? where ?karna?~ do is the aux-
iliary verb. However this cannot be generalized 
as there are instances when translations are word 
to word. E.g. ?said? is mapped to a single word 
Urdu verb ?kaha?. 
Complex predicates in Urdu can occur in the 
following POS combinations. /oun+Verb, Ad-
jective+Verb, Verb+Verb, Adverb+Verb. Table 2 
lists the main verb tags present in the Urdu POS 
tagset. (refer Penn Tree bank POS tagset for 
English tags). 
 
Urdu Tags Description 
VB Verb 
VBI Infinitive Verb 
VBL Light Verb 
VBLI Infinitive Light Verb 
VBT Verb to be 
AUXA Aspectual Auxiliary 
AUXT Tense Auxiliary 
Table 2: Verb tags 
 
Auxiliary verbs in Urdu occur alongside VB, 
VBI, VBL or VBLI tags. Sinha (2009) defines 
complex predicates as a group of words consist-
ing of a noun (NN/NNP), an adjective (JJ), a verb 
(VB) or an adverb (RB) followed by a light verb 
(VBL/VBLI). Light verbs are those which contri-
bute to the tense and agreement of the verb (Butt 
and Geuder, 2001). However, despite the exis-
tence of a light verb tag, it is noticed that in sev-
eral sentences, verbs followed by auxiliary verbs 
need to be grouped as a single predicate. Hence, 
we consider such combinations as belonging to 
the complex predicate category.  
E1G- According_VBG to_TO some_DT estimates_NNS 
the_DT rule_NN changes_NNS would_MD cut_VB insid-
er_NN filings_NNS by_IN more_JJR than_IN a_DT 
third_JJ 
URD- [Kuch_QN  andaazon_NN  ke_CM  muta-
biq_NNCM]_NP [kanoon_NN mein_CM]_NP [tabdee-
liayan_NN]_NP[ androni_JJ    drjbndywn_NN  
ko_CM]_NP [ayk_CD thayiy_FR se_CM]_NP [zyada_I 
kam_JJ]_JJP [karey_VBL gi_AUXT]_VGF 
Example 4 
Example 4 demonstrates the existence of a light 
verb in a complex predicate. The English verb 
?cut? is mapped to ??? ???? ??? (kam karey gi) 
belonging to the VBF chunk group.  
EG- Rolls_NNP -_: Royce_NNP Motor_NNP 
Cars_NNPS Inc._NNP said_VBD it_PRP expects_VBZ 
its_PRP$ U.S._NNP sales_NNS to_TO remain_VB 
steady_JJ at_IN about_IN 1 200_CD cars_NNS in_IN 
1990_CD 
URD - [Rolls  Royce motor car inc_NNPC ne_CM]_NP 
[kaha_VB]_VBNF [wo_PRP]_NP [apney_PRRFP$]_NP 
[U.S._NNP ki_CM]_NP [ frwKt_NN ko_CM]_NP 
[1990_CD mein_CM]_NP [takreeban_RB]_RBP [1200_CD 
karon_NN par_CM]_NP [mtwazn_JJ]_JJP [rakhne_VBI 
ki_CM]_VGNN [tawaqqo_NN]_NP [karte_VB 
hai_AUXT]_VGF 
Example 5 
 
In example 5, ?said? corresponds to one Urdu 
word ?????(kaha) that also captures the tense in-
formation (past). However, consider the verb 
?expects?. This is a clear case of noun-verb 
complex predicate where ?expects? is mapped to 
????? ???? ???(tawaqqo karte hai). 
E1G- /ot_RB all_PDT those_DT who_WP wrote_VBD 
oppose_VBP the_DT changes_NNS  
URD -wo tamaam  jinhon ne likha tabdeeliyon ke [mukha-
lif_JJ]_JJP [nahi_RB]_RBP [hain_VBT]_VGF 
Example 6 
 
In example 6, verb predicates are ?wrote? and 
?oppose?. Consider the word ?oppose?. There 
are two ways of representing this word in Urdu. 
As a verb chunk the translation would be ?muk-
halifat nahi karte? and as an adjectival chunk 
?mukhalif nahi hai?. The latter form of repre-
sentation is used widely in the available transla-
tion corpus. The Urdu equivalent of ?oppose? is 
?????? ????(mukhalif hai). 
Another interesting observation in example 6 is 
the existence of discontinuous predicates. 
Though ?oppose? is one word in English, the 
Urdu representation has two words that do not 
occur together. The adverb ?nahi? ~?not? oc-
curs between the adjective and the verb. Statisti-
cally dealing with this issue is extremely chal-
lenging and affects the boundaries of other ar-
guments. Generalizing the rules needed to identi-
fy discontinuous predicates requires more de-
tailed analysis of the corpus ? from the linguistic 
aspect ? and has not been attempted in this paper. 
We however map ? ??? ???? ????? ?(mukhalif nahi 
hai) to the predicate ?oppose?. ?nahi? is treated 
as an argument ARG_NEG in PropBank. 
4 Projection Results 
It is impossible for us to report our projection 
results on the entire data set as we do not have it 
manually annotated. For the purpose of evalua-
tion, we manually annotated 100 long sentences 
(L) and 100 short sentences (S) from the full 
2350 sentence set. All the results are reported on 
802
this 200 set of sentences. Set L has sentences that 
each has more than two verb predicates and sev-
eral arguments. The number of words per sen-
tence here is greater than 55.  S; on the other 
hand has sentences with about 40 words each and 
no complex SOV structures. 
The results shown in Table 3 are for all tags 
(verbs+args) that are projected from English onto 
Urdu. In order to understand why the perfor-
mance over L dips, consider the results in Table 
4 that are for verb projections only. Some long 
sentences in English have Urdu translations that 
do not maintain the same structure. For example 
an English phrase ? ?? might prompt individu-
als to get out of stocks altogether? is written in 
Urdu in a way that the English representation 
would be ?what makes individuals to get out of 
stocks is ??. The Urdu equivalent word for 
?prompt? is missing and the associated lemma 
gets assigned to the Urdu equivalent of ?get? 
(the next lemma). This also affects the argument 
projections. Another reason is the effect of word 
alignments itself. Clearly longer sentences have 
greater alignment errors. 
All tags8 100 long sentences 
100 short 
sentences 
Actual Tags 1267 372 
Correct Tags 943 325 
Found Tags 1212 353 
L :  Precision 77.8% Recall 74.4% F-Score 76% 
S:  Precision 92% Recall 87.4% F-Score 89.7% 
Table 3: when all tags are considered 
 
Comparing the results of Table 4 to Table 3, 
we see that argument projections affect the re-
call. This is because the projections of arguments 
depend not only on the word alignments but also 
on the verb predicates. Incorrect verb predicates 
affect the argument projections. 
Only lemma 100 long sentences 
100 short 
sentences 
Actual Tags 670 240 
Correct Tags 490 208 
Overall Tags 720 257 
L: Precision 68% Recall 73.1% F-Score 70.45% 
S : Precision 80.9% Recall 86.6% F-Score 83.65% 
Table 4: for verb projections only 
Table 5 summarizes the results obtained when 
only the word alignments are considered to 
                                                 
8 Tags -  lemma (verb predicates) + arguments, Actual tags 
? number of tags in the English set, Found tags ? number of 
tags transferred to Urdu, Correct Tags ? number of tags 
correctly transferred 
project all tags. But when virtual phrase bounda-
ries in English are also considered, the F-score 
improves by 8% (Table 6). This is because vir-
tual boundaries in a way mark context switch and 
when considered in proximity to the word align-
ments yield better predicate boundaries. 
100 long sentences : only alignments 
Actual Tags 1267 
Correct Tags 617 
Overall Tags 782 
Precision 78.9% Recall 48.7% F-Score 60.2% 
Table 5: with only word alignments  
 
100 long sentences : alignments + virtual boundaries 
Actual Tags 1267 
Correct Tags 792 
Overall Tags 1044 
Precision 75.8% Recall  62.5% F-Score 68.5% 
Table 6: with word alignments and virtual boundaries 
 
100 
Sentences 
ARG
0 
ARG
1 
ARG
2 
ARG
3 
ARG
M 
Long 124 271 67 25 140 
Found 111 203 36 12 114 
P % 89.5 74.9 53.7 48 81.42 
Short 34 47 4 2 19 
Found 30 45 4 2 19 
P % 88.2 95.7 100 100 100 
Table 7: results of argument projections 
Precision (P) on arguments 
 
Table 7 shows the results of argument projec-
tions over the first 4 arguments of PropBank ? 
ARG0, ARG1, ARG2 and ARG3 (out of 24 argu-
ments, majority are sparse in our test set) and the 
adjunct tag set ARGM.  
5 Automatic Detection 
The size of SRL annotated corpus generated for 
Urdu is limited with only 2350 sentences. To 
explore the possibilities of augmenting this data 
set, we train verb predicate and argument detec-
tion models. The results show great promise in 
generating large-scale automatic annotations. 
5.1 Verb Predicate Detection 
Verb predicate detection happens in two stag-
es. In the first stage, the predicate boundaries are 
marked using a CRF (Lafferty et al, 2001) based 
sequence labeling approach. The training data for 
the model is generated by annotating the auto-
matically annotated Urdu SRL corpus using BI 
803
annotations. E.g. kam B-VG, karne par I-VG. The 
non-verb predicates are labeled ?-1?. The model 
uses POS, chunk and lexical information as fea-
tures. We report the results on a set of 77 sen-
tences containing a mix of short and long sen-
tences.  
Number of verb predicates correctly marked 377 
Num of verb predicates found 484 
Actual num of verb predicates 451 
Precision 77.8% Recall 83.5% F-Score 80.54% 
Table 8: CRF results for verb boundaries 
Every verb predicate is associated with a lemma 
mapped from the English VerbNet map file9. E.g. 
the Urdu verb ???  ????  ??? (kam karne par) has 
the lemma ?lower?. The second stage includes 
assigning these lemmas. Lemma assignment is 
based on lookups from a VerbNet like map file. 
We have compiled a large set of Urdu verb pre-
dicates by mapping translations found in the au-
tomatically annotated corpus to the VerbNet map 
file. This Urdu verb predicate list also accommo-
dates complex predicates that occur along with 
verbs such as ?karna ? to do?, ?paana ? to get?, 
etc. (along with different variations of these 
verbs ? karte, kiya, paate etc.). This verb predi-
cate list (manually corrected) consists of 800 en-
tries. Since our gold standard test set is very 
small, the lemma assignment for all verb predi-
cates is 100% (no pb values and hence no 
senses). This list, however, has to be augmented 
further to meet the standards of the English 
VerbNet map file. 
5.2 Argument Detection 
Argument detection (SRL) is done in two steps: 
(1) argument boundary detection (2) argument 
label assignment. We perform tests for step 2 to 
show how well a standard SVM role detection 
model works on the automatically generated Ur-
du data set. For each pair of correct predicate p 
and an argument i we create a feature representa-
tion F p,a  ~ set T of all arguments. To train a mul-
ti-class role-classifier, given the set T of all ar-
guments, T can be rationalized as T arg i+  (positive 
instances) and T arg i?  (negative instances) for each 
argument i. In this way, individual ONE-vs-ALL 
(Gildea and Jurafsky, 2002) classifier for each 
                                                 
9 http://verbs.colorado.edu/semlink/semlink1.1/vn-
pb/README.TXT 
argument i is trained. In the testing phrase, given 
an unseen sentence, for each argument Fp,q is 
generated and classified by each individual clas-
sifier.  
We created a set of standard SRL features as 
shown in table 9. The results (Tables 10 and 11), 
though not impressive, are promising. We be-
lieve that by increasing the number of samples 
(for each argument) in the training set and intel-
ligently controlling the negative samples, the 
results can be improved significantly. 
Training ? 2270 sentences with 7315 argument instances. 
Test ? 77 sentences with 496 argument instances. (22 dif-
ferent role types) 
BaseLine 
Features 
(BL) 
phrase-type (syntactic category; NP, PP etc.), 
predicate (in our case, verb group), path (syn-
tactic path from the argument constituent to 
the predicate), head words (argument and the 
predicate respectively), position (whether the 
phrase is before or after the predicate)  
Detailed 
Features 
BL + POS (of the first word in the predicate), 
chunk tag of the predicate, POS (of the first 
word of the constituent argument), head word 
(of the verb group in a complex predicate), 
named entity (whether the argument contains 
any named entity, such as location, person, 
organization etc.) 
Table 9: Features for SRL 
 
Kernel/features Precision Recall F-Score 
LK ? BL 71.88 48.25 57.74 
LK ? all 73.91 47.55 57.87 
PK ? BL 74.19 48.25 58.47 
PK ?all (best) 73.47 49.65 59.26 
Table 10: Arg0 performance 
 
Kernel/features Precision Recall F-Score 
LK ? BL 69.35 22.87 34.40 
LK ? all 69.84 23.4 35.05 
PK ? BL 73.77 24.14 36.38 
PK ?all (best) 73.8 26.06 38.52 
Table 11: Arg1 Performances 
(PK - polynomial kernel LK ? Linear kernel) 
6 Conclusion 
In this work, we develop an alignment system 
that is tailor made to fit the SRL problem scope 
for Urdu. Furthermore, we have shown that de-
spite English being a totally different language, 
resources for Urdu can be generated if the subtle 
grammatical nuances of Urdu are accounted for 
while projecting the annotations. We plan to 
work on argument boundary detection and ex-
plore other features for argument detection. The 
lemma set generated for Urdu is being refined for 
finer granularity. 
804
References 
 
Ambati, Vamshi and Chen, Wei, 2007. Cross Lingual Syn-
tax Projection for Resource-Poor Languages. CMU. 
Baker, Collin .F., Charles J. Fillmore, John B. Lowe. 1998. 
The Berkeley Frame Net project. COLI/G-ACL. 
Bharati, Akshar, Dipti Misra Sharma, Lakshmi Bai and 
Rajeev Sangal. 2006. AnnCorra: Annotating Corpora 
Guidelines For POS And Chunk Annotation For Indian 
Language. Technical Report, Language Technologies 
Research Centre IIIT, Hyderabad. 
Burchardt, Aljoscha and Anette Frank. 2006. Approaching 
textual entailment with LFG and FrameNet frames. RTE-
2 Workshop. Venice, Italy. 
Butt, Miriam and Wilhelm Geuder. 2001.  On the 
(semi)lexical status of light verbs. /orbert Corver and 
Henk van Riemsdijk, (Eds.), Semi-lexical Categories: On 
the content of function words and the function of content 
words, Mouton de Gruyter, pp. 323?370, Berlin.  
Diab, Mona and Philip Resnik. 2002. An unsupervised me-
thod for word sense tagging using parallel corpora. 40th 
Annual Meeting of ACL, pp. 255-262, Philadelphia, PA. 
Dorr, Bonnie, J. 1994. Machine Translation Divergences: A 
Formal Description and Proposed Solution. ACL, Vol. 
20(4), pp. 597-631. 
Fillmore, Charles J. 1968. The case for case. Bach, & 
Harms( Eds.), Universals in Linguistic Theory, pp. 1-88. 
Holt, Rinehart, and Winston, New York. 
Fillmore, Charles J. 1982. Frame semantics. Linguistics in 
the Morning Calm, pp.111-137. Hanshin, Seoul, S. Ko-
rea. 
Fung, Pascale and Benfeng Chen. 2004. BiFrameNet: Bilin-
gual frame semantics resources construction by cross-
lingual induction. 20th International Conference on 
Computational Linguistics, pp. 931-935, Geneva, Swit-
zerland. 
Fung, Pascale, Zhaojun Wu, Yongsheng Yang and Dekai 
Wu. 2007. Learning bilingual semantic frames: Shallow 
semantic parsing vs. semantic role projection. 11th Con-
ference on Theoretical and Methodological Issues in 
Machine Translation, pp. 75-84, Skovde, Sweden. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labe-
ling of semantic roles. Computational Linguistics, Vol. 
28(3), pp. 245-288. 
Hi, Chenhai and Rebecca Hwa. 2005. A backoff model for 
bootstrapping resources for non-english languages. Joint 
Human Language Technology Conference and Confe-
rence on EM/LP, pp. 851-858, Vancouver, BC. 
Hwa, Rebecca, Philip Resnik, Amy Weinberg, and Okan 
Kolak. 2002. Evaluation translational correspondance us-
ing annotation projection. 40th Annual Meeting of ACL, 
pp. 392-399, Philadelphia, PA. 
Koehn, Phillip. 2005. ?Europarl: A parallel corpus for statis-
tical machine translation,? MT summit, Citeseer. 
Lafferty, John D., Andrew McCallum and C.N. Pereira. 
2001. Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. 18th Inter-
national Conference on Machine Learning, pp. 282-289. 
Liang, Percy, Ben Taskar, and Dan Klein. 2006. Alignment 
by Agreement, /AACL.  
Marcus, Mitchell P., Beatrice Santorini and Mary Ann Mar-
cinkiewicz. 2004. Building a large annotated corpus of 
English: The Penn Treebank. Computational Linguistics, 
Vol. 19(2), pp. 313-330.  
Moschitti, Alessandro. 2008. Kernel methods, syntax and 
semantics for relational text categorization. 17th ACM 
CIKM, pp. 253-262, Napa Valley, CA. 
Mukerjee, Amitabh , Ankit Soni and Achala M. Raina. 
2006. Detecting Complex Predicates in Hindi using POS 
Projection across Parallel Corpora. Workshop on Multi-
word Expressions: Identifying and Exploiting Underly-
ing Properties, pp. 11?18. Sydney. 
Mukund, S., Srihari, R. K., and Peterson, E. 2010. An In-
formation Extraction System for Urdu ? A Resource Poor 
Language. Special Issue on Information Retrieval for In-
dian Languages. TALIP. 
Pado, Sebastian and Mirella Lapata. 2009. Cross-Lingual 
annotation Projection of Semantic Roles. Journal of Ar-
tificial Intelligence Research, Vol. 36, pp. 307-340. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of semantic 
roles. Computational Linguistics, Vol. 31(1). 
Palmer, Martha, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, and Yeongmi Jeon. 2006. Korean Propbank. Lin-
guistic data consortium, Philadelphia. 
Philips, Lawrence. 2000. The Double Metaphone Search 
Algorithm. C/C++ Users Journal. 
Sinha, R. Mahesh K. 2009. Mining Complex Predicates In 
Hindi Using A Parallel Hindi-English Corpus. ACL In-
ternational Joint Conference in /atural Language 
Processing, pp 40. 
Surdeanu, Mihai, Sanda Harabagiu, John Williams and Paul 
Aarseth. 2003. Using predicate-argument structures for 
information extraction. 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 8-15, Sappo-
ro, Japan. 
Taule, Mariona, M. Antonio Marti, and Marta Recasens. 
2008. Ancora: Multi level annotated corpora for Catalan 
and Spanish. 6th International Conference on Language 
Resources and Evaluation, Marrakesh, Morocco. 
Xue, Nianwen and Martha Palmer. 2009. Adding semantic 
roles to the Chinese treebank. /atural Language Engi-
neering, Vol. 15(1), pp. 143-172. 
Yarowsky, David, Grace Ngai and Richard Wicentowski. 
2001. Inducing multi lingual text analysis tools via ro-
bust projection across aligned corpora. 1st Human Lan-
guage Technology Conference, pp. 161-168, San Fran-
cisco, CA. 
805
Coling 2010: Poster Volume, pages 860?868,
Beijing, August 2010
 
 
 
 
A Vector Space Model for Subjectivity Classification in Urdu 
aided by Co-Training 
 
Smruthi Mukund 
CEDAR 
University at Buffalo 
smukund@buffalo.edu 
Rohini K. Srihari 
CEDAR 
University at Buffalo 
rohini@cedar.buffalo.edu 
 
Abstract 
The goal of this work is to produce a 
classifier that can distinguish subjective 
sentences from objective sentences for 
the Urdu language. The amount of la-
beled data required for training automatic 
classifiers can be highly imbalanced es-
pecially in the multilingual paradigm as 
generating annotations is an expensive 
task. In this work, we propose a co-
training approach for subjectivity analy-
sis in the Urdu language that augments 
the positive set (subjective set) and gene-
rates a negative set (objective set) devoid 
of all samples close to the positive ones. 
Using the data set thus generated for 
training, we conduct experiments based 
on SVM and VSM algorithms, and show 
that our modified VSM based approach 
works remarkably well as a sentence lev-
el subjectivity classifier. 
1 Introduction 
Subjectivity tagging involves distinguishing 
sentences that express opinions from sentences 
that present factual information (Banfield 1982; 
Wiebe, 1994). A wide variety of affective 
nuances can be used while delivering a message 
pertaining to an event. Although the factual 
content remains the same, lexical selections and 
grammatical choices can considerably influence 
the affective nature of the text. Recognizing 
sentences that exhibit affective behavior will 
require, at the least, recognizing the structure of 
the sentence and the emotion bearing words.  
To date, much of the research in this area is 
focused on English. A variety of reliable 
resources that facilitate effective sentiment 
analysis and opinion mining, such as polarity 
lexicons (Senti-WordNet 1 ) and contextual 
valence shifters (Kennedy and Inkpen, 2005) are 
available for English. The MPQA corpus of 
10,000 sentences (Wiebe et al, 2005) provides 
detailed annotations for sources of opinions, 
targets, speech events and fragments that indicate 
attitudes for the English newswire data. The 
IMDB corpus contains 10,000 sentences 
categorized as subjective and objective in the 
movie review domain. Clearly, English is well 
supported with resources. There are other widely 
spoken resource poor languages that are not as 
privileged. When we consider social media, 
limiting our analysis to a language like English, 
however universal, will lead to loss of 
information. With the advent of virtual 
keyboards and extended Unicode support, the 
internet is rapidly getting flooded by users who 
use their native language in textual 
communication. There is a pressing need to 
perform non-topical text analysis in the 
multilingual paradigm. 
Subjectivity analysis is a precursor to 
numerous applications performing non-topical 
text analysis like sentiment analysis, emotion 
detection, and opinion extraction (Liu et al, 
2005; Ku et al, 2006; Titov and McDonald, 
2008). Creating the state-of-the-art subjectivity 
classifier using machine learning techniques 
require access to large amounts of annotated 
data. For less commonly taught languages like 
                                                 
1 http://swn.isti.cnr.it/download_1.0/ 
860
 
 
 
 
Urdu, Hindi, Bengali, Spanish and Romanian, 
the resources required to automate subjectivity 
analysis are either very sparse or unavailable. 
Generating annotated corpus for subjectivity 
detection is laborious and time consuming. 
However, several innovative techniques have 
been proposed by researchers in the past to 
generate annotated data and lexical resources for 
subjectivity analysis in resource poor languages. 
Mihalcea et al, (2007) and Banea et al, (2008) 
used machine translation technique to leverage 
English resources for analysis in Romanian and 
Spanish languages. Wan (2009) proposed a co-
training technique that leveraged an available 
English corpus for Chinese sentiment 
classification. Wan (2008) focused on improving 
Chinese sentiment analysis by using both 
Chinese and English lexicons. 
Unfortunately, not much work has been done 
in the area of subjectivity analysis for the Urdu 
language. This language lacks annotated 
resources required to generate even the basic 
NLP tools (POS tagger, NE tagger etc.) needed 
for text analysis. In order to facilitate subjectivity 
analysis in Urdu language, we annotated a small 
set of Urdu newswire articles for emotions (?2). 
The sentence level annotations provided in this 
dataset follow the annotation guidelines 
proposed by Wiebe et al, (2003). Although 
tremendous effort was put into generating this 
corpus, the data set is not very comprehensive 
and contains only about 500 sentences marked 
subjective. This is definitely insufficient to train 
a suitable subjectivity classifier.  
1.1 Issue with unbalanced data set 
A subjectivity classifier is a binary classifier. 
A traditional binary classifier is trained using 
universal representative sets for positive and 
negative categories. But in subjectivity analysis, 
especially for languages like Urdu that have no 
annotated data, generating universal 
representative sets is extremely difficult and 
almost an impossible task. Assimilating the 
negative set is especially a delicate task as the set 
should be carefully pruned of all the positive 
samples. Also, detecting subjectivity in a 
sentence is highly personalized. Annotators are 
sometimes prejudiced while marking samples. 
This bias, however small, produces errors with 
some true positive samples being unintentionally 
missed and categorized as negative. 
Traditionally, research in machine learning has 
assumed the class distribution in the training data 
to be reasonably balanced. However, when the 
training data is highly imbalanced, i.e., the 
number of positive examples is very small, the 
performance of text classification algorithms 
such as linear support vector machine (SVM) 
(Brank and Grobelnik, 2003), na?ve Bayes and 
decision trees (Kubat and Matwin, 1997) are 
adversely affected.  
In order to achieve a balanced training set, 
Japkowicz (2000) duplicates positive examples 
(oversampling) and discards negative ones 
(downsizing). Kubat and Matwin (1997) discard 
all samples that are close to the positive set to 
avoid misclassification. Chan and Stalfo (1998) 
have trained several classifiers on different ba-
lanced data subsets, each constructed to include 
all positive training samples and a set of negative 
samples of comparable size. The predictions are 
combined through stacking.  
For the task of subjectivity analysis, especially 
in the multilingual paradigm where the data set is 
highly unbalanced, using one of the techniques 
proposed above will yield benefit. To the best of 
our knowledge, co-training technique has not 
been applied before for the subjectivity detection 
task, in particular, for the Urdu language. 
1.2 Contribution 
Our first contribution is inspired by the work 
of Luo et al, (2008). We propose a similar co-
training technique that helps to create a likely 
negative set (objective sentences) and a filtered 
positive set (subjective sentences) 
simultaneously from the unlabeled set. We use 
two learning models trained using the linear 
SVM algorithm iteratively. In every iteration of 
co-training, the likely positive samples are 
filtered. The iterative process terminates when no 
more positive samples are found. The final 
negative set is the likely negative set, considered 
as the universal representative set for the non-
subjective category. The likely positive sample 
set is appended to the already existing positive 
set (annotated set). The SVM models are trained 
using part of speech, unigrams and emotion 
bearing words, as features.  
The second contribution of this work includes 
training a state-of-the-art Vector Space Model 
861
 
 
 
 
(VSM) for Urdu newswire data using the data 
sets generated by the co-training method. 
Experiments that use the SVM classifier are also 
performed. The results show that the 
performance of the proposed VSM based 
approach helps to achieve state-of-the-art 
sentence level subjectivity classifier. The F-
Measure of the VSM subjectivity classifier is 
82.72% with 78.7% F-measure for the subjective 
class and 86.7% F-Measure for the objective 
class.  
2 Data Set 
The data set used to generate a subjectivity 
classifier for Urdu newswire articles is obtained 
from BBC Urdu2. The annotating efforts are di-
rected towards achieving the final goal- emotion 
detection in Urdu newswire data and the annota-
tion guidelines are based on the MPQA standards 
set for English.  
The repository of articles provided by BBC is 
huge and needs to be filtered intelligently. Two 
levels of filters are applied. ? date and keyword 
search. The date filter is applied to retrieve ar-
ticles of three years, starting year 2003. The key-
word based filter consists of a set of seed words 
that are commonly used to express emotions in 
Urdu -ghussa (~anger), pyar (~love) etc. Clearly, 
this list will not cover all possible linguistic ex-
pressions that express emotion and opinion. But 
it is definitely a representative of a wide range of 
phenomena that naturally occurs in text express-
ing emotions.  
The data retrieved is parsed using an in-house 
HTML parser to produce clean data. To date, we 
have 500 articles, consisting of 700 sentences 
annotated for emotions. There are nearly 6000 
sentences that do not contain any emotions mak-
ing it highly unbalanced. This data set is divided 
into testing and training sets with 30% and 70% 
of the data respectively. Co-training is performed 
only on the 70% training set that consists of 470 
subjective sentences and about 4000 objective 
sentences. The purpose of co-training here is to 
remove samples that are close to subjective from 
the objective set and create a likely negative set. 
The samples removed are the likely positive set. 
This set of 4000 objective sentences can be con-
sidered as the un-annotated set. 
                                                 
2 http://www.bbc.co.uk/urdu/ 
3 Co-Training 
Identifying sentences that express emotions in 
Urdu newswire data is not trivial. Subjective sen-
tences do not always contain individual expres-
sions that indicate subjectivity. Analysis is high-
ly dependent on the contextual information. 
Wiebe et al, (2001) reported that nearly 44% of 
sentences in the MPQA corpus (English news-
wire data) are subjective. In newswire data, 
though most facts are reported objectively, there 
are cases when the tone of the sentence is very 
intense indicating the existence of emotion. Con-
sider Example 1. 
 
Example 1:  
Political news headline  
 ?????? ?? ????? ? ??????? ????????? ?? ??????? ?? ????
????? ???? ?? ?????? ???? 
[bhart ka pakstan kE sath jame mZakrat sE ankar, 
bharty lykcr snnE kE Kwaha" nhy"] 
[India refuses to have a dialog with Pakistan, In-
dians are not willing to listen to the lecture] 
Common Urdu 
????? ?? ????? ?? ??? ?? ??? ????? ??????? ??  ?????  
[India refuses to talk to Pakistan] 
Clearly, the news headline is extremely in-
tense and strongly expresses the opinion of India 
on Pakistan. However, the statement in common 
Urdu is not as affective.  
 
Example 2: 
?? ???? ???? ???? ??? ???? ???? ??? ?? ???? ???  ??????
??? ??? ???   
[anSary nE kha ?myry ray^E my" eamr shyl ayk 
bd dmaG awr Zdy XKS hy"? ]                                                      
[Ansari said, ?according to me Aamir Sohail is one 
crazy and stubborn man?] 
Statements in quotes that express emotions are 
subjective as shown in example 2. 
 
Consider example 3. Here, identifying the 
words that indicate subjectivity is not straight 
forward. The phrase, ?found it very difficult to 
hide his smile? is indicative of the emotion expe-
rienced by ?Habib Miya?.  
 
Example 3: 
 ???  ??? ??????? ?? ?? ????? ?? ?? ???? ???? ?? ???
?? ?? ???? ??????? ???? ????  
[rqm ky as wSwly pr yh Hbyb mya" kE ly^E bht 
mXkl t|ha kh wh apny mskrahT c|hpa sky"]                                  
 [At this event of money collection, Habib Miyan 
found it very difficult to hide his smile.]  
862
 
 
 
 
 
There are also several false positives that 
make subjective detection hard task. Example 4 
is an objective sentence despite the usage of 
word ?pyar? ~ love, an emotion bearing word.  
 
Example 4:  
 ?? ??? ???? ??? ???????? ?? ??? ????  
[n|Zmam ka nya pyar ka nam anzy pRa hE] 
[The new nickname for Inzaman is Inzi] 
 
Expressive elements in Urdu sentences were 
marked with an inter-annotator agreement of 0.8 
kappa score. Though high, there still exists a bias 
that can influence classification especially when 
the number of sentences in the positive set is rel-
atively less. In order to obtain a reliable positive 
and negative set for training a learning algorithm, 
we adopt a semi-supervised learning technique of 
co-training. Co-training (Blum and Mitchell, 
1998) is similar to self-training in that it increas-
es the amount of labeled data by automatically 
annotating unlabeled data. The intuition here is 
that if the conditional independence assumption 
holds, then on an average each selected docu-
ment will be as informative as a random docu-
ment, and the learning will progress. Co-training 
differs from self-training as it uses multiple 
learners to do the annotation. Each learner offers 
its own perspective that when combined gives 
more information. This technique is especially 
effective when the feature space of a particular 
type of problem can be divided into distinct 
groups and each group contains sufficient infor-
mation to perform the annotation. In other words, 
co-training algorithm involves training two dif-
ferent learning algorithms on two different fea-
ture spaces. The learning of one becomes condi-
tionally independent of the other and the predic-
tion made by each classifier is used on the unla-
beled data set to augment the training data of the 
other.  
A traditional co-training classifier is trained 
and later applied on the same unlabeled data set. 
Theoretically such classifiers are not likely to 
assign confident labels. In this work, the pro-
posed co-training method differs from the tradi-
tional co-training method in that the two classifi-
ers are based not on two different feature spaces 
but on two different training data sets with the 
same feature space.  
 
 
Figure 1: Co-Training model 
 
Figure 1 explains the overall working of the 
model. The negative set (which can also be the 
unlabeled set) is split into two equal parts N1 and 
N2. S represents the positive annotated set. Two 
linear SVM classifiers are trained iteratively to 
purify the negative data set. SVM1 is trained us-
ing S+N1i and SVM2 is trained using S+N2i data 
sets. In every iteration i, N1i data set is evaluated 
using SVM2 model and N2i data set is evaluated 
using SVM1 model. The samples that are classi-
fied as positive in a given iteration i are binned 
into sets P1i and P2i respectively. These samples 
are removed from N1i and N2i data sets to create 
new N1i+1 and N2i+1 sets that are used for training 
in the next iteration i+1. The iterations continue 
until no positive samples are marked by both 
SVM1 and SVM2 models. The final set of likely 
negatives is S = N1k + N2k sets, where N1k and 
N2k are sets created in the last k iteration of the 
algorithm. In order to obtain the likely positive 
set, the final P1 = {P11 + P12 + ?. + P1k} and P2 = 
{P21 + P22 + ?. + P2k} sets are combined and 
tested using the SVMs modeled in the last k ite-
ration of the co-training algorithm. Similar to the 
traditional co-training method the samples that 
are marked positive by both classifiers (P1o = P2o) 
are considered to be the likely positive set L.  
Several features are used to train the SVM 
learning models used for co-training. The best 
performance is obtained when word unigrams, 
parts of speech and likely emotion words are 
used as features.  
This technique of co-training provides us with 
a relatively huge set of likely positive samples 
863
 
 
 
 
(close to 400 sentences). Sentences in this set 
were examined by the annotators and nearly 60% 
of the sentences were subjective or near subjec-
tive in nature (Example 5 and 6). 
 
Labels R % P % IF % AF % 
Unigram 52.63 
 1 18.64 74.57 29.83 
-1 95.4 62.35 75.44 
Unigram+Bigram 50.25 
1 14.40 85 24.63 
-1 98.19 61.82 75.87 
Table 1: Performance of the model using  
un-balanced data set3 
 
Labels R % P % IF % AF % 
Annotated positive + likely positive + likely 
negative 
62.95 
 
1 39 70 50.09 
-1 87.28 67.34 79.9 
Annotated positive + likely negative 55.42 
1 30 61.2 40.26 
-1 86.1 64.23 73.57 
Table 2 ? Performance of the model after  
co-training method 
 
Table 1 shows the performance of the SVM 
model using the unbalanced data set for training. 
Table 2 shows the performance of the same 
model using data generated after co-training.  
 
Example 5:  
??? ???? ??????? ? ?????? ? ????? ?? ??? ?? ??? ?????? ? 
??? ? ????? ?? ????? ????? ??? ?????? ? ???? ????   
[pwtn nE kha kh lwg dwsrw" ky Ank|h my" tnka 
dyk|h lytE hy" lykn apny Ank|h my" pRa Xhtyr an-
hy" n|zr nhy" Ata .] 
[Potan said people who see dust in others eyes 
never realize that it is their eyes that are filled with 
dirt.] 
The above example is a metaphor indicating 
extreme anger. 
 
Example 6: 
?????? ?? ? ???? ???? ?? ?? ???? ???? ?? ??? ?????? ?????? ? 
??? ????? ???? ????? ???? ??? ?? ????? ?? ?? ??  
[e|ta& alrHmn XyK ka khna hE kh barh agst kw an-
hy" an kE byTw" kE samnE mkml |twr pr brhnh kr 
kE pryD kray^y gy^y] 
[etlaalrahman said that on 12th Aug they made him 
parade naked in front of his children.] 
                                                 
3 Convention used across tables -  Label 1: subjective sen-
tences Label -1: objective sentences R: Recall P: Precision 
IF: Individual F-Measure AF: Average F-Measure. 
 
Example 6 indicates extreme sad emotion. Such 
examples were found in the likely positive set. 
4 Features 
Features that are commonly used to train a 
subjectivity classifier for English are word uni-
grams, emotion keywords, part of speech infor-
mation and noun patterns (Pang et al, 2002). 
Due to difference in syntactic structure, vocabu-
lary and style, features that work for English may 
not work for Urdu. Also, Urdu is handicapped by 
the lack of resources required to perform basic 
NLP analysis. However, it is worth exploring the 
English feature set as subjectivity is more a se-
mantic phenomenon. Efforts to generate likely 
emotion word lexicons and subjectivity patterns 
for the Urdu language are underway. The sec-
tions that follow summarize the experimented 
features. 
4.1 Word Unigrams 
Unigram word features are very informative. 
Three different approaches are tried for selecting 
the unigrams. The first method involves selecting 
only those words that occur more than twice in 
the dataset. This eliminates proper nouns (low 
frequency named entities do not generally con-
tribute towards subjectivity detection) and spel-
ling errors (Pang et al, 2002). In the second me-
thod, only words that are adjectives and verbs 
along with the surrounding case markers are ac-
counted for as features. This has the advantage of 
drastically reducing the feature set. The third me-
thod involves including the nouns as well to the 
feature set. A simple list of stop words (common 
Urdu words ? pronouns such as ?us?, ?is?, ?aap?, 
?un?, salutations like ?shabba khair?, ?aadab? and 
honorifics along with punctuations and special 
symbols) are eliminated. The features are 
represented as Boolean features for the SVM 
model. The value is 1 if the feature word appears 
in the sentence to be classified and 0 otherwise. 
The best performance is obtained for the first 
method that considers all words with frequency 
greater than 2. This conforms to what is shown 
by Pang et al, (2002) for classification of Eng-
lish movie reviews. 
4.2 Part of Speech (POS) Information 
The work done by Mukund and Srihari (2009) 
provides suitable POS and NE tagger for Urdu. 
864
 
 
 
 
This POS tagger is used to generate parts of 
speech tags on the acquired data set (?3).  The 
POS tags associated with adjectives, verbs, 
common nouns and auxiliary words are consi-
dered and used as Boolean features for the SVM 
model. The proper noun words are normalized to 
one common word ?nnp? and are assigned the 
common noun tag. For the English language, 
when building a subjectivity classifier for review 
classification, the use of POS information did not 
benefit the system (Kennedy and Inkpen, 2006). 
However, for Urdu, the performance of the co-
training model with POS information showed 
1.2% improvement (table 3). 
4.3 Likely Emotion Lexicon 
In order to facilitate simple keyword based 
detection of subjectivity, access to a lexicon con-
sisting of likely emotion words is needed. Unfor-
tunately, no such lexicon is available off the 
shelf for Urdu. In this work, an Urdu specific 
emotion list is generated that contains transla-
tions from the English emotion list released by 
SemEval (2007) ?Word"et affect Emotion List?. 
Words for each emotion category - sadness (sad), 
fear, joy (happy), surprise, anger and disgust are 
obtained for Urdu by using an Urdu-English dic-
tionary. The list is pruned manually and cor-
rected to remove errors. Simple keyword lookup 
on the Urdu annotated corpus has an emotion 
detection rate of 29.27%. This shows that al-
though the contribution of the emotion lexicon 
for subjectivity classification is not significant, it 
contains information which when used along 
with other features aid subjectivity detection. 
4.4 Patterns 
Extracting syntactic patterns contribute to-
wards the affective orientation of a sentence (Ri-
loff et al, 2003). The Apriori algorithm (Agar-
wal and Srikant, 1994) for learning association 
rules is used here to mine the syntactic word pat-
terns commonly used in the positive and negative 
data set. The length of the candidate item set k = 
4. Starting from a small set of seed words (likely 
emotion words) and the associated POS tags, 
POS sequential patterns like ?adverb verb 
verbtransitive sentencemarker?, ?noun noun ca-
semarker verbtransitive?, etc., that are most 
commonly found in subjectivity set are extracted. 
23 patterns that strongly indicate subjectivity 
were found by this method and included as fea-
tures to train the SVM learning algorithm.  
4.5 Confidence Words 
The confidence word list positively aids the 
VSM classifier (?5). The words in the likely 
emotion list are not the only ones that contribute 
towards the emotion orientation of a sentence 
and also, not all of these words contribute effec-
tively. There are several stop words (eliminated 
while accounting for unigrams) (esp. case mark-
ers) that contribute significantly for categoriza-
tion. In order to identify all the keywords that 
actually contribute to subjectivity categorization, 
a technique proposed by Soucy and Mineau 
(2004) is used.  
The confidence weight of a given word w, 
based on the number of documents it is asso-
ciated with under each category, is measured us-
ing the Wilson Proportion Estimate (Wilson, 
1927). In order to compute the confidence of w 
for a specific category, the number of positive 
and negative documents associated with w has to 
be determined. A document is positive if it be-
longs to that category and negative otherwise. 
Thus, two kinds of word confidence metrics are 
computed, CPOS:w and C"EG:w as given below.  
                  ???     (Eq. 1) 
                   ???    (Eq. 2) 
where n is the total number of positive and nega-
tive documents,  is the ratio of the num-
ber of positive documents which contain w to the 
total number of documents, and  is the 
ratio of the number of negative documents which 
contain w to the total number of documents. The 
normal distribution is used when n > 30.  
Note that equations 1 and 2 give a range of 
values for CPOS:w and C"EG:w. If the lower bound 
of CPOS:w is greater than the upper bound of 
C"EG:w, we say that w is likely to be a word in 
that category. Now, we compute the strength of a 
word Sw in a particular category as 
 
( )
( )nz
nnzppz
n
zp
C
wPOSwPOS
z
wPOS
wPOS 2
2/
2
2/::2/
2/
:
: 1
]4?1?[2?
?
??
?
+
???
?
???
? +??+
=
( )
( )nz
nnzppz
n
zp
C
w"EGw"EG
z
w"EG
w"EG 2
2/
2
2/::2/
2/
:
: 1
]4?1?[2?
?
??
?
+
???
?
???
? +??+
=
wPOSp :?
( )
??
? ?= >                            ;                    0
 ;2log
otherwise
       )w:NEGub(C  )w:POSlb(C ifmPRFSw
"EG:w p ?
865
 
 
 
 
                                                 ??? (Eq. 3) 
where mPRF is given by  
                                                                              ???   (Eq. 4) 
and lb(?) and ub(?) are the lower and upper 
bounds of their arguments, respectively. 
Equations 1 through 4 generated a very good set 
of keywords that are used as category word fea-
tures in the SVM learning model. For VSM, the 
strength value is used as a boost factor along 
with the tf-idf weight when calculating the simi-
larity score (table 3). 
5 Final Subjectivity Classifier 
Wiebe et al, (2005) and Pang et al, (2002) 
have shown that an SVM based approach works 
well for subjectivity classification. Riloff et al, 
(2003) have conducted experiments that use Bag-
Of-Words (BoW) as features to generate a Na?ve 
Bayes subjectivity classifier for the MPQA cor-
pus in English. This method has an accuracy of 
73.3%. Su and Markert (2008) use BoW features 
termed as lexical features on the IMDB corpus to 
generate an accuracy of 60.5%. Das and Ban-
dyopadhyay (2009) use a CRF based approach to 
generate a subjectivity classifier for Bengali data 
with a precision of 72.16% for news and 74.6% 
for blogs domain. The same approach has a pre-
cision of 76.08% and 79.9% on the two domains 
respectively. Impressive results for emotion de-
tection are obtained by Danisman and Alpkocak, 
(2007) who use a VSM based approach. They 
show that their approach works much better than 
a traditional SVM based approach commonly 
used for emotion detection. 
In this work, we conduct subjectivity classifi-
cation experiments using two different learning 
algorithms ? linear SVM and VSM. The best 
performance is obtained using the VSM model as 
shown in table 4. All experiments are conducted 
on the data set obtained after applying the co-
training technique.  
5.1 VSM algorithm 
The final subjectivity classifier is based on the 
VSM approach. Inspired by the work done in 
?Feeler? (Danisman and Alpkocak, 2007), a sim-
ilar technique is used to train the final subjectivi-
ty classifier for Urdu. The algorithm is explained 
in table 3. The similarity metric is modified to 
include the confidence score for each word 
(pt.5). In VSM, documents and queries are 
represented as vectors, and the cosine angle be-
tween them indicates the similarity. 
1.  di = <w1i, w2i, ?. wni> where wki is the weight of 
the kth term in document i , di is the document 
vector. wki is computed using tf-idf weighting 
scheme. 
2. Mj={d1,d2,?,dc} where Mj is each class (subjec-
tive and objective) 
3.  Model vector for an arbitrary class Ej is created 
by taking the mean of dj vectors  
?
?
=
||1 j
ji
M
Md
i
j
j dM
E
 
where |Mj| represents number of documents in Mj. 
4. The whole system is represented with a set of 
model vectors, D={E1,E2,...,Es} where s represents 
the number of distinct classes to be recognized.  
5. The normalized similarity between a given query 
text Q, and a class, Ej, is defined as follows: 
kj
n
k
kqj EconfwEQsim *)(),(
1
?
=
+=  
conf is the confidence factor applied for lexical 
terms found in the word list. 
6. classification result is, 
 )),(max(arg)( jEQsimQVSM =  
Table 3: VSM Algorithm for subjectivity 
 Classification 
 
Labels R % P % IF % AF % 
Before Co-Training (all data) 62.95 
 1 65.85 70.85 67.4 
-1 85.58 83.33 84.44 
After Co-Training (pruned data) 86.73 
1 72.88 85.57 78.72 
-1 91.29 82.60 86.73 
Table 4: VSM approach, using all training data and 
using pruned training data (L+S+true) 
 
The confidence metric (strength) for each term 
is calculated using the Wilson proportion esti-
mate (?4.4) and added to the term score as the 
boost factor. Q is the test set. Model vectors are 
obtained using the data set that consists of true 
set (annotated positive samples), likely positive 
set L and likely negative set N. Sets L and N are 
obtained from the co-training method. The re-
sults are shown in table 4.  
The power of SVM cannot be ignored. Pang et 
al., (2002) use SVM to generate a subjectivity 
(polarity) classifier for English. Our second set 
of experiments is conducted to measure the per-
formance of a linear SVM classifier for subjec-
tivity analysis on the Urdu newswire data. The 
data set used for training is the pruned data set 
)ub(C )lb(C
)lb(C
w:NEGw:POS
w:POS
+=mPRF
866
 
 
 
 
obtained after applying the co-training technique. 
The features used and the performance of the 
model with each feature is documented in table 6.  
Labels R % P % IF % AF % 
Unigrams + POS 64.2 
 1 40.67 71.1 51.75 
-1 88.29 67.74 76.67 
Unigrams + POS + Patterns 65.68 
1 43.22 72.34 54.11 
-1 88.29 68.69 77.26 
Unigrams + POS + Patterns + emotion words  67.31 
1 48.31 70.81 57.43 
-1 85.88 70.09 77.19 
Table 6: SVM classifier on Urdu newswire data 
 
In order to provide a better understanding of 
the power of the VSM technique, we applied this 
model on the IMDB data set. The training data 
consists of 4000 positive (subjective) and 4000 
negative (objective) samples. Since the data set is 
already balanced, we skip the co-training method. 
Our aim here is to test the working of VSM clas-
sifier. The test set consists of 1000 positive and 
1000 negative samples. The classification result 
on this data set is shown in table 5. The results 
are comparable to the state-of-the-art perfor-
mance of English subjectivity classifier that uses 
SVM (Wiebe et al, 2005). 
Labels R % P % IF % AF % 
Balanced training 78.01 
 1 64 90.57 75 
-1 93.18 71.68 81.03 
Table 5: VSM approach on IMDB data set 
6 Analysis of results 
In this work, experiments were conducted us-
ing two different classification approaches; 1. 
VSM based 2. SVM based.  Results in table 4 
indicate that the VSM technique when combined 
with the modified boost factor (confidence 
measure) can be a very powerful technique for 
sentence level classification tasks. When model 
vectors were constructed using the entire training 
set (highly unbalanced), the performance was at 
62% F-Measure with the subjectivity detection 
rate of 70.85%. Post co-training, using the mod-
ified model vectors obtained from the pruned 
data set generated better scores. The increase in 
the recall of negative class and the increase in the 
overall F-Measure can be attributed to (i) in-
crease in the positive samples (~likely positive 
set), and (ii) cleaner negative set (no near posi-
tive samples).  
The results in table 6 for the SVM classifier 
also indicate the benefits of co-training. The sub-
jectivity classification performance show posi-
tive improvement. Although the performance of 
the SVM model is not as good as the VSM mod-
el, addition of each feature shows an improve-
ment in the subjectivity recognition rate. This 
performance indicates that the feature sets ex-
plored definitely contain positive information 
necessary for accurate detection.  
The poor performance of SVM (over VSM) 
can be attributed to 1. lack of balanced data for 
training a traditional SVM model and, 2. small 
number of positive samples. In VSM the problem 
of unbalanced data set in a way is overcome by 
using the confidence score at the time of calcu-
lating similarity. If these factors are compensated, 
the performance of the SVM model will signifi-
cantly improve. 
7 Conclusion 
This research provides interesting insights in 
modeling a subjectivity classifier for Urdu 
newswire data. We show that despite Urdu being 
a resource poor language, techniques like co-
training and statistical techniques based on tf-idf 
and word unigrams coupled with confidence 
measures help model the state-of-the-art subjec-
tivity classifier. We demonstrate the power of the 
co-training technique in generating likely nega-
tive and positive sets. The number of near sub-
jective samples in the likely positive set suggests 
that this method can be used as an adaptive 
learning technique to enable the annotators pro-
duce more samples. For a task like emotion de-
tection, that requires fine grained analysis, sen-
tences need to be analyzed at the semantic level 
and this goes beyond simple keyword based ap-
proach. Our efforts are now concentrated in this 
direction. 
References 
Agrawal R, Srikant R. 1994. Fast Algorithms for Mining 
Association Rules. In Proc. Of the Intl. Conf on Very 
Large databases. Santiago, Chile. Sept. Pp. 478-499. 
Banea, C., Mihalcea, R., Wiebe, J., and Hassan, S. 2008. 
Multilingual subjectivity analysis using machine transla-
tion. In Proceedings of EM"LP-2008.  
Banfield, A. 1982. Unspeakable Sentences. Routledge and 
Kegan Paul, Boston. 
867
 
 
 
 
Blum, A. and Mitchell, T. 1998. Combining labeled and 
unlabeled data with co-training. Proceedings of the ele-
venth annual conference on Computational learning 
theory, ACM. p. 100. 
Brank, J., Grobelnik, M., Milic-Frayling, N., and Mladenic, 
D. 2003. Training text classifiers with SVM on very few 
positive examples. Technical Report MSR-TR-2003-34, 
Microsoft Corp. 
Chan, Philip K. and Stolfo J. Salvatore. 1998. Toward Scal-
able Learning with Non-Uniform Class and Cost Distri-
butions: A Case Study in Credit Card Fraud Detection. 
Proc. 4th Int. Conf. on Knowledge Discovery and Data 
Mining (KDD-98), August 27?31, 1998, New York City, 
New York, USA, pp. 164?168. AAAI Press. 
Danisman, T., and Alpkocak, A. 2008. Feeler: Emotion 
Classification of Text Using Vector Space Model. AISB 
2008 Convention Communication, Interaction and Social 
Intelligence, p. 53. 
Das, A., and Bandyopadhyay, S. 2009. Subjectivity Detec-
tion in English and Bengali: A CRF-based Approach. Se-
venth International Conference on "atural Language 
Processing (ICON 2009), December. Hyderabad, India. 
Japkowicz Nathalie. 2000. Learning from Imbalanced Data 
Sets: A Comparison of Various Strategies. In "athalie 
Japkowicz (ed.), Learning from Imbalanced Data Sets: 
Papers from the AAAI Workshop (Austin, Texas, Mon-
day, July 31, 2000), AAAI Press, Technical Report WS-
00-05, pp. 10?15. 
Kennedy, A, & Inkpen, D. 2005. Sentiment classification of 
movie and product reviews using contextual valence shif-
ters. In Workshop on the analysis of informal and formal 
information exchange during negotiations (FINEXIN 
2005) 
Ku, L. W., Liang, Y. T., and Chen, H. H. 2006. Opinion 
extraction, summarization and tracking in news and blog 
corpora. In Proceedings of AAAI-2006. 
Kubat, Miroslav and Matwin Stan. 1997. Addressing the 
curse of imbalanced training sets: one-sided selection. 
Proc. 14th ICML, Nashville, Tennessee, USA, July 8?12, 
1997, pp. 179?186. 
Liu, B., Hu, M., and Cheng, J. 2005. Opinion observer: 
Analyzing and comparing opinions on the web. In Pro-
ceedings of WWW-2005. 
Luo, N., Yuan, F., and Zuo, W. 2008. Using CoTraining and 
Semantic Feature Extraction for Positive and Unlabeled 
Text Classification. International Seminar on Future In-
formation Technology and Management Engineering. 
Mihalcea, R., Banea, C., and Wiebe, J. 2007. Learning mul-
tilingual subjective language via cross-lingual projec-
tions. In Proceedings of ACL-2007. 
Mukund, S., and Srihari, R.K., 2009.  NE Tagging for Urdu 
based on Bootstrap POS Learning. Third International 
Workshop on Cross Lingual Information Access: Ad-
dressing the Information Need of Multilingual Societies 
(CLIAWS3), NAACL - 2009, Boulder, CO. 
Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs up? 
Sentiment classification using machine learning tech-
niques. In Proceedings of the Conference on EM"LP, 
pages 79?86. 
Riloff, E., Wiebe, J., and Wilson, T. 2003. Learning subjec-
tive nouns using extraction pattern bootstrapping. Pro-
ceedings of the seventh conference on "atural language 
learning at HLT-"AACL 2003 - Volume 4,  Edmonton, 
Canada: Association for Computational Linguistics, pp. 
25-32. 
Soucy, P., and Mineau, G. W. 2005. Beyond tfidf weighting 
for text categorization in the vector space model. Interna-
tional Joint Conference on Artificial Intelligence, Cite-
seer, p. 1130. 
Su, F., and Markert. K. 2008. From words to senses: a case 
study of subjectivity recognition. Proceedings of the 22nd 
International Conference on Computational Linguistics-
Volume 1, ACL, pp. 825-832. 
Titov, I., and McDonald, R. 2008. A joint model of text and 
aspect ratings for sentiment summarization. In Proceed-
ings of ACL-08:HLT. 
Wan, X. 2008. Using bilingual knowledge and ensemble 
techniques for unsupervised Chinese sentiment analysis. 
In Proceedings of EM"LP-2008. 
Wan, X. 2009. Co-Training for Cross-Lingual Sentiment 
Classification. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on "atural Language Processing 
of the AF"LP, Association for Computational Linguis-
tics, pp. 235-243. 
Wiebe, J. 1994. Tracking point of view in narrative. Com-
putational Linguistics, 20(2):233-287. 
Weibe, J., Bruce, R., and O?Hara, T. 1999. Development 
and use of a gold standard data set for subjectivity classi-
fications. In Proc. 37th Annual Meeting of the Assoc. for 
Computational Linguistics (ACL-99). 
Wiebe, J., and Riloff, E. 2005. Creating Subjective and 
Objective Sentence Classifiers from Unannotated Texts. 
Proceedings of the 6th International Conference on Intel-
ligent Text Processing and Computational Linguistics. 
Wiebe, J., Wilson, T., and Cardie, C. 2005. Annotating 
expressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, volume 39, issue 2-3, 
pp. 165-210. 
Wilson, B. Edward. 1927. Probable Inference, the Law of 
Succession, and Statistical Inference. Journal of the 
American Statistical Association, Vol. 22, No. 158 (Jun., 
1927), pp. 209-212. 
 
868
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 58?67,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using Sequence Kernels to identify Opinion Entities in Urdu 
 
 
Smruthi Mukund? and Debanjan Ghosh* Rohini K Srihari 
?SUNY at Buffalo, NY 
smukund@buffalo.edu 
*Thomson Reuters Corporate R&D 
debanjan.ghosh@thomsonreuters.com 
SUNY at Buffalo, NY 
rohini@cedar.buffalo.edu 
 
 
 
 
 
Abstract 
Automatic extraction of opinion holders 
and targets (together referred to as opinion 
entities) is an important subtask of senti-
ment analysis. In this work, we attempt to 
accurately extract opinion entities from 
Urdu newswire. Due to the lack of re-
sources required for training role labelers 
and dependency parsers (as in English) for 
Urdu, a more robust approach based on (i) 
generating candidate word sequences 
corresponding to opinion entities, and (ii) 
subsequently disambiguating these se-
quences as opinion holders or targets is 
presented. Detecting the boundaries of such 
candidate sequences in Urdu is very differ-
ent than in English since in Urdu, 
grammatical categories such as tense, 
gender and case are captured in word 
inflections. In this work, we exploit the 
morphological inflections associated with 
nouns and verbs to correctly identify 
sequence boundaries. Different levels of 
information that capture context are 
encoded to train standard linear and se-
quence kernels. To this end the best per-
formance obtained for opinion entity 
detection for Urdu sentiment analysis is 
58.06% F-Score using sequence kernels 
and 61.55% F-Score using a combination 
of sequence and linear kernels. 
1 Introduction 
Performing sentiment analysis on newswire da-
ta facilitates the development of systems capable 
of answering perspective questions like ?How did 
people react to the latest presidential speech?? and 
?Does General Musharraf support the Indo-Pak 
peace treaty??. The components involved in de-
veloping such systems require accurate identifica-
tion of opinion expressions and opinion entities. 
Several of the approaches proposed in the literature 
to automatically extract the opinion entities rely on 
the use of thematic role labels and dependency 
parsers to provide new lexical features for opinion 
words (Bethard et al, 2004). Semantic roles (SRL) 
also help to mark the semantic constituents (agent, 
theme, proposition) of a sentence. Such features 
are extremely valuable for a task like opinion en-
tity detection. 
English is a privileged language when it comes 
to the availability of resources needed to contribute 
features for opinion entity detection. There are 
other widely spoken, resource poor languages, 
which are still in the infantile stage of automatic 
natural language processing (NLP). Urdu is one 
such language. The main objective of our research 
is to provide a solution for opinion entity detection 
in the Urdu language. Despite Urdu lacking NLP 
resources required to contribute features similar to 
what works for the English language, the perform-
ance of our approach is comparable with English 
for this task (compared with the work of Weigand 
and Klalow, 2010 ~ 62.61% F1). The morphologi-
cal richness of the Urdu language enables us to 
extract features based on noun and verb inflections 
that effectively contribute to the opinion entity ex-
traction task. Most importantly, these features can 
be generalized to other Indic languages (Hindi, 
Bengali etc.) owing to the grammatical similarity 
between the languages. 
58
English has seen extensive use of sequence ker-
nels (string and tree kernels) for tasks such as rela-
tion extraction (Culotta and Sorensen, 2004) and 
semantic role labeling (Moschitti et al, 2008). But, 
the application of these kernels to a task like opin-
ion entity detection is scarcely explored (Weigand 
and Klalow, 2010). Moreover, existing works in 
English perform only opinion holder identification 
using these kernels. What makes our approach 
unique is that we use the power of sequence ker-
nels to simultaneously identify opinion holders and 
targets in the Urdu language. 
Sequence kernels allow efficient use of the 
learning algorithm exploiting massive number of 
features without the traditional explicit feature rep-
resentation (such as, Bag of Words). Often, in case 
of sequence kernels, the challenge lies in choosing 
meaningful subsequences as training samples in-
stead of utilizing the whole sequence. In Urdu 
newswire data, generating candidate sequences 
usable for training is complicated. Not only are the 
opinion entities diverse in that they can be con-
tained within noun phrases or clauses, the clues 
that help to identify these components can be con-
tained within any word group - speech events, 
opinion words, predicates and connectors. 
 
1 Pakistan ke swaat sarhad ke janoobi shahar Banno 
ka havayi adda zarayye ablaagk tavvju ka markaz 
ban gaya hai. 
[ Pakistan?s provincial border?s south city?s airbase 
has become the center of attraction for all reporters.] 
 
Here, the opinion target spans across four noun 
chunks, ? Pakistan?s | provincial border?s | south 
city?s | airbase ?. The case markers (connectors) 
?ke?and?ka ?  indicate the span. 
2  Habib miyan ka ghussa bad gaya aur wo apne aurat 
ko maara. 
[Habib miya?s anger increased and he hit his own 
wife.] 
 
Here, the gender (Masculine) inflection of the verb 
?maara? (hit) indicates that the agent performing 
this action is ? Habib miya?  (Masculine) 
3  Ansari ne kaha ? mere rayee mein Aamir Sohail eek 
badimaak aur Ziddi insaan hai?. 
[Ansari said, ? according to me Aamir Sohail is one 
crazy and stubborn man?] 
 
Here, cues similar to English such as ?mere rayee 
mein ? (according to)  indicate the opinion holder.  
Another interesting behavior here is the presence of 
nested opinion holders. ?kaha? (said)  indicates that 
this statement was made by Ansari only. 
4  Sutlan bahut khush tha, naseer key kaam se.  
[Sultan was very happy with Naseer?s work ] 
 
Here, the target of the expression ?khush? is after the 
verb ?khush tha?(was happy) ? SV O structure  
Table 1: Examples to outline the complexity of the task 
 
Another contributing factor is the free word or-
der of the Urdu language. Although the accepted 
form is SOV, there are several instances where the 
object comes after the verb or the object is before 
the subject. In Urdu newswire data, the average 
number of words in a sentence is 42 (Table 3). 
This generates a large number of candidate se-
quences that are not opinion entities, on account of 
which the data used for training is highly unbal-
anced. The lack of tools such as dependency pars-
ers makes boundary detection for Urdu different 
from English, which in turn makes opinion entity 
extraction a much harder task. Examples shown in 
table 1 illustrate the complexity of the task. 
One safe assumption that can be made for opin-
ion entities is that they are always contained in a 
phrase (or clause) that contains a noun (common 
noun, proper noun or pronoun), which is either the 
subject or the object of the predicate. Based on 
this, we generate candidate sequences by consider-
ing contextual information around noun phrases. In 
example 1 of Table 1, the subsequence that is gen-
erated will consider all four noun phrases ? Paki-
stan?s | provincial border?s | south city?s | 
airbase?  as a single group for opinion entity. 
We demonstrate that investigating postpositions 
to capture semantic relations between nouns and 
predicates is crucial in opinion entity identifica-
tion. Our approach shows encouraging perform-
ance. 
2  Related Work 
Choi et al, (2005) consider opinion entity iden-
tification as an information extraction task and the 
opinion holders are identified using a conditional 
random field (Lafferty et al, 2001) based se-
quence-labeling approach. Patterns are extracted 
using AutoSlog (Riloff et al, 2003). Bloom et al, 
(2006) use hand built lexicons for opinion entity 
identification. Their method is dependent on a 
combination of heuristic shallow parsing and de-
pendency parsing information. Kim and Hovy 
59
(2006) map the semantic frames of FrameNet 
(Baker et al, 1998) into opinion holder and target 
for adjectives and verbs to identify these compo-
nents.  Stoyanov and Cardie (2008) treat the task of 
identifying opinion holders and targets as a co-
reference resolution problem. Kim et al, (2008) 
used a set of communication words, appraisal 
words from Senti-WordNet (Esuli and Sebastiani, 
2006) and NLP tools such as NE taggers and syn-
tactic parsers to identify opinion holders accu-
rately. Kim and Hovy (2006) use structural 
features of the language to identify opinion enti-
ties. Their technique is based on syntactic path and 
dependency features along with heuristic features 
such as topic words and named entities. Weigand 
and Klalow (2010) use convolution kernels that 
use predicate argument structure and parse trees. 
For Urdu specifically, work in the area of clas-
sifying subjective and objective sentences is at-
tempted by Mukund and Srihari, (2010) using a 
vector space model. NLP tools that include POS 
taggers, shallow parser, NE tagger and morpho-
logical analyzer for Urdu is provided by Mukund 
et al, (2010). This is the only extensive work done 
for automating Urdu NLP, although other efforts to 
generate semantic role labels and dependency 
parsers are underway. 
3  Linguistic Analysis for Opinion Entities 
In this section we introduce the different cues 
used to capture the contextual information for cre-
ating candidate sequences in Urdu by exploiting 
the morphological richness of the language. 
Table 2: Case Inflections on Nouns  
Urdu is a head final language with post-
positional case markers. Some post-positions are 
associated with grammatical functions and some 
with specific roles associated with the meaning of 
verbs (Davison, 1999). Case markers play a very 
important role in determining the case inflections 
of nouns. The case inflections that are useful in the 
context of opinion entity detection are ?ergative?, 
?dative?, ?genitive?, ?instrumental?  and ?loca-
tive? . Table 2 outlines the constructs. 
Consider example 1 below. (a) is a case where 
? Ali ? is nominative. However, in (b) ? Ali?  is da-
tive. The case marker ?ko?  helps to identify sub-
jects of certain experiential and psychological 
predicates: sensations, psychological or mental 
states and obligation or compulsion. Such predi-
cates clearly require the subject to be sentient, and 
further, indicate that they are aected in some 
manner, correlating with the semantic properties 
ascribed to the dative?s primary use (Grimm, 
2007). 
 
E xample (1): 
(a)  Ali khush hua  (Ali became happy) 
(b)  Ali ko khushi hui (Ali became happy) 
E xample (2): 
(a) Sadaf kaam karne ki koshish karti hai ( Sadaf 
tries to do work)  
 
Semantic information in Urdu is encoded in a 
way that is very different from English. Aspect, 
tense and gender depend on the noun that a verb 
governs. Example 2 shows the dependency that 
verbs have on nouns without addressing the lin-
guistic details associated with complex predicates. 
In example 2, the verb ?karti?(do)  is feminine 
and the noun it governs ~ Sadaf  is also feminine. 
The doer for the predicate ?karti hai?(does)  is 
? Sadaf? and there exists a gender match. This 
shows that we can obtain strong features if we are 
able to accurately (i) identify the predicates, (ii) 
find the governing noun, and (iii) determine the 
gender. 
In this work, for the purpose of generating can-
didate sequences, we encompass the post-position 
responsible for case inflection in nouns, into the 
noun phrase and group the entire chunk as one sin-
gle candidate. In example 1, the dative inflection 
on ? Ali?  is due to the case marker ? ko?.  Here, ? Ali 
ko?  will always be considered together in all candi-
date sequences that this sentence generates. This 
Case Clitic 
Form 
Examples 
Ergative (ne) Ali  ne ghussa dikhaya ~ 
Ali showed anger  
Accusa-
tive 
(ko) Ali ko mainey maara ~ 
I hit Ali  
Dative (ko,ke) Similar to accusative 
Instru-
mental 
(se) Yeh kaam Ali  se hua ~ 
This work was done by 
Ali  
Genitive (ka, ke, ki) Ali ka ghussa, baap re 
baap! ~ Ali?s anger, oh 
my God!  
Locative (mein, par, 
tak, tale, 
talak) 
Ali mein ghussa zyaada 
hai ~ there is a lot of 
anger in Ali  
60
behavior can also be observed in example 1 of ta-
ble 1.  
We use SemantexTM (Srihari et al, 2008) - an 
end to end NLP framework for Urdu that provides 
POS, NE, shallow parser and morphological ana-
lyzer, to mark tense, mood, aspect, gender and 
number inflections of verbs and case inflections of 
nouns. For ease of parsing, we enclose dative and 
accusative inflected nouns and the respective case 
markers in a tag called POSSE S S . We also enclose 
locative, genitive and ergative inflections and case 
markers in a tag called DOER.  
4  Methodology 
Sequence boundaries are first constructed based 
on the POSSESS, DOER and NP (noun chunk) 
tags prioritized by the position of the tag while 
parsing. We refer to these chunks as ?candidates?  
as they are the possible opinion entity candidates. 
We generate candidate sequences by combining 
these candidates with opinion expressions (Mu-
kund and Srihari, 2010) and the predicates that 
contain or follow the expression words (~khushi in 
(b) of example 1 above). 
We evaluate our approach in two steps: 
(i) Boundary Detection - detecting opinion 
entities that contain both holders and tar-
gets 
(ii) Entity Disambiguation - disambiguating 
opinion holders from opinion targets 
In the following sections, we briefly describe 
our research methodology including sequence 
creation, choice of kernels and the challenges thus 
encountered. 
4.1  Data Set 
The data used for the experiments are newswire 
articles from BBC Urdu1 that are manually anno-
tated to reflect opinion holders, targets, and ex-
pressions (emotion bearing words).  
 
Number of  subjective sentences 824 
Average word length of each sentence 42 
Number of opinion holders  974 
Number of opinion targets 833 
Number of opinion expressions 894 
Table 3: Corpus Statistics 
 
                                                           
1 www.bbc.co.uk/urdu/ 
Table 3 summarizes the corpus statistics. The inter 
annotator agreement established between two an-
notators over 30 documents was found to be 0.85 
using Cohen?s Kappa score (averaged over all 
tags). The agreement is acceptable as tagging emo-
tions is a difficult and a personalized task. 
4.2  Support Vector Machines (SVM) and 
Kernel Methods 
SVMs belong to a class of supervised machine 
learning techniques that merge the nuances of sta-
tistical learning theory, kernel mapping and opti-
mization techniques to discover separating 
hyperplanes. Given a set of positive and negative 
data points, based on structural risk minimization, 
SVMs attempt to find not only a separating hyper-
plane that separates two categories (Vapnik and 
Kotz, 2006) but also maximize the boundary be-
tween them (maximal margin separation tech-
nique). In this work, we propose to use a variation 
of sequence kernels for opinion entity detection. 
4.3  Sequence Kernels 
The lack of parsers that capture dependencies in 
Urdu sentences inhibit the use of ?tree kernels? 
(Weigand and Klalow, 2010). In this work, we ex-
ploit the power of a set of sequence kernels known 
as ?gap sequence string kernels? (Lodhi et al, 
2002). These kernels provide numerical compari-
son of phrases as entire sequences rather than a 
probability at the chunk level. Gap sequence ker-
nels measure the similarity between two sequences 
(in this case a sequence of Urdu words) by count-
ing the number of common subsequences. Gaps 
between words are penalized with suitable use of 
decay factor to compensate for 
matches between lengthy word sequences. 
Formally, let  be the feature space over 
words. Consequently, we declare other disjoint 
feature spaces  (stem words, POS, 
chunks, gender inflections, etc.) 
and
.
For any two-feature 
vectors  let  compute the number 
of common features between s and t. Table 5 lists 
the features used to compute . 
Given two sequences, s and t and the kernel 
function  that calculates the number of 
61
weighted sparse subsequences of length n (say, 
n =2: bigram) common to both s and t, then 
is as shown in eq 1 (Bunescu and 
Mooney, 2005). 
 
(i,j,k are dimensions)                                ------ Eq 1. 
Generating correct sequences is a prior require-
ment for sequence kernels. For example, in the task 
of relation extraction, features included in the 
shortest path between the mentions of the two se-
quences (which hold the relation) play a decisive 
role (Bunescu and Mooney, 2005). Similarly, in 
the task of role labeling (SRL - Moschitti et al, 
2008), syntactic sub-trees containing the arguments 
are crucial in finding the correct associations. Our 
approach to create candidate sequences for opinion 
entity detection in Urdu is explained in the next 
section. 
4.4  Candidate Sequence Generation 
Each subjective sentence in Urdu contains sev-
eral noun phrases with one or more opinion ex-
pressions. The words that express opinions 
(expression words) can be contained within a verb 
predicate (if the predicate is complex) or precede 
the verb predicate. These subjective sentences are 
first pre-processed to mark the morphological in-
flections as mentioned in ?3. 
Table 4: Candidate Sequence Generation 
 
We define training candidate sequences as the 
shortest substring t which is a tuple that contains 
the candidate noun phrase (POSSESS, DOER or 
NP), an emotion expression and the closest predi-
cate. Table 4 outlines the steps taken to create the 
candidate sequences and figure 1 illustrates the 
different tuples for a sample sentence.  
Experiments conducted by Weigand and 
Klakow (2010) consider <candidate, predicate> 
and <candidate, expression> tuples. However, in 
Urdu the sense of expression and predicate are so 
tightly coupled (in many examples they subsume 
each other and hence inseparable), that specifically 
trying to gauge the influence of predicate and 
expression separately on candidates is impossible. 
There are three advantages in our approach to 
creating candidate sequences: (i) by pairing ex-
pressions with their nearest predicates, several un-
necessary candidate sequences are eliminated, (ii) 
phrases that do not contain nouns are automatically 
not considered (see RBP chunk in figure 1), and 
(iii) by considering only one candidate chunk at a 
time in generating the candidate sequence, we en-
sure that the sequence that is generated is short for 
better sequence kernel performance. 
 
4 . 4 .1  Linear Kernel features 
 
For linear kernels we define features explicitly 
based on the lexical relationship between the can-
didate and its context. Table 5 outlines the features 
used.  
 
Feature Sets and Description  
Set 1 
Baseline 
1. head word of candidate 
2. case marker contained within candidate? 
3. expression words  
4. head word of predicate 
5. POS sequence of predicate words 
6. # of NPs between candidate and emotion 
Set 2 7. the DOER 
8. expression right after candidate? 
Set 3  9. gender match between candidate and 
predicate 
10. predicate contains emotion words? 
Set 4  11. POS sequence of candidate 
Set 5   12. ?kah?  feature in the predicate 
13. locative feature? 
14. genitive feature on noun? 
Table 5: Linear Kernel Features 
 
 
 
 
1 A sentence is parsed to extract all likely candi-
date chunks ? POSSESS, DOER, NP in that 
order. 
2 <expression, predicate> t uples are first selected 
based on nearest neighbor rule : 
1. Predicates that are paired with the expres-
sion words either contain the expressions or 
follow the expressions.  
2. Stand alone predicates are simply ignored as 
they do not contribute to the holder identifi-
cation task (they contribute to either the sen-
tence topic or the reason for the emotion). 
3 For each candidate, 
<candidate, expression, predicate> tuples are 
generated without changing the word order.  
(Fig. 1 ? example candidates maintain the same 
word order) 
62
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Illustration of candidate sequences 
 
4 . 4 .1  Sequence Kernel features 
 
Features commonly used for sequence kernels 
are based on words (such as character-based or 
word-based sequence kernels). In this work, we 
consider to be a feature space over Urdu words 
along with other disjoint features such as POS, 
gender, case inflections. In the kernel, however, for 
each combination (see table 6) the similarity 
matching function that computes the num-
ber of similar features remains the same. 
Table 6: Disjoint feature set for sequence kernels 
 
Sequence kernels are robust and can deal with 
complex structures. There are several overlapping 
features between the feature sets used for linear 
kernel and sequence kernel.  Consider the POS 
path information feature. This is an important fea-
ture for the linear kernel. However this feature  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
need not be explicitly mentioned for the sequence 
kernel as the model internally learns the path in-
formation. In addition, several Boolean features 
explicitly described for the linear kernel (2 and 13 
in table 5) are also learned automatically in the 
sequence kernel by matching subsequences. 
5  Experiments 
The data used for our experiments is explained 
in ?4.1. Figure 2 gives a flow diagram of the entire 
process. LIBSVM?s (Cha ng and Lin, 2001) linear 
kernel is trained using the manually coded features 
mentioned in table 5. We integrated our proposed 
sequence kernel with the same toolkit. This se-
quence kernel uses the features mentioned in table 
6 and the decay factor is set to 0.5. 
 
 
 
 
 
 
 
 
 
Figure 2: Overall Process 
KID Kernel Type 
1 word based kernel (baseline) 
2 word + POS (parts of speech) 
3 word + POS + chunk  
4 word + POS + chunk + gender inflection  
63
The candidate sequence generation algorithm gen-
erated 8,329 candidate sequences (contains all opi-
nion holders and targets ? table 3) that are used for 
training both the kernels. The data is parsed using 
SemantexTM to apply POS, chunk and morphology 
information. Our evaluation is based on the exact 
candidate boundary (whether the candidate is en-
closed in a POSSESS, DOER or NP chunk).All 
scores are averaged over a 5-fold cross validation 
set. 
5.1  Comparison of Kernels 
We apply both linear kernels (LK) and se-
quence kernels (SK) to identify the entities as well 
as disambiguate between the opinion holders and 
targets. Table 7 illustrates the baselines and the 
best results for boundary detection of opinion enti-
ties. ID 1 of table 7 represents the result of using 
LK with feature set 1 (table 5). We interpret this as 
our baseline result. The best F1 score for this clas-
sifier is 50.17%. 
Table 7: Boundary detection of Opinion Entities 
 
Table 8 compares various kernels and combina-
tions. Set 1 of table 8 shows the relative effect of 
feature sets for LK and how each set contributes to 
detecting opinion entity boundaries. Although sev-
eral features are inspired by similar classification 
techniques (features used for SRL and opinion 
mining by Choi et al, (2005) ~ set 1, table 5), the 
free word nature of Urdu language renders these 
features futile. Moreover, due to larger average 
length of each sentence and high occurrences of 
NPs (candidates) in each sentence, the number of 
candidate instances (our algorithm creates 10 se-
quences per sentence on average) is also very high 
as compared to any English corpus.  This makes 
the training corpus highly imbalanced. Interest-
ingly, when features like ? occurrence of postposi-
tions, ?kah? predicate, gender inflections etc. are 
used, classification improves (set 1, Feature set 
1,2,3,4,5, table 8). 
Table 8: Kernel Performance 
 
ID 3 of table 7 displays the baseline result for SK. 
Interestingly enough, the baseline F1 for SK is 
very close to the best LK performance. This shows 
the robustness of SK and its capability to learn 
complex substructures with only words. A se-
quence kernel considers all possible subsequence 
matching and therefore implements a concept of 
partial (fuzzy) matching. Because of its tendency 
to learn all fuzzy matches while penalizing the 
gaps between words intelligently, the performance 
of SK in general has better recall (Wang, 2008). To 
explain the recall situation, consider set 2 of table 
8. This illustrates the effect of disjoint feature 
scopes of each feature (POS, chunk, gender). Each 
feature adds up and expands the feature space of 
sequence kernel and allows fuzzy matching there-
by improving the recall. Hence KID 4 has almost 
20% recall gain over the baseline (SK baseline).  
However, in many cases, this fuzzy matching 
accumulates in wrong classification and lowers 
precision. A fairly straightforward approach to 
overcome this problem is to employ a high preci-
sion kernel in addition to sequence kernel. Another 
limitation of SK is its inability to capture complex 
I
D Kernel 
Features  
(table 
5/6 ) 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
1 LK Baseline (Set 1) 39.58 51.49 44.75  
2 LK(best) Set 1, 2, 3, 4, 5 44.20 57.99 50.17  
3  SK Baseline (KID 1) 58.22 42.75 49.30  
4 SK (best) KID 4 54.00 62.79 58.06  
5 
Best LK 
+ best 
SK 
KID 4, 
Set 1, 2, 
3, 4, 5 
5 8 . 4 3  65 . 0 4  61.55  
Set Kernel KID Prec.  
(% )  
Rec.  
(% )  
F1 
(% )  
Baseline 
(Set 1) 
39.58  51.49  44.75  
Set 1,2 39.91  52.57  45.38  
Set 1, 2, 3 43.55  57.72  49.65  
Set 1,2,3,4 44.10  56.90  49.68  
 
 
 
 
1 
 
 
 
 
LK 
Feature set 
1,2,3,4,5 
4 4 . 2 0  57 . 9 9  50 .17  
Baseline - 
KID 1 
58.22 42.75  49.30  
KID 2 5 8 . 9 8  47.55  52.65 
KID 3 58.18  49.62  53.59  
 
 
2 
 
 
SK 
KID 4 54.00 6 2 . 7 9  58 . 0 6  
KID 1 + 
best LK 
51.44 6 8 . 8 9  58.90  
KID 2 + 
best LK 
5 9 .18  62.98  61.02 
KID 3 + 
best LK 
55.18 68.38  61.07  
 
 
 
3 
 
 
 
SK  +  
LK 
KID 4 + 
best LK 
58.43  65.04 61.55  
64
grammatical structure and dependencies making it 
highly dependent on only the order of the string 
sequence that is supplied. 
We also combine the similarity scores of SK 
and LK to obtain the benefits of both kernels. This 
permits SK to expand the feature space by natu-
rally adding structural features (POS, chunk) re-
sulting in high recall. At the same time, LK with 
strict features (such as the use of ?kah?  verb) or 
rigid word orders (several Boolean features) will 
help maintain acceptable precision. By summing 
the contribution of both kernels, we achieve an F1 
of 61.55% (Set 3, table 8), which is 17.8%, more 
(relative gain ? around 40%) than the LK baseline 
results (ID 1, table 7). 
 
Table 9: Opinion entity disa mbiguation for best features 
Our next sets of experiments are conducted to dis-
ambiguate opinion holders and targets. A large 
number of candidate sequences that are created are 
not candidates for opinion entities. This results in a 
huge imbalance in the data set. Jointly classify 
opinion holders, opinion targets and false candi-
dates with one model can be attempted if this im-
balance in the data set due to false candidates can 
be reduced. However, this has not been attempted 
in this work. In order to showcase the feasibility of 
our method, we train our model only on the gold 
standard candidate sequences that contain opinion 
entities for entity disambiguation. 
The two kernels are applied on just the two 
classes (opinion holder vs. opinion target). Com-
bined kernels identify holders with a 65.26% F1 
(table 9). However, LK performs best for target 
identification (61.23%). We believe that this is due 
to opinion holders and targets sharing similar syn-
tactic structures. Hence, the sequence information 
that SK learns affects accuracy but improves recall. 
6  Challenges 
Based on the error analysis, we observe some 
common mistakes and provide some examples. 
1. Mistakes resulting due to POS tagger and shal-
low chunker errors. 
2. Errors due to heuristic rules for morphological 
analysis. 
3.  Mistakes due to inaccurate identification of ex-
pression words by the subjectivity classifier. 
4. Errors due to complex and unusual sentence 
structures which the kernels failed to capture. 
 
Example (3):  
Is na-insaafi ka badla hamein zaroor layna chahiye. 
[ we  have to certainly take revenge for this injustice. ] 
E xample (4): 
Kya hum dayshadgardi ka shikar banna chahatein 
hai? 
[Do we  want to become victims of terrorism ? ] 
E xample (5): 
Jab secretary kisi aur say baat karke husthi hai, tho 
Pinto ko ghussa aata hai. 
[When the secretary talks to someone and laughs, 
Pinto  gets angry.] 
 
Example 3 is a false positive. The emotion is ?an-
ger?, indicated by ?na-insaafi ka badla? (revenge 
for injustice) and ?zaroor? (certainly) .  But only 
the second expression word is identified accu-
rately. The sequence kernel model determines na-
insaafi (injustice) to be the opinion holder when it 
is actually the reason for the emotion. However, it 
also identifies the correct opinion holder - hamein 
(we) .  Emotions associated with interrogative sen-
tences are not marked (example 4) as there exists 
no one word that captures the overall emotion. 
However, the subjectivity classifier identifies such 
sentences as subjective candidates. This results in 
false negatives for opinion entity detection. The 
target (secretary) in example 5, fails to be detected 
as no candidate sequence that we generate indi-
cates the noun ?secretary?  to be the target. We 
propose to address these issues in our future work. 
7  Conclusion 
We describe an approach to identify opinion en-
tities in Urdu using a combination of kernels. To 
the best of our knowledge this is the first attempt 
where such an approach is used to identify opinion 
entities in a language lacking the availability of 
resources for automatic text processing. The per-
formance for this task for Urdu is equivalent to the 
state of the art performance for English (Weigand 
and Klakow, 2010) on the same task.  
Kernel Opinion 
Entity 
Prec. 
(% ) 
Rec. 
(% ) 
F1 
(% ) 
Holder 58.71 66.67 62.44 LK 
(best) Target 6 5 . 5 3 57.48 61.23 
Holder 60.26 69.46 64.54 SK 
 Target 59.75 49.73 54.28 
Holder 62.90 6 9 . 81 65. 2 6 Both 
kernels Target 60.71 55.44 57.96 
65
References  
Collin F. Baker, Charles J. Fillmore, John B. Lowe. 
1998. The Berkeley FrameN et Project, Proceedings 
of the 17th international conference on Computa-
tional linguistics, August 10-14. Montreal, Quebec, 
Canada 
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios 
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic 
Extraction of Opinion Propositions and their Holders, 
AAAI Spring Symposium on Exploring Attitude and 
Affect in Text: Theories and Applications. 
Kenneth Bloom, Sterling Stein, and Shlomo Argamon. 
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings of NTCIR-6 Work-
shop Meeting, Tokyo, Japan. 
R. C. Bunescu and R. J. M ooney. 2005. A shortest path 
dependency kernel for relation extraction. In Pro-
ceedings of HLT/EMNLP. 
R. C. Bunescu and R. J.  Mooney. 2005. Subsequence 
Kernels for Relation Extraction. NIPS. Vancouver. 
December. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.cs ie.ntu.edu.tw/~cjlin/libsvm 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan. 2005. Identifying Sources of Opinions 
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human 
Language Technology and Empirical Methods in 
Natural Language Processing (HLT/EMNLP), Van-
couver, Canada.  
Aaron Culotta and Jeffery Sorensen. 2004. Dependency 
tree kernels for relation extraction. In Proceedings of 
the 42rd Annual Meeting of the Association for 
Computational Linguistics. pp. 423-429.   
Alice Davison. 1999. Syntax  and Morphology in Hindi 
and Urdu: A Lexical Resource.  University of Iowa.  
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiWordNet: A publicly available lexical resource for 
opinion mining. In Proc of LREC. Vol 6, pp 417-422.  
Scott Gimm. 2007. Subject Ma rking in Hindi/Urdu: A 
Study in Case and Agency. ESSLLI Student Session. 
Malaga, Spain. 
Youngho Kim, Seaongchan Kim and Sun-Hyon 
Myaeng. 2008. Extracting Topic-related Opinions 
and their Targets in NTCIR-7. In Proceedings of the 
7th NTCIR Workshop Meeting. Tokyo. Japan. 
John Lafferty, Andrew McCa llum and F. Pereira. 2001. 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In: Proc. 
18th International Conf. on Machine Learning, Mor-
gan Kaufmann, San Francisco, CA . pp. 282?289 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, 
Nello Cristianini, Chris Watkins. 2002. Text 
classification using string kernels. J. Mach. Learn. 
Res. 2 (March 2002), 419-44. 
Kim, Soo-Min. and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text. In ACL Workshop on Sen-
timent and Subjectivity in Text. 
Alessandro Moschitti, Daniele Pighin, Roberto Basili. 
2008. Tree kernels for semantic role labeling. Com-
putational Linguistics. Vol 34, num 2, pp 193-224. 
Smruthi Mukund and Rohini K. Srihari. 2010. A Vector 
Space Model for Subjectivity Classification in Urdu 
aided by Co-Training, In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics, 
Beijing, China.  
Smruthi Mukund, Rohini K. Srihari and Erik Peterson. 
2010. An Information Extraction System for Urdu ? 
A Resource Poor Language. Special Issue on Infor-
mation Retrieval for Indian Languages. 
Ellen Riloff, Janyce Wiebe and Theresa Wilson. 2003. 
Learning subjective nouns using extraction pattern 
bootstrapping. In Proceedings of the Seventh Confer-
ence on Natural Language Learning (CoNLL-03). 
Rohini K. Srihari, W. Li, C. Niu, and T. Cornell. 2008. 
InfoXtract: A Customizable Intermediate Level In-
formation Extraction Engine, Journal of Natural Lan-
guage Engineering, Cambridge U. Press, 14(1), pp. 
33-69. 
Veselin Stoyanov and Claire Cardie. 2008.  Annotating 
Topic Opinions. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2008), Marrakech, Morocco. 
John Shawe-Taylor and Nello  Cristianni. 2004. Kernel 
methods for pattern analysis. Cambridge University 
Press. 
Mengqiu Wang. 2008. A Re-examination of Depend-
ency Path Kernels for Relation Extraction, In 
Proceedings of IJCNLP 2008. 
Michael Wiegand and Dietrich Klalow. 2010. Convolu-
tion kernels for opinion holder extraction. In Proc. of 
Human Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. pp 795-
803, ACL 
66
Vladimir Vapnik, S.Kotz . 2006. Estimation of De-
pendences Based on Empirical Data. Springer,  510 
pages. 
67
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 1?8,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Analyzing Urdu Social Media for Sentiments using Transfer Learning 
with Controlled Translations 
 
 Author 2 
Smruthi Mukund Rohini K Srihari 
CEDAR, Davis Hall, Suite 113 CEDAR, Davis Hall, Suite 113 
University at Buffalo, SUNY, Buffalo, NY University at Buffalo, SUNY, Buffalo, NY 
smukund@buffalo.edu rohini@cedar.buffalo.edu 
 
 
 
 
 
Abstract 
The main aim of this work is to perform sen-
timent analysis on Urdu blog data. We use the 
method of structural correspondence learning 
(SCL) to transfer sentiment analysis learning 
from Urdu newswire data to Urdu blog data. 
The pivots needed to transfer learning from 
newswire domain to blog domain is not trivial 
as Urdu blog data, unlike newswire data is 
written in Latin script and exhibits code-
mixing and code-switching behavior. We con-
sider two oracles to generate the pivots. 1. 
Transliteration oracle, to accommodate script 
variation and spelling variation and 2. Trans-
lation oracle, to accommodate code-switching 
and code-mixing behavior.  In order to identi-
fy strong candidates for translation, we pro-
pose a novel part-of-speech tagging method 
that helps select words based on POS catego-
ries that strongly reflect code-mixing behav-
ior. We validate our approach against a 
supervised learning method and show that the 
performance of our proposed approach is 
comparable. 
1 Introduction 
The ability to break language barriers and under-
stand people's feelings and emotions towards soci-
etal issues can assist in bridging the gulf that exists 
today. Often emotions are captured in blogs or dis-
cussion forums where writers are common people 
empathizing with the situations they describe. As 
an example, the incident where a cricket team vis-
iting Pakistan was attacked caused widespread an-
guish among the youth in that country who thought 
that they will no longer be able to host internation-
al tournaments. The angry emotion was towards 
the failure of the government to provide adequate 
protection for citizens and visitors. Discussion fo-
rums and blogs on cricket, mainly written by Paki-
stani cricket fans, around the time, verbalized this 
emotion. Clearly analyzing blog data helps to esti-
mate emotion responses to domestic situations that 
are common to many societies. 
Traditional approaches to sentiment analysis re-
quire access to annotated data. But facilitating such 
data is laborious, time consuming and most im-
portantly fail to scale to new domains and capture 
peculiarities that blog data exhibits; 1. spelling var-
iations and 2. code mixing and code switching. 3. 
script difference (Nastaliq vs Latin script). In this 
work, we present a new approach to polarity classi-
fication of code-mixed data that builds on a theory 
called structural correspondence learning (SCL) 
for domain adaptation. This approach uses labeled 
polarity data from the base language (in this case, 
Urdu newswire data - source) along with two sim-
ple oracles that provide one-one mapping between 
the source and the target data set (Urdu blog data).  
Subsequent sections are organized as follows. 
Section 2 describes the issues seen in Urdu blog 
data followed by section 3 that explains the con-
cept of structural correspondence learning. Section 
4 details the code mixing and code switching be-
havior seen in blog data. Section 5 describes the 
statistical part of speech (POS) tagger developed 
for blog data required to identify mixing patterns 
followed by the sentiment analysis model in sec-
tion 6. We conclude with section 7 and briefly out-
line analysis and future work in section 8. 
1
2 Urdu Blog Data 
Though non-topical text analysis like emotion de-
tection and sentiment analysis, have been explored 
mostly in the English language, they have also 
gained some exposure in non-English languages 
like Urdu (Mukund and Srihari, 2010), Arabic 
(Mageed et al, 2011) and Hindi (Joshi and 
Bhattacharya, 2012). Urdu newswire data is writ-
ten using Nastaliq script and follows a relatively 
strict grammatical guideline. Many of the tech-
niques proposed either depend heavily on NLP 
features or annotated data. But, data in blogs and 
discussion forums especially written in a language 
like Urdu cannot be analyzed by using modules 
developed for Nastaliq script for the following rea-
sons; (1) the tone of the text in blogs and discus-
sion forums is informal and hence differs in the 
grammatical structure (2) the text is written using 
Latin script (3) the text exhibits code mixing and 
code switching behavior (with English) (4) there 
exists spelling errors which occur mostly due to the 
lack of predefined standards to represent Urdu data 
in Latin script. 
Urdish (Urdu blog data) is the term used for 
Urdu, which is (1) written either in Nastaliq or Lat-
in script, and (2) contains several English 
words/phrases/sentences.  In other words, Urdish is 
a name given to a language that has Urdu as the 
base language and English as the seasoning lan-
guage. With the wide spread use of English key-
boards these days, using Latin script to encode 
Urdu is very common. Data in Urdish is never in 
pure Urdu. English words and phrases are com-
monly used in the flow integrating tightly with the 
base language. Table 1 shows examples of differ-
ent flavors in which Urdu appears in the internet. 
Differ-
ent 
Forms 
of Data 
Main Issues Example Sentence 
1. Urdu 
written 
in Nasta-
liq 
1.  Lack of tools for 
basic operations such 
as segmentation and 
diacritic restoration 
2. Lack of sufficient 
annotated data for 
POS and NE tagging 
3.  Lack of annotated 
data for more ad-
vanced NLP  
??? ?????? ?? ???? 
????? ?? ??? ???? 
[ The soldiers were 
angry with a lot of 
people] 
 
2. Urdu 
written 
in ASCII 
1.  Several variations 
in spellings that need 
to be normalized 
Wo Mulk Jisko Hum 
nay 1000000 logoon 
sey zayada Loogoon 
(Eng-
lish) 
2.  No normalization 
standards 
3.  Preprocessing 
modules needed if 
tools for Urdu in 
Nastaliq are to be 
used 
4.  Developing a 
completely new NLP 
framework needs 
annotated data 
ki Qurbanian dey ker 
hasil kia usi mulk 
main yai kaisa waqt a 
gay hai ? 
 
[Look at what kind of 
time the land that had 
1000000?s of people 
sacrifice their lives is 
experiencing now] 
3. Urd-
ish writ-
ten in 
Nastaliq 
1.  No combined 
parser that deals with 
English and Urdu 
simultaneously 
2.  English is written 
in Urdu but with 
missing diacritics 
?? ??? ??? ????? ?? ?? 
??? ??? ???  
 
[the phones rang one 
after the other in the 
TV station] 
 
4. Urd-
ish writ-
ten in  
ASCII(
English) 
1.  No combined 
parser that deals 
with English and 
Urdu simultaneous-
ly 
2.  Issue of spelling 
variations that need 
to be normalized 
Afsoos key baat hai . 
kal tak jo batain 
Non Muslim bhi 
kartay hoay dartay 
thay abhi this man 
has brought it out in 
the open. 
 
[It is sad to see that 
those words that even 
a non muslim would 
fear to utter till yes-
terday, this man had 
brought it out in the 
open] 
Table 1: Different forms of using Urdu lan-
guage on the internet 
Blog data follows the order shown in example 
4 of table 1. Such a code-switching phenomenon is 
very common in multilingual societies that have 
significant exposure to English. Other languages 
exhibiting similar behaviors are Hinglish (Hindi 
and English), Arabic with English and Spanglish 
(Spanish with English). 
3   Structural Correspondence Learning 
For a problem where domain and data changes 
requires new training and learning, resorting to 
classical approaches that need annotated data be-
comes expensive. The need for domain adaptation 
arises in many NLP tasks ? part of speech tagging, 
semantic role labeling, dependency parsing, and 
sentiment analysis and has gained high visibility in 
the recent years (Daume III and Marcu, 2006; 
Daume III et al, 2007; Blitzer et al, 2006, Pret-
tenhofer and Stein et al, 2010). There exists two 
main approaches; supervised and semi-supervised.  
2
In the supervised domain adaptation approach 
along with labeled source data, there is also access 
to a small amount of labeled target data. Tech-
niques proposed by Gildea (2001), Roark and Bac-
chiani (2003), Daume III (2007) are based on the 
supervised approach. Studies have shown that 
baseline approaches (based on source only, target 
only or union of data) for supervised domain adap-
tion work reasonably well and beating this is sur-
prisingly difficult (Daume III, 2007).  
 In contract, the semi supervised domain adapta-
tion approach has access to labeled data only in the 
source domain (Blitzer et al, 2006; Dredze et al, 
2007; Prettenhofer and Stein et al, 2010). Since 
there is no access to labeled target data, achieving 
baseline performance exhibited in the supervised 
approach requires innovative thinking.  
The method of structural correspondence learn-
ing (SCL) is related to the structural learning para-
digm introduced by Ando and Zhang (2005). The 
basic idea of structural learning is to constrain the 
hypothesis space of a learning task by considering 
multiple different but related tasks on the same 
input space. SCL was first proposed by Blitzer et 
al., (2006) for the semi supervised domain adapta-
tion problem and works as follows (Shimizu and 
Nakagawa, 2007).  
1. A set of pivot features are defined on unla-
beled data from both the source domain and 
the target domain 
2. These pivot features are used to learn a map-
ping from the original feature spaces of both 
domains to a shared, low-dimensional real?
valued feature space. A high inner product in 
this new space indicates a high degree of cor-
respondence along that feature dimension  
3. Both the transformed and the original features 
in the source domain are used to train a learn-
ing model  
4. The effectiveness of the classifier in the source 
domain transfers to the target domain based on 
the mapping learnt 
This approach of SCL was applied in the field of 
cross language sentiment classification scenario by 
Prettenhofer and Stein (2010) where English was 
used as the source language and German, French 
and Japanese as target languages. Their approach 
induces correspondence among the words from 
both languages by means of a small number of 
pivot pairs that are words that process similar se-
mantics in both the source and the target lan-
guages. The correlation between the pivots is mod-
eled by a linear classifier and used as a language 
independent predictor for the two equivalent clas-
ses. This approach solves the classification prob-
lem directly, instead of resorting to a more general 
and potentially much harder problem such as ma-
chine translation. 
The problem of sentiment classification in blog 
data can be considered as falling in the realm of 
domain adaptation. In this work, we approach this 
problem using SCL tailored to accommodate the 
challenges that code-mixed data exhibits. Similar 
to the work done by Prettenhofer and Stein (2010), 
we look at generating pivot pairs that capture code-
mixing and code-switching behavior and language 
change.  
4 Code Switching and Code Mixing 
Code switching refers to the switch that exists from 
one language to another and typically involves the 
use of longer phrases or clauses of another lan-
guage while conversing in a totally different base 
language. Code mixing, on the other hand, is a 
phenomenon of mixing words and other smaller 
units of one language into the structure of another 
language. This is mostly inter-sentential.  
In a society that is bilingual such as that in Pa-
kistan and India, the use of English in the native 
language suggests power, social prestige and the 
status. The younger crowd that is technologically 
well equipped tends to use the switching phenom-
enon in their language, be it spoken or written. 
Several blogs, discussion forums, chat rooms etc. 
hold information that is expressed is intensely code 
mixed. Urdu blog data exhibits mix of Urdu lan-
guage with English. 
There are several challenges associated with 
developing NLP systems for code-switched lan-
guages. Work done by Kumar (1986) and Sinha & 
Thakur, (2005) address issues and challenges asso-
ciated with Hinglish (Hindi ? English) data.  
Dussias (2003) and Celia (1997) give an overview 
of the behavior of code switching occurring in 
Spanish - Spanglish. This phenomenon can be seen 
in other languages like Kannada and English, 
German and English.  Rasul (2006) analyzes the 
linguistic patterns occurring in Urdish (Urdu and 
English) language. He tries to quantize the extent 
to which code-mixing occurs in media data, in par-
ticular television. Most of his rules are based on 
3
what is proposed by Kachru (1978) for Hinglish 
and has a pure linguistic approach with manual 
intervention for both qualitative and quantitative 
analysis.  
Several automated techniques proposed for 
Hinglish and Spanglish are in the context of ma-
chine translation and may not be relevant for a task 
like information retrieval since converting the data 
to one standardized form is not required. A more 
recent work was by Goyal et al, (2003) where they 
developed a bilingual parser for Hindi and English 
by treating the code mixed language as a complete-
ly different variety. However, the credibility of the 
system depends on the availability of WordNet1.  
4.1 Understanding Mixing Patterns 
Performing analysis on data that exhibit code-
switching has been attempted by many across vari-
ous languages. Since the Urdu language is very 
similar to Hindi, in this section we discuss the 
code-mixing behavior based on a whole battery of 
work done by researchers in the Hindi language. 
Researchers have studied the behavior of the 
mixed patterns and generated rules and constraints 
on code-mixing. The study of code mixing with 
Hindi as the base language is attempted by Sinha 
and Thakur (2005) in the context of machine trans-
lation. They categorize the phenomenon into two 
types based on the extent to which mixing happens 
in text in the context of the main verb. Linguists 
such as Kachru (1996) and Poplack (1980) have 
tried to formalize the terminologies used in this 
kind of behavior. Kumar (1986) says that the moti-
vation for assuming that the switching occurs 
based on certain set of rules and constraints are 
based on the fact that users who use this can effec-
tively communicate with each other despite the 
mixed language. In his paper he proposes a set of 
rules and constraints for Hindi-English code 
switching. However, these rules and constraints 
have been countered by examples proposed in the 
literature (Agnihotri, 1998). This does not mean 
that researchers earlier had not considered all the 
possibilities. It only means that like any other lan-
guage, the language of code-mixing is evolving 
over time but at a very fast pace. 
One way to address this problem of code-mixing 
and code switching for our task of sentiment analy-
                                                          
1 http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
sis in blog data is rely on predefined rules to identi-
fy mixed words. But this can get laborious and the 
rules may be insufficient to capture the latest be-
havior. Our approach is to use a statistical POS 
model to determine part of speech categories of 
words that typically undergo such switches.  
5 Statistical Part of Speech Tagger  
Example 5.1 showcases a typical sentence seen in 
blog data. Example 5.2 shows the issue with 
spelling variations sometimes that occur in the 
same sentence 
Example 5.1: Otherwise humara bhi wohi haal hoga jo 
is time Palestine, Iraq, Afghanistan wagera ka hai ~ 
Otherwise our state will also be like what is in Pales-
tine, Iraq, Afghanistan etc. are experiencing at this time 
Example 5.2: Shariyat ke aitebaar se bhi ghaur kia jaey 
tu aap ko ilm ho jaega key joh haraam khata hai uska 
dil kis tarhan ka hota hey ~ If you look at it from morals 
point of you too you will understand the heart of people 
who cheat 
A statistical POS tagger for blog data has to take 
into consideration spelling variations, mixing pat-
terns and script change. The goal here is not to 
generate a perfect POS tagger for blog data 
(though the idea explained here can be extended 
for further improvisation) but to be able to identify 
POS categories that are candidates for switch and 
mix. The basic idea of our approach is as follows 
1. Train Latin script POS tagger (LS tagger) on 
pure Urdu Latin script data (Example 2 in table 
1 ? using Urdu POS tag set, Muaz et al, 2009) 
2. Train English POS tagger on English data 
(based on English tag sets, Santorini, 1990) 
3. Apply LS tagger and English tagger on Urdish 
data and note the confidence measures of the 
applied tags on each word 
4. Use confidence measures, LS tags, phoneme 
codes (to accommodate spelling variations) as 
features to train a new learning model on Urd-
ish data 
5. Those words that get tagged with the English 
tagset are potential place holders for mixing 
patterns  
 
Word Act Eng LS 
Urdu  
Urd 
CM 
Eng 
CM 
and CC CC NN 0.29 0.99 
most RB RB VM 0.16 0.83 
im-
portant 
JJ JJ VAUX 0.08 0.97 
thing NN NN CC 0.06 0.91 
4
Zardari NNP NNP NN 0.69 0.18 
ko PSP NNP PSP 0.99 0.28 
shoot VB NNP JJ 0.54 0.29 
ker NN NNP NN 0.73 0.29 
dena VM NNP VM 0.83 0.29 
chahiya VAUX NNP VAUX 0.98 0.21 
. SYM . SYM 0.99 0.99 
Table 2. POS tagger with confidence measures 
 
The training data needed to develop LS tagger for 
Urdu is obtained from Hindi. IIIT POS annotated 
corpus for Hindi contains data in the SSF format 
(Shakti Standard Format) (Bharati, 2006). This 
format tries to capture the pronunciation infor-
mation by assigning unique English characters to 
Hindi characters. Since this data is already in Latin 
script with each character capturing a unique pro-
nunciation, changing this data to a form that repli-
cates chat data using heuristic rules is trivial. 
However, this data is highly sanskritized and hence 
need to be changed by replacing Sanskrit words 
with equivalent Urdu words. This replacement is 
done by using online English to Urdu dictionaries 
(www.urduword.com and www.hamariweb.com). 
We have succeeded in replacing 20,000 pure San-
skrit words to Urdu by performing a manual 
lookup. The advantage with this method is that  
1. The whole process of setting up annotation 
guidelines and standards is eliminated.  
2. The replacement of pure Hindi words with Ur-
du words in most cases is one-one and the POS 
assignment is retained without disturbing the 
entire structure of the sentence. 
Our training data now consists of Urdu words writ-
ten in Latin script. We also generate phonemes for 
each word by running the phonetic model. A POS 
model is trained using CRF (Lafferty, 2001) learn-
ing method with current word, previous word and 
the phonemes as features. This model called the 
Latin Script (LS) POS model has an F-score of 
83%.  
English POS tagger is the Stanford tagger that 
has a tagging accuracy of about 98.7%2 . 
5.1 Approach 
Urdish blog data consists of Urdu code-mixed with 
English. Running simple Latin script based Urdu 
POS tagger results in 81.2% accuracy when POS 
tags on the entire corpus is considered and 52.3% 
                                                          
2 http://nlp.stanford.edu/software/tagger.shtml 
accuracy on only the English words. Running Eng-
lish tagger on the entire corpus improves the POS 
tagging accuracy of English words to 79.2% accu-
racy. However, the tagging accuracy on the entire 
corpus reduces considerably ? 55.4%. This indi-
cates that identifying the language of the words 
will definitely improve tagging.  
Identifying the language of the words can be 
done simply by a lexicon lookup. Since English 
words are easily accessible and more enriched, 
English Wordnet3 makes a good source to perform 
this lookup. Running Latin script POS tagger and 
English tagger on the language specific words re-
sulted in 79.82% accuracy for the entire corpus and 
59.2% accuracy for English words. Clearly there is 
no significant gain in the performance. This is on 
account of English equivalent Urdu representation 
of words (e.g. key ~ their, more ~ peacock, bat ~ 
speak). 
Since identifying the language explicitly yields 
less benefit, we showcase a new approach that is 
based on the confidence measures of the taggers. 
We first run the English POS tagger on the entire 
corpus. This tagger is trained using a CRF model. 
Scores that indicate the confidence with which this 
tagger has applied tags to each word in the corpus 
is also estimated (table 2). Next, the Latin script 
tagger is applied on the entire corpus and the con-
fidence scores for the selected tags are estimated. 
So, for each word, there exist two tags, one from 
the English tagger and the other from the Latin 
script Urdish tagger along with their confidence 
scores. This becomes our training corpus. 
The CRF learning model trained on the above 
corpus using features shown in table 3 generates a 
cross validation accuracy is 90.34%. The accuracy 
on the test set is 88.2%, clearly indicating the ad-
vantages of the statistical approach.  
 
Features used to train Urdish POS tagger 
Urdish word 
POS tag generated by LS tagger 
POS tag generated by English tagger 
Confidence measure by LS tagger 
Confidence measure by English tagger 
Double metaphone value 
Previous and next tags for English and Urdu 
Previous and next words 
Confidence priorities 
Table 3. Features used to train the final POS tagger 
for Urdish data 
                                                          
3 http://wordnet.princeton.edu/ 
5
Table 4 illustrates the POS categories used as po-
tential pattern switching place holders  
 
POS Category Example 
noun within a noun 
phrase 
uski life par itna control acha nahi 
hai ~ its not good to control his life 
this much 
Interjection  Comon Reema yaar! ~ Hey Man 
Reema! 
lol! ~ lol 
Adjective Yeh story bahut hi scary or ugly tha 
~ This story was really scary and 
ugly 
Adverb Babra Shareef ki koi bhi film lagti 
hai, hum definitely dekhtai ~ I would 
definitely watch any movie of Babra 
Shareef 
Gerund (tagged as a 
verb by English 
POS tagger) 
Yaha shooting mana hai ~ shooting 
is prohibited here 
Verb Iss movie main I dozed ~ I slept 
through the movie 
Verb Afridi.. Cool off!  
Table 4. POS categories that exhibit pattern switch 
6 Sentiment Polarity Detection 
The main goal of this work is to perform sentiment 
analysis in Urdu blog data. However, this task is 
not trivial owing to all the peculiarities that blog 
data exhibits. The work done on Urdu sentiment 
analysis (Mukund and Srihari, 2010) provided an-
notated data for sentiments in newswire domain. . 
Newspaper data make a good corpus to analyze 
different kinds of emotions and emotional traits of 
the people.  They reflect the collective sentiments 
and emotions of the people and in turn the society 
to which they cater. When specific frames are con-
sidered (such as semantic verb frames) in the con-
text of the triggering entities ? opinion holders 
(entities who express these emotions) and opinion 
targets (entities towards whom the emotion is di-
rected) - performing sentiment analysis becomes 
more meaningful and newspapers make an excel-
lent source to analyze such phenomena (Mukund et 
al., 2011). We use SCL to transfer sentiment anal-
ysis learning from this newswire data to blog data. 
Inspired by the work done by (Prettenhofer and 
Stein, 2010), we rely on oracles to generate pivot 
pairs. A pivot pair {wS, wT} where wS ? 9S (the 
source language ? Urdu newswire data) and wT ? 
VT (the target language ? Urdish data) should satis-
fy two conditions 1. high support and 2. high con-
fidence, making sure that the pairs are predictive of 
the task.  
Prettenhofer and Stein (2010) used a simple 
translation oracle in their experiments. However 
there exist several challenges with Urdish data that 
inhibits the use of a simple translation oracle.  
1. Script difference in the source and target 
languages. Source corpus (Urdu) is written 
in Nastaleeq and the target corpus (Urdish) 
is written in ASCII 
2. Spelling variations in roman Urdu 
3. Frequent use of English words to express 
strong emotions 
We use two oracles to generate pivot pairs.  
The first oracle accommodates the issue with 
spelling variations. Each Urdu word is converted to 
roman Urdu using IPA (1999) guidelines. Using 
the double metaphone algorithm4 phoneme code 
for the Urdu word is determined. This is also ap-
plied to Urdish data at the target end. Words that 
have the same metaphone code across the source 
and target languages are considered pivot pairs.  
The second oracle is a simple translation oracle 
between Urdu and English. Our first experiment 
(experiment 1) is using words that belong to the 
adjective part of speech category as candidates for 
pivots. We augment this set to include words that 
belong to other POS categories shown in table 4 
that exhibit pattern mixing (experiment 2).  
 
6.1 Implementation  
 
The feature used to train the learning algorithm is 
limited to unigrams. For linear classification, we 
use libSVM (Chang and Lin, 2011). The computa-
tional bottleneck of this method is in the SVD de-
composition of the dense parameter matrix W. We 
set the negative values of W to zero to get a sparse 
representation of the matrix. For SVD computation 
the Lanczos algorithm provided by SVDLIBC5 is 
employed. Each feature matrix used in libSVM is 
scaled between -1 and 1 and the final matrix for 
SVD is standardized to zero mean and unit vari-
ance estimated on DS U Du (source subset and tar-
get subset). 
6.2 Results 
The domain of the source data set is limited to 
cricket and movies in order to ensure domain over-
                                                          
4 http://en.wikipedia.org/wiki/Double_Metaphone 
5 http://tedlab.mit.edu/~dr/SVDLIBC 
6
lap between newswire data that we have and blog 
data. In order to benchmark the proposed tech-
nique, our baseline technique is based on the con-
ventional method of supervised learning approach 
on annotated data. Urdish data set used for polarity 
classification contains 705 sentences written in 
ASCII format (example 6.1). This corpus is manu-
ally annotated by one annotator (purely based on 
intuition and does not follow any predefined anno-
tation guidelines) to get 440 negative sentences 
and 265 positive sentences. The annotated corpus 
is purely used for testing and in this work consid-
ered as unlabeled data. A suitable linear kernel 
based support vector machine is modeled on the 
annotated data and a five-fold cross validation on 
this set gives an F-Measure of 64.3%. 
Example 6.1: 
General zia-ul-haq ke zamane mai qabayli elaqe Russia 
ke khilaf jang ka merkaz thea aur general Pervez 
Musharraf ke zamane mai ye qabayli elaqe Pakistan ke 
khilaf jang ka markaz ban gye . ~ negative 
Our first experiment is based on using the se-
cond oracle for translations on only adjectives 
(most obvious choice for emotion words). We use 
438 pivot pairs. The average F-measure for the 
performance is at 55.78% which is still much be-
low the baseline performance of 64.3% if we had 
access to annotated data. However, the results 
show the ability of this method. 
Our second experiment expands the power of 
the second oracle to provide translations to other 
POS categories that exhibit pattern switching. This 
increased the number of pivot pairs to 640. In-
crease in pivots improved the precision. Also we 
see significant improvement in the recall. The new-
ly added pivots brought more sentences under the 
radar of the transfer model. The average F-
Measure increased to 59.71%.  
The approach can be further enhanced by im-
proving the oracle used to select pivot features. 
One way is add more pivot pairs based on the cor-
relation in the topic space across language domains 
(future work).  
7 Conclusion 
In this work we show a way to perform sentiment 
analysis in blog data by using the method of struc-
tural correspondence learning. This method ac-
commodates the various issues with blog data such 
as spelling variations, script difference, pattern 
switching. 
 
Table 5. SCL based polarity classification for Urdish data 
We rely on two oracles, one that takes care of 
spelling variations and the other that provides 
translations. The words that are selected to be 
translated by the second oracle are carefully cho-
sen based on POS categories that exhibit emotions 
and pattern switching. We show that the perfor-
mance of this approach is comparable to what is 
achieved by training a supervised learning model. 
In order to identify the POS categories that exhibit 
pattern switching, we developed a statistical POS 
tagger for Urdish blog data using a method that 
does not require annotated data in the target lan-
guage. Through these two modules (sentiment 
analysis and POS tagger for Urdish data) we suc-
cessfully show that the efforts in performing non-
topical analysis in Urdu newswire data can easily 
be extended to work on Urdish data. 
8 Future work 
Analyzing the test data set for missing and false 
positives, here are some of the examples of where 
the model did not work 
Example 7.1: ?tring tring tring tring.. Phone to bar bar 
bajta hai. Annoying.? ~ tring tring tring tring tring.. 
the phone rings repeatedly. Annoying. 
Example 7.2: ?bookon ko padna tho ab na mumkin hai. 
Yaha thak mere friends mujhe blindee pukarthey hai? ~ 
cannot read books any more. Infact, my friends call me 
blindee. 
Example 7.3: ?Ek Tamana Hai Ke Faqt Mujh Pe 
Mehrban Raho, Tum Kise Or Ko Dekho To Bura Lagta 
Hai? ~ I have this one wish that destiny be kind to me 
If you see someone else I feel bad 
Our method fails to tag sentences like in example 
7.1 where English verbs are used by themselves. 
Our POS tagger fails to capture such stand-alone 
Precision (P %) Recall (R %) F-Measure (F %) 
Phonemes (Roman Urdu) 
37.97 58.82 46.15 
Metaphones based synonym mapping (adjectives) 
50.9 51 50.89 
56.6 56.4 55.62 
58.9 60.64 59.75 
Precision (P %) Recall (R %) F-Measure (F %) 
Metaphones based synonym mapping (adjectives + other 
POS categories) 
54.2 64.3 58.82 
58.4 60.85 59.6 
59.4 62.12 60.73 
7
verbs as verbs but tags them as nouns. Hence, 
GRHVQ?W RFFXU LQ WKH SLYRW VHW  
Our second issue is with Morpho syntactic 
switching, a behavior seen in example 7.2. 
Nadhkarni (1975) and Pandaripande (1983) have 
shown that when two or more languages come into 
contact, there is mutual feature transfer from one 
language to another. The languages influence each 
other considerably and constraints associated with 
free morphemes fail in most cases. The direction 
and frequency of influence depends on the social 
status associated with the languages used in mix-
ing. The language that has a high social status 
tends to use the morphemes of the lower language.  
Example 7.4: Bookon ? in books, Fileon ? in files, 
Companiyaa ? many companies 
Clearly we can see that English words due to their 
frequent contact with Urdu grammatical system 
tend to adopt the morphology associated with the 
base language and used mostly as native Urdu 
words. These are some issues, if addressed, will 
definitely improve the performance of the senti-
ment analysis model in Urdish data. 
References  
Abdul-Mageed, M., Diab, M., and Korayem, M. 2011.  Sub-
jectivity and Sentiment Analysis of Modern Standard Ara-
bic. In proceedings of the 49th Meeting of ACL. Portland, 
Oregon, USA, June 19-24 
Agnihotri, Rama Kant. 1998. Social Psychological Perspec-
tives on Second Language Learning. Sage Publications, 
New Delhi 
Bharati, Askhar, Rajeev Sangal and Dipti M Sharma. 2005. 
Shakti Analyser: SSF Representation 
Blitzer, John, Ryan McDonald, and Fernando Pereira. 2006. 
Domain adaptation with structural correspondence learning. 
In proceedings of the 2006 Conference on EMNLP, pp. 
120?128, Sydney, Australia 
Chang, Chih-Chung, Chih-Jen Lin. 2011. LIBSVM: a library 
for support vector machines. In the ACM Transactions on 
Intelligent Systems and Technology, Vol 2, no 27, pp 1-27 
Dredze, Mark., Blitzer, John., Talukdar,  Partha Pratim., 
Ganchev, Kuzman., Graca, Joao., Pereira, Fernando. 2007. 
Frustratingly Hard Domain Adaptation for Parsing. Shared 
Task  of CoNLL. 
Dussias, P. E. 2003. Spanish-English code-mixing at the auxil-
iary phrase: Evidence from eye-movements. Revista Inter-
nacional de Ling??stica Iberoamerican. Vol  2, pp. 7-34 
Gildea, Daniel and Jurafsky, Dan. 2002. Automatic Labeling 
of Semantic Roles, Computational Linguistics, 28(3):245?
288 
Goyal, P, Manav R. Mital, A. Mukerjee, Achla M. Raina, D. 
Sharma, P. Shukla, and K Vikram.  2003. Saarthaka - A Bi-
lingual Parser for Hindi, English and code-switching struc-
tures. In proceedings of the 11th Conference of the ECAL 
Hal Daume III and Daniel Marcu. 2006. Domain adaptation 
IRU VWDWLVWLFDO FODVVL?HUV Journal of Artificial Intelligence 
Research, Vol 26, pp. 101?126 
Hal Daume III. 2007. Frustratingly easy domain adaptation. In 
proceedings of the 45th Meeting of ACL, pp.  256?263 
International Phonetic Association (IPA). 1999. Handbook of 
the International Phonetic Association: A guide to the use of 
the International Phonetic Alphabet. Cambridge: Cam-
bridge University Press. ISBN 0-521-65236-7 (hb); ISBN 
0-521-63751-1  
Joshi, Adithya and Bhattacharyya, Pushpak. 2012. Cost and 
Benefit of Using WordNet Senses for Sentiment Analysis.  
LREC, Istanbul, Turkey 
Kachru, Braj. 1978. Conjunct verbs; verbs or verb phrases?. 
In proceedings of the XIIth International Congress of Lin-
guistics. pp. 366-70 
Lafferty, John, Andrew McCallum, Pereira. F. 2001. Condi-
tional random fields: Probabilistic models for segmenting 
and labeling sequence data. In proceedings of the 18th In-
ternational Conference on Machine Learning, Morgan 
Kaufmann, San Francisco, CA . pp. 282?289 
Muaz, Ahmed, Aasim Ali, and Sarmad Hussain. 2009. Analy-
sis and Development of Urdu POS Tagged Corpus. In pro-
ceedings of the 7th Workshop on ACL-IJCNLP, Suntec, 
Singapore, pp. 24?31, 6-7 August. 
Mukund, Smruthi, Rohini K. Srihari. 2010. A Vector Space 
Model for Subjectivity Classification in Urdu aided by Co-
Training, In proceedings of the 23rd COLING, Beijing, 
China 
Mukund, Smruthi, Debanjan Ghosh, Rohini K. Srihari, 2011. 
Using Sequence Kernels to Identify Opinion Entities in Ur-
du.  In Proceedings of CONLL 
Nadkarni, Mangesh. 1975. Bilingualism and Syntactic Change 
in Konkani Language, vol. 51, pp. 672 C 683. 
Pandaripande, R. 1981. Syntax and Semantics of the Passive 
Construction in selected South Asian Languages. PhD disser-
tation. University of Illinois, Illinois 
Prettenhofer, Peter and Benno Stein. 2010. Cross-Lingual 
Adaptation Using Structural Correspondence Learning. In 
proceedings of ACL  
Rasul, Sarwat. 2006. Language Hybridization and Code Mix-
ing in Pakistani Talk Shows. Bahaudin Zakriya University 
Journal 2nd Issue. pp. 29-41 
Roark, Brian and Michiel Bacchiani. 2003. Supervised and 
unsupervised PCFG adaptation to novel domains. 
In Proceedings of the 2003 Conference of NAACL, HLT - 
Volume 1 (NAACL '03) 
Rie-K. Ando and Tong Zhang. 2005. A framework for learning 
predictive structures from multiple tasks and unlabeled data. 
In Jornal of Machine Learning. Res., Vol 6, pp. 1817?1853 
Santorini, Beatrice. 1990. Part-of-speech tagging guidelines 
for the Penn Treebank Project. University of Pennsylvania, 
3rd Revision, 2nd Printing. 
Shimizu, Nobuyuki and Nakagawa, Hiroshi. 2007. Structural 
Correspondence Learning for Dependency Parsing. In pro-
ceedings of CoNLL Shared Task Session of EMNLP-
CoNLL.  
Sinha, R.M.K. and Anil Thakur. 2005. Machine Translation of 
Bi-lingual Hindi-English (Hinglish) Text. 10th Machine 
Translation summit (MT Summit X) 
Zentella, Ana Celia. 1997. A bilingual manual on how to raise 
Spanish Children. 
8
