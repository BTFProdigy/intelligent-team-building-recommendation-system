Proceedings of the ACL-HLT 2011 System Demonstrations, pages 74?79,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Wikulu: An Extensible Architecture for Integrating Natural Language
Processing Techniques with Wikis
Daniel Ba?r, Nicolai Erbs, Torsten Zesch, and Iryna Gurevych
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
www.ukp.tu-darmstadt.de
Abstract
We present Wikulu1, a system focusing on
supporting wiki users with their everyday
tasks by means of an intelligent interface.
Wikulu is implemented as an extensible archi-
tecture which transparently integrates natural
language processing (NLP) techniques with
wikis. It is designed to be deployed with any
wiki platform, and the current prototype inte-
grates a wide range of NLP algorithms such
as keyphrase extraction, link discovery, text
segmentation, summarization, or text similar-
ity. Additionally, we show how Wikulu can
be applied for visually analyzing the results
of NLP algorithms, educational purposes, and
enabling semantic wikis.
1 Introduction
Wikis are web-based, collaborative content author-
ing systems (Leuf and Cunningham, 2001). As they
offer fast and simple means for adding and editing
content, they are used for various purposes such as
creating encyclopedias (e.g. Wikipedia2), construct-
ing dictionaries (e.g. Wiktionary3), or hosting online
communities (e.g. ACLWiki4). However, as wikis do
not enforce their users to structure pages or add com-
plementary metadata, wikis often end up as a mass
of unmanageable pages with meaningless page titles
and no usable link structure (Buffa, 2006).
To solve this issue, we present the Wikulu sys-
tem which uses natural language processing to sup-
port wiki users with their typical tasks of adding,
1Portmanteau of the Hawaiian terms wiki (?fast?) and kukulu
(?to organize?)
2http://www.wikipedia.org
3http://www.wiktionary.org
4http://aclweb.org/aclwiki
organizing, and finding content. For example,
Wikulu supports users with reading longer texts by
highlighting keyphrases using keyphrase extraction
methods such as TextRank (Mihalcea and Tarau,
2004). Support integrated in Wikulu also includes
text segmentation to segment long pages, text simi-
larity for detecting potential duplicates, or text sum-
marization to facilitate reading of lengthy pages.
Generally, Wikulu allows to integrate any NLP com-
ponent which conforms to the standards of Apache
UIMA (Ferrucci and Lally, 2004).
Wikulu is designed to integrate seamlessly with
any wiki. Our system is implemented as an HTTP
proxy server which intercepts the communication
between the web browser and the underlying wiki
engine. No further modifications to the original wiki
installation are necessary. Currently, our system pro-
totype contains adaptors for two widely used wiki
engines: MediaWiki5 and TWiki6. Adaptors for other
wiki engines can be added with minimal effort. Gen-
erally, Wikulu could also be applied to any web-
based system other than wikis with only slight mod-
ifications to its architecture.
In Figure 1, we show the integration of Wikulu
with Wikipedia.7 The additional user interface com-
ponents are integrated into the default toolbar (high-
lighted by a red box in the screenshot). In this ex-
ample, the user has requested keyphrase highlight-
ing in order to quickly get an idea about the main
content of the wiki article. Wikulu then invokes the
5http://mediawiki.org (e.g. used by Wikipedia)
6http://twiki.org (often used for corporate wikis)
7As screenshots only provide a limited overview of
Wikulu?s capabilities, we refer the reader to a screencast:
http://www.ukp.tu-darmstadt.de/research/
projects/wikulu
74
Figure 1: Integration of Wikulu with Wikipedia. The aug-
mented toolbar (red box) and the results of a keyphrase
extraction algorithm (yellow text spans) are highlighted.
corresponding NLP component, and highlights the
returned keyphrases in the article. In the next sec-
tion, we give a more detailed overview of the differ-
ent types of support provided by Wikulu.
2 Supporting Wiki Users by Means of NLP
In this section, we present the different types of
NLP-enabled support provided by Wikulu.
Detecting Duplicates Whenever users add new
content to a wiki there is the danger of duplicating
already contained information. In order to avoid du-
plication, users would need comprehensive knowl-
edge of what content is already present in the wiki,
which is almost impossible for large wikis like
Wikipedia. Wikulu helps to detect potential du-
plicates by computing the text similarity between
newly added content and each existing wiki page.
If a potential duplicate is detected, the user is noti-
fied and may decide to augment the duplicate page
instead of adding a new one. Wikulu integrates text
similarity measures such as Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2007) and Latent
Semantic Analysis (Landauer et al, 1998).
Suggesting Links While many wiki users read-
ily add textual contents to wikis, they often re-
strain from also adding links to related pages. How-
ever, links in wikis are crucial as they allow users
to quickly navigate from one page to another, or
browse through the wiki. Therefore, it may be rea-
sonable to augment a page about the topic sentiment
Figure 2: Automatic discovery of links to other wiki ar-
ticles. Suitable text phrases to place a link on are high-
lighted in green.
analysis by a link to a page providing related in-
formation such as evaluation datasets. Wikulu sup-
ports users in this tedious task by automatically sug-
gesting links. Link suggestion thereby is a two-step
process: (a) first, suitable text phrases are extracted
which might be worth to place a link on (see Fig-
ure 2), and (b) for each phrase, related pages are
ranked by comparing their relevance to the current
page, and then presented to the user. The user may
thus decide whether she wants to use a detected
phrase as a link or not, and if so, which other wiki
page to link this phrase to. Wikulu currently inte-
grates link suggestion algorithms by Geva (2007)
and Itakura and Clarke (2007).
Semantic Searching The capabilities of a wiki?s
built-in search engine are typically rather limited
as it traditionally performs e.g. keyword-based re-
trieval. If that keyword is not found in the wiki, the
query returns an empty result set. However, a page
might exist which is semantically related to the key-
word, and should thus yield a match.
As the search engine is typically a core part of the
wiki system, it is rather difficult to modify its be-
havior. However, by leveraging Wikulu?s architec-
ture, we can replace the default search mechanisms
by algorithms which allow for semantic search to al-
leviate the vocabulary mismatch problem (Gurevych
et al, 2007).
Segmenting Long Pages Due to the open edit-
ing policy of wikis, pages tend to grow rather fast.
75
Figure 3: Analysis of a wiki article with respect to topical
coherence. Suggested segment breaks are highlighted by
yellow bars.
For users, it is thus a major challenge to keep an
overview of what content is present on a certain
page. Wikulu therefore supports users by analyzing
long pages through employing text segmentation al-
gorithms which detect topically coherent segments
of text. It then suggests segment boundaries which
the user may or may not accept for inserting a sub-
heading which makes pages easier to read and better
to navigate. As shown in Figure 3, users are also en-
couraged to set a title for each segment.8 When ac-
cepting one or more of these suggested boundaries,
Wikulu stores them persistently in the wiki. Wikulu
currently integrates text segmentation methods such
as TextTiling (Hearst, 1997) or C99 (Choi, 2000).
Summarizing Pages Similarly to segmenting
pages, Wikulu makes long wiki pages more acces-
sible by generating an extractive summary. While
generative summaries generate a summary in own
words, extractive summaries analyze the original
wiki text sentence-by-sentence, rank each sentence,
and return a list of the most important ones (see Fig-
ure 4). Wikulu integrates extractive text summariza-
tion methods such as LexRank (Erkan and Radev,
2004).
Highlighting Keyphrases Another approach to
assist users in better grasping the idea of a wiki page
at a glance is to highlight important keyphrases (see
Figure 1). As Tucker and Whittaker (2009) have
8In future work, we plan to suggest suitable titles for each
segment automatically.
Figure 4: Extractive summary of the original wiki page
shown in Figure 3
shown, highlighting important phrases assists users
with reading longer texts and yields faster under-
standing. Wikulu thus improves readability by em-
ploying automatic keyphrase extraction algorithms.
Additionally, Wikulu allows to dynamically adjust
the number of keyphrases shown by presenting a
slider to the user. We integrated keyphrase extrac-
tion methods such as TextRank (Mihalcea and Tarau,
2004) and KEA (Witten et al, 1999).
3 Further Use Cases
Further use cases for supporting wiki users include
(i) visually analyzing the results of NLP algorithms,
(ii) educational purposes, and (iii) enabling semantic
wikis.
Visually Analyzing the Results of NLP Algo-
rithms Wikulu facilitates analyzing the results of
NLP algorithms by using wiki pages as input doc-
uments and visualizing the results directly on that
page. Consider an NLP algorithm which performs
sentiment analysis. Typically, we were to put our
analysis sentences in a text file, launch the NLP ap-
plication, process the file, and would read the output
from either a built-in console or a separate output
file. This procedure suffers from two major draw-
backs: (a) it is inconvenient to copy existing data
into a custom input format which can be fed into the
NLP system, and (b) the textual output does not al-
low presenting the results in a visually rich manner.
Wikulu tackles both challenges by using wiki
pages as input/output documents. For instance,
76
by running the sentiment analysis component right
from within the wiki, its output can be written back
to the originating wiki page, resulting in visually
rich, possibly interactive presentations.
Educational Purposes Wikulu is a handy tool for
educational purposes as it allows to (a) rapidly create
test data in a collaborative manner (see Section 2),
and (b) visualize the results of NLP algorithms, as
described above. Students can gather hands-on ex-
perience by experimenting with NLP components in
an easy-to-use wiki system. They can both collab-
oratively edit input documents, and explore possi-
ble results of e.g. different configurations of NLP
components. In our system prototype, we integrated
highlighting parts-of-speech which have been deter-
mined by a POS tagger.
Enabling Semantic Wikis Semantic wikis such
as the Semantic MediaWiki (Kro?tzsch et al, 2006)
augment standard wikis with machine-readable se-
mantic annotations of pages and links. As those
annotations have to be entered manually, this step
is often skipped by users which severely limits the
usefulness of semantic wikis. Wikulu could support
users e.g. by automatically suggesting the type of a
link by means of relation detection or the type of a
page by means of text categorization. Thus, Wikulu
could constitute an important step towards the se-
mantification of the content contained in wikis.
4 System Architecture
In this section, we detail our system architecture and
describe what is necessary to make NLP algorithms
available through our system. We also give a walk-
through of Wikulu?s information flow.
4.1 Core Components
Wikulu builds upon a modular architecture, as de-
picted in Figure 5. It acts as an HTTP proxy server
which intercepts the communication between the
web browser and the target wiki engine, while it al-
lows to run any Apache UIMA-compliant NLP com-
ponent using an extensible plugin mechanism.
In the remainder of this section, we introduce each
module: (a) the proxy server which allows to add
Wikulu to any target wiki engine, (b) the JavaScript
injection that bridges the gap between the client- and
server-side code, (c) the plugin manager which gives
access to any Apache UIMA-based NLP component,
and (d) the wiki abstraction layer which offers a
high-level interface to typical wiki operations such
as reading and writing the wiki content.
Proxy Server Wikulu is designed to work with
any underlying wiki engine such as MediaWiki or
TWiki. Consequently, we implemented it as an
HTTP proxy server which allows it to be enabled at
any time by changing the proxy settings of a user?s
web browser.9 The proxy server intercepts all re-
quests between the user who interacts with her web
browser, and the underlying wiki engine. For ex-
ample, Wikulu passes certain requests to its lan-
guage processing components, or augments the de-
fault wiki toolbar by additional commands. We elab-
orate on the latter in the following paragraph.
JavaScript Injection Wikulu modifies the re-
quests between web browser and target wiki by in-
jecting custom client-side JavaScript code. Wikulu
is thus capable of altering the default behavior of
the wiki engine, e.g. replacing a keyword-based re-
trieval by enhanced search methods (cf. Section 2),
adding novel behavior such as additional toolbar
buttons or advanced input fields, or augmenting the
originating web page after a certain request has been
processed, e.g. an NLP algorithm has been run.
Plugin Manager Wikulu does not perform lan-
guage processing itself. It relies on Apache UIMA-
compliant NLP components which use wiki pages
(or parts thereof) as input texts. Wikulu offers a so-
phisticated plugin manager which takes care of dy-
namically loading those NLP components. The plu-
gin loader is designed to run plugins either every
time a wiki page loads, or manually by picking them
from the augmented wiki toolbar.
The NLP components are available as server-side
Java classes. Via direct web remoting10, those com-
ponents are made accessible through a JavaScript
proxy object. Wikulu offers a generic language pro-
cessing plugin which takes the current page contents
9The process of enabling a custom proxy server can be
simplified by using web browser extensions such as Mul-
tiproxy Switch (https://addons.mozilla.org/de/
firefox/addon/multiproxy-switch).
10http://directwebremoting.org
77
Browser
Duplicate Detection
JavaScript
Injection
P
l
u
g
i
n
M
a
n
a
g
e
r
Wiki Abstraction
Layer
Wiki
Semantic Search
Link Suggestion
Text Segmentation
Text Summarization
Keyphrase Highlighting
...
W
i
k
u
l
u
 
P
r
o
x
y
Apache UIMA-compliant
NLP components
User
Figure 5: Wikulu acts as a proxy server which intercepts
the communication between the web browser and the un-
derlying wiki engine. Its plugin manager allows to inte-
grate any Apache UIMA-compliant NLP component.
as input text, runs an NLP component, and writes its
output back to the wiki. To run a custom Apache
UIMA-compliant NLP component with Wikulu, one
just needs to plug that particular NLP component
into the generic plugin. No further adaptations to
the generic plugin are necessary. However, more ad-
vanced users may create fully customized plugins.
Wiki Abstraction Layer Wikulu communicates
with the underlying wiki engine via an abstraction
layer. That layer provides a generic interface for
accessing and manipulating the underlying wiki en-
gine. Thereby, Wikulu can both be tightly coupled to
a certain wiki instance such as MediaWiki or TWiki,
while being flexible at the same time to adapt to a
changing environment. New adaptors for other tar-
get wiki engines such as Confluence11 can be added
with minimal effort.
4.2 Walk-Through Example
Let?s assume that a user encounters a wiki page
which is rather lengthy. She realizes that Wikulu?s
keyphrase extraction component might help her to
better grasp the idea of this page at a glance, so
she activates Wikulu by setting her web browser to
pass all requests through the proxy server. After
11http://www.atlassian.com/software/
confluence
JS
Injection
Proxy
Server
Keyphr.
Plugin
Wiki 
Abstr. Lay.
Wiki
get content from wiki page
get
page
extract
keyphrases
Browser
highlight
keyphrases
inject
keyphrases
Figure 6: Illustration of Wikulu?s information flow when
a user has requested to highlight keyphrases on the cur-
rent page as described in Section 4.2
applying the settings, the JavaScript injection mod-
ule adds additional links to the wiki?s toolbar on
the originating wiki page. Having decided to ap-
ply keyphrase extraction, she then invokes that NLP
component by clicking the corresponding link (see
Figure 6). Before the request is passed to that com-
ponent, Wikulu extracts the wiki page contents us-
ing the high-level wiki abstraction layer. Thereafter,
the request is passed via direct web remoting to the
NLP component which has been loaded by Wikulu?s
plugin mechanism. After processing the request, the
extracted keyphrases are returned to Wikulu?s cus-
tom JavaScript handlers and finally highlighted in
the originating wiki page.
5 Related Work
Supporting wiki users with NLP techniques has not
attracted a lot of research attention yet. A no-
table exception is the work by Witte and Gitzinger
(2007). They propose an architecture to connect
wikis to services providing NLP functionality which
are based on the General Architecture for Text En-
gineering (Cunningham et al, 2002). Contrary to
Wikulu, though, their system does not integrate
transparently with an underlying wiki engine, but
rather uses a separate application to apply NLP tech-
niques. Thereby, wiki users can leverage the power
of NLP algorithms, but need to interrupt their cur-
rent workflow to switch to a different application.
78
Moreover, their system is only loosely coupled with
the underlying wiki engine. While it allows to read
and write existing pages, it does not allow further
modifications such as adding user interface controls.
A lot of work in the wiki community is done in the
context of Wikipedia. For example, the FastestFox12
plug-in for Wikipedia is able to suggest links to re-
lated articles. However, unlike Wikulu, FastestFox
is tailored towards Wikipedia and cannot be used
with any other wiki platform.
6 Summary
We presented Wikulu, an extensible system which
integrates natural language processing techniques
with wikis. Wikulu addresses the major challenge of
supporting wiki users with their everyday tasks. Be-
sides that, we demonstrated how Wikulu serves as
a flexible environment for (a) visually analyzing the
results of NLP algorithms, (b) educational purposes,
and (c) enabling semantic wikis. By its modular and
flexible architecture, we envision that Wikulu can
support wiki users both in small focused environ-
ments as well as in large-scale communities such as
Wikipedia.
Acknowledgments
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Klaus Tschira Foun-
dation under project No. 00.133.2008. We would like to
thank Johannes Hoffart for designing and implementing
the foundations of this work, as well as Artem Vovk and
Carolin Deeg for their contributions.
References
Michel Buffa. 2006. Intranet Wikis. In Proceedings
of the IntraWebs Workshop at the 15th International
Conference on World Wide Web.
Freddy Y. Y. Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proceedings of the
1st Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26?33.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In
Proc. of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 168?175.
12http://smarterfox.com
Gu?nes? Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence Re-
search, 22:457?479.
David Ferrucci and Adam Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information Pro-
cessing in the Corporate Research Environment. Nat-
ural Language Engineering, pages 1?26.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Shlomo Geva. 2007. GPX: Ad-Hoc Queries and Auto-
mated Link Discovery in the Wikipedia. In Prepro-
ceedings of the INEX Workshop, pages 404?416.
Iryna Gurevych, Christof Mu?ller, and Torsten Zesch.
2007. What to be??Electronic Career Guidance Based
on Semantic Relatedness. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 1032?1039.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Kelly Y. Itakura and Charles L. A. Clarke. 2007. Univer-
sity of Waterloo at INEX2007: Adhoc and Link-the-
Wiki Tracks. In INEX 2007 Workshop Preproceed-
ings, pages 417?425.
Markus Kro?tzsch, Denny Vrandec?ic?, and Max Vo?lkel.
2006. Semantic MediaWiki. In Proc. of the 5th Inter-
national Semantic Web Conference, pages 935?942.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to Latent Semantic Analysis.
Discourse Processes, 25(2):259?284.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way:
Collaboration and Sharing on the Internet. Addison-
Wesley Professional.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 404?411.
Simon Tucker and Steve Whittaker. 2009. Have A Say
Over What You See: Evaluating Interactive Compres-
sion Techniques. In Proceedings of the Intl. Confer-
ence on Intelligent User Interfaces, pages 37?46.
Rene? Witte and Thomas Gitzinger. 2007. Connecting
wikis and natural language processing systems. In
Proc. of the Intl. Symposium on Wikis, pages 165?176.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In Proceed-
ings of the 4th ACM Conference on Digital Libraries,
pages 254?255.
79
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 37?42,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DKPro WSD ? A Generalized UIMA-based Framework
for Word Sense Disambiguation
Tristan Miller1 Nicolai Erbs1 Hans-Peter Zorn1 Torsten Zesch1,2 Iryna Gurevych1,2
(1) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de/
Abstract
Implementations of word sense disam-
biguation (WSD) algorithms tend to be
tied to a particular test corpus format and
sense inventory. This makes it difficult to
test their performance on new data sets, or
to compare them against past algorithms
implemented for different data sets. In this
paper we present DKPro WSD, a freely
licensed, general-purpose framework for
WSD which is both modular and exten-
sible. DKPro WSD abstracts the WSD
process in such a way that test corpora,
sense inventories, and algorithms can be
freely swapped. Its UIMA-based architec-
ture makes it easy to add support for new
resources and algorithms. Related tasks
such as word sense induction and entity
linking are also supported.
1 Introduction
Word sense disambiguation, or WSD (Agirre and
Edmonds, 2006)?the task of determining which
of a word?s senses is the one intended in a par-
ticular context?has been a core research problem
in computational linguistics since the very incep-
tion of the field. Despite the task?s importance
and popularity as a subject of study, tools and re-
sources supporting WSD have seen relatively little
generalization and standardization. That is, most
prior implementations of WSD systems have been
hard-coded for particular algorithms, sense inven-
tories, and data sets. This makes it difficult to com-
pare systems or to adapt them to new scenarios
without extensive reimplementation.
In this paper we present DKPro WSD, a
general-purpose framework for word sense disam-
biguation which is both modular and extensible.
Its modularity means that it makes a logical sep-
aration between the data sets (e.g., the corpora
to be annotated, the answer keys, manually anno-
tated training examples, etc.), the sense invento-
ries (i.e., the lexical-semantic resources enumerat-
ing the senses to which words in the corpora are
assigned), and the algorithms (i.e., code which ac-
tually performs the sense assignments and prereq-
uisite linguistic annotations), and provides a stan-
dard interface for each of these component types.
Components which provide the same functional-
ity can be freely swapped, so that one can easily
run the same algorithm on different data sets (irre-
spective of which sense inventory they use), or test
several different algorithms on the same data set.
While DKPro WSD ships with support for a
number of common WSD algorithms, sense inven-
tories, and data set formats, its extensibility means
that it is easy to adapt to work with new meth-
ods and resources. The system is written in Java
and is based on UIMA (Lally et al, 2009), an
industry-standard architecture for analysis of un-
structured information. Support for new corpus
formats, sense inventories, and WSD algorithms
can be added by implementing new UIMA com-
ponents for them, or more conveniently by writing
UIMA wrappers around existing code. The frame-
work and all existing components are released un-
der the Apache License 2.0, a permissive free soft-
ware licence.
DKPro WSD was designed primarily to support
the needs of WSD researchers, who will appre-
ciate the convenience and flexibility it affords in
tuning and comparing algorithms and data sets.
However, as a general-purpose toolkit it could also
be used to implement a WSD module for a real-
world natural language processing application. Its
support for interactive visualization of the disam-
biguation process also makes it a powerful tool for
learning or teaching the principles of WSD.
The remainder of this paper is organized as fol-
lows: In ?2 we review previous work in WSD file
formats and implementations. In ?3 we describe
37
our system and further explain its capabilities and
advantages. Finally, in ?4 we discuss our plans for
further development of the framework.
2 Background
In the early days of WSD research, electronic
dictionaries and sense-annotated corpora tended
to be small and hand-crafted on an ad-hoc ba-
sis. It was not until the growing availability of
large-scale lexical resources and corpora in the
1990s that the need to establish a common plat-
form for the evaluation of WSD systems was rec-
ognized. This led to the founding of the Sens-
eval (and later SemEval) series of competitions,
the first of which was held in 1998. Each com-
petition defined a number of tasks with prescribed
evaluation metrics, sense inventories, corpus file
formats, and human-annotated test sets. For each
task it was therefore possible to compare algo-
rithms against each other. However, sense inven-
tories and file formats still vary across tasks and
competitions. There are also a number of increas-
ingly popular resources used outside Senseval and
SemEval, each with their own formats and struc-
tures: examples of sense-annotated corpora in-
clude SemCor (Miller et al, 1994), MASC (Ide et
al., 2010), and WebCAGe (Henrich et al, 2012),
and sense inventories include VerbNet (Kipper et
al., 2008), FrameNet (Ruppenhofer et al, 2010),
DANTE (Kilgarriff, 2010), BabelNet (Navigli and
Ponzetto, 2012), and online community-produced
resources such as Wiktionary and Wikipedia. So
despite attempts at standardization, the canon of
WSD resources remains quite fragmented.
The few publically available implementa-
tions of individual disambiguation algorithms,
such as SenseLearner (Mihalcea and Csomai,
2005), SenseRelate::TargetWord (Patwardhan et
al., 2005), UKB (Agirre and Soroa, 2009), and
IMS (Zhong and Ng, 2010), are all tied to a partic-
ular corpus and/or sense inventory, or define their
own custom formats into which existing resources
must be converted. Furthermore, where the al-
gorithm depends on linguistic annotations such as
part-of-speech tags, the users are expected to sup-
ply these themselves, or else must use the anno-
tators built into the system (which may not always
be appropriate for the corpus language or domain).
One alternative to coding WSD algorithms from
scratch is to use general-purpose NLP toolkits
such as NLTK (Bird, 2006) or DKPro (Gurevych
et al, 2007). Such toolkits provide individual
components potentially useful for WSD, such as
WordNet-based measures of sense similarity and
readers for the odd corpus format. However, these
toolkits are not specifically geared towards devel-
opment and evaluation of WSD systems; there is
no unified type system or architecture which al-
lows WSD-specific components to be combined or
substituted orthogonally.
The only general-purpose dedicated WSD sys-
tem we are aware of is I Can Sense It (Joshi et al,
2012), a Web-based interface for running and eval-
uating various WSD algorithms. It includes I/O
support for several corpus formats and implemen-
tations of a number of baseline and state-of-the-
art disambiguation algorithms. However, as with
previous single-algorithm systems, it is not possi-
ble to select the sense inventory, and the user is
responsible for pre-annotating the input text with
POS tags. The usability and extensibility of the
system are greatly restricted by the fact that it is a
proprietary, closed-source application fully hosted
by the developers.
3 DKPro WSD
Our system, DKPro WSD, is implemented as a
framework of UIMA components (type systems,
collection readers, annotators, CAS consumers,
resources) which the user combines into a data
processing pipeline. We can best illustrate this
with an example: Figure 1 shows a pipeline for
running two disambiguation algorithms on the Es-
tonian all-words task from Senseval-2. UIMA
components are the solid, rounded boxes in the
lower half of the diagram, and the data and algo-
rithms they encapsulate are the light grey shapes
in the upper half. The first component of the
pipeline is a collection reader which reads the
text of the XML-formatted corpus into a CAS (a
UIMA data structure for storing layers of data
and stand-off annotations) and marks the words
to be disambiguated (the ?instances?) with their
IDs. The next component is an annotator which
reads the answer key?a separate file which as-
sociates each instance ID with a sense ID from
the Estonian EuroWordNet?and adds the gold-
standard sense annotations to their respective in-
stances in the CAS. Processing then passes to
another annotator?in this case a UIMA wrap-
per for TreeTagger (Schmid, 1994)?which adds
POS and lemma annotations to the instances.
38
corpus 
reader 
answer key 
annotator 
linguistic 
annotator 
WSD 
annotator 
WSD 
annotator 
simplified 
Lesk 
evaluator 
sense 
inventory 
Senseval-2 
Estonian 
all-words 
test corpus 
Senseval-2 
Estonian 
all-words 
answer key 
results and 
statistics     JMWNL 
Estonian 
Euro- 
WordNet 
degree 
centrality 
Tree- 
Tagger 
Estonian 
language 
model 
Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2.
Then come the two disambiguation algorithms,
also modelled as UIMA annotators wrapping non-
UIMA-aware algorithms. Each WSD annotator it-
erates over the instances in the CAS and annotates
them with sense IDs from EuroWordNet. (Euro-
WordNet itself is accessed via a UIMA resource
which wraps JMWNL (Pazienza et al, 2008) and
which is bound to the two WSD annotators.) Fi-
nally, control passes to a CAS consumer which
compares the WSD algorithms? sense annotations
against the gold-standard annotations produced by
the answer key annotator, and outputs these sense
annotations along with various evaluation metrics
(precision, recall, etc.).
A pipeline of this sort can be written with just
a few lines of code: one or two to declare each
component and if necessary bind it to the appro-
priate resources, and a final one to string the com-
ponents together into a pipeline. Moreover, once
such a pipeline is written it is simple to substitute
functionally equivalent components. For example,
with only a few small changes the same pipeline
could be used for Senseval-3?s English lexical
sample task, which uses a corpus and sense inven-
tory in a different format and language. Specif-
ically, we would substitute the collection reader
with one capable of reading the Senseval lexical
sample format, we would pass an English instead
of Estonian language model to TreeTagger, and
we would substitute the sense inventory resource
exposing the Estonian EuroWordNet with one for
WordNet 1.7.1. Crucially, none of the WSD algo-
rithms need to be changed.
The most important features of our system are
as follows:
Corpora and data sets. DKPro WSD currently
has collection readers for all Senseval and Sem-
Eval all-words and lexical sample tasks, the AIDA
CoNLL-YAGO data set (Hoffart et al, 2011), the
TAC KBP entity linking tasks (McNamee and
Dang, 2009), and the aforementioned MASC,
SemCor, and WebCAGe corpora. Our prepack-
aged corpus analysis modules can compute statis-
tics on monosemous terms, average polysemy,
terms absent from the sense inventory, etc.
Sense inventories. Sense inventories are ab-
stracted into a system of types and interfaces ac-
cording to the sort of lexical-semantic information
they provide. There is currently support for Word-
Net (Fellbaum, 1998), WordNet++ (Ponzetto and
Navigli, 2010), EuroWordNet (Vossen, 1998), the
Turk Bootstrap Word Sense Inventory (Biemann,
2013), and UBY (Gurevych et al, 2012), which
provides access to WordNet, Wikipedia, Wik-
tionary, GermaNet, VerbNet, FrameNet, Omega-
Wiki, and various alignments between them. The
system can automatically convert between vari-
ous versions of WordNet using the UPC mappings
(Daude? et al, 2003).
Algorithms. As with sense inventories, WSD
algorithms have a type and interface hierarchy ac-
cording to what knowledge sources they require.
Algorithms and baselines already implemented in-
clude the analytically calculated random sense
baseline; the most frequent sense baseline; the
original, simplified, extended, and lexically ex-
panded Lesk variants (Miller et al, 2012); various
39
graph connectivity approaches from Navigli and
Lapata (2010); Personalized PageRank (Agirre
and Soroa, 2009); the supervised TWSI system
(Biemann, 2013); and IMS (Zhong and Ng, 2010).
Our open API permits users to program support
for further knowledge-based and supervised algo-
rithms.
Linguistic annotators. Many WSD algorithms
require linguistic annotations from segmenters,
lemmatizers, POS taggers, parsers, etc. Off-the-
shelf UIMA components for producing such an-
notations, such as those provided by DKPro Core
(Gurevych et al, 2007), can be used in a DKPro
WSD pipeline with little or no adaptation.
Visualization tools. We have enhanced some
families of algorithms with animated, interactive
visualizations of the disambiguation process. For
example, Figure 2 shows part of a screenshot from
the interactive running of the degree centrality al-
gorithm (Navigli and Lapata, 2010). The system is
disambiguating the three content words in the sen-
tence ?I drink milk with a straw.? Red, green, and
blue nodes represent senses (or more specifically,
WordNet sense keys) of the words drink, milk,
and straw, respectively; grey nodes are senses of
other words discovered by traversing semantic re-
lations (represented by arcs) in the sense inven-
tory. The current traversal (toast%2:34:00:: to
fuddle%2:34:00::) is drawn in a lighter colour.
Mouseover tooltips provide more detailed infor-
mation on senses. We have found such visualiza-
tions to be invaluable for understanding and de-
bugging algorithms.
Parameter sweeping. The behaviour of many
components (or entire pipelines) can be altered ac-
cording to various parameters. For example, for
the degree centrality algorithm one must specify
the maximum search depth, the minimum vertex
degree, and the context size. DKPro WSD can
perform a parameter sweep, automatically running
the pipeline once for every possible combination
of parameters in user-specified ranges and con-
catenating the results into a table from which the
optimal system configurations can be identified.
Reporting tools. There are several reporting
tools to support evaluation and error analysis. Raw
sense assignments can be output in a variety of for-
mats (XML, HTML, CSV, Senseval answer key,
etc.), some of which support colour-coding to
Figure 2: DKPro WSD?s interactive visualization
of a graph connectivity WSD algorithm.
highlight correct and incorrect assignments. The
system can also compute common evaluation met-
rics (Agirre and Edmonds, 2006, pp. 76?80) and
plot precision?recall curves for each algorithm in
the pipeline, as well as produce confusion matri-
ces for algorithm pairs. Users can specify backoff
algorithms, and have the system compute results
with and without the backoff. Results can also be
broken down by part of speech. Figure 3 shows
an example of an HTML report produced by the
system?on the left is the sense assignment table,
in the upper right is a table of evaluation metrics,
and in the lower right is a precision?recall graph.
DKPro WSD also has support for tasks closely
related to word sense disambiguation:
Entity linking. Entity linking (EL) is the task of
linking a named entity in a text (e.g., Washington)
to its correct representation in some knowledge
base (e.g., either George Washington or Washing-
ton, D.C. depending on the context). EL is very
similar to WSD in that both tasks involve connect-
ing ambiguous words in a text to entries in some
inventory. DKPro WSD supports EL-specific
sense inventories such as the list of Wikipedia
articles used in the Knowledge Base Population
workshop of the Text Analysis Conference (TAC
KBP). This workshop, held annually since 2009,
provides a means for comparing different EL sys-
tems in a controlled setting. DKPro WSD contains
a reader for the TAC KBP data set, components
for mapping other sense inventories to the TAC
KBP inventory, and evaluation components for the
40
Figure 3: An HTML report produced by DKPro WSD.
official metrics. Researchers can therefore miti-
gate the entry barrier for their first participation at
TAC KBP and experienced participants can extend
their systems by making use of further WSD algo-
rithms.
Word sense induction. WSD is usually per-
formed with respect to manually created sense in-
ventories such as WordNet. In word sense induc-
tion (WSI) a sense inventory for target words is
automatically constructed from an unlabelled cor-
pus. This can be useful for search result cluster-
ing, or for general applications of WSD for lan-
guages and domains for which a sense inventory
is not yet available. It is usually necessary to per-
form WSD at some point in the evaluation of WSI.
DKPro WSD supports WSI by providing state-of-
the art WSD algorithms capable of using arbitrary
sense inventories, including induced ones. It also
includes readers and writers for the SemEval-2007
and -2013 WSI data sets.
4 Conclusions and future work
In this paper we introduced DKPro WSD, a Java-
and UIMA-based framework for word sense dis-
ambiguation. Its primary advantages over exist-
ing tools are its modularity, its extensibility, and
its free licensing. By segregating and providing
layers of abstraction for code, data sets, and sense
inventories, DKPro WSD greatly simplifies the
comparison of WSD algorithms in heterogeneous
scenarios. Support for a wide variety of commonly
used algorithms, data sets, and sense inventories
has already been implemented.
The framework is under active development,
with work on several new features planned or in
progress. These include implementations or wrap-
pers for further algorithms and for the DANTE
and BabelNet sense inventories. A Web inter-
face is in the works and should be operational
by the time of publication. Source code, bi-
naries, documentation, tutorials, FAQs, an issue
tracker, and community mailing lists are avail-
able on the project?s website at https://code.
google.com/p/dkpro-wsd/.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg Professor-
ship Program under grant No? I/82806.
41
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proc.
EACL, pages 33?41.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Lang. Resour. and Eval., 47(1):97?122.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proc. ACL-COLING (Interactive Presen-
tation Sessions), pages 69?72.
Jordi Daude?, Llu??s Padro?, and German Rigau. 2003.
Validation and tuning of WordNet mapping tech-
niques. In Proc. RANLP, pages 117?123.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Reposi-
tory Based on UIMA. In Proc. UIMA Workshop at
GLDV.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY ? A large-scale unified
lexical-semantic resource. In Proc. EACL, pages
580?590.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2012. WebCAGe ? A Web-harvested corpus
annotated with GermaNet senses. In Proc. EACL,
pages 387?396.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fu?rstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proc. EMNLP, pages 782?792.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The Manually Annotated
Sub-Corpus: A community resource for and by the
people. In Proc. ACL (Short Papers), pages 68?73.
Salil Joshi, Mitesh M. Khapra, and Pushpak Bhat-
tacharyya. 2012. I Can Sense It: A comprehensive
online system for WSD. In Proc. COLING (Demo
Papers), pages 247?254.
Adam Kilgarriff. 2010. A detailed, accurate, exten-
sive, available English lexical database. In Proc.
NAACL-HLT, pages 21?24.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. Lang. Resour. and Eval., 42(1):21?
40.
Adam Lally, Karin Verspoor, and Eric Nyberg, editors.
2009. Unstructured Information Management Ar-
chitecture (UIMA) Version 1.0. OASIS.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Proc. TAC.
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word sense disambiguation for all words
in unrestricted text. In Proc. ACL (System Demos),
pages 53?56.
George A. Miller, Martin Chodorow, Shari Landes,
Claudio Leacock, and Robert G. Thomas. 1994. Us-
ing a semantic concordance for sense identification.
In Proc. HLT, pages 240?243.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proc. COLING, pages
1781?1796.
Roberto Navigli and Mirella Lapata. 2010. An experi-
mental study of graph connectivity for unsupervised
word sense disambiguation. IEEE Trans. on Pattern
Anal. and Machine Intel., 32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
An overview of BabelNet and its API for multilin-
gual language processing. In Iryna Gurevych and
Jungi Kim, editors, The People?s Web Meets NLP:
Collaboratively Constructed Language Resources.
Springer.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2005. SenseRelate::TargetWord ? A gen-
eralized framework for word sense disambiguation.
In Proc. ACL (System Demos), pages 73?76.
Maria Teresa Pazienza, Armando Stellato, and Alexan-
dra Tudorache. 2008. JMWNL: An extensible mul-
tilingual library for accessing wordnets in different
languages. In Proc. LREC, pages 28?30.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proc. ACL, pages 1522?
1531.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Helmud Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. NeMLaP.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Springer.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage word sense disambiguation system
for free text. In Proc. ACL (System Demos), pages
78?83.
42
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 31?36,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
DKPro Keyphrases: Flexible and Reusable Keyphrase Extraction
Experiments
Nicolai Erbs
?
, Pedro Bispo Santos
?
, Iryna Gurevych
??
and Torsten Zesch
??
? UKP Lab, Technische Universit?at Darmstadt
? Information Center for Education, DIPF, Frankfurt
? Language Technology Lab, University of Duisburg-Essen
http://www.ukp.tu-darmstadt.de
Abstract
DKPro Keyphrases is a keyphrase extrac-
tion framework based on UIMA. It offers
a wide range of state-of-the-art keyphrase
experiments approaches. At the same
time, it is a workbench for developing new
extraction approaches and evaluating their
impact. DKPro Keyphrases is publicly
available under an open-source license.
1
1 Introduction
Keyphrases are single words or phrases that pro-
vide a summary of a text (Tucker and Whittaker,
2009) and thus might improve searching (Song et
al., 2006) in a large collection of texts. As man-
ual extraction of keyphrases is a tedious task, a
wide variety of keyphrase extraction approaches
has been proposed. Only few of them are freely
available which makes it hard for researchers to
replicate previous results or use keyphrase extrac-
tion in some other application, such as informa-
tion retrieval (Manning et al., 2008), or question
answering (Kwok et al., 2001).
In this paper, we describe our keyphrase extrac-
tion framework called DKPro Keyphrases. It inte-
grates a wide range of state-of-the-art approaches
for keyphrase extraction that can be directly used
with limited knowledge of programming. How-
ever, for developers of new keyphrase extrac-
tion approaches, DKPro Keyphrases also offers a
programming framework for developing new ex-
traction algorithms and for evaluation of result-
ing effects. DKPro Keyphrases is based on the
Unstructured Information Management Architec-
ture (Ferrucci and Lally, 2004), which provides a
rich source of libraries with preprocessing compo-
nents.
1
http://code.google.com/p/dkpro-keyphrases/
Text
Preprocessing
Select keyphrases
Filter keyphrases
Rank keyphrases
Evaluate
Results
Figure 1: Architecture overview of DKPro
Keyphrases
2 Architecture
The architecture of DKPro Keyphrases models the
five fundamental steps of keyphrase extraction:
(i) Reading of input data and enriching it with
standard linguistic preprocessing, (ii) selecting
phrases as keyphrase candidates based on the pre-
processed text, (iii) filtering selected keyphrases,
(iv) ranking remaining keyphrases, and (v) evalu-
ating ranked keyphrases against a gold standard.
This process is visualized in Figure 1. In this
section, we will describe details of each step, in-
cluding components already included in DKPro
Keyphrases.
2.1 Preprocessing
DKPro Keyphrases relies on UIMA-based pre-
processing components developed in the natu-
ral language processing framework DKPro Core
(Gurevych et al., 2007; Eckart de Castilho and
Gurevych, 2009). Thus, a wide range of linguis-
tic preprocessing components are readily available
such as word segmentation, lemmatization, part-
of-speech tagging, named entity recognition, syn-
31
tactic parsing, or co-reference resolution.
2.2 Selecting Keyphrases
In this step, DKPro Keyphrases selects all phrases
as keyphrases that match user-specified criteria. A
criterium is typically a linguistic type, e.g. tokens,
or more sophisticated types such as noun phrases.
The resulting list of keyphrases should cover all
gold keyphrases and at the same time be as selec-
tive as possible. We use the following sentence
with the two gold keyphrases ?dog? and ?old cat?
as a step through example:
A [dog] chases an [old cat] in my gar-
den.
Taking all uni- and bi-grams as keyphrases will
easily match both gold keyphrases, but it will also
result in many other less useful keyphrases like ?in
my?.
In the given example, the keyphrase list consists
of nine tokens (lemmas, resp.) but covers only one
gold keyphrase (i.e. ?dog?). Noun chunks and
named entities are alternative keyphrases, limiting
the set of keyphrases further. Experiments where
noun chunks are selected as keyphrases perform
best for this example. Named entities are too re-
strictive, but applicable for identifying relevant en-
tities in a text. This is useful for tasks that are
targeted towards entities, e.g. for finding experts
(D?orner et al., 2007) in a collection of domain-
dependent texts. The selection of a linguistic type
is not limited as preprocessing components might
introduce further types.
2.3 Filtering
Filtering can be used together with over-
generating selection approaches like taking all n-
grams to decrease the number of keyphrases be-
fore ranking. One possible approach is based
on POS patterns. For example, using the POS
patterns, Adjective-Noun, Adjective, and
Noun limits the set of possible keyphrases to
?dog?, ?old cat?, ?cat?, and ?garden? in the pre-
vious example. This step can also been per-
formed as part of the selection step, however,
keeping it separated enables researchers to ap-
ply filters to keyphrases of any linguistic type.
DKPro Keyphrases provides the possibility to use
controlled-vocabulary keyphrase extraction by fil-
tering out all keyphrases which are not included in
a keyphrase list.
Developers of keyphrase extraction approaches
can create their own filter simply by extending
from a base class and adding filter-specific code.
Additionally, DKPro Keyphrases does not impose
workflow-specific requirements, such as a fixed
number of filters. This leaves room for keyphrase
extraction experiments testing new or extended fil-
ters.
2.4 Ranking
In this step, a ranker assigns a score to each re-
maining keyphrase candidate. DKPro Keyphrases
contains rankers based on the candidate position,
frequency, tf-idf, TextRank (Mihalcea and Tarau,
2004), and LexRank (Erkan and Radev, 2004).
DKPro Keyphrases also contains a special ex-
tension of tf-idf, called tf-idf
web
, for which Google
web1t (Brants and Franz, 2006) is used for obtain-
ing approximate df counts. In case of keyphrase
extraction for a single document or for domain-
independent keyphrase extraction, web1t provides
reliable n-gram statistics without any domain-
dependence.
2.5 Evaluation
DKPro Keyphrases ships with all the metrics
that have been traditionally used for evaluating
keyphrase extraction. Kim et al. (2010) use
precision and recall for a different number of
keyphrases (5, 10 and 15 keyphrases). These met-
rics are widely used for evaluation in information
retrieval. Precision @5 is the ratio of true pos-
itives in the set of extracted keyphrases when 5
keyphrases are extracted. Recall @5 is the ratio of
true positives in the set of gold keyphrases when
5 keyphrases are extracted. Moreover, DKPro
Keyphrases evaluates with MAP and R-precision.
MAP is the mean average precision of extracted
keyphrases from the highest scored keyphrase to
the total number of extracted keyphrases. For each
position in the rank, the precision at that position
will be computed. Summing up the precision at
each recall point and then taking its average will
return the average precision for the text being eval-
uated. The mean average precision will be the
mean from the sum of each text?s average preci-
sion from the dataset. R-precision is the ratio of
true positives in the set of extracted keyphrases,
when the set is limited to the same size as the set
of gold keyphrases (Zesch and Gurevych, 2009).
32
3 Experimental framework
In this section, we show how researchers can per-
form experiments covering many different config-
urations for preprocessing, selection, and ranking.
To facilitate the construction of experiments, the
framework contains a module to make its archi-
tecture compatible to the DKPro Lab framework
(Eckart de Castilho and Gurevych, 2011), thus al-
lowing to sweep through the parameter space of
configurations. The parameter space is the combi-
nation of all possible parameters, e.g. one parame-
ter with two possible values for preprocessing and
a second parameter with two values for rankers
lead to four possible combinations. We refer to pa-
rameter sweeping experiments when running the
experiment with all possible combinations.
DKPro Keyphrases divides the experimental
setup in three tasks. Tasks are processing steps
defined in the Lab framework, which ? in case of
keyphrase extraction ? are based on the steps de-
scribed in Section 2. In the first task, the input
text is fed into a pipeline and preprocessed. In the
second task, the keyphrases are selected and fil-
tered. In the third and final task they are ranked
and evaluated. The output of the first two tasks are
serialized objects which can be processed further
by the following task. The output of the third task
is a report containing all configurations and results
in terms of all evaluation metrics.
The division into three tasks speeds up process-
ing of the entire experiment. Each task has mul-
tiple configuration parameters which influence the
forthcoming tasks. Instead of running the prepro-
cessing tasks for every single possible combina-
tion, the intermediate objects are stored once and
then used for every possible configuration in the
keyphrase selection step.
To illustrate the advantages of experimental set-
tings in DKPro Keyphrases, we run the previously
used example sentence through the entire parame-
ter space. Hence, tokens, lemmas, n-grams, noun
chunks, and named entities will be combined with
all filters and all rankers (not yet considering all
possible parameters). This results in more than
10,000 configurations. Although the number of
configurations is high, the computation time is
low
2
as not the entire pipeline needs to run that
often. This scales well for longer texts.
The experimental framework runs all possible
2
Less than five minutes on a desktop computer with a 3.4
GHz 8-core processor.
combinations automatically and collects individ-
ual results in a report, such as a spreadsheet or
text file. This allows for comparing results of dif-
ferent rankers, mitigating the influence of differ-
ent preprocessing and filtering components. This
way, the optimal experimental configuration can
be found empirically. It is a great improvement
for researchers because a variety of system con-
figurations can be compared without the effort of
reimplementing the entire pipeline.
Code example 1 shows the main method of an
example experiment, selecting all tokens as pos-
sible keyphrases and ranking them with their tf-
idf values. Lines 1 to 34 show values for dimen-
sions which span the parameter space. A dimen-
sion consists of an identifier, followed by one or
more values. Lines 36 to 40 show the creation of
tasks, and in lines 42 to 48 the tasks and a re-
port are added to one batch task, which is then
executed. Researchers can run multiple configu-
rations by setting multiple values to a dimension.
Line 25 shows an example of a dimension with
two values (using the logarithm or unchanged text
frequency), in this case two configurations
3
for the
ranker based on tf-idf scores.
Code example 1: Example experiment
1 ParameterSpace params = new
ParameterSpace(
2 Dimension.create("language", "en"),
3 Dimension.create("frequencies",
"web1t"),
4 Dimension.create("tfidfFeaturePath",
Token.class"),
5
6 Dimension.create("dataset",
datasetPath),
7 Dimension.create("goldSuffix", ".key"),
8
9 //Selection
10 Dimension.create("segmenter",
OpenNlpSegmenter.class),
11 Dimension.create("keyphraseFeaturePath",
Token.class),
12
13 //PosSequence filter
14 Dimension.create("runPosSequenceFilter",
true),
15 Dimension.create("posSequence",
standard),
16
17 //Stopword filter
18 Dimension.create("runStopwordFilter",
true),
19 Dimension.create("stopwordlists",
"stopwords.txt"),
20
21 // Ranking
3
DKPro Keyphrases provides ways to configure experi-
ments using Groovy and JSON.
33
22 Dimension.create("rankerClass",
TfidfRanking.class),
23
24 //TfIdf
25 Dimension.create("weightingModeTf",
NORMAL, LOG),
26 Dimension.create("weightingModeIdf",
LOG),
27 Dimension.create("tfidfAggregate",
MAX),
28
29 //Evaluator
30 Dimension.create("evalMatchingType",
MatchingType.Exact),
31 Dimension.create("evalN", 50),
32 Dimension.create("evalLowercase",
true),
33 Dimension.create("evalType",
EvaluatorType.Lemma),
34 );
35
36 Task preprocessingTask = new
PreprocessingTask();
37 Task filteringTask = new
KeyphraseFilteringTask();
38 candidateSelectionTask.addImport(
preprocessingTask,
PreprocessingTask.OUTPUT,
KeyphraseFilteringTask.INPUT);
39 Task keyphraseRankingTask = new
KeyphraseRankingTask();
40 keyphraseRankingTask.addImport(
filteringTask,
KeyphraseFilteringTask.OUTPUT,
KeyphraseRankingTask.INPUT);
41
42 BatchTask batch = new BatchTask();
43 batch.setParameterSpace(params);
44 batch.addTask(preprocessingTask);
45 batch.addTask(candidateSelectionTask);
46 batch.addTask(keyphraseRankingTask);
47 batch.addReport(
KeyphraseExtractionReport.class);
48 Lab.getInstance().run(batch);
A use case for the experimental framework is
the evaluation of new preprocessing components.
For example, keyphrase extraction should be eval-
uated with Twitter data: One collects a dataset
with tweets and their corresponding keyphrases
(possibly, the hash tags). The standard preprocess-
ing will most likely fail as non-canonical language
will be hard to process (e.g. hash tags or emoti-
cons).
The preprocessing components can be set as a
parameter and compared directly without chang-
ing the remaining parameters for filters and
rankers. This allows researchers to perform reli-
able extrinsic evaluation of their components in a
keyphrase extraction setting.
Figure 2: Screenshot of web demo in DKPro
Keyphrases
4 Visualization and wrappers
To foster analysis of keyphrase extraction ex-
periments, we created a web-based visualization
framework with Spring
4
. It allows for running off-
the-shelf experiments and manually inspecting re-
sults without the need to install any additional soft-
ware. Figure 2 shows a visualization of one pre-
configured experiment. The web demo is avail-
able online.
5
Currently, a table overview of ex-
tracted keyphrases is implemented, but develop-
ers can change it to highlighting all keyphrases.
The latter is recommend for a binary classification
of keyphrases. This is the case, if a system only
returns keyphrases with a score above a certain
threshold. The table in Figure 2 shows keyphrases
with the assigned scores, which can be sorted to
get a ranking of keyphrases. However, the visual-
ization framework does not provide any evaluation
capabilities.
To help new users of DKPro Keyphrases, it in-
cludes a module with two demo experiments us-
ing preconfigured parameter sets. This is espe-
cially useful for applying keyphrase extraction in
other tasks, e.g. text summarization (Goldstein et
4
http://projects.spring.io/spring-ws/
5
https://dkpro.ukp.informatik.tu-
darmstadt.de/DKProWebDemo/livedemo/3
34
al., 2000). Both demo experiments are frequently
used keyphrase extraction systems. The first one
is based on TextRank (Mihalcea and Tarau, 2004)
and the second one is based on the supervised sys-
tem KEA (Witten et al., 1999). Both configura-
tions do not require any additional installation of
software packages.
This module offers setters to configure param-
eters, e.g. the size of co-occurrence windows in
case of the TextRank extractor.
5 Related work
Most work on keyphrase extraction is not accom-
panied with free and open software. The tools
listed in this section allow users to combine differ-
ent configurations with respect to preprocessing,
keyphrase selection, filtering, and ranking. In the
following, we give an overview of software tools
for keyphrase extraction.
KEA (Witten et al., 1999) provides a Java API,
which offers automatic keyphrase extraction from
texts. They provide a supervised approach for
keyphrase extraction. For each keyphrase, KEA
computes frequency, position, and semantic relat-
edness as features. Thus, for using KEA, the user
needs to provide annotated training data. KEA
generates keyphrases from n-grams with length
from 1 to 3 tokens. A controlled vocabulary can
be used to filter keyphrases. The configuration for
keyphrase selection and filtering is limited com-
pared to DKPro Keyphrases, which offers capa-
bilities for changing the entire preprocessing or
adding filters.
Maui (Medelyan et al., 2009) enhances KEA
by allowing the computation of semantic related-
ness of keyphrases. It uses Wikipedia as a the-
saurus and computes the keyphraseness of each
keyphrase, which is the number of times a can-
didate was used as keyphrase in the training data
(Medelyan et al., 2009).
Although Maui provides training data along
with their software, this training data is highly
domain-specific. A shortcoming of KEA and
Maui is the lack of any evaluation capabilities or
the possibility to run parameter sweeping exper-
iments. DKPro Keyphrases provides evaluation
tools for automatic testing of many parameter set-
tings.
Besides KEA and Mau, which are Java sys-
tems, there are several modules in Python,
e.g. topia.termextract
6
, which offer capabili-
ties for tokenization, part-of-speech tagging and
keyphrase extraction. Keyphrase extraction from
topia.termextract is based on noun phrases and
ranks them according to their frequencies.
BibClassify
7
is a python module which auto-
matically extracts keywords from a text based on
the occurrence of terms in a thesaurus. The ranker
is frequency-based like topia.termextract. Bib-
Classify and topia.termextract do not provide eval-
uation capabilities or parameter sweeping experi-
ments.
Besides these software tools, there exist web
services for keyphrase extraction. AlchemyAPI
8
offers a web service for keyword extraction. It
may return keyphrases encoded in various markup
languages. TerMine
9
offers a SOAP service for
extracting keyphrases from documents and a web
demo. The input must be a String and the extracted
terms will be returned as a String. Although web
services can be integrated easily due to their proto-
col stacks, they are not extensible and replicability
cannot be guaranteed over time.
6 Conclusions and future work
We presented DKPro Keyphrases, a framework for
flexible and reusable keyphrase extraction experi-
ments. This helps researchers to effectively de-
velop new keyphrase extraction components with-
out the need to re-implement state-of-the-art ap-
proaches.
The UIMA-based architecture of DKPro
Keyphrases allows users to easily evaluate
keyphrase extraction configurations. Researchers
can integrate keyphrase extraction with different
existing linguistic preprocessing components of-
fered by the open-source community and evaluate
them in terms of all commonly used evaluation
metrics.
As future work, we plan to wrap further
third-party libraries with keyphrase extraction ap-
proaches in DKPro Keyphrases and to add a super-
vised system using the unsupervised components
as features. We expect that a supervised system us-
ing a large variety of features would improve the
state of the art in keyphrase extraction.
6
https://pypi.python.org/pypi/topia.termextract/
7
http://invenio-demo.cern.ch/help/admin/bibclassify-
admin-guide
8
http://www.alchemyapi.com/api/keyword-extraction/
9
http://www.nactem.ac.uk/software/termine/
35
Acknowledgments
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
by the Klaus Tschira Foundation under project No.
00.133.2008, and by the German Federal Min-
istry of Education and Research (BMBF) within
the context of the Software Campus project open
window under grant No. 01IS12054. The authors
assume responsibility for the content. We thank
Richard Eckart de Castilho and all contributors for
their valuable collaboration and the we thank the
anonymous reviewers for their helpful comments.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
Gram Corpus Version 1.1. Technical report, Google
Research.
Christian D?orner, Volkmar Pipek, and Markus Won.
2007. Supporting Expertise Awareness: Finding
Out What Others Know. In Proceedings of the 2007
Symposium on Computer Human Interaction for the
Management of Information Technology.
Richard Eckart de Castilho and Iryna Gurevych. 2009.
DKPro-UGD: A Flexible Data-Cleansing Approach
to Processing User-Generated Discourse. In Online-
proceedings of the First French-speaking meeting
around the framework Apache UIMA.
Richard Eckart de Castilho and Iryna Gurevych. 2011.
A Lightweight Framework for Reproducible Param-
eter Sweeping in Information Retrieval. In Pro-
ceedings of the 2011 Workshop on Data Infrastruc-
tures for Supporting Information Retrieval Evalua-
tion, pages 7?10.
G?unes Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence
Research, 22:457?479.
David Ferrucci and Adam Lally. 2004. UIMA: An
Architectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-Document Summa-
rization By Sentence Extraction. In Proceedings of
the NAACL-ANLP 2000 Workshop: Automatic Sum-
marization, pages 40?48.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Repository
Based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the Society for
Computational Linguistics and Language Technol-
ogy.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 21?26.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling Question Answering to the Web. ACM
Transactions on Information Systems, 19(3):242?
262.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. An Introduction to Infor-
mation Retrieval. Cambridge University Press Cam-
bridge.
Olena Medelyan, Eibe Frank, and Ian H Witten.
2009. Human-competitive Tagging using Automatic
Keyphrase Extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 404?411.
Min Song, Il Yeol Song, Robert B. Allen, and Zo-
ran Obradovic. 2006. Keyphrase Extraction-based
Query Expansion in Digital Libraries. In Proceed-
ings of the 6th ACM/IEEE-CS Joint Conference on
Digital Libraries, pages 202?209.
Simon Tucker and Steve Whittaker. 2009. Have A Say
Over What You See: Evaluating Interactive Com-
pression Techniques. In Proceedings of the 2009
International Conference on Intelligent User Inter-
faces, pages 37?46.
Ian H. Witten, Gordon W. Paynter, Eibe Frank,
Carl Andrew Gutwin, and Craig G . Nevill-
Manning. 1999. KEA: Practical Automatic
Keyphrase Extraction. In Proceedings of the 4th
ACM Conference on Digital Libraries, pages 254?
255.
Torsten Zesch and Iryna Gurevych. 2009. Approx-
imate Matching for Evaluating Keyphrase Extrac-
tion. In Proceedings of the 7th International Confer-
ence on Recent Advances in Natural Language Pro-
cessing, pages 484?489.
36
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 30?39,
Dublin, Ireland, August 23-24 2014.
Sense and Similarity: A Study of Sense-level Similarity Measures
Nicolai Erbs
?
, Iryna Gurevych
??
and Torsten Zesch
?
? UKP Lab, Technische Universit?at Darmstadt
? Information Center for Education, DIPF, Frankfurt
? Language Technology Lab, University of Duisburg-Essen
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we investigate the differ-
ence between word and sense similarity
measures and present means to convert
a state-of-the-art word similarity measure
into a sense similarity measure. In or-
der to evaluate the new measure, we cre-
ate a special sense similarity dataset and
re-rate an existing word similarity dataset
using two different sense inventories from
WordNet and Wikipedia. We discover
that word-level measures were not able
to differentiate between different senses
of one word, while sense-level measures
actually increase correlation when shift-
ing to sense similarities. Sense-level sim-
ilarity measures improve when evaluated
with a re-rated sense-aware gold standard,
while correlation with word-level similar-
ity measures decreases.
1 Introduction
Measuring similarity between words is a very im-
portant task within NLP with applications in tasks
such as word sense disambiguation, information
retrieval, and question answering. However, most
of the existing approaches compute similarity on
the word-level instead of the sense-level. Conse-
quently, most evaluation datasets have so far been
annotated on the word level, which is problem-
atic as annotators might not know some infrequent
senses and are influenced by the more probable
senses. In this paper, we provide evidence that this
process heavily influences the annotation process.
For example, when people are presented the word
pair jaguar - gamepad only few people know that
Jaguar
Gamepad
Zoo
.0070.0016
.0000
Figure 1: Similarity between words.
jaguar is also the name of an Atari game console.
1
People rather know the more common senses of
jaguar, i.e. the car brand or the animal. Thus, the
word pair receives a low similarity score, while
computational measures are not so easily fooled
by popular senses. It is thus likely that existing
evaluation datasets give a wrong picture of the true
performance of similarity measures.
Thus, in this paper we investigate whether sim-
ilarity should be measured on the sense level. We
analyze state-of-the-art methods and describe how
the word-based Explicit Semantic Analysis (ESA)
measure (Gabrilovich and Markovitch, 2007) can
be transformed into a sense-level measure. We
create a sense similarity dataset, where senses are
clearly defined and evaluate similarity measures
with this novel dataset. We also re-annotate an ex-
isting word-level dataset on the sense level in order
to study the impact of sense-level computation of
similarity.
2 Word-level vs. Sense-level Similarity
Existing measures either compute similarity (i) on
the word level or (ii) on the sense level. Similarity
on the word level may cover any possible sense of
the word, where on the sense level only the actual
sense is considered. We use Wikipedia Link Mea-
1
If you knew that it is a certain sign that you are getting
old.
30
Atari Jaguar Jaguar (animal)
Gamepad
Zoo
.0000.0321 .0341.0000
.0000
Figure 2: Similarity between senses.
sure (Milne, 2007) and Lin (Lin, 1998) as exam-
ples of sense-level similarity measures
2
and ESA
as the prototypical word-level measure.
3
The Lin measure is a widely used graph-based
similarity measure from a family of similar ap-
proaches (Budanitsky and Hirst, 2006; Seco et al.,
2004; Banerjee and Pedersen, 2002; Resnik, 1999;
Jiang and Conrath, 1997; Grefenstette, 1992). It
computes the similarity between two senses based
on the information content (IC) of the lowest com-
mon subsumer (lcs) and both senses (see For-
mula 1).
sim
lin
=
2 IC(lcs)
IC(sense1) + IC(sense2)
(1)
Another type of sense-level similarity measure
is based on Wikipedia that can also be considered a
sense inventory, similar to WordNet. Milne (2007)
uses the link structure obtained from articles to
count the number of shared incoming links of ar-
ticles. Milne and Witten (2008) give a more effi-
cient variation for computing similarity (see For-
mula 2) based on the number of links for each ar-
ticle, shared links |A ?B| and the total number of
articles in Wikipedia|W |.
sim
LM
=
logmax(|A| ,|B|)? log|A ?B|
log|W | ? logmin(|A| ,|B|)
(2)
All sense-level similarity measures can be con-
verted into a word similarity measure by comput-
ing the maximum similarity between all possible
sense pairs. Formula 3 shows the heuristic, with
S
n
being the possible senses for word n, sim
w
the
word similarity, and sim
s
the sense similarity.
sim
w
(w
1
, w
2
) = max
s
1
?S
1
,s
2
?S
2
sim
s
(s1, s2) (3)
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007) is a widely used word-level
2
We selected these measures because they are intuitive but
still among the best performing measures.
3
Hassan and Mihalcea (2011) classify these measures as
corpus-based and knowledge-based.
similarity measure based on Wikipedia as a back-
ground document collection. ESA constructs a n-
dimensional space, where n is the number of arti-
cles in Wikipedia. A word is transformed in a vec-
tor with the length n. Values of the vector are de-
termined by the term frequency in the correspond-
ing dimension, i.e. in a certain Wikipedia article.
The similarity of two words is then computed as
the inner product (usually the cosine) of the two
word vectors.
We now show how ESA can be adapted success-
fully to work on the sense-level, too.
2.1 DESA: Disambiguated ESA
In the standard definintion, ESA computes the
term frequency based on the number of times a
term?usually a word?appears in a document. In
order to make it work on the sense level, we will
need a large sense-disambiguated corpus. Such
a corpus could be obtained by performing word
sense disambiguating (Agirre and Edmonds, 2006;
Navigli, 2009) on all words. However, as this
is an error-prone task and we are more inter-
ested to showcase the overall principle, we rely on
Wikipedia as an already manually disambiguated
corpus. Wikipedia is a highly linked resource and
articles can be considered as senses.
4
We ex-
tract all links from all articles, with the link tar-
get as the term. This approach is not restricted
to Wikipedia, but can be applied to any resource
containing connections between articles, such as
Wiktionary (Meyer and Gurevych, 2012b). An-
other reason to select Wikipedia as a corpus is that
it will allow us to directly compare similarity val-
ues with the Wikipedia Link Measure as described
above.
After this more high-level introduction, we now
focus on the mathematical foundation of ESA and
disambiguated ESA (called ESA on senses). ESA
and ESA on senses count the frequency of each
term (or sense) in each document. Table 1 shows
the corresponding term-document matrix for the
example in Figure 1. The term Jaguar appears in
all shown documents, but the term Zoo appears in
the articles Dublin Zoo and Wildlife Park.
5
A man-
ual analysis shows that Jaguar appears with differ-
ent senses in the articles D-pad
6
and Dublin Zoo.
4
Wikipedia also contains pages with a list of possible
senses called disambiguation pages, which we filter.
5
In total it appears in 30 articles but we shown only few
example articles.
6
A D-pad is a directional pad for playing computer games.
31
Articles Terms
Jaguar Gamepad Zoo
# articles 3,496 30 7,553
Dublin Zoo 1 0 25
Wildlife Park 1 0 3
D-pad 1 0 0
Gamepad 4 1 0
... ... ... ...
Table 1: Term-document-matrix for frequencies in
a corpus if words are used as terms
Articles Terms
Atari
Gamepad
Jaguar
Zoo
Jaguar (animal)
# articles 156 86 578 925
Dublin Zoo 0 0 2 1
Wildlife Park 0 0 1 1
D-pad 1 1 0 0
Gamepad 1 0 0 0
... ... ... ... ...
Table 2: Term-document-matrix for frequencies in
a corpus if senses are used as terms
By comparing the vectors without any modifi-
cation, we see that the word pairs Jaguar?Zoo
and Jaguar?Gamepad have vector entries for the
same document, thus leading to a non-zero simi-
larity. Vectors for the terms Gamepad and Zoo do
not share any documents, thus leading to a simi-
larity of zero.
Shifting from words to senses changes term fre-
quencies in the term-document-matrix in Table 2.
The word Jaguar is split in the senses Atari Jaguar
and Jaguar (animal). Overall, the term-document-
matrix for the sense-based similarity shows lower
frequencies, usually zero or one because in most
cases one article does not link to another article or
exactly once. Both senses of Jaguar do not appear
in the same document, hence, their vectors are or-
thogonal. The vector for the term Gamepad dif-
fers from the vector for the same term in Table 1.
This is due to two effects: (i) There is no link from
the article Gamepad to itself, but the term is men-
tioned in the article and (ii) there exists a link from
the article D-pad to Gamepad, but using another
term.
The term-document-matrices in Table 1 and 2
show unmodified frequencies of the terms. When
comparing two vectors, both are normalized in a
prior step. Values can be normalized by the inverse
logarithm of their document frequency. Term fre-
quencies can also be normalized by weighting
them with the inverse frequency of links pointing
to an article (document or articles with many links
pointing to them receive lower weights as docu-
ments with only few incoming links.) We normal-
ize vector values with the inverse logarithm of ar-
ticle frequencies.
Besides comparing two vectors by measuring
the angle between them (cosine), we also experi-
ment with a language model variant. In the lan-
guage model variant we calculate for both vec-
tors the ratio of links they both share. The fi-
nal similarity value is the average for both vec-
tors. This is somewhat similar to the approach of
Wikipedia Link Measure by Milne (2007). Both
rely on Wikipedia links and are based on frequen-
cies of these links. We show that?although, ESA
and Link Measure seem to be very different?they
both share a general idea and are identical with a
certain configuration.
2.2 Relation to the Wikipedia Link Measure
Link Measure counts the number of incoming
links to both articles and the number of shared
links. In the originally presented formula by Milne
(2007) the similarity is the cosine of vectors for
incoming or outgoing links from both articles. In-
coming links are also shown in term-document-
matrices in Table 1 and 2, thus providing the same
vector information. In Milne (2007), vector values
are weighted by the frequency of each link normal-
ized by the logarithmic inverse frequency of links
pointing to the target. This is one of the earlier de-
scribed normalization approaches. Thus, we argue
that the Wikipedia Link Measure is a special case
of our more general ESA on senses approach.
3 Annotation Study I: Rating Sense
Similarity
We argue that human judgment of similarity be-
tween words is influenced by the most probable
sense. We create a dataset with ambiguous terms
and ask annotators to rank the similarity of senses
and evaluate similarity measures with the novel
dataset.
3.1 Constructing an Ambiguous Dataset
In this section, we discuss how an evaluation
dataset should be constructed in order to correctly
asses the similarity of two senses. Typically, eval-
uation datasets for word similarity are constructed
by letting annotators rate the similarity between
32
both words without specifying any senses for these
words. It is common understanding that anno-
tators judge the similarity of the combination of
senses with the highest similarity.
We investigate this hypothesis by constructing
a new dataset consisting of 105 ambiguous word
pairs. Word pairs are constructed by adding one
word with two clearly distinct senses and a second
word, which has a high similarity to only one of
the senses. We first ask two annotators
7
to rate the
word pairs on a scale from 0 (not similar at all) to 4
(almost identical). In the second round, we ask the
same annotators to rate 277 sense
8
pairs for these
word pairs using the same scale.
The final dataset thus consists of two levels:
(i) word similarity ratings and (ii) sense similarity
ratings. The gold ratings are the averaged ratings
of both annotators, resulting in an agreement
9
of
.510 (Spearman: .598) for word ratings and .792
(Spearman: .806) for sense ratings.
Table 3 shows ratings of both annotators for two
word pairs and ratings for all sense combinations.
In the given example, the word bass has the senses
of the fish, the instrument, and the sound. Anno-
tators compare the words and senses to the words
Fish and Horn, which appear only in one sense
(most frequent sense) in the dataset.
The annotators? rankings contradict the assump-
tion that the word similarity equals the similar-
ity of the highest sense. Instead, the highest
sense similarity rating is higher than the word
similarity rating. This may be caused?among
others?by two effects: (i) the correct sense is not
known or not recalled, or (ii) the annotators (un-
consciously) adjust their ratings to the probabil-
ity of the sense. Although, the annotation manual
stated that Wikipedia (the source of the senses)
could be used to get informed about senses and
that any sense for the words can be selected, we
see both effects in the annotators? ratings. Both
annotators rated the similarity between Bass and
Fish as very low (1 and 2). However, when asked
to rate the similarity between the sense Bass (Fish)
and Fish, both annotators rated the similarity as
high (4). Accordingly, for the word pair Bass and
7
Annotators are near-native speakers of English and have
university degrees in cultural anthropology and computer sci-
ence.
8
The sense of a word is given in parentheses but annota-
tors have access to Wikipedia to get information about those
senses.
9
We report agreement as Krippendorf ? with a quadratic
weight function.
Horn, word similarity is low (1) while the highest
sense frequency is medium to high (3 and 4).
3.2 Results & Discussion
We evaluated similarity measures with the previ-
ously created new dataset. Table 4 shows corre-
lations of similarity measures with human ratings.
We divide the table into measures computing sim-
ilarity on word level and on sense level. ESA
works entirely on a word level, Lin (WordNet)
uses WordNet as a sense inventory, which means
that senses differ across sense inventories.
10
ESA
on senses and Wikipedia Link Measure (WLM)
compute similarity on a sense-level, however, sim-
ilarity on a word-level is computed by taking the
maximum similarity of all possible sense pairs.
Results in Table 4 show that word-level mea-
sures return the same rating independent from the
sense being used, thus, they perform good when
evaluated on a word-level, but perform poorly
on a sense-level. For the word pair Jaguar?
Zoo, there exist two sense pairs Atari Jaguar?
Zoo and Jaguar (animal)?Zoo. Word-level mea-
sures return the same similarity, thus leading to
a very low correlation. This was expected, as
only sense-based similarity measures can discrim-
inate between different senses of the same word.
Somewhat surprisingly, sense-level measures per-
form also well on a word-level, but their per-
formance increases strongly on sense-level. Our
novel measure ESA on senses provides the best
results. This is expected as the ambiguous dataset
contains many infrequently used senses, which an-
notators are not aware of.
Our analysis shows that the algorithm for com-
paring two vectors (i.e. cosine and language
model) only influences results for ESA on senses
when computed on a word-level. Correlation for
Wikipedia Link Measure (WLM) differs depend-
ing on whether the overlap of incoming or outgo-
ing links are computed. WLM on word-level using
incoming links performs better, while the differ-
ence on sense-level evaluation is only marginal.
Results show that an evaluation on the level of
words and senses may influence performance of
measures strongly.
3.3 Pair-wise Evaluation
In a second experiment, we evaluate how well
sense-based measures can decide, which one of
10
Although, there exists sense alignment resources, we did
not use any alignment.
33
Annotator 1 Annotator 2
Word 1 Word 2 Sense 1 Sense 2 Words Senses Words Senses
Bass Fish
Bass (Fish)
Fish (Animal) 1
4
1
4
Bass (Instrument) 1 1
Bass (Sound) 1 1
Bass Horn
Bass (Fish)
Horn (Instrument) 2
1
1
1
Bass (Instrument) 3 4
Bass (Sound) 3 3
Table 3: Examples of ratings for two word pairs and all sense combinations with the highest ratings
marked bold
Word-level Sense-level
measure Spearman Pearson Spearman Pearson
Word measures
ESA .456 .239 -.001 .017
Lin (WordNet) .298 .275 .038 .016
Sense measures
ESA on senses (Cosine) .292 .272 .642 .348
ESA on senses (Lang. Mod.) .185 .256 .642 .482
WLM (out) .190 .193 .537 .372
WLM (in) .287 .279 .535 .395
Table 4: Correlation of similarity measures with a human gold standard of ambiguous word pairs.
two sense pairs for one word pair have a higher
similarity. We thus create for every word pair all
possible sense pairs
11
and count cases where one
measure correctly decides, which is the sense pair
with a higher similarity.
Table 5 shows evaluation results based on a
minimal difference between two sense pairs. We
removed all sense pairs with a lower difference
of their gold similarity. Column #pairs gives the
number of remaining sense pairs. If a measure
classifies two sense pairs wrongly, it may either
be because it rated the sense pairs with an equal
similarity or because it reversed the order.
Results show that accuracy increases with in-
creasing minimum difference between sense pairs.
Figure 3 emphasizes this finding. Overall, accu-
racy for this task is high (between .70 and .83),
which shows that all the measures can discrim-
inate sense pairs. WLM (out) performs best for
most cases with a difference in accuracy of up to
.06.
When comparing these results to results from
Table 4, we see that correlation does not imply
accurate discrimination of sense pairs. Although,
ESA on senses has the highest correlation to hu-
man ratings, it is outperformed by WLM (out) on
the task of discriminating two sense pairs. We see
that results are not stable across both evaluation
11
For one word pair with two senses for one word, there are
two possible sense pairs. Three senses result in three sense
pairs.
0.5 1 1.5 2 2.5 3 3.5 4
0.7
0.75
0.8
0.85
0.9
Min. judgement difference
A
c
c
u
r
a
c
y
ESA on senses
WLM (in)
WLM (out)
Figure 3: Accuracy distribution depending on
minimum difference of similarity ratings
scenarios, however, ESA on senses achieves the
highest correlation and performs similar to WLM
(out) when comparing sense pairs pair-wise.
4 Annotation Study II: Re-rating of
RG65
We performed a second evaluation study where we
asked three human annotators
12
to rate the similar-
ity of word-level pairs in the dataset by Rubenstein
and Goodenough (1965). We hypothesize that
measures working on the sense-level should have a
disadvantage on word-level annotated datasets due
to the effects described above that influence anno-
tators towards frequent senses. In our annotation
12
As before, all three annotators are near-native speakers of
English and have a university degree in physics, engineering,
and computer science.
34
Min. Wrong
diff. #pairs measure Correct Reverse Values equal Accuracy
0.5
420
ESA on senses 296 44 80 .70
WLM (in) 296 62 62 .70
WLM (out) 310 76 34 .74
1.0
390
ESA on senses 286 38 66 .73
WLM (in) 282 52 56 .72
WLM (out) 294 64 32 .75
1.5
360
ESA on senses 264 34 62 .73
WLM (in) 260 48 52 .72
WLM (out) 280 54 26 .78
2.0
308
ESA on senses 232 28 48 .75
WLM (in) 226 36 46 .73
WLM (out) 244 46 18 .79
2.5
280
ESA on senses 216 22 42 .77
WLM (in) 206 32 42 .74
WLM (out) 224 38 18 .80
3.0
174
ESA on senses 134 10 30 .77
WLM (in) 128 20 26 .74
WLM (out) 136 22 16 .78
3.50
68
ESA on senses 56 4 8 .82
WLM (in) 50 6 12 .74
WLM (out) 52 6 10 .76
4.0
12
ESA on senses 10 2 0 .83
WLM (in) 10 2 0 .83
WLM (out) 10 2 0 .83
Table 5: Pair-wise comparison of measures: Results for ESA on senses (language model) and ESA on
senses (cosine) do not differ
studies, our aim is to minimize the effect of sense
weights.
In previous annotation studies, human annota-
tors could take sense weights into account when
judging the similarity of word pairs. Addition-
ally, some senses might not be known by anno-
tators and, thus receive a lower rating. We min-
imize these effects by asking annotators to select
the best sense for a word based on a short summary
of the corresponding sense. To mimic this pro-
cess, we created an annotation tool (see Figure 4),
for which an annotator first selects senses for both
words, which have the highest similarity. Then the
annotator ranks the similarity of these sense pairs
based on the complete sense definition.
A single word without any context cannot be
disambiguated properly. However, when word
pairs are given, annotators first select senses based
on the second word, e.g. if the word pair is Jaguar
and Zoo, an annotator will select the wild animal
for Jaguar. After disambiguating, an annotator
assigns a similarity score based on both selected
senses. To facilitate this process, a definition of
each possible sense is shown.
As in the previous experiment, similarity is an-
notated on a five-point-scale from 0 to 4. Al-
though, we ask annotators to select senses for
word pairs, we retrieve only one similarity rating
for each word pair, which is the sense combination
with the highest similarity.
No sense inventory To compare our results with
the original dataset from Rubenstein and Goode-
nough (1965), we asked annotators to rate similar-
ity of word pairs without any given sense reposi-
tory, i.e. comparing words directly. The annota-
tors reached an agreement of .73. The resulting
gold standard has a high correlation with the orig-
inal dataset (.923 Spearman and .938 Pearson).
This is in line with our expectations and previous
work that similarity ratings are stable across time
(B?ar et al., 2011).
Wikipedia sense inventory We now use the full
functionality of our annotation tool and ask an-
notators to first, select senses for each word and
second, rate the similarity. Possible senses and
definitions for these senses are extracted from
Wikipedia.
13
The same three annotators reached
13
We use the English Wikipedia version from June 15
th
,
2010.
35
Figure 4: User interface for annotation studies: The example shows the word pair glass?tumbler with
no senses selected. The interface shows WordNet definitons of possible senses in the text field below the
sense selection. The highest similarity is selected as sense 4496872 for tumbler is a drinking glass.
an agreement of .66. The correlation to the orig-
inal dataset is lower than for the re-rating (.881
Spearman, .896 Pearson). This effect is due
to many entities in Wikipedia, which annotators
would typically not know. Two annotators rated
the word pair graveyard?madhouse with a rather
high similarity because both are names of music
bands (still no very high similarity because one is
a rock and the other a jazz band).
WordNet sense inventory Similar to the previ-
ous experiment, we list possible senses for each
word from a sense inventory. In this experiment,
we use WordNet senses, thus, not using any named
entity. The annotators reached an agreement of .73
and the resulting gold standard has a high correla-
tion with the original dataset (.917 Spearman and
.928 Pearson).
Figure 5 shows average annotator ratings in
comparison to similarity judgments in the origi-
nal dataset. All re-rating studies follow the general
tendency of having higher annotator judgments for
similar pairs. However, there is a strong fluctua-
tion in the mid-similarity area (1 to 3). This is due
to fewer word pairs with such a similarity.
4.1 Results & Discussion
We evaluate the similarity measures using Spear-
man and Pearson correlation with human similar-
0 1 2 3 4
0
2
4
Original similarity
S
i
m
i
l
a
r
i
t
y
j
u
d
g
e
m
e
n
t
s
None
Wikipedia
WordNet
Figure 5: Correlation curve of rerating studies
ity judgments. We calculate correlations to four
human judgments: (i) from the original dataset
(Orig.), (ii) from our re-rating study (Rerat.), (iii)
from our study with senses from Wikipedia (WP),
and (iv) with senses from WordNet (WN). Ta-
ble 6 shows results for all described similarity
measures.
ESA
14
achieves a Spearman correlation of .751
and a slightly higher correlation (.765) on our
re-rating gold standard. Correlation then drops
when compared to gold standards with senses
from Wikipedia and WordNet. This is expected
as the gold standard becomes more sense-aware.
Lin is based on senses in WordNet but still out-
14
ESA is used with normalized text frequencies, a constant
document frequency, and a cosine comparison of vectors.
36
Spearman Pearson
measure Orig. Rerat. WP WN Orig. Rerat. WP WN
ESA .751 .765 .704 .705 .647 .694 .678 .625
Lin .815 .768 .705 .775 .873 .840 .798 .846
ESA on senses (lang. mod.) .733 .765 .782 .751 .703 .739 .739 .695
ESA on senses (cosine) .775 .810 .826 .795 .694 .712 .736 .699
WLM (in) .716 .745 .754 .733 .708 .712 .740 .707
WLM (out) .583 .607 .652 .599 .548 .583 .613 .568
Table 6: Correlation of similarity measures with a human gold standard on the word pairs by Rubenstein
and Goodenough (1965). Best results for each gold standard are marked bold.
performs all other measures on the original gold
standard. Correlation reaches a high value for
the gold standard based on WordNet, as the same
sense inventory for human annotations and mea-
sure is applied. Values for Pearson correlation em-
phasizes this effect: Lin reaches the maximum of
.846 on the WordNet-based gold standard.
Correspondingly, the similarity measures ESA
on senses and WLM reach their maximum on
the Wikipedia-based gold standard. As for the
ambiguous dataset in Section 3 ESA on senses
outperforms both WLM variants. Cosine vector
comparison again outperforms the language model
variant for Spearman correlation but impairs it in
terms of Pearson correlation. As before WLM (in)
outperforms WLM (out) across all datasets and
both correlation metrics.
Is word similarity sense-dependent? In gen-
eral, sense-level similarity measures improve
when evaluated with a sense-aware gold standard,
while correlation with word-level similarity mea-
sures decreases. A further manual analysis shows
that sense-level measures perform good when rat-
ing very similar word pairs. This is very useful for
applications such as information retrieval where a
user is only interested in very similar documents.
Our evaluation thus shows that word similar-
ity should not be considered without considering
the effect of the used sense inventory. The same
annotators rate word pairs differently if they can
specify senses explicitly (as seen in Table 3). Cor-
respondingly, results for similarity measures de-
pend on which senses can be selected. Wikipedia
contains many entities, e.g. music bands or ac-
tors, while WordNet contains fine-grained senses
for things (e.g. narrow senses of glass as shown in
Figure 4). Using the same sense inventory as the
one, which has been used in the annotation pro-
cess, leads to a higher correlation.
5 Related Work
The work by Schwartz and Gomez (2011) is the
closest to our approach in terms of sense anno-
tated datasets. They compare several sense-level
similarity measures based on the WordNet taxon-
omy on sense-annotated datasets. For their ex-
periments, annotators were asked to select senses
for every word pair in three similarity datasets.
Annotators were not asked to re-rate the similar-
ity of the word pairs, or the sense pairs, respec-
tively. Instead, similarity judgments from the orig-
inal datasets are used. Possible senses are given by
WordNet and the authors report an inter-annotator
agreement of .93 for the RG dataset.
The authors then compare Spearman correlation
between human judgments and judgments from
WordNet-based similarity measures. They focus
on differences between similarity measures using
the sense annotations and the maximum value for
all possible senses. The authors do not report im-
provements across all measures and datasets. Of
ten measures and three datasets, using sense an-
notations, improved results in nine cases. In 16
cases, results are higher when using the maxi-
mum similarity across all possible senses. In five
cases, both measures yielded an equal correlation.
The authors do not report any overall tendency
of results. However, these experiments show that
switching from words to senses has an effect on
the performance of similarity measures.
The work by Hassan and Mihalcea (2011) is
the closest to our approach in terms of similarity
measures. They introduce Salient Semantic Anal-
ysis (SAS), which is a sense-level measure based
on links and disambiguated senses in Wikipedia
articles. They create a word-sense-matrix and
37
compute similarity with a modified cosine met-
ric. However, they apply additional normaliza-
tion factors to optimize for the evaluation metrics
which makes a direct comparison of word-level
and sense-level variants difficult.
Meyer and Gurevych (2012a) analyze verb sim-
ilarity with a corpus from Yang and Powers
(2006) based on the work by Zesch et al. (2008).
They apply variations of the similarity measure
ESA by Gabrilovich and Markovitch (2007) us-
ing Wikipedia, Wiktionary, and WordNet. Meyer
and Gurevych (2012a) report improvements us-
ing a disambiguated version of Wiktionary. Links
in Wiktionary articles are disambiguated and thus
transform the resource to a sense-based resource.
In contrast to our work, they focus on the simi-
larity of verbs (in comparison to nouns in this pa-
per) and it applies disambiguation to improve the
underlying resource, while we switch the level,
which is processed by the measure to senses.
Shirakawa et al. (2013) apply ESA for compu-
tation of similarities between short texts. Texts
are extended with Wikipedia articles, which is one
step to a disambiguation of the input text. They
report an improvement of the sense-extended ESA
approach over the original version of ESA. In con-
trast to our work, the text itself is not changed and
similarity is computed on the level of texts.
6 Summary and Future Work
In this work, we investigated word-level and
sense-level similarity measures and investigated
their strengths and shortcomings. We evaluated
how correlations of similarity measures with a
gold standard depend on the sense inventory used
by the annotators.
We compared the similarity measures ESA
(corpus-based), Lin (WordNet), and Wikipedia
Link Measure (Wikipedia), and a sense-enabled
version of ESA and evaluated them with a dataset
containing ambiguous terms. Word-level mea-
sures were not able to differentiate between dif-
ferent senses of one word, while sense-level mea-
sures could even increase correlation when shift-
ing to sense similarities. Sense-level measures ob-
tained accuracies between .70 and .83 when decid-
ing which of two sense pairs has a higher similar-
ity.
We performed re-rating studies with three an-
notators based on the dataset by Rubenstein and
Goodenough (1965). Annotators were asked to
first annotate senses from Wikipedia and Word-
Net for word pairs and then judge their similar-
ity based on the selected senses. We evaluated
with these new human gold standards and found
that correlation heavily depends on the resource
used by the similarity measure and sense reposi-
tory a human annotator selected. Sense-level sim-
ilarity measures improve when evaluated with a
sense-aware gold standard, while correlation with
word-level similarity measures decreases. Using
the same sense inventory as the one, which has
been used in the annotation process, leads to a
higher correlation. This has implications for cre-
ating word similarity datasets and evaluating sim-
ilarity measures using different sense inventories.
In future work we would like to analyze how
we can improve sense-level similarity measures by
disambiguating a large document collection and
thus retrieving more accurate frequency values.
This might reduce the sparsity of term-document-
matrices for ESA on senses. We plan to use
word sense disambiguation components as a pre-
processing step to evaluate whether sense simi-
larity measures improve results for text similarity.
Additionally, we plan to use sense alignments be-
tween WordNet and Wikipedia to enrich the term-
document matrix with additional links based on
semantic relations.
The datasets, annotation guidelines, and our ex-
perimental framework are publicly available in or-
der to foster future research for computing sense
similarity.
15
Acknowledgments
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
by the Klaus Tschira Foundation under project No.
00.133.2008, and by the German Federal Min-
istry of Education and Research (BMBF) within
the context of the Software Campus project open
window under grant No. 01IS12054. The au-
thors assume responsibility for the content. We
thank Pedro Santos, Mich`ele Spankus and Markus
B?ucker for their valuable contribution. We thank
the anonymous reviewers for their helpful com-
ments.
15
www.ukp.tu-darmstadt.de/data/
text-similarity/sense-similarity/
38
References
Eneko Agirre and Philip Edmonds. 2006. Word
Sense Disambiguation: Algorithms and Applica-
tions. Springer.
Satanjeev Banerjee and Ted Pedersen. 2002. An
Adapted Lesk Algorithm for Word Sense Disam-
biguation using WordNet. In Computational Lin-
guistics and Intelligent Text, pages 136?-145.
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2011.
A Reflective View on Text Similarity. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 515?
520, Hissar, Bulgaria.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, pages 1606?1611.
Gregory Grefenstette. 1992. Sextant: Exploring Unex-
plored Contexts for Semantic Extraction from Syn-
tactic Analysis. In Proceedings of the 30th An-
nual Meeting of the Association for Computational
Linguistics, pages 324?-326, Newark, Delaware,
USA. Association for Computational Linguistics.
Samer Hassan and Rada Mihalcea. 2011. Semantic
Relatedness Using Salient Semantic Analysis. In
Proceedings of the 25th AAAI Conference on Artifi-
cial Intelligence, (AAAI 2011), pages 884?889, San
Francisco, CA, USA.
Jay J Jiang and David W Conrath. 1997. Seman-
tic Similarity based on Corpus Statistics and Lexi-
cal Taxonomy. In Proceedings of 10th International
Conference Research on Computational Linguistics,
pages 1?15.
Dekang Lin. 1998. An Information-theoretic Defini-
tion of Similarity. In In Proceedings of the Interna-
tional Conference on Machine Learning, volume 98,
pages 296?-304.
Christian M. Meyer and Iryna Gurevych. 2012a. To
Exhibit is not to Loiter: A Multilingual, Sense-
Disambiguated Wiktionary for Measuring Verb Sim-
ilarity. In Proceedings of the 24th International
Conference on Computational Linguistics, pages
1763?1780, Mumbai, India.
Christian M. Meyer and Iryna Gurevych. 2012b. Wik-
tionary: A new rival for expert-built lexicons? Ex-
ploring the possibilities of collaborative lexicogra-
phy. In Sylviane Granger and Magali Paquot, ed-
itors, Electronic Lexicography, chapter 13, pages
259?291. Oxford University Press, Oxford, UK,
November.
David Milne and Ian H Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of the 17th ACM
Conference on Information and Knowledge Man-
agement, pages 509?-518.
David Milne. 2007. Computing Semantic Relatedness
using Wikipedia Link Structure. In Proceedings of
the New Zealand Computer Science Research Stu-
dent Conference.
Roberto Navigli. 2009. Word Sense Disambiguation:
A Survey. ACM Computing Surveys, 41(2):1?69.
Philip Resnik. 1999. Semantic Similarity in a Tax-
onomy: An Information-based Measure and its Ap-
plication to Problems of Ambiguity in Natural Lan-
guage. Journal of Artificial Intelligence Research,
11:95?130.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual Correlates of Synonymy. Communica-
tions of the ACM, 8(10):627?-633.
Hansen A Schwartz and Fernando Gomez. 2011. Eval-
uating Semantic Metrics on Tasks of Concept Simi-
larity. In FLAIRS Conference.
Nuno Seco, Tony Veale, and Jer Hayes. 2004. An
Intrinsic Information Content Metric for Semantic
Similarity in WordNet. In Proceedings of European
Conference for Artificial Intelligence, number Ic,
pages 1089?1093.
Masumi Shirakawa, Kotaro Nakayama, Takahiro Hara,
and Shojiro Nishio. 2013. Probabilistic Seman-
tic Similarity Measurements for Noisy Short Texts
using Wikipedia Entities. In Proceedings of the
22nd ACM International Conference on Information
& Knowledge Management, pages 903?908, New
York, New York, USA. ACM Press.
Dongqiang Yang and David MW Powers. 2006. Verb
Similarity on the Taxonomy of WordNet. In Pro-
ceedings of GWC-06, pages 121?-128.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, pages
861?867, Chicago, IL, USA.
39
